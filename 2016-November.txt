From jdnewmil at dcn.davis.ca.us  Tue Nov  1 00:34:10 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 31 Oct 2016 16:34:10 -0700
Subject: [R] function ave() with seq_along returning char sequence
	instead of numeric
In-Reply-To: <F7E6D18CC2877149AB5296CE54EA276635B9A7CC@WAXMXOLYMB025.WAX.wa.lcl>
References: <F7E6D18CC2877149AB5296CE54EA276635B9A7CC@WAXMXOLYMB025.WAX.wa.lcl>
Message-ID: <95806726-F808-43B1-9AC7-5A8C59AAAF6F@dcn.davis.ca.us>

The help page describes the first argument x as a numeric... it is not designed to accept character, so the fact that you get anything even close to right is just a bonus.

As the doctor says, "if it hurts, don't do that".

ave( rep( 1, length( v ), v, FUN=seq_along )
-- 
Sent from my phone. Please excuse my brevity.

On October 31, 2016 3:29:01 PM PDT, "Nordlund, Dan (DSHS/RDA)" <NordlDJ at dshs.wa.gov> wrote:
>Given  the following R statements
>
>v <-  c('a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'c', 'c')
>ave(v, list(v), FUN=seq_along)
> [1] "1" "2" "3" "1" "2" "3" "1" "2" "3" "4" 
>
>I was expecting to get a numeric vector back.  I apparently have missed
>something in the documentation.  If vector v is character, then the
>numeric sequence is converted to character before returning.  I can
>work around by doing something like
>
>ave(seq_along(v), list(v), FUN=seq_along)
> [1] 1 2 3 1 2 3 1 2 3 4 
>
>Is the work around the best way to go, or have I missed an option in
>the documentation?
>
>
>Thanks,
>
>Dan
>
>Daniel Nordlund, PhD
>Research and Data Analysis Division
>Services & Enterprise Support Administration
>Washington State Department of Social and Health Services
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From esg at unife.it  Tue Nov  1 00:49:06 2016
From: esg at unife.it (Josef Eschgfaeller)
Date: Tue, 1 Nov 2016 00:49:06 +0100
Subject: [R] bzip2
Message-ID: <CAFBvUatx5vUMgJeumNo1YuQ6S9rDJ+GSqRG+XGJRkTp3V4Em4g@mail.gmail.com>

I am not able to compile R 3.3,
configure halting with:
--------------------------------------------------
checking for BZ2_bzlibVersion in -lbz2... yes
checking bzlib.h usability... yes
checking bzlib.h presence... yes
checking for bzlib.h... yes
checking if bzip2 version >= 1.0.6... no
checking whether bzip2 support suffices... configure: error:
                         bzip2 library and headers are required
--------------------------------------------------
But my bzip2 seems to be 1.0.6:

bzip2 --version
bzip2, a block-sorting file compressor.  Version 1.0.6, 6-Sept-2010.
type bzip2
bzip2 is hashed (/opt/local/bin/bzip2)
type R
R is hashed (/usr/bin/R)
R --version
R version 3.2.0 (2015-04-16) -- "Full of Ingredients"
--------------------------------------------------
Thanks for help

Josef Eschgfaeller


From NordlDJ at dshs.wa.gov  Tue Nov  1 01:11:47 2016
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS/RDA))
Date: Tue, 1 Nov 2016 00:11:47 +0000
Subject: [R] function ave() with seq_along returning char sequence
 instead of numeric
In-Reply-To: <95806726-F808-43B1-9AC7-5A8C59AAAF6F@dcn.davis.ca.us>
References: <F7E6D18CC2877149AB5296CE54EA276635B9A7CC@WAXMXOLYMB025.WAX.wa.lcl>
	<95806726-F808-43B1-9AC7-5A8C59AAAF6F@dcn.davis.ca.us>
Message-ID: <F7E6D18CC2877149AB5296CE54EA276635B9A899@WAXMXOLYMB025.WAX.wa.lcl>

Jeff,

Thanks for the response.  You are right of course. I went back and reread the help page.  I guess I just glossed over the 'x is numeric' statement because the default FUN was the mean, and seq_along doesn't care about type.  I should know better than to disregard what I read in the help pages.  But you are right, using any numeric vector of appropriate size as the first argument does what I need.

Dan

Daniel Nordlund, PhD
Research and Data Analysis Division
Services & Enterprise Support Administration
Washington State Department of Social and Health Services


> -----Original Message-----
> From: Jeff Newmiller [mailto:jdnewmil at dcn.davis.ca.us]
> Sent: Monday, October 31, 2016 4:34 PM
> To: Nordlund, Dan (DSHS/RDA); r-help at r-project.org
> Subject: Re: [R] function ave() with seq_along returning char sequence
> instead of numeric
> 
> The help page describes the first argument x as a numeric... it is not
> designed to accept character, so the fact that you get anything even close to
> right is just a bonus.
> 
> As the doctor says, "if it hurts, don't do that".
> 
> ave( rep( 1, length( v ), v, FUN=seq_along )
> --
> Sent from my phone. Please excuse my brevity.
> 
> On October 31, 2016 3:29:01 PM PDT, "Nordlund, Dan (DSHS/RDA)"
> <NordlDJ at dshs.wa.gov> wrote:
> >Given  the following R statements
> >
> >v <-  c('a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'c', 'c') ave(v,
> >list(v), FUN=seq_along)  [1] "1" "2" "3" "1" "2" "3" "1" "2" "3" "4"
> >
> >I was expecting to get a numeric vector back.  I apparently have missed
> >something in the documentation.  If vector v is character, then the
> >numeric sequence is converted to character before returning.  I can
> >work around by doing something like
> >
> >ave(seq_along(v), list(v), FUN=seq_along)  [1] 1 2 3 1 2 3 1 2 3 4
> >
> >Is the work around the best way to go, or have I missed an option in
> >the documentation?
> >
> >
> >Thanks,
> >
> >Dan
> >
> >Daniel Nordlund, PhD
> >Research and Data Analysis Division
> >Services & Enterprise Support Administration Washington State
> >Department of Social and Health Services
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.


From NordlDJ at dshs.wa.gov  Tue Nov  1 01:29:04 2016
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS/RDA))
Date: Tue, 1 Nov 2016 00:29:04 +0000
Subject: [R] bzip2
In-Reply-To: <CAFBvUatx5vUMgJeumNo1YuQ6S9rDJ+GSqRG+XGJRkTp3V4Em4g@mail.gmail.com>
References: <CAFBvUatx5vUMgJeumNo1YuQ6S9rDJ+GSqRG+XGJRkTp3V4Em4g@mail.gmail.com>
Message-ID: <F7E6D18CC2877149AB5296CE54EA276635B9A8B2@WAXMXOLYMB025.WAX.wa.lcl>

I don't know what version Linux or other OS you are using, but have you installed the bzip2 development package? It would be named something like  libbz2-dev (that is what it is in Ubuntu, I believe).

Dan

Daniel Nordlund, PhD
Research and Data Analysis Division
Services & Enterprise Support Administration
Washington State Department of Social and Health Services


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Josef
> Eschgfaeller
> Sent: Monday, October 31, 2016 4:49 PM
> To: r-help
> Subject: [R] bzip2
> 
> I am not able to compile R 3.3,
> configure halting with:
> --------------------------------------------------
> checking for BZ2_bzlibVersion in -lbz2... yes checking bzlib.h usability... yes
> checking bzlib.h presence... yes checking for bzlib.h... yes checking if bzip2
> version >= 1.0.6... no checking whether bzip2 support suffices... configure:
> error:
>                          bzip2 library and headers are required
> --------------------------------------------------
> But my bzip2 seems to be 1.0.6:
> 
> bzip2 --version
> bzip2, a block-sorting file compressor.  Version 1.0.6, 6-Sept-2010.
> type bzip2
> bzip2 is hashed (/opt/local/bin/bzip2)
> type R
> R is hashed (/usr/bin/R)
> R --version
> R version 3.2.0 (2015-04-16) -- "Full of Ingredients"
> --------------------------------------------------
> Thanks for help
> 
> Josef Eschgfaeller
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From cbenjami at BTBOCES.ORG  Tue Nov  1 02:05:57 2016
From: cbenjami at BTBOCES.ORG (Courtney Benjamin)
Date: Tue, 1 Nov 2016 01:05:57 +0000
Subject: [R] Resetting Baseline Level of Predictor in svyglm Function
Message-ID: <1477962352382.94941@BTBOCES.ORG>

Hello R Users:

I am using the survey package in R for modeling with complex survey data. I am trying to reset the baseline level of certain predictor variables being used in a logistic regression without success. The following is a reproducible example:

library(RCurl)
library(survey)

data <- getURL("https://raw.githubusercontent.com/cbenjamin1821/careertech-ed/master/elsq1adj.csv")
elsq1ch <- read.csv(text = data)

#Specifying the svyrepdesign object which applies the BRR weights
elsq1ch_brr<-svrepdesign(variables = elsq1ch[,1:16], repweights = elsq1ch[,18:217], weights = elsq1ch[,17], combined.weights = TRUE, type = "BRR")
elsq1ch_brr

#Log. Reg. model
allCC <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+F1RGPP2+F1HIMATH+F1RTRCC,family="binomial",design=elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1,na.action=na.omit)
summary(allCC)

##Attempting to reset baseline level for predictor variable
#Both attempts did not work
elsq1ch$F1HIMATH <- C(elsq1ch$F1HIMATH,contr.treatment, base=1)
elsq1ch$F1HIMATH <- relevel(elsq1ch$F1HIMATH,"PreAlg or Less")

#Log. Reg. model with no changes in baseline levels for the predictors
allCC <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+F1RGPP2+F1HIMATH+F1RTRCC,family="binomial",design=elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1,na.action=na.omit)
summary(allCC)


Any guidance is greatly appreciated.?

Sincerely,

Courtney?

Courtney Benjamin

Broome-Tioga BOCES

Automotive Technology II Teacher

Located at Gault Toyota

Doctoral Candidate-Educational Theory & Practice

State University of New York at Binghamton

cbenjami at btboces.org<mailto:cbenjami at btboces.org>

607-763-8633

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Tue Nov  1 02:44:19 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 31 Oct 2016 18:44:19 -0700
Subject: [R] bzip2
In-Reply-To: <F7E6D18CC2877149AB5296CE54EA276635B9A8B2@WAXMXOLYMB025.WAX.wa.lcl>
References: <CAFBvUatx5vUMgJeumNo1YuQ6S9rDJ+GSqRG+XGJRkTp3V4Em4g@mail.gmail.com>
	<F7E6D18CC2877149AB5296CE54EA276635B9A8B2@WAXMXOLYMB025.WAX.wa.lcl>
Message-ID: <D5442F74-62C9-494A-A2BE-FD57450B8120@comcast.net>


> On Oct 31, 2016, at 5:29 PM, Nordlund, Dan (DSHS/RDA) <NordlDJ at dshs.wa.gov> wrote:
> 
> I don't know what version Linux or other OS you are using, but have you installed the bzip2 development package? It would be named something like  libbz2-dev (that is what it is in Ubuntu, I believe).

This question has been asked several times by OSX users and the answer for them has always been:

http://r.research.att.com/libs/

... although it's not immediately obvious that the bzlib functions are to be found in the xz package.

Instruction for installation from a Unix console  are at the bottom of the page.
-- 
David.



> 
> Dan
> 
> Daniel Nordlund, PhD
> Research and Data Analysis Division
> Services & Enterprise Support Administration
> Washington State Department of Social and Health Services
> 
> 
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Josef
>> Eschgfaeller
>> Sent: Monday, October 31, 2016 4:49 PM
>> To: r-help
>> Subject: [R] bzip2
>> 
>> I am not able to compile R 3.3,
>> configure halting with:
>> --------------------------------------------------
>> checking for BZ2_bzlibVersion in -lbz2... yes checking bzlib.h usability... yes
>> checking bzlib.h presence... yes checking for bzlib.h... yes checking if bzip2
>> version >= 1.0.6... no checking whether bzip2 support suffices... configure:
>> error:
>>                         bzip2 library and headers are required
>> --------------------------------------------------
>> But my bzip2 seems to be 1.0.6:
>> 
>> bzip2 --version
>> bzip2, a block-sorting file compressor.  Version 1.0.6, 6-Sept-2010.
>> type bzip2
>> bzip2 is hashed (/opt/local/bin/bzip2)
>> type R
>> R is hashed (/usr/bin/R)
>> R --version
>> R version 3.2.0 (2015-04-16) -- "Full of Ingredients"
>> --------------------------------------------------
>> Thanks for help
>> 
>> Josef Eschgfaeller
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From ajdamico at gmail.com  Tue Nov  1 08:20:51 2016
From: ajdamico at gmail.com (Anthony Damico)
Date: Tue, 1 Nov 2016 03:20:51 -0400
Subject: [R] Resetting Baseline Level of Predictor in svyglm Function
In-Reply-To: <1477962352382.94941@BTBOCES.ORG>
References: <1477962352382.94941@BTBOCES.ORG>
Message-ID: <CAOwvMDx8krhLsJUD7Z9TJdwxB_QjiBOt-uBeBCQA3GpLwzvRUA@mail.gmail.com>

hi, i think you want

elsq1ch_brr <- update( elsq1ch_brr , F1HIMATH = relevel(F1HIMATH,"PreAlg or
Less") )





On Mon, Oct 31, 2016 at 9:05 PM, Courtney Benjamin <cbenjami at btboces.org>
wrote:

> Hello R Users:
>
> I am using the survey package in R for modeling with complex survey data.
> I am trying to reset the baseline level of certain predictor variables
> being used in a logistic regression without success. The following is a
> reproducible example:
>
> library(RCurl)
> library(survey)
>
> data <- getURL("https://raw.githubusercontent.com/
> cbenjamin1821/careertech-ed/master/elsq1adj.csv")
> elsq1ch <- read.csv(text = data)
>
> #Specifying the svyrepdesign object which applies the BRR weights
> elsq1ch_brr<-svrepdesign(variables = elsq1ch[,1:16], repweights =
> elsq1ch[,18:217], weights = elsq1ch[,17], combined.weights = TRUE, type =
> "BRR")
> elsq1ch_brr
>
> #Log. Reg. model
> allCC <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+
> F1RGPP2+F1HIMATH+F1RTRCC,family="binomial",design=
> elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1,na.action=na.omit)
> summary(allCC)
>
> ##Attempting to reset baseline level for predictor variable
> #Both attempts did not work
> elsq1ch$F1HIMATH <- C(elsq1ch$F1HIMATH,contr.treatment, base=1)
> elsq1ch$F1HIMATH <- relevel(elsq1ch$F1HIMATH,"PreAlg or Less")
>
> #Log. Reg. model with no changes in baseline levels for the predictors
> allCC <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+
> F1RGPP2+F1HIMATH+F1RTRCC,family="binomial",design=
> elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1,na.action=na.omit)
> summary(allCC)
>
>
> Any guidance is greatly appreciated.?
>
> Sincerely,
>
> Courtney?
>
> Courtney Benjamin
>
> Broome-Tioga BOCES
>
> Automotive Technology II Teacher
>
> Located at Gault Toyota
>
> Doctoral Candidate-Educational Theory & Practice
>
> State University of New York at Binghamton
>
> cbenjami at btboces.org<mailto:cbenjami at btboces.org>
>
> 607-763-8633
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From esg at unife.it  Tue Nov  1 10:23:26 2016
From: esg at unife.it (Josef Eschgfaeller)
Date: Tue, 1 Nov 2016 10:23:26 +0100
Subject: [R] bzip2
In-Reply-To: <D5442F74-62C9-494A-A2BE-FD57450B8120@comcast.net>
References: <CAFBvUatx5vUMgJeumNo1YuQ6S9rDJ+GSqRG+XGJRkTp3V4Em4g@mail.gmail.com>
	<F7E6D18CC2877149AB5296CE54EA276635B9A8B2@WAXMXOLYMB025.WAX.wa.lcl>
	<D5442F74-62C9-494A-A2BE-FD57450B8120@comcast.net>
Message-ID: <CAFBvUas1A=3+KvFx39yDbcOAxXsEHyicP19aRE7jwgfq+Mio+w@mail.gmail.com>

David Winsemius wrote:

> http://r.research.att.com/libs/

I installed xz with

    tar fvxz xz-5.0.5-darwin10-bin2.tar.gz -C /

but then for R

    ./configure --enable-R-shlib

gives me the same error as before:
--------------------------------------------------
checking for BZ2_bzlibVersion in -lbz2... yes
checking bzlib.h usability... yes
checking bzlib.h presence... yes
checking for bzlib.h... yes
checking if bzip2 version >= 1.0.6... no
checking whether bzip2 support suffices... configure: error:
           bzip2 library and headers are required
--------------------------------------------------
I'm using Mac OS 10.6.8 Snow Leopard.

Thanks
Josef Eschgfaeller


From dwinsemius at comcast.net  Tue Nov  1 16:20:37 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 1 Nov 2016 08:20:37 -0700
Subject: [R] bzip2
In-Reply-To: <CAFBvUas1A=3+KvFx39yDbcOAxXsEHyicP19aRE7jwgfq+Mio+w@mail.gmail.com>
References: <CAFBvUatx5vUMgJeumNo1YuQ6S9rDJ+GSqRG+XGJRkTp3V4Em4g@mail.gmail.com>
	<F7E6D18CC2877149AB5296CE54EA276635B9A8B2@WAXMXOLYMB025.WAX.wa.lcl>
	<D5442F74-62C9-494A-A2BE-FD57450B8120@comcast.net>
	<CAFBvUas1A=3+KvFx39yDbcOAxXsEHyicP19aRE7jwgfq+Mio+w@mail.gmail.com>
Message-ID: <A1B5DC41-50A6-4AE9-8A9C-930A319F9421@comcast.net>


> On Nov 1, 2016, at 2:23 AM, Josef Eschgfaeller <esg at unife.it> wrote:
> 
> David Winsemius wrote:
> 
>> http://r.research.att.com/libs/
> 
> I installed xz with
> 
>    tar fvxz xz-5.0.5-darwin10-bin2.tar.gz -C /
> 
> but then for R
> 
>    ./configure --enable-R-shlib
> 
> gives me the same error as before:
> --------------------------------------------------
> checking for BZ2_bzlibVersion in -lbz2... yes
> checking bzlib.h usability... yes
> checking bzlib.h presence... yes
> checking for bzlib.h... yes
> checking if bzip2 version >= 1.0.6... no
> checking whether bzip2 support suffices... configure: error:
>           bzip2 library and headers are required
> --------------------------------------------------
> I'm using Mac OS 10.6.8 Snow Leopard.

You ought to a) include more context in your replies, and b) post on R-SIG-Mac. 

The advice in the Admin-guide can be found here, although it says no testing has been done recently for SL:

https://cran.r-project.org/doc/manuals/r-release/R-admin.html#OS-X

I suspect there are more difficulties with SL builds than just this problem. You will probably need to include all your setup information and the full configure log when you post this on the correct mailing list.


> 
> Thanks
> Josef Eschgfaeller
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From S.Ellison at LGCGroup.com  Tue Nov  1 16:29:52 2016
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Tue, 1 Nov 2016 15:29:52 +0000
Subject: [R] function ave() with seq_along returning char
	sequence	instead of numeric
In-Reply-To: <95806726-F808-43B1-9AC7-5A8C59AAAF6F@dcn.davis.ca.us>
References: <F7E6D18CC2877149AB5296CE54EA276635B9A7CC@WAXMXOLYMB025.WAX.wa.lcl>
	<95806726-F808-43B1-9AC7-5A8C59AAAF6F@dcn.davis.ca.us>
Message-ID: <1A8C1289955EF649A09086A153E267240400996C8D@GBTEDVPEXCMB04.corp.lgc-group.com>


> The help page describes the first argument x as a numeric... 
It also describes the _value_ as numeric. One for the help page issue list?

In fact there seems no obvious reason for a hard restriction to numeric*; the return value will depend largely on what FUN does, as there's no argument class check in the code for ave or for split(), which ave()uses. The principal requirement is presumably that FUN must accept a vector of class class(x) and return a vector of the same length as its argument (or, if there's grouping, a scalar) that split<- (or, with no grouping factor, '<-') can use.

*though plenty of reason to warn of unexpected consequences if not, of course

S Ellison



*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From ccberry at ucsd.edu  Tue Nov  1 17:24:50 2016
From: ccberry at ucsd.edu (Charles C. Berry)
Date: Tue, 1 Nov 2016 09:24:50 -0700
Subject: [R] function ave() with seq_along returning char sequence
 instead of numeric
In-Reply-To: <95806726-F808-43B1-9AC7-5A8C59AAAF6F@dcn.davis.ca.us>
References: <F7E6D18CC2877149AB5296CE54EA276635B9A7CC@WAXMXOLYMB025.WAX.wa.lcl>
	<95806726-F808-43B1-9AC7-5A8C59AAAF6F@dcn.davis.ca.us>
Message-ID: <alpine.OSX.2.20.1611010844050.692@charles-berrys-macbook.local>

On Mon, 31 Oct 2016, Jeff Newmiller wrote:

> The help page describes the first argument x as a numeric... it is not 
> designed to accept character,

Actually it is so designed, but not advertised as such. See below.

> so the fact that you get anything even close to right is just a bonus.
>
> As the doctor says, "if it hurts, don't do that".
>
> ave( rep( 1, length( v ), v, FUN=seq_along )
> --

[snip]


Reading the code of `ave` and then `split<-.default`, you will see subset 
replacement, "x[i]<- ...", on the argument 'x'. So, the issue is having 
FUN and that replacement (and possible coercion) yield something 
useful/sensible. In other words, class(x) need not be "numeric".

For instance, operating on "Date" objects:

> # start at 2016-01-02, step 10 days, ...
> x <- as.Date("2016-01-01")+seq(1,1000,by=10)
> z <- rep(1:10, 10)
>  class(ave(x,z)) # Date class is preserved
[1] "Date"
> ave(x,z) # mean date
   [1] "2017-03-27" "2017-04-06" "2017-04-16" "2017-04-26" ... 
> ave(x,z,FUN=min) # earliest date
   [1] "2016-01-02" "2016-01-12" "2016-01-22" "2016-02-01" ...

However, trying to describe this feature in the help page without a lot of 
detail and examples might confuse more users than it would enlighten.

--
Chuck


From therneau at mayo.edu  Tue Nov  1 23:47:56 2016
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Tue, 01 Nov 2016 17:47:56 -0500
Subject: [R] Updated package: survival_2.40-1
Message-ID: <021cdb$4nidt8@ironport10.mayo.edu>

Survival version 2.40 has been relased to CRAN.  This is a warning that some users may see 
changes in results, however.

The heart of the issue can be shown with a simple example. Calculate the following simple 
set of intervals:
<<interval1>>=
birth <- as.Date("1973/03/10")
start <- as.Date("1998/09/13") + 1:40
end   <- as.Date("1998/12/03") + rep(1:10, 4)
interval <- (end-start)
table(interval)
51 61 71 81
10 10 10 10
@

Each interval has a different start and end date, but there are only 4 unique intervals, 
each of which appears 10 times.
Now convert this to an age scale.

<<interval2>>=
start.age <- as.numeric(start-birth)/365.25
end.age   <- as.numeric(end  -birth)/365.25
age.interval <- end.age - start.age
table(match(age.interval, unique(age.interval)))
1 2 3 4 5 6 7 8
9 1 5 5 1 9 7 3
@
There are now eight different age intervals instead of 4, and the 8 unique values appear 
between 1 and 9 times each.  Exact results likely will depend on your computer system. We 
have become a victim of round off error.

Some users prefer to use time in days and some prefer time in years, and those latter 
users expect, I am sure, survival analysis results to be identical on the two scales.  
Both the coxph and survfit routines treat tied event times in a special way, and this 
roundoff can make actual ties appear as non-tied values, however. Parametric survival such 
as \code{survreg} is not affected by this issue.

In survival version 2.40 this issue has been addressed for the coxph and survfit routines; 
input times are subjected to the same logic found in the all.equal routine in order to 
determine actual ties. The upshot is that some users may experience a changed results.

For the following test case cox1 and cox2 are identical coefficients in version 2.40, but 
different in prior versions.
<<>>=
ndata <- data.frame(id=1:30,
                       birth.dt = rep(as.Date("1953/03/10"), 30),
                       enroll.dt= as.Date("1993/03/10") + 1:30,
                       end.dt   = as.Date("1996/10/21") + 1:30 +
                           rep(1:10, 3),
                       status= rep(0:1, length=30),
                       x = 1:30)
ndata$enroll.age <- with(ndata, as.numeric(enroll.dt - birth.dt))/365.25
ndata$end.age    <- with(ndata, as.numeric(end.dt - birth.dt))/365.25

fudays <- with(ndata, as.numeric(end.dt - enroll.dt))
fuyrs  <- with(ndata, as.numeric(end.age- enroll.age))
cox1 <- coxph(Surv(fudays, status) ~ x, data=ndata)
cox2 <- coxph(Surv(fuyrs,  status) ~ x, data=ndata)
@

This general issue of floating point precision arises often enough in R that is part of 
the frequently asked questions, see FAQ 7.31 on CRAN. The author of the survival routines 
(me) has always used days as the scale
for analysis -- just by habit, not for any particluarly good reason -- so the issue had 
never appeared in my work nor in the survival package's test suite. Due to user input, 
this issue had been addressed earlier in the survfit routine, but only when the status 
variable was 0/1, not when it is a factor.

As a final footnote, the simple data set above also gives different results when coded in 
SAS: I am not alone in overlooking it.  As a consequence, the maintainer expects to get 
new emails that ``we have found a bug in your code: it gives a different answer than 
SAS''.  (This is an actual quote.)

Terry Therneau


From pdalgd at gmail.com  Wed Nov  2 10:58:07 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 2 Nov 2016 10:58:07 +0100
Subject: [R] Updated package: survival_2.40-1
In-Reply-To: <021cdb$4nidt8@ironport10.mayo.edu>
References: <021cdb$4nidt8@ironport10.mayo.edu>
Message-ID: <757607B0-FA00-4092-A06D-DB5A0C39477C@gmail.com>


On 01 Nov 2016, at 23:47 , Therneau, Terry M., Ph.D. <therneau at mayo.edu> wrote:

> As a final footnote, the simple data set above also gives different results when coded in SAS: I am not alone in overlooking it.  As a consequence, the maintainer expects to get new emails that ``we have found a bug in your code: it gives a different answer than SAS''.  (This is an actual quote.)


Fortune nomination.


-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From Anusha.Indhira at controlsdata.com  Wed Nov  2 12:50:20 2016
From: Anusha.Indhira at controlsdata.com (Indhira, Anusha)
Date: Wed, 2 Nov 2016 11:50:20 +0000
Subject: [R] point in polygon dataframe data
Message-ID: <CE1899A9C6A8D64099DFB344ECF0F7548F42@DERCORCEXH02.ds-s.com>

Hi ,

I have polygon vertices as given below

   ft_num         dv_ft dp30_mn_phase     dv_n2 p30_mn_phase tgt_mn_phase n2_bin    fuel_mn    epr_mn  min_dp30_mnp
19     28 zero-gradient -0.0006680832 transient     34.76701     493.6494    g75  304.56411 0.9593899 -0.0006680832
18     28  neg-gradient -0.0022879651 transient     30.90294     459.6093    g75  262.22673 0.9561083 -0.0022879651
22     31 zero-gradient -0.0001535604 transient     12.15854     304.8818    l50   29.82188 0.9487416 -0.0001535604
2      10 zero-gradient  0.0003930719 transient     21.80663     503.8669    l50  242.14284 0.9250054  0.0003930719
1      10  pos-gradient -0.0016072616 transient     23.35261     516.5950 b50-75  265.61233 0.9206513 -0.0016072616
5      13      zero-alt  0.0010052941 transient     31.93961     574.0899 b50-75 2092.06574 1.0072156  0.0010052941
6      14  neg-gradient -0.0017905848 transient     42.34545     626.4670 b50-75  465.59408 0.9588052 -0.0017905848
20     30  neg-gradient -0.0015010177 transient     46.30469     644.3314    g75  525.38626 0.9496521 -0.0015010177
    max_dp30_mnp min_p30_mnp max_p30_mnp
19 -0.0006680832    34.76701    34.76701
18 -0.0022879651    30.90294    30.90294
22 -0.0001535604    12.15854    12.15854
2   0.0003930719    21.80663    21.80663
1  -0.0016072616    23.35261    23.35261
5   0.0010052941    31.93961    31.93961
6  -0.0017905848    42.34545    42.34545
20 -0.0015010177    46.30469    46.30469

Is there a function in R which can tell whether a point from a data.frame having same columns is inside or outside the polygon( can remove ft_num attribute for comparision)?

Thanks,
Alily
This e-mail (including attachments) contains contents owned by Rolls-Royce plc and its subsidiaries, affiliated companies or customers and covered by the laws of England and Wales, Brazil, US, or Canada (federal, state or provincial). The information is intended to be confidential and may be legally privileged. If you are not the intended recipient, you are hereby notified that any retention, dissemination, distribution, interception or copying of this communication is strictly prohibited and may subject you to further legal action. Reply to the sender if you received this email by accident, and then delete the email and any attachments.

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Wed Nov  2 13:08:51 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 2 Nov 2016 12:08:51 +0000
Subject: [R] point in polygon dataframe data
In-Reply-To: <CE1899A9C6A8D64099DFB344ECF0F7548F42@DERCORCEXH02.ds-s.com>
References: <CE1899A9C6A8D64099DFB344ECF0F7548F42@DERCORCEXH02.ds-s.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5044D64@SRVEXCHMBX.precheza.cz>

Hi

There are plenty of options
https://stat.ethz.ch/R-manual/R-devel/library/mgcv/html/in.out.html
http://www.dpi.inpe.br/gilberto/tutorials/software/R-contrib/sp/html/point.in.polygon.html
http://finzi.psych.upenn.edu/library/DescTools/html/PtInPoly.html

did you try any of them?

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Indhira,
> Anusha
> Sent: Wednesday, November 2, 2016 12:50 PM
> To: r-help at r-project.org
> Subject: [R] point in polygon dataframe data
>
> Hi ,
>
> I have polygon vertices as given below
>
>    ft_num         dv_ft dp30_mn_phase     dv_n2 p30_mn_phase tgt_mn_phase
> n2_bin    fuel_mn    epr_mn  min_dp30_mnp
> 19     28 zero-gradient -0.0006680832 transient     34.76701     493.6494    g75
> 304.56411 0.9593899 -0.0006680832
> 18     28  neg-gradient -0.0022879651 transient     30.90294     459.6093    g75
> 262.22673 0.9561083 -0.0022879651
> 22     31 zero-gradient -0.0001535604 transient     12.15854     304.8818    l50
> 29.82188 0.9487416 -0.0001535604
> 2      10 zero-gradient  0.0003930719 transient     21.80663     503.8669    l50
> 242.14284 0.9250054  0.0003930719
> 1      10  pos-gradient -0.0016072616 transient     23.35261     516.5950 b50-75
> 265.61233 0.9206513 -0.0016072616
> 5      13      zero-alt  0.0010052941 transient     31.93961     574.0899 b50-75
> 2092.06574 1.0072156  0.0010052941
> 6      14  neg-gradient -0.0017905848 transient     42.34545     626.4670 b50-75
> 465.59408 0.9588052 -0.0017905848
> 20     30  neg-gradient -0.0015010177 transient     46.30469     644.3314    g75
> 525.38626 0.9496521 -0.0015010177
>     max_dp30_mnp min_p30_mnp max_p30_mnp
> 19 -0.0006680832    34.76701    34.76701
> 18 -0.0022879651    30.90294    30.90294
> 22 -0.0001535604    12.15854    12.15854
> 2   0.0003930719    21.80663    21.80663
> 1  -0.0016072616    23.35261    23.35261
> 5   0.0010052941    31.93961    31.93961
> 6  -0.0017905848    42.34545    42.34545
> 20 -0.0015010177    46.30469    46.30469
>
> Is there a function in R which can tell whether a point from a data.frame
> having same columns is inside or outside the polygon( can remove ft_num
> attribute for comparision)?
>
> Thanks,
> Alily
> This e-mail (including attachments) contains contents owned by Rolls-Royce
> plc and its subsidiaries, affiliated companies or customers and covered by the
> laws of England and Wales, Brazil, US, or Canada (federal, state or provincial).
> The information is intended to be confidential and may be legally privileged.
> If you are not the intended recipient, you are hereby notified that any
> retention, dissemination, distribution, interception or copying of this
> communication is strictly prohibited and may subject you to further legal
> action. Reply to the sender if you received this email by accident, and then
> delete the email and any attachments.
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From emmanuel.levy at gmail.com  Wed Nov  2 14:48:05 2016
From: emmanuel.levy at gmail.com (Emmanuel Levy)
Date: Wed, 2 Nov 2016 15:48:05 +0200
Subject: [R] Adding a column to an **empty** data.frame
Message-ID: <CAMUS-Mu616uFaRvusMSLKfYiD4SYRAwq-tSfJhk7Sjn1bKpcrQ@mail.gmail.com>

Dear All,

This sounds simple but can't figure out a good way to do it.

Let's say that I have an empty data frame "df":

## creates the df
df = data.frame( id=1, data=2)

## empties the df, perhaps there is a more elegant way to create an empty
df?
df = df[-c(1),]

> df
[1] id   data
<0 rows> (or 0-length row.names)

Now, how can I add a third column name to that empty df?

Normally I would use df$new.col = 1

But here I can't use this because it's empty!

I tried df$new.col=NULL -- this doesn't give an error but doesn't add the
column.

Thanks for your help,

Emmanuel

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Wed Nov  2 14:59:01 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 2 Nov 2016 06:59:01 -0700
Subject: [R] Adding a column to an **empty** data.frame
In-Reply-To: <CAMUS-Mu616uFaRvusMSLKfYiD4SYRAwq-tSfJhk7Sjn1bKpcrQ@mail.gmail.com>
References: <CAMUS-Mu616uFaRvusMSLKfYiD4SYRAwq-tSfJhk7Sjn1bKpcrQ@mail.gmail.com>
Message-ID: <CAF8bMcYbS+qW5wWmPBH9dAp3Dq2irZs+6X5kGZhku1R08GEsiQ@mail.gmail.com>

Give your new column a vector type - NULL cannot be extended beyond length
0.  Also, the syntax df$col<-NULL means to remove 'col' from 'df'.

> df <- data.frame(id=integer(0), data=numeric(0))
> df$new.col <- character(0)
> str(df)
'data.frame':   0 obs. of  3 variables:
 $ id     : int
 $ data   : num
 $ new.col: chr

(Think of 'zero-row' or 'zero-column' data.frames, not 'empty' data.frames,
which could be either.)



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Wed, Nov 2, 2016 at 6:48 AM, Emmanuel Levy <emmanuel.levy at gmail.com>
wrote:

> Dear All,
>
> This sounds simple but can't figure out a good way to do it.
>
> Let's say that I have an empty data frame "df":
>
> ## creates the df
> df = data.frame( id=1, data=2)
>
> ## empties the df, perhaps there is a more elegant way to create an empty
> df?
> df = df[-c(1),]
>
> > df
> [1] id   data
> <0 rows> (or 0-length row.names)
>
> Now, how can I add a third column name to that empty df?
>
> Normally I would use df$new.col = 1
>
> But here I can't use this because it's empty!
>
> I tried df$new.col=NULL -- this doesn't give an error but doesn't add the
> column.
>
> Thanks for your help,
>
> Emmanuel
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From emmanuel.levy at gmail.com  Wed Nov  2 15:07:06 2016
From: emmanuel.levy at gmail.com (Emmanuel Levy)
Date: Wed, 2 Nov 2016 16:07:06 +0200
Subject: [R] Adding a column to an **empty** data.frame
In-Reply-To: <CAF8bMcYbS+qW5wWmPBH9dAp3Dq2irZs+6X5kGZhku1R08GEsiQ@mail.gmail.com>
References: <CAMUS-Mu616uFaRvusMSLKfYiD4SYRAwq-tSfJhk7Sjn1bKpcrQ@mail.gmail.com>
	<CAF8bMcYbS+qW5wWmPBH9dAp3Dq2irZs+6X5kGZhku1R08GEsiQ@mail.gmail.com>
Message-ID: <CAMUS-MuLCSdKS0jD2bwCLf5+-3A2jEEGmUD874Lr0bUR=QedDw@mail.gmail.com>

Dear Bill,

Thanks a lot this is what I was looking for.

Emmanuel

On 2 November 2016 at 15:59, William Dunlap <wdunlap at tibco.com> wrote:

> Give your new column a vector type - NULL cannot be extended beyond length
> 0.  Also, the syntax df$col<-NULL means to remove 'col' from 'df'.
>
> > df <- data.frame(id=integer(0), data=numeric(0))
> > df$new.col <- character(0)
> > str(df)
> 'data.frame':   0 obs. of  3 variables:
>  $ id     : int
>  $ data   : num
>  $ new.col: chr
>
> (Think of 'zero-row' or 'zero-column' data.frames, not 'empty'
> data.frames, which could be either.)
>
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Wed, Nov 2, 2016 at 6:48 AM, Emmanuel Levy <emmanuel.levy at gmail.com>
> wrote:
>
>> Dear All,
>>
>> This sounds simple but can't figure out a good way to do it.
>>
>> Let's say that I have an empty data frame "df":
>>
>> ## creates the df
>> df = data.frame( id=1, data=2)
>>
>> ## empties the df, perhaps there is a more elegant way to create an empty
>> df?
>> df = df[-c(1),]
>>
>> > df
>> [1] id   data
>> <0 rows> (or 0-length row.names)
>>
>> Now, how can I add a third column name to that empty df?
>>
>> Normally I would use df$new.col = 1
>>
>> But here I can't use this because it's empty!
>>
>> I tried df$new.col=NULL -- this doesn't give an error but doesn't add the
>> column.
>>
>> Thanks for your help,
>>
>> Emmanuel
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From S.Ellison at LGCGroup.com  Wed Nov  2 15:07:49 2016
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Wed, 2 Nov 2016 14:07:49 +0000
Subject: [R] Adding a column to an **empty** data.frame
In-Reply-To: <CAMUS-Mu616uFaRvusMSLKfYiD4SYRAwq-tSfJhk7Sjn1bKpcrQ@mail.gmail.com>
References: <CAMUS-Mu616uFaRvusMSLKfYiD4SYRAwq-tSfJhk7Sjn1bKpcrQ@mail.gmail.com>
Message-ID: <1A8C1289955EF649A09086A153E267240400C10F1A@GBTEDVPEXCMB04.corp.lgc-group.com>

> Now, how can I add a third column name to that empty df?
You could use functions that generate zero-length objects. Examples:


df$newv <- vector(mode="numeric") #logical by default, so <-vector() would give you a zero-length logical

df$newi <- integer()

df$newf <- factor()


S Ellison



*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From petr.pikal at precheza.cz  Wed Nov  2 15:08:00 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 2 Nov 2016 14:08:00 +0000
Subject: [R] Adding a column to an **empty** data.frame
In-Reply-To: <CAMUS-Mu616uFaRvusMSLKfYiD4SYRAwq-tSfJhk7Sjn1bKpcrQ@mail.gmail.com>
References: <CAMUS-Mu616uFaRvusMSLKfYiD4SYRAwq-tSfJhk7Sjn1bKpcrQ@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5044DB5@SRVEXCHMBX.precheza.cz>

Hi

data frames are structures which need to have the same length of all variables. So if you have 0 length variables you need to add also 0 length vectors

> df$aa<-numeric(length=0)
> df
[1] id   data aa
<0 rows> (or 0-length row.names)
> df$bb<-character(length=0)
> df
[1] id   data aa   bb
<0 rows> (or 0-length row.names)

However it seems to me that you are asking for trouble. Incremental increase size of data frame is rarely needed and probably result of bad programming practice.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Emmanuel
> Levy
> Sent: Wednesday, November 2, 2016 2:48 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Adding a column to an **empty** data.frame
>
> Dear All,
>
> This sounds simple but can't figure out a good way to do it.
>
> Let's say that I have an empty data frame "df":
>
> ## creates the df
> df = data.frame( id=1, data=2)
>
> ## empties the df, perhaps there is a more elegant way to create an empty
> df?
> df = df[-c(1),]
>
> > df
> [1] id   data
> <0 rows> (or 0-length row.names)
>
> Now, how can I add a third column name to that empty df?
>
> Normally I would use df$new.col = 1
>
> But here I can't use this because it's empty!
>
> I tried df$new.col=NULL -- this doesn't give an error but doesn't add the
> column.
>
> Thanks for your help,
>
> Emmanuel
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From andersonaed at gmail.com  Wed Nov  2 15:37:04 2016
From: andersonaed at gmail.com (Anderson Eduardo)
Date: Wed, 2 Nov 2016 11:37:04 -0300
Subject: [R] MaxentVariableSelection problem
Message-ID: <CAEPvLry7PFe6dpbp8WbG9Cydk4M0qYY2wccMajyvpAgxfKpxBg@mail.gmail.com>

Hello

I am trying to run vignette
<https://cran.r-project.org/web/packages/MaxentVariableSelection/vignettes/MaxentVariableSelection.pdf>
example for the MaxentVariableSelection package, but something wrong is
happening. I can't figure out.

Here is the code:

maxentPath =
("/home/anderson/R/x86_64-pc-linux-gnu-library/3.3/dismo/java/maxent.jar")
gridfolder <- ("/home/anderson/Downloads/BioOracle_9090RV")
occurrencelocations <- system.file("extdata",
"Occurrencedata.csv",package="MaxentVariableSelection")
backgroundlocations <- system.file("extdata",
"Backgrounddata.csv",package="MaxentVariableSelection")
additionalargs="nolinear noquadratic noproduct nothreshold noautofeature"
contributionthreshold <- 5
correlationthreshold <- 0.9
betamultiplier=seq(2,6,0.5)

VariableSelection(maxent,
                  outdir,
                  gridfolder,
                  occurrencelocations,
                  backgroundlocations,
                  additionalargs,
                  contributionthreshold,
                  correlationthreshold,
                  betamultiplier
                  )

Aand the error message:

> VariableSelection(maxent,
+                   outdir,
+                   gridfolder,
+                   occurrencelocations,
+                   backgroundlocations,
+                   additionalargs,
+                   contributionthreshold,
+                   correlationthreshold,
+                   betamultiplier
+                   )
-----------------------
Choosing betamultiplier  2

 Number of remaining variables 4
Testing variable contributions...
Calculating average AUC values from 10 maxent models...
arguments 'show.output.on.console', 'minimized' and 'invisible' are for
Windows only
Error in as.vector(x, "character") :
  cannot coerce type 'closure' to vector of type 'character'


I have not found the solution in blogs and online forums and I would ask
earnestly the help of the members of this forum.

Thanks in advance.

Anderson A. Eduardo
------------------------------------------------------------------------------
Lattes <http://lattes.cnpq.br/3826166230581311> | Researcher ID
<http://orcid.org/0000-0001-8045-8043> | Google Acad?mico
<https://scholar.google.com.br/citations?user=oOUjq9IAAAAJ&hl=pt-BR> | Site
<http://andersonaireseduardo.xpg.uol.com.br/>
------------------------------------------------------------------------------

	[[alternative HTML version deleted]]


From behelfsadresse at googlemail.com  Wed Nov  2 10:34:04 2016
From: behelfsadresse at googlemail.com (Leo Leo)
Date: Wed, 2 Nov 2016 10:34:04 +0100
Subject: [R] Forging an R enthusiast meeting at EGU 2017
Message-ID: <CAOSEYu-s+VL81YZTF6Uy=h5Dav5fRM86X0M+38ggZ2a3zEYYOw@mail.gmail.com>

At the European Geosciences Union Meeting (Spring 2017 in Vienna) there
will be an interactive sessionthat aims to bring together scientists that
share a common indispensable tool: the
statistic software R.

http://meetingorganizer.copernicus.org/EGU2017/session/24971

We have the feeling that R users are like little boats on an ocean at
night, unaware of how close other people are that have already worked on
the same problem earlier. This is the main motivation for this session in
spring 2017.

If you feel interested to contribute your work we would be delighted to see
you enriching this session.

If you don't think you will have time to come to the EGU 2017 we would be
pleased if you could share this message among your colleagues to linking
these small boats on the ocean at night.

Kind regards and a pleasant autumn week,

Micha Dietze

	[[alternative HTML version deleted]]


From bryanmac.24 at gmail.com  Wed Nov  2 17:48:54 2016
From: bryanmac.24 at gmail.com (Bryan Mac)
Date: Wed, 2 Nov 2016 09:48:54 -0700
Subject: [R] Filtering out rows
Message-ID: <CANa8aasAxWmB68SvuDtWBfszGDWK1xhm7bWMfobbL0JWcjpmqQ@mail.gmail.com>

Hi,

I am looking to filter out rows in my data set, run my analysis and
then removing the filter. Here is my regression. I would like to
filter out a row. For example, I would like to filter out a row,
"Case"=1511.

npi=lm(npi_mvmt~cnavgpi,data=df)
summary(npi)
resid.vs.fitt.LS%<a-%{plot(npi,1)}
resid.vs.fitt.LS

npi_lms=lmsreg(npi_mvmt~cnavgpi,data=df)
fitted.npi_lms<-fitted(npi_lms)
resid.npi_lms<-residuals(npi_lms)
resid.vs.fitt.LMS %<a-%{plot(fitted.npi_lms,resid.npi_lms);
title("npi_mvmt~cnavgpi")}
resid.vs.fitt.LMS

plot_cnavgpi_npi_mvmt %<a-%{plot(cnavgpi,npi_mvmt);
abline(lm(npi_mvmt~cnavgpi,data=df),lwd=2,col=2,lty=1);
abline(lmsreg(npi_mvmt~cnavgpi,data=df),lwd=2, col=3,lty=1);
legend('topright',c('OLS','LMS'),lty=c(1,1),lwd=c(2.5,2.5), col=c(2,3));
title (npi_mvmt~cnavgpi) }
plot_cnavgpi_npi_mvmt

Best,
Bryan Mac


From dwinsemius at comcast.net  Wed Nov  2 20:53:32 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 2 Nov 2016 12:53:32 -0700
Subject: [R] Filtering out rows
In-Reply-To: <CANa8aasAxWmB68SvuDtWBfszGDWK1xhm7bWMfobbL0JWcjpmqQ@mail.gmail.com>
References: <CANa8aasAxWmB68SvuDtWBfszGDWK1xhm7bWMfobbL0JWcjpmqQ@mail.gmail.com>
Message-ID: <7DDB3F0A-DAEB-4994-AF6D-8842411A1AA1@comcast.net>


> On Nov 2, 2016, at 9:48 AM, Bryan Mac <bryanmac.24 at gmail.com> wrote:
> 
> Hi,
> 
> I am looking to filter out rows in my data set, run my analysis and
> then removing the filter. Here is my regression. I would like to
> filter out a row. For example, I would like to filter out a row,
> "Case"=1511.
> 
> npi=lm(npi_mvmt~cnavgpi,data=df)
> summary(npi)
> resid.vs.fitt.LS%<a-%{plot(npi,1)}
> resid.vs.fitt.LS
> 
> npi_lms=lmsreg(npi_mvmt~cnavgpi,data=df)
> fitted.npi_lms<-fitted(npi_lms)
> resid.npi_lms<-residuals(npi_lms)
> resid.vs.fitt.LMS %<a-%{plot(fitted.npi_lms,resid.npi_lms);
> title("npi_mvmt~cnavgpi")}
> resid.vs.fitt.LMS
> 
> plot_cnavgpi_npi_mvmt %<a-%{plot(cnavgpi,npi_mvmt);
> abline(lm(npi_mvmt~cnavgpi,data=df),lwd=2,col=2,lty=1);
> abline(lmsreg(npi_mvmt~cnavgpi,data=df),lwd=2, col=3,lty=1);
> legend('topright',c('OLS','LMS'),lty=c(1,1),lwd=c(2.5,2.5), col=c(2,3));
> title (npi_mvmt~cnavgpi) }
> plot_cnavgpi_npi_mvmt

When posting questions about code that come from non-base packages you should include library calls to all packages needed to run the code. You should also separate terms and operators with spaces.

> 
> Best,
> Bryan Mac
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From bgunter.4567 at gmail.com  Wed Nov  2 21:09:18 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 2 Nov 2016 13:09:18 -0700
Subject: [R] Filtering out rows
In-Reply-To: <CANa8aasAxWmB68SvuDtWBfszGDWK1xhm7bWMfobbL0JWcjpmqQ@mail.gmail.com>
References: <CANa8aasAxWmB68SvuDtWBfszGDWK1xhm7bWMfobbL0JWcjpmqQ@mail.gmail.com>
Message-ID: <CAGxFJbTdTpV-k_bZ+PSvt5OCPYMo+VE73MdXrgQd2y5OTya+mA@mail.gmail.com>

If I understand correctly, this is merely a matter of subscripting,
which is a basic and essential R operation. If you aren't familiar
with subscripting, stop and go through one of the many R web tutorials
that covers this before proceeding. See also ?subset for a simpler but
not  quite so flexible way to do this.

For fitting, lmsreg, like many/most of R's modeing functions has a
'data' argument. To "filter out" all rows of df that have df$case ==
"1511", simply specify

data = df[df$case != "1511", ]

in your call. However, note that if the other variables in your call
are *not* all in df, you'll have to subscript those that aren't ,too;
e.g.

use <- df$case != "1511"
npi_lms=lmsreg(npi_mvmt[use] ~ cnavgpi,data=df[use,])

where the rhs variable is in df but the lhs variable is not.

Finally, note that in the case where data contains all the variables
of the call, you can skip the indexing and just use the subset =
argument of lmsreg.

-- Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Nov 2, 2016 at 9:48 AM, Bryan Mac <bryanmac.24 at gmail.com> wrote:
> Hi,
>
> I am looking to filter out rows in my data set, run my analysis and
> then removing the filter. Here is my regression. I would like to
> filter out a row. For example, I would like to filter out a row,
> "Case"=1511.
>
> npi=lm(npi_mvmt~cnavgpi,data=df)
> summary(npi)
> resid.vs.fitt.LS%<a-%{plot(npi,1)}
> resid.vs.fitt.LS
>
> npi_lms=lmsreg(npi_mvmt~cnavgpi,data=df)
> fitted.npi_lms<-fitted(npi_lms)
> resid.npi_lms<-residuals(npi_lms)
> resid.vs.fitt.LMS %<a-%{plot(fitted.npi_lms,resid.npi_lms);
> title("npi_mvmt~cnavgpi")}
> resid.vs.fitt.LMS
>
> plot_cnavgpi_npi_mvmt %<a-%{plot(cnavgpi,npi_mvmt);
> abline(lm(npi_mvmt~cnavgpi,data=df),lwd=2,col=2,lty=1);
> abline(lmsreg(npi_mvmt~cnavgpi,data=df),lwd=2, col=3,lty=1);
> legend('topright',c('OLS','LMS'),lty=c(1,1),lwd=c(2.5,2.5), col=c(2,3));
> title (npi_mvmt~cnavgpi) }
> plot_cnavgpi_npi_mvmt
>
> Best,
> Bryan Mac
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From asales at utexas.edu  Wed Nov  2 21:00:54 2016
From: asales at utexas.edu (Adam C Sales)
Date: Wed, 2 Nov 2016 15:00:54 -0500
Subject: [R] svyglm in the survey package: large regression models with
 replicate weights give infinite standard errors
Message-ID: <CAJ4RTx+_g5QiFdo2M_V0mpaMh0jwdp2YnYs2XS1UijFVSgSPyA@mail.gmail.com>

I'm running some regressions on data from the American Community
Survey, with replicate weights for variance estimation, using the
svyglm function from the survey package.

For my larger regressions, all of the standard errors were reported as Inf.

I did a bit of digging through the code, and figure out why that was:
the svrepglm function (see here:
https://github.com/cran/survey/blob/9aea3247ea1966667aba3efeeaf4a574e819f924/R/surveyrep.R)
calculates standard errors using the formula I would expect, but then
adjusts the "degrees of freedom" for the fit with the following line:
full$df.residual<-degf(design)+1-length(coef(full)[!is.na(coef(full))])
(where 'full' is the name of the model and 'design' is the specified
survey design)

It turns out that degf(design) returns the rank of the weight matrix,
which, in the case of the ACS at least, is the number of replicate
weights, 80. If there are more than 80 columns in the regression
design matrix, the df.residual entry in the model is negative, which
causes the summary.glm() function to report infinite standard errors.

My question is this:
Is this a bug in the survey package, or are the standard errors really
unidentified for large regression models?
Or am I using the wrong settings?

If it's just a bug fixing it is easy enough, and that's what I've been
doing, but I'd like to make sure I'm not glossing over a serious
statistical limitation of the data.

Here's a small working example. It's artificial in that I chose an
arbitrary regression that had enough regressors. It takes a long time
to run.

### step 1: download from
http://www2.census.gov/programs-surveys/acs/data/pums/2014/1-Year/csv_pus.zip
### step 2: unzip, put file ss14pusa.csv in working directory
library(survey)
pums <- read.csv('ss14pusa.csv')

## some of the replication weights are negative. you might want to set
them to zero:
weightcols <- grep('pwgtp[1-9]',names(pums))
pums[,weightcols] <- apply(pums[,weightcols],2,function(x) ifelse(x<0,0,x))

## there's some lack of clarity online on how to set these things up.
Either way you get infinite standard errors
## Anthony Damico recommends (see:
https://stat.ethz.ch/pipermail/r-help/2014-September/422133.html):
options( "survey.replicates.mse" = TRUE )
des <- svrepdesign(data=pums,weights=~PWGTP,repweights="pwgtp[1-9]",scale=4/80,rscales=rep(1,80))

## However, I think the following is correct:
des <- svrepdesign(data=pums,weights=~PWGTP,repweights="pwgtp[1-9]",scale=4/80,rscales=rep(1,80),type='Fay',rho=0.5)

## Anyway, here's the problem:
summary(mod <- svyglm(AGEP~as.factor(ST)+as.factor(CIT)+as.factor(MAR)+as.factor(RELP)+as.factor(HISP)+as.factor(RAC2P),design=des))


-- 
UT College of Education
SMARTER Consulting


From javlacalle at yahoo.es  Wed Nov  2 22:48:00 2016
From: javlacalle at yahoo.es (=?UTF-8?Q?Javier_L=C3=B3pez-de-Lacalle?=)
Date: Wed, 2 Nov 2016 21:48:00 +0000 (UTC)
Subject: [R] Number of used observations in stats::arima with NAs
References: <1714283392.1489155.1478123280090.ref@mail.yahoo.com>
Message-ID: <1714283392.1489155.1478123280090@mail.yahoo.com>

When missing observations (NAs) are present in the data and the regular or differencing filters are applied in 
stats::arima, the number of used observations (nobs) reported by nobs() is bigger than the actual number of non-NA observations. This value is used to compute the covariance matrix of parameter estimates and by BIC(), so this is relevant for model selection.

Example: the same number of observations are used in both fits below, but nobs() reports different values:

    x <- round(log(AirPassengers),2)
    length(x)
    #[1] 144
    x[c(36,52)] <- NA
    # fit ARIMA(0,1,0) to the original series
    fit1 <- stats::arima(x, order=c(0,1,0), include.mean=FALSE, method="ML") 
    nobs(fit1)
    #[1] 141
    # fit ARIMA(0,0,0) to the differenced series
    fit2 <- stats::arima(diff(x), order=c(0,0,0), include.mean=FALSE, method="ML") 
    nobs(fit2)

    #[1] 139

In "stats::arima", nobs is obtained as:

    n.used <- sum(!is.na(x)) - length(Delta)

where "length(Delta)" is the number of observations missed due to differencing. The original data in "x" are used instead of the differenced data. This ignores the fact that a missing observation generates two NAs in the differenced series:
    diff(c(1,2,3,NA,5,6,7,8))

    #[1]  1  1 NA NA  1  1  1

"stats::arima" counts the NAs in the original data, but this does not always match the number of NAs in the differenced series, which is the series that is used to compute the log-likelihood. Although there is only *one* NA observation, the contribution to the log-likelihood of *two* observations will be missed due to this NA, hence the number of contributions to the likelihood is not "sum(!is.na(x)) - length(Delta)" but "sum(!is.na(diff(x)))".

The more NAs are in the data, the bigger will be the effect of this issue.

If the argument "order[2]=1", "seasonal$order[2]=0" and the NA is the first observation, then my comment above is not an issue. It is not either an issue when "seasonal$order[2]=1" and the NAs are in the first "seasonal$period" observations. But in general, I believe the way nobs is computed requires adjusting.
I would propose (reusing code from stats::arima):

    dx <- x
    if(d > 0L) {
      dx <- diff(dx, 1L, order[2L])
    }

    if(seasonal$period > 1L & seasonal$order[2L] > 0) {
      dx <- diff(dx, seasonal$period, seasonal$order[2L])    }
    n.used <- sum(!is.na(dx))

If the argument "xreg" is NULL, "dx" is not needed and the following may be more efficient than the above option (I checked it with just a few examples, so the above may be a safer option):

    d <- order[2L]
    D <- seasonal$order[2L]

    S <- seasonal$period
    n.used <- length(x) - sum(is.na(x)[seq_len(S)]) -       2*(d + D) * sum(is.na(x)[-seq_len(S)]) - length(Delta)

If "xreg" is not NULL, "dx" is required in other part of the code, so it is convenient to use it here as well; replacing "x" by "dx" and "xreg" by "dxreg":

    isna <- is.na(dx) | apply(dxreg, 1L, anyNA)
    n.used <- sum(!isna)

Are there any reasons for counting the number of available observations in the original series rather than in the differenced series? Should this be fixed?

Thanks

Javier L?pez-de-Lacalle
https://jalobe.com


From macqueen1 at llnl.gov  Thu Nov  3 01:22:48 2016
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Thu, 3 Nov 2016 00:22:48 +0000
Subject: [R] Prevent a table from crossing page boundary using
	rmarkdown/pandoc
Message-ID: <D43FD168.18E2AD%macqueen1@llnl.gov>

I'm using rmarkdown::render, which executes pandoc, to create a fairly
simple document, and I would like to prevent tables from breaking across
pages. I've been doing some web-searching but haven't found a solution.

In my report's context, the table is so short that breaking it across
pages looks shabby, whereas leaving some empty space at the bottom of the
page is acceptable.



## this R script creates a reproducible example
## on my installation
library(rmarkdown)
library(knitr)
sink('tmp.Rmd')

cat('---
title: A Title
output:
  html_document:
    toc: yes
    number_sections: yes
  word_document:
    reference_docx: report-template.docx
  pdf_document:
    number_sections: yes
    toc: yes
---\n\n')

for (i in 1:65) {
  cat('ln',i,'\n\n')
}


mytbl <- data.frame(A=1:5, B=1:5)

## then, within a chunk:

print(kable(mytbl))

for (i in 66:70) {
  cat('ln',i,'\n\n')
}

sink()

render('tmp.Rmd','pdf_document')

## open tmp.pdf and see that the table breaks
## between the first and second pages
## possibly, other latex installations will be different
## (e.g., I have default letter-size paper, A4 will
## be different. Change the 1:65 until the table starts
## two or three lines from the bottom of the page.)


An alternative to kable() that has a suitable arg, would be easiest, I
suppose. I've looked at pander::pandoc.table without success.

Naturally, I'd like the solution to work for word_document as well, but
I'd be more than grateful for a pdf solution. It's irrelevant for html
output.

If it matters, am working in R at a shell command line, not within RStudio.

Thanks in advance.
-Don


-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062


From murdoch.duncan at gmail.com  Thu Nov  3 10:50:05 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 3 Nov 2016 05:50:05 -0400
Subject: [R] Prevent a table from crossing page boundary using
 rmarkdown/pandoc
In-Reply-To: <D43FD168.18E2AD%macqueen1@llnl.gov>
References: <D43FD168.18E2AD%macqueen1@llnl.gov>
Message-ID: <f7cfbf00-109f-d9f3-aaca-aaee683cf426@gmail.com>

On 02/11/2016 8:22 PM, MacQueen, Don wrote:
> I'm using rmarkdown::render, which executes pandoc, to create a fairly
> simple document, and I would like to prevent tables from breaking across
> pages. I've been doing some web-searching but haven't found a solution.

If you make it into a float, LaTeX will try not to split it.  You can do 
that by specifying the caption arg to kable().

I don't know if you can specify where the float appears or if you're 
stuck with the default.

Duncan Murdoch

>
> In my report's context, the table is so short that breaking it across
> pages looks shabby, whereas leaving some empty space at the bottom of the
> page is acceptable.
>
>
>
> ## this R script creates a reproducible example
> ## on my installation
> library(rmarkdown)
> library(knitr)
> sink('tmp.Rmd')
>
> cat('---
> title: A Title
> output:
>   html_document:
>     toc: yes
>     number_sections: yes
>   word_document:
>     reference_docx: report-template.docx
>   pdf_document:
>     number_sections: yes
>     toc: yes
> ---\n\n')
>
> for (i in 1:65) {
>   cat('ln',i,'\n\n')
> }
>
>
> mytbl <- data.frame(A=1:5, B=1:5)
>
> ## then, within a chunk:
>
> print(kable(mytbl))
>
> for (i in 66:70) {
>   cat('ln',i,'\n\n')
> }
>
> sink()
>
> render('tmp.Rmd','pdf_document')
>
> ## open tmp.pdf and see that the table breaks
> ## between the first and second pages
> ## possibly, other latex installations will be different
> ## (e.g., I have default letter-size paper, A4 will
> ## be different. Change the 1:65 until the table starts
> ## two or three lines from the bottom of the page.)
>
>
> An alternative to kable() that has a suitable arg, would be easiest, I
> suppose. I've looked at pander::pandoc.table without success.
>
> Naturally, I'd like the solution to work for word_document as well, but
> I'd be more than grateful for a pdf solution. It's irrelevant for html
> output.
>
> If it matters, am working in R at a shell command line, not within RStudio.
>
> Thanks in advance.
> -Don
>
>


From f_j_rod at hotmail.com  Thu Nov  3 13:29:49 2016
From: f_j_rod at hotmail.com (Frank S.)
Date: Thu, 3 Nov 2016 12:29:49 +0000
Subject: [R] When customizing last line, the code stops working
Message-ID: <AM5PR0402MB26891443730150D40EAFF55BBAA30@AM5PR0402MB2689.eurprd04.prod.outlook.com>

Dear all,


The function I present works perfectly when I run it as written (that is, leaving NEW LINE as commented). However, when

I try to run the same function via this mentioned line (and therefore commenting LAST LINE) R gives an error message:
Error in FUN(X[[i]], ...) : object 'dt_sp_1' not found. I do not understand why I don't get the same result.


Thanks in advance for any help!


Frank S.


all.sp <- function(age.u, open, close) {

require(data.table)
dt <- data.table( id = c(rep(1, 2), 2:4, rep(5, 2)),
   sex = as.factor(rep(c(0, 1), c(3, 4))),
   fborn =  as.Date(c("1935-07-25", "1935-07-25", "1939-07-23", "1943-10-05",
                      "1944-01-01", "1944-09-07", "1944-09-07")) )

sp <- seq(open, close, by = "year")
dt_sp <- list()
for (i in 1:length(sp)) {
  vp <- as.POSIXlt(c(as.Date("1000-01-01"), sp))
  vp$year <- vp$year - age.u
  dt.cut <- as.numeric(cut(x = as.POSIXlt(dt$fborn), breaks = vp, right = TRUE, include.lowest = TRUE))
  dt_sp[i] <- split(dt, factor(dt.cut, i))
  dt_sp[[i]] <- data.table(dt_sp[[i]])[, entry_sp := sp[i]]
  assign(paste0("dt_sp_", 1:length(sp))[i], dt_sp[[i]])
  }

  union <- rbind(dt_sp_1, dt_sp_2, dt_sp_3, dt_sp_4)     # LAST LINE: IT WORKS


  # I TRY TO CUSTOMIZE LAST LINE, BUT THEN CODE STOPS WORKING
  # union <- do.call(rbind, lapply(paste0("dt_sp_", 1:length(sp)), get))     # NEW LINE
}

# Example:
result <- all.sp(age.u = 65, open = as.Date("2007-01-01"), close = as.Date("2010-05-01"))

	[[alternative HTML version deleted]]


From nabiyogesh at gmail.com  Thu Nov  3 13:43:31 2016
From: nabiyogesh at gmail.com (Yogesh Gupta)
Date: Thu, 3 Nov 2016 21:43:31 +0900
Subject: [R] Getting error in clustering analysis of RNAseq raw reads
Message-ID: <CAJE2if1yO6e4txut+h24vDvUyd_BFn=quFTri0zKOGU5x_gtgQ@mail.gmail.com>

Dear All,

I have raw read for around 40,000 transcripts at 3 development stages in
four tissue with two biological replicate. I want to do clustering to see
the correlation between biological replicate and tissue gene expression.
For that I used the following cammand but geeting error at the end.

Please suggest me how can I resolve this analysis or suggest some other to
show the correlation through dendrogram bet tissue and replicates.


> Tree=read.table("JiYoung_rawcount2.txt", header=T)
> attach(Tree)
> Tree=na.omit(Tree)
> Tree1=Tree[,1:37]
Error in `[.data.frame`(Tree, , 1:37) : undefined columns selected
> Tree1=Tree[,1:34]
> Tree1=scale(Tree1)
> fit=kmeans(Tree1,6)
> aggregate(Tree1,by=list(fit$cluster),FUN=mean)
  Group.1 X216_5W_Ca1 X216_5W_Ca2 X216_5W_Co1 X216_5W_CO2 X216_5W_Pa1
1       1  0.58858040  0.43817608  0.36369515  0.38041543  0.35233882
2       2  2.42908764  1.91210976  1.52605404  1.58443684  1.52558080
3       3 74.03474419 67.01505049 44.83518300 55.75108909 67.08145692
4       4 22.49829929 18.97402000 24.67343279 24.17280525 19.70225908
5       5  5.76673305  5.49181254  5.86183195  5.51501833  5.60290364
6       6 -0.08598393 -0.06821359 -0.05891407 -0.06141244 -0.05887959
  X216_5W_Pa2 X216_7W_Ca1 X216_7W_Co1 X216_7W_Pa1 X216_7W_Pa2 X216_9W_Ca1
1  0.31858916  0.40034893  0.56150029  0.23379224  0.16387140  0.69427423
2  1.42676388  1.67809954  2.45417069  1.06859194  0.79864768  2.74500617
3 67.70121040 61.23933043 63.72713464 59.05530410 56.49776253 72.58425992
4 18.95159487 13.49035976 28.20156499  7.03145799  6.75403043 20.06947106
5  5.11737426  4.47486147  9.27074003  2.95413750  2.21762466  6.78433102
6 -0.05486169 -0.06000421 -0.08838463 -0.03861134 -0.02995435 -0.09670259
  X216_9W_Ca2 X216_9W_Co1 X216_9W_Co2 X216_9W_Pa1 X216_9W_Pa2 X218_5W_Ca1
1   0.6912312  0.46185370  0.45662675  0.48050371   0.4102703   0.7362048
2   2.7313814  1.94074293  1.89761725  1.89666393   1.7667720   3.1811285
3  71.1276692 65.26586773 66.96323923 77.45296500  67.2184393  62.4966761
4  19.4695963 21.52139536 21.46531283 19.59961960  14.2639218  28.3129021
5   6.4045681  5.20213688  5.12189575  6.36399774   5.5607615   8.2983905
6  -0.0956101 -0.07048408 -0.06982468 -0.07355444  -0.0635680  -0.1064461
  X218_5W_Ca2 X218_5W_Co1 X218_5W_Co2 X218_5W_Pa1 X218_5W_Pa2 X218_7W_Ca1
1   0.7562678   0.6313518  0.60735044   0.5181326  0.62021074  0.58441077
2   3.2286191   2.7650505  2.55823136   2.3362218  2.78896881  2.67308175
3  50.6247201  57.6083926 53.04016813  57.0939083 51.33990373 61.67671029
4  32.4676288  30.6562363 31.71334139  30.3348985 33.54598480 27.14293723
5   8.9548249   9.3425124  8.59189383   8.0038275  8.75233324  8.76179017
6  -0.1089077  -0.0963359 -0.09178966  -0.0826968 -0.09523603 -0.09085913
  X218_7W_Ca2 X218_7W_Co1 X218_7W_Co2 X218_7W_Pa1 X218_7W_Pa2 X218_9W_Ca1
1  0.50019728  0.56001261  0.57024565  0.44437646  0.44479186   0.8400651
2  2.24049619  2.48926833  2.44390725  2.20582519  2.23325003   3.5810738
3 64.39243727 63.50454637 61.15957849 73.95971794 69.46363454  58.0319671
4 22.66822963 26.67150058 29.07678293 19.49939666 20.82222737  28.5564765
5  7.35115400  9.66796018  9.40071423  6.68316366  7.22795784   9.6139084
6 -0.07838953 -0.08849961 -0.08905063 -0.07339317 -0.07410584  -0.1185144
  X218_9W_Ca2 X218_9W_Co1 X218_9W_Co2 X218_9W_Pa1 X218_9W_Pa2
1   0.7386879  0.64248657  0.64966599  0.49104422  0.54706215
2   3.1917475  2.75317071  2.82258034  2.24793860  2.56671355
3  63.5127116 53.12081116 57.05732779 62.67926202 67.64227975
4  27.5673337 27.47366571 25.77146680 20.28975742 20.37098419
5   8.7078573  8.37873305  8.67721724  5.92146810  6.84227840
6  -0.1070434 -0.09456489 -0.09594609 -0.07535101 -0.08380287
> Tree1<-data.frame(Tree1,fit$cluster)
> d=dist(Tree1,method="euclidean")
> fit=hclust(d,method="ward.D2")

 *** caught segfault ***
address 0x7f51c12c0228, cause 'memory not mapped'

Traceback:
 1: hclust(d, method = "ward.D2")

Possible actions:
1: abort (with core dump, if enabled)
2: normal R exit
3: exit R without saving workspace
4: exit R saving workspace
Selection:

Thanks

*Yogesh Gupta*

	[[alternative HTML version deleted]]


From Mark.Fowler at dfo-mpo.gc.ca  Thu Nov  3 14:30:16 2016
From: Mark.Fowler at dfo-mpo.gc.ca (Fowler, Mark)
Date: Thu, 3 Nov 2016 13:30:16 +0000
Subject: [R] When customizing last line, the code stops working
In-Reply-To: <AM5PR0402MB26891443730150D40EAFF55BBAA30@AM5PR0402MB2689.eurprd04.prod.outlook.com>
References: <AM5PR0402MB26891443730150D40EAFF55BBAA30@AM5PR0402MB2689.eurprd04.prod.outlook.com>
Message-ID: <88388BFE50A61F408122CBAEB917FD574E9190A1@SVNSBIOMBX02.ENT.dfo-mpo.ca>

Hi Frank,

Maybe just the quotes?

paste0("dt_sp_", 1:length(sp))
[1] "dt_sp_1" "dt_sp_2" "dt_sp_3" "dt_sp_4"
noquote(paste0("dt_sp_", 1:length(sp)))
 [1] dt_sp_1 dt_sp_2 dt_sp_3 dt_sp_4


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Frank S.
Sent: November 3, 2016 9:30 AM
To: r-help at r-project.org
Subject: [R] When customizing last line, the code stops working

Dear all,


The function I present works perfectly when I run it as written (that is, leaving NEW LINE as commented). However, when

I try to run the same function via this mentioned line (and therefore commenting LAST LINE) R gives an error message:
Error in FUN(X[[i]], ...) : object 'dt_sp_1' not found. I do not understand why I don't get the same result.


Thanks in advance for any help!


Frank S.


all.sp <- function(age.u, open, close) {

require(data.table)
dt <- data.table( id = c(rep(1, 2), 2:4, rep(5, 2)),
   sex = as.factor(rep(c(0, 1), c(3, 4))),
   fborn =  as.Date(c("1935-07-25", "1935-07-25", "1939-07-23", "1943-10-05",
                      "1944-01-01", "1944-09-07", "1944-09-07")) )

sp <- seq(open, close, by = "year")
dt_sp <- list()
for (i in 1:length(sp)) {
  vp <- as.POSIXlt(c(as.Date("1000-01-01"), sp))
  vp$year <- vp$year - age.u
  dt.cut <- as.numeric(cut(x = as.POSIXlt(dt$fborn), breaks = vp, right = TRUE, include.lowest = TRUE))
  dt_sp[i] <- split(dt, factor(dt.cut, i))
  dt_sp[[i]] <- data.table(dt_sp[[i]])[, entry_sp := sp[i]]
  assign(paste0("dt_sp_", 1:length(sp))[i], dt_sp[[i]])
  }

  union <- rbind(dt_sp_1, dt_sp_2, dt_sp_3, dt_sp_4)     # LAST LINE: IT WORKS


  # I TRY TO CUSTOMIZE LAST LINE, BUT THEN CODE STOPS WORKING
  # union <- do.call(rbind, lapply(paste0("dt_sp_", 1:length(sp)), get))     # NEW LINE
}

# Example:
result <- all.sp(age.u = 65, open = as.Date("2007-01-01"), close = as.Date("2010-05-01"))

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Thu Nov  3 14:41:26 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 3 Nov 2016 09:41:26 -0400
Subject: [R] When customizing last line, the code stops working
In-Reply-To: <AM5PR0402MB26891443730150D40EAFF55BBAA30@AM5PR0402MB2689.eurprd04.prod.outlook.com>
References: <AM5PR0402MB26891443730150D40EAFF55BBAA30@AM5PR0402MB2689.eurprd04.prod.outlook.com>
Message-ID: <e734f920-1c70-174c-bed6-2d97a64db8a4@gmail.com>

On 03/11/2016 8:29 AM, Frank S. wrote:
> Dear all,
>
>
> The function I present works perfectly when I run it as written (that is, leaving NEW LINE as commented). However, when
>
> I try to run the same function via this mentioned line (and therefore commenting LAST LINE) R gives an error message:
> Error in FUN(X[[i]], ...) : object 'dt_sp_1' not found. I do not understand why I don't get the same result.

You aren't specifying the "envir" argument to "get", so it defaults to 
the environment from which you called get, and that's the evaluation 
frame of lapply() (or maybe of do.call()).

Write the last line as

union <- do.call(rbind, lapply(paste0("dt_sp_", 1:length(sp)), get, envir = environment()))


and it works.

Duncan Murdoch
>
>
> Thanks in advance for any help!
>
>
> Frank S.
>
>
> all.sp <- function(age.u, open, close) {
>
> require(data.table)
> dt <- data.table( id = c(rep(1, 2), 2:4, rep(5, 2)),
>     sex = as.factor(rep(c(0, 1), c(3, 4))),
>     fborn =  as.Date(c("1935-07-25", "1935-07-25", "1939-07-23", "1943-10-05",
>                        "1944-01-01", "1944-09-07", "1944-09-07")) )
>
> sp <- seq(open, close, by = "year")
> dt_sp <- list()
> for (i in 1:length(sp)) {
>    vp <- as.POSIXlt(c(as.Date("1000-01-01"), sp))
>    vp$year <- vp$year - age.u
>    dt.cut <- as.numeric(cut(x = as.POSIXlt(dt$fborn), breaks = vp, right = TRUE, include.lowest = TRUE))
>    dt_sp[i] <- split(dt, factor(dt.cut, i))
>    dt_sp[[i]] <- data.table(dt_sp[[i]])[, entry_sp := sp[i]]
>    assign(paste0("dt_sp_", 1:length(sp))[i], dt_sp[[i]])
>    }
>
>    union <- rbind(dt_sp_1, dt_sp_2, dt_sp_3, dt_sp_4)     # LAST LINE: IT WORKS
>
>
>    # I TRY TO CUSTOMIZE LAST LINE, BUT THEN CODE STOPS WORKING
>    # union <- do.call(rbind, lapply(paste0("dt_sp_", 1:length(sp)), get))     # NEW LINE
> }
>
> # Example:
> result <- all.sp(age.u = 65, open = as.Date("2007-01-01"), close = as.Date("2010-05-01"))
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From f.harrell at vanderbilt.edu  Thu Nov  3 14:51:28 2016
From: f.harrell at vanderbilt.edu (Frank Harrell)
Date: Thu, 3 Nov 2016 08:51:28 -0500
Subject: [R] [R-pkgs] Massive Update to Hmisc package
Message-ID: <CAMO-wTa0yqeeJm+KhqFwTqu0ntFxj0=+k6HQkUU_-H7jeKGzpA@mail.gmail.com>

Hmisc 4.0-0 is now on CRAN.  The package has undergone a massive update.
The most user-visable changes are;

- support for Rmarkdown html notebooks

- advanced html tables using the htmlTable package and summaryM function;
can copy and paste into word processors

- support for plotly interactive graphics, e.g.
  options(grType='plotly')
  plot(describe(mydata))

- new function ggfreqScatter

- new object markupSpecs that drives LaTeX and html translations

- describe function computes new statistics and relabels Unique as Distinct

The full list of changes is below.  You can look at examples of html
notebook reports produced using the new Hmisc at
http://biostat.mc.vanderbilt.edu/Hmisc .  These include interactive plotly
graphics.

Changes in version 4.0-0 (2016-10-31)
   * summaryP: fixed exclude1 logic - was not excluding enough levels
(inconsistent use of variable names vs. labels)
* latexTranslate: any NAs in first argument changed to "" before conversion
* minor.tick: added arguments to pass through (thanks: vhorvath)
* tests/latexpng.r: test conversion of LaTeX table to png
* latexTabular: made it properly call latexTranslate separately for each
column, convert argument to matrix or data frame if a vector
* tests/latexTabular.r: new test
* latexDotchart: added call to latexTranslate to escape special characters
for LaTeX such as underscore and pound sign
* ggfreqScatter: new function
* grType: new non-exported service function to sense if plotly is in effect
* plot.describe: new function to make plotly graph for describe objects
* describe: changed output: 3 decimal places for Info, new format for
frequency table, separated logic for 10 extreme values from logic for
frequency table, significant changes to print.describe
* dotchartp: new version of dotchart3 for plotly charts
* summaryD: modified to use dotchartp if options(grType='plotly') is in
effect
* nFm: added argument html
* label.default, label.Surv, labelPlotmath: added html argument to
implement HTML markup in place of plotmath (for plotly)
* now imports htmlTable, viridis
* html.contents.data.frame: return invisibly if file=''
* htmlVerbatim: new function
* html.contents.data.frame: improved html
* html.data.frame: changed column headers from h4 to <strong>
* ggfreqScatter: added html argument
* knitrSet: was ignoring default figure h and w
* html.summaryM: new function using new version of latex.summaryM
* markupSpecs: new list object defining markup elements for latex and html,
used by summaryM
* show.html, print.html: removed; conflicted with htmltools/rmarkdown
* label: required exact match on attribute name "label" to not retrieve the
"labels" attribute (thanks: Ivan Puzek)
* htmlSN: new function to convert floating point to scientific format usint
html
* upData, cleanup.import: fix NA becoming new factor levels (thanks: Beau
Bruce)
* htmlTranslate: new function
* plotlySave: new function
* histSpikep: new function
* upData: new argument html
* plot.describe: added ggplot2 graphics, improved continuous display using
ggplotly
* labelPlotmath: cleaned up by using markupSpecs
* capitalize: fixed bug - did not cap. first letter if other letters after
it were upper case (thanks: dchiu911)
* html.summaryM: added brmsd argument to put mean, SD on separate line
* Save: changed default back to use gzip for speed
* knitrSet: used new knitr way to set aliases for option names
* latexTabular: made translate argument apply to body of table also;
implemented multi-line header
* html.data.frame: added several arguments
* describe: added html method
* html markupSpecs object: added bibliographic database utility functions
* bppltp: new service function for extended box plots for plotly
* plot.summaryM: new plotly method
* prType: new service function used to detect user settings for html, latex
for print methods
* html.contents.data.frame: removed file and append arguments and output an
html character vector instead
* prList: new function
* GiniMd: moved to Hmisc from rms
* describe: added GMD (Gini's mean difference) and relabeled unique to
distinct
* latex.summaryM: added Needspace{2.7in} before 2nd and later strata
* ggplot.summaryP: added point labels for use with plotly as hover text
* latex.summaryP: fixed bad linebreak at strata boundaries
* dotchartpl: new plotly dot chart function especially for summaryP
* plot.summaryP: new plotly method using dotchartpl when
options(grType='plotly')
* putHfig: new function to render graphics with captions in HTML reports
* pngNeedle: new function, like latexNeedle but useful with HTML reports
* html.data.frame: added width and caption arguments
* plotlyParm: list object with plotly helper functions
* summaryD, dotchartp: added symbol and col arguments
* upData: improved efficiency, added labelled class to variables without it
but having labels (e.g., data imported using haven package)
* gbayesMixPost: mixed error in posterior odds of the mixing parameter that
resulted in incorrect posterior probabilities when the variance of the
statistic was appreciable; added option to compute posterior mean




------------------------------
Frank E Harrell Jr      Professor and Chairman      School of Medicine

Department of *Biostatistics*      *Vanderbilt University*

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From o.achieng28 at students.ku.ac.ke  Thu Nov  3 10:01:04 2016
From: o.achieng28 at students.ku.ac.ke (Achieng Okelo)
Date: Thu, 3 Nov 2016 10:01:04 +0100
Subject: [R] Generating Progressively Type-II Censored Samples
Message-ID: <CAAko+jFDOJ5rJdEfmq-eUkeE8sTHDnoGXL03qycdwRSfW95ATQ@mail.gmail.com>

Dear all,

Is there a built-in function in R that can be used to generate type-II
progressively censored samples from a given distribution (like the two
parameter Lindley distribution) using the algorithm in Balakrishnan and
Sandhu (1995).

The algorithm is in the document attached.

Thank you

-- 
Disclaimer: This e-mail and attachments  are confidential and may be 
privileged and protected from discovery or disclosure. If you are not the 
intended recipient be aware that any disclosure, copying, dissemination or 
use of this e-mail or any attachments  is prohibited. If you have received 
this e-mail in error please notify the sender immediately and erase all 
copies of the message and its attachments. Any views expressed are those of 
the individual sender(s) and may not necessarily reflect the views of the 
Kenyatta University. 
<https://www.facebook.com/kenyattauni> 
<https://plus.google.com/116350724834592418591/posts> 
<https://twitter.com/KenyattaUni> 
<https://www.linkedin.com/edu/school?id=14427&trk=edu-cp-title>

From roncaglia.laura at gmail.com  Thu Nov  3 10:05:41 2016
From: roncaglia.laura at gmail.com (laura roncaglia)
Date: Thu, 3 Nov 2016 10:05:41 +0100
Subject: [R] Manage unbalanced panel with plm
Message-ID: <CAApwzovB8JrqHrnzjqkBZ4Bn2BT0VkwVbV6oY4yt61DetpXuWg@mail.gmail.com>

Dear all,

I am using an unbalanced panel dataframe to run my analysis. The df is
based on a national survey and I know the reason why the df is unbalanced.

I tried to understand if plm package corrects for unbalanced panel, so that
I can use the plm package without problem.

I create a plm.data using the following code:

dd.p <- plm.data("id", "year")

and then run fixed effect regression, like this one:

fe1 <- plm(pens ~ woman + age + sqage + high + medium, model="within",
effect="individual", data=dd.p)

So is my approach correct? Does the plm actually correct for unbalanced
panel?

Instead if I need to balance the panel I'd use this code:

dd <- dd %>%
group_by(id) %>%
fileter(n()>2)

(the df is from 2010 to 2013, filtering only id present more than 2 times
would mean that I take only people present three times)

Thanks in advance

	[[alternative HTML version deleted]]


From martyn.plummer at r-project.org  Thu Nov  3 12:56:31 2016
From: martyn.plummer at r-project.org (Martyn Plummer)
Date: Thu, 3 Nov 2016 12:56:31 +0100
Subject: [R] Five new ordinary members of the R Foundation
Message-ID: <1478174191.2318.120.camel@r-project.org>

The R Foundation has elected five new ordinary members in recognition
of their contributions to the R project:

* Jennifer Bryan?
??https://www.stat.ubc.ca/~jenny/
* Diane Cook?
??http://dicook.github.io/
* Julie Josse?
??http://juliejosse.com/
* Tomas Kalibera
??https://github.com/kalibera?
* Balasubramanian Narasimhan
??https://statistics.stanford.edu/people/balasubramanian-narasimhan

On behalf of the R Foundation I would like to extent my congratulations
and a warm welcome to the new members.

Martyn Plummer
Co-president, The R Foundation
-----------------------------------------------------------------------
This message and its attachments are strictly confidenti...{{dropped:8}}

_______________________________________________
R-announce at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From kathabaswanth1314 at gmail.com  Thu Nov  3 10:09:02 2016
From: kathabaswanth1314 at gmail.com (Katha Baswanth)
Date: Thu, 3 Nov 2016 14:39:02 +0530
Subject: [R] Fwd: Where the modification has to be done i am en countouring
	problem
In-Reply-To: <CAEj4VWnmnF5aZhDbsEDu0heNRMkTSifU1ptWUVgwPVX9fJdFrA@mail.gmail.com>
References: <CAEj4VWnmnF5aZhDbsEDu0heNRMkTSifU1ptWUVgwPVX9fJdFrA@mail.gmail.com>
Message-ID: <CAEj4VWmkrLhu2uHJN2S7gj11KxeqOOqZk63DhL9JuAr1kf8GoA@mail.gmail.com>

---------- Forwarded message ----------
From: Katha Baswanth <kathabaswanth1314 at gmail.com>
Date: Thu, Nov 3, 2016 at 2:36 PM
Subject: Where the modification has to be done i am en countouring problem
To: yang.feng at canada.ca


Loading required package: tcltk
Error in daynormm[(i - i1):(i + i1), ] : subscript out of bounds


-- 
With Regards
Katha Reddy Baswanth Kumar
Senior Research Fellow
India Meteorological Department
MCHyderabad
Phone No:9912213003



-- 
With Regards
Katha Reddy Baswanth Kumar
Senior Research Fellow
India Meteorological Department
MCHyderabad
Phone No:9912213003

	[[alternative HTML version deleted]]


From r.wobben at home.nl  Thu Nov  3 17:40:07 2016
From: r.wobben at home.nl (Roelof Wobben)
Date: Thu, 3 Nov 2016 17:40:07 +0100
Subject: [R] coursera course - assgnment - error : Error in
 which(!is.Na(content[["sulfate"]])) : , could not find function "is.Na"
Message-ID: <b0d9a4f4-3082-147a-dcaf-ae19476da506@home.nl>

Hello,

I want to try to get all the data of a cloumn namend "sulfate" which is 
not Na.

So I did :

corr <- function(directory, threshold = 0) {

   # lezen van alle bestanden in  de directory

   file_list <-  list.files( directory, pattern = "*.csv", full.names = 
TRUE)

    # lezen van alle bestanden in a loop
   #  1) bepalen van de complete_cases
   #  2) controleren of het hoger/lager is dan de treshold.
   #  3) als het hoger is.
   #     a) lezen van de nitraat en controleren of het geen Na is
   #     b) lezen van de sulfaat en controleren of het geen Na is.
   #     c) als beide geen Na zijn, dan de correlatie bepalen

   for(file in file_list) {
     content <- read.csv(file)
     if (complete.cases(content) > 0) {
       sulfate <- which (!is.Na(content[["sulfate"]]))
       print(sulfate)

     }

     }

   }

but now I see the above error.

What did I do wrong ?

Regards,

Roelof


From ruipbarradas at sapo.pt  Thu Nov  3 18:34:31 2016
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Thu, 03 Nov 2016 17:34:31 +0000
Subject: [R] coursera course - assgnment - error : Error in
 which(!is.Na(content[["sulfate"]])) : , could not find function "is.Na"
In-Reply-To: <b0d9a4f4-3082-147a-dcaf-ae19476da506@home.nl>
References: <b0d9a4f4-3082-147a-dcaf-ae19476da506@home.nl>
Message-ID: <581B7527.9020004@sapo.pt>

Hello again,

I've just read your code more carefully and I believe you should read 
the help page ?complete.cases and run the examples therein.
Tip: don't compare the return value with 0, it's nonsense.

Rui Barradas

Em 03-11-2016 16:40, Roelof Wobben escreveu:
> Hello,
>
> I want to try to get all the data of a cloumn namend "sulfate" which is
> not Na.
>
> So I did :
>
> corr <- function(directory, threshold = 0) {
>
>    # lezen van alle bestanden in  de directory
>
>    file_list <-  list.files( directory, pattern = "*.csv", full.names =
> TRUE)
>
>     # lezen van alle bestanden in a loop
>    #  1) bepalen van de complete_cases
>    #  2) controleren of het hoger/lager is dan de treshold.
>    #  3) als het hoger is.
>    #     a) lezen van de nitraat en controleren of het geen Na is
>    #     b) lezen van de sulfaat en controleren of het geen Na is.
>    #     c) als beide geen Na zijn, dan de correlatie bepalen
>
>    for(file in file_list) {
>      content <- read.csv(file)
>      if (complete.cases(content) > 0) {
>        sulfate <- which (!is.Na(content[["sulfate"]]))
>        print(sulfate)
>
>      }
>
>      }
>
>    }
>
> but now I see the above error.
>
> What did I do wrong ?
>
> Regards,
>
> Roelof
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ruipbarradas at sapo.pt  Thu Nov  3 18:31:38 2016
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Thu, 03 Nov 2016 17:31:38 +0000
Subject: [R] coursera course - assgnment - error : Error in
 which(!is.Na(content[["sulfate"]])) : , could not find function "is.Na"
In-Reply-To: <b0d9a4f4-3082-147a-dcaf-ae19476da506@home.nl>
References: <b0d9a4f4-3082-147a-dcaf-ae19476da506@home.nl>
Message-ID: <581B747A.5020303@sapo.pt>

Hello,

Just read the error message more carefully.
The function name is the (obvious) problem! Try, with all lowercase,

is.na()

Hope this helps,

Rui Barradas

Em 03-11-2016 16:40, Roelof Wobben escreveu:
> Hello,
>
> I want to try to get all the data of a cloumn namend "sulfate" which is
> not Na.
>
> So I did :
>
> corr <- function(directory, threshold = 0) {
>
>    # lezen van alle bestanden in  de directory
>
>    file_list <-  list.files( directory, pattern = "*.csv", full.names =
> TRUE)
>
>     # lezen van alle bestanden in a loop
>    #  1) bepalen van de complete_cases
>    #  2) controleren of het hoger/lager is dan de treshold.
>    #  3) als het hoger is.
>    #     a) lezen van de nitraat en controleren of het geen Na is
>    #     b) lezen van de sulfaat en controleren of het geen Na is.
>    #     c) als beide geen Na zijn, dan de correlatie bepalen
>
>    for(file in file_list) {
>      content <- read.csv(file)
>      if (complete.cases(content) > 0) {
>        sulfate <- which (!is.Na(content[["sulfate"]]))
>        print(sulfate)
>
>      }
>
>      }
>
>    }
>
> but now I see the above error.
>
> What did I do wrong ?
>
> Regards,
>
> Roelof
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marammagdysalem at gmail.com  Thu Nov  3 18:55:50 2016
From: marammagdysalem at gmail.com (Maram SAlem)
Date: Thu, 3 Nov 2016 19:55:50 +0200
Subject: [R] The code itself disappears after starting to execute the for
	loop
Message-ID: <59BE1118-9D4B-4298-B605-9BADFBC43E88@gmail.com>

Hi all,

I've a question concerning the R 3.3.1 version. I have a long code that I used to run on versions earlier to the 3.3.1 version, and when I copied the code to the R console, I can still see the code while the loop is executing , along with the output printed after each iteration of the loop. 

Now, on the 3.3.1 version, after I copy the code to the console, it disappears and I only see the printed output of only one iteration at a time, that is, after the first iteration the printed output disappears ( though it's only 6 lines, just giving me some guidance, not a long output). 
This is causing me some problems, so I don't know if there is a general option for R that enables me to still see the code and the output of all the iterations till the loop is over, as was the case with earlier R versions. 

I didn't include the code as it's a long one. 

Thanks a lot in advance,

Maram


Sent from my iPhone

From dwinsemius at comcast.net  Thu Nov  3 18:59:33 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 3 Nov 2016 10:59:33 -0700
Subject: [R] coursera course - assgnment - error : Error in
	which(!is.Na(content[["sulfate"]])) : ,
	could not find function "is.Na"
In-Reply-To: <b0d9a4f4-3082-147a-dcaf-ae19476da506@home.nl>
References: <b0d9a4f4-3082-147a-dcaf-ae19476da506@home.nl>
Message-ID: <BFBB4461-CC1A-4843-8F4B-606F4BDDA1F3@comcast.net>


> On Nov 3, 2016, at 9:40 AM, Roelof Wobben <r.wobben at home.nl> wrote:
> 
> Hello,
> 
> I want to try to get all the data of a cloumn namend "sulfate" which is not Na.
> 
> So I did :
> 
> corr <- function(directory, threshold = 0) {
> 
>  # lezen van alle bestanden in  de directory
> 
>  file_list <-  list.files( directory, pattern = "*.csv", full.names = TRUE)
> 
>   # lezen van alle bestanden in a loop
>  #  1) bepalen van de complete_cases
>  #  2) controleren of het hoger/lager is dan de treshold.
>  #  3) als het hoger is.
>  #     a) lezen van de nitraat en controleren of het geen Na is
>  #     b) lezen van de sulfaat en controleren of het geen Na is.
>  #     c) als beide geen Na zijn, dan de correlatie bepalen
> 
>  for(file in file_list) {
>    content <- read.csv(file)
>    if (complete.cases(content) > 0) {
>      sulfate <- which (!is.Na(content[["sulfate"]]))
>      print(sulfate)
> 
>    }
> 
>    }
> 
>  }
> 
> but now I see the above error.
> 
> What did I do wrong ?

You are sending a request for academic assistance to a mailing list that specifically describes an expectation that you will use the academic assistance resources are your institution of higher learning. Since Coursera is a fee-based enterprise now, I would expect that they have some sort of web-page interface for collaborating with your coursemates.

For this and other advice about constructing non-homework questions likely to get answers on Rhelp, you should read the Posting Guide whose link is appended to every distributed post to Rhelp.

-- 
David
> 
> Regards,
> 
> Roelof
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jholtman at gmail.com  Thu Nov  3 19:30:14 2016
From: jholtman at gmail.com (jim holtman)
Date: Thu, 3 Nov 2016 14:30:14 -0400
Subject: [R] The code itself disappears after starting to execute the
	for loop
In-Reply-To: <59BE1118-9D4B-4298-B605-9BADFBC43E88@gmail.com>
References: <59BE1118-9D4B-4298-B605-9BADFBC43E88@gmail.com>
Message-ID: <CAAxdm-5FWvfnpwEJtr5eNw0AKv+cjOfgXfQKTLSmYMkk1RgPcA@mail.gmail.com>

A little more information would help.  How exactly are out creating the
output to the console?  Are you using 'print', 'cat' or something else?  Do
you have buffered output checked on the GUI (you probably don't want it
checked or you output will be delayed till the buffer is full -- this might
be the cause of your problem.


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Thu, Nov 3, 2016 at 1:55 PM, Maram SAlem <marammagdysalem at gmail.com>
wrote:

> Hi all,
>
> I've a question concerning the R 3.3.1 version. I have a long code that I
> used to run on versions earlier to the 3.3.1 version, and when I copied the
> code to the R console, I can still see the code while the loop is executing
> , along with the output printed after each iteration of the loop.
>
> Now, on the 3.3.1 version, after I copy the code to the console, it
> disappears and I only see the printed output of only one iteration at a
> time, that is, after the first iteration the printed output disappears (
> though it's only 6 lines, just giving me some guidance, not a long output).
> This is causing me some problems, so I don't know if there is a general
> option for R that enables me to still see the code and the output of all
> the iterations till the loop is over, as was the case with earlier R
> versions.
>
> I didn't include the code as it's a long one.
>
> Thanks a lot in advance,
>
> Maram
>
>
> Sent from my iPhone
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From tgraves_cs at yahoo.com  Thu Nov  3 21:44:06 2016
From: tgraves_cs at yahoo.com (Tom Graves)
Date: Thu, 3 Nov 2016 20:44:06 +0000 (UTC)
Subject: [R] install R in relative path
In-Reply-To: <22535.12519.823865.163315@stat.math.ethz.ch>
References: <289312802.345844.1476824814057.ref@mail.yahoo.com>
	<289312802.345844.1476824814057@mail.yahoo.com>
	<22535.12519.823865.163315@stat.math.ethz.ch>
Message-ID: <2125285045.777926.1478205846447@mail.yahoo.com>

Do you know if there is anyway to not have to put it in the PATH or symlink to the original build location? ?I was hoping just calling it in relative path like ./R-3.3.1/bin/Rscript would do it but it errors?Rscript execution error: No such file or directory. ?is there any sort of RPATH (similar to like a PYTHONPATH) that I could use. ?It looks like just building it is still expecting it to be in the path it was built. ?In your example you are still symlinking R to be the original build location so everything is there.
What I need to do is build it, tar it up and ship it with my hadoop job which would untar it into something like ./R_install/.... ?I need to point to that ./R_install/bin/Rscript. ?
I am currently using R-3.2.1 so I'll try upgrading too.
Thanks,Tom 

    On Wednesday, October 19, 2016 3:38 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
 

 >>>>> Tom Graves via R-help <r-help at r-project.org>
>>>>>? ? on Tue, 18 Oct 2016 21:06:54 +0000 writes:

? ? > Hello everyone, I am trying to figure out if I can install
? ? > R in a relative path?

Yes.? Even better you don't have to "install" it at all in the
strict sense.
Just *build* it and run it from the build directory.

If you use the source tarball 
currently,? R-3.3.1.tar.gz,? in 12 days will be R-3.2.2.tar.gz

Let's assume you'd want everything in your

 $HOME/R-inst/

Then you do

----------------------------------------------------

cd ~/R-inst
tar xfz <whereever>/R-3.3.1.tar.gz
? ? # now has created? R-3.3.1
? ? # we strongly recommend to use a *separate* build directory :
mkdir R-3.3.1-build
cd? R-3.3.1-build
../R-3.3.1/configure
make
[[elided Yahoo spam]]
make check-all??? 

----------------------------------------------------

Note that you do *NEVER* type? 'make install'? in the above setup

The only important remaining step is
make a symbolic link of? ? R-3.3.1-build/bin/R
to a directory part of your PATH.? If you are on a standard
unix/linux/(Mac?) setup :

? mkdir -p ~/bin
? cd ~/bin
? ln -s ~/R-inst/R-3.3.1-build/bin/R .

Alternatively, you could do

? export PATH=$HOME/R-inst/R-3.3.1-build/bin/R:$PATH

but I never do that.
Indeed, I use many R versions in this way, and in the above case
would use

? ln -s? ........../R-3.3.1-build/bin/R? R-3.3.1

and then have R-3.0.0, R-3.0.1, ..., R-3.2.5? R-3.3.0? R-3.3.1 
all in my PATH and all via symbolic links

(and ESS = Emacs Speaks Statistics finds all these
 automagically, so I can each start easily from within Emacs).

Martin Maechler
ETH Zurich

? ? > ?The reason I need to do this is to
? ? > send R along to a Hadoop cluster so that I can use sparkR
? ? > with the R version I shipped. The Hadoop cluster doesn't
? ? > have R installed and the admin won't install it.?? I tried
? ? > a few things but the things I had tried didn't work. Are
? ? > there any options to configure or PATHs I could use to do
? ? > this?? Any help is appreciated.? Thanks,Tom [[alternative
? ? > HTML version deleted]]

? ? > ______________________________________________
? ? > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
? ? > more, see https://stat.ethz.ch/mailman/listinfo/r-help
? ? > PLEASE do read the posting guide
? ? > http://www.R-project.org/posting-guide.html
? ? > and provide commented, minimal, self-contained, reproducible code.


   
	[[alternative HTML version deleted]]


From dimitri.liakhovitski at gmail.com  Thu Nov  3 22:05:12 2016
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Thu, 3 Nov 2016 17:05:12 -0400
Subject: [R] multiplying a matrix by a vector
Message-ID: <CAN2xGJabZSAq+Z=fFEB_nuNKHGKQExMB2L3aifnQzZMCouC34w@mail.gmail.com>

Hello!

I have a matrix x and a vector y:

x <- matrix(1:6, ncol = 2)
y <- c(2,3)

I need to multiply the first column of x by 2 (y[1]) and the second
column of x by 3 (y[2]).

Of course, I could do this - but it's column by column:

x[,1] <- x[,1] * y[1]
x[,2] <- x[,2] * y[2]
x

Or I could repeat each element of y and multiply two matrices - that's better:

rep.row<-function(x,n){
  matrix(rep(x,each=n),nrow=n)
}
y <- rep.row(y, nrow(x))
x * y

However, maybe there is a more elegant r-like way of doing it?
Thank you!

-- 
Dimitri Liakhovitski


From ruipbarradas at sapo.pt  Thu Nov  3 22:32:39 2016
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Thu, 03 Nov 2016 21:32:39 +0000
Subject: [R] multiplying a matrix by a vector
In-Reply-To: <CAN2xGJabZSAq+Z=fFEB_nuNKHGKQExMB2L3aifnQzZMCouC34w@mail.gmail.com>
References: <CAN2xGJabZSAq+Z=fFEB_nuNKHGKQExMB2L3aifnQzZMCouC34w@mail.gmail.com>
Message-ID: <581BACF7.9000201@sapo.pt>

Hello,

Take advantage that in R '*' recycles its arguments (the shorter one) 
and that the operation is performed column-wise:

t(y * t(x))

Hope this helps,

Rui Barradas

Em 03-11-2016 21:05, Dimitri Liakhovitski escreveu:
> Hello!
>
> I have a matrix x and a vector y:
>
> x <- matrix(1:6, ncol = 2)
> y <- c(2,3)
>
> I need to multiply the first column of x by 2 (y[1]) and the second
> column of x by 3 (y[2]).
>
> Of course, I could do this - but it's column by column:
>
> x[,1] <- x[,1] * y[1]
> x[,2] <- x[,2] * y[2]
> x
>
> Or I could repeat each element of y and multiply two matrices - that's better:
>
> rep.row<-function(x,n){
>    matrix(rep(x,each=n),nrow=n)
> }
> y <- rep.row(y, nrow(x))
> x * y
>
> However, maybe there is a more elegant r-like way of doing it?
> Thank you!
>


From sarah.goslee at gmail.com  Thu Nov  3 22:33:08 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 3 Nov 2016 17:33:08 -0400
Subject: [R] multiplying a matrix by a vector
In-Reply-To: <CAN2xGJabZSAq+Z=fFEB_nuNKHGKQExMB2L3aifnQzZMCouC34w@mail.gmail.com>
References: <CAN2xGJabZSAq+Z=fFEB_nuNKHGKQExMB2L3aifnQzZMCouC34w@mail.gmail.com>
Message-ID: <CAM_vjuktunUoBvhN42iPaNZuGLC+P9QdkDA4WM538gSn2Qq5ug@mail.gmail.com>

Like this?

> sweep(x, 2, y, "*")
     [,1] [,2]
[1,]    2   12
[2,]    4   15
[3,]    6   18
>


On Thu, Nov 3, 2016 at 5:05 PM, Dimitri Liakhovitski
<dimitri.liakhovitski at gmail.com> wrote:
> Hello!
>
> I have a matrix x and a vector y:
>
> x <- matrix(1:6, ncol = 2)
> y <- c(2,3)
>
> I need to multiply the first column of x by 2 (y[1]) and the second
> column of x by 3 (y[2]).
>
> Of course, I could do this - but it's column by column:
>
> x[,1] <- x[,1] * y[1]
> x[,2] <- x[,2] * y[2]
> x
>
> Or I could repeat each element of y and multiply two matrices - that's better:
>
> rep.row<-function(x,n){
>   matrix(rep(x,each=n),nrow=n)
> }
> y <- rep.row(y, nrow(x))
> x * y
>
> However, maybe there is a more elegant r-like way of doing it?
> Thank you!
>
> --
> Dimitri Liakhovitski
>

-- 
Sarah Goslee
http://www.functionaldiversity.org


From cbenjami at BTBOCES.ORG  Fri Nov  4 01:57:06 2016
From: cbenjami at BTBOCES.ORG (Courtney Benjamin)
Date: Fri, 4 Nov 2016 00:57:06 +0000
Subject: [R] Resetting Baseline Level of Predictor in svyglm Function
In-Reply-To: <CAOwvMDx8krhLsJUD7Z9TJdwxB_QjiBOt-uBeBCQA3GpLwzvRUA@mail.gmail.com>
References: <1477962352382.94941@BTBOCES.ORG>,
	<CAOwvMDx8krhLsJUD7Z9TJdwxB_QjiBOt-uBeBCQA3GpLwzvRUA@mail.gmail.com>
Message-ID: <1478221025734.58088@BTBOCES.ORG>

Thank you, Anthony; it worked flawlessly.?


Courtney Benjamin

Broome-Tioga BOCES

Automotive Technology II Teacher

Located at Gault Toyota

Doctoral Candidate-Educational Theory & Practice

State University of New York at Binghamton

cbenjami at btboces.org<mailto:cbenjami at btboces.org>

607-763-8633

________________________________
From: Anthony Damico <ajdamico at gmail.com>
Sent: Tuesday, November 1, 2016 3:20 AM
To: Courtney Benjamin
Cc: r-help at r-project.org
Subject: Re: [R] Resetting Baseline Level of Predictor in svyglm Function

hi, i think you want

elsq1ch_brr <- update( elsq1ch_brr , F1HIMATH = relevel(F1HIMATH,"PreAlg or Less") )





On Mon, Oct 31, 2016 at 9:05 PM, Courtney Benjamin <cbenjami at btboces.org<mailto:cbenjami at btboces.org>> wrote:
Hello R Users:

I am using the survey package in R for modeling with complex survey data. I am trying to reset the baseline level of certain predictor variables being used in a logistic regression without success. The following is a reproducible example:

library(RCurl)
library(survey)

data <- getURL("https://raw.githubusercontent.com/cbenjamin1821/careertech-ed/master/elsq1adj.csv")
elsq1ch <- read.csv(text = data)

#Specifying the svyrepdesign object which applies the BRR weights
elsq1ch_brr<-svrepdesign(variables = elsq1ch[,1:16], repweights = elsq1ch[,18:217], weights = elsq1ch[,17], combined.weights = TRUE, type = "BRR")
elsq1ch_brr

#Log. Reg. model
allCC <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+F1RGPP2+F1HIMATH+F1RTRCC,family="binomial",design=elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1,na.action=na.omit)
summary(allCC)

##Attempting to reset baseline level for predictor variable
#Both attempts did not work
elsq1ch$F1HIMATH <- C(elsq1ch$F1HIMATH,contr.treatment, base=1)
elsq1ch$F1HIMATH <- relevel(elsq1ch$F1HIMATH,"PreAlg or Less")

#Log. Reg. model with no changes in baseline levels for the predictors
allCC <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+F1RGPP2+F1HIMATH+F1RTRCC,family="binomial",design=elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1,na.action=na.omit)
summary(allCC)


Any guidance is greatly appreciated.?

Sincerely,

Courtney?

Courtney Benjamin

Broome-Tioga BOCES

Automotive Technology II Teacher

Located at Gault Toyota

Doctoral Candidate-Educational Theory & Practice

State University of New York at Binghamton

cbenjami at btboces.org<mailto:cbenjami at btboces.org><mailto:cbenjami at btboces.org<mailto:cbenjami at btboces.org>>

607-763-8633<tel:607-763-8633>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From kagba2006 at yahoo.com  Fri Nov  4 08:00:34 2016
From: kagba2006 at yahoo.com (FMH)
Date: Fri, 4 Nov 2016 07:00:34 +0000 (UTC)
Subject: [R] Continuous Wavelet tranform in Biwavelet Package
References: <1400673742.22398.1478242834602.ref@mail.yahoo.com>
Message-ID: <1400673742.22398.1478242834602@mail.yahoo.com>

Dear All,
I have 15 minutes of temperature data for 3 years and would like calculate and plot the CWT using wt command in Biwavelet package. 

Appreciate if someone could tell me the way to define it in wt command.
Thank you,
	[[alternative HTML version deleted]]


From f_j_rod at hotmail.com  Fri Nov  4 12:33:11 2016
From: f_j_rod at hotmail.com (Frank S.)
Date: Fri, 4 Nov 2016 11:33:11 +0000
Subject: [R] When customizing last line, the code stops working
In-Reply-To: <e734f920-1c70-174c-bed6-2d97a64db8a4@gmail.com>
References: <AM5PR0402MB26891443730150D40EAFF55BBAA30@AM5PR0402MB2689.eurprd04.prod.outlook.com>,
	<e734f920-1c70-174c-bed6-2d97a64db8a4@gmail.com>
Message-ID: <HE1PR0402MB26984961AABAD4058082A3CBBAA20@HE1PR0402MB2698.eurprd04.prod.outlook.com>

Many thanks to Mark and Duncan for your help!


As Duncan Murdoch indicated in his answer, it was a matter of envir argument from function do.call.


Thank you!


Frank S.

________________________________
De: Duncan Murdoch <murdoch.duncan at gmail.com>
Enviado: jueves, 3 de noviembre de 2016 14:41:26
Para: Frank S.; r-help at r-project.org
Asunto: Re: [R] When customizing last line, the code stops working

On 03/11/2016 8:29 AM, Frank S. wrote:
> Dear all,
>
>
> The function I present works perfectly when I run it as written (that is, leaving NEW LINE as commented). However, when
>
> I try to run the same function via this mentioned line (and therefore commenting LAST LINE) R gives an error message:
> Error in FUN(X[[i]], ...) : object 'dt_sp_1' not found. I do not understand why I don't get the same result.

You aren't specifying the "envir" argument to "get", so it defaults to
the environment from which you called get, and that's the evaluation
frame of lapply() (or maybe of do.call()).

Write the last line as

union <- do.call(rbind, lapply(paste0("dt_sp_", 1:length(sp)), get, envir = environment()))


and it works.

Duncan Murdoch
>
>
[[elided Hotmail spam]]
>
>
> Frank S.
>
>
> all.sp <- function(age.u, open, close) {
>
> require(data.table)
> dt <- data.table( id = c(rep(1, 2), 2:4, rep(5, 2)),
>     sex = as.factor(rep(c(0, 1), c(3, 4))),
>     fborn =  as.Date(c("1935-07-25", "1935-07-25", "1939-07-23", "1943-10-05",
>                        "1944-01-01", "1944-09-07", "1944-09-07")) )
>
> sp <- seq(open, close, by = "year")
> dt_sp <- list()
> for (i in 1:length(sp)) {
>    vp <- as.POSIXlt(c(as.Date("1000-01-01"), sp))
>    vp$year <- vp$year - age.u
>    dt.cut <- as.numeric(cut(x = as.POSIXlt(dt$fborn), breaks = vp, right = TRUE, include.lowest = TRUE))
>    dt_sp[i] <- split(dt, factor(dt.cut, i))
>    dt_sp[[i]] <- data.table(dt_sp[[i]])[, entry_sp := sp[i]]
>    assign(paste0("dt_sp_", 1:length(sp))[i], dt_sp[[i]])
>    }
>
>    union <- rbind(dt_sp_1, dt_sp_2, dt_sp_3, dt_sp_4)     # LAST LINE: IT WORKS
>
>
>    # I TRY TO CUSTOMIZE LAST LINE, BUT THEN CODE STOPS WORKING
>    # union <- do.call(rbind, lapply(paste0("dt_sp_", 1:length(sp)), get))     # NEW LINE
> }
>
> # Example:
> result <- all.sp(age.u = 65, open = as.Date("2007-01-01"), close = as.Date("2010-05-01"))
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



	[[alternative HTML version deleted]]


From f_j_rod at hotmail.com  Fri Nov  4 13:22:43 2016
From: f_j_rod at hotmail.com (Frank S.)
Date: Fri, 4 Nov 2016 12:22:43 +0000
Subject: [R] When customizing last line, the code stops working
In-Reply-To: <HE1PR0402MB26984961AABAD4058082A3CBBAA20@HE1PR0402MB2698.eurprd04.prod.outlook.com>
References: <AM5PR0402MB26891443730150D40EAFF55BBAA30@AM5PR0402MB2689.eurprd04.prod.outlook.com>,
	<e734f920-1c70-174c-bed6-2d97a64db8a4@gmail.com>,
	<HE1PR0402MB26984961AABAD4058082A3CBBAA20@HE1PR0402MB2698.eurprd04.prod.outlook.com>
Message-ID: <HE1PR0402MB2698699CA3E668C0514BB26CBAA20@HE1PR0402MB2698.eurprd04.prod.outlook.com>

A clarification regarding my previous post for those ineterested in the issue:


Looking carefully at Duncan's solution, the key point would be introducing envir = environment()

as argument of lapply function (the default envir of do.call function is parent.frame() ).

)

 union <- do.call(
               what = rbind,
               args = lapply(
                        X = paste0("dt_sp_", 1:length(sp)),
                        FUN = get,
                       envir = environment()
                       )
 )


________________________________
De: Frank S.
Enviado: viernes, 4 de noviembre de 2016 12:33:11
Para: Duncan Murdoch; r-help at r-project.org; Mark.Fowler at dfo-mpo.gc.ca
Asunto: Re: [R] When customizing last line, the code stops working


[[elided Hotmail spam]]


As Duncan Murdoch indicated in his answer, it was a matter of envir argument from function do.call.


Thank you!


Frank S.

________________________________
De: Duncan Murdoch <murdoch.duncan at gmail.com>
Enviado: jueves, 3 de noviembre de 2016 14:41:26
Para: Frank S.; r-help at r-project.org
Asunto: Re: [R] When customizing last line, the code stops working

On 03/11/2016 8:29 AM, Frank S. wrote:
> Dear all,
>
>
> The function I present works perfectly when I run it as written (that is, leaving NEW LINE as commented). However, when
>
> I try to run the same function via this mentioned line (and therefore commenting LAST LINE) R gives an error message:
> Error in FUN(X[[i]], ...) : object 'dt_sp_1' not found. I do not understand why I don't get the same result.

You aren't specifying the "envir" argument to "get", so it defaults to
the environment from which you called get, and that's the evaluation
frame of lapply() (or maybe of do.call()).

Write the last line as

union <- do.call(rbind, lapply(paste0("dt_sp_", 1:length(sp)), get, envir = environment()))


and it works.

Duncan Murdoch
>
>
[[elided Hotmail spam]]
>
>
> Frank S.
>
>
> all.sp <- function(age.u, open, close) {
>
> require(data.table)
> dt <- data.table( id = c(rep(1, 2), 2:4, rep(5, 2)),
>     sex = as.factor(rep(c(0, 1), c(3, 4))),
>     fborn =  as.Date(c("1935-07-25", "1935-07-25", "1939-07-23", "1943-10-05",
>                        "1944-01-01", "1944-09-07", "1944-09-07")) )
>
> sp <- seq(open, close, by = "year")
> dt_sp <- list()
> for (i in 1:length(sp)) {
>    vp <- as.POSIXlt(c(as.Date("1000-01-01"), sp))
>    vp$year <- vp$year - age.u
>    dt.cut <- as.numeric(cut(x = as.POSIXlt(dt$fborn), breaks = vp, right = TRUE, include.lowest = TRUE))
>    dt_sp[i] <- split(dt, factor(dt.cut, i))
>    dt_sp[[i]] <- data.table(dt_sp[[i]])[, entry_sp := sp[i]]
>    assign(paste0("dt_sp_", 1:length(sp))[i], dt_sp[[i]])
>    }
>
>    union <- rbind(dt_sp_1, dt_sp_2, dt_sp_3, dt_sp_4)     # LAST LINE: IT WORKS
>
>
>    # I TRY TO CUSTOMIZE LAST LINE, BUT THEN CODE STOPS WORKING
>    # union <- do.call(rbind, lapply(paste0("dt_sp_", 1:length(sp)), get))     # NEW LINE
> }
>
> # Example:
> result <- all.sp(age.u = 65, open = as.Date("2007-01-01"), close = as.Date("2010-05-01"))
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



	[[alternative HTML version deleted]]


From kevin.thorpe at utoronto.ca  Fri Nov  4 13:37:59 2016
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Fri, 4 Nov 2016 08:37:59 -0400
Subject: [R] Converting a list to a data frame
Message-ID: <b1593d86-4154-3319-b85a-a3a8cbf1686b@utoronto.ca>

There is probably a very simple elegant way to do this, but I have been 
unable to find it. Here is a toy example. Suppose I have a list of data 
frames like this.

  print(x <- 
list('1'=data.frame(id=1:4,expand.grid(x1=0:1,x2=0:1)),'2'=data.frame(id=5:8,expand.grid(x1=2:3,x2=2:3))))
$`1`
   id x1 x2
1  1  0  0
2  2  1  0
3  3  0  1
4  4  1  1

$`2`
   id x1 x2
1  5  2  2
2  6  3  2
3  7  2  3
4  8  3  3

The real application will have more than 2 elements so I'm looking for a 
general approach. I basically want to rbind the data frames in each list 
element and add a variable that adds the element name. In this example 
the result would look something like this.

rbind(data.frame(set='1',x[[1]]),data.frame(set='2',x[[2]]))
   set id x1 x2
1   1  1  0  0
2   1  2  1  0
3   1  3  0  1
4   1  4  1  1
5   2  5  2  2
6   2  6  3  2
7   2  7  2  3
8   2  8  3  3

Obviously, for 2 elements the simple rbind works but I would like a 
general solution for arbitrary length lists. Hopefully that is clear.

Kevin

-- 
Kevin E. Thorpe
Head of Biostatistics,  Applied Health Research Centre (AHRC)
Li Ka Shing Knowledge Institute of St. Michael's Hospital
Assistant Professor, Dalla Lana School of Public Health
University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.3016


From cdetermanjr at gmail.com  Fri Nov  4 13:50:53 2016
From: cdetermanjr at gmail.com (Charles Determan)
Date: Fri, 4 Nov 2016 07:50:53 -0500
Subject: [R] Converting a list to a data frame
In-Reply-To: <b1593d86-4154-3319-b85a-a3a8cbf1686b@utoronto.ca>
References: <b1593d86-4154-3319-b85a-a3a8cbf1686b@utoronto.ca>
Message-ID: <CAKxd1KN=dicg2-Zx_uwbQ55mHj5Tp_V59EiYrrQ2=TwmHJ+uvA@mail.gmail.com>

Hi Kevin,

There may be a more elegant way but the following do.call and lapply should
solve your problem.

do.call(rbind, lapply(seq(length(x)), function(i) data.frame(set=i,
x[[i]])))

Regards,
Charles

On Fri, Nov 4, 2016 at 7:37 AM, Kevin E. Thorpe <kevin.thorpe at utoronto.ca>
wrote:

> There is probably a very simple elegant way to do this, but I have been
> unable to find it. Here is a toy example. Suppose I have a list of data
> frames like this.
>
>  print(x <- list('1'=data.frame(id=1:4,expand.grid(x1=0:1,x2=0:1)),'2'=
> data.frame(id=5:8,expand.grid(x1=2:3,x2=2:3))))
> $`1`
>   id x1 x2
> 1  1  0  0
> 2  2  1  0
> 3  3  0  1
> 4  4  1  1
>
> $`2`
>   id x1 x2
> 1  5  2  2
> 2  6  3  2
> 3  7  2  3
> 4  8  3  3
>
> The real application will have more than 2 elements so I'm looking for a
> general approach. I basically want to rbind the data frames in each list
> element and add a variable that adds the element name. In this example the
> result would look something like this.
>
> rbind(data.frame(set='1',x[[1]]),data.frame(set='2',x[[2]]))
>   set id x1 x2
> 1   1  1  0  0
> 2   1  2  1  0
> 3   1  3  0  1
> 4   1  4  1  1
> 5   2  5  2  2
> 6   2  6  3  2
> 7   2  7  2  3
> 8   2  8  3  3
>
> Obviously, for 2 elements the simple rbind works but I would like a
> general solution for arbitrary length lists. Hopefully that is clear.
>
> Kevin
>
> --
> Kevin E. Thorpe
> Head of Biostatistics,  Applied Health Research Centre (AHRC)
> Li Ka Shing Knowledge Institute of St. Michael's Hospital
> Assistant Professor, Dalla Lana School of Public Health
> University of Toronto
> email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.3016
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From cof at qualityexcellence.es  Fri Nov  4 14:26:20 2016
From: cof at qualityexcellence.es (Carlos Ortega)
Date: Fri, 4 Nov 2016 14:26:20 +0100
Subject: [R] Converting a list to a data frame
In-Reply-To: <b1593d86-4154-3319-b85a-a3a8cbf1686b@utoronto.ca>
References: <b1593d86-4154-3319-b85a-a3a8cbf1686b@utoronto.ca>
Message-ID: <CAOKbq8hTD-KUA8hj4Mh1LPLK44CkR7ipOfgSzry-F5GBZ7PbcA@mail.gmail.com>

Hi,

You have also "rbindlist()" function in package "data.table" that does
exactly what you need.

Kind Regards,
Carlos Ortega
www.qualityexcellence.es



2016-11-04 13:37 GMT+01:00 Kevin E. Thorpe <kevin.thorpe at utoronto.ca>:

> There is probably a very simple elegant way to do this, but I have been
> unable to find it. Here is a toy example. Suppose I have a list of data
> frames like this.
>
>  print(x <- list('1'=data.frame(id=1:4,expand.grid(x1=0:1,x2=0:1)),'2'=
> data.frame(id=5:8,expand.grid(x1=2:3,x2=2:3))))
> $`1`
>   id x1 x2
> 1  1  0  0
> 2  2  1  0
> 3  3  0  1
> 4  4  1  1
>
> $`2`
>   id x1 x2
> 1  5  2  2
> 2  6  3  2
> 3  7  2  3
> 4  8  3  3
>
> The real application will have more than 2 elements so I'm looking for a
> general approach. I basically want to rbind the data frames in each list
> element and add a variable that adds the element name. In this example the
> result would look something like this.
>
> rbind(data.frame(set='1',x[[1]]),data.frame(set='2',x[[2]]))
>   set id x1 x2
> 1   1  1  0  0
> 2   1  2  1  0
> 3   1  3  0  1
> 4   1  4  1  1
> 5   2  5  2  2
> 6   2  6  3  2
> 7   2  7  2  3
> 8   2  8  3  3
>
> Obviously, for 2 elements the simple rbind works but I would like a
> general solution for arbitrary length lists. Hopefully that is clear.
>
> Kevin
>
> --
> Kevin E. Thorpe
> Head of Biostatistics,  Applied Health Research Centre (AHRC)
> Li Ka Shing Knowledge Institute of St. Michael's Hospital
> Assistant Professor, Dalla Lana School of Public Health
> University of Toronto
> email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.3016
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Saludos,
Carlos Ortega
www.qualityexcellence.es

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Fri Nov  4 15:35:29 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 4 Nov 2016 07:35:29 -0700
Subject: [R] multiplying a matrix by a vector
In-Reply-To: <CAM_vjuktunUoBvhN42iPaNZuGLC+P9QdkDA4WM538gSn2Qq5ug@mail.gmail.com>
References: <CAN2xGJabZSAq+Z=fFEB_nuNKHGKQExMB2L3aifnQzZMCouC34w@mail.gmail.com>
	<CAM_vjuktunUoBvhN42iPaNZuGLC+P9QdkDA4WM538gSn2Qq5ug@mail.gmail.com>
Message-ID: <CAGxFJbSg3cmP44YRmU08s=ZGzUxp=qtBFLkKfgMNe+S284rKXQ@mail.gmail.com>

My goodness!

> x %*% diag(y)

     [,1] [,2]
[1,]    2   12
[2,]    4   15
[3,]    6   18

will do.

-- Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Nov 3, 2016 at 2:33 PM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
> Like this?
>
>> sweep(x, 2, y, "*")
>      [,1] [,2]
> [1,]    2   12
> [2,]    4   15
> [3,]    6   18
>>
>
>
> On Thu, Nov 3, 2016 at 5:05 PM, Dimitri Liakhovitski
> <dimitri.liakhovitski at gmail.com> wrote:
>> Hello!
>>
>> I have a matrix x and a vector y:
>>
>> x <- matrix(1:6, ncol = 2)
>> y <- c(2,3)
>>
>> I need to multiply the first column of x by 2 (y[1]) and the second
>> column of x by 3 (y[2]).
>>
>> Of course, I could do this - but it's column by column:
>>
>> x[,1] <- x[,1] * y[1]
>> x[,2] <- x[,2] * y[2]
>> x
>>
>> Or I could repeat each element of y and multiply two matrices - that's better:
>>
>> rep.row<-function(x,n){
>>   matrix(rep(x,each=n),nrow=n)
>> }
>> y <- rep.row(y, nrow(x))
>> x * y
>>
>> However, maybe there is a more elegant r-like way of doing it?
>> Thank you!
>>
>> --
>> Dimitri Liakhovitski
>>
>
> --
> Sarah Goslee
> http://www.functionaldiversity.org
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From david.bucklin at gmail.com  Wed Nov  2 15:59:39 2016
From: david.bucklin at gmail.com (David Bucklin)
Date: Wed, 2 Nov 2016 10:59:39 -0400
Subject: [R] [R-pkgs] new package: rpostgisLT
Message-ID: <CAECT9fk1E=1GkNAKpgjTsStfpG0yR=2AhBuJ+6Tbc3PkszsAuA@mail.gmail.com>

We're announcing the initial CRAN release of 'rpostgisLT' (v0.4.0), which
is an extension to our 'rpostgis
<https://cran.r-project.org/web/packages/rpostgis/index.html>' package.
rpostgisLT is aimed at those using R and/or the PostgreSQL/PostGIS database
system to manage and analyze animal trajectory (movement) data. Package
functions allow bi-directional transfer between the database and "ltraj"
objects from the R package "adehabitatLT
<https://cran.r-project.org/web/packages/adehabitatLT/index.html>", which
has an extensive set of trajectory manipulation and analysis functions.

To install the package:

install.packages("rpostgisLT")

To get started, please take a look at the Readme
<https://cran.r-project.org/web/packages/rpostgisLT/README.html>, and to
learn more, check out the package vignettes on the database data model
<https://cran.r-project.org/web/packages/rpostgisLT/vignettes/data-model.html>
and more advanced use cases
<https://cran.r-project.org/web/packages/rpostgisLT/vignettes/use-cases.html>
.

The package main development area can be found on GitHub; any bugs, issues,
or feature requests can be submitted through the "issues" page there:

https://github.com/mablab/rpostgisLT

David Bucklin

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From bgunter.4567 at gmail.com  Fri Nov  4 16:11:44 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 4 Nov 2016 08:11:44 -0700
Subject: [R] Converting a list to a data frame
In-Reply-To: <CAKxd1KN=dicg2-Zx_uwbQ55mHj5Tp_V59EiYrrQ2=TwmHJ+uvA@mail.gmail.com>
References: <b1593d86-4154-3319-b85a-a3a8cbf1686b@utoronto.ca>
	<CAKxd1KN=dicg2-Zx_uwbQ55mHj5Tp_V59EiYrrQ2=TwmHJ+uvA@mail.gmail.com>
Message-ID: <CAGxFJbTh9kDsqe9V+UW5im+FMrR3WT0gOYTFJdVeZT4pUW94Gg@mail.gmail.com>

I believe that a slightly more efficient way of doing this without
leaving base R is:

cbind(do.call(rbind,x), set = rep(seq_along(x), vapply(x,nrow,1)) )


Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Nov 4, 2016 at 5:50 AM, Charles Determan <cdetermanjr at gmail.com> wrote:
> Hi Kevin,
>
> There may be a more elegant way but the following do.call and lapply should
> solve your problem.
>
> do.call(rbind, lapply(seq(length(x)), function(i) data.frame(set=i,
> x[[i]])))
>
> Regards,
> Charles
>
> On Fri, Nov 4, 2016 at 7:37 AM, Kevin E. Thorpe <kevin.thorpe at utoronto.ca>
> wrote:
>
>> There is probably a very simple elegant way to do this, but I have been
>> unable to find it. Here is a toy example. Suppose I have a list of data
>> frames like this.
>>
>>  print(x <- list('1'=data.frame(id=1:4,expand.grid(x1=0:1,x2=0:1)),'2'=
>> data.frame(id=5:8,expand.grid(x1=2:3,x2=2:3))))
>> $`1`
>>   id x1 x2
>> 1  1  0  0
>> 2  2  1  0
>> 3  3  0  1
>> 4  4  1  1
>>
>> $`2`
>>   id x1 x2
>> 1  5  2  2
>> 2  6  3  2
>> 3  7  2  3
>> 4  8  3  3
>>
>> The real application will have more than 2 elements so I'm looking for a
>> general approach. I basically want to rbind the data frames in each list
>> element and add a variable that adds the element name. In this example the
>> result would look something like this.
>>
>> rbind(data.frame(set='1',x[[1]]),data.frame(set='2',x[[2]]))
>>   set id x1 x2
>> 1   1  1  0  0
>> 2   1  2  1  0
>> 3   1  3  0  1
>> 4   1  4  1  1
>> 5   2  5  2  2
>> 6   2  6  3  2
>> 7   2  7  2  3
>> 8   2  8  3  3
>>
>> Obviously, for 2 elements the simple rbind works but I would like a
>> general solution for arbitrary length lists. Hopefully that is clear.
>>
>> Kevin
>>
>> --
>> Kevin E. Thorpe
>> Head of Biostatistics,  Applied Health Research Centre (AHRC)
>> Li Ka Shing Knowledge Institute of St. Michael's Hospital
>> Assistant Professor, Dalla Lana School of Public Health
>> University of Toronto
>> email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.3016
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dimitri.liakhovitski at gmail.com  Fri Nov  4 16:15:56 2016
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Fri, 4 Nov 2016 11:15:56 -0400
Subject: [R] multiplying a matrix by a vector
In-Reply-To: <CAGxFJbSg3cmP44YRmU08s=ZGzUxp=qtBFLkKfgMNe+S284rKXQ@mail.gmail.com>
References: <CAN2xGJabZSAq+Z=fFEB_nuNKHGKQExMB2L3aifnQzZMCouC34w@mail.gmail.com>
	<CAM_vjuktunUoBvhN42iPaNZuGLC+P9QdkDA4WM538gSn2Qq5ug@mail.gmail.com>
	<CAGxFJbSg3cmP44YRmU08s=ZGzUxp=qtBFLkKfgMNe+S284rKXQ@mail.gmail.com>
Message-ID: <CAN2xGJYYAtpQ673=yOqj4Fp8jM7xjm4wqYdjbgDy7WwKZ4CM4Q@mail.gmail.com>

Nice!
Thanks a lot, everybody!
Dimitri

On Fri, Nov 4, 2016 at 10:35 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> My goodness!
>
>> x %*% diag(y)
>
>      [,1] [,2]
> [1,]    2   12
> [2,]    4   15
> [3,]    6   18
>
> will do.
>
> -- Bert
>
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Thu, Nov 3, 2016 at 2:33 PM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
>> Like this?
>>
>>> sweep(x, 2, y, "*")
>>      [,1] [,2]
>> [1,]    2   12
>> [2,]    4   15
>> [3,]    6   18
>>>
>>
>>
>> On Thu, Nov 3, 2016 at 5:05 PM, Dimitri Liakhovitski
>> <dimitri.liakhovitski at gmail.com> wrote:
>>> Hello!
>>>
>>> I have a matrix x and a vector y:
>>>
>>> x <- matrix(1:6, ncol = 2)
>>> y <- c(2,3)
>>>
>>> I need to multiply the first column of x by 2 (y[1]) and the second
>>> column of x by 3 (y[2]).
>>>
>>> Of course, I could do this - but it's column by column:
>>>
>>> x[,1] <- x[,1] * y[1]
>>> x[,2] <- x[,2] * y[2]
>>> x
>>>
>>> Or I could repeat each element of y and multiply two matrices - that's better:
>>>
>>> rep.row<-function(x,n){
>>>   matrix(rep(x,each=n),nrow=n)
>>> }
>>> y <- rep.row(y, nrow(x))
>>> x * y
>>>
>>> However, maybe there is a more elegant r-like way of doing it?
>>> Thank you!
>>>
>>> --
>>> Dimitri Liakhovitski
>>>
>>
>> --
>> Sarah Goslee
>> http://www.functionaldiversity.org
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.



-- 
Dimitri Liakhovitski


From jdnewmil at dcn.davis.ca.us  Fri Nov  4 16:41:52 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 4 Nov 2016 08:41:52 -0700 (PDT)
Subject: [R] multiplying a matrix by a vector
In-Reply-To: <CAN2xGJYYAtpQ673=yOqj4Fp8jM7xjm4wqYdjbgDy7WwKZ4CM4Q@mail.gmail.com>
References: <CAN2xGJabZSAq+Z=fFEB_nuNKHGKQExMB2L3aifnQzZMCouC34w@mail.gmail.com>
	<CAM_vjuktunUoBvhN42iPaNZuGLC+P9QdkDA4WM538gSn2Qq5ug@mail.gmail.com>
	<CAGxFJbSg3cmP44YRmU08s=ZGzUxp=qtBFLkKfgMNe+S284rKXQ@mail.gmail.com>
	<CAN2xGJYYAtpQ673=yOqj4Fp8jM7xjm4wqYdjbgDy7WwKZ4CM4Q@mail.gmail.com>
Message-ID: <alpine.BSF.2.00.1611040834120.79947@pedal.dcn.davis.ca.us>

Sara wins on memory use.

Rui wins on speed.

Bert wins on clarity.

library(microbenchmark)

N <- 1000
x <- matrix( runif( N*N ), ncol=N )
y <- seq.int( N )

microbenchmark( { t( y * t(x) ) }
               , { x %*% diag( y ) }
               , { sweep( x, 2, y, `*` ) }
               )
Unit: milliseconds
                         expr       min        lq    median        uq      max neval
          {     t(y * t(x)) }  6.659562  7.475414  7.871341  8.182623 47.01105 100
        {     x %*% diag(y) }  9.859292 11.014021 11.281334 11.733825 48.79463 100
  {     sweep(x, 2, y, `*`) } 16.535938 17.682175 18.283572 18.712342 55.47159 100

On Fri, 4 Nov 2016, Dimitri Liakhovitski wrote:

> Nice!
> Thanks a lot, everybody!
> Dimitri
>
> On Fri, Nov 4, 2016 at 10:35 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>> My goodness!
>>
>>> x %*% diag(y)
>>
>>      [,1] [,2]
>> [1,]    2   12
>> [2,]    4   15
>> [3,]    6   18
>>
>> will do.
>>
>> -- Bert
>>
>>
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Thu, Nov 3, 2016 at 2:33 PM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
>>> Like this?
>>>
>>>> sweep(x, 2, y, "*")
>>>      [,1] [,2]
>>> [1,]    2   12
>>> [2,]    4   15
>>> [3,]    6   18
>>>>
>>>
>>>
>>> On Thu, Nov 3, 2016 at 5:05 PM, Dimitri Liakhovitski
>>> <dimitri.liakhovitski at gmail.com> wrote:
>>>> Hello!
>>>>
>>>> I have a matrix x and a vector y:
>>>>
>>>> x <- matrix(1:6, ncol = 2)
>>>> y <- c(2,3)
>>>>
>>>> I need to multiply the first column of x by 2 (y[1]) and the second
>>>> column of x by 3 (y[2]).
>>>>
>>>> Of course, I could do this - but it's column by column:
>>>>
>>>> x[,1] <- x[,1] * y[1]
>>>> x[,2] <- x[,2] * y[2]
>>>> x
>>>>
>>>> Or I could repeat each element of y and multiply two matrices - that's better:
>>>>
>>>> rep.row<-function(x,n){
>>>>   matrix(rep(x,each=n),nrow=n)
>>>> }
>>>> y <- rep.row(y, nrow(x))
>>>> x * y
>>>>
>>>> However, maybe there is a more elegant r-like way of doing it?
>>>> Thank you!
>>>>
>>>> --
>>>> Dimitri Liakhovitski
>>>>
>>>
>>> --
>>> Sarah Goslee
>>> http://www.functionaldiversity.org
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>
>
>
> -- 
> Dimitri Liakhovitski
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From pdalgd at gmail.com  Fri Nov  4 17:35:32 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 4 Nov 2016 17:35:32 +0100
Subject: [R] multiplying a matrix by a vector
In-Reply-To: <alpine.BSF.2.00.1611040834120.79947@pedal.dcn.davis.ca.us>
References: <CAN2xGJabZSAq+Z=fFEB_nuNKHGKQExMB2L3aifnQzZMCouC34w@mail.gmail.com>
	<CAM_vjuktunUoBvhN42iPaNZuGLC+P9QdkDA4WM538gSn2Qq5ug@mail.gmail.com>
	<CAGxFJbSg3cmP44YRmU08s=ZGzUxp=qtBFLkKfgMNe+S284rKXQ@mail.gmail.com>
	<CAN2xGJYYAtpQ673=yOqj4Fp8jM7xjm4wqYdjbgDy7WwKZ4CM4Q@mail.gmail.com>
	<alpine.BSF.2.00.1611040834120.79947@pedal.dcn.davis.ca.us>
Message-ID: <091E511F-89A0-45CA-82D0-0DE20F4AF664@gmail.com>

Notice though, that Bert loses (or _should_ lose) for larger values of N, since that method involves O(N^3) operations whereas the other two are O(N^2). I am a bit surprised that sweep() is so inefficient even at N=1000.

-pd


On 04 Nov 2016, at 16:41 , Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:

> Sara wins on memory use.
> 
> Rui wins on speed.
> 
> Bert wins on clarity.
> 
> library(microbenchmark)
> 
> N <- 1000
> x <- matrix( runif( N*N ), ncol=N )
> y <- seq.int( N )
> 
> microbenchmark( { t( y * t(x) ) }
>              , { x %*% diag( y ) }
>              , { sweep( x, 2, y, `*` ) }
>              )
> Unit: milliseconds
>                        expr       min        lq    median        uq      max neval
>         {     t(y * t(x)) }  6.659562  7.475414  7.871341  8.182623 47.01105 100
>       {     x %*% diag(y) }  9.859292 11.014021 11.281334 11.733825 48.79463 100
> {     sweep(x, 2, y, `*`) } 16.535938 17.682175 18.283572 18.712342 55.47159 100
> 
> On Fri, 4 Nov 2016, Dimitri Liakhovitski wrote:
> 
>> Nice!
>> Thanks a lot, everybody!
>> Dimitri
>> 
>> On Fri, Nov 4, 2016 at 10:35 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>> My goodness!
>>> 
>>>> x %*% diag(y)
>>> 
>>>     [,1] [,2]
>>> [1,]    2   12
>>> [2,]    4   15
>>> [3,]    6   18
>>> 
>>> will do.
>>> 
>>> -- Bert
>>> 
>>> 
>>> 
>>> Bert Gunter
>>> 
>>> "The trouble with having an open mind is that people keep coming along
>>> and sticking things into it."
>>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>> 
>>> 
>>> On Thu, Nov 3, 2016 at 2:33 PM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
>>>> Like this?
>>>> 
>>>>> sweep(x, 2, y, "*")
>>>>     [,1] [,2]
>>>> [1,]    2   12
>>>> [2,]    4   15
>>>> [3,]    6   18
>>>>> 
>>>> 
>>>> 
>>>> On Thu, Nov 3, 2016 at 5:05 PM, Dimitri Liakhovitski
>>>> <dimitri.liakhovitski at gmail.com> wrote:
>>>>> Hello!
>>>>> 
>>>>> I have a matrix x and a vector y:
>>>>> 
>>>>> x <- matrix(1:6, ncol = 2)
>>>>> y <- c(2,3)
>>>>> 
>>>>> I need to multiply the first column of x by 2 (y[1]) and the second
>>>>> column of x by 3 (y[2]).
>>>>> 
>>>>> Of course, I could do this - but it's column by column:
>>>>> 
>>>>> x[,1] <- x[,1] * y[1]
>>>>> x[,2] <- x[,2] * y[2]
>>>>> x
>>>>> 
>>>>> Or I could repeat each element of y and multiply two matrices - that's better:
>>>>> 
>>>>> rep.row<-function(x,n){
>>>>>  matrix(rep(x,each=n),nrow=n)
>>>>> }
>>>>> y <- rep.row(y, nrow(x))
>>>>> x * y
>>>>> 
>>>>> However, maybe there is a more elegant r-like way of doing it?
>>>>> Thank you!
>>>>> 
>>>>> --
>>>>> Dimitri Liakhovitski
>>>>> 
>>>> 
>>>> --
>>>> Sarah Goslee
>>>> http://www.functionaldiversity.org
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
>> 
>> -- 
>> Dimitri Liakhovitski
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                      Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From f.harrell at vanderbilt.edu  Fri Nov  4 19:17:05 2016
From: f.harrell at vanderbilt.edu (Frank Harrell)
Date: Fri, 4 Nov 2016 13:17:05 -0500
Subject: [R] [R-pkgs] Major Update to rms package: 5.0-0
Message-ID: <CAMO-wTbJaPUwnWWAEXGfvB4AhDZqvtiFHh1VHvXmo5BjyDQKEQ@mail.gmail.com>

A major new version of the rms package is now on CRAN.  The most
user-visible changes are:

- interactive plotly graphic methods for model fits.  The best example of
this is survplot for npsurv (Kaplan-Meier) estimates where the number of
risk pop up as you hover over the curves, and you can click to bring up
confidence bands for differences in survival curves

- html methods for model fit summaries especially when using Rmarkdown html
notebooks

- instead of running print(fit, latex=TRUE) use
   options(prType='latex')
   print(fit)
   Or: options(prType='html')

A complete list of changes is below.  See
http://biostat.mc.vanderbilt.edu/Rrms for an html notebook showing examples
of new features.

Changes in version 5.0-0 (2016-10-31)
   * plot.summary.rms: implemented plotly interactive plots if
options(grType='plotly') in effect
* plot.anova.rms: implemented plotly interactive plots if
options(grType='plotly') in effect; remove psmall argument; changed margin
default to chisq and P
* ggplot.Predict: implemented plotly interactive plots if
options(grType='plotly') in effect
* print(fit, md=TRUE), prModFit, prStats: added latex/html methods using
htmlTable package and MathJax for latex math
* html.anova.rms, html.validate, html.summary.rms: new functions for use
with html and MathJax/knitr/RStudio
* latex methods for model fits: added md=TRUE argument to produce
MathJax-compatible latex and html code for fitted models when using R
Markdown
* html: new methods for model fit objects for use with R Markdown
* formatNP: fixed error when digits=NA
* latex.anova.rms: fixed error in not rounding enough columns doe to using
all.is.numeric intead of is.numeric
* catg: corrected bug that disallowed explicit catg() in formulas
* ggplot.Predict: added height and width for plotly
* survplot: respected xlim with diffbands.  Thanks: Toni G
* reVector: changed to reListclean and stored model stats components as
lists so can handle mixture of numeric and character, e.g., name of
clustering variable
* survplotp.npsurv: new function for interactive survival curve graphs
using plotly
* anova, summary, latex, print for model fits: use options(grType='html')
or 'latex' to set output type, output htmltools::HTML marked html so that
chunk header doesn't need results='asis'
* latex methods - set file default to ''
* GiniMd: moved to Hmisc package
* plot.nomogram: fixed bug where abbreviations were being ignored.  Thanks:
Zongheng Zhang
* nomogram: improved examples in help file
* survplot.npsurv: fixed n.risk when competing risks
* survest.cph, survfit.cph, others: fixed large problems due to
incompatibility with survival 2.39-5 for survival predictions; changed
object Strata to strata in cph to be compatible with survival package
* new test survest.r to more comprehensively check survfit.cph and
survest.cph
* ggplot.Predict: quit ignoring xlim; suppress confidence bands if two
group bariables because ggplot2 geom_ribbon doesn't support multiple
aesthetics
* predictrms: set contrasts for ordered factors to contr.treatment instead
of contr.poly
* val.prob: changed line of identity to use wide grayscale line instead of
dashed line

------------------------------
Frank E Harrell Jr      Professor and Chairman      School of Medicine

Department of *Biostatistics*      *Vanderbilt University*

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From bhh at xs4all.nl  Fri Nov  4 19:27:55 2016
From: bhh at xs4all.nl (Berend Hasselman)
Date: Fri, 4 Nov 2016 19:27:55 +0100
Subject: [R] multiplying a matrix by a vector
In-Reply-To: <alpine.BSF.2.00.1611040834120.79947@pedal.dcn.davis.ca.us>
References: <CAN2xGJabZSAq+Z=fFEB_nuNKHGKQExMB2L3aifnQzZMCouC34w@mail.gmail.com>
	<CAM_vjuktunUoBvhN42iPaNZuGLC+P9QdkDA4WM538gSn2Qq5ug@mail.gmail.com>
	<CAGxFJbSg3cmP44YRmU08s=ZGzUxp=qtBFLkKfgMNe+S284rKXQ@mail.gmail.com>
	<CAN2xGJYYAtpQ673=yOqj4Fp8jM7xjm4wqYdjbgDy7WwKZ4CM4Q@mail.gmail.com>
	<alpine.BSF.2.00.1611040834120.79947@pedal.dcn.davis.ca.us>
Message-ID: <692B6077-31C8-4CFA-94F2-F79542043B6E@xs4all.nl>


> On 4 Nov 2016, at 16:41, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> Sara wins on memory use.
> 
> Rui wins on speed.
> 
> Bert wins on clarity.
> 
> library(microbenchmark)
> 
> N <- 1000
> x <- matrix( runif( N*N ), ncol=N )
> y <- seq.int( N )
> 
> microbenchmark( { t( y * t(x) ) }
>              , { x %*% diag( y ) }
>              , { sweep( x, 2, y, `*` ) }
>              )
> Unit: milliseconds
>                        expr       min        lq    median        uq      max neval
>         {     t(y * t(x)) }  6.659562  7.475414  7.871341  8.182623 47.01105 100
>       {     x %*% diag(y) }  9.859292 11.014021 11.281334 11.733825 48.79463 100
> {     sweep(x, 2, y, `*`) } 16.535938 17.682175 18.283572 18.712342 55.47159 100


I get different results with R3.2.2 on Mac OS X (using reference BLAS).


library(rbenchmark)

N <- 1000
x <- matrix( runif( N*N ), ncol=N )
y <- seq.int( N )

benchmark(  t( y * t(x) ), x %*% diag( y ), sweep( x, 2, y, `*` ) ,
           columns=c("test","elapsed","relative"), replications=10
         )

#                  test elapsed relative
# 3 sweep(x, 2, y, `*`)   0.132    1.000
# 1         t(y * t(x))   0.189    1.432
# 2       x %*% diag(y)   7.928   60.061

library(microbenchmark)
microbenchmark( { t( y * t(x) ) }
             , { x %*% diag( y ) }
             , { sweep( x, 2, y, `*` ) },
             times=10, unit="s"
             )

# Unit: seconds
#                         expr        min         lq       mean     median uq        max neval cld
#          {     t(y * t(x)) } 0.01099410 0.01132096 0.01597058 0.01332414  0.01430672 0.04447432    10  a
#        {     x %*% diag(y) } 0.76588887 0.78581717 0.79878255 0.79811626  0.80607877 0.82903945    10   b
#  {     sweep(x, 2, y, `*`) } 0.01177478 0.01246777 0.01409457 0.01314718   0.01600818 0.01802171    10  a


sweep appears to very good.

I don't quite understand why I get a very different ranking.

Berend


From bhh at xs4all.nl  Fri Nov  4 19:35:02 2016
From: bhh at xs4all.nl (Berend Hasselman)
Date: Fri, 4 Nov 2016 19:35:02 +0100
Subject: [R] multiplying a matrix by a vector
In-Reply-To: <692B6077-31C8-4CFA-94F2-F79542043B6E@xs4all.nl>
References: <CAN2xGJabZSAq+Z=fFEB_nuNKHGKQExMB2L3aifnQzZMCouC34w@mail.gmail.com>
	<CAM_vjuktunUoBvhN42iPaNZuGLC+P9QdkDA4WM538gSn2Qq5ug@mail.gmail.com>
	<CAGxFJbSg3cmP44YRmU08s=ZGzUxp=qtBFLkKfgMNe+S284rKXQ@mail.gmail.com>
	<CAN2xGJYYAtpQ673=yOqj4Fp8jM7xjm4wqYdjbgDy7WwKZ4CM4Q@mail.gmail.com>
	<alpine.BSF.2.00.1611040834120.79947@pedal.dcn.davis.ca.us>
	<692B6077-31C8-4CFA-94F2-F79542043B6E@xs4all.nl>
Message-ID: <BD1E82FE-0DEC-450D-AB91-532B7DFED3B8@xs4all.nl>


> On 4 Nov 2016, at 19:27, Berend Hasselman <bhh at xs4all.nl> wrote:
> 
>> 
>> On 4 Nov 2016, at 16:41, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>> 
>> Sara wins on memory use.
>> 
>> Rui wins on speed.
>> 
>> Bert wins on clarity.
>> 
>> library(microbenchmark)
>> 
>> N <- 1000
>> x <- matrix( runif( N*N ), ncol=N )
>> y <- seq.int( N )
>> 
>> microbenchmark( { t( y * t(x) ) }
>>             , { x %*% diag( y ) }
>>             , { sweep( x, 2, y, `*` ) }
>>             )
>> Unit: milliseconds
>>                       expr       min        lq    median        uq      max neval
>>        {     t(y * t(x)) }  6.659562  7.475414  7.871341  8.182623 47.01105 100
>>      {     x %*% diag(y) }  9.859292 11.014021 11.281334 11.733825 48.79463 100
>> {     sweep(x, 2, y, `*`) } 16.535938 17.682175 18.283572 18.712342 55.47159 100
> 
> 
> I get different results with R3.2.2 on Mac OS X (using reference BLAS).
> 

Correction: I meant R3.3.2

Berend

From bgunter.4567 at gmail.com  Fri Nov  4 19:49:08 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 4 Nov 2016 11:49:08 -0700
Subject: [R] multiplying a matrix by a vector
In-Reply-To: <692B6077-31C8-4CFA-94F2-F79542043B6E@xs4all.nl>
References: <CAN2xGJabZSAq+Z=fFEB_nuNKHGKQExMB2L3aifnQzZMCouC34w@mail.gmail.com>
	<CAM_vjuktunUoBvhN42iPaNZuGLC+P9QdkDA4WM538gSn2Qq5ug@mail.gmail.com>
	<CAGxFJbSg3cmP44YRmU08s=ZGzUxp=qtBFLkKfgMNe+S284rKXQ@mail.gmail.com>
	<CAN2xGJYYAtpQ673=yOqj4Fp8jM7xjm4wqYdjbgDy7WwKZ4CM4Q@mail.gmail.com>
	<alpine.BSF.2.00.1611040834120.79947@pedal.dcn.davis.ca.us>
	<692B6077-31C8-4CFA-94F2-F79542043B6E@xs4all.nl>
Message-ID: <CAGxFJbQ+32EUy087PaHQtTGkB5AcmZ+OAM8bzvTW=dG=homcNA@mail.gmail.com>

Berend, et. al.:

"I don't quite understand why I get a very different ranking."

Nor do I. But, as Peter noted, my matrix multiplication solution
"should" be worst in terms of efficiency, and it clearly is. The other
two *should* be "similar", and they are. So your results are
consistent with what one should expect, whereas, as Peter noted,
Jeff's are not.

But as we all know, *actual* efficiency in use can be tricky to
determine (asymptopia is always questionable), as it may depend on
state, problem details, exact algorithms in use, etc. Which is why the
standard first principle in code optimization is: don't.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Nov 4, 2016 at 11:27 AM, Berend Hasselman <bhh at xs4all.nl> wrote:
>
>> On 4 Nov 2016, at 16:41, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>
>> Sara wins on memory use.
>>
>> Rui wins on speed.
>>
>> Bert wins on clarity.
>>
>> library(microbenchmark)
>>
>> N <- 1000
>> x <- matrix( runif( N*N ), ncol=N )
>> y <- seq.int( N )
>>
>> microbenchmark( { t( y * t(x) ) }
>>              , { x %*% diag( y ) }
>>              , { sweep( x, 2, y, `*` ) }
>>              )
>> Unit: milliseconds
>>                        expr       min        lq    median        uq      max neval
>>         {     t(y * t(x)) }  6.659562  7.475414  7.871341  8.182623 47.01105 100
>>       {     x %*% diag(y) }  9.859292 11.014021 11.281334 11.733825 48.79463 100
>> {     sweep(x, 2, y, `*`) } 16.535938 17.682175 18.283572 18.712342 55.47159 100
>
>
> I get different results with R3.2.2 on Mac OS X (using reference BLAS).
>
>
> library(rbenchmark)
>
> N <- 1000
> x <- matrix( runif( N*N ), ncol=N )
> y <- seq.int( N )
>
> benchmark(  t( y * t(x) ), x %*% diag( y ), sweep( x, 2, y, `*` ) ,
>            columns=c("test","elapsed","relative"), replications=10
>          )
>
> #                  test elapsed relative
> # 3 sweep(x, 2, y, `*`)   0.132    1.000
> # 1         t(y * t(x))   0.189    1.432
> # 2       x %*% diag(y)   7.928   60.061
>
> library(microbenchmark)
> microbenchmark( { t( y * t(x) ) }
>              , { x %*% diag( y ) }
>              , { sweep( x, 2, y, `*` ) },
>              times=10, unit="s"
>              )
>
> # Unit: seconds
> #                         expr        min         lq       mean     median uq        max neval cld
> #          {     t(y * t(x)) } 0.01099410 0.01132096 0.01597058 0.01332414  0.01430672 0.04447432    10  a
> #        {     x %*% diag(y) } 0.76588887 0.78581717 0.79878255 0.79811626  0.80607877 0.82903945    10   b
> #  {     sweep(x, 2, y, `*`) } 0.01177478 0.01246777 0.01409457 0.01314718   0.01600818 0.01802171    10  a
>
>
> sweep appears to very good.
>
> I don't quite understand why I get a very different ranking.
>
> Berend
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dimitri.liakhovitski at gmail.com  Fri Nov  4 20:16:37 2016
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Fri, 4 Nov 2016 15:16:37 -0400
Subject: [R] multiplying a matrix by a vector
In-Reply-To: <581BACF7.9000201@sapo.pt>
References: <CAN2xGJabZSAq+Z=fFEB_nuNKHGKQExMB2L3aifnQzZMCouC34w@mail.gmail.com>
	<581BACF7.9000201@sapo.pt>
Message-ID: <CAN2xGJZGwWXQFQLE4ifckDgBQp9Q_2x9bxB_E8HKNn=uR92i_Q@mail.gmail.com>

I love R for it: if you experiment enough with t() you can find a way
to multiply almost anything by almost anything!
:)

On Thu, Nov 3, 2016 at 5:32 PM, Rui Barradas <ruipbarradas at sapo.pt> wrote:
> Hello,
>
> Take advantage that in R '*' recycles its arguments (the shorter one) and
> that the operation is performed column-wise:
>
> t(y * t(x))
>
> Hope this helps,
>
> Rui Barradas
>
>
> Em 03-11-2016 21:05, Dimitri Liakhovitski escreveu:
>>
>> Hello!
>>
>> I have a matrix x and a vector y:
>>
>> x <- matrix(1:6, ncol = 2)
>> y <- c(2,3)
>>
>> I need to multiply the first column of x by 2 (y[1]) and the second
>> column of x by 3 (y[2]).
>>
>> Of course, I could do this - but it's column by column:
>>
>> x[,1] <- x[,1] * y[1]
>> x[,2] <- x[,2] * y[2]
>> x
>>
>> Or I could repeat each element of y and multiply two matrices - that's
>> better:
>>
>> rep.row<-function(x,n){
>>    matrix(rep(x,each=n),nrow=n)
>> }
>> y <- rep.row(y, nrow(x))
>> x * y
>>
>> However, maybe there is a more elegant r-like way of doing it?
>> Thank you!
>>
>



-- 
Dimitri Liakhovitski


From jdnewmil at dcn.davis.ca.us  Fri Nov  4 20:23:47 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 04 Nov 2016 12:23:47 -0700
Subject: [R] multiplying a matrix by a vector
In-Reply-To: <BD1E82FE-0DEC-450D-AB91-532B7DFED3B8@xs4all.nl>
References: <CAN2xGJabZSAq+Z=fFEB_nuNKHGKQExMB2L3aifnQzZMCouC34w@mail.gmail.com>
	<CAM_vjuktunUoBvhN42iPaNZuGLC+P9QdkDA4WM538gSn2Qq5ug@mail.gmail.com>
	<CAGxFJbSg3cmP44YRmU08s=ZGzUxp=qtBFLkKfgMNe+S284rKXQ@mail.gmail.com>
	<CAN2xGJYYAtpQ673=yOqj4Fp8jM7xjm4wqYdjbgDy7WwKZ4CM4Q@mail.gmail.com>
	<alpine.BSF.2.00.1611040834120.79947@pedal.dcn.davis.ca.us>
	<692B6077-31C8-4CFA-94F2-F79542043B6E@xs4all.nl>
	<BD1E82FE-0DEC-450D-AB91-532B7DFED3B8@xs4all.nl>
Message-ID: <0259A1FA-7056-41EC-BD1A-ED29A9C1C136@dcn.davis.ca.us>

If you want to get a comparable result, then run the same code. You may not want a comparable result though... your version straightens out the issue Peter was puzzled by. 
-- 
Sent from my phone. Please excuse my brevity.

On November 4, 2016 11:35:02 AM PDT, Berend Hasselman <bhh at xs4all.nl> wrote:
>
>> On 4 Nov 2016, at 19:27, Berend Hasselman <bhh at xs4all.nl> wrote:
>> 
>>> 
>>> On 4 Nov 2016, at 16:41, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>wrote:
>>> 
>>> Sara wins on memory use.
>>> 
>>> Rui wins on speed.
>>> 
>>> Bert wins on clarity.
>>> 
>>> library(microbenchmark)
>>> 
>>> N <- 1000
>>> x <- matrix( runif( N*N ), ncol=N )
>>> y <- seq.int( N )
>>> 
>>> microbenchmark( { t( y * t(x) ) }
>>>             , { x %*% diag( y ) }
>>>             , { sweep( x, 2, y, `*` ) }
>>>             )
>>> Unit: milliseconds
>>>                       expr       min        lq    median        uq  
>   max neval
>>>        {     t(y * t(x)) }  6.659562  7.475414  7.871341  8.182623
>47.01105 100
>>>      {     x %*% diag(y) }  9.859292 11.014021 11.281334 11.733825
>48.79463 100
>>> {     sweep(x, 2, y, `*`) } 16.535938 17.682175 18.283572 18.712342
>55.47159 100
>> 
>> 
>> I get different results with R3.2.2 on Mac OS X (using reference
>BLAS).
>> 
>
>Correction: I meant R3.3.2
>
>Berend


From tmrsg11 at gmail.com  Fri Nov  4 21:39:15 2016
From: tmrsg11 at gmail.com (C W)
Date: Fri, 4 Nov 2016 16:39:15 -0400
Subject: [R] R segfault at startup, causing R crash
Message-ID: <CAE2FW2nQKEuoW2qf=xWBrGotqw6LBfUMRDX=SRknnxW14Qe3ug@mail.gmail.com>

Dear R list,

Every time I start R, the following error message come up:

 *** caught segfault ***
address 0x0, cause 'unknown'

Possible actions:
1: abort (with core dump, if enabled)
2: normal R exit
3: exit R without saving workspace
4: exit R saving workspace
>
Selection:

I am unable to do a sessionInfo(), but I am using:
R version 3.2.4 (2016-03-10)
Platform: x86_64-apple-darwin13.4.0 (64-bit)


I would appreciate any help, thanks so much!

	[[alternative HTML version deleted]]


From tmrsg11 at gmail.com  Fri Nov  4 21:50:58 2016
From: tmrsg11 at gmail.com (C W)
Date: Fri, 4 Nov 2016 16:50:58 -0400
Subject: [R] R segfault at startup, causing R crash
In-Reply-To: <CAE2FW2nQKEuoW2qf=xWBrGotqw6LBfUMRDX=SRknnxW14Qe3ug@mail.gmail.com>
References: <CAE2FW2nQKEuoW2qf=xWBrGotqw6LBfUMRDX=SRknnxW14Qe3ug@mail.gmail.com>
Message-ID: <CAE2FW2kNiq=gXpj2JW5xT8oHBKjJ3zxktPeOJqY9wC4GCs+n+A@mail.gmail.com>

I just got the following error instead of the previous one,

2016-11-04 16:50:08.446 R[23372:326103] -[NSDrawerWindow
setRuleThickness:]: unrecognized selector sent to instance 0x7fabc94a74b0
2016-11-04 16:50:08.454 R[23372:326103] *** RController: caught ObjC
exception while processing system events. Update to the latest GUI version
and consider reporting this properly (see FAQ) if it persists and is not
known.
*** reason: -[NSDrawerWindow setRuleThickness:]: unrecognized selector sent
to instance 0x7fabc94a74b0
*** name: NSInvalidArgumentException, info: (null)
*** Version: R 3.3.2 (71607) R.app R 3.3.2 GUI 1.68 Mavericks build
Consider saving your work soon in case this develops into a problem.


On Fri, Nov 4, 2016 at 4:39 PM, C W <tmrsg11 at gmail.com> wrote:

> Dear R list,
>
> Every time I start R, the following error message come up:
>
>  *** caught segfault ***
> address 0x0, cause 'unknown'
>
> Possible actions:
> 1: abort (with core dump, if enabled)
> 2: normal R exit
> 3: exit R without saving workspace
> 4: exit R saving workspace
> >
> Selection:
>
> I am unable to do a sessionInfo(), but I am using:
> R version 3.2.4 (2016-03-10)
> Platform: x86_64-apple-darwin13.4.0 (64-bit)
>
>
> I would appreciate any help, thanks so much!
>

	[[alternative HTML version deleted]]


From tmrsg11 at gmail.com  Fri Nov  4 21:52:28 2016
From: tmrsg11 at gmail.com (C W)
Date: Fri, 4 Nov 2016 16:52:28 -0400
Subject: [R] R segfault at startup, causing R crash
In-Reply-To: <CAE2FW2kNiq=gXpj2JW5xT8oHBKjJ3zxktPeOJqY9wC4GCs+n+A@mail.gmail.com>
References: <CAE2FW2nQKEuoW2qf=xWBrGotqw6LBfUMRDX=SRknnxW14Qe3ug@mail.gmail.com>
	<CAE2FW2kNiq=gXpj2JW5xT8oHBKjJ3zxktPeOJqY9wC4GCs+n+A@mail.gmail.com>
Message-ID: <CAE2FW2=Sq4v-b4RcdYXLxxWecmdXZz0pnbOUtMna9T0TEN7Tiw@mail.gmail.com>

Here's the sessionInfo()

> sessionInfo()
R version 3.3.2 (2016-10-31)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: macOS Sierra 10.12.1

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods
[7] base

On Fri, Nov 4, 2016 at 4:50 PM, C W <tmrsg11 at gmail.com> wrote:

> I just got the following error instead of the previous one,
>
> 2016-11-04 16:50:08.446 R[23372:326103] -[NSDrawerWindow
> setRuleThickness:]: unrecognized selector sent to instance 0x7fabc94a74b0
> 2016-11-04 16:50:08.454 R[23372:326103] *** RController: caught ObjC
> exception while processing system events. Update to the latest GUI version
> and consider reporting this properly (see FAQ) if it persists and is not
> known.
> *** reason: -[NSDrawerWindow setRuleThickness:]: unrecognized selector
> sent to instance 0x7fabc94a74b0
> *** name: NSInvalidArgumentException, info: (null)
> *** Version: R 3.3.2 (71607) R.app R 3.3.2 GUI 1.68 Mavericks build
> Consider saving your work soon in case this develops into a problem.
>
>
> On Fri, Nov 4, 2016 at 4:39 PM, C W <tmrsg11 at gmail.com> wrote:
>
>> Dear R list,
>>
>> Every time I start R, the following error message come up:
>>
>>  *** caught segfault ***
>> address 0x0, cause 'unknown'
>>
>> Possible actions:
>> 1: abort (with core dump, if enabled)
>> 2: normal R exit
>> 3: exit R without saving workspace
>> 4: exit R saving workspace
>> >
>> Selection:
>>
>> I am unable to do a sessionInfo(), but I am using:
>> R version 3.2.4 (2016-03-10)
>> Platform: x86_64-apple-darwin13.4.0 (64-bit)
>>
>>
>> I would appreciate any help, thanks so much!
>>
>
>

	[[alternative HTML version deleted]]


From stefanML at collocations.de  Fri Nov  4 21:52:55 2016
From: stefanML at collocations.de (Stefan Evert)
Date: Fri, 4 Nov 2016 21:52:55 +0100
Subject: [R] multiplying a matrix by a vector
In-Reply-To: <091E511F-89A0-45CA-82D0-0DE20F4AF664@gmail.com>
References: <CAN2xGJabZSAq+Z=fFEB_nuNKHGKQExMB2L3aifnQzZMCouC34w@mail.gmail.com>
	<CAM_vjuktunUoBvhN42iPaNZuGLC+P9QdkDA4WM538gSn2Qq5ug@mail.gmail.com>
	<CAGxFJbSg3cmP44YRmU08s=ZGzUxp=qtBFLkKfgMNe+S284rKXQ@mail.gmail.com>
	<CAN2xGJYYAtpQ673=yOqj4Fp8jM7xjm4wqYdjbgDy7WwKZ4CM4Q@mail.gmail.com>
	<alpine.BSF.2.00.1611040834120.79947@pedal.dcn.davis.ca.us>
	<091E511F-89A0-45CA-82D0-0DE20F4AF664@gmail.com>
Message-ID: <365E3A97-540B-4462-A663-318A338D2B9B@collocations.de>


> On 4 Nov 2016, at 17:35, peter dalgaard <pdalgd at gmail.com> wrote:
> 
> Notice though, that Bert loses (or _should_ lose) for larger values of N, since that method involves O(N^3) operations whereas the other two are O(N^2). I am a bit surprised that sweep() is so inefficient even at N=1000.

I also got different results on Mac OS X (with R 3.3.1 and vecLib BLAS):

Unit: milliseconds
                        expr      min       lq     mean   median       uq       max neval
         {     t(y * t(x)) } 18.46369 19.61954 25.77548 21.24242 22.72943  88.03469   100
       {     x %*% diag(y) } 28.12786 29.87109 37.83814 31.59671 32.77839 101.68553   100
 {     sweep(x, 2, y, `*`) } 11.19871 12.76442 21.48670 14.51618 15.70665 100.51444   100

Note that sweep() is considerably _faster_ than the other two methods (which I found quite surprising).

Of course, if you care about speed (and memory efficiency), you could also

	library(wordspace)
	scaleMargins(x, cols=y)

Best,
Stefan


From bgunter.4567 at gmail.com  Fri Nov  4 23:36:14 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 4 Nov 2016 15:36:14 -0700
Subject: [R] multiplying a matrix by a vector
In-Reply-To: <365E3A97-540B-4462-A663-318A338D2B9B@collocations.de>
References: <CAN2xGJabZSAq+Z=fFEB_nuNKHGKQExMB2L3aifnQzZMCouC34w@mail.gmail.com>
	<CAM_vjuktunUoBvhN42iPaNZuGLC+P9QdkDA4WM538gSn2Qq5ug@mail.gmail.com>
	<CAGxFJbSg3cmP44YRmU08s=ZGzUxp=qtBFLkKfgMNe+S284rKXQ@mail.gmail.com>
	<CAN2xGJYYAtpQ673=yOqj4Fp8jM7xjm4wqYdjbgDy7WwKZ4CM4Q@mail.gmail.com>
	<alpine.BSF.2.00.1611040834120.79947@pedal.dcn.davis.ca.us>
	<091E511F-89A0-45CA-82D0-0DE20F4AF664@gmail.com>
	<365E3A97-540B-4462-A663-318A338D2B9B@collocations.de>
Message-ID: <CAGxFJbSB+Ro-owGhHybt1Dm=jK3fMMr7iDHoHr5DS0E70LCrPA@mail.gmail.com>

OK, just for fun, I added a couple of *obvious* (or should have been)
approaches  and got results that exactly agreed with Peter D's
comments: all O(n^2) results were essentially the same, and my matrix
multiplication O(n^3) solution was **far** worse. So I'm not sure
quite where Jeff's result came from.

microbenchmark( { t( y * t(x) ) }
                ,{ x %*% diag( y ) }
                ,{ sweep( x, 2, y, `*` ) }
                ,{vapply(seq_along(y),function(i)x[,i]*y[i],.1*seq_along(y))}
                ,{scale(x,center = FALSE,scale = 1/y)}
)

Unit: milliseconds

 expr        min         lq      mean
                                                         {     t(y *
t(x)) }   8.163510   9.880709  17.29899
                                                       {     x %*%
diag(y) } 637.639318 665.391992 686.12427
                                                 {     sweep(x, 2, y,
`*`) }   9.383453  10.497793  20.81000
 {     vapply(seq_along(y), function(i) x[, i] * y[i], 0.1 *
seq_along(y)) }   9.980690  11.748857  18.05533
                               {     scale(x, center = FALSE, scale =
1/y) }  10.427894  12.342690  19.57287

    median        uq       max neval
  11.72617  13.36316 100.00340   100
 679.38066 699.03779 827.17972   100
  13.11326  15.18546  95.41382   100
  13.47730  14.50548  98.80840   100
  14.08266  15.70222  99.52050   100


Note that scale() is basically a wrapper for sweep() with the api the
OP requested, so it would seem to be the "natural" choice. Also, my
matrix solution was an awful suggestion and should be tossed intp the
trash.

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Nov 4, 2016 at 1:52 PM, Stefan Evert <stefanML at collocations.de> wrote:
>
>> On 4 Nov 2016, at 17:35, peter dalgaard <pdalgd at gmail.com> wrote:
>>
>> Notice though, that Bert loses (or _should_ lose) for larger values of N, since that method involves O(N^3) operations whereas the other two are O(N^2). I am a bit surprised that sweep() is so inefficient even at N=1000.
>
> I also got different results on Mac OS X (with R 3.3.1 and vecLib BLAS):
>
> Unit: milliseconds
>                         expr      min       lq     mean   median       uq       max neval
>          {     t(y * t(x)) } 18.46369 19.61954 25.77548 21.24242 22.72943  88.03469   100
>        {     x %*% diag(y) } 28.12786 29.87109 37.83814 31.59671 32.77839 101.68553   100
>  {     sweep(x, 2, y, `*`) } 11.19871 12.76442 21.48670 14.51618 15.70665 100.51444   100
>
> Note that sweep() is considerably _faster_ than the other two methods (which I found quite surprising).
>
> Of course, if you care about speed (and memory efficiency), you could also
>
>         library(wordspace)
>         scaleMargins(x, cols=y)
>
> Best,
> Stefan
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Fri Nov  4 23:46:17 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 4 Nov 2016 15:46:17 -0700
Subject: [R] R segfault at startup, causing R crash
In-Reply-To: <CAE2FW2nQKEuoW2qf=xWBrGotqw6LBfUMRDX=SRknnxW14Qe3ug@mail.gmail.com>
References: <CAE2FW2nQKEuoW2qf=xWBrGotqw6LBfUMRDX=SRknnxW14Qe3ug@mail.gmail.com>
Message-ID: <09BD6E13-865B-4C41-A391-F50B5AD2DF13@comcast.net>


> On Nov 4, 2016, at 1:39 PM, C W <tmrsg11 at gmail.com> wrote:
> 
> Dear R list,
> 
> Every time I start R, the following error message come up:
> 
> *** caught segfault ***
> address 0x0, cause 'unknown'
> 
> Possible actions:
> 1: abort (with core dump, if enabled)
> 2: normal R exit
> 3: exit R without saving workspace
> 4: exit R saving workspace
>> 
> Selection:
> 
> I am unable to do a sessionInfo(), but I am using:
> R version 3.2.4 (2016-03-10)
> Platform: x86_64-apple-darwin13.4.0 (64-bit)

The usual cause for this is a corrupted .Rdata file although sometime the .Rhistory file is the culprit. These are often not displayed (so-called "invisible files") by the OS file browsers so you may need to seek out OS-specific approaches to that possibility. Trash both of those possible sources of instability and restart.


> 
> I would appreciate any help, thanks so much!
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From paolo.brunori at uniba.it  Fri Nov  4 17:53:44 2016
From: paolo.brunori at uniba.it (paolo brunori)
Date: Fri, 4 Nov 2016 17:53:44 +0100
Subject: [R] loop with variable names
Message-ID: <aa2306b6-839b-3787-6c41-8037d3f8d33e@uniba.it>

Suppose I have the following data:

y<-rnorm(10)
age<-rnorm(10)
sex<-rbinom(10,1, 0.5)
edu<-round(runif(10, 1, 20))
edu2<-edu^2

df<-data.frame(y,age,sex,edu,edu2)

I want to run a large number of models, for example:

lm(y~age)
lm(y~age+sex)
lm(y~age+sex+edu)
lm(y~age+sex+edu+edu2)
lm(y~sex+edu2)
lm(y~age+edu+edu2)
....

But I would like to first define a list containing all possible sets of 
regressors, and then execute each one in a loop/lapply. Unfortunately I 
got lost in trying to paste variables' name in the formula with no result.

many thanks in advance.

paolo



-- 
Paolo Brunori
Ricercatore in Economia Politica & Life Course Centre Fellow
Dipartimento di Scienze Economiche - Universit? di Bari
www.uniba.it/docenti/brunori-paolo
www.equalchances.org
www.lifecoursecentre.org.au


From jcharlto16 at gmail.com  Fri Nov  4 19:26:00 2016
From: jcharlto16 at gmail.com (Jeff Charlton)
Date: Fri, 4 Nov 2016 12:26:00 -0600
Subject: [R] Using map with filled.contour doesn't display map
Message-ID: <CAFeyMuw0SJLqVDg70KFYTwb3eLYzBV6SweAZD_ONSWBq7csDzw@mail.gmail.com>

Hello,

I'm trying to overlay a map on top of data showing temperatures across the
world. The code I'm using is:

      filledContour(tempdata, plot.axes={axis(1); axis(2); map("world2",
add=TRUE)})

where tempdata is a raster file made from a netcdf file downloaded from the
NOAA website. The filledContour is a wrapper that translates the raster
image into something that can be used by filled.contour.

If I run the code:

  filledContour(tempdata)

I get the image I want, but when I add the plot.axes parameter, I get a
"plot.new has not been called yet" error in axis(1). If I run plot.new()
and the original filledContour call, I get the same result as if I just did
filledContour(tempdata) (i.e. no map is overlayed).

The kicker in all this is that this code worked a couple of years ago. I
ran into this error when I recently dusted of my code and installed the
latest version of R and its various packages. I have a feeling that
something has changed in the maps package but I can't find any reference to
it online.

Can anyone help?

Thank you,

Jeff

Just in case, here's my session info:

R version 3.3.2 (2016-10-31) Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

locale: [1] LC_COLLATE=English_United States.1252 LC_CTYPE=English_United
States.1252 LC_MONETARY=English_United States.1252 [4] LC_NUMERIC=C
LC_TIME=English_United States.1252

attached base packages: [1] stats graphics grDevices utils datasets methods
base

other attached packages: [1] ncdf4_1.15 maps_3.1.1 XML_3.98-1.4
RCurl_1.95-4.8 bitops_1.0-6 raster_2.5-8 sp_1.2-3

loaded via a namespace (and not attached): [1] parallel_3.3.2 tools_3.3.2
Rcpp_0.12.7 grid_3.3.2 lattice_0.20-34

	[[alternative HTML version deleted]]


From istazahn at gmail.com  Sat Nov  5 01:25:53 2016
From: istazahn at gmail.com (Ista Zahn)
Date: Fri, 4 Nov 2016 20:25:53 -0400
Subject: [R] loop with variable names
In-Reply-To: <aa2306b6-839b-3787-6c41-8037d3f8d33e@uniba.it>
References: <aa2306b6-839b-3787-6c41-8037d3f8d33e@uniba.it>
Message-ID: <CA+vqiLGgCaijwV_FFQhm-dH+L=aA_3f31w4HHUnF8Qkexs55pg@mail.gmail.com>

It's hard to imagine a situation where this makes sense, but of course
you can do it if you want. Perhaps

rhs <- unlist(sapply(1:(ncol(df)-1), function(x)
apply(combn(names(df)[-1], x), 2, paste, collapse = " + ")))

lapply(rhs, function(x) lm(as.formula(paste("y ~", x)), data = df))

--Ista
On Fri, Nov 4, 2016 at 12:53 PM, paolo brunori <paolo.brunori at uniba.it> wrote:
> Suppose I have the following data:
>
> y<-rnorm(10)
> age<-rnorm(10)
> sex<-rbinom(10,1, 0.5)
> edu<-round(runif(10, 1, 20))
> edu2<-edu^2
>
> df<-data.frame(y,age,sex,edu,edu2)
>
> I want to run a large number of models, for example:
>
> lm(y~age)
> lm(y~age+sex)
> lm(y~age+sex+edu)
> lm(y~age+sex+edu+edu2)
> lm(y~sex+edu2)
> lm(y~age+edu+edu2)
> ....
>
> But I would like to first define a list containing all possible sets of
> regressors, and then execute each one in a loop/lapply. Unfortunately I got
> lost in trying to paste variables' name in the formula with no result.
>
> many thanks in advance.
>
> paolo
>
>
>
> --
> Paolo Brunori
> Ricercatore in Economia Politica & Life Course Centre Fellow
> Dipartimento di Scienze Economiche - Universit? di Bari
> www.uniba.it/docenti/brunori-paolo
> www.equalchances.org
> www.lifecoursecentre.org.au
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Sat Nov  5 01:43:24 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 4 Nov 2016 17:43:24 -0700
Subject: [R] Using map with filled.contour doesn't display map
In-Reply-To: <CAFeyMuw0SJLqVDg70KFYTwb3eLYzBV6SweAZD_ONSWBq7csDzw@mail.gmail.com>
References: <CAFeyMuw0SJLqVDg70KFYTwb3eLYzBV6SweAZD_ONSWBq7csDzw@mail.gmail.com>
Message-ID: <D57DD8C2-4D93-4853-95B5-4FFACDEADB71@comcast.net>


> On Nov 4, 2016, at 11:26 AM, Jeff Charlton <jcharlto16 at gmail.com> wrote:
> 
> Hello,
> 
> I'm trying to overlay a map on top of data showing temperatures across the
> world. The code I'm using is:
> 
>      filledContour(tempdata, plot.axes={axis(1); axis(2); map("world2",
> add=TRUE)})
> 
> where tempdata is a raster file made from a netcdf file downloaded from the
> NOAA website. The filledContour is a wrapper that translates the raster
> image into something that can be used by filled.contour.
> 
> If I run the code:
> 
>  filledContour(tempdata)

It's somewhat difficult to advise since in one place you say you are using filled.contour, but above you are writing filledContour (but are not saying where this function comes from or offering the required `library` call to set this up.). You are also not offering any data example. Suggest you make this a complete example.

> 
> I get the image I want, but when I add the plot.axes parameter, I get a
> "plot.new has not been called yet" error in axis(1). If I run plot.new()
> and the original filledContour call, I get the same result as if I just did
> filledContour(tempdata) (i.e. no map is overlayed).
> 
> The kicker in all this is that this code worked a couple of years ago. I
> ran into this error when I recently dusted of my code and installed the
> latest version of R and its various packages. I have a feeling that
> something has changed in the maps package but I can't find any reference to
> it online.
> 
> Can anyone help?
> 
> Thank you,
> 
> Jeff
> 
> Just in case, here's my session info:
> 
> R version 3.3.2 (2016-10-31) Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 7 x64 (build 7601) Service Pack 1
> 
> locale: [1] LC_COLLATE=English_United States.1252 LC_CTYPE=English_United
> States.1252 LC_MONETARY=English_United States.1252 [4] LC_NUMERIC=C
> LC_TIME=English_United States.1252
> 
> attached base packages: [1] stats graphics grDevices utils datasets methods
> base
> 
> other attached packages: [1] ncdf4_1.15 maps_3.1.1 XML_3.98-1.4
> RCurl_1.95-4.8 bitops_1.0-6 raster_2.5-8 sp_1.2-3
> 
> loaded via a namespace (and not attached): [1] parallel_3.3.2 tools_3.3.2
> Rcpp_0.12.7 grid_3.3.2 lattice_0.20-34
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jcharlto16 at gmail.com  Sat Nov  5 05:53:18 2016
From: jcharlto16 at gmail.com (Jeff Charlton)
Date: Fri, 4 Nov 2016 22:53:18 -0600
Subject: [R] Using map with filled.contour doesn't display map
In-Reply-To: <D57DD8C2-4D93-4853-95B5-4FFACDEADB71@comcast.net>
References: <CAFeyMuw0SJLqVDg70KFYTwb3eLYzBV6SweAZD_ONSWBq7csDzw@mail.gmail.com>
	<D57DD8C2-4D93-4853-95B5-4FFACDEADB71@comcast.net>
Message-ID: <CAFeyMuymQpSCquJP_jXsZYveVif2KfKQYotBS0djS7nGXhb=hg@mail.gmail.com>

Hi David,

Thank you for the response.  I apologize that I didn't give a more complete
example.  Here's code that recreates the issue I'm having on my system.
You'll have to enter the path to your packages and where you want to store
the downloaded file.

library("maps", lib.loc="<your library location>")
library("raster", lib.loc="<your library location>")
library("ncdf", lib.loc="<your library location>")

download.file("
http://schubert.atmos.colostate.edu/~cslocum/code/air.sig995.2012.nc",
"<your path>/temp_file.nc")
temp_file = raster("<your path>/temp_file.nc")
filledContour(temp_file, plot.axes={axis(1); axis(2); map("world2",
add=TRUE)})

When this is run, you should get a plot that shows some contours overlaid
on a map of world.  I originally wrote this several years ago. This code
worked for me with the following versions of packages: R (3.0.1); ncdf
(1.6.6); maps (2.3-6); and raster (2.1-49).

Now, on my new, upgraded system, when I run this, I get a "plot.new has not
been called yet" error.  If I then execute a plot.new() command and try the
filledContour command again, I get the contours, but no map.  As I said
below, I believe something has changed in the maps package, but I can't
find anything in the documentation on it or an indication that others are
having the same issue.  My current versions are:  R (3.3.2); ncdf4 (1.15);
maps (3.1.1); and raster (2.5-8).

I've changed from the ncdf package to the ncdf4 package, but I wouldn't
expect that to be an issue as that package is for reading netCDF files into
R and that isn't an issue (i.e. I can do a plot(temp_file) to see that the
file has been loaded properly)

Thanks for your time,
Jeff



On Fri, Nov 4, 2016 at 6:43 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Nov 4, 2016, at 11:26 AM, Jeff Charlton <jcharlto16 at gmail.com> wrote:
> >
> > Hello,
> >
> > I'm trying to overlay a map on top of data showing temperatures across
> the
> > world. The code I'm using is:
> >
> >      filledContour(tempdata, plot.axes={axis(1); axis(2); map("world2",
> > add=TRUE)})
> >
> > where tempdata is a raster file made from a netcdf file downloaded from
> the
> > NOAA website. The filledContour is a wrapper that translates the raster
> > image into something that can be used by filled.contour.
> >
> > If I run the code:
> >
> >  filledContour(tempdata)
>
> It's somewhat difficult to advise since in one place you say you are using
> filled.contour, but above you are writing filledContour (but are not saying
> where this function comes from or offering the required `library` call to
> set this up.). You are also not offering any data example. Suggest you make
> this a complete example.
>
> >
> > I get the image I want, but when I add the plot.axes parameter, I get a
> > "plot.new has not been called yet" error in axis(1). If I run plot.new()
> > and the original filledContour call, I get the same result as if I just
> did
> > filledContour(tempdata) (i.e. no map is overlayed).
> >
> > The kicker in all this is that this code worked a couple of years ago. I
> > ran into this error when I recently dusted of my code and installed the
> > latest version of R and its various packages. I have a feeling that
> > something has changed in the maps package but I can't find any reference
> to
> > it online.
> >
> > Can anyone help?
> >
> > Thank you,
> >
> > Jeff
> >
> > Just in case, here's my session info:
> >
> > R version 3.3.2 (2016-10-31) Platform: x86_64-w64-mingw32/x64 (64-bit)
> > Running under: Windows 7 x64 (build 7601) Service Pack 1
> >
> > locale: [1] LC_COLLATE=English_United States.1252 LC_CTYPE=English_United
> > States.1252 LC_MONETARY=English_United States.1252 [4] LC_NUMERIC=C
> > LC_TIME=English_United States.1252
> >
> > attached base packages: [1] stats graphics grDevices utils datasets
> methods
> > base
> >
> > other attached packages: [1] ncdf4_1.15 maps_3.1.1 XML_3.98-1.4
> > RCurl_1.95-4.8 bitops_1.0-6 raster_2.5-8 sp_1.2-3
> >
> > loaded via a namespace (and not attached): [1] parallel_3.3.2 tools_3.3.2
> > Rcpp_0.12.7 grid_3.3.2 lattice_0.20-34
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
>

	[[alternative HTML version deleted]]


From galaxie2485 at yahoo.co.in  Sat Nov  5 06:28:48 2016
From: galaxie2485 at yahoo.co.in (Priya Arasu)
Date: Sat, 5 Nov 2016 05:28:48 +0000 (UTC)
Subject: [R] Help-Text file
References: <1207914484.662606.1478323728775.ref@mail.yahoo.com>
Message-ID: <1207914484.662606.1478323728775@mail.yahoo.com>

Hi, 
???? I am using R 3.3.1 in R studio version 1.0.44 (Windows10, 64 bit system). I could import text file with Boolean rules, but when I start doing analysis in R, I get the error: I have been trying to do attractor analysis using R package Boolnet, using the command: attr <- getAttractors(net, type="asynchronous"). This command throws up the below error:

Error in matrix(nrow = n, ncol = length(network$genes)) : 
  invalid 'nrow' value (too large or NA)
In addition: Warning message:
In matrix(nrow = n, ncol = length(network$genes)) :
  NAs introduced by coercion to integer range


?Have anyone come across this error?. I have attached the text file with Boolean rules. Pls find it. Any suggestions or ideas, would be of great help
Thank you
Priya
















-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: New Text Document-1.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20161105/a90c4d27/attachment.txt>

From galaxie2485 at yahoo.co.in  Sat Nov  5 07:07:20 2016
From: galaxie2485 at yahoo.co.in (Priya Arasu)
Date: Sat, 5 Nov 2016 06:07:20 +0000 (UTC)
Subject: [R] Stop R script from running, when the memory goes beyond 15.4GB
References: <247829627.697258.1478326040716.ref@mail.yahoo.com>
Message-ID: <247829627.697258.1478326040716@mail.yahoo.com>

Hi, 
???? I am using R 3.3.1 in RedHat Linux7 , 64bit system (16GB RAM).? I could run simulation in R package, Boolnet for 25 genes without any problem. When I add more gene i.e. 26 genes, the system becomes slow. Simulation keeps running for more than a day. I have to stop the simulation, as it takes a lot of memory. I need a suggestion here, I want to stop the script from running when the script takes memory beyond 15.4 GB. How do I do it? Should I generate warning in my script or use typeCatch?
Any idea or advice, would be of great help
Thank you
Priya


?

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sat Nov  5 14:59:22 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 5 Nov 2016 06:59:22 -0700
Subject: [R] loop with variable names
In-Reply-To: <aa2306b6-839b-3787-6c41-8037d3f8d33e@uniba.it>
References: <aa2306b6-839b-3787-6c41-8037d3f8d33e@uniba.it>
Message-ID: <CAGxFJbQzNQogPbGn9YxMbeCjLkt-Kxdraa=2F169zYmphRehVA@mail.gmail.com>

1. Statistically, you probably don't want to do this at all (but
that's another story).

2. Programatically, you probably want to use one of several packages
that do it already rather than trying to reinvent the wheel. A quick
search on rseek.org for "all subsets regression" brought up this:

http://rpubs.com/kaz_yos/all-subset

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Nov 4, 2016 at 9:53 AM, paolo brunori <paolo.brunori at uniba.it> wrote:
> Suppose I have the following data:
>
> y<-rnorm(10)
> age<-rnorm(10)
> sex<-rbinom(10,1, 0.5)
> edu<-round(runif(10, 1, 20))
> edu2<-edu^2
>
> df<-data.frame(y,age,sex,edu,edu2)
>
> I want to run a large number of models, for example:
>
> lm(y~age)
> lm(y~age+sex)
> lm(y~age+sex+edu)
> lm(y~age+sex+edu+edu2)
> lm(y~sex+edu2)
> lm(y~age+edu+edu2)
> ....
>
> But I would like to first define a list containing all possible sets of
> regressors, and then execute each one in a loop/lapply. Unfortunately I got
> lost in trying to paste variables' name in the formula with no result.
>
> many thanks in advance.
>
> paolo
>
>
>
> --
> Paolo Brunori
> Ricercatore in Economia Politica & Life Course Centre Fellow
> Dipartimento di Scienze Economiche - Universit? di Bari
> www.uniba.it/docenti/brunori-paolo
> www.equalchances.org
> www.lifecoursecentre.org.au
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From istazahn at gmail.com  Sat Nov  5 15:41:40 2016
From: istazahn at gmail.com (Ista Zahn)
Date: Sat, 5 Nov 2016 10:41:40 -0400
Subject: [R] Stop R script from running,
	when the memory goes beyond 15.4GB
In-Reply-To: <247829627.697258.1478326040716@mail.yahoo.com>
References: <247829627.697258.1478326040716.ref@mail.yahoo.com>
	<247829627.697258.1478326040716@mail.yahoo.com>
Message-ID: <CA+vqiLFNEt=JcUdgF46VJ26TLBeAsXYt39VRbAn0Mk2vQR-yRA@mail.gmail.com>

See

man ulimit

Best,
Ista

On Nov 5, 2016 8:11 AM, "Priya Arasu via R-help" <r-help at r-project.org>
wrote:

> Hi,
>      I am using R 3.3.1 in RedHat Linux7 , 64bit system (16GB RAM).  I
> could run simulation in R package, Boolnet for 25 genes without any
> problem. When I add more gene i.e. 26 genes, the system becomes slow.
> Simulation keeps running for more than a day. I have to stop the
> simulation, as it takes a lot of memory. I need a suggestion here, I want
> to stop the script from running when the script takes memory beyond 15.4
> GB. How do I do it? Should I generate warning in my script or use typeCatch?
> Any idea or advice, would be of great help
> Thank you
> Priya
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Sat Nov  5 17:33:51 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 05 Nov 2016 09:33:51 -0700
Subject: [R] Using map with filled.contour doesn't display map
In-Reply-To: <CAFeyMuymQpSCquJP_jXsZYveVif2KfKQYotBS0djS7nGXhb=hg@mail.gmail.com>
References: <CAFeyMuw0SJLqVDg70KFYTwb3eLYzBV6SweAZD_ONSWBq7csDzw@mail.gmail.com>
	<D57DD8C2-4D93-4853-95B5-4FFACDEADB71@comcast.net>
	<CAFeyMuymQpSCquJP_jXsZYveVif2KfKQYotBS0djS7nGXhb=hg@mail.gmail.com>
Message-ID: <E4EE261E-507D-4F9E-9777-B583CF2AD221@dcn.davis.ca.us>

I get the error also (R3.3.1/raster2.5-8).

Works fine if I call plot.new() before calling filledContour.

Given this is a contributed package you should be corresponding with the package maintainer if you think this solution is inappropriate or the documentation needs fixing, but I suspect you are better off being able to initialize the plot yourself.
-- 
Sent from my phone. Please excuse my brevity.

On November 4, 2016 9:53:18 PM PDT, Jeff Charlton <jcharlto16 at gmail.com> wrote:
>Hi David,
>
>Thank you for the response.  I apologize that I didn't give a more
>complete
>example.  Here's code that recreates the issue I'm having on my system.
>You'll have to enter the path to your packages and where you want to
>store
>the downloaded file.
>
>library("maps", lib.loc="<your library location>")
>library("raster", lib.loc="<your library location>")
>library("ncdf", lib.loc="<your library location>")
>
>download.file("
>http://schubert.atmos.colostate.edu/~cslocum/code/air.sig995.2012.nc",
>"<your path>/temp_file.nc")
>temp_file = raster("<your path>/temp_file.nc")
>filledContour(temp_file, plot.axes={axis(1); axis(2); map("world2",
>add=TRUE)})
>
>When this is run, you should get a plot that shows some contours
>overlaid
>on a map of world.  I originally wrote this several years ago. This
>code
>worked for me with the following versions of packages: R (3.0.1); ncdf
>(1.6.6); maps (2.3-6); and raster (2.1-49).
>
>Now, on my new, upgraded system, when I run this, I get a "plot.new has
>not
>been called yet" error.  If I then execute a plot.new() command and try
>the
>filledContour command again, I get the contours, but no map.  As I said
>below, I believe something has changed in the maps package, but I can't
>find anything in the documentation on it or an indication that others
>are
>having the same issue.  My current versions are:  R (3.3.2); ncdf4
>(1.15);
>maps (3.1.1); and raster (2.5-8).
>
>I've changed from the ncdf package to the ncdf4 package, but I wouldn't
>expect that to be an issue as that package is for reading netCDF files
>into
>R and that isn't an issue (i.e. I can do a plot(temp_file) to see that
>the
>file has been loaded properly)
>
>Thanks for your time,
>Jeff
>
>
>
>On Fri, Nov 4, 2016 at 6:43 PM, David Winsemius
><dwinsemius at comcast.net>
>wrote:
>
>>
>> > On Nov 4, 2016, at 11:26 AM, Jeff Charlton <jcharlto16 at gmail.com>
>wrote:
>> >
>> > Hello,
>> >
>> > I'm trying to overlay a map on top of data showing temperatures
>across
>> the
>> > world. The code I'm using is:
>> >
>> >      filledContour(tempdata, plot.axes={axis(1); axis(2);
>map("world2",
>> > add=TRUE)})
>> >
>> > where tempdata is a raster file made from a netcdf file downloaded
>from
>> the
>> > NOAA website. The filledContour is a wrapper that translates the
>raster
>> > image into something that can be used by filled.contour.
>> >
>> > If I run the code:
>> >
>> >  filledContour(tempdata)
>>
>> It's somewhat difficult to advise since in one place you say you are
>using
>> filled.contour, but above you are writing filledContour (but are not
>saying
>> where this function comes from or offering the required `library`
>call to
>> set this up.). You are also not offering any data example. Suggest
>you make
>> this a complete example.
>>
>> >
>> > I get the image I want, but when I add the plot.axes parameter, I
>get a
>> > "plot.new has not been called yet" error in axis(1). If I run
>plot.new()
>> > and the original filledContour call, I get the same result as if I
>just
>> did
>> > filledContour(tempdata) (i.e. no map is overlayed).
>> >
>> > The kicker in all this is that this code worked a couple of years
>ago. I
>> > ran into this error when I recently dusted of my code and installed
>the
>> > latest version of R and its various packages. I have a feeling that
>> > something has changed in the maps package but I can't find any
>reference
>> to
>> > it online.
>> >
>> > Can anyone help?
>> >
>> > Thank you,
>> >
>> > Jeff
>> >
>> > Just in case, here's my session info:
>> >
>> > R version 3.3.2 (2016-10-31) Platform: x86_64-w64-mingw32/x64
>(64-bit)
>> > Running under: Windows 7 x64 (build 7601) Service Pack 1
>> >
>> > locale: [1] LC_COLLATE=English_United States.1252
>LC_CTYPE=English_United
>> > States.1252 LC_MONETARY=English_United States.1252 [4] LC_NUMERIC=C
>> > LC_TIME=English_United States.1252
>> >
>> > attached base packages: [1] stats graphics grDevices utils datasets
>> methods
>> > base
>> >
>> > other attached packages: [1] ncdf4_1.15 maps_3.1.1 XML_3.98-1.4
>> > RCurl_1.95-4.8 bitops_1.0-6 raster_2.5-8 sp_1.2-3
>> >
>> > loaded via a namespace (and not attached): [1] parallel_3.3.2
>tools_3.3.2
>> > Rcpp_0.12.7 grid_3.3.2 lattice_0.20-34
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> David Winsemius
>> Alameda, CA, USA
>>
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Sat Nov  5 17:58:49 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 05 Nov 2016 09:58:49 -0700
Subject: [R] Help-Text file
In-Reply-To: <1207914484.662606.1478323728775@mail.yahoo.com>
References: <1207914484.662606.1478323728775.ref@mail.yahoo.com>
	<1207914484.662606.1478323728775@mail.yahoo.com>
Message-ID: <84D0569E-481A-4615-8556-9A08E306824E@dcn.davis.ca.us>

Sorry, but this request is quite hard to understand... you are basically directing your question at an incredibly narrow set of people who happen to have combined your kind of data read in the way you read it in with your kind of analysis algorithms while telling us almost nothing about how you did those things.

Please read the Posting Guide mentioned in the footer of every posting on this list and provide a reproducible example [1][2]. If the problem is in your use of R then someone here will be able to help. If your problem is inside a contributed package then you may need the assistance of the package maintainer, but they will also need a reproducible example so your effort will not be in vain. Also, the act of creating a RE often leads you to your own solution even without getting outside help. 

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

[2] http://adv-r.had.co.nz/Reproducibility.html

-- 
Sent from my phone. Please excuse my brevity.

On November 4, 2016 10:28:48 PM PDT, Priya Arasu via R-help <r-help at r-project.org> wrote:
>Hi, 
>???? I am using R 3.3.1 in R studio version 1.0.44 (Windows10, 64 bit
>system). I could import text file with Boolean rules, but when I start
>doing analysis in R, I get the error: I have been trying to do
>attractor analysis using R package Boolnet, using the command: attr <-
>getAttractors(net, type="asynchronous"). This command throws up the
>below error:
>
>Error in matrix(nrow = n, ncol = length(network$genes)) : 
>  invalid 'nrow' value (too large or NA)
>In addition: Warning message:
>In matrix(nrow = n, ncol = length(network$genes)) :
>  NAs introduced by coercion to integer range
>
>
>?Have anyone come across this error?. I have attached the text file
>with Boolean rules. Pls find it. Any suggestions or ideas, would be of
>great help
>Thank you
>Priya
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Sat Nov  5 18:31:40 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 5 Nov 2016 10:31:40 -0700
Subject: [R] Using map with filled.contour doesn't display map
In-Reply-To: <E4EE261E-507D-4F9E-9777-B583CF2AD221@dcn.davis.ca.us>
References: <CAFeyMuw0SJLqVDg70KFYTwb3eLYzBV6SweAZD_ONSWBq7csDzw@mail.gmail.com>
	<D57DD8C2-4D93-4853-95B5-4FFACDEADB71@comcast.net>
	<CAFeyMuymQpSCquJP_jXsZYveVif2KfKQYotBS0djS7nGXhb=hg@mail.gmail.com>
	<E4EE261E-507D-4F9E-9777-B583CF2AD221@dcn.davis.ca.us>
Message-ID: <1390143C-633C-46C7-A150-19E68C978AFC@comcast.net>


> On Nov 5, 2016, at 9:33 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> I get the error also (R3.3.1/raster2.5-8).
> 
> Works fine if I call plot.new() before calling filledContour.

It didn't "work fine" when I did that. I first briefly see a generic plotting frame (x and y axes with no data) which was then blanked out and replaced with a colored  "strip  map" of temperatures but no map of continents. Further I get the message:

> library("ncdf")
Package 'ncdf' is deprecated and will be removed from CRAN shortly.
Use instead package RNetCDF or ncdf4 (for Windows from CRANextras)

I would further note that in the original sessionInfo() output there was no instance of `ncdf` only `ncdf4`. However, that doesn't explain the problem. The same behavior appears if I restart R and instead load ncdf4.

I was also unable to get any further plotting ... which was the original question, I thought. In particular this failed to update the <non-map-outlined> image.

filledContour( temp_file, plot.axes={axis(1); axis(2); map("world2", add=TRUE); points(0,0) })

If you look at map() you see that it also messes with the par('mar') values. I think the add=TRUE is overriding the plot=TRUE parameter to map.

I can get a subsequent `points(0,0)`-call to plot a point but the location is not correct on the y axis.

I think you would be better advised to start with map() and then fill in the colors using the values extracted from your NCDF file with the methods in the setup portion of filledContour.

David.




> 
> Given this is a contributed package you should be corresponding with the package maintainer if you think this solution is inappropriate or the documentation needs fixing, but I suspect you are better off being able to initialize the plot yourself.
> -- 
> Sent from my phone. Please excuse my brevity.
> 
> On November 4, 2016 9:53:18 PM PDT, Jeff Charlton <jcharlto16 at gmail.com> wrote:
>> Hi David,
>> 
>> Thank you for the response.  I apologize that I didn't give a more
>> complete
>> example.  Here's code that recreates the issue I'm having on my system.
>> You'll have to enter the path to your packages and where you want to
>> store
>> the downloaded file.
>> 
>> library("maps", lib.loc="<your library location>")
>> library("raster", lib.loc="<your library location>")
>> library("ncdf", lib.loc="<your library location>")
>> 
>> download.file("
>> http://schubert.atmos.colostate.edu/~cslocum/code/air.sig995.2012.nc",
>> "<your path>/temp_file.nc")
>> temp_file = raster("<your path>/temp_file.nc")
>> filledContour(temp_file, plot.axes={axis(1); axis(2); map("world2",
>> add=TRUE)})
>> 
>> When this is run, you should get a plot that shows some contours
>> overlaid
>> on a map of world.  I originally wrote this several years ago. This
>> code
>> worked for me with the following versions of packages: R (3.0.1); ncdf
>> (1.6.6); maps (2.3-6); and raster (2.1-49).
>> 
>> Now, on my new, upgraded system, when I run this, I get a "plot.new has
>> not
>> been called yet" error.  If I then execute a plot.new() command and try
>> the
>> filledContour command again, I get the contours, but no map.  As I said
>> below, I believe something has changed in the maps package, but I can't
>> find anything in the documentation on it or an indication that others
>> are
>> having the same issue.  My current versions are:  R (3.3.2); ncdf4
>> (1.15);
>> maps (3.1.1); and raster (2.5-8).
>> 
>> I've changed from the ncdf package to the ncdf4 package, but I wouldn't
>> expect that to be an issue as that package is for reading netCDF files
>> into
>> R and that isn't an issue (i.e. I can do a plot(temp_file) to see that
>> the
>> file has been loaded properly)
>> 
>> Thanks for your time,
>> Jeff
>> 
>> 
>> 
>> On Fri, Nov 4, 2016 at 6:43 PM, David Winsemius
>> <dwinsemius at comcast.net>
>> wrote:
>> 
>>> 
>>>> On Nov 4, 2016, at 11:26 AM, Jeff Charlton <jcharlto16 at gmail.com>
>> wrote:
>>>> 
>>>> Hello,
>>>> 
>>>> I'm trying to overlay a map on top of data showing temperatures
>> across
>>> the
>>>> world. The code I'm using is:
>>>> 
>>>>     filledContour(tempdata, plot.axes={axis(1); axis(2);
>> map("world2",
>>>> add=TRUE)})
>>>> 
>>>> where tempdata is a raster file made from a netcdf file downloaded
>> from
>>> the
>>>> NOAA website. The filledContour is a wrapper that translates the
>> raster
>>>> image into something that can be used by filled.contour.
>>>> 
>>>> If I run the code:
>>>> 
>>>> filledContour(tempdata)
>>> 
>>> It's somewhat difficult to advise since in one place you say you are
>> using
>>> filled.contour, but above you are writing filledContour (but are not
>> saying
>>> where this function comes from or offering the required `library`
>> call to
>>> set this up.). You are also not offering any data example. Suggest
>> you make
>>> this a complete example.
>>> 
>>>> 
>>>> I get the image I want, but when I add the plot.axes parameter, I
>> get a
>>>> "plot.new has not been called yet" error in axis(1). If I run
>> plot.new()
>>>> and the original filledContour call, I get the same result as if I
>> just
>>> did
>>>> filledContour(tempdata) (i.e. no map is overlayed).
>>>> 
>>>> The kicker in all this is that this code worked a couple of years
>> ago. I
>>>> ran into this error when I recently dusted of my code and installed
>> the
>>>> latest version of R and its various packages. I have a feeling that
>>>> something has changed in the maps package but I can't find any
>> reference
>>> to
>>>> it online.
>>>> 
>>>> Can anyone help?
>>>> 
>>>> Thank you,
>>>> 
>>>> Jeff
>>>> 
>>>> Just in case, here's my session info:
>>>> 
>>>> R version 3.3.2 (2016-10-31) Platform: x86_64-w64-mingw32/x64
>> (64-bit)
>>>> Running under: Windows 7 x64 (build 7601) Service Pack 1
>>>> 
>>>> locale: [1] LC_COLLATE=English_United States.1252
>> LC_CTYPE=English_United
>>>> States.1252 LC_MONETARY=English_United States.1252 [4] LC_NUMERIC=C
>>>> LC_TIME=English_United States.1252
>>>> 
>>>> attached base packages: [1] stats graphics grDevices utils datasets
>>> methods
>>>> base
>>>> 
>>>> other attached packages: [1] ncdf4_1.15 maps_3.1.1 XML_3.98-1.4
>>>> RCurl_1.95-4.8 bitops_1.0-6 raster_2.5-8 sp_1.2-3
>>>> 
>>>> loaded via a namespace (and not attached): [1] parallel_3.3.2
>> tools_3.3.2
>>>> Rcpp_0.12.7 grid_3.3.2 lattice_0.20-34
>>>> 
>>>>      [[alternative HTML version deleted]]
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/
>>> posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> David Winsemius
>>> Alameda, CA, USA
>>> 
>>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 

David Winsemius
Alameda, CA, USA


From varinsacha at yahoo.fr  Sat Nov  5 19:27:03 2016
From: varinsacha at yahoo.fr (varin sacha)
Date: Sat, 5 Nov 2016 18:27:03 +0000 (UTC)
Subject: [R] Problem with downloading library(Rcmdr)
References: <496599242.1873911.1478370423476.ref@mail.yahoo.com>
Message-ID: <496599242.1873911.1478370423476@mail.yahoo.com>

Dear R-experts,

Here below you will find my R sessionInfo. I have a problem with the library(Rcmdr). The error message is here below. How can I solve that problem ?


Many thanks.



sessionInfo()
R version 3.3.2 (2016-10-31)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: macOS Sierra 10.12

locale:
[1] fr_CH.UTF-8/fr_CH.UTF-8/fr_CH.UTF-8/C/fr_CH.UTF-8/fr_CH.UTF-8

attached base packages:
[1] splines   stats     graphics  grDevices utils     datasets  methods   base 

other attached packages:
[1] RcmdrMisc_1.0-5 sandwich_2.3-4  car_2.1-3 

loaded via a namespace (and not attached):
[1] Rcpp_0.12.7         tcltk2_1.2-11       nloptr_1.0.4        RColorBrewer_1.1-2  plyr_1.8.4          class_7.3-14 
[7] tools_3.3.2         digest_0.6.10       rpart_4.1-10        lme4_1.1-12         gtable_0.2.0        nlme_3.1-128 
[13] htmlTable_1.7       lattice_0.20-34     mgcv_1.8-15         Matrix_1.2-7.1      parallel_3.3.2      SparseM_1.72 
[19] relimp_1.0-5        e1071_1.6-7         gridExtra_2.2.1     Rcmdr_2.3-1         stringr_1.1.0       cluster_2.0.5 
[25] knitr_1.14          MatrixModels_0.4-1  grid_3.3.2          nnet_7.3-12         data.table_1.9.6    tcltk_3.3.2 
[31] readxl_0.1.1        survival_2.40-1     foreign_0.8-67      latticeExtra_0.6-28 minqa_1.2.4         Formula_1.2-1 
[37] ggplot2_2.1.0       magrittr_1.5        htmltools_0.3.5     Hmisc_4.0-0         scales_0.4.0        MASS_7.3-45 
[43] abind_1.4-5         pbkrtest_0.4-6      colorspace_1.2-7    quantreg_5.29       stringi_1.1.2       acepack_1.4.1 
[49] munsell_0.4.3       chron_2.3-47        zoo_1.7-13 


> library(Rcmdr)
Error : .onAttach a ?chou? dans attachNamespace() pour 'Rcmdr', d?tails :
appel : structure(.External(.C_dotTclObjv, objv), class = "tclObj")
erreur : [tcl] invalid command name "image".

Erreur : le chargement du package ou de l'espace de noms a ?chou? pour 'Rcmdr'


From dwinsemius at comcast.net  Sat Nov  5 20:42:50 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 5 Nov 2016 12:42:50 -0700
Subject: [R] Problem with downloading library(Rcmdr)
In-Reply-To: <496599242.1873911.1478370423476@mail.yahoo.com>
References: <496599242.1873911.1478370423476.ref@mail.yahoo.com>
	<496599242.1873911.1478370423476@mail.yahoo.com>
Message-ID: <CF09170F-C599-4EDF-9C79-FE94B5490590@comcast.net>


> On Nov 5, 2016, at 11:27 AM, varin sacha via R-help <r-help at r-project.org> wrote:
> 
> Dear R-experts,
> 
> Here below you will find my R sessionInfo. I have a problem with the library(Rcmdr). The error message is here below. How can I solve that problem ?

I'm not an Rcmdr user and my Mac is too old to support Sierra so unable to attempt replication. All I am able offer is a search on what appear to be similar issues in the past. You do need to indicate your XQuartz version. I see there have been three recent releases suggesting to me that there might have been teething difficulties with the birth of the neanate Sierra.

	? XQuartz 2.7.11 - 2016-10-29

	? XQuartz 2.7.10 - 2016-10-22

	? XQuartz 2.7.9 - 2016-05-05

You might also mention in any follow-up whether this is being done inside MacGUI or at an R console launched from Terminal.app.

Make sure you have reviewed:
http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/installation-notes.html


Over the years there have been many questions regarding the three-way marriage of R, MacOS, and Rcmdr that relate to Tk/Tcl functions. Here's a relatively recent one that mentions the image function:

http://markmail.org/search/?q=list%3Aorg.r-project.r-sig-mac+tcl+Rcmdr+image#query:list%3Aorg.r-project.r-sig-mac%20tcl%20Rcmdr%20image+page:1+mid:y5mgjtgtw65eqzfu+state:results


So I think I would make sure you have a version of R that has Tcl and have the most uptodate X11 version (XQuartz) and once you do that and then try to reload R, load Rcmdr _before_ RcmdrMisc and before all those other packages. If you still get an error, then repost with version details of _all_ the components mentioned to the  R-SIG-Mac mailing list and copy John Fox. (Might not be necessary to give him a copy since I think he monitors R-SIG-Mac for Rcmdr issues.)


-- 
David.
> 
> 
> Many thanks.
> 
> 
> 
> sessionInfo()
> R version 3.3.2 (2016-10-31)
> Platform: x86_64-apple-darwin13.4.0 (64-bit)
> Running under: macOS Sierra 10.12
> 
> locale:
> [1] fr_CH.UTF-8/fr_CH.UTF-8/fr_CH.UTF-8/C/fr_CH.UTF-8/fr_CH.UTF-8
> 
> attached base packages:
> [1] splines   stats     graphics  grDevices utils     datasets  methods   base 
> 
> other attached packages:
> [1] RcmdrMisc_1.0-5 sandwich_2.3-4  car_2.1-3 
> 
> loaded via a namespace (and not attached):
> [1] Rcpp_0.12.7         tcltk2_1.2-11       nloptr_1.0.4        RColorBrewer_1.1-2  plyr_1.8.4          class_7.3-14 
> [7] tools_3.3.2         digest_0.6.10       rpart_4.1-10        lme4_1.1-12         gtable_0.2.0        nlme_3.1-128 
> [13] htmlTable_1.7       lattice_0.20-34     mgcv_1.8-15         Matrix_1.2-7.1      parallel_3.3.2      SparseM_1.72 
> [19] relimp_1.0-5        e1071_1.6-7         gridExtra_2.2.1     Rcmdr_2.3-1         stringr_1.1.0       cluster_2.0.5 
> [25] knitr_1.14          MatrixModels_0.4-1  grid_3.3.2          nnet_7.3-12         data.table_1.9.6    tcltk_3.3.2 
> [31] readxl_0.1.1        survival_2.40-1     foreign_0.8-67      latticeExtra_0.6-28 minqa_1.2.4         Formula_1.2-1 
> [37] ggplot2_2.1.0       magrittr_1.5        htmltools_0.3.5     Hmisc_4.0-0         scales_0.4.0        MASS_7.3-45 
> [43] abind_1.4-5         pbkrtest_0.4-6      colorspace_1.2-7    quantreg_5.29       stringi_1.1.2       acepack_1.4.1 
> [49] munsell_0.4.3       chron_2.3-47        zoo_1.7-13 
> 
> 
>> library(Rcmdr)
> Error : .onAttach a ?chou? dans attachNamespace() pour 'Rcmdr', d?tails :
> appel : structure(.External(.C_dotTclObjv, objv), class = "tclObj")
> erreur : [tcl] invalid command name "image".
> 
> Erreur : le chargement du package ou de l'espace de noms a ?chou? pour 'Rcmdr'
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From klebyn at yahoo.com.br  Sat Nov  5 17:06:26 2016
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Sat, 5 Nov 2016 14:06:26 -0200
Subject: [R] tcl('tk::fontchooser', 'show') : Hoe to use? R-devel Tcl 8.6
Message-ID: <d2def164-1798-e130-614c-70acf4f4b5f3@yahoo.com.br>

Hello,
Somebody would indicate the correct way to use the option the
fontchooser widget into tcltk package (in R-devel, Tcl 8.6) ?
I tried to use as other traditional widgets but no success. (code below)
Thanks
Cleber

 > tt <- tktoplevel(); but <- ttkbutton( tt, text='Test'); tcl('pack', but )
<Tcl>
 > tcl( but, 'configure', '-text' )
<Tcl> -text text Text {} Test
 > tcl( but, 'configure', text=NULL )
<Tcl> -text text Text {} Test
 > tcl( but, 'cget', '-text')
<Tcl> Test
 >
 >
 > tcl('tk::fontchooser', 'show')
<Tcl>
 > tcl( 'tk::fontchooser', 'configure', '-font')
<Tcl>
 >
 >
 > sessionInfo()
R Under development (unstable) (2016-11-01 r71616)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7600)

locale:
[1] LC_COLLATE=Portuguese_Brazil.1252 LC_CTYPE=Portuguese_Brazil.1252
LC_MONETARY=Portuguese_Brazil.1252
[4] LC_NUMERIC=C LC_TIME=Portuguese_Brazil.1252

attached base packages:
[1] tcltk     stats     graphics  grDevices utils     datasets methods
base

loaded via a namespace (and not attached):
[1] tools_3.4.0
 >




---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus

	[[alternative HTML version deleted]]


From cfranga1 at jhu.edu  Sat Nov  5 19:37:49 2016
From: cfranga1 at jhu.edu (Constantine Frangakis)
Date: Sat, 5 Nov 2016 18:37:49 +0000
Subject: [R] question on lasso
Message-ID: <174D36C3-7AD6-43AC-AED2-3E0EE7ED4159@jhu.edu>

I would appreciate any comments to the following question.
I am trying to build a model for survival based on 155 patients and 70 covariates using lasso. Lasso picks, three variables only, say X1,X2,X3, and  omits the others. I wanted to check why a particular (clinically important) variable, say X4, is omitted by lasso. One of the things I did was I ran lasso on X1,X2,X3 and X4 only. The results (coefs) I get are different from running all 70 variables, and in fact now X4 is not omitted.
Why is that ? should it not be that the global (among all 70 variables) optimum, which is X1,X2,X3 and not X4, be also the local (among the four only) optimum ?
Thank you for your consideration


Constantine Frangakis, PhD
Professor
Departments of Biostatistics
Psychiatry, and Radiology
Johns Hopkins University






	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sat Nov  5 21:43:47 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 5 Nov 2016 13:43:47 -0700
Subject: [R] question on lasso
In-Reply-To: <174D36C3-7AD6-43AC-AED2-3E0EE7ED4159@jhu.edu>
References: <174D36C3-7AD6-43AC-AED2-3E0EE7ED4159@jhu.edu>
Message-ID: <CAGxFJbTyE+jhJ-ui4BQ+GSHQ-7gc2dy8313ZL42dPsX5h_cZpA@mail.gmail.com>

Wrong list. This list is about R programming not statistics. Try
stats.stackexchange.com for statistics questions.

However, I will make a suggestion: Find local statistical experts with
whom to consult. Your understanding appears to fall short of the
necessary background to use such complex statistical procedures
reliably. I am not confident that online lists will suffice.

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Nov 5, 2016 at 11:37 AM, Constantine Frangakis <cfranga1 at jhu.edu> wrote:
> I would appreciate any comments to the following question.
> I am trying to build a model for survival based on 155 patients and 70 covariates using lasso. Lasso picks, three variables only, say X1,X2,X3, and  omits the others. I wanted to check why a particular (clinically important) variable, say X4, is omitted by lasso. One of the things I did was I ran lasso on X1,X2,X3 and X4 only. The results (coefs) I get are different from running all 70 variables, and in fact now X4 is not omitted.
> Why is that ? should it not be that the global (among all 70 variables) optimum, which is X1,X2,X3 and not X4, be also the local (among the four only) optimum ?
> Thank you for your consideration
>
>
> Constantine Frangakis, PhD
> Professor
> Departments of Biostatistics
> Psychiatry, and Radiology
> Johns Hopkins University
>
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From hokut1 at yahoo.com  Sat Nov  5 22:22:20 2016
From: hokut1 at yahoo.com (oslo)
Date: Sat, 5 Nov 2016 21:22:20 +0000 (UTC)
Subject: [R] question on lasso
In-Reply-To: <174D36C3-7AD6-43AC-AED2-3E0EE7ED4159@jhu.edu>
References: <174D36C3-7AD6-43AC-AED2-3E0EE7ED4159@jhu.edu>
Message-ID: <1513944055.1005756.1478380940108@mail.yahoo.com>

Hi Dr. Franggakis;
This can be explained because of collinearity and suppressor variable in multiple regression models. In the first scenario, you have both correlated variables and suppressor variables in the second scenario you do not have this problem. I do wonder why to do not use the scale elastic net for this particular problem.
Good luck,Oslo 

    On Saturday, November 5, 2016 4:29 PM, Constantine Frangakis <cfranga1 at jhu.edu> wrote:
 

 I would appreciate any comments to the following question.
I am trying to build a model for survival based on 155 patients and 70 covariates using lasso. Lasso picks, three variables only, say X1,X2,X3, and? omits the others. I wanted to check why a particular (clinically important) variable, say X4, is omitted by lasso. One of the things I did was I ran lasso on X1,X2,X3 and X4 only. The results (coefs) I get are different from running all 70 variables, and in fact now X4 is not omitted.
Why is that ? should it not be that the global (among all 70 variables) optimum, which is X1,X2,X3 and not X4, be also the local (among the four only) optimum ?
Thank you for your consideration


Constantine Frangakis, PhD
Professor
Departments of Biostatistics
Psychiatry, and Radiology
Johns Hopkins University






??? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


   
	[[alternative HTML version deleted]]


From varinsacha at yahoo.fr  Sat Nov  5 23:33:47 2016
From: varinsacha at yahoo.fr (varin sacha)
Date: Sat, 5 Nov 2016 22:33:47 +0000 (UTC)
Subject: [R] Problem with downloading library(Rcmdr)
In-Reply-To: <CF09170F-C599-4EDF-9C79-FE94B5490590@comcast.net>
References: <496599242.1873911.1478370423476.ref@mail.yahoo.com>
	<496599242.1873911.1478370423476@mail.yahoo.com>
	<CF09170F-C599-4EDF-9C79-FE94B5490590@comcast.net>
Message-ID: <753312018.1959901.1478385227937@mail.yahoo.com>

Many thanks David,

My Rcmdr problem is solved but now I have another one with rlme package !

I want to use the hbrwts_gr function from the rlme package.
I have tried to download the rlme package, but rlme is not available (for R version 3.3.2) as you can see here below the error message.

How can I solve my problem ?


> install.packages("rlme")
Installing package into ?/Users/Caro/Library/R/3.3/library?
(as ?lib? is unspecified)
Warning in install.packages :
package ?rlme? is not available (for R version 3.3.2)






________________________________
De : David Winsemius <dwinsemius at comcast.net>

Cc : R-help Mailing List <r-help at r-project.org>
Envoy? le : Samedi 5 novembre 2016 20h42
Objet : Re: [R] Problem with downloading library(Rcmdr)



> On Nov 5, 2016, at 11:27 AM, varin sacha via R-help <r-help at r-project.org> wrote:
> 
> Dear R-experts,
> 
> Here below you will find my R sessionInfo. I have a problem with the library(Rcmdr). The error message is here below. How can I solve that problem ?

I'm not an Rcmdr user and my Mac is too old to support Sierra so unable to attempt replication. All I am able offer is a search on what appear to be similar issues in the past. You do need to indicate your XQuartz version. I see there have been three recent releases suggesting to me that there might have been teething difficulties with the birth of the neanate Sierra.

    ? XQuartz 2.7.11 - 2016-10-29

    ? XQuartz 2.7.10 - 2016-10-22

    ? XQuartz 2.7.9 - 2016-05-05

You might also mention in any follow-up whether this is being done inside MacGUI or at an R console launched from Terminal.app.

Make sure you have reviewed:
http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/installation-notes.html


Over the years there have been many questions regarding the three-way marriage of R, MacOS, and Rcmdr that relate to Tk/Tcl functions. Here's a relatively recent one that mentions the image function:

http://markmail.org/search/?q=list%3Aorg.r-project.r-sig-mac+tcl+Rcmdr+image#query:list%3Aorg.r-project.r-sig-mac%20tcl%20Rcmdr%20image+page:1+mid:y5mgjtgtw65eqzfu+state:results


So I think I would make sure you have a version of R that has Tcl and have the most uptodate X11 version (XQuartz) and once you do that and then try to reload R, load Rcmdr _before_ RcmdrMisc and before all those other packages. If you still get an error, then repost with version details of _all_ the components mentioned to the  R-SIG-Mac mailing list and copy John Fox. (Might not be necessary to give him a copy since I think he monitors R-SIG-Mac for Rcmdr issues.)


-- 
David.

> 
> 
> Many thanks.
> 
> 
> 
> sessionInfo()
> R version 3.3.2 (2016-10-31)
> Platform: x86_64-apple-darwin13.4.0 (64-bit)
> Running under: macOS Sierra 10.12
> 
> locale:
> [1] fr_CH.UTF-8/fr_CH.UTF-8/fr_CH.UTF-8/C/fr_CH.UTF-8/fr_CH.UTF-8
> 
> attached base packages:
> [1] splines   stats     graphics  grDevices utils     datasets  methods   base 
> 
> other attached packages:
> [1] RcmdrMisc_1.0-5 sandwich_2.3-4  car_2.1-3 
> 
> loaded via a namespace (and not attached):
> [1] Rcpp_0.12.7         tcltk2_1.2-11       nloptr_1.0.4        RColorBrewer_1.1-2  plyr_1.8.4          class_7.3-14 
> [7] tools_3.3.2         digest_0.6.10       rpart_4.1-10        lme4_1.1-12         gtable_0.2.0        nlme_3.1-128 
> [13] htmlTable_1.7       lattice_0.20-34     mgcv_1.8-15         Matrix_1.2-7.1      parallel_3.3.2      SparseM_1.72 
> [19] relimp_1.0-5        e1071_1.6-7         gridExtra_2.2.1     Rcmdr_2.3-1         stringr_1.1.0       cluster_2.0.5 
> [25] knitr_1.14          MatrixModels_0.4-1  grid_3.3.2          nnet_7.3-12         data.table_1.9.6    tcltk_3.3.2 
> [31] readxl_0.1.1        survival_2.40-1     foreign_0.8-67      latticeExtra_0.6-28 minqa_1.2.4         Formula_1.2-1 
> [37] ggplot2_2.1.0       magrittr_1.5        htmltools_0.3.5     Hmisc_4.0-0         scales_0.4.0        MASS_7.3-45 
> [43] abind_1.4-5         pbkrtest_0.4-6      colorspace_1.2-7    quantreg_5.29       stringi_1.1.2       acepack_1.4.1 
> [49] munsell_0.4.3       chron_2.3-47        zoo_1.7-13 
> 
> 
>> library(Rcmdr)
> Error : .onAttach a ?chou? dans attachNamespace() pour 'Rcmdr', d?tails :
> appel : structure(.External(.C_dotTclObjv, objv), class = "tclObj")
> erreur : [tcl] invalid command name "image".
> 
> Erreur : le chargement du package ou de l'espace de noms a ?chou? pour 'Rcmdr'
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From naresh_gurbuxani at hotmail.com  Sat Nov  5 23:59:19 2016
From: naresh_gurbuxani at hotmail.com (Naresh Gurbuxani)
Date: Sat, 5 Nov 2016 22:59:19 +0000
Subject: [R] lattice graph with free scales and reversed axis values
Message-ID: <CY1PR07MB258847C14A60369E4039037EFAA50@CY1PR07MB2588.namprd07.prod.outlook.com>

I want to draw a lattice graph where different panels have different ranges AND the order of values is reversed.  I am struggling to achieve both at the same time.  Can you help?

Thanks,
Naresh

# Create dummy data for illustration
my.df <- data.frame(x = runif(100, min = -10, max = -5), name = "A")
my.df <- rbind(my.df, data.frame(x = rnorm(100), name = "B"))
my.df <- within(my.df, {y <- x + 0.2 * rnorm(200)})

# This works.  Both x and y axes show values in reverse order. 
xyplot(y ~ x, groups = name, data = my.df, xlim = rev(range(my.df$x)), ylim = rev(range(my.df$y)), type = c("p", "g"))

# This works.  Both x and y axes show values in reverse order. 
xyplot(y ~ x | name, data = my.df, xlim = rev(range(my.df$x)), ylim = rev(range(my.df$y)), type = c("p", "g"))

# This does not work as intended.  x and y values are in reverse order.  But both panels have the same and x and y ranges.  scales argument does not seem to have any effect.   
xyplot(y ~ x | name, data = my.df, xlim = rev(range(my.df$x)), ylim = rev(range(my.df$y)), scales = list(x = "free", y = "free"), type = c("p", "g"))

# This does not work at all.  panel.xyplot does not see to take xlim or ylim arguments.  
xyplot(y ~ x | name, data = my.df, scales = list(x = "free", y = "free"), panel = function(x,y,subscripts, ...) {
	panel.xyplot(x,y, ylim = rev(range(y[subscripts]), xlim = rev(range(x[subscripts]))))
})


From jdnewmil at dcn.davis.ca.us  Sun Nov  6 00:07:38 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 05 Nov 2016 16:07:38 -0700
Subject: [R] Problem with downloading library(Rcmdr)
In-Reply-To: <753312018.1959901.1478385227937@mail.yahoo.com>
References: <496599242.1873911.1478370423476.ref@mail.yahoo.com>
	<496599242.1873911.1478370423476@mail.yahoo.com>
	<CF09170F-C599-4EDF-9C79-FE94B5490590@comcast.net>
	<753312018.1959901.1478385227937@mail.yahoo.com>
Message-ID: <B0DDF401-3287-41D6-8428-BCECB61B465A@dcn.davis.ca.us>

Google is your friend... try searching for "CRAN rlme".

The bad thing about contributed packages is that sometimes the maintainers stop maintaining their packages. The good thing is that the source is still there so you can go fix it if you really need it. 
-- 
Sent from my phone. Please excuse my brevity.

On November 5, 2016 3:33:47 PM PDT, varin sacha via R-help <r-help at r-project.org> wrote:
>Many thanks David,
>
>My Rcmdr problem is solved but now I have another one with rlme package
>!
>
>I want to use the hbrwts_gr function from the rlme package.
>I have tried to download the rlme package, but rlme is not available
>(for R version 3.3.2) as you can see here below the error message.
>
>How can I solve my problem ?
>
>
>> install.packages("rlme")
>Installing package into ?/Users/Caro/Library/R/3.3/library?
>(as ?lib? is unspecified)
>Warning in install.packages :
>package ?rlme? is not available (for R version 3.3.2)
>
>
>
>
>
>
>________________________________
>De : David Winsemius <dwinsemius at comcast.net>
>
>Cc : R-help Mailing List <r-help at r-project.org>
>Envoy? le : Samedi 5 novembre 2016 20h42
>Objet : Re: [R] Problem with downloading library(Rcmdr)
>
>
>
>> On Nov 5, 2016, at 11:27 AM, varin sacha via R-help
><r-help at r-project.org> wrote:
>> 
>> Dear R-experts,
>> 
>> Here below you will find my R sessionInfo. I have a problem with the
>library(Rcmdr). The error message is here below. How can I solve that
>problem ?
>
>I'm not an Rcmdr user and my Mac is too old to support Sierra so unable
>to attempt replication. All I am able offer is a search on what appear
>to be similar issues in the past. You do need to indicate your XQuartz
>version. I see there have been three recent releases suggesting to me
>that there might have been teething difficulties with the birth of the
>neanate Sierra.
>
>    ? XQuartz 2.7.11 - 2016-10-29
>
>    ? XQuartz 2.7.10 - 2016-10-22
>
>    ? XQuartz 2.7.9 - 2016-05-05
>
>You might also mention in any follow-up whether this is being done
>inside MacGUI or at an R console launched from Terminal.app.
>
>Make sure you have reviewed:
>http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/installation-notes.html
>
>
>Over the years there have been many questions regarding the three-way
>marriage of R, MacOS, and Rcmdr that relate to Tk/Tcl functions. Here's
>a relatively recent one that mentions the image function:
>
>http://markmail.org/search/?q=list%3Aorg.r-project.r-sig-mac+tcl+Rcmdr+image#query:list%3Aorg.r-project.r-sig-mac%20tcl%20Rcmdr%20image+page:1+mid:y5mgjtgtw65eqzfu+state:results
>
>
>So I think I would make sure you have a version of R that has Tcl and
>have the most uptodate X11 version (XQuartz) and once you do that and
>then try to reload R, load Rcmdr _before_ RcmdrMisc and before all
>those other packages. If you still get an error, then repost with
>version details of _all_ the components mentioned to the  R-SIG-Mac
>mailing list and copy John Fox. (Might not be necessary to give him a
>copy since I think he monitors R-SIG-Mac for Rcmdr issues.)
>
>
>-- 
>David.
>
>> 
>> 
>> Many thanks.
>> 
>> 
>> 
>> sessionInfo()
>> R version 3.3.2 (2016-10-31)
>> Platform: x86_64-apple-darwin13.4.0 (64-bit)
>> Running under: macOS Sierra 10.12
>> 
>> locale:
>> [1] fr_CH.UTF-8/fr_CH.UTF-8/fr_CH.UTF-8/C/fr_CH.UTF-8/fr_CH.UTF-8
>> 
>> attached base packages:
>> [1] splines   stats     graphics  grDevices utils     datasets 
>methods   base 
>> 
>> other attached packages:
>> [1] RcmdrMisc_1.0-5 sandwich_2.3-4  car_2.1-3 
>> 
>> loaded via a namespace (and not attached):
>> [1] Rcpp_0.12.7         tcltk2_1.2-11       nloptr_1.0.4       
>RColorBrewer_1.1-2  plyr_1.8.4          class_7.3-14 
>> [7] tools_3.3.2         digest_0.6.10       rpart_4.1-10       
>lme4_1.1-12         gtable_0.2.0        nlme_3.1-128 
>> [13] htmlTable_1.7       lattice_0.20-34     mgcv_1.8-15        
>Matrix_1.2-7.1      parallel_3.3.2      SparseM_1.72 
>> [19] relimp_1.0-5        e1071_1.6-7         gridExtra_2.2.1    
>Rcmdr_2.3-1         stringr_1.1.0       cluster_2.0.5 
>> [25] knitr_1.14          MatrixModels_0.4-1  grid_3.3.2         
>nnet_7.3-12         data.table_1.9.6    tcltk_3.3.2 
>> [31] readxl_0.1.1        survival_2.40-1     foreign_0.8-67     
>latticeExtra_0.6-28 minqa_1.2.4         Formula_1.2-1 
>> [37] ggplot2_2.1.0       magrittr_1.5        htmltools_0.3.5    
>Hmisc_4.0-0         scales_0.4.0        MASS_7.3-45 
>> [43] abind_1.4-5         pbkrtest_0.4-6      colorspace_1.2-7   
>quantreg_5.29       stringi_1.1.2       acepack_1.4.1 
>> [49] munsell_0.4.3       chron_2.3-47        zoo_1.7-13 
>> 
>> 
>>> library(Rcmdr)
>> Error : .onAttach a ?chou? dans attachNamespace() pour 'Rcmdr',
>d?tails :
>> appel : structure(.External(.C_dotTclObjv, objv), class = "tclObj")
>> erreur : [tcl] invalid command name "image".
>> 
>> Erreur : le chargement du package ou de l'espace de noms a ?chou?
>pour 'Rcmdr'
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>David Winsemius
>Alameda, CA, USA
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Sun Nov  6 00:12:24 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 5 Nov 2016 16:12:24 -0700
Subject: [R] Problem with downloading library(Rcmdr)
In-Reply-To: <753312018.1959901.1478385227937@mail.yahoo.com>
References: <496599242.1873911.1478370423476.ref@mail.yahoo.com>
	<496599242.1873911.1478370423476@mail.yahoo.com>
	<CF09170F-C599-4EDF-9C79-FE94B5490590@comcast.net>
	<753312018.1959901.1478385227937@mail.yahoo.com>
Message-ID: <76C1D680-6A74-437A-8746-5FFB9F4EB2FE@comcast.net>


> On Nov 5, 2016, at 3:33 PM, varin sacha <varinsacha at yahoo.fr> wrote:
> 
> Many thanks David,
> 
> My Rcmdr problem is solved but now I have another one with rlme package !
> 
> I want to use the hbrwts_gr function from the rlme package.
> I have tried to download the rlme package, but rlme is not available (for R version 3.3.2) as you can see here below the error message.
> 
> How can I solve my problem ?

https://github.com/cran/rlme

-- 
David.
> 
> 
>> install.packages("rlme")
> Installing package into ?/Users/Caro/Library/R/3.3/library?
> (as ?lib? is unspecified)
> Warning in install.packages :
> package ?rlme? is not available (for R version 3.3.2)
> 
> 
> 
> 
> 
> 
> ________________________________
> De : David Winsemius <dwinsemius at comcast.net>
> ? : varin sacha <varinsacha at yahoo.fr> 
> Cc : R-help Mailing List <r-help at r-project.org>
> Envoy? le : Samedi 5 novembre 2016 20h42
> Objet : Re: [R] Problem with downloading library(Rcmdr)
> 
> 
> 
>> On Nov 5, 2016, at 11:27 AM, varin sacha via R-help <r-help at r-project.org> wrote:
>> 
>> Dear R-experts,
>> 
>> Here below you will find my R sessionInfo. I have a problem with the library(Rcmdr). The error message is here below. How can I solve that problem ?
> 
> I'm not an Rcmdr user and my Mac is too old to support Sierra so unable to attempt replication. All I am able offer is a search on what appear to be similar issues in the past. You do need to indicate your XQuartz version. I see there have been three recent releases suggesting to me that there might have been teething difficulties with the birth of the neanate Sierra.
> 
>    ? XQuartz 2.7.11 - 2016-10-29
> 
>    ? XQuartz 2.7.10 - 2016-10-22
> 
>    ? XQuartz 2.7.9 - 2016-05-05
> 
> You might also mention in any follow-up whether this is being done inside MacGUI or at an R console launched from Terminal.app.
> 
> Make sure you have reviewed:
> http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/installation-notes.html
> 
> 
> Over the years there have been many questions regarding the three-way marriage of R, MacOS, and Rcmdr that relate to Tk/Tcl functions. Here's a relatively recent one that mentions the image function:
> 
> http://markmail.org/search/?q=list%3Aorg.r-project.r-sig-mac+tcl+Rcmdr+image#query:list%3Aorg.r-project.r-sig-mac%20tcl%20Rcmdr%20image+page:1+mid:y5mgjtgtw65eqzfu+state:results
> 
> 
> So I think I would make sure you have a version of R that has Tcl and have the most uptodate X11 version (XQuartz) and once you do that and then try to reload R, load Rcmdr _before_ RcmdrMisc and before all those other packages. If you still get an error, then repost with version details of _all_ the components mentioned to the  R-SIG-Mac mailing list and copy John Fox. (Might not be necessary to give him a copy since I think he monitors R-SIG-Mac for Rcmdr issues.)
> 
> 
> -- 
> David.
> 
>> 
>> 
>> Many thanks.
>> 
>> 
>> 
>> sessionInfo()
>> R version 3.3.2 (2016-10-31)
>> Platform: x86_64-apple-darwin13.4.0 (64-bit)
>> Running under: macOS Sierra 10.12
>> 
>> locale:
>> [1] fr_CH.UTF-8/fr_CH.UTF-8/fr_CH.UTF-8/C/fr_CH.UTF-8/fr_CH.UTF-8
>> 
>> attached base packages:
>> [1] splines   stats     graphics  grDevices utils     datasets  methods   base 
>> 
>> other attached packages:
>> [1] RcmdrMisc_1.0-5 sandwich_2.3-4  car_2.1-3 
>> 
>> loaded via a namespace (and not attached):
>> [1] Rcpp_0.12.7         tcltk2_1.2-11       nloptr_1.0.4        RColorBrewer_1.1-2  plyr_1.8.4          class_7.3-14 
>> [7] tools_3.3.2         digest_0.6.10       rpart_4.1-10        lme4_1.1-12         gtable_0.2.0        nlme_3.1-128 
>> [13] htmlTable_1.7       lattice_0.20-34     mgcv_1.8-15         Matrix_1.2-7.1      parallel_3.3.2      SparseM_1.72 
>> [19] relimp_1.0-5        e1071_1.6-7         gridExtra_2.2.1     Rcmdr_2.3-1         stringr_1.1.0       cluster_2.0.5 
>> [25] knitr_1.14          MatrixModels_0.4-1  grid_3.3.2          nnet_7.3-12         data.table_1.9.6    tcltk_3.3.2 
>> [31] readxl_0.1.1        survival_2.40-1     foreign_0.8-67      latticeExtra_0.6-28 minqa_1.2.4         Formula_1.2-1 
>> [37] ggplot2_2.1.0       magrittr_1.5        htmltools_0.3.5     Hmisc_4.0-0         scales_0.4.0        MASS_7.3-45 
>> [43] abind_1.4-5         pbkrtest_0.4-6      colorspace_1.2-7    quantreg_5.29       stringi_1.1.2       acepack_1.4.1 
>> [49] munsell_0.4.3       chron_2.3-47        zoo_1.7-13 
>> 
>> 
>>> library(Rcmdr)
>> Error : .onAttach a ?chou? dans attachNamespace() pour 'Rcmdr', d?tails :
>> appel : structure(.External(.C_dotTclObjv, objv), class = "tclObj")
>> erreur : [tcl] invalid command name "image".
>> 
>> Erreur : le chargement du package ou de l'espace de noms a ?chou? pour 'Rcmdr'
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA

David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Sun Nov  6 00:39:12 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 5 Nov 2016 16:39:12 -0700
Subject: [R] lattice graph with free scales and reversed axis values
In-Reply-To: <CY1PR07MB258847C14A60369E4039037EFAA50@CY1PR07MB2588.namprd07.prod.outlook.com>
References: <CY1PR07MB258847C14A60369E4039037EFAA50@CY1PR07MB2588.namprd07.prod.outlook.com>
Message-ID: <89F16EAB-20FC-4DB0-BCA5-78638AAC5050@comcast.net>


> On Nov 5, 2016, at 3:59 PM, Naresh Gurbuxani <naresh_gurbuxani at hotmail.com> wrote:
> 
> I want to draw a lattice graph where different panels have different ranges AND the order of values is reversed.  I am struggling to achieve both at the same time.  Can you help?
> 
> Thanks,
> Naresh
> 
> # Create dummy data for illustration
> my.df <- data.frame(x = runif(100, min = -10, max = -5), name = "A")
> my.df <- rbind(my.df, data.frame(x = rnorm(100), name = "B"))
> my.df <- within(my.df, {y <- x + 0.2 * rnorm(200)})
> 
> # This works.  Both x and y axes show values in reverse order. 
> xyplot(y ~ x, groups = name, data = my.df, xlim = rev(range(my.df$x)), ylim = rev(range(my.df$y)), type = c("p", "g"))
> 
> # This works.  Both x and y axes show values in reverse order. 
> xyplot(y ~ x | name, data = my.df, xlim = rev(range(my.df$x)), ylim = rev(range(my.df$y)), type = c("p", "g"))
> 
> # This does not work as intended.  x and y values are in reverse order.  But both panels have the same and x and y ranges.  scales argument does not seem to have any effect.   
> xyplot(y ~ x | name, data = my.df, xlim = rev(range(my.df$x)), ylim = rev(range(my.df$y)), scales = list(x = "free", y = "free"), type = c("p", "g"))
> 
> # This does not work at all.  panel.xyplot does not see to take xlim or ylim arguments.  

But prepanel does ....

xyplot(y ~ x | name, data = my.df, scales = list(x = "free", y = "free"), panel = function(x,y,subscripts, ...) {
> 	panel.xyplot(x,y, ylim = rev(range(y[subscripts]), xlim = rev(range(x[subscripts]))))
> })

I used this posting to r-help from 2006 by xyplot's author, Sarkar, to solve the problem:

https://stat.ethz.ch/pipermail/r-help/2006-March/101248.html

 xyplot(y ~ x | name, data = my.df, 
    scales =    list(relation="free"),  
    prepanel= function(x,y, ...) { list(xlim=  rev(range(my.df$x)) , 
                                        ylim = rev(range(my.df$y)) )}, 
    type =     c("p", "g"))

-- 
David Winsemius
Alameda, CA, USA


From philipt900 at iinet.net.au  Sun Nov  6 00:57:58 2016
From: philipt900 at iinet.net.au (P Tennant)
Date: Sun, 06 Nov 2016 10:57:58 +1100
Subject: [R] lattice graph with free scales and reversed axis values
In-Reply-To: <CY1PR07MB258847C14A60369E4039037EFAA50@CY1PR07MB2588.namprd07.prod.outlook.com>
References: <CY1PR07MB258847C14A60369E4039037EFAA50@CY1PR07MB2588.namprd07.prod.outlook.com>
Message-ID: <581E7206.6090805@iinet.net.au>

Hi Naresh,

You could calculate the ranges explicitly and then supply to scales():

holdRange <- vector('list', length(unique(my.df$name)))
for(i in 1:length(holdRange)){
     holdRange[[i]] <- 
rev(range(my.df$x[my.df$name==unique(my.df$name)[i]]))
     }

holdRange

# [[1]]
# [1] -5.052890 -9.967671

# [[2]]
# [1]  2.648870 -2.629866

xyplot(y ~ x | name, data = my.df,
     type = c("p", "g"),
     scales = list(relation="free", limits=holdRange))


Philip


On 6/11/2016 9:59 AM, Naresh Gurbuxani wrote:
> I want to draw a lattice graph where different panels have different ranges AND the order of values is reversed.  I am struggling to achieve both at the same time.  Can you help?
>
> Thanks,
> Naresh
>
> # Create dummy data for illustration
> my.df<- data.frame(x = runif(100, min = -10, max = -5), name = "A")
> my.df<- rbind(my.df, data.frame(x = rnorm(100), name = "B"))
> my.df<- within(my.df, {y<- x + 0.2 * rnorm(200)})
>
> # This works.  Both x and y axes show values in reverse order.
> xyplot(y ~ x, groups = name, data = my.df, xlim = rev(range(my.df$x)), ylim = rev(range(my.df$y)), type = c("p", "g"))
>
> # This works.  Both x and y axes show values in reverse order.
> xyplot(y ~ x | name, data = my.df, xlim = rev(range(my.df$x)), ylim = rev(range(my.df$y)), type = c("p", "g"))
>
> # This does not work as intended.  x and y values are in reverse order.  But both panels have the same and x and y ranges.  scales argument does not seem to have any effect.
> xyplot(y ~ x | name, data = my.df, xlim = rev(range(my.df$x)), ylim = rev(range(my.df$y)), scales = list(x = "free", y = "free"), type = c("p", "g"))
>
> # This does not work at all.  panel.xyplot does not see to take xlim or ylim arguments.
> xyplot(y ~ x | name, data = my.df, scales = list(x = "free", y = "free"), panel = function(x,y,subscripts, ...) {
> 	panel.xyplot(x,y, ylim = rev(range(y[subscripts]), xlim = rev(range(x[subscripts]))))
> })
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jfox at mcmaster.ca  Sun Nov  6 01:13:03 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Sun, 6 Nov 2016 00:13:03 +0000
Subject: [R] Problem with downloading library(Rcmdr)
In-Reply-To: <CF09170F-C599-4EDF-9C79-FE94B5490590@comcast.net>
References: <496599242.1873911.1478370423476.ref@mail.yahoo.com>
	<496599242.1873911.1478370423476@mail.yahoo.com>
	<CF09170F-C599-4EDF-9C79-FE94B5490590@comcast.net>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC83659BE25@FHSDB2D11-2.csu.mcmaster.ca>

Dear David,

Sorry to chime in late -- I didn't read the messages in this thread until just now.

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of David
> Winsemius
> Sent: November 5, 2016 3:43 PM
> To: varin sacha <varinsacha at yahoo.fr>
> Cc: R-help Mailing List <r-help at r-project.org>
> Subject: Re: [R] Problem with downloading library(Rcmdr)
> 
> 
> > On Nov 5, 2016, at 11:27 AM, varin sacha via R-help <r-help at r-project.org>
> wrote:
> >
> > Dear R-experts,
> >
> > Here below you will find my R sessionInfo. I have a problem with the
> library(Rcmdr). The error message is here below. How can I solve that problem
> ?
> 
> I'm not an Rcmdr user and my Mac is too old to support Sierra so unable to
> attempt replication. All I am able offer is a search on what appear to be similar
> issues in the past. You do need to indicate your XQuartz version. I see there
> have been three recent releases suggesting to me that there might have been
> teething difficulties with the birth of the neanate Sierra.
> 
> 	? XQuartz 2.7.11 - 2016-10-29
> 
> 	? XQuartz 2.7.10 - 2016-10-22
> 
> 	? XQuartz 2.7.9 - 2016-05-05
> 
> You might also mention in any follow-up whether this is being done inside
> MacGUI or at an R console launched from Terminal.app.
> 
> Make sure you have reviewed:
> http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/installation-notes.html
> 
> 
> Over the years there have been many questions regarding the three-way
> marriage of R, MacOS, and Rcmdr that relate to Tk/Tcl functions. Here's a
> relatively recent one that mentions the image function:
> 
> http://markmail.org/search/?q=list%3Aorg.r-project.r-sig-
> mac+tcl+Rcmdr+image#query:list%3Aorg.r-project.r-sig-
> mac%20tcl%20Rcmdr%20image+page:1+mid:y5mgjtgtw65eqzfu+state:results
> 

The current Rcmdr Mac installation notes, to which you also pointed, reflect this discussion.

> 
> So I think I would make sure you have a version of R that has Tcl and have the
> most uptodate X11 version (XQuartz) and once you do that and then try to
> reload R, load Rcmdr _before_ RcmdrMisc and before all those other packages.
> If you still get an error, then repost with version details of _all_ the
> components mentioned to the  R-SIG-Mac mailing list and copy John Fox.
> (Might not be necessary to give him a copy since I think he monitors R-SIG-Mac
> for Rcmdr issues.)

If one loads the Rcmdr package at the beginning of a session via library(Rcmdr), then the dependencies that it needs to start up, such as RcmdrMisc, will be loaded automatically. Other packages may be loaded during an Rcmdr session as needed.

By far the most common problem with getting the Rcmdr to work on Macs is failure to install XQuartz or failure to reinstall it after updating the OS.

I see from a subsequent message that varin sasha's problem has been solved.

Thanks for trying to help.

Best,
 John

> 
> 
> --
> David.
> >
> >
> > Many thanks.
> >
> >
> >
> > sessionInfo()
> > R version 3.3.2 (2016-10-31)
> > Platform: x86_64-apple-darwin13.4.0 (64-bit) Running under: macOS
> > Sierra 10.12
> >
> > locale:
> > [1] fr_CH.UTF-8/fr_CH.UTF-8/fr_CH.UTF-8/C/fr_CH.UTF-8/fr_CH.UTF-8
> >
> > attached base packages:
> > [1] splines   stats     graphics  grDevices utils     datasets  methods   base
> >
> > other attached packages:
> > [1] RcmdrMisc_1.0-5 sandwich_2.3-4  car_2.1-3
> >
> > loaded via a namespace (and not attached):
> > [1] Rcpp_0.12.7         tcltk2_1.2-11       nloptr_1.0.4        RColorBrewer_1.1-2
> plyr_1.8.4          class_7.3-14
> > [7] tools_3.3.2         digest_0.6.10       rpart_4.1-10        lme4_1.1-12
> gtable_0.2.0        nlme_3.1-128
> > [13] htmlTable_1.7       lattice_0.20-34     mgcv_1.8-15         Matrix_1.2-7.1
> parallel_3.3.2      SparseM_1.72
> > [19] relimp_1.0-5        e1071_1.6-7         gridExtra_2.2.1     Rcmdr_2.3-1
> stringr_1.1.0       cluster_2.0.5
> > [25] knitr_1.14          MatrixModels_0.4-1  grid_3.3.2          nnet_7.3-12
> data.table_1.9.6    tcltk_3.3.2
> > [31] readxl_0.1.1        survival_2.40-1     foreign_0.8-67      latticeExtra_0.6-28
> minqa_1.2.4         Formula_1.2-1
> > [37] ggplot2_2.1.0       magrittr_1.5        htmltools_0.3.5     Hmisc_4.0-0
> scales_0.4.0        MASS_7.3-45
> > [43] abind_1.4-5         pbkrtest_0.4-6      colorspace_1.2-7    quantreg_5.29
> stringi_1.1.2       acepack_1.4.1
> > [49] munsell_0.4.3       chron_2.3-47        zoo_1.7-13
> >
> >
> >> library(Rcmdr)
> > Error : .onAttach a ?chou? dans attachNamespace() pour 'Rcmdr', d?tails :
> > appel : structure(.External(.C_dotTclObjv, objv), class = "tclObj")
> > erreur : [tcl] invalid command name "image".
> >
> > Erreur : le chargement du package ou de l'espace de noms a ?chou? pour
> 'Rcmdr'
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From dwinsemius at comcast.net  Sun Nov  6 01:19:37 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 5 Nov 2016 17:19:37 -0700
Subject: [R] Problem with downloading library(Rcmdr)
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC83659BE25@FHSDB2D11-2.csu.mcmaster.ca>
References: <496599242.1873911.1478370423476.ref@mail.yahoo.com>
	<496599242.1873911.1478370423476@mail.yahoo.com>
	<CF09170F-C599-4EDF-9C79-FE94B5490590@comcast.net>
	<ACD1644AA6C67E4FBD0C350625508EC83659BE25@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <3B429984-7335-4690-AC05-F8FB1B4F8D79@comcast.net>


> On Nov 5, 2016, at 5:13 PM, Fox, John <jfox at mcmaster.ca> wrote:
> 
> Dear David,
> 
> Sorry to chime in late -- I didn't read the messages in this thread until just now.

I see no reason for an apology. 6 hour service on a weekend is damned good software support in my opinion.

I'm glad to see my inferences confirmed about the XQuartz versioning as prime suspect. 


Best;

David.
> 
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of David
>> Winsemius
>> Sent: November 5, 2016 3:43 PM
>> To: varin sacha <varinsacha at yahoo.fr>
>> Cc: R-help Mailing List <r-help at r-project.org>
>> Subject: Re: [R] Problem with downloading library(Rcmdr)
>> 
>> 
>>> On Nov 5, 2016, at 11:27 AM, varin sacha via R-help <r-help at r-project.org>
>> wrote:
>>> 
>>> Dear R-experts,
>>> 
>>> Here below you will find my R sessionInfo. I have a problem with the
>> library(Rcmdr). The error message is here below. How can I solve that problem
>> ?
>> 
>> I'm not an Rcmdr user and my Mac is too old to support Sierra so unable to
>> attempt replication. All I am able offer is a search on what appear to be similar
>> issues in the past. You do need to indicate your XQuartz version. I see there
>> have been three recent releases suggesting to me that there might have been
>> teething difficulties with the birth of the neanate Sierra.
>> 
>> 	? XQuartz 2.7.11 - 2016-10-29
>> 
>> 	? XQuartz 2.7.10 - 2016-10-22
>> 
>> 	? XQuartz 2.7.9 - 2016-05-05
>> 
>> You might also mention in any follow-up whether this is being done inside
>> MacGUI or at an R console launched from Terminal.app.
>> 
>> Make sure you have reviewed:
>> http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/installation-notes.html
>> 
>> 
>> Over the years there have been many questions regarding the three-way
>> marriage of R, MacOS, and Rcmdr that relate to Tk/Tcl functions. Here's a
>> relatively recent one that mentions the image function:
>> 
>> http://markmail.org/search/?q=list%3Aorg.r-project.r-sig-
>> mac+tcl+Rcmdr+image#query:list%3Aorg.r-project.r-sig-
>> mac%20tcl%20Rcmdr%20image+page:1+mid:y5mgjtgtw65eqzfu+state:results
>> 
> 
> The current Rcmdr Mac installation notes, to which you also pointed, reflect this discussion.
> 
>> 
>> So I think I would make sure you have a version of R that has Tcl and have the
>> most uptodate X11 version (XQuartz) and once you do that and then try to
>> reload R, load Rcmdr _before_ RcmdrMisc and before all those other packages.
>> If you still get an error, then repost with version details of _all_ the
>> components mentioned to the  R-SIG-Mac mailing list and copy John Fox.
>> (Might not be necessary to give him a copy since I think he monitors R-SIG-Mac
>> for Rcmdr issues.)
> 
> If one loads the Rcmdr package at the beginning of a session via library(Rcmdr), then the dependencies that it needs to start up, such as RcmdrMisc, will be loaded automatically. Other packages may be loaded during an Rcmdr session as needed.
> 
> By far the most common problem with getting the Rcmdr to work on Macs is failure to install XQuartz or failure to reinstall it after updating the OS.
> 
> I see from a subsequent message that varin sasha's problem has been solved.
> 
> Thanks for trying to help.
> 
> Best,
> John
> 
>> 
>> 
>> --
>> David.
>>> 
>>> 
>>> Many thanks.
>>> 
>>> 
>>> 
>>> sessionInfo()
>>> R version 3.3.2 (2016-10-31)
>>> Platform: x86_64-apple-darwin13.4.0 (64-bit) Running under: macOS
>>> Sierra 10.12
>>> 
>>> locale:
>>> [1] fr_CH.UTF-8/fr_CH.UTF-8/fr_CH.UTF-8/C/fr_CH.UTF-8/fr_CH.UTF-8
>>> 
>>> attached base packages:
>>> [1] splines   stats     graphics  grDevices utils     datasets  methods   base
>>> 
>>> other attached packages:
>>> [1] RcmdrMisc_1.0-5 sandwich_2.3-4  car_2.1-3
>>> 
>>> loaded via a namespace (and not attached):
>>> [1] Rcpp_0.12.7         tcltk2_1.2-11       nloptr_1.0.4        RColorBrewer_1.1-2
>> plyr_1.8.4          class_7.3-14
>>> [7] tools_3.3.2         digest_0.6.10       rpart_4.1-10        lme4_1.1-12
>> gtable_0.2.0        nlme_3.1-128
>>> [13] htmlTable_1.7       lattice_0.20-34     mgcv_1.8-15         Matrix_1.2-7.1
>> parallel_3.3.2      SparseM_1.72
>>> [19] relimp_1.0-5        e1071_1.6-7         gridExtra_2.2.1     Rcmdr_2.3-1
>> stringr_1.1.0       cluster_2.0.5
>>> [25] knitr_1.14          MatrixModels_0.4-1  grid_3.3.2          nnet_7.3-12
>> data.table_1.9.6    tcltk_3.3.2
>>> [31] readxl_0.1.1        survival_2.40-1     foreign_0.8-67      latticeExtra_0.6-28
>> minqa_1.2.4         Formula_1.2-1
>>> [37] ggplot2_2.1.0       magrittr_1.5        htmltools_0.3.5     Hmisc_4.0-0
>> scales_0.4.0        MASS_7.3-45
>>> [43] abind_1.4-5         pbkrtest_0.4-6      colorspace_1.2-7    quantreg_5.29
>> stringi_1.1.2       acepack_1.4.1
>>> [49] munsell_0.4.3       chron_2.3-47        zoo_1.7-13
>>> 
>>> 
>>>> library(Rcmdr)
>>> Error : .onAttach a ?chou? dans attachNamespace() pour 'Rcmdr', d?tails :
>>> appel : structure(.External(.C_dotTclObjv, objv), class = "tclObj")
>>> erreur : [tcl] invalid command name "image".
>>> 
>>> Erreur : le chargement du package ou de l'espace de noms a ?chou? pour
>> 'Rcmdr'
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> David Winsemius
>> Alameda, CA, USA
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From james.hirschorn at hotmail.com  Sun Nov  6 02:10:51 2016
From: james.hirschorn at hotmail.com (James Hirschorn)
Date: Sun, 6 Nov 2016 01:10:51 +0000
Subject: [R] Is this foreach behaviour correct?
Message-ID: <CY1PR11MB0444BE97C0B5173ADB45A892E9A40@CY1PR11MB0444.namprd11.prod.outlook.com>

This seemed odd so I wanted to check:

 > x <- foreach(i=10000:10100, .combine='c') %do% { as.Date(i) }

yields a numeric vector for x:

 > class(x)
[1] "numeric"

Should it not be a vector of Date?


From naresh_gurbuxani at hotmail.com  Sun Nov  6 04:27:24 2016
From: naresh_gurbuxani at hotmail.com (Naresh Gurbuxani)
Date: Sun, 6 Nov 2016 03:27:24 +0000
Subject: [R] lattice graph with free scales and reversed axis values
In-Reply-To: <581E7206.6090805@iinet.net.au>
References: <CY1PR07MB258847C14A60369E4039037EFAA50@CY1PR07MB2588.namprd07.prod.outlook.com>,
	<581E7206.6090805@iinet.net.au>
Message-ID: <CY1PR07MB2588FC0C6B7449E7273AAC94FAA40@CY1PR07MB2588.namprd07.prod.outlook.com>

This worked very well for me.


Thanks,

Naresh


________________________________
From: P Tennant <philipt900 at iinet.net.au>
Sent: Saturday, November 5, 2016 7:57 PM
To: Naresh Gurbuxani
Cc: R-help at r-project.org
Subject: Re: [R] lattice graph with free scales and reversed axis values

Hi Naresh,

You could calculate the ranges explicitly and then supply to scales():

holdRange <- vector('list', length(unique(my.df$name)))
for(i in 1:length(holdRange)){
     holdRange[[i]] <-
rev(range(my.df$x[my.df$name==unique(my.df$name)[i]]))
     }

holdRange

# [[1]]
# [1] -5.052890 -9.967671

# [[2]]
# [1]  2.648870 -2.629866

xyplot(y ~ x | name, data = my.df,
     type = c("p", "g"),
     scales = list(relation="free", limits=holdRange))


Philip


On 6/11/2016 9:59 AM, Naresh Gurbuxani wrote:
> I want to draw a lattice graph where different panels have different ranges AND the order of values is reversed.  I am struggling to achieve both at the same time.  Can you help?
>
> Thanks,
> Naresh
>
> # Create dummy data for illustration
> my.df<- data.frame(x = runif(100, min = -10, max = -5), name = "A")
> my.df<- rbind(my.df, data.frame(x = rnorm(100), name = "B"))
> my.df<- within(my.df, {y<- x + 0.2 * rnorm(200)})
>
> # This works.  Both x and y axes show values in reverse order.
> xyplot(y ~ x, groups = name, data = my.df, xlim = rev(range(my.df$x)), ylim = rev(range(my.df$y)), type = c("p", "g"))
>
> # This works.  Both x and y axes show values in reverse order.
> xyplot(y ~ x | name, data = my.df, xlim = rev(range(my.df$x)), ylim = rev(range(my.df$y)), type = c("p", "g"))
>
> # This does not work as intended.  x and y values are in reverse order.  But both panels have the same and x and y ranges.  scales argument does not seem to have any effect.
> xyplot(y ~ x | name, data = my.df, xlim = rev(range(my.df$x)), ylim = rev(range(my.df$y)), scales = list(x = "free", y = "free"), type = c("p", "g"))
>
> # This does not work at all.  panel.xyplot does not see to take xlim or ylim arguments.
> xyplot(y ~ x | name, data = my.df, scales = list(x = "free", y = "free"), panel = function(x,y,subscripts, ...) {
>        panel.xyplot(x,y, ylim = rev(range(y[subscripts]), xlim = rev(range(x[subscripts]))))
> })
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From varinsacha at yahoo.fr  Sun Nov  6 14:17:12 2016
From: varinsacha at yahoo.fr (varin sacha)
Date: Sun, 6 Nov 2016 13:17:12 +0000 (UTC)
Subject: [R] Problem with downloading library(Rcmdr)
In-Reply-To: <B0DDF401-3287-41D6-8428-BCECB61B465A@dcn.davis.ca.us>
References: <496599242.1873911.1478370423476.ref@mail.yahoo.com>
	<496599242.1873911.1478370423476@mail.yahoo.com>
	<CF09170F-C599-4EDF-9C79-FE94B5490590@comcast.net>
	<753312018.1959901.1478385227937@mail.yahoo.com>
	<B0DDF401-3287-41D6-8428-BCECB61B465A@dcn.davis.ca.us>
Message-ID: <540398699.2330409.1478438232697@mail.yahoo.com>

Hi Jeff,

Many thanks for your reply.I know that R packages can be downloaded as "tar.gz" file for Mac OS X from CRAN sources. I have found on Internet that the command is the following :

R CMD INSTALL -1 rlme_0.4.tar.gz

When I run that command on my R console, I get an error as you can read here below.

What am I doing wrong ?


> R CMD INSTALL -1 rlme_0.4.tar.gz
Erreur : unexpected symbol in "R CMD"


----- Mail original -----
De : Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
? : varin sacha <varinsacha at yahoo.fr>; varin sacha via R-help <r-help at r-project.org>; David Winsemius <dwinsemius at comcast.net>
Cc : R-help Mailing List <r-help at r-project.org>
Envoy? le : Dimanche 6 novembre 2016 0h07
Objet : Re: [R] Problem with downloading library(Rcmdr)

Google is your friend... try searching for "CRAN rlme".

The bad thing about contributed packages is that sometimes the maintainers stop maintaining their packages. The good thing is that the source is still there so you can go fix it if you really need it. 
-- 
Sent from my phone. Please excuse my brevity.


On November 5, 2016 3:33:47 PM PDT, varin sacha via R-help <r-help at r-project.org> wrote:
>Many thanks David,
>
>My Rcmdr problem is solved but now I have another one with rlme package
>!
>
>I want to use the hbrwts_gr function from the rlme package.
>I have tried to download the rlme package, but rlme is not available
>(for R version 3.3.2) as you can see here below the error message.
>
>How can I solve my problem ?
>
>
>> install.packages("rlme")
>Installing package into ?/Users/Caro/Library/R/3.3/library?
>(as ?lib? is unspecified)
>Warning in install.packages :
>package ?rlme? is not available (for R version 3.3.2)
>
>
>
>
>
>
>________________________________
>De : David Winsemius <dwinsemius at comcast.net>
>
>Cc : R-help Mailing List <r-help at r-project.org>
>Envoy? le : Samedi 5 novembre 2016 20h42
>Objet : Re: [R] Problem with downloading library(Rcmdr)
>
>
>
>> On Nov 5, 2016, at 11:27 AM, varin sacha via R-help
><r-help at r-project.org> wrote:
>> 
>> Dear R-experts,
>> 
>> Here below you will find my R sessionInfo. I have a problem with the
>library(Rcmdr). The error message is here below. How can I solve that
>problem ?
>
>I'm not an Rcmdr user and my Mac is too old to support Sierra so unable
>to attempt replication. All I am able offer is a search on what appear
>to be similar issues in the past. You do need to indicate your XQuartz
>version. I see there have been three recent releases suggesting to me
>that there might have been teething difficulties with the birth of the
>neanate Sierra.
>
>    ? XQuartz 2.7.11 - 2016-10-29
>
>    ? XQuartz 2.7.10 - 2016-10-22
>
>    ? XQuartz 2.7.9 - 2016-05-05
>
>You might also mention in any follow-up whether this is being done
>inside MacGUI or at an R console launched from Terminal.app.
>
>Make sure you have reviewed:
>http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/installation-notes.html
>
>
>Over the years there have been many questions regarding the three-way
>marriage of R, MacOS, and Rcmdr that relate to Tk/Tcl functions. Here's
>a relatively recent one that mentions the image function:
>
>http://markmail.org/search/?q=list%3Aorg.r-project.r-sig-mac+tcl+Rcmdr+image#query:list%3Aorg.r-project.r-sig-mac%20tcl%20Rcmdr%20image+page:1+mid:y5mgjtgtw65eqzfu+state:results
>
>
>So I think I would make sure you have a version of R that has Tcl and
>have the most uptodate X11 version (XQuartz) and once you do that and
>then try to reload R, load Rcmdr _before_ RcmdrMisc and before all
>those other packages. If you still get an error, then repost with
>version details of _all_ the components mentioned to the  R-SIG-Mac
>mailing list and copy John Fox. (Might not be necessary to give him a
>copy since I think he monitors R-SIG-Mac for Rcmdr issues.)
>
>
>-- 
>David.
>
>> 
>> 
>> Many thanks.
>> 
>> 
>> 
>> sessionInfo()
>> R version 3.3.2 (2016-10-31)
>> Platform: x86_64-apple-darwin13.4.0 (64-bit)
>> Running under: macOS Sierra 10.12
>> 
>> locale:
>> [1] fr_CH.UTF-8/fr_CH.UTF-8/fr_CH.UTF-8/C/fr_CH.UTF-8/fr_CH.UTF-8
>> 
>> attached base packages:
>> [1] splines   stats     graphics  grDevices utils     datasets 
>methods   base 
>> 
>> other attached packages:
>> [1] RcmdrMisc_1.0-5 sandwich_2.3-4  car_2.1-3 
>> 
>> loaded via a namespace (and not attached):
>> [1] Rcpp_0.12.7         tcltk2_1.2-11       nloptr_1.0.4      
>RColorBrewer_1.1-2  plyr_1.8.4          class_7.3-14 
>> [7] tools_3.3.2         digest_0.6.10       rpart_4.1-10      
>lme4_1.1-12         gtable_0.2.0        nlme_3.1-128 
>> [13] htmlTable_1.7       lattice_0.20-34     mgcv_1.8-15        
>Matrix_1.2-7.1      parallel_3.3.2      SparseM_1.72 
>> [19] relimp_1.0-5        e1071_1.6-7         gridExtra_2.2.1    
>Rcmdr_2.3-1         stringr_1.1.0       cluster_2.0.5 
>> [25] knitr_1.14          MatrixModels_0.4-1  grid_3.3.2        
>nnet_7.3-12         data.table_1.9.6    tcltk_3.3.2 
>> [31] readxl_0.1.1        survival_2.40-1     foreign_0.8-67    
>latticeExtra_0.6-28 minqa_1.2.4         Formula_1.2-1 
>> [37] ggplot2_2.1.0       magrittr_1.5        htmltools_0.3.5    
>Hmisc_4.0-0         scales_0.4.0        MASS_7.3-45 
>> [43] abind_1.4-5         pbkrtest_0.4-6      colorspace_1.2-7  
>quantreg_5.29       stringi_1.1.2       acepack_1.4.1 
>> [49] munsell_0.4.3       chron_2.3-47        zoo_1.7-13 
>> 
>> 
>>> library(Rcmdr)
>> Error : .onAttach a ?chou? dans attachNamespace() pour 'Rcmdr',
>d?tails :
>> appel : structure(.External(.C_dotTclObjv, objv), class = "tclObj")
>> erreur : [tcl] invalid command name "image".
>> 
>> Erreur : le chargement du package ou de l'espace de noms a ?chou?
>pour 'Rcmdr'
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>David Winsemius
>Alameda, CA, USA
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Sun Nov  6 15:10:45 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sun, 06 Nov 2016 06:10:45 -0800
Subject: [R] Problem with downloading library(Rcmdr)
In-Reply-To: <540398699.2330409.1478438232697@mail.yahoo.com>
References: <496599242.1873911.1478370423476.ref@mail.yahoo.com>
	<496599242.1873911.1478370423476@mail.yahoo.com>
	<CF09170F-C599-4EDF-9C79-FE94B5490590@comcast.net>
	<753312018.1959901.1478385227937@mail.yahoo.com>
	<B0DDF401-3287-41D6-8428-BCECB61B465A@dcn.davis.ca.us>
	<540398699.2330409.1478438232697@mail.yahoo.com>
Message-ID: <D3604677-C4C3-48CB-A932-1DCF20DF0376@dcn.davis.ca.us>

I am not aware of a "-1" option for R CMD INSTALL  [1]. Nor do I have a Macintosh.

Do be prepared to keep reading the documentation once you get past this point, as the whole reason you are doing this is that the original maintainer chose to not fix whatever problems existed in that package, and I have never used that package. Those problems might be minor, or they might be extensive.

[1] https://cran.r-project.org/doc/manuals/r-release/R-admin.html#Installing-packages
-- 
Sent from my phone. Please excuse my brevity.

On November 6, 2016 5:17:12 AM PST, varin sacha <varinsacha at yahoo.fr> wrote:
>Hi Jeff,
>
>Many thanks for your reply.I know that R packages can be downloaded as
>"tar.gz" file for Mac OS X from CRAN sources. I have found on Internet
>that the command is the following :
>
>R CMD INSTALL -1 rlme_0.4.tar.gz
>
>When I run that command on my R console, I get an error as you can read
>here below.
>
>What am I doing wrong ?
>
>
>> R CMD INSTALL -1 rlme_0.4.tar.gz
>Erreur : unexpected symbol in "R CMD"
>
>
>----- Mail original -----
>De : Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>? : varin sacha <varinsacha at yahoo.fr>; varin sacha via R-help
><r-help at r-project.org>; David Winsemius <dwinsemius at comcast.net>
>Cc : R-help Mailing List <r-help at r-project.org>
>Envoy? le : Dimanche 6 novembre 2016 0h07
>Objet : Re: [R] Problem with downloading library(Rcmdr)
>
>Google is your friend... try searching for "CRAN rlme".
>
>The bad thing about contributed packages is that sometimes the
>maintainers stop maintaining their packages. The good thing is that the
>source is still there so you can go fix it if you really need it.


From dwinsemius at comcast.net  Sun Nov  6 16:35:05 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 6 Nov 2016 07:35:05 -0800
Subject: [R] Problem with downloading library(Rcmdr)
In-Reply-To: <D3604677-C4C3-48CB-A932-1DCF20DF0376@dcn.davis.ca.us>
References: <496599242.1873911.1478370423476.ref@mail.yahoo.com>
	<496599242.1873911.1478370423476@mail.yahoo.com>
	<CF09170F-C599-4EDF-9C79-FE94B5490590@comcast.net>
	<753312018.1959901.1478385227937@mail.yahoo.com>
	<B0DDF401-3287-41D6-8428-BCECB61B465A@dcn.davis.ca.us>
	<540398699.2330409.1478438232697@mail.yahoo.com>
	<D3604677-C4C3-48CB-A932-1DCF20DF0376@dcn.davis.ca.us>
Message-ID: <3A6DBF96-F028-40C9-9E39-2F8AA16F0073@comcast.net>


> On Nov 6, 2016, at 6:10 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> I am not aware of a "-1" option for R CMD INSTALL  [1]. Nor do I have a Macintosh.

It compiles from the MacGUI console using an ordinary call (no extra environment variables need be set) to install.packages, and then loads without error on my Mac running R 3.3.1 patched. (I do have a full set of tools and a current version of Rcpp, reference to which which showed up in the compiler messages.)  If I had been using the Unix command line I would have offer a full path to the *.tar.gz file and not used that "minus one" option.

The Imports line in the DESCRIPTION file:

Imports: MASS, quantreg, nlme, mgcv, stringr, magic, robustbase, Rcpp(>= 0.11.1)

-- 
David.
> 
> Do be prepared to keep reading the documentation once you get past this point, as the whole reason you are doing this is that the original maintainer chose to not fix whatever problems existed in that package, and I have never used that package. Those problems might be minor, or they might be extensive.
> 
> [1] https://cran.r-project.org/doc/manuals/r-release/R-admin.html#Installing-packages
> -- 
> Sent from my phone. Please excuse my brevity.
> 
> On November 6, 2016 5:17:12 AM PST, varin sacha <varinsacha at yahoo.fr> wrote:
>> Hi Jeff,
>> 
>> Many thanks for your reply.I know that R packages can be downloaded as
>> "tar.gz" file for Mac OS X from CRAN sources. I have found on Internet
>> that the command is the following :
>> 
>> R CMD INSTALL -1 rlme_0.4.tar.gz
>> 
>> When I run that command on my R console, I get an error as you can read
>> here below.
>> 
>> What am I doing wrong ?
>> 
>> 
>>> R CMD INSTALL -1 rlme_0.4.tar.gz
>> Erreur : unexpected symbol in "R CMD"
>> 
>> 
>> ----- Mail original -----
>> De : Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>> ? : varin sacha <varinsacha at yahoo.fr>; varin sacha via R-help
>> <r-help at r-project.org>; David Winsemius <dwinsemius at comcast.net>
>> Cc : R-help Mailing List <r-help at r-project.org>
>> Envoy? le : Dimanche 6 novembre 2016 0h07
>> Objet : Re: [R] Problem with downloading library(Rcmdr)
>> 
>> Google is your friend... try searching for "CRAN rlme".
>> 
>> The bad thing about contributed packages is that sometimes the
>> maintainers stop maintaining their packages. The good thing is that the
>> source is still there so you can go fix it if you really need it. 
> 

David Winsemius
Alameda, CA, USA


From lucasmation at gmail.com  Sun Nov  6 14:36:46 2016
From: lucasmation at gmail.com (Lucas Ferreira Mation)
Date: Sun, 6 Nov 2016 11:36:46 -0200
Subject: [R] How to pre-process fwf or csv files to remove unexpected
 characters in R?
Message-ID: <CAA+7z9GuAh+SdABh0kzBeBOv+2M_CBg2pkM01PpejGudMSWJpQ@mail.gmail.com>

I have some large .txt files about ~100GB containing a dataset in fixed
width file. This contains some errors:
- character characters in column that are supposed to be numeric,
- invalid characters
- rows with too many characters, possibly due to invalid characters or some
missing end of line character (so two rows in the original data become one
row in the .txt file).

The errors are not very frequent, but stop me from importing with readr
::read_fwf()


Is there some package, or workflow, in R to pre-process the files,
separating the valid from the not-valid rows into different files? This can
be done by ETL point-click tools, such as Pentaho PDI. Is there some
equivalent code in R to do this?

I googled it and could not find a solution. I also asked this in
StackOverflow and got no answer (here
<http://stackoverflow.com/questions/39414886/fix-errors-in-csv-and-fwf-files-corrupted-characters-when-importing-to-r>
).

regards
Lucas Mation
IPEA - Brasil

	[[alternative HTML version deleted]]


From klebyn at yahoo.com.br  Sun Nov  6 14:38:54 2016
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Sun, 6 Nov 2016 11:38:54 -0200
Subject: [R] tcl('tk::fontchooser', 'show') : How to use? R-devel Tcl 8.6
Message-ID: <61d50a38-bf03-f7f3-c802-0355167eb6a0@yahoo.com.br>

Hello,
Somebody would indicate the correct way to use the option the
fontchooser widget into tcltk package (in R-devel, Tcl 8.6) ?
I tried to use as other traditional widgets but no success. (code below)
Thanks
Cleber

 > tt <- tktoplevel(); but <- ttkbutton( tt, text='Test'); tcl('pack', but )
<Tcl>
 > tcl( but, 'configure', '-text' )
<Tcl> -text text Text {} Test
 > tcl( but, 'configure', text=NULL )
<Tcl> -text text Text {} Test
 > tcl( but, 'cget', '-text')
<Tcl> Test
 >
 >
 > tcl('tk::fontchooser', 'show')
<Tcl>
 > tcl( 'tk::fontchooser', 'configure', '-font')
<Tcl>
 >
 >
 > sessionInfo()
R Under development (unstable) (2016-11-01 r71616)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7600)

locale:
[1] LC_COLLATE=Portuguese_Brazil.1252 LC_CTYPE=Portuguese_Brazil.1252
LC_MONETARY=Portuguese_Brazil.1252
[4] LC_NUMERIC=C LC_TIME=Portuguese_Brazil.1252

attached base packages:
[1] tcltk     stats     graphics  grDevices utils     datasets methods
base

loaded via a namespace (and not attached):
[1] tools_3.4.0
 >




---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Sun Nov  6 17:12:09 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sun, 06 Nov 2016 08:12:09 -0800
Subject: [R] How to pre-process fwf or csv files to remove unexpected
	characters in R?
In-Reply-To: <CAA+7z9GuAh+SdABh0kzBeBOv+2M_CBg2pkM01PpejGudMSWJpQ@mail.gmail.com>
References: <CAA+7z9GuAh+SdABh0kzBeBOv+2M_CBg2pkM01PpejGudMSWJpQ@mail.gmail.com>
Message-ID: <6C6A27D0-012B-428D-8C77-C12E606ACCA3@dcn.davis.ca.us>

?readLines ... given the large size of file you may need to process chunks by specifying a file connection rather than a character string file name and using the "n" argument. 

?grepl

?Extract

?tools::showNonASCII

There are many ways for data to be corrupted... in particular when invalid characters appear the possibilities explode, so more specifics are needed if this is not enough. Of course, reading the Posting Guide, posting with plain text to avoid HTML corruption, and giving reproducible examples will improve the quality of responses to those questions. 

-- 
Sent from my phone. Please excuse my brevity.

On November 6, 2016 5:36:46 AM PST, Lucas Ferreira Mation <lucasmation at gmail.com> wrote:
>I have some large .txt files about ~100GB containing a dataset in fixed
>width file. This contains some errors:
>- character characters in column that are supposed to be numeric,
>- invalid characters
>- rows with too many characters, possibly due to invalid characters or
>some
>missing end of line character (so two rows in the original data become
>one
>row in the .txt file).
>
>The errors are not very frequent, but stop me from importing with readr
>::read_fwf()
>
>
>Is there some package, or workflow, in R to pre-process the files,
>separating the valid from the not-valid rows into different files? This
>can
>be done by ETL point-click tools, such as Pentaho PDI. Is there some
>equivalent code in R to do this?
>
>I googled it and could not find a solution. I also asked this in
>StackOverflow and got no answer (here
><http://stackoverflow.com/questions/39414886/fix-errors-in-csv-and-fwf-files-corrupted-characters-when-importing-to-r>
>).
>
>regards
>Lucas Mation
>IPEA - Brasil
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Sun Nov  6 17:16:19 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 6 Nov 2016 08:16:19 -0800
Subject: [R] How to pre-process fwf or csv files to remove unexpected
	characters in R?
In-Reply-To: <CAA+7z9GuAh+SdABh0kzBeBOv+2M_CBg2pkM01PpejGudMSWJpQ@mail.gmail.com>
References: <CAA+7z9GuAh+SdABh0kzBeBOv+2M_CBg2pkM01PpejGudMSWJpQ@mail.gmail.com>
Message-ID: <D0DA4567-5C61-4200-BDF2-185EF5E40DA7@comcast.net>


> On Nov 6, 2016, at 5:36 AM, Lucas Ferreira Mation <lucasmation at gmail.com> wrote:
> 
> I have some large .txt files about ~100GB containing a dataset in fixed
> width file. This contains some errors:
> - character characters in column that are supposed to be numeric,
> - invalid characters
> - rows with too many characters, possibly due to invalid characters or some
> missing end of line character (so two rows in the original data become one
> row in the .txt file).
> 
> The errors are not very frequent, but stop me from importing with readr
> ::read_fwf()
> 
> 
> Is there some package, or workflow, in R to pre-process the files,
> separating the valid from the not-valid rows into different files? This can
> be done by ETL point-click tools, such as Pentaho PDI. Is there some
> equivalent code in R to do this?
> 
> I googled it and could not find a solution. I also asked this in
> StackOverflow and got no answer (here
> <http://stackoverflow.com/questions/39414886/fix-errors-in-csv-and-fwf-files-corrupted-characters-when-importing-to-r>
> ).

Had I seen it there I would have voted to close (and just did) that SO question as too broad, although it is too vague because of lack of definition of "corrupted characters", and furthermore basically a request for a package recommendation (which is also off-topic on SO). 

For the csv part on a smaller file task (which you didn't repeat here) I would have pointed you to this answer:

http://stackoverflow.com/questions/19082490/how-can-i-use-r-to-find-malformed-rows-and-fields-in-a-file-too-big-to-read-into/19083665#19083665

For the fwf part (in a file that fits into RAM), I would have suggested wrapping table(nchar( . )) around readLines(file=filename). And then drilling down with which( nchar( . ) == <chosen_line_length> ) . 

I believe searching Rhelp will bring up examples of how to handle file input in chunks which should allow you to cobble together a strategy if you insist on using R ... the wrong tool. If you need to narrow your Rhelp archive search I suggest using the name "Jim Holtman" or "William Dunlap", or "Gabor Grothendieck" since they frequently have the most elegant strategies in my opinion.

Here's search strategy implemented via MarkMail:
http://markmail.org/search/?q=list%3Aorg.r-project.r-help+file+chunks+readlines

But for files of the size you contemplate I would suggest using databases, awk or other editing software that is designed for streaming processing from disk. R is not so designed.

-- 
David.


> 
> regards
> Lucas Mation
> IPEA - Brasil
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From marceloperlin at gmail.com  Sun Nov  6 17:02:50 2016
From: marceloperlin at gmail.com (Marcelo Perlin)
Date: Sun, 6 Nov 2016 14:02:50 -0200
Subject: [R] [R-pkgs] Package BatchGetSymbols
Message-ID: <CANMhtdw+6E9DCBfs6d2rrnc5YndaW1ZJQ1jpzNSyvH5Vxq09FQ@mail.gmail.com>

Dear R users,

If you use quantmod::GetSymbols to download financial data from yahoo or
google finance, you might find my new package  *BatchGetSymbols* very
useful. It no only downloads data for multiple tickers but also organizes
it in an efficient way, making it easy to use dplyr and ggplot2 in the
resulting dataframe.

You can check the vignette here:
https://cran.r-project.org/web/packages/BatchGetSymbols/vignettes/BatchGetSymbols-vignette.html

And the package here:
https://cran.r-project.org/web/packages/BatchGetSymbols/index.html

As usual, comments and suggestions are very welcome.

Best,

-- 
Marcelo Perlin
Professor Adjunto | Escola de Administra??o
Universidade Federal do Rio Grande do Sul
Rua Washington Luiz, 855 | 90010-460| Porto Alegre RS| Brasil
Tel.: (51) 3308-3303 | www.ea.ufrgs.br
http://lattes.cnpq.br/3262699324398819
https://sites.google.com/site/marceloperlin/

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages

From r.turner at auckland.ac.nz  Sun Nov  6 22:17:13 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 7 Nov 2016 10:17:13 +1300
Subject: [R] Dealing with -Inf in a maximisation problem.
Message-ID: <f2617dd1-3958-7c31-d2b6-02eec4153c9d@auckland.ac.nz>


I am trying to deal with a maximisation problem in which it is possible 
for the objective function to (quite legitimately) return the value 
-Inf, which causes the numerical optimisers that I have tried to fall over.

The -Inf values arise from expressions of the form "a * log(b)", with b 
= 0.  Under the *starting* values of the parameters, a must equal equal 
0 whenever b = 0, so we can legitimately say that a * log(b) = 0 in 
these circumstances.  However as the maximisation algorithm searches 
over parameters it is possible for b to take the value 0 for values of
a that are strictly positive.  (The values of "a" do not change during
this search, although they *do* change between "successive searches".)

Clearly if one is *maximising* the objective then -Inf is not a value of
particular interest, and we should be able to "move away".  But the 
optimising function just stops.

It is also clear that "moving away" is not a simple task; you can't 
estimate a gradient or Hessian at a point where the function value is -Inf.

Can anyone suggest a way out of this dilemma, perhaps an optimiser that 
is equipped to cope with -Inf values in some sneaky way?

Various ad hoc kludges spring to mind, but they all seem to be fraught 
with peril.

I have tried changing the value returned by the objective function from
"v" to exp(v) --- which maps -Inf to 0, which is nice and finite. 
However this seemed to flatten out the objective surface too much, and 
the search stalled at the 0 value, which is the antithesis of optimal.

The problem arises in a context of applying the EM algorithm where the 
M-step cannot be carried out explicitly, whence numerical optimisation.
I can give more detail if anyone thinks that it could be relevant.

I would appreciate advice from younger and wiser heads! :-)

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From drjimlemon at gmail.com  Sun Nov  6 22:56:40 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Mon, 7 Nov 2016 08:56:40 +1100
Subject: [R] How to pre-process fwf or csv files to remove unexpected
 characters in R?
In-Reply-To: <CAA+7z9GuAh+SdABh0kzBeBOv+2M_CBg2pkM01PpejGudMSWJpQ@mail.gmail.com>
References: <CAA+7z9GuAh+SdABh0kzBeBOv+2M_CBg2pkM01PpejGudMSWJpQ@mail.gmail.com>
Message-ID: <CA+8X3fUy8120T5kfSVR95PS0OX3pgx1=qeU8_H-=8FO843n+VA@mail.gmail.com>

Hi Lucas,
This is a rough outline of something I programmed years ago for data
cleaning (that was programmed in C). The basic idea is to read the
file line by line and check for a problem (in the initial application
this was a discrepancy between two lines that were supposed to be
identical). Here, if the line is the wrong length (optional) or
contains an unwanted character (this can be specified either as the
set of acceptable or unacceptable characters), the line is displayed
in an editor in which the user can manually fix it. The old file is
written line by line to a new file which replaces the old one. For
files in which bad lines are uncommon, this worked very well, as the
user only had to deal with errors. It is also only useful for files
containing only printable characters in the lines. Note that this is
only a sketch and I have not tested it.

cleanFile(filename,llength=NA,goodchars="[:print:]",badchars=NA) {
 infile<-file(filename,open="r")
 if(class(infile)=="connection") {
  done<-FALSE
  outfile<-file(paste("cF",filename,sep=""),"w")
  while(!done) {
   nextline<-readlines(infile,1)
   if(nchar(nextline) != llength && !is.na(llength)) nextline<-edit(nextline)
   if(!grepl(goodchars,nextline)) nextline<-edit(nextline)
   if(grep((badchars,nextline && !is.na(badchars)) nextline<-edit(nextline)
   writeLines(nextline,outfile)
   done<-nchar(nextline)<2
  }
  close(infile)
  close(outfile)
  file.remove(infile)
  file.rename(outfile,infile)
 } else {
  cat("Cannot open",file,"\n")
 }
}

Jim


On Mon, Nov 7, 2016 at 12:36 AM, Lucas Ferreira Mation
<lucasmation at gmail.com> wrote:
> I have some large .txt files about ~100GB containing a dataset in fixed
> width file. This contains some errors:
> - character characters in column that are supposed to be numeric,
> - invalid characters
> - rows with too many characters, possibly due to invalid characters or some
> missing end of line character (so two rows in the original data become one
> row in the .txt file).
>
> The errors are not very frequent, but stop me from importing with readr
> ::read_fwf()
>
>
> Is there some package, or workflow, in R to pre-process the files,
> separating the valid from the not-valid rows into different files? This can
> be done by ETL point-click tools, such as Pentaho PDI. Is there some
> equivalent code in R to do this?
>
> I googled it and could not find a solution. I also asked this in
> StackOverflow and got no answer (here
> <http://stackoverflow.com/questions/39414886/fix-errors-in-csv-and-fwf-files-corrupted-characters-when-importing-to-r>
> ).
>
> regards
> Lucas Mation
> IPEA - Brasil
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Sun Nov  6 23:02:09 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Mon, 7 Nov 2016 09:02:09 +1100
Subject: [R] Is this foreach behaviour correct?
In-Reply-To: <CY1PR11MB0444BE97C0B5173ADB45A892E9A40@CY1PR11MB0444.namprd11.prod.outlook.com>
References: <CY1PR11MB0444BE97C0B5173ADB45A892E9A40@CY1PR11MB0444.namprd11.prod.outlook.com>
Message-ID: <CA+8X3fXJxE=WEYQhs+Jw=nXvoS1hEnCDHivUG=Djz9fuyQN7Cw@mail.gmail.com>

hi James,
I think you have to have a starting date ("origin") for as.Date to
convert numbers to dates.

Jim

On Sun, Nov 6, 2016 at 12:10 PM, James Hirschorn
<james.hirschorn at hotmail.com> wrote:
> This seemed odd so I wanted to check:
>
>  > x <- foreach(i=10000:10100, .combine='c') %do% { as.Date(i) }
>
> yields a numeric vector for x:
>
>  > class(x)
> [1] "numeric"
>
> Should it not be a vector of Date?
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Sun Nov  6 23:20:07 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 6 Nov 2016 17:20:07 -0500
Subject: [R] Is this foreach behaviour correct?
In-Reply-To: <CA+8X3fXJxE=WEYQhs+Jw=nXvoS1hEnCDHivUG=Djz9fuyQN7Cw@mail.gmail.com>
References: <CY1PR11MB0444BE97C0B5173ADB45A892E9A40@CY1PR11MB0444.namprd11.prod.outlook.com>
	<CA+8X3fXJxE=WEYQhs+Jw=nXvoS1hEnCDHivUG=Djz9fuyQN7Cw@mail.gmail.com>
Message-ID: <0a16dda9-d58a-d6ff-31c6-1e6b20a637f3@gmail.com>

On 06/11/2016 5:02 PM, Jim Lemon wrote:
> hi James,
> I think you have to have a starting date ("origin") for as.Date to
> convert numbers to dates.

That's true with the function in the base package, but the zoo package 
also has an as.Date() function, which defaults the origin to 
"1970-01-01".  If James is using zoo his code would be okay.  If he's 
not, he would have got an error, so I think he must have been.

Duncan Murdoch

>
> Jim
>
> On Sun, Nov 6, 2016 at 12:10 PM, James Hirschorn
> <james.hirschorn at hotmail.com> wrote:
>> This seemed odd so I wanted to check:
>>
>>  > x <- foreach(i=10000:10100, .combine='c') %do% { as.Date(i) }
>>
>> yields a numeric vector for x:
>>
>>  > class(x)
>> [1] "numeric"
>>
>> Should it not be a vector of Date?
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wdunlap at tibco.com  Mon Nov  7 01:02:16 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Sun, 6 Nov 2016 16:02:16 -0800
Subject: [R] Is this foreach behaviour correct?
In-Reply-To: <0a16dda9-d58a-d6ff-31c6-1e6b20a637f3@gmail.com>
References: <CY1PR11MB0444BE97C0B5173ADB45A892E9A40@CY1PR11MB0444.namprd11.prod.outlook.com>
	<CA+8X3fXJxE=WEYQhs+Jw=nXvoS1hEnCDHivUG=Djz9fuyQN7Cw@mail.gmail.com>
	<0a16dda9-d58a-d6ff-31c6-1e6b20a637f3@gmail.com>
Message-ID: <CAF8bMcZ-RA6d43qj0Sdv-G2x6T5C_+sPqXoMv-wGW=2MKWinQQ@mail.gmail.com>

Note that in the OP's example c.Date is never invoked.  c.Date is called if
.combine
calls c rather than if .combine is c:

> library(zoo)
> trace(c.Date, quote(print(sys.call())))
Tracing function "c.Date" in package "base"
[1] "c.Date"
> foreach(i=10000:10003, .combine=c) %do% { as.Date(i) }
[1] 10000 10001 10002 10003
> foreach(i=10000:10003, .combine=function(...)c(...)) %do% { as.Date(i) }
Tracing c.Date(...) on entry
eval(expr, envir, enclos)
Tracing c.Date(...) on entry
eval(expr, envir, enclos)
Tracing c.Date(...) on entry
eval(expr, envir, enclos)
[1] "1997-05-19" "1997-05-20" "1997-05-21" "1997-05-22"


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Sun, Nov 6, 2016 at 2:20 PM, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 06/11/2016 5:02 PM, Jim Lemon wrote:
>
>> hi James,
>> I think you have to have a starting date ("origin") for as.Date to
>> convert numbers to dates.
>>
>
> That's true with the function in the base package, but the zoo package
> also has an as.Date() function, which defaults the origin to "1970-01-01".
> If James is using zoo his code would be okay.  If he's not, he would have
> got an error, so I think he must have been.
>
> Duncan Murdoch
>
>
>
>> Jim
>>
>> On Sun, Nov 6, 2016 at 12:10 PM, James Hirschorn
>> <james.hirschorn at hotmail.com> wrote:
>>
>>> This seemed odd so I wanted to check:
>>>
>>>  > x <- foreach(i=10000:10100, .combine='c') %do% { as.Date(i) }
>>>
>>> yields a numeric vector for x:
>>>
>>>  > class(x)
>>> [1] "numeric"
>>>
>>> Should it not be a vector of Date?
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>> ng-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Mon Nov  7 01:07:43 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Sun, 6 Nov 2016 16:07:43 -0800
Subject: [R] Dealing with -Inf in a maximisation problem.
In-Reply-To: <f2617dd1-3958-7c31-d2b6-02eec4153c9d@auckland.ac.nz>
References: <f2617dd1-3958-7c31-d2b6-02eec4153c9d@auckland.ac.nz>
Message-ID: <CAF8bMcYjDEe90H4H46u41eio2n1FX89ML1-FP7K9b56s=qNn0Q@mail.gmail.com>

Have you tried reparameterizing, using logb (=log(b)) instead of b?

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Sun, Nov 6, 2016 at 1:17 PM, Rolf Turner <r.turner at auckland.ac.nz> wrote:

>
> I am trying to deal with a maximisation problem in which it is possible
> for the objective function to (quite legitimately) return the value -Inf,
> which causes the numerical optimisers that I have tried to fall over.
>
> The -Inf values arise from expressions of the form "a * log(b)", with b =
> 0.  Under the *starting* values of the parameters, a must equal equal 0
> whenever b = 0, so we can legitimately say that a * log(b) = 0 in these
> circumstances.  However as the maximisation algorithm searches over
> parameters it is possible for b to take the value 0 for values of
> a that are strictly positive.  (The values of "a" do not change during
> this search, although they *do* change between "successive searches".)
>
> Clearly if one is *maximising* the objective then -Inf is not a value of
> particular interest, and we should be able to "move away".  But the
> optimising function just stops.
>
> It is also clear that "moving away" is not a simple task; you can't
> estimate a gradient or Hessian at a point where the function value is -Inf.
>
> Can anyone suggest a way out of this dilemma, perhaps an optimiser that is
> equipped to cope with -Inf values in some sneaky way?
>
> Various ad hoc kludges spring to mind, but they all seem to be fraught
> with peril.
>
> I have tried changing the value returned by the objective function from
> "v" to exp(v) --- which maps -Inf to 0, which is nice and finite. However
> this seemed to flatten out the objective surface too much, and the search
> stalled at the 0 value, which is the antithesis of optimal.
>
> The problem arises in a context of applying the EM algorithm where the
> M-step cannot be carried out explicitly, whence numerical optimisation.
> I can give more detail if anyone thinks that it could be relevant.
>
> I would appreciate advice from younger and wiser heads! :-)
>
> cheers,
>
> Rolf Turner
>
> --
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jbanta at uttyler.edu  Sun Nov  6 18:25:46 2016
From: jbanta at uttyler.edu (Joshua Banta)
Date: Sun, 6 Nov 2016 17:25:46 +0000
Subject: [R] Pesky file encoding problem
Message-ID: <156EFFA6-2538-4A6B-8E22-3AE489FF3569@uttyler.edu>

Dear everyone,

Please consider the following code, which I am using to make a custom text file. (I need to build the text file line-by-line like this for some idiosyncratic reasons that I will not get into here. But basically the real text file I am making is longer and more complicated than this.)

outfile = "output.arp"
cat('Title="data"', file=outfile, append=FALSE, sep = "\r\n")
cat('DataType=DNA', file=outfile, append=TRUE, sep = "\r\n")
cat('GenotypicData=1', file=outfile, append=TRUE, sep = "\r\n")
cat('GameticPhase=0', file=outfile, append=TRUE, sep = "\r\n")
cat('LocusSeparator=WHITESPACE', file=outfile, append=TRUE, sep = "\r\n")
cat("", file=outfile, append=TRUE, sep = "\r\n")
cat("[Data]", file=outfile, append=TRUE, sep = "\r\n")
cat("[[Samples]]", file=outfile, append=TRUE, sep = ?")

Here is my question:

When the output file (output.arp) is opened using TextEdit on a Mac, each line is shown separated by one or more carriage returns, which is what I want. When the output file is opened using Notepad on a PC, each line of text runs into the next with no carriage returns, which is not what I want.

How can I make this text file, or convert it, in such a way that it will open on Notepad on a PC showing each line separated by one or more carriage returns?

I am aware of solutions to this problem using other software as an intermediary, such as TextWrangler, but I would like to accomplish this entirely using r itself.

I would be most appreciative of the full code needed to get the job done.

Thanks very much in advance!
-----------------------------------
Josh Banta, Ph.D
Assistant Professor
Department of Biology and the Center for Environment, Biodiversity and Conservation
The University of Texas at Tyler
Tyler, TX 75799
http://plantevolutionaryecology.org

	[[alternative HTML version deleted]]


From lucasmation at gmail.com  Sun Nov  6 23:07:21 2016
From: lucasmation at gmail.com (Lucas Ferreira Mation)
Date: Sun, 6 Nov 2016 20:07:21 -0200
Subject: [R] How to pre-process fwf or csv files to remove unexpected
 characters in R?
In-Reply-To: <D0DA4567-5C61-4200-BDF2-185EF5E40DA7@comcast.net>
References: <CAA+7z9GuAh+SdABh0kzBeBOv+2M_CBg2pkM01PpejGudMSWJpQ@mail.gmail.com>
	<D0DA4567-5C61-4200-BDF2-185EF5E40DA7@comcast.net>
Message-ID: <CAA+7z9Hs6sKaKqWi8ULPDQudJWUDFOt6BoAOyOHDnGE0Mm7cfg@mail.gmail.com>

Thank you Bert, Jeff and  David for great answers.
Let me provide more context to clarify the question:

- I am running this on a large server (512GB), so the data still fits
into memory (and I also know how to process in chunks if necessary)
- I agree that DBMS and other software would me better suited, but our
team (part of gvt-body) is mostly comprised of statisticians and
economist who know R but not much of other languages or have admin
permissions to set up DBMS. It support is available but much more
scarce. So a pure R solution may be useful, and I want to understand
how much worst or more impractical it is. Performance is a secondary
concern.
- I haven't provided more concrete problems because I did not even
know how to find the lines that contain errors. With the suggestions
given, I think I will be able to do that


The mockup of the ideal function I would dream of having is something like:

clean_my_fwf <- function(infile, outfile, vector_of_error_row_numbers,
col_positions, col_types){
  - import_data from infile
  - in each row, check if the content of each caracter postion
correponds to the expected content of that variable given
col_positions and col_types. This must be true for all columns
  - write all lines that pass this test to outfile
  - return(vector_of_error_row_numbers) : containing the row number
(of the original dataset) of all rows that fail the test

}

where col_positions and col_types follow the syntax of readr::read_fwf
could be parallelized and C++ based






2016-11-06 14:16 GMT-02:00 David Winsemius <dwinsemius at comcast.net>:
>
>> On Nov 6, 2016, at 5:36 AM, Lucas Ferreira Mation <lucasmation at gmail.com> wrote:
>>
>> I have some large .txt files about ~100GB containing a dataset in fixed
>> width file. This contains some errors:
>> - character characters in column that are supposed to be numeric,
>> - invalid characters
>> - rows with too many characters, possibly due to invalid characters or some
>> missing end of line character (so two rows in the original data become one
>> row in the .txt file).
>>
>> The errors are not very frequent, but stop me from importing with readr
>> ::read_fwf()
>>
>>
>> Is there some package, or workflow, in R to pre-process the files,
>> separating the valid from the not-valid rows into different files? This can
>> be done by ETL point-click tools, such as Pentaho PDI. Is there some
>> equivalent code in R to do this?
>>
>> I googled it and could not find a solution. I also asked this in
>> StackOverflow and got no answer (here
>> <http://stackoverflow.com/questions/39414886/fix-errors-in-csv-and-fwf-files-corrupted-characters-when-importing-to-r>
>> ).
>
> Had I seen it there I would have voted to close (and just did) that SO question as too broad, although it is too vague because of lack of definition of "corrupted characters", and furthermore basically a request for a package recommendation (which is also off-topic on SO).
>
> For the csv part on a smaller file task (which you didn't repeat here) I would have pointed you to this answer:
>
> http://stackoverflow.com/questions/19082490/how-can-i-use-r-to-find-malformed-rows-and-fields-in-a-file-too-big-to-read-into/19083665#19083665
>
> For the fwf part (in a file that fits into RAM), I would have suggested wrapping table(nchar( . )) around readLines(file=filename). And then drilling down with which( nchar( . ) == <chosen_line_length> ) .
>
> I believe searching Rhelp will bring up examples of how to handle file input in chunks which should allow you to cobble together a strategy if you insist on using R ... the wrong tool. If you need to narrow your Rhelp archive search I suggest using the name "Jim Holtman" or "William Dunlap", or "Gabor Grothendieck" since they frequently have the most elegant strategies in my opinion.
>
> Here's search strategy implemented via MarkMail:
> http://markmail.org/search/?q=list%3Aorg.r-project.r-help+file+chunks+readlines
>
> But for files of the size you contemplate I would suggest using databases, awk or other editing software that is designed for streaming processing from disk. R is not so designed.
>
> --
> David.
>
>
>>
>> regards
>> Lucas Mation
>> IPEA - Brasil
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>


From drjimlemon at gmail.com  Mon Nov  7 01:41:41 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Mon, 7 Nov 2016 11:41:41 +1100
Subject: [R] Pesky file encoding problem
In-Reply-To: <156EFFA6-2538-4A6B-8E22-3AE489FF3569@uttyler.edu>
References: <156EFFA6-2538-4A6B-8E22-3AE489FF3569@uttyler.edu>
Message-ID: <CA+8X3fVgUuvftQYLCxDPx5s0jPyM2Y2=qmjvheaoka=qEjUnGQ@mail.gmail.com>

Hi Joshua,
Use Notepad++. It will also convert the linefeed EOLs to CR/LF.

Jim

On Mon, Nov 7, 2016 at 4:25 AM, Joshua Banta <jbanta at uttyler.edu> wrote:
> Dear everyone,
>
> Please consider the following code, which I am using to make a custom text file. (I need to build the text file line-by-line like this for some idiosyncratic reasons that I will not get into here. But basically the real text file I am making is longer and more complicated than this.)
>
> outfile = "output.arp"
> cat('Title="data"', file=outfile, append=FALSE, sep = "\r\n")
> cat('DataType=DNA', file=outfile, append=TRUE, sep = "\r\n")
> cat('GenotypicData=1', file=outfile, append=TRUE, sep = "\r\n")
> cat('GameticPhase=0', file=outfile, append=TRUE, sep = "\r\n")
> cat('LocusSeparator=WHITESPACE', file=outfile, append=TRUE, sep = "\r\n")
> cat("", file=outfile, append=TRUE, sep = "\r\n")
> cat("[Data]", file=outfile, append=TRUE, sep = "\r\n")
> cat("[[Samples]]", file=outfile, append=TRUE, sep = ?")
>
> Here is my question:
>
> When the output file (output.arp) is opened using TextEdit on a Mac, each line is shown separated by one or more carriage returns, which is what I want. When the output file is opened using Notepad on a PC, each line of text runs into the next with no carriage returns, which is not what I want.
>
> How can I make this text file, or convert it, in such a way that it will open on Notepad on a PC showing each line separated by one or more carriage returns?
>
> I am aware of solutions to this problem using other software as an intermediary, such as TextWrangler, but I would like to accomplish this entirely using r itself.
>
> I would be most appreciative of the full code needed to get the job done.
>
> Thanks very much in advance!
> -----------------------------------
> Josh Banta, Ph.D
> Assistant Professor
> Department of Biology and the Center for Environment, Biodiversity and Conservation
> The University of Texas at Tyler
> Tyler, TX 75799
> http://plantevolutionaryecology.org
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Mon Nov  7 02:07:08 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 6 Nov 2016 20:07:08 -0500
Subject: [R] Pesky file encoding problem
In-Reply-To: <156EFFA6-2538-4A6B-8E22-3AE489FF3569@uttyler.edu>
References: <156EFFA6-2538-4A6B-8E22-3AE489FF3569@uttyler.edu>
Message-ID: <27d1f2e8-3c6b-fd57-544b-c4a22f1a5852@gmail.com>

On 06/11/2016 12:25 PM, Joshua Banta wrote:
> Dear everyone,
>
> Please consider the following code, which I am using to make a custom text file. (I need to build the text file line-by-line like this for some idiosyncratic reasons that I will not get into here. But basically the real text file I am making is longer and more complicated than this.)
>
> outfile = "output.arp"
> cat('Title="data"', file=outfile, append=FALSE, sep = "\r\n")
> cat('DataType=DNA', file=outfile, append=TRUE, sep = "\r\n")
> cat('GenotypicData=1', file=outfile, append=TRUE, sep = "\r\n")
> cat('GameticPhase=0', file=outfile, append=TRUE, sep = "\r\n")
> cat('LocusSeparator=WHITESPACE', file=outfile, append=TRUE, sep = "\r\n")
> cat("", file=outfile, append=TRUE, sep = "\r\n")
> cat("[Data]", file=outfile, append=TRUE, sep = "\r\n")
> cat("[[Samples]]", file=outfile, append=TRUE, sep = ?")
>
> Here is my question:
>
> When the output file (output.arp) is opened using TextEdit on a Mac, each line is shown separated by one or more carriage returns, which is what I want. When the output file is opened using Notepad on a PC, each line of text runs into the next with no carriage returns, which is not what I want.
>
> How can I make this text file, or convert it, in such a way that it will open on Notepad on a PC showing each line separated by one or more carriage returns?
>
> I am aware of solutions to this problem using other software as an intermediary, such as TextWrangler, but I would like to accomplish this entirely using r itself.
>
> I would be most appreciative of the full code needed to get the job done.

Notepad is a really weak text editor.  If you want it to recognize line 
breaks, they need to be in Windows style.

Even though your "sep" argument looks like Windows CR/LF, it is never 
used except to change the behaviour of cat, which will write a single \n 
after each entry.  (See the note in ?cat that explains this strange 
behaviour.)

So what you could do is something like this:

crlf <- "\r\n"
outfile = "output.arp"
cat('Title="data"', crlf, file=outfile, append=FALSE)
cat('DataType=DNA', crlf, file=outfile, append=TRUE)

etc.

Duncan Murdoch


From profjcnash at gmail.com  Mon Nov  7 02:14:07 2016
From: profjcnash at gmail.com (ProfJCNash)
Date: Sun, 6 Nov 2016 20:14:07 -0500
Subject: [R] Dealing with -Inf in a maximisation problem.
In-Reply-To: <CAF8bMcYjDEe90H4H46u41eio2n1FX89ML1-FP7K9b56s=qNn0Q@mail.gmail.com>
References: <f2617dd1-3958-7c31-d2b6-02eec4153c9d@auckland.ac.nz>
	<CAF8bMcYjDEe90H4H46u41eio2n1FX89ML1-FP7K9b56s=qNn0Q@mail.gmail.com>
Message-ID: <2157b02c-8b68-a38d-e36a-61ff03dc2c94@gmail.com>

Rolf, What optimizers did you try? There are a few in the optimrx package on R-forge that handle bounds, and it may be
useful to set bounds in this case. Transformations using log or exp can be helpful if done carefully, but as you note,
they can make the function more difficult to optimize.

Be cautious about using the default numerical gradient approximations. optimrx allows selection of the numDeriv grad()
function, which is quite good. Complex step would be better, but you need a function which can be computed with complex
arguments. Unfortunately, numerical gradients often step over the cliff edge of computability of the function. The
bounds are not checked for the step to compute things like (f(x+h) - f(x) / h.

Cheers, JN

On 16-11-06 07:07 PM, William Dunlap via R-help wrote:
> Have you tried reparameterizing, using logb (=log(b)) instead of b?
> 
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
> 
> On Sun, Nov 6, 2016 at 1:17 PM, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> 
>>
>> I am trying to deal with a maximisation problem in which it is possible
>> for the objective function to (quite legitimately) return the value -Inf,
>> which causes the numerical optimisers that I have tried to fall over.
>>
>> The -Inf values arise from expressions of the form "a * log(b)", with b =
>> 0.  Under the *starting* values of the parameters, a must equal equal 0
>> whenever b = 0, so we can legitimately say that a * log(b) = 0 in these
>> circumstances.  However as the maximisation algorithm searches over
>> parameters it is possible for b to take the value 0 for values of
>> a that are strictly positive.  (The values of "a" do not change during
>> this search, although they *do* change between "successive searches".)
>>
>> Clearly if one is *maximising* the objective then -Inf is not a value of
>> particular interest, and we should be able to "move away".  But the
>> optimising function just stops.
>>
>> It is also clear that "moving away" is not a simple task; you can't
>> estimate a gradient or Hessian at a point where the function value is -Inf.
>>
>> Can anyone suggest a way out of this dilemma, perhaps an optimiser that is
>> equipped to cope with -Inf values in some sneaky way?
>>
>> Various ad hoc kludges spring to mind, but they all seem to be fraught
>> with peril.
>>
>> I have tried changing the value returned by the objective function from
>> "v" to exp(v) --- which maps -Inf to 0, which is nice and finite. However
>> this seemed to flatten out the objective surface too much, and the search
>> stalled at the 0 value, which is the antithesis of optimal.
>>
>> The problem arises in a context of applying the EM algorithm where the
>> M-step cannot be carried out explicitly, whence numerical optimisation.
>> I can give more detail if anyone thinks that it could be relevant.
>>
>> I would appreciate advice from younger and wiser heads! :-)
>>
>> cheers,
>>
>> Rolf Turner
>>
>> --
>> Technical Editor ANZJS
>> Department of Statistics
>> University of Auckland
>> Phone: +64-9-373-7599 ext. 88276
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From r.turner at auckland.ac.nz  Mon Nov  7 02:25:59 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 7 Nov 2016 14:25:59 +1300
Subject: [R] Dealing with -Inf in a maximisation problem.
In-Reply-To: <CAF8bMcYjDEe90H4H46u41eio2n1FX89ML1-FP7K9b56s=qNn0Q@mail.gmail.com>
References: <f2617dd1-3958-7c31-d2b6-02eec4153c9d@auckland.ac.nz>
	<CAF8bMcYjDEe90H4H46u41eio2n1FX89ML1-FP7K9b56s=qNn0Q@mail.gmail.com>
Message-ID: <c915dd45-a057-bbe1-95b9-ce99befd44d8@auckland.ac.nz>

On 07/11/16 13:07, William Dunlap wrote:
> Have you tried reparameterizing, using logb (=log(b)) instead of b?

Uh, no.  I don't think that that makes any sense in my context.

The "b" values are probabilities and must satisfy a "sum-to-1" 
constraint.  To accommodate this constraint I re-parametrise via a 
"logistic" style parametrisation --- basically

    b_i = exp(z_i)/[sum_j exp(z_j)], j = 1, ... n

with the parameters that the optimiser works with being z_1, ..., 
z_{n-1} (and with z_n == 0 for identifiability).  The objective function 
is of the form sum_i(a_i * log(b_i)), so I transform back
from the z_i to the b_i in order calculate the value of the objective
function.  But when the z_i get moderately large-negative, the b_i 
become numerically 0 and then log(b_i) becomes -Inf.  And the optimiser 
falls over.

cheers,

Rolf

>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com <http://tibco.com>
>
> On Sun, Nov 6, 2016 at 1:17 PM, Rolf Turner <r.turner at auckland.ac.nz
> <mailto:r.turner at auckland.ac.nz>> wrote:
>
>
>     I am trying to deal with a maximisation problem in which it is
>     possible for the objective function to (quite legitimately) return
>     the value -Inf, which causes the numerical optimisers that I have
>     tried to fall over.
>
>     The -Inf values arise from expressions of the form "a * log(b)",
>     with b = 0.  Under the *starting* values of the parameters, a must
>     equal equal 0 whenever b = 0, so we can legitimately say that a *
>     log(b) = 0 in these circumstances.  However as the maximisation
>     algorithm searches over parameters it is possible for b to take the
>     value 0 for values of
>     a that are strictly positive.  (The values of "a" do not change during
>     this search, although they *do* change between "successive searches".)
>
>     Clearly if one is *maximising* the objective then -Inf is not a value of
>     particular interest, and we should be able to "move away".  But the
>     optimising function just stops.
>
>     It is also clear that "moving away" is not a simple task; you can't
>     estimate a gradient or Hessian at a point where the function value
>     is -Inf.
>
>     Can anyone suggest a way out of this dilemma, perhaps an optimiser
>     that is equipped to cope with -Inf values in some sneaky way?
>
>     Various ad hoc kludges spring to mind, but they all seem to be
>     fraught with peril.
>
>     I have tried changing the value returned by the objective function from
>     "v" to exp(v) --- which maps -Inf to 0, which is nice and finite.
>     However this seemed to flatten out the objective surface too much,
>     and the search stalled at the 0 value, which is the antithesis of
>     optimal.
>
>     The problem arises in a context of applying the EM algorithm where
>     the M-step cannot be carried out explicitly, whence numerical
>     optimisation.
>     I can give more detail if anyone thinks that it could be relevant.
>
>     I would appreciate advice from younger and wiser heads! :-)
>
>     cheers,
>
>     Rolf Turner
>
>     --
>     Technical Editor ANZJS
>     Department of Statistics
>     University of Auckland
>     Phone: +64-9-373-7599 ext. 88276 <tel:%2B64-9-373-7599%20ext.%2088276>
>
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>     and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r.turner at auckland.ac.nz  Mon Nov  7 03:39:15 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 7 Nov 2016 15:39:15 +1300
Subject: [R] Dealing with -Inf in a maximisation problem.
In-Reply-To: <2157b02c-8b68-a38d-e36a-61ff03dc2c94@gmail.com>
References: <f2617dd1-3958-7c31-d2b6-02eec4153c9d@auckland.ac.nz>
	<CAF8bMcYjDEe90H4H46u41eio2n1FX89ML1-FP7K9b56s=qNn0Q@mail.gmail.com>
	<2157b02c-8b68-a38d-e36a-61ff03dc2c94@gmail.com>
Message-ID: <d627521b-5aa4-5c76-c7fd-e99a5d65106b@auckland.ac.nz>


On 07/11/16 14:14, ProfJCNash wrote:

> Rolf, What optimizers did you try? There are a few in the optimrx package on R-forge that handle bounds, and it may be
> useful to set bounds in this case. Transformations using log or exp can be helpful if done carefully, but as you note,
> they can make the function more difficult to optimize.

I can't see how to impose bounds in my circumstances.  As you will have 
seen from my previous answer to Bill Dunlap's post, the b_i are 
probabilities, parametrised in terms of z_i:

     b_i = exp(z_i)/[sum_j exp(z_j)] .

It is not at all clear to me how to impose constraints on the z_i that 
will bound the b_i away from 0.

I can constrain L <= z_i <= U for all i and get b_i >= exp(L)/[n*exp(U)]
--- I think; I may have things upsidedown and backwards --- but this 
leaves an infinitude of choices for L and U.

Also the starting values at each M-step are "naturally" given in terms 
of the b_i.  I.e. I can calculate "reasonable" values for the b_i and 
then transform these to provide starting values for the z_i.  The 
starting values for z_i might not satisfy a given set of constraints.
I guess I could simply truncate the starting values to fall within the 
constraints, but that "feels wrong" to me.

I also worry about the impact that imposing constraints will have on
the monotonicity of the successive values of the expected log likelihood 
in the EM algorithm context.

> Be cautious about using the default numerical gradient approximations. optimrx allows selection of the numDeriv grad()
> function, which is quite good. Complex step would be better, but you need a function which can be computed with complex
> arguments. Unfortunately, numerical gradients often step over the cliff edge of computability of the function. The
> bounds are not checked for the step to compute things like (f(x+h) - f(x) / h.

I think I can program up an analytic gradient function.  Maybe I'll try 
that.  I have been reluctant to do so because I have had peculiar (bad) 
experiences in the past in trying to use analytic gradients with nlm().

Of course the (analytic) gradient becomes undefined if one of the b_i is 0.

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From ccberry at ucsd.edu  Mon Nov  7 03:46:28 2016
From: ccberry at ucsd.edu (Charles C. Berry)
Date: Sun, 6 Nov 2016 18:46:28 -0800
Subject: [R] Dealing with -Inf in a maximisation problem.
In-Reply-To: <c915dd45-a057-bbe1-95b9-ce99befd44d8@auckland.ac.nz>
References: <f2617dd1-3958-7c31-d2b6-02eec4153c9d@auckland.ac.nz>
	<CAF8bMcYjDEe90H4H46u41eio2n1FX89ML1-FP7K9b56s=qNn0Q@mail.gmail.com>
	<c915dd45-a057-bbe1-95b9-ce99befd44d8@auckland.ac.nz>
Message-ID: <alpine.OSX.2.20.1611061832450.1250@charles-berrys-macbook.local>

On Mon, 7 Nov 2016, Rolf Turner wrote:

> On 07/11/16 13:07, William Dunlap wrote:
>> Have you tried reparameterizing, using logb (=log(b)) instead of b?
>
> Uh, no.  I don't think that that makes any sense in my context.
>
> The "b" values are probabilities and must satisfy a "sum-to-1" constraint. 
> To accommodate this constraint I re-parametrise via a "logistic" style 
> parametrisation --- basically
>
>   b_i = exp(z_i)/[sum_j exp(z_j)], j = 1, ... n
>
> with the parameters that the optimiser works with being z_1, ..., z_{n-1} 
> (and with z_n == 0 for identifiability).  The objective function is of the 
> form sum_i(a_i * log(b_i)),


This is sum_i(a_i * z_i) - sum(a_i)*log(sum_j(exp(z_j)), isn't it?

So you don't need to evaluate b_i here, do you?

Large values of z_j will lead to exp(z_j) == Inf, but using

 	sum_i(a_i * (z_i-max.z)) - sum(a_i)*log(sum_j(exp(z_j-max.z))

will handle that.

HTH,

Chuck

p.s. Regarding "advice from younger and wiser heads", I probably cannot 
claim to be either.


From r.turner at auckland.ac.nz  Mon Nov  7 04:15:48 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 7 Nov 2016 16:15:48 +1300
Subject: [R] Dealing with -Inf in a maximisation problem.
In-Reply-To: <alpine.OSX.2.20.1611061832450.1250@charles-berrys-macbook.local>
References: <f2617dd1-3958-7c31-d2b6-02eec4153c9d@auckland.ac.nz>
	<CAF8bMcYjDEe90H4H46u41eio2n1FX89ML1-FP7K9b56s=qNn0Q@mail.gmail.com>
	<c915dd45-a057-bbe1-95b9-ce99befd44d8@auckland.ac.nz>
	<alpine.OSX.2.20.1611061832450.1250@charles-berrys-macbook.local>
Message-ID: <c4a40f75-b6c0-a5ff-f41f-c1ef19b52acd@auckland.ac.nz>

On 07/11/16 15:46, Charles C. Berry wrote:
> On Mon, 7 Nov 2016, Rolf Turner wrote:
>
>> On 07/11/16 13:07, William Dunlap wrote:
>>> Have you tried reparameterizing, using logb (=log(b)) instead of b?
>>
>> Uh, no.  I don't think that that makes any sense in my context.
>>
>> The "b" values are probabilities and must satisfy a "sum-to-1"
>> constraint. To accommodate this constraint I re-parametrise via a
>> "logistic" style parametrisation --- basically
>>
>>   b_i = exp(z_i)/[sum_j exp(z_j)], j = 1, ... n
>>
>> with the parameters that the optimiser works with being z_1, ...,
>> z_{n-1} (and with z_n == 0 for identifiability).  The objective
>> function is of the form sum_i(a_i * log(b_i)),
>
>
> This is sum_i(a_i * z_i) - sum(a_i)*log(sum_j(exp(z_j)), isn't it?
>
> So you don't need to evaluate b_i here, do you?
>
> Large values of z_j will lead to exp(z_j) == Inf, but using
>
>     sum_i(a_i * (z_i-max.z)) - sum(a_i)*log(sum_j(exp(z_j-max.z))
>
> will handle that.

Wow!!!  That looks like it will work!!!  I won't completely believe it 
until I've programmed it up and tried it --- but for the first time in 
days I'm feeling hopeful.
>
> HTH,
>
> Chuck
>
> p.s. Regarding "advice from younger and wiser heads", I probably cannot
> claim to be either.

On present evidence you certainly appear to be one hell of a lot wiser!!!

Thanks.

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From megan.fight at gmail.com  Mon Nov  7 05:12:38 2016
From: megan.fight at gmail.com (Megan)
Date: Sun, 6 Nov 2016 23:12:38 -0500
Subject: [R] Kosaraju's SCC Algorithm Running
Message-ID: <F0E571C1-E334-437D-BB53-D7A45A8F55DF@gmail.com>

To whom it may concerns,

We encountered stack overflow issues when we implemented DFS(depth first search) algorithm on a directed graph having 800,000+ vertices and millions of edges.  The purpose of running DFS is to use Kosaraju Algorithm to calculate the size of SCC(strongly connected componment) in the graph. This is an assignment from Coursea.org <http://coursea.org/>. We know the maximum size of SCC in the graph is 434,821, which means the maximum recursion depth during code running is 434,821. We know it is a large calculation behind the scene, therefore, we?ve done the below pre-setup hopefully to solve the stack overflow issue. However, we have not got the luck yet. We?ve tried running on both R and RStudio.

We would really appreciate if someone in your team can help to investigate. We can?t wait to see if our code is working in R. 

System Information:
Model Name:	MacBook Pro
  Model Identifier:	MacBookPro11,1
  Processor Name:	Intel Core i5
  Processor Speed:	2.4 GHz
  Number of Processors:	1
  Total Number of Cores:	2
  L2 Cache (per Core):	256 KB
  L3 Cache:	3 MB
  Memory:	8 GB

System settings we have tried:
1. ulimit- s 65000 (to increase stack size before starting up R)
2. R --max-ppsize =500000 (start R with max ppsize)
3. options(expression=500000)


The data we used can be found on 
https://d18ky98rnyall9.cloudfront.net/_410e934e6553ac56409b2cb7096a44aa_SCC.txt?Expires=1478649600&Signature=YC0OjTn4hmzWpzAMw3WkQjXgxkAc0qhZrCd07JAPtud3dGQqywpQgkAASf-bWX6qgGafVvObciJ3ww-7wbNqY0YhWxwcg-IxmCnz1xJu1SORdDobiVKmhhfSaqgTyullX1hFmcxAA4y6Ud33hY1dhIP~UTAlW~IV8Y-zSliAts8_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A <https://d18ky98rnyall9.cloudfront.net/_410e934e6553ac56409b2cb7096a44aa_SCC.txt?Expires=1478649600&Signature=YC0OjTn4hmzWpzAMw3WkQjXgxkAc0qhZrCd07JAPtud3dGQqywpQgkAASf-bWX6qgGafVvObciJ3ww-7wbNqY0YhWxwcg-IxmCnz1xJu1SORdDobiVKmhhfSaqgTyullX1hFmcxAA4y6Ud33hY1dhIP~UTAlW~IV8Y-zSliAts8_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A>

Data discription provided as follow:
https://www.coursera.org/learn/algorithm-design-analysis/exam/rOtFq/programming-assignment-4 <https://www.coursera.org/learn/algorithm-design-analysis/exam/rOtFq/programming-assignment-4> 

Below is our code:

options(expressions=500000)
#Prepare input test file  
g=read.table("Downloads/scc.txt")
x<<-as.matrix(g)
remove(g)
vector.is.empty<<-function(x) {return(length(x)==0)}
'%!in%' <- function(x,y)!('%in%'(x,y))
#g<-c(2,1,3,1,3,4,4,2,5,4,5,6,6,7,7,8,8,9,2,3)
#x<<-matrix(g,nrow=10,ncol=2, byrow=TRUE)
#g<-c(1,4,2,8,3,6,4,7,5,2,6,9,7,1,8,5,8,6,9,3,9,7,10,2)
#x<<-matrix(g,nrow=12,ncol=2, byrow=TRUE)
#g<-c(1,2,2,3,2,4,3,4,3,5,4,1,4,13,5,6,6,7,7,8,8,9,9,6,10,9,10,11,11,8,11,12,12,13,12,14,13,10,14,15)
#x<<-matrix(g,20,2,byrow=TRUE)

u1<-unique(x[,1])
u2<-unique(x[,2])
u<-c(u1,u2)
n<<-length(unique(u))
remove(u1,u2,u)

G <<- vector("list", length=n)
G_REV <<- vector("list", length=n)
P = numeric(n)
FT = numeric(n)

for (i in 1:nrow(x)) {
  a = x[i,1]
  b = x[i,2]
  #for G
  if (is.null(G[[a]])) {
    G[[a]] = c(b)
  } else {
    G[[a]] = c(G[[a]], b)
  }
  if (is.null(G[[b]])) {
    G[[b]] = numeric()
  }
  #for G_VEV
  if (is.null(G_REV[[b]])) {
    G_REV[[b]] = c(a)
  } else {
    G_REV[[b]] = c(G_REV[[b]], a)
  }
  if (is.null(G_REV[[a]])) {
    G_REV[[a]] = numeric()
  }
}

G_TEMP <<- G_REV

#G_REV<<-x[,c(2,1)]

#P<<-matrix(c(1:n,rep(0,2*n)),nrow=n,ncol=3) #tracking whether vertex is explored or not

#colnames(P)<-c("node","f","parent_vertex")
explore<<-numeric()
assign("time",0,envir=globalenv())
#Algorithm -DFS

DFS_LOOP<<-function(G){
  counter = n 
  for(i in n : 1){
    if (i < counter) {
      counter = counter - 1000;
      print(i);
    }
    if (i %in% explore){next}
    else {
      DFS(i)
    }
  }
}

DFS<<-function(i){
  #if(time>=n){return(p)}
  #else{
  #print(c("i=",i))
  explore<<-c(explore,i)
  #print(c("explore",explore))
  
  #P[i,2] <<- 1 # gray
  v=G_TEMP[[i]]
  
  #print(c("v=",v))
  if (vector.is.empty(v) ==TRUE){
    len=1
    v=i
  }
  
  if(vector.is.empty(v)==FALSE){
    len=length(v)
  }
  
  for(j in 1: len){
    if(v[j] %!in% explore){
      DFS(v[j])
      #P[v[j],3] <<-i
      P[v[j]] <<- i
      # print(c("child",j,"parent",i))
    }
  }
  
  time<<-time + 1
  FT[i] <<- time
  #P[i,2] <<- time
  #P[i,2] <<- 2 #black
  # } <<-else
}
print('Starting DFS_loop on G_REV')
DFS_LOOP(G_REV)
###################################################
#temp0<-matrix(1:n,n,1)
# temp1<-P[,c(1,2)]
# colnames(temp1)<-c("before","after")
# temp1<-as.data.frame(temp1)
# colnames(x)<-c("vertex","edge")
# X<-as.data.frame(x)
# X_NEW<-merge(x=X,y=temp1,by.x="vertex",by.y="before")
# remove(X)
# names(X_NEW)[names(X_NEW)=="after"]<-"vertex_new"
# X_NEW2<-merge(x=X_NEW,y=temp1,by.x="edge",by.y="before")
# remove(X_NEW,temp1)
# names(X_NEW2)[names(X_NEW2)=="after"]<-"edge_new"
# G2<-as.matrix(X_NEW2)
# remove(X_NEW2)
# G2<-G2[,c(3,4)]
# u1<-unique(G2[,1])
# u2<-unique(G2[,2])
# u<-c(u1,u2)
# n<<-length(unique(u))
# remove(u1,u2,u)

FT_SORTED = numeric(n)
for (i in length(FT):1) {
  finish_time = FT[i]
  FT_SORTED[finish_time] = i
}

P = numeric(n)
FT = numeric(n)

#P<<-matrix(c(1:n,rep(0,2*n)),nrow=n,ncol=3) #tracking whether vertex is explored or not
#colnames(P)<-c("vertex","f","parent_vertex")
explore<<-numeric()
assign("time",0,envir=globalenv())

print('Starting DFS_loop on G')
#DFS_LOOP(G2)#2nd DFS
explore<<-numeric()

G_TEMP <<- G

for (i in length(FT_SORTED):1) { 
  k = FT_SORTED[i]
  if (k %!in% explore) {
    DFS(k)
  }
}

mscc_temp = which(P==0)
scc_temp=FT[mscc_temp]
#scc_temp<-P[P[,3]==0,2]
scc_temp=sort(scc_temp,decreasing=TRUE)
m=length(scc_temp)
scc=numeric()
for (i in 1:(m-1)){
  scc[i]=scc_temp[i]-scc_temp[i+1]
}
scc[m]<-scc_temp[m]
scc_top_5<-tail(sort(scc),5)


Thanks,
Jing
	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Mon Nov  7 05:53:17 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Sun, 6 Nov 2016 20:53:17 -0800
Subject: [R] Dealing with -Inf in a maximisation problem.
In-Reply-To: <c915dd45-a057-bbe1-95b9-ce99befd44d8@auckland.ac.nz>
References: <f2617dd1-3958-7c31-d2b6-02eec4153c9d@auckland.ac.nz>
	<CAF8bMcYjDEe90H4H46u41eio2n1FX89ML1-FP7K9b56s=qNn0Q@mail.gmail.com>
	<c915dd45-a057-bbe1-95b9-ce99befd44d8@auckland.ac.nz>
Message-ID: <CAF8bMcZ9cZ0+e4+pykrwqTHyL4c1+coaToM5qL2jOiNcS8r3eg@mail.gmail.com>

Perhaps the C function Rf_logspace_sum(double *x, int n) would help in
computing log(b).  It computes log(sum(exp(x_i))) for i in 1..n, avoiding
unnecessary under- and overflow.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Sun, Nov 6, 2016 at 5:25 PM, Rolf Turner <r.turner at auckland.ac.nz> wrote:

> On 07/11/16 13:07, William Dunlap wrote:
>
>> Have you tried reparameterizing, using logb (=log(b)) instead of b?
>>
>
> Uh, no.  I don't think that that makes any sense in my context.
>
> The "b" values are probabilities and must satisfy a "sum-to-1"
> constraint.  To accommodate this constraint I re-parametrise via a
> "logistic" style parametrisation --- basically
>
>    b_i = exp(z_i)/[sum_j exp(z_j)], j = 1, ... n
>
> with the parameters that the optimiser works with being z_1, ..., z_{n-1}
> (and with z_n == 0 for identifiability).  The objective function is of the
> form sum_i(a_i * log(b_i)), so I transform back
> from the z_i to the b_i in order calculate the value of the objective
> function.  But when the z_i get moderately large-negative, the b_i become
> numerically 0 and then log(b_i) becomes -Inf.  And the optimiser falls over.
>
> cheers,
>
> Rolf
>
>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com <http://tibco.com>
>>
>> On Sun, Nov 6, 2016 at 1:17 PM, Rolf Turner <r.turner at auckland.ac.nz
>> <mailto:r.turner at auckland.ac.nz>> wrote:
>>
>>
>>     I am trying to deal with a maximisation problem in which it is
>>     possible for the objective function to (quite legitimately) return
>>     the value -Inf, which causes the numerical optimisers that I have
>>     tried to fall over.
>>
>>     The -Inf values arise from expressions of the form "a * log(b)",
>>     with b = 0.  Under the *starting* values of the parameters, a must
>>     equal equal 0 whenever b = 0, so we can legitimately say that a *
>>     log(b) = 0 in these circumstances.  However as the maximisation
>>     algorithm searches over parameters it is possible for b to take the
>>     value 0 for values of
>>     a that are strictly positive.  (The values of "a" do not change during
>>     this search, although they *do* change between "successive searches".)
>>
>>     Clearly if one is *maximising* the objective then -Inf is not a value
>> of
>>     particular interest, and we should be able to "move away".  But the
>>     optimising function just stops.
>>
>>     It is also clear that "moving away" is not a simple task; you can't
>>     estimate a gradient or Hessian at a point where the function value
>>     is -Inf.
>>
>>     Can anyone suggest a way out of this dilemma, perhaps an optimiser
>>     that is equipped to cope with -Inf values in some sneaky way?
>>
>>     Various ad hoc kludges spring to mind, but they all seem to be
>>     fraught with peril.
>>
>>     I have tried changing the value returned by the objective function
>> from
>>     "v" to exp(v) --- which maps -Inf to 0, which is nice and finite.
>>     However this seemed to flatten out the objective surface too much,
>>     and the search stalled at the 0 value, which is the antithesis of
>>     optimal.
>>
>>     The problem arises in a context of applying the EM algorithm where
>>     the M-step cannot be carried out explicitly, whence numerical
>>     optimisation.
>>     I can give more detail if anyone thinks that it could be relevant.
>>
>>     I would appreciate advice from younger and wiser heads! :-)
>>
>>     cheers,
>>
>>     Rolf Turner
>>
>>     --
>>     Technical Editor ANZJS
>>     Department of Statistics
>>     University of Auckland
>>     Phone: +64-9-373-7599 ext. 88276 <tel:%2B64-9-373-7599%20ext.%2
>> 088276>
>>
>>     ______________________________________________
>>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>>     To UNSUBSCRIBE and more, see
>>     https://stat.ethz.ch/mailman/listinfo/r-help
>>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>>     PLEASE do read the posting guide
>>     http://www.R-project.org/posting-guide.html
>>     <http://www.R-project.org/posting-guide.html>
>>     and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>
> --
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Mon Nov  7 08:28:05 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sun, 06 Nov 2016 23:28:05 -0800
Subject: [R] Kosaraju's SCC Algorithm Running
In-Reply-To: <F0E571C1-E334-437D-BB53-D7A45A8F55DF@gmail.com>
References: <F0E571C1-E334-437D-BB53-D7A45A8F55DF@gmail.com>
Message-ID: <93CE03A9-AF34-441C-ADD4-4E7D813E42D6@dcn.davis.ca.us>

Please read the Posting Guide mentioned at the bottom of this and every post, which warns you that homework is off topic on this mailing list. Use the support provided by your institution of learning (Coursera in this case).
-- 
Sent from my phone. Please excuse my brevity.

On November 6, 2016 8:12:38 PM PST, Megan <megan.fight at gmail.com> wrote:
>To whom it may concerns,
>
>We encountered stack overflow issues when we implemented DFS(depth
>first search) algorithm on a directed graph having 800,000+ vertices
>and millions of edges.  The purpose of running DFS is to use Kosaraju
>Algorithm to calculate the size of SCC(strongly connected componment)
>in the graph. This is an assignment from Coursea.org
><http://coursea.org/>. We know the maximum size of SCC in the graph is
>434,821, which means the maximum recursion depth during code running is
>434,821. We know it is a large calculation behind the scene, therefore,
>we?ve done the below pre-setup hopefully to solve the stack overflow
>issue. However, we have not got the luck yet. We?ve tried running on
>both R and RStudio.
>
>We would really appreciate if someone in your team can help to
>investigate. We can?t wait to see if our code is working in R. 
>
>System Information:
>Model Name:	MacBook Pro
>  Model Identifier:	MacBookPro11,1
>  Processor Name:	Intel Core i5
>  Processor Speed:	2.4 GHz
>  Number of Processors:	1
>  Total Number of Cores:	2
>  L2 Cache (per Core):	256 KB
>  L3 Cache:	3 MB
>  Memory:	8 GB
>
>System settings we have tried:
>1. ulimit- s 65000 (to increase stack size before starting up R)
>2. R --max-ppsize =500000 (start R with max ppsize)
>3. options(expression=500000)
>
>
>The data we used can be found on 
>https://d18ky98rnyall9.cloudfront.net/_410e934e6553ac56409b2cb7096a44aa_SCC.txt?Expires=1478649600&Signature=YC0OjTn4hmzWpzAMw3WkQjXgxkAc0qhZrCd07JAPtud3dGQqywpQgkAASf-bWX6qgGafVvObciJ3ww-7wbNqY0YhWxwcg-IxmCnz1xJu1SORdDobiVKmhhfSaqgTyullX1hFmcxAA4y6Ud33hY1dhIP~UTAlW~IV8Y-zSliAts8_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A
><https://d18ky98rnyall9.cloudfront.net/_410e934e6553ac56409b2cb7096a44aa_SCC.txt?Expires=1478649600&Signature=YC0OjTn4hmzWpzAMw3WkQjXgxkAc0qhZrCd07JAPtud3dGQqywpQgkAASf-bWX6qgGafVvObciJ3ww-7wbNqY0YhWxwcg-IxmCnz1xJu1SORdDobiVKmhhfSaqgTyullX1hFmcxAA4y6Ud33hY1dhIP~UTAlW~IV8Y-zSliAts8_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A>
>
>Data discription provided as follow:
>https://www.coursera.org/learn/algorithm-design-analysis/exam/rOtFq/programming-assignment-4
><https://www.coursera.org/learn/algorithm-design-analysis/exam/rOtFq/programming-assignment-4>
>
>
>Below is our code:
>
>options(expressions=500000)
>#Prepare input test file  
>g=read.table("Downloads/scc.txt")
>x<<-as.matrix(g)
>remove(g)
>vector.is.empty<<-function(x) {return(length(x)==0)}
>'%!in%' <- function(x,y)!('%in%'(x,y))
>#g<-c(2,1,3,1,3,4,4,2,5,4,5,6,6,7,7,8,8,9,2,3)
>#x<<-matrix(g,nrow=10,ncol=2, byrow=TRUE)
>#g<-c(1,4,2,8,3,6,4,7,5,2,6,9,7,1,8,5,8,6,9,3,9,7,10,2)
>#x<<-matrix(g,nrow=12,ncol=2, byrow=TRUE)
>#g<-c(1,2,2,3,2,4,3,4,3,5,4,1,4,13,5,6,6,7,7,8,8,9,9,6,10,9,10,11,11,8,11,12,12,13,12,14,13,10,14,15)
>#x<<-matrix(g,20,2,byrow=TRUE)
>
>u1<-unique(x[,1])
>u2<-unique(x[,2])
>u<-c(u1,u2)
>n<<-length(unique(u))
>remove(u1,u2,u)
>
>G <<- vector("list", length=n)
>G_REV <<- vector("list", length=n)
>P = numeric(n)
>FT = numeric(n)
>
>for (i in 1:nrow(x)) {
>  a = x[i,1]
>  b = x[i,2]
>  #for G
>  if (is.null(G[[a]])) {
>    G[[a]] = c(b)
>  } else {
>    G[[a]] = c(G[[a]], b)
>  }
>  if (is.null(G[[b]])) {
>    G[[b]] = numeric()
>  }
>  #for G_VEV
>  if (is.null(G_REV[[b]])) {
>    G_REV[[b]] = c(a)
>  } else {
>    G_REV[[b]] = c(G_REV[[b]], a)
>  }
>  if (is.null(G_REV[[a]])) {
>    G_REV[[a]] = numeric()
>  }
>}
>
>G_TEMP <<- G_REV
>
>#G_REV<<-x[,c(2,1)]
>
>#P<<-matrix(c(1:n,rep(0,2*n)),nrow=n,ncol=3) #tracking whether vertex
>is explored or not
>
>#colnames(P)<-c("node","f","parent_vertex")
>explore<<-numeric()
>assign("time",0,envir=globalenv())
>#Algorithm -DFS
>
>DFS_LOOP<<-function(G){
>  counter = n 
>  for(i in n : 1){
>    if (i < counter) {
>      counter = counter - 1000;
>      print(i);
>    }
>    if (i %in% explore){next}
>    else {
>      DFS(i)
>    }
>  }
>}
>
>DFS<<-function(i){
>  #if(time>=n){return(p)}
>  #else{
>  #print(c("i=",i))
>  explore<<-c(explore,i)
>  #print(c("explore",explore))
>  
>  #P[i,2] <<- 1 # gray
>  v=G_TEMP[[i]]
>  
>  #print(c("v=",v))
>  if (vector.is.empty(v) ==TRUE){
>    len=1
>    v=i
>  }
>  
>  if(vector.is.empty(v)==FALSE){
>    len=length(v)
>  }
>  
>  for(j in 1: len){
>    if(v[j] %!in% explore){
>      DFS(v[j])
>      #P[v[j],3] <<-i
>      P[v[j]] <<- i
>      # print(c("child",j,"parent",i))
>    }
>  }
>  
>  time<<-time + 1
>  FT[i] <<- time
>  #P[i,2] <<- time
>  #P[i,2] <<- 2 #black
>  # } <<-else
>}
>print('Starting DFS_loop on G_REV')
>DFS_LOOP(G_REV)
>###################################################
>#temp0<-matrix(1:n,n,1)
># temp1<-P[,c(1,2)]
># colnames(temp1)<-c("before","after")
># temp1<-as.data.frame(temp1)
># colnames(x)<-c("vertex","edge")
># X<-as.data.frame(x)
># X_NEW<-merge(x=X,y=temp1,by.x="vertex",by.y="before")
># remove(X)
># names(X_NEW)[names(X_NEW)=="after"]<-"vertex_new"
># X_NEW2<-merge(x=X_NEW,y=temp1,by.x="edge",by.y="before")
># remove(X_NEW,temp1)
># names(X_NEW2)[names(X_NEW2)=="after"]<-"edge_new"
># G2<-as.matrix(X_NEW2)
># remove(X_NEW2)
># G2<-G2[,c(3,4)]
># u1<-unique(G2[,1])
># u2<-unique(G2[,2])
># u<-c(u1,u2)
># n<<-length(unique(u))
># remove(u1,u2,u)
>
>FT_SORTED = numeric(n)
>for (i in length(FT):1) {
>  finish_time = FT[i]
>  FT_SORTED[finish_time] = i
>}
>
>P = numeric(n)
>FT = numeric(n)
>
>#P<<-matrix(c(1:n,rep(0,2*n)),nrow=n,ncol=3) #tracking whether vertex
>is explored or not
>#colnames(P)<-c("vertex","f","parent_vertex")
>explore<<-numeric()
>assign("time",0,envir=globalenv())
>
>print('Starting DFS_loop on G')
>#DFS_LOOP(G2)#2nd DFS
>explore<<-numeric()
>
>G_TEMP <<- G
>
>for (i in length(FT_SORTED):1) { 
>  k = FT_SORTED[i]
>  if (k %!in% explore) {
>    DFS(k)
>  }
>}
>
>mscc_temp = which(P==0)
>scc_temp=FT[mscc_temp]
>#scc_temp<-P[P[,3]==0,2]
>scc_temp=sort(scc_temp,decreasing=TRUE)
>m=length(scc_temp)
>scc=numeric()
>for (i in 1:(m-1)){
>  scc[i]=scc_temp[i]-scc_temp[i+1]
>}
>scc[m]<-scc_temp[m]
>scc_top_5<-tail(sort(scc),5)
>
>
>Thanks,
>Jing
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From tring at gvdnet.dk  Mon Nov  7 10:09:02 2016
From: tring at gvdnet.dk (Troels Ring)
Date: Mon, 7 Nov 2016 10:09:02 +0100
Subject: [R] Rmpfr into data.frame
Message-ID: <f7299dfc-3290-fb24-e39d-f21952bde296@gvdnet.dk>

Dear friends - Windows, R version 3.2.1

I wanted to make a ggplot2 using Rmpfr high precision data - but cannot 
make the data into a data.frame, as wanted by ggplot2. Hence

library(Rmpfr)
a <- mpfr(1,120)
b <- mpfr(2,120)
dff <- data.frame(a=a,b=b)

elicits errors

Error in as.data.frame.default(x[[i]], optional = TRUE) :
   cannot force class ?structure("mpfr1", package = "Rmpfr")? into 
data.frame

(my translation of Danish error)

How can I do it best?


From maechler at stat.math.ethz.ch  Mon Nov  7 12:00:19 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 7 Nov 2016 12:00:19 +0100
Subject: [R] Rmpfr into data.frame
In-Reply-To: <f7299dfc-3290-fb24-e39d-f21952bde296@gvdnet.dk>
References: <f7299dfc-3290-fb24-e39d-f21952bde296@gvdnet.dk>
Message-ID: <22560.24259.114932.319436@stat.math.ethz.ch>

>>>>> Troels Ring <tring at gvdnet.dk>
>>>>>     on Mon, 7 Nov 2016 10:09:02 +0100 writes:

    > Dear friends - Windows, R version 3.2.1
    > I wanted to make a ggplot2 using Rmpfr high precision data - but cannot 
    > make the data into a data.frame, as wanted by ggplot2. Hence

    > library(Rmpfr)
    > a <- mpfr(1,120)
    > b <- mpfr(2,120)
    > dff <- data.frame(a=a,b=b)

    > elicits errors

    > Error in as.data.frame.default(x[[i]], optional = TRUE) :
    > cannot force class ?structure("mpfr1", package = "Rmpfr")? into 
    > data.frame

ggplot2 nor any other "R-based" (*) plotting engine can make use
of the extra precision in the mpfr numbers.

So for plotting (and other purposes), there's the   asNumeric(.)
function in Rmpfr, which keeps Rmpfr - arrays or matrices.
For the present case (of mpfr-vectors with no attributes), you
can also use  as.numeric() :

> (Pi <- Const("pi"))
1 'mpfr' number of precision  120   bits 
[1] 3.1415926535897932384626433832795028847
> asNumeric(Pi)
[1] 3.141593
> all.equal(pi, asNumeric(Pi), tol=1e-17)
[1] TRUE
> identical(asNumeric(Pi), as.numeric(Pi))
[1] TRUE
> 

and your case
    data.frame(a=as.numeric(a), b=as.numeric(b))

As author and maintainer of Rmpfr, I'm curious why you'd be
interested to use high-precision numbers in plots, because even
double precision is usually far from being needed for graphics.

Martin Maechler, ETH Zurich

--
*) in the sense of either relying on "base R"s packages 'graphics' or 'grid';
   the latter is used by lattice and ggplot2.


    > (my translation of Danish error)

    > How can I do it best?


From maechler at stat.math.ethz.ch  Mon Nov  7 12:08:23 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 7 Nov 2016 12:08:23 +0100
Subject: [R] Dealing with -Inf in a maximisation problem.
In-Reply-To: <CAF8bMcZ9cZ0+e4+pykrwqTHyL4c1+coaToM5qL2jOiNcS8r3eg@mail.gmail.com>
References: <f2617dd1-3958-7c31-d2b6-02eec4153c9d@auckland.ac.nz>
	<CAF8bMcYjDEe90H4H46u41eio2n1FX89ML1-FP7K9b56s=qNn0Q@mail.gmail.com>
	<c915dd45-a057-bbe1-95b9-ce99befd44d8@auckland.ac.nz>
	<CAF8bMcZ9cZ0+e4+pykrwqTHyL4c1+coaToM5qL2jOiNcS8r3eg@mail.gmail.com>
Message-ID: <22560.24743.813869.416034@stat.math.ethz.ch>

>>>>> William Dunlap via R-help <r-help at r-project.org>
>>>>>     on Sun, 6 Nov 2016 20:53:17 -0800 writes:

    > Perhaps the C function Rf_logspace_sum(double *x, int n) would help in
    > computing log(b).  It computes log(sum(exp(x_i))) for i in 1..n, avoiding
    > unnecessary under- and overflow.

Indeed!

I had thought more than twice to also export it to the R level
notably as we have been using two R level versions in a package
I maintain ('copula'). They are vectorized there in a way that
seemed particularly useful to our (Marius Hofert and my) use cases.

More on this -- making these available in R, how exactly? --
probably should move to the R-devel list.

Thank you Bill for bringing it up!
Martin

    > Bill Dunlap
    > TIBCO Software
    > wdunlap tibco.com

    > On Sun, Nov 6, 2016 at 5:25 PM, Rolf Turner <r.turner at auckland.ac.nz> wrote:

    >> On 07/11/16 13:07, William Dunlap wrote:
    >> 
    >>> Have you tried reparameterizing, using logb (=log(b)) instead of b?
    >>> 
    >> 
    >> Uh, no.  I don't think that that makes any sense in my context.
    >> 
    >> The "b" values are probabilities and must satisfy a "sum-to-1"
    >> constraint.  To accommodate this constraint I re-parametrise via a
    >> "logistic" style parametrisation --- basically
    >> 
    >> b_i = exp(z_i)/[sum_j exp(z_j)], j = 1, ... n
    >> 
    >> with the parameters that the optimiser works with being z_1, ..., z_{n-1}
    >> (and with z_n == 0 for identifiability).  The objective function is of the
    >> form sum_i(a_i * log(b_i)), so I transform back
    >> from the z_i to the b_i in order calculate the value of the objective
    >> function.  But when the z_i get moderately large-negative, the b_i become
    >> numerically 0 and then log(b_i) becomes -Inf.  And the optimiser falls over.
    >> 
    >> cheers,
    >> 
    >> Rolf
    >> 
    >> 
    >>> Bill Dunlap
    >>> TIBCO Software
    >>> wdunlap tibco.com <http://tibco.com>
    >>> 
    >>> On Sun, Nov 6, 2016 at 1:17 PM, Rolf Turner <r.turner at auckland.ac.nz
    >>> <mailto:r.turner at auckland.ac.nz>> wrote:
    >>> 
    >>> 
    >>> I am trying to deal with a maximisation problem in which it is
    >>> possible for the objective function to (quite legitimately) return
    >>> the value -Inf, which causes the numerical optimisers that I have
    >>> tried to fall over.
    >>> 
    >>> The -Inf values arise from expressions of the form "a * log(b)",
    >>> with b = 0.  Under the *starting* values of the parameters, a must
    >>> equal equal 0 whenever b = 0, so we can legitimately say that a *
    >>> log(b) = 0 in these circumstances.  However as the maximisation
    >>> algorithm searches over parameters it is possible for b to take the
    >>> value 0 for values of
    >>> a that are strictly positive.  (The values of "a" do not change during
    >>> this search, although they *do* change between "successive searches".)
    >>> 
    >>> Clearly if one is *maximising* the objective then -Inf is not a value
    >>> of
    >>> particular interest, and we should be able to "move away".  But the
    >>> optimising function just stops.
    >>> 
    >>> It is also clear that "moving away" is not a simple task; you can't
    >>> estimate a gradient or Hessian at a point where the function value
    >>> is -Inf.
    >>> 
    >>> Can anyone suggest a way out of this dilemma, perhaps an optimiser
    >>> that is equipped to cope with -Inf values in some sneaky way?
    >>> 
    >>> Various ad hoc kludges spring to mind, but they all seem to be
    >>> fraught with peril.
    >>> 
    >>> I have tried changing the value returned by the objective function
    >>> from
    >>> "v" to exp(v) --- which maps -Inf to 0, which is nice and finite.
    >>> However this seemed to flatten out the objective surface too much,
    >>> and the search stalled at the 0 value, which is the antithesis of
    >>> optimal.
    >>> 
    >>> The problem arises in a context of applying the EM algorithm where
    >>> the M-step cannot be carried out explicitly, whence numerical
    >>> optimisation.
    >>> I can give more detail if anyone thinks that it could be relevant.
    >>> 
    >>> I would appreciate advice from younger and wiser heads! :-)
    >>> 
    >>> cheers,
    >>> 
    >>> Rolf Turner
    >>> 
    >>> --
    >>> Technical Editor ANZJS
    >>> Department of Statistics
    >>> University of Auckland
    >>> Phone: +64-9-373-7599 ext. 88276 <tel:%2B64-9-373-7599%20ext.%2
    088276> 
    >>> 
    >>> ______________________________________________
    >>> R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
    >>> To UNSUBSCRIBE and more, see
    >>> https://stat.ethz.ch/mailman/listinfo/r-help
    >>> <https://stat.ethz.ch/mailman/listinfo/r-help>
    >>> PLEASE do read the posting guide
    >>> http://www.R-project.org/posting-guide.html
    >>> <http://www.R-project.org/posting-guide.html>
    >>> and provide commented, minimal, self-contained, reproducible code.
    >>> 
    >>> 
    >>> 
    >> 
    >> --
    >> Technical Editor ANZJS
    >> Department of Statistics
    >> University of Auckland
    >> Phone: +64-9-373-7599 ext. 88276
    >> 

    > [[alternative HTML version deleted]]

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


From pgilbert902 at gmail.com  Mon Nov  7 17:45:20 2016
From: pgilbert902 at gmail.com (Paul Gilbert)
Date: Mon, 7 Nov 2016 11:45:20 -0500
Subject: [R] Dealing with -Inf in a maximisation problem.
In-Reply-To: <mailman.3.1478516401.21216.r-help@r-project.org>
References: <mailman.3.1478516401.21216.r-help@r-project.org>
Message-ID: <6364a394-fbc6-4740-8d5c-1bef90b48b86@gmail.com>

>
> I am trying to deal with a maximisation problem in which it is possible
> for the objective function to (quite legitimately) return the value
> -Inf,

(Just to add to the pedantic part of the discuss by those of us that do 
not qualify as younger and wiser:)

Setting log(0) to -Inf is often convenient but really I think the log 
function is undefined at zero, so I would not refer to this as "legitimate".

>which causes the numerical optimisers that I have tried to fall over.

In theory as well as practice. You need to have a function that is 
defined on the whole domain.

>
> The -Inf values arise from expressions of the form "a * log(b)", with b
> = 0.  Under the *starting* values of the parameters, a must equal equal
> 0 whenever b = 0, so we can legitimately say that a * log(b) = 0 in

This also is undefined and not "legitimate". I think there is no reason 
it should be equal zero. We tend to want to set it to the value we think 
of as the "limit": for a=0 the limit as b goes to zero would be zero, 
but the limit of a*(-inf) is -inf as a goes to zero.

So, you really do need to avoid zero because your function is not 
defined there, or find a redefinition that works properly at zero. I 
think you have a solution from another post.

Paul

> these circumstances.  However as the maximisation algorithm searches
> over parameters it is possible for b to take the value 0 for values of
> a that are strictly positive.  (The values of "a" do not change during
> this search, although they *do* change between "successive searches".)
>
> Clearly if one is *maximising* the objective then -Inf is not a value of
> particular interest, and we should be able to "move away".  But the
> optimising function just stops.
>
> It is also clear that "moving away" is not a simple task; you can't
> estimate a gradient or Hessian at a point where the function value is -Inf.
>
> Can anyone suggest a way out of this dilemma, perhaps an optimiser that
> is equipped to cope with -Inf values in some sneaky way?
>
> Various ad hoc kludges spring to mind, but they all seem to be fraught
> with peril.
>
> I have tried changing the value returned by the objective function from
> "v" to exp(v) --- which maps -Inf to 0, which is nice and finite.
> However this seemed to flatten out the objective surface too much, and
> the search stalled at the 0 value, which is the antithesis of optimal.
>
> The problem arises in a context of applying the EM algorithm where the
> M-step cannot be carried out explicitly, whence numerical optimisation.
> I can give more detail if anyone thinks that it could be relevant.
>
> I would appreciate advice from younger and wiser heads! :-)
>
> cheers,
>
> Rolf Turner
>
> -- Technical Editor ANZJS Department of Statistics University of
> Auckland Phone: +64-9-373-7599 ext. 88276


From danilo.carita at uniparthenope.it  Mon Nov  7 13:42:48 2016
From: danilo.carita at uniparthenope.it (danilo.carita at uniparthenope.it)
Date: Mon, 07 Nov 2016 13:42:48 +0100
Subject: [R] Three-component Negative Binomial Mixture: R code
Message-ID: <20161107134248.Horde.9trur60Ob_4h26wDm7GtSw1@webmail.uniparthenope.it>

I need a function for R software which computes a mixture of Negative
Binomial distributions with at least three components.

I found on another site the following function "mixnbinom". It works very
well, but it computes a mixture of only two components:


> mixnbinom=function(y,k1,mu1,k2,mu2,prob,eps=1/100000)
> {
>  new.parms=c(k1,mu1,k2,mu2,prob)
>  err=1
>  iter=1
>  maxiter=100
>  hist(y,probability=T,nclass=30,col="lightgrey",main="The EM algorithm")
>  xvals=seq(min(y),max(y),1)
>  lines(xvals,prob*dnbinom(xvals,size=k1,mu=mu1)+
>            (1-prob)*dnbinom(xvals,size=k2,mu=mu2),col="green")
>  while(err>eps){
>      if(iter<=maxiter){
>          lines(xvals,prob*dnbinom(xvals,size=k1,mu=mu1)+
>                    (1-prob)*dnbinom(xvals,size=k2,mu=mu2),lty=3)
>      }
>      bayes=(prob*dnbinom(y,size=k1,mu=mu1))/((prob*
>      dnbinom(y,size=k1,mu=mu1))+((1-prob)*dnbinom(y,size=k2,mu=mu2)))
>      mu1=sum(bayes*y)/sum(bayes)
>      mu2=sum((1-bayes)*y)/sum((1-bayes))
>      var1=sum(bayes*(y-mu1)^2)/sum(bayes)
>      var2=sum((1-bayes)*(y-mu2)^2)/sum((1-bayes))
>      k1=abs(mu1/((var1/mu1)-1))
>      k2=abs(mu2/((var2/mu2)-1))
>      prob=mean(bayes)
>      old.parms=new.parms
>      new.parms=c(k1,mu1,k2,mu2,prob)
>      err=max(abs((old.parms-new.parms)/new.parms))
>      iter=iter+1
>  }
>  lines(xvals,prob*dnbinom(xvals,size=k1,mu=mu1)+
>            (1-prob)*dnbinom(xvals,size=k2,mu=mu2),col="red")
>  print(list(k1=k1,mu1=mu1,k2=k2,mu2=mu2,p=prob,iter=iter,err=err))
> }


I would be grateful if someone can modify the previous function to
model a three-component mixture instead of a two-component one.

I also tried to look for a package which does the same job: I have
used the package "mixdist", but I am not able to set up a suitable set
of starting parameters for the function mix (they always converge to
zero). Hereafter, I found the package "DEXUS", but the related function
does not provide good estimates for the model, even in the event that
I already know what results I have to expect.

Any help is highly appreciated.


Danilo Carit?

-------------------------------------------------------------
Danilo Carit?

PhD Candidate
University of Naples "Parthenope"
Dipartimento di Studi Aziendali e Quantitativi
via G. Parisi, 13, 80132 Napoli - Italy


From varinsacha at yahoo.fr  Mon Nov  7 14:23:21 2016
From: varinsacha at yahoo.fr (varin sacha)
Date: Mon, 7 Nov 2016 13:23:21 +0000 (UTC)
Subject: [R] Problem with downloading library(Rcmdr)
In-Reply-To: <3A6DBF96-F028-40C9-9E39-2F8AA16F0073@comcast.net>
References: <496599242.1873911.1478370423476.ref@mail.yahoo.com>
	<496599242.1873911.1478370423476@mail.yahoo.com>
	<CF09170F-C599-4EDF-9C79-FE94B5490590@comcast.net>
	<753312018.1959901.1478385227937@mail.yahoo.com>
	<B0DDF401-3287-41D6-8428-BCECB61B465A@dcn.davis.ca.us>
	<540398699.2330409.1478438232697@mail.yahoo.com>
	<D3604677-C4C3-48CB-A932-1DCF20DF0376@dcn.davis.ca.us>
	<3A6DBF96-F028-40C9-9E39-2F8AA16F0073@comcast.net>
Message-ID: <1179901541.3733606.1478525001748@mail.yahoo.com>

David, Jeff,

Many thanks, 

I have installed an older version of R and rmle packages works.

Best





________________________________
De : David Winsemius <dwinsemius at comcast.net>
? : Jeff Newmiller <jdnewmil at dcn.davis.ca.us> 

oject.org>
Envoy? le : Dimanche 6 novembre 2016 16h35
Objet : Re: [R] Problem with downloading library(Rcmdr)



> On Nov 6, 2016, at 6:10 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> I am not aware of a "-1" option for R CMD INSTALL  [1]. Nor do I have a Macintosh.

It compiles from the MacGUI console using an ordinary call (no extra environment variables need be set) to install.packages, and then loads without error on my Mac running R 3.3.1 patched. (I do have a full set of tools and a current version of Rcpp, reference to which which showed up in the compiler messages.)  If I had been using the Unix command line I would have offer a full path to the *.tar.gz file and not used that "minus one" option.

The Imports line in the DESCRIPTION file:

Imports: MASS, quantreg, nlme, mgcv, stringr, magic, robustbase, Rcpp(>= 0.11.1)

-- 
David.
> 
> Do be prepared to keep reading the documentation once you get past this point, as the whole reason you are doing this is that the original maintainer chose to not fix whatever problems existed in that package, and I have never used that package. Those problems might be minor, or they might be extensive.
> 
> [1] https://cran.r-project.org/doc/manuals/r-release/R-admin.html#Installing-packages
> -- 
> Sent from my phone. Please excuse my brevity.
> 

te:
>> Hi Jeff,
>> 
>> Many thanks for your reply.I know that R packages can be downloaded as
>> "tar.gz" file for Mac OS X from CRAN sources. I have found on Internet
>> that the command is the following :
>> 
>> R CMD INSTALL -1 rlme_0.4.tar.gz
>> 
>> When I run that command on my R console, I get an error as you can read
>> here below.
>> 
>> What am I doing wrong ?
>> 
>> 
>>> R CMD INSTALL -1 rlme_0.4.tar.gz
>> Erreur : unexpected symbol in "R CMD"
>> 
>> 
>> ----- Mail original -----
>> De : Jeff Newmiller <jdnewmil at dcn.davis.ca.us>

>> <r-help at r-project.org>; David Winsemius <dwinsemius at comcast.net>
>> Cc : R-help Mailing List <r-help at r-project.org>
>> Envoy? le : Dimanche 6 novembre 2016 0h07
>> Objet : Re: [R] Problem with downloading library(Rcmdr)
>> 
>> Google is your friend... try searching for "CRAN rlme".
>> 
>> The bad thing about contributed packages is that sometimes the
>> maintainers stop maintaining their packages. The good thing is that the
>> source is still there so you can go fix it if you really need it. 
> 

David Winsemius
Alameda, CA, USA


From ginojay at gmail.com  Mon Nov  7 16:49:48 2016
From: ginojay at gmail.com (Jie C)
Date: Mon, 7 Nov 2016 10:49:48 -0500
Subject: [R] Kosaraju's SCC Algorithm Running
In-Reply-To: <93CE03A9-AF34-441C-ADD4-4E7D813E42D6@dcn.davis.ca.us>
References: <F0E571C1-E334-437D-BB53-D7A45A8F55DF@gmail.com>
	<93CE03A9-AF34-441C-ADD4-4E7D813E42D6@dcn.davis.ca.us>
Message-ID: <CAC-efpu7MxDtETmsK560w07qwWXY4M+6CPm4sdGsLPW-p3VhtA@mail.gmail.com>

Hi Jeff,

Thanks for your reply! We are definitely not seeking for help with the
assignment per se. Maybe we should have given you a simpler code to just
illustrate the problem. We found this to be an interesting case for
illustrating the recursion limit of R. For other languages such as Python,
Java, and C++, it seems that they are less likely to hit the problem as R
does. Even if they do, it is relatively easy to increase the recursion
depth to the desired numbers. I want to clarify that our purpose is trying
to understand the limitation of R language in handling big computational
problems that relies on deep recursion. If you could point us to technical
documents regarding this, that would be highly appreciated. Also, we could
definitely re-post a simple code snippet that follows the Posting Guide to
illustrate that R hits the recursion limitation. Looking forward to your
advise!

Thanks,

Jie





On Mon, Nov 7, 2016 at 2:28 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Please read the Posting Guide mentioned at the bottom of this and every
> post, which warns you that homework is off topic on this mailing list. Use
> the support provided by your institution of learning (Coursera in this
> case).
> --
> Sent from my phone. Please excuse my brevity.
>
> On November 6, 2016 8:12:38 PM PST, Megan <megan.fight at gmail.com> wrote:
> >To whom it may concerns,
> >
> >We encountered stack overflow issues when we implemented DFS(depth
> >first search) algorithm on a directed graph having 800,000+ vertices
> >and millions of edges.  The purpose of running DFS is to use Kosaraju
> >Algorithm to calculate the size of SCC(strongly connected componment)
> >in the graph. This is an assignment from Coursea.org
> ><http://coursea.org/>. We know the maximum size of SCC in the graph is
> >434,821, which means the maximum recursion depth during code running is
> >434,821. We know it is a large calculation behind the scene, therefore,
> >we?ve done the below pre-setup hopefully to solve the stack overflow
> >issue. However, we have not got the luck yet. We?ve tried running on
> >both R and RStudio.
> >
> >We would really appreciate if someone in your team can help to
> >investigate. We can?t wait to see if our code is working in R.
> >
> >System Information:
> >Model Name:    MacBook Pro
> >  Model Identifier:    MacBookPro11,1
> >  Processor Name:      Intel Core i5
> >  Processor Speed:     2.4 GHz
> >  Number of Processors:        1
> >  Total Number of Cores:       2
> >  L2 Cache (per Core): 256 KB
> >  L3 Cache:    3 MB
> >  Memory:      8 GB
> >
> >System settings we have tried:
> >1. ulimit- s 65000 (to increase stack size before starting up R)
> >2. R --max-ppsize =500000 (start R with max ppsize)
> >3. options(expression=500000)
> >
> >
> >The data we used can be found on
> >https://d18ky98rnyall9.cloudfront.net/_410e934e6553ac56409b2cb7096a44
> aa_SCC.txt?Expires=1478649600&Signature=YC0OjTn4hmzWpzAMw3WkQjXgxkAc0q
> hZrCd07JAPtud3dGQqywpQgkAASf-bWX6qgGafVvObciJ3ww-7wbNqY0YhWxwcg-
> IxmCnz1xJu1SORdDobiVKmhhfSaqgTyullX1hFmcxAA4y6Ud33hY1dhIP~
> UTAlW~IV8Y-zSliAts8_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A
> ><https://d18ky98rnyall9.cloudfront.net/_410e934e6553ac56409b2cb7096a44
> aa_SCC.txt?Expires=1478649600&Signature=YC0OjTn4hmzWpzAMw3WkQjXgxkAc0q
> hZrCd07JAPtud3dGQqywpQgkAASf-bWX6qgGafVvObciJ3ww-7wbNqY0YhWxwcg-
> IxmCnz1xJu1SORdDobiVKmhhfSaqgTyullX1hFmcxAA4y6Ud33hY1dhIP~
> UTAlW~IV8Y-zSliAts8_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A>
> >
> >Data discription provided as follow:
> >https://www.coursera.org/learn/algorithm-design-analysis/exam/rOtFq/
> programming-assignment-4
> ><https://www.coursera.org/learn/algorithm-design-analysis/exam/rOtFq/
> programming-assignment-4>
> >
> >
> >Below is our code:
> >
> >options(expressions=500000)
> >#Prepare input test file
> >g=read.table("Downloads/scc.txt")
> >x<<-as.matrix(g)
> >remove(g)
> >vector.is.empty<<-function(x) {return(length(x)==0)}
> >'%!in%' <- function(x,y)!('%in%'(x,y))
> >#g<-c(2,1,3,1,3,4,4,2,5,4,5,6,6,7,7,8,8,9,2,3)
> >#x<<-matrix(g,nrow=10,ncol=2, byrow=TRUE)
> >#g<-c(1,4,2,8,3,6,4,7,5,2,6,9,7,1,8,5,8,6,9,3,9,7,10,2)
> >#x<<-matrix(g,nrow=12,ncol=2, byrow=TRUE)
> >#g<-c(1,2,2,3,2,4,3,4,3,5,4,1,4,13,5,6,6,7,7,8,8,9,9,6,10,
> 9,10,11,11,8,11,12,12,13,12,14,13,10,14,15)
> >#x<<-matrix(g,20,2,byrow=TRUE)
> >
> >u1<-unique(x[,1])
> >u2<-unique(x[,2])
> >u<-c(u1,u2)
> >n<<-length(unique(u))
> >remove(u1,u2,u)
> >
> >G <<- vector("list", length=n)
> >G_REV <<- vector("list", length=n)
> >P = numeric(n)
> >FT = numeric(n)
> >
> >for (i in 1:nrow(x)) {
> >  a = x[i,1]
> >  b = x[i,2]
> >  #for G
> >  if (is.null(G[[a]])) {
> >    G[[a]] = c(b)
> >  } else {
> >    G[[a]] = c(G[[a]], b)
> >  }
> >  if (is.null(G[[b]])) {
> >    G[[b]] = numeric()
> >  }
> >  #for G_VEV
> >  if (is.null(G_REV[[b]])) {
> >    G_REV[[b]] = c(a)
> >  } else {
> >    G_REV[[b]] = c(G_REV[[b]], a)
> >  }
> >  if (is.null(G_REV[[a]])) {
> >    G_REV[[a]] = numeric()
> >  }
> >}
> >
> >G_TEMP <<- G_REV
> >
> >#G_REV<<-x[,c(2,1)]
> >
> >#P<<-matrix(c(1:n,rep(0,2*n)),nrow=n,ncol=3) #tracking whether vertex
> >is explored or not
> >
> >#colnames(P)<-c("node","f","parent_vertex")
> >explore<<-numeric()
> >assign("time",0,envir=globalenv())
> >#Algorithm -DFS
> >
> >DFS_LOOP<<-function(G){
> >  counter = n
> >  for(i in n : 1){
> >    if (i < counter) {
> >      counter = counter - 1000;
> >      print(i);
> >    }
> >    if (i %in% explore){next}
> >    else {
> >      DFS(i)
> >    }
> >  }
> >}
> >
> >DFS<<-function(i){
> >  #if(time>=n){return(p)}
> >  #else{
> >  #print(c("i=",i))
> >  explore<<-c(explore,i)
> >  #print(c("explore",explore))
> >
> >  #P[i,2] <<- 1 # gray
> >  v=G_TEMP[[i]]
> >
> >  #print(c("v=",v))
> >  if (vector.is.empty(v) ==TRUE){
> >    len=1
> >    v=i
> >  }
> >
> >  if(vector.is.empty(v)==FALSE){
> >    len=length(v)
> >  }
> >
> >  for(j in 1: len){
> >    if(v[j] %!in% explore){
> >      DFS(v[j])
> >      #P[v[j],3] <<-i
> >      P[v[j]] <<- i
> >      # print(c("child",j,"parent",i))
> >    }
> >  }
> >
> >  time<<-time + 1
> >  FT[i] <<- time
> >  #P[i,2] <<- time
> >  #P[i,2] <<- 2 #black
> >  # } <<-else
> >}
> >print('Starting DFS_loop on G_REV')
> >DFS_LOOP(G_REV)
> >###################################################
> >#temp0<-matrix(1:n,n,1)
> ># temp1<-P[,c(1,2)]
> ># colnames(temp1)<-c("before","after")
> ># temp1<-as.data.frame(temp1)
> ># colnames(x)<-c("vertex","edge")
> ># X<-as.data.frame(x)
> ># X_NEW<-merge(x=X,y=temp1,by.x="vertex",by.y="before")
> ># remove(X)
> ># names(X_NEW)[names(X_NEW)=="after"]<-"vertex_new"
> ># X_NEW2<-merge(x=X_NEW,y=temp1,by.x="edge",by.y="before")
> ># remove(X_NEW,temp1)
> ># names(X_NEW2)[names(X_NEW2)=="after"]<-"edge_new"
> ># G2<-as.matrix(X_NEW2)
> ># remove(X_NEW2)
> ># G2<-G2[,c(3,4)]
> ># u1<-unique(G2[,1])
> ># u2<-unique(G2[,2])
> ># u<-c(u1,u2)
> ># n<<-length(unique(u))
> ># remove(u1,u2,u)
> >
> >FT_SORTED = numeric(n)
> >for (i in length(FT):1) {
> >  finish_time = FT[i]
> >  FT_SORTED[finish_time] = i
> >}
> >
> >P = numeric(n)
> >FT = numeric(n)
> >
> >#P<<-matrix(c(1:n,rep(0,2*n)),nrow=n,ncol=3) #tracking whether vertex
> >is explored or not
> >#colnames(P)<-c("vertex","f","parent_vertex")
> >explore<<-numeric()
> >assign("time",0,envir=globalenv())
> >
> >print('Starting DFS_loop on G')
> >#DFS_LOOP(G2)#2nd DFS
> >explore<<-numeric()
> >
> >G_TEMP <<- G
> >
> >for (i in length(FT_SORTED):1) {
> >  k = FT_SORTED[i]
> >  if (k %!in% explore) {
> >    DFS(k)
> >  }
> >}
> >
> >mscc_temp = which(P==0)
> >scc_temp=FT[mscc_temp]
> >#scc_temp<-P[P[,3]==0,2]
> >scc_temp=sort(scc_temp,decreasing=TRUE)
> >m=length(scc_temp)
> >scc=numeric()
> >for (i in 1:(m-1)){
> >  scc[i]=scc_temp[i]-scc_temp[i+1]
> >}
> >scc[m]<-scc_temp[m]
> >scc_top_5<-tail(sort(scc),5)
> >
> >
> >Thanks,
> >Jing
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Best regards,

Jie

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Mon Nov  7 18:30:48 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 7 Nov 2016 09:30:48 -0800
Subject: [R] Kosaraju's SCC Algorithm Running
In-Reply-To: <CAC-efpu7MxDtETmsK560w07qwWXY4M+6CPm4sdGsLPW-p3VhtA@mail.gmail.com>
References: <F0E571C1-E334-437D-BB53-D7A45A8F55DF@gmail.com>
	<93CE03A9-AF34-441C-ADD4-4E7D813E42D6@dcn.davis.ca.us>
	<CAC-efpu7MxDtETmsK560w07qwWXY4M+6CPm4sdGsLPW-p3VhtA@mail.gmail.com>
Message-ID: <CAGxFJbQzghsmTv-5oH7FkgPkR9fF0=jz+A6rz2evd8s=uprB2g@mail.gmail.com>

See here -- found by googling "recursion limits in R"

http://stackoverflow.com/questions/26797537/r-programming-level-of-allowed-recursion-depth-differs-when-calling-a-local-hel

-- Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Nov 7, 2016 at 7:49 AM, Jie C <ginojay at gmail.com> wrote:
> Hi Jeff,
>
> Thanks for your reply! We are definitely not seeking for help with the
> assignment per se. Maybe we should have given you a simpler code to just
> illustrate the problem. We found this to be an interesting case for
> illustrating the recursion limit of R. For other languages such as Python,
> Java, and C++, it seems that they are less likely to hit the problem as R
> does. Even if they do, it is relatively easy to increase the recursion
> depth to the desired numbers. I want to clarify that our purpose is trying
> to understand the limitation of R language in handling big computational
> problems that relies on deep recursion. If you could point us to technical
> documents regarding this, that would be highly appreciated. Also, we could
> definitely re-post a simple code snippet that follows the Posting Guide to
> illustrate that R hits the recursion limitation. Looking forward to your
> advise!
>
> Thanks,
>
> Jie
>
>
>
>
>
> On Mon, Nov 7, 2016 at 2:28 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
>
>> Please read the Posting Guide mentioned at the bottom of this and every
>> post, which warns you that homework is off topic on this mailing list. Use
>> the support provided by your institution of learning (Coursera in this
>> case).
>> --
>> Sent from my phone. Please excuse my brevity.
>>
>> On November 6, 2016 8:12:38 PM PST, Megan <megan.fight at gmail.com> wrote:
>> >To whom it may concerns,
>> >
>> >We encountered stack overflow issues when we implemented DFS(depth
>> >first search) algorithm on a directed graph having 800,000+ vertices
>> >and millions of edges.  The purpose of running DFS is to use Kosaraju
>> >Algorithm to calculate the size of SCC(strongly connected componment)
>> >in the graph. This is an assignment from Coursea.org
>> ><http://coursea.org/>. We know the maximum size of SCC in the graph is
>> >434,821, which means the maximum recursion depth during code running is
>> >434,821. We know it is a large calculation behind the scene, therefore,
>> >we?ve done the below pre-setup hopefully to solve the stack overflow
>> >issue. However, we have not got the luck yet. We?ve tried running on
>> >both R and RStudio.
>> >
>> >We would really appreciate if someone in your team can help to
>> >investigate. We can?t wait to see if our code is working in R.
>> >
>> >System Information:
>> >Model Name:    MacBook Pro
>> >  Model Identifier:    MacBookPro11,1
>> >  Processor Name:      Intel Core i5
>> >  Processor Speed:     2.4 GHz
>> >  Number of Processors:        1
>> >  Total Number of Cores:       2
>> >  L2 Cache (per Core): 256 KB
>> >  L3 Cache:    3 MB
>> >  Memory:      8 GB
>> >
>> >System settings we have tried:
>> >1. ulimit- s 65000 (to increase stack size before starting up R)
>> >2. R --max-ppsize =500000 (start R with max ppsize)
>> >3. options(expression=500000)
>> >
>> >
>> >The data we used can be found on
>> >https://d18ky98rnyall9.cloudfront.net/_410e934e6553ac56409b2cb7096a44
>> aa_SCC.txt?Expires=1478649600&Signature=YC0OjTn4hmzWpzAMw3WkQjXgxkAc0q
>> hZrCd07JAPtud3dGQqywpQgkAASf-bWX6qgGafVvObciJ3ww-7wbNqY0YhWxwcg-
>> IxmCnz1xJu1SORdDobiVKmhhfSaqgTyullX1hFmcxAA4y6Ud33hY1dhIP~
>> UTAlW~IV8Y-zSliAts8_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A
>> ><https://d18ky98rnyall9.cloudfront.net/_410e934e6553ac56409b2cb7096a44
>> aa_SCC.txt?Expires=1478649600&Signature=YC0OjTn4hmzWpzAMw3WkQjXgxkAc0q
>> hZrCd07JAPtud3dGQqywpQgkAASf-bWX6qgGafVvObciJ3ww-7wbNqY0YhWxwcg-
>> IxmCnz1xJu1SORdDobiVKmhhfSaqgTyullX1hFmcxAA4y6Ud33hY1dhIP~
>> UTAlW~IV8Y-zSliAts8_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A>
>> >
>> >Data discription provided as follow:
>> >https://www.coursera.org/learn/algorithm-design-analysis/exam/rOtFq/
>> programming-assignment-4
>> ><https://www.coursera.org/learn/algorithm-design-analysis/exam/rOtFq/
>> programming-assignment-4>
>> >
>> >
>> >Below is our code:
>> >
>> >options(expressions=500000)
>> >#Prepare input test file
>> >g=read.table("Downloads/scc.txt")
>> >x<<-as.matrix(g)
>> >remove(g)
>> >vector.is.empty<<-function(x) {return(length(x)==0)}
>> >'%!in%' <- function(x,y)!('%in%'(x,y))
>> >#g<-c(2,1,3,1,3,4,4,2,5,4,5,6,6,7,7,8,8,9,2,3)
>> >#x<<-matrix(g,nrow=10,ncol=2, byrow=TRUE)
>> >#g<-c(1,4,2,8,3,6,4,7,5,2,6,9,7,1,8,5,8,6,9,3,9,7,10,2)
>> >#x<<-matrix(g,nrow=12,ncol=2, byrow=TRUE)
>> >#g<-c(1,2,2,3,2,4,3,4,3,5,4,1,4,13,5,6,6,7,7,8,8,9,9,6,10,
>> 9,10,11,11,8,11,12,12,13,12,14,13,10,14,15)
>> >#x<<-matrix(g,20,2,byrow=TRUE)
>> >
>> >u1<-unique(x[,1])
>> >u2<-unique(x[,2])
>> >u<-c(u1,u2)
>> >n<<-length(unique(u))
>> >remove(u1,u2,u)
>> >
>> >G <<- vector("list", length=n)
>> >G_REV <<- vector("list", length=n)
>> >P = numeric(n)
>> >FT = numeric(n)
>> >
>> >for (i in 1:nrow(x)) {
>> >  a = x[i,1]
>> >  b = x[i,2]
>> >  #for G
>> >  if (is.null(G[[a]])) {
>> >    G[[a]] = c(b)
>> >  } else {
>> >    G[[a]] = c(G[[a]], b)
>> >  }
>> >  if (is.null(G[[b]])) {
>> >    G[[b]] = numeric()
>> >  }
>> >  #for G_VEV
>> >  if (is.null(G_REV[[b]])) {
>> >    G_REV[[b]] = c(a)
>> >  } else {
>> >    G_REV[[b]] = c(G_REV[[b]], a)
>> >  }
>> >  if (is.null(G_REV[[a]])) {
>> >    G_REV[[a]] = numeric()
>> >  }
>> >}
>> >
>> >G_TEMP <<- G_REV
>> >
>> >#G_REV<<-x[,c(2,1)]
>> >
>> >#P<<-matrix(c(1:n,rep(0,2*n)),nrow=n,ncol=3) #tracking whether vertex
>> >is explored or not
>> >
>> >#colnames(P)<-c("node","f","parent_vertex")
>> >explore<<-numeric()
>> >assign("time",0,envir=globalenv())
>> >#Algorithm -DFS
>> >
>> >DFS_LOOP<<-function(G){
>> >  counter = n
>> >  for(i in n : 1){
>> >    if (i < counter) {
>> >      counter = counter - 1000;
>> >      print(i);
>> >    }
>> >    if (i %in% explore){next}
>> >    else {
>> >      DFS(i)
>> >    }
>> >  }
>> >}
>> >
>> >DFS<<-function(i){
>> >  #if(time>=n){return(p)}
>> >  #else{
>> >  #print(c("i=",i))
>> >  explore<<-c(explore,i)
>> >  #print(c("explore",explore))
>> >
>> >  #P[i,2] <<- 1 # gray
>> >  v=G_TEMP[[i]]
>> >
>> >  #print(c("v=",v))
>> >  if (vector.is.empty(v) ==TRUE){
>> >    len=1
>> >    v=i
>> >  }
>> >
>> >  if(vector.is.empty(v)==FALSE){
>> >    len=length(v)
>> >  }
>> >
>> >  for(j in 1: len){
>> >    if(v[j] %!in% explore){
>> >      DFS(v[j])
>> >      #P[v[j],3] <<-i
>> >      P[v[j]] <<- i
>> >      # print(c("child",j,"parent",i))
>> >    }
>> >  }
>> >
>> >  time<<-time + 1
>> >  FT[i] <<- time
>> >  #P[i,2] <<- time
>> >  #P[i,2] <<- 2 #black
>> >  # } <<-else
>> >}
>> >print('Starting DFS_loop on G_REV')
>> >DFS_LOOP(G_REV)
>> >###################################################
>> >#temp0<-matrix(1:n,n,1)
>> ># temp1<-P[,c(1,2)]
>> ># colnames(temp1)<-c("before","after")
>> ># temp1<-as.data.frame(temp1)
>> ># colnames(x)<-c("vertex","edge")
>> ># X<-as.data.frame(x)
>> ># X_NEW<-merge(x=X,y=temp1,by.x="vertex",by.y="before")
>> ># remove(X)
>> ># names(X_NEW)[names(X_NEW)=="after"]<-"vertex_new"
>> ># X_NEW2<-merge(x=X_NEW,y=temp1,by.x="edge",by.y="before")
>> ># remove(X_NEW,temp1)
>> ># names(X_NEW2)[names(X_NEW2)=="after"]<-"edge_new"
>> ># G2<-as.matrix(X_NEW2)
>> ># remove(X_NEW2)
>> ># G2<-G2[,c(3,4)]
>> ># u1<-unique(G2[,1])
>> ># u2<-unique(G2[,2])
>> ># u<-c(u1,u2)
>> ># n<<-length(unique(u))
>> ># remove(u1,u2,u)
>> >
>> >FT_SORTED = numeric(n)
>> >for (i in length(FT):1) {
>> >  finish_time = FT[i]
>> >  FT_SORTED[finish_time] = i
>> >}
>> >
>> >P = numeric(n)
>> >FT = numeric(n)
>> >
>> >#P<<-matrix(c(1:n,rep(0,2*n)),nrow=n,ncol=3) #tracking whether vertex
>> >is explored or not
>> >#colnames(P)<-c("vertex","f","parent_vertex")
>> >explore<<-numeric()
>> >assign("time",0,envir=globalenv())
>> >
>> >print('Starting DFS_loop on G')
>> >#DFS_LOOP(G2)#2nd DFS
>> >explore<<-numeric()
>> >
>> >G_TEMP <<- G
>> >
>> >for (i in length(FT_SORTED):1) {
>> >  k = FT_SORTED[i]
>> >  if (k %!in% explore) {
>> >    DFS(k)
>> >  }
>> >}
>> >
>> >mscc_temp = which(P==0)
>> >scc_temp=FT[mscc_temp]
>> >#scc_temp<-P[P[,3]==0,2]
>> >scc_temp=sort(scc_temp,decreasing=TRUE)
>> >m=length(scc_temp)
>> >scc=numeric()
>> >for (i in 1:(m-1)){
>> >  scc[i]=scc_temp[i]-scc_temp[i+1]
>> >}
>> >scc[m]<-scc_temp[m]
>> >scc_top_5<-tail(sort(scc),5)
>> >
>> >
>> >Thanks,
>> >Jing
>> >       [[alternative HTML version deleted]]
>> >
>> >______________________________________________
>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
>
> --
> Best regards,
>
> Jie
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Mon Nov  7 19:04:12 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 7 Nov 2016 10:04:12 -0800
Subject: [R] Three-component Negative Binomial Mixture: R code
In-Reply-To: <20161107134248.Horde.9trur60Ob_4h26wDm7GtSw1@webmail.uniparthenope.it>
References: <20161107134248.Horde.9trur60Ob_4h26wDm7GtSw1@webmail.uniparthenope.it>
Message-ID: <CAGxFJbR699Ca_8h=tVXa1omALaKSEiv9r39G3bkRc6RskoeJgA@mail.gmail.com>

Opinion only (and therefore ignorable):

Unless you have a lot (?) of data, fitting such a model successfully
is likely to be very challenging. I suggest that you consult a local
statistical expert to see if you might take a different approach.



On Mon, Nov 7, 2016 at 4:42 AM,  <danilo.carita at uniparthenope.it> wrote:
> I need a function for R software which computes a mixture of Negative
> Binomial distributions with at least three components.
>
> I found on another site the following function "mixnbinom". It works very
> well, but it computes a mixture of only two components:
>
>
>> mixnbinom=function(y,k1,mu1,k2,mu2,prob,eps=1/100000)
>> {
>>  new.parms=c(k1,mu1,k2,mu2,prob)
>>  err=1
>>  iter=1
>>  maxiter=100
>>  hist(y,probability=T,nclass=30,col="lightgrey",main="The EM algorithm")
>>  xvals=seq(min(y),max(y),1)
>>  lines(xvals,prob*dnbinom(xvals,size=k1,mu=mu1)+
>>            (1-prob)*dnbinom(xvals,size=k2,mu=mu2),col="green")
>>  while(err>eps){
>>      if(iter<=maxiter){
>>          lines(xvals,prob*dnbinom(xvals,size=k1,mu=mu1)+
>>                    (1-prob)*dnbinom(xvals,size=k2,mu=mu2),lty=3)
>>      }
>>      bayes=(prob*dnbinom(y,size=k1,mu=mu1))/((prob*
>>      dnbinom(y,size=k1,mu=mu1))+((1-prob)*dnbinom(y,size=k2,mu=mu2)))
>>      mu1=sum(bayes*y)/sum(bayes)
>>      mu2=sum((1-bayes)*y)/sum((1-bayes))
>>      var1=sum(bayes*(y-mu1)^2)/sum(bayes)
>>      var2=sum((1-bayes)*(y-mu2)^2)/sum((1-bayes))
>>      k1=abs(mu1/((var1/mu1)-1))
>>      k2=abs(mu2/((var2/mu2)-1))
>>      prob=mean(bayes)
>>      old.parms=new.parms
>>      new.parms=c(k1,mu1,k2,mu2,prob)
>>      err=max(abs((old.parms-new.parms)/new.parms))
>>      iter=iter+1
>>  }
>>  lines(xvals,prob*dnbinom(xvals,size=k1,mu=mu1)+
>>            (1-prob)*dnbinom(xvals,size=k2,mu=mu2),col="red")
>>  print(list(k1=k1,mu1=mu1,k2=k2,mu2=mu2,p=prob,iter=iter,err=err))
>> }
>
>
>
> I would be grateful if someone can modify the previous function to
> model a three-component mixture instead of a two-component one.

Isn't that your job?

-- Bert

>
> I also tried to look for a package which does the same job: I have
> used the package "mixdist", but I am not able to set up a suitable set
> of starting parameters for the function mix (they always converge to
> zero). Hereafter, I found the package "DEXUS", but the related function
> does not provide good estimates for the model, even in the event that
> I already know what results I have to expect.
>
> Any help is highly appreciated.
>
>
> Danilo Carit?
>
> -------------------------------------------------------------
> Danilo Carit?
>
> PhD Candidate
> University of Naples "Parthenope"
> Dipartimento di Studi Aziendali e Quantitativi
> via G. Parisi, 13, 80132 Napoli - Italy
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tom at vims.edu  Mon Nov  7 18:26:12 2016
From: tom at vims.edu (Tom Mosca)
Date: Mon, 7 Nov 2016 17:26:12 +0000
Subject: [R] Euler & Runge-Kutta
Message-ID: <CB5791F5EA3D82408B277900997482D27F7E28F1@mboxes2.campus.vims.edu>

Can someone help me with R code to perform approximations to second order differential equations and systems of first order differential equations using Euler's method and Runge-Kutta?  I am not a student and this is not for a test or graded assignment.

Examples (unrelated to each other):

h = 0.1

1.  3(t^2)y'' - 5ty' + 5y = 0
     y(1) = 0, y'(1) = 2/3

2.  Lotka-Volterra
     x' = x(3-y)
     y' = y(x-3)




	[[alternative HTML version deleted]]


From bernd.lentes at helmholtz-muenchen.de  Mon Nov  7 19:27:04 2016
From: bernd.lentes at helmholtz-muenchen.de (Lentes, Bernd)
Date: Mon, 7 Nov 2016 19:27:04 +0100 (CET)
Subject: [R] problem installing R 2.5
Message-ID: <423605812.1775271.1478543224277.JavaMail.zimbra@helmholtz-muenchen.de>

Hi,

i'd like to install R 2.5 on an Ubuntu 14.04.
I have a special software requiring this old version.

While ./configure, i get the following error:

checking for mbstate_t... yes
checking for X... no
configure: error: --with-x=yes (default) and X11 headers/libs are not available

The problem is that i don't know which headers or libraries are missing, and when i search with aptitude in the Ubuntu repositories i find a ton of them and don't know which one to install.

Can anyone help me ?

Thanks.


Bernd

-- 
Bernd Lentes 

Systemadministration 
institute of developmental genetics 
Geb?ude 35.34 - Raum 208 
HelmholtzZentrum M?nchen 
bernd.lentes at helmholtz-muenchen.de 
phone: +49 (0)89 3187 1241 
fax: +49 (0)89 3187 2294 

Erst wenn man sich auf etwas festlegt kann man Unrecht haben 
Scott Adams
 

Helmholtz Zentrum Muenchen
Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH)
Ingolstaedter Landstr. 1
85764 Neuherberg
www.helmholtz-muenchen.de
Aufsichtsratsvorsitzende: MinDir'in Baerbel Brumme-Bothe
Geschaeftsfuehrer: Prof. Dr. Guenther Wess, Dr. Alfons Enhsen
Registergericht: Amtsgericht Muenchen HRB 6466
USt-IdNr: DE 129521671


From sezenismail at gmail.com  Mon Nov  7 19:52:59 2016
From: sezenismail at gmail.com (Ismail SEZEN)
Date: Mon, 7 Nov 2016 20:52:59 +0200
Subject: [R] Euler & Runge-Kutta
In-Reply-To: <CB5791F5EA3D82408B277900997482D27F7E28F1@mboxes2.campus.vims.edu>
References: <CB5791F5EA3D82408B277900997482D27F7E28F1@mboxes2.campus.vims.edu>
Message-ID: <2435239F-5668-40EB-9405-65B14AA73C85@gmail.com>


> On 07 Nov 2016, at 19:26, Tom Mosca <tom at vims.edu> wrote:
> 
> Can someone help me with R code to perform approximations to second order differential equations and systems of first order differential equations using Euler's method and Runge-Kutta?  I am not a student and this is not for a test or graded assignment.

What is this for?

> 
> Examples (unrelated to each other):
> 
> h = 0.1
> 
> 1.  3(t^2)y'' - 5ty' + 5y = 0
>     y(1) = 0, y'(1) = 2/3
> 
> 2.  Lotka-Volterra
>     x' = x(3-y)
>     y' = y(x-3)
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Mon Nov  7 20:09:43 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 7 Nov 2016 11:09:43 -0800
Subject: [R] Dealing with -Inf in a maximisation problem.
In-Reply-To: <22560.24743.813869.416034@stat.math.ethz.ch>
References: <f2617dd1-3958-7c31-d2b6-02eec4153c9d@auckland.ac.nz>
	<CAF8bMcYjDEe90H4H46u41eio2n1FX89ML1-FP7K9b56s=qNn0Q@mail.gmail.com>
	<c915dd45-a057-bbe1-95b9-ce99befd44d8@auckland.ac.nz>
	<CAF8bMcZ9cZ0+e4+pykrwqTHyL4c1+coaToM5qL2jOiNcS8r3eg@mail.gmail.com>
	<22560.24743.813869.416034@stat.math.ethz.ch>
Message-ID: <CAF8bMcahzVycRDE5Hu=Q8iOKf4Vp38oAZvdUSvSLQBo-C6gviA@mail.gmail.com>

It would be nice if the C functions Rf_logspace_sum, Rf_logspace_add, and
Rf_logspace_sub were available as R functions.  (I wish the '_sub' were
'_subtract' because 'sub' means too many things in R.)

I think Rf_logspace_sum in R could be a little better. E.g., using the C
code

#include <R.h>
#include <Rinternals.h>
#include <Rmath.h>

SEXP Call_logspace_sum(SEXP x)
{
    if (TYPEOF(x) != REALSXP)
    {
        Rf_error("'x' must be a numeric vector");
    }
    return ScalarReal(Rf_logspace_sum(REAL(x), length(x)));
}

and the R functions

logspace_sum <- function (x) .Call("Call_logspace_sum", as.numeric(x))

and

test <- function (x) {
    x <- as.numeric(x)
    rbind(Rmpfr = as.numeric(log(sum(exp(Rmpfr::mpfr(x, precBits=5000))))),
          Rf_logspace_sum = logspace_sum(x),
          subtract_xmax = log(sum(exp(x - max(x)))) + max(x),
          naive = log(sum(exp(x))))
}


R-3.3.2 on Linux gives, after options(digits=17)
> test(c(0, -50))
                                  [,1]
Rmpfr           1.9287498479639178e-22
Rf_logspace_sum 1.9287498479639178e-22
subtract_xmax   0.0000000000000000e+00
naive           0.0000000000000000e+00

which is nice, but also the not so nice

> test(c(0, -50, -50))
                                  [,1]
Rmpfr           3.8574996959278356e-22
Rf_logspace_sum 0.0000000000000000e+00
subtract_xmax   0.0000000000000000e+00
naive           0.0000000000000000e+00

With TERR the second test has Rmpfr==Rf_logspace_sum for that example.



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Nov 7, 2016 at 3:08 AM, Martin Maechler <maechler at stat.math.ethz.ch>
wrote:

> >>>>> William Dunlap via R-help <r-help at r-project.org>
> >>>>>     on Sun, 6 Nov 2016 20:53:17 -0800 writes:
>
>     > Perhaps the C function Rf_logspace_sum(double *x, int n) would help
> in
>     > computing log(b).  It computes log(sum(exp(x_i))) for i in 1..n,
> avoiding
>     > unnecessary under- and overflow.
>
> Indeed!
>
> I had thought more than twice to also export it to the R level
> notably as we have been using two R level versions in a package
> I maintain ('copula'). They are vectorized there in a way that
> seemed particularly useful to our (Marius Hofert and my) use cases.
>
> More on this -- making these available in R, how exactly? --
> probably should move to the R-devel list.
>
> Thank you Bill for bringing it up!
> Martin
>
>     > Bill Dunlap
>     > TIBCO Software
>     > wdunlap tibco.com
>
>     > On Sun, Nov 6, 2016 at 5:25 PM, Rolf Turner <r.turner at auckland.ac.nz>
> wrote:
>
>     >> On 07/11/16 13:07, William Dunlap wrote:
>     >>
>     >>> Have you tried reparameterizing, using logb (=log(b)) instead of b?
>     >>>
>     >>
>     >> Uh, no.  I don't think that that makes any sense in my context.
>     >>
>     >> The "b" values are probabilities and must satisfy a "sum-to-1"
>     >> constraint.  To accommodate this constraint I re-parametrise via a
>     >> "logistic" style parametrisation --- basically
>     >>
>     >> b_i = exp(z_i)/[sum_j exp(z_j)], j = 1, ... n
>     >>
>     >> with the parameters that the optimiser works with being z_1, ...,
> z_{n-1}
>     >> (and with z_n == 0 for identifiability).  The objective function is
> of the
>     >> form sum_i(a_i * log(b_i)), so I transform back
>     >> from the z_i to the b_i in order calculate the value of the
> objective
>     >> function.  But when the z_i get moderately large-negative, the b_i
> become
>     >> numerically 0 and then log(b_i) becomes -Inf.  And the optimiser
> falls over.
>     >>
>     >> cheers,
>     >>
>     >> Rolf
>     >>
>     >>
>     >>> Bill Dunlap
>     >>> TIBCO Software
>     >>> wdunlap tibco.com <http://tibco.com>
>     >>>
>     >>> On Sun, Nov 6, 2016 at 1:17 PM, Rolf Turner <
> r.turner at auckland.ac.nz
>     >>> <mailto:r.turner at auckland.ac.nz>> wrote:
>     >>>
>     >>>
>     >>> I am trying to deal with a maximisation problem in which it is
>     >>> possible for the objective function to (quite legitimately) return
>     >>> the value -Inf, which causes the numerical optimisers that I have
>     >>> tried to fall over.
>     >>>
>     >>> The -Inf values arise from expressions of the form "a * log(b)",
>     >>> with b = 0.  Under the *starting* values of the parameters, a must
>     >>> equal equal 0 whenever b = 0, so we can legitimately say that a *
>     >>> log(b) = 0 in these circumstances.  However as the maximisation
>     >>> algorithm searches over parameters it is possible for b to take the
>     >>> value 0 for values of
>     >>> a that are strictly positive.  (The values of "a" do not change
> during
>     >>> this search, although they *do* change between "successive
> searches".)
>     >>>
>     >>> Clearly if one is *maximising* the objective then -Inf is not a
> value
>     >>> of
>     >>> particular interest, and we should be able to "move away".  But the
>     >>> optimising function just stops.
>     >>>
>     >>> It is also clear that "moving away" is not a simple task; you can't
>     >>> estimate a gradient or Hessian at a point where the function value
>     >>> is -Inf.
>     >>>
>     >>> Can anyone suggest a way out of this dilemma, perhaps an optimiser
>     >>> that is equipped to cope with -Inf values in some sneaky way?
>     >>>
>     >>> Various ad hoc kludges spring to mind, but they all seem to be
>     >>> fraught with peril.
>     >>>
>     >>> I have tried changing the value returned by the objective function
>     >>> from
>     >>> "v" to exp(v) --- which maps -Inf to 0, which is nice and finite.
>     >>> However this seemed to flatten out the objective surface too much,
>     >>> and the search stalled at the 0 value, which is the antithesis of
>     >>> optimal.
>     >>>
>     >>> The problem arises in a context of applying the EM algorithm where
>     >>> the M-step cannot be carried out explicitly, whence numerical
>     >>> optimisation.
>     >>> I can give more detail if anyone thinks that it could be relevant.
>     >>>
>     >>> I would appreciate advice from younger and wiser heads! :-)
>     >>>
>     >>> cheers,
>     >>>
>     >>> Rolf Turner
>     >>>
>     >>> --
>     >>> Technical Editor ANZJS
>     >>> Department of Statistics
>     >>> University of Auckland
>     >>> Phone: +64-9-373-7599 ext. 88276 <tel:%2B64-9-373-7599%20ext.%2
>     088276>
>     >>>
>     >>> ______________________________________________
>     >>> R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     >>> To UNSUBSCRIBE and more, see
>     >>> https://stat.ethz.ch/mailman/listinfo/r-help
>     >>> <https://stat.ethz.ch/mailman/listinfo/r-help>
>     >>> PLEASE do read the posting guide
>     >>> http://www.R-project.org/posting-guide.html
>     >>> <http://www.R-project.org/posting-guide.html>
>     >>> and provide commented, minimal, self-contained, reproducible code.
>     >>>
>     >>>
>     >>>
>     >>
>     >> --
>     >> Technical Editor ANZJS
>     >> Department of Statistics
>     >> University of Auckland
>     >> Phone: +64-9-373-7599 ext. 88276
>     >>
>
>     > [[alternative HTML version deleted]]
>
>     > ______________________________________________
>     > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>     > https://stat.ethz.ch/mailman/listinfo/r-help
>     > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
>     > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Mon Nov  7 20:16:21 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 07 Nov 2016 11:16:21 -0800
Subject: [R] Euler & Runge-Kutta
In-Reply-To: <CB5791F5EA3D82408B277900997482D27F7E28F1@mboxes2.campus.vims.edu>
References: <CB5791F5EA3D82408B277900997482D27F7E28F1@mboxes2.campus.vims.edu>
Message-ID: <0526DC64-A0BC-4B08-A9DE-67B1C400CBB7@dcn.davis.ca.us>

Google is your friend "R Runge kutta" [1][2]

This actually is not the "do my research for me" list... when you have a specific problem with R, this is a good place to get help by providing a reproducible example. You are expected to have some reasonable familiarity with the theory underlying the task you are working on and with basic R syntax so we can focus on where R is not doing what you want.

Why are you interested in Euler AND Runge-Kutta? Why aren't you just interested in solving the DEs to within some uncertainty?

[1] https://www.jstatsoft.org/article/view/v033i09/v033i09.pdf
[2] https://cran.r-project.org/web/views/DifferentialEquations.html
-- 
Sent from my phone. Please excuse my brevity.

On November 7, 2016 9:26:12 AM PST, Tom Mosca <tom at vims.edu> wrote:
>Can someone help me with R code to perform approximations to second
>order differential equations and systems of first order differential
>equations using Euler's method and Runge-Kutta?  I am not a student and
>this is not for a test or graded assignment.
>
>Examples (unrelated to each other):
>
>h = 0.1
>
>1.  3(t^2)y'' - 5ty' + 5y = 0
>     y(1) = 0, y'(1) = 2/3
>
>2.  Lotka-Volterra
>     x' = x(3-y)
>     y' = y(x-3)
>
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From Achim.Zeileis at uibk.ac.at  Mon Nov  7 20:51:28 2016
From: Achim.Zeileis at uibk.ac.at (Achim Zeileis)
Date: Mon, 7 Nov 2016 20:51:28 +0100 (CET)
Subject: [R] Three-component Negative Binomial Mixture: R code
In-Reply-To: <20161107134248.Horde.9trur60Ob_4h26wDm7GtSw1@webmail.uniparthenope.it>
References: <20161107134248.Horde.9trur60Ob_4h26wDm7GtSw1@webmail.uniparthenope.it>
Message-ID: <alpine.DEB.2.20.1611072045061.13437@paninaro>

On Mon, 7 Nov 2016, danilo.carita at uniparthenope.it wrote:

> I need a function for R software which computes a mixture of Negative
> Binomial distributions with at least three components.

The package "countreg" on R-Forge provides a driver FLXMRnegbin() that can 
be combined with the "flexmix" package (i.e., functions flexmix() and 
stepFlexmix()). The manual page provides some worked illustrations in 
example("FLXMRnegbin", package = "countreg").

Note that the driver is mainly designed for negative binomial _regression_ 
models. But if you just regress on a constant (y ~ 1) you can also get 
negative binomial mixture distributions without covariates.

> I found on another site the following function "mixnbinom". It works very
> well, but it computes a mixture of only two components:
>
>
>> mixnbinom=function(y,k1,mu1,k2,mu2,prob,eps=1/100000)
>> {
>>  new.parms=c(k1,mu1,k2,mu2,prob)
>>  err=1
>>  iter=1
>>  maxiter=100
>>  hist(y,probability=T,nclass=30,col="lightgrey",main="The EM algorithm")
>>  xvals=seq(min(y),max(y),1)
>>  lines(xvals,prob*dnbinom(xvals,size=k1,mu=mu1)+
>>            (1-prob)*dnbinom(xvals,size=k2,mu=mu2),col="green")
>>  while(err>eps){
>>      if(iter<=maxiter){
>>          lines(xvals,prob*dnbinom(xvals,size=k1,mu=mu1)+
>>                    (1-prob)*dnbinom(xvals,size=k2,mu=mu2),lty=3)
>>      }
>>      bayes=(prob*dnbinom(y,size=k1,mu=mu1))/((prob*
>>      dnbinom(y,size=k1,mu=mu1))+((1-prob)*dnbinom(y,size=k2,mu=mu2)))
>>      mu1=sum(bayes*y)/sum(bayes)
>>      mu2=sum((1-bayes)*y)/sum((1-bayes))
>>      var1=sum(bayes*(y-mu1)^2)/sum(bayes)
>>      var2=sum((1-bayes)*(y-mu2)^2)/sum((1-bayes))
>>      k1=abs(mu1/((var1/mu1)-1))
>>      k2=abs(mu2/((var2/mu2)-1))
>>      prob=mean(bayes)
>>      old.parms=new.parms
>>      new.parms=c(k1,mu1,k2,mu2,prob)
>>      err=max(abs((old.parms-new.parms)/new.parms))
>>      iter=iter+1
>>  }
>>  lines(xvals,prob*dnbinom(xvals,size=k1,mu=mu1)+
>>            (1-prob)*dnbinom(xvals,size=k2,mu=mu2),col="red")
>>  print(list(k1=k1,mu1=mu1,k2=k2,mu2=mu2,p=prob,iter=iter,err=err))
>> }
>
>
> I would be grateful if someone can modify the previous function to
> model a three-component mixture instead of a two-component one.
>
> I also tried to look for a package which does the same job: I have
> used the package "mixdist", but I am not able to set up a suitable set
> of starting parameters for the function mix (they always converge to
> zero). Hereafter, I found the package "DEXUS", but the related function
> does not provide good estimates for the model, even in the event that
> I already know what results I have to expect.
>
> Any help is highly appreciated.
>
>
> Danilo Carit?
>
> -------------------------------------------------------------
> Danilo Carit?
>
> PhD Candidate
> University of Naples "Parthenope"
> Dipartimento di Studi Aziendali e Quantitativi
> via G. Parisi, 13, 80132 Napoli - Italy
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From wdunlap at tibco.com  Mon Nov  7 21:08:19 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 7 Nov 2016 12:08:19 -0800
Subject: [R] Dealing with -Inf in a maximisation problem.
In-Reply-To: <CAF8bMcahzVycRDE5Hu=Q8iOKf4Vp38oAZvdUSvSLQBo-C6gviA@mail.gmail.com>
References: <f2617dd1-3958-7c31-d2b6-02eec4153c9d@auckland.ac.nz>
	<CAF8bMcYjDEe90H4H46u41eio2n1FX89ML1-FP7K9b56s=qNn0Q@mail.gmail.com>
	<c915dd45-a057-bbe1-95b9-ce99befd44d8@auckland.ac.nz>
	<CAF8bMcZ9cZ0+e4+pykrwqTHyL4c1+coaToM5qL2jOiNcS8r3eg@mail.gmail.com>
	<22560.24743.813869.416034@stat.math.ethz.ch>
	<CAF8bMcahzVycRDE5Hu=Q8iOKf4Vp38oAZvdUSvSLQBo-C6gviA@mail.gmail.com>
Message-ID: <CAF8bMcYtsf1AbpwiEixudRj0mK1WR340jskOac3o86VhLzZdvg@mail.gmail.com>

Using log1p instead of log improves the accuracy of the 'subtract xmax'
algorithm when the largest x is very close to zero.  Perhaps that is what
is missing in the Rf_logspace_add.

test <- function (x) {
    x <- as.numeric(x)
    i <- which.max(x)
    rbind(Rmpfr = as.numeric(log(sum(exp(Rmpfr::mpfr(x, precBits=5000))))),
          Rf_logspace_sum = logspace_sum(x),
          subtract_xmax_log1p = log1p(sum(exp(x[-i] - x[i]))) + x[i],
          subtract_xmax_naive = log(sum(exp(x - x[i]))) + x[i],
          naive = log(sum(exp(x))))
}

> test(c(-1e-50, -46, -47))
                                    [,1]
Rmpfr                1.4404614986241e-20
Rf_logspace_sum     -1.0000000000000e-50
subtract_xmax_log1p  1.4404614986241e-20
subtract_xmax_naive -1.0000000000000e-50
naive                0.0000000000000e+00
> test(c(0, -46, -47))
                                   [,1]
Rmpfr               1.4404614986241e-20
Rf_logspace_sum     0.0000000000000e+00
subtract_xmax_log1p 1.4404614986241e-20
subtract_xmax_naive 0.0000000000000e+00
naive               0.0000000000000e+00


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Nov 7, 2016 at 11:09 AM, William Dunlap <wdunlap at tibco.com> wrote:

> It would be nice if the C functions Rf_logspace_sum, Rf_logspace_add, and
> Rf_logspace_sub were available as R functions.  (I wish the '_sub' were
> '_subtract' because 'sub' means too many things in R.)
>
> I think Rf_logspace_sum in R could be a little better. E.g., using the C
> code
>
> #include <R.h>
> #include <Rinternals.h>
> #include <Rmath.h>
>
> SEXP Call_logspace_sum(SEXP x)
> {
>     if (TYPEOF(x) != REALSXP)
>     {
>         Rf_error("'x' must be a numeric vector");
>     }
>     return ScalarReal(Rf_logspace_sum(REAL(x), length(x)));
> }
>
> and the R functions
>
> logspace_sum <- function (x) .Call("Call_logspace_sum", as.numeric(x))
>
> and
>
> test <- function (x) {
>     x <- as.numeric(x)
>     rbind(Rmpfr = as.numeric(log(sum(exp(Rmpfr::mpfr(x,
> precBits=5000))))),
>           Rf_logspace_sum = logspace_sum(x),
>           subtract_xmax = log(sum(exp(x - max(x)))) + max(x),
>           naive = log(sum(exp(x))))
> }
>
>
> R-3.3.2 on Linux gives, after options(digits=17)
> > test(c(0, -50))
>                                   [,1]
> Rmpfr           1.9287498479639178e-22
> Rf_logspace_sum 1.9287498479639178e-22
> subtract_xmax   0.0000000000000000e+00
> naive           0.0000000000000000e+00
>
> which is nice, but also the not so nice
>
> > test(c(0, -50, -50))
>                                   [,1]
> Rmpfr           3.8574996959278356e-22
> Rf_logspace_sum 0.0000000000000000e+00
> subtract_xmax   0.0000000000000000e+00
> naive           0.0000000000000000e+00
>
> With TERR the second test has Rmpfr==Rf_logspace_sum for that example.
>
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Mon, Nov 7, 2016 at 3:08 AM, Martin Maechler <
> maechler at stat.math.ethz.ch> wrote:
>
>> >>>>> William Dunlap via R-help <r-help at r-project.org>
>> >>>>>     on Sun, 6 Nov 2016 20:53:17 -0800 writes:
>>
>>     > Perhaps the C function Rf_logspace_sum(double *x, int n) would help
>> in
>>     > computing log(b).  It computes log(sum(exp(x_i))) for i in 1..n,
>> avoiding
>>     > unnecessary under- and overflow.
>>
>> Indeed!
>>
>> I had thought more than twice to also export it to the R level
>> notably as we have been using two R level versions in a package
>> I maintain ('copula'). They are vectorized there in a way that
>> seemed particularly useful to our (Marius Hofert and my) use cases.
>>
>> More on this -- making these available in R, how exactly? --
>> probably should move to the R-devel list.
>>
>> Thank you Bill for bringing it up!
>> Martin
>>
>>     > Bill Dunlap
>>     > TIBCO Software
>>     > wdunlap tibco.com
>>
>>     > On Sun, Nov 6, 2016 at 5:25 PM, Rolf Turner <
>> r.turner at auckland.ac.nz> wrote:
>>
>>     >> On 07/11/16 13:07, William Dunlap wrote:
>>     >>
>>     >>> Have you tried reparameterizing, using logb (=log(b)) instead of
>> b?
>>     >>>
>>     >>
>>     >> Uh, no.  I don't think that that makes any sense in my context.
>>     >>
>>     >> The "b" values are probabilities and must satisfy a "sum-to-1"
>>     >> constraint.  To accommodate this constraint I re-parametrise via a
>>     >> "logistic" style parametrisation --- basically
>>     >>
>>     >> b_i = exp(z_i)/[sum_j exp(z_j)], j = 1, ... n
>>     >>
>>     >> with the parameters that the optimiser works with being z_1, ...,
>> z_{n-1}
>>     >> (and with z_n == 0 for identifiability).  The objective function
>> is of the
>>     >> form sum_i(a_i * log(b_i)), so I transform back
>>     >> from the z_i to the b_i in order calculate the value of the
>> objective
>>     >> function.  But when the z_i get moderately large-negative, the b_i
>> become
>>     >> numerically 0 and then log(b_i) becomes -Inf.  And the optimiser
>> falls over.
>>     >>
>>     >> cheers,
>>     >>
>>     >> Rolf
>>     >>
>>     >>
>>     >>> Bill Dunlap
>>     >>> TIBCO Software
>>     >>> wdunlap tibco.com <http://tibco.com>
>>     >>>
>>     >>> On Sun, Nov 6, 2016 at 1:17 PM, Rolf Turner <
>> r.turner at auckland.ac.nz
>>     >>> <mailto:r.turner at auckland.ac.nz>> wrote:
>>     >>>
>>     >>>
>>     >>> I am trying to deal with a maximisation problem in which it is
>>     >>> possible for the objective function to (quite legitimately) return
>>     >>> the value -Inf, which causes the numerical optimisers that I have
>>     >>> tried to fall over.
>>     >>>
>>     >>> The -Inf values arise from expressions of the form "a * log(b)",
>>     >>> with b = 0.  Under the *starting* values of the parameters, a must
>>     >>> equal equal 0 whenever b = 0, so we can legitimately say that a *
>>     >>> log(b) = 0 in these circumstances.  However as the maximisation
>>     >>> algorithm searches over parameters it is possible for b to take
>> the
>>     >>> value 0 for values of
>>     >>> a that are strictly positive.  (The values of "a" do not change
>> during
>>     >>> this search, although they *do* change between "successive
>> searches".)
>>     >>>
>>     >>> Clearly if one is *maximising* the objective then -Inf is not a
>> value
>>     >>> of
>>     >>> particular interest, and we should be able to "move away".  But
>> the
>>     >>> optimising function just stops.
>>     >>>
>>     >>> It is also clear that "moving away" is not a simple task; you
>> can't
>>     >>> estimate a gradient or Hessian at a point where the function value
>>     >>> is -Inf.
>>     >>>
>>     >>> Can anyone suggest a way out of this dilemma, perhaps an optimiser
>>     >>> that is equipped to cope with -Inf values in some sneaky way?
>>     >>>
>>     >>> Various ad hoc kludges spring to mind, but they all seem to be
>>     >>> fraught with peril.
>>     >>>
>>     >>> I have tried changing the value returned by the objective function
>>     >>> from
>>     >>> "v" to exp(v) --- which maps -Inf to 0, which is nice and finite.
>>     >>> However this seemed to flatten out the objective surface too much,
>>     >>> and the search stalled at the 0 value, which is the antithesis of
>>     >>> optimal.
>>     >>>
>>     >>> The problem arises in a context of applying the EM algorithm where
>>     >>> the M-step cannot be carried out explicitly, whence numerical
>>     >>> optimisation.
>>     >>> I can give more detail if anyone thinks that it could be relevant.
>>     >>>
>>     >>> I would appreciate advice from younger and wiser heads! :-)
>>     >>>
>>     >>> cheers,
>>     >>>
>>     >>> Rolf Turner
>>     >>>
>>     >>> --
>>     >>> Technical Editor ANZJS
>>     >>> Department of Statistics
>>     >>> University of Auckland
>>     >>> Phone: +64-9-373-7599 ext. 88276 <tel:%2B64-9-373-7599%20ext.%2
>>     088276>
>>     >>>
>>     >>> ______________________________________________
>>     >>> R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>> --
>>     >>> To UNSUBSCRIBE and more, see
>>     >>> https://stat.ethz.ch/mailman/listinfo/r-help
>>     >>> <https://stat.ethz.ch/mailman/listinfo/r-help>
>>     >>> PLEASE do read the posting guide
>>     >>> http://www.R-project.org/posting-guide.html
>>     >>> <http://www.R-project.org/posting-guide.html>
>>     >>> and provide commented, minimal, self-contained, reproducible code.
>>     >>>
>>     >>>
>>     >>>
>>     >>
>>     >> --
>>     >> Technical Editor ANZJS
>>     >> Department of Statistics
>>     >> University of Auckland
>>     >> Phone: +64-9-373-7599 ext. 88276
>>     >>
>>
>>     > [[alternative HTML version deleted]]
>>
>>     > ______________________________________________
>>     > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>     > https://stat.ethz.ch/mailman/listinfo/r-help
>>     > PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>>     > and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From chocold12 at gmail.com  Mon Nov  7 21:24:57 2016
From: chocold12 at gmail.com (lily li)
Date: Mon, 7 Nov 2016 13:24:57 -0700
Subject: [R] about data structure
Message-ID: <CAN5afy8kTeDxV0-gdALXkVVaTwhe_TNVGWT4p+grg+ZV4JiKLg@mail.gmail.com>

Hi R users,

I'm wondering why the values changed when I try to transform factors into
numerics.

For example, for a data frame DF, there is one column called precipitation,
which is usually lower than 100mm. But this column is factor when I read in
the data. When transform to numeric values, some rows can be higher than
1000mm. What is the problem? Thanks.

DF$prec_new = as.numeric(DF$precip)

	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Mon Nov  7 21:30:53 2016
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Mon, 07 Nov 2016 20:30:53 +0000
Subject: [R] about data structure
In-Reply-To: <CAN5afy8kTeDxV0-gdALXkVVaTwhe_TNVGWT4p+grg+ZV4JiKLg@mail.gmail.com>
References: <CAN5afy8kTeDxV0-gdALXkVVaTwhe_TNVGWT4p+grg+ZV4JiKLg@mail.gmail.com>
Message-ID: <5820E47D.2010204@sapo.pt>

Hello,

Try

as.numeric(as.character(DF$precip))

Hope this helps,

Rui Barradas

Em 07-11-2016 20:24, lily li escreveu:
> Hi R users,
>
> I'm wondering why the values changed when I try to transform factors into
> numerics.
>
> For example, for a data frame DF, there is one column called precipitation,
> which is usually lower than 100mm. But this column is factor when I read in
> the data. When transform to numeric values, some rows can be higher than
> 1000mm. What is the problem? Thanks.
>
> DF$prec_new = as.numeric(DF$precip)
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bretschr at xs4all.nl  Mon Nov  7 21:52:55 2016
From: bretschr at xs4all.nl (Franklin Bretschneider)
Date: Mon, 7 Nov 2016 21:52:55 +0100
Subject: [R] Euler & Runge-Kutta
In-Reply-To: <CB5791F5EA3D82408B277900997482D27F7E28F1@mboxes2.campus.vims.edu>
References: <CB5791F5EA3D82408B277900997482D27F7E28F1@mboxes2.campus.vims.edu>
Message-ID: <4DAF30AB-7184-40E2-BAE1-B6399872E6E3@xs4all.nl>

Hello Tom Mosca,

Re:

> Can someone help me with R code to perform approximations to second order differential equations and systems of first order differential equations using Euler's method and Runge-Kutta?  I am not a student and this is not for a test or graded assignment.
> 
> Examples (unrelated to each other):
> 
> h = 0.1
> 
> 1.  3(t^2)y'' - 5ty' + 5y = 0
>     y(1) = 0, y'(1) = 2/3
> 
> 2.  Lotka-Volterra
>     x' = x(3-y)
>     y' = y(x-3)
> 


For solving differential equations, the famous "lsoda" solver is best, far better than the simple Euler method, and also better than Runge & Kutta.
Angels, or very nice people, implemented solving methods using the lsoda into R. Download the package "deSolve" and the world of numerical solutions is at your feet.

Succes, Franklin
-----




Franklin Bretschneider
Dept of Biology
Utrecht University
bretschr at xs4all.nl


From thpe at simecol.de  Mon Nov  7 22:53:36 2016
From: thpe at simecol.de (Thomas Petzoldt)
Date: Mon, 7 Nov 2016 22:53:36 +0100
Subject: [R] Euler & Runge-Kutta
In-Reply-To: <4DAF30AB-7184-40E2-BAE1-B6399872E6E3@xs4all.nl>
References: <CB5791F5EA3D82408B277900997482D27F7E28F1@mboxes2.campus.vims.edu>
	<4DAF30AB-7184-40E2-BAE1-B6399872E6E3@xs4all.nl>
Message-ID: <a20a2069-f8bb-f7fa-d9f2-a7ef1e6fc1a7@simecol.de>

Am 07.11.2016 um 21:52 schrieb Franklin Bretschneider:
> Hello Tom Mosca,
>
> Re:
>
>> Can someone help me with R code to perform approximations to second order differential equations and systems of first order differential equations using Euler's method and Runge-Kutta?  I am not a student and this is not for a test or graded assignment.
>>
>> Examples (unrelated to each other):
>>
>> h = 0.1
>>
>> 1.  3(t^2)y'' - 5ty' + 5y = 0
>>     y(1) = 0, y'(1) = 2/3
>>
>> 2.  Lotka-Volterra
>>     x' = x(3-y)
>>     y' = y(x-3)
>>
>
>
> For solving differential equations, the famous "lsoda" solver is best, far better than the simple Euler method, and also better than Runge & Kutta.
> Angels, or very nice people, implemented solving methods using the lsoda into R. Download the package "deSolve" and the world of numerical solutions is at your feet.

deSolve contains all of this: Euler, Runge-Kutta, lsoda and more ... and 
you will find many examples, papers, tutorials and books, and a special 
mailing list (special interest group) at:

http://desolve.r-forge.r-project.org

Have fun!

Thomas Petzoldt

>
> Succes, Franklin
> -----

Thanks, Franklin :)

>
>
>
>
> Franklin Bretschneider
> Dept of Biology
> Utrecht University
> bretschr at xs4all.nl
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jdnewmil at dcn.davis.ca.us  Mon Nov  7 23:34:01 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 07 Nov 2016 14:34:01 -0800
Subject: [R] about data structure
In-Reply-To: <CAN5afy8kTeDxV0-gdALXkVVaTwhe_TNVGWT4p+grg+ZV4JiKLg@mail.gmail.com>
References: <CAN5afy8kTeDxV0-gdALXkVVaTwhe_TNVGWT4p+grg+ZV4JiKLg@mail.gmail.com>
Message-ID: <8FF28321-D8BC-4FA5-A3F0-7FD1FF3A471C@dcn.davis.ca.us>

You have non-numeric data somewhere in that column, and factor is already a numeric type (Read about factors in the Introduction to R document that comes with R). Try 

DF$prec_new = as.numeric(as.character (DF$precip)

and then look at

DF$precip[ is.na( DF$precip_new ) ]

to see which values are corrupt. Eliminating the whole factor conversion in the first place might be as simple as adding an na.string argument to your read.table or read.csv call that imports the data.

If you need further help, next time send plain text format email with a reproducible example. 

http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

http://adv-r.had.co.nz/Reproducibility.html
-- 
Sent from my phone. Please excuse my brevity.

On November 7, 2016 12:24:57 PM PST, lily li <chocold12 at gmail.com> wrote:
>Hi R users,
>
>I'm wondering why the values changed when I try to transform factors
>into
>numerics.
>
>For example, for a data frame DF, there is one column called
>precipitation,
>which is usually lower than 100mm. But this column is factor when I
>read in
>the data. When transform to numeric values, some rows can be higher
>than
>1000mm. What is the problem? Thanks.
>
>DF$prec_new = as.numeric(DF$precip)
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From macqueen1 at llnl.gov  Mon Nov  7 23:47:44 2016
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Mon, 7 Nov 2016 22:47:44 +0000
Subject: [R] Help with decrypting
Message-ID: <D446448F.18E7FC%macqueen1@llnl.gov>

I have a file containing encrypted contents. The contents can be decrypted
using perl, like this:

open (FILEHANDLE, "/path/to/file")
chomp ($ciphertext = <FILEHANDLE>);


use Crypt::CBC;
$cipher = Crypt::CBC->new( -key    => 'my secret key',
                           -cipher => 'Blowfish'
                          );

$plaintext  = $cipher->decrypt($ciphertext);


(See http://search.cpan.org/~lds/Crypt-CBC-2.33/CBC.pm)

M goal is to have the value of $plaintext in an R object, so, is there an
R equivalent to this decrypt() perl function?

I've found R packages
  bcrypt
  sodium
that appear to have potential, but I don't understand this business well
enough to figure out how to use them, if indeed they can be used, for
this. Help would be much appreciated.

Thanks
-Don

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062


From marc_schwartz at me.com  Tue Nov  8 00:30:42 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Mon, 07 Nov 2016 17:30:42 -0600
Subject: [R] Help with decrypting
In-Reply-To: <D446448F.18E7FC%macqueen1@llnl.gov>
References: <D446448F.18E7FC%macqueen1@llnl.gov>
Message-ID: <4D19B06A-E71A-4EF5-B47B-F34931932573@me.com>


> On Nov 7, 2016, at 4:47 PM, MacQueen, Don <macqueen1 at llnl.gov> wrote:
> 
> I have a file containing encrypted contents. The contents can be decrypted
> using perl, like this:
> 
> open (FILEHANDLE, "/path/to/file")
> chomp ($ciphertext = <FILEHANDLE>);
> 
> 
> use Crypt::CBC;
> $cipher = Crypt::CBC->new( -key    => 'my secret key',
>                           -cipher => 'Blowfish'
>                          );
> 
> $plaintext  = $cipher->decrypt($ciphertext);
> 
> 
> (See http://search.cpan.org/~lds/Crypt-CBC-2.33/CBC.pm)
> 
> M goal is to have the value of $plaintext in an R object, so, is there an
> R equivalent to this decrypt() perl function?
> 
> I've found R packages
>  bcrypt
>  sodium
> that appear to have potential, but I don't understand this business well
> enough to figure out how to use them, if indeed they can be used, for
> this. Help would be much appreciated.
> 
> Thanks
> -Don
> 



Hi Don,

Blowfish is Bruce Schneier's algorithm from the early 90's, which even Bruce suggested some time ago not be used. Bruce has some alternative Blowfish implementations available via his web site:

  https://www.schneier.com/academic/blowfish/download.html <https://www.schneier.com/academic/blowfish/download.html>

and there is a link there for some third party products that still have it and might provide for a CLI based interface as an alternative (e.g. GnuPG) if you need to use it.

>From what I can tell, 'sodium' does not support Blowfish and the implementation in bcrypt, if I am reading correctly, only provides for a one-way hash implementation, as opposed to encrypt/decrypt functions.

Thus, barring that my searching for alternative R implementations of Blowfish resulted in a Type II error, I do not see any R implementations of Blowfish that support encrypt/decrypt.

If you want to use the Perl module implementation via R, you can take a look at my WriteXLS package on GitHub:

  https://github.com/marcschwartz/WriteXLS <https://github.com/marcschwartz/WriteXLS>

and see how I call Perl scripts within WriteXLS.R:

  https://github.com/marcschwartz/WriteXLS/blob/master/R/WriteXLS.R <https://github.com/marcschwartz/WriteXLS/blob/master/R/WriteXLS.R>

around line 242.

That might provide one method for you.

That all being said, as per Bruce's recommendation, there are "better" encryption/decryption algorithms these days (some in the 'digest' package by Dirk), depending upon who you are trying to protect the data from... :-)

Regards,

Marc Schwartz



	[[alternative HTML version deleted]]


From bob at rud.is  Tue Nov  8 02:29:23 2016
From: bob at rud.is (Bob Rudis)
Date: Mon, 7 Nov 2016 20:29:23 -0500
Subject: [R] Help with decrypting
In-Reply-To: <D446448F.18E7FC%macqueen1@llnl.gov>
References: <D446448F.18E7FC%macqueen1@llnl.gov>
Message-ID: <CAA-FpKXQoxweXbMqz+ta5efqw_74xgYDAjhZ-KoynrHFMWCVDA@mail.gmail.com>

Perhaps https://cran.r-project.org/web/packages/bcrypt/index.html
might be of assistance.

If not, drop a note back to the list as it'll be trivial to expand on
that to give you an R alternative to Perl.

On Mon, Nov 7, 2016 at 5:47 PM, MacQueen, Don <macqueen1 at llnl.gov> wrote:
> I have a file containing encrypted contents. The contents can be decrypted
> using perl, like this:
>
> open (FILEHANDLE, "/path/to/file")
> chomp ($ciphertext = <FILEHANDLE>);
>
>
> use Crypt::CBC;
> $cipher = Crypt::CBC->new( -key    => 'my secret key',
>                            -cipher => 'Blowfish'
>                           );
>
> $plaintext  = $cipher->decrypt($ciphertext);
>
>
> (See http://search.cpan.org/~lds/Crypt-CBC-2.33/CBC.pm)
>
> M goal is to have the value of $plaintext in an R object, so, is there an
> R equivalent to this decrypt() perl function?
>
> I've found R packages
>   bcrypt
>   sodium
> that appear to have potential, but I don't understand this business well
> enough to figure out how to use them, if indeed they can be used, for
> this. Help would be much appreciated.
>
> Thanks
> -Don
>
> --
> Don MacQueen
>
> Lawrence Livermore National Laboratory
> 7000 East Ave., L-627
> Livermore, CA 94550
> 925-423-1062
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From erinm.hodgess at gmail.com  Tue Nov  8 05:13:12 2016
From: erinm.hodgess at gmail.com (Erin Hodgess)
Date: Mon, 7 Nov 2016 22:13:12 -0600
Subject: [R] a book recommendation, please [O/T]
Message-ID: <CACxE24kw9nH4i_TAwC-v1bmUDwcRGJUra3m31o=kfdoCN0VKNw@mail.gmail.com>

Hello!

Could someone recommend a good book on Design of Experiments for a Master's
in Data Analytics, please?

I use Montgomery's book for my undergrad course, but was thinking about
something a little more advanced for this one.

Any help much appreciated, particularly with R-related texts.

Sincerely,
Erin


-- 
Erin Hodgess
Associate Professor
Department of Mathematical and Statistics
University of Houston - Downtown
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Nov  8 05:59:27 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 7 Nov 2016 20:59:27 -0800
Subject: [R] a book recommendation, please [O/T]
In-Reply-To: <CACxE24kw9nH4i_TAwC-v1bmUDwcRGJUra3m31o=kfdoCN0VKNw@mail.gmail.com>
References: <CACxE24kw9nH4i_TAwC-v1bmUDwcRGJUra3m31o=kfdoCN0VKNw@mail.gmail.com>
Message-ID: <CAGxFJbRV+=0QufAki-jX_yAX0VbLe4LY3O8Lt=yKhV3X+JTs4Q@mail.gmail.com>

Have you looked here:

https://www.amazon.com/s/ref=sr_pg_2?rh=n%3A283155%2Ck%3Aexperimental+design&page=2&keywords=experimental+design&ie=UTF8&qid=1478580868

I would think your choice depends strongly on the arena of application.

Of course I like BH^2, but that was because I was taught by them.

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Nov 7, 2016 at 8:13 PM, Erin Hodgess <erinm.hodgess at gmail.com> wrote:
> Hello!
>
> Could someone recommend a good book on Design of Experiments for a Master's
> in Data Analytics, please?
>
> I use Montgomery's book for my undergrad course, but was thinking about
> something a little more advanced for this one.
>
> Any help much appreciated, particularly with R-related texts.
>
> Sincerely,
> Erin
>
>
> --
> Erin Hodgess
> Associate Professor
> Department of Mathematical and Statistics
> University of Houston - Downtown
> mailto: erinm.hodgess at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From erinm.hodgess at gmail.com  Tue Nov  8 07:36:00 2016
From: erinm.hodgess at gmail.com (Erin Hodgess)
Date: Tue, 8 Nov 2016 00:36:00 -0600
Subject: [R] a book recommendation, please [O/T]
In-Reply-To: <CAGxFJbRV+=0QufAki-jX_yAX0VbLe4LY3O8Lt=yKhV3X+JTs4Q@mail.gmail.com>
References: <CACxE24kw9nH4i_TAwC-v1bmUDwcRGJUra3m31o=kfdoCN0VKNw@mail.gmail.com>
	<CAGxFJbRV+=0QufAki-jX_yAX0VbLe4LY3O8Lt=yKhV3X+JTs4Q@mail.gmail.com>
Message-ID: <CACxE24mxvQGpcnpbwTE3Eh1kvu8ck-diKcAuWiiqPmkvez9iQQ@mail.gmail.com>

I like BH^2 as well as a reference book!  I actually think I will go with
the DOE with R by Larson.  Thanks to all for the help!

Sincerely,
Erin


On Mon, Nov 7, 2016 at 10:59 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Have you looked here:
>
> https://www.amazon.com/s/ref=sr_pg_2?rh=n%3A283155%2Ck%
> 3Aexperimental+design&page=2&keywords=experimental+design&
> ie=UTF8&qid=1478580868
>
> I would think your choice depends strongly on the arena of application.
>
> Of course I like BH^2, but that was because I was taught by them.
>
> Cheers,
> Bert
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Mon, Nov 7, 2016 at 8:13 PM, Erin Hodgess <erinm.hodgess at gmail.com>
> wrote:
> > Hello!
> >
> > Could someone recommend a good book on Design of Experiments for a
> Master's
> > in Data Analytics, please?
> >
> > I use Montgomery's book for my undergrad course, but was thinking about
> > something a little more advanced for this one.
> >
> > Any help much appreciated, particularly with R-related texts.
> >
> > Sincerely,
> > Erin
> >
> >
> > --
> > Erin Hodgess
> > Associate Professor
> > Department of Mathematical and Statistics
> > University of Houston - Downtown
> > mailto: erinm.hodgess at gmail.com
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>



-- 
Erin Hodgess
Associate Professor
Department of Mathematical and Statistics
University of Houston - Downtown
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From goran.brostrom at umu.se  Tue Nov  8 07:53:52 2016
From: goran.brostrom at umu.se (=?UTF-8?Q?G=c3=b6ran_Brostr=c3=b6m?=)
Date: Tue, 8 Nov 2016 07:53:52 +0100
Subject: [R] problem installing R 2.5
In-Reply-To: <423605812.1775271.1478543224277.JavaMail.zimbra@helmholtz-muenchen.de>
References: <423605812.1775271.1478543224277.JavaMail.zimbra@helmholtz-muenchen.de>
Message-ID: <627762b0-f1d9-8e1d-0d11-14d0fa4c83ab@umu.se>

libX11-dev?

G?ran

On 2016-11-07 19:27, Lentes, Bernd via R-help wrote:
> Hi,
>
> i'd like to install R 2.5 on an Ubuntu 14.04. I have a special
> software requiring this old version.
>
> While ./configure, i get the following error:
>
> checking for mbstate_t... yes checking for X... no configure: error:
> --with-x=yes (default) and X11 headers/libs are not available
>
> The problem is that i don't know which headers or libraries are
> missing, and when i search with aptitude in the Ubuntu repositories i
> find a ton of them and don't know which one to install.
>
> Can anyone help me ?
>
> Thanks.
>
>
> Bernd
>


From hannah.hlx at gmail.com  Tue Nov  8 03:16:19 2016
From: hannah.hlx at gmail.com (li li)
Date: Mon, 7 Nov 2016 21:16:19 -0500
Subject: [R] drm function in drc package for fitting dose response curve
Message-ID: <CAHLnndYMePxtwz9pUMuaYm8ZbxRFtyORtPf3h3ETXWmgJUx_2w@mail.gmail.com>

Hi all,
  When using drm function in drc package, we can fit several does response
curves simultaneously or fitting each curve separately. If we fit
simultaneous curves without any constraining condition, for example,
parallelism condition, will we get the same results as the results obtained
by fitting separate curves, particularly parameter estimates?
    If the results are different, what is the root differences, underlying
model and so on?
    Thanks much.
       Hanna

	[[alternative HTML version deleted]]


From spencer.graves at effectivedefense.org  Tue Nov  8 13:24:24 2016
From: spencer.graves at effectivedefense.org (Spencer Graves)
Date: Tue, 8 Nov 2016 06:24:24 -0600
Subject: [R] a book recommendation, please [O/T]
In-Reply-To: <CACxE24mxvQGpcnpbwTE3Eh1kvu8ck-diKcAuWiiqPmkvez9iQQ@mail.gmail.com>
References: <CACxE24kw9nH4i_TAwC-v1bmUDwcRGJUra3m31o=kfdoCN0VKNw@mail.gmail.com>
	<CAGxFJbRV+=0QufAki-jX_yAX0VbLe4LY3O8Lt=yKhV3X+JTs4Q@mail.gmail.com>
	<CACxE24mxvQGpcnpbwTE3Eh1kvu8ck-diKcAuWiiqPmkvez9iQQ@mail.gmail.com>
Message-ID: <330037ae-ecf9-d382-a566-ba23ecd65504@effectivedefense.org>

       Have you considered Box and Draper (2007) Response Surfaces, 
Mixtures, and Ridge Analyses, 2nd Edition?


       You probably know that George Box invented the field of Response 
Surfaces with  Box, G. E. P. and Wilson, K.B. (1951) On the Experimental 
Attainment of Optimum Conditions (with discussion). Journal of the Royal 
Statistical Society Series B13(1):1?45.


       This book describes how to design experiments to get the data to 
optimize a physical process.  I haven't been teaching in academia for 
the past 25 years, but I taught an advanced course from the first 
edition of this book when I did.


       Still, any title "with R" sounds like it's worth reviewing and 
maybe using.


        Spencer Graves


On 11/8/2016 12:36 AM, Erin Hodgess wrote:
> I like BH^2 as well as a reference book!  I actually think I will go with
> the DOE with R by Larson.  Thanks to all for the help!
>
> Sincerely,
> Erin
>
>
> On Mon, Nov 7, 2016 at 10:59 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
>> Have you looked here:
>>
>> https://www.amazon.com/s/ref=sr_pg_2?rh=n%3A283155%2Ck%
>> 3Aexperimental+design&page=2&keywords=experimental+design&
>> ie=UTF8&qid=1478580868
>>
>> I would think your choice depends strongly on the arena of application.
>>
>> Of course I like BH^2, but that was because I was taught by them.
>>
>> Cheers,
>> Bert
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Mon, Nov 7, 2016 at 8:13 PM, Erin Hodgess <erinm.hodgess at gmail.com>
>> wrote:
>>> Hello!
>>>
>>> Could someone recommend a good book on Design of Experiments for a
>> Master's
>>> in Data Analytics, please?
>>>
>>> I use Montgomery's book for my undergrad course, but was thinking about
>>> something a little more advanced for this one.
>>>
>>> Any help much appreciated, particularly with R-related texts.
>>>
>>> Sincerely,
>>> Erin
>>>
>>>
>>> --
>>> Erin Hodgess
>>> Associate Professor
>>> Department of Mathematical and Statistics
>>> University of Houston - Downtown
>>> mailto: erinm.hodgess at gmail.com
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>
>


From bernd.lentes at helmholtz-muenchen.de  Tue Nov  8 14:08:12 2016
From: bernd.lentes at helmholtz-muenchen.de (Lentes, Bernd)
Date: Tue, 8 Nov 2016 14:08:12 +0100 (CET)
Subject: [R] problem installing R 2.5
In-Reply-To: <627762b0-f1d9-8e1d-0d11-14d0fa4c83ab@umu.se>
References: <423605812.1775271.1478543224277.JavaMail.zimbra@helmholtz-muenchen.de>
	<627762b0-f1d9-8e1d-0d11-14d0fa4c83ab@umu.se>
Message-ID: <1948658879.1974166.1478610492772.JavaMail.zimbra@helmholtz-muenchen.de>



----- Am 8. Nov 2016 um 7:53 schrieb G?ran Brostr?m goran.brostrom at umu.se:

> libX11-dev?
> 
> G?ran

Hi,

is already installed:

i A libx11-dev                      - Client-seitige Bibliothek f?r X11 (Entwick

(ubuntu system)


Bernd
 

Helmholtz Zentrum Muenchen
Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH)
Ingolstaedter Landstr. 1
85764 Neuherberg
www.helmholtz-muenchen.de
Aufsichtsratsvorsitzende: MinDir'in Baerbel Brumme-Bothe
Geschaeftsfuehrer: Prof. Dr. Guenther Wess, Dr. Alfons Enhsen
Registergericht: Amtsgericht Muenchen HRB 6466
USt-IdNr: DE 129521671


From galaxie2485 at yahoo.co.in  Tue Nov  8 09:32:50 2016
From: galaxie2485 at yahoo.co.in (Priya Arasu)
Date: Tue, 8 Nov 2016 08:32:50 +0000 (UTC)
Subject: [R] Help-Text file
In-Reply-To: <998305639.379401.1478587759559@mail.yahoo.com>
References: <1207914484.662606.1478323728775.ref@mail.yahoo.com>
	<1207914484.662606.1478323728775@mail.yahoo.com>
	<84D0569E-481A-4615-8556-9A08E306824E@dcn.davis.ca.us>
	<1321924128.371959.1478587544305@mail.yahoo.com>
	<998305639.379401.1478587759559@mail.yahoo.com>
Message-ID: <608157737.517436.1478593970635@mail.yahoo.com>

 Hi, 
???? I have taken Mr.Jeff's suggestions in producing a reproducible example and edited my question again. Kindly someone in the mailing list, help to solve the problem, which I have mentioned in the below mail
Thank youPriya

 
 Dear Mr. Jeff Newmiller, 
???????????????????? ? ? ? ? ? ? ? ? ?? Thank you. I have gone through the links that you have sent me to produce reproducible example. I will try to explain the problem clearly this time. 

I have been using Boolnet package, to simulate interaction between certain genes. I have a problem after importing the text file (New_Text_Document_1)? in R and start doing the attractor analysis in Boolnet. I have pasted the steps below, when I give the command "getAttractors" for doing attractor analysis, I get the error (highlighted red and italicized).

> library("BoolNet", lib.loc="~/R/win-library/3.3")
> library(readr)#Imported txt file
> New_Text_Document_1 <- read_csv("C:/Users/Priya/Desktop/New Text Document-1.txt") 
Parsed with column specification:
cols(
  Targets = col_character(),
  factors = col_character()
)
> View(New_Text_Document_1)
> net <- loadNetwork("C:/Users/Priya/Desktop/New Text Document-1.txt") #loaded the txt file as network (net) for analysis
> attr <- getAttractors(net, type="asynchronous") # attractor analysis for the txt file
Error in matrix(nrow = n, ncol = length(network$genes)) : 
  invalid 'nrow' value (too large or NA)
In addition: Warning message:
In matrix(nrow = n, ncol = length(network$genes)) :
  NAs introduced by coercion to integer range


Using dput, I have pasted the data (New_Text_Document_1), which I 
have imported for doing attractor analysis.I have also attached the New_Text_Document_1 along with this mail. I didn't get any reply from the publishers of Boolnet package, so Kindly help. Have anyone come across this error and how do I fix this problem? 


dput(New_Text_Document_1)
New_Text_Document_1 <- structure(list(Targets = c("DISC1", "MITF", "GATA1", "TAL1", "EZH2", "AR", "TCF3", "miR124", "miR377", "let7b", "let7a", "let7d", "PCM1", "miR33a", "EGR1", "SALL4", "POU3F2", "THAP11", "HCFC1","ZFP281", "WT1", "SOX2", "CUX1", "SIN3B", "KDM5B", "HOXB4", "ELK1", "FLI1", "HNF4A", "RUNX1", "BBS4", "miR432", "miR382", "miR127", "E2F1", "TFAP2A", "EOMES", "GABP"), 
factors = c("(!miR124 & !let7b & !let7a & !let7d) | (PCM1 & BBS4) | (MITF | GATA1 | TAL1 | EZH2 | AR | TCF3 | HNF4A | RUNX1)", 
"(MITF)", "(GATA1)", "(!miR377) | (TAL1)", "(!miR124) | (EZH2)", 
"(!miR124) | (AR)", "(!miR124) | (TCF3)", "(EGR1)", "(miR377)", 
"(let7b)", "(let7a)", "(let7d)", "(! miR33a) | (DISC1 & BBS4) | (EGR1 | SALL4 | POU3F2 | THAP11 | HCFC1 | ZFP281 | WT1 | SOX2 | CUX1 | SIN3B | KDM5B | HOXB4 | ELK1 | TCF3 | FLI1 | HNF4A | RUNX1 | MITF)", "(miR33a)", "(! miR124) | (EGR1)", "(SALL4)", "(POU3F2)", "(THAP11)","(HCFC1)", "(ZFP281)", "(WT1)", "(SOX2)", "(CUX1)", "(SIN3B)",
"(KDM5B)", "(HOXB4)", "(ELK1)", "(FLI1)", "(HNF4A)", "(RUNX1)", 
"(! miR432 & ! miR382 & ! miR127 & ! miR377) | (DISC1 & PCM1) | (E2F1 | GATA1 | FLI1 | MITF | ELK1 | CUX1 | KDM5B | SIN3B) | GABP | EOMES | TFAP2A)", "(miR432)", "(miR382)", "(miR127)", "(! let7a) | (E2F1)", "(TFAP2A)", "(EOMES)", "(GABP)")), 
class = c("tbl_df", "tbl", "data.frame"), 
row.names = c(NA, -38L), .
Names = c("Targets", "factors"), 
spec = structure(list(cols = structure(list(Targets = structure(list(), class = c("collector_character","collector")), 
factors = structure(list(), class = c("collector_character", "collector"))), .
Names = c("Targets", "factors")), 
default = structure(list(),
class = c("collector_guess", "collector"))), .
Names = c("cols", "default"), class = "col_spec"))

Regards
Priya
?
 

    On Saturday, 5 November 2016 10:28 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
 

 Sorry, but this request is quite hard to understand... you are basically directing your question at an incredibly narrow set of people who happen to have combined your kind of data read in the way you read it in with your kind of analysis algorithms while telling us almost nothing about how you did those things.

Please read the Posting Guide mentioned in the footer of every posting on this list and provide a reproducible example [1][2]. If the problem is in your use of R then someone here will be able to help. If your problem is inside a contributed package then you may need the assistance of the package maintainer, but they will also need a reproducible example so your effort will not be in vain. Also, the act of creating a RE often leads you to your own solution even without getting outside help. 

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

[2] http://adv-r.had.co.nz/Reproducibility.html

-- 
Sent from my phone. Please excuse my brevity.

On November 4, 2016 10:28:48 PM PDT, Priya Arasu via R-help <r-help at r-project.org> wrote:
>Hi, 
>???? I am using R 3.3.1 in R studio version 1.0.44 (Windows10, 64 bit
>system). I could import text file with Boolean rules, but when I start
>doing analysis in R, I get the error: I have been trying to do
>attractor analysis using R package Boolnet, using the command: attr <-
>getAttractors(net, type="asynchronous"). This command throws up the
>below error:
>
>Error in matrix(nrow = n, ncol = length(network$genes)) : 
>? invalid 'nrow' value (too large or NA)
>In addition: Warning message:
>In matrix(nrow = n, ncol = length(network$genes)) :
>? NAs introduced by coercion to integer range
>
>
>?Have anyone come across this error?. I have attached the text file
>with Boolean rules. Pls find it. Any suggestions or ideas, would be of
>great help
>Thank you
>Priya
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.



   

 

  

   
	[[alternative HTML version deleted]]


From Pascal.Niklaus at ieu.uzh.ch  Tue Nov  8 13:18:39 2016
From: Pascal.Niklaus at ieu.uzh.ch (Pascal A. Niklaus)
Date: Tue, 8 Nov 2016 13:18:39 +0100
Subject: [R] Sorting of character vectors
Message-ID: <29a863ff-70b3-17bd-70ab-63313aec623b@ieu.uzh.ch>

I just got caught by the way in character vectors are sorted.

It seems that on my machine "sort" (and related functions like "order") 
only consider characters related to punctuation (at least here the "+" 
and "-") when there is no difference in the remaining characters:

 > x1 <- c("-A","+A")
 > x2 <- c("+A","-A")
 > sort(x1)    # sorting is according to "-" and "+"
[1] "-A" "+A"
 > sort(x2)
[1] "-A" "+A"

 > x3 <- c("-Aa","-Ab")
 > x4 <- c("-Aa","+Ab")
 > x5 <- c("+Aa","-Ab")
 > sort(x3)
[1] "-Aa" "-Ab" # here the "+" and "-" are ignored
 > sort(x4)
[1] "-Aa" "+Ab"
 > sort(x5)
[1] "+Aa" "-Ab"

I understand from the help that this depends on how characters are 
collated, and that this scheme follows the multi-level comparison in 
unicode (http://www.unicode.org/reports/tr10/).

However, what I need is a strict left-to-right comparison of the sort 
provided by strcmp or wcscmp in glibc. The particular ordering of 
special characters is not so important, but there should be no 
"multi-level" aspect to the sorting.

Is there a way to achieve this in R?

Thanks for your help

Pascal


From pdalgd at gmail.com  Tue Nov  8 14:36:34 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 8 Nov 2016 14:36:34 +0100
Subject: [R] Sorting of character vectors
In-Reply-To: <29a863ff-70b3-17bd-70ab-63313aec623b@ieu.uzh.ch>
References: <29a863ff-70b3-17bd-70ab-63313aec623b@ieu.uzh.ch>
Message-ID: <220DB8DF-A76A-4C48-AB4A-8719D09765CC@gmail.com>


On 08 Nov 2016, at 13:18 , Pascal A. Niklaus <pascal.niklaus at ieu.uzh.ch> wrote:

> I just got caught by the way in character vectors are sorted.
> 
> It seems that on my machine "sort" (and related functions like "order") only consider characters related to punctuation (at least here the "+" and "-") when there is no difference in the remaining characters:
> 
> > x1 <- c("-A","+A")
> > x2 <- c("+A","-A")
> > sort(x1)    # sorting is according to "-" and "+"
> [1] "-A" "+A"
> > sort(x2)
> [1] "-A" "+A"
> 
> > x3 <- c("-Aa","-Ab")
> > x4 <- c("-Aa","+Ab")
> > x5 <- c("+Aa","-Ab")
> > sort(x3)
> [1] "-Aa" "-Ab" # here the "+" and "-" are ignored
> > sort(x4)
> [1] "-Aa" "+Ab"
> > sort(x5)
> [1] "+Aa" "-Ab"
> 
> I understand from the help that this depends on how characters are collated, and that this scheme follows the multi-level comparison in unicode (http://www.unicode.org/reports/tr10/).
> 
> However, what I need is a strict left-to-right comparison of the sort provided by strcmp or wcscmp in glibc. The particular ordering of special characters is not so important, but there should be no "multi-level" aspect to the sorting.
> 
> Is there a way to achieve this in R?
> 

I'd try one of two ways (the above is not happening for me, so I cannot test):

(1) Temporarily set the Locale to "C": Sys.setlocale("LC_COLLATE", "C"). That should work as long as you stay in good ol' ASCII.
(2) Figure out (Don't look at me!) how to diddle the ICU settings for your system, icuSetCollate() is claimed to be your friend.

-pd


> Thanks for your help
> 
> Pascal
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From ruipbarradas at sapo.pt  Tue Nov  8 14:43:07 2016
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Tue, 08 Nov 2016 13:43:07 +0000
Subject: [R] Sorting of character vectors
In-Reply-To: <29a863ff-70b3-17bd-70ab-63313aec623b@ieu.uzh.ch>
References: <29a863ff-70b3-17bd-70ab-63313aec623b@ieu.uzh.ch>
Message-ID: <5821D66B.1010907@sapo.pt>

Hello,

What is your sessionInfo()?
With me it works as expected:

 > sort(c("-", "+"))
[1] "-" "+"
 > sort(c("+", "-"))
[1] "-" "+"
 > x5 <- c("+Aa","-Ab")
 > sort(x5)
[1] "-Ab" "+Aa"

 > sessionInfo()
R version 3.3.2 (2016-10-31)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

locale:
[1] LC_COLLATE=Portuguese_Portugal.1252 
LC_CTYPE=Portuguese_Portugal.1252
[3] LC_MONETARY=Portuguese_Portugal.1252 LC_NUMERIC=C 

[5] LC_TIME=Portuguese_Portugal.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

Hope this helps,

Rui Barradas


Em 08-11-2016 12:18, Pascal A. Niklaus escreveu:
> I just got caught by the way in character vectors are sorted.
>
> It seems that on my machine "sort" (and related functions like "order")
> only consider characters related to punctuation (at least here the "+"
> and "-") when there is no difference in the remaining characters:
>
>  > x1 <- c("-A","+A")
>  > x2 <- c("+A","-A")
>  > sort(x1)    # sorting is according to "-" and "+"
> [1] "-A" "+A"
>  > sort(x2)
> [1] "-A" "+A"
>
>  > x3 <- c("-Aa","-Ab")
>  > x4 <- c("-Aa","+Ab")
>  > x5 <- c("+Aa","-Ab")
>  > sort(x3)
> [1] "-Aa" "-Ab" # here the "+" and "-" are ignored
>  > sort(x4)
> [1] "-Aa" "+Ab"
>  > sort(x5)
> [1] "+Aa" "-Ab"
>
> I understand from the help that this depends on how characters are
> collated, and that this scheme follows the multi-level comparison in
> unicode (http://www.unicode.org/reports/tr10/).
>
> However, what I need is a strict left-to-right comparison of the sort
> provided by strcmp or wcscmp in glibc. The particular ordering of
> special characters is not so important, but there should be no
> "multi-level" aspect to the sorting.
>
> Is there a way to achieve this in R?
>
> Thanks for your help
>
> Pascal
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bernd.lentes at helmholtz-muenchen.de  Tue Nov  8 15:07:37 2016
From: bernd.lentes at helmholtz-muenchen.de (Lentes, Bernd)
Date: Tue, 8 Nov 2016 15:07:37 +0100 (CET)
Subject: [R] problem installing R 2.5
In-Reply-To: <81E0EBB2-8809-4654-AE32-4ECD5C3640BE@me.com>
References: <423605812.1775271.1478543224277.JavaMail.zimbra@helmholtz-muenchen.de>
	<627762b0-f1d9-8e1d-0d11-14d0fa4c83ab@umu.se>
	<1948658879.1974166.1478610492772.JavaMail.zimbra@helmholtz-muenchen.de>
	<81E0EBB2-8809-4654-AE32-4ECD5C3640BE@me.com>
Message-ID: <1560619505.1995254.1478614057690.JavaMail.zimbra@helmholtz-muenchen.de>



> Hi,

> I would like to suggest that this discussion be moved to R-SIG-Debian, since
> this is Linux distribution specific:

> https://stat.ethz.ch/mailman/listinfo/r-sig-debian

> You will avail yourself of a more focused audience there as well.

> Regards,

> Marc Schwartz

Ok. I subscribed to that ML and asked there. But if someone here has a further idea
i would appreciate it.

Thanks.

Bernd
 

Helmholtz Zentrum Muenchen
Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH)
Ingolstaedter Landstr. 1
85764 Neuherberg
www.helmholtz-muenchen.de
Aufsichtsratsvorsitzende: MinDir'in Baerbel Brumme-Bothe
Geschaeftsfuehrer: Prof. Dr. Guenther Wess, Dr. Alfons Enhsen
Registergericht: Amtsgericht Muenchen HRB 6466
USt-IdNr: DE 129521671


From rmh at temple.edu  Tue Nov  8 15:19:27 2016
From: rmh at temple.edu (Richard M. Heiberger)
Date: Tue, 8 Nov 2016 09:19:27 -0500
Subject: [R] a book recommendation, please [O/T]
In-Reply-To: <CACxE24kw9nH4i_TAwC-v1bmUDwcRGJUra3m31o=kfdoCN0VKNw@mail.gmail.com>
References: <CACxE24kw9nH4i_TAwC-v1bmUDwcRGJUra3m31o=kfdoCN0VKNw@mail.gmail.com>
Message-ID: <CAGx1TMBue5SU8fyV=7X+=bLEPgEE8Yz9Rz1MCqVOhJA6Mrw3BQ@mail.gmail.com>

It depends what "Data Analytics" means.  Is it more or less, or just a
synonym, for "Statistics"?  My book, "Statistical Analysis and Data
Display: An Intermediate Course with Examples in R" 2nd edition,
Springer 2015 could support about half of a Design of Experiments
course for MS in Statistics.  Use Chapters 6,7,12,13,14.

On Mon, Nov 7, 2016 at 11:13 PM, Erin Hodgess <erinm.hodgess at gmail.com> wrote:
> Hello!
>
> Could someone recommend a good book on Design of Experiments for a Master's
> in Data Analytics, please?
>
> I use Montgomery's book for my undergrad course, but was thinking about
> something a little more advanced for this one.
>
> Any help much appreciated, particularly with R-related texts.
>
> Sincerely,
> Erin
>
>
> --
> Erin Hodgess
> Associate Professor
> Department of Mathematical and Statistics
> University of Houston - Downtown
> mailto: erinm.hodgess at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at me.com  Tue Nov  8 14:19:39 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Tue, 08 Nov 2016 07:19:39 -0600
Subject: [R] problem installing R 2.5
In-Reply-To: <1948658879.1974166.1478610492772.JavaMail.zimbra@helmholtz-muenchen.de>
References: <423605812.1775271.1478543224277.JavaMail.zimbra@helmholtz-muenchen.de>
	<627762b0-f1d9-8e1d-0d11-14d0fa4c83ab@umu.se>
	<1948658879.1974166.1478610492772.JavaMail.zimbra@helmholtz-muenchen.de>
Message-ID: <81E0EBB2-8809-4654-AE32-4ECD5C3640BE@me.com>


> On Nov 8, 2016, at 7:08 AM, Lentes, Bernd via R-help <r-help at r-project.org> wrote:
> 
> 
> 
> ----- Am 8. Nov 2016 um 7:53 schrieb G?ran Brostr?m goran.brostrom at umu.se:
> 
>> libX11-dev?
>> 
>> G?ran
> 
> Hi,
> 
> is already installed:
> 
> i A libx11-dev                      - Client-seitige Bibliothek f?r X11 (Entwick
> 
> (ubuntu system)
> 
> 
> Bernd


Hi,

I would like to suggest that this discussion be moved to R-SIG-Debian, since this is Linux distribution specific:

  https://stat.ethz.ch/mailman/listinfo/r-sig-debian <https://stat.ethz.ch/mailman/listinfo/r-sig-debian>

You will avail yourself of a more focused audience there as well.

Regards,

Marc Schwartz


	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Tue Nov  8 15:59:58 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 8 Nov 2016 15:59:58 +0100
Subject: [R] problem installing R 2.5
In-Reply-To: <1560619505.1995254.1478614057690.JavaMail.zimbra@helmholtz-muenchen.de>
References: <423605812.1775271.1478543224277.JavaMail.zimbra@helmholtz-muenchen.de>
	<627762b0-f1d9-8e1d-0d11-14d0fa4c83ab@umu.se>
	<1948658879.1974166.1478610492772.JavaMail.zimbra@helmholtz-muenchen.de>
	<81E0EBB2-8809-4654-AE32-4ECD5C3640BE@me.com>
	<1560619505.1995254.1478614057690.JavaMail.zimbra@helmholtz-muenchen.de>
Message-ID: <569862F2-7BA0-49F0-BBC9-2EB0E2142295@gmail.com>

Notice that it is pretty much mandatory to study the Admin manual, e.g.,

https://cran.r-project.org/doc/manuals/r-release/R-admin.html

and its appendices in particular. It is also included in the sources (in .texi format). As you are building an old R on a recent OS, some interpolation may be required.

-pd

On 08 Nov 2016, at 15:07 , Lentes, Bernd via R-help <r-help at r-project.org> wrote:

> 
> 
>> Hi,
> 
>> I would like to suggest that this discussion be moved to R-SIG-Debian, since
>> this is Linux distribution specific:
> 
>> https://stat.ethz.ch/mailman/listinfo/r-sig-debian
> 
>> You will avail yourself of a more focused audience there as well.
> 
>> Regards,
> 
>> Marc Schwartz
> 
> Ok. I subscribed to that ML and asked there. But if someone here has a further idea
> i would appreciate it.
> 
> Thanks.
> 
> Bernd
> 
> 
> Helmholtz Zentrum Muenchen
> Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH)
> Ingolstaedter Landstr. 1
> 85764 Neuherberg
> www.helmholtz-muenchen.de
> Aufsichtsratsvorsitzende: MinDir'in Baerbel Brumme-Bothe
> Geschaeftsfuehrer: Prof. Dr. Guenther Wess, Dr. Alfons Enhsen
> Registergericht: Amtsgericht Muenchen HRB 6466
> USt-IdNr: DE 129521671
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From danilo.carita at uniparthenope.it  Tue Nov  8 16:18:50 2016
From: danilo.carita at uniparthenope.it (danilo.carita at uniparthenope.it)
Date: Tue, 08 Nov 2016 16:18:50 +0100
Subject: [R] Three-component Negative Binomial Mixture: R code
In-Reply-To: <alpine.DEB.2.20.1611072045061.13437@paninaro>
References: <20161107134248.Horde.9trur60Ob_4h26wDm7GtSw1@webmail.uniparthenope.it>
	<alpine.DEB.2.20.1611072045061.13437@paninaro>
Message-ID: <20161108161850.Horde.KClSE0Kaw2uB2jrwDIHehQ1@webmail.uniparthenope.it>

I tried the function flexmix() with the driver FLXMRnegbin() with two  
components first, in order to compare its results with those provided  
by my function mixnbinom(). In particular, I ran the following code:


> fm0 <- flexmix(y ~ 1, data = data.frame(y), k = 2, model = FLXMRnegbin())


where "y" is my vector of counts. The previous function provided me  
the following parameters:


>                    Comp.1   Comp.2
> coef.(Intercept) 1.2746536 1.788578
> theta            0.1418201 5.028766


with priors 0.342874 and 0.657126, respectively. I assume that the  
coefficients "Intercept" represent the two means of the model (mu1 and  
mu2), while the "theta" coefficients are the size parameters (size1  
and size2).

Unfortunately, unlike my function mixnbinom(), the model computed with  
flexmix() did not provide a good fit to my data (p-value ~0).

Is there something wrong in the process above?



Achim Zeileis <Achim.Zeileis at uibk.ac.at> ha scritto:

> On Mon, 7 Nov 2016, danilo.carita at uniparthenope.it wrote:
>
>> I need a function for R software which computes a mixture of Negative
>> Binomial distributions with at least three components.
>
> The package "countreg" on R-Forge provides a driver FLXMRnegbin()  
> that can be combined with the "flexmix" package (i.e., functions  
> flexmix() and stepFlexmix()). The manual page provides some worked  
> illustrations in example("FLXMRnegbin", package = "countreg").
>
> Note that the driver is mainly designed for negative binomial  
> _regression_ models. But if you just regress on a constant (y ~ 1)  
> you can also get negative binomial mixture distributions without  
> covariates.


-------------------------------------------------------------
Danilo Carit?

PhD Candidate
University of Naples "Parthenope"
Dipartimento di Studi Aziendali e Quantitativi
via G. Parisi, 13, 80132 Napoli - Italy


From Achim.Zeileis at uibk.ac.at  Tue Nov  8 17:05:35 2016
From: Achim.Zeileis at uibk.ac.at (Achim Zeileis)
Date: Tue, 8 Nov 2016 17:05:35 +0100 (CET)
Subject: [R] Three-component Negative Binomial Mixture: R code
In-Reply-To: <20161108161850.Horde.KClSE0Kaw2uB2jrwDIHehQ1@webmail.uniparthenope.it>
References: <20161107134248.Horde.9trur60Ob_4h26wDm7GtSw1@webmail.uniparthenope.it>
	<alpine.DEB.2.20.1611072045061.13437@paninaro>
	<20161108161850.Horde.KClSE0Kaw2uB2jrwDIHehQ1@webmail.uniparthenope.it>
Message-ID: <alpine.DEB.2.20.1611081658050.20628@paninaro>

On Tue, 8 Nov 2016, danilo.carita at uniparthenope.it wrote:

> I tried the function flexmix() with the driver FLXMRnegbin() with two 
> components first, in order to compare its results with those provided by my 
> function mixnbinom(). In particular, I ran the following code:
>
>
>> fm0 <- flexmix(y ~ 1, data = data.frame(y), k = 2, model = FLXMRnegbin())
>
>
> where "y" is my vector of counts. The previous function provided me the 
> following parameters:
>
>
>>                   Comp.1   Comp.2
>> coef.(Intercept) 1.2746536 1.788578
>> theta            0.1418201 5.028766
>
>
> with priors 0.342874 and 0.657126, respectively. I assume that the 
> coefficients "Intercept" represent the two means of the model (mu1 and mu2),

No, a log link is employed, i.e., exp(1.2746536) and exp(1.788578) are the 
means.

> while the "theta" coefficients are the size parameters (size1 and size2).

Yes.

> Unfortunately, unlike my function mixnbinom(), the model computed with 
> flexmix() did not provide a good fit to my data (p-value ~0).
>
> Is there something wrong in the process above?

Hard to say without a reproducible example. Using parameter values similar 
to the ones you cite above, the following seems to do a reasonable job:

## packages
library("countreg")
library("flexmix")

## artificial data from two NB distributions:
## 1/3 is NB(mu = 3.5, theta = 0.2) and
## 2/3 is NB(mu = 6.0, theta = 5.0)
set.seed(1)
y <- c(rnbinom(200, mu = 3.5, size = 0.2), rnbinom(400, mu = 6, size = 5))

## fit 2-component mixture model
set.seed(1)
fm <- flexmix(y ~ 1, k = 2, model = FLXMRnegbin())

## inspect estimated parameters -> look acceptable
parameters(fm)
exp(parameters(fm)[1,])

My experience was that finding good starting values may be a problem for 
flexmix(). So maybe setting these in some better way would be beneficial.

> Achim Zeileis <Achim.Zeileis at uibk.ac.at> ha scritto:
>
>> On Mon, 7 Nov 2016, danilo.carita at uniparthenope.it wrote:
>> 
>>> I need a function for R software which computes a mixture of Negative
>>> Binomial distributions with at least three components.
>> 
>> The package "countreg" on R-Forge provides a driver FLXMRnegbin() that can 
>> be combined with the "flexmix" package (i.e., functions flexmix() and 
>> stepFlexmix()). The manual page provides some worked illustrations in 
>> example("FLXMRnegbin", package = "countreg").
>> 
>> Note that the driver is mainly designed for negative binomial _regression_ 
>> models. But if you just regress on a constant (y ~ 1) you can also get 
>> negative binomial mixture distributions without covariates.
>
>
> -------------------------------------------------------------
> Danilo Carit?
>
> PhD Candidate
> University of Naples "Parthenope"
> Dipartimento di Studi Aziendali e Quantitativi
> via G. Parisi, 13, 80132 Napoli - Italy
> -------------------------------------------------------------
>

From HDoran at air.org  Tue Nov  8 16:57:51 2016
From: HDoran at air.org (Doran, Harold)
Date: Tue, 8 Nov 2016 15:57:51 +0000
Subject: [R] Alternative to apply in base R
Message-ID: <B08B6AF0CF8CA44F81B9983EEBDCD68601358B79FD@DC1VEX10MB02.air.org>

Without reaching out to another package in R, I wonder what the best way is to speed enhance the following toy example? Over the years I have become very comfortable with the family of apply functions and generally not good at finding an improvement for speed.

This toy example is small, but my real data has many millions of rows and the same operations is repeated many times and so finding a less expensive alternative would be helpful.

mm <- matrix(rnorm(100), ncol = 10)
rn <- apply(mm, 1, prod)

	[[alternative HTML version deleted]]


From alpertunakavlak at gmail.com  Tue Nov  8 14:45:42 2016
From: alpertunakavlak at gmail.com (alper tuna kavlak)
Date: Tue, 8 Nov 2016 15:45:42 +0200
Subject: [R] Unsupervised Clustering Algorithms(Possibilistic C-Means and
	Noisy Clustering)
Message-ID: <CAKGym1hbuE5EHOJRJwT5zZroomsCuNd+DqZVTb-DpNmtexpCYg@mail.gmail.com>

Hi,

I need some help about coding on clustering algorithms. To compose the
structure of clusters I have already codes for K-Means and Fuzzy C-Means. I
need also for PCM and NC but the problem that I can't write the proper code
for those algorithms. So if there is a responsible person who is related
about this topic it would be really nice to complete my code list for my
thesis work. I am looking for your response about this.

Best Regards

Alper Tuna KAVLAK

	[[alternative HTML version deleted]]


From Pascal.Niklaus at ieu.uzh.ch  Tue Nov  8 15:52:02 2016
From: Pascal.Niklaus at ieu.uzh.ch (Pascal A. Niklaus)
Date: Tue, 8 Nov 2016 15:52:02 +0100
Subject: [R] Sorting of character vectors
In-Reply-To: <5821D66B.1010907@sapo.pt>
References: <29a863ff-70b3-17bd-70ab-63313aec623b@ieu.uzh.ch>
	<5821D66B.1010907@sapo.pt>
Message-ID: <9cf4fc13-a41a-482e-9fd1-58d01e4f7655@ieu.uzh.ch>

Thanks for all suggestions.

With my build (from the CRAN repo) I don't get ICU support, and setting 
LC_COLLATE to "C" did not help.

 > capabilities("ICU")
   ICU
FALSE

 > sessionInfo()
R version 3.3.2 (2016-10-31)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 14.04.5 LTS

locale:
  [1] LC_CTYPE=en_DK.UTF-8       LC_NUMERIC=C
  [3] LC_TIME=en_DK.UTF-8        LC_COLLATE=C
  [5] LC_MONETARY=en_DK.UTF-8    LC_MESSAGES=en_DK.UTF-8
  [7] LC_PAPER=en_DK.UTF-8       LC_NAME=C
  [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_DK.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_3.3.2 tools_3.3.2



However, using stringi::stri_sort did the trick !


From ccberry at ucsd.edu  Tue Nov  8 17:55:02 2016
From: ccberry at ucsd.edu (Charles C. Berry)
Date: Tue, 8 Nov 2016 08:55:02 -0800
Subject: [R] Alternative to apply in base R
In-Reply-To: <B08B6AF0CF8CA44F81B9983EEBDCD68601358B79FD@DC1VEX10MB02.air.org>
References: <B08B6AF0CF8CA44F81B9983EEBDCD68601358B79FD@DC1VEX10MB02.air.org>
Message-ID: <alpine.OSX.2.20.1611080850170.702@charles-berrys-macbook.local>

On Tue, 8 Nov 2016, Doran, Harold wrote:

> Without reaching out to another package in R, I wonder what the best way is to speed enhance the following toy example? Over the years I have become very comfortable with the family of apply functions and generally not good at finding an improvement for speed.
>
> This toy example is small, but my real data has many millions of rows 
> and the same operations is repeated many times and so finding a less 
> expensive alternative would be helpful.
>
> mm <- matrix(rnorm(100), ncol = 10)
> rn <- apply(mm, 1, prod)


If the real example has only 10 columns, try this:

> y <- mm[,1]
> for (i in 2:10) y[] <- y*mm[,i]
> all.equal(y,rn)

If it has many more columns, I would `reach out' to the inline package and 
write 3 lines of C or Fortran to do the operation.

HTH,

Chuck


From dwinsemius at comcast.net  Tue Nov  8 18:04:39 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 8 Nov 2016 09:04:39 -0800
Subject: [R] Alternative to apply in base R
In-Reply-To: <B08B6AF0CF8CA44F81B9983EEBDCD68601358B79FD@DC1VEX10MB02.air.org>
References: <B08B6AF0CF8CA44F81B9983EEBDCD68601358B79FD@DC1VEX10MB02.air.org>
Message-ID: <3911F540-8577-41AE-BE3F-714C5D2A7350@comcast.net>


> On Nov 8, 2016, at 7:57 AM, Doran, Harold <HDoran at air.org> wrote:
> 
> Without reaching out to another package in R, I wonder what the best way is to speed enhance the following toy example? Over the years I have become very comfortable with the family of apply functions and generally not good at finding an improvement for speed.
> 
> This toy example is small, but my real data has many millions of rows and the same operations is repeated many times and so finding a less expensive alternative would be helpful.
> 
> mm <- matrix(rnorm(100), ncol = 10)
> rn <- apply(mm, 1, prod)

I believe you will find that a for-loop is faster.

library(microbenchmark)
help(pac=microbenchmark)
microbenchmark( forloop={ y = mm[,1]; for (i in 2:dim(mm)[2]) y=mm[,i]}, 
    apply = apply(mm,2,prod) )

+----------
Unit: microseconds
    expr    min      lq     mean  median      uq     max neval cld
 forloop 10.735 11.6450 15.00425 13.1295 13.7455 115.600   100  a 
   apply 60.775 63.2025 71.95027 64.4530 71.3525 209.309   100   b



> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From ed_isfahani at yahoo.com  Tue Nov  8 18:01:53 2016
From: ed_isfahani at yahoo.com (Elham -)
Date: Tue, 8 Nov 2016 17:01:53 +0000 (UTC)
Subject: [R] Duplicate row.names of lncRNA
References: <1453419819.778861.1478624513459.ref@mail.yahoo.com>
Message-ID: <1453419819.778861.1478624513459@mail.yahoo.com>

I want to do meta analysis for lncRNA,but there is this error : Duplicate row.names are not allowedso some genes that are duplicate were detected,for example:PGM5-AS1-001 ENST00000417887.1PGM5-AS1-001 ENST00000613309.4when I search them in ensembl by its Transcript ID, one of them is antisense and another is lincRNA, with different bp. why?for meta analysis,I should aggregate them as mean.is it true in this situation?
	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Nov  8 18:14:25 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 8 Nov 2016 09:14:25 -0800
Subject: [R] Unsupervised Clustering Algorithms(Possibilistic C-Means
 and Noisy Clustering)
In-Reply-To: <CAKGym1hbuE5EHOJRJwT5zZroomsCuNd+DqZVTb-DpNmtexpCYg@mail.gmail.com>
References: <CAKGym1hbuE5EHOJRJwT5zZroomsCuNd+DqZVTb-DpNmtexpCYg@mail.gmail.com>
Message-ID: <CAGxFJbS6vayZ7UjPRutpbxFH3U9mkbTkxUk+YAi6-eCt9Rk=uQ@mail.gmail.com>

Get familiar with the R ecosystem.

https://cran.r-project.org/web/views/Cluster.html

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Nov 8, 2016 at 5:45 AM, alper tuna kavlak
<alpertunakavlak at gmail.com> wrote:
> Hi,
>
> I need some help about coding on clustering algorithms. To compose the
> structure of clusters I have already codes for K-Means and Fuzzy C-Means. I
> need also for PCM and NC but the problem that I can't write the proper code
> for those algorithms. So if there is a responsible person who is related
> about this topic it would be really nice to complete my code list for my
> thesis work. I am looking for your response about this.
>
> Best Regards
>
> Alper Tuna KAVLAK
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ed_isfahani at yahoo.com  Tue Nov  8 18:15:17 2016
From: ed_isfahani at yahoo.com (Elham -)
Date: Tue, 8 Nov 2016 17:15:17 +0000 (UTC)
Subject: [R] Duplicate row.names of lncRNA
References: <878183485.830789.1478625317812.ref@mail.yahoo.com>
Message-ID: <878183485.830789.1478625317812@mail.yahoo.com>

I want to do meta analysis for lncRNA,but there is this error : Duplicate row.names are not allowed.?
so 35 genes that are duplicate were detected,for example:
PGM5-AS1-001 ENST00000417887.1 ? ? ?
PGM5-AS1-001 ENST00000613309.4 ? ??
when I search them in ensembl by its Transcript ID, one of them is antisense and another is lincRNA, with different bp. why?
for meta analysis,I should aggregate them as mean.is it true in this situation?
is it possible to change name of them by add 1,2.. for example: PGM5-AS1-001.1 and PGM5-AS1-001.2 ?
	[[alternative HTML version deleted]]


From vd4mmind at gmail.com  Tue Nov  8 18:38:44 2016
From: vd4mmind at gmail.com (Vivek Das)
Date: Tue, 8 Nov 2016 18:38:44 +0100
Subject: [R] Duplicate row.names of lncRNA
In-Reply-To: <1453419819.778861.1478624513459@mail.yahoo.com>
References: <1453419819.778861.1478624513459.ref@mail.yahoo.com>
	<1453419819.778861.1478624513459@mail.yahoo.com>
Message-ID: <CAFkF=gFo+S1GWCwDmZ=2FHJkfW1GqO6RMW5jVupHf1+WnszW=Q@mail.gmail.com>

This is cross posted in Biostars https://www.biostars.org/p/221116/ . I do
not see any problem with duplicate row names unless you have specific
reasons to keep them. Here you are performing with transcript ids either
with refseq or ensemble. Now every gene does not have unique transcript
ids, they have alternative splicing events and more than one trascripts.
That is one main reason we perform gene summarisation. So you can go for a
gene level count matrix generation or expression value generation , then
this should be handled unless you want only transcript level output.

-- V.D

On Tue, Nov 8, 2016 at 6:01 PM, Elham - via R-help <r-help at r-project.org>
wrote:

> I want to do meta analysis for lncRNA,but there is this error : Duplicate
> row.names are not allowedso some genes that are duplicate were detected,for
> example:PGM5-AS1-001 ENST00000417887.1PGM5-AS1-001 ENST00000613309.4when I
> search them in ensembl by its Transcript ID, one of them is antisense and
> another is lincRNA, with different bp. why?for meta analysis,I should
> aggregate them as mean.is it true in this situation?
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From kjshaney at uta.edu  Tue Nov  8 18:09:12 2016
From: kjshaney at uta.edu (Shaney, Kyle J)
Date: Tue, 8 Nov 2016 17:09:12 +0000
Subject: [R] AdehabitatHR Write Spatial Polygon Problem
Message-ID: <1478624952855.27680@uta.edu>

Hi All,

I am trying to export a shapefile of a minimum convex polygon, but I receive the following error message:


Error: is(x, "SpatialPolygonsDataFrame") is not TRUE


I was able to successfully get my minimum convex polygon and plot it in R using the following code:


PID208 <- read.csv(file="PID208.csv", header = TRUE, sep = ",")

head(PID208)


    Name      X       Y        X1       Y2 sex age        Receive.Time
1 PID208 391734 6744220 -148.9906 60.81855   2   3 2015.06.18 21:12:20
2 PID208 391738 6744219 -148.9905 60.81855   2   3 2015.06.18 22:50:30
3 PID208 391739 6744229 -148.9905 60.81864   2   3 2015.06.04 20:41:52
4 PID208 391748 6744229 -148.9904 60.81864   2   3 2015.06.04 20:44:27
5 PID208 391748 6744229 -148.9904 60.81864   2   3 2015.06.04 20:44:27
6 PID208 391743 6744229 -148.9904 60.81864   2   3 2015.06.04 22:07:07?

xy<- SpatialPoints(PID208 [2:3])
mcp(xy, percent=95)
mcp(xy, percent=95)$area #to find actual area covered
plot(mcp(xy,percent=100))


Anybody know what I might be missing to successfully export the file for use in GIS?

Thanks!

Kyle

	[[alternative HTML version deleted]]


From bernd.lentes at helmholtz-muenchen.de  Tue Nov  8 20:35:44 2016
From: bernd.lentes at helmholtz-muenchen.de (Lentes, Bernd)
Date: Tue, 8 Nov 2016 20:35:44 +0100 (CET)
Subject: [R] [R-sig-Debian] proplems installing R 2.5 on Ubuntu 14.04
In-Reply-To: <1539464.bpKbDh3alr@tux>
References: <659739099.1985341.1478612375127.JavaMail.zimbra@helmholtz-muenchen.de>
	<1823564.gp2a6Zk5RD@tux>
	<2008357711.2007929.1478616315978.JavaMail.zimbra@helmholtz-muenchen.de>
	<1539464.bpKbDh3alr@tux>
Message-ID: <2122345786.2058702.1478633744965.JavaMail.zimbra@helmholtz-muenchen.de>



----- Am 8. Nov 2016 um 16:22 schrieb Johannes Ranke johannes.ranke at jrwb.de:

> Hallo Bernd,
> 
> das heisst die gleiche Fehlermeldung bei "configure"?
> 
> Gru?,
> 
> Johannes
> 
> Am Dienstag, 8. November 2016, 15:45:15 schrieb Lentes, Bernd:
>> ----- Am 8. Nov 2016 um 15:26 schrieb Johannes Ranke johannes.ranke at jrwb.de:
>> > Hi
>> > 
>> > try
>> > 
>> > apt-get build-dep r-base
>> > 
>> > this installs the headers required for building current R, which probably
>> > also covers older versions - at least it will give you X11 headers.
>> > 
>> > Good luck,
>> > 
>> > Johannes
>> 
>> Hi Johannes,
>> 
>> i tried that and it didn't work.
>> 
>> Bernd
>> 
>> 

I installed a source package:
apt-get source r-base
Afterwards:
apt-get build-dep r-base

Then started ./configure again, but the same error appeared.

Bernd

 

Helmholtz Zentrum Muenchen
Deutsches Forschungszentrum fuer Gesundheit und Umwelt (GmbH)
Ingolstaedter Landstr. 1
85764 Neuherberg
www.helmholtz-muenchen.de
Aufsichtsratsvorsitzende: MinDir'in Baerbel Brumme-Bothe
Geschaeftsfuehrer: Prof. Dr. Guenther Wess, Dr. Alfons Enhsen
Registergericht: Amtsgericht Muenchen HRB 6466
USt-IdNr: DE 129521671


From murdoch.duncan at gmail.com  Tue Nov  8 20:44:17 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 8 Nov 2016 14:44:17 -0500
Subject: [R] AdehabitatHR Write Spatial Polygon Problem
In-Reply-To: <1478624952855.27680@uta.edu>
References: <1478624952855.27680@uta.edu>
Message-ID: <1c41e07f-5ddd-e1b9-29da-3e5b2609e20b@gmail.com>

On 08/11/2016 12:09 PM, Shaney, Kyle J wrote:
> Hi All,
>
> I am trying to export a shapefile of a minimum convex polygon, but I receive the following error message:
>
>
> Error: is(x, "SpatialPolygonsDataFrame") is not TRUE
>
>
> I was able to successfully get my minimum convex polygon and plot it in R using the following code:
>
>
> PID208 <- read.csv(file="PID208.csv", header = TRUE, sep = ",")
>
> head(PID208)
>
>
>      Name      X       Y        X1       Y2 sex age        Receive.Time
> 1 PID208 391734 6744220 -148.9906 60.81855   2   3 2015.06.18 21:12:20
> 2 PID208 391738 6744219 -148.9905 60.81855   2   3 2015.06.18 22:50:30
> 3 PID208 391739 6744229 -148.9905 60.81864   2   3 2015.06.04 20:41:52
> 4 PID208 391748 6744229 -148.9904 60.81864   2   3 2015.06.04 20:44:27
> 5 PID208 391748 6744229 -148.9904 60.81864   2   3 2015.06.04 20:44:27
> 6 PID208 391743 6744229 -148.9904 60.81864   2   3 2015.06.04 22:07:07?
>
> xy<- SpatialPoints(PID208 [2:3])
> mcp(xy, percent=95)
> mcp(xy, percent=95)$area #to find actual area covered
> plot(mcp(xy,percent=100))
>
>
> Anybody know what I might be missing to successfully export the file for use in GIS?

You don't say how you're trying to export it, but you are creating a 
SpatialPoints object, and the error message says something wants a 
SpatialPolygonsDataFrame object.  So you need to convert it.

Duncan Murdoch


From jfox at mcmaster.ca  Tue Nov  8 21:00:10 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Tue, 8 Nov 2016 20:00:10 +0000
Subject: [R] Alternative to apply in base R
In-Reply-To: <B08B6AF0CF8CA44F81B9983EEBDCD68601358B79FD@DC1VEX10MB02.air.org>
References: <B08B6AF0CF8CA44F81B9983EEBDCD68601358B79FD@DC1VEX10MB02.air.org>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC83659CA73@FHSDB2D11-2.csu.mcmaster.ca>

Dear Harold,

If the actual data with which you're dealing are non-negative, you could log all the values, and use colSums() on the logs. That might also have the advantage of greater numerical accuracy than multiplying millions of numbers. Depending on the numbers, the products may be too large or small to be represented. Of course, logs won't work with your toy example, where rnorm() will generate values that are both negative and positive.

I hope this helps,
 John
-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
web: socserv.mcmaster.ca/jfox


________________________________________
From: R-help [r-help-bounces at r-project.org] on behalf of Doran, Harold [HDoran at air.org]
Sent: November 8, 2016 10:57 AM
To: r-help at r-project.org
Subject: [R] Alternative to apply in base R

Without reaching out to another package in R, I wonder what the best way is to speed enhance the following toy example? Over the years I have become very comfortable with the family of apply functions and generally not good at finding an improvement for speed.

This toy example is small, but my real data has many millions of rows and the same operations is repeated many times and so finding a less expensive alternative would be helpful.

mm <- matrix(rnorm(100), ncol = 10)
rn <- apply(mm, 1, prod)

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From HDoran at air.org  Tue Nov  8 21:23:04 2016
From: HDoran at air.org (Doran, Harold)
Date: Tue, 8 Nov 2016 20:23:04 +0000
Subject: [R] Alternative to apply in base R
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC83659CA73@FHSDB2D11-2.csu.mcmaster.ca>
References: <B08B6AF0CF8CA44F81B9983EEBDCD68601358B79FD@DC1VEX10MB02.air.org>
	<ACD1644AA6C67E4FBD0C350625508EC83659CA73@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <D4479DCE.40DB6%hdoran@air.org>

It?s a good suggestion. Multiplication in this case is over 7 columns in
the data, but the number of rows is millions. Unfortunately, the values
are negative as these are actually gauss-quad nodes used to evaluate a
multidimensional integral.

colSums is better than something like apply(dat, 2, sum); I was hoping
there was something similar to colSums/rowSums using prod().

On 11/8/16, 3:00 PM, "Fox, John" <jfox at mcmaster.ca> wrote:

>Dear Harold,
>
>If the actual data with which you're dealing are non-negative, you could
>log all the values, and use colSums() on the logs. That might also have
>the advantage of greater numerical accuracy than multiplying millions of
>numbers. Depending on the numbers, the products may be too large or small
>to be represented. Of course, logs won't work with your toy example,
>where rnorm() will generate values that are both negative and positive.
>
>I hope this helps,
> John
>-----------------------------
>John Fox, Professor
>McMaster University
>Hamilton, Ontario
>Canada L8S 4M4
>web: socserv.mcmaster.ca/jfox
>
>
>________________________________________
>From: R-help [r-help-bounces at r-project.org] on behalf of Doran, Harold
>[HDoran at air.org]
>Sent: November 8, 2016 10:57 AM
>To: r-help at r-project.org
>Subject: [R] Alternative to apply in base R
>
>Without reaching out to another package in R, I wonder what the best way
>is to speed enhance the following toy example? Over the years I have
>become very comfortable with the family of apply functions and generally
>not good at finding an improvement for speed.
>
>This toy example is small, but my real data has many millions of rows and
>the same operations is repeated many times and so finding a less
>expensive alternative would be helpful.
>
>mm <- matrix(rnorm(100), ncol = 10)
>rn <- apply(mm, 1, prod)
>
>        [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Tue Nov  8 22:02:25 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 08 Nov 2016 13:02:25 -0800
Subject: [R] Alternative to apply in base R
In-Reply-To: <D4479DCE.40DB6%hdoran@air.org>
References: <B08B6AF0CF8CA44F81B9983EEBDCD68601358B79FD@DC1VEX10MB02.air.org>
	<ACD1644AA6C67E4FBD0C350625508EC83659CA73@FHSDB2D11-2.csu.mcmaster.ca>
	<D4479DCE.40DB6%hdoran@air.org>
Message-ID: <E1FEB10E-BED8-4BF2-BE55-5169AF07B1EA@dcn.davis.ca.us>

Log-sum-antilog is faster than apply by several times, but vector multiplication in a for loop as David and Chuck have suggested is several times faster than that. 
-- 
Sent from my phone. Please excuse my brevity.

On November 8, 2016 12:23:04 PM PST, "Doran, Harold" <HDoran at air.org> wrote:
>It?s a good suggestion. Multiplication in this case is over 7 columns
>in
>the data, but the number of rows is millions. Unfortunately, the values
>are negative as these are actually gauss-quad nodes used to evaluate a
>multidimensional integral.
>
>colSums is better than something like apply(dat, 2, sum); I was hoping
>there was something similar to colSums/rowSums using prod().
>
>On 11/8/16, 3:00 PM, "Fox, John" <jfox at mcmaster.ca> wrote:
>
>>Dear Harold,
>>
>>If the actual data with which you're dealing are non-negative, you
>could
>>log all the values, and use colSums() on the logs. That might also
>have
>>the advantage of greater numerical accuracy than multiplying millions
>of
>>numbers. Depending on the numbers, the products may be too large or
>small
>>to be represented. Of course, logs won't work with your toy example,
>>where rnorm() will generate values that are both negative and
>positive.
>>
>>I hope this helps,
>> John
>>-----------------------------
>>John Fox, Professor
>>McMaster University
>>Hamilton, Ontario
>>Canada L8S 4M4
>>web: socserv.mcmaster.ca/jfox
>>
>>
>>________________________________________
>>From: R-help [r-help-bounces at r-project.org] on behalf of Doran, Harold
>>[HDoran at air.org]
>>Sent: November 8, 2016 10:57 AM
>>To: r-help at r-project.org
>>Subject: [R] Alternative to apply in base R
>>
>>Without reaching out to another package in R, I wonder what the best
>way
>>is to speed enhance the following toy example? Over the years I have
>>become very comfortable with the family of apply functions and
>generally
>>not good at finding an improvement for speed.
>>
>>This toy example is small, but my real data has many millions of rows
>and
>>the same operations is repeated many times and so finding a less
>>expensive alternative would be helpful.
>>
>>mm <- matrix(rnorm(100), ncol = 10)
>>rn <- apply(mm, 1, prod)
>>
>>        [[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Tue Nov  8 22:37:27 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 8 Nov 2016 22:37:27 +0100
Subject: [R] Alternative to apply in base R
In-Reply-To: <D4479DCE.40DB6%hdoran@air.org>
References: <B08B6AF0CF8CA44F81B9983EEBDCD68601358B79FD@DC1VEX10MB02.air.org>
	<ACD1644AA6C67E4FBD0C350625508EC83659CA73@FHSDB2D11-2.csu.mcmaster.ca>
	<D4479DCE.40DB6%hdoran@air.org>
Message-ID: <A73130D7-1D72-46BB-9080-C6148B169C17@gmail.com>


> On 08 Nov 2016, at 21:23 , Doran, Harold <HDoran at air.org> wrote:
> 
> It?s a good suggestion. Multiplication in this case is over 7 columns in
> the data, but the number of rows is millions. Unfortunately, the values
> are negative as these are actually gauss-quad nodes used to evaluate a
> multidimensional integral.

If there really are only 7 cols, then there's also the blindingly obvious

mm[,1]*mm[,2]*mm[,3]*mm[,4]*mm[,5]*mm[,6]*mm[,7]

-pd


> 
> colSums is better than something like apply(dat, 2, sum); I was hoping
> there was something similar to colSums/rowSums using prod().
> 
> On 11/8/16, 3:00 PM, "Fox, John" <jfox at mcmaster.ca> wrote:
> 
>> Dear Harold,
>> 
>> If the actual data with which you're dealing are non-negative, you could
>> log all the values, and use colSums() on the logs. That might also have
>> the advantage of greater numerical accuracy than multiplying millions of
>> numbers. Depending on the numbers, the products may be too large or small
>> to be represented. Of course, logs won't work with your toy example,
>> where rnorm() will generate values that are both negative and positive.
>> 
>> I hope this helps,
>> John
>> -----------------------------
>> John Fox, Professor
>> McMaster University
>> Hamilton, Ontario
>> Canada L8S 4M4
>> web: socserv.mcmaster.ca/jfox
>> 
>> 
>> ________________________________________
>> From: R-help [r-help-bounces at r-project.org] on behalf of Doran, Harold
>> [HDoran at air.org]
>> Sent: November 8, 2016 10:57 AM
>> To: r-help at r-project.org
>> Subject: [R] Alternative to apply in base R
>> 
>> Without reaching out to another package in R, I wonder what the best way
>> is to speed enhance the following toy example? Over the years I have
>> become very comfortable with the family of apply functions and generally
>> not good at finding an improvement for speed.
>> 
>> This toy example is small, but my real data has many millions of rows and
>> the same operations is repeated many times and so finding a less
>> expensive alternative would be helpful.
>> 
>> mm <- matrix(rnorm(100), ncol = 10)
>> rn <- apply(mm, 1, prod)
>> 
>>       [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From HDoran at air.org  Tue Nov  8 22:58:32 2016
From: HDoran at air.org (Doran, Harold)
Date: Tue, 8 Nov 2016 21:58:32 +0000
Subject: [R] Alternative to apply in base R
In-Reply-To: <A73130D7-1D72-46BB-9080-C6148B169C17@gmail.com>
References: <B08B6AF0CF8CA44F81B9983EEBDCD68601358B79FD@DC1VEX10MB02.air.org>
	<ACD1644AA6C67E4FBD0C350625508EC83659CA73@FHSDB2D11-2.csu.mcmaster.ca>
	<D4479DCE.40DB6%hdoran@air.org>
	<A73130D7-1D72-46BB-9080-C6148B169C17@gmail.com>
Message-ID: <D447B405.40DCE%hdoran@air.org>

Well, I wish R-help had a ?like? button as I would most certainly like
this reply :)

As usual, you?re right. I should have added a disclaimer that ?in this
instance? there are 7 columns as the function I wrote evaluates an
N-dimensional integral and so as the dimensions change, so do the number
of columns in this matrix (plus another factor). But the number of columns
is never all that large.



On 11/8/16, 4:37 PM, "peter dalgaard" <pdalgd at gmail.com> wrote:

>
>> On 08 Nov 2016, at 21:23 , Doran, Harold <HDoran at air.org> wrote:
>> 
>> It?s a good suggestion. Multiplication in this case is over 7 columns in
>> the data, but the number of rows is millions. Unfortunately, the values
>> are negative as these are actually gauss-quad nodes used to evaluate a
>> multidimensional integral.
>
>If there really are only 7 cols, then there's also the blindingly obvious
>
>mm[,1]*mm[,2]*mm[,3]*mm[,4]*mm[,5]*mm[,6]*mm[,7]
>
>-pd
>
>
>> 
>> colSums is better than something like apply(dat, 2, sum); I was hoping
>> there was something similar to colSums/rowSums using prod().
>> 
>> On 11/8/16, 3:00 PM, "Fox, John" <jfox at mcmaster.ca> wrote:
>> 
>>> Dear Harold,
>>> 
>>> If the actual data with which you're dealing are non-negative, you
>>>could
>>> log all the values, and use colSums() on the logs. That might also have
>>> the advantage of greater numerical accuracy than multiplying millions
>>>of
>>> numbers. Depending on the numbers, the products may be too large or
>>>small
>>> to be represented. Of course, logs won't work with your toy example,
>>> where rnorm() will generate values that are both negative and positive.
>>> 
>>> I hope this helps,
>>> John
>>> -----------------------------
>>> John Fox, Professor
>>> McMaster University
>>> Hamilton, Ontario
>>> Canada L8S 4M4
>>> web: socserv.mcmaster.ca/jfox
>>> 
>>> 
>>> ________________________________________
>>> From: R-help [r-help-bounces at r-project.org] on behalf of Doran, Harold
>>> [HDoran at air.org]
>>> Sent: November 8, 2016 10:57 AM
>>> To: r-help at r-project.org
>>> Subject: [R] Alternative to apply in base R
>>> 
>>> Without reaching out to another package in R, I wonder what the best
>>>way
>>> is to speed enhance the following toy example? Over the years I have
>>> become very comfortable with the family of apply functions and
>>>generally
>>> not good at finding an improvement for speed.
>>> 
>>> This toy example is small, but my real data has many millions of rows
>>>and
>>> the same operations is repeated many times and so finding a less
>>> expensive alternative would be helpful.
>>> 
>>> mm <- matrix(rnorm(100), ncol = 10)
>>> rn <- apply(mm, 1, prod)
>>> 
>>>       [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>-- 
>Peter Dalgaard, Professor,
>Center for Statistics, Copenhagen Business School
>Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>Phone: (+45)38153501
>Office: A 4.23
>Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>


From wdunlap at tibco.com  Tue Nov  8 23:14:15 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 8 Nov 2016 14:14:15 -0800
Subject: [R] Alternative to apply in base R
In-Reply-To: <D447B405.40DCE%hdoran@air.org>
References: <B08B6AF0CF8CA44F81B9983EEBDCD68601358B79FD@DC1VEX10MB02.air.org>
	<ACD1644AA6C67E4FBD0C350625508EC83659CA73@FHSDB2D11-2.csu.mcmaster.ca>
	<D4479DCE.40DB6%hdoran@air.org>
	<A73130D7-1D72-46BB-9080-C6148B169C17@gmail.com>
	<D447B405.40DCE%hdoran@air.org>
Message-ID: <CAF8bMcYBtpDxypedX6MWG73Eech5B4pC+N=Hg0zOWb5-M48C4A@mail.gmail.com>

The version which allows any number of columns does not take
much more time than the one that requires exactly 7 columns.
If you have a zillion columns then these are not so good.

> f1 <- function(x) x[,1]*x[,2]*x[,3]*x[,4]*x[,5]*x[,6]*x[,7]
> f2 <- function(x) {
+    val <- rep(1, nrow(x))
+    for(i in seq_len(ncol(x))) {
+       val <- val * x[,i]
+    }
+    val
+ }
> z <- matrix(runif(10e6 * 7), ncol=7)
> system.time(v1 <- f1(z))
   user  system elapsed
  0.686   0.140   0.826
> system.time(v2 <- f2(z))
   user  system elapsed
  0.663   0.196   0.860
> all.equal(v1,v2,tolerance=0)
[1] TRUE

You might speed up f2 a tad by special-casing the ncol==0,
ncol==1, and ncol>1 cases.

The versions that call prod() nrow(x) times take about 25 seconds
on this machine and dataset.



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Nov 8, 2016 at 1:58 PM, Doran, Harold <HDoran at air.org> wrote:

> Well, I wish R-help had a ?like? button as I would most certainly like
> this reply :)
>
> As usual, you?re right. I should have added a disclaimer that ?in this
> instance? there are 7 columns as the function I wrote evaluates an
> N-dimensional integral and so as the dimensions change, so do the number
> of columns in this matrix (plus another factor). But the number of columns
> is never all that large.
>
>
>
> On 11/8/16, 4:37 PM, "peter dalgaard" <pdalgd at gmail.com> wrote:
>
> >
> >> On 08 Nov 2016, at 21:23 , Doran, Harold <HDoran at air.org> wrote:
> >>
> >> It?s a good suggestion. Multiplication in this case is over 7 columns in
> >> the data, but the number of rows is millions. Unfortunately, the values
> >> are negative as these are actually gauss-quad nodes used to evaluate a
> >> multidimensional integral.
> >
> >If there really are only 7 cols, then there's also the blindingly obvious
> >
> >mm[,1]*mm[,2]*mm[,3]*mm[,4]*mm[,5]*mm[,6]*mm[,7]
> >
> >-pd
> >
> >
> >>
> >> colSums is better than something like apply(dat, 2, sum); I was hoping
> >> there was something similar to colSums/rowSums using prod().
> >>
> >> On 11/8/16, 3:00 PM, "Fox, John" <jfox at mcmaster.ca> wrote:
> >>
> >>> Dear Harold,
> >>>
> >>> If the actual data with which you're dealing are non-negative, you
> >>>could
> >>> log all the values, and use colSums() on the logs. That might also have
> >>> the advantage of greater numerical accuracy than multiplying millions
> >>>of
> >>> numbers. Depending on the numbers, the products may be too large or
> >>>small
> >>> to be represented. Of course, logs won't work with your toy example,
> >>> where rnorm() will generate values that are both negative and positive.
> >>>
> >>> I hope this helps,
> >>> John
> >>> -----------------------------
> >>> John Fox, Professor
> >>> McMaster University
> >>> Hamilton, Ontario
> >>> Canada L8S 4M4
> >>> web: socserv.mcmaster.ca/jfox
> >>>
> >>>
> >>> ________________________________________
> >>> From: R-help [r-help-bounces at r-project.org] on behalf of Doran, Harold
> >>> [HDoran at air.org]
> >>> Sent: November 8, 2016 10:57 AM
> >>> To: r-help at r-project.org
> >>> Subject: [R] Alternative to apply in base R
> >>>
> >>> Without reaching out to another package in R, I wonder what the best
> >>>way
> >>> is to speed enhance the following toy example? Over the years I have
> >>> become very comfortable with the family of apply functions and
> >>>generally
> >>> not good at finding an improvement for speed.
> >>>
> >>> This toy example is small, but my real data has many millions of rows
> >>>and
> >>> the same operations is repeated many times and so finding a less
> >>> expensive alternative would be helpful.
> >>>
> >>> mm <- matrix(rnorm(100), ncol = 10)
> >>> rn <- apply(mm, 1, prod)
> >>>
> >>>       [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >>http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> >--
> >Peter Dalgaard, Professor,
> >Center for Statistics, Copenhagen Business School
> >Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> >Phone: (+45)38153501
> >Office: A 4.23
> >Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >
> >
> >
> >
> >
> >
> >
> >
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From s.blomberg1 at uq.edu.au  Tue Nov  8 23:56:36 2016
From: s.blomberg1 at uq.edu.au (Simon Blomberg)
Date: Wed, 9 Nov 2016 08:56:36 +1000
Subject: [R] a book recommendation, please [O/T]
In-Reply-To: <330037ae-ecf9-d382-a566-ba23ecd65504@effectivedefense.org>
References: <CACxE24kw9nH4i_TAwC-v1bmUDwcRGJUra3m31o=kfdoCN0VKNw@mail.gmail.com>
	<CAGxFJbRV+=0QufAki-jX_yAX0VbLe4LY3O8Lt=yKhV3X+JTs4Q@mail.gmail.com>
	<CACxE24mxvQGpcnpbwTE3Eh1kvu8ck-diKcAuWiiqPmkvez9iQQ@mail.gmail.com>
	<330037ae-ecf9-d382-a566-ba23ecd65504@effectivedefense.org>
Message-ID: <29ed1a20-a52c-f75d-39e2-913f54b0fd0b@uq.edu.au>

How about Cox and Reid (2000) The Theory of the Design of Experiments. 
It has S-PLUS code in the back, which should pretty much work with R.


On 08/11/16 22:24, Spencer Graves wrote:
>       Have you considered Box and Draper (2007) Response Surfaces, 
> Mixtures, and Ridge Analyses, 2nd Edition?
>
>
>       You probably know that George Box invented the field of Response 
> Surfaces with  Box, G. E. P. and Wilson, K.B. (1951) On the 
> Experimental Attainment of Optimum Conditions (with discussion). 
> Journal of the Royal Statistical Society Series B13(1):1?45.
>
>
>       This book describes how to design experiments to get the data to 
> optimize a physical process.  I haven't been teaching in academia for 
> the past 25 years, but I taught an advanced course from the first 
> edition of this book when I did.
>
>
>       Still, any title "with R" sounds like it's worth reviewing and 
> maybe using.
>
>
>        Spencer Graves
>
>
> On 11/8/2016 12:36 AM, Erin Hodgess wrote:
>> I like BH^2 as well as a reference book! I actually think I will go with
>> the DOE with R by Larson.  Thanks to all for the help!
>>
>> Sincerely,
>> Erin
>>
>>
>> On Mon, Nov 7, 2016 at 10:59 PM, Bert Gunter <bgunter.4567 at gmail.com> 
>> wrote:
>>
>>> Have you looked here:
>>>
>>> https://www.amazon.com/s/ref=sr_pg_2?rh=n%3A283155%2Ck%
>>> 3Aexperimental+design&page=2&keywords=experimental+design&
>>> ie=UTF8&qid=1478580868
>>>
>>> I would think your choice depends strongly on the arena of application.
>>>
>>> Of course I like BH^2, but that was because I was taught by them.
>>>
>>> Cheers,
>>> Bert
>>> Bert Gunter
>>>
>>> "The trouble with having an open mind is that people keep coming along
>>> and sticking things into it."
>>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>>
>>>
>>> On Mon, Nov 7, 2016 at 8:13 PM, Erin Hodgess <erinm.hodgess at gmail.com>
>>> wrote:
>>>> Hello!
>>>>
>>>> Could someone recommend a good book on Design of Experiments for a
>>> Master's
>>>> in Data Analytics, please?
>>>>
>>>> I use Montgomery's book for my undergrad course, but was thinking 
>>>> about
>>>> something a little more advanced for this one.
>>>>
>>>> Any help much appreciated, particularly with R-related texts.
>>>>
>>>> Sincerely,
>>>> Erin
>>>>
>>>>
>>>> -- 
>>>> Erin Hodgess
>>>> Associate Professor
>>>> Department of Mathematical and Statistics
>>>> University of Houston - Downtown
>>>> mailto: erinm.hodgess at gmail.com
>>>>
>>>>          [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/
>>> posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Simon Blomberg, BSc (Hons), PhD, MAppStat, AStat.
Senior Lecturer and Consultant Statistician
School of Biological Sciences
The University of Queensland
St. Lucia Queensland 4072
Australia
T: +61 7 3365 2506
email: S.Blomberg1_at_uq.edu.au
http://www.evolutionarystatistics.org

Policies:
1.  I will NOT analyse your data for you.
2.  Your deadline is your problem.

Basically, I'm not interested in doing research and
I never have been. I'm interested in understanding,
which is quite a different thing. And often to
understand something you have to work it out
for yourself because no one else has done it.
- David Blackwell


From chocold12 at gmail.com  Wed Nov  9 03:42:52 2016
From: chocold12 at gmail.com (lily li)
Date: Tue, 8 Nov 2016 19:42:52 -0700
Subject: [R] Read in files in r
Message-ID: <CAN5afy8PFG7RRrsD6dMweTR4RVJ+k3oufx2Nkin77T0gPvYQgg@mail.gmail.com>

Hi R users,

In the current directory, there are several folders (such as fold1, fold2,
fold3, etc.), while each folder includes the same named files, such as
file1.txt, file2.txt, file3.txt, etc. The structures of each folder and
each file are the same, but with different values. I want to read the files
from each folder, but can't get it to work. Could you please tell me why?
Thanks.

rd1 = read.table(file=list.files('fold1')[1], head=T)
rd2 = read.table(file=list.files('fold1')[2], head=T)

rt1 = read.table(file=list.files('fold2')[1], head=T)
rt2 = read.table(file=list.files('fold2')[2], head=T)

Then there are the warning sign that 'cannot open file 'file1.txt': No such
file or directory'

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Wed Nov  9 04:02:34 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 9 Nov 2016 14:02:34 +1100
Subject: [R] Read in files in r
In-Reply-To: <CAN5afy8PFG7RRrsD6dMweTR4RVJ+k3oufx2Nkin77T0gPvYQgg@mail.gmail.com>
References: <CAN5afy8PFG7RRrsD6dMweTR4RVJ+k3oufx2Nkin77T0gPvYQgg@mail.gmail.com>
Message-ID: <CA+8X3fXy2eBRS-_iMnUw922JJN=-Bq3xtTG6DrzDZusBRsi0DQ@mail.gmail.com>

Hi lily,
My first guess is that the errors are due to trying to open a file like:

"fold1/file1.txt"

as:

"file1.txt"

That is, your code will generate filenames in the directories
fold1,..., without prepending the folder names. Maybe:

result_list<-list()
read_dirs<-paste("fold",1:3,sep="")
rn<-1
for(read_dir in read_dirs) {
 filelist<-list.files(read_dir)
 for(nextfile in filelist) {
  filepath<-paste(read_dir,nextfile,sep="/")
  result_list[[rn]]<-read.table(filepath)
  names(result_list)[rn]<-paste(read_dri,nextfile,sep="_")
  rn<-rn+1
 }
}

Beware: untested.

Jim


On Wed, Nov 9, 2016 at 1:42 PM, lily li <chocold12 at gmail.com> wrote:
> Hi R users,
>
> In the current directory, there are several folders (such as fold1, fold2,
> fold3, etc.), while each folder includes the same named files, such as
> file1.txt, file2.txt, file3.txt, etc. The structures of each folder and
> each file are the same, but with different values. I want to read the files
> from each folder, but can't get it to work. Could you please tell me why?
> Thanks.
>
> rd1 = read.table(file=list.files('fold1')[1], head=T)
> rd2 = read.table(file=list.files('fold1')[2], head=T)
>
> rt1 = read.table(file=list.files('fold2')[1], head=T)
> rt2 = read.table(file=list.files('fold2')[2], head=T)
>
> Then there are the warning sign that 'cannot open file 'file1.txt': No such
> file or directory'
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From heutila at gmail.com  Wed Nov  9 07:04:23 2016
From: heutila at gmail.com (Henry Utila)
Date: Wed, 9 Nov 2016 08:04:23 +0200
Subject: [R] Spatial & Temporal Analysis of Daily Rainfall,
	Temperature Data
Message-ID: <000401d23a4f$2366c210$6a344630$@gmail.com>

Dear R experts,
I have a problem which I don't seem to get to pass. I want to analyze daily
rainfall & temperature data with a lot of missing or unavailable points. I
have been asked to use evd, evir, ismev and geoR packages. I have not used R
before neither do I know any computer languages let alone programming.
I was asked to put my data in excel spreadsheet then convert it into tab
delimited or txt format. The problem am having is importing it into R. it
says factors are not numeric or something like that with a lot of warnings.
Would someone kindly advise how I can solve this problem, please.
Looking forward to your support as R experts.
Thanks.
Henry Utila
MALAWI

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Spencer
Graves
Sent: Tuesday, 8 November, 2016 2:24 PM
To: r-help at r-project.org
Subject: Re: [R] a book recommendation, please [O/T]

       Have you considered Box and Draper (2007) Response Surfaces,
Mixtures, and Ridge Analyses, 2nd Edition?


       You probably know that George Box invented the field of Response 
Surfaces with  Box, G. E. P. and Wilson, K.B. (1951) On the Experimental 
Attainment of Optimum Conditions (with discussion). Journal of the Royal 
Statistical Society Series B13(1):1-45.


       This book describes how to design experiments to get the data to 
optimize a physical process.  I haven't been teaching in academia for 
the past 25 years, but I taught an advanced course from the first 
edition of this book when I did.


       Still, any title "with R" sounds like it's worth reviewing and 
maybe using.


        Spencer Graves


On 11/8/2016 12:36 AM, Erin Hodgess wrote:
> I like BH^2 as well as a reference book!  I actually think I will go with
> the DOE with R by Larson.  Thanks to all for the help!
>
> Sincerely,
> Erin
>
>
> On Mon, Nov 7, 2016 at 10:59 PM, Bert Gunter <bgunter.4567 at gmail.com>
wrote:
>
>> Have you looked here:
>>
>> https://www.amazon.com/s/ref=sr_pg_2?rh=n%3A283155%2Ck%
>> 3Aexperimental+design&page=2&keywords=experimental+design&
>> ie=UTF8&qid=1478580868
>>
>> I would think your choice depends strongly on the arena of application.
>>
>> Of course I like BH^2, but that was because I was taught by them.
>>
>> Cheers,
>> Bert
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Mon, Nov 7, 2016 at 8:13 PM, Erin Hodgess <erinm.hodgess at gmail.com>
>> wrote:
>>> Hello!
>>>
>>> Could someone recommend a good book on Design of Experiments for a
>> Master's
>>> in Data Analytics, please?
>>>
>>> I use Montgomery's book for my undergrad course, but was thinking about
>>> something a little more advanced for this one.
>>>
>>> Any help much appreciated, particularly with R-related texts.
>>>
>>> Sincerely,
>>> Erin
>>>
>>>
>>> --
>>> Erin Hodgess
>>> Associate Professor
>>> Department of Mathematical and Statistics
>>> University of Houston - Downtown
>>> mailto: erinm.hodgess at gmail.com
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>
>

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Wed Nov  9 09:44:48 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 9 Nov 2016 19:44:48 +1100
Subject: [R] Spatial & Temporal Analysis of Daily Rainfall,
	Temperature Data
In-Reply-To: <CA+8X3fWoPpQF3dW92Y1D8QO0B_7_vqQEN908zYmqHSaLJatarQ@mail.gmail.com>
References: <000401d23a4f$2366c210$6a344630$@gmail.com>
	<CA+8X3fWoPpQF3dW92Y1D8QO0B_7_vqQEN908zYmqHSaLJatarQ@mail.gmail.com>
Message-ID: <CA+8X3fVFbqjmo73__MKDrWazuJs4n5GEbB1UwqJLC8grbsPQ_A@mail.gmail.com>

Hi Henry,
You are certainly starting from the beginning. first, when you import
the data from a CSV file, remember to add:

read.csv(...,stringsAsFactors=TRUE)

There will doubtless be other problems, but you have to start somewhere.

Jim


From drjimlemon at gmail.com  Wed Nov  9 09:45:58 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 9 Nov 2016 19:45:58 +1100
Subject: [R] Spatial & Temporal Analysis of Daily Rainfall,
	Temperature Data
In-Reply-To: <CA+8X3fVFbqjmo73__MKDrWazuJs4n5GEbB1UwqJLC8grbsPQ_A@mail.gmail.com>
References: <000401d23a4f$2366c210$6a344630$@gmail.com>
	<CA+8X3fWoPpQF3dW92Y1D8QO0B_7_vqQEN908zYmqHSaLJatarQ@mail.gmail.com>
	<CA+8X3fVFbqjmo73__MKDrWazuJs4n5GEbB1UwqJLC8grbsPQ_A@mail.gmail.com>
Message-ID: <CA+8X3fXr-vL5QaB2g6JcR6_PTqmNvg1+HM6Q5U2KgqWG4tuCuw@mail.gmail.com>

Geez, I must be too excited. I meant:

stringsAsFactors=FALSE

Jim

On Wed, Nov 9, 2016 at 7:44 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
> Hi Henry,
> You are certainly starting from the beginning. first, when you import
> the data from a CSV file, remember to add:
>
> read.csv(...,stringsAsFactors=TRUE)
>
> There will doubtless be other problems, but you have to start somewhere.
>
> Jim


From bgunter.4567 at gmail.com  Wed Nov  9 11:09:56 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 9 Nov 2016 02:09:56 -0800
Subject: [R] Spatial & Temporal Analysis of Daily Rainfall,
	Temperature Data
In-Reply-To: <000401d23a4f$2366c210$6a344630$@gmail.com>
References: <000401d23a4f$2366c210$6a344630$@gmail.com>
Message-ID: <CAGxFJbRd2qoiL=XSVJNRRXGOpWyLKzmtjS6E0yfA7vMS+C4SYQ@mail.gmail.com>

There are many good R tutorials on the web. Some are listed here:
https://www.rstudio.com/online-learning/#R

But you can search around for others.

If you are unable or unwilling to put in the time and effort to learn
R, there's not much we can do.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Nov 8, 2016 at 10:04 PM, Henry Utila <heutila at gmail.com> wrote:
> Dear R experts,
> I have a problem which I don't seem to get to pass. I want to analyze daily
> rainfall & temperature data with a lot of missing or unavailable points. I
> have been asked to use evd, evir, ismev and geoR packages. I have not used R
> before neither do I know any computer languages let alone programming.
> I was asked to put my data in excel spreadsheet then convert it into tab
> delimited or txt format. The problem am having is importing it into R. it
> says factors are not numeric or something like that with a lot of warnings.
> Would someone kindly advise how I can solve this problem, please.
> Looking forward to your support as R experts.
> Thanks.
> Henry Utila
> MALAWI
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Spencer
> Graves
> Sent: Tuesday, 8 November, 2016 2:24 PM
> To: r-help at r-project.org
> Subject: Re: [R] a book recommendation, please [O/T]
>
>        Have you considered Box and Draper (2007) Response Surfaces,
> Mixtures, and Ridge Analyses, 2nd Edition?
>
>
>        You probably know that George Box invented the field of Response
> Surfaces with  Box, G. E. P. and Wilson, K.B. (1951) On the Experimental
> Attainment of Optimum Conditions (with discussion). Journal of the Royal
> Statistical Society Series B13(1):1-45.
>
>
>        This book describes how to design experiments to get the data to
> optimize a physical process.  I haven't been teaching in academia for
> the past 25 years, but I taught an advanced course from the first
> edition of this book when I did.
>
>
>        Still, any title "with R" sounds like it's worth reviewing and
> maybe using.
>
>
>         Spencer Graves
>
>
> On 11/8/2016 12:36 AM, Erin Hodgess wrote:
>> I like BH^2 as well as a reference book!  I actually think I will go with
>> the DOE with R by Larson.  Thanks to all for the help!
>>
>> Sincerely,
>> Erin
>>
>>
>> On Mon, Nov 7, 2016 at 10:59 PM, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
>>
>>> Have you looked here:
>>>
>>> https://www.amazon.com/s/ref=sr_pg_2?rh=n%3A283155%2Ck%
>>> 3Aexperimental+design&page=2&keywords=experimental+design&
>>> ie=UTF8&qid=1478580868
>>>
>>> I would think your choice depends strongly on the arena of application.
>>>
>>> Of course I like BH^2, but that was because I was taught by them.
>>>
>>> Cheers,
>>> Bert
>>> Bert Gunter
>>>
>>> "The trouble with having an open mind is that people keep coming along
>>> and sticking things into it."
>>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>>
>>>
>>> On Mon, Nov 7, 2016 at 8:13 PM, Erin Hodgess <erinm.hodgess at gmail.com>
>>> wrote:
>>>> Hello!
>>>>
>>>> Could someone recommend a good book on Design of Experiments for a
>>> Master's
>>>> in Data Analytics, please?
>>>>
>>>> I use Montgomery's book for my undergrad course, but was thinking about
>>>> something a little more advanced for this one.
>>>>
>>>> Any help much appreciated, particularly with R-related texts.
>>>>
>>>> Sincerely,
>>>> Erin
>>>>
>>>>
>>>> --
>>>> Erin Hodgess
>>>> Associate Professor
>>>> Department of Mathematical and Statistics
>>>> University of Houston - Downtown
>>>> mailto: erinm.hodgess at gmail.com
>>>>
>>>>          [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/
>>> posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Marios.BARLAS at cea.fr  Wed Nov  9 12:24:49 2016
From: Marios.BARLAS at cea.fr (BARLAS Marios 247554)
Date: Wed, 9 Nov 2016 11:24:49 +0000
Subject: [R] PDE Solver and Modeling with R
Message-ID: <01BFC0B2B4ABFC4CB432008F852D766101398A3D@EXDAG0-A1.intra.cea.fr>


Hello every1,

Could you please suggest a good package (if any) for solving PDEs using FEMs approach in R ? 

I'm building a model where I need to solve Poisson, heat transport eq ( linear + heatsource in 2D or 3D in cylindrical Coords ) and combine the results through a semi analytical model. 

Otherwise, do you think I can plug into the old FORTRAN libraries like SLATEC ? 

I've never used R for this type of modeling so far, any advice on where to start from is more than welcome! 

Thank you in advance,

Marios Barlas
PhD Candidate
CMOS & Memory Integration
Advanced Memory Group

Leti, technology research institute 
Commissariat ? l'?nergie atomique et aux ?nergies alternatives
T. +33 4 38 78 11 50 M. +33 6 02 61 83 49
www.leti.fr  | Leti is a member of the Carnot Institutes network



  
          


From HDoran at air.org  Wed Nov  9 13:30:30 2016
From: HDoran at air.org (Doran, Harold)
Date: Wed, 9 Nov 2016 12:30:30 +0000
Subject: [R] Alternative to apply in base R
In-Reply-To: <CAF8bMcYBtpDxypedX6MWG73Eech5B4pC+N=Hg0zOWb5-M48C4A@mail.gmail.com>
References: <B08B6AF0CF8CA44F81B9983EEBDCD68601358B79FD@DC1VEX10MB02.air.org>
	<ACD1644AA6C67E4FBD0C350625508EC83659CA73@FHSDB2D11-2.csu.mcmaster.ca>
	<D4479DCE.40DB6%hdoran@air.org>
	<A73130D7-1D72-46BB-9080-C6148B169C17@gmail.com>
	<D447B405.40DCE%hdoran@air.org>
	<CAF8bMcYBtpDxypedX6MWG73Eech5B4pC+N=Hg0zOWb5-M48C4A@mail.gmail.com>
Message-ID: <B08B6AF0CF8CA44F81B9983EEBDCD68601358B9173@DC1VEX10MB02.air.org>

The speed enhancement using your f2() is remarkable when compared to the apply() method I implemented. In the larger context of my actual problem, this essentially now solves the big computational hog and I can do some real work in a meaningful timeframe as a result.

Thank you for all the suggestions to those on this thread.



From: William Dunlap [mailto:wdunlap at tibco.com]
Sent: Tuesday, November 08, 2016 5:14 PM
To: Doran, Harold <HDoran at air.org>
Cc: peter dalgaard <pdalgd at gmail.com>; r-help at r-project.org; Fox, John <jfox at mcmaster.ca>
Subject: Re: [R] Alternative to apply in base R

The version which allows any number of columns does not take
much more time than the one that requires exactly 7 columns.
If you have a zillion columns then these are not so good.

> f1 <- function(x) x[,1]*x[,2]*x[,3]*x[,4]*x[,5]*x[,6]*x[,7]
> f2 <- function(x) {
+    val <- rep(1, nrow(x))
+    for(i in seq_len(ncol(x))) {
+       val <- val * x[,i]
+    }
+    val
+ }
> z <- matrix(runif(10e6 * 7), ncol=7)
> system.time(v1 <- f1(z))
   user  system elapsed
  0.686   0.140   0.826
> system.time(v2 <- f2(z))
   user  system elapsed
  0.663   0.196   0.860
> all.equal(v1,v2,tolerance=0)
[1] TRUE

You might speed up f2 a tad by special-casing the ncol==0,
ncol==1, and ncol>1 cases.

The versions that call prod() nrow(x) times take about 25 seconds
on this machine and dataset.



Bill Dunlap
TIBCO Software
wdunlap tibco.com<http://tibco.com>

On Tue, Nov 8, 2016 at 1:58 PM, Doran, Harold <HDoran at air.org<mailto:HDoran at air.org>> wrote:
Well, I wish R-help had a ?like? button as I would most certainly like
this reply :)

As usual, you?re right. I should have added a disclaimer that ?in this
instance? there are 7 columns as the function I wrote evaluates an
N-dimensional integral and so as the dimensions change, so do the number
of columns in this matrix (plus another factor). But the number of columns
is never all that large.



On 11/8/16, 4:37 PM, "peter dalgaard" <pdalgd at gmail.com<mailto:pdalgd at gmail.com>> wrote:

>
>> On 08 Nov 2016, at 21:23 , Doran, Harold <HDoran at air.org<mailto:HDoran at air.org>> wrote:
>>
>> It?s a good suggestion. Multiplication in this case is over 7 columns in
>> the data, but the number of rows is millions. Unfortunately, the values
>> are negative as these are actually gauss-quad nodes used to evaluate a
>> multidimensional integral.
>
>If there really are only 7 cols, then there's also the blindingly obvious
>
>mm[,1]*mm[,2]*mm[,3]*mm[,4]*mm[,5]*mm[,6]*mm[,7]
>
>-pd
>
>
>>
>> colSums is better than something like apply(dat, 2, sum); I was hoping
>> there was something similar to colSums/rowSums using prod().
>>
>> On 11/8/16, 3:00 PM, "Fox, John" <jfox at mcmaster.ca<mailto:jfox at mcmaster.ca>> wrote:
>>
>>> Dear Harold,
>>>
>>> If the actual data with which you're dealing are non-negative, you
>>>could
>>> log all the values, and use colSums() on the logs. That might also have
>>> the advantage of greater numerical accuracy than multiplying millions
>>>of
>>> numbers. Depending on the numbers, the products may be too large or
>>>small
>>> to be represented. Of course, logs won't work with your toy example,
>>> where rnorm() will generate values that are both negative and positive.
>>>
>>> I hope this helps,
>>> John
>>> -----------------------------
>>> John Fox, Professor
>>> McMaster University
>>> Hamilton, Ontario
>>> Canada L8S 4M4
>>> web: socserv.mcmaster.ca/jfox<http://socserv.mcmaster.ca/jfox>
>>>
>>>
>>> ________________________________________
>>> From: R-help [r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] on behalf of Doran, Harold
>>> [HDoran at air.org<mailto:HDoran at air.org>]
>>> Sent: November 8, 2016 10:57 AM
>>> To: r-help at r-project.org<mailto:r-help at r-project.org>
>>> Subject: [R] Alternative to apply in base R
>>>
>>> Without reaching out to another package in R, I wonder what the best
>>>way
>>> is to speed enhance the following toy example? Over the years I have
>>> become very comfortable with the family of apply functions and
>>>generally
>>> not good at finding an improvement for speed.
>>>
>>> This toy example is small, but my real data has many millions of rows
>>>and
>>> the same operations is repeated many times and so finding a less
>>> expensive alternative would be helpful.
>>>
>>> mm <- matrix(rnorm(100), ncol = 10)
>>> rn <- apply(mm, 1, prod)
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>--
>Peter Dalgaard, Professor,
>Center for Statistics, Copenhagen Business School
>Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>Phone: (+45)38153501<tel:%28%2B45%2938153501>
>Office: A 4.23
>Email: pd.mes at cbs.dk<mailto:pd.mes at cbs.dk>  Priv: PDalgd at gmail.com<mailto:PDalgd at gmail.com>
>
>
>
>
>
>
>
>
>

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From tr206 at kent.ac.uk  Wed Nov  9 13:45:46 2016
From: tr206 at kent.ac.uk (T.Riedle)
Date: Wed, 9 Nov 2016 12:45:46 +0000
Subject: [R] prcomp() on correlation matrix
Message-ID: <1478695545911.13751@kent.ac.uk>

Dear R users,

I am trying to do a Principal Components Analysis using the prcomp() function based on the correlation matrix. How can I determine to calculate PCA on a correlation or covariance matrix using prcomp()?


Thanks in advance.

	[[alternative HTML version deleted]]


From mak.hholly at gmail.com  Wed Nov  9 15:37:58 2016
From: mak.hholly at gmail.com (greg holly)
Date: Wed, 9 Nov 2016 09:37:58 -0500
Subject: [R] ggplot
Message-ID: <CAM9Qe4jgApUnmi1urB3NMB2pzhkhDSSK=fDaTqkHsD4anF6YBg@mail.gmail.com>

Dear all;

I can not get legend position at the top when using the following code. It
does give at the right.


Your help is greatly appreciated.

p3 <- ggplot(a,

             aes(x = SUPER.PATHWAY,  y = SI)) +

        #theme_classic()

theme_classic(legend.position="top",  axis.text=element_text(size = 16))

(p4 <- p3 + geom_point(aes(color = SUPER.PATHWAY),

                       alpha = 1,

                       size = 2.5,

                       position = position_jitter(width = 1, height = 0.5)))

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Wed Nov  9 16:40:18 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 9 Nov 2016 07:40:18 -0800
Subject: [R] PDE Solver and Modeling with R
In-Reply-To: <01BFC0B2B4ABFC4CB432008F852D766101398A3D@EXDAG0-A1.intra.cea.fr>
References: <01BFC0B2B4ABFC4CB432008F852D766101398A3D@EXDAG0-A1.intra.cea.fr>
Message-ID: <CAGxFJbTZHfZkqJ7WudxAereJJM+a5Z7zGFTaj_KS-+6bssySUA@mail.gmail.com>

Please learn to use the R ecosystem -- or web search (e.g.
"differential equations in R")  -- both of which would have led you
to:

https://cran.r-project.org/web/views/DifferentialEquations.html


-- Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Nov 9, 2016 at 3:24 AM, BARLAS Marios 247554
<Marios.BARLAS at cea.fr> wrote:
>
> Hello every1,
>
> Could you please suggest a good package (if any) for solving PDEs using FEMs approach in R ?
>
> I'm building a model where I need to solve Poisson, heat transport eq ( linear + heatsource in 2D or 3D in cylindrical Coords ) and combine the results through a semi analytical model.
>
> Otherwise, do you think I can plug into the old FORTRAN libraries like SLATEC ?
>
> I've never used R for this type of modeling so far, any advice on where to start from is more than welcome!
>
> Thank you in advance,
>
> Marios Barlas
> PhD Candidate
> CMOS & Memory Integration
> Advanced Memory Group
>
> Leti, technology research institute
> Commissariat ? l'?nergie atomique et aux ?nergies alternatives
> T. +33 4 38 78 11 50 M. +33 6 02 61 83 49
> www.leti.fr  | Leti is a member of the Carnot Institutes network
>
>
>
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From milujisb at gmail.com  Wed Nov  9 17:29:54 2016
From: milujisb at gmail.com (Miluji Sb)
Date: Wed, 9 Nov 2016 17:29:54 +0100
Subject: [R] Average every 4 columns
Message-ID: <CAMLwc7O7R=Bg2jdZJDBcBV6oyU1fGRR4eBdi5R6SpJgKSPvdFQ@mail.gmail.com>

Dear all,

I have a dataset with hundreds of columns, I am only providing only 12
columns. Is it possible to take the mean of every four (or 12) columns
 (value601, value602, value603, value604 etc.in this case) and repeat for
the hundreds of columns? Thank you.

Sincerely,

Milu

structure(list(value601 = c(10.1738710403442, 3.54112911224365,
12.9192342758179, 3.17447590827942, 11.7332258224487, 7.68282270431519,
-7.11564493179321, 0.987620949745178, 13.0476207733154, 6.36939525604248
), value602 = c(13.0642414093018, 5.53129482269287, 16.0519638061523,
2.88946437835693, 14.9204912185669, 9.42428588867188, -6.80674123764038,
-0.614241063594818, 16.7947769165039, 7.9541072845459), value603 =
c(22.0399188995361,
14.398024559021, 24.9523792266846, 12.0878629684448, 23.6459674835205,
18.3277816772461, -2.54092741012573, 10.5550804138184, 25.1016540527344,
16.2166938781738), value604 = c(27.7165412902832, 20.3255825042725,
30.8430004119873, 16.6856250762939, 29.2485408782959, 24.3775005340576,
6.47758340835571, 15.5897912979126, 30.7387924194336, 22.3637084960938
), value605 = c(31.6644763946533, 23.4093952178955, 35.1488723754883,
19.7132263183594, 33.3924179077148, 29.5846366882324, 10.2083873748779,
19.3551616668701, 35.3076629638672, 27.4299201965332), value606 =
c(33.9698333740234,
26.8574161529541, 36.8900833129883, 22.8604583740234, 34.8642921447754,
33.8158760070801, 14.7055835723877, 22.1144580841064, 37.0545425415039,
32.1087913513184), value607 = c(36.0279846191406, 26.9297180175781,
38.2701225280762, 23.2643146514893, 36.7398796081543, 34.1216125488281,
17.1387901306152, 24.0419750213623, 37.8542327880859, 32.7677421569824
), value608 = c(34.0242347717285, 25.7720966339111, 36.4897193908691,
22.0332260131836, 34.8011703491211, 32.6856842041016, 16.6232261657715,
21.5571365356445, 36.1491546630859, 31.1716938018799), value609 =
c(27.5402088165283,
21.7590408325195, 30.5214176177979, 18.4252090454102, 29.1156253814697,
26.9878330230713, 12.4962501525879, 17.7259578704834, 30.9099159240723,
25.4832077026367), value610 = c(23.4706859588623, 17.0126209259033,
26.8166942596436, 15.297459602356, 25.1733055114746, 23.5616931915283,
8.86995983123779, 13.5793552398682, 27.5732250213623, 22.1691932678223
), value611 = c(14.5820417404175, 9.08279132843018, 17.8419170379639,
8.36016654968262, 16.5633754730225, 14.8123331069946, 1.32095837593079,
5.73408317565918, 18.9752082824707, 13.5729999542236), value612 =
c(9.12979793548584,
2.79943537712097, 11.6504030227661, 2.21584677696228, 10.5404834747314,
7.55471754074097, -5.58141136169434, -0.566209673881531, 12.3264112472534,
6.65576601028442)), .Names = c("value601", "value602", "value603",
"value604", "value605", "value606", "value607", "value608", "value609",
"value610", "value611", "value612"), row.names = c("1", "2",
"3", "4", "5", "6", "7", "8", "9", "10"), class = "data.frame")

	[[alternative HTML version deleted]]


From ddalthorp at usgs.gov  Wed Nov  9 17:45:09 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Wed, 9 Nov 2016 08:45:09 -0800
Subject: [R] Average every 4 columns
In-Reply-To: <CAMLwc7O7R=Bg2jdZJDBcBV6oyU1fGRR4eBdi5R6SpJgKSPvdFQ@mail.gmail.com>
References: <CAMLwc7O7R=Bg2jdZJDBcBV6oyU1fGRR4eBdi5R6SpJgKSPvdFQ@mail.gmail.com>
Message-ID: <CAJeYpE_5ant_SMKyEmOpivW=8FS_AYYx671_dKrJYwrmt6S+rg@mail.gmail.com>

Hi Milu,
The following should work for an array x (provided dim(x)[2] is divisible
by 4):

colMeans(x[,0:(dim(x)[2]/4-1)*4+1])

-Dan

On Wed, Nov 9, 2016 at 8:29 AM, Miluji Sb <milujisb at gmail.com> wrote:

> Dear all,
>
> I have a dataset with hundreds of columns, I am only providing only 12
> columns. Is it possible to take the mean of every four (or 12) columns
>  (value601, value602, value603, value604 etc.in this case) and repeat for
> the hundreds of columns? Thank you.
>
> Sincerely,
>
> Milu
>
> structure(list(value601 = c(10.1738710403442, 3.54112911224365,
> 12.9192342758179, 3.17447590827942, 11.7332258224487, 7.68282270431519,
> -7.11564493179321, 0.987620949745178, 13.0476207733154, 6.36939525604248
> ), value602 = c(13.0642414093018, 5.53129482269287, 16.0519638061523,
> 2.88946437835693, 14.9204912185669, 9.42428588867188, -6.80674123764038,
> -0.614241063594818, 16.7947769165039, 7.9541072845459), value603 =
> c(22.0399188995361,
> 14.398024559021, 24.9523792266846, 12.0878629684448, 23.6459674835205,
> 18.3277816772461, -2.54092741012573, 10.5550804138184, 25.1016540527344,
> 16.2166938781738), value604 = c(27.7165412902832, 20.3255825042725,
> 30.8430004119873, 16.6856250762939, 29.2485408782959, 24.3775005340576,
> 6.47758340835571, 15.5897912979126, 30.7387924194336, 22.3637084960938
> ), value605 = c(31.6644763946533, 23.4093952178955, 35.1488723754883,
> 19.7132263183594, 33.3924179077148, 29.5846366882324, 10.2083873748779,
> 19.3551616668701, 35.3076629638672, 27.4299201965332), value606 =
> c(33.9698333740234,
> 26.8574161529541, 36.8900833129883, 22.8604583740234, 34.8642921447754,
> 33.8158760070801, 14.7055835723877, 22.1144580841064, 37.0545425415039,
> 32.1087913513184), value607 = c(36.0279846191406, 26.9297180175781,
> 38.2701225280762, 23.2643146514893, 36.7398796081543, 34.1216125488281,
> 17.1387901306152, 24.0419750213623, 37.8542327880859, 32.7677421569824
> ), value608 = c(34.0242347717285, 25.7720966339111, 36.4897193908691,
> 22.0332260131836, 34.8011703491211, 32.6856842041016, 16.6232261657715,
> 21.5571365356445, 36.1491546630859, 31.1716938018799), value609 =
> c(27.5402088165283,
> 21.7590408325195, 30.5214176177979, 18.4252090454102, 29.1156253814697,
> 26.9878330230713, 12.4962501525879, 17.7259578704834, 30.9099159240723,
> 25.4832077026367), value610 = c(23.4706859588623, 17.0126209259033,
> 26.8166942596436, 15.297459602356, 25.1733055114746, 23.5616931915283,
> 8.86995983123779, 13.5793552398682, 27.5732250213623, 22.1691932678223
> ), value611 = c(14.5820417404175, 9.08279132843018, 17.8419170379639,
> 8.36016654968262, 16.5633754730225, 14.8123331069946, 1.32095837593079,
> 5.73408317565918, 18.9752082824707, 13.5729999542236), value612 =
> c(9.12979793548584,
> 2.79943537712097, 11.6504030227661, 2.21584677696228, 10.5404834747314,
> 7.55471754074097, -5.58141136169434, -0.566209673881531, 12.3264112472534,
> 6.65576601028442)), .Names = c("value601", "value602", "value603",
> "value604", "value605", "value606", "value607", "value608", "value609",
> "value610", "value611", "value612"), row.names = c("1", "2",
> "3", "4", "5", "6", "7", "8", "9", "10"), class = "data.frame")
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From milujisb at gmail.com  Wed Nov  9 18:00:07 2016
From: milujisb at gmail.com (Miluji Sb)
Date: Wed, 9 Nov 2016 18:00:07 +0100
Subject: [R] Average every 4 columns
In-Reply-To: <CAJeYpE_5ant_SMKyEmOpivW=8FS_AYYx671_dKrJYwrmt6S+rg@mail.gmail.com>
References: <CAMLwc7O7R=Bg2jdZJDBcBV6oyU1fGRR4eBdi5R6SpJgKSPvdFQ@mail.gmail.com>
	<CAJeYpE_5ant_SMKyEmOpivW=8FS_AYYx671_dKrJYwrmt6S+rg@mail.gmail.com>
Message-ID: <CAMLwc7N-TUbc_xk5gaFhM4sXt007tL-iofVu8AFjBtSJ6HnJjg@mail.gmail.com>

Thanks a lot for your quick reply. I made a mistake in the question, I
meant to ask every 4 (or 12) rows not columns. Apologies. Thanks again!

Sincerely,

Milu

On Wed, Nov 9, 2016 at 5:45 PM, Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:

> Hi Milu,
> The following should work for an array x (provided dim(x)[2] is divisible
> by 4):
>
> colMeans(x[,0:(dim(x)[2]/4-1)*4+1])
>
> -Dan
>
> On Wed, Nov 9, 2016 at 8:29 AM, Miluji Sb <milujisb at gmail.com> wrote:
>
>> Dear all,
>>
>> I have a dataset with hundreds of columns, I am only providing only 12
>> columns. Is it possible to take the mean of every four (or 12) columns
>>  (value601, value602, value603, value604 etc.in this case) and repeat for
>> the hundreds of columns? Thank you.
>>
>> Sincerely,
>>
>> Milu
>>
>> structure(list(value601 = c(10.1738710403442, 3.54112911224365,
>> 12.9192342758179, 3.17447590827942, 11.7332258224487, 7.68282270431519,
>> -7.11564493179321, 0.987620949745178, 13.0476207733154, 6.36939525604248
>> ), value602 = c(13.0642414093018, 5.53129482269287, 16.0519638061523,
>> 2.88946437835693, 14.9204912185669, 9.42428588867188, -6.80674123764038,
>> -0.614241063594818, 16.7947769165039, 7.9541072845459), value603 =
>> c(22.0399188995361,
>> 14.398024559021, 24.9523792266846, 12.0878629684448, 23.6459674835205,
>> 18.3277816772461, -2.54092741012573, 10.5550804138184, 25.1016540527344,
>> 16.2166938781738), value604 = c(27.7165412902832, 20.3255825042725,
>> 30.8430004119873, 16.6856250762939, 29.2485408782959, 24.3775005340576,
>> 6.47758340835571, 15.5897912979126, 30.7387924194336, 22.3637084960938
>> ), value605 = c(31.6644763946533, 23.4093952178955, 35.1488723754883,
>> 19.7132263183594, 33.3924179077148, 29.5846366882324, 10.2083873748779,
>> 19.3551616668701, 35.3076629638672, 27.4299201965332), value606 =
>> c(33.9698333740234,
>> 26.8574161529541, 36.8900833129883, 22.8604583740234, 34.8642921447754,
>> 33.8158760070801, 14.7055835723877, 22.1144580841064, 37.0545425415039,
>> 32.1087913513184), value607 = c(36.0279846191406, 26.9297180175781,
>> 38.2701225280762, 23.2643146514893, 36.7398796081543, 34.1216125488281,
>> 17.1387901306152, 24.0419750213623, 37.8542327880859, 32.7677421569824
>> ), value608 = c(34.0242347717285, 25.7720966339111, 36.4897193908691,
>> 22.0332260131836, 34.8011703491211, 32.6856842041016, 16.6232261657715,
>> 21.5571365356445, 36.1491546630859, 31.1716938018799), value609 =
>> c(27.5402088165283,
>> 21.7590408325195, 30.5214176177979, 18.4252090454102, 29.1156253814697,
>> 26.9878330230713, 12.4962501525879, 17.7259578704834, 30.9099159240723,
>> 25.4832077026367), value610 = c(23.4706859588623, 17.0126209259033,
>> 26.8166942596436, 15.297459602356, 25.1733055114746, 23.5616931915283,
>> 8.86995983123779, 13.5793552398682, 27.5732250213623, 22.1691932678223
>> ), value611 = c(14.5820417404175, 9.08279132843018, 17.8419170379639,
>> 8.36016654968262, 16.5633754730225, 14.8123331069946, 1.32095837593079,
>> 5.73408317565918, 18.9752082824707, 13.5729999542236), value612 =
>> c(9.12979793548584,
>> 2.79943537712097, 11.6504030227661, 2.21584677696228, 10.5404834747314,
>> 7.55471754074097, -5.58141136169434, -0.566209673881531, 12.3264112472534,
>> 6.65576601028442)), .Names = c("value601", "value602", "value603",
>> "value604", "value605", "value606", "value607", "value608", "value609",
>> "value610", "value611", "value612"), row.names = c("1", "2",
>> "3", "4", "5", "6", "7", "8", "9", "10"), class = "data.frame")
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
>
> --
> Dan Dalthorp, PhD
> USGS Forest and Rangeland Ecosystem Science Center
> Forest Sciences Lab, Rm 189
> 3200 SW Jefferson Way
> Corvallis, OR 97331
> ph: 541-750-0953
> ddalthorp at usgs.gov
>
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Wed Nov  9 17:57:48 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 9 Nov 2016 08:57:48 -0800
Subject: [R] prcomp() on correlation matrix
In-Reply-To: <1478695545911.13751@kent.ac.uk>
References: <1478695545911.13751@kent.ac.uk>
Message-ID: <CAGxFJbRsCthC=Ap0TxboU=4dQnxT+0ttOKaxSmW10bW9zm+Qow@mail.gmail.com>

Well, it seems you can't -- prcomp() seems to want the data matrix.

But it would be trivial using svd() -- or possibly even eigen() -- if
you understand the underlying linear algebra.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Nov 9, 2016 at 4:45 AM, T.Riedle <tr206 at kent.ac.uk> wrote:
> Dear R users,
>
> I am trying to do a Principal Components Analysis using the prcomp() function based on the correlation matrix. How can I determine to calculate PCA on a correlation or covariance matrix using prcomp()?
>
>
> Thanks in advance.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dcarlson at tamu.edu  Wed Nov  9 18:06:05 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Wed, 9 Nov 2016 17:06:05 +0000
Subject: [R] prcomp() on correlation matrix
In-Reply-To: <1478695545911.13751@kent.ac.uk>
References: <1478695545911.13751@kent.ac.uk>
Message-ID: <882c91a1533246aaa2b97817a48fa70b@exch-2p-mbx-w2.ads.tamu.edu>

Have you read the manual page for prcomp()?

?prcomp

Look at the examples, particularly the use of the scale.= argument.

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352



-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of T.Riedle
Sent: Wednesday, November 9, 2016 6:46 AM
To: R-help at r-project.org
Subject: [R] prcomp() on correlation matrix

Dear R users,

I am trying to do a Principal Components Analysis using the prcomp() function based on the correlation matrix. How can I determine to calculate PCA on a correlation or covariance matrix using prcomp()?


Thanks in advance.

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ddalthorp at usgs.gov  Wed Nov  9 18:14:13 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Wed, 9 Nov 2016 09:14:13 -0800
Subject: [R] Average every 4 columns
In-Reply-To: <CAMLwc7N-TUbc_xk5gaFhM4sXt007tL-iofVu8AFjBtSJ6HnJjg@mail.gmail.com>
References: <CAMLwc7O7R=Bg2jdZJDBcBV6oyU1fGRR4eBdi5R6SpJgKSPvdFQ@mail.gmail.com>
	<CAJeYpE_5ant_SMKyEmOpivW=8FS_AYYx671_dKrJYwrmt6S+rg@mail.gmail.com>
	<CAMLwc7N-TUbc_xk5gaFhM4sXt007tL-iofVu8AFjBtSJ6HnJjg@mail.gmail.com>
Message-ID: <CAJeYpE91yceGkNBzWcFb+gahotVzMRBV1zj9ZrJbxt3FBj+mpw@mail.gmail.com>

rowMeans(x[0:(dim(x)[1]/4-1)*4+1, ])

On Wed, Nov 9, 2016 at 9:00 AM, Miluji Sb <milujisb at gmail.com> wrote:

> Thanks a lot for your quick reply. I made a mistake in the question, I
> meant to ask every 4 (or 12) rows not columns. Apologies. Thanks again!
>
> Sincerely,
>
> Milu
>
> On Wed, Nov 9, 2016 at 5:45 PM, Dalthorp, Daniel <ddalthorp at usgs.gov>
> wrote:
>
>> Hi Milu,
>> The following should work for an array x (provided dim(x)[2] is divisible
>> by 4):
>>
>> colMeans(x[,0:(dim(x)[2]/4-1)*4+1])
>>
>> -Dan
>>
>> On Wed, Nov 9, 2016 at 8:29 AM, Miluji Sb <milujisb at gmail.com> wrote:
>>
>>> Dear all,
>>>
>>> I have a dataset with hundreds of columns, I am only providing only 12
>>> columns. Is it possible to take the mean of every four (or 12) columns
>>>  (value601, value602, value603, value604 etc.in this case) and repeat
>>> for
>>> the hundreds of columns? Thank you.
>>>
>>> Sincerely,
>>>
>>> Milu
>>>
>>> structure(list(value601 = c(10.1738710403442, 3.54112911224365,
>>> 12.9192342758179, 3.17447590827942, 11.7332258224487, 7.68282270431519,
>>> -7.11564493179321, 0.987620949745178, 13.0476207733154, 6.36939525604248
>>> ), value602 = c(13.0642414093018, 5.53129482269287, 16.0519638061523,
>>> 2.88946437835693, 14.9204912185669, 9.42428588867188, -6.80674123764038,
>>> -0.614241063594818, 16.7947769165039, 7.9541072845459), value603 =
>>> c(22.0399188995361,
>>> 14.398024559021, 24.9523792266846, 12.0878629684448, 23.6459674835205,
>>> 18.3277816772461, -2.54092741012573, 10.5550804138184, 25.1016540527344,
>>> 16.2166938781738), value604 = c(27.7165412902832, 20.3255825042725,
>>> 30.8430004119873, 16.6856250762939, 29.2485408782959, 24.3775005340576,
>>> 6.47758340835571, 15.5897912979126, 30.7387924194336, 22.3637084960938
>>> ), value605 = c(31.6644763946533, 23.4093952178955, 35.1488723754883,
>>> 19.7132263183594, 33.3924179077148, 29.5846366882324, 10.2083873748779,
>>> 19.3551616668701, 35.3076629638672, 27.4299201965332), value606 =
>>> c(33.9698333740234,
>>> 26.8574161529541, 36.8900833129883, 22.8604583740234, 34.8642921447754,
>>> 33.8158760070801, 14.7055835723877, 22.1144580841064, 37.0545425415039,
>>> 32.1087913513184), value607 = c(36.0279846191406, 26.9297180175781,
>>> 38.2701225280762, 23.2643146514893, 36.7398796081543, 34.1216125488281,
>>> 17.1387901306152, 24.0419750213623, 37.8542327880859, 32.7677421569824
>>> ), value608 = c(34.0242347717285, 25.7720966339111, 36.4897193908691,
>>> 22.0332260131836, 34.8011703491211, 32.6856842041016, 16.6232261657715,
>>> 21.5571365356445, 36.1491546630859, 31.1716938018799), value609 =
>>> c(27.5402088165283,
>>> 21.7590408325195, 30.5214176177979, 18.4252090454102, 29.1156253814697,
>>> 26.9878330230713, 12.4962501525879, 17.7259578704834, 30.9099159240723,
>>> 25.4832077026367), value610 = c(23.4706859588623, 17.0126209259033,
>>> 26.8166942596436, 15.297459602356, 25.1733055114746, 23.5616931915283,
>>> 8.86995983123779, 13.5793552398682, 27.5732250213623, 22.1691932678223
>>> ), value611 = c(14.5820417404175, 9.08279132843018, 17.8419170379639,
>>> 8.36016654968262, 16.5633754730225, 14.8123331069946, 1.32095837593079,
>>> 5.73408317565918, 18.9752082824707, 13.5729999542236), value612 =
>>> c(9.12979793548584,
>>> 2.79943537712097, 11.6504030227661, 2.21584677696228, 10.5404834747314,
>>> 7.55471754074097, -5.58141136169434, -0.566209673881531,
>>> 12.3264112472534,
>>> 6.65576601028442)), .Names = c("value601", "value602", "value603",
>>> "value604", "value605", "value606", "value607", "value608", "value609",
>>> "value610", "value611", "value612"), row.names = c("1", "2",
>>> "3", "4", "5", "6", "7", "8", "9", "10"), class = "data.frame")
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>> ng-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>
>>
>> --
>> Dan Dalthorp, PhD
>> USGS Forest and Rangeland Ecosystem Science Center
>> Forest Sciences Lab, Rm 189
>> 3200 SW Jefferson Way
>> Corvallis, OR 97331
>> ph: 541-750-0953
>> ddalthorp at usgs.gov
>>
>>
>


-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From ddalthorp at usgs.gov  Wed Nov  9 18:20:39 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Wed, 9 Nov 2016 09:20:39 -0800
Subject: [R] Average every 4 columns
In-Reply-To: <CAMLwc7N-TUbc_xk5gaFhM4sXt007tL-iofVu8AFjBtSJ6HnJjg@mail.gmail.com>
References: <CAMLwc7O7R=Bg2jdZJDBcBV6oyU1fGRR4eBdi5R6SpJgKSPvdFQ@mail.gmail.com>
	<CAJeYpE_5ant_SMKyEmOpivW=8FS_AYYx671_dKrJYwrmt6S+rg@mail.gmail.com>
	<CAMLwc7N-TUbc_xk5gaFhM4sXt007tL-iofVu8AFjBtSJ6HnJjg@mail.gmail.com>
Message-ID: <CAJeYpE8nUQg5puOboxNdKUF5g8KY_ou-KARH0i+DCfGGZeZnUA@mail.gmail.com>

Better is...

rowMeans(x[0:((dim(x)[1]-1)/4)*4+1, ])

On Wed, Nov 9, 2016 at 9:00 AM, Miluji Sb <milujisb at gmail.com> wrote:

> Thanks a lot for your quick reply. I made a mistake in the question, I
> meant to ask every 4 (or 12) rows not columns. Apologies. Thanks again!
>
> Sincerely,
>
> Milu
>
> On Wed, Nov 9, 2016 at 5:45 PM, Dalthorp, Daniel <ddalthorp at usgs.gov>
> wrote:
>
>> Hi Milu,
>> The following should work for an array x (provided dim(x)[2] is divisible
>> by 4):
>>
>> colMeans(x[,0:(dim(x)[2]/4-1)*4+1])
>>
>> -Dan
>>
>> On Wed, Nov 9, 2016 at 8:29 AM, Miluji Sb <milujisb at gmail.com> wrote:
>>
>>> Dear all,
>>>
>>> I have a dataset with hundreds of columns, I am only providing only 12
>>> columns. Is it possible to take the mean of every four (or 12) columns
>>>  (value601, value602, value603, value604 etc.in this case) and repeat
>>> for
>>> the hundreds of columns? Thank you.
>>>
>>> Sincerely,
>>>
>>> Milu
>>>
>>> structure(list(value601 = c(10.1738710403442, 3.54112911224365,
>>> 12.9192342758179, 3.17447590827942, 11.7332258224487, 7.68282270431519,
>>> -7.11564493179321, 0.987620949745178, 13.0476207733154, 6.36939525604248
>>> ), value602 = c(13.0642414093018, 5.53129482269287, 16.0519638061523,
>>> 2.88946437835693, 14.9204912185669, 9.42428588867188, -6.80674123764038,
>>> -0.614241063594818, 16.7947769165039, 7.9541072845459), value603 =
>>> c(22.0399188995361,
>>> 14.398024559021, 24.9523792266846, 12.0878629684448, 23.6459674835205,
>>> 18.3277816772461, -2.54092741012573, 10.5550804138184, 25.1016540527344,
>>> 16.2166938781738), value604 = c(27.7165412902832, 20.3255825042725,
>>> 30.8430004119873, 16.6856250762939, 29.2485408782959, 24.3775005340576,
>>> 6.47758340835571, 15.5897912979126, 30.7387924194336, 22.3637084960938
>>> ), value605 = c(31.6644763946533, 23.4093952178955, 35.1488723754883,
>>> 19.7132263183594, 33.3924179077148, 29.5846366882324, 10.2083873748779,
>>> 19.3551616668701, 35.3076629638672, 27.4299201965332), value606 =
>>> c(33.9698333740234,
>>> 26.8574161529541, 36.8900833129883, 22.8604583740234, 34.8642921447754,
>>> 33.8158760070801, 14.7055835723877, 22.1144580841064, 37.0545425415039,
>>> 32.1087913513184), value607 = c(36.0279846191406, 26.9297180175781,
>>> 38.2701225280762, 23.2643146514893, 36.7398796081543, 34.1216125488281,
>>> 17.1387901306152, 24.0419750213623, 37.8542327880859, 32.7677421569824
>>> ), value608 = c(34.0242347717285, 25.7720966339111, 36.4897193908691,
>>> 22.0332260131836, 34.8011703491211, 32.6856842041016, 16.6232261657715,
>>> 21.5571365356445, 36.1491546630859, 31.1716938018799), value609 =
>>> c(27.5402088165283,
>>> 21.7590408325195, 30.5214176177979, 18.4252090454102, 29.1156253814697,
>>> 26.9878330230713, 12.4962501525879, 17.7259578704834, 30.9099159240723,
>>> 25.4832077026367), value610 = c(23.4706859588623, 17.0126209259033,
>>> 26.8166942596436, 15.297459602356, 25.1733055114746, 23.5616931915283,
>>> 8.86995983123779, 13.5793552398682, 27.5732250213623, 22.1691932678223
>>> ), value611 = c(14.5820417404175, 9.08279132843018, 17.8419170379639,
>>> 8.36016654968262, 16.5633754730225, 14.8123331069946, 1.32095837593079,
>>> 5.73408317565918, 18.9752082824707, 13.5729999542236), value612 =
>>> c(9.12979793548584,
>>> 2.79943537712097, 11.6504030227661, 2.21584677696228, 10.5404834747314,
>>> 7.55471754074097, -5.58141136169434, -0.566209673881531,
>>> 12.3264112472534,
>>> 6.65576601028442)), .Names = c("value601", "value602", "value603",
>>> "value604", "value605", "value606", "value607", "value608", "value609",
>>> "value610", "value611", "value612"), row.names = c("1", "2",
>>> "3", "4", "5", "6", "7", "8", "9", "10"), class = "data.frame")
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>> ng-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>
>>
>> --
>> Dan Dalthorp, PhD
>> USGS Forest and Rangeland Ecosystem Science Center
>> Forest Sciences Lab, Rm 189
>> 3200 SW Jefferson Way
>> Corvallis, OR 97331
>> ph: 541-750-0953
>> ddalthorp at usgs.gov
>>
>>
>


-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Nov  9 18:24:33 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 9 Nov 2016 09:24:33 -0800
Subject: [R] Average every 4 columns
In-Reply-To: <CAMLwc7N-TUbc_xk5gaFhM4sXt007tL-iofVu8AFjBtSJ6HnJjg@mail.gmail.com>
References: <CAMLwc7O7R=Bg2jdZJDBcBV6oyU1fGRR4eBdi5R6SpJgKSPvdFQ@mail.gmail.com>
	<CAJeYpE_5ant_SMKyEmOpivW=8FS_AYYx671_dKrJYwrmt6S+rg@mail.gmail.com>
	<CAMLwc7N-TUbc_xk5gaFhM4sXt007tL-iofVu8AFjBtSJ6HnJjg@mail.gmail.com>
Message-ID: <EDC40B0A-FB40-465D-A31C-16F2A3B7F37C@comcast.net>


> On Nov 9, 2016, at 9:00 AM, Miluji Sb <milujisb at gmail.com> wrote:
> 
> Thanks a lot for your quick reply. I made a mistake in the question, I
> meant to ask every 4 (or 12) rows not columns. Apologies. Thanks again!

Then try matrix( sapply( split( x , 0:(dim(x)[2]/4-1)*4+1] ), mean) 

Again assuming that dim(x)[1] is evenly divisible by 4. (It's not clear if you wanted these means "within columns" or "blocks defined by rows". This is lightly tested but should provide the latter. 

sapply( split ( unlist(x) , 0:(dim(x)[2]/4-1)*4+1 ), mean)
       1        5        9 
19.44819 19.22003 19.12812 

> 
> Sincerely,
> 
> Milu
> 
> On Wed, Nov 9, 2016 at 5:45 PM, Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
> 
>> Hi Milu,
>> The following should work for an array x (provided dim(x)[2] is divisible
>> by 4):
>> 
>> colMeans(x[,
>> 
>> -Dan
>> 
>> On Wed, Nov 9, 2016 at 8:29 AM, Miluji Sb <milujisb at gmail.com> wrote:
>> 
>>> Dear all,
>>> 
>>> I have a dataset with hundreds of columns, I am only providing only 12
>>> columns. Is it possible to take the mean of every four (or 12) columns
>>> (value601, value602, value603, value604 etc.in this case) and repeat for
>>> the hundreds of columns? Thank you.
>>> 
>>> Sincerely,
>>> 
>>> Milu
>>> 
>>> structure(list(value601 = c(10.1738710403442, 3.54112911224365,
>>> 12.9192342758179, 3.17447590827942, 11.7332258224487, 7.68282270431519,
>>> -7.11564493179321, 0.987620949745178, 13.0476207733154, 6.36939525604248
>>> ), value602 = c(13.0642414093018, 5.53129482269287, 16.0519638061523,
>>> 2.88946437835693, 14.9204912185669, 9.42428588867188, -6.80674123764038,
>>> -0.614241063594818, 16.7947769165039, 7.9541072845459), value603 =
>>> c(22.0399188995361,
>>> 14.398024559021, 24.9523792266846, 12.0878629684448, 23.6459674835205,
>>> 18.3277816772461, -2.54092741012573, 10.5550804138184, 25.1016540527344,
>>> 16.2166938781738), value604 = c(27.7165412902832, 20.3255825042725,
>>> 30.8430004119873, 16.6856250762939, 29.2485408782959, 24.3775005340576,
>>> 6.47758340835571, 15.5897912979126, 30.7387924194336, 22.3637084960938
>>> ), value605 = c(31.6644763946533, 23.4093952178955, 35.1488723754883,
>>> 19.7132263183594, 33.3924179077148, 29.5846366882324, 10.2083873748779,
>>> 19.3551616668701, 35.3076629638672, 27.4299201965332), value606 =
>>> c(33.9698333740234,
>>> 26.8574161529541, 36.8900833129883, 22.8604583740234, 34.8642921447754,
>>> 33.8158760070801, 14.7055835723877, 22.1144580841064, 37.0545425415039,
>>> 32.1087913513184), value607 = c(36.0279846191406, 26.9297180175781,
>>> 38.2701225280762, 23.2643146514893, 36.7398796081543, 34.1216125488281,
>>> 17.1387901306152, 24.0419750213623, 37.8542327880859, 32.7677421569824
>>> ), value608 = c(34.0242347717285, 25.7720966339111, 36.4897193908691,
>>> 22.0332260131836, 34.8011703491211, 32.6856842041016, 16.6232261657715,
>>> 21.5571365356445, 36.1491546630859, 31.1716938018799), value609 =
>>> c(27.5402088165283,
>>> 21.7590408325195, 30.5214176177979, 18.4252090454102, 29.1156253814697,
>>> 26.9878330230713, 12.4962501525879, 17.7259578704834, 30.9099159240723,
>>> 25.4832077026367), value610 = c(23.4706859588623, 17.0126209259033,
>>> 26.8166942596436, 15.297459602356, 25.1733055114746, 23.5616931915283,
>>> 8.86995983123779, 13.5793552398682, 27.5732250213623, 22.1691932678223
>>> ), value611 = c(14.5820417404175, 9.08279132843018, 17.8419170379639,
>>> 8.36016654968262, 16.5633754730225, 14.8123331069946, 1.32095837593079,
>>> 5.73408317565918, 18.9752082824707, 13.5729999542236), value612 =
>>> c(9.12979793548584,
>>> 2.79943537712097, 11.6504030227661, 2.21584677696228, 10.5404834747314,
>>> 7.55471754074097, -5.58141136169434, -0.566209673881531, 12.3264112472534,
>>> 6.65576601028442)), .Names = c("value601", "value602", "value603",
>>> "value604", "value605", "value606", "value607", "value608", "value609",
>>> "value610", "value611", "value612"), row.names = c("1", "2",
>>> "3", "4", "5", "6", "7", "8", "9", "10"), class = "data.frame")
>>> 
>>>        [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>> ng-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> 
>> 
>> 
>> --
>> Dan Dalthorp, PhD
>> USGS Forest and Rangeland Ecosystem Science Center
>> Forest Sciences Lab, Rm 189
>> 3200 SW Jefferson Way
>> Corvallis, OR 97331
>> ph: 541-750-0953
>> ddalthorp at usgs.gov
>> 
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From dcarlson at tamu.edu  Wed Nov  9 18:33:17 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Wed, 9 Nov 2016 17:33:17 +0000
Subject: [R] prcomp() on correlation matrix
In-Reply-To: <CAGxFJbRsCthC=Ap0TxboU=4dQnxT+0ttOKaxSmW10bW9zm+Qow@mail.gmail.com>
References: <1478695545911.13751@kent.ac.uk>
	<CAGxFJbRsCthC=Ap0TxboU=4dQnxT+0ttOKaxSmW10bW9zm+Qow@mail.gmail.com>
Message-ID: <12e5772723be440e8e9d11181d93f167@exch-2p-mbx-w2.ads.tamu.edu>

I was assuming you had the data in my earlier reply. You could also look at the covmat= argument in princomp():

# Create some random data
> set.seed(42)
> dat <- mapply(rnorm, n=rep(100, 5), mean=(1:5)*5, sd=1:5)
# Construct covariance and correlation matrices
> dat.cov <- cov(dat)
> dat.cor <- cor(dat)
# Covariance matrix
> dat.cov.pca <- princomp(covmat=dat.cov)
> dat.cov.pca$loadings

Loadings:
     Comp.1 Comp.2 Comp.3 Comp.4 Comp.5
[1,]                              0.997
[2,]                      -0.994       
[3,]         0.269 -0.958              
[4,] -0.220 -0.941 -0.253              
[5,] -0.973  0.200                     

               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5
SS loadings       1.0    1.0    1.0    1.0    1.0
Proportion Var    0.2    0.2    0.2    0.2    0.2
Cumulative Var    0.2    0.4    0.6    0.8    1.0
# Correlation matrix
> dat.cor.pca <- princomp(covmat=dat.cor)
> dat.cor.pca$loadings

Loadings:
     Comp.1 Comp.2 Comp.3 Comp.4 Comp.5
[1,] -0.453 -0.486  0.234  0.619  0.348
[2,]  0.262 -0.193  0.851        -0.409
[3,]  0.279  0.691  0.313  0.374  0.455
[4,] -0.559  0.159  0.351 -0.629  0.377
[5,] -0.579  0.472         0.281 -0.602

               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5
SS loadings       1.0    1.0    1.0    1.0    1.0
Proportion Var    0.2    0.2    0.2    0.2    0.2
Cumulative Var    0.2    0.4    0.6    0.8    1.0

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bert Gunter
Sent: Wednesday, November 9, 2016 10:58 AM
To: T.Riedle
Cc: R-help at r-project.org
Subject: Re: [R] prcomp() on correlation matrix

Well, it seems you can't -- prcomp() seems to want the data matrix.

But it would be trivial using svd() -- or possibly even eigen() -- if
you understand the underlying linear algebra.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Nov 9, 2016 at 4:45 AM, T.Riedle <tr206 at kent.ac.uk> wrote:
> Dear R users,
>
> I am trying to do a Principal Components Analysis using the prcomp() function based on the correlation matrix. How can I determine to calculate PCA on a correlation or covariance matrix using prcomp()?
>
>
> Thanks in advance.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Wed Nov  9 18:36:24 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 9 Nov 2016 09:36:24 -0800
Subject: [R] Average every 4 columns
In-Reply-To: <EDC40B0A-FB40-465D-A31C-16F2A3B7F37C@comcast.net>
References: <CAMLwc7O7R=Bg2jdZJDBcBV6oyU1fGRR4eBdi5R6SpJgKSPvdFQ@mail.gmail.com>
	<CAJeYpE_5ant_SMKyEmOpivW=8FS_AYYx671_dKrJYwrmt6S+rg@mail.gmail.com>
	<CAMLwc7N-TUbc_xk5gaFhM4sXt007tL-iofVu8AFjBtSJ6HnJjg@mail.gmail.com>
	<EDC40B0A-FB40-465D-A31C-16F2A3B7F37C@comcast.net>
Message-ID: <6750D4D2-433B-4B03-BD77-9119084EA48A@comcast.net>


> On Nov 9, 2016, at 9:24 AM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
>> On Nov 9, 2016, at 9:00 AM, Miluji Sb <milujisb at gmail.com> wrote:
>> 
>> Thanks a lot for your quick reply. I made a mistake in the question, I
>> meant to ask every 4 (or 12) rows not columns. Apologies. Thanks again!
> 
> Then try matrix( sapply( split( x , 0:(dim(x)[2]/4-1)*4+1] ), mean) 
> 
> Again assuming that dim(x)[1] is evenly divisible by 4. (It's not clear if you wanted these means "within columns" or "blocks defined by rows". This is lightly tested but should provide the latter. 
> 
> sapply( split ( unlist(x) , 0:(dim(x)[2]/4-1)*4+1 ), mean)
>       1        5        9 
> 19.44819 19.22003 19.12812 

I failed to understand Daniel's logic an blindly copied his sequence which was not what I thought it would produce. 

Your request for "every 4" seems problematic in that your example has 10 rows and you haven't said what to do with the "bottom" 2 rows.

-- 
David
> 
>> 
>> Sincerely,
>> 
>> Milu
>> 
>> On Wed, Nov 9, 2016 at 5:45 PM, Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
>> 
>>> Hi Milu,
>>> The following should work for an array x (provided dim(x)[2] is divisible
>>> by 4):
>>> 
>>> colMeans(x[,
>>> 
>>> -Dan
>>> 
>>> On Wed, Nov 9, 2016 at 8:29 AM, Miluji Sb <milujisb at gmail.com> wrote:
>>> 
>>>> Dear all,
>>>> 
>>>> I have a dataset with hundreds of columns, I am only providing only 12
>>>> columns. Is it possible to take the mean of every four (or 12) columns
>>>> (value601, value602, value603, value604 etc.in this case) and repeat for
>>>> the hundreds of columns? Thank you.
>>>> 
>>>> Sincerely,
>>>> 
>>>> Milu
>>>> 
>>>> structure(list(value601 = c(10.1738710403442, 3.54112911224365,
>>>> 12.9192342758179, 3.17447590827942, 11.7332258224487, 7.68282270431519,
>>>> -7.11564493179321, 0.987620949745178, 13.0476207733154, 6.36939525604248
>>>> ), value602 = c(13.0642414093018, 5.53129482269287, 16.0519638061523,
>>>> 2.88946437835693, 14.9204912185669, 9.42428588867188, -6.80674123764038,
>>>> -0.614241063594818, 16.7947769165039, 7.9541072845459), value603 =
>>>> c(22.0399188995361,
>>>> 14.398024559021, 24.9523792266846, 12.0878629684448, 23.6459674835205,
>>>> 18.3277816772461, -2.54092741012573, 10.5550804138184, 25.1016540527344,
>>>> 16.2166938781738), value604 = c(27.7165412902832, 20.3255825042725,
>>>> 30.8430004119873, 16.6856250762939, 29.2485408782959, 24.3775005340576,
>>>> 6.47758340835571, 15.5897912979126, 30.7387924194336, 22.3637084960938
>>>> ), value605 = c(31.6644763946533, 23.4093952178955, 35.1488723754883,
>>>> 19.7132263183594, 33.3924179077148, 29.5846366882324, 10.2083873748779,
>>>> 19.3551616668701, 35.3076629638672, 27.4299201965332), value606 =
>>>> c(33.9698333740234,
>>>> 26.8574161529541, 36.8900833129883, 22.8604583740234, 34.8642921447754,
>>>> 33.8158760070801, 14.7055835723877, 22.1144580841064, 37.0545425415039,
>>>> 32.1087913513184), value607 = c(36.0279846191406, 26.9297180175781,
>>>> 38.2701225280762, 23.2643146514893, 36.7398796081543, 34.1216125488281,
>>>> 17.1387901306152, 24.0419750213623, 37.8542327880859, 32.7677421569824
>>>> ), value608 = c(34.0242347717285, 25.7720966339111, 36.4897193908691,
>>>> 22.0332260131836, 34.8011703491211, 32.6856842041016, 16.6232261657715,
>>>> 21.5571365356445, 36.1491546630859, 31.1716938018799), value609 =
>>>> c(27.5402088165283,
>>>> 21.7590408325195, 30.5214176177979, 18.4252090454102, 29.1156253814697,
>>>> 26.9878330230713, 12.4962501525879, 17.7259578704834, 30.9099159240723,
>>>> 25.4832077026367), value610 = c(23.4706859588623, 17.0126209259033,
>>>> 26.8166942596436, 15.297459602356, 25.1733055114746, 23.5616931915283,
>>>> 8.86995983123779, 13.5793552398682, 27.5732250213623, 22.1691932678223
>>>> ), value611 = c(14.5820417404175, 9.08279132843018, 17.8419170379639,
>>>> 8.36016654968262, 16.5633754730225, 14.8123331069946, 1.32095837593079,
>>>> 5.73408317565918, 18.9752082824707, 13.5729999542236), value612 =
>>>> c(9.12979793548584,
>>>> 2.79943537712097, 11.6504030227661, 2.21584677696228, 10.5404834747314,
>>>> 7.55471754074097, -5.58141136169434, -0.566209673881531, 12.3264112472534,
>>>> 6.65576601028442)), .Names = c("value601", "value602", "value603",
>>>> "value604", "value605", "value606", "value607", "value608", "value609",
>>>> "value610", "value611", "value612"), row.names = c("1", "2",
>>>> "3", "4", "5", "6", "7", "8", "9", "10"), class = "data.frame")
>>>> 
>>>>       [[alternative HTML version deleted]]
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>>> ng-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> 
>>> 
>>> 
>>> --
>>> Dan Dalthorp, PhD
>>> USGS Forest and Rangeland Ecosystem Science Center
>>> Forest Sciences Lab, Rm 189
>>> 3200 SW Jefferson Way
>>> Corvallis, OR 97331
>>> ph: 541-750-0953
>>> ddalthorp at usgs.gov
>>> 
>>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From macqueen1 at llnl.gov  Wed Nov  9 19:11:46 2016
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Wed, 9 Nov 2016 18:11:46 +0000
Subject: [R] Help with decrypting
In-Reply-To: <4D19B06A-E71A-4EF5-B47B-F34931932573@me.com>
References: <D446448F.18E7FC%macqueen1@llnl.gov>
	<4D19B06A-E71A-4EF5-B47B-F34931932573@me.com>
Message-ID: <D448975F.18ED96%macqueen1@llnl.gov>

Thanks, Marc,

Unfortunately, I didn't choose the encryption method. My (only) need is to be able to decrypt an existing file that is maintained by others. I am able to use system() on a simple perl script to do the job, but I'm hoping for an R-only solution, primarily for portability.

-Don

--
Don MacQueen
Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062


From: Marc Schwartz <marc_schwartz at me.com<mailto:marc_schwartz at me.com>>
Date: Monday, November 7, 2016 at 3:30 PM
To: dh m <macqueen1 at llnl.gov<mailto:macqueen1 at llnl.gov>>
Cc: R-help <R-help at r-project.org<mailto:R-help at r-project.org>>
Subject: Re: [R] Help with decrypting


On Nov 7, 2016, at 4:47 PM, MacQueen, Don <macqueen1 at llnl.gov<mailto:macqueen1 at llnl.gov>> wrote:

I have a file containing encrypted contents. The contents can be decrypted
using perl, like this:

open (FILEHANDLE, "/path/to/file")
chomp ($ciphertext = <FILEHANDLE>);


use Crypt::CBC;
$cipher = Crypt::CBC->new( -key    => 'my secret key',
                          -cipher => 'Blowfish'
                         );

$plaintext  = $cipher->decrypt($ciphertext);


(See http://search.cpan.org/~lds/Crypt-CBC-2.33/CBC.pm)

M goal is to have the value of $plaintext in an R object, so, is there an
R equivalent to this decrypt() perl function?

I've found R packages
 bcrypt
 sodium
that appear to have potential, but I don't understand this business well
enough to figure out how to use them, if indeed they can be used, for
this. Help would be much appreciated.

Thanks
-Don



Hi Don,

Blowfish is Bruce Schneier's algorithm from the early 90's, which even Bruce suggested some time ago not be used. Bruce has some alternative Blowfish implementations available via his web site:

  https://www.schneier.com/academic/blowfish/download.html

and there is a link there for some third party products that still have it and might provide for a CLI based interface as an alternative (e.g. GnuPG) if you need to use it.

>From what I can tell, 'sodium' does not support Blowfish and the implementation in bcrypt, if I am reading correctly, only provides for a one-way hash implementation, as opposed to encrypt/decrypt functions.

Thus, barring that my searching for alternative R implementations of Blowfish resulted in a Type II error, I do not see any R implementations of Blowfish that support encrypt/decrypt.

If you want to use the Perl module implementation via R, you can take a look at my WriteXLS package on GitHub:

  https://github.com/marcschwartz/WriteXLS

and see how I call Perl scripts within WriteXLS.R:

  https://github.com/marcschwartz/WriteXLS/blob/master/R/WriteXLS.R

around line 242.

That might provide one method for you.

That all being said, as per Bruce's recommendation, there are "better" encryption/decryption algorithms these days (some in the 'digest' package by Dirk), depending upon who you are trying to protect the data from... :-)

Regards,

Marc Schwartz



	[[alternative HTML version deleted]]


From macqueen1 at llnl.gov  Wed Nov  9 19:35:56 2016
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Wed, 9 Nov 2016 18:35:56 +0000
Subject: [R] Help with decrypting
In-Reply-To: <CAA-FpKXQoxweXbMqz+ta5efqw_74xgYDAjhZ-KoynrHFMWCVDA@mail.gmail.com>
References: <D446448F.18E7FC%macqueen1@llnl.gov>
	<CAA-FpKXQoxweXbMqz+ta5efqw_74xgYDAjhZ-KoynrHFMWCVDA@mail.gmail.com>
Message-ID: <D448A97C.18EE35%macqueen1@llnl.gov>

Bob,

Thanks for responding. I've tried the functions in bcrypt, and get, for
example,

> infl <- 'path.to.the.encrypted.file'
> junk <- 'the.password'
> foo <- readLines(infl)
Warning message:
In readLines(infl) :
  incomplete final line found on 'path.to.the.encrypted.file'
> tmp <- checkpw(junk, foo)
Error in hashpw(password, hash) : Invalid salt

Thus demonstrating that I don't know what I'm doing.

If it's easy to expand, as you mention, I would indeed appreciate it.

-Don


-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 11/7/16, 5:29 PM, "Bob Rudis" <bob at rud.is> wrote:

>Perhaps https://cran.r-project.org/web/packages/bcrypt/index.html
>might be of assistance.
>
>If not, drop a note back to the list as it'll be trivial to expand on
>that to give you an R alternative to Perl.
>
>On Mon, Nov 7, 2016 at 5:47 PM, MacQueen, Don <macqueen1 at llnl.gov> wrote:
>> I have a file containing encrypted contents. The contents can be
>>decrypted
>> using perl, like this:
>>
>> open (FILEHANDLE, "/path/to/file")
>> chomp ($ciphertext = <FILEHANDLE>);
>>
>>
>> use Crypt::CBC;
>> $cipher = Crypt::CBC->new( -key    => 'my secret key',
>>                            -cipher => 'Blowfish'
>>                           );
>>
>> $plaintext  = $cipher->decrypt($ciphertext);
>>
>>
>> (See http://search.cpan.org/~lds/Crypt-CBC-2.33/CBC.pm)
>>
>> M goal is to have the value of $plaintext in an R object, so, is there
>>an
>> R equivalent to this decrypt() perl function?
>>
>> I've found R packages
>>   bcrypt
>>   sodium
>> that appear to have potential, but I don't understand this business well
>> enough to figure out how to use them, if indeed they can be used, for
>> this. Help would be much appreciated.
>>
>> Thanks
>> -Don
>>
>> --
>> Don MacQueen
>>
>> Lawrence Livermore National Laboratory
>> 7000 East Ave., L-627
>> Livermore, CA 94550
>> 925-423-1062
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From evan.cooch at gmail.com  Wed Nov  9 19:32:54 2016
From: evan.cooch at gmail.com (Evan Cooch)
Date: Wed, 9 Nov 2016 13:32:54 -0500
Subject: [R] creating lists of random matrices
Message-ID: <d36e3fce-a4f9-7f89-da86-c5c3ec6df7e7@gmail.com>

So, its easy enough to create a random matrix, using something like (say)

matrix(rnorm(4),2,2)

which generates a (2x2) matrix with random N(0,1) in each cell.

But, what I need to be able to do is create a 'list' of such random 
matrices, where the length of the list (i.e., the number of said random 
matrices I store in the list) is some variable I can pass to the 
function (or loop).

I tried the obvious like

hold <- list()
for (i in 1:5) {
     hold[[i]] <- matrix(rnorm(4),2,2)
     }


While this works, it seems inelegant, and I'm wondering if there is a 
better (more efficient) way to accomplish the same thing -- perhaps 
avoiding the loop.

Thanks in advance...


From marc_schwartz at me.com  Wed Nov  9 19:41:38 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Wed, 09 Nov 2016 12:41:38 -0600
Subject: [R] creating lists of random matrices
In-Reply-To: <d36e3fce-a4f9-7f89-da86-c5c3ec6df7e7@gmail.com>
References: <d36e3fce-a4f9-7f89-da86-c5c3ec6df7e7@gmail.com>
Message-ID: <08720B15-3889-4CCA-9EC5-A601A5094F30@me.com>


> On Nov 9, 2016, at 12:32 PM, Evan Cooch <evan.cooch at gmail.com> wrote:
> 
> So, its easy enough to create a random matrix, using something like (say)
> 
> matrix(rnorm(4),2,2)
> 
> which generates a (2x2) matrix with random N(0,1) in each cell.
> 
> But, what I need to be able to do is create a 'list' of such random matrices, where the length of the list (i.e., the number of said random matrices I store in the list) is some variable I can pass to the function (or loop).
> 
> I tried the obvious like
> 
> hold <- list()
> for (i in 1:5) {
>    hold[[i]] <- matrix(rnorm(4),2,2)
>    }
> 
> 
> While this works, it seems inelegant, and I'm wondering if there is a better (more efficient) way to accomplish the same thing -- perhaps avoiding the loop.
> 
> Thanks in advance...


Hi,

See ?replicate

Example:

## Create a list of 5 2x2 matrices

> replicate(5, matrix(rnorm(4), 2, 2))
, , 1

           [,1]      [,2]
[1,] -0.1695775 1.0306685
[2,]  0.1636667 0.1044762

, , 2

           [,1]      [,2]
[1,] -0.3098566 2.1758363
[2,] -0.8029768 0.9697776

, , 3

           [,1]      [,2]
[1,]  0.5702972 0.7165806
[2,] -0.9731331 0.8332827

, , 4

           [,1]       [,2]
[1,] -0.8089588 0.09195256
[2,] -0.2026994 0.67545827

, , 5

          [,1]       [,2]
[1,] 0.5093008 -0.3097362
[2,] 0.6467358  0.3536414


Regards,

Marc Schwartz


	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Wed Nov  9 19:43:52 2016
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Wed, 09 Nov 2016 18:43:52 +0000
Subject: [R] creating lists of random matrices
In-Reply-To: <d36e3fce-a4f9-7f89-da86-c5c3ec6df7e7@gmail.com>
References: <d36e3fce-a4f9-7f89-da86-c5c3ec6df7e7@gmail.com>
Message-ID: <58236E68.6050809@sapo.pt>

Hello,

Try using ?lapply, its return value is a list. Note however that the 
*apply functions are loops in disguise. There's nothing wrong with your 
solution to the problem, but it's true that most R users find lapply 
more elegant, like you say.

hold <- lapply(1:5, function(x) matrix(rnorm(4),2,2) )

Hope this helps,

Rui Barradas

Em 09-11-2016 18:32, Evan Cooch escreveu:
> So, its easy enough to create a random matrix, using something like (say)
>
> matrix(rnorm(4),2,2)
>
> which generates a (2x2) matrix with random N(0,1) in each cell.
>
> But, what I need to be able to do is create a 'list' of such random
> matrices, where the length of the list (i.e., the number of said random
> matrices I store in the list) is some variable I can pass to the
> function (or loop).
>
> I tried the obvious like
>
> hold <- list()
> for (i in 1:5) {
>      hold[[i]] <- matrix(rnorm(4),2,2)
>      }
>
>
> While this works, it seems inelegant, and I'm wondering if there is a
> better (more efficient) way to accomplish the same thing -- perhaps
> avoiding the loop.
>
> Thanks in advance...
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at me.com  Wed Nov  9 19:51:19 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Wed, 09 Nov 2016 12:51:19 -0600
Subject: [R] creating lists of random matrices
In-Reply-To: <08720B15-3889-4CCA-9EC5-A601A5094F30@me.com>
References: <d36e3fce-a4f9-7f89-da86-c5c3ec6df7e7@gmail.com>
	<08720B15-3889-4CCA-9EC5-A601A5094F30@me.com>
Message-ID: <C393DA92-4A2E-4F76-9087-5FAFB20AAEA4@me.com>


> On Nov 9, 2016, at 12:41 PM, Marc Schwartz <marc_schwartz at me.com> wrote:
> 
> 
>> On Nov 9, 2016, at 12:32 PM, Evan Cooch <evan.cooch at gmail.com> wrote:
>> 
>> So, its easy enough to create a random matrix, using something like (say)
>> 
>> matrix(rnorm(4),2,2)
>> 
>> which generates a (2x2) matrix with random N(0,1) in each cell.
>> 
>> But, what I need to be able to do is create a 'list' of such random matrices, where the length of the list (i.e., the number of said random matrices I store in the list) is some variable I can pass to the function (or loop).
>> 
>> I tried the obvious like
>> 
>> hold <- list()
>> for (i in 1:5) {
>>   hold[[i]] <- matrix(rnorm(4),2,2)
>>   }
>> 
>> 
>> While this works, it seems inelegant, and I'm wondering if there is a better (more efficient) way to accomplish the same thing -- perhaps avoiding the loop.
>> 
>> Thanks in advance...
> 
> 
> Hi,
> 
> See ?replicate
> 
> Example:
> 
> ## Create a list of 5 2x2 matrices



Sorry, correction on my reply.

I copied the wrong output, It should be:

> replicate(5, matrix(rnorm(4), 2, 2), simplify = FALSE)
[[1]]
          [,1]      [,2]
[1,] 0.9700486 1.4249251
[2,] 0.7621312 0.8267747

[[2]]
          [,1]       [,2]
[1,] 0.4517927  0.2047509
[2,] 0.6336959 -0.6028124

[[3]]
          [,1]      [,2]
[1,] 0.6468823 0.4268734
[2,] 0.1664907 0.3905180

[[4]]
           [,1]       [,2]
[1,] -0.3170839 -1.0113201
[2,] -1.5356600  0.9658132

[[5]]
           [,1]       [,2]
[1,] -1.1937503 -0.2653502
[2,]  0.9319919  0.1780254


The 'simplify' argument should be FALSE, so that an array is not created.

Regards,

Marc Schwartz


	[[alternative HTML version deleted]]


From evan.cooch at gmail.com  Wed Nov  9 20:12:48 2016
From: evan.cooch at gmail.com (Evan Cooch)
Date: Wed, 9 Nov 2016 14:12:48 -0500
Subject: [R] creating lists of random matrices
In-Reply-To: <C393DA92-4A2E-4F76-9087-5FAFB20AAEA4@me.com>
References: <d36e3fce-a4f9-7f89-da86-c5c3ec6df7e7@gmail.com>
	<08720B15-3889-4CCA-9EC5-A601A5094F30@me.com>
	<C393DA92-4A2E-4F76-9087-5FAFB20AAEA4@me.com>
Message-ID: <187987d1-2c69-d5c4-4540-bd9a9ef86a4c@gmail.com>


>>
>> Hi,
>>
>> See ?replicate
>>
>> Example:
>>
>> ## Create a list of 5 2x2 matrices
>
>
> Sorry, correction on my reply.
>
> I copied the wrong output, It should be:
>
> > replicate(5, matrix(rnorm(4), 2, 2), simplify = FALSE)
>


Perfect does the trick. Thanks also for the lapply solutions from 
others. The replicate approach is more 'transparent' at least to me.

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Nov  9 20:27:12 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 9 Nov 2016 11:27:12 -0800
Subject: [R] Help with decrypting
In-Reply-To: <D448A97C.18EE35%macqueen1@llnl.gov>
References: <D446448F.18E7FC%macqueen1@llnl.gov>
	<CAA-FpKXQoxweXbMqz+ta5efqw_74xgYDAjhZ-KoynrHFMWCVDA@mail.gmail.com>
	<D448A97C.18EE35%macqueen1@llnl.gov>
Message-ID: <67157AD2-37A0-4B64-812C-FB5FCF21F741@comcast.net>


> On Nov 9, 2016, at 10:35 AM, MacQueen, Don <macqueen1 at llnl.gov> wrote:
> 
> Bob,
> 
> Thanks for responding. I've tried the functions in bcrypt, and get, for
> example,
> 
>> infl <- 'path.to.the.encrypted.file'
>> junk <- 'the.password'
>> foo <- readLines(infl)
> Warning message:
> In readLines(infl) :
>  incomplete final line found on 'path.to.the.encrypted.file'

I would not have expected that to succeed. readLines drops cr's and lf's and it certainly seems possible that those characters might be encrypted values randomly sprinkled through the image. What happens when you read it as raw? (may need to first get `file.info` to determine file length.)

perhaps something along the lines of 

foo <- readBin( infl, "raw", file.info(infl)[1, "size"])

-- 
David


>> tmp <- checkpw(junk, foo)
> Error in hashpw(password, hash) : Invalid salt
> 
> Thus demonstrating that I don't know what I'm doing.
> 
> If it's easy to expand, as you mention, I would indeed appreciate it.
> 
> -Don
> 
> 
> -- 
> Don MacQueen
> 
> Lawrence Livermore National Laboratory
> 7000 East Ave., L-627
> Livermore, CA 94550
> 925-423-1062
> 
> 
> 
> 
> 
> On 11/7/16, 5:29 PM, "Bob Rudis" <bob at rud.is> wrote:
> 
>> Perhaps https://cran.r-project.org/web/packages/bcrypt/index.html
>> might be of assistance.
>> 
>> If not, drop a note back to the list as it'll be trivial to expand on
>> that to give you an R alternative to Perl.
>> 
>> On Mon, Nov 7, 2016 at 5:47 PM, MacQueen, Don <macqueen1 at llnl.gov> wrote:
>>> I have a file containing encrypted contents. The contents can be
>>> decrypted
>>> using perl, like this:
>>> 
>>> open (FILEHANDLE, "/path/to/file")
>>> chomp ($ciphertext = <FILEHANDLE>);
>>> 
>>> 
>>> use Crypt::CBC;
>>> $cipher = Crypt::CBC->new( -key    => 'my secret key',
>>>                           -cipher => 'Blowfish'
>>>                          );
>>> 
>>> $plaintext  = $cipher->decrypt($ciphertext);
>>> 
>>> 
>>> (See http://search.cpan.org/~lds/Crypt-CBC-2.33/CBC.pm)
>>> 
>>> M goal is to have the value of $plaintext in an R object, so, is there
>>> an
>>> R equivalent to this decrypt() perl function?
>>> 
>>> I've found R packages
>>>  bcrypt
>>>  sodium
>>> that appear to have potential, but I don't understand this business well
>>> enough to figure out how to use them, if indeed they can be used, for
>>> this. Help would be much appreciated.
>>> 
>>> Thanks
>>> -Don
>>> 
>>> --
>>> Don MacQueen
>>> 
>>> Lawrence Livermore National Laboratory
>>> 7000 East Ave., L-627
>>> Livermore, CA 94550
>>> 925-423-1062
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From ligges at statistik.tu-dortmund.de  Wed Nov  9 20:31:14 2016
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Wed, 9 Nov 2016 20:31:14 +0100
Subject: [R] creating lists of random matrices
In-Reply-To: <C393DA92-4A2E-4F76-9087-5FAFB20AAEA4@me.com>
References: <d36e3fce-a4f9-7f89-da86-c5c3ec6df7e7@gmail.com>
	<08720B15-3889-4CCA-9EC5-A601A5094F30@me.com>
	<C393DA92-4A2E-4F76-9087-5FAFB20AAEA4@me.com>
Message-ID: <9e79f1d5-ee85-8f0c-0426-ca247a9f82ae@statistik.tu-dortmund.de>

Not answering the question, but if you have the same dimensions of the 
matrices everywhere, it is much more efficient to sample all numbers in 
a row and put stuff into an array:

array(rnorm(5*4), dim=c(2,2,5)))

Best,
Uwe Ligges




On 09.11.2016 19:51, Marc Schwartz wrote:
>
>> On Nov 9, 2016, at 12:41 PM, Marc Schwartz <marc_schwartz at me.com> wrote:
>>
>>
>>> On Nov 9, 2016, at 12:32 PM, Evan Cooch <evan.cooch at gmail.com> wrote:
>>>
>>> So, its easy enough to create a random matrix, using something like (say)
>>>
>>> matrix(rnorm(4),2,2)
>>>
>>> which generates a (2x2) matrix with random N(0,1) in each cell.
>>>
>>> But, what I need to be able to do is create a 'list' of such random matrices, where the length of the list (i.e., the number of said random matrices I store in the list) is some variable I can pass to the function (or loop).
>>>
>>> I tried the obvious like
>>>
>>> hold <- list()
>>> for (i in 1:5) {
>>>   hold[[i]] <- matrix(rnorm(4),2,2)
>>>   }
>>>
>>>
>>> While this works, it seems inelegant, and I'm wondering if there is a better (more efficient) way to accomplish the same thing -- perhaps avoiding the loop.
>>>
>>> Thanks in advance...
>>
>>
>> Hi,
>>
>> See ?replicate
>>
>> Example:
>>
>> ## Create a list of 5 2x2 matrices
>
>
>
> Sorry, correction on my reply.
>
> I copied the wrong output, It should be:
>
>> replicate(5, matrix(rnorm(4), 2, 2), simplify = FALSE)
> [[1]]
>           [,1]      [,2]
> [1,] 0.9700486 1.4249251
> [2,] 0.7621312 0.8267747
>
> [[2]]
>           [,1]       [,2]
> [1,] 0.4517927  0.2047509
> [2,] 0.6336959 -0.6028124
>
> [[3]]
>           [,1]      [,2]
> [1,] 0.6468823 0.4268734
> [2,] 0.1664907 0.3905180
>
> [[4]]
>            [,1]       [,2]
> [1,] -0.3170839 -1.0113201
> [2,] -1.5356600  0.9658132
>
> [[5]]
>            [,1]       [,2]
> [1,] -1.1937503 -0.2653502
> [2,]  0.9319919  0.1780254
>
>
> The 'simplify' argument should be FALSE, so that an array is not created.
>
> Regards,
>
> Marc Schwartz
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ruipbarradas at sapo.pt  Wed Nov  9 19:45:51 2016
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Wed, 09 Nov 2016 18:45:51 +0000
Subject: [R] creating lists of random matrices
In-Reply-To: <08720B15-3889-4CCA-9EC5-A601A5094F30@me.com>
References: <d36e3fce-a4f9-7f89-da86-c5c3ec6df7e7@gmail.com>
	<08720B15-3889-4CCA-9EC5-A601A5094F30@me.com>
Message-ID: <58236EDF.6020709@sapo.pt>

Hello,

I also thought of replicate() but it creates an 2x2x5 array, not a list.
Maybe it's all the same for the OP.

Rui Barradas

Em 09-11-2016 18:41, Marc Schwartz escreveu:
>
>> On Nov 9, 2016, at 12:32 PM, Evan Cooch <evan.cooch at gmail.com> wrote:
>>
>> So, its easy enough to create a random matrix, using something like (say)
>>
>> matrix(rnorm(4),2,2)
>>
>> which generates a (2x2) matrix with random N(0,1) in each cell.
>>
>> But, what I need to be able to do is create a 'list' of such random matrices, where the length of the list (i.e., the number of said random matrices I store in the list) is some variable I can pass to the function (or loop).
>>
>> I tried the obvious like
>>
>> hold <- list()
>> for (i in 1:5) {
>>     hold[[i]] <- matrix(rnorm(4),2,2)
>>     }
>>
>>
>> While this works, it seems inelegant, and I'm wondering if there is a better (more efficient) way to accomplish the same thing -- perhaps avoiding the loop.
>>
>> Thanks in advance...
>
>
> Hi,
>
> See ?replicate
>
> Example:
>
> ## Create a list of 5 2x2 matrices
>
>> replicate(5, matrix(rnorm(4), 2, 2))
> , , 1
>
>             [,1]      [,2]
> [1,] -0.1695775 1.0306685
> [2,]  0.1636667 0.1044762
>
> , , 2
>
>             [,1]      [,2]
> [1,] -0.3098566 2.1758363
> [2,] -0.8029768 0.9697776
>
> , , 3
>
>             [,1]      [,2]
> [1,]  0.5702972 0.7165806
> [2,] -0.9731331 0.8332827
>
> , , 4
>
>             [,1]       [,2]
> [1,] -0.8089588 0.09195256
> [2,] -0.2026994 0.67545827
>
> , , 5
>
>            [,1]       [,2]
> [1,] 0.5093008 -0.3097362
> [2,] 0.6467358  0.3536414
>
>
> Regards,
>
> Marc Schwartz
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From marc_schwartz at me.com  Wed Nov  9 20:33:18 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Wed, 09 Nov 2016 13:33:18 -0600
Subject: [R] Help with decrypting
In-Reply-To: <D448975F.18ED96%macqueen1@llnl.gov>
References: <D446448F.18E7FC%macqueen1@llnl.gov>
	<4D19B06A-E71A-4EF5-B47B-F34931932573@me.com>
	<D448975F.18ED96%macqueen1@llnl.gov>
Message-ID: <32344178-D344-4714-A3C9-D8728D06D23E@me.com>

Hi Don,

Since there is not a current R package implementation of the blowfish algorithm for encyrpt/decrypt, as far as I can tell, your only options via R would be to call an external script/program that provides that functionality from within R, or consider integrating one of the compiled language versions (e.g. C/C++) into an R package that you would create, presuming that the licenses for those implementations would allow for that. The algorithm itself is public domain thanks to Bruce.

There are "interpreted" versions of the Blowfish implementation available, including one in pure Perl in the Crypt module and a Visual Basic version, which you could possibly follow and re-write in pure R. But going that route risks introducing errors in the implementation, which needless to say, is not a risk you really want to take.

However, in general, encrypt/decrypt algorithms are designed to be intentionally difficult and are usually CPU bound, since they are doing multiple rounds of bit-level manipulations. Thus, using interpreted program versions for anything but a small amount of text is typically going to be problematic compute time-wise.

That is why they are generally implemented in compiled languages or for more complex algorithms, in firmware.

BTW, vis-a-vis your reply back to Bob, as I noted, the variant of Blowfish that is in the bcrypt package on CRAN will not do what you want. It is designed to do one thing, encrypt passwords using a non-reversible algorithm (hash) for storage and later comparison with an entered cleartext password, that is encrypted using the same algorithm. 

Regards,

Marc


> On Nov 9, 2016, at 12:11 PM, MacQueen, Don <macqueen1 at llnl.gov> wrote:
> 
> Thanks, Marc,
> 
> Unfortunately, I didn't choose the encryption method. My (only) need is to be able to decrypt an existing file that is maintained by others. I am able to use system() on a simple perl script to do the job, but I'm hoping for an R-only solution, primarily for portability.
> 
> -Don
> 
> -- 
> Don MacQueen
> Lawrence Livermore National Laboratory
> 7000 East Ave., L-627
> Livermore, CA 94550
> 925-423-1062
> 
> 
> From: Marc Schwartz <marc_schwartz at me.com <mailto:marc_schwartz at me.com>>
> Date: Monday, November 7, 2016 at 3:30 PM
> To: dh m <macqueen1 at llnl.gov <mailto:macqueen1 at llnl.gov>>
> Cc: R-help <R-help at r-project.org <mailto:R-help at r-project.org>>
> Subject: Re: [R] Help with decrypting
> 
> 
>> On Nov 7, 2016, at 4:47 PM, MacQueen, Don <macqueen1 at llnl.gov <mailto:macqueen1 at llnl.gov>> wrote:
>> 
>> I have a file containing encrypted contents. The contents can be decrypted
>> using perl, like this:
>> 
>> open (FILEHANDLE, "/path/to/file")
>> chomp ($ciphertext = <FILEHANDLE>);
>> 
>> 
>> use Crypt::CBC;
>> $cipher = Crypt::CBC->new( -key    => 'my secret key',
>>                           -cipher => 'Blowfish'
>>                          );
>> 
>> $plaintext  = $cipher->decrypt($ciphertext);
>> 
>> 
>> (See http://search.cpan.org/~lds/Crypt-CBC-2.33/CBC.pm <http://search.cpan.org/~lds/Crypt-CBC-2.33/CBC.pm>)
>> 
>> M goal is to have the value of $plaintext in an R object, so, is there an
>> R equivalent to this decrypt() perl function?
>> 
>> I've found R packages
>>  bcrypt
>>  sodium
>> that appear to have potential, but I don't understand this business well
>> enough to figure out how to use them, if indeed they can be used, for
>> this. Help would be much appreciated.
>> 
>> Thanks
>> -Don
>> 
> 
> 
> 
> Hi Don,
> 
> Blowfish is Bruce Schneier's algorithm from the early 90's, which even Bruce suggested some time ago not be used. Bruce has some alternative Blowfish implementations available via his web site:
> 
>   https://www.schneier.com/academic/blowfish/download.html <https://www.schneier.com/academic/blowfish/download.html>
> 
> and there is a link there for some third party products that still have it and might provide for a CLI based interface as an alternative (e.g. GnuPG) if you need to use it.
> 
> From what I can tell, 'sodium' does not support Blowfish and the implementation in bcrypt, if I am reading correctly, only provides for a one-way hash implementation, as opposed to encrypt/decrypt functions.
> 
> Thus, barring that my searching for alternative R implementations of Blowfish resulted in a Type II error, I do not see any R implementations of Blowfish that support encrypt/decrypt.
> 
> If you want to use the Perl module implementation via R, you can take a look at my WriteXLS package on GitHub:
> 
>   https://github.com/marcschwartz/WriteXLS <https://github.com/marcschwartz/WriteXLS>
> 
> and see how I call Perl scripts within WriteXLS.R:
> 
>   https://github.com/marcschwartz/WriteXLS/blob/master/R/WriteXLS.R <https://github.com/marcschwartz/WriteXLS/blob/master/R/WriteXLS.R>
> 
> around line 242.
> 
> That might provide one method for you.
> 
> That all being said, as per Bruce's recommendation, there are "better" encryption/decryption algorithms these days (some in the 'digest' package by Dirk), depending upon who you are trying to protect the data from... :-)
> 
> Regards,
> 
> Marc Schwartz
> 
> 


	[[alternative HTML version deleted]]


From macqueen1 at llnl.gov  Wed Nov  9 21:50:18 2016
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Wed, 9 Nov 2016 20:50:18 +0000
Subject: [R] Help with decrypting
In-Reply-To: <67157AD2-37A0-4B64-812C-FB5FCF21F741@comcast.net>
References: <D446448F.18E7FC%macqueen1@llnl.gov>
	<CAA-FpKXQoxweXbMqz+ta5efqw_74xgYDAjhZ-KoynrHFMWCVDA@mail.gmail.com>
	<D448A97C.18EE35%macqueen1@llnl.gov>
	<67157AD2-37A0-4B64-812C-FB5FCF21F741@comcast.net>
Message-ID: <D448C9A4.18EEA6%macqueen1@llnl.gov>

Thanks, David.

It does read without error using readBin. And using file.info as you
suggest seem to be a step in the right direction (the length of foo agrees
with 'wc -l' on the file). I just get different errors on subsequent
attempts to use it.

(I should have mentioned in the beginning, whatever I do, I'd like it to
work on both Mac and Linux.)

And thanks again to all three of you. I'm about ready to give up on an R
solution, and call an external script as Marc suggests.

-Don

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 11/9/16, 11:27 AM, "David Winsemius" <dwinsemius at comcast.net> wrote:

>
>> On Nov 9, 2016, at 10:35 AM, MacQueen, Don <macqueen1 at llnl.gov> wrote:
>> 
>> Bob,
>> 
>> Thanks for responding. I've tried the functions in bcrypt, and get, for
>> example,
>> 
>>> infl <- 'path.to.the.encrypted.file'
>>> junk <- 'the.password'
>>> foo <- readLines(infl)
>> Warning message:
>> In readLines(infl) :
>>  incomplete final line found on 'path.to.the.encrypted.file'
>
>I would not have expected that to succeed. readLines drops cr's and lf's
>and it certainly seems possible that those characters might be encrypted
>values randomly sprinkled through the image. What happens when you read
>it as raw? (may need to first get `file.info` to determine file length.)
>
>perhaps something along the lines of
>
>foo <- readBin( infl, "raw", file.info(infl)[1, "size"])
>
>-- 
>David
>
>
>>> tmp <- checkpw(junk, foo)
>> Error in hashpw(password, hash) : Invalid salt
>> 
>> Thus demonstrating that I don't know what I'm doing.
>> 
>> If it's easy to expand, as you mention, I would indeed appreciate it.
>> 
>> -Don
>> 
>> 
>> -- 
>> Don MacQueen
>> 
>> Lawrence Livermore National Laboratory
>> 7000 East Ave., L-627
>> Livermore, CA 94550
>> 925-423-1062
>> 
>> 
>> 
>> 
>> 
>> On 11/7/16, 5:29 PM, "Bob Rudis" <bob at rud.is> wrote:
>> 
>>> Perhaps https://cran.r-project.org/web/packages/bcrypt/index.html
>>> might be of assistance.
>>> 
>>> If not, drop a note back to the list as it'll be trivial to expand on
>>> that to give you an R alternative to Perl.
>>> 
>>> On Mon, Nov 7, 2016 at 5:47 PM, MacQueen, Don <macqueen1 at llnl.gov>
>>>wrote:
>>>> I have a file containing encrypted contents. The contents can be
>>>> decrypted
>>>> using perl, like this:
>>>> 
>>>> open (FILEHANDLE, "/path/to/file")
>>>> chomp ($ciphertext = <FILEHANDLE>);
>>>> 
>>>> 
>>>> use Crypt::CBC;
>>>> $cipher = Crypt::CBC->new( -key    => 'my secret key',
>>>>                           -cipher => 'Blowfish'
>>>>                          );
>>>> 
>>>> $plaintext  = $cipher->decrypt($ciphertext);
>>>> 
>>>> 
>>>> (See http://search.cpan.org/~lds/Crypt-CBC-2.33/CBC.pm)
>>>> 
>>>> M goal is to have the value of $plaintext in an R object, so, is there
>>>> an
>>>> R equivalent to this decrypt() perl function?
>>>> 
>>>> I've found R packages
>>>>  bcrypt
>>>>  sodium
>>>> that appear to have potential, but I don't understand this business
>>>>well
>>>> enough to figure out how to use them, if indeed they can be used, for
>>>> this. Help would be much appreciated.
>>>> 
>>>> Thanks
>>>> -Don
>>>> 
>>>> --
>>>> Don MacQueen
>>>> 
>>>> Lawrence Livermore National Laboratory
>>>> 7000 East Ave., L-627
>>>> Livermore, CA 94550
>>>> 925-423-1062
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>David Winsemius
>Alameda, CA, USA
>


From drjimlemon at gmail.com  Wed Nov  9 22:06:26 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 10 Nov 2016 08:06:26 +1100
Subject: [R] Average every 4 columns
In-Reply-To: <CAMLwc7N-TUbc_xk5gaFhM4sXt007tL-iofVu8AFjBtSJ6HnJjg@mail.gmail.com>
References: <CAMLwc7O7R=Bg2jdZJDBcBV6oyU1fGRR4eBdi5R6SpJgKSPvdFQ@mail.gmail.com>
	<CAJeYpE_5ant_SMKyEmOpivW=8FS_AYYx671_dKrJYwrmt6S+rg@mail.gmail.com>
	<CAMLwc7N-TUbc_xk5gaFhM4sXt007tL-iofVu8AFjBtSJ6HnJjg@mail.gmail.com>
Message-ID: <CA+8X3fUshKQBi1U7SR5zkWE=3cAK2NxAjvrSY-TNHB6z=Oxopg@mail.gmail.com>

Hi Milu,
Perhaps this will help:

apply(as.matrix(x[-1,seq(1:dim(x)[1],by=4)]),1,mean)

Jim


On Thu, Nov 10, 2016 at 4:00 AM, Miluji Sb <milujisb at gmail.com> wrote:
> Thanks a lot for your quick reply. I made a mistake in the question, I
> meant to ask every 4 (or 12) rows not columns. Apologies. Thanks again!
>
> Sincerely,
>
> Milu
>
> On Wed, Nov 9, 2016 at 5:45 PM, Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
>
>> Hi Milu,
>> The following should work for an array x (provided dim(x)[2] is divisible
>> by 4):
>>
>> colMeans(x[,0:(dim(x)[2]/4-1)*4+1])
>>
>> -Dan
>>
>> On Wed, Nov 9, 2016 at 8:29 AM, Miluji Sb <milujisb at gmail.com> wrote:
>>
>>> Dear all,
>>>
>>> I have a dataset with hundreds of columns, I am only providing only 12
>>> columns. Is it possible to take the mean of every four (or 12) columns
>>>  (value601, value602, value603, value604 etc.in this case) and repeat for
>>> the hundreds of columns? Thank you.
>>>
>>> Sincerely,
>>>
>>> Milu
>>>
>>> structure(list(value601 = c(10.1738710403442, 3.54112911224365,
>>> 12.9192342758179, 3.17447590827942, 11.7332258224487, 7.68282270431519,
>>> -7.11564493179321, 0.987620949745178, 13.0476207733154, 6.36939525604248
>>> ), value602 = c(13.0642414093018, 5.53129482269287, 16.0519638061523,
>>> 2.88946437835693, 14.9204912185669, 9.42428588867188, -6.80674123764038,
>>> -0.614241063594818, 16.7947769165039, 7.9541072845459), value603 =
>>> c(22.0399188995361,
>>> 14.398024559021, 24.9523792266846, 12.0878629684448, 23.6459674835205,
>>> 18.3277816772461, -2.54092741012573, 10.5550804138184, 25.1016540527344,
>>> 16.2166938781738), value604 = c(27.7165412902832, 20.3255825042725,
>>> 30.8430004119873, 16.6856250762939, 29.2485408782959, 24.3775005340576,
>>> 6.47758340835571, 15.5897912979126, 30.7387924194336, 22.3637084960938
>>> ), value605 = c(31.6644763946533, 23.4093952178955, 35.1488723754883,
>>> 19.7132263183594, 33.3924179077148, 29.5846366882324, 10.2083873748779,
>>> 19.3551616668701, 35.3076629638672, 27.4299201965332), value606 =
>>> c(33.9698333740234,
>>> 26.8574161529541, 36.8900833129883, 22.8604583740234, 34.8642921447754,
>>> 33.8158760070801, 14.7055835723877, 22.1144580841064, 37.0545425415039,
>>> 32.1087913513184), value607 = c(36.0279846191406, 26.9297180175781,
>>> 38.2701225280762, 23.2643146514893, 36.7398796081543, 34.1216125488281,
>>> 17.1387901306152, 24.0419750213623, 37.8542327880859, 32.7677421569824
>>> ), value608 = c(34.0242347717285, 25.7720966339111, 36.4897193908691,
>>> 22.0332260131836, 34.8011703491211, 32.6856842041016, 16.6232261657715,
>>> 21.5571365356445, 36.1491546630859, 31.1716938018799), value609 =
>>> c(27.5402088165283,
>>> 21.7590408325195, 30.5214176177979, 18.4252090454102, 29.1156253814697,
>>> 26.9878330230713, 12.4962501525879, 17.7259578704834, 30.9099159240723,
>>> 25.4832077026367), value610 = c(23.4706859588623, 17.0126209259033,
>>> 26.8166942596436, 15.297459602356, 25.1733055114746, 23.5616931915283,
>>> 8.86995983123779, 13.5793552398682, 27.5732250213623, 22.1691932678223
>>> ), value611 = c(14.5820417404175, 9.08279132843018, 17.8419170379639,
>>> 8.36016654968262, 16.5633754730225, 14.8123331069946, 1.32095837593079,
>>> 5.73408317565918, 18.9752082824707, 13.5729999542236), value612 =
>>> c(9.12979793548584,
>>> 2.79943537712097, 11.6504030227661, 2.21584677696228, 10.5404834747314,
>>> 7.55471754074097, -5.58141136169434, -0.566209673881531, 12.3264112472534,
>>> 6.65576601028442)), .Names = c("value601", "value602", "value603",
>>> "value604", "value605", "value606", "value607", "value608", "value609",
>>> "value610", "value611", "value612"), row.names = c("1", "2",
>>> "3", "4", "5", "6", "7", "8", "9", "10"), class = "data.frame")
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>> ng-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>
>>
>> --
>> Dan Dalthorp, PhD
>> USGS Forest and Rangeland Ecosystem Science Center
>> Forest Sciences Lab, Rm 189
>> 3200 SW Jefferson Way
>> Corvallis, OR 97331
>> ph: 541-750-0953
>> ddalthorp at usgs.gov
>>
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.tu-dortmund.de  Wed Nov  9 22:30:11 2016
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Wed, 9 Nov 2016 22:30:11 +0100
Subject: [R] Average every 4 columns
In-Reply-To: <CA+8X3fUshKQBi1U7SR5zkWE=3cAK2NxAjvrSY-TNHB6z=Oxopg@mail.gmail.com>
References: <CAMLwc7O7R=Bg2jdZJDBcBV6oyU1fGRR4eBdi5R6SpJgKSPvdFQ@mail.gmail.com>
	<CAJeYpE_5ant_SMKyEmOpivW=8FS_AYYx671_dKrJYwrmt6S+rg@mail.gmail.com>
	<CAMLwc7N-TUbc_xk5gaFhM4sXt007tL-iofVu8AFjBtSJ6HnJjg@mail.gmail.com>
	<CA+8X3fUshKQBi1U7SR5zkWE=3cAK2NxAjvrSY-TNHB6z=Oxopg@mail.gmail.com>
Message-ID: <c4ab9448-ca38-854c-6165-0263c20e558f@statistik.tu-dortmund.de>



On 09.11.2016 22:06, Jim Lemon wrote:
> Hi Milu,
> Perhaps this will help:
>
> apply(as.matrix(x[-1,seq(1:dim(x)[1],by=4)]),1,mean)

More efficient than apply(..., 1, mean): rowMeans()

Best,
Uwe Ligges



>
> Jim
>
>
> On Thu, Nov 10, 2016 at 4:00 AM, Miluji Sb <milujisb at gmail.com> wrote:
>> Thanks a lot for your quick reply. I made a mistake in the question, I
>> meant to ask every 4 (or 12) rows not columns. Apologies. Thanks again!
>>
>> Sincerely,
>>
>> Milu
>>
>> On Wed, Nov 9, 2016 at 5:45 PM, Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
>>
>>> Hi Milu,
>>> The following should work for an array x (provided dim(x)[2] is divisible
>>> by 4):
>>>
>>> colMeans(x[,0:(dim(x)[2]/4-1)*4+1])
>>>
>>> -Dan
>>>
>>> On Wed, Nov 9, 2016 at 8:29 AM, Miluji Sb <milujisb at gmail.com> wrote:
>>>
>>>> Dear all,
>>>>
>>>> I have a dataset with hundreds of columns, I am only providing only 12
>>>> columns. Is it possible to take the mean of every four (or 12) columns
>>>>  (value601, value602, value603, value604 etc.in this case) and repeat for
>>>> the hundreds of columns? Thank you.
>>>>
>>>> Sincerely,
>>>>
>>>> Milu
>>>>
>>>> structure(list(value601 = c(10.1738710403442, 3.54112911224365,
>>>> 12.9192342758179, 3.17447590827942, 11.7332258224487, 7.68282270431519,
>>>> -7.11564493179321, 0.987620949745178, 13.0476207733154, 6.36939525604248
>>>> ), value602 = c(13.0642414093018, 5.53129482269287, 16.0519638061523,
>>>> 2.88946437835693, 14.9204912185669, 9.42428588867188, -6.80674123764038,
>>>> -0.614241063594818, 16.7947769165039, 7.9541072845459), value603 =
>>>> c(22.0399188995361,
>>>> 14.398024559021, 24.9523792266846, 12.0878629684448, 23.6459674835205,
>>>> 18.3277816772461, -2.54092741012573, 10.5550804138184, 25.1016540527344,
>>>> 16.2166938781738), value604 = c(27.7165412902832, 20.3255825042725,
>>>> 30.8430004119873, 16.6856250762939, 29.2485408782959, 24.3775005340576,
>>>> 6.47758340835571, 15.5897912979126, 30.7387924194336, 22.3637084960938
>>>> ), value605 = c(31.6644763946533, 23.4093952178955, 35.1488723754883,
>>>> 19.7132263183594, 33.3924179077148, 29.5846366882324, 10.2083873748779,
>>>> 19.3551616668701, 35.3076629638672, 27.4299201965332), value606 =
>>>> c(33.9698333740234,
>>>> 26.8574161529541, 36.8900833129883, 22.8604583740234, 34.8642921447754,
>>>> 33.8158760070801, 14.7055835723877, 22.1144580841064, 37.0545425415039,
>>>> 32.1087913513184), value607 = c(36.0279846191406, 26.9297180175781,
>>>> 38.2701225280762, 23.2643146514893, 36.7398796081543, 34.1216125488281,
>>>> 17.1387901306152, 24.0419750213623, 37.8542327880859, 32.7677421569824
>>>> ), value608 = c(34.0242347717285, 25.7720966339111, 36.4897193908691,
>>>> 22.0332260131836, 34.8011703491211, 32.6856842041016, 16.6232261657715,
>>>> 21.5571365356445, 36.1491546630859, 31.1716938018799), value609 =
>>>> c(27.5402088165283,
>>>> 21.7590408325195, 30.5214176177979, 18.4252090454102, 29.1156253814697,
>>>> 26.9878330230713, 12.4962501525879, 17.7259578704834, 30.9099159240723,
>>>> 25.4832077026367), value610 = c(23.4706859588623, 17.0126209259033,
>>>> 26.8166942596436, 15.297459602356, 25.1733055114746, 23.5616931915283,
>>>> 8.86995983123779, 13.5793552398682, 27.5732250213623, 22.1691932678223
>>>> ), value611 = c(14.5820417404175, 9.08279132843018, 17.8419170379639,
>>>> 8.36016654968262, 16.5633754730225, 14.8123331069946, 1.32095837593079,
>>>> 5.73408317565918, 18.9752082824707, 13.5729999542236), value612 =
>>>> c(9.12979793548584,
>>>> 2.79943537712097, 11.6504030227661, 2.21584677696228, 10.5404834747314,
>>>> 7.55471754074097, -5.58141136169434, -0.566209673881531, 12.3264112472534,
>>>> 6.65576601028442)), .Names = c("value601", "value602", "value603",
>>>> "value604", "value605", "value606", "value607", "value608", "value609",
>>>> "value610", "value611", "value612"), row.names = c("1", "2",
>>>> "3", "4", "5", "6", "7", "8", "9", "10"), class = "data.frame")
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>>> ng-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>
>>>
>>>
>>> --
>>> Dan Dalthorp, PhD
>>> USGS Forest and Rangeland Ecosystem Science Center
>>> Forest Sciences Lab, Rm 189
>>> 3200 SW Jefferson Way
>>> Corvallis, OR 97331
>>> ph: 541-750-0953
>>> ddalthorp at usgs.gov
>>>
>>>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From drjimlemon at gmail.com  Wed Nov  9 23:00:38 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 10 Nov 2016 09:00:38 +1100
Subject: [R] Average every 4 columns
In-Reply-To: <c4ab9448-ca38-854c-6165-0263c20e558f@statistik.tu-dortmund.de>
References: <CAMLwc7O7R=Bg2jdZJDBcBV6oyU1fGRR4eBdi5R6SpJgKSPvdFQ@mail.gmail.com>
	<CAJeYpE_5ant_SMKyEmOpivW=8FS_AYYx671_dKrJYwrmt6S+rg@mail.gmail.com>
	<CAMLwc7N-TUbc_xk5gaFhM4sXt007tL-iofVu8AFjBtSJ6HnJjg@mail.gmail.com>
	<CA+8X3fUshKQBi1U7SR5zkWE=3cAK2NxAjvrSY-TNHB6z=Oxopg@mail.gmail.com>
	<c4ab9448-ca38-854c-6165-0263c20e558f@statistik.tu-dortmund.de>
Message-ID: <CA+8X3fUOv7qSoAzbTZr4BDS6s4cy1bpkYWBjbkK15C_5VFHeTg@mail.gmail.com>

Thanks - it made me realize that I had reversed the column and row selection

rowMeans(x[seq(1:dim(x)[1],by=4),-1])

Jim


On Thu, Nov 10, 2016 at 8:30 AM, Uwe Ligges
<ligges at statistik.tu-dortmund.de> wrote:
>
>
> On 09.11.2016 22:06, Jim Lemon wrote:
>>
>> Hi Milu,
>> Perhaps this will help:
>>
>> apply(as.matrix(x[-1,seq(1:dim(x)[1],by=4)]),1,mean)
>
>
> More efficient than apply(..., 1, mean): rowMeans()
>
> Best,
> Uwe Ligges
>
>
>
>
>>
>> Jim
>>
>>
>> On Thu, Nov 10, 2016 at 4:00 AM, Miluji Sb <milujisb at gmail.com> wrote:
>>>
>>> Thanks a lot for your quick reply. I made a mistake in the question, I
>>> meant to ask every 4 (or 12) rows not columns. Apologies. Thanks again!
>>>
>>> Sincerely,
>>>
>>> Milu
>>>
>>> On Wed, Nov 9, 2016 at 5:45 PM, Dalthorp, Daniel <ddalthorp at usgs.gov>
>>> wrote:
>>>
>>>> Hi Milu,
>>>> The following should work for an array x (provided dim(x)[2] is
>>>> divisible
>>>> by 4):
>>>>
>>>> colMeans(x[,0:(dim(x)[2]/4-1)*4+1])
>>>>
>>>> -Dan
>>>>
>>>> On Wed, Nov 9, 2016 at 8:29 AM, Miluji Sb <milujisb at gmail.com> wrote:
>>>>
>>>>> Dear all,
>>>>>
>>>>> I have a dataset with hundreds of columns, I am only providing only 12
>>>>> columns. Is it possible to take the mean of every four (or 12) columns
>>>>>  (value601, value602, value603, value604 etc.in this case) and repeat
>>>>> for
>>>>> the hundreds of columns? Thank you.
>>>>>
>>>>> Sincerely,
>>>>>
>>>>> Milu
>>>>>
>>>>> structure(list(value601 = c(10.1738710403442, 3.54112911224365,
>>>>> 12.9192342758179, 3.17447590827942, 11.7332258224487, 7.68282270431519,
>>>>> -7.11564493179321, 0.987620949745178, 13.0476207733154,
>>>>> 6.36939525604248
>>>>> ), value602 = c(13.0642414093018, 5.53129482269287, 16.0519638061523,
>>>>> 2.88946437835693, 14.9204912185669, 9.42428588867188,
>>>>> -6.80674123764038,
>>>>> -0.614241063594818, 16.7947769165039, 7.9541072845459), value603 =
>>>>> c(22.0399188995361,
>>>>> 14.398024559021, 24.9523792266846, 12.0878629684448, 23.6459674835205,
>>>>> 18.3277816772461, -2.54092741012573, 10.5550804138184,
>>>>> 25.1016540527344,
>>>>> 16.2166938781738), value604 = c(27.7165412902832, 20.3255825042725,
>>>>> 30.8430004119873, 16.6856250762939, 29.2485408782959, 24.3775005340576,
>>>>> 6.47758340835571, 15.5897912979126, 30.7387924194336, 22.3637084960938
>>>>> ), value605 = c(31.6644763946533, 23.4093952178955, 35.1488723754883,
>>>>> 19.7132263183594, 33.3924179077148, 29.5846366882324, 10.2083873748779,
>>>>> 19.3551616668701, 35.3076629638672, 27.4299201965332), value606 =
>>>>> c(33.9698333740234,
>>>>> 26.8574161529541, 36.8900833129883, 22.8604583740234, 34.8642921447754,
>>>>> 33.8158760070801, 14.7055835723877, 22.1144580841064, 37.0545425415039,
>>>>> 32.1087913513184), value607 = c(36.0279846191406, 26.9297180175781,
>>>>> 38.2701225280762, 23.2643146514893, 36.7398796081543, 34.1216125488281,
>>>>> 17.1387901306152, 24.0419750213623, 37.8542327880859, 32.7677421569824
>>>>> ), value608 = c(34.0242347717285, 25.7720966339111, 36.4897193908691,
>>>>> 22.0332260131836, 34.8011703491211, 32.6856842041016, 16.6232261657715,
>>>>> 21.5571365356445, 36.1491546630859, 31.1716938018799), value609 =
>>>>> c(27.5402088165283,
>>>>> 21.7590408325195, 30.5214176177979, 18.4252090454102, 29.1156253814697,
>>>>> 26.9878330230713, 12.4962501525879, 17.7259578704834, 30.9099159240723,
>>>>> 25.4832077026367), value610 = c(23.4706859588623, 17.0126209259033,
>>>>> 26.8166942596436, 15.297459602356, 25.1733055114746, 23.5616931915283,
>>>>> 8.86995983123779, 13.5793552398682, 27.5732250213623, 22.1691932678223
>>>>> ), value611 = c(14.5820417404175, 9.08279132843018, 17.8419170379639,
>>>>> 8.36016654968262, 16.5633754730225, 14.8123331069946, 1.32095837593079,
>>>>> 5.73408317565918, 18.9752082824707, 13.5729999542236), value612 =
>>>>> c(9.12979793548584,
>>>>> 2.79943537712097, 11.6504030227661, 2.21584677696228, 10.5404834747314,
>>>>> 7.55471754074097, -5.58141136169434, -0.566209673881531,
>>>>> 12.3264112472534,
>>>>> 6.65576601028442)), .Names = c("value601", "value602", "value603",
>>>>> "value604", "value605", "value606", "value607", "value608", "value609",
>>>>> "value610", "value611", "value612"), row.names = c("1", "2",
>>>>> "3", "4", "5", "6", "7", "8", "9", "10"), class = "data.frame")
>>>>>
>>>>>         [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>>>> ng-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>>
>>>>
>>>>
>>>> --
>>>> Dan Dalthorp, PhD
>>>> USGS Forest and Rangeland Ecosystem Science Center
>>>> Forest Sciences Lab, Rm 189
>>>> 3200 SW Jefferson Way
>>>> Corvallis, OR 97331
>>>> ph: 541-750-0953
>>>> ddalthorp at usgs.gov
>>>>
>>>>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From peter.langfelder at gmail.com  Thu Nov 10 02:58:00 2016
From: peter.langfelder at gmail.com (Peter Langfelder)
Date: Wed, 9 Nov 2016 17:58:00 -0800
Subject: [R] creating lists of random matrices
In-Reply-To: <58236EDF.6020709@sapo.pt>
References: <d36e3fce-a4f9-7f89-da86-c5c3ec6df7e7@gmail.com>
	<08720B15-3889-4CCA-9EC5-A601A5094F30@me.com>
	<58236EDF.6020709@sapo.pt>
Message-ID: <CA+hbrhU2KCrcC9ER-DjVFMn0P5F1q1pDOz3CMrz4m6p0mR0Qhg@mail.gmail.com>

Add a

simplify = FALSE

to the call to replicate, and you'll get a list.

replicate(5, matrix(rnorm(4), 2, 2), simplify = FALSE)

Peter

On Wed, Nov 9, 2016 at 10:45 AM, Rui Barradas <ruipbarradas at sapo.pt> wrote:
> Hello,
>
> I also thought of replicate() but it creates an 2x2x5 array, not a list.
> Maybe it's all the same for the OP.
>
> Rui Barradas
>
> Em 09-11-2016 18:41, Marc Schwartz escreveu:
>>
>>
>>> On Nov 9, 2016, at 12:32 PM, Evan Cooch <evan.cooch at gmail.com> wrote:
>>>
>>> So, its easy enough to create a random matrix, using something like (say)
>>>
>>> matrix(rnorm(4),2,2)
>>>
>>> which generates a (2x2) matrix with random N(0,1) in each cell.
>>>
>>> But, what I need to be able to do is create a 'list' of such random
>>> matrices, where the length of the list (i.e., the number of said random
>>> matrices I store in the list) is some variable I can pass to the function
>>> (or loop).
>>>
>>> I tried the obvious like
>>>
>>> hold <- list()
>>> for (i in 1:5) {
>>>     hold[[i]] <- matrix(rnorm(4),2,2)
>>>     }
>>>
>>>
>>> While this works, it seems inelegant, and I'm wondering if there is a
>>> better (more efficient) way to accomplish the same thing -- perhaps avoiding
>>> the loop.
>>>
>>> Thanks in advance...
>>
>>
>>
>> Hi,
>>
>> See ?replicate
>>
>> Example:
>>
>> ## Create a list of 5 2x2 matrices
>>
>>> replicate(5, matrix(rnorm(4), 2, 2))
>>
>> , , 1
>>
>>             [,1]      [,2]
>> [1,] -0.1695775 1.0306685
>> [2,]  0.1636667 0.1044762
>>
>> , , 2
>>
>>             [,1]      [,2]
>> [1,] -0.3098566 2.1758363
>> [2,] -0.8029768 0.9697776
>>
>> , , 3
>>
>>             [,1]      [,2]
>> [1,]  0.5702972 0.7165806
>> [2,] -0.9731331 0.8332827
>>
>> , , 4
>>
>>             [,1]       [,2]
>> [1,] -0.8089588 0.09195256
>> [2,] -0.2026994 0.67545827
>>
>> , , 5
>>
>>            [,1]       [,2]
>> [1,] 0.5093008 -0.3097362
>> [2,] 0.6467358  0.3536414
>>
>>
>> Regards,
>>
>> Marc Schwartz
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From james.hirschorn at hotmail.com  Thu Nov 10 05:35:22 2016
From: james.hirschorn at hotmail.com (James Hirschorn)
Date: Thu, 10 Nov 2016 04:35:22 +0000
Subject: [R] Is this foreach behaviour correct?
In-Reply-To: <0a16dda9-d58a-d6ff-31c6-1e6b20a637f3@gmail.com>
References: <CY1PR11MB0444BE97C0B5173ADB45A892E9A40@CY1PR11MB0444.namprd11.prod.outlook.com>
	<CA+8X3fXJxE=WEYQhs+Jw=nXvoS1hEnCDHivUG=Djz9fuyQN7Cw@mail.gmail.com>
	<0a16dda9-d58a-d6ff-31c6-1e6b20a637f3@gmail.com>
Message-ID: <CY1PR11MB0444DDACD129A28953195A7AE9B80@CY1PR11MB0444.namprd11.prod.outlook.com>

Yes, I should have put

 > library(foreach)

 > library(zoo)

at the top.

On 11/06/2016 05:20 PM, Duncan Murdoch wrote:
> On 06/11/2016 5:02 PM, Jim Lemon wrote:
>> hi James,
>> I think you have to have a starting date ("origin") for as.Date to
>> convert numbers to dates.
>
> That's true with the function in the base package, but the zoo package 
> also has an as.Date() function, which defaults the origin to 
> "1970-01-01".  If James is using zoo his code would be okay. If he's 
> not, he would have got an error, so I think he must have been.
>
> Duncan Murdoch
>
>>
>> Jim
>>
>> On Sun, Nov 6, 2016 at 12:10 PM, James Hirschorn
>> <james.hirschorn at hotmail.com> wrote:
>>> This seemed odd so I wanted to check:
>>>
>>>  > x <- foreach(i=10000:10100, .combine='c') %do% { as.Date(i) }
>>>
>>> yields a numeric vector for x:
>>>
>>>  > class(x)
>>> [1] "numeric"
>>>
>>> Should it not be a vector of Date?
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide 
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> .
>


From danilo.carita at uniparthenope.it  Thu Nov 10 10:55:52 2016
From: danilo.carita at uniparthenope.it (danilo.carita at uniparthenope.it)
Date: Thu, 10 Nov 2016 10:55:52 +0100
Subject: [R] Three-component Negative Binomial Mixture: R code
In-Reply-To: <alpine.DEB.2.20.1611081658050.20628@paninaro>
References: <20161107134248.Horde.9trur60Ob_4h26wDm7GtSw1@webmail.uniparthenope.it>
	<alpine.DEB.2.20.1611072045061.13437@paninaro>
	<20161108161850.Horde.KClSE0Kaw2uB2jrwDIHehQ1@webmail.uniparthenope.it>
	<alpine.DEB.2.20.1611081658050.20628@paninaro>
Message-ID: <20161110105552.Horde.jja0PD34oP9u3unxyBhjJA1@webmail.uniparthenope.it>

Thank you for your hints, now the goodness of fit test provides me  
good results, but surprisingly for me the three-component model turns  
out to be worse than the two-component one (indeed, I focused on the  
three-component mixture because the two-component one exhibits a low  
p-value).

In addition, I have noticed that for some data the function fails to  
find good starting values, as you have mentioned in your previuous  
answer. The problem is that the driver FLXMRnegbin() allows to specify  
only the theta parameter (and only one value, even in the event of  
mixtures of two or more components).

I have read the description of flexmix() function too, but it seems  
that it does not allow to set starting values for the parameters of  
the model. Am I right? Or is there a way to do it?




Achim Zeileis <Achim.Zeileis at uibk.ac.at> ha scritto:

> On Tue, 8 Nov 2016, danilo.carita at uniparthenope.it wrote:
>
>> I tried the function flexmix() with the driver FLXMRnegbin() with  
>> two components first, in order to compare its results with those  
>> provided by my function mixnbinom(). In particular, I ran the  
>> following code:
>>
>>
>>> fm0 <- flexmix(y ~ 1, data = data.frame(y), k = 2, model = FLXMRnegbin())
>>
>>
>> where "y" is my vector of counts. The previous function provided me  
>> the following parameters:
>>
>>
>>>                  Comp.1   Comp.2
>>> coef.(Intercept) 1.2746536 1.788578
>>> theta            0.1418201 5.028766
>>
>>
>> with priors 0.342874 and 0.657126, respectively. I assume that the  
>> coefficients "Intercept" represent the two means of the model (mu1  
>> and mu2),
>
> No, a log link is employed, i.e., exp(1.2746536) and exp(1.788578)  
> are the means.
>
>> while the "theta" coefficients are the size parameters (size1 and size2).
>
> Yes.
>
>> Unfortunately, unlike my function mixnbinom(), the model computed  
>> with flexmix() did not provide a good fit to my data (p-value ~0).
>>
>> Is there something wrong in the process above?
>
> Hard to say without a reproducible example. Using parameter values  
> similar to the ones you cite above, the following seems to do a  
> reasonable job:
>
> ## packages
> library("countreg")
> library("flexmix")
>
> ## artificial data from two NB distributions:
> ## 1/3 is NB(mu = 3.5, theta = 0.2) and
> ## 2/3 is NB(mu = 6.0, theta = 5.0)
> set.seed(1)
> y <- c(rnbinom(200, mu = 3.5, size = 0.2), rnbinom(400, mu = 6, size = 5))
>
> ## fit 2-component mixture model
> set.seed(1)
> fm <- flexmix(y ~ 1, k = 2, model = FLXMRnegbin())
>
> ## inspect estimated parameters -> look acceptable
> parameters(fm)
> exp(parameters(fm)[1,])
>
> My experience was that finding good starting values may be a problem  
> for flexmix(). So maybe setting these in some better way would be  
> beneficial.



-------------------------------------------------------------
Danilo Carit?

PhD Candidate
University of Naples "Parthenope"
Dipartimento di Studi Aziendali e Quantitativi
via G. Parisi, 13, 80132 Napoli - Italy


From Thomas.Chesney at nottingham.ac.uk  Thu Nov 10 14:06:07 2016
From: Thomas.Chesney at nottingham.ac.uk (Thomas Chesney)
Date: Thu, 10 Nov 2016 13:06:07 +0000
Subject: [R] Vectorization in a random order
Message-ID: <53E4D6D1-940B-4C50-B07D-A4AC8759CF57@exmail.nottingham.ac.uk>

Is there a way to use vectorization where the elements are evaluated in a random order?

For instance, if the code is to be run on each row in a matrix of length nBuy the following will do the job

for (b in sample(1:nBuy,nBuy, replace=FALSE)){

}

but

apply(nBuyMat, 1, function(x))

will be run I believe, in the same order each time (Row1, then Row2, then Row3 etc.)

This is important for building agent based models (the classic explanation of this is probably Huberman & Glance's response to Nowak & May's 1992 Nature article - Evolutionary games and computer simulations, http://www.pnas.org/content/90/16/7716.abstract)

Thank you,

Thomas
http://www.nottingham.ac.uk/~liztc/Personal/index.html



This message and any attachment are intended solely for the addressee
and may contain confidential information. If you have received this
message in error, please send it back to me, and immediately delete it. 

Please do not use, copy or disclose the information contained in this
message or in any attachment.  Any views or opinions expressed by the
author of this email do not necessarily reflect the views of the
University of Nottingham.

This message has been checked for viruses but the contents of an
attachment may still contain software viruses which could damage your
computer system, you are advised to perform your own checks. Email
communications with the University of Nottingham may be monitored as
permitted by UK legislation.


From jdnewmil at dcn.davis.ca.us  Thu Nov 10 16:43:05 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 10 Nov 2016 07:43:05 -0800
Subject: [R] Vectorization in a random order
In-Reply-To: <53E4D6D1-940B-4C50-B07D-A4AC8759CF57@exmail.nottingham.ac.uk>
References: <53E4D6D1-940B-4C50-B07D-A4AC8759CF57@exmail.nottingham.ac.uk>
Message-ID: <1E03A5BB-BB96-4235-BB4D-B543ABA58C83@dcn.davis.ca.us>

I think you answered your own question. For loops are not a boogeyman... poor memory management is.

Algorithms that are sensitive to evaluation sequence are often not very re-usable, and certainly not parallelizable. If you have a specific algorithm in mind, there may be some advice we can give you about optimization, but as it stands think you know how to get a working implementation. 
-- 
Sent from my phone. Please excuse my brevity.

On November 10, 2016 5:06:07 AM PST, Thomas Chesney <Thomas.Chesney at nottingham.ac.uk> wrote:
>Is there a way to use vectorization where the elements are evaluated in
>a random order?
>
>For instance, if the code is to be run on each row in a matrix of
>length nBuy the following will do the job
>
>for (b in sample(1:nBuy,nBuy, replace=FALSE)){
>
>}
>
>but
>
>apply(nBuyMat, 1, function(x))
>
>will be run I believe, in the same order each time (Row1, then Row2,
>then Row3 etc.)
>
>This is important for building agent based models (the classic
>explanation of this is probably Huberman & Glance's response to Nowak &
>May's 1992 Nature article - Evolutionary games and computer
>simulations, http://www.pnas.org/content/90/16/7716.abstract)
>
>Thank you,
>
>Thomas
>http://www.nottingham.ac.uk/~liztc/Personal/index.html
>
>
>
>This message and any attachment are intended solely for the addressee
>and may contain confidential information. If you have received this
>message in error, please send it back to me, and immediately delete it.
>
>
>Please do not use, copy or disclose the information contained in this
>message or in any attachment.  Any views or opinions expressed by the
>author of this email do not necessarily reflect the views of the
>University of Nottingham.
>
>This message has been checked for viruses but the contents of an
>attachment may still contain software viruses which could damage your
>computer system, you are advised to perform your own checks. Email
>communications with the University of Nottingham may be monitored as
>permitted by UK legislation.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From rmh at temple.edu  Thu Nov 10 17:01:45 2016
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu, 10 Nov 2016 11:01:45 -0500
Subject: [R] Vectorization in a random order
In-Reply-To: <53E4D6D1-940B-4C50-B07D-A4AC8759CF57@exmail.nottingham.ac.uk>
References: <53E4D6D1-940B-4C50-B07D-A4AC8759CF57@exmail.nottingham.ac.uk>
Message-ID: <CAGx1TMCgrUa8cXLnn1WKVCu4pmoTEpaEj8Uq+JmGSkTT0MF53Q@mail.gmail.com>

nBuyMat <- data.frame(matrix(rnorm(28), 7, 4))
nBuyMat
nBuy <- nrow(nBuyMat)
sample(1:nBuy, nBuy, replace=FALSE)
sample(1:nBuy)
sample(nBuy)
?sample
apply(nBuyMat[sample(1:nBuy,nBuy, replace=FALSE),], 1, function(x) sum(x))

apply(nBuyMat[sample(nBuy),], 1, function(x) sum(x))


The defaults for sample do what you have requested.

If the original row identification matters, then be sure the to use
either a matrix with rownames or a data.frame.

Rich


Sent from my iPhone

> On Nov 10, 2016, at 08:06, Thomas Chesney <Thomas.Chesney at nottingham.ac.uk> wrote:
>
> for (b in sample(1:nBuy,nBuy, replace=FALSE)){
>
> }
>
> but
>
> apply(nBuyMat, 1, function(x))


From bgunter.4567 at gmail.com  Thu Nov 10 17:27:32 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 10 Nov 2016 08:27:32 -0800
Subject: [R] Vectorization in a random order
In-Reply-To: <53E4D6D1-940B-4C50-B07D-A4AC8759CF57@exmail.nottingham.ac.uk>
References: <53E4D6D1-940B-4C50-B07D-A4AC8759CF57@exmail.nottingham.ac.uk>
Message-ID: <CAGxFJbRLBcGizGL_2jD_YauK7DkSo+qwk_nEP0bZHjXG+1b=vA@mail.gmail.com>

You are mistaken. apply() is *not* vectorized. It is a disguised loop.

For true vectorization at the C level, the answer must be no, as the
whole point is to treat the argument as a whole object and hide the
iterative details.

However, as you indicated, you can always manually randomize the
indexing that is being iterated over and even write a function to do
it if you like; e.g. (warning: esentially untested and probably clumsy
as well as buggy)

randapply <- function(X, MARGIN, FUN,...)
{
   d <- dim(X)
   ix <- as.list(rep(TRUE,length(d)))
   for(i in MARGIN) ix[[i]] <- sample(seq_len(d[i]),d[i])
   X <- do.call("[", c(list(X), ix))
   apply(X,MARGIN,FUN,...)
}

> a <- array(1:24,dim = 2:4)

> randapply(a, 3,mean)
[1]  9.5 21.5 15.5  3.5

> randapply(a,3,mean)
[1] 21.5  3.5 15.5  9.5


Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Nov 10, 2016 at 5:06 AM, Thomas Chesney
<Thomas.Chesney at nottingham.ac.uk> wrote:
> Is there a way to use vectorization where the elements are evaluated in a random order?
>
> For instance, if the code is to be run on each row in a matrix of length nBuy the following will do the job
>
> for (b in sample(1:nBuy,nBuy, replace=FALSE)){
>
> }
>
> but
>
> apply(nBuyMat, 1, function(x))
>
> will be run I believe, in the same order each time (Row1, then Row2, then Row3 etc.)
>
> This is important for building agent based models (the classic explanation of this is probably Huberman & Glance's response to Nowak & May's 1992 Nature article - Evolutionary games and computer simulations, http://www.pnas.org/content/90/16/7716.abstract)
>
> Thank you,
>
> Thomas
> http://www.nottingham.ac.uk/~liztc/Personal/index.html
>
>
>
> This message and any attachment are intended solely for the addressee
> and may contain confidential information. If you have received this
> message in error, please send it back to me, and immediately delete it.
>
> Please do not use, copy or disclose the information contained in this
> message or in any attachment.  Any views or opinions expressed by the
> author of this email do not necessarily reflect the views of the
> University of Nottingham.
>
> This message has been checked for viruses but the contents of an
> attachment may still contain software viruses which could damage your
> computer system, you are advised to perform your own checks. Email
> communications with the University of Nottingham may be monitored as
> permitted by UK legislation.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From erlis.ruli at gmail.com  Thu Nov 10 16:30:38 2016
From: erlis.ruli at gmail.com (erlis ruli)
Date: Thu, 10 Nov 2016 16:30:38 +0100
Subject: [R] Helmert contrasts and weighted means
Message-ID: <2FB4CC43-6982-464D-A9C0-F83769741041@gmail.com>

Hi all!

Suppose that we have got a response y and an unbalanced treatment x with three levels or groups. 
The treatment is unbalanced by design. Indeed, the first group has 3 replications and the other two have two replications each.
 
For instance, in R the data might look like this:

y = c(66.18, 66.69, 50.31, 51.99, 52.07, 52.87, 54.03)
group = as.factor(c(rep("a", 3), rep("b", 2), rep("c",2)))

The group means are:
    a       b       c 
61.06   52.03   53.45 

and the overall mean is 56.31.

Using Helmert contrasts in lm I get the following

> my.mod = lm(y~group, contrasts = list(group = "contr.helmert"))

> summary(my.mod)

Call:
lm(formula = y ~ group, contrasts = list(group = "contr.helmert"))

Residuals:
     1      2      3      4      5      6      7 
  5.12   5.63 -10.75  -0.04   0.04  -0.58   0.58 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   55.513      2.540  21.858 2.59e-05 ***
group1        -4.515      3.012  -1.499    0.208    
group2        -1.032      1.851  -0.557    0.607    
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 6.598 on 4 degrees of freedom
Multiple R-squared:  0.4093,	Adjusted R-squared:  0.114 
F-statistic: 1.386 on 2 and 4 DF,  p-value: 0.3489

Here comes the questions. Is it possible to modify Helmert contrasts in order to use weighted means instead of means of means? 

Thanks

Erlis


From ganendexin36 at gmail.com  Thu Nov 10 06:09:22 2016
From: ganendexin36 at gmail.com (Steven Brown)
Date: Wed, 9 Nov 2016 23:09:22 -0600
Subject: [R] Estimate Weighted Euclidean Formula
Message-ID: <CAGSwJqYjgynb40_H40r1FOa5B6A7H9ZJGXZRbU63NK7B4eGPug@mail.gmail.com>

Given these two kinds of data set, and data set2 was obtained through the
weighted Euclidean formula. Can we estimate the weight parameter for each
variable in R based on the steepest descent method ? Thanks very much.

data set 1:

9 164 78 0 0 32.8 0.148 45
4 134 72 0 0 23.8 0.277 60
5 166 72 19 175 25.8 0.587 51
1 79 60 42 48 43.5 0.678 23
3 78 50 32 88 31 0.248 26
7 147 76 0 0 39.4 0.257 43
1 71 62 0 0 21.8 0.416 26
4 146 85 27 100 28.9 0.189 27
2 91 62 0 0 27.3 0.525 22
12 92 62 7 258 27.6 0.926 44
9 119 80 35 0 29 0.263 29
1 119 88 41 170 45.3 0.507 26
6 87 80 0 0 23.2 0.084 32
9 164 84 21 0 30.8 0.831 32
...
...

data set 2:

Pair No = 1 First Record = 153 Second Record = 362 Distance =
0.195109293281901
 Pair No = 2 First Record = 244 Second Record = 444 Distance =
0.276686595232122
 Pair No = 3 First Record = 102 Second Record = 349 Distance =
0.117089030740308
 Pair No = 4 First Record = 186 Second Record = 278 Distance =
0.110082950942552
 Pair No = 5 First Record = 85 Second Record = 362 Distance =
0.163096250231904
 Pair No = 6 First Record = 229 Second Record = 286 Distance =
0.133259083865808
 Pair No = 7 First Record = 157 Second Record = 372 Distance =
0.0801890745331076
 Pair No = 8 First Record = 400 Second Record = 440 Distance =
0.180080273679888
 Pair No = 9 First Record = 104 Second Record = 65 Distance =
0.270852896379353
 Pair No = 10 First Record = 339 Second Record = 504 Distance =
0.0903208989422145
 Pair No = 11 First Record = 243 Second Record = 472 Distance =
0.15667825467493
 Pair No = 12 First Record = 332 Second Record = 285 Distance =
0.198595164521116
 Pair No = 13 First Record = 235 Second Record = 254 Distance =
0.274602506212988
 Pair No = 14 First Record = 474 Second Record = 298 Distance =
0.220495386986385
 Pair No = 15 First Record = 192 Second Record = 186 Distance =
0.128319782237726
 Pair No = 16 First Record = 266 Second Record = 460 Distance =
0.138253740286321
 Pair No = 17 First Record = 357 Second Record = 211 Distance =
0.112028810325944
 Pair No = 18 First Record = 460 Second Record = 390 Distance =
0.125758538023743
 Pair No = 19 First Record = 332 Second Record = 31 Distance =
0.197584497870041
 Pair No = 20 First Record = 264 Second Record = 491 Distance =
0.143571812789276
...
...

	[[alternative HTML version deleted]]


From thayse.nery at hotmail.com  Thu Nov 10 09:20:49 2016
From: thayse.nery at hotmail.com (Thayse Nery)
Date: Thu, 10 Nov 2016 08:20:49 +0000
Subject: [R] Moran's I test for spatial autocorrelation - "spdep" package
Message-ID: <BN3PR05MB2449C8406AF65C8A2A7318AFF3B80@BN3PR05MB2449.namprd05.prod.outlook.com>

Dear all,


I would like to use the Moran's I test for residual spatial autocorrelation. My dataset is in the long format [70000 , 17]  and represents a time series of Land Use and Land Cover Changes. Since I have identical x and y coordinates, I am having trouble in performing the Moran's test using the "spdep" package. I am able to perform the Moran's test if I consider only one year at time, but I need to perform the analyse for the whole dataset. My dataframe is called trainData.


Below are the steps I have done:

xy <- as.matrix(trainData [, c(5:6)])

neighb.k1 <- knn2nb(knearneigh(xy , k=2, longlat=FALSE))
distance <- max(unlist(nbdists(neighb.k1, xy, longlat=FALSE)))
summary(distance)

# Error in  assign neighbors based on a specified distance, all values = 0
#Because of this error I am not able to continue the analysis
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
#    0       0       0       0       0       0

gc.nb <- dnearneigh(xy, 0, distance, longlat=FALSE)

### 2. Assign weights to the areas that are linked by by creating a spatial weights matrix
MyData_neighb_w <- nb2listw(gc.nb, zero.policy=T) #

## 3. Run statistical test to examine spatial autocorrelation (Moran's I on the DV)
DV_SpatialAut <- moran.test(trainData$currentState, listw=MyData_neighb_w)

### 4. Test the Spatial autocorrelation in residuals:
## 4.1. Run the Multinomial Logit Model

FitVglm is the Model

## Calculate the weighted matrix for the residuals from multinomial logit model
MyDataFinal2 <- MyDataFinal
MyDataFinal2$mlmresid <- residuals(FitVgamx)

lm.morantest(FitVgamx, resfun=MyData_neighb_w)

Is it possible to test for residual spatial autocorrelation for a time series data with identical x and y coordinates using the "spdep" package?

Thank you in advance for any help you can provide.
Regards
Thayse Nery
PhD. Student
The University of Western Australia


	[[alternative HTML version deleted]]


From ferri.leberl at gmx.at  Thu Nov 10 18:14:55 2016
From: ferri.leberl at gmx.at (Ferri Leberl)
Date: Thu, 10 Nov 2016 18:14:55 +0100
Subject: [R] writeLines: Write several lines
Message-ID: <trinity-24273efd-b420-44d0-8bad-c179f04e5c70-1478798095265@3capp-gmx-bs10>

Dear All,

If I have a vector V consisting of 9 strings ? how can I paste them into a single string without programming a loop?

Thank you in advance!


From ruipbarradas at sapo.pt  Thu Nov 10 18:20:10 2016
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Thu, 10 Nov 2016 17:20:10 +0000
Subject: [R] writeLines: Write several lines
In-Reply-To: <trinity-24273efd-b420-44d0-8bad-c179f04e5c70-1478798095265@3capp-gmx-bs10>
References: <trinity-24273efd-b420-44d0-8bad-c179f04e5c70-1478798095265@3capp-gmx-bs10>
Message-ID: <5824AC4A.3040903@sapo.pt>

Hello,

Just read the help page ?paste, in particular the argument collapse.

x <- LETTERS[1:9]
paste(x, collapse = "")

Hope this helps,

Rui Barradas

Em 10-11-2016 17:14, Ferri Leberl escreveu:
> Dear All,
>
> If I have a vector V consisting of 9 strings ? how can I paste them into a single string without programming a loop?
>
> Thank you in advance!
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From pdalgd at gmail.com  Thu Nov 10 19:04:36 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 10 Nov 2016 19:04:36 +0100
Subject: [R] Helmert contrasts and weighted means
In-Reply-To: <2FB4CC43-6982-464D-A9C0-F83769741041@gmail.com>
References: <2FB4CC43-6982-464D-A9C0-F83769741041@gmail.com>
Message-ID: <BD6089BD-C08E-4014-A748-04471423FAE7@gmail.com>


> On 10 Nov 2016, at 16:30 , erlis ruli <erlis.ruli at gmail.com> wrote:
> 
> Here comes the questions. Is it possible to modify Helmert contrasts in order to use weighted means instead of means of means? 

I think not. Not in general. I suppose you can set up a contrast matrix that would make the intercept equal to the overall mean, but the definition would depend on the group sizes. Contrast matrices are by design something that specifies a mapping FROM parameter space TO observation space and shouldn't depend on which observations you have or don't have. 

Put differently, you would have to modify the entire design matrix if you removed a few rows.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From Achim.Zeileis at uibk.ac.at  Thu Nov 10 21:22:23 2016
From: Achim.Zeileis at uibk.ac.at (Achim Zeileis)
Date: Thu, 10 Nov 2016 21:22:23 +0100 (CET)
Subject: [R] Three-component Negative Binomial Mixture: R code
In-Reply-To: <20161110105552.Horde.jja0PD34oP9u3unxyBhjJA1@webmail.uniparthenope.it>
References: <20161107134248.Horde.9trur60Ob_4h26wDm7GtSw1@webmail.uniparthenope.it>
	<alpine.DEB.2.20.1611072045061.13437@paninaro>
	<20161108161850.Horde.KClSE0Kaw2uB2jrwDIHehQ1@webmail.uniparthenope.it>
	<alpine.DEB.2.20.1611081658050.20628@paninaro>
	<20161110105552.Horde.jja0PD34oP9u3unxyBhjJA1@webmail.uniparthenope.it>
Message-ID: <alpine.DEB.2.20.1611102117100.32565@paninaro>

On Thu, 10 Nov 2016, danilo.carita at uniparthenope.it wrote:

> Thank you for your hints, now the goodness of fit test provides me good 
> results, but surprisingly for me the three-component model turns out to be 
> worse than the two-component one (indeed, I focused on the three-component 
> mixture because the two-component one exhibits a low p-value).
>
> In addition, I have noticed that for some data the function fails to find 
> good starting values, as you have mentioned in your previuous answer. The 
> problem is that the driver FLXMRnegbin() allows to specify only the theta 
> parameter (and only one value, even in the event of mixtures of two or more 
> components).
>
> I have read the description of flexmix() function too, but it seems that it 
> does not allow to set starting values for the parameters of the model. Am I 
> right? Or is there a way to do it?

No, I don't think so. You could look at the FLXMRnegbin() driver code and 
tweak this directly to use better starting values etc. But I think that 
the main issue is that the "theta" parameter often diverges to infinity 
(i.e., a Poisson distribution) if theta is too large in (at least) one of 
the components.

Given that the negbin distribution is a continuous mixture of Poisson 
distributions, I'm not sure whether approaching such data with a finite 
mixture of such continuous mixtures.

What to do with this situation, certainly depends on the data you have and 
the questions you have about it. And I concur with Bert's advice of 
contacting a local statistics expert for discussion of such issues.

hth,
Z

>
>
>
> Achim Zeileis <Achim.Zeileis at uibk.ac.at> ha scritto:
>
>> On Tue, 8 Nov 2016, danilo.carita at uniparthenope.it wrote:
>> 
>>> I tried the function flexmix() with the driver FLXMRnegbin() with two 
>>> components first, in order to compare its results with those provided by 
>>> my function mixnbinom(). In particular, I ran the following code:
>>> 
>>> 
>>>> fm0 <- flexmix(y ~ 1, data = data.frame(y), k = 2, model = FLXMRnegbin())
>>> 
>>> 
>>> where "y" is my vector of counts. The previous function provided me the 
>>> following parameters:
>>> 
>>>
>>>>                 Comp.1   Comp.2
>>>> coef.(Intercept) 1.2746536 1.788578
>>>> theta            0.1418201 5.028766
>>> 
>>> 
>>> with priors 0.342874 and 0.657126, respectively. I assume that the 
>>> coefficients "Intercept" represent the two means of the model (mu1 and 
>>> mu2),
>> 
>> No, a log link is employed, i.e., exp(1.2746536) and exp(1.788578) are the 
>> means.
>> 
>>> while the "theta" coefficients are the size parameters (size1 and size2).
>> 
>> Yes.
>> 
>>> Unfortunately, unlike my function mixnbinom(), the model computed with 
>>> flexmix() did not provide a good fit to my data (p-value ~0).
>>> 
>>> Is there something wrong in the process above?
>> 
>> Hard to say without a reproducible example. Using parameter values similar 
>> to the ones you cite above, the following seems to do a reasonable job:
>> 
>> ## packages
>> library("countreg")
>> library("flexmix")
>> 
>> ## artificial data from two NB distributions:
>> ## 1/3 is NB(mu = 3.5, theta = 0.2) and
>> ## 2/3 is NB(mu = 6.0, theta = 5.0)
>> set.seed(1)
>> y <- c(rnbinom(200, mu = 3.5, size = 0.2), rnbinom(400, mu = 6, size = 5))
>> 
>> ## fit 2-component mixture model
>> set.seed(1)
>> fm <- flexmix(y ~ 1, k = 2, model = FLXMRnegbin())
>> 
>> ## inspect estimated parameters -> look acceptable
>> parameters(fm)
>> exp(parameters(fm)[1,])
>> 
>> My experience was that finding good starting values may be a problem for 
>> flexmix(). So maybe setting these in some better way would be beneficial.
>
>
>
> -------------------------------------------------------------
> Danilo Carit?
>
> PhD Candidate
> University of Naples "Parthenope"
> Dipartimento di Studi Aziendali e Quantitativi
> via G. Parisi, 13, 80132 Napoli - Italy
> -------------------------------------------------------------
>

From erlis.ruli at gmail.com  Thu Nov 10 22:29:50 2016
From: erlis.ruli at gmail.com (erlis ruli)
Date: Thu, 10 Nov 2016 22:29:50 +0100
Subject: [R] Helmert contrasts and weighted means
In-Reply-To: <BD6089BD-C08E-4014-A748-04471423FAE7@gmail.com>
References: <2FB4CC43-6982-464D-A9C0-F83769741041@gmail.com>
	<BD6089BD-C08E-4014-A748-04471423FAE7@gmail.com>
Message-ID: <FE42C4BE-E113-4A53-8887-2657C86222B5@gmail.com>

Thanks for the reply.

> I suppose you can set up a contrast matrix that would make the intercept equal to the overall mean, but the definition would depend on the group sizes.

Any suggestion on how to set up one?

Erlis

From ganendexin36 at gmail.com  Thu Nov 10 23:59:59 2016
From: ganendexin36 at gmail.com (Steven Brown)
Date: Thu, 10 Nov 2016 16:59:59 -0600
Subject: [R] Estimate Weighted Euclidean Formula
Message-ID: <CAGSwJqYOpe4y5VYwhLpOdjMn8r8+YmbgmT26yShq7EV=5xuRBw@mail.gmail.com>

Given these two kinds of data set, and data set2 was obtained through the
weighted Euclidean formula. Can we estimate the weight parameter for each
variable in R based on the steepest descent method ? Thanks very much.

data set 1:

9 164 78 0 0 32.8 0.148 45
4 134 72 0 0 23.8 0.277 60
5 166 72 19 175 25.8 0.587 51
1 79 60 42 48 43.5 0.678 23
3 78 50 32 88 31 0.248 26
7 147 76 0 0 39.4 0.257 43
1 71 62 0 0 21.8 0.416 26
4 146 85 27 100 28.9 0.189 27
2 91 62 0 0 27.3 0.525 22
12 92 62 7 258 27.6 0.926 44
9 119 80 35 0 29 0.263 29
1 119 88 41 170 45.3 0.507 26
6 87 80 0 0 23.2 0.084 32
9 164 84 21 0 30.8 0.831 32
...
...

data set 2:

Pair No = 1 First Record = 153 Second Record = 362 Distance =
0.195109293281901
 Pair No = 2 First Record = 244 Second Record = 444 Distance =
0.276686595232122
 Pair No = 3 First Record = 102 Second Record = 349 Distance =
0.117089030740308
 Pair No = 4 First Record = 186 Second Record = 278 Distance =
0.110082950942552
 Pair No = 5 First Record = 85 Second Record = 362 Distance =
0.163096250231904
 Pair No = 6 First Record = 229 Second Record = 286 Distance =
0.133259083865808
 Pair No = 7 First Record = 157 Second Record = 372 Distance =
0.0801890745331076
 Pair No = 8 First Record = 400 Second Record = 440 Distance =
0.180080273679888
 Pair No = 9 First Record = 104 Second Record = 65 Distance =
0.270852896379353
 Pair No = 10 First Record = 339 Second Record = 504 Distance =
0.0903208989422145
 Pair No = 11 First Record = 243 Second Record = 472 Distance =
0.15667825467493
 Pair No = 12 First Record = 332 Second Record = 285 Distance =
0.198595164521116
 Pair No = 13 First Record = 235 Second Record = 254 Distance =
0.274602506212988
 Pair No = 14 First Record = 474 Second Record = 298 Distance =
0.220495386986385
 Pair No = 15 First Record = 192 Second Record = 186 Distance =
0.128319782237726
 Pair No = 16 First Record = 266 Second Record = 460 Distance =
0.138253740286321
 Pair No = 17 First Record = 357 Second Record = 211 Distance =
0.112028810325944
 Pair No = 18 First Record = 460 Second Record = 390 Distance =
0.125758538023743
 Pair No = 19 First Record = 332 Second Record = 31 Distance =
0.197584497870041
 Pair No = 20 First Record = 264 Second Record = 491 Distance =
0.143571812789276
...
...

	[[alternative HTML version deleted]]


From emorway at usgs.gov  Fri Nov 11 05:26:38 2016
From: emorway at usgs.gov (Morway, Eric)
Date: Thu, 10 Nov 2016 20:26:38 -0800
Subject: [R] Gobbling up a repeating, irregular list of data
Message-ID: <CAPoqHzr1DrM4003agYkdWBMSGHiUvLe0_QPqqfh3s_3MUd7PsA@mail.gmail.com>

What would be the sophisticated R method for reading the data shown below
into a list?  The data is output from a numerical model.  Pasting the
second block of example R commands (at the end of the message) results in a
failure ("Error in scan...line 2 did not have 6 elements").  I no doubt
could cobble together some script for reading line-by-line using for loops,
and then appending vectors with values from each line, but this strikes me
as bad form.

One final note, the lines with 6 values contain important values that
should somehow remain associated with the data appearing in columns 5 & 6
(the continuous data).  The first value, which is always 1, can be
discarded, but the second value on these lines contain the time step number
("1.00E+00", "2.00E+00", etc.), the 3rd and 4th values are contain a depth
and thickness, respectively. Columns 5 & 6 are a depth and water content
pairing and should be associated with the time steps.

Thanks, Eric

Start of example output data (Use of an R script to read in this data below)

  1    1.00E+00  1.24E+03  7.79E+00  1.925E-01  1.88E-01
                                     3.850E-01  1.88E-01
                                     5.775E-01  1.88E-01
                                     7.700E-01  1.88E-01
                                     9.626E-01  1.88E-01
                                     1.155E+00  1.88E-01
                                     1.347E+00  1.88E-01
  1    2.00E+00  1.26E+03  7.80E+00  1.925E-01  2.80E-01
                                     1.732E+00  2.80E-01
                                     1.925E+00  2.80E-01
                                     2.310E+00  2.93E-01
                                     2.502E+00  2.22E-01
                                     2.695E+00  1.88E-01
                                     2.887E+00  1.88E-01
  1    3.00E+00  1.28E+03  7.70E+00  1.925E-01  1.03E-01
                                     3.850E-01  1.30E-01
                                     5.775E-01  1.48E-01
                                     7.701E-01  1.61E-01
                                     9.626E-01  1.72E-01
                                     1.155E+00  1.86E-01
                                     1.347E+00  1.93E-01
  1    4.00E+00  1.29E+03  7.60E+00  1.901E-01  1.80E-01
                                     3.803E-01  1.80E-01
                                     5.705E-01  1.38E-01
                                     7.607E-01  1.32E-01
                                     2.282E+00  1.86E-01
                                     2.472E+00  1.98E-01
                                     2.662E+00  2.00E-01

Same data as above, but scan function fails.

dat <- read.table(textConnection("  1    1.00E+00  1.24E+03  7.79E+00
 1.925E-01  1.88E-01
                                     3.850E-01  1.88E-01
                                     5.775E-01  1.88E-01
                                     7.700E-01  1.88E-01
                                     9.626E-01  1.88E-01
                                     1.155E+00  1.88E-01
                                     1.347E+00  1.88E-01
  1    2.00E+00  1.26E+03  7.80E+00  1.925E-01  2.80E-01
                                     1.732E+00  2.80E-01
                                     1.925E+00  2.80E-01
                                     2.310E+00  2.93E-01
                                     2.502E+00  2.22E-01
                                     2.695E+00  1.88E-01
                                     2.887E+00  1.88E-01
  1    3.00E+00  1.28E+03  7.70E+00  1.925E-01  1.03E-01
                                     3.850E-01  1.30E-01
                                     5.775E-01  1.48E-01
                                     7.701E-01  1.61E-01
                                     9.626E-01  1.72E-01
                                     1.155E+00  1.86E-01
                                     1.347E+00  1.93E-01
  1    4.00E+00  1.29E+03  7.60E+00  1.901E-01  1.80E-01
                                     3.803E-01  1.80E-01
                                     5.705E-01  1.38E-01
                                     7.607E-01  1.32E-01
                                     2.282E+00  1.86E-01
                                     2.472E+00  1.98E-01
                                     2.662E+00  2.00E-01"),header=FALSE)

	[[alternative HTML version deleted]]


From peter.langfelder at gmail.com  Fri Nov 11 05:53:17 2016
From: peter.langfelder at gmail.com (Peter Langfelder)
Date: Thu, 10 Nov 2016 20:53:17 -0800
Subject: [R] Gobbling up a repeating, irregular list of data
In-Reply-To: <CAPoqHzr1DrM4003agYkdWBMSGHiUvLe0_QPqqfh3s_3MUd7PsA@mail.gmail.com>
References: <CAPoqHzr1DrM4003agYkdWBMSGHiUvLe0_QPqqfh3s_3MUd7PsA@mail.gmail.com>
Message-ID: <CA+hbrhWOBgzK1nCzbqFx6Y1ghaejVD1tgj1zDuoEhpu5pJz6MQ@mail.gmail.com>

It's not clear whether your numbers are tab or space-separated, I will
assume space-separated. My lowtech (and not R) solution would be to
dump the output into a text file (call it data.in), then run a sed
command to first replace two initial spaces from each line, then
replace initial spaces with 4 (if I count correctly) tabs, then
replace all contiguous blocks of spaces by tabs, something like

sed 's/^  //' data.in | sed 's/^  */\t\t\t\t/' | sed 's/  */\t/g' > data.txt

This should produce a regular 6-column table that should be readable
using standard read.delim or read.table. You will then have figure out
how to deal with the empty cells in R.

Peter

On Thu, Nov 10, 2016 at 8:26 PM, Morway, Eric <emorway at usgs.gov> wrote:
> What would be the sophisticated R method for reading the data shown below
> into a list?  The data is output from a numerical model.  Pasting the
> second block of example R commands (at the end of the message) results in a
> failure ("Error in scan...line 2 did not have 6 elements").  I no doubt
> could cobble together some script for reading line-by-line using for loops,
> and then appending vectors with values from each line, but this strikes me
> as bad form.
>
> One final note, the lines with 6 values contain important values that
> should somehow remain associated with the data appearing in columns 5 & 6
> (the continuous data).  The first value, which is always 1, can be
> discarded, but the second value on these lines contain the time step number
> ("1.00E+00", "2.00E+00", etc.), the 3rd and 4th values are contain a depth
> and thickness, respectively. Columns 5 & 6 are a depth and water content
> pairing and should be associated with the time steps.
>
> Thanks, Eric
>
> Start of example output data (Use of an R script to read in this data below)
>
>   1    1.00E+00  1.24E+03  7.79E+00  1.925E-01  1.88E-01
>                                      3.850E-01  1.88E-01
>                                      5.775E-01  1.88E-01
>                                      7.700E-01  1.88E-01
>                                      9.626E-01  1.88E-01
>                                      1.155E+00  1.88E-01
>                                      1.347E+00  1.88E-01
>   1    2.00E+00  1.26E+03  7.80E+00  1.925E-01  2.80E-01
>                                      1.732E+00  2.80E-01
>                                      1.925E+00  2.80E-01
>                                      2.310E+00  2.93E-01
>                                      2.502E+00  2.22E-01
>                                      2.695E+00  1.88E-01
>                                      2.887E+00  1.88E-01
>   1    3.00E+00  1.28E+03  7.70E+00  1.925E-01  1.03E-01
>                                      3.850E-01  1.30E-01
>                                      5.775E-01  1.48E-01
>                                      7.701E-01  1.61E-01
>                                      9.626E-01  1.72E-01
>                                      1.155E+00  1.86E-01
>                                      1.347E+00  1.93E-01
>   1    4.00E+00  1.29E+03  7.60E+00  1.901E-01  1.80E-01
>                                      3.803E-01  1.80E-01
>                                      5.705E-01  1.38E-01
>                                      7.607E-01  1.32E-01
>                                      2.282E+00  1.86E-01
>                                      2.472E+00  1.98E-01
>                                      2.662E+00  2.00E-01
>
> Same data as above, but scan function fails.
>
> dat <- read.table(textConnection("  1    1.00E+00  1.24E+03  7.79E+00
>  1.925E-01  1.88E-01
>                                      3.850E-01  1.88E-01
>                                      5.775E-01  1.88E-01
>                                      7.700E-01  1.88E-01
>                                      9.626E-01  1.88E-01
>                                      1.155E+00  1.88E-01
>                                      1.347E+00  1.88E-01
>   1    2.00E+00  1.26E+03  7.80E+00  1.925E-01  2.80E-01
>                                      1.732E+00  2.80E-01
>                                      1.925E+00  2.80E-01
>                                      2.310E+00  2.93E-01
>                                      2.502E+00  2.22E-01
>                                      2.695E+00  1.88E-01
>                                      2.887E+00  1.88E-01
>   1    3.00E+00  1.28E+03  7.70E+00  1.925E-01  1.03E-01
>                                      3.850E-01  1.30E-01
>                                      5.775E-01  1.48E-01
>                                      7.701E-01  1.61E-01
>                                      9.626E-01  1.72E-01
>                                      1.155E+00  1.86E-01
>                                      1.347E+00  1.93E-01
>   1    4.00E+00  1.29E+03  7.60E+00  1.901E-01  1.80E-01
>                                      3.803E-01  1.80E-01
>                                      5.705E-01  1.38E-01
>                                      7.607E-01  1.32E-01
>                                      2.282E+00  1.86E-01
>                                      2.472E+00  1.98E-01
>                                      2.662E+00  2.00E-01"),header=FALSE)
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Paul.Louisell at pw.utc.com  Thu Nov 10 22:24:40 2016
From: Paul.Louisell at pw.utc.com (Louisell, Paul T           PW)
Date: Thu, 10 Nov 2016 21:24:40 +0000
Subject: [R] nlme & VarIdent
Message-ID: <96E6B0084A068C43B02C637889A5E0B73F7D06@UUSNWE1N.na.utcmail.com>

Hello,

All the help I've read (including Pinheiro and Bates book, 'Mixed Effects Models in S and S-PLUS') regarding how to fit a linear mixed-effects model where variances change with a factor's levels indicates this is done through the 'weights' argument to 'lme', using something like 'weights=varIdent(form=~v|g)' where 'v' is a variance covariate and 'g' is the grouping factor whose strata have different random effect variances.

My question: Suppose I have more than 1 variance covariate, say v1, ..., vk, and I want _each_ of these to have variances that change with the levels of g giving a total of k*nlevels(g) parameters (k*nlevels(g) - k allowing for identifiability). How is this handled in the nlme package? A simple example would be random slope and intercepts, _both_ of which have variances changing with the levels of g. I haven't found any examples of this online or in Pinheiro & Bates, and I haven't been able to figure this out using the various varFunc/pdMat classes.

Help/advice would be greatly appreciated.

Thanks,

Paul Louisell
Statistical Specialist
Paul.Louisell at pw.utc.com
860-565-8104

Still, tomorrow's going to be another working day, and I'm trying to get some rest.
That's all, I'm trying to get some rest. 
Paul Simon, "American Tune"


From ferri.leberl at gmx.at  Fri Nov 11 10:36:34 2016
From: ferri.leberl at gmx.at (Ferri Leberl)
Date: Fri, 11 Nov 2016 10:36:34 +0100
Subject: [R] Importing and exporting threedimensional arrays
Message-ID: <trinity-81515856-9a88-405c-a637-dd75f65347af-1478856994584@3capp-gmx-bs71>


Dear all,
?
I want to process a list of XML-Elements.
In one dimension the elements are listed; in the other their respective properties (name, comment, parent, children, attributes).
?
I am writing a script that processes such tables, and another one that produces sample tables to test the first script.
So the first script?should import?tabular-separated lists, and the second should export them with write.table (or anything better you suggest me).
?
As long as we stay two dimensional I see no difficulties.
?
However, there can be several children and several attributes. So my idea was to add a third dimension ? but some tests I did showed me that e.g.
?
write.table(array(1:60,dim=c(4,3,5)),"beispielmatrix",sep="\t",quote=FALSE,row.names=F,col.names=F,dec=",")
?
produces a 2D-tsv.
?
So, how can I handle the fact, that there may be several items in a children resp. atrribute field?
?
Thank you in advance!


From petr.pikal at precheza.cz  Fri Nov 11 11:47:11 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 11 Nov 2016 10:47:11 +0000
Subject: [R] Help-Text file
In-Reply-To: <608157737.517436.1478593970635@mail.yahoo.com>
References: <1207914484.662606.1478323728775.ref@mail.yahoo.com>
	<1207914484.662606.1478323728775@mail.yahoo.com>
	<84D0569E-481A-4615-8556-9A08E306824E@dcn.davis.ca.us>
	<1321924128.371959.1478587544305@mail.yahoo.com>
	<998305639.379401.1478587759559@mail.yahoo.com>
	<608157737.517436.1478593970635@mail.yahoo.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5045C90@SRVEXCHMBX.precheza.cz>

Hi

I did not use BoolNet package, but from documentation it seems that function getAttractor requires object of class BooleanNetwork. However your object New Text Document1 has class

 [1] "tbl_df"     "tbl"        "data.frame"

This is probably the reason for error. But for the getAttractors you use other file (net), and we do not have faintest idea about it, so it is hard to be precise.

Check e.g. how cellcycle data is organised and check your net object if it has similar structure.

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Priya
> Arasu
> Sent: Tuesday, November 8, 2016 9:33 AM
> To: r-help at r-project.org; jdnewmil at dcn.davis.ca.us
> Subject: Re: [R] Help-Text file
>
>  Hi,
>      I have taken Mr.Jeff's suggestions in producing a reproducible example
> and edited my question again. Kindly someone in the mailing list, help to
> solve the problem, which I have mentioned in the below mail Thank youPriya
>
>
>  Dear Mr. Jeff Newmiller,
>                                         Thank you. I have gone through the links that you have
> sent me to produce reproducible example. I will try to explain the problem
> clearly this time.
>
> I have been using Boolnet package, to simulate interaction between certain
> genes. I have a problem after importing the text file
> (New_Text_Document_1)  in R and start doing the attractor analysis in
> Boolnet. I have pasted the steps below, when I give the command
> "getAttractors" for doing attractor analysis, I get the error (highlighted red
> and italicized).
>
> > library("BoolNet", lib.loc="~/R/win-library/3.3")
> > library(readr)#Imported txt file
> > New_Text_Document_1 <- read_csv("C:/Users/Priya/Desktop/New Text
> > Document-1.txt")
> Parsed with column specification:
> cols(
>   Targets = col_character(),
>   factors = col_character()
> )
> > View(New_Text_Document_1)
> > net <- loadNetwork("C:/Users/Priya/Desktop/New Text Document-1.txt")
> > #loaded the txt file as network (net) for analysis attr <-
> > getAttractors(net, type="asynchronous") # attractor analysis for the
> > txt file
> Error in matrix(nrow = n, ncol = length(network$genes)) :
>   invalid 'nrow' value (too large or NA) In addition: Warning message:
> In matrix(nrow = n, ncol = length(network$genes)) :
>   NAs introduced by coercion to integer range
>
>
> Using dput, I have pasted the data (New_Text_Document_1), which I have
> imported for doing attractor analysis.I have also attached the
> New_Text_Document_1 along with this mail. I didn't get any reply from the
> publishers of Boolnet package, so Kindly help. Have anyone come across this
> error and how do I fix this problem?
>
>
> dput(New_Text_Document_1)
> New_Text_Document_1 <- structure(list(Targets = c("DISC1", "MITF",
> "GATA1", "TAL1", "EZH2", "AR", "TCF3", "miR124", "miR377", "let7b", "let7a",
> "let7d", "PCM1", "miR33a", "EGR1", "SALL4", "POU3F2", "THAP11",
> "HCFC1","ZFP281", "WT1", "SOX2", "CUX1", "SIN3B", "KDM5B", "HOXB4",
> "ELK1", "FLI1", "HNF4A", "RUNX1", "BBS4", "miR432", "miR382", "miR127",
> "E2F1", "TFAP2A", "EOMES", "GABP"),
> factors = c("(!miR124 & !let7b & !let7a & !let7d) | (PCM1 & BBS4) | (MITF |
> GATA1 | TAL1 | EZH2 | AR | TCF3 | HNF4A | RUNX1)",
> "(MITF)", "(GATA1)", "(!miR377) | (TAL1)", "(!miR124) | (EZH2)",
> "(!miR124) | (AR)", "(!miR124) | (TCF3)", "(EGR1)", "(miR377)",
> "(let7b)", "(let7a)", "(let7d)", "(! miR33a) | (DISC1 & BBS4) | (EGR1 | SALL4 |
> POU3F2 | THAP11 | HCFC1 | ZFP281 | WT1 | SOX2 | CUX1 | SIN3B | KDM5B |
> HOXB4 | ELK1 | TCF3 | FLI1 | HNF4A | RUNX1 | MITF)", "(miR33a)", "(!
> miR124) | (EGR1)", "(SALL4)", "(POU3F2)", "(THAP11)","(HCFC1)", "(ZFP281)",
> "(WT1)", "(SOX2)", "(CUX1)", "(SIN3B)",
> "(KDM5B)", "(HOXB4)", "(ELK1)", "(FLI1)", "(HNF4A)", "(RUNX1)",
> "(! miR432 & ! miR382 & ! miR127 & ! miR377) | (DISC1 & PCM1) | (E2F1 |
> GATA1 | FLI1 | MITF | ELK1 | CUX1 | KDM5B | SIN3B) | GABP | EOMES |
> TFAP2A)", "(miR432)", "(miR382)", "(miR127)", "(! let7a) | (E2F1)",
> "(TFAP2A)", "(EOMES)", "(GABP)")),
> class = c("tbl_df", "tbl", "data.frame"),
> row.names = c(NA, -38L), .
> Names = c("Targets", "factors"),
> spec = structure(list(cols = structure(list(Targets = structure(list(), class =
> c("collector_character","collector")),
> factors = structure(list(), class = c("collector_character", "collector"))), .
> Names = c("Targets", "factors")),
> default = structure(list(),
> class = c("collector_guess", "collector"))), .
> Names = c("cols", "default"), class = "col_spec"))
>
> Regards
> Priya
>
>
>
>     On Saturday, 5 November 2016 10:28 PM, Jeff Newmiller
> <jdnewmil at dcn.davis.ca.us> wrote:
>
>
>  Sorry, but this request is quite hard to understand... you are basically
> directing your question at an incredibly narrow set of people who happen to
> have combined your kind of data read in the way you read it in with your kind
> of analysis algorithms while telling us almost nothing about how you did
> those things.
>
> Please read the Posting Guide mentioned in the footer of every posting on
> this list and provide a reproducible example [1][2]. If the problem is in your
> use of R then someone here will be able to help. If your problem is inside a
> contributed package then you may need the assistance of the package
> maintainer, but they will also need a reproducible example so your effort will
> not be in vain. Also, the act of creating a RE often leads you to your own
> solution even without getting outside help.
>
> [1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-
> reproducible-example
>
> [2] http://adv-r.had.co.nz/Reproducibility.html
>
> --
> Sent from my phone. Please excuse my brevity.
>
> On November 4, 2016 10:28:48 PM PDT, Priya Arasu via R-help <r-help at r-
> project.org> wrote:
> >Hi,
> >     I am using R 3.3.1 in R studio version 1.0.44 (Windows10, 64 bit
> >system). I could import text file with Boolean rules, but when I start
> >doing analysis in R, I get the error: I have been trying to do
> >attractor analysis using R package Boolnet, using the command: attr <-
> >getAttractors(net, type="asynchronous"). This command throws up the
> >below error:
> >
> >Error in matrix(nrow = n, ncol = length(network$genes)) :
> >  invalid 'nrow' value (too large or NA)
> >In addition: Warning message:
> >In matrix(nrow = n, ncol = length(network$genes)) :
> >  NAs introduced by coercion to integer range
> >
> >
> > Have anyone come across this error?. I have attached the text file
> >with Boolean rules. Pls find it. Any suggestions or ideas, would be of
> >great help
> >Thank you
> >Priya
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >------------------------------------------------------------------------
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
>
>
>
>
>
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From ruipbarradas at sapo.pt  Fri Nov 11 12:02:09 2016
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Fri, 11 Nov 2016 11:02:09 +0000
Subject: [R] Importing and exporting threedimensional arrays
In-Reply-To: <trinity-81515856-9a88-405c-a637-dd75f65347af-1478856994584@3capp-gmx-bs71>
References: <trinity-81515856-9a88-405c-a637-dd75f65347af-1478856994584@3capp-gmx-bs71>
Message-ID: <5825A531.7010301@sapo.pt>

Hello,

You can save 3D objects (or objects of any form or shape) by using ?save.
You would then retrieve them with ?load.

Hope this helps,

Rui Barradas

Em 11-11-2016 09:36, Ferri Leberl escreveu:
>
> Dear all,
>
> I want to process a list of XML-Elements.
> In one dimension the elements are listed; in the other their respective properties (name, comment, parent, children, attributes).
>
> I am writing a script that processes such tables, and another one that produces sample tables to test the first script.
> So the first script should import tabular-separated lists, and the second should export them with write.table (or anything better you suggest me).
>
> As long as we stay two dimensional I see no difficulties.
>
> However, there can be several children and several attributes. So my idea was to add a third dimension ? but some tests I did showed me that e.g.
>
> write.table(array(1:60,dim=c(4,3,5)),"beispielmatrix",sep="\t",quote=FALSE,row.names=F,col.names=F,dec=",")
>
> produces a 2D-tsv.
>
> So, how can I handle the fact, that there may be several items in a children resp. atrribute field?
>
> Thank you in advance!
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ferri.leberl at gmx.at  Fri Nov 11 13:36:44 2016
From: ferri.leberl at gmx.at (Ferri Leberl)
Date: Fri, 11 Nov 2016 13:36:44 +0100
Subject: [R] Importing and exporting threedimensional arrays
In-Reply-To: <5825A531.7010301@sapo.pt>
References: <trinity-81515856-9a88-405c-a637-dd75f65347af-1478856994584@3capp-gmx-bs71>,
	<5825A531.7010301@sapo.pt>
Message-ID: <trinity-48969ad2-5b5d-4a12-bd00-30f32888503a-1478867804922@3capp-gmx-bs71>



Thanks for your answer.
?
I'm afraid it doesn't.
?
The planned workflow should be:
?
Somebody delivers me a table containing the essential relations of an XML-schema in form of a list that should, once it is functional, be generated out of R. So I need a form that is as universal as possible ? therefore I thought of a tabular separated list.
If I make an array:

feld<-array(1:60,dim=c(4,3,5))

save it

save(file="feld",list="feld")

and read it in the bash

cat feld

I receive

U?G?@
? ? ?@Q??z???,9? ?!??w??,??3c[?/g?2mSD4????C??r?>?#?'?]?????P5?a??(b?#?$RH#?,r??"J(??*j???&Zh??.z?c?!F?c?)f?c
? ? ? ? ? ? ? ? ? ? K???[?????????;?o|?????Y??N
?
So I guess it will be a science of its own to understand, how the file is structured.

So I need either a format that allows to easily parse the import 3D-Matrix outside R, or I need any good idea how to avoid the third dimension (e.g., might it function to use a second separation character within the collumns which may contain several items?

Thank you in advance!
Have a pleasant weekend.
Yours, Ferri





Gesendet:?Freitag, 11. November 2016 um 12:02 Uhr
Von:?"Rui Barradas" <ruipbarradas at sapo.pt>
An:?"Ferri Leberl" <ferri.leberl at gmx.at>, "r-helpr-project.org" <r-help at r-project.org>
Betreff:?Re: [R] Importing and exporting threedimensional arrays
Hello,

You can save 3D objects (or objects of any form or shape) by using ?save.
You would then retrieve them with ?load.

Hope this helps,

Rui Barradas

Em 11-11-2016 09:36, Ferri Leberl escreveu:
>
> Dear all,
>
> I want to process a list of XML-Elements.
> In one dimension the elements are listed; in the other their respective properties (name, comment, parent, children, attributes).
>
> I am writing a script that processes such tables, and another one that produces sample tables to test the first script.
> So the first script should import tabular-separated lists, and the second should export them with write.table (or anything better you suggest me).
>
> As long as we stay two dimensional I see no difficulties.
>
> However, there can be several children and several attributes. So my idea was to add a third dimension ? but some tests I did showed me that e.g.
>
> write.table(array(1:60,dim=c(4,3,5)),"beispielmatrix",sep="\t",quote=FALSE,row.names=F,col.names=F,dec=",")
>
> produces a 2D-tsv.
>
> So, how can I handle the fact, that there may be several items in a children resp. atrribute field?
>
> Thank you in advance!
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html[http://www.R-project.org/posting-guide.html]
> and provide commented, minimal, self-contained, reproducible code.
>


From ulrik.stervbo at gmail.com  Fri Nov 11 14:22:15 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Fri, 11 Nov 2016 13:22:15 +0000
Subject: [R] Importing and exporting threedimensional arrays
In-Reply-To: <trinity-48969ad2-5b5d-4a12-bd00-30f32888503a-1478867804922@3capp-gmx-bs71>
References: <trinity-81515856-9a88-405c-a637-dd75f65347af-1478856994584@3capp-gmx-bs71>
	<5825A531.7010301@sapo.pt>
	<trinity-48969ad2-5b5d-4a12-bd00-30f32888503a-1478867804922@3capp-gmx-bs71>
Message-ID: <CAKVAULPzrjXJB4T7C+ebaSX43BzaV6J9HrGknuNbSM2K6c17Dg@mail.gmail.com>

I think that setting the ascii argument in the save command to TRUE might
give a human readable file.

If it'll work for what you want to do later, I don't know

HTH
Ulrik

On Fri, 11 Nov 2016 at 14:13 Ferri Leberl <ferri.leberl at gmx.at> wrote:

>
>
> Thanks for your answer.
>
> I'm afraid it doesn't.
>
> The planned workflow should be:
>
> Somebody delivers me a table containing the essential relations of an
> XML-schema in form of a list that should, once it is functional, be
> generated out of R. So I need a form that is as universal as possible ?
> therefore I thought of a tabular separated list.
> If I make an array:
>
> feld<-array(1:60,dim=c(4,3,5))
>
> save it
>
> save(file="feld",list="feld")
>
> and read it in the bash
>
> cat feld
>
> I receive
>
> U?G?@
>      @Q??z???,9?
>  !??w??,??3c[?/g?2mSD4????C??r?>?#?'?]?????P5?a??(b?#?$RH#?,r??"J(??*j???&Zh??.z?c?!F?c?)f?c
>                     K???[?????????;?o|?????Y??N
>
> So I guess it will be a science of its own to understand, how the file is
> structured.
>
> So I need either a format that allows to easily parse the import 3D-Matrix
> outside R, or I need any good idea how to avoid the third dimension (e.g.,
> might it function to use a second separation character within the collumns
> which may contain several items?
>
> Thank you in advance!
> Have a pleasant weekend.
> Yours, Ferri
>
>
>
>
>
> Gesendet: Freitag, 11. November 2016 um 12:02 Uhr
> Von: "Rui Barradas" <ruipbarradas at sapo.pt>
> An: "Ferri Leberl" <ferri.leberl at gmx.at>, "r-helpr-project.org" <
> r-help at r-project.org>
> Betreff: Re: [R] Importing and exporting threedimensional arrays
> Hello,
>
> You can save 3D objects (or objects of any form or shape) by using ?save.
> You would then retrieve them with ?load.
>
> Hope this helps,
>
> Rui Barradas
>
> Em 11-11-2016 09:36, Ferri Leberl escreveu:
> >
> > Dear all,
> >
> > I want to process a list of XML-Elements.
> > In one dimension the elements are listed; in the other their respective
> properties (name, comment, parent, children, attributes).
> >
> > I am writing a script that processes such tables, and another one that
> produces sample tables to test the first script.
> > So the first script should import tabular-separated lists, and the
> second should export them with write.table (or anything better you suggest
> me).
> >
> > As long as we stay two dimensional I see no difficulties.
> >
> > However, there can be several children and several attributes. So my
> idea was to add a third dimension ? but some tests I did showed me that e.g.
> >
> >
> write.table(array(1:60,dim=c(4,3,5)),"beispielmatrix",sep="\t",quote=FALSE,row.names=F,col.names=F,dec=",")
> >
> > produces a 2D-tsv.
> >
> > So, how can I handle the fact, that there may be several items in a
> children resp. atrribute field?
> >
> > Thank you in advance!
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html[http://www.R-project.org/posting-guide.html]
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Fri Nov 11 16:46:57 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 11 Nov 2016 07:46:57 -0800
Subject: [R] nlme & VarIdent
In-Reply-To: <96E6B0084A068C43B02C637889A5E0B73F7D06@UUSNWE1N.na.utcmail.com>
References: <96E6B0084A068C43B02C637889A5E0B73F7D06@UUSNWE1N.na.utcmail.com>
Message-ID: <CAGxFJbSC8Cbr0mp3XvzcfatnsxXoBLc0FtKbWU-akbK+LD3LZw@mail.gmail.com>

I suggest that you post instead on the r-sig-mixed-models list which
specializes in just this sort of query, and where you are therefore
likely to receive a quicker more authoritative response.


Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Nov 10, 2016 at 1:24 PM, Louisell, Paul T           PW
<Paul.Louisell at pw.utc.com> wrote:
> Hello,
>
> All the help I've read (including Pinheiro and Bates book, 'Mixed Effects Models in S and S-PLUS') regarding how to fit a linear mixed-effects model where variances change with a factor's levels indicates this is done through the 'weights' argument to 'lme', using something like 'weights=varIdent(form=~v|g)' where 'v' is a variance covariate and 'g' is the grouping factor whose strata have different random effect variances.
>
> My question: Suppose I have more than 1 variance covariate, say v1, ..., vk, and I want _each_ of these to have variances that change with the levels of g giving a total of k*nlevels(g) parameters (k*nlevels(g) - k allowing for identifiability). How is this handled in the nlme package? A simple example would be random slope and intercepts, _both_ of which have variances changing with the levels of g. I haven't found any examples of this online or in Pinheiro & Bates, and I haven't been able to figure this out using the various varFunc/pdMat classes.
>
> Help/advice would be greatly appreciated.
>
> Thanks,
>
> Paul Louisell
> Statistical Specialist
> Paul.Louisell at pw.utc.com
> 860-565-8104
>
> Still, tomorrow's going to be another working day, and I'm trying to get some rest.
> That's all, I'm trying to get some rest.
> Paul Simon, "American Tune"
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From macqueen1 at llnl.gov  Fri Nov 11 16:58:44 2016
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Fri, 11 Nov 2016 15:58:44 +0000
Subject: [R] Gobbling up a repeating, irregular list of data
In-Reply-To: <CAPoqHzr1DrM4003agYkdWBMSGHiUvLe0_QPqqfh3s_3MUd7PsA@mail.gmail.com>
References: <CAPoqHzr1DrM4003agYkdWBMSGHiUvLe0_QPqqfh3s_3MUd7PsA@mail.gmail.com>
Message-ID: <D44B25D6.18F1A8%macqueen1@llnl.gov>

Like Peter, I too will assume that all the white space consists of space
characters, not tabs.

In that case, I would probably start with read.fwf().
I would expect that to get me a data frame with lots of NA in the first
four columns. Then (also like Peter says) you'll have to figure out how to
fill the empty cells.

By the way, I wouldn't worry too much about using "bad form." If it works,
would be reasonably easy for someone else looking at your code to
understand
(or for you to understand 5 years from now), and runs fast enough,
that's good enough. But I do appreciate the satisfaction of doing
something "the R way."


Here's another way:

dat <- scan(textConnection("  1    1.00E+00  1.24E+03  7.79E+00  1.925E-01
 1.88E-01
                                     3.850E-01  1.88E-01
                                     5.775E-01  1.88E-01
                                     7.700E-01  1.88E-01
                                     9.626E-01  1.88E-01
                                     1.155E+00  1.88E-01
                                     1.347E+00  1.88E-01
  1    2.00E+00  1.26E+03  7.80E+00  1.925E-01  2.80E-01
                                     1.732E+00  2.80E-01
                                     1.925E+00  2.80E-01
                                     2.310E+00  2.93E-01
                                     2.502E+00  2.22E-01
                                     2.695E+00  1.88E-01
                                     2.887E+00  1.88E-01
  1    3.00E+00  1.28E+03  7.70E+00  1.925E-01  1.03E-01
                                     3.850E-01  1.30E-01
                                     5.775E-01  1.48E-01
                                     7.701E-01  1.61E-01
                                     9.626E-01  1.72E-01
                                     1.155E+00  1.86E-01
                                     1.347E+00  1.93E-01
  1    4.00E+00  1.29E+03  7.60E+00  1.901E-01  1.80E-01
                                     3.803E-01  1.80E-01
                                     5.705E-01  1.38E-01
                                     7.607E-01  1.32E-01
                                     2.282E+00  1.86E-01
                                     2.472E+00  1.98E-01
                                     2.662E+00  2.00E-01"),
  what=list(0,0,0,0,0,0),fill=TRUE
  )
datf <- do.call(cbind, dat)

Then in datf you just have to move the first 2 columns over to be the last
two, in rows where there are missing values, and then fill in the missing
values in the first four columns from the non-missing values above them.



-Don

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 11/10/16, 8:26 PM, "R-help on behalf of Morway, Eric"
<r-help-bounces at r-project.org on behalf of emorway at usgs.gov> wrote:

>What would be the sophisticated R method for reading the data shown below
>into a list?  The data is output from a numerical model.  Pasting the
>second block of example R commands (at the end of the message) results in
>a
>failure ("Error in scan...line 2 did not have 6 elements").  I no doubt
>could cobble together some script for reading line-by-line using for
>loops,
>and then appending vectors with values from each line, but this strikes me
>as bad form.
>
>One final note, the lines with 6 values contain important values that
>should somehow remain associated with the data appearing in columns 5 & 6
>(the continuous data).  The first value, which is always 1, can be
>discarded, but the second value on these lines contain the time step
>number
>("1.00E+00", "2.00E+00", etc.), the 3rd and 4th values are contain a depth
>and thickness, respectively. Columns 5 & 6 are a depth and water content
>pairing and should be associated with the time steps.
>
>Thanks, Eric
>
>Start of example output data (Use of an R script to read in this data
>below)
>
>  1    1.00E+00  1.24E+03  7.79E+00  1.925E-01  1.88E-01
>                                     3.850E-01  1.88E-01
>                                     5.775E-01  1.88E-01
>                                     7.700E-01  1.88E-01
>                                     9.626E-01  1.88E-01
>                                     1.155E+00  1.88E-01
>                                     1.347E+00  1.88E-01
>  1    2.00E+00  1.26E+03  7.80E+00  1.925E-01  2.80E-01
>                                     1.732E+00  2.80E-01
>                                     1.925E+00  2.80E-01
>                                     2.310E+00  2.93E-01
>                                     2.502E+00  2.22E-01
>                                     2.695E+00  1.88E-01
>                                     2.887E+00  1.88E-01
>  1    3.00E+00  1.28E+03  7.70E+00  1.925E-01  1.03E-01
>                                     3.850E-01  1.30E-01
>                                     5.775E-01  1.48E-01
>                                     7.701E-01  1.61E-01
>                                     9.626E-01  1.72E-01
>                                     1.155E+00  1.86E-01
>                                     1.347E+00  1.93E-01
>  1    4.00E+00  1.29E+03  7.60E+00  1.901E-01  1.80E-01
>                                     3.803E-01  1.80E-01
>                                     5.705E-01  1.38E-01
>                                     7.607E-01  1.32E-01
>                                     2.282E+00  1.86E-01
>                                     2.472E+00  1.98E-01
>                                     2.662E+00  2.00E-01
>
>Same data as above, but scan function fails.
>
>dat <- read.table(textConnection("  1    1.00E+00  1.24E+03  7.79E+00
> 1.925E-01  1.88E-01
>                                     3.850E-01  1.88E-01
>                                     5.775E-01  1.88E-01
>                                     7.700E-01  1.88E-01
>                                     9.626E-01  1.88E-01
>                                     1.155E+00  1.88E-01
>                                     1.347E+00  1.88E-01
>  1    2.00E+00  1.26E+03  7.80E+00  1.925E-01  2.80E-01
>                                     1.732E+00  2.80E-01
>                                     1.925E+00  2.80E-01
>                                     2.310E+00  2.93E-01
>                                     2.502E+00  2.22E-01
>                                     2.695E+00  1.88E-01
>                                     2.887E+00  1.88E-01
>  1    3.00E+00  1.28E+03  7.70E+00  1.925E-01  1.03E-01
>                                     3.850E-01  1.30E-01
>                                     5.775E-01  1.48E-01
>                                     7.701E-01  1.61E-01
>                                     9.626E-01  1.72E-01
>                                     1.155E+00  1.86E-01
>                                     1.347E+00  1.93E-01
>  1    4.00E+00  1.29E+03  7.60E+00  1.901E-01  1.80E-01
>                                     3.803E-01  1.80E-01
>                                     5.705E-01  1.38E-01
>                                     7.607E-01  1.32E-01
>                                     2.282E+00  1.86E-01
>                                     2.472E+00  1.98E-01
>                                     2.662E+00  2.00E-01"),header=FALSE)
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From J.delasHeras at ed.ac.uk  Fri Nov 11 17:12:16 2016
From: J.delasHeras at ed.ac.uk (DE LAS HERAS Jose)
Date: Fri, 11 Nov 2016 16:12:16 +0000
Subject: [R] How to remove box in Venn plots (Vennerable package,
 uses grid) - similar to bty="n" in standard plots
Message-ID: <AM3PR05MB1396C5F07B1471C2E98A580DB1BB0@AM3PR05MB1396.eurprd05.prod.outlook.com>

I'm using the package Vennerable to make Venn diagrams, but it always makes a box around the diagram.

Using standard R plots I could eliminate that by indicating


bty="n"


but it seems Vennerable uses the Grid package to generate its plots and I'm not really familiar enough with Grid. I was looking at the documentation but I can't seem to find a way to achieve that. I'd even be happy drawing a white rectangle with wide lines to overplot the box, but there must be a way to not draw the box in the first place.


Anybody knows how?


Jose

--

Dr. Jose I. de las Heras
The Wellcome Trust Centre for Cell Biology
Swann Building
Max Born Crescent
University of Edinburgh
Edinburgh EH9 3BF
UK

Phone: +44 (0)131 6507090

Fax:  +44 (0)131 6507360
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20161111/dab75946/attachment.pl>

From bgunter.4567 at gmail.com  Fri Nov 11 17:30:32 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 11 Nov 2016 08:30:32 -0800
Subject: [R] Importing and exporting threedimensional arrays
In-Reply-To: <trinity-48969ad2-5b5d-4a12-bd00-30f32888503a-1478867804922@3capp-gmx-bs71>
References: <trinity-81515856-9a88-405c-a637-dd75f65347af-1478856994584@3capp-gmx-bs71>
	<5825A531.7010301@sapo.pt>
	<trinity-48969ad2-5b5d-4a12-bd00-30f32888503a-1478867804922@3capp-gmx-bs71>
Message-ID: <CAGxFJbRMy7oFNtQ0ncQ17y+sNDqa71an4yU84P=UW3e9YuG_1g@mail.gmail.com>

Ferri:

I am not sure that I adequately understand your question, but I'll
give it a shot.

1) Do a Web Search on "long form data"; in particular, the following
seemed to be most helpful, but you may prefer one of the other hits:

http://stanford.edu/~ejdemyr/r-tutorials/wide-and-long/

2) See ?reshape  in R for R's take on this.

3) If this is not helpful, note that any 3d array can be
conceptualized as a list of 2d matrices, and this immediately gives an
algorithm to convert a 3-d array to a 2-d matrix which can then be
outputted as an ascii table via write.table() . e.g.

> a <- array(1:24,dim=2:4)
> a
, , 1

     [,1] [,2] [,3]
[1,]    1    3    5
[2,]    2    4    6

, , 2

     [,1] [,2] [,3]
[1,]    7    9   11
[2,]    8   10   12

, , 3

     [,1] [,2] [,3]
[1,]   13   15   17
[2,]   14   16   18

, , 4

     [,1] [,2] [,3]
[1,]   19   21   23
[2,]   20   22   24

> b <-do.call(rbind,lapply(1:4,function(i)I(a[,,i])))
> b
     [,1] [,2] [,3]
[1,]    1    3    5
[2,]    2    4    6
[3,]    7    9   11
[4,]    8   10   12
[5,]   13   15   17
[6,]   14   16   18
[7,]   19   21   23
[8,]   20   22   24

Note, however, that this is quite inefficient, and a much faster way
to do this would be to take advantage of the column major order in
which arrays are stored; i.e. an array is actually a vector with a
"dim" attribute. However, I'm too lazy to fool with that now .

But if I understand correctly, your probably better off using long
form data in R to process your data as a (tabular) data frame and
forget about the 3d business altogether.

HTH

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Nov 11, 2016 at 4:36 AM, Ferri Leberl <ferri.leberl at gmx.at> wrote:
>
>
> Thanks for your answer.
>
> I'm afraid it doesn't.
>
> The planned workflow should be:
>
> Somebody delivers me a table containing the essential relations of an XML-schema in form of a list that should, once it is functional, be generated out of R. So I need a form that is as universal as possible ? therefore I thought of a tabular separated list.
> If I make an array:
>
> feld<-array(1:60,dim=c(4,3,5))
>
> save it
>
> save(file="feld",list="feld")
>
> and read it in the bash
>
> cat feld
>
> I receive
>
> U?G?@
>      @Q??z???,9?  !??w??,??3c[?/g?2mSD4????C??r?>?#?'?]?????P5?a??(b?#?$RH#?,r??"J(??*j???&Zh??.z?c?!F?c?)f?c
>                     K???[?????????;?o|?????Y??N
>
> So I guess it will be a science of its own to understand, how the file is structured.
>
> So I need either a format that allows to easily parse the import 3D-Matrix outside R, or I need any good idea how to avoid the third dimension (e.g., might it function to use a second separation character within the collumns which may contain several items?
>
> Thank you in advance!
> Have a pleasant weekend.
> Yours, Ferri
>
>
>
>
>
> Gesendet: Freitag, 11. November 2016 um 12:02 Uhr
> Von: "Rui Barradas" <ruipbarradas at sapo.pt>
> An: "Ferri Leberl" <ferri.leberl at gmx.at>, "r-helpr-project.org" <r-help at r-project.org>
> Betreff: Re: [R] Importing and exporting threedimensional arrays
> Hello,
>
> You can save 3D objects (or objects of any form or shape) by using ?save.
> You would then retrieve them with ?load.
>
> Hope this helps,
>
> Rui Barradas
>
> Em 11-11-2016 09:36, Ferri Leberl escreveu:
>>
>> Dear all,
>>
>> I want to process a list of XML-Elements.
>> In one dimension the elements are listed; in the other their respective properties (name, comment, parent, children, attributes).
>>
>> I am writing a script that processes such tables, and another one that produces sample tables to test the first script.
>> So the first script should import tabular-separated lists, and the second should export them with write.table (or anything better you suggest me).
>>
>> As long as we stay two dimensional I see no difficulties.
>>
>> However, there can be several children and several attributes. So my idea was to add a third dimension ? but some tests I did showed me that e.g.
>>
>> write.table(array(1:60,dim=c(4,3,5)),"beispielmatrix",sep="\t",quote=FALSE,row.names=F,col.names=F,dec=",")
>>
>> produces a 2D-tsv.
>>
>> So, how can I handle the fact, that there may be several items in a children resp. atrribute field?
>>
>> Thank you in advance!
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html[http://www.R-project.org/posting-guide.html]
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From oluola2011 at yahoo.com  Fri Nov 11 21:45:22 2016
From: oluola2011 at yahoo.com (Olu Ola)
Date: Fri, 11 Nov 2016 20:45:22 +0000 (UTC)
Subject: [R] Alternative to "apply" in R 3.2.2
References: <669988520.2060803.1478897122732.ref@mail.yahoo.com>
Message-ID: <669988520.2060803.1478897122732@mail.yahoo.com>

 Hello,I have a dataset that is similar to the one?as follows:

> Df.1 <- data.frame(A = c(5,4,7,6,8,4),B = (c(1,5,2,4,9,1)),C=(c(2,3,NA,5,NA,9)))
> Df.1
  A B  C
1 5 1  2
2 4 5  3
3 7 2 NA
4 6 4  5
5 8 9 NA
6 4 1  9
> Df.1$D = apply(Df.1, 1, prod, na.rm=T)
> Df.1$D[1]  10  60  14 120  72  36
> Df.1
  A B  C   D
1 5 1  2  10
2 4 5  3  60
3 7 2 NA  14
4 6 4  5 120
5 8 9 NA  72
6 4 1  9  36I intend to obtain a column D that takes into account na.rm=T but 'apply' does not work in R 3.2.2

A way forward will be greatly appreciated.

	[[alternative HTML version deleted]]


From macqueen1 at llnl.gov  Fri Nov 11 22:08:01 2016
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Fri, 11 Nov 2016 21:08:01 +0000
Subject: [R] Alternative to "apply" in R 3.2.2
In-Reply-To: <669988520.2060803.1478897122732@mail.yahoo.com>
References: <669988520.2060803.1478897122732.ref@mail.yahoo.com>
	<669988520.2060803.1478897122732@mail.yahoo.com>
Message-ID: <D44B7300.18F227%macqueen1@llnl.gov>

Df.1$D looks correct to me. For example, in the third row, 7*2=14 is
correct with the NA removed.

-Don

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 11/11/16, 12:45 PM, "R-help on behalf of Olu Ola via R-help"
<r-help-bounces at r-project.org on behalf of r-help at r-project.org> wrote:

> Hello,I have a dataset that is similar to the one as follows:
>
>> Df.1 <- data.frame(A = c(5,4,7,6,8,4),B =
>>(c(1,5,2,4,9,1)),C=(c(2,3,NA,5,NA,9)))
>> Df.1
>  A B  C
>1 5 1  2
>2 4 5  3
>3 7 2 NA
>4 6 4  5
>5 8 9 NA
>6 4 1  9
>> Df.1$D = apply(Df.1, 1, prod, na.rm=T)
>> Df.1$D[1]  10  60  14 120  72  36
>> Df.1
>  A B  C   D
>1 5 1  2  10
>2 4 5  3  60
>3 7 2 NA  14
>4 6 4  5 120
>5 8 9 NA  72
>6 4 1  9  36I intend to obtain a column D that takes into account na.rm=T
>but 'apply' does not work in R 3.2.2
>
>A way forward will be greatly appreciated.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From msharp at txbiomed.org  Fri Nov 11 22:08:07 2016
From: msharp at txbiomed.org (Mark Sharp)
Date: Fri, 11 Nov 2016 21:08:07 +0000
Subject: [R] Alternative to "apply" in R 3.2.2
In-Reply-To: <669988520.2060803.1478897122732@mail.yahoo.com>
References: <669988520.2060803.1478897122732.ref@mail.yahoo.com>
	<669988520.2060803.1478897122732@mail.yahoo.com>
Message-ID: <7FE55A75-41E7-442F-AF73-C0B3D7BED622@TxBiomed.org>

Olu,

I think you may have misread what na.rm is supposed to do. I think you are getting the correct value. If you want the vectors that contain NA values to be evaluated to NA then you will need to set na.rm to FALSE (which is the default for prod()).

prod(c(7, 2, NA), na.rm = TRUE)
[1] 14
prod(c(7, 2, NA), na.rm = FALSE)
[1] NA

Mark
R. Mark Sharp, Ph.D.
Director of Data Science Core
Southwest National Primate Research Center
Texas Biomedical Research Institute
P.O. Box 760549
San Antonio, TX 78245-0549
Telephone: (210)258-9476
e-mail: msharp at TxBiomed.org









> On Nov 11, 2016, at 2:45 PM, Olu Ola via R-help <r-help at r-project.org> wrote:
>
> Hello,I have a dataset that is similar to the one as follows:
>
>> Df.1 <- data.frame(A = c(5,4,7,6,8,4),B = (c(1,5,2,4,9,1)),C=(c(2,3,NA,5,NA,9)))
>> Df.1
>  A B  C
> 1 5 1  2
> 2 4 5  3
> 3 7 2 NA
> 4 6 4  5
> 5 8 9 NA
> 6 4 1  9
>> Df.1$D = apply(Df.1, 1, prod, na.rm=T)
>> Df.1$D[1]  10  60  14 120  72  36
>> Df.1
>  A B  C   D
> 1 5 1  2  10
> 2 4 5  3  60
> 3 7 2 NA  14
> 4 6 4  5 120
> 5 8 9 NA  72
> 6 4 1  9  36I intend to obtain a column D that takes into account na.rm=T but 'apply' does not work in R 3.2.2
>
> A way forward will be greatly appreciated.
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

CONFIDENTIALITY NOTICE: This e-mail and any files and/or...{{dropped:10}}


From oluola2011 at yahoo.com  Sat Nov 12 00:23:08 2016
From: oluola2011 at yahoo.com (Olu Ola)
Date: Fri, 11 Nov 2016 23:23:08 +0000 (UTC)
Subject: [R] Alternative to "apply" in R 3.2.2
In-Reply-To: <D44B7300.18F227%macqueen1@llnl.gov>
References: <669988520.2060803.1478897122732.ref@mail.yahoo.com>
	<669988520.2060803.1478897122732@mail.yahoo.com>
	<D44B7300.18F227%macqueen1@llnl.gov>
Message-ID: <394112823.2129444.1478906588933@mail.yahoo.com>

Hello,I quite understand that apply is doing what it is supposed to do. I actually found that example online. However, "apply" package is not compatible with R 3.2.2 and as a result, I could not use the code. That is why I am asking for an alternative to "apply" that can be used to do the same thing.
Regards 

    On Friday, November 11, 2016 4:08 PM, "MacQueen, Don" <macqueen1 at llnl.gov> wrote:
 

 Df.1$D looks correct to me. For example, in the third row, 7*2=14 is
correct with the NA removed.

-Don

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 11/11/16, 12:45 PM, "R-help on behalf of Olu Ola via R-help"
<r-help-bounces at r-project.org on behalf of r-help at r-project.org> wrote:

> Hello,I have a dataset that is similar to the one as follows:
>
>> Df.1 <- data.frame(A = c(5,4,7,6,8,4),B =
>>(c(1,5,2,4,9,1)),C=(c(2,3,NA,5,NA,9)))
>> Df.1
>? A B? C
>1 5 1? 2
>2 4 5? 3
>3 7 2 NA
>4 6 4? 5
>5 8 9 NA
>6 4 1? 9
>> Df.1$D = apply(Df.1, 1, prod, na.rm=T)
>> Df.1$D[1]? 10? 60? 14 120? 72? 36
>> Df.1
>? A B? C? D
>1 5 1? 2? 10
>2 4 5? 3? 60
>3 7 2 NA? 14
>4 6 4? 5 120
>5 8 9 NA? 72
>6 4 1? 9? 36I intend to obtain a column D that takes into account na.rm=T
>but 'apply' does not work in R 3.2.2
>
>A way forward will be greatly appreciated.
>
>??? [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


   
	[[alternative HTML version deleted]]


From istazahn at gmail.com  Sat Nov 12 00:37:00 2016
From: istazahn at gmail.com (Ista Zahn)
Date: Fri, 11 Nov 2016 18:37:00 -0500
Subject: [R] Alternative to "apply" in R 3.2.2
In-Reply-To: <394112823.2129444.1478906588933@mail.yahoo.com>
References: <669988520.2060803.1478897122732.ref@mail.yahoo.com>
	<669988520.2060803.1478897122732@mail.yahoo.com>
	<D44B7300.18F227%macqueen1@llnl.gov>
	<394112823.2129444.1478906588933@mail.yahoo.com>
Message-ID: <CA+vqiLHbZg+CKfc2_8pSbYWvBsCTF_Ef+KeXcFj+UgCNyU36Ng@mail.gmail.com>

What makes you think " apply' does not work in R 3.2.2"?

On Nov 11, 2016 6:24 PM, "Olu Ola via R-help" <r-help at r-project.org> wrote:

> Hello,I quite understand that apply is doing what it is supposed to do. I
> actually found that example online. However, "apply" package is not
> compatible with R 3.2.2 and as a result, I could not use the code. That is
> why I am asking for an alternative to "apply" that can be used to do the
> same thing.
> Regards
>
>     On Friday, November 11, 2016 4:08 PM, "MacQueen, Don" <
> macqueen1 at llnl.gov> wrote:
>
>
>  Df.1$D looks correct to me. For example, in the third row, 7*2=14 is
> correct with the NA removed.
>
> -Don
>
> --
> Don MacQueen
>
> Lawrence Livermore National Laboratory
> 7000 East Ave., L-627
> Livermore, CA 94550
> 925-423-1062
>
>
>
>
>
> On 11/11/16, 12:45 PM, "R-help on behalf of Olu Ola via R-help"
> <r-help-bounces at r-project.org on behalf of r-help at r-project.org> wrote:
>
> > Hello,I have a dataset that is similar to the one as follows:
> >
> >> Df.1 <- data.frame(A = c(5,4,7,6,8,4),B =
> >>(c(1,5,2,4,9,1)),C=(c(2,3,NA,5,NA,9)))
> >> Df.1
> >  A B  C
> >1 5 1  2
> >2 4 5  3
> >3 7 2 NA
> >4 6 4  5
> >5 8 9 NA
> >6 4 1  9
> >> Df.1$D = apply(Df.1, 1, prod, na.rm=T)
> >> Df.1$D[1]  10  60  14 120  72  36
> >> Df.1
> >  A B  C  D
> >1 5 1  2  10
> >2 4 5  3  60
> >3 7 2 NA  14
> >4 6 4  5 120
> >5 8 9 NA  72
> >6 4 1  9  36I intend to obtain a column D that takes into account na.rm=T
> >but 'apply' does not work in R 3.2.2
> >
> >A way forward will be greatly appreciated.
> >
> >    [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From oluola2011 at yahoo.com  Sat Nov 12 00:39:53 2016
From: oluola2011 at yahoo.com (Olu Ola)
Date: Fri, 11 Nov 2016 23:39:53 +0000 (UTC)
Subject: [R] Alternative to "apply" in R 3.2.2
In-Reply-To: <CA+vqiLHbZg+CKfc2_8pSbYWvBsCTF_Ef+KeXcFj+UgCNyU36Ng@mail.gmail.com>
References: <669988520.2060803.1478897122732.ref@mail.yahoo.com>
	<669988520.2060803.1478897122732@mail.yahoo.com>
	<D44B7300.18F227%macqueen1@llnl.gov>
	<394112823.2129444.1478906588933@mail.yahoo.com>
	<CA+vqiLHbZg+CKfc2_8pSbYWvBsCTF_Ef+KeXcFj+UgCNyU36Ng@mail.gmail.com>
Message-ID: <1963649199.2137120.1478907593888@mail.yahoo.com>

Here is the warning message that appears when I tried installing the apply package:
install.packages("apply")Installing package into ?C:/Users/Olufemi/Documents/R/win-library/3.2?(as ?lib? is unspecified)Warning in install.packages : package ?apply? is not available (for R version 3.2.2)
Regards 

    On Friday, November 11, 2016 6:37 PM, Ista Zahn <istazahn at gmail.com> wrote:
 

 What makes you think " apply' does not work in R 3.2.2"?
On Nov 11, 2016 6:24 PM, "Olu Ola via R-help" <r-help at r-project.org> wrote:

Hello,I quite understand that apply is doing what it is supposed to do. I actually found that example online. However, "apply" package is not compatible with R 3.2.2 and as a result, I could not use the code. That is why I am asking for an alternative to "apply" that can be used to do the same thing.
Regards

? ? On Friday, November 11, 2016 4:08 PM, "MacQueen, Don" <macqueen1 at llnl.gov> wrote:


?Df.1$D looks correct to me. For example, in the third row, 7*2=14 is
correct with the NA removed.

-Don

--
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 11/11/16, 12:45 PM, "R-help on behalf of Olu Ola via R-help"
<r-help-bounces at r-project.org on behalf of r-help at r-project.org> wrote:

> Hello,I have a dataset that is similar to the one as follows:
>
>> Df.1 <- data.frame(A = c(5,4,7,6,8,4),B =
>>(c(1,5,2,4,9,1)),C=(c(2,3, NA,5,NA,9)))
>> Df.1
>? A B? C
>1 5 1? 2
>2 4 5? 3
>3 7 2 NA
>4 6 4? 5
>5 8 9 NA
>6 4 1? 9
>> Df.1$D = apply(Df.1, 1, prod, na.rm=T)
>> Df.1$D[1]? 10? 60? 14 120? 72? 36
>> Df.1
>? A B? C? D
>1 5 1? 2? 10
>2 4 5? 3? 60
>3 7 2 NA? 14
>4 6 4? 5 120
>5 8 9 NA? 72
>6 4 1? 9? 36I intend to obtain a column D that takes into account na.rm=T
>but 'apply' does not work in R 3.2.2
>
>A way forward will be greatly appreciated.
>
>??? [[alternative HTML version deleted]]
>
>_____________________________ _________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/ listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/ posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.



? ? ? ? [[alternative HTML version deleted]]

______________________________ ________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/ listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/ posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


   
	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Sat Nov 12 00:44:40 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 11 Nov 2016 15:44:40 -0800
Subject: [R] Alternative to "apply" in R 3.2.2
In-Reply-To: <1963649199.2137120.1478907593888@mail.yahoo.com>
References: <669988520.2060803.1478897122732.ref@mail.yahoo.com>
	<669988520.2060803.1478897122732@mail.yahoo.com>
	<D44B7300.18F227%macqueen1@llnl.gov>
	<394112823.2129444.1478906588933@mail.yahoo.com>
	<CA+vqiLHbZg+CKfc2_8pSbYWvBsCTF_Ef+KeXcFj+UgCNyU36Ng@mail.gmail.com>
	<1963649199.2137120.1478907593888@mail.yahoo.com>
Message-ID: <0952DAD8-DD55-421C-9B2F-886DFDA26602@dcn.davis.ca.us>

The "apply" I am familiar with us a FUNCTION in base R, not a contributed package. That is to say, it is always available in R, not something you need to load. 
-- 
Sent from my phone. Please excuse my brevity.

On November 11, 2016 3:39:53 PM PST, Olu Ola via R-help <r-help at r-project.org> wrote:
>Here is the warning message that appears when I tried installing the
>apply package:
>install.packages("apply")Installing package into
>?C:/Users/Olufemi/Documents/R/win-library/3.2?(as ?lib? is
>unspecified)Warning in install.packages : package ?apply? is not
>available (for R version 3.2.2)
>Regards 
>
>On Friday, November 11, 2016 6:37 PM, Ista Zahn <istazahn at gmail.com>
>wrote:
> 
>
> What makes you think " apply' does not work in R 3.2.2"?
>On Nov 11, 2016 6:24 PM, "Olu Ola via R-help" <r-help at r-project.org>
>wrote:
>
>Hello,I quite understand that apply is doing what it is supposed to do.
>I actually found that example online. However, "apply" package is not
>compatible with R 3.2.2 and as a result, I could not use the code. That
>is why I am asking for an alternative to "apply" that can be used to do
>the same thing.
>Regards
>
>? ? On Friday, November 11, 2016 4:08 PM, "MacQueen, Don"
><macqueen1 at llnl.gov> wrote:
>
>
>?Df.1$D looks correct to me. For example, in the third row, 7*2=14 is
>correct with the NA removed.
>
>-Don
>
>--
>Don MacQueen
>
>Lawrence Livermore National Laboratory
>7000 East Ave., L-627
>Livermore, CA 94550
>925-423-1062
>
>
>
>
>
>On 11/11/16, 12:45 PM, "R-help on behalf of Olu Ola via R-help"
><r-help-bounces at r-project.org on behalf of r-help at r-project.org> wrote:
>
>> Hello,I have a dataset that is similar to the one as follows:
>>
>>> Df.1 <- data.frame(A = c(5,4,7,6,8,4),B =
>>>(c(1,5,2,4,9,1)),C=(c(2,3, NA,5,NA,9)))
>>> Df.1
>>? A B? C
>>1 5 1? 2
>>2 4 5? 3
>>3 7 2 NA
>>4 6 4? 5
>>5 8 9 NA
>>6 4 1? 9
>>> Df.1$D = apply(Df.1, 1, prod, na.rm=T)
>>> Df.1$D[1]? 10? 60? 14 120? 72? 36
>>> Df.1
>>? A B? C? D
>>1 5 1? 2? 10
>>2 4 5? 3? 60
>>3 7 2 NA? 14
>>4 6 4? 5 120
>>5 8 9 NA? 72
>>6 4 1? 9? 36I intend to obtain a column D that takes into account
>na.rm=T
>>but 'apply' does not work in R 3.2.2
>>
>>A way forward will be greatly appreciated.
>>
>>??? [[alternative HTML version deleted]]
>>
>>_____________________________ _________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/ listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/ posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>
>
>? ? ? ? [[alternative HTML version deleted]]
>
>______________________________ ________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/ listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/
>posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>   
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From oluola2011 at yahoo.com  Sat Nov 12 00:55:38 2016
From: oluola2011 at yahoo.com (Olu Ola)
Date: Fri, 11 Nov 2016 23:55:38 +0000 (UTC)
Subject: [R] Alternative to "apply" in R 3.2.2
In-Reply-To: <0952DAD8-DD55-421C-9B2F-886DFDA26602@dcn.davis.ca.us>
References: <669988520.2060803.1478897122732.ref@mail.yahoo.com>
	<669988520.2060803.1478897122732@mail.yahoo.com>
	<D44B7300.18F227%macqueen1@llnl.gov>
	<394112823.2129444.1478906588933@mail.yahoo.com>
	<CA+vqiLHbZg+CKfc2_8pSbYWvBsCTF_Ef+KeXcFj+UgCNyU36Ng@mail.gmail.com>
	<1963649199.2137120.1478907593888@mail.yahoo.com>
	<0952DAD8-DD55-421C-9B2F-886DFDA26602@dcn.davis.ca.us>
Message-ID: <386745346.2135175.1478908538990@mail.yahoo.com>

Thank you.
Regards 

    On Friday, November 11, 2016 6:44 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
 

 The "apply" I am familiar with us a FUNCTION in base R, not a contributed package. That is to say, it is always available in R, not something you need to load. 
-- 
Sent from my phone. Please excuse my brevity.

On November 11, 2016 3:39:53 PM PST, Olu Ola via R-help <r-help at r-project.org> wrote:
>Here is the warning message that appears when I tried installing the
>apply package:
>install.packages("apply")Installing package into
>?C:/Users/Olufemi/Documents/R/win-library/3.2?(as ?lib? is
>unspecified)Warning in install.packages : package ?apply? is not
>available (for R version 3.2.2)
>Regards 
>
>On Friday, November 11, 2016 6:37 PM, Ista Zahn <istazahn at gmail.com>
>wrote:
> 
>
> What makes you think " apply' does not work in R 3.2.2"?
>On Nov 11, 2016 6:24 PM, "Olu Ola via R-help" <r-help at r-project.org>
>wrote:
>
>Hello,I quite understand that apply is doing what it is supposed to do.
>I actually found that example online. However, "apply" package is not
>compatible with R 3.2.2 and as a result, I could not use the code. That
>is why I am asking for an alternative to "apply" that can be used to do
>the same thing.
>Regards
>
>? ? On Friday, November 11, 2016 4:08 PM, "MacQueen, Don"
><macqueen1 at llnl.gov> wrote:
>
>
>?Df.1$D looks correct to me. For example, in the third row, 7*2=14 is
>correct with the NA removed.
>
>-Don
>
>--
>Don MacQueen
>
>Lawrence Livermore National Laboratory
>7000 East Ave., L-627
>Livermore, CA 94550
>925-423-1062
>
>
>
>
>
>On 11/11/16, 12:45 PM, "R-help on behalf of Olu Ola via R-help"
><r-help-bounces at r-project.org on behalf of r-help at r-project.org> wrote:
>
>> Hello,I have a dataset that is similar to the one as follows:
>>
>>> Df.1 <- data.frame(A = c(5,4,7,6,8,4),B =
>>>(c(1,5,2,4,9,1)),C=(c(2,3, NA,5,NA,9)))
>>> Df.1
>>? A B? C
>>1 5 1? 2
>>2 4 5? 3
>>3 7 2 NA
>>4 6 4? 5
>>5 8 9 NA
>>6 4 1? 9
>>> Df.1$D = apply(Df.1, 1, prod, na.rm=T)
>>> Df.1$D[1]? 10? 60? 14 120? 72? 36
>>> Df.1
>>? A B? C? D
>>1 5 1? 2? 10
>>2 4 5? 3? 60
>>3 7 2 NA? 14
>>4 6 4? 5 120
>>5 8 9 NA? 72
>>6 4 1? 9? 36I intend to obtain a column D that takes into account
>na.rm=T
>>but 'apply' does not work in R 3.2.2
>>
>>A way forward will be greatly appreciated.
>>
>>??? [[alternative HTML version deleted]]
>>
>>_____________________________ _________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/ listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/ posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>
>
>? ? ? ? [[alternative HTML version deleted]]
>
>______________________________ ________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/ listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/
>posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>? 
>??? [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.



   
	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sat Nov 12 01:50:59 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 11 Nov 2016 16:50:59 -0800
Subject: [R] Alternative to "apply" in R 3.2.2
In-Reply-To: <1963649199.2137120.1478907593888@mail.yahoo.com>
References: <669988520.2060803.1478897122732.ref@mail.yahoo.com>
	<669988520.2060803.1478897122732@mail.yahoo.com>
	<D44B7300.18F227%macqueen1@llnl.gov>
	<394112823.2129444.1478906588933@mail.yahoo.com>
	<CA+vqiLHbZg+CKfc2_8pSbYWvBsCTF_Ef+KeXcFj+UgCNyU36Ng@mail.gmail.com>
	<1963649199.2137120.1478907593888@mail.yahoo.com>
Message-ID: <CAGxFJbSQTbmQdskvC6P+USGGsm+5qfqsDG93hu+AjLspax=oVA@mail.gmail.com>

Wow, you are quite confused! Have you gone through any basic R
tutorials (there are many good ones on the web), as it sure looks like
you have not and therefore have almost no idea how to use R. If that
is the case, it is unlikely that this list (or R!) is going to be much
use to you.

Anyway, apply() is a standard *built-in function* in base R. There is
NO apply package.


-- Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Nov 11, 2016 at 3:39 PM, Olu Ola via R-help
<r-help at r-project.org> wrote:
> Here is the warning message that appears when I tried installing the apply package:
> install.packages("apply")Installing package into ?C:/Users/Olufemi/Documents/R/win-library/3.2?(as ?lib? is unspecified)Warning in install.packages : package ?apply? is not available (for R version 3.2.2)
> Regards
>
>     On Friday, November 11, 2016 6:37 PM, Ista Zahn <istazahn at gmail.com> wrote:
>
>
>  What makes you think " apply' does not work in R 3.2.2"?
> On Nov 11, 2016 6:24 PM, "Olu Ola via R-help" <r-help at r-project.org> wrote:
>
> Hello,I quite understand that apply is doing what it is supposed to do. I actually found that example online. However, "apply" package is not compatible with R 3.2.2 and as a result, I could not use the code. That is why I am asking for an alternative to "apply" that can be used to do the same thing.
> Regards
>
>     On Friday, November 11, 2016 4:08 PM, "MacQueen, Don" <macqueen1 at llnl.gov> wrote:
>
>
>  Df.1$D looks correct to me. For example, in the third row, 7*2=14 is
> correct with the NA removed.
>
> -Don
>
> --
> Don MacQueen
>
> Lawrence Livermore National Laboratory
> 7000 East Ave., L-627
> Livermore, CA 94550
> 925-423-1062
>
>
>
>
>
> On 11/11/16, 12:45 PM, "R-help on behalf of Olu Ola via R-help"
> <r-help-bounces at r-project.org on behalf of r-help at r-project.org> wrote:
>
>> Hello,I have a dataset that is similar to the one as follows:
>>
>>> Df.1 <- data.frame(A = c(5,4,7,6,8,4),B =
>>>(c(1,5,2,4,9,1)),C=(c(2,3, NA,5,NA,9)))
>>> Df.1
>>  A B  C
>>1 5 1  2
>>2 4 5  3
>>3 7 2 NA
>>4 6 4  5
>>5 8 9 NA
>>6 4 1  9
>>> Df.1$D = apply(Df.1, 1, prod, na.rm=T)
>>> Df.1$D[1]  10  60  14 120  72  36
>>> Df.1
>>  A B  C  D
>>1 5 1  2  10
>>2 4 5  3  60
>>3 7 2 NA  14
>>4 6 4  5 120
>>5 8 9 NA  72
>>6 4 1  9  36I intend to obtain a column D that takes into account na.rm=T
>>but 'apply' does not work in R 3.2.2
>>
>>A way forward will be greatly appreciated.
>>
>>    [[alternative HTML version deleted]]
>>
>>_____________________________ _________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/ listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/ posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________ ________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/ listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/ posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From songxp15 at lzu.edu.cn  Fri Nov 11 10:31:23 2016
From: songxp15 at lzu.edu.cn (=?UTF-8?B?5a6L5pet6JCN?=)
Date: Fri, 11 Nov 2016 17:31:23 +0800 (GMT+08:00)
Subject: [R] Message from Lanzhou University, China. (Help)
Message-ID: <6f90a404.73da.15852ba30a2.Coremail.songxp15@lzu.edu.cn>

Dear moderators,
I am a student come from Lanzhou University, Gansu, China. I have two questions on R (mgcv), which puzzled me for a long time. I did not find answer in Chinese forum. I hope you could answer me in your free time or post it in R-help. Thank you very much.
                                                                       Xuping Song
                                                                        2016.11.11
We want to investigate the association between exposure to ambient temperature and blood pressure. The ?mgcv? package was used to perform GAM analyses in R. This is the formula.
ct=gam(Blood pressure~s(Temp,k=5)+s(BMI,k=4)+s(Age,k=6)+s(RH,k=4)+s(Pa,k=5)+Gender+Season,family=gaussian,data=mydata)             

I have two questions. Firstly, personal characteristic (Age, Gender, BMI and Season) could influence blood pressure. How to control these variables? BMI and Age are continuous variable. I smoothed these two variables like formula 1. Season (1,2,3,4) and Gender(0,1) were add to formula, which were assumed as linear relationship. Is that right?

Secondly, I want to get plot 2, which x-axis presented ?Temperature? and y-axis presented ?Change in Blood Pressure for every 1?C decrease of temperature?. Plot 2 came from a paper published recently (see appendix). Therefore, I ran formula 1 in R and run ?plot(ct)?. Then I get plot 1. I don?t understand the meaning of ?s?Tmean?3.87?? in y-axis. Does it represent ?Blood pressure?? If the hypothesis is right, plot 2 is the derivative of plot 1. However, I don?t know the specific code. I am a beginner of R and this problem puzzled me for a long time. Please help me. Thank you very much.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Paper (plot 2).pdf
Type: application/pdf
Size: 913655 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20161111/4c9d7aec/attachment-0001.pdf>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Plot 2.png
Type: image/png
Size: 37455 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20161111/4c9d7aec/attachment-0001.png>

From akewander at yahoo.ie  Fri Nov 11 18:51:32 2016
From: akewander at yahoo.ie (ahmed meftah)
Date: Fri, 11 Nov 2016 17:51:32 +0000 (UTC)
Subject: [R] Variable 'A' is not a factor Error message
References: <1957360193.3935030.1478886692398.ref@mail.yahoo.com>
Message-ID: <1957360193.3935030.1478886692398@mail.yahoo.com>

I am running a DOE with the following code   library(Rcmdr)
    library(RcmdrMisc)
    library(RcmdrPlugin.DoE)
# Define plackett burman experiment
    PB.DOE <- pb(nruns= 12 ,n12.taguchi= FALSE ,nfactors= 12 -1, ncenter= 0 ,
                 replications= 1 ,repeat.only= FALSE ,randomize= TRUE ,seed= 27241 , 
                 factor.names=list( A=c(100,1000),B=c(100,200),C=c(1,3),D=c(1,1.7),
                                    E=c(1000,1500),G=c(-2,2) ) )

    as.numeric2 <- function(x) as.numeric(as.character(x))

# Calculate response column
    IP <- with(PB.DOE,(as.numeric2(A)*as.numeric2(B)*(5000-3000))/(141.2*as.numeric2(C)*as.numeric2(D)*(log(as.numeric2(E)/0.25)-(1/2)+as.numeric2(G))))
# Combine response column with exp design table
    final_set <- within(PB.DOE, {
      IP<- ((as.numeric2(A)*as.numeric2(B)*(5000-3000))/(141.2*as.numeric2(C)*as.numeric2(D)*(log(as.numeric2(E)/0.25)-(1/2)+as.numeric2(G))))
    })I then ran a regression as follows:LinearModel.1 <- lm(IP ~ A + B + C + D + E + G, 
                    data=final_set)
summary(LinearModel.1)Following this i wanted to run a predict using specified values as predictors in a Monte Carlo:n = 10000
# Define probability distributions of predictors
A = rnorm(n,450,100)
hist(A,col = "blue",breaks = 50)

B = rnorm(n, 150,10)
hist(B,col = "blue",breaks = 50)

C = rnorm(n, 1.5, 0.5)
hist(C,col = "blue",breaks = 50)

D = runif(n,1.2,1.7)
hist(D,col = "blue",breaks = 50)

E = rnorm(n,1250,50)
hist(E,col = "blue",breaks = 50)

G = rnorm(n,0,0.5)
hist(G,col = "blue",breaks = 50)

MCtable <- data.frame(A=A,B=B,C=C,D=D,E=E,G=G)

for (n in 1:n) {
  N=predict(LinearModel.1,MCtable)
}

hist(N,col = "yellow",breaks = 10)I end up getting this error:"Warning in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) :
  variable 'A' is not a factor"Using str() to get some info on the LinearModel.1 and from what I understand seems to indicates that since the predictors A,B,C etc are factors with 2 levels I have to convert my data.frame table to factors aswell. Is that correct?Doing this would mean I would also need to specify the number of levels which would mean that since I have set my n to 10000 would mean 10000 levels for each factor. How would I go about doing this? Is there a better solution? Any help would be appreciated.
	[[alternative HTML version deleted]]


From davetgerrard at gmail.com  Fri Nov 11 11:51:20 2016
From: davetgerrard at gmail.com (Dave Gerrard)
Date: Fri, 11 Nov 2016 10:51:20 +0000
Subject: [R] Trailing path separators and dirname(), file.exists(),
	file.path()
Message-ID: <CADFyZRVafHyqKP6PeY2qsU7h+n6CTdT_Fh3wAG2NyxVKccqUyg@mail.gmail.com>

I've found something irritating in the behaviour of dirname() and
subsequent interactions with file.path() and file.exists().  The problem is
how it and they deal with trailing path separators.   It seems the code is
correct to specification but I think it could be better. Part of the
problem is restricted to Windows and this is alluded to in the docs but the
implications are not fully explicit. Providing a path with a trailing path
separator to dirname() causes the final directory to be removed.


In Linux (R 3.3.1)

> file.path("","p1","p2","p3")
[1] "/p1/p2/p3"
> dirname(file.path("","p1","p2","p3"))     # p3 is treated as a filename,
not a directory in the path - fine.
[1] "/p1/p2"
> file.path("","p1","p2","p3","/") # not the "//" by file.path"
[1] "/p1/p2/p3//"
> dirname(file.path("","p1","p2","p3","/"))    # p3 is now treated as a
directory  - fine.
[1] "/p1/p2/p3"
> dirname("/p1/p2/p3/") # but when I give a path with no file at the end.
What I consider a directory, is chopped off as a final filename.
[1] "/p1/p2"



In Windows (R 3.3.1)

>  file.path("","p1","p2","p3") # as per linux
[1] "/p1/p2/p3"
> dirname(file.path("","p1","p2","p3"))  # as per linux
[1] "/p1/p2"
> file.path("","p1","p2","p3","/") # N.B. no second "/"
[1] "/p1/p2/p3/"
> dirname(file.path("","p1","p2","p3","/")) # last 'directory' lost
[1] "/p1/p2"
> dirname("/p1/p2/p3/") # last 'directory' lost
[1] "/p1/p2"


This is a problem for me when testing for the existence of directories
(e.g. have I already made an output directory for this analysis)

In Windows (R 3.3.1)

> dir.create("temp")
> file.exists("temp")
[1] TRUE
> file.exists("temp/") # R on windows not recognising and removing the
trailing path separator.
[1] FALSE
> file.exists(dirname("temp/"))
[1] TRUE
> dirname("temp/")
[1] "."


On Linux, I can use a trailing separator, but I still shouldn't use
dirname() :-

> dir.create("temp")
> file.exists("temp")
[1] TRUE
> file.exists("temp/")
[1] TRUE
> file.exists(dirname("temp/"))       # false positive
[1] TRUE
> dirname("temp/")                        # what it is really testing for.
[1] "."



Does anyone else feel this is not as good as it should be?


I don't know how old this behavious is. I've only tested R 3.3.1 but the
docs for file.path() suggest there may have been some recent tinkering:-

"Trailing path separators are invalid for Windows file paths apart from ?/?
and ?d:/? (although some functions/utilities do accept them), so as from R
3.1.0 a trailing / or \ is removed."

Dave Gerrard

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Sat Nov 12 23:12:38 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sun, 13 Nov 2016 09:12:38 +1100
Subject: [R] Average every 4 columns
In-Reply-To: <CAMLwc7O7R=Bg2jdZJDBcBV6oyU1fGRR4eBdi5R6SpJgKSPvdFQ@mail.gmail.com>
References: <CAMLwc7O7R=Bg2jdZJDBcBV6oyU1fGRR4eBdi5R6SpJgKSPvdFQ@mail.gmail.com>
Message-ID: <CA+8X3fXxcU0MnY3ebObHBSnKtksp1QxY8hDvcCZFhVP_p7hWgA@mail.gmail.com>

Hi Miluj,
Perhaps you didn't get my previous email. Let your data frame be named "msdf":

block_col_summ<-function(x,step,block_size,FUN="mean") {
 dimx<-dim(x)
 return_value<-NA
 start<-1
 end<-start+block_size-1
 block_count<-1
 while(end <= dimx[2]) {
  return_value[block_count]<-
   do.call(FUN,list(as.matrix(x[,start:end]),na.rm=TRUE))
  block_count<-block_count+1
  start<-start+step
  end<-start+block_size-1
 }
 return(return_value)
}
# use the step and block size that you want
block_col_summ(msdf,4,4)

Jim


On Thu, Nov 10, 2016 at 3:29 AM, Miluji Sb <milujisb at gmail.com> wrote:
> Dear all,
>
> I have a dataset with hundreds of columns, I am only providing only 12
> columns. Is it possible to take the mean of every four (or 12) columns
>  (value601, value602, value603, value604 etc.in this case) and repeat for
> the hundreds of columns? Thank you.
>
> Sincerely,
>
> Milu
>
> structure(list(value601 = c(10.1738710403442, 3.54112911224365,
> 12.9192342758179, 3.17447590827942, 11.7332258224487, 7.68282270431519,
> -7.11564493179321, 0.987620949745178, 13.0476207733154, 6.36939525604248
> ), value602 = c(13.0642414093018, 5.53129482269287, 16.0519638061523,
> 2.88946437835693, 14.9204912185669, 9.42428588867188, -6.80674123764038,
> -0.614241063594818, 16.7947769165039, 7.9541072845459), value603 =
> c(22.0399188995361,
> 14.398024559021, 24.9523792266846, 12.0878629684448, 23.6459674835205,
> 18.3277816772461, -2.54092741012573, 10.5550804138184, 25.1016540527344,
> 16.2166938781738), value604 = c(27.7165412902832, 20.3255825042725,
> 30.8430004119873, 16.6856250762939, 29.2485408782959, 24.3775005340576,
> 6.47758340835571, 15.5897912979126, 30.7387924194336, 22.3637084960938
> ), value605 = c(31.6644763946533, 23.4093952178955, 35.1488723754883,
> 19.7132263183594, 33.3924179077148, 29.5846366882324, 10.2083873748779,
> 19.3551616668701, 35.3076629638672, 27.4299201965332), value606 =
> c(33.9698333740234,
> 26.8574161529541, 36.8900833129883, 22.8604583740234, 34.8642921447754,
> 33.8158760070801, 14.7055835723877, 22.1144580841064, 37.0545425415039,
> 32.1087913513184), value607 = c(36.0279846191406, 26.9297180175781,
> 38.2701225280762, 23.2643146514893, 36.7398796081543, 34.1216125488281,
> 17.1387901306152, 24.0419750213623, 37.8542327880859, 32.7677421569824
> ), value608 = c(34.0242347717285, 25.7720966339111, 36.4897193908691,
> 22.0332260131836, 34.8011703491211, 32.6856842041016, 16.6232261657715,
> 21.5571365356445, 36.1491546630859, 31.1716938018799), value609 =
> c(27.5402088165283,
> 21.7590408325195, 30.5214176177979, 18.4252090454102, 29.1156253814697,
> 26.9878330230713, 12.4962501525879, 17.7259578704834, 30.9099159240723,
> 25.4832077026367), value610 = c(23.4706859588623, 17.0126209259033,
> 26.8166942596436, 15.297459602356, 25.1733055114746, 23.5616931915283,
> 8.86995983123779, 13.5793552398682, 27.5732250213623, 22.1691932678223
> ), value611 = c(14.5820417404175, 9.08279132843018, 17.8419170379639,
> 8.36016654968262, 16.5633754730225, 14.8123331069946, 1.32095837593079,
> 5.73408317565918, 18.9752082824707, 13.5729999542236), value612 =
> c(9.12979793548584,
> 2.79943537712097, 11.6504030227661, 2.21584677696228, 10.5404834747314,
> 7.55471754074097, -5.58141136169434, -0.566209673881531, 12.3264112472534,
> 6.65576601028442)), .Names = c("value601", "value602", "value603",
> "value604", "value605", "value606", "value607", "value608", "value609",
> "value610", "value611", "value612"), row.names = c("1", "2",
> "3", "4", "5", "6", "7", "8", "9", "10"), class = "data.frame")
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From oriolebaltimore at gmail.com  Sat Nov 12 23:25:51 2016
From: oriolebaltimore at gmail.com (Adrian Johnson)
Date: Sat, 12 Nov 2016 17:25:51 -0500
Subject: [R] replace items in vector with character based on logical operator
Message-ID: <CAL2fYnPFMmj0ZzL4KM5VmgLLrbrRe2Z-U3uh0e-76BF7HSJtiA@mail.gmail.com>

Hi group,

I have a vector of numeric items and I want to replace based on
logical operator (> or <)  with 'up', 'down' or 'nochange'

The way I am doing works sometimes and does not work sometime. I dont
understand the problem. In this example, that works for condition DN
first, gets over-written by UP and then by NC.


 I appreciate your help.
Thanks
Adrian

In the example below (dput code given below).

qt = quantile(kx)
kx[kx <= qt[[2]]] <- 'DN'
kx[kx >=qt [[4]]] <- 'UP'
kx[kx < qt[[4]] | kx > qt[[2]]] <- 'NC'


> qt
       0%       25%       50%       75%      100%
 9.341531  9.995026 10.186305 10.444237 11.013304

> kx[kx <= qt[[2]]] <- 'DN'
> kx
      TCGA-3H-AB3K       TCGA-3H-AB3L       TCGA-3H-AB3M
TCGA-3H-AB3O       TCGA-3H-AB3S       TCGA-3H-AB3T       TCGA-3H-AB3U
     TCGA-3H-AB3X
"11.0133035117116"               "DN" "10.2976687932597"
"10.080008468093" "10.3661039885332"               "DN"
"10.8971723299346" "10.0327245097586"
      TCGA-3U-A98D       TCGA-3U-A98E       TCGA-3U-A98F
TCGA-3U-A98G       TCGA-3U-A98H       TCGA-3U-A98I       TCGA-3U-A98J
     TCGA-LK-A4NW
"10.0094206497379" "10.4816534666287" "10.4317651665439"
"DN" "10.2313364037012"               "DN" "10.1412738417844"
     "DN"
      TCGA-LK-A4NY       TCGA-LK-A4NZ       TCGA-LK-A4O0       TCGA-LK-A4O2
 "10.603941260297" "10.7959493941044" "10.0501324788234" "10.3877479259697"







kx <- structure(c(11.0133035117116, 9.95184228858716, 10.2976687932597,
10.080008468093, 10.3661039885332, 9.34153148077152, 10.8971723299346,
10.0327245097586, 10.0094206497379, 10.4816534666287, 10.4317651665439,
9.57711797674944, 10.2313364037012, 9.39562974544228, 10.1412738417844,
9.80706011783492, 10.603941260297, 10.7959493941044, 10.0501324788234,
10.3877479259697), .Names = c("TCGA-3H-AB3K", "TCGA-3H-AB3L",
"TCGA-3H-AB3M", "TCGA-3H-AB3O", "TCGA-3H-AB3S", "TCGA-3H-AB3T",
"TCGA-3H-AB3U", "TCGA-3H-AB3X", "TCGA-3U-A98D", "TCGA-3U-A98E",
"TCGA-3U-A98F", "TCGA-3U-A98G", "TCGA-3U-A98H", "TCGA-3U-A98I",
"TCGA-3U-A98J", "TCGA-LK-A4NW", "TCGA-LK-A4NY", "TCGA-LK-A4NZ",
"TCGA-LK-A4O0", "TCGA-LK-A4O2"))


From jdnewmil at dcn.davis.ca.us  Sat Nov 12 23:51:39 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 12 Nov 2016 14:51:39 -0800
Subject: [R] replace items in vector with character based on logical
	operator
In-Reply-To: <CAL2fYnPFMmj0ZzL4KM5VmgLLrbrRe2Z-U3uh0e-76BF7HSJtiA@mail.gmail.com>
References: <CAL2fYnPFMmj0ZzL4KM5VmgLLrbrRe2Z-U3uh0e-76BF7HSJtiA@mail.gmail.com>
Message-ID: <CAB288D2-CDE1-47E9-9BD6-488A1705AE07@dcn.davis.ca.us>

Your goal is fundamentally flawed,  since all elements of a vector must be if the same type. 
Create another vector to hold your character information. 

kc <- rep( 'NC', length( kx ) )
kc[ kx <= qt[[2]] ] <- 'DN'
kc[ kx >=qt [[4]] ] <- 'UP'

-- 
Sent from my phone. Please excuse my brevity.

On November 12, 2016 2:25:51 PM PST, Adrian Johnson <oriolebaltimore at gmail.com> wrote:
>Hi group,
>
>I have a vector of numeric items and I want to replace based on
>logical operator (> or <)  with 'up', 'down' or 'nochange'
>
>The way I am doing works sometimes and does not work sometime. I dont
>understand the problem. In this example, that works for condition DN
>first, gets over-written by UP and then by NC.
>
>
> I appreciate your help.
>Thanks
>Adrian
>
>In the example below (dput code given below).
>
>qt = quantile(kx)
>kx[kx <= qt[[2]]] <- 'DN'
>kx[kx >=qt [[4]]] <- 'UP'
>kx[kx < qt[[4]] | kx > qt[[2]]] <- 'NC'
>
>
>> qt
>       0%       25%       50%       75%      100%
> 9.341531  9.995026 10.186305 10.444237 11.013304
>
>> kx[kx <= qt[[2]]] <- 'DN'
>> kx
>      TCGA-3H-AB3K       TCGA-3H-AB3L       TCGA-3H-AB3M
>TCGA-3H-AB3O       TCGA-3H-AB3S       TCGA-3H-AB3T       TCGA-3H-AB3U
>     TCGA-3H-AB3X
>"11.0133035117116"               "DN" "10.2976687932597"
>"10.080008468093" "10.3661039885332"               "DN"
>"10.8971723299346" "10.0327245097586"
>      TCGA-3U-A98D       TCGA-3U-A98E       TCGA-3U-A98F
>TCGA-3U-A98G       TCGA-3U-A98H       TCGA-3U-A98I       TCGA-3U-A98J
>     TCGA-LK-A4NW
>"10.0094206497379" "10.4816534666287" "10.4317651665439"
>"DN" "10.2313364037012"               "DN" "10.1412738417844"
>     "DN"
>  TCGA-LK-A4NY       TCGA-LK-A4NZ       TCGA-LK-A4O0       TCGA-LK-A4O2
>"10.603941260297" "10.7959493941044" "10.0501324788234"
>"10.3877479259697"
>
>
>
>
>
>
>
>kx <- structure(c(11.0133035117116, 9.95184228858716, 10.2976687932597,
>10.080008468093, 10.3661039885332, 9.34153148077152, 10.8971723299346,
>10.0327245097586, 10.0094206497379, 10.4816534666287, 10.4317651665439,
>9.57711797674944, 10.2313364037012, 9.39562974544228, 10.1412738417844,
>9.80706011783492, 10.603941260297, 10.7959493941044, 10.0501324788234,
>10.3877479259697), .Names = c("TCGA-3H-AB3K", "TCGA-3H-AB3L",
>"TCGA-3H-AB3M", "TCGA-3H-AB3O", "TCGA-3H-AB3S", "TCGA-3H-AB3T",
>"TCGA-3H-AB3U", "TCGA-3H-AB3X", "TCGA-3U-A98D", "TCGA-3U-A98E",
>"TCGA-3U-A98F", "TCGA-3U-A98G", "TCGA-3U-A98H", "TCGA-3U-A98I",
>"TCGA-3U-A98J", "TCGA-LK-A4NW", "TCGA-LK-A4NY", "TCGA-LK-A4NZ",
>"TCGA-LK-A4O0", "TCGA-LK-A4O2"))
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Sun Nov 13 00:29:20 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Sat, 12 Nov 2016 15:29:20 -0800
Subject: [R] replace items in vector with character based on logical
	operator
In-Reply-To: <CAL2fYnPFMmj0ZzL4KM5VmgLLrbrRe2Z-U3uh0e-76BF7HSJtiA@mail.gmail.com>
References: <CAL2fYnPFMmj0ZzL4KM5VmgLLrbrRe2Z-U3uh0e-76BF7HSJtiA@mail.gmail.com>
Message-ID: <CAF8bMcbNpX7aY=PfjGuXPCwaPq0nr1xJQ+6Mnrv=V2rZPWYdUw@mail.gmail.com>

Your first replacement in kx changed it to a character vector and
comparisons
of character strings given different results than comparisons of numbers

> kx <- 1:10
> qt <- quantile(kx)
> kx[kx <= qt[2]] <- "DN"
> kx
 [1] "DN" "DN" "DN" "4"  "5"  "6"  "7"  "8"  "9"  "10"
> "10" < "6"
[1] TRUE
>
> # compare to
> kx <- 1:10
> code <- character(length(kx))
> qt <- quantile(kx)
> code[kx <= qt[2]] <- "DN"
> code
 [1] "DN" "DN" "DN" ""   ""   ""   ""   ""   ""   ""
> code[kx >= qt[4]] <- "DN"
> code
 [1] "DN" "DN" "DN" ""   ""   ""   ""   "DN" "DN" "DN"


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Sat, Nov 12, 2016 at 2:25 PM, Adrian Johnson <oriolebaltimore at gmail.com>
wrote:

> Hi group,
>
> I have a vector of numeric items and I want to replace based on
> logical operator (> or <)  with 'up', 'down' or 'nochange'
>
> The way I am doing works sometimes and does not work sometime. I dont
> understand the problem. In this example, that works for condition DN
> first, gets over-written by UP and then by NC.
>
>
>  I appreciate your help.
> Thanks
> Adrian
>
> In the example below (dput code given below).
>
> qt = quantile(kx)
> kx[kx <= qt[[2]]] <- 'DN'
> kx[kx >=qt [[4]]] <- 'UP'
> kx[kx < qt[[4]] | kx > qt[[2]]] <- 'NC'
>
>
> > qt
>        0%       25%       50%       75%      100%
>  9.341531  9.995026 10.186305 10.444237 11.013304
>
> > kx[kx <= qt[[2]]] <- 'DN'
> > kx
>       TCGA-3H-AB3K       TCGA-3H-AB3L       TCGA-3H-AB3M
> TCGA-3H-AB3O       TCGA-3H-AB3S       TCGA-3H-AB3T       TCGA-3H-AB3U
>      TCGA-3H-AB3X
> "11.0133035117116"               "DN" "10.2976687932597"
> "10.080008468093" "10.3661039885332"               "DN"
> "10.8971723299346" "10.0327245097586"
>       TCGA-3U-A98D       TCGA-3U-A98E       TCGA-3U-A98F
> TCGA-3U-A98G       TCGA-3U-A98H       TCGA-3U-A98I       TCGA-3U-A98J
>      TCGA-LK-A4NW
> "10.0094206497379" "10.4816534666287" "10.4317651665439"
> "DN" "10.2313364037012"               "DN" "10.1412738417844"
>      "DN"
>       TCGA-LK-A4NY       TCGA-LK-A4NZ       TCGA-LK-A4O0       TCGA-LK-A4O2
>  "10.603941260297" "10.7959493941044" "10.0501324788234" "10.3877479259697"
>
>
>
>
>
>
>
> kx <- structure(c(11.0133035117116, 9.95184228858716, 10.2976687932597,
> 10.080008468093, 10.3661039885332, 9.34153148077152, 10.8971723299346,
> 10.0327245097586, 10.0094206497379, 10.4816534666287, 10.4317651665439,
> 9.57711797674944, 10.2313364037012, 9.39562974544228, 10.1412738417844,
> 9.80706011783492, 10.603941260297, 10.7959493941044, 10.0501324788234,
> 10.3877479259697), .Names = c("TCGA-3H-AB3K", "TCGA-3H-AB3L",
> "TCGA-3H-AB3M", "TCGA-3H-AB3O", "TCGA-3H-AB3S", "TCGA-3H-AB3T",
> "TCGA-3H-AB3U", "TCGA-3H-AB3X", "TCGA-3U-A98D", "TCGA-3U-A98E",
> "TCGA-3U-A98F", "TCGA-3U-A98G", "TCGA-3U-A98H", "TCGA-3U-A98I",
> "TCGA-3U-A98J", "TCGA-LK-A4NW", "TCGA-LK-A4NY", "TCGA-LK-A4NZ",
> "TCGA-LK-A4O0", "TCGA-LK-A4O2"))
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From mak.hholly at gmail.com  Sun Nov 13 01:33:26 2016
From: mak.hholly at gmail.com (greg holly)
Date: Sat, 12 Nov 2016 19:33:26 -0500
Subject: [R] Error in Summary.factor(c(24L, 28L, 29L, 30L, 31L, 32L, 33L, 34L,
 35L, :
Message-ID: <CAM9Qe4hw5aYtTMeP+7M-mZ+XvMBb4DXmXj7rMc0jj43FsBr+uA@mail.gmail.com>

Dear all;

I am getting the following error message when I run maSigPro package by
using

make.design.matrix commend. I could not figure out why this error happens.
Thanks for your help

Greg

Error in Summary.factor(c(24L, 28L, 29L, 30L, 31L, 32L, 33L, 34L, 35L,  :
  ?min? not meaningful for factors

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Sun Nov 13 01:54:53 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 12 Nov 2016 19:54:53 -0500
Subject: [R] Error in Summary.factor(c(24L, 28L, 29L, 30L, 31L, 32L, 33L,
 34L, 35L, :
In-Reply-To: <CAM9Qe4hw5aYtTMeP+7M-mZ+XvMBb4DXmXj7rMc0jj43FsBr+uA@mail.gmail.com>
References: <CAM9Qe4hw5aYtTMeP+7M-mZ+XvMBb4DXmXj7rMc0jj43FsBr+uA@mail.gmail.com>
Message-ID: <a28ff201-6dfd-b035-55f3-150dcd1d83c9@gmail.com>

On 12/11/2016 7:33 PM, greg holly wrote:
> Dear all;
>
> I am getting the following error message when I run maSigPro package by
> using
>
> make.design.matrix commend. I could not figure out why this error happens.
> Thanks for your help
>
> Greg
>
> Error in Summary.factor(c(24L, 28L, 29L, 30L, 31L, 32L, 33L, 34L, 35L,  :
>   ?min? not meaningful for factors
>

You'll need to give a minimal reproducible example (i.e. something 
others can run, but not containing a lot of unnecessary stuff) if you 
want help with this.

Duncan Murdoch


From mak.hholly at gmail.com  Sun Nov 13 02:14:55 2016
From: mak.hholly at gmail.com (greg holly)
Date: Sat, 12 Nov 2016 20:14:55 -0500
Subject: [R] Error in Summary.factor(c(24L, 28L, 29L, 30L, 31L, 32L, 33L,
 34L, 35L, :
In-Reply-To: <a28ff201-6dfd-b035-55f3-150dcd1d83c9@gmail.com>
References: <CAM9Qe4hw5aYtTMeP+7M-mZ+XvMBb4DXmXj7rMc0jj43FsBr+uA@mail.gmail.com>
	<a28ff201-6dfd-b035-55f3-150dcd1d83c9@gmail.com>
Message-ID: <CAM9Qe4gSP--dnWU7aKnKhcyPHV-cJP6Ue26_85WhurkjysCpQQ@mail.gmail.com>

Hi Duncan;


Thanks for this. I used followed command. In command line: blood is the
name of the data file, degree=11 because I have 12-time point. I need to
create the design matrix of dummies (named des_blood) for fitting time
series micorarray gene expression experiments using maSigPro program.


des_blood=make.design.matrix(blood, degree=11)

On Sat, Nov 12, 2016 at 7:54 PM, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 12/11/2016 7:33 PM, greg holly wrote:
>
>> Dear all;
>>
>> I am getting the following error message when I run maSigPro package by
>> using
>>
>> make.design.matrix commend. I could not figure out why this error happens.
>> Thanks for your help
>>
>> Greg
>>
>> Error in Summary.factor(c(24L, 28L, 29L, 30L, 31L, 32L, 33L, 34L, 35L,  :
>>   ?min? not meaningful for factors
>>
>>
> You'll need to give a minimal reproducible example (i.e. something others
> can run, but not containing a lot of unnecessary stuff) if you want help
> with this.
>
> Duncan Murdoch
>
>

	[[alternative HTML version deleted]]


From miaojpm at gmail.com  Sun Nov 13 02:36:37 2016
From: miaojpm at gmail.com (John)
Date: Sat, 12 Nov 2016 17:36:37 -0800
Subject: [R] value matching %in% for a number pair
Message-ID: <CABcx46AtrdcQj6wrVnS4HB8jCYDbV=e6jrTn1kNFB4P2zVUb2Q@mail.gmail.com>

Hi,

   We can match one numerical value as follows
> 3 %in% c(4,5)
[1] FALSE
> 3 %in% c(4,5,3)
[1] TRUE

   To see whether value pairs are identical,
> identical(c(3,4), c(3,5))
[1] FALSE
> identical(c(3,4), c(3,4))
[1] TRUE

   Is there any way to test whether ?A value pair is in a set of value
pairs?? For example, can we test whether the pair (2,3) is identical to one
of the pairs in the set S={(1,2), (4,3), (3,3), (2,3), (4,5)}?
   In this case, the answer is yes because the 4th element of S is (2,3).
Is there any simple way to code it? Thanks!

John

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Sun Nov 13 03:01:27 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 12 Nov 2016 21:01:27 -0500
Subject: [R] value matching %in% for a number pair
In-Reply-To: <CABcx46AtrdcQj6wrVnS4HB8jCYDbV=e6jrTn1kNFB4P2zVUb2Q@mail.gmail.com>
References: <CABcx46AtrdcQj6wrVnS4HB8jCYDbV=e6jrTn1kNFB4P2zVUb2Q@mail.gmail.com>
Message-ID: <27bf059e-12db-b5c6-2f7b-5aae898a3239@gmail.com>

On 12/11/2016 8:36 PM, John wrote:
> Hi,
>
>    We can match one numerical value as follows
>> 3 %in% c(4,5)
> [1] FALSE
>> 3 %in% c(4,5,3)
> [1] TRUE
>
>    To see whether value pairs are identical,
>> identical(c(3,4), c(3,5))
> [1] FALSE
>> identical(c(3,4), c(3,4))
> [1] TRUE
>
>    Is there any way to test whether ?A value pair is in a set of value
> pairs?? For example, can we test whether the pair (2,3) is identical to one
> of the pairs in the set S={(1,2), (4,3), (3,3), (2,3), (4,5)}?
>    In this case, the answer is yes because the 4th element of S is (2,3).
> Is there any simple way to code it? Thanks!

You'll have to type a long expression or write your own function for it. 
  Here's one way, if you store your set as a list:

inlist <- function(x, thelist)
   any(sapply(thelist, identical, x))

For example:

 > S <- list(c(1, 2), c(4, 3))
 > inlist(c(4, 3), S)
[1] TRUE
 > inlist(c(3, 4), S)
[1] FALSE

Duncan Murdoch


From murdoch.duncan at gmail.com  Sun Nov 13 03:02:30 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 12 Nov 2016 21:02:30 -0500
Subject: [R] Error in Summary.factor(c(24L, 28L, 29L, 30L, 31L, 32L, 33L,
 34L, 35L, :
In-Reply-To: <CAM9Qe4gSP--dnWU7aKnKhcyPHV-cJP6Ue26_85WhurkjysCpQQ@mail.gmail.com>
References: <CAM9Qe4hw5aYtTMeP+7M-mZ+XvMBb4DXmXj7rMc0jj43FsBr+uA@mail.gmail.com>
	<a28ff201-6dfd-b035-55f3-150dcd1d83c9@gmail.com>
	<CAM9Qe4gSP--dnWU7aKnKhcyPHV-cJP6Ue26_85WhurkjysCpQQ@mail.gmail.com>
Message-ID: <8f953b2c-8720-ee7f-2952-e86d4f1f94da@gmail.com>

On 12/11/2016 8:14 PM, greg holly wrote:
> Hi Duncan;
>
>
> Thanks for this. I used followed command. In command line: blood is the
> name of the data file, degree=11 because I have 12-time point. I need to
> create the design matrix of dummies (named des_blood) for fitting time
> series micorarray gene expression experiments using maSigPro program.

Who but you can run this?  We don't have "blood".

Duncan Murdoch

>
> des_blood=make.design.matrix(blood, degree=11)
>
>
> On Sat, Nov 12, 2016 at 7:54 PM, Duncan Murdoch
> <murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>> wrote:
>
>     On 12/11/2016 7:33 PM, greg holly wrote:
>
>         Dear all;
>
>         I am getting the following error message when I run maSigPro
>         package by
>         using
>
>         make.design.matrix commend. I could not figure out why this
>         error happens.
>         Thanks for your help
>
>         Greg
>
>         Error in Summary.factor(c(24L, 28L, 29L, 30L, 31L, 32L, 33L,
>         34L, 35L,  :
>           ?min? not meaningful for factors
>
>
>     You'll need to give a minimal reproducible example (i.e. something
>     others can run, but not containing a lot of unnecessary stuff) if
>     you want help with this.
>
>     Duncan Murdoch
>
>


From jdnewmil at dcn.davis.ca.us  Sun Nov 13 03:11:13 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 12 Nov 2016 18:11:13 -0800
Subject: [R] value matching %in% for a number pair
In-Reply-To: <CABcx46AtrdcQj6wrVnS4HB8jCYDbV=e6jrTn1kNFB4P2zVUb2Q@mail.gmail.com>
References: <CABcx46AtrdcQj6wrVnS4HB8jCYDbV=e6jrTn1kNFB4P2zVUb2Q@mail.gmail.com>
Message-ID: <8872E19C-A4F3-4287-A57F-B68FCCF0510E@dcn.davis.ca.us>

This really depends on the way you represent these pairs. Here is some food for thought:

p <- matrix( c( 1,2, 4,3, 3,3, 2,3, 4,5 ), byrow=TRUE, ncol=2 )
( 2 %in% p[,1] ) & ( 3 %in% p[,2] ) # (2,3)
( c( 2, 1, 5 ) %in% p[,1] ) & ( c( 3, 2, 5 ) %in% p[,2] ) # (2,3) (1,2) (5,5)

-- 
Sent from my phone. Please excuse my brevity.

On November 12, 2016 5:36:37 PM PST, John <miaojpm at gmail.com> wrote:
>Hi,
>
>   We can match one numerical value as follows
>> 3 %in% c(4,5)
>[1] FALSE
>> 3 %in% c(4,5,3)
>[1] TRUE
>
>   To see whether value pairs are identical,
>> identical(c(3,4), c(3,5))
>[1] FALSE
>> identical(c(3,4), c(3,4))
>[1] TRUE
>
>   Is there any way to test whether ?A value pair is in a set of value
>pairs?? For example, can we test whether the pair (2,3) is identical to
>one
>of the pairs in the set S={(1,2), (4,3), (3,3), (2,3), (4,5)}?
> In this case, the answer is yes because the 4th element of S is (2,3).
>Is there any simple way to code it? Thanks!
>
>John
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Sun Nov 13 03:19:47 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Sat, 12 Nov 2016 18:19:47 -0800
Subject: [R] value matching %in% for a number pair
In-Reply-To: <CABcx46AtrdcQj6wrVnS4HB8jCYDbV=e6jrTn1kNFB4P2zVUb2Q@mail.gmail.com>
References: <CABcx46AtrdcQj6wrVnS4HB8jCYDbV=e6jrTn1kNFB4P2zVUb2Q@mail.gmail.com>
Message-ID: <CAF8bMcZUjN-7WQ-Mf8CGd8DgDr+sMh1xGWPiNFHfUVFZjJpBZQ@mail.gmail.com>

> S <- list(c(1,2), c(4,3), c(3,3), c(2,3), c(4,5))
> list(c(1,2), c(3,4), c(2,3)) %in% S # is in S?
[1]  TRUE FALSE  TRUE
> match(list(c(1,2), c(3,4), c(2,3)), S) # which element of S?
[1]  1 NA  4


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Sat, Nov 12, 2016 at 5:36 PM, John <miaojpm at gmail.com> wrote:

> Hi,
>
>    We can match one numerical value as follows
> > 3 %in% c(4,5)
> [1] FALSE
> > 3 %in% c(4,5,3)
> [1] TRUE
>
>    To see whether value pairs are identical,
> > identical(c(3,4), c(3,5))
> [1] FALSE
> > identical(c(3,4), c(3,4))
> [1] TRUE
>
>    Is there any way to test whether ?A value pair is in a set of value
> pairs?? For example, can we test whether the pair (2,3) is identical to one
> of the pairs in the set S={(1,2), (4,3), (3,3), (2,3), (4,5)}?
>    In this case, the answer is yes because the 4th element of S is (2,3).
> Is there any simple way to code it? Thanks!
>
> John
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Sun Nov 13 03:32:57 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 12 Nov 2016 18:32:57 -0800
Subject: [R] value matching %in% for a number pair
In-Reply-To: <8872E19C-A4F3-4287-A57F-B68FCCF0510E@dcn.davis.ca.us>
References: <CABcx46AtrdcQj6wrVnS4HB8jCYDbV=e6jrTn1kNFB4P2zVUb2Q@mail.gmail.com>
	<8872E19C-A4F3-4287-A57F-B68FCCF0510E@dcn.davis.ca.us>
Message-ID: <09F4F79B-4BD9-4367-B5B4-C750608470E4@dcn.davis.ca.us>

Sorry, that was a fail. Better to think about:

any( 2 == p[,1] & 3 == p[,2] )
v <- matrix( c( 2,3, 1,2, 5,5 ), byrow=TRUE, ncol=2 )
apply( v, 1, function(x) { any( x[1]==p[,1] & x[2]==p[,2] ) } )

-- 
Sent from my phone. Please excuse my brevity.

On November 12, 2016 6:11:13 PM PST, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>This really depends on the way you represent these pairs. Here is some
>food for thought:
>
>p <- matrix( c( 1,2, 4,3, 3,3, 2,3, 4,5 ), byrow=TRUE, ncol=2 )
>( 2 %in% p[,1] ) & ( 3 %in% p[,2] ) # (2,3)
>( c( 2, 1, 5 ) %in% p[,1] ) & ( c( 3, 2, 5 ) %in% p[,2] ) # (2,3) (1,2)
>(5,5)
>
>-- 
>Sent from my phone. Please excuse my brevity.
>
>On November 12, 2016 5:36:37 PM PST, John <miaojpm at gmail.com> wrote:
>>Hi,
>>
>>   We can match one numerical value as follows
>>> 3 %in% c(4,5)
>>[1] FALSE
>>> 3 %in% c(4,5,3)
>>[1] TRUE
>>
>>   To see whether value pairs are identical,
>>> identical(c(3,4), c(3,5))
>>[1] FALSE
>>> identical(c(3,4), c(3,4))
>>[1] TRUE
>>
>>   Is there any way to test whether ?A value pair is in a set of value
>>pairs?? For example, can we test whether the pair (2,3) is identical
>to
>>one
>>of the pairs in the set S={(1,2), (4,3), (3,3), (2,3), (4,5)}?
>> In this case, the answer is yes because the 4th element of S is
>(2,3).
>>Is there any simple way to code it? Thanks!
>>
>>John
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Sun Nov 13 06:06:48 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sun, 13 Nov 2016 16:06:48 +1100
Subject: [R] value matching %in% for a number pair
In-Reply-To: <09F4F79B-4BD9-4367-B5B4-C750608470E4@dcn.davis.ca.us>
References: <CABcx46AtrdcQj6wrVnS4HB8jCYDbV=e6jrTn1kNFB4P2zVUb2Q@mail.gmail.com>
	<8872E19C-A4F3-4287-A57F-B68FCCF0510E@dcn.davis.ca.us>
	<09F4F79B-4BD9-4367-B5B4-C750608470E4@dcn.davis.ca.us>
Message-ID: <CA+8X3fW4iEYT=nxSDci_i9xc3SUX4M_hVibFgzycmQbQBS1U_g@mail.gmail.com>

Hi john,
I don't know whether this breaks any rules, but:

target_pair<-c(3,4)
pair_list<-list(c(1,2),c(3,4),c(5,6))
sapply(pair_list,identical,target_pair)
[1] FALSE  TRUE FALSE

Jim


On Sun, Nov 13, 2016 at 1:32 PM, Jeff Newmiller
<jdnewmil at dcn.davis.ca.us> wrote:
> Sorry, that was a fail. Better to think about:
>
> any( 2 == p[,1] & 3 == p[,2] )
> v <- matrix( c( 2,3, 1,2, 5,5 ), byrow=TRUE, ncol=2 )
> apply( v, 1, function(x) { any( x[1]==p[,1] & x[2]==p[,2] ) } )
>
> --
> Sent from my phone. Please excuse my brevity.
>
> On November 12, 2016 6:11:13 PM PST, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>This really depends on the way you represent these pairs. Here is some
>>food for thought:
>>
>>p <- matrix( c( 1,2, 4,3, 3,3, 2,3, 4,5 ), byrow=TRUE, ncol=2 )
>>( 2 %in% p[,1] ) & ( 3 %in% p[,2] ) # (2,3)
>>( c( 2, 1, 5 ) %in% p[,1] ) & ( c( 3, 2, 5 ) %in% p[,2] ) # (2,3) (1,2)
>>(5,5)
>>
>>--
>>Sent from my phone. Please excuse my brevity.
>>
>>On November 12, 2016 5:36:37 PM PST, John <miaojpm at gmail.com> wrote:
>>>Hi,
>>>
>>>   We can match one numerical value as follows
>>>> 3 %in% c(4,5)
>>>[1] FALSE
>>>> 3 %in% c(4,5,3)
>>>[1] TRUE
>>>
>>>   To see whether value pairs are identical,
>>>> identical(c(3,4), c(3,5))
>>>[1] FALSE
>>>> identical(c(3,4), c(3,4))
>>>[1] TRUE
>>>
>>>   Is there any way to test whether ?A value pair is in a set of value
>>>pairs?? For example, can we test whether the pair (2,3) is identical
>>to
>>>one
>>>of the pairs in the set S={(1,2), (4,3), (3,3), (2,3), (4,5)}?
>>> In this case, the answer is yes because the 4th element of S is
>>(2,3).
>>>Is there any simple way to code it? Thanks!
>>>
>>>John
>>>
>>>      [[alternative HTML version deleted]]
>>>
>>>______________________________________________
>>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide
>>>http://www.R-project.org/posting-guide.html
>>>and provide commented, minimal, self-contained, reproducible code.
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From james.hirschorn at hotmail.com  Sun Nov 13 07:14:49 2016
From: james.hirschorn at hotmail.com (James Hirschorn)
Date: Sun, 13 Nov 2016 06:14:49 +0000
Subject: [R] Is this foreach behaviour correct?
In-Reply-To: <CAF8bMcZ-RA6d43qj0Sdv-G2x6T5C_+sPqXoMv-wGW=2MKWinQQ@mail.gmail.com>
References: <CY1PR11MB0444BE97C0B5173ADB45A892E9A40@CY1PR11MB0444.namprd11.prod.outlook.com>
	<CA+8X3fXJxE=WEYQhs+Jw=nXvoS1hEnCDHivUG=Djz9fuyQN7Cw@mail.gmail.com>
	<0a16dda9-d58a-d6ff-31c6-1e6b20a637f3@gmail.com>
	<CAF8bMcZ-RA6d43qj0Sdv-G2x6T5C_+sPqXoMv-wGW=2MKWinQQ@mail.gmail.com>
Message-ID: <CY1PR11MB04447CDE68EE8B392CDD2983E9BD0@CY1PR11MB0444.namprd11.prod.outlook.com>

I'm still not clear about whether this is a bug in foreach. Should c.Date be invoked by foreach with .combine='c'?

On 11/06/2016 07:02 PM, William Dunlap wrote:
Note that in the OP's example c.Date is never invoked.  c.Date is called if .combine
calls c rather than if .combine is c:

> library(zoo)
> trace(c.Date, quote(print(sys.call())))
Tracing function "c.Date" in package "base"
[1] "c.Date"
> foreach(i=10000:10003, .combine=c) %do% { as.Date(i) }
[1] 10000 10001 10002 10003
> foreach(i=10000:10003, .combine=function(...)c(...)) %do% { as.Date(i) }
Tracing c.Date(...) on entry
eval(expr, envir, enclos)
Tracing c.Date(...) on entry
eval(expr, envir, enclos)
Tracing c.Date(...) on entry
eval(expr, envir, enclos)
[1] "1997-05-19" "1997-05-20" "1997-05-21" "1997-05-22"


Bill Dunlap
TIBCO Software
wdunlap tibco.com<http://tibco.com>

On Sun, Nov 6, 2016 at 2:20 PM, Duncan Murdoch <murdoch.duncan at gmail.com<mailto:murdoch.duncan at gmail.com>> wrote:
On 06/11/2016 5:02 PM, Jim Lemon wrote:
hi James,
I think you have to have a starting date ("origin") for as.Date to
convert numbers to dates.

That's true with the function in the base package, but the zoo package also has an as.Date() function, which defaults the origin to "1970-01-01".  If James is using zoo his code would be okay.  If he's not, he would have got an error, so I think he must have been.

Duncan Murdoch



Jim

On Sun, Nov 6, 2016 at 12:10 PM, James Hirschorn
<james.hirschorn at hotmail.com<mailto:james.hirschorn at hotmail.com>> wrote:
This seemed odd so I wanted to check:

 > x <- foreach(i=10000:10100, .combine='c') %do% { as.Date(i) }

yields a numeric vector for x:

 > class(x)
[1] "numeric"

Should it not be a vector of Date?

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



	[[alternative HTML version deleted]]


From ddebarr at uw.edu  Sun Nov 13 06:50:42 2016
From: ddebarr at uw.edu (Dave DeBarr)
Date: Sat, 12 Nov 2016 21:50:42 -0800
Subject: [R] Question about expression parser for "return" statement
Message-ID: <CAOogVzUOtMUXnvjFg37aGhDqtT9NmULGFiez4_jv1vGYZLDw0g@mail.gmail.com>

I've noticed that if I don't include parentheses around the intended return
value for the "return" statement, R will assume the first parenthetical
expression is the intended return value ... even if that parenthetical
expression is only part of a larger expression.

Is this intentional?

I'm guessing it is intentional; but since there is no warning about
ignoring the rest of the expression, it could lead to hard-to-find bugs.

Thanks,
Dave

Here's an example ...

dnorm(2, 0, 1)
normalDensityFunction = function(x, Mean, Variance) {
    # no parentheses surrounding the entire "return" value
    return (1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance)
}
normalDensityFunction(2, 0, 1)    # incorrect answer
normalDensityFunction = function(x, Mean, Variance) {
    # parentheses surrounding the entire "return" value
    return ((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance))
}
normalDensityFunction(2, 0, 1)    # correct answer

	[[alternative HTML version deleted]]


From klebyn at yahoo.com.br  Sun Nov 13 02:05:25 2016
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Sat, 12 Nov 2016 23:05:25 -0200
Subject: [R] tcl('tk::fontchooser', 'show') : How to use? R-devel Tcl 8.6
Message-ID: <8ae97587-e1a9-0dda-ec80-57aa14e7e46c@yahoo.com.br>

Hello,
Somebody would indicate the correct way to use the option the
fontchooser widget into tcltk package (in R-devel, Tcl 8.6) ?
I tried to use as other traditional widgets but no success. (code below)
Thanks
Cleber

 > tt <- tktoplevel(); but <- ttkbutton( tt, text='Test'); tcl('pack', but )
<Tcl>
 > tcl( but, 'configure', '-text' )
<Tcl> -text text Text {} Test
 > tcl( but, 'configure', text=NULL )
<Tcl> -text text Text {} Test
 > tcl( but, 'cget', '-text')
<Tcl> Test
 >
 >
 > tcl('tk::fontchooser', 'show')
<Tcl>
 > tcl( 'tk::fontchooser', 'configure', '-font')
<Tcl>
 >
 >
 > sessionInfo()
R Under development (unstable) (2016-11-01 r71616)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7600)

locale:
[1] LC_COLLATE=Portuguese_Brazil.1252 LC_CTYPE=Portuguese_Brazil.1252
LC_MONETARY=Portuguese_Brazil.1252
[4] LC_NUMERIC=C LC_TIME=Portuguese_Brazil.1252

attached base packages:
[1] tcltk     stats     graphics  grDevices utils     datasets methods
base

loaded via a namespace (and not attached):
[1] tools_3.4.0
 >




---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus

	[[alternative HTML version deleted]]


From sidoti.23 at buckeyemail.osu.edu  Sun Nov 13 06:46:13 2016
From: sidoti.23 at buckeyemail.osu.edu (Sidoti, Salvatore A.)
Date: Sun, 13 Nov 2016 05:46:13 +0000
Subject: [R] Principle Component Analysis: Ranking Animal Size Based On
 Combined Metrics
Message-ID: <BLUPR0101MB147499AE1D09AA1DD4EB4A54ABBD0@BLUPR0101MB1474.prod.exchangelabs.com>

Let's say I perform 4 measurements on an animal: three are linear measurements in millimeters and the fourth is its weight in milligrams. So, we have a data set with mixed units.

Based on these four correlated measurements, I would like to obtain one "score" or value that describes an individual animal's size. I considered simply taking the geometric mean of these 4 measurements, and that would give me a "score" - larger values would be for larger animals, etc.

However, this assumes that all 4 of these measurements contribute equally to an animal's size. Of course, more than likely this is not the case. I then performed a PCA to discover how much influence each variable had on the overall data set. I was hoping to use this analysis to refine my original approach.

I honestly do not know how to apply the information from the PCA to this particular problem...

I do know, however, that principle components 1 and 2 capture enough of the variation to reduce the number of dimensions down to 2 (see analysis below with the original data set).

Note: animal weights were ln() transformed to increase correlation with the 3 other variables.

df <- data.frame(
  weight = log(1000*c(0.0980, 0.0622, 0.0600, 0.1098, 0.0538, 0.0701, 0.1138, 0.0540, 0.0629, 0.0930,
             0.0443, 0.1115, 0.1157, 0.0734, 0.0616, 0.0640, 0.0480, 0.1339, 0.0547, 0.0844,
             0.0431, 0.0472, 0.0752, 0.0604, 0.0713, 0.0658, 0.0538, 0.0585, 0.0645, 0.0529,
             0.0448, 0.0574, 0.0577, 0.0514, 0.0758, 0.0424, 0.0997, 0.0758, 0.0649, 0.0465,
             0.0748, 0.0540, 0.0819, 0.0732, 0.0725, 0.0730, 0.0777, 0.0630, 0.0466)),
  interoc = c(0.853, 0.865, 0.811, 0.840, 0.783, 0.868, 0.818, 0.847, 0.838, 0.799,
              0.737, 0.788, 0.731, 0.777, 0.863, 0.877, 0.814, 0.926, 0.767, 0.746,
              0.700, 0.768, 0.807, 0.753, 0.809, 0.788, 0.750, 0.815, 0.757, 0.737,
              0.759, 0.863, 0.747, 0.838, 0.790, 0.676, 0.857, 0.728, 0.743, 0.870,
              0.787, 0.773, 0.829, 0.785, 0.746, 0.834, 0.829, 0.750, 0.842),
  cwidth = c(3.152, 3.046, 3.139, 3.181, 3.023, 3.452, 2.803, 3.050, 3.160, 3.186,
             2.801, 2.862, 3.183, 2.770, 3.207, 3.188, 2.969, 3.033, 2.972, 3.291,
             2.772, 2.875, 2.978, 3.094, 2.956, 2.966, 2.896, 3.149, 2.813, 2.935,
             2.839, 3.152, 2.984, 3.037, 2.888, 2.723, 3.342, 2.562, 2.827, 2.909,
             3.093, 2.990, 3.097, 2.751, 2.877, 2.901, 2.895, 2.721, 2.942),
  clength = c(3.889, 3.733, 3.762, 4.059, 3.911, 3.822, 3.768, 3.814, 3.721, 3.794,
              3.483, 3.863, 3.856, 3.457, 3.996, 3.876, 3.642, 3.978, 3.534, 3.967,
              3.429, 3.518, 3.766, 3.755, 3.706, 3.785, 3.607, 3.922, 3.453, 3.589,
              3.508, 3.861, 3.706, 3.593, 3.570, 3.341, 3.916, 3.336, 3.504, 3.688,
              3.735, 3.724, 3.860, 3.405, 3.493, 3.586, 3.545, 3.443, 3.640))

pca_morpho <- princomp(df, cor = TRUE)

summary(pca_morpho)

Importance of components:
                         		Comp.1    	Comp.2    	Comp.3    	Comp.4
Standard deviation     	1.604107 	0.8827323 	0.7061206 	0.3860275
Proportion of Variance 	0.643290 	0.1948041 	0.1246516 	0.0372543
Cumulative Proportion  	0.643290 	0.8380941 	0.9627457 	1.0000000

Loadings:
        		Comp.1 	Comp.2	Comp.3 	Comp.4
weight  	-0.371  	0.907        			-0.201
interoc 	-0.486 	-0.227 	-0.840       
cwidth  	-0.537 	-0.349  	0.466 		-0.611
clength 	-0.582         			0.278  	0.761

               		Comp.1	Comp.2	Comp.3	Comp.4
SS loadings      	1.00   		1.00   		1.00   		1.00
Proportion Var   	0.25   		0.25   		0.25 		0.25
Cumulative Var   	0.25   		0.50   		0.75   		1.00

Any guidance will be greatly appreciated!

Salvatore A. Sidoti
PhD Student
The Ohio State University
Behavioral Ecology


From murdoch.duncan at gmail.com  Sun Nov 13 12:47:10 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 13 Nov 2016 06:47:10 -0500
Subject: [R] Question about expression parser for "return" statement
In-Reply-To: <CAOogVzUOtMUXnvjFg37aGhDqtT9NmULGFiez4_jv1vGYZLDw0g@mail.gmail.com>
References: <CAOogVzUOtMUXnvjFg37aGhDqtT9NmULGFiez4_jv1vGYZLDw0g@mail.gmail.com>
Message-ID: <52831814-c240-04f8-3069-b670552a3941@gmail.com>

On 13/11/2016 12:50 AM, Dave DeBarr wrote:
> I've noticed that if I don't include parentheses around the intended return
> value for the "return" statement, R will assume the first parenthetical
> expression is the intended return value ... even if that parenthetical
> expression is only part of a larger expression.
>
> Is this intentional?

Yes, return is just a function call that has side effects.  As far as 
the parser is concerned,

return ((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance))

is basically the same as

f((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance))

Duncan Murdoch

>
> I'm guessing it is intentional; but since there is no warning about
> ignoring the rest of the expression, it could lead to hard-to-find bugs.
>
> Thanks,
> Dave
>
> Here's an example ...
>
> dnorm(2, 0, 1)
> normalDensityFunction = function(x, Mean, Variance) {
>     # no parentheses surrounding the entire "return" value
>     return (1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance)
> }
> normalDensityFunction(2, 0, 1)    # incorrect answer
> normalDensityFunction = function(x, Mean, Variance) {
>     # parentheses surrounding the entire "return" value
>     return ((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance))
> }
> normalDensityFunction(2, 0, 1)    # correct answer
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From henrik.bengtsson at gmail.com  Sun Nov 13 13:54:13 2016
From: henrik.bengtsson at gmail.com (Henrik Bengtsson)
Date: Sun, 13 Nov 2016 13:54:13 +0100
Subject: [R] Is this foreach behaviour correct?
In-Reply-To: <CY1PR11MB04447CDE68EE8B392CDD2983E9BD0@CY1PR11MB0444.namprd11.prod.outlook.com>
References: <CY1PR11MB0444BE97C0B5173ADB45A892E9A40@CY1PR11MB0444.namprd11.prod.outlook.com>
	<CA+8X3fXJxE=WEYQhs+Jw=nXvoS1hEnCDHivUG=Djz9fuyQN7Cw@mail.gmail.com>
	<0a16dda9-d58a-d6ff-31c6-1e6b20a637f3@gmail.com>
	<CAF8bMcZ-RA6d43qj0Sdv-G2x6T5C_+sPqXoMv-wGW=2MKWinQQ@mail.gmail.com>
	<CY1PR11MB04447CDE68EE8B392CDD2983E9BD0@CY1PR11MB0444.namprd11.prod.outlook.com>
Message-ID: <CAFDcVCQO8Oed=OJcV5JbcXBn_cyhxmNYG_GqGE+v5v3mb_YHQQ@mail.gmail.com>

It looks like a bug.  I don't think c.Date() is every called, because:

> trace(c.Date, tracer = quote(message("c.Date() called")))
Tracing function "c.Date" in package "base"
[1] "c.Date"

Tracing works:

> c(as.Date(10000L), as.Date(10001L))
Tracing c.Date(as.Date(10000L), as.Date(10001L)) on entry
c.Date() called
[1] "1997-05-19" "1997-05-20"

but c.Date() is not called here:

> x <- foreach(i=10000:10100, .combine = function(...) c(...)) %do% { as.Date(i) }
> str(x)
 num [1:101] 10000 10001 10002 10003 10004 ...


The following hack works:

> library("foreach")
> library("zoo")
> x <- foreach(i=10000:10100, .combine = function(...) c(...)) %do% { as.Date(i) }
> str(x)
 Date[1:101], format: "1997-05-19" "1997-05-20" "1997-05-21" "1997-05-22" ...

Alternatively, one can use append() which works like c() if no other
arguments are specified:

> x <- foreach(i=10000:10100, .combine = append) %do% { as.Date(i) }
> str(x)
 Date[1:101], format: "1997-05-19" "1997-05-20" "1997-05-21" "1997-05-22" ...

It looks like foreach is treating the .combine = c case specially and
someone fail to properly dispatch c() on the object (or something).

/Henrik


On Sun, Nov 13, 2016 at 7:14 AM, James Hirschorn
<james.hirschorn at hotmail.com> wrote:
> I'm still not clear about whether this is a bug in foreach. Should c.Date be invoked by foreach with .combine='c'?
>
> On 11/06/2016 07:02 PM, William Dunlap wrote:
> Note that in the OP's example c.Date is never invoked.  c.Date is called if .combine
> calls c rather than if .combine is c:
>
>> library(zoo)
>> trace(c.Date, quote(print(sys.call())))
> Tracing function "c.Date" in package "base"
> [1] "c.Date"
>> foreach(i=10000:10003, .combine=c) %do% { as.Date(i) }
> [1] 10000 10001 10002 10003
>> foreach(i=10000:10003, .combine=function(...)c(...)) %do% { as.Date(i) }
> Tracing c.Date(...) on entry
> eval(expr, envir, enclos)
> Tracing c.Date(...) on entry
> eval(expr, envir, enclos)
> Tracing c.Date(...) on entry
> eval(expr, envir, enclos)
> [1] "1997-05-19" "1997-05-20" "1997-05-21" "1997-05-22"
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com<http://tibco.com>
>
> On Sun, Nov 6, 2016 at 2:20 PM, Duncan Murdoch <murdoch.duncan at gmail.com<mailto:murdoch.duncan at gmail.com>> wrote:
> On 06/11/2016 5:02 PM, Jim Lemon wrote:
> hi James,
> I think you have to have a starting date ("origin") for as.Date to
> convert numbers to dates.
>
> That's true with the function in the base package, but the zoo package also has an as.Date() function, which defaults the origin to "1970-01-01".  If James is using zoo his code would be okay.  If he's not, he would have got an error, so I think he must have been.
>
> Duncan Murdoch
>
>
>
> Jim
>
> On Sun, Nov 6, 2016 at 12:10 PM, James Hirschorn
> <james.hirschorn at hotmail.com<mailto:james.hirschorn at hotmail.com>> wrote:
> This seemed odd so I wanted to check:
>
>  > x <- foreach(i=10000:10100, .combine='c') %do% { as.Date(i) }
>
> yields a numeric vector for x:
>
>  > class(x)
> [1] "numeric"
>
> Should it not be a vector of Date?
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Sun Nov 13 13:58:15 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 13 Nov 2016 07:58:15 -0500
Subject: [R] Question about expression parser for "return" statement
In-Reply-To: <52831814-c240-04f8-3069-b670552a3941@gmail.com>
References: <CAOogVzUOtMUXnvjFg37aGhDqtT9NmULGFiez4_jv1vGYZLDw0g@mail.gmail.com>
	<52831814-c240-04f8-3069-b670552a3941@gmail.com>
Message-ID: <665597dc-55fd-9916-6564-38400262a270@gmail.com>

On 13/11/2016 6:47 AM, Duncan Murdoch wrote:
> On 13/11/2016 12:50 AM, Dave DeBarr wrote:
>> I've noticed that if I don't include parentheses around the intended return
>> value for the "return" statement, R will assume the first parenthetical
>> expression is the intended return value ... even if that parenthetical
>> expression is only part of a larger expression.
>>
>> Is this intentional?
>
> Yes, return is just a function call that has side effects.  As far as
> the parser is concerned,
>
> return ((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance))
>
> is basically the same as
>
> f((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance))

By the way, out of curiosity I took a look at the source of CRAN 
packages to see if this actually occurs.  It turns out that "return" is 
used as a variable name often enough to make automatic tests tricky, so 
I don't know the answer to my question.  However, I did turn up a number 
of cases where people have code like this:

     if (name == "") return;

(from the bio.infer package), which never calls return(), so doesn't 
actually do what the author likely intended.

Duncan Murdoch

>
> Duncan Murdoch
>
>>
>> I'm guessing it is intentional; but since there is no warning about
>> ignoring the rest of the expression, it could lead to hard-to-find bugs.
>>
>> Thanks,
>> Dave
>>
>> Here's an example ...
>>
>> dnorm(2, 0, 1)
>> normalDensityFunction = function(x, Mean, Variance) {
>>     # no parentheses surrounding the entire "return" value
>>     return (1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance)
>> }
>> normalDensityFunction(2, 0, 1)    # incorrect answer
>> normalDensityFunction = function(x, Mean, Variance) {
>>     # parentheses surrounding the entire "return" value
>>     return ((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance))
>> }
>> normalDensityFunction(2, 0, 1)    # correct answer
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From henrik.bengtsson at gmail.com  Sun Nov 13 14:04:27 2016
From: henrik.bengtsson at gmail.com (Henrik Bengtsson)
Date: Sun, 13 Nov 2016 14:04:27 +0100
Subject: [R] Is this foreach behaviour correct?
In-Reply-To: <CAFDcVCQO8Oed=OJcV5JbcXBn_cyhxmNYG_GqGE+v5v3mb_YHQQ@mail.gmail.com>
References: <CY1PR11MB0444BE97C0B5173ADB45A892E9A40@CY1PR11MB0444.namprd11.prod.outlook.com>
	<CA+8X3fXJxE=WEYQhs+Jw=nXvoS1hEnCDHivUG=Djz9fuyQN7Cw@mail.gmail.com>
	<0a16dda9-d58a-d6ff-31c6-1e6b20a637f3@gmail.com>
	<CAF8bMcZ-RA6d43qj0Sdv-G2x6T5C_+sPqXoMv-wGW=2MKWinQQ@mail.gmail.com>
	<CY1PR11MB04447CDE68EE8B392CDD2983E9BD0@CY1PR11MB0444.namprd11.prod.outlook.com>
	<CAFDcVCQO8Oed=OJcV5JbcXBn_cyhxmNYG_GqGE+v5v3mb_YHQQ@mail.gmail.com>
Message-ID: <CAFDcVCQqv4mSMWEK8LEPVzyO8qedNxPGHNDm-PKCTuHrfjWT-w@mail.gmail.com>

On Nov 13, 2016 13:54, "Henrik Bengtsson" <henrik.bengtsson at gmail.com>
wrote:
>
> It looks like a bug.  I don't think c.Date() is every called, because:
>
> > trace(c.Date, tracer = quote(message("c.Date() called")))
> Tracing function "c.Date" in package "base"
> [1] "c.Date"
>
> Tracing works:
>
> > c(as.Date(10000L), as.Date(10001L))
> Tracing c.Date(as.Date(10000L), as.Date(10001L)) on entry
> c.Date() called
> [1] "1997-05-19" "1997-05-20"
>
> but c.Date() is not called here:
>
> > x <- foreach(i=10000:10100, .combine = function(...) c(...)) %do% {
as.Date(i) }
> > str(x)
>  num [1:101] 10000 10001 10002 10003 10004 ...

Cut'n'paste error above. It is

x <- foreach(i=10000:10100, .combine = "c") %do% { as.Date(i) }

that doesn't call c.Date(). Same if you try with .combine = c.

>
>
> The following hack works:
>
> > library("foreach")
> > library("zoo")
> > x <- foreach(i=10000:10100, .combine = function(...) c(...)) %do% {
as.Date(i) }
> > str(x)
>  Date[1:101], format: "1997-05-19" "1997-05-20" "1997-05-21" "1997-05-22"
...
>
> Alternatively, one can use append() which works like c() if no other
> arguments are specified:
>
> > x <- foreach(i=10000:10100, .combine = append) %do% { as.Date(i) }
> > str(x)
>  Date[1:101], format: "1997-05-19" "1997-05-20" "1997-05-21" "1997-05-22"
...
>
> It looks like foreach is treating the .combine = c case specially and
> someone fail to properly dispatch c() on the object (or something).
>
> /Henrik
>
>
> On Sun, Nov 13, 2016 at 7:14 AM, James Hirschorn
> <james.hirschorn at hotmail.com> wrote:
> > I'm still not clear about whether this is a bug in foreach. Should
c.Date be invoked by foreach with .combine='c'?
> >
> > On 11/06/2016 07:02 PM, William Dunlap wrote:
> > Note that in the OP's example c.Date is never invoked.  c.Date is
called if .combine
> > calls c rather than if .combine is c:
> >
> >> library(zoo)
> >> trace(c.Date, quote(print(sys.call())))
> > Tracing function "c.Date" in package "base"
> > [1] "c.Date"
> >> foreach(i=10000:10003, .combine=c) %do% { as.Date(i) }
> > [1] 10000 10001 10002 10003
> >> foreach(i=10000:10003, .combine=function(...)c(...)) %do% { as.Date(i)
}
> > Tracing c.Date(...) on entry
> > eval(expr, envir, enclos)
> > Tracing c.Date(...) on entry
> > eval(expr, envir, enclos)
> > Tracing c.Date(...) on entry
> > eval(expr, envir, enclos)
> > [1] "1997-05-19" "1997-05-20" "1997-05-21" "1997-05-22"
> >
> >
> > Bill Dunlap
> > TIBCO Software
> > wdunlap tibco.com<http://tibco.com>
> >
> > On Sun, Nov 6, 2016 at 2:20 PM, Duncan Murdoch <murdoch.duncan at gmail.com
<mailto:murdoch.duncan at gmail.com>> wrote:
> > On 06/11/2016 5:02 PM, Jim Lemon wrote:
> > hi James,
> > I think you have to have a starting date ("origin") for as.Date to
> > convert numbers to dates.
> >
> > That's true with the function in the base package, but the zoo package
also has an as.Date() function, which defaults the origin to "1970-01-01".
If James is using zoo his code would be okay.  If he's not, he would have
got an error, so I think he must have been.
> >
> > Duncan Murdoch
> >
> >
> >
> > Jim
> >
> > On Sun, Nov 6, 2016 at 12:10 PM, James Hirschorn
> > <james.hirschorn at hotmail.com<mailto:james.hirschorn at hotmail.com>> wrote:
> > This seemed odd so I wanted to check:
> >
> >  > x <- foreach(i=10000:10100, .combine='c') %do% { as.Date(i) }
> >
> > yields a numeric vector for x:
> >
> >  > class(x)
> > [1] "numeric"
> >
> > Should it not be a vector of Date?
> >
> > ______________________________________________
> > R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> > ______________________________________________
> > R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


Henrik

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sun Nov 13 16:25:03 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sun, 13 Nov 2016 07:25:03 -0800
Subject: [R] Principle Component Analysis: Ranking Animal Size Based On
 Combined Metrics
In-Reply-To: <BLUPR0101MB147499AE1D09AA1DD4EB4A54ABBD0@BLUPR0101MB1474.prod.exchangelabs.com>
References: <BLUPR0101MB147499AE1D09AA1DD4EB4A54ABBD0@BLUPR0101MB1474.prod.exchangelabs.com>
Message-ID: <CAGxFJbRWWeowMpssNsWLc-7C05YRp1dZJe8OK-=27R6X6MbOGw@mail.gmail.com>

While you may get a reply here, this list is about R programming, not
about statistics. So

1. Do your homework and read a tutorial on PCA on the web or
elsewhere. Isn't this what a PhD student is supposed to do?

2. Post on a statistics list like stats.stackexchange.com.

3. Consult your professor or other local statistical resource.

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Nov 12, 2016 at 9:46 PM, Sidoti, Salvatore A.
<sidoti.23 at buckeyemail.osu.edu> wrote:
> Let's say I perform 4 measurements on an animal: three are linear measurements in millimeters and the fourth is its weight in milligrams. So, we have a data set with mixed units.
>
> Based on these four correlated measurements, I would like to obtain one "score" or value that describes an individual animal's size. I considered simply taking the geometric mean of these 4 measurements, and that would give me a "score" - larger values would be for larger animals, etc.
>
> However, this assumes that all 4 of these measurements contribute equally to an animal's size. Of course, more than likely this is not the case. I then performed a PCA to discover how much influence each variable had on the overall data set. I was hoping to use this analysis to refine my original approach.
>
> I honestly do not know how to apply the information from the PCA to this particular problem...
>
> I do know, however, that principle components 1 and 2 capture enough of the variation to reduce the number of dimensions down to 2 (see analysis below with the original data set).
>
> Note: animal weights were ln() transformed to increase correlation with the 3 other variables.
>
> df <- data.frame(
>   weight = log(1000*c(0.0980, 0.0622, 0.0600, 0.1098, 0.0538, 0.0701, 0.1138, 0.0540, 0.0629, 0.0930,
>              0.0443, 0.1115, 0.1157, 0.0734, 0.0616, 0.0640, 0.0480, 0.1339, 0.0547, 0.0844,
>              0.0431, 0.0472, 0.0752, 0.0604, 0.0713, 0.0658, 0.0538, 0.0585, 0.0645, 0.0529,
>              0.0448, 0.0574, 0.0577, 0.0514, 0.0758, 0.0424, 0.0997, 0.0758, 0.0649, 0.0465,
>              0.0748, 0.0540, 0.0819, 0.0732, 0.0725, 0.0730, 0.0777, 0.0630, 0.0466)),
>   interoc = c(0.853, 0.865, 0.811, 0.840, 0.783, 0.868, 0.818, 0.847, 0.838, 0.799,
>               0.737, 0.788, 0.731, 0.777, 0.863, 0.877, 0.814, 0.926, 0.767, 0.746,
>               0.700, 0.768, 0.807, 0.753, 0.809, 0.788, 0.750, 0.815, 0.757, 0.737,
>               0.759, 0.863, 0.747, 0.838, 0.790, 0.676, 0.857, 0.728, 0.743, 0.870,
>               0.787, 0.773, 0.829, 0.785, 0.746, 0.834, 0.829, 0.750, 0.842),
>   cwidth = c(3.152, 3.046, 3.139, 3.181, 3.023, 3.452, 2.803, 3.050, 3.160, 3.186,
>              2.801, 2.862, 3.183, 2.770, 3.207, 3.188, 2.969, 3.033, 2.972, 3.291,
>              2.772, 2.875, 2.978, 3.094, 2.956, 2.966, 2.896, 3.149, 2.813, 2.935,
>              2.839, 3.152, 2.984, 3.037, 2.888, 2.723, 3.342, 2.562, 2.827, 2.909,
>              3.093, 2.990, 3.097, 2.751, 2.877, 2.901, 2.895, 2.721, 2.942),
>   clength = c(3.889, 3.733, 3.762, 4.059, 3.911, 3.822, 3.768, 3.814, 3.721, 3.794,
>               3.483, 3.863, 3.856, 3.457, 3.996, 3.876, 3.642, 3.978, 3.534, 3.967,
>               3.429, 3.518, 3.766, 3.755, 3.706, 3.785, 3.607, 3.922, 3.453, 3.589,
>               3.508, 3.861, 3.706, 3.593, 3.570, 3.341, 3.916, 3.336, 3.504, 3.688,
>               3.735, 3.724, 3.860, 3.405, 3.493, 3.586, 3.545, 3.443, 3.640))
>
> pca_morpho <- princomp(df, cor = TRUE)
>
> summary(pca_morpho)
>
> Importance of components:
>                                         Comp.1          Comp.2          Comp.3          Comp.4
> Standard deviation      1.604107        0.8827323       0.7061206       0.3860275
> Proportion of Variance  0.643290        0.1948041       0.1246516       0.0372543
> Cumulative Proportion   0.643290        0.8380941       0.9627457       1.0000000
>
> Loadings:
>                         Comp.1  Comp.2  Comp.3  Comp.4
> weight          -0.371          0.907                           -0.201
> interoc         -0.486  -0.227  -0.840
> cwidth          -0.537  -0.349          0.466           -0.611
> clength         -0.582                          0.278   0.761
>
>                         Comp.1  Comp.2  Comp.3  Comp.4
> SS loadings             1.00            1.00            1.00            1.00
> Proportion Var          0.25            0.25            0.25            0.25
> Cumulative Var          0.25            0.50            0.75            1.00
>
> Any guidance will be greatly appreciated!
>
> Salvatore A. Sidoti
> PhD Student
> The Ohio State University
> Behavioral Ecology
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Sun Nov 13 19:35:57 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 13 Nov 2016 13:35:57 -0500
Subject: [R] Question about expression parser for "return" statement
In-Reply-To: <665597dc-55fd-9916-6564-38400262a270@gmail.com>
References: <CAOogVzUOtMUXnvjFg37aGhDqtT9NmULGFiez4_jv1vGYZLDw0g@mail.gmail.com>
	<52831814-c240-04f8-3069-b670552a3941@gmail.com>
	<665597dc-55fd-9916-6564-38400262a270@gmail.com>
Message-ID: <90a8ce98-ffe5-0484-e5c1-1e0fd6bc332c@gmail.com>

On 13/11/2016 7:58 AM, Duncan Murdoch wrote:
> On 13/11/2016 6:47 AM, Duncan Murdoch wrote:
>> On 13/11/2016 12:50 AM, Dave DeBarr wrote:
>>> I've noticed that if I don't include parentheses around the intended return
>>> value for the "return" statement, R will assume the first parenthetical
>>> expression is the intended return value ... even if that parenthetical
>>> expression is only part of a larger expression.
>>>
>>> Is this intentional?
>>
>> Yes, return is just a function call that has side effects.  As far as
>> the parser is concerned,
>>
>> return ((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance))
>>
>> is basically the same as
>>
>> f((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance))
>
> By the way, out of curiosity I took a look at the source of CRAN
> packages to see if this actually occurs.  It turns out that "return" is
> used as a variable name often enough to make automatic tests tricky, so
> I don't know the answer to my question.  However, I did turn up a number
> of cases where people have code like this:
>
>      if (name == "") return;
>
> (from the bio.infer package), which never calls return(), so doesn't
> actually do what the author likely intended

I searched the R sources and the sources of CRAN packages, and found 
this is a reasonably common problem:  it's in 111 packages, including 
one in base R.  I'll be emailing the maintainers to let them know.

I'll see about putting a check for this into R CMD check.

Duncan Murdoch


From paul at stat.auckland.ac.nz  Sun Nov 13 20:57:15 2016
From: paul at stat.auckland.ac.nz (Paul Murrell)
Date: Mon, 14 Nov 2016 08:57:15 +1300
Subject: [R] [FORGED] How to remove box in Venn plots (Vennerable
 package, uses grid) - similar to bty="n" in standard plots
In-Reply-To: <AM3PR05MB1396C5F07B1471C2E98A580DB1BB0@AM3PR05MB1396.eurprd05.prod.outlook.com>
References: <AM3PR05MB1396C5F07B1471C2E98A580DB1BB0@AM3PR05MB1396.eurprd05.prod.outlook.com>
Message-ID: <9684dcbb-4934-c9be-fa0f-58623edfc525@stat.auckland.ac.nz>

Hi

Can you supply some example code?

You might get some joy from grid.ls() to identify the box followed by 
grid.remove() to get rid of it;  some example code would allow me to 
provide more detailed advice.

Paul

On 12/11/16 05:12, DE LAS HERAS Jose wrote:
> I'm using the package Vennerable to make Venn diagrams, but it always
> makes a box around the diagram.
>
> Using standard R plots I could eliminate that by indicating
>
>
> bty="n"
>
>
> but it seems Vennerable uses the Grid package to generate its plots
> and I'm not really familiar enough with Grid. I was looking at the
> documentation but I can't seem to find a way to achieve that. I'd
> even be happy drawing a white rectangle with wide lines to overplot
> the box, but there must be a way to not draw the box in the first
> place.
>
>
> Anybody knows how?
>
>
> Jose
>
> --
>
> Dr. Jose I. de las Heras The Wellcome Trust Centre for Cell Biology
> Swann Building Max Born Crescent University of Edinburgh Edinburgh
> EH9 3BF UK
>
> Phone: +44 (0)131 6507090
>
> Fax:  +44 (0)131 6507360
>
>
>
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
>
>
> ______________________________________________ R-help at r-project.org
> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide http://www.R-project.org/posting-guide.html and provide
> commented, minimal, self-contained, reproducible code.
>

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From drjimlemon at gmail.com  Sun Nov 13 21:53:03 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Mon, 14 Nov 2016 07:53:03 +1100
Subject: [R] Principle Component Analysis: Ranking Animal Size Based On
 Combined Metrics
In-Reply-To: <BLUPR0101MB147499AE1D09AA1DD4EB4A54ABBD0@BLUPR0101MB1474.prod.exchangelabs.com>
References: <BLUPR0101MB147499AE1D09AA1DD4EB4A54ABBD0@BLUPR0101MB1474.prod.exchangelabs.com>
Message-ID: <CA+8X3fVrqY9447vuiRPbeAmnrZ01AZ8Kvp5nQHbQ4uDr5s8Sfw@mail.gmail.com>

Hi Salvatore,
If by "size" you mean volume, why not directly measure the volume of
your animals? They appear to be fairly small. Sometimes working out
what the critical value actually means can inform the way to measure
it.

Jim


On Sun, Nov 13, 2016 at 4:46 PM, Sidoti, Salvatore A.
<sidoti.23 at buckeyemail.osu.edu> wrote:
> Let's say I perform 4 measurements on an animal: three are linear measurements in millimeters and the fourth is its weight in milligrams. So, we have a data set with mixed units.
>
> Based on these four correlated measurements, I would like to obtain one "score" or value that describes an individual animal's size. I considered simply taking the geometric mean of these 4 measurements, and that would give me a "score" - larger values would be for larger animals, etc.
>
> However, this assumes that all 4 of these measurements contribute equally to an animal's size. Of course, more than likely this is not the case. I then performed a PCA to discover how much influence each variable had on the overall data set. I was hoping to use this analysis to refine my original approach.
>
> I honestly do not know how to apply the information from the PCA to this particular problem...
>
> I do know, however, that principle components 1 and 2 capture enough of the variation to reduce the number of dimensions down to 2 (see analysis below with the original data set).
>
> Note: animal weights were ln() transformed to increase correlation with the 3 other variables.
>
> df <- data.frame(
>   weight = log(1000*c(0.0980, 0.0622, 0.0600, 0.1098, 0.0538, 0.0701, 0.1138, 0.0540, 0.0629, 0.0930,
>              0.0443, 0.1115, 0.1157, 0.0734, 0.0616, 0.0640, 0.0480, 0.1339, 0.0547, 0.0844,
>              0.0431, 0.0472, 0.0752, 0.0604, 0.0713, 0.0658, 0.0538, 0.0585, 0.0645, 0.0529,
>              0.0448, 0.0574, 0.0577, 0.0514, 0.0758, 0.0424, 0.0997, 0.0758, 0.0649, 0.0465,
>              0.0748, 0.0540, 0.0819, 0.0732, 0.0725, 0.0730, 0.0777, 0.0630, 0.0466)),
>   interoc = c(0.853, 0.865, 0.811, 0.840, 0.783, 0.868, 0.818, 0.847, 0.838, 0.799,
>               0.737, 0.788, 0.731, 0.777, 0.863, 0.877, 0.814, 0.926, 0.767, 0.746,
>               0.700, 0.768, 0.807, 0.753, 0.809, 0.788, 0.750, 0.815, 0.757, 0.737,
>               0.759, 0.863, 0.747, 0.838, 0.790, 0.676, 0.857, 0.728, 0.743, 0.870,
>               0.787, 0.773, 0.829, 0.785, 0.746, 0.834, 0.829, 0.750, 0.842),
>   cwidth = c(3.152, 3.046, 3.139, 3.181, 3.023, 3.452, 2.803, 3.050, 3.160, 3.186,
>              2.801, 2.862, 3.183, 2.770, 3.207, 3.188, 2.969, 3.033, 2.972, 3.291,
>              2.772, 2.875, 2.978, 3.094, 2.956, 2.966, 2.896, 3.149, 2.813, 2.935,
>              2.839, 3.152, 2.984, 3.037, 2.888, 2.723, 3.342, 2.562, 2.827, 2.909,
>              3.093, 2.990, 3.097, 2.751, 2.877, 2.901, 2.895, 2.721, 2.942),
>   clength = c(3.889, 3.733, 3.762, 4.059, 3.911, 3.822, 3.768, 3.814, 3.721, 3.794,
>               3.483, 3.863, 3.856, 3.457, 3.996, 3.876, 3.642, 3.978, 3.534, 3.967,
>               3.429, 3.518, 3.766, 3.755, 3.706, 3.785, 3.607, 3.922, 3.453, 3.589,
>               3.508, 3.861, 3.706, 3.593, 3.570, 3.341, 3.916, 3.336, 3.504, 3.688,
>               3.735, 3.724, 3.860, 3.405, 3.493, 3.586, 3.545, 3.443, 3.640))
>
> pca_morpho <- princomp(df, cor = TRUE)
>
> summary(pca_morpho)
>
> Importance of components:
>                                         Comp.1          Comp.2          Comp.3          Comp.4
> Standard deviation      1.604107        0.8827323       0.7061206       0.3860275
> Proportion of Variance  0.643290        0.1948041       0.1246516       0.0372543
> Cumulative Proportion   0.643290        0.8380941       0.9627457       1.0000000
>
> Loadings:
>                         Comp.1  Comp.2  Comp.3  Comp.4
> weight          -0.371          0.907                           -0.201
> interoc         -0.486  -0.227  -0.840
> cwidth          -0.537  -0.349          0.466           -0.611
> clength         -0.582                          0.278   0.761
>
>                         Comp.1  Comp.2  Comp.3  Comp.4
> SS loadings             1.00            1.00            1.00            1.00
> Proportion Var          0.25            0.25            0.25            0.25
> Cumulative Var          0.25            0.50            0.75            1.00
>
> Any guidance will be greatly appreciated!
>
> Salvatore A. Sidoti
> PhD Student
> The Ohio State University
> Behavioral Ecology
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From klebyn at yahoo.com.br  Sun Nov 13 12:47:19 2016
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Sun, 13 Nov 2016 09:47:19 -0200
Subject: [R] how to use the fontchooser tcl tk widget ?
Message-ID: <f69b2a9d-bcc9-3595-43de-b5c23899e543@yahoo.com.br>

hello all r users,
somebody has a example how to use fontchooser widget?
I haven't success in my try :-(
Thanks
Cleber


 > library( tcltk ) # in R-devel,

 > tclVersion()
[1] "8.6.4"

 > tclvalue( tcl('tk::fontchooser', 'show', command='' )  )
Error in (function (name, pos = -1L, envir = as.environment(pos), 
all.names = FALSE,  :
   unused argument ("{DejaVu Sans Mono} 26 bold italic")
Error in (function (name, pos = -1L, envir = as.environment(pos), 
all.names = FALSE,  :
   unused argument ("{DejaVu Sans Mono} 26 bold italic")
[1] ""
 >


---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus


From overholic10 at ajou.ac.kr  Sun Nov 13 16:18:24 2016
From: overholic10 at ajou.ac.kr (overholic10 at ajou.ac.kr)
Date: Mon, 14 Nov 2016 00:18:24 +0900
Subject: [R] Question about using ggplot
Message-ID: <5828845f.85af420a.d2f53.1a80@mx.google.com>

Hi. I?m a student from South Korea, and I?m studying R by myself.
While I am studying, I have a trouble dealing with ggplot(especially, about parameter ?family?)
> b <- biopsy
> b$classn[b$class == "benign"] <- 0
> b$classn[b$class == "malignant"] <- 1
> ggplot(b, aes(x = V1, y = classn)) + geom_point(position = position_jitter(width = 0.3, height = 0.06), alpha = 0.4, shape = 21, size = 1.5) + stat_smooth(method = "glm", family = ?bimomial")) #first code
Warning: Ignoring unknown parameters: family

While studying, I am wondering why there is a warning message. And also, there comes a wrong logistic model(Above picture). But in fact, I expect the below picture from the above code.


And the below code yields ?the model? that I expected(below picture)
ggplot(b, aes(x = V1, y = classn)) + geom_point(position = position_jitter(width = 0.3, height = 0.06), alpha = 0.4, shape = 21, size = 1.5) + stat_smooth(method = "glm", method.args = list(family = "binomial"))




What?s wrong with the first code? When I searched about that from online, other people don?t seem to have a problem using family parameters.
(I currently updated ggplot2, MASS, and sjPlot (Also, double checked it)
Plus, I used code w/ or w/o using ??)


I hope getting a good response from you.
Thanks for reading my mail, and if my message has a rude expression, I?m sorry for my bad english skills..:(


From btupper at bigelow.org  Mon Nov 14 01:54:26 2016
From: btupper at bigelow.org (Ben Tupper)
Date: Sun, 13 Nov 2016 19:54:26 -0500
Subject: [R] Question about using ggplot
In-Reply-To: <5828845f.85af420a.d2f53.1a80@mx.google.com>
References: <5828845f.85af420a.d2f53.1a80@mx.google.com>
Message-ID: <6FBBA20C-229D-47C7-847E-534B723B5AD3@bigelow.org>

Hi,

It's hard to know for sure, but perhaps the spelling of 'binomial' is not correct?

Ben

> On Nov 13, 2016, at 10:18 AM, overholic10 at ajou.ac.kr wrote:
> 
> Hi. I?m a student from South Korea, and I?m studying R by myself.
> While I am studying, I have a trouble dealing with ggplot(especially, about parameter ?family?)
>> b <- biopsy
>> b$classn[b$class == "benign"] <- 0
>> b$classn[b$class == "malignant"] <- 1
>> ggplot(b, aes(x = V1, y = classn)) + geom_point(position = position_jitter(width = 0.3, height = 0.06), alpha = 0.4, shape = 21, size = 1.5) + stat_smooth(method = "glm", family = ?bimomial")) #first code
> Warning: Ignoring unknown parameters: family
> 
> While studying, I am wondering why there is a warning message. And also, there comes a wrong logistic model(Above picture). But in fact, I expect the below picture from the above code.
> 
> 
> And the below code yields ?the model? that I expected(below picture)
> ggplot(b, aes(x = V1, y = classn)) + geom_point(position = position_jitter(width = 0.3, height = 0.06), alpha = 0.4, shape = 21, size = 1.5) + stat_smooth(method = "glm", method.args = list(family = "binomial"))
> 
> 
> 
> 
> What?s wrong with the first code? When I searched about that from online, other people don?t seem to have a problem using family parameters.
> (I currently updated ggplot2, MASS, and sjPlot (Also, double checked it)
> Plus, I used code w/ or w/o using ??)
> 
> 
> I hope getting a good response from you.
> Thanks for reading my mail, and if my message has a rude expression, I?m sorry for my bad english skills..:(
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org


From dosc3612 at colorado.edu  Mon Nov 14 01:55:01 2016
From: dosc3612 at colorado.edu (Dominik Schneider)
Date: Sun, 13 Nov 2016 17:55:01 -0700
Subject: [R] Question about using ggplot
In-Reply-To: <5828845f.85af420a.d2f53.1a80@mx.google.com>
References: <5828845f.85af420a.d2f53.1a80@mx.google.com>
Message-ID: <CAF1jk_m0PzTymmCAewsrtXbvnjA4bzC3We=n1M8XsrMjLBtZeQ@mail.gmail.com>

past versions of ggplot2 used to accept family as an argument directly, but
the latest ggplot2 (perhaps starting with v2?) requires method.args=list().
So the online sources you found using family directly were for an older
version of ggplot.

On Sun, Nov 13, 2016 at 8:18 AM, <overholic10 at ajou.ac.kr> wrote:

> Hi. I?m a student from South Korea, and I?m studying R by myself.
> While I am studying, I have a trouble dealing with ggplot(especially,
> about parameter ?family?)
> > b <- biopsy
> > b$classn[b$class == "benign"] <- 0
> > b$classn[b$class == "malignant"] <- 1
> > ggplot(b, aes(x = V1, y = classn)) + geom_point(position =
> position_jitter(width = 0.3, height = 0.06), alpha = 0.4, shape = 21, size
> = 1.5) + stat_smooth(method = "glm", family = ?bimomial")) #first code
> Warning: Ignoring unknown parameters: family
>
> While studying, I am wondering why there is a warning message. And also,
> there comes a wrong logistic model(Above picture). But in fact, I expect
> the below picture from the above code.
>
>
> And the below code yields ?the model? that I expected(below picture)
> ggplot(b, aes(x = V1, y = classn)) + geom_point(position =
> position_jitter(width = 0.3, height = 0.06), alpha = 0.4, shape = 21, size
> = 1.5) + stat_smooth(method = "glm", method.args = list(family =
> "binomial"))
>
>
>
>
> What?s wrong with the first code? When I searched about that from online,
> other people don?t seem to have a problem using family parameters.
> (I currently updated ggplot2, MASS, and sjPlot (Also, double checked it)
> Plus, I used code w/ or w/o using ??)
>
>
> I hope getting a good response from you.
> Thanks for reading my mail, and if my message has a rude expression, I?m
> sorry for my bad english skills..:(
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Mon Nov 14 03:04:15 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Mon, 14 Nov 2016 13:04:15 +1100
Subject: [R] Principle Component Analysis: Ranking Animal Size Based On
 Combined Metrics
In-Reply-To: <BLUPR0101MB1474BE9248D293A65F3A7F3BABBC0@BLUPR0101MB1474.prod.exchangelabs.com>
References: <BLUPR0101MB147499AE1D09AA1DD4EB4A54ABBD0@BLUPR0101MB1474.prod.exchangelabs.com>
	<CA+8X3fVrqY9447vuiRPbeAmnrZ01AZ8Kvp5nQHbQ4uDr5s8Sfw@mail.gmail.com>
	<BLUPR0101MB1474BE9248D293A65F3A7F3BABBC0@BLUPR0101MB1474.prod.exchangelabs.com>
Message-ID: <CA+8X3fUPBJwNtAHH5TF74SHz4a5SHh5jrxNxWDPXfFPhm4Zcvw@mail.gmail.com>

Hi Salvatore,
Depending upon your concept of "size" the use of the weighted sum may
well suit your purpose. The first principal component, being three
lengths and a mass, is likely to be strongly related to any sensible
concept of "size". My comment was meant to ensure that the local
definition of "size" was what you wanted. Using scaled values is a
good idea as it provides an intuitive measure of comparison within the
population. Remember that if your animal is long and thin, you have
already reduced the importance of the former measurement by scaling.

Jim


On Mon, Nov 14, 2016 at 12:37 PM, Sidoti, Salvatore A.
<sidoti.23 at buckeyemail.osu.edu> wrote:
> Hi Jim,
>
> Nice to see you again! First of all, apologies to all for bending the rules a bit with respect to the mailing list. I know this is a list for R programming specifically, and I have received some great advice in this regard in the past. I just thought this was an interesting applied problem that would generate some discussion about PCA in R.
>
> Yes, that is an excellent question! Indeed, why not just volume? Since this is still a work in progress and we have not published as of yet, I would rather not be more specific about the type of animal at this time ;>}. Nonetheless, I can say that the animals I study change "size" depending on their feeding and hydration state. The abdomen in particular undergoes drastic size changes. That being said, there are key anatomical features that remain fixed in the adult.
>
> Now, there *might* be a way to work volume into the PCA. Although volume is not a reliable metric since the abdomen size is so changeable while the animal is alive, but what about preserved specimens? I have many that have been marinating in ethanol for months. Wouldn't the tissues have equilibrated by now? Probably... I could measure volume by displacement or suspension, I suppose.
>
> In the meantime, here's a few thoughts:
>
> 1)      Use the contribution % (known as C% hereafter) of each variable on principle components 1 and 2.
>
> 2)      The total contribution of a variable that explains the variations retained by PC1 an PC2 is calculated by:
>
>         sum(C%1 * eigenvalue1, C%2 * eigenvalue2)
>
> 3) Scale() to mean-center the columns of the data set.
>
> 4) Use these total contributions as the weights of an arithmetic mean.
>
> For example, we have an animal with the following data (mean-centered):
> weight: 1.334
> interoc:        -0.225
> clength:        0.046
> cwidth: -0.847
>
> The contributions of these variables on PC1 and PC2 are (% changed to proportions):
> weight: 0.556
> interoc:        0.357
> clength:        0.493
> cwidth: 0.291
>
> To calculate size:
> 1.334(0.556) - 0.225(0.357) + 0.046(0.493) - 0.847(0.291) = 0.43758
> Then divide by the sum of the weights:
> 0.43758 / 1.697 = 0.257855 = "animal size"
>
> This value can then be used to rank the animal according to its size for further analysis...
>
> Does this sound like a reasonable application of my PCA data?
>
> Salvatore A. Sidoti
> PhD Student
> Behavioral Ecology
>
> -----Original Message-----
> From: Jim Lemon [mailto:drjimlemon at gmail.com]
> Sent: Sunday, November 13, 2016 3:53 PM
> To: Sidoti, Salvatore A. <sidoti.23 at buckeyemail.osu.edu>; r-help mailing list <r-help at r-project.org>
> Subject: Re: [R] Principle Component Analysis: Ranking Animal Size Based On Combined Metrics
>
> Hi Salvatore,
> If by "size" you mean volume, why not directly measure the volume of your animals? They appear to be fairly small. Sometimes working out what the critical value actually means can inform the way to measure it.
>
> Jim
>
>
> On Sun, Nov 13, 2016 at 4:46 PM, Sidoti, Salvatore A.
> <sidoti.23 at buckeyemail.osu.edu> wrote:
>> Let's say I perform 4 measurements on an animal: three are linear measurements in millimeters and the fourth is its weight in milligrams. So, we have a data set with mixed units.
>>
>> Based on these four correlated measurements, I would like to obtain one "score" or value that describes an individual animal's size. I considered simply taking the geometric mean of these 4 measurements, and that would give me a "score" - larger values would be for larger animals, etc.
>>
>> However, this assumes that all 4 of these measurements contribute equally to an animal's size. Of course, more than likely this is not the case. I then performed a PCA to discover how much influence each variable had on the overall data set. I was hoping to use this analysis to refine my original approach.
>>
>> I honestly do not know how to apply the information from the PCA to this particular problem...
>>
>> I do know, however, that principle components 1 and 2 capture enough of the variation to reduce the number of dimensions down to 2 (see analysis below with the original data set).
>>
>> Note: animal weights were ln() transformed to increase correlation with the 3 other variables.
>>
>> df <- data.frame(
>>   weight = log(1000*c(0.0980, 0.0622, 0.0600, 0.1098, 0.0538, 0.0701, 0.1138, 0.0540, 0.0629, 0.0930,
>>              0.0443, 0.1115, 0.1157, 0.0734, 0.0616, 0.0640, 0.0480, 0.1339, 0.0547, 0.0844,
>>              0.0431, 0.0472, 0.0752, 0.0604, 0.0713, 0.0658, 0.0538, 0.0585, 0.0645, 0.0529,
>>              0.0448, 0.0574, 0.0577, 0.0514, 0.0758, 0.0424, 0.0997, 0.0758, 0.0649, 0.0465,
>>              0.0748, 0.0540, 0.0819, 0.0732, 0.0725, 0.0730, 0.0777, 0.0630, 0.0466)),
>>   interoc = c(0.853, 0.865, 0.811, 0.840, 0.783, 0.868, 0.818, 0.847, 0.838, 0.799,
>>               0.737, 0.788, 0.731, 0.777, 0.863, 0.877, 0.814, 0.926, 0.767, 0.746,
>>               0.700, 0.768, 0.807, 0.753, 0.809, 0.788, 0.750, 0.815, 0.757, 0.737,
>>               0.759, 0.863, 0.747, 0.838, 0.790, 0.676, 0.857, 0.728, 0.743, 0.870,
>>               0.787, 0.773, 0.829, 0.785, 0.746, 0.834, 0.829, 0.750, 0.842),
>>   cwidth = c(3.152, 3.046, 3.139, 3.181, 3.023, 3.452, 2.803, 3.050, 3.160, 3.186,
>>              2.801, 2.862, 3.183, 2.770, 3.207, 3.188, 2.969, 3.033, 2.972, 3.291,
>>              2.772, 2.875, 2.978, 3.094, 2.956, 2.966, 2.896, 3.149, 2.813, 2.935,
>>              2.839, 3.152, 2.984, 3.037, 2.888, 2.723, 3.342, 2.562, 2.827, 2.909,
>>              3.093, 2.990, 3.097, 2.751, 2.877, 2.901, 2.895, 2.721, 2.942),
>>   clength = c(3.889, 3.733, 3.762, 4.059, 3.911, 3.822, 3.768, 3.814, 3.721, 3.794,
>>               3.483, 3.863, 3.856, 3.457, 3.996, 3.876, 3.642, 3.978, 3.534, 3.967,
>>               3.429, 3.518, 3.766, 3.755, 3.706, 3.785, 3.607, 3.922, 3.453, 3.589,
>>               3.508, 3.861, 3.706, 3.593, 3.570, 3.341, 3.916, 3.336, 3.504, 3.688,
>>               3.735, 3.724, 3.860, 3.405, 3.493, 3.586, 3.545, 3.443,
>> 3.640))
>>
>> pca_morpho <- princomp(df, cor = TRUE)
>>
>> summary(pca_morpho)
>>
>> Importance of components:
>>                                         Comp.1          Comp.2          Comp.3          Comp.4
>> Standard deviation      1.604107        0.8827323       0.7061206       0.3860275
>> Proportion of Variance  0.643290        0.1948041       0.1246516       0.0372543
>> Cumulative Proportion   0.643290        0.8380941       0.9627457       1.0000000
>>
>> Loadings:
>>                         Comp.1  Comp.2  Comp.3  Comp.4
>> weight          -0.371          0.907                           -0.201
>> interoc         -0.486  -0.227  -0.840
>> cwidth          -0.537  -0.349          0.466           -0.611
>> clength         -0.582                          0.278   0.761
>>
>>                         Comp.1  Comp.2  Comp.3  Comp.4
>> SS loadings             1.00            1.00            1.00            1.00
>> Proportion Var          0.25            0.25            0.25            0.25
>> Cumulative Var          0.25            0.50            0.75            1.00
>>
>> Any guidance will be greatly appreciated!
>>
>> Salvatore A. Sidoti
>> PhD Student
>> The Ohio State University
>> Behavioral Ecology
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From friendly at yorku.ca  Mon Nov 14 03:10:25 2016
From: friendly at yorku.ca (Michael Friendly)
Date: Sun, 13 Nov 2016 21:10:25 -0500
Subject: [R] Principle Component Analysis: Ranking Animal Size Based On
 Combined Metrics
In-Reply-To: <BLUPR0101MB147499AE1D09AA1DD4EB4A54ABBD0@BLUPR0101MB1474.prod.exchangelabs.com>
References: <BLUPR0101MB147499AE1D09AA1DD4EB4A54ABBD0@BLUPR0101MB1474.prod.exchangelabs.com>
Message-ID: <c59be6bd-313b-652a-27d9-aa0e53334424@yorku.ca>

Salvatore,

I won't comment on whether to use log weight "to increase the 
correlation"  -- that depends on whether that makes sense, and whether 
the relationships with other variables is more nearly linear.

Try this with your pca of the correlation matrix:

biplot(pca_morpho)

You'll see that the  first component is defined largely by the
large correlations among length, interoc,and cwidth
while component2 is largely determined by weight.

You should probably do some reading on PCA or get some
statistical consulting at OSU to decide what to do with this.

  hope this helps

-Michael


On 11/13/16 12:46 AM, Sidoti, Salvatore A. wrote:
> Let's say I perform 4 measurements on an animal: three are linear measurements in millimeters and the fourth is its weight in milligrams. So, we have a data set with mixed units.
>
> Based on these four correlated measurements, I would like to obtain one "score" or value that describes an individual animal's size. I considered simply taking the geometric mean of these 4 measurements, and that would give me a "score" - larger values would be for larger animals, etc.
>
> However, this assumes that all 4 of these measurements contribute equally to an animal's size. Of course, more than likely this is not the case. I then performed a PCA to discover how much influence each variable had on the overall data set. I was hoping to use this analysis to refine my original approach.
>
> I honestly do not know how to apply the information from the PCA to this particular problem...
>
> I do know, however, that principle components 1 and 2 capture enough of the variation to reduce the number of dimensions down to 2 (see analysis below with the original data set).
>
> Note: animal weights were ln() transformed to increase correlation with the 3 other variables.
>
> df <- data.frame(
>   weight = log(1000*c(0.0980, 0.0622, 0.0600, 0.1098, 0.0538, 0.0701, 0.1138, 0.0540, 0.0629, 0.0930,
>              0.0443, 0.1115, 0.1157, 0.0734, 0.0616, 0.0640, 0.0480, 0.1339, 0.0547, 0.0844,
>              0.0431, 0.0472, 0.0752, 0.0604, 0.0713, 0.0658, 0.0538, 0.0585, 0.0645, 0.0529,
>              0.0448, 0.0574, 0.0577, 0.0514, 0.0758, 0.0424, 0.0997, 0.0758, 0.0649, 0.0465,
>              0.0748, 0.0540, 0.0819, 0.0732, 0.0725, 0.0730, 0.0777, 0.0630, 0.0466)),
>   interoc = c(0.853, 0.865, 0.811, 0.840, 0.783, 0.868, 0.818, 0.847, 0.838, 0.799,
>               0.737, 0.788, 0.731, 0.777, 0.863, 0.877, 0.814, 0.926, 0.767, 0.746,
>               0.700, 0.768, 0.807, 0.753, 0.809, 0.788, 0.750, 0.815, 0.757, 0.737,
>               0.759, 0.863, 0.747, 0.838, 0.790, 0.676, 0.857, 0.728, 0.743, 0.870,
>               0.787, 0.773, 0.829, 0.785, 0.746, 0.834, 0.829, 0.750, 0.842),
>   cwidth = c(3.152, 3.046, 3.139, 3.181, 3.023, 3.452, 2.803, 3.050, 3.160, 3.186,
>              2.801, 2.862, 3.183, 2.770, 3.207, 3.188, 2.969, 3.033, 2.972, 3.291,
>              2.772, 2.875, 2.978, 3.094, 2.956, 2.966, 2.896, 3.149, 2.813, 2.935,
>              2.839, 3.152, 2.984, 3.037, 2.888, 2.723, 3.342, 2.562, 2.827, 2.909,
>              3.093, 2.990, 3.097, 2.751, 2.877, 2.901, 2.895, 2.721, 2.942),
>   clength = c(3.889, 3.733, 3.762, 4.059, 3.911, 3.822, 3.768, 3.814, 3.721, 3.794,
>               3.483, 3.863, 3.856, 3.457, 3.996, 3.876, 3.642, 3.978, 3.534, 3.967,
>               3.429, 3.518, 3.766, 3.755, 3.706, 3.785, 3.607, 3.922, 3.453, 3.589,
>               3.508, 3.861, 3.706, 3.593, 3.570, 3.341, 3.916, 3.336, 3.504, 3.688,
>               3.735, 3.724, 3.860, 3.405, 3.493, 3.586, 3.545, 3.443, 3.640))
>
> pca_morpho <- princomp(df, cor = TRUE)
>
> summary(pca_morpho)
>
> Importance of components:
>                          		Comp.1    	Comp.2    	Comp.3    	Comp.4
> Standard deviation     	1.604107 	0.8827323 	0.7061206 	0.3860275
> Proportion of Variance 	0.643290 	0.1948041 	0.1246516 	0.0372543
> Cumulative Proportion  	0.643290 	0.8380941 	0.9627457 	1.0000000
>
> Loadings:
>         		Comp.1 	Comp.2	Comp.3 	Comp.4
> weight  	-0.371  	0.907        			-0.201
> interoc 	-0.486 	-0.227 	-0.840
> cwidth  	-0.537 	-0.349  	0.466 		-0.611
> clength 	-0.582         			0.278  	0.761
>
>                		Comp.1	Comp.2	Comp.3	Comp.4
> SS loadings      	1.00   		1.00   		1.00   		1.00
> Proportion Var   	0.25   		0.25   		0.25 		0.25
> Cumulative Var   	0.25   		0.50   		0.75   		1.00
>
> Any guidance will be greatly appreciated!
>
> Salvatore A. Sidoti
> PhD Student
> The Ohio State University
> Behavioral Ecology
>


From jdnewmil at dcn.davis.ca.us  Mon Nov 14 03:42:00 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sun, 13 Nov 2016 18:42:00 -0800
Subject: [R] Question about expression parser for "return" statement
In-Reply-To: <52831814-c240-04f8-3069-b670552a3941@gmail.com>
References: <CAOogVzUOtMUXnvjFg37aGhDqtT9NmULGFiez4_jv1vGYZLDw0g@mail.gmail.com>
	<52831814-c240-04f8-3069-b670552a3941@gmail.com>
Message-ID: <89968F61-40C5-472B-98CE-F56AF662B0F0@dcn.davis.ca.us>

I find your response here inconsistent... either including `return` causes a "wasted" function call to occur (same result achieved slower) or the parser has an optimization in it to prevent the wasted function call (only behaviorally the same).

I carefully avoid using the return function in R. Both because using it before the end of a function usually makes the logic harder to follow and because I am under the impression that using it at the end of the function is a small but pointless waste of CPU cycles. That some people might be prone to writing a C-like use of "return;" which causes a function object to be returned only increases my aversion to using it. 
-- 
Sent from my phone. Please excuse my brevity.

On November 13, 2016 3:47:10 AM PST, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>On 13/11/2016 12:50 AM, Dave DeBarr wrote:
>> I've noticed that if I don't include parentheses around the intended
>return
>> value for the "return" statement, R will assume the first
>parenthetical
>> expression is the intended return value ... even if that
>parenthetical
>> expression is only part of a larger expression.
>>
>> Is this intentional?
>
>Yes, return is just a function call that has side effects.  As far as 
>the parser is concerned,
>
>return ((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance))
>
>is basically the same as
>
>f((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance))
>
>Duncan Murdoch
>
>>
>> I'm guessing it is intentional; but since there is no warning about
>> ignoring the rest of the expression, it could lead to hard-to-find
>bugs.
>>
>> Thanks,
>> Dave
>>
>> Here's an example ...
>>
>> dnorm(2, 0, 1)
>> normalDensityFunction = function(x, Mean, Variance) {
>>     # no parentheses surrounding the entire "return" value
>>     return (1/sqrt(2*pi*Variance))*exp(-(1/2)*((x -
>Mean)^2)/Variance)
>> }
>> normalDensityFunction(2, 0, 1)    # incorrect answer
>> normalDensityFunction = function(x, Mean, Variance) {
>>     # parentheses surrounding the entire "return" value
>>     return ((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x -
>Mean)^2)/Variance))
>> }
>> normalDensityFunction(2, 0, 1)    # correct answer
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From berdore at gmail.com  Sun Nov 13 23:09:03 2016
From: berdore at gmail.com (=?UTF-8?Q?Bernardo_Dor=C3=A9?=)
Date: Sun, 13 Nov 2016 20:09:03 -0200
Subject: [R] Function argument and scope
Message-ID: <CAEqmCMTX4VEjnwzbcx9vx6NC7f1xh3a8RC+TGantvHJOvy=Wow@mail.gmail.com>

Hello list,

my first post but I've been using this list as a help source for a while.
Couldn't live without it.

I am writing a function that takes a dataframe as an argument and in the
end I intend to assign the result of some computation back to the
dataframe. This is what I have so far:

myFunction <- function(x){
  y <- x[1,1]
  z <- strsplit(as.character(y), split = " ")
  if(length(z[[1]] > 1)){
    predictedWord <- z[[1]][length(z[[1]])]
    z <- z[[1]][-c(length(z[[1]]))]
    z <- paste(z, collapse = " ")
  }
  x[1,1] <- z
}

And lets say I create my dataframe like this:
test <- data.frame(var1=c("a","b","c"),var2=c("d","e","f"))

and then call
myFunction(test)

The problem is when I assign x[1,1] to y in the first operation inside the
function, x becomes a dataframe inside the function scope and loses the
reference to the dataframe "test" passed as argument. In the end when I
assign z to what should be row 1 and column 1 of the "test" dataframe, it
assigns to x inside the function scope and no modification is made on
"test".

I hope the problem statement is clear.

Thank you,

Bernardo Dor?

	[[alternative HTML version deleted]]


From sidoti.23 at buckeyemail.osu.edu  Mon Nov 14 02:37:33 2016
From: sidoti.23 at buckeyemail.osu.edu (Sidoti, Salvatore A.)
Date: Mon, 14 Nov 2016 01:37:33 +0000
Subject: [R] Principle Component Analysis: Ranking Animal Size Based On
 Combined Metrics
In-Reply-To: <CA+8X3fVrqY9447vuiRPbeAmnrZ01AZ8Kvp5nQHbQ4uDr5s8Sfw@mail.gmail.com>
References: <BLUPR0101MB147499AE1D09AA1DD4EB4A54ABBD0@BLUPR0101MB1474.prod.exchangelabs.com>
	<CA+8X3fVrqY9447vuiRPbeAmnrZ01AZ8Kvp5nQHbQ4uDr5s8Sfw@mail.gmail.com>
Message-ID: <BLUPR0101MB1474BE9248D293A65F3A7F3BABBC0@BLUPR0101MB1474.prod.exchangelabs.com>

Hi Jim,

Nice to see you again! First of all, apologies to all for bending the rules a bit with respect to the mailing list. I know this is a list for R programming specifically, and I have received some great advice in this regard in the past. I just thought this was an interesting applied problem that would generate some discussion about PCA in R.

Yes, that is an excellent question! Indeed, why not just volume? Since this is still a work in progress and we have not published as of yet, I would rather not be more specific about the type of animal at this time ;>}. Nonetheless, I can say that the animals I study change "size" depending on their feeding and hydration state. The abdomen in particular undergoes drastic size changes. That being said, there are key anatomical features that remain fixed in the adult.

Now, there *might* be a way to work volume into the PCA. Although volume is not a reliable metric since the abdomen size is so changeable while the animal is alive, but what about preserved specimens? I have many that have been marinating in ethanol for months. Wouldn't the tissues have equilibrated by now? Probably... I could measure volume by displacement or suspension, I suppose.

In the meantime, here's a few thoughts:

1) 	Use the contribution % (known as C% hereafter) of each variable on principle components 1 and 2.

2) 	The total contribution of a variable that explains the variations retained by PC1 an PC2 is calculated by:
	
	sum(C%1 * eigenvalue1, C%2 * eigenvalue2)

3) Scale() to mean-center the columns of the data set.

4) Use these total contributions as the weights of an arithmetic mean.

For example, we have an animal with the following data (mean-centered):
weight:	1.334
interoc:	-0.225
clength:	0.046
cwidth:	-0.847

The contributions of these variables on PC1 and PC2 are (% changed to proportions):
weight:	0.556
interoc:	0.357
clength:	0.493
cwidth:	0.291

To calculate size:
1.334(0.556) - 0.225(0.357) + 0.046(0.493) - 0.847(0.291) = 0.43758
Then divide by the sum of the weights:
0.43758 / 1.697 = 0.257855 = "animal size"

This value can then be used to rank the animal according to its size for further analysis...

Does this sound like a reasonable application of my PCA data?

Salvatore A. Sidoti
PhD Student
Behavioral Ecology

-----Original Message-----
From: Jim Lemon [mailto:drjimlemon at gmail.com] 
Sent: Sunday, November 13, 2016 3:53 PM
To: Sidoti, Salvatore A. <sidoti.23 at buckeyemail.osu.edu>; r-help mailing list <r-help at r-project.org>
Subject: Re: [R] Principle Component Analysis: Ranking Animal Size Based On Combined Metrics

Hi Salvatore,
If by "size" you mean volume, why not directly measure the volume of your animals? They appear to be fairly small. Sometimes working out what the critical value actually means can inform the way to measure it.

Jim


On Sun, Nov 13, 2016 at 4:46 PM, Sidoti, Salvatore A.
<sidoti.23 at buckeyemail.osu.edu> wrote:
> Let's say I perform 4 measurements on an animal: three are linear measurements in millimeters and the fourth is its weight in milligrams. So, we have a data set with mixed units.
>
> Based on these four correlated measurements, I would like to obtain one "score" or value that describes an individual animal's size. I considered simply taking the geometric mean of these 4 measurements, and that would give me a "score" - larger values would be for larger animals, etc.
>
> However, this assumes that all 4 of these measurements contribute equally to an animal's size. Of course, more than likely this is not the case. I then performed a PCA to discover how much influence each variable had on the overall data set. I was hoping to use this analysis to refine my original approach.
>
> I honestly do not know how to apply the information from the PCA to this particular problem...
>
> I do know, however, that principle components 1 and 2 capture enough of the variation to reduce the number of dimensions down to 2 (see analysis below with the original data set).
>
> Note: animal weights were ln() transformed to increase correlation with the 3 other variables.
>
> df <- data.frame(
>   weight = log(1000*c(0.0980, 0.0622, 0.0600, 0.1098, 0.0538, 0.0701, 0.1138, 0.0540, 0.0629, 0.0930,
>              0.0443, 0.1115, 0.1157, 0.0734, 0.0616, 0.0640, 0.0480, 0.1339, 0.0547, 0.0844,
>              0.0431, 0.0472, 0.0752, 0.0604, 0.0713, 0.0658, 0.0538, 0.0585, 0.0645, 0.0529,
>              0.0448, 0.0574, 0.0577, 0.0514, 0.0758, 0.0424, 0.0997, 0.0758, 0.0649, 0.0465,
>              0.0748, 0.0540, 0.0819, 0.0732, 0.0725, 0.0730, 0.0777, 0.0630, 0.0466)),
>   interoc = c(0.853, 0.865, 0.811, 0.840, 0.783, 0.868, 0.818, 0.847, 0.838, 0.799,
>               0.737, 0.788, 0.731, 0.777, 0.863, 0.877, 0.814, 0.926, 0.767, 0.746,
>               0.700, 0.768, 0.807, 0.753, 0.809, 0.788, 0.750, 0.815, 0.757, 0.737,
>               0.759, 0.863, 0.747, 0.838, 0.790, 0.676, 0.857, 0.728, 0.743, 0.870,
>               0.787, 0.773, 0.829, 0.785, 0.746, 0.834, 0.829, 0.750, 0.842),
>   cwidth = c(3.152, 3.046, 3.139, 3.181, 3.023, 3.452, 2.803, 3.050, 3.160, 3.186,
>              2.801, 2.862, 3.183, 2.770, 3.207, 3.188, 2.969, 3.033, 2.972, 3.291,
>              2.772, 2.875, 2.978, 3.094, 2.956, 2.966, 2.896, 3.149, 2.813, 2.935,
>              2.839, 3.152, 2.984, 3.037, 2.888, 2.723, 3.342, 2.562, 2.827, 2.909,
>              3.093, 2.990, 3.097, 2.751, 2.877, 2.901, 2.895, 2.721, 2.942),
>   clength = c(3.889, 3.733, 3.762, 4.059, 3.911, 3.822, 3.768, 3.814, 3.721, 3.794,
>               3.483, 3.863, 3.856, 3.457, 3.996, 3.876, 3.642, 3.978, 3.534, 3.967,
>               3.429, 3.518, 3.766, 3.755, 3.706, 3.785, 3.607, 3.922, 3.453, 3.589,
>               3.508, 3.861, 3.706, 3.593, 3.570, 3.341, 3.916, 3.336, 3.504, 3.688,
>               3.735, 3.724, 3.860, 3.405, 3.493, 3.586, 3.545, 3.443, 
> 3.640))
>
> pca_morpho <- princomp(df, cor = TRUE)
>
> summary(pca_morpho)
>
> Importance of components:
>                                         Comp.1          Comp.2          Comp.3          Comp.4
> Standard deviation      1.604107        0.8827323       0.7061206       0.3860275
> Proportion of Variance  0.643290        0.1948041       0.1246516       0.0372543
> Cumulative Proportion   0.643290        0.8380941       0.9627457       1.0000000
>
> Loadings:
>                         Comp.1  Comp.2  Comp.3  Comp.4
> weight          -0.371          0.907                           -0.201
> interoc         -0.486  -0.227  -0.840
> cwidth          -0.537  -0.349          0.466           -0.611
> clength         -0.582                          0.278   0.761
>
>                         Comp.1  Comp.2  Comp.3  Comp.4
> SS loadings             1.00            1.00            1.00            1.00
> Proportion Var          0.25            0.25            0.25            0.25
> Cumulative Var          0.25            0.50            0.75            1.00
>
> Any guidance will be greatly appreciated!
>
> Salvatore A. Sidoti
> PhD Student
> The Ohio State University
> Behavioral Ecology
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From mokuram at sina.com  Mon Nov 14 04:01:25 2016
From: mokuram at sina.com (mokuram at sina.com)
Date: Mon, 14 Nov 2016 11:01:25 +0800
Subject: [R] question on mean, sum
Message-ID: <20161114030125.B75F56800AB@webmail.sinamail.sina.com.cn>

 Hi, 
I am working on functions such as sum(), mean() ... 
> sum(mtcars)[1] 13942.2> mean(mtcars)[1] NAWarning message:In mean.default(mtcars) : NA> sd(mtcars)Error in is.data.frame(x) : ()'double'
why got different reply?Is this a BUG for the current version of R?my version info:version.string R version 3.3.1 (2016-06-21)

Thank you very much for the help.
mokuram

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Mon Nov 14 08:33:02 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Mon, 14 Nov 2016 07:33:02 +0000
Subject: [R] question on mean, sum
In-Reply-To: <20161114030125.B75F56800AB@webmail.sinamail.sina.com.cn>
References: <20161114030125.B75F56800AB@webmail.sinamail.sina.com.cn>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5045EF7@SRVEXCHMBX.precheza.cz>

Hi

Your HTML formatted message is scrambled so it is hard to understand what is your problem. The probable cause is the fact that sum, mean and sd behave differently when applied to different objects (vector, data.frame, ...)

Post in plain text together with at least result of
str(mtcars)

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> mokuram at sina.com
> Sent: Monday, November 14, 2016 4:01 AM
> To: r-help <r-help at r-project.org>
> Subject: [R] question on mean, sum
>
>  Hi,
> I am working on functions such as sum(), mean() ...
> > sum(mtcars)[1] 13942.2> mean(mtcars)[1] NAWarning message:In
> mean.default(mtcars) : NA> sd(mtcars)Error in is.data.frame(x) : ()'double'
> why got different reply?Is this a BUG for the current version of R?my version
> info:version.string R version 3.3.1 (2016-06-21)
>
> Thank you very much for the help.
> mokuram
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From bhh at xs4all.nl  Mon Nov 14 08:44:24 2016
From: bhh at xs4all.nl (Berend Hasselman)
Date: Mon, 14 Nov 2016 08:44:24 +0100
Subject: [R] question on mean, sum
In-Reply-To: <20161114030125.B75F56800AB@webmail.sinamail.sina.com.cn>
References: <20161114030125.B75F56800AB@webmail.sinamail.sina.com.cn>
Message-ID: <202DA9D6-2BAC-4C37-BEE7-F0E1C99E3B09@xs4all.nl>


In addition to Petr's remarks:

- why are you doing sum(mtcars)[1]?  Do you want the sum of first column of mtcars? In that case you should do sum(mtcars[,1]).

- similar remarks apply to your use of  mean and sd.


Have you read an introduction to R?


Berend

> On 14 Nov 2016, at 04:01, mokuram at sina.com wrote:
> 
> Hi, 
> I am working on functions such as sum(), mean() ... 
>> sum(mtcars)[1] 13942.2> mean(mtcars)[1] NAWarning message:In mean.default(mtcars) : NA> sd(mtcars)Error in is.data.frame(x) : ()'double'
> why got different reply?Is this a BUG for the current version of R?my version info:version.string R version 3.3.1 (2016-06-21)
> 
> Thank you very much for the help.
> mokuram
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Mon Nov 14 08:55:54 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Mon, 14 Nov 2016 18:55:54 +1100
Subject: [R] Function argument and scope
In-Reply-To: <CAEqmCMTX4VEjnwzbcx9vx6NC7f1xh3a8RC+TGantvHJOvy=Wow@mail.gmail.com>
References: <CAEqmCMTX4VEjnwzbcx9vx6NC7f1xh3a8RC+TGantvHJOvy=Wow@mail.gmail.com>
Message-ID: <CA+8X3fUjq3+JBxR+kqTAo8+B_EcmOfEyBybsz1GwnZ_NSn6arA@mail.gmail.com>

Hi Bernardo,
I don't think that your function is doing anything like you expect it to do:

test <- data.frame(var1=c("a","b","c"),var2=c("d","e","f"))
test
 var1 var2
1    a    d
2    b    e
3    c    f

You have a data frame with two columns, the first thing you do is
extract the first value in the first column:

test[1,1]
[1] a
Levels: a b c

Okay, looks like the first column is a factor. The next operation,
strsplit, requires a character argument

strsplit(test[1,1]," ")
Error in strsplit(test[1, 1], " ") : non-character argument

Not what you have. Go back to the beginning and create the data frame
without the values becoming factors:

test <- data.frame(var1=c("a","b","c"),var2=c("d","e","f"),
 stringsAsFactors=FALSE)

> strsplit(test[1,1]," ")
[[1]]
[1] "a"

Now strsplit doesn't complain, but just gives you back the first
value. As it is only of length 1, the conditional fails and the
function returns the initial data frame. Try as I might, I cannot work
out what you are trying to do, so I will await further information.

Jim


On Mon, Nov 14, 2016 at 9:09 AM, Bernardo Dor? <berdore at gmail.com> wrote:
> Hello list,
>
> my first post but I've been using this list as a help source for a while.
> Couldn't live without it.
>
> I am writing a function that takes a dataframe as an argument and in the
> end I intend to assign the result of some computation back to the
> dataframe. This is what I have so far:
>
> myFunction <- function(x){
>   y <- x[1,1]
>   z <- strsplit(as.character(y), split = " ")
>   if(length(z[[1]] > 1)){
>     predictedWord <- z[[1]][length(z[[1]])]
>     z <- z[[1]][-c(length(z[[1]]))]
>     z <- paste(z, collapse = " ")
>   }
>   x[1,1] <- z
> }
>
> And lets say I create my dataframe like this:
> test <- data.frame(var1=c("a","b","c"),var2=c("d","e","f"))
>
> and then call
> myFunction(test)
>
> The problem is when I assign x[1,1] to y in the first operation inside the
> function, x becomes a dataframe inside the function scope and loses the
> reference to the dataframe "test" passed as argument. In the end when I
> assign z to what should be row 1 and column 1 of the "test" dataframe, it
> assigns to x inside the function scope and no modification is made on
> "test".
>
> I hope the problem statement is clear.
>
> Thank you,
>
> Bernardo Dor?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From roundsjeremiah at gmail.com  Mon Nov 14 09:16:38 2016
From: roundsjeremiah at gmail.com (jeremiah rounds)
Date: Mon, 14 Nov 2016 00:16:38 -0800
Subject: [R] Function argument and scope
In-Reply-To: <CAEqmCMTX4VEjnwzbcx9vx6NC7f1xh3a8RC+TGantvHJOvy=Wow@mail.gmail.com>
References: <CAEqmCMTX4VEjnwzbcx9vx6NC7f1xh3a8RC+TGantvHJOvy=Wow@mail.gmail.com>
Message-ID: <CAOjnRsYyy9uzx36W2rNTyq-WQYAt6YM6dyVFyU7GJTdwsQEBaw@mail.gmail.com>

Hi,

Didn't bother to run the code because someone else said it might do what
you intended, and also your problem description was complete unto itself.

The issue is that R copies on change.  You are thinking like you have a
reference, which you do not.  That is not very R like in style, but it
certainly can be accomplished if you want via change of input class (See
new.env()).  A typical R style would be to make the modifications to the
input argument, return it, and then assign it back to the input object.

e.g.
test = myFunction(test)

If you really have some reason to want to change the data.frame in a
function without re-assigning it then check out data.table, which has that
as a side effect of how it operates.

Thanks,



On Sun, Nov 13, 2016 at 2:09 PM, Bernardo Dor? <berdore at gmail.com> wrote:

> Hello list,
>
> my first post but I've been using this list as a help source for a while.
> Couldn't live without it.
>
> I am writing a function that takes a dataframe as an argument and in the
> end I intend to assign the result of some computation back to the
> dataframe. This is what I have so far:
>
> myFunction <- function(x){
>   y <- x[1,1]
>   z <- strsplit(as.character(y), split = " ")
>   if(length(z[[1]] > 1)){
>     predictedWord <- z[[1]][length(z[[1]])]
>     z <- z[[1]][-c(length(z[[1]]))]
>     z <- paste(z, collapse = " ")
>   }
>   x[1,1] <- z
> }
>
> And lets say I create my dataframe like this:
> test <- data.frame(var1=c("a","b","c"),var2=c("d","e","f"))
>
> and then call
> myFunction(test)
>
> The problem is when I assign x[1,1] to y in the first operation inside the
> function, x becomes a dataframe inside the function scope and loses the
> reference to the dataframe "test" passed as argument. In the end when I
> assign z to what should be row 1 and column 1 of the "test" dataframe, it
> assigns to x inside the function scope and no modification is made on
> "test".
>
> I hope the problem statement is clear.
>
> Thank you,
>
> Bernardo Dor?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Mon Nov 14 09:26:34 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Mon, 14 Nov 2016 19:26:34 +1100
Subject: [R] question on mean, sum
In-Reply-To: <20161114030125.B75F56800AB@webmail.sinamail.sina.com.cn>
References: <20161114030125.B75F56800AB@webmail.sinamail.sina.com.cn>
Message-ID: <CA+8X3fV--0FVZaMz+YX-ojpcHr8t9C+o65h46TESCyqqE31GCg@mail.gmail.com>

Hi mokuram,

As others have noted, you will profit from a bit more knowledge about
"extraction":

sum(mtcars)
[1] 13942.2

This works because you have "extracted" the first column of the
"mtcars" data frame _as a data frame_

mtcars[1]
                    mpg
Mazda RX4           21.0
Mazda RX4 Wag       21.0
...
Volvo 142E          21.4

is.data.frame(mtcars[1])
[1] TRUE

mean(mtcars)
[1] NA
Warning message:
In mean.default(mtcars) :
argument is not numeric or logical: returning NA

Here you are trying to take the mean of the entire data frame. While
the mean function will "coerce" a single column to a vector, it won't
do so for an entire data frame. However, if you coerce the data frame
to a matrix:

mean(as.matrix(mtcars))
[1] 39.60853

That's the good news. The bad news is that the result is meaningless.
The same thing goes for:

sd(as.matrix(mtcars))
[1] 84.20792

It is almost always a good idea to read the error messages carefully
and try to understand them.

Jim


On Mon, Nov 14, 2016 at 2:01 PM,  <mokuram at sina.com> wrote:
>  Hi,
> I am working on functions such as sum(), mean() ...
>> sum(mtcars)[1] 13942.2> mean(mtcars)[1] NAWarning message:In mean.default(mtcars) : NA> sd(mtcars)Error in is.data.frame(x) : ()'double'
> why got different reply?Is this a BUG for the current version of R?my version info:version.string R version 3.3.1 (2016-06-21)
>
> Thank you very much for the help.
> mokuram
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ferri.leberl at gmx.at  Mon Nov 14 09:44:10 2016
From: ferri.leberl at gmx.at (Ferri Leberl)
Date: Mon, 14 Nov 2016 09:44:10 +0100
Subject: [R] Frequency of a character in a string
Message-ID: <trinity-bcc6f83f-cf25-4f2d-8e01-78230c379a8e-1479113050372@3capp-gmx-bs45>


Dear All,
Is there a function to count the occurences of a certain character in a string resp. in a vector of strings?
Thank you in advance!
Yours, Ferri


From sezenismail at gmail.com  Mon Nov 14 09:56:45 2016
From: sezenismail at gmail.com (Ismail SEZEN)
Date: Mon, 14 Nov 2016 11:56:45 +0300
Subject: [R] Frequency of a character in a string
In-Reply-To: <trinity-bcc6f83f-cf25-4f2d-8e01-78230c379a8e-1479113050372@3capp-gmx-bs45>
References: <trinity-bcc6f83f-cf25-4f2d-8e01-78230c379a8e-1479113050372@3capp-gmx-bs45>
Message-ID: <07DE66F1-08A4-4C68-8529-DCA2A76A3BD9@gmail.com>


> On 14 Nov 2016, at 11:44, Ferri Leberl <ferri.leberl at gmx.at> wrote:
> 
> 
> Dear All,
> Is there a function to count the occurences of a certain character in a string resp. in a vector of strings?
> Thank you in advance!
> Yours, Ferri
> 

library(stringr)
?str_count


From murdoch.duncan at gmail.com  Mon Nov 14 11:12:49 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 14 Nov 2016 05:12:49 -0500
Subject: [R] Question about expression parser for "return" statement
In-Reply-To: <89968F61-40C5-472B-98CE-F56AF662B0F0@dcn.davis.ca.us>
References: <CAOogVzUOtMUXnvjFg37aGhDqtT9NmULGFiez4_jv1vGYZLDw0g@mail.gmail.com>
	<52831814-c240-04f8-3069-b670552a3941@gmail.com>
	<89968F61-40C5-472B-98CE-F56AF662B0F0@dcn.davis.ca.us>
Message-ID: <c8c4aa4c-76ca-90da-b41a-08d44ebe7f6e@gmail.com>

On 13/11/2016 9:42 PM, Jeff Newmiller wrote:
> I find your response here inconsistent... either including `return` causes a "wasted" function call to occur (same result achieved slower) or the parser has an optimization in it to prevent the wasted function call (only behaviorally the same).

I don't understand what you are finding inconsistent.  I wasn't talking 
about wasting anything.  I was just saying that expressions like

return (a)*b

are evaluated by calling return(a) first, because return() is a 
function, and then they'll never get to the multiplication.

BTW, there don't appear to be many instances of this particular bug in 
CRAN packages, though I don't have a reliable test for it yet.  The most 
common error seems to be using just "return", as mentioned before.  The 
fix for that is to add parens, e.g. "return()".  The next most common is 
something like

invisible(return(x))

which returns x before making it invisible.  The fix for this is to use

return(invisible(x))


> I carefully avoid using the return function in R. Both because using it before the end of a function usually makes the logic harder to follow and because I am under the impression that using it at the end of the function is a small but pointless waste of CPU cycles. That some people might be prone to writing a C-like use of "return;" which causes a function object to be returned only increases my aversion to using it.

Sometimes it is fine to use return(x), but it shouldn't be used routinely.

Duncan Murdoch

> -- Sent from my phone. Please excuse my brevity. On November 13, 2016
> 3:47:10 AM PST, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>> >On 13/11/2016 12:50 AM, Dave DeBarr wrote:
>>> >> I've noticed that if I don't include parentheses around the intended
>> >return
>>> >> value for the "return" statement, R will assume the first
>> >parenthetical
>>> >> expression is the intended return value ... even if that
>> >parenthetical
>>> >> expression is only part of a larger expression.
>>> >>
>>> >> Is this intentional?
>> >
>> >Yes, return is just a function call that has side effects.  As far as
>> >the parser is concerned,
>> >
>> >return ((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance))
>> >
>> >is basically the same as
>> >
>> >f((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance))
>> >
>> >Duncan Murdoch


From lorenzo.isella at gmail.com  Mon Nov 14 12:07:20 2016
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Mon, 14 Nov 2016 12:07:20 +0100
Subject: [R] Discarding Models in Caret During Model Training
Message-ID: <20161114110720.GA6790@chicca>

Dear All,
Maybe some of you has come across this problem.
Let's say that you use caret for hyperparameter tuning.
You train several models and you then select the best performing one
according to some performance metric.
My problem is that, sometimes, I would like to tune really many models
(in the order of hundreds of them). Time is not a problem, but I run
out of memory.
My question is: for any model, its performance is calculated while it
is running. I am only interested in the best performing model (or, to
keep it large, let's say in the 5 best performing models).
Would it be possible to script something that ranks the models while
they are being generated and automatically updates the list of the
best performing 5 models and deletes all the others (for which,
frankly speaking, I have no use).
Is there a flaw in my idea? That would not save me time, but a lot of
memory for sure.
Any suggestion is helpful.
Regards

Lorenzo


From J.delasHeras at ed.ac.uk  Mon Nov 14 14:08:17 2016
From: J.delasHeras at ed.ac.uk (DE LAS HERAS Jose)
Date: Mon, 14 Nov 2016 13:08:17 +0000
Subject: [R] [FORGED] How to remove box in Venn plots (Vennerable
 package, uses grid) - similar to bty="n" in standard plots
In-Reply-To: <9684dcbb-4934-c9be-fa0f-58623edfc525@stat.auckland.ac.nz>
References: <AM3PR05MB1396C5F07B1471C2E98A580DB1BB0@AM3PR05MB1396.eurprd05.prod.outlook.com>,
	<9684dcbb-4934-c9be-fa0f-58623edfc525@stat.auckland.ac.nz>
Message-ID: <AM3PR05MB13965DCEE2E6E02EB134B3D5B1BC0@AM3PR05MB1396.eurprd05.prod.outlook.com>

Hi,


The grid.ls() and grid.remove() approach worked beautifully to remove the box, thank you! Because the box is the first thing to be drawn, it is the first object shown by grid.ls(), so I can easily add a line of code to automatically remove the box. Result!


Although I'd still like to know how one chooses not to plot that box in the first place. I really must study a little the grid package. I've survived using base R plots and they work very nicely, but it looks like you can do a lot of cool stuff with grid.


In case you might know the answer and feel like adding a comment here (I'm already very happy with the grid.ls() approach, thanks! :)) this is a simple code example without making it look pretty or anything:




library(Vennerable)
groups<-list(set1=1:100, set2=80:120)
V<-Venn(groups)
C<-compute.Venn(V)
X11(w=7,h=7)
grid.newpage()
plot(C)

class(C)
[1] "VennDrawing"
attr(,"package")
[1] "Vennerable"

I don't want the black box around the diagram.
I was able to overwrite it with a white box like this:

vp=viewport(x=0.5, y=0.5, width=0.95, height=0.75)
pushViewport(vp)
grid.rect(gp=gpar(lty=1, col="white", lwd=15))
upViewport() # needs to be executed to return focus upwards

but as the box has different dimensions depending on the actual sets being drawn, the width and height must be found empirically each time. A bit boring it you need to produce a bunch of figures at once.

I was wondering what parameter I could include in the call to 'plot' that would prevent the box from being drawn. Usually this is achieved with bty="n", but the method for plotting a "VennDrawing" structure uses the grid package and I'm lost there at the moment.

Thank you for your help again, grid.ls() etc is a very cool and flexible approach

Jose




________________________________
From: Paul Murrell <paul at stat.auckland.ac.nz>
Sent: 13 November 2016 19:57
To: DE LAS HERAS Jose; R-help at r-project.org
Subject: Re: [FORGED] [R] How to remove box in Venn plots (Vennerable package, uses grid) - similar to bty="n" in standard plots

Hi

Can you supply some example code?

You might get some joy from grid.ls() to identify the box followed by
grid.remove() to get rid of it;  some example code would allow me to
provide more detailed advice.

Paul

On 12/11/16 05:12, DE LAS HERAS Jose wrote:
> I'm using the package Vennerable to make Venn diagrams, but it always
> makes a box around the diagram.
>
> Using standard R plots I could eliminate that by indicating
>
>
> bty="n"
>
>
> but it seems Vennerable uses the Grid package to generate its plots
> and I'm not really familiar enough with Grid. I was looking at the
> documentation but I can't seem to find a way to achieve that. I'd
> even be happy drawing a white rectangle with wide lines to overplot
> the box, but there must be a way to not draw the box in the first
> place.
>
>
> Anybody knows how?
>
>
> Jose
>
> --
>
> Dr. Jose I. de las Heras The Wellcome Trust Centre for Cell Biology
> Swann Building Max Born Crescent University of Edinburgh Edinburgh
> EH9 3BF UK
>
> Phone: +44 (0)131 6507090
>
> Fax:  +44 (0)131 6507360
>
>
>
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
>
>
> ______________________________________________ R-help at r-project.org
> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
R-help -- Main R Mailing List: Primary help - Homepage - SfS<https://stat.ethz.ch/mailman/listinfo/r-help>
stat.ethz.ch
The main R mailing list, for announcements about the development of R and the availability of new code, questions and answers about problems and solutions using R ...



> posting guide http://www.R-project.org/posting-guide.html and provide
> commented, minimal, self-contained, reproducible code.
>

--
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/
Paul Murrell's Home Page<http://www.stat.auckland.ac.nz/~paul/>
www.stat.auckland.ac.nz
Department. My department home page. Research. The home page for R: A language and environment for computing and graphics (my R graphics todo list).



-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20161114/763f5215/attachment.pl>

From venkynov10 at gmail.com  Mon Nov 14 14:16:22 2016
From: venkynov10 at gmail.com (Venky)
Date: Mon, 14 Nov 2016 18:46:22 +0530
Subject: [R] Text categories based on the sentences
Message-ID: <CAAM-fZ5UWQ1M61E4=VF-HqQ=6E7E2wAL8opBtCidY=NJjcSAxg@mail.gmail.com>

Hi team,

I have data set contains one variable "*Description*"

*Description**                                                  Category*

1. i want ice cream                                         food
2. i like banana very much                              fruit
3. tomorrow i will eat chicken                          food
4. yesterday i went to birthday party                festival
5. i lost my mobile last week                           mobile

Please remember that i have only "*Description*" Variables only.How can i
get the categories column based on the sentences of *Description *column.

kindly do the needful help for that

Advance in Thanks.






Thanks and Regards
Venkatesan

	[[alternative HTML version deleted]]


From berdore at gmail.com  Mon Nov 14 11:46:04 2016
From: berdore at gmail.com (=?UTF-8?Q?Bernardo_Dor=C3=A9?=)
Date: Mon, 14 Nov 2016 08:46:04 -0200
Subject: [R] Function argument and scope
Message-ID: <CAEqmCMQSQ+JOh71W1xCL05sM6jKZSzgTfvZ0WEqP2O+=iKZUog@mail.gmail.com>

Thank you all for replying so quickly.

@Jim
You are right, I ran into that. You can see as.character() being called to
remedy the situation you described. I dropped the factors from the data
frame in a line outside the function. Creating the dataframe with
stringsAsFactors = F is the easiest way to go. Tks.

@Thomas, @Jeremiah
R really creates a copy and forgets the reference. That was clearly visible
in the debugger. In an earlier version of this I was returning a list of
the compute values. I will go back to that.

Tks a lot for helping me with this.

Bernardo Dor?



>
> On Sun, Nov 13, 2016 at 2:09 PM, Bernardo Dor? <berdore at gmail.com> wrote:
>
>> Hello list,
>>
>> my first post but I've been using this list as a help source for a while.
>> Couldn't live without it.
>>
>> I am writing a function that takes a dataframe as an argument and in the
>> end I intend to assign the result of some computation back to the
>> dataframe. This is what I have so far:
>>
>> myFunction <- function(x){
>>   y <- x[1,1]
>>   z <- strsplit(as.character(y), split = " ")
>>   if(length(z[[1]] > 1)){
>>     predictedWord <- z[[1]][length(z[[1]])]
>>     z <- z[[1]][-c(length(z[[1]]))]
>>     z <- paste(z, collapse = " ")
>>   }
>>   x[1,1] <- z
>> }
>>
>> And lets say I create my dataframe like this:
>> test <- data.frame(var1=c("a","b","c"),var2=c("d","e","f"))
>>
>> and then call
>> myFunction(test)
>>
>> The problem is when I assign x[1,1] to y in the first operation inside the
>> function, x becomes a dataframe inside the function scope and loses the
>> reference to the dataframe "test" passed as argument. In the end when I
>> assign z to what should be row 1 and column 1 of the "test" dataframe, it
>> assigns to x inside the function scope and no modification is made on
>> "test".
>>
>> I hope the problem statement is clear.
>>
>> Thank you,
>>
>> Bernardo Dor?
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>

	[[alternative HTML version deleted]]


From brijeshkmishra at gmail.com  Mon Nov 14 14:52:50 2016
From: brijeshkmishra at gmail.com (Brijesh Mishra)
Date: Mon, 14 Nov 2016 19:22:50 +0530
Subject: [R] Frequency of a character in a string
In-Reply-To: <07DE66F1-08A4-4C68-8529-DCA2A76A3BD9@gmail.com>
References: <trinity-bcc6f83f-cf25-4f2d-8e01-78230c379a8e-1479113050372@3capp-gmx-bs45>
	<07DE66F1-08A4-4C68-8529-DCA2A76A3BD9@gmail.com>
Message-ID: <CAHUdqRvjOGPVKdm4DX7HkeXWtec-yneD-dK_YfgDnpyUH242tQ@mail.gmail.com>

?nchar in the base R should also help...

On Mon, Nov 14, 2016 at 2:26 PM, Ismail SEZEN <sezenismail at gmail.com> wrote:

>
> > On 14 Nov 2016, at 11:44, Ferri Leberl <ferri.leberl at gmx.at> wrote:
> >
> >
> > Dear All,
> > Is there a function to count the occurences of a certain character in a
> string resp. in a vector of strings?
> > Thank you in advance!
> > Yours, Ferri
> >
>
> library(stringr)
> ?str_count
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From Vincent.Goulet at act.ulaval.ca  Mon Nov 14 15:06:23 2016
From: Vincent.Goulet at act.ulaval.ca (Vincent Goulet)
Date: Mon, 14 Nov 2016 14:06:23 +0000
Subject: [R] [R-pkgs] Major update of package actuar
Message-ID: <C9ED9D22-1783-450B-9C81-D7AB6ECC7E0B@act.ulaval.ca>

Dear useRs,

I'm happy to announce a substantial update of package actuar that bumps the version number to 2.0-0. This release focuses on additional support for continuous and discrete distributions, new functions to simulate data from compound models and mixtures, and revised and improved documentation.

A slightly shortened version of the NEWS file follows:

NEW FEATURES

? Support for the inverse Gaussian distribution. The pdf, cdf and
  quantile functions are C (read: faster) implementations of otherwise 
  equivalent functions in package ?statmod?.

? Support for the Gumbel extreme value distribution.

? Extended range of admissible values for many limited
  expected value functions thanks to new C-level functions
  ?expint?, ?betaint? and ?gammaint?. These provide special
  integrals presented in the introduction of Appendix A of
  Klugman et al. (2012); see also ?vignette("distributions")?.

  Affected functions are: ?levtrbeta?, ?levgenpareto?,
  ?levburr?, ?levinvburr?, ?levpareto?, ?levinvpareto?,
  ?levllogis?, ?levparalogis?, ?levinvparalogis? in the
  Transformed Beta family, and ?levinvtrgamma?, ?levinvgamma?,
  ?levinvweibull? in the Transformed Gamma family.

? Functions ?expint?, ?betaint? and ?gammaint? to compute
  the special integrals mentioned above. These are merely
  convenience R interfaces to the C level functions. They are
  _not_ exported by the package.

? Support for the Poisson-inverse Gaussian discrete distribution.

? Support for the logarithmic (or log-series) and zero-modified
  logarithmic distributions.

? Support for the zero-truncated and zero-modified Poisson
  distributions.

? Support for the zero-truncated and zero-modified negative 
  binomial distributions.

? Support for the zero-truncated and zero-modified geometric
  distributions.

? Support for the zero-truncated and zero-modified binomial
  distributions.

? New vignette ?"distributions"? that reviews in great detail the
  continuous and discrete distributions provided in the
  package, along with implementation details.

? ?aggregateDist? now accepts ?"zero-truncated binomial"?,
  ?"zero-truncated geometric"?, ?"zero-truncated negative
  binomial"?, ?"zero-truncated poisson"?, ?"zero-modified
  binomial"?, ?"zero-modified geometric"?, ?"zero-modified
  negative binomial"?, ?"zero-modified poisson"? and
  ?"zero-modified logarithmic"? for argument ?model.freq? with
  the ?"recursive"? method.

? New function ?rmixture? to generate random variates from
  discrete mixtures, that is from random variables with
  densities of the form f(x) = p_1 f_1(x) + ... + p_n f_n(x).

? New function ?rcompound? to generate random variates from (non
  hierarchical) compound models of the form S = X_1 + ... + X_N.
  Function ?simul? could already do that, but ?rcompound? is
  substantially faster for non hierarchical models.

? New function 'rcomppois' that is a simplified version of
  ?rcompound? for the very common compound Poisson case.

? Function ?simul? now accepts an atomic (named or not) vector for
  argument ?nodes? when simulating from a non hierarchical
  compound model. But really, one should use ?rcompound? for
  such cases.

? New alias ?rcomphierarc? for ?simul? that better fits within
  the usual naming scheme of random generation functions.

? Functions ?grouped.data? and ?ogive? now accept individual
  data in argument. The former will group the data using
  ?hist? (therefore, all the algorithms to compute the number
  of breakpoints available in ?hist? are also available in
  ?grouped.data?). ?ogive? will first create a grouped data
  object and then compute the ogive.

  While there is no guarantee that the two functions are
  backward compatible (the number and position of the
  arguments have changed), standard calls should not be
  affected.

USER VISIBLE CHANGES

? The material on probability laws in vignette ?"lossdist"?
  has been moved to the new vignette ?"distributions"? (see
  the previous section).

? The first argument of the ?mgf<dist>? functions has changed
  from ?x? to ?t?. This is a more common notation for moment
  generating functions.

? In ?aggregateDist? with the ?"recursive"? method, if the
  length of ?p0? is greater than one, only the first element
  is used, with a warning.

? ?aggregateDist? with the ?"recursive"? method and
  ?model.freq = "logarithmic"? now uses the new ?dlogarithmic?
  family of functions. Therefore, parametrization has changed
  from the one of Klugman et al. (2012) to the standard
  parametrization for the logarithmic distribution. Basically,
  any value of ?prob? for the logarithmic parameter in
  previous versions of ?actuar? should now be ?1 - prob?.

? The aim of vignette ?"simulation"? is changed from
  ?simulation of compound hierarchical models? to ?simulation
  of insurance data with ?actuar?? as it also covers the new
  functions ?rmixture? and ?rcompound?.

? Vignette ?"lossdist"? is renamed to ?"modeling"? and it is
  revised to cover the new functionalities of ?grouped.data?
  and ?ogive?.

BUG FIX

?  An old and nasty out-of-bounds bug could crash R when using
  the "recursive" method of 'aggregateDist' with a frequency
  distribution from the (a, b, 1) family.


I hope this update will prove useful.

Vincent Goulet, Ph.D.
Professeur titulaire
?cole d'actuariat, Universit? Laval

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages

From janekim at zenithn.com  Mon Nov 14 09:00:52 2016
From: janekim at zenithn.com (=?ks_c_5601-1987?B?sei8vMjx?=)
Date: Mon, 14 Nov 2016 08:00:52 +0000
Subject: [R] =?ks_c_5601-1987?q?Question_about_=A1=AEThe_R_Project=A1=AF?=
	=?ks_c_5601-1987?q?=2E?=
Message-ID: <SG2PR04MB07104C85853D6DDCF5FBCA4AB0BC0@SG2PR04MB0710.apcprd04.prod.outlook.com>

Hello,

I??m Jane Kim from Zenith and Company.

We have a question about ??The R Project??.

It looks like it??s an open source software, but the document from the website shows that it??s free of use not free of price.

Please, confirm us the if it cost fees to use it for commercial use.

If needed, could you inform us the price for it, too?

Best regards,
Jane Kim.

________________________________
Zenith&Company?? ???????? ???? ???? ???? ?????? ?????????? ???????? ????????. ?? ???????? ?????????? ???????? ???? ?? ??????, ???? ?????? ?????? ???? ?????? ???? ?????????? ?????? ???? ?????? ???????? ?????? ?? ????????. ???? ???????? ???? ???? ?? ?????? ????, ????, ?????? ?????? ?????? ?????? ???? ?????? ???? ???? ???? ?????????? ?????????? ?????? ?????? ????????.
The Zenith&Company does not collect and distribute personal information without the consent of the recipient. This e-mail may contain confidential information and content of this e-mail (including any attachments) is strictly confidential and may be commercially sensitive. If you are not the named addressee, you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received it by mistake and delete it from your system.

	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Mon Nov 14 17:06:44 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Mon, 14 Nov 2016 16:06:44 +0000
Subject: [R] Principle Component Analysis: Ranking Animal Size Based On
 Combined Metrics
In-Reply-To: <BLUPR0101MB1474BE9248D293A65F3A7F3BABBC0@BLUPR0101MB1474.prod.exchangelabs.com>
References: <BLUPR0101MB147499AE1D09AA1DD4EB4A54ABBD0@BLUPR0101MB1474.prod.exchangelabs.com>
	<CA+8X3fVrqY9447vuiRPbeAmnrZ01AZ8Kvp5nQHbQ4uDr5s8Sfw@mail.gmail.com>
	<BLUPR0101MB1474BE9248D293A65F3A7F3BABBC0@BLUPR0101MB1474.prod.exchangelabs.com>
Message-ID: <bfec840522ef498b9838c6dc49119b3d@exch-2p-mbx-w2.ads.tamu.edu>

The first principal component should be your estimate of "size" since it captures the correlations between all 4 variables. The second principle component must be orthogonal to the first so that if the first is "size", the second pc is independent of size, perhaps some measure of "shape". As would be expected, the first principal component is highly correlated with the geometric mean of the three linear measurements and moderately correlated with weight:

> gm <- apply(df[, -1], 1, prod)^(1/3)
> pc1 <- prcomp(df, scale.=TRUE)$x[, 1]
> plot(pc1, gm)
> cor(cbind(pc1, gm, wgt=df$weight))
           pc1         gm        wgt
pc1  1.0000000 -0.9716317 -0.5943594
gm  -0.9716317  1.0000000  0.3967369
wgt -0.5943594  0.3967369  1.0000000

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Sidoti, Salvatore A.
Sent: Sunday, November 13, 2016 7:38 PM
To: Jim Lemon; r-help mailing list
Subject: Re: [R] Principle Component Analysis: Ranking Animal Size Based On Combined Metrics

Hi Jim,

Nice to see you again! First of all, apologies to all for bending the rules a bit with respect to the mailing list. I know this is a list for R programming specifically, and I have received some great advice in this regard in the past. I just thought this was an interesting applied problem that would generate some discussion about PCA in R.

Yes, that is an excellent question! Indeed, why not just volume? Since this is still a work in progress and we have not published as of yet, I would rather not be more specific about the type of animal at this time ;>}. Nonetheless, I can say that the animals I study change "size" depending on their feeding and hydration state. The abdomen in particular undergoes drastic size changes. That being said, there are key anatomical features that remain fixed in the adult.

Now, there *might* be a way to work volume into the PCA. Although volume is not a reliable metric since the abdomen size is so changeable while the animal is alive, but what about preserved specimens? I have many that have been marinating in ethanol for months. Wouldn't the tissues have equilibrated by now? Probably... I could measure volume by displacement or suspension, I suppose.

In the meantime, here's a few thoughts:

1) 	Use the contribution % (known as C% hereafter) of each variable on principle components 1 and 2.

2) 	The total contribution of a variable that explains the variations retained by PC1 an PC2 is calculated by:
	
	sum(C%1 * eigenvalue1, C%2 * eigenvalue2)

3) Scale() to mean-center the columns of the data set.

4) Use these total contributions as the weights of an arithmetic mean.

For example, we have an animal with the following data (mean-centered):
weight:	1.334
interoc:	-0.225
clength:	0.046
cwidth:	-0.847

The contributions of these variables on PC1 and PC2 are (% changed to proportions):
weight:	0.556
interoc:	0.357
clength:	0.493
cwidth:	0.291

To calculate size:
1.334(0.556) - 0.225(0.357) + 0.046(0.493) - 0.847(0.291) = 0.43758
Then divide by the sum of the weights:
0.43758 / 1.697 = 0.257855 = "animal size"

This value can then be used to rank the animal according to its size for further analysis...

Does this sound like a reasonable application of my PCA data?

Salvatore A. Sidoti
PhD Student
Behavioral Ecology

-----Original Message-----
From: Jim Lemon [mailto:drjimlemon at gmail.com] 
Sent: Sunday, November 13, 2016 3:53 PM
To: Sidoti, Salvatore A. <sidoti.23 at buckeyemail.osu.edu>; r-help mailing list <r-help at r-project.org>
Subject: Re: [R] Principle Component Analysis: Ranking Animal Size Based On Combined Metrics

Hi Salvatore,
If by "size" you mean volume, why not directly measure the volume of your animals? They appear to be fairly small. Sometimes working out what the critical value actually means can inform the way to measure it.

Jim


On Sun, Nov 13, 2016 at 4:46 PM, Sidoti, Salvatore A.
<sidoti.23 at buckeyemail.osu.edu> wrote:
> Let's say I perform 4 measurements on an animal: three are linear measurements in millimeters and the fourth is its weight in milligrams. So, we have a data set with mixed units.
>
> Based on these four correlated measurements, I would like to obtain one "score" or value that describes an individual animal's size. I considered simply taking the geometric mean of these 4 measurements, and that would give me a "score" - larger values would be for larger animals, etc.
>
> However, this assumes that all 4 of these measurements contribute equally to an animal's size. Of course, more than likely this is not the case. I then performed a PCA to discover how much influence each variable had on the overall data set. I was hoping to use this analysis to refine my original approach.
>
> I honestly do not know how to apply the information from the PCA to this particular problem...
>
> I do know, however, that principle components 1 and 2 capture enough of the variation to reduce the number of dimensions down to 2 (see analysis below with the original data set).
>
> Note: animal weights were ln() transformed to increase correlation with the 3 other variables.
>
> df <- data.frame(
>   weight = log(1000*c(0.0980, 0.0622, 0.0600, 0.1098, 0.0538, 0.0701, 0.1138, 0.0540, 0.0629, 0.0930,
>              0.0443, 0.1115, 0.1157, 0.0734, 0.0616, 0.0640, 0.0480, 0.1339, 0.0547, 0.0844,
>              0.0431, 0.0472, 0.0752, 0.0604, 0.0713, 0.0658, 0.0538, 0.0585, 0.0645, 0.0529,
>              0.0448, 0.0574, 0.0577, 0.0514, 0.0758, 0.0424, 0.0997, 0.0758, 0.0649, 0.0465,
>              0.0748, 0.0540, 0.0819, 0.0732, 0.0725, 0.0730, 0.0777, 0.0630, 0.0466)),
>   interoc = c(0.853, 0.865, 0.811, 0.840, 0.783, 0.868, 0.818, 0.847, 0.838, 0.799,
>               0.737, 0.788, 0.731, 0.777, 0.863, 0.877, 0.814, 0.926, 0.767, 0.746,
>               0.700, 0.768, 0.807, 0.753, 0.809, 0.788, 0.750, 0.815, 0.757, 0.737,
>               0.759, 0.863, 0.747, 0.838, 0.790, 0.676, 0.857, 0.728, 0.743, 0.870,
>               0.787, 0.773, 0.829, 0.785, 0.746, 0.834, 0.829, 0.750, 0.842),
>   cwidth = c(3.152, 3.046, 3.139, 3.181, 3.023, 3.452, 2.803, 3.050, 3.160, 3.186,
>              2.801, 2.862, 3.183, 2.770, 3.207, 3.188, 2.969, 3.033, 2.972, 3.291,
>              2.772, 2.875, 2.978, 3.094, 2.956, 2.966, 2.896, 3.149, 2.813, 2.935,
>              2.839, 3.152, 2.984, 3.037, 2.888, 2.723, 3.342, 2.562, 2.827, 2.909,
>              3.093, 2.990, 3.097, 2.751, 2.877, 2.901, 2.895, 2.721, 2.942),
>   clength = c(3.889, 3.733, 3.762, 4.059, 3.911, 3.822, 3.768, 3.814, 3.721, 3.794,
>               3.483, 3.863, 3.856, 3.457, 3.996, 3.876, 3.642, 3.978, 3.534, 3.967,
>               3.429, 3.518, 3.766, 3.755, 3.706, 3.785, 3.607, 3.922, 3.453, 3.589,
>               3.508, 3.861, 3.706, 3.593, 3.570, 3.341, 3.916, 3.336, 3.504, 3.688,
>               3.735, 3.724, 3.860, 3.405, 3.493, 3.586, 3.545, 3.443, 
> 3.640))
>
> pca_morpho <- princomp(df, cor = TRUE)
>
> summary(pca_morpho)
>
> Importance of components:
>                                         Comp.1          Comp.2          Comp.3          Comp.4
> Standard deviation      1.604107        0.8827323       0.7061206       0.3860275
> Proportion of Variance  0.643290        0.1948041       0.1246516       0.0372543
> Cumulative Proportion   0.643290        0.8380941       0.9627457       1.0000000
>
> Loadings:
>                         Comp.1  Comp.2  Comp.3  Comp.4
> weight          -0.371          0.907                           -0.201
> interoc         -0.486  -0.227  -0.840
> cwidth          -0.537  -0.349          0.466           -0.611
> clength         -0.582                          0.278   0.761
>
>                         Comp.1  Comp.2  Comp.3  Comp.4
> SS loadings             1.00            1.00            1.00            1.00
> Proportion Var          0.25            0.25            0.25            0.25
> Cumulative Var          0.25            0.50            0.75            1.00
>
> Any guidance will be greatly appreciated!
>
> Salvatore A. Sidoti
> PhD Student
> The Ohio State University
> Behavioral Ecology
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Mon Nov 14 17:09:58 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 14 Nov 2016 08:09:58 -0800
Subject: [R] Frequency of a character in a string
In-Reply-To: <CAHUdqRvjOGPVKdm4DX7HkeXWtec-yneD-dK_YfgDnpyUH242tQ@mail.gmail.com>
References: <trinity-bcc6f83f-cf25-4f2d-8e01-78230c379a8e-1479113050372@3capp-gmx-bs45>
	<07DE66F1-08A4-4C68-8529-DCA2A76A3BD9@gmail.com>
	<CAHUdqRvjOGPVKdm4DX7HkeXWtec-yneD-dK_YfgDnpyUH242tQ@mail.gmail.com>
Message-ID: <CAGxFJbS8PZP7frCA_v74q53kqY8bLHwfi-HGp_gjfYbm7_cK1g@mail.gmail.com>

Yes, but it need some help, since nchar gives the length of the
*entire* string; e.g.

## to count "a" 's  :

> x <-(c("abbababba","bbabbabbaaaba"))
> nchar(gsub("[^a]","",x))
[1] 4 6

This is one of about 8 zillion ways to do this in base R if you don't
want to use a specialized package.

Just for curiosity: Can anyone comment on what is the most efficient
way to do this using base R pattern matching?

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Nov 14, 2016 at 5:52 AM, Brijesh Mishra
<brijeshkmishra at gmail.com> wrote:
> ?nchar in the base R should also help...
>
> On Mon, Nov 14, 2016 at 2:26 PM, Ismail SEZEN <sezenismail at gmail.com> wrote:
>
>>
>> > On 14 Nov 2016, at 11:44, Ferri Leberl <ferri.leberl at gmx.at> wrote:
>> >
>> >
>> > Dear All,
>> > Is there a function to count the occurences of a certain character in a
>> string resp. in a vector of strings?
>> > Thank you in advance!
>> > Yours, Ferri
>> >
>>
>> library(stringr)
>> ?str_count
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sezenismail at gmail.com  Mon Nov 14 17:12:34 2016
From: sezenismail at gmail.com (Ismail SEZEN)
Date: Mon, 14 Nov 2016 19:12:34 +0300
Subject: [R] =?utf-8?b?UXVlc3Rpb24gYWJvdXQg4oCYVGhlIFIgUHJvamVjdOKAmS4=?=
In-Reply-To: <SG2PR04MB07104C85853D6DDCF5FBCA4AB0BC0@SG2PR04MB0710.apcprd04.prod.outlook.com>
References: <SG2PR04MB07104C85853D6DDCF5FBCA4AB0BC0@SG2PR04MB0710.apcprd04.prod.outlook.com>
Message-ID: <33164EE7-B4B5-44AA-B0A0-7CEB6114D31B@gmail.com>


> On 14 Nov 2016, at 11:00, ??? <janekim at zenithn.com> wrote:
> 
> Hello,
> 
> I?m Jane Kim from Zenith and Company.
> 
> We have a question about ?The R Project?.
> 
> It looks like it?s an open source software, but the document from the website shows that it?s free of use not free of price.
> 
> Please, confirm us the if it cost fees to use it for commercial use.
> 
> If needed, could you inform us the price for it, too?
> 
> Best regards,
> Jane Kim.
> 

Can I use R for commercial purposes?
https://cran.r-project.org/doc/FAQ/R-FAQ.html#Can-I-use-R-for-commercial-purposes_003f <https://cran.r-project.org/doc/FAQ/R-FAQ.html#Can-I-use-R-for-commercial-purposes_003f>

If you mean RStudio you have to pay for commercial use. RStudio and R are different.
https://www.rstudio.com/pricing/ <https://www.rstudio.com/pricing/>




	[[alternative HTML version deleted]]


From john.archie.mckown at gmail.com  Mon Nov 14 17:51:44 2016
From: john.archie.mckown at gmail.com (John McKown)
Date: Mon, 14 Nov 2016 10:51:44 -0600
Subject: [R] =?utf-8?b?UXVlc3Rpb24gYWJvdXQg4oCYVGhlIFIgUHJvamVjdOKAmS4=?=
In-Reply-To: <SG2PR04MB07104C85853D6DDCF5FBCA4AB0BC0@SG2PR04MB0710.apcprd04.prod.outlook.com>
References: <SG2PR04MB07104C85853D6DDCF5FBCA4AB0BC0@SG2PR04MB0710.apcprd04.prod.outlook.com>
Message-ID: <CAAJSdjgMJ0nmHv=a-z-59_=vUbsbYDH_pSpdBxz4N-zPy6A3-w@mail.gmail.com>

On Mon, Nov 14, 2016 at 2:00 AM, ??? <janekim at zenithn.com> wrote:

> Hello,
>
> I?m Jane Kim from Zenith and Company.
>
> We have a question about ?The R Project?.
>
> It looks like it?s an open source software, but the document from the
> website shows that it?s free of use not free of price.
>
> Please, confirm us the if it cost fees to use it for commercial use.


> If needed, could you inform us the price for it, too?
>
> Best regards,
> Jane Kim.
>
>
?According to
https://cran.r-project.org/doc/FAQ/R-FAQ.html#Can-I-use-R-for-commercial-purposes_003f?
, R is released under the "GNU Public License" version 2.0. This means it
is "libre" (the source is available). Also, it is "gratis" (it is free from
cost to use, although it is permissible to charge a one-time "distribution"
fee - such as might be done if you want someone to mail you a DVD with the
source on it.)

?[quote]

2.11 Can I use R for commercial purposes?

R is released under the GNU General Public License (GPL), version 2
<https://www.gnu.org/licenses/old-licenses/gpl-2.0.html>. If you have any
questions regarding the legality of using R in any particular situation you
should bring it up with your legal counsel. We are in no position to offer
legal advice.

It is the opinion of the R Core Team that one can use R for commercial
purposes (e.g., in business or in consulting). The GPL, like all Open
Source licenses, permits all and any use of the package. It only restricts
distribution of R or of other programs containing code from R. This is made
clear in clause 6 (?No Discrimination Against Fields of Endeavor?) of the Open
Source Definition <https://opensource.org/docs/definition.html>:

The license must not restrict anyone from making use of the program in a
specific field of endeavor. For example, it may not restrict the program
from being used in a business, or from being used for genetic research.

It is also explicitly stated in clause 0 of the GPL, which says in part

Activities other than copying, distribution and modification are not
covered by this License; they are outside its scope. The act of running the
Program is not restricted, and the output from the Program is covered only
if its contents constitute a work based on the Program.

Most add-on packages, including all recommended ones, also explicitly allow
commercial use in this way. A few packages are restricted to
?non-commercial use?; you should contact the author to clarify whether
these may be used or seek the advice of your legal counsel.

None of the discussion in this section constitutes legal advice. The R Core
Team does not provide legal advice under any circumstances.

[quote/]?



-- 
Heisenberg may have been here.

Unicode: http://xkcd.com/1726/

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From h.wickham at gmail.com  Mon Nov 14 18:00:44 2016
From: h.wickham at gmail.com (Hadley Wickham)
Date: Mon, 14 Nov 2016 11:00:44 -0600
Subject: [R] =?utf-8?b?UXVlc3Rpb24gYWJvdXQg4oCYVGhlIFIgUHJvamVjdOKAmS4=?=
In-Reply-To: <33164EE7-B4B5-44AA-B0A0-7CEB6114D31B@gmail.com>
References: <SG2PR04MB07104C85853D6DDCF5FBCA4AB0BC0@SG2PR04MB0710.apcprd04.prod.outlook.com>
	<33164EE7-B4B5-44AA-B0A0-7CEB6114D31B@gmail.com>
Message-ID: <CABdHhvHiY6NpsEY0oVbzQ=J-+w=5t1FLh0Ww6-o+higurjSVGQ@mail.gmail.com>

>> We have a question about ?The R Project?.
>>
>> It looks like it?s an open source software, but the document from the website shows that it?s free of use not free of price.
>>
>> Please, confirm us the if it cost fees to use it for commercial use.
>>
>> If needed, could you inform us the price for it, too?
>>
>> Best regards,
>> Jane Kim.
>>
>
> Can I use R for commercial purposes?
> https://cran.r-project.org/doc/FAQ/R-FAQ.html#Can-I-use-R-for-commercial-purposes_003f <https://cran.r-project.org/doc/FAQ/R-FAQ.html#Can-I-use-R-for-commercial-purposes_003f>
>
> If you mean RStudio you have to pay for commercial use. RStudio and R are different.
> https://www.rstudio.com/pricing/ <https://www.rstudio.com/pricing/>

That's not true, as RStudio is also open source.  You don't have to
pay to use it commercially, but you might want to pay to use it
commercial because we provide additional features of use to people in
bigger companies.

Hadley

-- 
http://hadley.nz


From murdoch.duncan at gmail.com  Mon Nov 14 18:22:31 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 14 Nov 2016 12:22:31 -0500
Subject: [R] Question about expression parser for "return" statement
In-Reply-To: <1479140805.2360.4.camel@ecu.edu>
References: <CAOogVzUOtMUXnvjFg37aGhDqtT9NmULGFiez4_jv1vGYZLDw0g@mail.gmail.com>
	<52831814-c240-04f8-3069-b670552a3941@gmail.com>
	<665597dc-55fd-9916-6564-38400262a270@gmail.com>
	<90a8ce98-ffe5-0484-e5c1-1e0fd6bc332c@gmail.com>
	<1479140805.2360.4.camel@ecu.edu>
Message-ID: <e743c77a-b536-dcb7-ee34-723d3660af77@gmail.com>

On 14/11/2016 11:26 AM, Wolf, Steven wrote:
> Just to add on a bit, please note that the return is superfluous.  If 
> you write this:
>
> normalDensityFunction = function(x, Mean, Variance) {
>      # no  "return" value given at all
>      (1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance)
> }
> normalDensityFunction(2,0,1)
>
> ...you get the right answer again.
>
> This is not "best practices", and Duncan will probably give you 10 
> reasons why you should never do it this way.  But if the parentheses 
> behavior bothers you enough, you can subvert it.  This probably won't 
> work so well if you try to make any more complicated output.

Why do you say that's not best practice?  I would say that's preferable 
to an explicit return().

Duncan
>
> Caveat Emptor.
>
> -SW
>
> -- 
> Steven Wolf, PhD
> Assistant Professor
> Department of Physics
> STEM CoRE -- STEM Collaborative for Research in Education
> http://www.ecu.edu/cs-acad/aa/StemCore
> East Carolina University
> Phone: 252-737-5229
>
>
>
> On Sun, 2016-11-13 at 13:35 -0500, Duncan Murdoch wrote:
>> On 13/11/2016 7:58 AM, Duncan Murdoch wrote:
>>> On 13/11/2016 6:47 AM, Duncan Murdoch wrote:
>>>> On 13/11/2016 12:50 AM, Dave DeBarr wrote:
>>>>> I've noticed that if I don't include parentheses around the 
>>>>> intended return value for the "return" statement, R will assume 
>>>>> the first parenthetical expression is the intended return value 
>>>>> ... even if that parenthetical expression is only part of a larger 
>>>>> expression. Is this intentional? 
>>>> Yes, return is just a function call that has side effects. As far 
>>>> as the parser is concerned, return 
>>>> ((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance)) is 
>>>> basically the same as f((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - 
>>>> Mean)^2)/Variance)) 
>>> By the way, out of curiosity I took a look at the source of CRAN 
>>> packages to see if this actually occurs. It turns out that "return" 
>>> is used as a variable name often enough to make automatic tests 
>>> tricky, so I don't know the answer to my question. However, I did 
>>> turn up a number of cases where people have code like this: if (name 
>>> == "") return; (from the bio.infer package), which never calls 
>>> return(), so doesn't actually do what the author likely intended 
>>
>>
>> I searched the R sources and the sources of CRAN packages, and found
>> this is a reasonably common problem:  it's in 111 packages, including
>> one in base R.  I'll be emailing the maintainers to let them know.
>>
>> I'll see about putting a check for this into R CMD check.
>>
>> Duncan Murdoch
>>
>>


From ccberry at ucsd.edu  Mon Nov 14 18:26:13 2016
From: ccberry at ucsd.edu (Charles C. Berry)
Date: Mon, 14 Nov 2016 09:26:13 -0800
Subject: [R] Frequency of a character in a string
In-Reply-To: <CAGxFJbS8PZP7frCA_v74q53kqY8bLHwfi-HGp_gjfYbm7_cK1g@mail.gmail.com>
References: <trinity-bcc6f83f-cf25-4f2d-8e01-78230c379a8e-1479113050372@3capp-gmx-bs45>
	<07DE66F1-08A4-4C68-8529-DCA2A76A3BD9@gmail.com>
	<CAHUdqRvjOGPVKdm4DX7HkeXWtec-yneD-dK_YfgDnpyUH242tQ@mail.gmail.com>
	<CAGxFJbS8PZP7frCA_v74q53kqY8bLHwfi-HGp_gjfYbm7_cK1g@mail.gmail.com>
Message-ID: <alpine.OSX.2.20.1611140916100.809@charles-berrys-macbook.local>

On Mon, 14 Nov 2016, Bert Gunter wrote:

> Yes, but it need some help, since nchar gives the length of the
> *entire* string; e.g.
>
> ## to count "a" 's  :
>
>> x <-(c("abbababba","bbabbabbaaaba"))
>> nchar(gsub("[^a]","",x))
> [1] 4 6
>
> This is one of about 8 zillion ways to do this in base R if you don't
> want to use a specialized package.
>
> Just for curiosity: Can anyone comment on what is the most efficient
> way to do this using base R pattern matching?
>

Most efficient? There probably is no uniformly most efficient way to do 
this as the timing will depend on the distribution of "a" in the atoms of 
any vector as well as the length of the vector.

But here is one way to avoid the regular expression matching:

lengths(strsplit(paste0("X", x, "X"),"a",fixed=TRUE)) - 1


Chuck


From wolfste4 at msu.edu  Mon Nov 14 18:26:42 2016
From: wolfste4 at msu.edu (Wolf, Steven)
Date: Mon, 14 Nov 2016 17:26:42 +0000
Subject: [R] Question about expression parser for "return" statement
In-Reply-To: <e743c77a-b536-dcb7-ee34-723d3660af77@gmail.com>
References: <CAOogVzUOtMUXnvjFg37aGhDqtT9NmULGFiez4_jv1vGYZLDw0g@mail.gmail.com>
	<52831814-c240-04f8-3069-b670552a3941@gmail.com>
	<665597dc-55fd-9916-6564-38400262a270@gmail.com>
	<90a8ce98-ffe5-0484-e5c1-1e0fd6bc332c@gmail.com>
	<1479140805.2360.4.camel@ecu.edu>
	<e743c77a-b536-dcb7-ee34-723d3660af77@gmail.com>
Message-ID: <1479144402.2360.36.camel@campusad.msu.edu>

I stand corrected.  I have been chided in the past for not explicitly returning my output by someone claiming it is not best practices.

-Steve

On Mon, 2016-11-14 at 12:22 -0500, Duncan Murdoch wrote:

On 14/11/2016 11:26 AM, Wolf, Steven wrote:


Just to add on a bit, please note that the return is superfluous.  If
you write this:

normalDensityFunction = function(x, Mean, Variance) {
     # no  "return" value given at all
     (1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance)
}
normalDensityFunction(2,0,1)

...you get the right answer again.

This is not "best practices", and Duncan will probably give you 10
reasons why you should never do it this way.  But if the parentheses
behavior bothers you enough, you can subvert it.  This probably won't
work so well if you try to make any more complicated output.



Why do you say that's not best practice?  I would say that's preferable
to an explicit return().

Duncan



Caveat Emptor.

-SW



	[[alternative HTML version deleted]]


From marc_schwartz at me.com  Mon Nov 14 18:48:18 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Mon, 14 Nov 2016 11:48:18 -0600
Subject: [R] Frequency of a character in a string
In-Reply-To: <alpine.OSX.2.20.1611140916100.809@charles-berrys-macbook.local>
References: <trinity-bcc6f83f-cf25-4f2d-8e01-78230c379a8e-1479113050372@3capp-gmx-bs45>
	<07DE66F1-08A4-4C68-8529-DCA2A76A3BD9@gmail.com>
	<CAHUdqRvjOGPVKdm4DX7HkeXWtec-yneD-dK_YfgDnpyUH242tQ@mail.gmail.com>
	<CAGxFJbS8PZP7frCA_v74q53kqY8bLHwfi-HGp_gjfYbm7_cK1g@mail.gmail.com>
	<alpine.OSX.2.20.1611140916100.809@charles-berrys-macbook.local>
Message-ID: <FAE947AF-3CF8-4E1F-92E9-44325CC96964@me.com>


> On Nov 14, 2016, at 11:26 AM, Charles C. Berry <ccberry at ucsd.edu> wrote:
> 
> On Mon, 14 Nov 2016, Bert Gunter wrote:
> 
>> Yes, but it need some help, since nchar gives the length of the
>> *entire* string; e.g.
>> 
>> ## to count "a" 's  :
>> 
>>> x <-(c("abbababba","bbabbabbaaaba"))
>>> nchar(gsub("[^a]","",x))
>> [1] 4 6
>> 
>> This is one of about 8 zillion ways to do this in base R if you don't
>> want to use a specialized package.
>> 
>> Just for curiosity: Can anyone comment on what is the most efficient
>> way to do this using base R pattern matching?
>> 
> 
> Most efficient? There probably is no uniformly most efficient way to do this as the timing will depend on the distribution of "a" in the atoms of any vector as well as the length of the vector.
> 
> But here is one way to avoid the regular expression matching:
> 
> lengths(strsplit(paste0("X", x, "X"),"a",fixed=TRUE)) - 1
> 
> 
> Chuck
> 


Hi,

Both gsub() and strsplit() are using regex based pattern matching internally. That being said, they are ultimately calling .Internal code, so both are pretty fast.

For comparison:

## Create a 1,000,000 character vector
set.seed(1)
Vec <- paste(sample(letters, 1000000, replace = TRUE), collapse = "")

> nchar(Vec)
[1] 1000000

## Split the vector into single characters and tabulate 
> table(strsplit(Vec, split = "")[[1]])

    a     b     c     d     e     f     g     h     i     j     k     l 
38664 38442 38282 38496 38540 38623 38548 38288 38143 38493 38184 38621 
    m     n     o     p     q     r     s     t     u     v     w     x 
38306 38725 38705 38144 38529 38809 38575 38355 38386 38364 38904 38310 
    y     z 
38265 38299 


## Get just the count of "a"
> table(strsplit(Vec, split = "")[[1]])["a"]
    a 
38664 

> nchar(gsub("[^a]", "", Vec))
[1] 38664


## Check performance
> system.time(table(strsplit(Vec, split = "")[[1]])["a"])
   user  system elapsed 
  0.100   0.007   0.107 

> system.time(nchar(gsub("[^a]", "", Vec)))
   user  system elapsed 
  0.270   0.001   0.272 


So, the above would suggest that using strsplit() is somewhat faster than using gsub(). However, as Chuck notes, in the absence of more exhaustive benchmarking, the difference may or may not be more generalizable.

Regards,

Marc Schwartz


From jdnewmil at dcn.davis.ca.us  Mon Nov 14 18:57:45 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 14 Nov 2016 09:57:45 -0800
Subject: [R] Question about expression parser for "return" statement
In-Reply-To: <c8c4aa4c-76ca-90da-b41a-08d44ebe7f6e@gmail.com>
References: <CAOogVzUOtMUXnvjFg37aGhDqtT9NmULGFiez4_jv1vGYZLDw0g@mail.gmail.com>
	<52831814-c240-04f8-3069-b670552a3941@gmail.com>
	<89968F61-40C5-472B-98CE-F56AF662B0F0@dcn.davis.ca.us>
	<c8c4aa4c-76ca-90da-b41a-08d44ebe7f6e@gmail.com>
Message-ID: <F8E98E43-D701-4B86-A112-5338D34577D5@dcn.davis.ca.us>

Sorry, I missed the operation-after-function call aspect of the OP question.

However, I think my policy of avoiding the return function as much as possible serves as an effective antibugging strategy for this problem, in addition to its other benefits.
-- 
Sent from my phone. Please excuse my brevity.

On November 14, 2016 2:12:49 AM PST, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>On 13/11/2016 9:42 PM, Jeff Newmiller wrote:
>> I find your response here inconsistent... either including `return`
>causes a "wasted" function call to occur (same result achieved slower)
>or the parser has an optimization in it to prevent the wasted function
>call (only behaviorally the same).
>
>I don't understand what you are finding inconsistent.  I wasn't talking
>
>about wasting anything.  I was just saying that expressions like
>
>return (a)*b
>
>are evaluated by calling return(a) first, because return() is a 
>function, and then they'll never get to the multiplication.
>
>BTW, there don't appear to be many instances of this particular bug in 
>CRAN packages, though I don't have a reliable test for it yet.  The
>most 
>common error seems to be using just "return", as mentioned before.  The
>
>fix for that is to add parens, e.g. "return()".  The next most common
>is 
>something like
>
>invisible(return(x))
>
>which returns x before making it invisible.  The fix for this is to use
>
>return(invisible(x))
>
>
>> I carefully avoid using the return function in R. Both because using
>it before the end of a function usually makes the logic harder to
>follow and because I am under the impression that using it at the end
>of the function is a small but pointless waste of CPU cycles. That some
>people might be prone to writing a C-like use of "return;" which causes
>a function object to be returned only increases my aversion to using
>it.
>
>Sometimes it is fine to use return(x), but it shouldn't be used
>routinely.
>
>Duncan Murdoch
>
>> -- Sent from my phone. Please excuse my brevity. On November 13, 2016
>> 3:47:10 AM PST, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>> >On 13/11/2016 12:50 AM, Dave DeBarr wrote:
>>>> >> I've noticed that if I don't include parentheses around the
>intended
>>> >return
>>>> >> value for the "return" statement, R will assume the first
>>> >parenthetical
>>>> >> expression is the intended return value ... even if that
>>> >parenthetical
>>>> >> expression is only part of a larger expression.
>>>> >>
>>>> >> Is this intentional?
>>> >
>>> >Yes, return is just a function call that has side effects.  As far
>as
>>> >the parser is concerned,
>>> >
>>> >return ((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x -
>Mean)^2)/Variance))
>>> >
>>> >is basically the same as
>>> >
>>> >f((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance))
>>> >
>>> >Duncan Murdoch


From dcarlson at tamu.edu  Mon Nov 14 19:04:41 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Mon, 14 Nov 2016 18:04:41 +0000
Subject: [R] Principle Component Analysis: Ranking Animal Size Based On
 Combined Metrics
In-Reply-To: <BLUPR0101MB147484D3A6AC65AD19FB2576ABBC0@BLUPR0101MB1474.prod.exchangelabs.com>
References: <BLUPR0101MB147499AE1D09AA1DD4EB4A54ABBD0@BLUPR0101MB1474.prod.exchangelabs.com>
	<CA+8X3fVrqY9447vuiRPbeAmnrZ01AZ8Kvp5nQHbQ4uDr5s8Sfw@mail.gmail.com>
	<BLUPR0101MB1474BE9248D293A65F3A7F3BABBC0@BLUPR0101MB1474.prod.exchangelabs.com>
	<bfec840522ef498b9838c6dc49119b3d@exch-2p-mbx-w2.ads.tamu.edu>
	<BLUPR0101MB147484D3A6AC65AD19FB2576ABBC0@BLUPR0101MB1474.prod.exchangelabs.com>
Message-ID: <b9268ddfb16245d7a89b9b1cd3b44d16@exch-2p-mbx-w2.ads.tamu.edu>

Usually you want to use the geometric mean on variables measured on the same scale, but in your case, transforming weight didn't change much. Adding cube root transformation as another approach (since weight should increase as the cube of the linear measures), the correlations with the 3 linear measurements are about the same for the transformed values and the 3 transformations are very strongly correlated:

> wgt.log <- log(df$weight)
> wgt.cube <- df$weight^(1/3)
> cor(cbind(weight=df$weight, wgt.log, wgt.cube), df[, -1])
           interoc    cwidth   clength
weight   0.3048239 0.2545593 0.4884446
wgt.log  0.3096511 0.2807528 0.4841830
wgt.cube 0.3077714 0.2724312 0.4863528
> cor(cbind(weight=df$weight, wgt.log, wgt.cube))
            weight   wgt.log  wgt.cube
weight   1.0000000 0.9862879 0.9939102
wgt.log  0.9862879 1.0000000 0.9984574
wgt.cube 0.9939102 0.9984574 1.0000000

David C

-----Original Message-----
From: Sidoti, Salvatore A. [mailto:sidoti.23 at buckeyemail.osu.edu] 
Sent: Monday, November 14, 2016 11:41 AM
To: David L Carlson; Jim Lemon; r-help mailing list
Subject: RE: [R] Principle Component Analysis: Ranking Animal Size Based On Combined Metrics

Fascinating! So it appears that I can simply take the geometric mean of all 4 metrics (unscaled), including weight, then designate that value as a relative measure of "size" within my sample population. The justification for using the geometric mean is shown by the high correlation between PC1 and the size values:

		pc1         gm
pc1  1.0000000 -0.8458024
gm  -0.8458024  1.0000000

Pearson's product-moment correlation
data:  pc1 and gm
t = -10.869, df = 47, p-value = 2.032e-14
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.9104585 -0.7407939
sample estimates:
       cor 
-0.8458024

Salvatore A. Sidoti
PhD Student
Behavioral Ecology

-----Original Message-----
From: David L Carlson [mailto:dcarlson at tamu.edu] 
Sent: Monday, November 14, 2016 11:07 AM
To: Sidoti, Salvatore A. <sidoti.23 at buckeyemail.osu.edu>; Jim Lemon <drjimlemon at gmail.com>; r-help mailing list <r-help at r-project.org>
Subject: RE: [R] Principle Component Analysis: Ranking Animal Size Based On Combined Metrics

The first principal component should be your estimate of "size" since it captures the correlations between all 4 variables. The second principle component must be orthogonal to the first so that if the first is "size", the second pc is independent of size, perhaps some measure of "shape". As would be expected, the first principal component is highly correlated with the geometric mean of the three linear measurements and moderately correlated with weight:

> gm <- apply(df[, -1], 1, prod)^(1/3)
> pc1 <- prcomp(df, scale.=TRUE)$x[, 1]
> plot(pc1, gm)
> cor(cbind(pc1, gm, wgt=df$weight))
           pc1         gm        wgt
pc1  1.0000000 -0.9716317 -0.5943594
gm  -0.9716317  1.0000000  0.3967369
wgt -0.5943594  0.3967369  1.0000000

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Sidoti, Salvatore A.
Sent: Sunday, November 13, 2016 7:38 PM
To: Jim Lemon; r-help mailing list
Subject: Re: [R] Principle Component Analysis: Ranking Animal Size Based On Combined Metrics

Hi Jim,

Nice to see you again! First of all, apologies to all for bending the rules a bit with respect to the mailing list. I know this is a list for R programming specifically, and I have received some great advice in this regard in the past. I just thought this was an interesting applied problem that would generate some discussion about PCA in R.

Yes, that is an excellent question! Indeed, why not just volume? Since this is still a work in progress and we have not published as of yet, I would rather not be more specific about the type of animal at this time ;>}. Nonetheless, I can say that the animals I study change "size" depending on their feeding and hydration state. The abdomen in particular undergoes drastic size changes. That being said, there are key anatomical features that remain fixed in the adult.

Now, there *might* be a way to work volume into the PCA. Although volume is not a reliable metric since the abdomen size is so changeable while the animal is alive, but what about preserved specimens? I have many that have been marinating in ethanol for months. Wouldn't the tissues have equilibrated by now? Probably... I could measure volume by displacement or suspension, I suppose.

In the meantime, here's a few thoughts:

1) 	Use the contribution % (known as C% hereafter) of each variable on principle components 1 and 2.

2) 	The total contribution of a variable that explains the variations retained by PC1 an PC2 is calculated by:
	
	sum(C%1 * eigenvalue1, C%2 * eigenvalue2)

3) Scale() to mean-center the columns of the data set.

4) Use these total contributions as the weights of an arithmetic mean.

For example, we have an animal with the following data (mean-centered):
weight:	1.334
interoc:	-0.225
clength:	0.046
cwidth:	-0.847

The contributions of these variables on PC1 and PC2 are (% changed to proportions):
weight:	0.556
interoc:	0.357
clength:	0.493
cwidth:	0.291

To calculate size:
1.334(0.556) - 0.225(0.357) + 0.046(0.493) - 0.847(0.291) = 0.43758 Then divide by the sum of the weights:
0.43758 / 1.697 = 0.257855 = "animal size"

This value can then be used to rank the animal according to its size for further analysis...

Does this sound like a reasonable application of my PCA data?

Salvatore A. Sidoti
PhD Student
Behavioral Ecology

-----Original Message-----
From: Jim Lemon [mailto:drjimlemon at gmail.com]
Sent: Sunday, November 13, 2016 3:53 PM
To: Sidoti, Salvatore A. <sidoti.23 at buckeyemail.osu.edu>; r-help mailing list <r-help at r-project.org>
Subject: Re: [R] Principle Component Analysis: Ranking Animal Size Based On Combined Metrics

Hi Salvatore,
If by "size" you mean volume, why not directly measure the volume of your animals? They appear to be fairly small. Sometimes working out what the critical value actually means can inform the way to measure it.

Jim


On Sun, Nov 13, 2016 at 4:46 PM, Sidoti, Salvatore A.
<sidoti.23 at buckeyemail.osu.edu> wrote:
> Let's say I perform 4 measurements on an animal: three are linear measurements in millimeters and the fourth is its weight in milligrams. So, we have a data set with mixed units.
>
> Based on these four correlated measurements, I would like to obtain one "score" or value that describes an individual animal's size. I considered simply taking the geometric mean of these 4 measurements, and that would give me a "score" - larger values would be for larger animals, etc.
>
> However, this assumes that all 4 of these measurements contribute equally to an animal's size. Of course, more than likely this is not the case. I then performed a PCA to discover how much influence each variable had on the overall data set. I was hoping to use this analysis to refine my original approach.
>
> I honestly do not know how to apply the information from the PCA to this particular problem...
>
> I do know, however, that principle components 1 and 2 capture enough of the variation to reduce the number of dimensions down to 2 (see analysis below with the original data set).
>
> Note: animal weights were ln() transformed to increase correlation with the 3 other variables.
>
> df <- data.frame(
>   weight = log(1000*c(0.0980, 0.0622, 0.0600, 0.1098, 0.0538, 0.0701, 0.1138, 0.0540, 0.0629, 0.0930,
>              0.0443, 0.1115, 0.1157, 0.0734, 0.0616, 0.0640, 0.0480, 0.1339, 0.0547, 0.0844,
>              0.0431, 0.0472, 0.0752, 0.0604, 0.0713, 0.0658, 0.0538, 0.0585, 0.0645, 0.0529,
>              0.0448, 0.0574, 0.0577, 0.0514, 0.0758, 0.0424, 0.0997, 0.0758, 0.0649, 0.0465,
>              0.0748, 0.0540, 0.0819, 0.0732, 0.0725, 0.0730, 0.0777, 0.0630, 0.0466)),
>   interoc = c(0.853, 0.865, 0.811, 0.840, 0.783, 0.868, 0.818, 0.847, 0.838, 0.799,
>               0.737, 0.788, 0.731, 0.777, 0.863, 0.877, 0.814, 0.926, 0.767, 0.746,
>               0.700, 0.768, 0.807, 0.753, 0.809, 0.788, 0.750, 0.815, 0.757, 0.737,
>               0.759, 0.863, 0.747, 0.838, 0.790, 0.676, 0.857, 0.728, 0.743, 0.870,
>               0.787, 0.773, 0.829, 0.785, 0.746, 0.834, 0.829, 0.750, 0.842),
>   cwidth = c(3.152, 3.046, 3.139, 3.181, 3.023, 3.452, 2.803, 3.050, 3.160, 3.186,
>              2.801, 2.862, 3.183, 2.770, 3.207, 3.188, 2.969, 3.033, 2.972, 3.291,
>              2.772, 2.875, 2.978, 3.094, 2.956, 2.966, 2.896, 3.149, 2.813, 2.935,
>              2.839, 3.152, 2.984, 3.037, 2.888, 2.723, 3.342, 2.562, 2.827, 2.909,
>              3.093, 2.990, 3.097, 2.751, 2.877, 2.901, 2.895, 2.721, 2.942),
>   clength = c(3.889, 3.733, 3.762, 4.059, 3.911, 3.822, 3.768, 3.814, 3.721, 3.794,
>               3.483, 3.863, 3.856, 3.457, 3.996, 3.876, 3.642, 3.978, 3.534, 3.967,
>               3.429, 3.518, 3.766, 3.755, 3.706, 3.785, 3.607, 3.922, 3.453, 3.589,
>               3.508, 3.861, 3.706, 3.593, 3.570, 3.341, 3.916, 3.336, 3.504, 3.688,
>               3.735, 3.724, 3.860, 3.405, 3.493, 3.586, 3.545, 3.443,
> 3.640))
>
> pca_morpho <- princomp(df, cor = TRUE)
>
> summary(pca_morpho)
>
> Importance of components:
>                                         Comp.1          Comp.2          Comp.3          Comp.4
> Standard deviation      1.604107        0.8827323       0.7061206       0.3860275
> Proportion of Variance  0.643290        0.1948041       0.1246516       0.0372543
> Cumulative Proportion   0.643290        0.8380941       0.9627457       1.0000000
>
> Loadings:
>                         Comp.1  Comp.2  Comp.3  Comp.4
> weight          -0.371          0.907                           -0.201
> interoc         -0.486  -0.227  -0.840
> cwidth          -0.537  -0.349          0.466           -0.611
> clength         -0.582                          0.278   0.761
>
>                         Comp.1  Comp.2  Comp.3  Comp.4
> SS loadings             1.00            1.00            1.00            1.00
> Proportion Var          0.25            0.25            0.25            0.25
> Cumulative Var          0.25            0.50            0.75            1.00
>
> Any guidance will be greatly appreciated!
>
> Salvatore A. Sidoti
> PhD Student
> The Ohio State University
> Behavioral Ecology
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ccberry at ucsd.edu  Mon Nov 14 20:55:44 2016
From: ccberry at ucsd.edu (Charles C. Berry)
Date: Mon, 14 Nov 2016 11:55:44 -0800
Subject: [R] Frequency of a character in a string
In-Reply-To: <FAE947AF-3CF8-4E1F-92E9-44325CC96964@me.com>
References: <trinity-bcc6f83f-cf25-4f2d-8e01-78230c379a8e-1479113050372@3capp-gmx-bs45>
	<07DE66F1-08A4-4C68-8529-DCA2A76A3BD9@gmail.com>
	<CAHUdqRvjOGPVKdm4DX7HkeXWtec-yneD-dK_YfgDnpyUH242tQ@mail.gmail.com>
	<CAGxFJbS8PZP7frCA_v74q53kqY8bLHwfi-HGp_gjfYbm7_cK1g@mail.gmail.com>
	<alpine.OSX.2.20.1611140916100.809@charles-berrys-macbook.local>
	<FAE947AF-3CF8-4E1F-92E9-44325CC96964@me.com>
Message-ID: <alpine.OSX.2.20.1611141141390.1014@charles-berrys-macbook.local>

On Mon, 14 Nov 2016, Marc Schwartz wrote:

>
>> On Nov 14, 2016, at 11:26 AM, Charles C. Berry <ccberry at ucsd.edu> wrote:
>>
>> On Mon, 14 Nov 2016, Bert Gunter wrote:
>>
[stuff deleted]

> Hi,
>
> Both gsub() and strsplit() are using regex based pattern matching 
> internally. That being said, they are ultimately calling .Internal code, 
> so both are pretty fast.
>
> For comparison:
>
> ## Create a 1,000,000 character vector
> set.seed(1)
> Vec <- paste(sample(letters, 1000000, replace = TRUE), collapse = "")
>
>> nchar(Vec)
> [1] 1000000
>
> ## Split the vector into single characters and tabulate
>> table(strsplit(Vec, split = "")[[1]])
>
>    a     b     c     d     e     f     g     h     i     j     k     l
> 38664 38442 38282 38496 38540 38623 38548 38288 38143 38493 38184 38621
>    m     n     o     p     q     r     s     t     u     v     w     x
> 38306 38725 38705 38144 38529 38809 38575 38355 38386 38364 38904 38310
>    y     z
> 38265 38299
>
>
> ## Get just the count of "a"
>> table(strsplit(Vec, split = "")[[1]])["a"]
>    a
> 38664
>
>> nchar(gsub("[^a]", "", Vec))
> [1] 38664
>
>
> ## Check performance
>> system.time(table(strsplit(Vec, split = "")[[1]])["a"])
>   user  system elapsed
>  0.100   0.007   0.107
>
>> system.time(nchar(gsub("[^a]", "", Vec)))
>   user  system elapsed
>  0.270   0.001   0.272
>
>
> So, the above would suggest that using strsplit() is somewhat faster 
> than using gsub(). However, as Chuck notes, in the absence of more 
> exhaustive benchmarking, the difference may or may not be more 
> generalizable.


Whether splitting on fixed strings rather than treating them as
regex'es (i.e.`fixed=TRUE') makes a big difference seems to depend on
what you split:

First repeating what Marc did...

> system.time(table(strsplit(Vec, split = "",fixed=TRUE)[[1]])["a"])
    user  system elapsed
   0.132   0.010   0.139 
> system.time(table(strsplit(Vec, split = "",fixed=FALSE)[[1]])["a"])
    user  system elapsed
   0.130   0.010   0.138

... fixed=TRUE hardly matters. But the idiom I proposed...

> system.time(sum(lengths(strsplit(paste0("X", Vec, "X"),"a",fixed=TRUE)) - 1))
    user  system elapsed
   0.017   0.000   0.018 
> system.time(sum(lengths(strsplit(paste0("X", Vec, "X"),"a",fixed=FALSE)) - 1))
    user  system elapsed
   0.104   0.000   0.104
>

... is 5 times faster with fixed=TRUE for this case.

This result matchea Marc's count:

> sum(lengths(strsplit(paste0("X", Vec, "X"),"a",fixed=FALSE)) - 1)
[1] 38664
>

Chuck


From bgunter.4567 at gmail.com  Mon Nov 14 21:23:28 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 14 Nov 2016 12:23:28 -0800
Subject: [R] Frequency of a character in a string
In-Reply-To: <alpine.OSX.2.20.1611141141390.1014@charles-berrys-macbook.local>
References: <trinity-bcc6f83f-cf25-4f2d-8e01-78230c379a8e-1479113050372@3capp-gmx-bs45>
	<07DE66F1-08A4-4C68-8529-DCA2A76A3BD9@gmail.com>
	<CAHUdqRvjOGPVKdm4DX7HkeXWtec-yneD-dK_YfgDnpyUH242tQ@mail.gmail.com>
	<CAGxFJbS8PZP7frCA_v74q53kqY8bLHwfi-HGp_gjfYbm7_cK1g@mail.gmail.com>
	<alpine.OSX.2.20.1611140916100.809@charles-berrys-macbook.local>
	<FAE947AF-3CF8-4E1F-92E9-44325CC96964@me.com>
	<alpine.OSX.2.20.1611141141390.1014@charles-berrys-macbook.local>
Message-ID: <CAGxFJbSHO-zf93zv5YYkxssUdp=ygRep-A-hf2GaDmyXxAtwXw@mail.gmail.com>

Chuck, Marc, and anyone else who still has interest in this odd little
discussion ...

Yes, and with fixed = TRUE my approach took 1/3 as much time as
Chuck's with a 10 element vector each element of which is a character
string of length 1e5:

> set.seed(1001)
> x <- sapply(1:10, function(x)paste0(sample(letters,1e5,rep=TRUE),collapse = ""))

> system.time(sum(lengths(strsplit(paste0("X", x, "X"),"a",fixed=TRUE)) - 1))
   user  system elapsed
  0.012   0.000   0.012
> system.time(nchar(gsub("[^a]", "", x,fixed = TRUE)))
   user  system elapsed
  0.004   0.000   0.004

Best,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Nov 14, 2016 at 11:55 AM, Charles C. Berry <ccberry at ucsd.edu> wrote:
> On Mon, 14 Nov 2016, Marc Schwartz wrote:
>
>>
>>> On Nov 14, 2016, at 11:26 AM, Charles C. Berry <ccberry at ucsd.edu> wrote:
>>>
>>> On Mon, 14 Nov 2016, Bert Gunter wrote:
>>>
> [stuff deleted]
>
>
>> Hi,
>>
>> Both gsub() and strsplit() are using regex based pattern matching
>> internally. That being said, they are ultimately calling .Internal code, so
>> both are pretty fast.
>>
>> For comparison:
>>
>> ## Create a 1,000,000 character vector
>> set.seed(1)
>> Vec <- paste(sample(letters, 1000000, replace = TRUE), collapse = "")
>>
>>> nchar(Vec)
>>
>> [1] 1000000
>>
>> ## Split the vector into single characters and tabulate
>>>
>>> table(strsplit(Vec, split = "")[[1]])
>>
>>
>>    a     b     c     d     e     f     g     h     i     j     k     l
>> 38664 38442 38282 38496 38540 38623 38548 38288 38143 38493 38184 38621
>>    m     n     o     p     q     r     s     t     u     v     w     x
>> 38306 38725 38705 38144 38529 38809 38575 38355 38386 38364 38904 38310
>>    y     z
>> 38265 38299
>>
>>
>> ## Get just the count of "a"
>>>
>>> table(strsplit(Vec, split = "")[[1]])["a"]
>>
>>    a
>> 38664
>>
>>> nchar(gsub("[^a]", "", Vec))
>>
>> [1] 38664
>>
>>
>> ## Check performance
>>>
>>> system.time(table(strsplit(Vec, split = "")[[1]])["a"])
>>
>>   user  system elapsed
>>  0.100   0.007   0.107
>>
>>> system.time(nchar(gsub("[^a]", "", Vec)))
>>
>>   user  system elapsed
>>  0.270   0.001   0.272
>>
>>
>> So, the above would suggest that using strsplit() is somewhat faster than
>> using gsub(). However, as Chuck notes, in the absence of more exhaustive
>> benchmarking, the difference may or may not be more generalizable.
>
>
>
> Whether splitting on fixed strings rather than treating them as
> regex'es (i.e.`fixed=TRUE') makes a big difference seems to depend on
> what you split:
>
> First repeating what Marc did...
>
>> system.time(table(strsplit(Vec, split = "",fixed=TRUE)[[1]])["a"])
>
>    user  system elapsed
>   0.132   0.010   0.139
>>
>> system.time(table(strsplit(Vec, split = "",fixed=FALSE)[[1]])["a"])
>
>    user  system elapsed
>   0.130   0.010   0.138
>
> ... fixed=TRUE hardly matters. But the idiom I proposed...
>
>> system.time(sum(lengths(strsplit(paste0("X", Vec, "X"),"a",fixed=TRUE)) -
>> 1))
>
>    user  system elapsed
>   0.017   0.000   0.018
>>
>> system.time(sum(lengths(strsplit(paste0("X", Vec, "X"),"a",fixed=FALSE)) -
>> 1))
>
>    user  system elapsed
>   0.104   0.000   0.104
>>
>>
>
> ... is 5 times faster with fixed=TRUE for this case.
>
> This result matchea Marc's count:
>
>> sum(lengths(strsplit(paste0("X", Vec, "X"),"a",fixed=FALSE)) - 1)
>
> [1] 38664
>>
>>
>
> Chuck


From hpages at fredhutch.org  Mon Nov 14 21:26:50 2016
From: hpages at fredhutch.org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Mon, 14 Nov 2016 12:26:50 -0800
Subject: [R] Frequency of a character in a string
In-Reply-To: <alpine.OSX.2.20.1611141141390.1014@charles-berrys-macbook.local>
References: <trinity-bcc6f83f-cf25-4f2d-8e01-78230c379a8e-1479113050372@3capp-gmx-bs45>
	<07DE66F1-08A4-4C68-8529-DCA2A76A3BD9@gmail.com>
	<CAHUdqRvjOGPVKdm4DX7HkeXWtec-yneD-dK_YfgDnpyUH242tQ@mail.gmail.com>
	<CAGxFJbS8PZP7frCA_v74q53kqY8bLHwfi-HGp_gjfYbm7_cK1g@mail.gmail.com>
	<alpine.OSX.2.20.1611140916100.809@charles-berrys-macbook.local>
	<FAE947AF-3CF8-4E1F-92E9-44325CC96964@me.com>
	<alpine.OSX.2.20.1611141141390.1014@charles-berrys-macbook.local>
Message-ID: <6c44e6c6-8ef1-d506-d0ed-1a9c5d555998@fredhutch.org>

Hi,

FWIW using gsub( , fixed=TRUE) is faster than using gsub( , fixed=FALSE)
or strsplit( , fixed=TRUE):

   set.seed(1)
   Vec <- paste(sample(letters, 5000000, replace = TRUE), collapse = "")

   system.time(res1 <- nchar(gsub("[^a]", "", Vec)))
   #  user  system elapsed
   # 0.585   0.000   0.586

   system.time(res2 <- lengths(strsplit(Vec,"a",fixed=TRUE)) - 1L)
   #  user  system elapsed
   # 0.061   0.000   0.061

   system.time(res3 <- nchar(Vec) - nchar(gsub("a", "", Vec, fixed=TRUE)))
   #  user  system elapsed
   # 0.039   0.000   0.039

   identical(res1, res2)
   # [1] TRUE
   identical(res1, res3)
   # [1] TRUE

The gsub( , fixed=TRUE) solution also uses slightly less memory than the
strsplit( , fixed=TRUE) solution.

Cheers,
H.


On 11/14/2016 11:55 AM, Charles C. Berry wrote:
> On Mon, 14 Nov 2016, Marc Schwartz wrote:
>
>>
>>> On Nov 14, 2016, at 11:26 AM, Charles C. Berry <ccberry at ucsd.edu> wrote:
>>>
>>> On Mon, 14 Nov 2016, Bert Gunter wrote:
>>>
> [stuff deleted]
>
>> Hi,
>>
>> Both gsub() and strsplit() are using regex based pattern matching
>> internally. That being said, they are ultimately calling .Internal
>> code, so both are pretty fast.
>>
>> For comparison:
>>
>> ## Create a 1,000,000 character vector
>> set.seed(1)
>> Vec <- paste(sample(letters, 1000000, replace = TRUE), collapse = "")
>>
>>> nchar(Vec)
>> [1] 1000000
>>
>> ## Split the vector into single characters and tabulate
>>> table(strsplit(Vec, split = "")[[1]])
>>
>>    a     b     c     d     e     f     g     h     i     j     k     l
>> 38664 38442 38282 38496 38540 38623 38548 38288 38143 38493 38184 38621
>>    m     n     o     p     q     r     s     t     u     v     w     x
>> 38306 38725 38705 38144 38529 38809 38575 38355 38386 38364 38904 38310
>>    y     z
>> 38265 38299
>>
>>
>> ## Get just the count of "a"
>>> table(strsplit(Vec, split = "")[[1]])["a"]
>>    a
>> 38664
>>
>>> nchar(gsub("[^a]", "", Vec))
>> [1] 38664
>>
>>
>> ## Check performance
>>> system.time(table(strsplit(Vec, split = "")[[1]])["a"])
>>   user  system elapsed
>>  0.100   0.007   0.107
>>
>>> system.time(nchar(gsub("[^a]", "", Vec)))
>>   user  system elapsed
>>  0.270   0.001   0.272
>>
>>
>> So, the above would suggest that using strsplit() is somewhat faster
>> than using gsub(). However, as Chuck notes, in the absence of more
>> exhaustive benchmarking, the difference may or may not be more
>> generalizable.
>
>
> Whether splitting on fixed strings rather than treating them as
> regex'es (i.e.`fixed=TRUE') makes a big difference seems to depend on
> what you split:
>
> First repeating what Marc did...
>
>> system.time(table(strsplit(Vec, split = "",fixed=TRUE)[[1]])["a"])
>    user  system elapsed
>   0.132   0.010   0.139
>> system.time(table(strsplit(Vec, split = "",fixed=FALSE)[[1]])["a"])
>    user  system elapsed
>   0.130   0.010   0.138
>
> ... fixed=TRUE hardly matters. But the idiom I proposed...
>
>> system.time(sum(lengths(strsplit(paste0("X", Vec,
>> "X"),"a",fixed=TRUE)) - 1))
>    user  system elapsed
>   0.017   0.000   0.018
>> system.time(sum(lengths(strsplit(paste0("X", Vec,
>> "X"),"a",fixed=FALSE)) - 1))
>    user  system elapsed
>   0.104   0.000   0.104
>>
>
> ... is 5 times faster with fixed=TRUE for this case.
>
> This result matchea Marc's count:
>
>> sum(lengths(strsplit(paste0("X", Vec, "X"),"a",fixed=FALSE)) - 1)
> [1] 38664
>>
>
> Chuck
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From bgunter.4567 at gmail.com  Mon Nov 14 21:44:10 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 14 Nov 2016 12:44:10 -0800
Subject: [R] Frequency of a character in a string
In-Reply-To: <6c44e6c6-8ef1-d506-d0ed-1a9c5d555998@fredhutch.org>
References: <trinity-bcc6f83f-cf25-4f2d-8e01-78230c379a8e-1479113050372@3capp-gmx-bs45>
	<07DE66F1-08A4-4C68-8529-DCA2A76A3BD9@gmail.com>
	<CAHUdqRvjOGPVKdm4DX7HkeXWtec-yneD-dK_YfgDnpyUH242tQ@mail.gmail.com>
	<CAGxFJbS8PZP7frCA_v74q53kqY8bLHwfi-HGp_gjfYbm7_cK1g@mail.gmail.com>
	<alpine.OSX.2.20.1611140916100.809@charles-berrys-macbook.local>
	<FAE947AF-3CF8-4E1F-92E9-44325CC96964@me.com>
	<alpine.OSX.2.20.1611141141390.1014@charles-berrys-macbook.local>
	<6c44e6c6-8ef1-d506-d0ed-1a9c5d555998@fredhutch.org>
Message-ID: <CAGxFJbTREpkZzrvFGj0arz=PMfBfU9K+eK+u5BukNC6ezQOzPQ@mail.gmail.com>

(Sheepishly)...

Yes, thank you Herv?. It would have been nice if I had given correct
soutions. Fixed = TRUE could not have of course worked with ["a"]
character class!

Here's what I found with a 10 element vector each member of which is a
1e5 length string:

> system.time((lengths(strsplit(paste0("X", x, "X"),"a",fixed=TRUE)) - 1))
   user  system elapsed
  0.013   0.000   0.013

> system.time(nchar(gsub("[^a]", "", x,fixed = FALSE)))
   user  system elapsed
  0.251   0.000   0.252
## WAYYYY slower


> system.time(nchar(x) - nchar(gsub("a", "", x,fixed = TRUE)))
   user  system elapsed
  0.007   0.000   0.007
## twice as fast



Clearly and unsurprisingly, the message is to avoid fixed = FALSE;
after that, it seems mostly to be: who cares?!


Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Nov 14, 2016 at 12:26 PM, Herv? Pag?s <hpages at fredhutch.org> wrote:
> Hi,
>
> FWIW using gsub( , fixed=TRUE) is faster than using gsub( , fixed=FALSE)
> or strsplit( , fixed=TRUE):
>
>   set.seed(1)
>   Vec <- paste(sample(letters, 5000000, replace = TRUE), collapse = "")
>
>   system.time(res1 <- nchar(gsub("[^a]", "", Vec)))
>   #  user  system elapsed
>   # 0.585   0.000   0.586
>
>   system.time(res2 <- lengths(strsplit(Vec,"a",fixed=TRUE)) - 1L)
>   #  user  system elapsed
>   # 0.061   0.000   0.061
>
>   system.time(res3 <- nchar(Vec) - nchar(gsub("a", "", Vec, fixed=TRUE)))
>   #  user  system elapsed
>   # 0.039   0.000   0.039
>
>   identical(res1, res2)
>   # [1] TRUE
>   identical(res1, res3)
>   # [1] TRUE
>
> The gsub( , fixed=TRUE) solution also uses slightly less memory than the
> strsplit( , fixed=TRUE) solution.
>
> Cheers,
> H.
>
>
> On 11/14/2016 11:55 AM, Charles C. Berry wrote:
>>
>> On Mon, 14 Nov 2016, Marc Schwartz wrote:
>>
>>>
>>>> On Nov 14, 2016, at 11:26 AM, Charles C. Berry <ccberry at ucsd.edu> wrote:
>>>>
>>>> On Mon, 14 Nov 2016, Bert Gunter wrote:
>>>>
>> [stuff deleted]
>>
>>> Hi,
>>>
>>> Both gsub() and strsplit() are using regex based pattern matching
>>> internally. That being said, they are ultimately calling .Internal
>>> code, so both are pretty fast.
>>>
>>> For comparison:
>>>
>>> ## Create a 1,000,000 character vector
>>> set.seed(1)
>>> Vec <- paste(sample(letters, 1000000, replace = TRUE), collapse = "")
>>>
>>>> nchar(Vec)
>>>
>>> [1] 1000000
>>>
>>> ## Split the vector into single characters and tabulate
>>>>
>>>> table(strsplit(Vec, split = "")[[1]])
>>>
>>>
>>>    a     b     c     d     e     f     g     h     i     j     k     l
>>> 38664 38442 38282 38496 38540 38623 38548 38288 38143 38493 38184 38621
>>>    m     n     o     p     q     r     s     t     u     v     w     x
>>> 38306 38725 38705 38144 38529 38809 38575 38355 38386 38364 38904 38310
>>>    y     z
>>> 38265 38299
>>>
>>>
>>> ## Get just the count of "a"
>>>>
>>>> table(strsplit(Vec, split = "")[[1]])["a"]
>>>
>>>    a
>>> 38664
>>>
>>>> nchar(gsub("[^a]", "", Vec))
>>>
>>> [1] 38664
>>>
>>>
>>> ## Check performance
>>>>
>>>> system.time(table(strsplit(Vec, split = "")[[1]])["a"])
>>>
>>>   user  system elapsed
>>>  0.100   0.007   0.107
>>>
>>>> system.time(nchar(gsub("[^a]", "", Vec)))
>>>
>>>   user  system elapsed
>>>  0.270   0.001   0.272
>>>
>>>
>>> So, the above would suggest that using strsplit() is somewhat faster
>>> than using gsub(). However, as Chuck notes, in the absence of more
>>> exhaustive benchmarking, the difference may or may not be more
>>> generalizable.
>>
>>
>>
>> Whether splitting on fixed strings rather than treating them as
>> regex'es (i.e.`fixed=TRUE') makes a big difference seems to depend on
>> what you split:
>>
>> First repeating what Marc did...
>>
>>> system.time(table(strsplit(Vec, split = "",fixed=TRUE)[[1]])["a"])
>>
>>    user  system elapsed
>>   0.132   0.010   0.139
>>>
>>> system.time(table(strsplit(Vec, split = "",fixed=FALSE)[[1]])["a"])
>>
>>    user  system elapsed
>>   0.130   0.010   0.138
>>
>> ... fixed=TRUE hardly matters. But the idiom I proposed...
>>
>>> system.time(sum(lengths(strsplit(paste0("X", Vec,
>>> "X"),"a",fixed=TRUE)) - 1))
>>
>>    user  system elapsed
>>   0.017   0.000   0.018
>>>
>>> system.time(sum(lengths(strsplit(paste0("X", Vec,
>>> "X"),"a",fixed=FALSE)) - 1))
>>
>>    user  system elapsed
>>   0.104   0.000   0.104
>>>
>>>
>>
>> ... is 5 times faster with fixed=TRUE for this case.
>>
>> This result matchea Marc's count:
>>
>>> sum(lengths(strsplit(paste0("X", Vec, "X"),"a",fixed=FALSE)) - 1)
>>
>> [1] 38664
>>>
>>>
>>
>> Chuck
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
> --
> Herv? Pag?s
>
> Program in Computational Biology
> Division of Public Health Sciences
> Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N, M1-B514
> P.O. Box 19024
> Seattle, WA 98109-1024
>
> E-mail: hpages at fredhutch.org
> Phone:  (206) 667-5791
> Fax:    (206) 667-1319
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From oluola2011 at yahoo.com  Mon Nov 14 21:52:28 2016
From: oluola2011 at yahoo.com (Olu Ola)
Date: Mon, 14 Nov 2016 20:52:28 +0000 (UTC)
Subject: [R] Issues with the way Apply handled NA's
References: <683034987.3530459.1479156748587.ref@mail.yahoo.com>
Message-ID: <683034987.3530459.1479156748587@mail.yahoo.com>

 Hello,I have a data set called plabor and have the following format:

| ColA | ColB | Colc |
| 6 | 25 | 3 |
| NA | NA | NA |
| 3 | 2 | 19 |
| 4 | 7 | NA |


I wanted to find the product of the three columns?for each of the rows?and I used the apply function follows:
plabor$colD = apply(plabor[c("colA","colB","colc")],1,prod,na.rm=T)
The result are as follows:

| ColA | ColB | Colc | colD |
| 6 | 25 | 3 | 450 |
| NA | NA | NA | 1 |
| 3 | 2 | 19 | 114 |
| 4 | 7 | NA | 28 |


The second row results is 1 instead of being ignored.
How do I deal with this issue because I do not want to exclude these data points with all NA's?
Regards

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Mon Nov 14 21:53:15 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 15 Nov 2016 07:53:15 +1100
Subject: [R] Text categories based on the sentences
In-Reply-To: <CAAM-fZ5UWQ1M61E4=VF-HqQ=6E7E2wAL8opBtCidY=NJjcSAxg@mail.gmail.com>
References: <CAAM-fZ5UWQ1M61E4=VF-HqQ=6E7E2wAL8opBtCidY=NJjcSAxg@mail.gmail.com>
Message-ID: <CA+8X3fUwSFz=cNaXTqFrfNduznjzPpkf0sCUwkxoQPrxExPDQA@mail.gmail.com>

Hi Venky,
Unfortunately the MindReader package produces the following:

1. I want ice cream                                         Desire
2. I like banana very much                               Pleasure
3. Tomorrow i will eat chicken                           Expectation
4. Yesterday i went to birthday party                 Reminiscence
5. I lost my mobile last week                           Disappointment

I recall that there is a computer named Watson that might be of assistance.

Jim

On Tue, Nov 15, 2016 at 12:16 AM, Venky <venkynov10 at gmail.com> wrote:
> Hi team,
>
> I have data set contains one variable "*Description*"
>
> *Description**                                                  Category*
>
> 1. i want ice cream                                         food
> 2. i like banana very much                              fruit
> 3. tomorrow i will eat chicken                          food
> 4. yesterday i went to birthday party                festival
> 5. i lost my mobile last week                           mobile
>
> Please remember that i have only "*Description*" Variables only.How can i
> get the categories column based on the sentences of *Description *column.
>
> kindly do the needful help for that
>
> Advance in Thanks.
>
>
>
>
>
>
> Thanks and Regards
> Venkatesan
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Mon Nov 14 21:57:17 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 14 Nov 2016 12:57:17 -0800
Subject: [R] Frequency of a character in a string
In-Reply-To: <CAGxFJbSHO-zf93zv5YYkxssUdp=ygRep-A-hf2GaDmyXxAtwXw@mail.gmail.com>
References: <trinity-bcc6f83f-cf25-4f2d-8e01-78230c379a8e-1479113050372@3capp-gmx-bs45>
	<07DE66F1-08A4-4C68-8529-DCA2A76A3BD9@gmail.com>
	<CAHUdqRvjOGPVKdm4DX7HkeXWtec-yneD-dK_YfgDnpyUH242tQ@mail.gmail.com>
	<CAGxFJbS8PZP7frCA_v74q53kqY8bLHwfi-HGp_gjfYbm7_cK1g@mail.gmail.com>
	<alpine.OSX.2.20.1611140916100.809@charles-berrys-macbook.local>
	<FAE947AF-3CF8-4E1F-92E9-44325CC96964@me.com>
	<alpine.OSX.2.20.1611141141390.1014@charles-berrys-macbook.local>
	<CAGxFJbSHO-zf93zv5YYkxssUdp=ygRep-A-hf2GaDmyXxAtwXw@mail.gmail.com>
Message-ID: <CAF8bMcb2LRYE5H4cB44g3ZBnN51XOXM+i5tS4BE67sZ50P2XvQ@mail.gmail.com>

Here is another variant, v3, and a change to your first example
so it returns the same value as your second example.

> set.seed(1001)
> x <- sapply(1:100,
function(x)paste0(sample(letters,rpois(1,1e5),rep=TRUE),collapse = ""))
> system.time(v1 <- lengths(strsplit(paste0("X", x, "X"),"a",fixed=TRUE)) -
1)
   user  system elapsed
   0.47    0.00    0.49
> system.time(v2 <- nchar(gsub("[^a]", "", x)))
   user  system elapsed
   2.53    0.00    2.53
> system.time(v3 <- nchar(x) - nchar(gsub("a", "", x, fixed=TRUE)))
   user  system elapsed
   0.08    0.00    0.08
>
> all.equal(v1,v2)
[1] TRUE
> all.equal(v1,v3)
[1] TRUE


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Nov 14, 2016 at 12:23 PM, Bert Gunter <bgunter.4567 at gmail.com>
wrote:

> Chuck, Marc, and anyone else who still has interest in this odd little
> discussion ...
>
> Yes, and with fixed = TRUE my approach took 1/3 as much time as
> Chuck's with a 10 element vector each element of which is a character
> string of length 1e5:
>
> > set.seed(1001)
> > x <- sapply(1:10, function(x)paste0(sample(letters,1e5,rep=TRUE),collapse
> = ""))
>
> > system.time(sum(lengths(strsplit(paste0("X", x, "X"),"a",fixed=TRUE)) -
> 1))
>    user  system elapsed
>   0.012   0.000   0.012
> > system.time(nchar(gsub("[^a]", "", x,fixed = TRUE)))
>    user  system elapsed
>   0.004   0.000   0.004
>
> Best,
> Bert
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Mon, Nov 14, 2016 at 11:55 AM, Charles C. Berry <ccberry at ucsd.edu>
> wrote:
> > On Mon, 14 Nov 2016, Marc Schwartz wrote:
> >
> >>
> >>> On Nov 14, 2016, at 11:26 AM, Charles C. Berry <ccberry at ucsd.edu>
> wrote:
> >>>
> >>> On Mon, 14 Nov 2016, Bert Gunter wrote:
> >>>
> > [stuff deleted]
> >
> >
> >> Hi,
> >>
> >> Both gsub() and strsplit() are using regex based pattern matching
> >> internally. That being said, they are ultimately calling .Internal
> code, so
> >> both are pretty fast.
> >>
> >> For comparison:
> >>
> >> ## Create a 1,000,000 character vector
> >> set.seed(1)
> >> Vec <- paste(sample(letters, 1000000, replace = TRUE), collapse = "")
> >>
> >>> nchar(Vec)
> >>
> >> [1] 1000000
> >>
> >> ## Split the vector into single characters and tabulate
> >>>
> >>> table(strsplit(Vec, split = "")[[1]])
> >>
> >>
> >>    a     b     c     d     e     f     g     h     i     j     k     l
> >> 38664 38442 38282 38496 38540 38623 38548 38288 38143 38493 38184 38621
> >>    m     n     o     p     q     r     s     t     u     v     w     x
> >> 38306 38725 38705 38144 38529 38809 38575 38355 38386 38364 38904 38310
> >>    y     z
> >> 38265 38299
> >>
> >>
> >> ## Get just the count of "a"
> >>>
> >>> table(strsplit(Vec, split = "")[[1]])["a"]
> >>
> >>    a
> >> 38664
> >>
> >>> nchar(gsub("[^a]", "", Vec))
> >>
> >> [1] 38664
> >>
> >>
> >> ## Check performance
> >>>
> >>> system.time(table(strsplit(Vec, split = "")[[1]])["a"])
> >>
> >>   user  system elapsed
> >>  0.100   0.007   0.107
> >>
> >>> system.time(nchar(gsub("[^a]", "", Vec)))
> >>
> >>   user  system elapsed
> >>  0.270   0.001   0.272
> >>
> >>
> >> So, the above would suggest that using strsplit() is somewhat faster
> than
> >> using gsub(). However, as Chuck notes, in the absence of more exhaustive
> >> benchmarking, the difference may or may not be more generalizable.
> >
> >
> >
> > Whether splitting on fixed strings rather than treating them as
> > regex'es (i.e.`fixed=TRUE') makes a big difference seems to depend on
> > what you split:
> >
> > First repeating what Marc did...
> >
> >> system.time(table(strsplit(Vec, split = "",fixed=TRUE)[[1]])["a"])
> >
> >    user  system elapsed
> >   0.132   0.010   0.139
> >>
> >> system.time(table(strsplit(Vec, split = "",fixed=FALSE)[[1]])["a"])
> >
> >    user  system elapsed
> >   0.130   0.010   0.138
> >
> > ... fixed=TRUE hardly matters. But the idiom I proposed...
> >
> >> system.time(sum(lengths(strsplit(paste0("X", Vec,
> "X"),"a",fixed=TRUE)) -
> >> 1))
> >
> >    user  system elapsed
> >   0.017   0.000   0.018
> >>
> >> system.time(sum(lengths(strsplit(paste0("X", Vec,
> "X"),"a",fixed=FALSE)) -
> >> 1))
> >
> >    user  system elapsed
> >   0.104   0.000   0.104
> >>
> >>
> >
> > ... is 5 times faster with fixed=TRUE for this case.
> >
> > This result matchea Marc's count:
> >
> >> sum(lengths(strsplit(paste0("X", Vec, "X"),"a",fixed=FALSE)) - 1)
> >
> > [1] 38664
> >>
> >>
> >
> > Chuck
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Mon Nov 14 22:04:20 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 15 Nov 2016 10:04:20 +1300
Subject: [R] [FORGED]  Issues with the way Apply handled NA's
In-Reply-To: <683034987.3530459.1479156748587@mail.yahoo.com>
References: <683034987.3530459.1479156748587.ref@mail.yahoo.com>
	<683034987.3530459.1479156748587@mail.yahoo.com>
Message-ID: <69af06dd-c8c3-5969-348c-95f74df86654@auckland.ac.nz>

On 15/11/16 09:52, Olu Ola via R-help wrote:
>  Hello,I have a data set called plabor and have the following format:
>
> | ColA | ColB | Colc |
> | 6 | 25 | 3 |
> | NA | NA | NA |
> | 3 | 2 | 19 |
> | 4 | 7 | NA |
>
>
> I wanted to find the product of the three columns for each of the rows and I used the apply function follows:
> plabor$colD = apply(plabor[c("colA","colB","colc")],1,prod,na.rm=T)
> The result are as follows:
>
> | ColA | ColB | Colc | colD |
> | 6 | 25 | 3 | 450 |
> | NA | NA | NA | 1 |
> | 3 | 2 | 19 | 114 |
> | 4 | 7 | NA | 28 |
>
>
> The second row results is 1 instead of being ignored.
> How do I deal with this issue because I do not want to exclude these data points with all NA's?

What do you mean by "ignored"?  If you really want rows of your matrix 
that are all NA to be omitted from consideration, delete such rows from 
your matrix a priori.

If you want the product of such rows to be NA rather than 1, use a 
"customised" function rather than prod() in your apply, with an 
appropriate if-else construction in the customised function.

It's very easy, and I won't tell you the details because I think it's 
time you actually learned something about R (given that you are using 
R), and one learns by doing.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From jasa at fe.uc.pt  Mon Nov 14 22:09:01 2016
From: jasa at fe.uc.pt (fe_jasa)
Date: Mon, 14 Nov 2016 21:09:01 +0000
Subject: [R] Zenga - inequality index - Do you know any package to compute
	it?
Message-ID: <864a4096-dec2-82ce-e81b-6bf7ff8e0f2c@fe.uc.pt>


-- 
><><><><><><><><><><><><><><>
Jo?o Sousa Andrade
jasa04011950 at gmail.com
><><><><><><><><><><><><><><>






	[[alternative HTML version deleted]]


From cimentadaj at gmail.com  Mon Nov 14 22:17:14 2016
From: cimentadaj at gmail.com (Jorge Cimentada)
Date: Mon, 14 Nov 2016 17:17:14 -0400
Subject: [R] Zenga - inequality index - Do you know any package to
	compute it?
In-Reply-To: <864a4096-dec2-82ce-e81b-6bf7ff8e0f2c@fe.uc.pt>
References: <864a4096-dec2-82ce-e81b-6bf7ff8e0f2c@fe.uc.pt>
Message-ID: <CALdB+JGyJrZqYtgTapVCQmwJHFSThpMT9fxL37nT2J8UiGX5rg@mail.gmail.com>

A simple google search directs me to the 'convey' package in CRAN which has
a function called svyzenga. Here
<https://cran.r-project.org/web/packages/convey/convey.pdf> for more
details. Maybe that's what you want.

	[[alternative HTML version deleted]]


From alemu.tadesse at gmail.com  Mon Nov 14 19:37:58 2016
From: alemu.tadesse at gmail.com (Alemu Tadesse)
Date: Mon, 14 Nov 2016 10:37:58 -0800
Subject: [R] I have a python API script that works and would like to
 translate it to R
Message-ID: <CACGkHRNFLjhXsaECgd2=m=+WRPpKaPPNyGWVgezvc8zJUJPqHA@mail.gmail.com>

Hi R-Geeks,

I have a python rest API  script that works very well.  I am learning R and
would like to translate it to R. I am wondering if there is a person who
uses API and knows both langues (Python and R) and willing to help me so
that I can share the Python script.

Thanks,

AT

	[[alternative HTML version deleted]]


From sidoti.23 at buckeyemail.osu.edu  Mon Nov 14 18:41:06 2016
From: sidoti.23 at buckeyemail.osu.edu (Sidoti, Salvatore A.)
Date: Mon, 14 Nov 2016 17:41:06 +0000
Subject: [R] Principle Component Analysis: Ranking Animal Size Based On
 Combined Metrics
In-Reply-To: <bfec840522ef498b9838c6dc49119b3d@exch-2p-mbx-w2.ads.tamu.edu>
References: <BLUPR0101MB147499AE1D09AA1DD4EB4A54ABBD0@BLUPR0101MB1474.prod.exchangelabs.com>
	<CA+8X3fVrqY9447vuiRPbeAmnrZ01AZ8Kvp5nQHbQ4uDr5s8Sfw@mail.gmail.com>
	<BLUPR0101MB1474BE9248D293A65F3A7F3BABBC0@BLUPR0101MB1474.prod.exchangelabs.com>
	<bfec840522ef498b9838c6dc49119b3d@exch-2p-mbx-w2.ads.tamu.edu>
Message-ID: <BLUPR0101MB147484D3A6AC65AD19FB2576ABBC0@BLUPR0101MB1474.prod.exchangelabs.com>

Fascinating! So it appears that I can simply take the geometric mean of all 4 metrics (unscaled), including weight, then designate that value as a relative measure of "size" within my sample population. The justification for using the geometric mean is shown by the high correlation between PC1 and the size values:

		pc1         gm
pc1  1.0000000 -0.8458024
gm  -0.8458024  1.0000000

Pearson's product-moment correlation
data:  pc1 and gm
t = -10.869, df = 47, p-value = 2.032e-14
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.9104585 -0.7407939
sample estimates:
       cor 
-0.8458024

Salvatore A. Sidoti
PhD Student
Behavioral Ecology

-----Original Message-----
From: David L Carlson [mailto:dcarlson at tamu.edu] 
Sent: Monday, November 14, 2016 11:07 AM
To: Sidoti, Salvatore A. <sidoti.23 at buckeyemail.osu.edu>; Jim Lemon <drjimlemon at gmail.com>; r-help mailing list <r-help at r-project.org>
Subject: RE: [R] Principle Component Analysis: Ranking Animal Size Based On Combined Metrics

The first principal component should be your estimate of "size" since it captures the correlations between all 4 variables. The second principle component must be orthogonal to the first so that if the first is "size", the second pc is independent of size, perhaps some measure of "shape". As would be expected, the first principal component is highly correlated with the geometric mean of the three linear measurements and moderately correlated with weight:

> gm <- apply(df[, -1], 1, prod)^(1/3)
> pc1 <- prcomp(df, scale.=TRUE)$x[, 1]
> plot(pc1, gm)
> cor(cbind(pc1, gm, wgt=df$weight))
           pc1         gm        wgt
pc1  1.0000000 -0.9716317 -0.5943594
gm  -0.9716317  1.0000000  0.3967369
wgt -0.5943594  0.3967369  1.0000000

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Sidoti, Salvatore A.
Sent: Sunday, November 13, 2016 7:38 PM
To: Jim Lemon; r-help mailing list
Subject: Re: [R] Principle Component Analysis: Ranking Animal Size Based On Combined Metrics

Hi Jim,

Nice to see you again! First of all, apologies to all for bending the rules a bit with respect to the mailing list. I know this is a list for R programming specifically, and I have received some great advice in this regard in the past. I just thought this was an interesting applied problem that would generate some discussion about PCA in R.

Yes, that is an excellent question! Indeed, why not just volume? Since this is still a work in progress and we have not published as of yet, I would rather not be more specific about the type of animal at this time ;>}. Nonetheless, I can say that the animals I study change "size" depending on their feeding and hydration state. The abdomen in particular undergoes drastic size changes. That being said, there are key anatomical features that remain fixed in the adult.

Now, there *might* be a way to work volume into the PCA. Although volume is not a reliable metric since the abdomen size is so changeable while the animal is alive, but what about preserved specimens? I have many that have been marinating in ethanol for months. Wouldn't the tissues have equilibrated by now? Probably... I could measure volume by displacement or suspension, I suppose.

In the meantime, here's a few thoughts:

1) 	Use the contribution % (known as C% hereafter) of each variable on principle components 1 and 2.

2) 	The total contribution of a variable that explains the variations retained by PC1 an PC2 is calculated by:
	
	sum(C%1 * eigenvalue1, C%2 * eigenvalue2)

3) Scale() to mean-center the columns of the data set.

4) Use these total contributions as the weights of an arithmetic mean.

For example, we have an animal with the following data (mean-centered):
weight:	1.334
interoc:	-0.225
clength:	0.046
cwidth:	-0.847

The contributions of these variables on PC1 and PC2 are (% changed to proportions):
weight:	0.556
interoc:	0.357
clength:	0.493
cwidth:	0.291

To calculate size:
1.334(0.556) - 0.225(0.357) + 0.046(0.493) - 0.847(0.291) = 0.43758 Then divide by the sum of the weights:
0.43758 / 1.697 = 0.257855 = "animal size"

This value can then be used to rank the animal according to its size for further analysis...

Does this sound like a reasonable application of my PCA data?

Salvatore A. Sidoti
PhD Student
Behavioral Ecology

-----Original Message-----
From: Jim Lemon [mailto:drjimlemon at gmail.com]
Sent: Sunday, November 13, 2016 3:53 PM
To: Sidoti, Salvatore A. <sidoti.23 at buckeyemail.osu.edu>; r-help mailing list <r-help at r-project.org>
Subject: Re: [R] Principle Component Analysis: Ranking Animal Size Based On Combined Metrics

Hi Salvatore,
If by "size" you mean volume, why not directly measure the volume of your animals? They appear to be fairly small. Sometimes working out what the critical value actually means can inform the way to measure it.

Jim


On Sun, Nov 13, 2016 at 4:46 PM, Sidoti, Salvatore A.
<sidoti.23 at buckeyemail.osu.edu> wrote:
> Let's say I perform 4 measurements on an animal: three are linear measurements in millimeters and the fourth is its weight in milligrams. So, we have a data set with mixed units.
>
> Based on these four correlated measurements, I would like to obtain one "score" or value that describes an individual animal's size. I considered simply taking the geometric mean of these 4 measurements, and that would give me a "score" - larger values would be for larger animals, etc.
>
> However, this assumes that all 4 of these measurements contribute equally to an animal's size. Of course, more than likely this is not the case. I then performed a PCA to discover how much influence each variable had on the overall data set. I was hoping to use this analysis to refine my original approach.
>
> I honestly do not know how to apply the information from the PCA to this particular problem...
>
> I do know, however, that principle components 1 and 2 capture enough of the variation to reduce the number of dimensions down to 2 (see analysis below with the original data set).
>
> Note: animal weights were ln() transformed to increase correlation with the 3 other variables.
>
> df <- data.frame(
>   weight = log(1000*c(0.0980, 0.0622, 0.0600, 0.1098, 0.0538, 0.0701, 0.1138, 0.0540, 0.0629, 0.0930,
>              0.0443, 0.1115, 0.1157, 0.0734, 0.0616, 0.0640, 0.0480, 0.1339, 0.0547, 0.0844,
>              0.0431, 0.0472, 0.0752, 0.0604, 0.0713, 0.0658, 0.0538, 0.0585, 0.0645, 0.0529,
>              0.0448, 0.0574, 0.0577, 0.0514, 0.0758, 0.0424, 0.0997, 0.0758, 0.0649, 0.0465,
>              0.0748, 0.0540, 0.0819, 0.0732, 0.0725, 0.0730, 0.0777, 0.0630, 0.0466)),
>   interoc = c(0.853, 0.865, 0.811, 0.840, 0.783, 0.868, 0.818, 0.847, 0.838, 0.799,
>               0.737, 0.788, 0.731, 0.777, 0.863, 0.877, 0.814, 0.926, 0.767, 0.746,
>               0.700, 0.768, 0.807, 0.753, 0.809, 0.788, 0.750, 0.815, 0.757, 0.737,
>               0.759, 0.863, 0.747, 0.838, 0.790, 0.676, 0.857, 0.728, 0.743, 0.870,
>               0.787, 0.773, 0.829, 0.785, 0.746, 0.834, 0.829, 0.750, 0.842),
>   cwidth = c(3.152, 3.046, 3.139, 3.181, 3.023, 3.452, 2.803, 3.050, 3.160, 3.186,
>              2.801, 2.862, 3.183, 2.770, 3.207, 3.188, 2.969, 3.033, 2.972, 3.291,
>              2.772, 2.875, 2.978, 3.094, 2.956, 2.966, 2.896, 3.149, 2.813, 2.935,
>              2.839, 3.152, 2.984, 3.037, 2.888, 2.723, 3.342, 2.562, 2.827, 2.909,
>              3.093, 2.990, 3.097, 2.751, 2.877, 2.901, 2.895, 2.721, 2.942),
>   clength = c(3.889, 3.733, 3.762, 4.059, 3.911, 3.822, 3.768, 3.814, 3.721, 3.794,
>               3.483, 3.863, 3.856, 3.457, 3.996, 3.876, 3.642, 3.978, 3.534, 3.967,
>               3.429, 3.518, 3.766, 3.755, 3.706, 3.785, 3.607, 3.922, 3.453, 3.589,
>               3.508, 3.861, 3.706, 3.593, 3.570, 3.341, 3.916, 3.336, 3.504, 3.688,
>               3.735, 3.724, 3.860, 3.405, 3.493, 3.586, 3.545, 3.443,
> 3.640))
>
> pca_morpho <- princomp(df, cor = TRUE)
>
> summary(pca_morpho)
>
> Importance of components:
>                                         Comp.1          Comp.2          Comp.3          Comp.4
> Standard deviation      1.604107        0.8827323       0.7061206       0.3860275
> Proportion of Variance  0.643290        0.1948041       0.1246516       0.0372543
> Cumulative Proportion   0.643290        0.8380941       0.9627457       1.0000000
>
> Loadings:
>                         Comp.1  Comp.2  Comp.3  Comp.4
> weight          -0.371          0.907                           -0.201
> interoc         -0.486  -0.227  -0.840
> cwidth          -0.537  -0.349          0.466           -0.611
> clength         -0.582                          0.278   0.761
>
>                         Comp.1  Comp.2  Comp.3  Comp.4
> SS loadings             1.00            1.00            1.00            1.00
> Proportion Var          0.25            0.25            0.25            0.25
> Cumulative Var          0.25            0.50            0.75            1.00
>
> Any guidance will be greatly appreciated!
>
> Salvatore A. Sidoti
> PhD Student
> The Ohio State University
> Behavioral Ecology
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From WOLFS15 at ecu.edu  Mon Nov 14 17:26:45 2016
From: WOLFS15 at ecu.edu (Wolf, Steven)
Date: Mon, 14 Nov 2016 16:26:45 +0000
Subject: [R] Question about expression parser for "return" statement
In-Reply-To: <90a8ce98-ffe5-0484-e5c1-1e0fd6bc332c@gmail.com>
References: <CAOogVzUOtMUXnvjFg37aGhDqtT9NmULGFiez4_jv1vGYZLDw0g@mail.gmail.com>
	<52831814-c240-04f8-3069-b670552a3941@gmail.com>
	<665597dc-55fd-9916-6564-38400262a270@gmail.com>
	<90a8ce98-ffe5-0484-e5c1-1e0fd6bc332c@gmail.com>
Message-ID: <1479140805.2360.4.camel@ecu.edu>

Just to add on a bit, please note that the return is superfluous.  If you write this:


normalDensityFunction = function(x, Mean, Variance) {

    # no  "return" value given at all

    (1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance)

}

normalDensityFunction(2,0,1)

...you get the right answer again.

This is not "best practices", and Duncan will probably give you 10 reasons why you should never do it this way.  But if the parentheses behavior bothers you enough, you can subvert it.  This probably won't work so well if you try to make any more complicated output.

Caveat Emptor.

-SW


--
Steven Wolf, PhD
Assistant Professor
Department of Physics
STEM CoRE -- STEM Collaborative for Research in Education
http://www.ecu.edu/cs-acad/aa/StemCore
East Carolina University
Phone: 252-737-5229




On Sun, 2016-11-13 at 13:35 -0500, Duncan Murdoch wrote:

On 13/11/2016 7:58 AM, Duncan Murdoch wrote:


On 13/11/2016 6:47 AM, Duncan Murdoch wrote:


On 13/11/2016 12:50 AM, Dave DeBarr wrote:


I've noticed that if I don't include parentheses around the intended return
value for the "return" statement, R will assume the first parenthetical
expression is the intended return value ... even if that parenthetical
expression is only part of a larger expression.

Is this intentional?



Yes, return is just a function call that has side effects.  As far as
the parser is concerned,

return ((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance))

is basically the same as

f((1/sqrt(2*pi*Variance))*exp(-(1/2)*((x - Mean)^2)/Variance))



By the way, out of curiosity I took a look at the source of CRAN
packages to see if this actually occurs.  It turns out that "return" is
used as a variable name often enough to make automatic tests tricky, so
I don't know the answer to my question.  However, I did turn up a number
of cases where people have code like this:

     if (name == "") return;

(from the bio.infer package), which never calls return(), so doesn't
actually do what the author likely intended



I searched the R sources and the sources of CRAN packages, and found
this is a reasonably common problem:  it's in 111 packages, including
one in base R.  I'll be emailing the maintainers to let them know.

I'll see about putting a check for this into R CMD check.

Duncan Murdoch




	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Mon Nov 14 22:56:50 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Mon, 14 Nov 2016 21:56:50 +0000
Subject: [R] Issues with the way Apply handled NA's
Message-ID: <b324d2641512438cb3a9cc70c6b2cc2f@exch-2p-mbx-w2.ads.tamu.edu>

This behavior is documented in the manual page:

> prod(NULL)
[1] 1

You can check for an empty vector as follows:

plabor <- structure(list(colA = c(6, NA, 3, 4), colB = c(25, NA, 2, 7), 
    colC = c(3, NA, 19, NA)), .Names = c("colA", "colB", "colC"),
    class = "data.frame", row.names = c(NA, -4L))
# Use dput() to send data to the list

plabor$colD = apply(plabor[c("colA","colB","colC")], 1, prod, na.rm=TRUE)
# Use TRUE and FALSE since the abbreviations T and F are not reserved and
# could be redefined

vals <- apply(plabor[c("colA","colB","colC")],1,function(x) length(na.omit(x)))
vals
# [1] 3 0 3 2
plabor$colD <- ifelse(vals>0, plabor$colD, NA)
plabor

#   colA colB colC colD
# 1    6   25    3  450
# 2   NA   NA   NA   NA
# 3    3    2   19  114
# 4    4    7   NA   28

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Olu Ola via R-help
Sent: Monday, November 14, 2016 2:52 PM
To: R-help Mailing List
Subject: [R] Issues with the way Apply handled NA's

 Hello,I have a data set called plabor and have the following format:

| ColA | ColB | Colc |
| 6 | 25 | 3 |
| NA | NA | NA |
| 3 | 2 | 19 |
| 4 | 7 | NA |


I wanted to find the product of the three columns?for each of the rows?and I used the apply function follows:
plabor$colD = apply(plabor[c("colA","colB","colc")],1,prod,na.rm=T)
The result are as follows:

| ColA | ColB | Colc | colD |
| 6 | 25 | 3 | 450 |
| NA | NA | NA | 1 |
| 3 | 2 | 19 | 114 |
| 4 | 7 | NA | 28 |


The second row results is 1 instead of being ignored.
How do I deal with this issue because I do not want to exclude these data points with all NA's?
Regards

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From hpages at fredhutch.org  Mon Nov 14 23:19:47 2016
From: hpages at fredhutch.org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Mon, 14 Nov 2016 14:19:47 -0800
Subject: [R] Frequency of a character in a string
In-Reply-To: <CAGxFJbTREpkZzrvFGj0arz=PMfBfU9K+eK+u5BukNC6ezQOzPQ@mail.gmail.com>
References: <trinity-bcc6f83f-cf25-4f2d-8e01-78230c379a8e-1479113050372@3capp-gmx-bs45>
	<07DE66F1-08A4-4C68-8529-DCA2A76A3BD9@gmail.com>
	<CAHUdqRvjOGPVKdm4DX7HkeXWtec-yneD-dK_YfgDnpyUH242tQ@mail.gmail.com>
	<CAGxFJbS8PZP7frCA_v74q53kqY8bLHwfi-HGp_gjfYbm7_cK1g@mail.gmail.com>
	<alpine.OSX.2.20.1611140916100.809@charles-berrys-macbook.local>
	<FAE947AF-3CF8-4E1F-92E9-44325CC96964@me.com>
	<alpine.OSX.2.20.1611141141390.1014@charles-berrys-macbook.local>
	<6c44e6c6-8ef1-d506-d0ed-1a9c5d555998@fredhutch.org>
	<CAGxFJbTREpkZzrvFGj0arz=PMfBfU9K+eK+u5BukNC6ezQOzPQ@mail.gmail.com>
Message-ID: <c012bea9-4755-3643-2934-a20e790e75ba@fredhutch.org>



On 11/14/2016 12:44 PM, Bert Gunter wrote:
> (Sheepishly)...
>
> Yes, thank you Herv?. It would have been nice if I had given correct
> soutions. Fixed = TRUE could not have of course worked with ["a"]
> character class!
>
> Here's what I found with a 10 element vector each member of which is a
> 1e5 length string:
>
>> system.time((lengths(strsplit(paste0("X", x, "X"),"a",fixed=TRUE)) - 1))
>    user  system elapsed
>   0.013   0.000   0.013
>
>> system.time(nchar(gsub("[^a]", "", x,fixed = FALSE)))
>    user  system elapsed
>   0.251   0.000   0.252
> ## WAYYYY slower
>
>
>> system.time(nchar(x) - nchar(gsub("a", "", x,fixed = TRUE)))
>    user  system elapsed
>   0.007   0.000   0.007
> ## twice as fast
>
>
>
> Clearly and unsurprisingly, the message is to avoid fixed = FALSE;
> after that, it seems mostly to be: who cares?!

Another message is to pay attention to the "cost" of generating a
big intermediate objects like the list returned by strsplit(). On a
big character vector made of 5000 strings of about 1e5 random letters
each, the strsplit-based solution uses more than 2Gb of RAM on my
Ubuntu system. The gsub( , fixed=TRUE) solution uses less than 1Gb.

Cheers,
H.

>
>
> Cheers,
> Bert
>
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Mon, Nov 14, 2016 at 12:26 PM, Herv? Pag?s <hpages at fredhutch.org> wrote:
>> Hi,
>>
>> FWIW using gsub( , fixed=TRUE) is faster than using gsub( , fixed=FALSE)
>> or strsplit( , fixed=TRUE):
>>
>>   set.seed(1)
>>   Vec <- paste(sample(letters, 5000000, replace = TRUE), collapse = "")
>>
>>   system.time(res1 <- nchar(gsub("[^a]", "", Vec)))
>>   #  user  system elapsed
>>   # 0.585   0.000   0.586
>>
>>   system.time(res2 <- lengths(strsplit(Vec,"a",fixed=TRUE)) - 1L)
>>   #  user  system elapsed
>>   # 0.061   0.000   0.061
>>
>>   system.time(res3 <- nchar(Vec) - nchar(gsub("a", "", Vec, fixed=TRUE)))
>>   #  user  system elapsed
>>   # 0.039   0.000   0.039
>>
>>   identical(res1, res2)
>>   # [1] TRUE
>>   identical(res1, res3)
>>   # [1] TRUE
>>
>> The gsub( , fixed=TRUE) solution also uses slightly less memory than the
>> strsplit( , fixed=TRUE) solution.
>>
>> Cheers,
>> H.
>>
>>
>> On 11/14/2016 11:55 AM, Charles C. Berry wrote:
>>>
>>> On Mon, 14 Nov 2016, Marc Schwartz wrote:
>>>
>>>>
>>>>> On Nov 14, 2016, at 11:26 AM, Charles C. Berry <ccberry at ucsd.edu> wrote:
>>>>>
>>>>> On Mon, 14 Nov 2016, Bert Gunter wrote:
>>>>>
>>> [stuff deleted]
>>>
>>>> Hi,
>>>>
>>>> Both gsub() and strsplit() are using regex based pattern matching
>>>> internally. That being said, they are ultimately calling .Internal
>>>> code, so both are pretty fast.
>>>>
>>>> For comparison:
>>>>
>>>> ## Create a 1,000,000 character vector
>>>> set.seed(1)
>>>> Vec <- paste(sample(letters, 1000000, replace = TRUE), collapse = "")
>>>>
>>>>> nchar(Vec)
>>>>
>>>> [1] 1000000
>>>>
>>>> ## Split the vector into single characters and tabulate
>>>>>
>>>>> table(strsplit(Vec, split = "")[[1]])
>>>>
>>>>
>>>>    a     b     c     d     e     f     g     h     i     j     k     l
>>>> 38664 38442 38282 38496 38540 38623 38548 38288 38143 38493 38184 38621
>>>>    m     n     o     p     q     r     s     t     u     v     w     x
>>>> 38306 38725 38705 38144 38529 38809 38575 38355 38386 38364 38904 38310
>>>>    y     z
>>>> 38265 38299
>>>>
>>>>
>>>> ## Get just the count of "a"
>>>>>
>>>>> table(strsplit(Vec, split = "")[[1]])["a"]
>>>>
>>>>    a
>>>> 38664
>>>>
>>>>> nchar(gsub("[^a]", "", Vec))
>>>>
>>>> [1] 38664
>>>>
>>>>
>>>> ## Check performance
>>>>>
>>>>> system.time(table(strsplit(Vec, split = "")[[1]])["a"])
>>>>
>>>>   user  system elapsed
>>>>  0.100   0.007   0.107
>>>>
>>>>> system.time(nchar(gsub("[^a]", "", Vec)))
>>>>
>>>>   user  system elapsed
>>>>  0.270   0.001   0.272
>>>>
>>>>
>>>> So, the above would suggest that using strsplit() is somewhat faster
>>>> than using gsub(). However, as Chuck notes, in the absence of more
>>>> exhaustive benchmarking, the difference may or may not be more
>>>> generalizable.
>>>
>>>
>>>
>>> Whether splitting on fixed strings rather than treating them as
>>> regex'es (i.e.`fixed=TRUE') makes a big difference seems to depend on
>>> what you split:
>>>
>>> First repeating what Marc did...
>>>
>>>> system.time(table(strsplit(Vec, split = "",fixed=TRUE)[[1]])["a"])
>>>
>>>    user  system elapsed
>>>   0.132   0.010   0.139
>>>>
>>>> system.time(table(strsplit(Vec, split = "",fixed=FALSE)[[1]])["a"])
>>>
>>>    user  system elapsed
>>>   0.130   0.010   0.138
>>>
>>> ... fixed=TRUE hardly matters. But the idiom I proposed...
>>>
>>>> system.time(sum(lengths(strsplit(paste0("X", Vec,
>>>> "X"),"a",fixed=TRUE)) - 1))
>>>
>>>    user  system elapsed
>>>   0.017   0.000   0.018
>>>>
>>>> system.time(sum(lengths(strsplit(paste0("X", Vec,
>>>> "X"),"a",fixed=FALSE)) - 1))
>>>
>>>    user  system elapsed
>>>   0.104   0.000   0.104
>>>>
>>>>
>>>
>>> ... is 5 times faster with fixed=TRUE for this case.
>>>
>>> This result matchea Marc's count:
>>>
>>>> sum(lengths(strsplit(paste0("X", Vec, "X"),"a",fixed=FALSE)) - 1)
>>>
>>> [1] 38664
>>>>
>>>>
>>>
>>> Chuck
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>> --
>> Herv? Pag?s
>>
>> Program in Computational Biology
>> Division of Public Health Sciences
>> Fred Hutchinson Cancer Research Center
>> 1100 Fairview Ave. N, M1-B514
>> P.O. Box 19024
>> Seattle, WA 98109-1024
>>
>> E-mail: hpages at fredhutch.org
>> Phone:  (206) 667-5791
>> Fax:    (206) 667-1319
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From davidsmi at microsoft.com  Mon Nov 14 23:25:59 2016
From: davidsmi at microsoft.com (David Smith)
Date: Mon, 14 Nov 2016 22:25:59 +0000
Subject: [R] Revolutions blog: October 2016 roundup
Message-ID: <CY1PR0301MB2105EF8AE00E9E40AC9A4D63C8BC0@CY1PR0301MB2105.namprd03.prod.outlook.com>

Since 2008, Microsoft (formerly Revolution Analytics) staff and guests have written about R every weekday at the
Revolutions blog: http://blog.revolutionanalytics.com
and every month I post a summary of articles from the previous month of particular interest to readers of r-help.

And in case you missed them, here are some articles related to R from the month of October:

A brief summary of the R 3.3.2 release: http://blog.revolutionanalytics.com/2016/10/r-332-now-available.html

"Data Science with SQL Server 2016", a free E-book featuring several in-depth R examples, is now available for download:
http://blog.revolutionanalytics.com/2016/10/data-science-with-sql-server-2016.html

The ReporterRs package makes it easy to insert R output, tables and graphics into Word and Powerpoint templates:
http://blog.revolutionanalytics.com/2016/10/reporters.html

R-hub, an on-line service to build and check R packages on multiple platforms, is now in public beta test:
http://blog.revolutionanalytics.com/2016/10/r-hub-public-beta.html

A style guide for R programs from Graham Williams, creator of rattle:
http://blog.revolutionanalytics.com/2016/10/sharing-r-code-with-style.html

The Economist used R and the Emotion API to track emotions of the US presidential candidates during the debates:
http://blog.revolutionanalytics.com/2016/10/debate-emotions.html

A new R Graph Gallery by Yan Holtz contains hundreds of data charts and their R code:
http://blog.revolutionanalytics.com/2016/10/the-r-graph-gallery-is-back.html

R Tools for Visual Studio 0.5 adds support for publishing R code as a SQL Server stored procedure:
http://blog.revolutionanalytics.com/2016/10/rtvs-05-now-available.html

After an accident, a data scientist estimates the value of a written-off vehicle with R:
http://blog.revolutionanalytics.com/2016/10/car-valuation.html

The "Team Data Science Process" and two new open-source projects from Microsoft: a visualization and exploration
framework; and a statistical reporting tool based on caret:
http://blog.revolutionanalytics.com/2016/10/the-team-data-science-process.html

An R function for "tilegrams", like US maps with states scaled to electoral college votes:
http://blog.revolutionanalytics.com/2016/10/tilegrams-in-r.html

Upcoming data science courses in Zurich, Oslo and Stockholm:
http://blog.revolutionanalytics.com/2016/10/practical-data-science.html

A tutorial on using R on Spark with SparkR, sparklyr, and RevoScaleR:
http://blog.revolutionanalytics.com/2016/10/tutorial-scalable-r-on-spark.html

An animated globe showing the impact of climate change, created with R:
http://blog.revolutionanalytics.com/2016/10/warming-globe.html

The ggiraph package makes it easy to add interactivity to ggplot2 graphics on the web:
http://blog.revolutionanalytics.com/2016/10/make-ggplot-graphics2-interactive-with-ggiraph.html

The haven package supports reading SAS, SPSS, Stata and other data file formats into R:
http://blog.revolutionanalytics.com/2016/10/import-data-to-r-from-other-statistics-tools-with-haven.html

More than half of published papers in Psychology contain at least one statistical reporting error, the statcheck package
reveals: http://blog.revolutionanalytics.com/2016/10/statcheck.html

Build data pipelines with Azure Data Factory and Microsoft R Server:
http://blog.revolutionanalytics.com/2016/10/r-server-data-factory.html

R used to analyze the scripts of "The Simpsons", and create a chart in the cartoon's unique style:
http://blog.revolutionanalytics.com/2016/10/homer-not-bart-is-the-star-of-the-simpsons.html

General interest stories (not related to R) in the past month included: rules for rulers
(http://blog.revolutionanalytics.com/2016/10/because-its-friday-dictators.html), a Hitchcock-Kubrick video mashup
(http://blog.revolutionanalytics.com/2016/10/because-its-friday-hitchcock-vs-kubrick.html), the Earth from the Moon
(http://blog.revolutionanalytics.com/2016/10/because-its-friday-earthrise.html), and the Dear Data project
(http://blog.revolutionanalytics.com/2016/10/because-its-friday-dear-data.html).

If you're looking for more articles about R, you can find summaries from previous months at
http://blog.revolutionanalytics.com/roundups/. You can receive daily blog posts via email using services like
blogtrottr.com.

As always, thanks for the comments and please keep sending suggestions to me at davidsmi at microsoft.com or via Twitter
(I'm @revodavid).

Cheers,
# David

-- 
David M Smith <davidsmi at microsoft.com>
R Community Lead, Microsoft? 
Tel: +1 (312) 9205766 (Chicago IL, USA)
Twitter: @revodavid | Blog: ?http://blog.revolutionanalytics.com


From paul at stat.auckland.ac.nz  Tue Nov 15 01:46:36 2016
From: paul at stat.auckland.ac.nz (Paul Murrell)
Date: Tue, 15 Nov 2016 13:46:36 +1300
Subject: [R] [FORGED] Re: [FORGED] How to remove box in Venn plots
 (Vennerable package, uses grid) - similar to bty="n" in standard plots
In-Reply-To: <AM3PR05MB13965DCEE2E6E02EB134B3D5B1BC0@AM3PR05MB1396.eurprd05.prod.outlook.com>
References: <AM3PR05MB1396C5F07B1471C2E98A580DB1BB0@AM3PR05MB1396.eurprd05.prod.outlook.com>
	<9684dcbb-4934-c9be-fa0f-58623edfc525@stat.auckland.ac.nz>
	<AM3PR05MB13965DCEE2E6E02EB134B3D5B1BC0@AM3PR05MB1396.eurprd05.prod.outlook.com>
Message-ID: <25ace1d4-ea95-8c9a-a3b1-5573f960bd15@stat.auckland.ac.nz>

Hi

Glad I could help.

Here's a way you could get rid of the rectangle in the first place ...

library(Vennerable)
groups<-list(set1=1:100, set2=80:120)
V<-Venn(groups)
C<-compute.Venn(V)
X11(w=7,h=7)
grid.newpage()

plot(C, show=list(Universe=FALSE))

It required crawling through the 'Vennerable' source code a bit to find 
that 'show' argument, but that appears to do the trick.

Paul

On 15/11/16 02:08, DE LAS HERAS Jose wrote:
> Hi,
>
>
> The grid.ls() and grid.remove() approach worked beautifully to remove
> the box, thank you! Because the box is the first thing to be drawn, it
> is the first object shown by grid.ls(), so I can easily add a line of
> code to automatically remove the box. Result!
>
>
> Although I'd still like to know how one chooses not to plot that box in
> the first place. I really must study a little the grid package. I've
> survived using base R plots and they work very nicely, but it looks like
> you can do a lot of cool stuff with grid.
>
>
> In case you might know the answer and feel like adding a comment here
> (I'm already very happy with the grid.ls() approach, thanks! :)) this is
> a simple code example without making it look pretty or anything:
>
>
>
>
> library(Vennerable)
> groups<-list(set1=1:100, set2=80:120)
> V<-Venn(groups)
> C<-compute.Venn(V)
> X11(w=7,h=7)
> grid.newpage()
> plot(C)
>
> class(C)
> [1] "VennDrawing"
> attr(,"package")
> [1] "Vennerable"
>
> I don't want the black box around the diagram.
> I was able to overwrite it with a white box like this:
>
> vp=viewport(x=0.5, y=0.5, width=0.95, height=0.75)
> pushViewport(vp)
> grid.rect(gp=gpar(lty=1, col="white", lwd=15))
> upViewport() # needs to be executed to return focus upwards
>
> but as the box has different dimensions depending on the actual sets
> being drawn, the width and height must be found empirically each time. A
> bit boring it you need to produce a bunch of figures at once.
>
> I was wondering what parameter I could include in the call to 'plot'
> that would prevent the box from being drawn. Usually this is achieved
> with bty="n", but the method for plotting a "VennDrawing" structure uses
> the grid package and I'm lost there at the moment.
>
> Thank you for your help again, grid.ls() etc is a very cool and
> flexible approach
>
> Jose
>
>
>
>
> ------------------------------------------------------------------------
> *From:* Paul Murrell <paul at stat.auckland.ac.nz>
> *Sent:* 13 November 2016 19:57
> *To:* DE LAS HERAS Jose; R-help at r-project.org
> *Subject:* Re: [FORGED] [R] How to remove box in Venn plots (Vennerable
> package, uses grid) - similar to bty="n" in standard plots
>
> Hi
>
> Can you supply some example code?
>
> You might get some joy from grid.ls() to identify the box followed by
> grid.remove() to get rid of it;  some example code would allow me to
> provide more detailed advice.
>
> Paul
>
> On 12/11/16 05:12, DE LAS HERAS Jose wrote:
>> I'm using the package Vennerable to make Venn diagrams, but it always
>> makes a box around the diagram.
>>
>> Using standard R plots I could eliminate that by indicating
>>
>>
>> bty="n"
>>
>>
>> but it seems Vennerable uses the Grid package to generate its plots
>> and I'm not really familiar enough with Grid. I was looking at the
>> documentation but I can't seem to find a way to achieve that. I'd
>> even be happy drawing a white rectangle with wide lines to overplot
>> the box, but there must be a way to not draw the box in the first
>> place.
>>
>>
>> Anybody knows how?
>>
>>
>> Jose
>>
>> --
>>
>> Dr. Jose I. de las Heras The Wellcome Trust Centre for Cell Biology
>> Swann Building Max Born Crescent University of Edinburgh Edinburgh
>> EH9 3BF UK
>>
>> Phone: +44 (0)131 6507090
>>
>> Fax:  +44 (0)131 6507360
>>
>>
>>
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>>
>>
>> ______________________________________________ R-help at r-project.org
>> mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> R-help -- Main R Mailing List: Primary help - Homepage - SfS
> <https://stat.ethz.ch/mailman/listinfo/r-help>
> stat.ethz.ch
> The main R mailing list, for announcements about the development of R
> and the availability of new code, questions and answers about problems
> and solutions using R ...
>
>
>
>> posting guide http://www.R-project.org/posting-guide.html and provide
>> commented, minimal, self-contained, reproducible code.
>>
>
> --
> Dr Paul Murrell
> Department of Statistics
> The University of Auckland
> Private Bag 92019
> Auckland
> New Zealand
> 64 9 3737599 x85392
> paul at stat.auckland.ac.nz
> http://www.stat.auckland.ac.nz/~paul/
> Paul Murrell's Home Page <http://www.stat.auckland.ac.nz/~paul/>
> www.stat.auckland.ac.nz
> Department. My department home page. Research. The home page for R: A
> language and environment for computing and graphics (my R graphics todo
> list).
>
>
>
>
>
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From farnoosh_81 at yahoo.com  Mon Nov 14 23:38:23 2016
From: farnoosh_81 at yahoo.com (Farnoosh Sheikhi)
Date: Mon, 14 Nov 2016 22:38:23 +0000 (UTC)
Subject: [R] unique dates per ID
References: <961660187.4189324.1479163103170.ref@mail.yahoo.com>
Message-ID: <961660187.4189324.1479163103170@mail.yahoo.com>

Hi,?
I have a data set like below:
Subject<- c("2", "2", "2", "3", "3", "3", "4", "4", "5", "5", "5", "5")dates<-c("2011-01-01", "2011-01-01", "2011-01-03" ,"2011-01-04", "2011-01-05", "2011-01-06" ,"2011-01-07", "2011-01-07", "2011-01-09" ,"2011-01-10"? ? ? ? ?,"2011-01-11" ,"2011-01-11")deps<-c("A", "B", "CC", "C", "CC", "A", "F", "DD", "A", "F", "FF", "D")df <- data.frame(Subject, dates, deps); df
I want to choose unique dates per ID in a way there are not duplicate dates per ID. I don't mind what department to pick.?I really appreciate any help.?Best,Farnoosh


	[[alternative HTML version deleted]]


From ulrik.stervbo at gmail.com  Tue Nov 15 08:21:50 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Tue, 15 Nov 2016 07:21:50 +0000
Subject: [R] unique dates per ID
In-Reply-To: <961660187.4189324.1479163103170@mail.yahoo.com>
References: <961660187.4189324.1479163103170.ref@mail.yahoo.com>
	<961660187.4189324.1479163103170@mail.yahoo.com>
Message-ID: <CAKVAULOZMieycoQA55gSNN_TOmp0yYNQ18w9RJD8wkGrqeGUOA@mail.gmail.com>

Hi Farnoosh,

you can use unique in the R-base or distinct from the dplyr library.

Best
Ulrik

On Tue, 15 Nov 2016 at 06:59 Farnoosh Sheikhi via R-help <
r-help at r-project.org> wrote:

> Hi,
> I have a data set like below:
> Subject<- c("2", "2", "2", "3", "3", "3", "4", "4", "5", "5", "5",
> "5")dates<-c("2011-01-01", "2011-01-01", "2011-01-03" ,"2011-01-04",
> "2011-01-05", "2011-01-06" ,"2011-01-07", "2011-01-07", "2011-01-09"
> ,"2011-01-10"         ,"2011-01-11" ,"2011-01-11")deps<-c("A", "B", "CC",
> "C", "CC", "A", "F", "DD", "A", "F", "FF", "D")df <- data.frame(Subject,
> dates, deps); df
> I want to choose unique dates per ID in a way there are not duplicate
> dates per ID. I don't mind what department to pick. I really appreciate any
> help. Best,Farnoosh
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Tue Nov 15 08:38:07 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 15 Nov 2016 18:38:07 +1100
Subject: [R] unique dates per ID
In-Reply-To: <961660187.4189324.1479163103170@mail.yahoo.com>
References: <961660187.4189324.1479163103170.ref@mail.yahoo.com>
	<961660187.4189324.1479163103170@mail.yahoo.com>
Message-ID: <CA+8X3fX8O0BVGvHoGB9Pj5ZZ5HiZUZ+8nJtU15gfwPk4+JWtpw@mail.gmail.com>

Hi Farnoosh,
Try this:

for(id in unique(df$Subject)) {
 whichsub<-df$Subject==id
 if(exists("newdf"))
  newdf<-rbind(newdf,df[whichsub,][which(!duplicated(df$dates[whichsub])),])
 else newdf<-df[whichsub,][which(!duplicated(df$dates[whichsub])),]
}

Jim


On Tue, Nov 15, 2016 at 9:38 AM, Farnoosh Sheikhi via R-help
<r-help at r-project.org> wrote:
> Hi,
> I have a data set like below:
> Subject<- c("2", "2", "2", "3", "3", "3", "4", "4", "5", "5", "5", "5")dates<-c("2011-01-01", "2011-01-01", "2011-01-03" ,"2011-01-04", "2011-01-05", "2011-01-06" ,"2011-01-07", "2011-01-07", "2011-01-09" ,"2011-01-10"         ,"2011-01-11" ,"2011-01-11")deps<-c("A", "B", "CC", "C", "CC", "A", "F", "DD", "A", "F", "FF", "D")df <- data.frame(Subject, dates, deps); df
> I want to choose unique dates per ID in a way there are not duplicate dates per ID. I don't mind what department to pick. I really appreciate any help. Best,Farnoosh
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From roundsjeremiah at gmail.com  Tue Nov 15 09:40:38 2016
From: roundsjeremiah at gmail.com (jeremiah rounds)
Date: Tue, 15 Nov 2016 00:40:38 -0800
Subject: [R] unique dates per ID
In-Reply-To: <CA+8X3fX8O0BVGvHoGB9Pj5ZZ5HiZUZ+8nJtU15gfwPk4+JWtpw@mail.gmail.com>
References: <961660187.4189324.1479163103170.ref@mail.yahoo.com>
	<961660187.4189324.1479163103170@mail.yahoo.com>
	<CA+8X3fX8O0BVGvHoGB9Pj5ZZ5HiZUZ+8nJtU15gfwPk4+JWtpw@mail.gmail.com>
Message-ID: <CAOjnRsZg03c+AgHBKyQZwzfj1maCmOqnVi4Ddz81je_2VqhkRA@mail.gmail.com>

library(data.table)
setDT(df)
setkeyv(df, c("Subject", "dates"))
unique(df)  #gets what you want.

On Mon, Nov 14, 2016 at 11:38 PM, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Farnoosh,
> Try this:
>
> for(id in unique(df$Subject)) {
>  whichsub<-df$Subject==id
>  if(exists("newdf"))
>   newdf<-rbind(newdf,df[whichsub,][which(!duplicated(
> df$dates[whichsub])),])
>  else newdf<-df[whichsub,][which(!duplicated(df$dates[whichsub])),]
> }
>
> Jim
>
>
> On Tue, Nov 15, 2016 at 9:38 AM, Farnoosh Sheikhi via R-help
> <r-help at r-project.org> wrote:
> > Hi,
> > I have a data set like below:
> > Subject<- c("2", "2", "2", "3", "3", "3", "4", "4", "5", "5", "5",
> "5")dates<-c("2011-01-01", "2011-01-01", "2011-01-03" ,"2011-01-04",
> "2011-01-05", "2011-01-06" ,"2011-01-07", "2011-01-07", "2011-01-09"
> ,"2011-01-10"         ,"2011-01-11" ,"2011-01-11")deps<-c("A", "B", "CC",
> "C", "CC", "A", "F", "DD", "A", "F", "FF", "D")df <- data.frame(Subject,
> dates, deps); df
> > I want to choose unique dates per ID in a way there are not duplicate
> dates per ID. I don't mind what department to pick. I really appreciate any
> help. Best,Farnoosh
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jl.iccp at gmail.com  Tue Nov 15 11:26:00 2016
From: jl.iccp at gmail.com (Jue Lin-Ye)
Date: Tue, 15 Nov 2016 11:26:00 +0100
Subject: [R]
	=?utf-8?q?=E2=80=8BMessage_from_Lanzhou_University=2C_China?=
	=?utf-8?b?LiAoSGVscCk=?=
Message-ID: <CAAMkPW9L2w64PgGJ5hPXaQ1iePa-WroMqmwrQYMPwnEfw06+bA@mail.gmail.com>

?
?
Dear Mr. Song,

S
??
orry for the late response.

?My response to you, is that, first, y
ou are not assuming a linear relationship in your formula, because the
argument k of your smooth term s() is not 1.

?Now, ?
I cannot see figure 1, but you can obtain figure 2 from the formula that
you describe here. In order to obtain a
?plot with the ?
derivative, you have to create a ne
?w?
column in your matrix or data-frame that is the derivative of the blood
pressure, and then apply gam()
? as in

?
?
ct=gam(
?derivative(?
Blood pressure
?)?
~s(Temp,k=5)+s(BMI,k=4)+s(Age,k=6)+s(RH,k=4)+s(Pa,k=
5)+Gender+Season,family=gaussian,data=mydata)
?
.

H
?ope that helps?
. Best regards,

Jue



On Saturday, 12 November 2016, <r-help-request at r-project.org> wrote:
>
>
> Date: Fri, 11 Nov 2016 17:31:23 +0800 (GMT+08:00)
> From: ??? <songxp15 at lzu.edu.cn>
> To: r-help at r-project.org
> Subject: [R]
> ??
> Message from Lanzhou University, China. (Help)
> Message-ID: <6f90a404.73da.15852ba30a2.Coremail.songxp15 at lzu.edu.cn>
> Content-Type: text/plain; charset="utf-8"
>
> Dear moderators,
> I am a student come from Lanzhou University, Gansu, China. I have two
> questions on R (mgcv), which puzzled me for a long time. I did not find
> answer in Chinese forum. I hope you could answer me in your free time or
> post it in R-help. Thank you very much.
>
>  Xuping Song
>
> 2016.11.11
> We want to investigate the association between exposure to ambient
> temperature and blood pressure. The ?mgcv? package was used to perform GAM
> analyses in R. This is the formula.
> ??
> ct=gam(Blood pressure~s(Temp,k=5)+s(BMI,k=4)+s(Age,k=6)+s(RH,k=4)+s(Pa,k=
> 5)+Gender+Season,family=gaussian,data=mydata)
>
> I have two questions. Firstly, personal characteristic (Age, Gender, BMI
> and Season) could influence blood pressure. How to control these variables?
> BMI and Age are continuous variable. I smoothed these two variables like
> formula 1. Season (1,2,3,4) and Gender(0,1) were add to formula, which were
> assumed as linear relationship. Is that right?
>
> Secondly, I want to get plot 2, which x-axis presented ?Temperature? and
> y-axis presented ?Change in Blood Pressure for every 1?C decrease of
> temperature?. Plot 2 came from a paper published recently (see appendix).
> Therefore, I ran formula 1 in R and run ?plot(ct)?. Then I get plot 1. I
> don?t understand the meaning of ?s?Tmean?3.87?? in y-axis. Does it
> represent ?Blood pressure?? If the hypothesis is right, plot 2 is the
> derivative of plot 1. However, I don?t know the specific code. I am a
> beginner of R and this problem puzzled me for a long time. Please help me.
> Thank you very much.
> -------------- next part --------------
> A non-text attachment was scrubbed...
> Name: Paper (plot 2).pdf
> Type: application/pdf
> Size: 913655 bytes
> Desc: not available
> URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20161111/
> 4c9d7aec/attachment.pdf>
> -------------- next part --------------
> A non-text attachment was scrubbed...
> Name: Plot 2.png
> Type: image/png
> Size: 37455 bytes
> Desc: not available
> URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20161111/
> 4c9d7aec/attachment.png>
>
> ?

	[[alternative HTML version deleted]]


From J.delasHeras at ed.ac.uk  Tue Nov 15 11:55:11 2016
From: J.delasHeras at ed.ac.uk (DE LAS HERAS Jose)
Date: Tue, 15 Nov 2016 10:55:11 +0000
Subject: [R] [FORGED] Re: [FORGED] How to remove box in Venn plots
 (Vennerable package, uses grid) - similar to bty="n" in standard plots
In-Reply-To: <25ace1d4-ea95-8c9a-a3b1-5573f960bd15@stat.auckland.ac.nz>
References: <AM3PR05MB1396C5F07B1471C2E98A580DB1BB0@AM3PR05MB1396.eurprd05.prod.outlook.com>
	<9684dcbb-4934-c9be-fa0f-58623edfc525@stat.auckland.ac.nz>
	<AM3PR05MB13965DCEE2E6E02EB134B3D5B1BC0@AM3PR05MB1396.eurprd05.prod.outlook.com>,
	<25ace1d4-ea95-8c9a-a3b1-5573f960bd15@stat.auckland.ac.nz>
Message-ID: <AM3PR05MB1396EAA2C4693206F059EBA8B1BF0@AM3PR05MB1396.eurprd05.prod.outlook.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20161115/56c31e7c/attachment.pl>

From G.Maubach at weinwolf.de  Tue Nov 15 13:48:50 2016
From: G.Maubach at weinwolf.de (G.Maubach at weinwolf.de)
Date: Tue, 15 Nov 2016 13:48:50 +0100
Subject: [R] openxlsx Error: length of rows and cols must be
Message-ID: <OF488C2669.3FCB1A95-ONC125806C.0045E439-C125806C.00467C27@lotus.hawesko.de>

Hi All,

when using 

-- cut --

    number_style <- openxlsx::createStyle(
      numFmt = "COMMA"
    )

    openxlsx::addStyle(
      wb = xlsx_workbook,
      sheet = "Kundenliste",
      style = number_style,
      rows = 2:nrow(customer_list),
      cols = 4:5
      )
--cut --

I get the error

Error in openxlsx::addStyle(wb = xlsx_workbook, sheet = "Kundenliste",  : 
  Length of rows and cols must be equal.

The customer_list can be of any arbritrary length due to subgroup 
definitons. I do not see why the argument "rows" and "cols" should be of 
the same length. This would mean that number formatting can only be done 
for rectangular areas.

What do I need to change to format my numbers in the given area correctly?

Kind regards

Georg


From jdnewmil at dcn.davis.ca.us  Tue Nov 15 14:45:13 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 15 Nov 2016 05:45:13 -0800
Subject: [R] openxlsx Error: length of rows and cols must be
In-Reply-To: <OF488C2669.3FCB1A95-ONC125806C.0045E439-C125806C.00467C27@lotus.hawesko.de>
References: <OF488C2669.3FCB1A95-ONC125806C.0045E439-C125806C.00467C27@lotus.hawesko.de>
Message-ID: <B7B947EC-052F-4AFB-B87D-42170CAC2DAC@dcn.davis.ca.us>

This behavior is a basic limitation of spreadsheets. This R API is (inappropriately) more flexible than the underlying Java API is. You have to apply your formatting with multiple calls to the addStyle function, just as you would interactively in a spreadsheet. 
-- 
Sent from my phone. Please excuse my brevity.

On November 15, 2016 4:48:50 AM PST, G.Maubach at weinwolf.de wrote:
>Hi All,
>
>when using 
>
>-- cut --
>
>    number_style <- openxlsx::createStyle(
>      numFmt = "COMMA"
>    )
>
>    openxlsx::addStyle(
>      wb = xlsx_workbook,
>      sheet = "Kundenliste",
>      style = number_style,
>      rows = 2:nrow(customer_list),
>      cols = 4:5
>      )
>--cut --
>
>I get the error
>
>Error in openxlsx::addStyle(wb = xlsx_workbook, sheet = "Kundenliste", 
>: 
>  Length of rows and cols must be equal.
>
>The customer_list can be of any arbritrary length due to subgroup 
>definitons. I do not see why the argument "rows" and "cols" should be
>of 
>the same length. This would mean that number formatting can only be
>done 
>for rectangular areas.
>
>What do I need to change to format my numbers in the given area
>correctly?
>
>Kind regards
>
>Georg
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From amos.elberg at icloud.com  Tue Nov 15 04:37:52 2016
From: amos.elberg at icloud.com (Amos Elberg)
Date: Mon, 14 Nov 2016 22:37:52 -0500
Subject: [R] [R-pkgs] New Package: largeVis
Message-ID: <etPan.582a8310.754f1c19.70aa@icloud.com>

Dear R users,  

I?m please to announce the available on CRAN of new package largeVis.(*)   

largeVis offers three major features:

	- A fast implementation of the LargeVis algorithm. LargeVis is for visualizing high-dimensional datasets, similar to (and of similar quality to) t-SNE. But, LargeVis runs in O(n) time, which makes it feasible to use on datasets with millions of rows and thousands of columns. LargeVis is also insensitive to hyperparameter changes, which is important when running on large datasets that take time to compute.  

	- Very fast approximate nearest neighbor search. I believe it to be the fastest nearest neighbor search available for R.  

	- A fast implementation of the HDBSCAN clustering algorithm. HDBSCAN is a density-based clustering similar to DBSCAN and OPTICS (which are also implemented), but HDBSCAN allows the density threshold for clusters to vary. This makes it insensitive to hyperparameter changes and more flexible than either DBSCAN or OPTICS.

There are other features as well, such as functions to visualize image embeddings using largeVis.?

Some examples are available here: ?https://github.com/elbamos/largevis
Benchmarks comparing the speed of the nearest neighbor search to RcppAnnoy are here:?https://github.com/elbamos/largeVis/blob/master/benchmarks.md
Examples of HDBSCAN are here: ?https://cran.r-project.org/web/packages/largeVis/vignettes/momentumandusedata.html

The package is available here: ?https://cran.r-project.org/web/packages/largeVis/index.html? and for best results, to take advantage of 64-bit machines and multiple cores, should be installed from source.?

Thank you!

(*) A prior version was available on CRAN but not announced.
	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages

From bgnumis at gmail.com  Tue Nov 15 13:33:32 2016
From: bgnumis at gmail.com (bgnumis bgnum)
Date: Tue, 15 Nov 2016 13:33:32 +0100
Subject: [R] extracting all different items in a matrix colum or vector
Message-ID: <CAN25tHRrBZhgFWjjr_=K4vxNMm0eGavziC-ENbfuq1_STgc4qA@mail.gmail.com>

Hi all,


>From many time ago, I have return to R, I have a matrix with this values.

C(jose, pepe, jose, luis, pepe, raul)

I want to "read" this matrix or element and extract all different values,
so the output matrix (that I want to download is:

c(jose, pepe, luis, raul).

There is a function to do this?

Many thanks in advance.

	[[alternative HTML version deleted]]


From upasani.sampada at gmail.com  Tue Nov 15 18:09:09 2016
From: upasani.sampada at gmail.com (sampada upasani)
Date: Tue, 15 Nov 2016 12:09:09 -0500
Subject: [R] Project on Course Recommender using R language
Message-ID: <CAOhJpGmc9S6vHMRroF2jD0-qDatF5aETL=vJ7oveqtmtfhd6xA@mail.gmail.com>

Hello,

I am a student of Georgia Tech and currently working on a project : Course
Recommender using R language. Before moving forward I wanted to clear up
things and hence posting this question. Will I be able to build this
project using the Shiny App ? (Developing web applications using R). After
reading about shiny, I came to know that it is recommended for deploying
web applications and requires no knowledge of Javascript but is not
recommended for complex web apps. I was looking for developing a web app on
similar lines due to the time constraint.

Can you please share your thoughts on is it a good idea to use this app for
developing a web application for College Course Recommender?

Thanks,
Sampada Upasani
Graduate Student
Georgia Tech

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Tue Nov 15 18:56:18 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 15 Nov 2016 12:56:18 -0500
Subject: [R] extracting all different items in a matrix colum or vector
In-Reply-To: <CAN25tHRrBZhgFWjjr_=K4vxNMm0eGavziC-ENbfuq1_STgc4qA@mail.gmail.com>
References: <CAN25tHRrBZhgFWjjr_=K4vxNMm0eGavziC-ENbfuq1_STgc4qA@mail.gmail.com>
Message-ID: <CAM_vju=ZTzXxvx=v+rTSHGFUxapJV8MD70Up84JQBRYL1EO42A@mail.gmail.com>

Your question strongly suggests that you need to reread at least one
introductory guide to R.

But the answer to your specific question is the unique() function.

Sarah

On Tue, Nov 15, 2016 at 7:33 AM, bgnumis bgnum <bgnumis at gmail.com> wrote:
> Hi all,
>
>
> >From many time ago, I have return to R, I have a matrix with this values.
>
> C(jose, pepe, jose, luis, pepe, raul)
>
> I want to "read" this matrix or element and extract all different values,
> so the output matrix (that I want to download is:
>
> c(jose, pepe, luis, raul).
>
> There is a function to do this?
>
> Many thanks in advance.
>
-- 
Sarah Goslee
http://www.functionaldiversity.org


From f_j_rod at hotmail.com  Tue Nov 15 19:44:04 2016
From: f_j_rod at hotmail.com (Frank S.)
Date: Tue, 15 Nov 2016 18:44:04 +0000
Subject: [R] Condirional row removing and replacing in small data.table
Message-ID: <AM5PR0402MB26891E96747F84D58BEE864ABABF0@AM5PR0402MB2689.eurprd04.prod.outlook.com>

Dear R list members,


I have a data table of which here is an example:

dt <- data.table(id = rep(1:3, c(5, 1, 2)),
     date = as.Date(rep(c("2005-07-25", "2006-09-17",  "1998-11-06", "2001-04-19"), c(3, 2, 1, 2))),
     fam = factor(c(1, 1, 3, 1, 1, 5, 4, 2)),

     code = factor(c(90, 91, 300, 75, 91, 500, 400, 90)))


I would want to conduct 3 operations:

A) Remove rows whose fam is not {1, 2 or 3}, except where this would lead to the disappearance

     of subject (case of id = 2), where we will keep the row but assigning fam=0 and code=0.

B) If within same id and date there are 2 rows with code=90 and code=91   (regardless the order

     of appearance), then remove that with code=91.

C) If  within same id and date there is only 1 row with code=91,  then this row will be kept but

     changing its value to code=90.


The right solution would be:

id             date  fam code
 1  25/07/2005    1    90
 1  25/07/2005    3  300
 1  17/09/2006    1    75
 1  17/09/2006    1    90
 2  06/11/1998    0      0
 3  19/04/2001    2    90


I have tried to implement step A, but I get an error message when executing. Moreover, I'm aware
that the code I present may be not the optimal way to do so (since I need too many code lines):


dtcount <- dt[, count1 := .N, by = id][, count2 := .N, by = list(id, date)] # add two counts

dtA <- dtcount[, {
  if (!(fam %in% 1:3) && count1 == 1) {
    result <- list(date = date, fam = factor(0), code = factor(0))
    } else {
  if (fam %in% 1:3) {
    result <- list(date = date, fam = fam, code = code)
    }
  }
  result
}, by = id]


Any help would be appreciated!


Frank S.


	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Tue Nov 15 19:46:05 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 15 Nov 2016 10:46:05 -0800
Subject: [R] Project on Course Recommender using R language
In-Reply-To: <CAOhJpGmc9S6vHMRroF2jD0-qDatF5aETL=vJ7oveqtmtfhd6xA@mail.gmail.com>
References: <CAOhJpGmc9S6vHMRroF2jD0-qDatF5aETL=vJ7oveqtmtfhd6xA@mail.gmail.com>
Message-ID: <8EDEE25B-3236-4C6A-888D-84BF2C396E4D@dcn.davis.ca.us>

My recommendation is that you separate your algorithm development from your Web implementation. Once you have the algorithm working you can decide how you will implement it. This forum is inappropriate for both of these subjects... when you need help with coding in R this is a good place to ask questions. Q questions about Shiny should be asked on the RStudio shiny.
-- 
Sent from my phone. Please excuse my brevity.

On November 15, 2016 9:09:09 AM PST, sampada upasani <upasani.sampada at gmail.com> wrote:
>Hello,
>
>I am a student of Georgia Tech and currently working on a project :
>Course
>Recommender using R language. Before moving forward I wanted to clear
>up
>things and hence posting this question. Will I be able to build this
>project using the Shiny App ? (Developing web applications using R).
>After
>reading about shiny, I came to know that it is recommended for
>deploying
>web applications and requires no knowledge of Javascript but is not
>recommended for complex web apps. I was looking for developing a web
>app on
>similar lines due to the time constraint.
>
>Can you please share your thoughts on is it a good idea to use this app
>for
>developing a web application for College Course Recommender?
>
>Thanks,
>Sampada Upasani
>Graduate Student
>Georgia Tech
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From bgnumis at gmail.com  Tue Nov 15 21:47:40 2016
From: bgnumis at gmail.com (bgnumis bgnum)
Date: Tue, 15 Nov 2016 21:47:40 +0100
Subject: [R] extracting all different items in a matrix colum or vector
In-Reply-To: <CAM_vju=ZTzXxvx=v+rTSHGFUxapJV8MD70Up84JQBRYL1EO42A@mail.gmail.com>
References: <CAN25tHRrBZhgFWjjr_=K4vxNMm0eGavziC-ENbfuq1_STgc4qA@mail.gmail.com>
	<CAM_vju=ZTzXxvx=v+rTSHGFUxapJV8MD70Up84JQBRYL1EO42A@mail.gmail.com>
Message-ID: <CAN25tHTCBFKqfeicQxoSPEVf+h7gATLUq_XEXU4HknxntBwR3g@mail.gmail.com>

Many Thanks Sarah

Really I?m going to do it.

If you can suggest me one complete and didactic I will be very gratefull.
Anyway many thanks for your answer.



2016-11-15 18:56 GMT+01:00 Sarah Goslee <sarah.goslee at gmail.com>:

> Your question strongly suggests that you need to reread at least one
> introductory guide to R.
>
> But the answer to your specific question is the unique() function.
>
> Sarah
>
> On Tue, Nov 15, 2016 at 7:33 AM, bgnumis bgnum <bgnumis at gmail.com> wrote:
> > Hi all,
> >
> >
> > >From many time ago, I have return to R, I have a matrix with this
> values.
> >
> > C(jose, pepe, jose, luis, pepe, raul)
> >
> > I want to "read" this matrix or element and extract all different values,
> > so the output matrix (that I want to download is:
> >
> > c(jose, pepe, luis, raul).
> >
> > There is a function to do this?
> >
> > Many thanks in advance.
> >
> --
> Sarah Goslee
> http://www.functionaldiversity.org
>

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Wed Nov 16 11:09:32 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Wed, 16 Nov 2016 10:09:32 +0000
Subject: [R] extracting all different items in a matrix colum or vector
In-Reply-To: <CAN25tHTCBFKqfeicQxoSPEVf+h7gATLUq_XEXU4HknxntBwR3g@mail.gmail.com>
References: <CAN25tHRrBZhgFWjjr_=K4vxNMm0eGavziC-ENbfuq1_STgc4qA@mail.gmail.com>
	<CAM_vju=ZTzXxvx=v+rTSHGFUxapJV8MD70Up84JQBRYL1EO42A@mail.gmail.com>
	<CAN25tHTCBFKqfeicQxoSPEVf+h7gATLUq_XEXU4HknxntBwR3g@mail.gmail.com>
Message-ID: <563e77c5-811d-7951-0ba9-71b4886a56d2@dewey.myzen.co.uk>

You say you are coming back to R after a pause. I think the key word in 
Sarah's response is re-reading. Why not start with the material you used 
before and re-read it? If you have never read the Introduction to R 
which comes with your installation that is worth a read too.

On 15/11/2016 20:47, bgnumis bgnum wrote:
> Many Thanks Sarah
>
> Really I?m going to do it.
>
> If you can suggest me one complete and didactic I will be very gratefull.
> Anyway many thanks for your answer.
>
>
>
> 2016-11-15 18:56 GMT+01:00 Sarah Goslee <sarah.goslee at gmail.com>:
>
>> Your question strongly suggests that you need to reread at least one
>> introductory guide to R.
>>
>> But the answer to your specific question is the unique() function.
>>
>> Sarah
>>
>> On Tue, Nov 15, 2016 at 7:33 AM, bgnumis bgnum <bgnumis at gmail.com> wrote:
>>> Hi all,
>>>
>>>
>>> >From many time ago, I have return to R, I have a matrix with this
>> values.
>>>
>>> C(jose, pepe, jose, luis, pepe, raul)
>>>
>>> I want to "read" this matrix or element and extract all different values,
>>> so the output matrix (that I want to download is:
>>>
>>> c(jose, pepe, luis, raul).
>>>
>>> There is a function to do this?
>>>
>>> Many thanks in advance.
>>>
>> --
>> Sarah Goslee
>> http://www.functionaldiversity.org
>>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From S.Ellison at LGCGroup.com  Wed Nov 16 13:46:06 2016
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Wed, 16 Nov 2016 12:46:06 +0000
Subject: [R] Text categories based on the sentences
In-Reply-To: <CAAM-fZ5UWQ1M61E4=VF-HqQ=6E7E2wAL8opBtCidY=NJjcSAxg@mail.gmail.com>
References: <CAAM-fZ5UWQ1M61E4=VF-HqQ=6E7E2wAL8opBtCidY=NJjcSAxg@mail.gmail.com>
Message-ID: <1A8C1289955EF649A09086A153E267240400CF6B3B@GBTEDVPEXCMB04.corp.lgc-group.com>

> I have data set contains one variable "*Description*"
> 
> *Description**                                                  Category*
> 
> 1. i want ice cream                                         food
> 2. i like banana very much                              fruit
> 3. tomorrow i will eat chicken                          food
> 4. yesterday i went to birthday party                festival
> 5. i lost my mobile last week                           mobile
> 
> Please remember that i have only "*Description*" Variables only.How can i
> get the categories column based on the sentences of *Description *column.

You could look at something like ReadMe (http://gking.harvard.edu/readme) to generate a classifier based on a suitable subsample of your data that then classifies the rest of your data set.

Alternatively you could do it the hard way; use a natural language parser to extract all the noun phrases (or just split oout the words),  list the unique noun phrases, manually classify all of them  using your own criteria to give a pair list (phrase->category), and then match classes back to the rows that contain each noun phrase.





*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From S.Ellison at LGCGroup.com  Wed Nov 16 15:57:59 2016
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Wed, 16 Nov 2016 14:57:59 +0000
Subject: [R] Issues with the way Apply handled NA's
In-Reply-To: <b324d2641512438cb3a9cc70c6b2cc2f@exch-2p-mbx-w2.ads.tamu.edu>
References: <b324d2641512438cb3a9cc70c6b2cc2f@exch-2p-mbx-w2.ads.tamu.edu>
Message-ID: <1A8C1289955EF649A09086A153E267240400CF6BE0@GBTEDVPEXCMB04.corp.lgc-group.com>



> -----Original Message-----
> 
> You can check for an empty vector as follows:
> ...
> vals <- apply(plabor[c("colA","colB","colC")],1,function(x) length(na.omit(x)))
> vals # [1] 3 0 3 2 
> <- ifelse(vals>0, plabor$colD, NA) 
> 
plabor

A slightly more compact variant that avoids the intermediate 'vals' variable is to apply an anonymous function that does the check internally:

plabor$colD <- apply(plabor, 1, function(x) if(all(is.na(x))) NA else prod(x, na.rm=TRUE))

S Ellison




*******************************************************************
This email and any attachments are confidential. Any use, copying or
disclosure other than by the intended recipient is unauthorised. If 
you have received this message in error, please notify the sender 
immediately via +44(0)20 8943 7000 or notify postmaster at lgcgroup.com 
and delete this message and any copies from your computer and network. 
LGC Limited. Registered in England 2991879. 
Registered office: Queens Road, Teddington, Middlesex, TW11 0LY, UK

From G.Maubach at weinwolf.de  Wed Nov 16 16:20:38 2016
From: G.Maubach at weinwolf.de (G.Maubach at weinwolf.de)
Date: Wed, 16 Nov 2016 16:20:38 +0100
Subject: [R] Different results when converting a matrix to a data.frame
Message-ID: <OFDF7F1C60.D781A6A4-ONC125806D.005417D4-C125806D.00546203@lotus.hawesko.de>

Hi All,

I build an empty dataframe to fill it will values later. I did the 
following:

-- cut --
matrix(NA, 2, 2)
     [,1] [,2]
[1,]   NA   NA
[2,]   NA   NA
> data.frame(matrix(NA, 2, 2))
  X1 X2
1 NA NA
2 NA NA
> as.data.frame(matrix(NA, 2, 2))
  V1 V2
1 NA NA
2 NA NA
-- cut --

Why does data.frame deliver different results than as.data.frame with 
regard to the variable names (V instead of X)?

Kind regards

Georg

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Wed Nov 16 17:43:13 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 16 Nov 2016 08:43:13 -0800
Subject: [R] Different results when converting a matrix to a data.frame
In-Reply-To: <OFDF7F1C60.D781A6A4-ONC125806D.005417D4-C125806D.00546203@lotus.hawesko.de>
References: <OFDF7F1C60.D781A6A4-ONC125806D.005417D4-C125806D.00546203@lotus.hawesko.de>
Message-ID: <486FCAC4-4608-4415-8371-20585AB7EE04@dcn.davis.ca.us>

I will start by admitting I don't know the answer to your question.

However, I am responding because I think this should not be an issue in real life use of R. Data frames are lists of distinct vectors, each of which has its own reason for being present in the data, and normally each has its own storage mode. Your use of a matrix as a short cut way to create many columns at once does not change this fundamental difference between data frames and matrices. You should not be surprised that putting the finishing touches on this transformation takes some personal attention. 

Normally you should give explicit names to each column using the argument names in the data.frame function. When using a matrix as a shortcut, you should either immediately follow the creation of the data frame with a names(DF)<- assignment, or wrap it in a setNames function call. 

setNames( data.frame(matrix(NA, 2, 2)), c( "ColA", "ColB" ) )

Note that using a matrix to create many columns is memory inefficient, because you start by setting aside a single block of memory (the matrix) and then you move that data column at a time to separate vectors for use in the data frame. If working with large data you might want to consider allocating each column separately from the beginning. 

N <- 2
nms <- c( "A", "B" )
as.data.frame( setNames( lapply( nms, function(n){ rep( NA, 2 ) } ), nms ) )

which is not as convenient, but illustrates that data frames are truly different than matrices.
-- 
Sent from my phone. Please excuse my brevity.

On November 16, 2016 7:20:38 AM PST, G.Maubach at weinwolf.de wrote:
>Hi All,
>
>I build an empty dataframe to fill it will values later. I did the 
>following:
>
>-- cut --
>matrix(NA, 2, 2)
>     [,1] [,2]
>[1,]   NA   NA
>[2,]   NA   NA
>> data.frame(matrix(NA, 2, 2))
>  X1 X2
>1 NA NA
>2 NA NA
>> as.data.frame(matrix(NA, 2, 2))
>  V1 V2
>1 NA NA
>2 NA NA
>-- cut --
>
>Why does data.frame deliver different results than as.data.frame with 
>regard to the variable names (V instead of X)?
>
>Kind regards
>
>Georg
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Wed Nov 16 18:20:34 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 16 Nov 2016 09:20:34 -0800
Subject: [R] Different results when converting a matrix to a data.frame
In-Reply-To: <486FCAC4-4608-4415-8371-20585AB7EE04@dcn.davis.ca.us>
References: <OFDF7F1C60.D781A6A4-ONC125806D.005417D4-C125806D.00546203@lotus.hawesko.de>
	<486FCAC4-4608-4415-8371-20585AB7EE04@dcn.davis.ca.us>
Message-ID: <9F3DE9F9-27DE-4B55-9D63-528276278354@comcast.net>


> On Nov 16, 2016, at 8:43 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> I will start by admitting I don't know the answer to your question.
> 
> However, I am responding because I think this should not be an issue in real life use of R. Data frames are lists of distinct vectors, each of which has its own reason for being present in the data, and normally each has its own storage mode. Your use of a matrix as a short cut way to create many columns at once does not change this fundamental difference between data frames and matrices. You should not be surprised that putting the finishing touches on this transformation takes some personal attention. 
> 
> Normally you should give explicit names to each column using the argument names in the data.frame function. When using a matrix as a shortcut, you should either immediately follow the creation of the data frame with a names(DF)<- assignment, or wrap it in a setNames function call. 
> 
> setNames( data.frame(matrix(NA, 2, 2)), c( "ColA", "ColB" ) )
> 
> Note that using a matrix to create many columns is memory inefficient, because you start by setting aside a single block of memory (the matrix) and then you move that data column at a time to separate vectors for use in the data frame. If working with large data you might want to consider allocating each column separately from the beginning. 
> 
> N <- 2
> nms <- c( "A", "B" )
> as.data.frame( setNames( lapply( nms, function(n){ rep( NA, 2 ) } ), nms ) )
> 
> which is not as convenient, but illustrates that data frames are truly different than matrices.
> -- 
> Sent from my phone. Please excuse my brevity.
> 
> On November 16, 2016 7:20:38 AM PST, G.Maubach at weinwolf.de wrote:
>> Hi All,
>> 
>> I build an empty dataframe to fill it will values later. I did the 
>> following:
>> 
>> -- cut --
>> matrix(NA, 2, 2)
>>    [,1] [,2]
>> [1,]   NA   NA
>> [2,]   NA   NA
>>> data.frame(matrix(NA, 2, 2))
>> X1 X2
>> 1 NA NA
>> 2 NA NA
>>> as.data.frame(matrix(NA, 2, 2))
>> V1 V2
>> 1 NA NA
>> 2 NA NA
>> -- cut --
>> 
>> Why does data.frame deliver different results than as.data.frame with 
>> regard to the variable names (V instead of X)?

They are two different functions:

It's fairly easy to see by looking at the code:

as.data.frame.matrix uses: names(value) <- paste0("V", ic)  when there are no column names and data.frame calls make.names which prepends an "X" as the first letter of invalid or missing names.


As to why the authors did it this way, I'm unable to comment.

>> 
>> Kind regards
>> 
>> Georg
>> 
>> 	[[alternative HTML version deleted]]


David Winsemius
Alameda, CA, USA


From mehall at blm.gov  Wed Nov 16 19:19:16 2016
From: mehall at blm.gov (Hall, Mark)
Date: Wed, 16 Nov 2016 10:19:16 -0800
Subject: [R] Some basic time series questions
Message-ID: <CANNh0zfaMvop6uQvG68OLqs0BKfpKyZzDe3imfDhNQwms49iXg@mail.gmail.com>

Hi,

As I sit and learn how to work with time series, I've run into a problem
that is eluding a quick easy answer (and googling for answers seems to
really slow the process...)

Question #1--
In a simple example on R 3.3.1 (sorry my employer hasn't upgraded to 3.3.2
yet):

x=rnorm(26,0,1)
x.ts<-ts(x,start=c(2014,9),frequency=12)

inputting x.ts at the prompt gives me a table with the rooms denoted by
year and columns denoted by months and everything lines up wonderfully.

Now my problem comes when I type at the prompt

plot(x.ts)  or
plot(x.ts, xlab="") or
plot.ts(x.ts,xlab="")

I get a plot of the values, but my x-axis labels are 2015.0, 2015.5,
2016.0, and 2016.5 .  January 2015 is coming out as 2015.0...

Is there a way of getting a more intelligible x-axis labeling?  Even 2015.1
for Janaury, etc. would work, or even getting an index (either Septemebr
2014 representing 0 or 1 and it incrementally increasing each month).

Question #2--
If I have a time series of decadal events, how best should I set the
frequency.  It is historical data, in the form of say AD 610-619 5 events,
AD 620-629 7 events, etc.

Sorry for such a basic questions.  Any advice would be appreciated.

Thanks in advance, MEH



Mark E. Hall, PhD
Assistant Field Manager
Black Rock Field Office
Winnemucca District Office
775-623-1529.

	[[alternative HTML version deleted]]


From S.Ellison at LGCGroup.com  Wed Nov 16 20:01:27 2016
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Wed, 16 Nov 2016 19:01:27 +0000
Subject: [R] Variable 'A' is not a factor Error message
In-Reply-To: <1957360193.3935030.1478886692398@mail.yahoo.com>
References: <1957360193.3935030.1478886692398.ref@mail.yahoo.com>
	<1957360193.3935030.1478886692398@mail.yahoo.com>
Message-ID: <1A8C1289955EF649A09086A153E267240400CF6C65@GBTEDVPEXCMB04.corp.lgc-group.com>

This looks like one of those 'please talk to a statistician' questions ...

You appear to have requested a 12-run placket-burman experiment, which is a design that requires up to 11 two-level factors. You then fitted (I think) simulated data to that design, using those factors converted to their integer representation - which is a completely different thing in model matrix terms. So your predictors are still two-level factors,  and your linear model has, after building a model matrix from the default contrasts,  probably got one coefficient for each upper level of your two level factors and one for the intercept. (I'm assuming default contrasts here).

You then decided to redefine your predictors as numeric variables with a very large number of levels given by rnorm, none of which (except by very rare coincidence)  were in your original design. So your model cannot possibly predict the output - it has no coefficients for all those new levels. To avoid that, R quite accurately told you that you can't do that.

If you want to fit a linear model with continuous variables, you need to set up your DOE data frame with (meaningful) numeric predictors, not factors. You will then get a numerical gradient for each factor instead of a single offset for each upper level. That isn't really what Placket and Burman had in mind, so I would not normally start with a P-B design if I wanted to do that. Consider a response surface model instead.

S Ellison


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of ahmed
> meftah
> Sent: 11 November 2016 17:52
> To: r-help at r-project.org
> Subject: [R] Variable 'A' is not a factor Error message
> 
> I am running a DOE with the following code   library(Rcmdr)
>     library(RcmdrMisc)
>     library(RcmdrPlugin.DoE)
> # Define plackett burman experiment
>     PB.DOE <- pb(nruns= 12 ,n12.taguchi= FALSE ,nfactors= 12 -1, ncenter= 0 ,
>                  replications= 1 ,repeat.only= FALSE ,randomize= TRUE ,seed= 27241 ,
>                  factor.names=list( A=c(100,1000),B=c(100,200),C=c(1,3),D=c(1,1.7),
>                                     E=c(1000,1500),G=c(-2,2) ) )
> 
>     as.numeric2 <- function(x) as.numeric(as.character(x))
> 
> # Calculate response column
>     IP <- with(PB.DOE,(as.numeric2(A)*as.numeric2(B)*(5000-
> 3000))/(141.2*as.numeric2(C)*as.numeric2(D)*(log(as.numeric2(E)/0.25)-
> (1/2)+as.numeric2(G))))
> # Combine response column with exp design table
>     final_set <- within(PB.DOE, {
>       IP<- ((as.numeric2(A)*as.numeric2(B)*(5000-
> 3000))/(141.2*as.numeric2(C)*as.numeric2(D)*(log(as.numeric2(E)/0.25)-
> (1/2)+as.numeric2(G))))
>     })I then ran a regression as follows:LinearModel.1 <- lm(IP ~ A + B + C + D +
> E + G,
>                     data=final_set)
> summary(LinearModel.1)Following this i wanted to run a predict using
> specified values as predictors in a Monte Carlo:n = 10000 # Define probability
> distributions of predictors A = rnorm(n,450,100) hist(A,col = "blue",breaks =
> 50)
> 
> B = rnorm(n, 150,10)
> hist(B,col = "blue",breaks = 50)
> 
> C = rnorm(n, 1.5, 0.5)
> hist(C,col = "blue",breaks = 50)
> 
> D = runif(n,1.2,1.7)
> hist(D,col = "blue",breaks = 50)
> 
> E = rnorm(n,1250,50)
> hist(E,col = "blue",breaks = 50)
> 
> G = rnorm(n,0,0.5)
> hist(G,col = "blue",breaks = 50)
> 
> MCtable <- data.frame(A=A,B=B,C=C,D=D,E=E,G=G)
> 
> for (n in 1:n) {
>   N=predict(LinearModel.1,MCtable)
> }
> 
> hist(N,col = "yellow",breaks = 10)I end up getting this error:"Warning in
> model.frame.default(Terms, newdata, na.action = na.action, xlev =
> object$xlevels) :
>   variable 'A' is not a factor"Using str() to get some info on the LinearModel.1
> and from what I understand seems to indicates that since the predictors
> A,B,C etc are factors with 2 levels I have to convert my data.frame table to
> factors aswell. Is that correct?Doing this would mean I would also need to
> specify the number of levels which would mean that since I have set my n to
> 10000 would mean 10000 levels for each factor. How would I go about doing
> this? Is there a better solution? Any help would be appreciated.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From pabloemilio.verde at hhu.de  Wed Nov 16 15:02:49 2016
From: pabloemilio.verde at hhu.de (PD Dr. Pablo Emilio Verde)
Date: Wed, 16 Nov 2016 15:02:49 +0100
Subject: [R] Course: "Statistical Data Analysis with R"
Message-ID: <582C6709.5030206@hhu.de>

Dear list members,

I am very glad to announce the introductory course: "Statistical Data 
Analysis with R".

I have running this course for 10 years and previous participants have 
had a nice time learning R at the LinuxHotel.
You are invited to bring their own data to analyze during the course.

Below you find more information about this course. If you have any 
question don't hesitate to contact me.

Best regards,

Pablo

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Course: Introduction to Statistical Data Analysis with R
Where: Linux Hotel, Essen-Horst, Germany
When: 15.12 - 17.12.2016 week 50

Instructor: PD Dr. rer. nat. Pablo Emilio Verde
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
*Target audience*
This course is for data analyst who are familiar with classical 
statistics software
like SPSS, SAS, etc. and they want to get a working knowledge in R.

This is a 3 days intensive training course with 8 hours per day 
including lecturing
and exercises. The course presentation is practical with many worked 
examples.
To attend the course you do NOT need experience with R. Lectures are given
in English. Discussions can be in English, German or Spanish.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Day 1
* Introduction to concepts in R
* Data structures, objects and classes
* Data management with R (indexing and other advanced techniques)
* Graphics with R (classical functions, lattice plots and ggplot2)

Day 2
* Introduction to statistical functions in R
* Bootstrap methods and Monte Carlo simulation inference in R
* Linear models and regression techniques in R
* Model checking and model diagnostics

Day 3
* Logistic regression, Lognormal models and GLM with R
* Exploratory multivariate analysis
* Exploratory regression analysis with Classification Trees and Random 
Forest
* Own projects
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Costs:

Public sector and commercial: 3 days, 998.00 ? + 19% vat = 1,187.62?
(three days course, full eight clock hours per day, complete set of 
literature,
WiFi, complimentary notebook, full board, drinks, pastries, homemade cakes,
sauna, social program)

Additionally acommodation on demand: 63,13 ? shared double room
per night (incl. VAT) or 138,03 ? single room per night (inc. VAT).

Student:
675 ? three days course

Some of the courses are frequently fully booked. So please notice that 
you may
have to try several times, until you get a spare place.

For more information, please contact: info at linuxhotel.de
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

	[[alternative HTML version deleted]]


From marine.regis at hotmail.fr  Wed Nov 16 21:43:15 2016
From: marine.regis at hotmail.fr (Marine Regis)
Date: Wed, 16 Nov 2016 20:43:15 +0000
Subject: [R] GAMs: test of simple effects following a significant interaction
Message-ID: <AM5PR0701MB23381B32C198C429745C4FC6E2BE0@AM5PR0701MB2338.eurprd07.prod.outlook.com>

Hello,
I am a novice in Generalized Additive Models (GAMs) and I would need some advice on these models. From capture data, I would like to assess the effect of longitudinal changes in proportion of forests on abundance of skunks. To test this, I built this GAM where the dependent variable is the number of unique skunks and the independent variables are the X coordinates of the centroids of trapping sites (called "X" in the GAM) and the proportion of forests within the trapping sites (called "prop_forest" in the GAM):
mod <- gam(nb_unique ~ s(x,prop_forest), offset=log_trap_eff, family=nb(theta=NULL, link="log"), data=succ_capt_skunk, method = "REML", select = TRUE)
summary(mod)

Family: Negative Binomial(13.446)
Link function: log

Formula:
nb_unique ~ s(x, prop_forest)

Parametric coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept) -2.02095    0.03896  -51.87   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Approximate significance of smooth terms:
                   edf Ref.df Chi.sq  p-value
s(x,prop_forest) 3.182     29  17.76 0.000102 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

R-sq.(adj) =   0.37   Deviance explained =   49%
-REML = 268.61  Scale est. = 1         n = 58

Should I include the simple effects of independent variables "X" and "prop_forest" into the GAM when the interaction is significant? I ask this question because the longitude and latitude are often included as an interaction term in a GAM (i.e., s(X,Y)) without the simple effects (however, I tested for the simple effects and they were not significant in my case).
Is it correct to include the interaction between X and proportion of forests when my objective is to test longitudinal changes in proportion of forests?
Thanks a lot for your time.
Have a nice day.
Marine


	[[alternative HTML version deleted]]


From dimitri.liakhovitski at gmail.com  Wed Nov 16 21:58:34 2016
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Wed, 16 Nov 2016 15:58:34 -0500
Subject: [R] using pmax in presence of NAs
Message-ID: <CAN2xGJY7dO4KRf1DqRUg96Z57HU_vn2-UFR0UCFu5sxPmMoEaw@mail.gmail.com>

Hello!

I need to calculate the maximum of each row of a data frame.
This works:

  x <- data.frame(a = 1:5, b=11:15, c=111:115)
  x
  do.call(pmax, x)
[1] 111 112 113 114 115

However, how should I modify it if my data frame has NAs?
I'd like it to ignore NAs and return the maximum of all non-NAs in each row:

  x <- data.frame(a = c(1:5), b=11:15, c=c(111:114,NA))
  x
I'd like it to return:
[1] 111 112 113 114 15

Thanks a lot!

-- 
Dimitri Liakhovitski


From dimitri.liakhovitski at gmail.com  Wed Nov 16 22:03:16 2016
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Wed, 16 Nov 2016 16:03:16 -0500
Subject: [R] using pmax in presence of NAs
In-Reply-To: <CAN2xGJY7dO4KRf1DqRUg96Z57HU_vn2-UFR0UCFu5sxPmMoEaw@mail.gmail.com>
References: <CAN2xGJY7dO4KRf1DqRUg96Z57HU_vn2-UFR0UCFu5sxPmMoEaw@mail.gmail.com>
Message-ID: <CAN2xGJaTHCc2L43CfziA=8AhUauSfZnLBXx7p9uffScarqFzwg@mail.gmail.com>

To clarify:
I know I could do:
apply(x, 1, max, na.rm = T)

But I was wondering if one can modify the pmax one...

On Wed, Nov 16, 2016 at 3:58 PM, Dimitri Liakhovitski
<dimitri.liakhovitski at gmail.com> wrote:
> Hello!
>
> I need to calculate the maximum of each row of a data frame.
> This works:
>
>   x <- data.frame(a = 1:5, b=11:15, c=111:115)
>   x
>   do.call(pmax, x)
> [1] 111 112 113 114 115
>
> However, how should I modify it if my data frame has NAs?
> I'd like it to ignore NAs and return the maximum of all non-NAs in each row:
>
>   x <- data.frame(a = c(1:5), b=11:15, c=c(111:114,NA))
>   x
> I'd like it to return:
> [1] 111 112 113 114 15
>
> Thanks a lot!
>
> --
> Dimitri Liakhovitski



-- 
Dimitri Liakhovitski


From sarah.goslee at gmail.com  Wed Nov 16 22:06:03 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 16 Nov 2016 16:06:03 -0500
Subject: [R] using pmax in presence of NAs
In-Reply-To: <CAN2xGJY7dO4KRf1DqRUg96Z57HU_vn2-UFR0UCFu5sxPmMoEaw@mail.gmail.com>
References: <CAN2xGJY7dO4KRf1DqRUg96Z57HU_vn2-UFR0UCFu5sxPmMoEaw@mail.gmail.com>
Message-ID: <CAM_vju=bTyDyqHswvFBgoXw9BDj5EzGSsdYuuMX-okOuu1mHpw@mail.gmail.com>

pmax has a na.rm argument. Why not just use that?

x <- data.frame(a = c(1:5), b=11:15, c=c(111:114,NA))

> do.call(pmax, c(x, na.rm=TRUE))
[1] 111 112 113 114  15

On Wed, Nov 16, 2016 at 3:58 PM, Dimitri Liakhovitski
<dimitri.liakhovitski at gmail.com> wrote:
> Hello!
>
> I need to calculate the maximum of each row of a data frame.
> This works:
>
>   x <- data.frame(a = 1:5, b=11:15, c=111:115)
>   x
>   do.call(pmax, x)
> [1] 111 112 113 114 115
>
> However, how should I modify it if my data frame has NAs?
> I'd like it to ignore NAs and return the maximum of all non-NAs in each row:
>
>   x <- data.frame(a = c(1:5), b=11:15, c=c(111:114,NA))
>   x
> I'd like it to return:
> [1] 111 112 113 114 15
>
> Thanks a lot!
>
> --
> Dimitri Liakhovitski
>


From dimitri.liakhovitski at gmail.com  Wed Nov 16 22:08:06 2016
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Wed, 16 Nov 2016 16:08:06 -0500
Subject: [R] using pmax in presence of NAs
In-Reply-To: <CAM_vju=bTyDyqHswvFBgoXw9BDj5EzGSsdYuuMX-okOuu1mHpw@mail.gmail.com>
References: <CAN2xGJY7dO4KRf1DqRUg96Z57HU_vn2-UFR0UCFu5sxPmMoEaw@mail.gmail.com>
	<CAM_vju=bTyDyqHswvFBgoXw9BDj5EzGSsdYuuMX-okOuu1mHpw@mail.gmail.com>
Message-ID: <CAN2xGJaKL9AcrBgC8XSyeY1cpt6ssh=vPpZjQ7JbRU7KsrymRQ@mail.gmail.com>

Thanks a lot, Sarah.
I just had no idea where to put na.rm = T in the do.call call.
Appreciate it!
Dimitri

On Wed, Nov 16, 2016 at 4:06 PM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
> pmax has a na.rm argument. Why not just use that?
>
> x <- data.frame(a = c(1:5), b=11:15, c=c(111:114,NA))
>
>> do.call(pmax, c(x, na.rm=TRUE))
> [1] 111 112 113 114  15
>
> On Wed, Nov 16, 2016 at 3:58 PM, Dimitri Liakhovitski
> <dimitri.liakhovitski at gmail.com> wrote:
>> Hello!
>>
>> I need to calculate the maximum of each row of a data frame.
>> This works:
>>
>>   x <- data.frame(a = 1:5, b=11:15, c=111:115)
>>   x
>>   do.call(pmax, x)
>> [1] 111 112 113 114 115
>>
>> However, how should I modify it if my data frame has NAs?
>> I'd like it to ignore NAs and return the maximum of all non-NAs in each row:
>>
>>   x <- data.frame(a = c(1:5), b=11:15, c=c(111:114,NA))
>>   x
>> I'd like it to return:
>> [1] 111 112 113 114 15
>>
>> Thanks a lot!
>>
>> --
>> Dimitri Liakhovitski
>>



-- 
Dimitri Liakhovitski


From pdalgd at gmail.com  Wed Nov 16 22:21:34 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 16 Nov 2016 22:21:34 +0100
Subject: [R] using pmax in presence of NAs
In-Reply-To: <CAN2xGJY7dO4KRf1DqRUg96Z57HU_vn2-UFR0UCFu5sxPmMoEaw@mail.gmail.com>
References: <CAN2xGJY7dO4KRf1DqRUg96Z57HU_vn2-UFR0UCFu5sxPmMoEaw@mail.gmail.com>
Message-ID: <AD616EAB-65ED-405C-8ABC-2C56D16CA5F6@gmail.com>


> On 16 Nov 2016, at 21:58 , Dimitri Liakhovitski <dimitri.liakhovitski at gmail.com> wrote:
> 
> Hello!
> 
> I need to calculate the maximum of each row of a data frame.
> This works:
> 
>  x <- data.frame(a = 1:5, b=11:15, c=111:115)
>  x
>  do.call(pmax, x)
> [1] 111 112 113 114 115
> 
> However, how should I modify it if my data frame has NAs?
> I'd like it to ignore NAs and return the maximum of all non-NAs in each row:
> 
>  x <- data.frame(a = c(1:5), b=11:15, c=c(111:114,NA))
>  x
> I'd like it to return:
> [1] 111 112 113 114 15
> 
> Thanks a lot!

The first thing to notice is that pmax allows na.rm=TRUE, so it works fine to do 

> pmax(a = c(1:5), b=11:15, c=c(111:114,NA), na.rm=TRUE)
[1] 111 112 113 114  15

I.e., it is your desire to use do.call() that is the challenge. The 2nd argument should be a list containing the arguments as in the above call, so this works too

> do.call(pmax, list(a = c(1:5), b=11:15, c=c(111:114,NA), na.rm=TRUE))
[1] 111 112 113 114  15

The form do.call(pmax,x) uses the fact that a data.frame is a kind of list. What you need to do is to tack on the element na.rm=TRUE. This works

> do.call(pmax, c(x, list(na.rm=TRUE)))
[1] 111 112 113 114  15

and it even works without the list() 

> do.call(pmax, c(x, na.rm=TRUE))
[1] 111 112 113 114  15

-pd


> 
> -- 
> Dimitri Liakhovitski
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From dimitri.liakhovitski at gmail.com  Wed Nov 16 22:36:58 2016
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Wed, 16 Nov 2016 16:36:58 -0500
Subject: [R] using pmax in presence of NAs
In-Reply-To: <AD616EAB-65ED-405C-8ABC-2C56D16CA5F6@gmail.com>
References: <CAN2xGJY7dO4KRf1DqRUg96Z57HU_vn2-UFR0UCFu5sxPmMoEaw@mail.gmail.com>
	<AD616EAB-65ED-405C-8ABC-2C56D16CA5F6@gmail.com>
Message-ID: <CAN2xGJaNdVMKxgHih81=Jd-dNNk9Lw9r-BKg7XJM4c8LAT=K1Q@mail.gmail.com>

Thank you, Peter!

On Wed, Nov 16, 2016 at 4:21 PM, peter dalgaard <pdalgd at gmail.com> wrote:
>
>> On 16 Nov 2016, at 21:58 , Dimitri Liakhovitski <dimitri.liakhovitski at gmail.com> wrote:
>>
>> Hello!
>>
>> I need to calculate the maximum of each row of a data frame.
>> This works:
>>
>>  x <- data.frame(a = 1:5, b=11:15, c=111:115)
>>  x
>>  do.call(pmax, x)
>> [1] 111 112 113 114 115
>>
>> However, how should I modify it if my data frame has NAs?
>> I'd like it to ignore NAs and return the maximum of all non-NAs in each row:
>>
>>  x <- data.frame(a = c(1:5), b=11:15, c=c(111:114,NA))
>>  x
>> I'd like it to return:
>> [1] 111 112 113 114 15
>>
>> Thanks a lot!
>
> The first thing to notice is that pmax allows na.rm=TRUE, so it works fine to do
>
>> pmax(a = c(1:5), b=11:15, c=c(111:114,NA), na.rm=TRUE)
> [1] 111 112 113 114  15
>
> I.e., it is your desire to use do.call() that is the challenge. The 2nd argument should be a list containing the arguments as in the above call, so this works too
>
>> do.call(pmax, list(a = c(1:5), b=11:15, c=c(111:114,NA), na.rm=TRUE))
> [1] 111 112 113 114  15
>
> The form do.call(pmax,x) uses the fact that a data.frame is a kind of list. What you need to do is to tack on the element na.rm=TRUE. This works
>
>> do.call(pmax, c(x, list(na.rm=TRUE)))
> [1] 111 112 113 114  15
>
> and it even works without the list()
>
>> do.call(pmax, c(x, na.rm=TRUE))
> [1] 111 112 113 114  15
>
> -pd
>
>
>>
>> --
>> Dimitri Liakhovitski
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>



-- 
Dimitri Liakhovitski


From nell.redu at hotmail.fr  Wed Nov 16 23:53:50 2016
From: nell.redu at hotmail.fr (Nelly Reduan)
Date: Wed, 16 Nov 2016 22:53:50 +0000
Subject: [R] Run a Python code from R
Message-ID: <CY1PR05MB273092A72F36BAEB24AA5C9199BE0@CY1PR05MB2730.namprd05.prod.outlook.com>

Hello,


How can I run this Python code from R ?


>>> import nlmpy
>>> nlm = nlmpy.mpd(nRow=50, nCol=50, h=0.75)
>>> nlmpy.exportASCIIGrid("raster.asc", nlm)


Nlmpy is a Python package to build neutral landscape models

https://pypi.python.org/pypi/nlmpy . The example comes from this website. I tried to use the function system2 but I don't know how to use it.


path_script_python <- "C:/Users/Anaconda2/Lib/site-packages/nlmpy/nlmpy.py"

test <- system2("python", args = c(path_script_python, as.character(nRow), as.character(nCol), as.character(h)))

Thanks a lot for your help.
Nell


nlmpy 0.1.3 : Python Package Index<https://pypi.python.org/pypi/nlmpy>
pypi.python.org
NLMpy. NLMpy is a Python package for the creation of neutral landscape models that are widely used in the modelling of ecological patterns and processes across ...



	[[alternative HTML version deleted]]


From liuwensui at gmail.com  Thu Nov 17 01:00:03 2016
From: liuwensui at gmail.com (Wensui Liu)
Date: Wed, 16 Nov 2016 18:00:03 -0600
Subject: [R] Run a Python code from R
In-Reply-To: <CY1PR05MB273092A72F36BAEB24AA5C9199BE0@CY1PR05MB2730.namprd05.prod.outlook.com>
References: <CY1PR05MB273092A72F36BAEB24AA5C9199BE0@CY1PR05MB2730.namprd05.prod.outlook.com>
Message-ID: <CAKyN3iBNQDUuzNRVvJMvw+HVzcsuqrBTj545ZfM=D8B0+nG9SQ@mail.gmail.com>

take a look at rpython or rPithon package

On Wed, Nov 16, 2016 at 4:53 PM, Nelly Reduan <nell.redu at hotmail.fr> wrote:
> Hello,
>
>
> How can I run this Python code from R ?
>
>
>>>> import nlmpy
>>>> nlm = nlmpy.mpd(nRow=50, nCol=50, h=0.75)
>>>> nlmpy.exportASCIIGrid("raster.asc", nlm)
>
>
> Nlmpy is a Python package to build neutral landscape models
>
> https://pypi.python.org/pypi/nlmpy . The example comes from this website. I tried to use the function system2 but I don't know how to use it.
>
>
> path_script_python <- "C:/Users/Anaconda2/Lib/site-packages/nlmpy/nlmpy.py"
>
> test <- system2("python", args = c(path_script_python, as.character(nRow), as.character(nCol), as.character(h)))
>
> Thanks a lot for your help.
> Nell
>
>
> nlmpy 0.1.3 : Python Package Index<https://pypi.python.org/pypi/nlmpy>
> pypi.python.org
> NLMpy. NLMpy is a Python package for the creation of neutral landscape models that are widely used in the modelling of ecological patterns and processes across ...
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From nell.redu at hotmail.fr  Thu Nov 17 01:53:01 2016
From: nell.redu at hotmail.fr (Nelly Reduan)
Date: Thu, 17 Nov 2016 00:53:01 +0000
Subject: [R] Run a Python code from R
In-Reply-To: <CAKyN3iBNQDUuzNRVvJMvw+HVzcsuqrBTj545ZfM=D8B0+nG9SQ@mail.gmail.com>
References: <CY1PR05MB273092A72F36BAEB24AA5C9199BE0@CY1PR05MB2730.namprd05.prod.outlook.com>,
	<CAKyN3iBNQDUuzNRVvJMvw+HVzcsuqrBTj545ZfM=D8B0+nG9SQ@mail.gmail.com>
Message-ID: <SN2PR05MB2734537C1D81907B21E8063999B10@SN2PR05MB2734.namprd05.prod.outlook.com>

Thank you very much for your help !


I 'm trying to use the package "rPithon" but I obtain this error message:


> if (pithon.available())

+ {

+   nRow <-50

+   nCol <-50

+   h <- 0.75

+

+   # this file contains the definition of function concat

+   pithon.load("C:/Users/Anaconda2/Lib/site-packages/nlmpy/nlmpy.py")

+   pithon.call( "mpd", nRow, nCol, h)

+

+ } else {

+   print("Unable to execute python")

+ }

 Show Traceback



 Rerun with Debug



Error in pithon.get("_r_call_return", instance.name = instname) :

  Couldn't retrieve variable: Traceback (most recent call last):

  File "C:/Users/Documents/R/win-library/3.3/rPithon/pythonwrapperscript.py", line 110, in <module>

    reallyReallyLongAndUnnecessaryPrefix.data = json.dumps([eval(reallyReallyLongAndUnnecessaryPrefix.argData)])

  File "C:\Users\ANACON~1\lib\json\__init__.py", line 244, in dumps

    return _default_encoder.encode(obj)

  File "C:\Users\ANACON~1\lib\json\encoder.py", line 207, in encode

    chunks = self.iterencode(o, _one_shot=True)

  File "C:\Users\ANACON~1\lib\json\encoder.py", line 270, in iterencode

    return _iterencode(o, 0)

  File "C:\Users\ANACON~1\lib\json\encoder.py", line 184, in default

    raise TypeError(repr(o) + " is not JSON serializable")

TypeError: array([[ 0.36534654,  0.31962481,  0.44229946, ...,  0.11513079,

         0.07156331,  0.00286971],

       [ 0.41534291,  0.41333479,  0.48118995, ...,  0.19203674,

         0.04192771,  0.03679473],

       [ 0.5188





Nell


________________________________
De : Wensui Liu <liuwensui at gmail.com>
Envoy? : mercredi 16 novembre 2016 16:00:03
? : Nelly Reduan
Cc : r-help at r-project.org
Objet : Re: [R] Run a Python code from R

take a look at rpython or rPithon package

On Wed, Nov 16, 2016 at 4:53 PM, Nelly Reduan <nell.redu at hotmail.fr> wrote:
> Hello,
>
>
> How can I run this Python code from R ?
>
>
>>>> import nlmpy
>>>> nlm = nlmpy.mpd(nRow=50, nCol=50, h=0.75)
>>>> nlmpy.exportASCIIGrid("raster.asc", nlm)
>
>
> Nlmpy is a Python package to build neutral landscape models
>
> https://pypi.python.org/pypi/nlmpy . The example comes from this website. I tried to use the function system2 but I don't know how to use it.
>
>
> path_script_python <- "C:/Users/Anaconda2/Lib/site-packages/nlmpy/nlmpy.py"
>
> test <- system2("python", args = c(path_script_python, as.character(nRow), as.character(nCol), as.character(h)))
>
> Thanks a lot for your help.
> Nell
>
>
> nlmpy 0.1.3 : Python Package Index<https://pypi.python.org/pypi/nlmpy>
> pypi.python.org
> NLMpy. NLMpy is a Python package for the creation of neutral landscape models that are widely used in the modelling of ecological patterns and processes across ...
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Thu Nov 17 02:10:48 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 16 Nov 2016 17:10:48 -0800
Subject: [R] Run a Python code from R
In-Reply-To: <SN2PR05MB2734537C1D81907B21E8063999B10@SN2PR05MB2734.namprd05.prod.outlook.com>
References: <CY1PR05MB273092A72F36BAEB24AA5C9199BE0@CY1PR05MB2730.namprd05.prod.outlook.com>
	<CAKyN3iBNQDUuzNRVvJMvw+HVzcsuqrBTj545ZfM=D8B0+nG9SQ@mail.gmail.com>
	<SN2PR05MB2734537C1D81907B21E8063999B10@SN2PR05MB2734.namprd05.prod.outlook.com>
Message-ID: <3F13A4EB-7546-4DF8-8515-BE2A2ADDA8D6@comcast.net>


> On Nov 16, 2016, at 4:53 PM, Nelly Reduan <nell.redu at hotmail.fr> wrote:
> 
> Thank you very much for your help !
> 
> 
> I 'm trying to use the package "rPithon" but I obtain this error message:

Are you sure you are not just misspelling rPython? If that's not the issue than you need to say where you got rPithon,


> 
> 
>> if (pithon.available())
> 
> + {
> 
> +   nRow <-50
> 
> +   nCol <-50
> 
> +   h <- 0.75
> 
> +
> 
> +   # this file contains the definition of function concat
> 
> +   pithon.load("C:/Users/Anaconda2/Lib/site-packages/nlmpy/nlmpy.py")
> 
> +   pithon.call( "mpd", nRow, nCol, h)
> 
> +
> 
> + } else {
> 
> +   print("Unable to execute python")
> 
> + }
> 
> Show Traceback
> 
> 
> 
> Rerun with Debug
> 
> 
> 
> Error in pithon.get("_r_call_return", instance.name = instname) :
> 
>  Couldn't retrieve variable: Traceback (most recent call last):
> 
>  File "C:/Users/Documents/R/win-library/3.3/rPithon/pythonwrapperscript.py", line 110, in <module>
> 
>    reallyReallyLongAndUnnecessaryPrefix.data = json.dumps([eval(reallyReallyLongAndUnnecessaryPrefix.argData)])
> 
>  File "C:\Users\ANACON~1\lib\json\__init__.py", line 244, in dumps
> 
>    return _default_encoder.encode(obj)
> 
>  File "C:\Users\ANACON~1\lib\json\encoder.py", line 207, in encode
> 
>    chunks = self.iterencode(o, _one_shot=True)
> 
>  File "C:\Users\ANACON~1\lib\json\encoder.py", line 270, in iterencode
> 
>    return _iterencode(o, 0)
> 
>  File "C:\Users\ANACON~1\lib\json\encoder.py", line 184, in default
> 
>    raise TypeError(repr(o) + " is not JSON serializable")
> 
> TypeError: array([[ 0.36534654,  0.31962481,  0.44229946, ...,  0.11513079,
> 
>         0.07156331,  0.00286971],
> 
>       [ 0.41534291,  0.41333479,  0.48118995, ...,  0.19203674,
> 
>         0.04192771,  0.03679473],
> 
>       [ 0.5188
> 
> 
> 
> 
> 
> Nell
> 
> 
> ________________________________
> De : Wensui Liu <liuwensui at gmail.com>
> Envoy? : mercredi 16 novembre 2016 16:00:03
> ? : Nelly Reduan
> Cc : r-help at r-project.org
> Objet : Re: [R] Run a Python code from R
> 
> take a look at rpython or rPithon package
> 
> On Wed, Nov 16, 2016 at 4:53 PM, Nelly Reduan <nell.redu at hotmail.fr> wrote:
>> Hello,
>> 
>> 
>> How can I run this Python code from R ?
>> 
>> 
>>>>> import nlmpy
>>>>> nlm = nlmpy.mpd(nRow=50, nCol=50, h=0.75)
>>>>> nlmpy.exportASCIIGrid("raster.asc", nlm)
>> 
>> 
>> Nlmpy is a Python package to build neutral landscape models
>> 
>> https://pypi.python.org/pypi/nlmpy . The example comes from this website. I tried to use the function system2 but I don't know how to use it.
>> 
>> 
>> path_script_python <- "C:/Users/Anaconda2/Lib/site-packages/nlmpy/nlmpy.py"
>> 
>> test <- system2("python", args = c(path_script_python, as.character(nRow), as.character(nCol), as.character(h)))
>> 
>> Thanks a lot for your help.
>> Nell
>> 
>> 
>> nlmpy 0.1.3 : Python Package Index<https://pypi.python.org/pypi/nlmpy>
>> pypi.python.org
>> NLMpy. NLMpy is a Python package for the creation of neutral landscape models that are widely used in the modelling of ecological patterns and processes across ...
>> 
>> 
>> 
>>        [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From cbenjami at BTBOCES.ORG  Thu Nov 17 04:15:00 2016
From: cbenjami at BTBOCES.ORG (Courtney Benjamin)
Date: Thu, 17 Nov 2016 03:15:00 +0000
Subject: [R] Archer-Lemeshow Goodness of Fit Test for Survey Data with Log.
 Regression
Message-ID: <1479352496057.0@BTBOCES.ORG>

?Hello R Experts,

I am trying to implement the Archer-Lemeshow GOF Test for survey data on a logistic regression model using the survey package based upon an R Help Archive post that I found where Dr. Thomas Lumley advised how to do it: http://r.789695.n4.nabble.com/Goodness-of-t-tests-for-Complex-Survey-Logistic-Regression-td4668233.html

Everything is going well until I get to the point where I have to add the objects 'r' and 'g' as variables to the data frame by either using the transform function or the update function to update the svrepdesign object.  The log. regression model involved uses a subset of data and some of the values in the data frame are NA, so that is affecting my ability to add 'r' and 'g' as variables; I am getting an error because I only have 8397 rows for the new variables and 16197 in the data frame and svrepdesign object.  I am not sure how to overcome this error.

The following is a MRE:

##Archer Lemeshow Goodness of Fit Test for Complex Survey Data with Logistic Regression

library(RCurl)
library(survey)

data <- getURL("https://raw.githubusercontent.com/cbenjamin1821/careertech-ed/master/elsq1adj.csv")
elsq1ch <- read.csv(text = data)

#Specifying the svyrepdesign object which applies the BRR weights
elsq1ch_brr<-svrepdesign(variables = elsq1ch[,1:16], repweights = elsq1ch[,18:217], weights = elsq1ch[,17], combined.weights = TRUE, type = "BRR")
elsq1ch_brr

##Resetting baseline levels for predictors
elsq1ch_brr <- update( elsq1ch_brr , F1HIMATH = relevel(F1HIMATH,"PreAlg or Less") )
elsq1ch_brr <- update( elsq1ch_brr , BYINCOME = relevel(BYINCOME,"0-25K") )
elsq1ch_brr <- update( elsq1ch_brr , F1RACE = relevel(F1RACE,"White") )
elsq1ch_brr <- update( elsq1ch_brr , F1SEX = relevel(F1SEX,"Male") )
elsq1ch_brr <- update( elsq1ch_brr , F1RTRCC = relevel(F1RTRCC,"Academic") )

#Log. Reg. model-all curric. concentrations including F1RTRCC as a predictor
allCC <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+F1RGPP2+F1HIMATH+F1RTRCC,family="binomial",design=elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1,na.action=na.omit)
summary(allCC)

#Recommendations from Lumley (from R Help Archive) on implementing the Archer Lemeshow GOF test
r <- residuals(allCC, type="response")
f<-fitted(allCC)
g<- cut(f, c(-Inf, quantile(f,  (1:9)/10, Inf)))

# now create a new design object with r and g added as variables
#This is the area where I am having problems as my model involves a subset and some values are NA as well
#I am also not sure if I am naming/specifying the new variables of r and g properly
transform(elsq1ch,r=r,g=g)
elsq1ch_brr <- update(elsq1ch_brr,tag=g,tag=r)
#then:
decilemodel<- svyglm(r~g, design=newdesign)
regTermTest(decilemodel, ~g)
#is the F-adjusted mean residual test from the Archer Lemeshow paper

Thank you,
Courtney

?

Courtney Benjamin

Broome-Tioga BOCES

Automotive Technology II Teacher

Located at Gault Toyota

Doctoral Candidate-Educational Theory & Practice

State University of New York at Binghamton

cbenjami at btboces.org<mailto:cbenjami at btboces.org>

607-763-8633

	[[alternative HTML version deleted]]


From ajdamico at gmail.com  Thu Nov 17 10:28:49 2016
From: ajdamico at gmail.com (Anthony Damico)
Date: Thu, 17 Nov 2016 04:28:49 -0500
Subject: [R] Archer-Lemeshow Goodness of Fit Test for Survey Data with
 Log. Regression
In-Reply-To: <1479352496057.0@BTBOCES.ORG>
References: <1479352496057.0@BTBOCES.ORG>
Message-ID: <CAOwvMDwmzFKdkULYRF=dhQuXHS1s=SAeyGecO5t34QxT16EBNQ@mail.gmail.com>

great minimal reproducible example, thanks.  does something like this work?



#Log. Reg. model-all curric. concentrations including F1RTRCC as a predictor
allCC <-
svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+F1RGPP2+F1HIMATH+F1RTRCC,family="binomial",design=elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1,na.action=na.exclude)
summary(allCC)

r <- residuals(allCC, type="response")
f<-fitted(allCC)
your_g<- cut(f, c(-Inf, quantile(f,  (1:9)/10, Inf)))

elsq1ch[ elsq1ch$BYSCTRL==1&elsq1ch$G10COHRT==1 , 'your_g' ] <- your_g
elsq1ch[ elsq1ch$BYSCTRL==1&elsq1ch$G10COHRT==1 , 'r' ] <- r
newdesign<-svrepdesign(variables = elsq1ch, repweights = elsq1ch[,18:217],
weights = elsq1ch[,17], combined.weights = TRUE, type = "BRR")

decilemodel<- svyglm(r~your_g,
design=newdesign,subset=BYSCTRL==1&G10COHRT==1)
regTermTest(decilemodel, ~your_g)




On Wed, Nov 16, 2016 at 10:15 PM, Courtney Benjamin <cbenjami at btboces.org>
wrote:

> ?Hello R Experts,
>
> I am trying to implement the Archer-Lemeshow GOF Test for survey data on a
> logistic regression model using the survey package based upon an R Help
> Archive post that I found where Dr. Thomas Lumley advised how to do it:
> http://r.789695.n4.nabble.com/Goodness-of-t-tests-for-
> Complex-Survey-Logistic-Regression-td4668233.html
>
> Everything is going well until I get to the point where I have to add the
> objects 'r' and 'g' as variables to the data frame by either using the
> transform function or the update function to update the svrepdesign
> object.  The log. regression model involved uses a subset of data and some
> of the values in the data frame are NA, so that is affecting my ability to
> add 'r' and 'g' as variables; I am getting an error because I only have
> 8397 rows for the new variables and 16197 in the data frame and svrepdesign
> object.  I am not sure how to overcome this error.
>
> The following is a MRE:
>
> ##Archer Lemeshow Goodness of Fit Test for Complex Survey Data with
> Logistic Regression
>
> library(RCurl)
> library(survey)
>
> data <- getURL("https://raw.githubusercontent.com/
> cbenjamin1821/careertech-ed/master/elsq1adj.csv")
> elsq1ch <- read.csv(text = data)
>
> #Specifying the svyrepdesign object which applies the BRR weights
> elsq1ch_brr<-svrepdesign(variables = elsq1ch[,1:16], repweights =
> elsq1ch[,18:217], weights = elsq1ch[,17], combined.weights = TRUE, type =
> "BRR")
> elsq1ch_brr
>
> ##Resetting baseline levels for predictors
> elsq1ch_brr <- update( elsq1ch_brr , F1HIMATH = relevel(F1HIMATH,"PreAlg
> or Less") )
> elsq1ch_brr <- update( elsq1ch_brr , BYINCOME = relevel(BYINCOME,"0-25K") )
> elsq1ch_brr <- update( elsq1ch_brr , F1RACE = relevel(F1RACE,"White") )
> elsq1ch_brr <- update( elsq1ch_brr , F1SEX = relevel(F1SEX,"Male") )
> elsq1ch_brr <- update( elsq1ch_brr , F1RTRCC = relevel(F1RTRCC,"Academic")
> )
>
> #Log. Reg. model-all curric. concentrations including F1RTRCC as a
> predictor
> allCC <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+
> F1RGPP2+F1HIMATH+F1RTRCC,family="binomial",design=
> elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1,na.action=na.omit)
> summary(allCC)
>
> #Recommendations from Lumley (from R Help Archive) on implementing the
> Archer Lemeshow GOF test
> r <- residuals(allCC, type="response")
> f<-fitted(allCC)
> g<- cut(f, c(-Inf, quantile(f,  (1:9)/10, Inf)))
>
> # now create a new design object with r and g added as variables
> #This is the area where I am having problems as my model involves a subset
> and some values are NA as well
> #I am also not sure if I am naming/specifying the new variables of r and g
> properly
> transform(elsq1ch,r=r,g=g)
> elsq1ch_brr <- update(elsq1ch_brr,tag=g,tag=r)
> #then:
> decilemodel<- svyglm(r~g, design=newdesign)
> regTermTest(decilemodel, ~g)
> #is the F-adjusted mean residual test from the Archer Lemeshow paper
>
> Thank you,
> Courtney
>
> ?
>
> Courtney Benjamin
>
> Broome-Tioga BOCES
>
> Automotive Technology II Teacher
>
> Located at Gault Toyota
>
> Doctoral Candidate-Educational Theory & Practice
>
> State University of New York at Binghamton
>
> cbenjami at btboces.org<mailto:cbenjami at btboces.org>
>
> 607-763-8633
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From vincenzo.prestigiacomo at unibas.ch  Wed Nov 16 21:06:39 2016
From: vincenzo.prestigiacomo at unibas.ch (Vincenzo Prestigiacomo)
Date: Wed, 16 Nov 2016 20:06:39 +0000
Subject: [R] R help
Message-ID: <4145E5CA2805E44EA270F3A98B4494A921924F9D@urz-mbx-4.urz.unibas.ch>

Hello everybody,

I am a student at Unibas and I am trying to run a two sample t-test on a matrix which is a
255*13 matrix (see attachment). I want to run the t-test, row-wise, with the
columns 3:7 being a part of the first group and columns
8-12 being a part of the second group.

I tried running something like (temp.matrix being my 255*13
matrix)

t.test(temp.matrix[,3:7],temp.matrix[,8:12],paired=TRUE)

or somthing like

as.numeric(t.test(temp.matrix[,3:7],temp.matrix[,8:12],paired=TRUE)[[1]])
so as to only capture the t-value alone and

and I get a result for the whole matrix instead of a row-wise
result.

I really hope you can help me

Thanks

Vincenzo
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: tg.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20161116/a96201a9/attachment.txt>

From G.Maubach at weinwolf.de  Thu Nov 17 12:14:13 2016
From: G.Maubach at weinwolf.de (G.Maubach at weinwolf.de)
Date: Thu, 17 Nov 2016 12:14:13 +0100
Subject: [R] for loop is looping only once
Message-ID: <OF837F0F07.BB36B30B-ONC125806E.003CBB00-C125806E.003DD33A@lotus.hawesko.de>

Hi All,

I need to execute a loop on variables to compute several KPIs. 
Unfortunately the for loop is executed only once for the last KPI given. 
The code below illustrates my current solution but is not completely 
necessary to spot the problem. I just give an idea what I am doing 
overall. Looks much but isn't if copied and run in RStudio. The problem 
occurs in function f_create_kpi_table() in lines 150 to 157:

  for (item in length(kpis))  # This loop runs only once!
  {
    print(kpis[[item]])
    ds_kpi <- f_compute_kpi(
      years    = years,
      kpi      = kpis[[item]],
      kpi_base = kpi_bases[[item]])
    print(ds_kpi)

Here is the complete example code with example data:

- cut --
dataset <-
  structure(
    list(
      to_2012 = c(
        85,
        822,
        891,
        700,
        386,
        127,
        938,
        381,
        871,
        254,
        793,
        0,
        934,
        217,
        163,
        755,
        607,
        794,
        477
      ),
      to_2013 = c(
        289,
        0,
        963,
        243,
        608,
        47,
        0,
        941,
        998,
        775,
        326,
        0,
        0,
        470,
        248,
        439,
        212,
        0,
        0
      ),
      to_2014 = c(0, 0, 71, 0, 0, 434, 0, 282, 0,
                  0, 405, 0, 0, 642, 0, 0, 0, 47, 299),
      to_2015 = c(
        705,
        134,
        659,
        0,
        609,
        807,
        783,
        0,
        0,
        304,
        141,
        500,
        0,
        0,
        764,
        790,
        851,
        0,
        802
      ),
      kpi1_2013 = c(0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
                    0, 0, 0, 1, 1),
      kpi1_2014 = c(1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
                    1, 1, 0, 1, 1, 1, 0, 0),
      kpi1_2015 = c(0, 0, 0, 1, 0, 0, 0, 1,
                    1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0),
      kpi1_2016 = c(0, 1, 0, 1, 0,
                    1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1),
      kpi2_2013 = c(1, 0,
                    1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0),
      kpi2_2014 = c(0,
                    0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1),
      kpi2_2015 = c(1,
                    1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1),
      kpi2_2016 = c(1,
                    0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0)
    ),
    .Names = c(
      "to_2012",
      "to_2013",
      "to_2014",
      "to_2015",
      "kpi1_2013",
      "kpi1_2014",
      "kpi1_2015",
      "kpi1_2016",
      "kpi2_2013",
      "kpi2_2014",
      "kpi2_2015",
      "kpi2_2016"
    ),
    row.names = c(NA, 19L),
    class = "data.frame"
  )

f_compute_kpi <- function(
  years,
  kpi,
  kpi_base)
{
  print(years)
  print(kpi)
  print(kpi_base)

  ds_result <- data.frame()

  for (year in years) {
    current_year  <- year
    previous_year <- year - 1
    result <- sum(dataset[dataset[[paste0(kpi,
                                          "_",
                                          current_year)]] == 1 ,
                          paste0(kpi_base,
                                 "_", previous_year)],
                  na.rm = TRUE)
    ds_result <- rbind(ds_result, result)
  }

  ds_result           <- t(ds_result)
  rownames(ds_result) <- kpi
  colnames(ds_result) <- years

  invisible(ds_result)
}

f_create_kpi_table <- function(
  years,
  kpis,
  kpi_bases)
{
  print(length(kpis))

#-- Problematic loop --
  for (item in length(kpis))  # This loop runs only once!
  {
    print(kpis[[item]])
    ds_kpi <- f_compute_kpi(
      years    = years,
      kpi      = kpis[[item]],
      kpi_base = kpi_bases[[item]])
    print(ds_kpi)
  }
  # This for loop is executed only once for kpi2 instead of
  # as many times as given kpis in length(kpis), i. e.
  # kpi1 AND kpi2.
  # Why?
  # What do I do wrong?
}
-- cut --

What do I need to change to get the loop work correctly and loop over two 
elements instead of one when calling the function

f_create_kpi_table(years = 2013:2016, kpis = c("kpi1", "kpi2"), kpi_bases 
= c("to", "to"))

Kind regards

Georg


From ulrik.stervbo at gmail.com  Thu Nov 17 12:26:10 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Thu, 17 Nov 2016 11:26:10 +0000
Subject: [R] for loop is looping only once
In-Reply-To: <OF837F0F07.BB36B30B-ONC125806E.003CBB00-C125806E.003DD33A@lotus.hawesko.de>
References: <OF837F0F07.BB36B30B-ONC125806E.003CBB00-C125806E.003DD33A@lotus.hawesko.de>
Message-ID: <CAKVAULMKK-BXTZiBStNvguebiJu6dphiSOufprBas=tSwk_aLQ@mail.gmail.com>

Hi Georg,

Your for loop iterates over just one value, to get it to work as you intend
use for(item in 1:length(kpis)){}

HTH
Ulrik

On Thu, 17 Nov 2016 at 12:18 <G.Maubach at weinwolf.de> wrote:

> Hi All,
>
> I need to execute a loop on variables to compute several KPIs.
> Unfortunately the for loop is executed only once for the last KPI given.
> The code below illustrates my current solution but is not completely
> necessary to spot the problem. I just give an idea what I am doing
> overall. Looks much but isn't if copied and run in RStudio. The problem
> occurs in function f_create_kpi_table() in lines 150 to 157:
>
>   for (item in length(kpis))  # This loop runs only once!
>   {
>     print(kpis[[item]])
>     ds_kpi <- f_compute_kpi(
>       years    = years,
>       kpi      = kpis[[item]],
>       kpi_base = kpi_bases[[item]])
>     print(ds_kpi)
>
> Here is the complete example code with example data:
>
> - cut --
> dataset <-
>   structure(
>     list(
>       to_2012 = c(
>         85,
>         822,
>         891,
>         700,
>         386,
>         127,
>         938,
>         381,
>         871,
>         254,
>         793,
>         0,
>         934,
>         217,
>         163,
>         755,
>         607,
>         794,
>         477
>       ),
>       to_2013 = c(
>         289,
>         0,
>         963,
>         243,
>         608,
>         47,
>         0,
>         941,
>         998,
>         775,
>         326,
>         0,
>         0,
>         470,
>         248,
>         439,
>         212,
>         0,
>         0
>       ),
>       to_2014 = c(0, 0, 71, 0, 0, 434, 0, 282, 0,
>                   0, 405, 0, 0, 642, 0, 0, 0, 47, 299),
>       to_2015 = c(
>         705,
>         134,
>         659,
>         0,
>         609,
>         807,
>         783,
>         0,
>         0,
>         304,
>         141,
>         500,
>         0,
>         0,
>         764,
>         790,
>         851,
>         0,
>         802
>       ),
>       kpi1_2013 = c(0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
>                     0, 0, 0, 1, 1),
>       kpi1_2014 = c(1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
>                     1, 1, 0, 1, 1, 1, 0, 0),
>       kpi1_2015 = c(0, 0, 0, 1, 0, 0, 0, 1,
>                     1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0),
>       kpi1_2016 = c(0, 1, 0, 1, 0,
>                     1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1),
>       kpi2_2013 = c(1, 0,
>                     1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0),
>       kpi2_2014 = c(0,
>                     0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1),
>       kpi2_2015 = c(1,
>                     1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1),
>       kpi2_2016 = c(1,
>                     0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0)
>     ),
>     .Names = c(
>       "to_2012",
>       "to_2013",
>       "to_2014",
>       "to_2015",
>       "kpi1_2013",
>       "kpi1_2014",
>       "kpi1_2015",
>       "kpi1_2016",
>       "kpi2_2013",
>       "kpi2_2014",
>       "kpi2_2015",
>       "kpi2_2016"
>     ),
>     row.names = c(NA, 19L),
>     class = "data.frame"
>   )
>
> f_compute_kpi <- function(
>   years,
>   kpi,
>   kpi_base)
> {
>   print(years)
>   print(kpi)
>   print(kpi_base)
>
>   ds_result <- data.frame()
>
>   for (year in years) {
>     current_year  <- year
>     previous_year <- year - 1
>     result <- sum(dataset[dataset[[paste0(kpi,
>                                           "_",
>                                           current_year)]] == 1 ,
>                           paste0(kpi_base,
>                                  "_", previous_year)],
>                   na.rm = TRUE)
>     ds_result <- rbind(ds_result, result)
>   }
>
>   ds_result           <- t(ds_result)
>   rownames(ds_result) <- kpi
>   colnames(ds_result) <- years
>
>   invisible(ds_result)
> }
>
> f_create_kpi_table <- function(
>   years,
>   kpis,
>   kpi_bases)
> {
>   print(length(kpis))
>
> #-- Problematic loop --
>   for (item in length(kpis))  # This loop runs only once!
>   {
>     print(kpis[[item]])
>     ds_kpi <- f_compute_kpi(
>       years    = years,
>       kpi      = kpis[[item]],
>       kpi_base = kpi_bases[[item]])
>     print(ds_kpi)
>   }
>   # This for loop is executed only once for kpi2 instead of
>   # as many times as given kpis in length(kpis), i. e.
>   # kpi1 AND kpi2.
>   # Why?
>   # What do I do wrong?
> }
> -- cut --
>
> What do I need to change to get the loop work correctly and loop over two
> elements instead of one when calling the function
>
> f_create_kpi_table(years = 2013:2016, kpis = c("kpi1", "kpi2"), kpi_bases
> = c("to", "to"))
>
> Kind regards
>
> Georg
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From G.Maubach at weinwolf.de  Thu Nov 17 12:33:26 2016
From: G.Maubach at weinwolf.de (G.Maubach at weinwolf.de)
Date: Thu, 17 Nov 2016 12:33:26 +0100
Subject: [R] Antwort: Re:  for loop is looping only once [SOLVED]
In-Reply-To: <CAKVAULMKK-BXTZiBStNvguebiJu6dphiSOufprBas=tSwk_aLQ@mail.gmail.com>
References: <OF837F0F07.BB36B30B-ONC125806E.003CBB00-C125806E.003DD33A@lotus.hawesko.de>
	<CAKVAULMKK-BXTZiBStNvguebiJu6dphiSOufprBas=tSwk_aLQ@mail.gmail.com>
Message-ID: <OFD7096647.DE896903-ONC125806E.003F5436-C125806E.003F956F@lotus.hawesko.de>

Hi Ulrik,

oh no! What a mistake did I make. But I definitely did not see the 
failure.

Many thanks for helping me.

Kind regards

Georg




Von:    Ulrik Stervbo <ulrik.stervbo at gmail.com>
An:     G.Maubach at weinwolf.de, r-help at r-project.org, 
Datum:  17.11.2016 12:24
Betreff:        Re: [R] for loop is looping only once



Hi Georg,

Your for loop iterates over just one value, to get it to work as you 
intend use for(item in 1:length(kpis)){}

HTH
Ulrik

On Thu, 17 Nov 2016 at 12:18 <G.Maubach at weinwolf.de> wrote:
Hi All,

I need to execute a loop on variables to compute several KPIs.
Unfortunately the for loop is executed only once for the last KPI given.
The code below illustrates my current solution but is not completely
necessary to spot the problem. I just give an idea what I am doing
overall. Looks much but isn't if copied and run in RStudio. The problem
occurs in function f_create_kpi_table() in lines 150 to 157:

  for (item in length(kpis))  # This loop runs only once!
  {
    print(kpis[[item]])
    ds_kpi <- f_compute_kpi(
      years    = years,
      kpi      = kpis[[item]],
      kpi_base = kpi_bases[[item]])
    print(ds_kpi)

Here is the complete example code with example data:

- cut --
dataset <-
  structure(
    list(
      to_2012 = c(
        85,
        822,
        891,
        700,
        386,
        127,
        938,
        381,
        871,
        254,
        793,
        0,
        934,
        217,
        163,
        755,
        607,
        794,
        477
      ),
      to_2013 = c(
        289,
        0,
        963,
        243,
        608,
        47,
        0,
        941,
        998,
        775,
        326,
        0,
        0,
        470,
        248,
        439,
        212,
        0,
        0
      ),
      to_2014 = c(0, 0, 71, 0, 0, 434, 0, 282, 0,
                  0, 405, 0, 0, 642, 0, 0, 0, 47, 299),
      to_2015 = c(
        705,
        134,
        659,
        0,
        609,
        807,
        783,
        0,
        0,
        304,
        141,
        500,
        0,
        0,
        764,
        790,
        851,
        0,
        802
      ),
      kpi1_2013 = c(0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
                    0, 0, 0, 1, 1),
      kpi1_2014 = c(1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
                    1, 1, 0, 1, 1, 1, 0, 0),
      kpi1_2015 = c(0, 0, 0, 1, 0, 0, 0, 1,
                    1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0),
      kpi1_2016 = c(0, 1, 0, 1, 0,
                    1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1),
      kpi2_2013 = c(1, 0,
                    1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0),
      kpi2_2014 = c(0,
                    0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1),
      kpi2_2015 = c(1,
                    1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1),
      kpi2_2016 = c(1,
                    0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0)
    ),
    .Names = c(
      "to_2012",
      "to_2013",
      "to_2014",
      "to_2015",
      "kpi1_2013",
      "kpi1_2014",
      "kpi1_2015",
      "kpi1_2016",
      "kpi2_2013",
      "kpi2_2014",
      "kpi2_2015",
      "kpi2_2016"
    ),
    row.names = c(NA, 19L),
    class = "data.frame"
  )

f_compute_kpi <- function(
  years,
  kpi,
  kpi_base)
{
  print(years)
  print(kpi)
  print(kpi_base)

  ds_result <- data.frame()

  for (year in years) {
    current_year  <- year
    previous_year <- year - 1
    result <- sum(dataset[dataset[[paste0(kpi,
                                          "_",
                                          current_year)]] == 1 ,
                          paste0(kpi_base,
                                 "_", previous_year)],
                  na.rm = TRUE)
    ds_result <- rbind(ds_result, result)
  }

  ds_result           <- t(ds_result)
  rownames(ds_result) <- kpi
  colnames(ds_result) <- years

  invisible(ds_result)
}

f_create_kpi_table <- function(
  years,
  kpis,
  kpi_bases)
{
  print(length(kpis))

#-- Problematic loop --
  for (item in length(kpis))  # This loop runs only once!
  {
    print(kpis[[item]])
    ds_kpi <- f_compute_kpi(
      years    = years,
      kpi      = kpis[[item]],
      kpi_base = kpi_bases[[item]])
    print(ds_kpi)
  }
  # This for loop is executed only once for kpi2 instead of
  # as many times as given kpis in length(kpis), i. e.
  # kpi1 AND kpi2.
  # Why?
  # What do I do wrong?
}
-- cut --

What do I need to change to get the loop work correctly and loop over two
elements instead of one when calling the function

f_create_kpi_table(years = 2013:2016, kpis = c("kpi1", "kpi2"), kpi_bases
= c("to", "to"))

Kind regards

Georg

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Thu Nov 17 12:44:20 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 17 Nov 2016 06:44:20 -0500
Subject: [R] for loop is looping only once
In-Reply-To: <CAKVAULMKK-BXTZiBStNvguebiJu6dphiSOufprBas=tSwk_aLQ@mail.gmail.com>
References: <OF837F0F07.BB36B30B-ONC125806E.003CBB00-C125806E.003DD33A@lotus.hawesko.de>
	<CAKVAULMKK-BXTZiBStNvguebiJu6dphiSOufprBas=tSwk_aLQ@mail.gmail.com>
Message-ID: <f77326af-10ec-669a-30c5-1780cd326ec2@gmail.com>

On 17/11/2016 6:26 AM, Ulrik Stervbo wrote:
> Hi Georg,
>
> Your for loop iterates over just one value, to get it to work as you intend
> use for(item in 1:length(kpis)){}

That usually works, but fails often enough that we recommend using

for (item in seq_along(kpis)) {}

(The failures happen if length(kpis) is zero.  1:0 is a length 2 vector, 
not a length 0 one.)

Duncan Murdoch

>
> HTH
> Ulrik
>
> On Thu, 17 Nov 2016 at 12:18 <G.Maubach at weinwolf.de> wrote:
>
>> Hi All,
>>
>> I need to execute a loop on variables to compute several KPIs.
>> Unfortunately the for loop is executed only once for the last KPI given.
>> The code below illustrates my current solution but is not completely
>> necessary to spot the problem. I just give an idea what I am doing
>> overall. Looks much but isn't if copied and run in RStudio. The problem
>> occurs in function f_create_kpi_table() in lines 150 to 157:
>>
>>   for (item in length(kpis))  # This loop runs only once!
>>   {
>>     print(kpis[[item]])
>>     ds_kpi <- f_compute_kpi(
>>       years    = years,
>>       kpi      = kpis[[item]],
>>       kpi_base = kpi_bases[[item]])
>>     print(ds_kpi)
>>
>> Here is the complete example code with example data:
>>
>> - cut --
>> dataset <-
>>   structure(
>>     list(
>>       to_2012 = c(
>>         85,
>>         822,
>>         891,
>>         700,
>>         386,
>>         127,
>>         938,
>>         381,
>>         871,
>>         254,
>>         793,
>>         0,
>>         934,
>>         217,
>>         163,
>>         755,
>>         607,
>>         794,
>>         477
>>       ),
>>       to_2013 = c(
>>         289,
>>         0,
>>         963,
>>         243,
>>         608,
>>         47,
>>         0,
>>         941,
>>         998,
>>         775,
>>         326,
>>         0,
>>         0,
>>         470,
>>         248,
>>         439,
>>         212,
>>         0,
>>         0
>>       ),
>>       to_2014 = c(0, 0, 71, 0, 0, 434, 0, 282, 0,
>>                   0, 405, 0, 0, 642, 0, 0, 0, 47, 299),
>>       to_2015 = c(
>>         705,
>>         134,
>>         659,
>>         0,
>>         609,
>>         807,
>>         783,
>>         0,
>>         0,
>>         304,
>>         141,
>>         500,
>>         0,
>>         0,
>>         764,
>>         790,
>>         851,
>>         0,
>>         802
>>       ),
>>       kpi1_2013 = c(0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
>>                     0, 0, 0, 1, 1),
>>       kpi1_2014 = c(1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
>>                     1, 1, 0, 1, 1, 1, 0, 0),
>>       kpi1_2015 = c(0, 0, 0, 1, 0, 0, 0, 1,
>>                     1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0),
>>       kpi1_2016 = c(0, 1, 0, 1, 0,
>>                     1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1),
>>       kpi2_2013 = c(1, 0,
>>                     1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0),
>>       kpi2_2014 = c(0,
>>                     0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1),
>>       kpi2_2015 = c(1,
>>                     1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1),
>>       kpi2_2016 = c(1,
>>                     0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0)
>>     ),
>>     .Names = c(
>>       "to_2012",
>>       "to_2013",
>>       "to_2014",
>>       "to_2015",
>>       "kpi1_2013",
>>       "kpi1_2014",
>>       "kpi1_2015",
>>       "kpi1_2016",
>>       "kpi2_2013",
>>       "kpi2_2014",
>>       "kpi2_2015",
>>       "kpi2_2016"
>>     ),
>>     row.names = c(NA, 19L),
>>     class = "data.frame"
>>   )
>>
>> f_compute_kpi <- function(
>>   years,
>>   kpi,
>>   kpi_base)
>> {
>>   print(years)
>>   print(kpi)
>>   print(kpi_base)
>>
>>   ds_result <- data.frame()
>>
>>   for (year in years) {
>>     current_year  <- year
>>     previous_year <- year - 1
>>     result <- sum(dataset[dataset[[paste0(kpi,
>>                                           "_",
>>                                           current_year)]] == 1 ,
>>                           paste0(kpi_base,
>>                                  "_", previous_year)],
>>                   na.rm = TRUE)
>>     ds_result <- rbind(ds_result, result)
>>   }
>>
>>   ds_result           <- t(ds_result)
>>   rownames(ds_result) <- kpi
>>   colnames(ds_result) <- years
>>
>>   invisible(ds_result)
>> }
>>
>> f_create_kpi_table <- function(
>>   years,
>>   kpis,
>>   kpi_bases)
>> {
>>   print(length(kpis))
>>
>> #-- Problematic loop --
>>   for (item in length(kpis))  # This loop runs only once!
>>   {
>>     print(kpis[[item]])
>>     ds_kpi <- f_compute_kpi(
>>       years    = years,
>>       kpi      = kpis[[item]],
>>       kpi_base = kpi_bases[[item]])
>>     print(ds_kpi)
>>   }
>>   # This for loop is executed only once for kpi2 instead of
>>   # as many times as given kpis in length(kpis), i. e.
>>   # kpi1 AND kpi2.
>>   # Why?
>>   # What do I do wrong?
>> }
>> -- cut --
>>
>> What do I need to change to get the loop work correctly and loop over two
>> elements instead of one when calling the function
>>
>> f_create_kpi_table(years = 2013:2016, kpis = c("kpi1", "kpi2"), kpi_bases
>> = c("to", "to"))
>>
>> Kind regards
>>
>> Georg
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dcarlson at tamu.edu  Thu Nov 17 16:04:45 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Thu, 17 Nov 2016 15:04:45 +0000
Subject: [R] R help
In-Reply-To: <4145E5CA2805E44EA270F3A98B4494A921924F9D@urz-mbx-4.urz.unibas.ch>
References: <4145E5CA2805E44EA270F3A98B4494A921924F9D@urz-mbx-4.urz.unibas.ch>
Message-ID: <65d11a6779bd4851beb90d2c0cbf4b35@exch-2p-mbx-w2.ads.tamu.edu>

This should work. First create some reproducible data:

> set.seed(42)
> temp.matrix <- matrix(rnorm(255*13), 255, 13)

Then you need to send each row to t.test() using apply() extracting the statistic with [[1]]:

> result <- apply(temp.matrix, 1, function(x) (t.test(x[3:7], x[8:12], paired=TRUE)[[1]]))
> str(result)
 num [1:255] -0.195 0.307 1.214 2.221 0.633 ...
> quantile(result)
         0%         25%         50%         75%        100% 
-3.39194814 -0.80146023  0.02383907  0.71836716  5.88806695

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Vincenzo Prestigiacomo
Sent: Wednesday, November 16, 2016 2:07 PM
To: r-help at r-project.org
Subject: [R] R help

Hello everybody,

I am a student at Unibas and I am trying to run a two sample t-test on a matrix which is a
255*13 matrix (see attachment). I want to run the t-test, row-wise, with the
columns 3:7 being a part of the first group and columns
8-12 being a part of the second group.

I tried running something like (temp.matrix being my 255*13
matrix)

t.test(temp.matrix[,3:7],temp.matrix[,8:12],paired=TRUE)

or somthing like

as.numeric(t.test(temp.matrix[,3:7],temp.matrix[,8:12],paired=TRUE)[[1]])
so as to only capture the t-value alone and

and I get a result for the whole matrix instead of a row-wise
result.

I really hope you can help me

Thanks

Vincenzo


From qwertyui_period at yahoo.co.jp  Thu Nov 17 13:44:10 2016
From: qwertyui_period at yahoo.co.jp (qwertyui_period at yahoo.co.jp)
Date: Thu, 17 Nov 2016 21:44:10 +0900 (JST)
Subject: [R] Question about proxy setting of R
Message-ID: <16550.83163.qm@web102314.mail.kks.yahoo.co.jp>

Hello,

I use R 3.0.2 on Win 7 through proxy server using ".Rprofile" in home
directory that includes "Sys.setenv(http_proxy=proxy_server:port)".
There has been no problem to access the internet for some years.
In this situation, I installed R 3.3.1 and then entered "update.packages
()", however, "Proxy Authentification" window didn't show up and
failed to access the internet. Error messages are as below.

----------------------------------------------------------------------------------------------------------------------------
> update.packages(ask='graphics',checkBuilt=TRUE)
--- Please select a CRAN mirror for use in this session ---
Warning: failed to download mirrors file (cannot open URL
'https://cran.r-project.org/CRAN_mirrors.csv'); using local file
'C:/PROGRA~1/R/R-33~1.2/doc/CRAN_mirrors.csv'
Warning: unable to access index for repository
https://cran.ism.ac.jp/src/contrib: 
? cannot open URL 'https://cran.ism.ac.jp/src/contrib/PACKAGES' 
Warning: unable to access index for repository
http://www.stats.ox.ac.uk/pub/RWin/src/contrib: 
? cannot open URL 'http://www.stats.ox.ac.uk/pub/RWin/src/contrib/PACKAGES' 
Warning message:
In download.file(url, destfile = f, quiet = TRUE) :
? cannot open URL 'https://cran.r-project.org/CRAN_mirrors.csv': HTTP
status was '407 Proxy Authentication Required'

----------------------------------------------------------------------------------------------------------------------------

Strange to say, R 3.0.2 is able to access to the internet, and R 3.3.1
shows collect proxy setting in ".Rprofile"? by "Sys.getenv("http_proxy")"
From internet information, I added "http_proxy_user=ask" to ".Rprofile", or
" --internet2" to the desktop icon of R 3.3.1, ending up in the same
result.

Please show me the way of proxy setting of R 3.3.1.



From zhengda1936 at gmail.com  Thu Nov 17 20:19:09 2016
From: zhengda1936 at gmail.com (Da Zheng)
Date: Thu, 17 Nov 2016 14:19:09 -0500
Subject: [R] Inconsistency of 1^NA=1 vs. 1.1^NA=NA
Message-ID: <CAFLer83BnbhWm3yAhrzXTHoXWr8JT6EfqDSUG0ii3pBTT+Ovkg@mail.gmail.com>

Hello,

I just realized that 1^NA outputs 1 while 1.1^NA outputs NA in R v3.3.1 and
R v3.2.3.
I tried other values such as 0^NA and 2^NA, and they all output NA.
I don't understand this inconsistency here. Shouldn't 1^NA output NA as
well? Why does R handle it differently? Or is this a bug in these
particular versions of R?

Thanks,
Da

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Thu Nov 17 20:54:12 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Thu, 17 Nov 2016 20:54:12 +0100
Subject: [R] Inconsistency of 1^NA=1 vs. 1.1^NA=NA
In-Reply-To: <CAFLer83BnbhWm3yAhrzXTHoXWr8JT6EfqDSUG0ii3pBTT+Ovkg@mail.gmail.com>
References: <CAFLer83BnbhWm3yAhrzXTHoXWr8JT6EfqDSUG0ii3pBTT+Ovkg@mail.gmail.com>
Message-ID: <CAJuCY5yPi9B9BrkDTe32R0NRTwdve1VbW1HjHab2D7y4bL_3sw@mail.gmail.com>

Dear Da,

NA represents an unknown value x. 1 ^ x = 1 for all possible values of x.
Hence 1 ^ NA = 1.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-11-17 20:19 GMT+01:00 Da Zheng <zhengda1936 at gmail.com>:

> Hello,
>
> I just realized that 1^NA outputs 1 while 1.1^NA outputs NA in R v3.3.1 and
> R v3.2.3.
> I tried other values such as 0^NA and 2^NA, and they all output NA.
> I don't understand this inconsistency here. Shouldn't 1^NA output NA as
> well? Why does R handle it differently? Or is this a bug in these
> particular versions of R?
>
> Thanks,
> Da
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Thu Nov 17 21:13:00 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 17 Nov 2016 15:13:00 -0500
Subject: [R] Inconsistency of 1^NA=1 vs. 1.1^NA=NA
In-Reply-To: <CAFLer83BnbhWm3yAhrzXTHoXWr8JT6EfqDSUG0ii3pBTT+Ovkg@mail.gmail.com>
References: <CAFLer83BnbhWm3yAhrzXTHoXWr8JT6EfqDSUG0ii3pBTT+Ovkg@mail.gmail.com>
Message-ID: <5c048b9c-d95e-5eb0-a52b-319b9c1906ef@gmail.com>

On 17/11/2016 2:19 PM, Da Zheng wrote:
> Hello,
>
> I just realized that 1^NA outputs 1 while 1.1^NA outputs NA in R v3.3.1 and
> R v3.2.3.
> I tried other values such as 0^NA and 2^NA, and they all output NA.
> I don't understand this inconsistency here. Shouldn't 1^NA output NA as
> well? Why does R handle it differently? Or is this a bug in these
> particular versions of R?

Our usual interpretation of NA is "an unknown value".  So 1^NA would 
give NA if there were multiple possible values for it depending on what 
number you substitute for NA, and should give the unique answer if 
there's only one possibility (as for example NA | TRUE gives TRUE).

As far as I can see, 1 raised to any power (even infinite ones) should 
give 1, so the answer looks fine to me.  That's not true of any of the 
other bases you mention (just as NA | FALSE gives NA).

Duncan Murdoch


From bgunter.4567 at gmail.com  Thu Nov 17 21:42:15 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 17 Nov 2016 12:42:15 -0800
Subject: [R] Inconsistency of 1^NA=1 vs. 1.1^NA=NA
In-Reply-To: <CAJuCY5yPi9B9BrkDTe32R0NRTwdve1VbW1HjHab2D7y4bL_3sw@mail.gmail.com>
References: <CAFLer83BnbhWm3yAhrzXTHoXWr8JT6EfqDSUG0ii3pBTT+Ovkg@mail.gmail.com>
	<CAJuCY5yPi9B9BrkDTe32R0NRTwdve1VbW1HjHab2D7y4bL_3sw@mail.gmail.com>
Message-ID: <CAGxFJbRrTmNCkHV25TwUxPWoDeP+Wwp4rXkiBukYKvvXQTC2YA@mail.gmail.com>

On Nov 17, 2016 11:55 AM, "Thierry Onkelinx" <thierry.onkelinx at inbo.be>
wrote:
>
> Dear Da,
>
> NA represents an unknown value x. 1 ^ x = 1 for all possible values of x.
> Hence 1 ^ NA = 1.
>
That is false. For any n, n-1 of the nth roots of 1 differ from 1(they are
complex). I don't have my computer with me. What does (1+ 0i)^ NA give?

Bert

> Best regards,
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to
say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of
data.
> ~ John Tukey
>
> 2016-11-17 20:19 GMT+01:00 Da Zheng <zhengda1936 at gmail.com>:
>
> > Hello,
> >
> > I just realized that 1^NA outputs 1 while 1.1^NA outputs NA in R v3.3.1
and
> > R v3.2.3.
> > I tried other values such as 0^NA and 2^NA, and they all output NA.
> > I don't understand this inconsistency here. Shouldn't 1^NA output NA as
> > well? Why does R handle it differently? Or is this a bug in these
> > particular versions of R?
> >
> > Thanks,
> > Da
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> > posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From zhengda1936 at gmail.com  Thu Nov 17 21:47:58 2016
From: zhengda1936 at gmail.com (Da Zheng)
Date: Thu, 17 Nov 2016 15:47:58 -0500
Subject: [R] Inconsistency of 1^NA=1 vs. 1.1^NA=NA
In-Reply-To: <CAGxFJbRrTmNCkHV25TwUxPWoDeP+Wwp4rXkiBukYKvvXQTC2YA@mail.gmail.com>
References: <CAFLer83BnbhWm3yAhrzXTHoXWr8JT6EfqDSUG0ii3pBTT+Ovkg@mail.gmail.com>
	<CAJuCY5yPi9B9BrkDTe32R0NRTwdve1VbW1HjHab2D7y4bL_3sw@mail.gmail.com>
	<CAGxFJbRrTmNCkHV25TwUxPWoDeP+Wwp4rXkiBukYKvvXQTC2YA@mail.gmail.com>
Message-ID: <CAFLer80-i_O5GdHxSC0AWZcz20y-5HaxZSXU7471ccEnj1vutA@mail.gmail.com>

I tried on my computer.
> (1+ 0i)^ NA
[1] NaN+NaNi


On Thu, Nov 17, 2016 at 3:42 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:

> On Nov 17, 2016 11:55 AM, "Thierry Onkelinx" <thierry.onkelinx at inbo.be>
> wrote:
> >
> > Dear Da,
> >
> > NA represents an unknown value x. 1 ^ x = 1 for all possible values of x.
> > Hence 1 ^ NA = 1.
> >
> That is false. For any n, n-1 of the nth roots of 1 differ from 1(they are
> complex). I don't have my computer with me. What does (1+ 0i)^ NA give?
>
> Bert
>
> > Best regards,
> >
> > ir. Thierry Onkelinx
> > Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and
> > Forest
> > team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> > Kliniekstraat 25
> > 1070 Anderlecht
> > Belgium
> >
> > To call in the statistician after the experiment is done may be no more
> > than asking him to perform a post-mortem examination: he may be able to
> say
> > what the experiment died of. ~ Sir Ronald Aylmer Fisher
> > The plural of anecdote is not data. ~ Roger Brinner
> > The combination of some data and an aching desire for an answer does not
> > ensure that a reasonable answer can be extracted from a given body of
> data.
> > ~ John Tukey
> >
> > 2016-11-17 20:19 GMT+01:00 Da Zheng <zhengda1936 at gmail.com>:
> >
> > > Hello,
> > >
> > > I just realized that 1^NA outputs 1 while 1.1^NA outputs NA in R
> v3.3.1 and
> > > R v3.2.3.
> > > I tried other values such as 0^NA and 2^NA, and they all output NA.
> > > I don't understand this inconsistency here. Shouldn't 1^NA output NA as
> > > well? Why does R handle it differently? Or is this a bug in these
> > > particular versions of R?
> > >
> > > Thanks,
> > > Da
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/
> > > posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From spencer.graves at effectivedefense.org  Thu Nov 17 22:00:06 2016
From: spencer.graves at effectivedefense.org (Spencer Graves)
Date: Thu, 17 Nov 2016 15:00:06 -0600
Subject: [R] Inconsistency of 1^NA=1 vs. 1.1^NA=NA
In-Reply-To: <CAFLer80-i_O5GdHxSC0AWZcz20y-5HaxZSXU7471ccEnj1vutA@mail.gmail.com>
References: <CAFLer83BnbhWm3yAhrzXTHoXWr8JT6EfqDSUG0ii3pBTT+Ovkg@mail.gmail.com>
	<CAJuCY5yPi9B9BrkDTe32R0NRTwdve1VbW1HjHab2D7y4bL_3sw@mail.gmail.com>
	<CAGxFJbRrTmNCkHV25TwUxPWoDeP+Wwp4rXkiBukYKvvXQTC2YA@mail.gmail.com>
	<CAFLer80-i_O5GdHxSC0AWZcz20y-5HaxZSXU7471ccEnj1vutA@mail.gmail.com>
Message-ID: <8f5a5ce1-60b0-4910-9f95-4de7cffff8e4@effectivedefense.org>

 > (1+ 0i)^ NA
[1] NA
 > sessionInfo()
R version 3.3.1 (2016-06-21)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: OS X 10.11.6 (El Capitan)

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils
[5] datasets  methods   base

other attached packages:
[1] lubridate_1.6.0

loaded via a namespace (and not attached):
[1] magrittr_1.5  rsconnect_0.5 tools_3.3.1
[4] stringi_1.1.2 stringr_1.1.0
 >

On 11/17/2016 2:47 PM, Da Zheng wrote:
> I tried on my computer.
>> (1+ 0i)^ NA
> [1] NaN+NaNi
>
>
> On Thu, Nov 17, 2016 at 3:42 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
>> On Nov 17, 2016 11:55 AM, "Thierry Onkelinx" <thierry.onkelinx at inbo.be>
>> wrote:
>>> Dear Da,
>>>
>>> NA represents an unknown value x. 1 ^ x = 1 for all possible values of x.
>>> Hence 1 ^ NA = 1.
>>>
>> That is false. For any n, n-1 of the nth roots of 1 differ from 1(they are
>> complex). I don't have my computer with me. What does (1+ 0i)^ NA give?
>>
>> Bert
>>
>>> Best regards,
>>>
>>> ir. Thierry Onkelinx
>>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>> and
>>> Forest
>>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>>> Kliniekstraat 25
>>> 1070 Anderlecht
>>> Belgium
>>>
>>> To call in the statistician after the experiment is done may be no more
>>> than asking him to perform a post-mortem examination: he may be able to
>> say
>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> The plural of anecdote is not data. ~ Roger Brinner
>>> The combination of some data and an aching desire for an answer does not
>>> ensure that a reasonable answer can be extracted from a given body of
>> data.
>>> ~ John Tukey
>>>
>>> 2016-11-17 20:19 GMT+01:00 Da Zheng <zhengda1936 at gmail.com>:
>>>
>>>> Hello,
>>>>
>>>> I just realized that 1^NA outputs 1 while 1.1^NA outputs NA in R
>> v3.3.1 and
>>>> R v3.2.3.
>>>> I tried other values such as 0^NA and 2^NA, and they all output NA.
>>>> I don't understand this inconsistency here. Shouldn't 1^NA output NA as
>>>> well? Why does R handle it differently? Or is this a bug in these
>>>> particular versions of R?
>>>>
>>>> Thanks,
>>>> Da
>>>>
>>>>          [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/
>>>> posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pgilbert902 at gmail.com  Thu Nov 17 22:06:34 2016
From: pgilbert902 at gmail.com (Paul Gilbert)
Date: Thu, 17 Nov 2016 16:06:34 -0500
Subject: [R] Some basic time series questions
In-Reply-To: <mailman.1.1479380402.12219.r-help@r-project.org>
References: <mailman.1.1479380402.12219.r-help@r-project.org>
Message-ID: <2efcda98-c478-f116-2453-66203b4019c4@gmail.com>


On 11/17/2016 06:00 AM, r-help-request at r-project.org wrote:
> Hi,
>
> As I sit and learn how to work with time series, I've run into a problem
> that is eluding a quick easy answer (and googling for answers seems to
> really slow the process...)
>
> Question #1--
> In a simple example on R 3.3.1 (sorry my employer hasn't upgraded to 3.3.2
> yet):
>
> x=rnorm(26,0,1)
> x.ts<-ts(x,start=c(2014,9),frequency=12)
>
> inputting x.ts at the prompt gives me a table with the rooms denoted by
> year and columns denoted by months and everything lines up wonderfully.
>
> Now my problem comes when I type at the prompt
>
> plot(x.ts)  or
> plot(x.ts, xlab="") or
> plot.ts(x.ts,xlab="")
>
> I get a plot of the values, but my x-axis labels are 2015.0, 2015.5,
> 2016.0, and 2016.5 .  January 2015 is coming out as 2015.0...
>
> Is there a way of getting a more intelligible x-axis labeling?  Even 2015.1
> for Janaury, etc. would work, or even getting an index (either Septemebr
> 2014 representing 0 or 1 and it incrementally increasing each month).

There are several ways to do this. Having some bias, I would do

   require(tfplot)
   tfplot(x.ts)

>
> Question #2--
> If I have a time series of decadal events, how best should I set the
> frequency.  It is historical data, in the form of say AD 610-619 5 events,
> AD 620-629 7 events, etc.

For anything other than annual, quarterly, and monthly data you probably 
should consider zoo. (There are other options, but I think zoo is the 
most widely used for some time now.) Guessing a bit how you think of 
this data, I would say you want the date index to be the first year of 
the decade. To illustrate, I can generate a hundred decades of random 
data and index it thus:

require(zoo)
x <- zoo(round(10 * runif(100)) , order.by= 10 * 61:160)

Paul

>
> Sorry for such a basic questions.  Any advice would be appreciated.
>
> Thanks in advance, MEH
>
>
>
> Mark E. Hall, PhD
> Assistant Field Manager
> Black Rock Field Office
> Winnemucca District Office
> 775-623-1529.


From jdnewmil at dcn.davis.ca.us  Thu Nov 17 22:49:18 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 17 Nov 2016 13:49:18 -0800
Subject: [R] require vs library ( Some basic time series questions)
In-Reply-To: <2efcda98-c478-f116-2453-66203b4019c4@gmail.com>
References: <mailman.1.1479380402.12219.r-help@r-project.org>
	<2efcda98-c478-f116-2453-66203b4019c4@gmail.com>
Message-ID: <7F8852CB-0B17-448B-B16E-69C4BDC97462@dcn.davis.ca.us>

> require(tfplot)
> tfplot(x.ts)

Would just like to point out that require() should not be treated as interchangeable with library(). The former returns a logical status indicating success or failure, while the latter throws an error if it falls.  You should reserve use of require() for cases when you are implementing an alternative path of execution for failure, and in nearly all usual cases use the library() function instead so hapless users of your script don't have to sift through all the subsequent errors to figure out the problem.
-- 
Sent from my phone. Please excuse my brevity.


From pgilbert902 at gmail.com  Thu Nov 17 23:49:06 2016
From: pgilbert902 at gmail.com (Paul Gilbert)
Date: Thu, 17 Nov 2016 17:49:06 -0500
Subject: [R] require vs library ( Some basic time series questions)
In-Reply-To: <7F8852CB-0B17-448B-B16E-69C4BDC97462@dcn.davis.ca.us>
References: <mailman.1.1479380402.12219.r-help@r-project.org>
	<2efcda98-c478-f116-2453-66203b4019c4@gmail.com>
	<7F8852CB-0B17-448B-B16E-69C4BDC97462@dcn.davis.ca.us>
Message-ID: <d7155fb0-2ca1-d9ab-85e1-c548111d6fd0@gmail.com>



On 11/17/2016 04:49 PM, Jeff Newmiller wrote:
>> require(tfplot)
>> tfplot(x.ts)
>
> Would just like to point out that require() should not be treated as
> interchangeable with library(). The former returns a logical status
> indicating success or failure, while the latter throws an error if it
> falls.  You should reserve use of require() for cases when you are
> implementing an alternative path of execution for failure, and in
> nearly all usual cases use the library() function instead so hapless
> users of your script don't have to sift through all the subsequent
> errors to figure out the problem.
>
Mea culpa.  Force of habit from usually writing with an alternative path 
of execution. In this example I should have used library(), especially 
since 'tfplot' may not have been installed on the user's system and so 
the library() error message would be more explicit than the warning from 
require(). But "in nearly all usual cases" seems a bit strong. It 
implies R users don't usually program much and thus do not implement an 
alternative path of execution for failure. (Some of us consider that the 
most usual case.)

Paul


From klebyn at yahoo.com.br  Fri Nov 18 00:12:17 2016
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Thu, 17 Nov 2016 21:12:17 -0200
Subject: [R] tcltk table "validateCommand"
In-Reply-To: <3058A258-836E-4EF2-8F6D-EBF1F4270AD1@gmail.com>
References: <CAJeYpE-1O4AL00pnhtX8Nx=6gTe9ha5RXKLR3Va8OkvHfDb=Lw@mail.gmail.com>
	<3058A258-836E-4EF2-8F6D-EBF1F4270AD1@gmail.com>
Message-ID: <4711d8ed-36d5-5e26-1466-33561abb0078@yahoo.com.br>

Hi Dan
Were you able to find a way to access the % S values ??

Do you have any examples of how this works?

Thank you for your attention.
Cleber


Em 25/01/2016 08:21, peter dalgaard escreveu:
> It's been so long that I have forgotten how to get the package with the table widget installed on OSX, so I cannot check things for you. However, the canonical way to handle %S type arguments is to pass them as formal arguments to the callback, e.g.
>
>> .Tcl.callback(function(x,y)x+y)
> [1] "R_call 0x7f9a34806ca0 %x %y"
>
> so I would assume that you should just define your
>
> CellValidation <- function(S){....}
>
> and then just access S as a variable inside the function.
>
> As far as I remember, this only works at entry completion, though. That does sort of make sense since not every prefix of a valid entry is valid ("1e-2" is a double, "1e-" is not). If you want to actually disable certain keys during entry, then you have a larger task on your hand.
>
> -pd
>
> On 22 Jan 2016, at 21:25 , Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
>
>> I'd like to allow users to edit data in tcltk tables and to use vcmd to
>> validate data entry, e.g., not allowing non-numbers to be entered in
>> numeric cells and not allowing '\n' to be entered in text cells.
>>
>> The problem is that I can't figure out how to "see" their data entry before
>> it is entered, although it looks like %S can be somehow used in vcmd to get
>> this information.
>>
>> Example: to disallow '\n' to be entered into a cell in an editable table:
>>
>> require(tcltk2)
>> tt<-tktoplevel(); tfr<-tkframe(tt); tkgrid(tfr)
>> tableData<-tclArray()
>> tableData[[0,0]]<-"junk"
>>
>> CellValidation<-function(){
>>
>> ## http://www.tcl.tk/community/hobbs/tcl/capp/tkTable/tkTable.html says:
>> ## *%S* For *ValidateCommand*, it is the potential new value of the cell
>> being validated.
>> ## which is exactly what I want, but I can't figure out how to do that.
>> ## The following allows one bad character and then disallows further edits
>>
>>   testval<-tclvalue(tcl(table1,"curvalue"))
>>
>>   if (length(grep("\n",testval))>0)  return(tcl("expr", FALSE))  else
>> return(tcl("expr", TRUE))
>> }
>>
>> table1<<-tk2table(tfr,
>>   rows=1,cols=1,
>>   selectmode="extended",
>>   variable=tableData,
>>   validate=T,
>>   vcmd=CellValidation
>> )
>>
>> tcl(table1,"tag","configure", "active", fg='black',bg=colors()[411])
>> tkgrid(table1)
>>
>> How can I get the %S value rather than the tcl(table1,"curvalue")?
>>
>> Much thanks for any help.
>>
>> -Dan
>>


---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Fri Nov 18 00:51:11 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 17 Nov 2016 15:51:11 -0800
Subject: [R] require vs library ( Some basic time series questions)
In-Reply-To: <7F8852CB-0B17-448B-B16E-69C4BDC97462@dcn.davis.ca.us>
References: <mailman.1.1479380402.12219.r-help@r-project.org>
	<2efcda98-c478-f116-2453-66203b4019c4@gmail.com>
	<7F8852CB-0B17-448B-B16E-69C4BDC97462@dcn.davis.ca.us>
Message-ID: <BCC4C1D2-33AC-4890-B15C-802F930CB7C9@dcn.davis.ca.us>

Perhaps more people write end-user-ready applications in R than I am aware of and my bias is too strong.  For working at the console I prefer not to have my scripts installing packages on their own (one possible alternative execution path), and it is too much trouble to implement multiple routes to the end of an analysis in most cases, so library() is usually best for me. 
-- 
Sent from my phone. Please excuse my brevity.

On November 17, 2016 1:49:18 PM PST, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>> require(tfplot)
>> tfplot(x.ts)
>
>Would just like to point out that require() should not be treated as
>interchangeable with library(). The former returns a logical status
>indicating success or failure, while the latter throws an error if it
>falls.  You should reserve use of require() for cases when you are
>implementing an alternative path of execution for failure, and in
>nearly all usual cases use the library() function instead so hapless
>users of your script don't have to sift through all the subsequent
>errors to figure out the problem.


From ddalthorp at usgs.gov  Fri Nov 18 00:55:44 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Thu, 17 Nov 2016 15:55:44 -0800
Subject: [R] tcltk table "validateCommand"
In-Reply-To: <4711d8ed-36d5-5e26-1466-33561abb0078@yahoo.com.br>
References: <CAJeYpE-1O4AL00pnhtX8Nx=6gTe9ha5RXKLR3Va8OkvHfDb=Lw@mail.gmail.com>
	<3058A258-836E-4EF2-8F6D-EBF1F4270AD1@gmail.com>
	<4711d8ed-36d5-5e26-1466-33561abb0078@yahoo.com.br>
Message-ID: <CAJeYpE8O0SQN==EgKinKn3Z3ny-Ebr4q+kSH_4wdYMgndYRZsA@mail.gmail.com>

Yes, Peter's suggestion worked just right.

Here's an example that prints out the proposed cell content and will not
allow 'w' or '\n' to be entered into the table:

require(tcltk2)
tt<-tktoplevel(); tfr<-tkframe(tt); tkgrid(tfr)
tableData<-tclArray()

valChar<-function(S){
  print(S); flush.console()
  if (length(grep("\n",S))>0){ #
    return(tcl("expr", FALSE))
  } else if (length(grep("w",S))>0){ #
    return(tcl("expr", FALSE))
  } else {
    return(tcl("expr", TRUE))# but other kinds of space are not-->
error-checking if good value is added
  }
}

table1<-tk2table(tfr,
  rows = 1,cols = 1,
  variable = tableData,
  validate = T,
  vcmd = valChar
)

tcl(table1, "tag", "configure", "active", fg='black', bg='yellow')
tkgrid(table1)

# I believe when you define vcmd, it has the effect of reserving the %
arguments for their meanings as defined "in the manual"
# there's no need to think about the %'s thereafter.
# for some arguments, it doesn't seem to work as expected

-Dan


On Thu, Nov 17, 2016 at 3:12 PM, Cleber N.Borges via R-help <
r-help at r-project.org> wrote:

> Hi Dan
> Were you able to find a way to access the % S values ??
>
> Do you have any examples of how this works?
>
> Thank you for your attention.
> Cleber
>
>
> Em 25/01/2016 08:21, peter dalgaard escreveu:
> > It's been so long that I have forgotten how to get the package with the
> table widget installed on OSX, so I cannot check things for you. However,
> the canonical way to handle %S type arguments is to pass them as formal
> arguments to the callback, e.g.
> >
> >> .Tcl.callback(function(x,y)x+y)
> > [1] "R_call 0x7f9a34806ca0 %x %y"
> >
> > so I would assume that you should just define your
> >
> > CellValidation <- function(S){....}
> >
> > and then just access S as a variable inside the function.
> >
> > As far as I remember, this only works at entry completion, though. That
> does sort of make sense since not every prefix of a valid entry is valid
> ("1e-2" is a double, "1e-" is not). If you want to actually disable certain
> keys during entry, then you have a larger task on your hand.
> >
> > -pd
> >
> > On 22 Jan 2016, at 21:25 , Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
> >
> >> I'd like to allow users to edit data in tcltk tables and to use vcmd to
> >> validate data entry, e.g., not allowing non-numbers to be entered in
> >> numeric cells and not allowing '\n' to be entered in text cells.
> >>
> >> The problem is that I can't figure out how to "see" their data entry
> before
> >> it is entered, although it looks like %S can be somehow used in vcmd to
> get
> >> this information.
> >>
> >> Example: to disallow '\n' to be entered into a cell in an editable
> table:
> >>
> >> require(tcltk2)
> >> tt<-tktoplevel(); tfr<-tkframe(tt); tkgrid(tfr)
> >> tableData<-tclArray()
> >> tableData[[0,0]]<-"junk"
> >>
> >> CellValidation<-function(){
> >>
> >> ## http://www.tcl.tk/community/hobbs/tcl/capp/tkTable/tkTable.html
> says:
> >> ## *%S* For *ValidateCommand*, it is the potential new value of the cell
> >> being validated.
> >> ## which is exactly what I want, but I can't figure out how to do that.
> >> ## The following allows one bad character and then disallows further
> edits
> >>
> >>   testval<-tclvalue(tcl(table1,"curvalue"))
> >>
> >>   if (length(grep("\n",testval))>0)  return(tcl("expr", FALSE))  else
> >> return(tcl("expr", TRUE))
> >> }
> >>
> >> table1<<-tk2table(tfr,
> >>   rows=1,cols=1,
> >>   selectmode="extended",
> >>   variable=tableData,
> >>   validate=T,
> >>   vcmd=CellValidation
> >> )
> >>
> >> tcl(table1,"tag","configure", "active", fg='black',bg=colors()[411])
> >> tkgrid(table1)
> >>
> >> How can I get the %S value rather than the tcl(table1,"curvalue")?
> >>
> >> Much thanks for any help.
> >>
> >> -Dan
> >>
>
>
> ---
> Este email foi escaneado pelo Avast antiv?rus.
> https://www.avast.com/antivirus
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From kestrel1978 at gmail.com  Thu Nov 17 23:13:48 2016
From: kestrel1978 at gmail.com (Ronny Steen)
Date: Thu, 17 Nov 2016 23:13:48 +0100
Subject: [R] How to set up a Negative Exponential regression mixed effect
 model 'nlme' ?
Message-ID: <CABVupNb70EYtUxsiteatpbw1hY89RjW5npau23OAokeBmAdW9g@mail.gmail.com>

Hi,

I need help with fitting a non-linear mixed effects model (nested random
effect). I look at the relationship between 'wingbeat frequency' (beats per
sec) and 'wing length' in hummingbirds, and I will find a model with the
best fit.

I have posted the question at R-SIG-mixed-models list, but no answer so
far, therefore I give it a new try here.

I manage to fit Negative Natural Exponential regression and Assymptotic
(SSasymp-function) nonlinear regression, but I don't figure out how to fit
Negative Exponential regression (similar to nls {stats} function
'negexp.SSival').

*My question:* Could I get some help adapting the Negative Exponential
regression model (*'nls'*, without random effect) to a model with nested
random effect (*'nlme' *)?

Script and access to data:

library(nlme)

library(MASS)


WBF <-read.csv(url("https://www.dropbox.com/s/hin8o27i1kmdloe/
Species2016.csv.csv?raw=1"))

plot(WBF$WL,WBF$Beat_freq,type="p",ylab="WBF")# Make a plot

#Negative Natural Exponential regression with nested random effect

fm1<-lme(Beat_freq ~ exp(-WL),

data = WBF,

random = ~ 1|Species/ID, method = "ML")

#Assymptotic (SSasymp-function) nonlinear regression with nested random
effect

fm2 <- nlme(Beat_freq ~ SSasymp(WL, Asym, R0, lrc),

        data = WBF,

        fixed = Asym + R0 + lrc ~ 1,

        random = Asym ~ 1|Species/ID,

        start = c(Asym = 27, R0 = 6000, lrc = 0.19))

#Negative Exponential regression - HELP NEEDED TO INCLUDE NESTED RANDOM
EFFECT ('nlme')

negexp<-selfStart(model = ~b0 + b1*exp(-x/th), initial =

    negexp.SSival,parameters=c("b0","b1","th"),

    template=function(x,b0,b1,th){})

fm3<-nls(Beat_freq~negexp(WL,B0,B1,theta),

     data=WBF,control=list(maxiter =5000),trace=T)

Regards,

Ron

	[[alternative HTML version deleted]]


From kmezhoud at gmail.com  Fri Nov 18 08:27:25 2016
From: kmezhoud at gmail.com (Karim Mezhoud)
Date: Fri, 18 Nov 2016 08:27:25 +0100
Subject: [R] aggregate dataframe by multiple factors
Message-ID: <CALJKBv9S5=3rYffCbW098eVm8_9XtdE4-FrdWOHTH9AcGWJu4g@mail.gmail.com>

Dear all,

the dat  has missing values NA,

    first.Name   Name Department  DCE   DP       date
5      Auction Videos        YME 0.57 0.56 2013-09-30
18       Amish  Wives        TAS 0.59 0.56 2013-09-30
34     Ancient Nation        QLH 0.54 0.58 2013-09-30
53     Auction Videos        YME   NA   NA 2013-12-28
66       Amish  Wives        TAS   NA   NA 2013-12-28
82     Ancient Nation        QLH 0.28 0.29 2013-12-28
102    Auction Videos        YME 0.57 0.56 2014-03-30
115      Amish  Wives        TAS 0.59 0.56 2014-03-30
131    Ancient Nation        QLH 0.54 0.58 2014-03-30
150    Auction Videos        YME   NA   NA 2014-06-28
163      Amish  Wives        TAS   NA   NA 2014-06-28
179    Ancient Nation        QLH 0.28 0.29 2014-06-28


agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
list(dat$first.Name, dat$Name, dat$Department) , "sort"))


agg has list of value. I would separate value in different columns.

  Group.1 Group.2 Group.3                    DCE                     DP
1 Ancient  Nation     QLH 0.28, 0.28, 0.54, 0.54 0.29, 0.29, 0.58, 0.58
2   Amish   Wives     TAS             0.59, 0.59             0.56, 0.56
3 Auction  Videos     YME             0.57, 0.57             0.56, 0.56

The  goal:

Group.1 Group.2 Group.3  DCE.1 DCE.2 DCE.3  DCE.4  DP.1  DP.2  DP.3  DP.4
1 Ancient  Nation     QLH    0.28     0.28    0.54     0.54     0.29, 0.29,
0.58, 0.58
2   Amish   Wives     TAS        NA     NA     0.59, 0.59           NA
NA  0.56, 0.56
3 Auction  Videos     YME         NA   NA      0.57, 0.57             NA
NA  0.56, 0.56



dat <- structure(list(first.Name = structure(c(3L, 1L, 2L, 3L, 1L, 2L,
3L, 1L, 2L, 3L, 1L, 2L), .Label = c("Amish", "Ancient", "Auction",
"Ax", "Bachelorette", "Basketball", "BBQ", "Cake", "Celebrity",
"Chef", "Clean", "Colonial", "Comedy", "Comic", "Crocodile",
"Dog", "Empire", "Extreme", "Farm", "Half Pint", "Hollywood",
"House", "Ice Road", "Jersey", "Justice", "Love", "Mega", "Model",
"Modern", "Mountain", "Mystery", "Myth", "New York", "Paradise",
"Pioneer", "Queer", "Restaurant", "Road", "Royal", "Spouse",
"Star", "Storage", "Survival", "The Great American", "Tool",
"Treasure", "Wedding", "Wife"), class = "factor"), Name = structure(c(43L,
47L, 29L, 43L, 47L, 29L, 43L, 47L, 29L, 43L, 47L, 29L), .Label =
c("Aliens",
"Behavior", "Casino", "Casting Call", "Challenge", "Contest",
"Crashers", "Crew", "Dad", "Dancing", "Date", "Disasters", "Dynasty",
"Family", "Garage", "Greenlight", "Gypsies", "Haul", "Hot Rod",
"Inventor", "Jail", "Job", "Justice", "Marvels", "Master", "Mates",
"Model", "Moms", "Nation", "Ninja", "Patrol", "People", "Pitmasters",
"Queens", "Rescue", "Rivals", "Room", "Rooms", "Rules", "Star",
"Stars", "Superhero", "Videos", "VIP", "Wars", "Wishes", "Wives",
"Wrangler"), class = "factor"), Department = structure(c(8L,
6L, 2L, 8L, 6L, 2L, 8L, 6L, 2L, 8L, 6L, 2L), .Label = c("HXW",
"QLH", "RAR", "RYC", "SYI", "TAS", "VUV", "YME"), class = "factor"),
    DCE = c(0.57, 0.59, 0.54, NA, NA, 0.28, 0.57, 0.59, 0.54,
    NA, NA, 0.28), DP = c(0.56, 0.56, 0.58, NA, NA, 0.29, 0.56,
    0.56, 0.58, NA, NA, 0.29), date = structure(c(15978, 15978,
    15978, 16067, 16067, 16067, 16159, 16159, 16159, 16249, 16249,
    16249), class = "Date")), description = "", row.names = c(5L,
18L, 34L, 53L, 66L, 82L, 102L, 115L, 131L, 150L, 163L, 179L), class =
"data.frame", .Names = c("first.Name",
"Name", "Department", "DCE", "DP", "date"))

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Fri Nov 18 09:30:11 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 18 Nov 2016 00:30:11 -0800
Subject: [R] aggregate dataframe by multiple factors
In-Reply-To: <CALJKBv9S5=3rYffCbW098eVm8_9XtdE4-FrdWOHTH9AcGWJu4g@mail.gmail.com>
References: <CALJKBv9S5=3rYffCbW098eVm8_9XtdE4-FrdWOHTH9AcGWJu4g@mail.gmail.com>
Message-ID: <89CF0820-5338-4150-A300-78D517CC019B@comcast.net>


> On Nov 17, 2016, at 11:27 PM, Karim Mezhoud <kmezhoud at gmail.com> wrote:
> 
> Dear all,
> 
> the dat  has missing values NA,
> 
>    first.Name   Name Department  DCE   DP       date
> 5      Auction Videos        YME 0.57 0.56 2013-09-30
> 18       Amish  Wives        TAS 0.59 0.56 2013-09-30
> 34     Ancient Nation        QLH 0.54 0.58 2013-09-30
> 53     Auction Videos        YME   NA   NA 2013-12-28
> 66       Amish  Wives        TAS   NA   NA 2013-12-28
> 82     Ancient Nation        QLH 0.28 0.29 2013-12-28
> 102    Auction Videos        YME 0.57 0.56 2014-03-30
> 115      Amish  Wives        TAS 0.59 0.56 2014-03-30
> 131    Ancient Nation        QLH 0.54 0.58 2014-03-30
> 150    Auction Videos        YME   NA   NA 2014-06-28
> 163      Amish  Wives        TAS   NA   NA 2014-06-28
> 179    Ancient Nation        QLH 0.28 0.29 2014-06-28
> 
> 
> agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> list(dat$first.Name, dat$Name, dat$Department) , "sort"))

The closest I could get on a few attempts was:

(agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
list(dat$first.Name, dat$Name, dat$Department) , function(d) { unlist(d)}))
 )

  Group.1 Group.2 Group.3 DCE.1 DCE.2 DCE.3 DCE.4 DP.1 DP.2 DP.3 DP.4
1 Ancient  Nation     QLH  0.54  0.28  0.54  0.28 0.58 0.29 0.58 0.29
2   Amish   Wives     TAS  0.59    NA  0.59    NA 0.56   NA 0.56   NA
3 Auction  Videos     YME  0.57    NA  0.57    NA 0.56   NA 0.56   NA

I think the sort operation might be somewhat ambiguous in this instance. I tried:

 (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
list(dat$first.Name, dat$Name, dat$Department) , function(d) { unlist(lapply(d,sort))}))
 )

With no success, not even a sorted result.

-- 
David.
> 
> 
> agg has list of value. I would separate value in different columns.
> 
>  Group.1 Group.2 Group.3                    DCE                     DP
> 1 Ancient  Nation     QLH 0.28, 0.28, 0.54, 0.54 0.29, 0.29, 0.58, 0.58
> 2   Amish   Wives     TAS             0.59, 0.59             0.56, 0.56
> 3 Auction  Videos     YME             0.57, 0.57             0.56, 0.56
> 
> The  goal:
> 
> Group.1 Group.2 Group.3  DCE.1 DCE.2 DCE.3  DCE.4  DP.1  DP.2  DP.3  DP.4
> 1 Ancient  Nation     QLH    0.28     0.28    0.54     0.54     0.29, 0.29,
> 0.58, 0.58
> 2   Amish   Wives     TAS        NA     NA     0.59, 0.59           NA
> NA  0.56, 0.56
> 3 Auction  Videos     YME         NA   NA      0.57, 0.57             NA
> NA  0.56, 0.56
> 
> 
> 
> dat <- structure(list(first.Name = structure(c(3L, 1L, 2L, 3L, 1L, 2L,
> 3L, 1L, 2L, 3L, 1L, 2L), .Label = c("Amish", "Ancient", "Auction",
> "Ax", "Bachelorette", "Basketball", "BBQ", "Cake", "Celebrity",
> "Chef", "Clean", "Colonial", "Comedy", "Comic", "Crocodile",
> "Dog", "Empire", "Extreme", "Farm", "Half Pint", "Hollywood",
> "House", "Ice Road", "Jersey", "Justice", "Love", "Mega", "Model",
> "Modern", "Mountain", "Mystery", "Myth", "New York", "Paradise",
> "Pioneer", "Queer", "Restaurant", "Road", "Royal", "Spouse",
> "Star", "Storage", "Survival", "The Great American", "Tool",
> "Treasure", "Wedding", "Wife"), class = "factor"), Name = structure(c(43L,
> 47L, 29L, 43L, 47L, 29L, 43L, 47L, 29L, 43L, 47L, 29L), .Label =
> c("Aliens",
> "Behavior", "Casino", "Casting Call", "Challenge", "Contest",
> "Crashers", "Crew", "Dad", "Dancing", "Date", "Disasters", "Dynasty",
> "Family", "Garage", "Greenlight", "Gypsies", "Haul", "Hot Rod",
> "Inventor", "Jail", "Job", "Justice", "Marvels", "Master", "Mates",
> "Model", "Moms", "Nation", "Ninja", "Patrol", "People", "Pitmasters",
> "Queens", "Rescue", "Rivals", "Room", "Rooms", "Rules", "Star",
> "Stars", "Superhero", "Videos", "VIP", "Wars", "Wishes", "Wives",
> "Wrangler"), class = "factor"), Department = structure(c(8L,
> 6L, 2L, 8L, 6L, 2L, 8L, 6L, 2L, 8L, 6L, 2L), .Label = c("HXW",
> "QLH", "RAR", "RYC", "SYI", "TAS", "VUV", "YME"), class = "factor"),
>    DCE = c(0.57, 0.59, 0.54, NA, NA, 0.28, 0.57, 0.59, 0.54,
>    NA, NA, 0.28), DP = c(0.56, 0.56, 0.58, NA, NA, 0.29, 0.56,
>    0.56, 0.58, NA, NA, 0.29), date = structure(c(15978, 15978,
>    15978, 16067, 16067, 16067, 16159, 16159, 16159, 16249, 16249,
>    16249), class = "Date")), description = "", row.names = c(5L,
> 18L, 34L, 53L, 66L, 82L, 102L, 115L, 131L, 150L, 163L, 179L), class =
> "data.frame", .Names = c("first.Name",
> "Name", "Department", "DCE", "DP", "date"))
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From cbenjami at BTBOCES.ORG  Fri Nov 18 11:06:20 2016
From: cbenjami at BTBOCES.ORG (Courtney Benjamin)
Date: Fri, 18 Nov 2016 10:06:20 +0000
Subject: [R] Archer-Lemeshow Goodness of Fit Test for Survey Data with
 Log. Regression
In-Reply-To: <CAOwvMDwmzFKdkULYRF=dhQuXHS1s=SAeyGecO5t34QxT16EBNQ@mail.gmail.com>
References: <1479352496057.0@BTBOCES.ORG>,
	<CAOwvMDwmzFKdkULYRF=dhQuXHS1s=SAeyGecO5t34QxT16EBNQ@mail.gmail.com>
Message-ID: <1479463575470.51513@BTBOCES.ORG>

?Thank you, Anthony.  Your approach does work; however, I am concerned to some degree about subsetting the data prior to creating the new svrepdesign as I know that it is not recommended to subset the data prior to creating the svrepdesign object.  I am not sure if this is a significant concern in this context as the model was fitted with the original svrepdesign that was created prior to subsetting any data and the new svrepdesign is being used to run the diagnostic for the model.  Any thoughts on that issue?

Also, from my understanding of the outcome of the diagnostic, low p values indicate a poor model fit.

Sincerely,

Courtney


Courtney Benjamin

Broome-Tioga BOCES

Automotive Technology II Teacher

Located at Gault Toyota

Doctoral Candidate-Educational Theory & Practice

State University of New York at Binghamton

cbenjami at btboces.org<mailto:cbenjami at btboces.org>

607-763-8633

________________________________
From: Anthony Damico <ajdamico at gmail.com>
Sent: Thursday, November 17, 2016 4:28 AM
To: Courtney Benjamin
Cc: r-help at r-project.org
Subject: Re: [R] Archer-Lemeshow Goodness of Fit Test for Survey Data with Log. Regression

great minimal reproducible example, thanks.  does something like this work?



#Log. Reg. model-all curric. concentrations including F1RTRCC as a predictor
allCC <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+F1RGPP2+F1HIMATH+F1RTRCC,family="binomial",design=elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1,na.action=na.exclude)
summary(allCC)

r <- residuals(allCC, type="response")
f<-fitted(allCC)
your_g<- cut(f, c(-Inf, quantile(f,  (1:9)/10, Inf)))

elsq1ch[ elsq1ch$BYSCTRL==1&elsq1ch$G10COHRT==1 , 'your_g' ] <- your_g
elsq1ch[ elsq1ch$BYSCTRL==1&elsq1ch$G10COHRT==1 , 'r' ] <- r
newdesign<-svrepdesign(variables = elsq1ch, repweights = elsq1ch[,18:217], weights = elsq1ch[,17], combined.weights = TRUE, type = "BRR")

decilemodel<- svyglm(r~your_g, design=newdesign,subset=BYSCTRL==1&G10COHRT==1)
regTermTest(decilemodel, ~your_g)




On Wed, Nov 16, 2016 at 10:15 PM, Courtney Benjamin <cbenjami at btboces.org<mailto:cbenjami at btboces.org>> wrote:
?Hello R Experts,

I am trying to implement the Archer-Lemeshow GOF Test for survey data on a logistic regression model using the survey package based upon an R Help Archive post that I found where Dr. Thomas Lumley advised how to do it: http://r.789695.n4.nabble.com/Goodness-of-t-tests-for-Complex-Survey-Logistic-Regression-td4668233.html

Everything is going well until I get to the point where I have to add the objects 'r' and 'g' as variables to the data frame by either using the transform function or the update function to update the svrepdesign object.  The log. regression model involved uses a subset of data and some of the values in the data frame are NA, so that is affecting my ability to add 'r' and 'g' as variables; I am getting an error because I only have 8397 rows for the new variables and 16197 in the data frame and svrepdesign object.  I am not sure how to overcome this error.

The following is a MRE:

##Archer Lemeshow Goodness of Fit Test for Complex Survey Data with Logistic Regression

library(RCurl)
library(survey)

data <- getURL("https://raw.githubusercontent.com/cbenjamin1821/careertech-ed/master/elsq1adj.csv")
elsq1ch <- read.csv(text = data)

#Specifying the svyrepdesign object which applies the BRR weights
elsq1ch_brr<-svrepdesign(variables = elsq1ch[,1:16], repweights = elsq1ch[,18:217], weights = elsq1ch[,17], combined.weights = TRUE, type = "BRR")
elsq1ch_brr

##Resetting baseline levels for predictors
elsq1ch_brr <- update( elsq1ch_brr , F1HIMATH = relevel(F1HIMATH,"PreAlg or Less") )
elsq1ch_brr <- update( elsq1ch_brr , BYINCOME = relevel(BYINCOME,"0-25K") )
elsq1ch_brr <- update( elsq1ch_brr , F1RACE = relevel(F1RACE,"White") )
elsq1ch_brr <- update( elsq1ch_brr , F1SEX = relevel(F1SEX,"Male") )
elsq1ch_brr <- update( elsq1ch_brr , F1RTRCC = relevel(F1RTRCC,"Academic") )

#Log. Reg. model-all curric. concentrations including F1RTRCC as a predictor
allCC <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+F1RGPP2+F1HIMATH+F1RTRCC,family="binomial",design=elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1,na.action=na.omit)
summary(allCC)

#Recommendations from Lumley (from R Help Archive) on implementing the Archer Lemeshow GOF test
r <- residuals(allCC, type="response")
f<-fitted(allCC)
g<- cut(f, c(-Inf, quantile(f,  (1:9)/10, Inf)))

# now create a new design object with r and g added as variables
#This is the area where I am having problems as my model involves a subset and some values are NA as well
#I am also not sure if I am naming/specifying the new variables of r and g properly
transform(elsq1ch,r=r,g=g)
elsq1ch_brr <- update(elsq1ch_brr,tag=g,tag=r)
#then:
decilemodel<- svyglm(r~g, design=newdesign)
regTermTest(decilemodel, ~g)
#is the F-adjusted mean residual test from the Archer Lemeshow paper

Thank you,
Courtney

?

Courtney Benjamin

Broome-Tioga BOCES

Automotive Technology II Teacher

Located at Gault Toyota

Doctoral Candidate-Educational Theory & Practice

State University of New York at Binghamton

cbenjami at btboces.org<mailto:cbenjami at btboces.org><mailto:cbenjami at btboces.org<mailto:cbenjami at btboces.org>>

607-763-8633<tel:607-763-8633>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From ajdamico at gmail.com  Fri Nov 18 11:15:23 2016
From: ajdamico at gmail.com (Anthony Damico)
Date: Fri, 18 Nov 2016 05:15:23 -0500
Subject: [R] Archer-Lemeshow Goodness of Fit Test for Survey Data with
 Log. Regression
In-Reply-To: <1479463575470.51513@BTBOCES.ORG>
References: <1479352496057.0@BTBOCES.ORG>
	<CAOwvMDwmzFKdkULYRF=dhQuXHS1s=SAeyGecO5t34QxT16EBNQ@mail.gmail.com>
	<1479463575470.51513@BTBOCES.ORG>
Message-ID: <CAOwvMDyK3GqKBAyqWv+R2+hrFjaO3GB5dw_TJQ-dLnweysTRAw@mail.gmail.com>

hi, my code does not subset the survey design on the line that creates the
svrepdesign().  subsetting in order to create a variable while your data is
still a data.frame is probably okay, so long as you expect the observations
outside of the subset to be NAs like they are in this case.
nrow(elsq1ch_brr)==nrow(newdesign)

On Fri, Nov 18, 2016 at 5:06 AM, Courtney Benjamin <cbenjami at btboces.org>
wrote:

> ?Thank you, Anthony.  Your approach does work; however, I am concerned to
> some degree about subsetting the data prior to creating the new svrepdesign
> as I know that it is not recommended to subset the data prior to creating
> the svrepdesign object.  I am not sure if this is a significant concern in
> this context as the model was fitted with the original svrepdesign that was
> created prior to subsetting any data and the new svrepdesign is being used
> to run the diagnostic for the model.  Any thoughts on that issue?
>
> Also, from my understanding of the outcome of the diagnostic, low p values
> indicate a poor model fit.
>
> Sincerely,
>
> Courtney
>
>
> Courtney Benjamin
>
> Broome-Tioga BOCES
>
> Automotive Technology II Teacher
>
> Located at Gault Toyota
>
> Doctoral Candidate-Educational Theory & Practice
>
> State University of New York at Binghamton
>
> cbenjami at btboces.org
>
> 607-763-8633
> ------------------------------
> *From:* Anthony Damico <ajdamico at gmail.com>
> *Sent:* Thursday, November 17, 2016 4:28 AM
> *To:* Courtney Benjamin
> *Cc:* r-help at r-project.org
> *Subject:* Re: [R] Archer-Lemeshow Goodness of Fit Test for Survey Data
> with Log. Regression
>
> great minimal reproducible example, thanks.  does something like this work?
>
>
>
> #Log. Reg. model-all curric. concentrations including F1RTRCC as a
> predictor
> allCC <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+
> F1RGPP2+F1HIMATH+F1RTRCC,family="binomial",design=
> elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1,na.action=na.exclude)
> summary(allCC)
>
> r <- residuals(allCC, type="response")
> f<-fitted(allCC)
> your_g<- cut(f, c(-Inf, quantile(f,  (1:9)/10, Inf)))
>
> elsq1ch[ elsq1ch$BYSCTRL==1&elsq1ch$G10COHRT==1 , 'your_g' ] <- your_g
> elsq1ch[ elsq1ch$BYSCTRL==1&elsq1ch$G10COHRT==1 , 'r' ] <- r
> newdesign<-svrepdesign(variables = elsq1ch, repweights =
> elsq1ch[,18:217], weights = elsq1ch[,17], combined.weights = TRUE, type =
> "BRR")
>
> decilemodel<- svyglm(r~your_g, design=newdesign,subset=
> BYSCTRL==1&G10COHRT==1)
> regTermTest(decilemodel, ~your_g)
>
>
>
>
> On Wed, Nov 16, 2016 at 10:15 PM, Courtney Benjamin <cbenjami at btboces.org>
> wrote:
>
>> ?Hello R Experts,
>>
>> I am trying to implement the Archer-Lemeshow GOF Test for survey data on
>> a logistic regression model using the survey package based upon an R Help
>> Archive post that I found where Dr. Thomas Lumley advised how to do it:
>> http://r.789695.n4.nabble.com/Goodness-of-t-tests-for-Comple
>> x-Survey-Logistic-Regression-td4668233.html
>>
>> Everything is going well until I get to the point where I have to add the
>> objects 'r' and 'g' as variables to the data frame by either using the
>> transform function or the update function to update the svrepdesign
>> object.  The log. regression model involved uses a subset of data and some
>> of the values in the data frame are NA, so that is affecting my ability to
>> add 'r' and 'g' as variables; I am getting an error because I only have
>> 8397 rows for the new variables and 16197 in the data frame and svrepdesign
>> object.  I am not sure how to overcome this error.
>>
>> The following is a MRE:
>>
>> ##Archer Lemeshow Goodness of Fit Test for Complex Survey Data with
>> Logistic Regression
>>
>> library(RCurl)
>> library(survey)
>>
>> data <- getURL("https://raw.githubusercontent.com/cbenjamin1821/
>> careertech-ed/master/elsq1adj.csv")
>> elsq1ch <- read.csv(text = data)
>>
>> #Specifying the svyrepdesign object which applies the BRR weights
>> elsq1ch_brr<-svrepdesign(variables = elsq1ch[,1:16], repweights =
>> elsq1ch[,18:217], weights = elsq1ch[,17], combined.weights = TRUE, type =
>> "BRR")
>> elsq1ch_brr
>>
>> ##Resetting baseline levels for predictors
>> elsq1ch_brr <- update( elsq1ch_brr , F1HIMATH = relevel(F1HIMATH,"PreAlg
>> or Less") )
>> elsq1ch_brr <- update( elsq1ch_brr , BYINCOME = relevel(BYINCOME,"0-25K")
>> )
>> elsq1ch_brr <- update( elsq1ch_brr , F1RACE = relevel(F1RACE,"White") )
>> elsq1ch_brr <- update( elsq1ch_brr , F1SEX = relevel(F1SEX,"Male") )
>> elsq1ch_brr <- update( elsq1ch_brr , F1RTRCC =
>> relevel(F1RTRCC,"Academic") )
>>
>> #Log. Reg. model-all curric. concentrations including F1RTRCC as a
>> predictor
>> allCC <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+F1RGP
>> P2+F1HIMATH+F1RTRCC,family="binomial",design=elsq1ch_brr,
>> subset=BYSCTRL==1&G10COHRT==1,na.action=na.omit)
>> summary(allCC)
>>
>> #Recommendations from Lumley (from R Help Archive) on implementing the
>> Archer Lemeshow GOF test
>> r <- residuals(allCC, type="response")
>> f<-fitted(allCC)
>> g<- cut(f, c(-Inf, quantile(f,  (1:9)/10, Inf)))
>>
>> # now create a new design object with r and g added as variables
>> #This is the area where I am having problems as my model involves a
>> subset and some values are NA as well
>> #I am also not sure if I am naming/specifying the new variables of r and
>> g properly
>> transform(elsq1ch,r=r,g=g)
>> elsq1ch_brr <- update(elsq1ch_brr,tag=g,tag=r)
>> #then:
>> decilemodel<- svyglm(r~g, design=newdesign)
>> regTermTest(decilemodel, ~g)
>> #is the F-adjusted mean residual test from the Archer Lemeshow paper
>>
>> Thank you,
>> Courtney
>>
>> ?
>>
>> Courtney Benjamin
>>
>> Broome-Tioga BOCES
>>
>> Automotive Technology II Teacher
>>
>> Located at Gault Toyota
>>
>> Doctoral Candidate-Educational Theory & Practice
>>
>> State University of New York at Binghamton
>>
>> cbenjami at btboces.org<mailto:cbenjami at btboces.org>
>>
>> 607-763-8633
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From cbenjami at BTBOCES.ORG  Fri Nov 18 11:22:47 2016
From: cbenjami at BTBOCES.ORG (Courtney Benjamin)
Date: Fri, 18 Nov 2016 10:22:47 +0000
Subject: [R] Archer-Lemeshow Goodness of Fit Test for Survey Data with
 Log. Regression
In-Reply-To: <CAOwvMDyK3GqKBAyqWv+R2+hrFjaO3GB5dw_TJQ-dLnweysTRAw@mail.gmail.com>
References: <1479352496057.0@BTBOCES.ORG>
	<CAOwvMDwmzFKdkULYRF=dhQuXHS1s=SAeyGecO5t34QxT16EBNQ@mail.gmail.com>
	<1479463575470.51513@BTBOCES.ORG>,
	<CAOwvMDyK3GqKBAyqWv+R2+hrFjaO3GB5dw_TJQ-dLnweysTRAw@mail.gmail.com>
Message-ID: <1479464562556.11593@BTBOCES.ORG>

?Thank you; I appreciate your advisement.


Courtney Benjamin

Broome-Tioga BOCES

Automotive Technology II Teacher

Located at Gault Toyota

Doctoral Candidate-Educational Theory & Practice

State University of New York at Binghamton

cbenjami at btboces.org<mailto:cbenjami at btboces.org>

607-763-8633

________________________________
From: Anthony Damico <ajdamico at gmail.com>
Sent: Friday, November 18, 2016 5:15 AM
To: Courtney Benjamin
Cc: r-help at r-project.org
Subject: Re: [R] Archer-Lemeshow Goodness of Fit Test for Survey Data with Log. Regression

hi, my code does not subset the survey design on the line that creates the svrepdesign().  subsetting in order to create a variable while your data is still a data.frame is probably okay, so long as you expect the observations outside of the subset to be NAs like they are in this case.  nrow(elsq1ch_brr)==nrow(newdesign)

On Fri, Nov 18, 2016 at 5:06 AM, Courtney Benjamin <cbenjami at btboces.org<mailto:cbenjami at btboces.org>> wrote:

?Thank you, Anthony.  Your approach does work; however, I am concerned to some degree about subsetting the data prior to creating the new svrepdesign as I know that it is not recommended to subset the data prior to creating the svrepdesign object.  I am not sure if this is a significant concern in this context as the model was fitted with the original svrepdesign that was created prior to subsetting any data and the new svrepdesign is being used to run the diagnostic for the model.  Any thoughts on that issue?

Also, from my understanding of the outcome of the diagnostic, low p values indicate a poor model fit.

Sincerely,

Courtney


Courtney Benjamin

Broome-Tioga BOCES

Automotive Technology II Teacher

Located at Gault Toyota

Doctoral Candidate-Educational Theory & Practice

State University of New York at Binghamton

cbenjami at btboces.org<mailto:cbenjami at btboces.org>

607-763-8633<tel:607-763-8633>

________________________________
From: Anthony Damico <ajdamico at gmail.com<mailto:ajdamico at gmail.com>>
Sent: Thursday, November 17, 2016 4:28 AM
To: Courtney Benjamin
Cc: r-help at r-project.org<mailto:r-help at r-project.org>
Subject: Re: [R] Archer-Lemeshow Goodness of Fit Test for Survey Data with Log. Regression

great minimal reproducible example, thanks.  does something like this work?



#Log. Reg. model-all curric. concentrations including F1RTRCC as a predictor
allCC <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+F1RGPP2+F1HIMATH+F1RTRCC,family="binomial",design=elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1,na.action=na.exclude)
summary(allCC)

r <- residuals(allCC, type="response")
f<-fitted(allCC)
your_g<- cut(f, c(-Inf, quantile(f,  (1:9)/10, Inf)))

elsq1ch[ elsq1ch$BYSCTRL==1&elsq1ch$G10COHRT==1 , 'your_g' ] <- your_g
elsq1ch[ elsq1ch$BYSCTRL==1&elsq1ch$G10COHRT==1 , 'r' ] <- r
newdesign<-svrepdesign(variables = elsq1ch, repweights = elsq1ch[,18:217], weights = elsq1ch[,17], combined.weights = TRUE, type = "BRR")

decilemodel<- svyglm(r~your_g, design=newdesign,subset=BYSCTRL==1&G10COHRT==1)
regTermTest(decilemodel, ~your_g)




On Wed, Nov 16, 2016 at 10:15 PM, Courtney Benjamin <cbenjami at btboces.org<mailto:cbenjami at btboces.org>> wrote:
?Hello R Experts,

I am trying to implement the Archer-Lemeshow GOF Test for survey data on a logistic regression model using the survey package based upon an R Help Archive post that I found where Dr. Thomas Lumley advised how to do it: http://r.789695.n4.nabble.com/Goodness-of-t-tests-for-Complex-Survey-Logistic-Regression-td4668233.html

Everything is going well until I get to the point where I have to add the objects 'r' and 'g' as variables to the data frame by either using the transform function or the update function to update the svrepdesign object.  The log. regression model involved uses a subset of data and some of the values in the data frame are NA, so that is affecting my ability to add 'r' and 'g' as variables; I am getting an error because I only have 8397 rows for the new variables and 16197 in the data frame and svrepdesign object.  I am not sure how to overcome this error.

The following is a MRE:

##Archer Lemeshow Goodness of Fit Test for Complex Survey Data with Logistic Regression

library(RCurl)
library(survey)

data <- getURL("https://raw.githubusercontent.com/cbenjamin1821/careertech-ed/master/elsq1adj.csv")
elsq1ch <- read.csv(text = data)

#Specifying the svyrepdesign object which applies the BRR weights
elsq1ch_brr<-svrepdesign(variables = elsq1ch[,1:16], repweights = elsq1ch[,18:217], weights = elsq1ch[,17], combined.weights = TRUE, type = "BRR")
elsq1ch_brr

##Resetting baseline levels for predictors
elsq1ch_brr <- update( elsq1ch_brr , F1HIMATH = relevel(F1HIMATH,"PreAlg or Less") )
elsq1ch_brr <- update( elsq1ch_brr , BYINCOME = relevel(BYINCOME,"0-25K") )
elsq1ch_brr <- update( elsq1ch_brr , F1RACE = relevel(F1RACE,"White") )
elsq1ch_brr <- update( elsq1ch_brr , F1SEX = relevel(F1SEX,"Male") )
elsq1ch_brr <- update( elsq1ch_brr , F1RTRCC = relevel(F1RTRCC,"Academic") )

#Log. Reg. model-all curric. concentrations including F1RTRCC as a predictor
allCC <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+F1RGPP2+F1HIMATH+F1RTRCC,family="binomial",design=elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1,na.action=na.omit)
summary(allCC)

#Recommendations from Lumley (from R Help Archive) on implementing the Archer Lemeshow GOF test
r <- residuals(allCC, type="response")
f<-fitted(allCC)
g<- cut(f, c(-Inf, quantile(f,  (1:9)/10, Inf)))

# now create a new design object with r and g added as variables
#This is the area where I am having problems as my model involves a subset and some values are NA as well
#I am also not sure if I am naming/specifying the new variables of r and g properly
transform(elsq1ch,r=r,g=g)
elsq1ch_brr <- update(elsq1ch_brr,tag=g,tag=r)
#then:
decilemodel<- svyglm(r~g, design=newdesign)
regTermTest(decilemodel, ~g)
#is the F-adjusted mean residual test from the Archer Lemeshow paper

Thank you,
Courtney

?

Courtney Benjamin

Broome-Tioga BOCES

Automotive Technology II Teacher

Located at Gault Toyota

Doctoral Candidate-Educational Theory & Practice

State University of New York at Binghamton

cbenjami at btboces.org<mailto:cbenjami at btboces.org><mailto:cbenjami at btboces.org<mailto:cbenjami at btboces.org>>

607-763-8633<tel:607-763-8633>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Fri Nov 18 11:34:21 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 18 Nov 2016 10:34:21 +0000
Subject: [R] aggregate dataframe by multiple factors
In-Reply-To: <89CF0820-5338-4150-A300-78D517CC019B@comcast.net>
References: <CALJKBv9S5=3rYffCbW098eVm8_9XtdE4-FrdWOHTH9AcGWJu4g@mail.gmail.com>
	<89CF0820-5338-4150-A300-78D517CC019B@comcast.net>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C504684B@SRVEXCHMBX.precheza.cz>

Hi

same result can be achieved by

dat.ag<-aggregate(dat[ , c("DCE","DP")], by= list(dat$first.Name, dat$Name, dat$Department) , "I")

Sorting according to the first row seems to be quite tricky. You could probably get closer by using some combination of split and order and arranging back chunks  of data

ooo1<-order(split(dat$DCE,interaction(dat$first.Name, dat$Name, dat$Department, drop=T))[[1]])
data.frame(sapply(split(dat$DCE,interaction(dat$first.Name, dat$Name, dat$Department, drop=T)), rbind))[ooo1,]
  Ancient.Nation.QLH Amish.Wives.TAS Auction.Videos.YME
2               0.28              NA                 NA
4               0.28              NA                 NA
1               0.54            0.59               0.57
3               0.54            0.59               0.57

however I wonder why the order according to the first row is necessary if all NAs are on correct positions?

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of David
> Winsemius
> Sent: Friday, November 18, 2016 9:30 AM
> To: Karim Mezhoud <kmezhoud at gmail.com>
> Cc: r-help at r-project.org
> Subject: Re: [R] aggregate dataframe by multiple factors
>
>
> > On Nov 17, 2016, at 11:27 PM, Karim Mezhoud <kmezhoud at gmail.com>
> wrote:
> >
> > Dear all,
> >
> > the dat  has missing values NA,
> >
> >    first.Name   Name Department  DCE   DP       date
> > 5      Auction Videos        YME 0.57 0.56 2013-09-30
> > 18       Amish  Wives        TAS 0.59 0.56 2013-09-30
> > 34     Ancient Nation        QLH 0.54 0.58 2013-09-30
> > 53     Auction Videos        YME   NA   NA 2013-12-28
> > 66       Amish  Wives        TAS   NA   NA 2013-12-28
> > 82     Ancient Nation        QLH 0.28 0.29 2013-12-28
> > 102    Auction Videos        YME 0.57 0.56 2014-03-30
> > 115      Amish  Wives        TAS 0.59 0.56 2014-03-30
> > 131    Ancient Nation        QLH 0.54 0.58 2014-03-30
> > 150    Auction Videos        YME   NA   NA 2014-06-28
> > 163      Amish  Wives        TAS   NA   NA 2014-06-28
> > 179    Ancient Nation        QLH 0.28 0.29 2014-06-28
> >
> >
> > agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> > list(dat$first.Name, dat$Name, dat$Department) , "sort"))
>
> The closest I could get on a few attempts was:
>
> (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> list(dat$first.Name, dat$Name, dat$Department) , function(d) { unlist(d)}))
>  )
>
>   Group.1 Group.2 Group.3 DCE.1 DCE.2 DCE.3 DCE.4 DP.1 DP.2 DP.3 DP.4
> 1 Ancient  Nation     QLH  0.54  0.28  0.54  0.28 0.58 0.29 0.58 0.29
> 2   Amish   Wives     TAS  0.59    NA  0.59    NA 0.56   NA 0.56   NA
> 3 Auction  Videos     YME  0.57    NA  0.57    NA 0.56   NA 0.56   NA
>
> I think the sort operation might be somewhat ambiguous in this instance. I
> tried:
>
>  (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> list(dat$first.Name, dat$Name, dat$Department) , function(d) {
> unlist(lapply(d,sort))}))
>  )
>
> With no success, not even a sorted result.
>
> --
> David.
> >
> >
> > agg has list of value. I would separate value in different columns.
> >
> >  Group.1 Group.2 Group.3                    DCE                     DP
> > 1 Ancient  Nation     QLH 0.28, 0.28, 0.54, 0.54 0.29, 0.29, 0.58, 0.58
> > 2   Amish   Wives     TAS             0.59, 0.59             0.56, 0.56
> > 3 Auction  Videos     YME             0.57, 0.57             0.56, 0.56
> >
> > The  goal:
> >
> > Group.1 Group.2 Group.3  DCE.1 DCE.2 DCE.3  DCE.4  DP.1  DP.2  DP.3  DP.4
> > 1 Ancient  Nation     QLH    0.28     0.28    0.54     0.54     0.29, 0.29,
> > 0.58, 0.58
> > 2   Amish   Wives     TAS        NA     NA     0.59, 0.59           NA
> > NA  0.56, 0.56
> > 3 Auction  Videos     YME         NA   NA      0.57, 0.57             NA
> > NA  0.56, 0.56
> >
> >
> >
> > dat <- structure(list(first.Name = structure(c(3L, 1L, 2L, 3L, 1L, 2L,
> > 3L, 1L, 2L, 3L, 1L, 2L), .Label = c("Amish", "Ancient", "Auction",
> > "Ax", "Bachelorette", "Basketball", "BBQ", "Cake", "Celebrity",
> > "Chef", "Clean", "Colonial", "Comedy", "Comic", "Crocodile", "Dog",
> > "Empire", "Extreme", "Farm", "Half Pint", "Hollywood", "House", "Ice
> > Road", "Jersey", "Justice", "Love", "Mega", "Model", "Modern",
> > "Mountain", "Mystery", "Myth", "New York", "Paradise", "Pioneer",
> > "Queer", "Restaurant", "Road", "Royal", "Spouse", "Star", "Storage",
> > "Survival", "The Great American", "Tool", "Treasure", "Wedding",
> > "Wife"), class = "factor"), Name = structure(c(43L, 47L, 29L, 43L,
> > 47L, 29L, 43L, 47L, 29L, 43L, 47L, 29L), .Label = c("Aliens",
> > "Behavior", "Casino", "Casting Call", "Challenge", "Contest",
> > "Crashers", "Crew", "Dad", "Dancing", "Date", "Disasters", "Dynasty",
> > "Family", "Garage", "Greenlight", "Gypsies", "Haul", "Hot Rod",
> > "Inventor", "Jail", "Job", "Justice", "Marvels", "Master", "Mates",
> > "Model", "Moms", "Nation", "Ninja", "Patrol", "People", "Pitmasters",
> > "Queens", "Rescue", "Rivals", "Room", "Rooms", "Rules", "Star",
> > "Stars", "Superhero", "Videos", "VIP", "Wars", "Wishes", "Wives",
> > "Wrangler"), class = "factor"), Department = structure(c(8L, 6L, 2L,
> > 8L, 6L, 2L, 8L, 6L, 2L, 8L, 6L, 2L), .Label = c("HXW", "QLH", "RAR",
> > "RYC", "SYI", "TAS", "VUV", "YME"), class = "factor"),
> >    DCE = c(0.57, 0.59, 0.54, NA, NA, 0.28, 0.57, 0.59, 0.54,
> >    NA, NA, 0.28), DP = c(0.56, 0.56, 0.58, NA, NA, 0.29, 0.56,
> >    0.56, 0.58, NA, NA, 0.29), date = structure(c(15978, 15978,
> >    15978, 16067, 16067, 16067, 16159, 16159, 16159, 16249, 16249,
> >    16249), class = "Date")), description = "", row.names = c(5L, 18L,
> > 34L, 53L, 66L, 82L, 102L, 115L, 131L, 150L, 163L, 179L), class =
> > "data.frame", .Names = c("first.Name", "Name", "Department", "DCE",
> > "DP", "date"))
> >
> >     [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From cbenjami at BTBOCES.ORG  Fri Nov 18 12:17:10 2016
From: cbenjami at BTBOCES.ORG (Courtney Benjamin)
Date: Fri, 18 Nov 2016 11:17:10 +0000
Subject: [R] Archer-Lemeshow Goodness of Fit Test for Survey Data with
 Log. Regression
In-Reply-To: <CAOwvMDyK3GqKBAyqWv+R2+hrFjaO3GB5dw_TJQ-dLnweysTRAw@mail.gmail.com>
References: <1479352496057.0@BTBOCES.ORG>
	<CAOwvMDwmzFKdkULYRF=dhQuXHS1s=SAeyGecO5t34QxT16EBNQ@mail.gmail.com>
	<1479463575470.51513@BTBOCES.ORG>,
	<CAOwvMDyK3GqKBAyqWv+R2+hrFjaO3GB5dw_TJQ-dLnweysTRAw@mail.gmail.com>
Message-ID: <1479467824841.35141@BTBOCES.ORG>

Like I had said, I was concerned with the low p value outcome of this diagnostic test, so I went on to test other interactions that I had not previously tested.  I did find an interaction that is significant.  Now when I go to run the diagnostic to check for any improvement, I am coming up with an error.

##Testing with interaction model
allCCI12 <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+F1RGPP2+F1HIMATH+F1RTRCC+F1HIMATH*F1RGPP2,family="binomial",design=elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1,na.action=na.exclude)
summary(allCCI12)

r <- residuals(allCCI12, type="response")
f<-fitted(allCCI12)
your_g<- cut(f, c(-Inf, quantile(f,  (1:9)/10, Inf)))

elsq1ch[ elsq1ch$BYSCTRL==1&elsq1ch$G10COHRT==1 , 'your_g' ] <- your_g
elsq1ch[ elsq1ch$BYSCTRL==1&elsq1ch$G10COHRT==1 , 'r' ] <- r
newdesign<-svrepdesign(variables = elsq1ch, repweights = elsq1ch[,18:217], weights = elsq1ch[,17], combined.weights = TRUE, type = "BRR")

decilemodel<- svyglm(r~your_g, design=newdesign,subset=BYSCTRL==1&G10COHRT==1)
regTermTest(decilemodel, ~your_g)

?

?


Courtney Benjamin

Broome-Tioga BOCES

Automotive Technology II Teacher

Located at Gault Toyota

Doctoral Candidate-Educational Theory & Practice

State University of New York at Binghamton

cbenjami at btboces.org<mailto:cbenjami at btboces.org>

607-763-8633

________________________________
From: Anthony Damico <ajdamico at gmail.com>
Sent: Friday, November 18, 2016 5:15 AM
To: Courtney Benjamin
Cc: r-help at r-project.org
Subject: Re: [R] Archer-Lemeshow Goodness of Fit Test for Survey Data with Log. Regression

hi, my code does not subset the survey design on the line that creates the svrepdesign().  subsetting in order to create a variable while your data is still a data.frame is probably okay, so long as you expect the observations outside of the subset to be NAs like they are in this case.  nrow(elsq1ch_brr)==nrow(newdesign)

On Fri, Nov 18, 2016 at 5:06 AM, Courtney Benjamin <cbenjami at btboces.org<mailto:cbenjami at btboces.org>> wrote:

?Thank you, Anthony.  Your approach does work; however, I am concerned to some degree about subsetting the data prior to creating the new svrepdesign as I know that it is not recommended to subset the data prior to creating the svrepdesign object.  I am not sure if this is a significant concern in this context as the model was fitted with the original svrepdesign that was created prior to subsetting any data and the new svrepdesign is being used to run the diagnostic for the model.  Any thoughts on that issue?

Also, from my understanding of the outcome of the diagnostic, low p values indicate a poor model fit.

Sincerely,

Courtney


Courtney Benjamin

Broome-Tioga BOCES

Automotive Technology II Teacher

Located at Gault Toyota

Doctoral Candidate-Educational Theory & Practice

State University of New York at Binghamton

cbenjami at btboces.org<mailto:cbenjami at btboces.org>

607-763-8633<tel:607-763-8633>

________________________________
From: Anthony Damico <ajdamico at gmail.com<mailto:ajdamico at gmail.com>>
Sent: Thursday, November 17, 2016 4:28 AM
To: Courtney Benjamin
Cc: r-help at r-project.org<mailto:r-help at r-project.org>
Subject: Re: [R] Archer-Lemeshow Goodness of Fit Test for Survey Data with Log. Regression

great minimal reproducible example, thanks.  does something like this work?



#Log. Reg. model-all curric. concentrations including F1RTRCC as a predictor
allCC <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+F1RGPP2+F1HIMATH+F1RTRCC,family="binomial",design=elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1,na.action=na.exclude)
summary(allCC)

r <- residuals(allCC, type="response")
f<-fitted(allCC)
your_g<- cut(f, c(-Inf, quantile(f,  (1:9)/10, Inf)))

elsq1ch[ elsq1ch$BYSCTRL==1&elsq1ch$G10COHRT==1 , 'your_g' ] <- your_g
elsq1ch[ elsq1ch$BYSCTRL==1&elsq1ch$G10COHRT==1 , 'r' ] <- r
newdesign<-svrepdesign(variables = elsq1ch, repweights = elsq1ch[,18:217], weights = elsq1ch[,17], combined.weights = TRUE, type = "BRR")

decilemodel<- svyglm(r~your_g, design=newdesign,subset=BYSCTRL==1&G10COHRT==1)
regTermTest(decilemodel, ~your_g)




On Wed, Nov 16, 2016 at 10:15 PM, Courtney Benjamin <cbenjami at btboces.org<mailto:cbenjami at btboces.org>> wrote:
?Hello R Experts,

I am trying to implement the Archer-Lemeshow GOF Test for survey data on a logistic regression model using the survey package based upon an R Help Archive post that I found where Dr. Thomas Lumley advised how to do it: http://r.789695.n4.nabble.com/Goodness-of-t-tests-for-Complex-Survey-Logistic-Regression-td4668233.html

Everything is going well until I get to the point where I have to add the objects 'r' and 'g' as variables to the data frame by either using the transform function or the update function to update the svrepdesign object.  The log. regression model involved uses a subset of data and some of the values in the data frame are NA, so that is affecting my ability to add 'r' and 'g' as variables; I am getting an error because I only have 8397 rows for the new variables and 16197 in the data frame and svrepdesign object.  I am not sure how to overcome this error.

The following is a MRE:

##Archer Lemeshow Goodness of Fit Test for Complex Survey Data with Logistic Regression

library(RCurl)
library(survey)

data <- getURL("https://raw.githubusercontent.com/cbenjamin1821/careertech-ed/master/elsq1adj.csv")
elsq1ch <- read.csv(text = data)

#Specifying the svyrepdesign object which applies the BRR weights
elsq1ch_brr<-svrepdesign(variables = elsq1ch[,1:16], repweights = elsq1ch[,18:217], weights = elsq1ch[,17], combined.weights = TRUE, type = "BRR")
elsq1ch_brr

##Resetting baseline levels for predictors
elsq1ch_brr <- update( elsq1ch_brr , F1HIMATH = relevel(F1HIMATH,"PreAlg or Less") )
elsq1ch_brr <- update( elsq1ch_brr , BYINCOME = relevel(BYINCOME,"0-25K") )
elsq1ch_brr <- update( elsq1ch_brr , F1RACE = relevel(F1RACE,"White") )
elsq1ch_brr <- update( elsq1ch_brr , F1SEX = relevel(F1SEX,"Male") )
elsq1ch_brr <- update( elsq1ch_brr , F1RTRCC = relevel(F1RTRCC,"Academic") )

#Log. Reg. model-all curric. concentrations including F1RTRCC as a predictor
allCC <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+F1RGPP2+F1HIMATH+F1RTRCC,family="binomial",design=elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1,na.action=na.omit)
summary(allCC)

#Recommendations from Lumley (from R Help Archive) on implementing the Archer Lemeshow GOF test
r <- residuals(allCC, type="response")
f<-fitted(allCC)
g<- cut(f, c(-Inf, quantile(f,  (1:9)/10, Inf)))

# now create a new design object with r and g added as variables
#This is the area where I am having problems as my model involves a subset and some values are NA as well
#I am also not sure if I am naming/specifying the new variables of r and g properly
transform(elsq1ch,r=r,g=g)
elsq1ch_brr <- update(elsq1ch_brr,tag=g,tag=r)
#then:
decilemodel<- svyglm(r~g, design=newdesign)
regTermTest(decilemodel, ~g)
#is the F-adjusted mean residual test from the Archer Lemeshow paper

Thank you,
Courtney

?

Courtney Benjamin

Broome-Tioga BOCES

Automotive Technology II Teacher

Located at Gault Toyota

Doctoral Candidate-Educational Theory & Practice

State University of New York at Binghamton

cbenjami at btboces.org<mailto:cbenjami at btboces.org><mailto:cbenjami at btboces.org<mailto:cbenjami at btboces.org>>

607-763-8633<tel:607-763-8633>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



	[[alternative HTML version deleted]]


From milujisb at gmail.com  Fri Nov 18 13:49:05 2016
From: milujisb at gmail.com (Miluji Sb)
Date: Fri, 18 Nov 2016 13:49:05 +0100
Subject: [R] Melt and compute Max, Mean, Min
Message-ID: <CAMLwc7PNUzOtLFv+yemQZEXsf535nsuzMbf0ShZZ_i3BqsRrHw@mail.gmail.com>

Dear all,

I have 51 years of data (1960 - 2010) in csv format, where each file
represents one year of data. Below is what each file looks like.

These are temperature data by coordinates, my goal is to to compute max,
min, and mean by year for each of the coordinates and construct a panel
dataset. Any help will be appreciated, thank you!

Sincerely,

Milu

temp <- dput(head(df,5))
structure(list(ISO3 = structure(c(28L, 28L, 28L, NA, 28L), .Label =
c("AFG",
"AGO", "ALB", "ARE", "ARG", "ARM", "AUS", "AUT", "AZE", "BDI",
"BEL", "BEN", "BFA", "BGD", "BGR", "BHS", "BIH", "BLR", "BLZ",
"BOL", "BRA", "BRN", "BTN", "BWA", "CAF", "CAN", "CHE", "CHL",
"CHN", "CIV", "CMR", "COD", "COG", "COL", "CRI", "CUB", "CYP",
"CZE", "DEU", "DJI", "DNK", "DOM", "DZA", "ECU", "EGY", "ERI",
"ESH", "ESP", "EST", "ETH", "FIN", "FJI", "FLK", "FRA", "GAB",
"GBR", "GEO", "GHA", "GIN", "GNB", "GNQ", "GRC", "GRL", "GTM",
"GUF", "GUY", "HND", "HRV", "HTI", "HUN", "IDN", "IND", "IRL",
"IRN", "IRQ", "ISL", "ISR", "ITA", "JAM", "JOR", "JPN", "KAZ",
"KEN", "KGZ", "KHM", "KIR", "KOR", "KWT", "LAO", "LBN", "LBR",
"LBY", "LCA", "LKA", "LSO", "LTU", "LUX", "LVA", "MAR", "MDA",
"MDG", "MEX", "MKD", "MLI", "MMR", "MNE", "MNG", "MOZ", "MRT",
"MWI", "MYS", "NAM", "NCL", "NER", "NGA", "NIC", "NLD", "NOR",
"NPL", "NZL", "OMN", "PAK", "PAN", "PER", "PHL", "PNG", "POL",
"PRI", "PRK", "PRT", "PRY", "QAT", "ROU", "RUS", "RWA", "SAU",
"SDN", "SEN", "SJM", "SLB", "SLE", "SLV", "SOM", "SRB", "SUR",
"SVK", "SVN", "SWE", "SWZ", "SYR", "TCD", "TGO", "THA", "TJK",
"TKM", "TLS", "TUN", "TUR", "TWN", "TZA", "UGA", "UKR", "URY",
"USA", "UZB", "VEN", "VNM", "VUT", "YEM", "ZAF", "ZMB", "ZWE"
), class = "factor"), lon = c(-69L, -68L, -72L, -71L, -70L),
    lat = c(-55L, -55L, -54L, -54L, -54L), day_1 = c(NA, NA,
    0, 0, 0), day_2 = c(NA, NA, 0, 0, 0), day_3 = c(NA, NA, 0,
    23.37984, 0), day_4 = c(NA, NA, 0, 0, 0), day_5 = c(NA, NA,
    0, 0, 0), day_6 = c(NA, NA, 0, 0, 0), day_7 = c(NA, NA, 2.83824,
    11.80116, 1.24956), day_8 = c(NA, NA, 0, 1.68588, 14.69448
    ), day_9 = c(NA, NA, 0, 0, 1.09296), day_10 = c(NA, NA, 0,
    0, 0), day_11 = c(NA, NA, 3.78, 3.7422, 0), day_12 = c(NA,
    NA, 0.54, 0, 0), day_13 = c(NA, NA, 0, 0, 0), day_14 = c(NA,
    NA, 0, 0, 0.39204), day_15 = c(NA, NA, 0, 0, 11.58732), day_16 = c(NA,
    NA, 0, 0, 0), day_17 = c(NA, NA, 0, 1.14048, 12.26448), day_18 = c(NA,
    NA, 0, 1.1934, 7.59024), day_19 = c(NA, NA, 9.74268, 0, 0
    ), day_20 = c(NA, NA, 0, 0, 0), day_21 = c(NA, NA, 1.96776,
    0, 0), day_22 = c(NA, NA, 0, 0, 0), day_23 = c(NA, NA, 0,
    0, 0), day_24 = c(NA, NA, 6.21756, 2.74752, 0), day_25 = c(NA,
    NA, 0, 0, 3.37932), day_26 = c(NA, NA, 4.8384, 0, 0), day_27 = c(NA,
    NA, 0, 0, 0), day_28 = c(NA, NA, 0, 0, 0), day_29 = c(NA,
    NA, 22.37328, 0, 0), day_30 = c(NA, NA, 28.97424, 11.25468,
    0), day_31 = c(NA, NA, 0, 0, 0), day_32 = c(NA, NA, 0, 0,
    2.00448), day_33 = c(NA, NA, 0, 0, 0), day_34 = c(NA, NA,
    0, 0, 0), day_35 = c(NA, NA, 0, 0, 0), day_36 = c(NA, NA,
    0, 0, 0), day_37 = c(NA, NA, 0, 0, 0), day_38 = c(NA, NA,
    32.7132, 31.71852, 0), day_39 = c(NA, NA, 0, 0, 5.84604),
    day_40 = c(NA, NA, 0, 0, 0), day_41 = c(NA, NA, 0, 0, 0),
    day_42 = c(NA, NA, 0, 0, 0), day_43 = c(NA, NA, 0, 0, 0),
    day_44 = c(NA, NA, 0, 0, 1.78416), day_45 = c(NA, NA, 0,
    0, 0), day_46 = c(NA, NA, 33.84504, 0, 0), day_47 = c(NA,
    NA, 0, 0, 0), day_48 = c(NA, NA, 0, 0, 0), day_49 = c(NA,
    NA, 0, 0, 0), day_50 = c(NA, NA, 0, 0.4752, 0), day_51 = c(NA,
    NA, 0, 0, 22.02012), day_52 = c(NA, NA, 0, 0, 0), day_53 = c(NA,
    NA, 0, 0, 3.48084), day_54 = c(NA, NA, 0, 0, 0), day_55 = c(NA,
    NA, 0.58212, 0, 0), day_56 = c(NA, NA, 0.35316, 0, 0), day_57 = c(NA,
    NA, 0, 0, 12.65436), day_58 = c(NA, NA, 0, 0, 0), day_59 = c(NA,
    NA, 0, 0, 0), day_60 = c(NA, NA, 3.03372, 22.05576, 0), day_61 = c(NA,
    NA, 2.5758, 0, 0), day_62 = c(NA, NA, 0, 0, 0), day_63 = c(NA,
    NA, 3.67416, 25.22016, 4.21524), day_64 = c(NA, NA, 0.52488,
    3.60288, 0), day_65 = c(NA, NA, 12.82608, 0, 0), day_66 = c(NA,
    NA, 0, 0, 0), day_67 = c(NA, NA, 0, 0, 0), day_68 = c(NA,
    NA, 0, 0, 0), day_69 = c(NA, NA, 0, 0, 0), day_70 = c(NA,
    NA, 1.11564, 5.17536, 0), day_71 = c(NA, NA, 1.18584, 0,
    0), day_72 = c(NA, NA, 0, 0, 0.10584), day_73 = c(NA, NA,
    0.62748, 14.39748, 7.50708), day_74 = c(NA, NA, 7.20252,
    20.02644, 1.07244), day_75 = c(NA, NA, 1.87488, 0, 0), day_76 = c(NA,
    NA, 0.26784, 0, 0), day_77 = c(NA, NA, 0, 0, 0), day_78 = c(NA,
    NA, 0, 0, 2.81664), day_79 = c(NA, NA, 0, 0, 0), day_80 = c(NA,
    NA, 0, 0, 0), day_81 = c(NA, NA, 0, 0, 0), day_82 = c(NA,
    NA, 1.29276, 0, 0), day_83 = c(NA, NA, 0.18468, 0, 1.46124
    ), day_84 = c(NA, NA, 0, 0, 0), day_85 = c(NA, NA, 0, 0,
    0), day_86 = c(NA, NA, 57.36528, 0, 0), day_87 = c(NA, NA,
    8.19504, 0, 0), day_88 = c(NA, NA, 0, 0, 0), day_89 = c(NA,
    NA, 6.45732, 0, 0), day_90 = c(NA, NA, 0, 0, 0), day_91 = c(NA,
    NA, 0, 0, 44.20332), day_92 = c(NA, NA, 0, 0, 6.31476), day_93 = c(NA,
    NA, 0, 0, 0.35748), day_94 = c(NA, NA, 16.74972, 30.35988,
    5.0436), day_95 = c(NA, NA, 4.93992, 1.46556, 19.86768),
    day_96 = c(NA, NA, 0, 0, 0.88128), day_97 = c(NA, NA, 5.751,
    19.02096, 0), day_98 = c(NA, NA, 11.5452, 13.37148, 0), day_99 = c(NA,
    NA, 0, 0, 0), day_100 = c(NA, NA, 0, 0, 0), day_101 = c(NA,
    NA, 4.70124, 23.80644, 7.61832), day_102 = c(NA, NA, 0, 1.02492,
    0), day_103 = c(NA, NA, 0, 0, 15.86304), day_104 = c(NA,
    NA, 0, 0, 0.26352), day_105 = c(NA, NA, 0, 0, 21.60864),
    day_106 = c(NA, NA, 56.93436, 0, 0.22464), day_107 = c(NA,
    NA, 8.13348, 0, 0), day_108 = c(NA, NA, 6.83748, 0, 0), day_109 = c(NA,
    NA, 0, 0, 0), day_110 = c(NA, NA, 14.36724, 0, 0), day_111 = c(NA,
    NA, 0.63936, 2.43864, 4.0554), day_112 = c(NA, NA, 1.21392,
    1.15452, 0), day_113 = c(NA, NA, 0.7722, 0, 0), day_114 = c(NA,
    NA, 0, 0, 1.08864), day_115 = c(NA, NA, 1.47528, 0, 0), day_116 = c(NA,
    NA, 0, 1.73124, 0), day_117 = c(NA, NA, 0, 0, 0), day_118 = c(NA,
    NA, 2.4516, 0, 0), day_119 = c(NA, NA, 0, 3.14388, 0), day_120 = c(NA,
    NA, 1.81872, 0, 0), day_121 = c(NA, NA, 2.77236, 0, 0), day_122 = c(NA,
    NA, 1.34028, 0.70632, 0), day_123 = c(NA, NA, 0, 0, 0), day_124 = c(NA,
    NA, 0, 0, 0), day_125 = c(NA, NA, 0.56484, 0.74412, 0), day_126 = c(NA,
    NA, 1.11888, 0.06264, 0), day_127 = c(NA, NA, 0, 0, 0), day_128 = c(NA,
    NA, 1.05624, 0, 0), day_129 = c(NA, NA, 26.63928, 34.04268,
    0), day_130 = c(NA, NA, 6.89796, 0, 0), day_131 = c(NA, NA,
    1.91592, 2.241, 0), day_132 = c(NA, NA, 0, 2.23668, 45.23904
    ), day_133 = c(NA, NA, 0, 0, 6.46272), day_134 = c(NA, NA,
    0, 0, 0), day_135 = c(NA, NA, 0, 0, 0), day_136 = c(NA, NA,
    0, 0, 0), day_137 = c(NA, NA, 0, 0, 0), day_138 = c(NA, NA,
    0, 0, 0), day_139 = c(NA, NA, 0, 0, 0), day_140 = c(NA, NA,
    0, 0, 0), day_141 = c(NA, NA, 0, 0, 0), day_142 = c(NA, NA,
    0, 0, 0), day_143 = c(NA, NA, 0, 0, 0), day_144 = c(NA, NA,
    2.943, 5.17536, 0), day_145 = c(NA, NA, 0, 0, 0), day_146 = c(NA,
    NA, 0, 0, 0), day_147 = c(NA, NA, 0, 0, 0), day_148 = c(NA,
    NA, 10.96308, 2.98188, 0), day_149 = c(NA, NA, 20.4822, 0.43632,
    0), day_150 = c(NA, NA, 1.5282, 0, 0), day_151 = c(NA, NA,
    0, 0, 0), day_152 = c(NA, NA, 0, 0, 0), day_153 = c(NA, NA,
    0, 0, 0), day_154 = c(NA, NA, 0, 0, 0), day_155 = c(NA, NA,
    0, 0, 0), day_156 = c(NA, NA, 0, 0, 0), day_157 = c(NA, NA,
    0, 0, 0), day_158 = c(NA, NA, 0, 0, 0), day_159 = c(NA, NA,
    0, 0, 0), day_160 = c(NA, NA, 0, 0, 0), day_161 = c(NA, NA,
    0, 0, 0), day_162 = c(NA, NA, 0, 0, 0), day_163 = c(NA, NA,
    0, 26.3412, 4.07376), day_164 = c(NA, NA, 0, 4.28328, 3.03156
    ), day_165 = c(NA, NA, 0, 0, 0), day_166 = c(NA, NA, 0, 0,
    4.60404), day_167 = c(NA, NA, 0, 0, 0.70848), day_168 = c(NA,
    NA, 0, 0, 0), day_169 = c(NA, NA, 0, 0, 0), day_170 = c(NA,
    NA, 0, 0, 0), day_171 = c(NA, NA, 0, 0, 0), day_172 = c(NA,
    NA, 0, 0, 0), day_173 = c(NA, NA, 0, 0, 3.7854), day_174 = c(NA,
    NA, 0, 0, 0), day_175 = c(NA, NA, 0, 0, 0), day_176 = c(NA,
    NA, 0, 0, 0), day_177 = c(NA, NA, 0, 0, 0), day_178 = c(NA,
    NA, 0, 0, 0), day_179 = c(NA, NA, 0, 0, 0), day_180 = c(NA,
    NA, 0, 0, 0), day_181 = c(NA, NA, 0, 0, 0), day_182 = c(NA,
    NA, 0, 0, 0), day_183 = c(NA, NA, 0, 0, 0), day_184 = c(NA,
    NA, 0, 0, 0), day_185 = c(NA, NA, 0, 0, 0), day_186 = c(NA,
    NA, 0, 0, 0), day_187 = c(NA, NA, 7.30728, 4.1202, 0), day_188 = c(NA,
    NA, 2.56608, 0.5886, 0), day_189 = c(NA, NA, 0, 0, 0), day_190 = c(NA,
    NA, 21.93156, 8.0082, 11.4318), day_191 = c(NA, NA, 3.13308,
    0, 0), day_192 = c(NA, NA, 0, 0, 0.10692), day_193 = c(NA,
    NA, 0, 0, 4.65912), day_194 = c(NA, NA, 0, 0, 0), day_195 = c(NA,
    NA, 0, 0, 0), day_196 = c(NA, NA, 0, 0, 0), day_197 = c(NA,
    NA, 0, 0, 0), day_198 = c(NA, NA, 0, 0, 0), day_199 = c(NA,
    NA, 0, 0, 0), day_200 = c(NA, NA, 0, 0, 0), day_201 = c(NA,
    NA, 0, 0, 0), day_202 = c(NA, NA, 0, 7.77276, 4.6602), day_203 = c(NA,
    NA, 0, 0.86292, 0), day_204 = c(NA, NA, 0, 0, 0), day_205 = c(NA,
    NA, 21.45528, 8.69616, 0), day_206 = c(NA, NA, 0, 0, 0),
    day_207 = c(NA, NA, 0, 0, 0), day_208 = c(NA, NA, 0, 0, 0
    ), day_209 = c(NA, NA, 0, 0, 0), day_210 = c(NA, NA, 0, 0,
    0), day_211 = c(NA, NA, 0, 0, 0), day_212 = c(NA, NA, 0,
    0, 0), day_213 = c(NA, NA, 0, 0, 0), day_214 = c(NA, NA,
    0, 0, 0), day_215 = c(NA, NA, 0, 0, 0), day_216 = c(NA, NA,
    0, 0, 0), day_217 = c(NA, NA, 0, 0, 0), day_218 = c(NA, NA,
    0, 0, 0), day_219 = c(NA, NA, 0, 0, 0), day_220 = c(NA, NA,
    6.10092, 10.85508, 13.22244), day_221 = c(NA, NA, 0.87156,
    0, 0), day_222 = c(NA, NA, 0, 0, 15.46452), day_223 = c(NA,
    NA, 0, 0, 9.83664), day_224 = c(NA, NA, 0, 0, 0), day_225 = c(NA,
    NA, 0, 0, 0), day_226 = c(NA, NA, 16.46028, 0, 0), day_227 = c(NA,
    NA, 0, 0, 0), day_228 = c(NA, NA, 0, 0, 0), day_229 = c(NA,
    NA, 0, 0, 0), day_230 = c(NA, NA, 0, 0, 0), day_231 = c(NA,
    NA, 2.7108, 0, 0), day_232 = c(NA, NA, 0, 0, 0), day_233 = c(NA,
    NA, 0, 0, 0), day_234 = c(NA, NA, 0, 0, 0), day_235 = c(NA,
    NA, 0, 0, 0), day_236 = c(NA, NA, 0, 0, 3.5586), day_237 = c(NA,
    NA, 0, 0, 0), day_238 = c(NA, NA, 0, 0, 0), day_239 = c(NA,
    NA, 10.23192, 0, 0), day_240 = c(NA, NA, 0, 0, 0), day_241 = c(NA,
    NA, 0, 0, 0), day_242 = c(NA, NA, 0, 0, 0), day_243 = c(NA,
    NA, 0, 0, 0), day_244 = c(NA, NA, 0, 0, 0), day_245 = c(NA,
    NA, 0, 0, 0), day_246 = c(NA, NA, 0, 0, 0), day_247 = c(NA,
    NA, 0, 0, 0), day_248 = c(NA, NA, 0, 0, 0), day_249 = c(NA,
    NA, 0.50544, 0, 0), day_250 = c(NA, NA, 0.12636, 0, 0), day_251 = c(NA,
    NA, 7.02432, 0, 5.39784), day_252 = c(NA, NA, 3.33828, 8.00064,
    7.08372), day_253 = c(NA, NA, 0, 0, 0), day_254 = c(NA, NA,
    0, 0, 0), day_255 = c(NA, NA, 2.5704, 4.71636, 11.99772),
    day_256 = c(NA, NA, 0.3672, 0.75384, 0), day_257 = c(NA,
    NA, 0, 0, 0), day_258 = c(NA, NA, 0.50328, 0, 0), day_259 = c(NA,
    NA, 6.78888, 0, 0), day_260 = c(NA, NA, 0.96984, 0, 0), day_261 = c(NA,
    NA, 4.62672, 0, 0), day_262 = c(NA, NA, 0, 0, 0), day_263 = c(NA,
    NA, 3.16224, 0.27864, 0), day_264 = c(NA, NA, 0, 1.31112,
    0), day_265 = c(NA, NA, 0.37692, 0, 0), day_266 = c(NA, NA,
    0, 0, 0), day_267 = c(NA, NA, 0.70524, 0.43524, 0), day_268 = c(NA,
    NA, 0.18792, 0.12744, 0), day_269 = c(NA, NA, 0, 1.79064,
    0.96012), day_270 = c(NA, NA, 0, 0, 0.58644), day_271 = c(NA,
    NA, 4.4982, 0, 0), day_272 = c(NA, NA, 0, 0, 0), day_273 = c(NA,
    NA, 2.04552, 6.56964, 0), day_274 = c(NA, NA, 0.71712, 0.93852,
    0), day_275 = c(NA, NA, 0, 0, 0), day_276 = c(NA, NA, 0,
    0, 0), day_277 = c(NA, NA, 3.31452, 0, 0), day_278 = c(NA,
    NA, 1.20204, 0, 0), day_279 = c(NA, NA, 0, 0, 0), day_280 = c(NA,
    NA, 0, 0, 0), day_281 = c(NA, NA, 0, 0, 0), day_282 = c(NA,
    NA, 17.955, 5.7942, 9.93816), day_283 = c(NA, NA, 4.79304,
    4.8006, 0), day_284 = c(NA, NA, 3.9366, 0.78084, 0), day_285 = c(NA,
    NA, 0, 0, 0), day_286 = c(NA, NA, 0, 0, 0), day_287 = c(NA,
    NA, 0, 0, 0), day_288 = c(NA, NA, 0, 0, 0), day_289 = c(NA,
    NA, 0, 0, 0), day_290 = c(NA, NA, 0, 0, 0), day_291 = c(NA,
    NA, 0, 0, 0), day_292 = c(NA, NA, 0, 0, 0), day_293 = c(NA,
    NA, 1.55736, 0, 0), day_294 = c(NA, NA, 4.28328, 0, 0), day_295 = c(NA,
    NA, 0, 0, 0), day_296 = c(NA, NA, 0, 0, 0), day_297 = c(NA,
    NA, 1.6362, 0, 0), day_298 = c(NA, NA, 1.28844, 0, 6.14088
    ), day_299 = c(NA, NA, 0, 0, 0.50112), day_300 = c(NA, NA,
    0, 0, 0), day_301 = c(NA, NA, 0, 0.13824, 0.03456), day_302 = c(NA,
    NA, 0, 2.92572, 9.24264), day_303 = c(NA, NA, 2.8188, 0.41796,
    0), day_304 = c(NA, NA, 2.04876, 11.28384, 0), day_305 = c(NA,
    NA, 0, 0.3564, 0), day_306 = c(NA, NA, 0, 0, 0), day_307 = c(NA,
    NA, 0, 2.36736, 0), day_308 = c(NA, NA, 0, 0, 0), day_309 = c(NA,
    NA, 34.91856, 20.42604, 0), day_310 = c(NA, NA, 0, 0, 0),
    day_311 = c(NA, NA, 0, 0, 0), day_312 = c(NA, NA, 0, 0.40392,
    0), day_313 = c(NA, NA, 0, 0.5292, 0), day_314 = c(NA, NA,
    0, 0, 5.21424), day_315 = c(NA, NA, 0, 0, 0), day_316 = c(NA,
    NA, 0, 0, 0.4266), day_317 = c(NA, NA, 0, 0, 0), day_318 = c(NA,
    NA, 0, 0, 0), day_319 = c(NA, NA, 0, 0, 0), day_320 = c(NA,
    NA, 0.23436, 0.6048, 14.9256), day_321 = c(NA, NA, 0, 0.10908,
    0), day_322 = c(NA, NA, 7.68096, 6.66036, 4.53924), day_323 = c(NA,
    NA, 1.09728, 1.59732, 8.51148), day_324 = c(NA, NA, 0, 0,
    0), day_325 = c(NA, NA, 1.46016, 0, 0), day_326 = c(NA, NA,
    0, 0, 8.70048), day_327 = c(NA, NA, 0, 0, 0), day_328 = c(NA,
    NA, 0, 0, 0), day_329 = c(NA, NA, 0, 0, 0), day_330 = c(NA,
    NA, 5.3082, 0, 0), day_331 = c(NA, NA, 2.5866, 0, 0), day_332 = c(NA,
    NA, 8.03628, 6.3666, 4.3308), day_333 = c(NA, NA, 0, 0, 0
    ), day_334 = c(NA, NA, 0, 0, 0), day_335 = c(NA, NA, 0, 0,
    0), day_336 = c(NA, NA, 0, 0, 5.29632), day_337 = c(NA, NA,
    0, 1.77444, 2.7216), day_338 = c(NA, NA, 0.40608, 0, 0.83052
    ), day_339 = c(NA, NA, 0, 0, 0), day_340 = c(NA, NA, 0, 0,
    0), day_341 = c(NA, NA, 0, 0, 0), day_342 = c(NA, NA, 0,
    0, 0), day_343 = c(NA, NA, 0, 0, 0), day_344 = c(NA, NA,
    7.73388, 0, 0), day_345 = c(NA, NA, 4.80384, 0, 0), day_346 = c(NA,
    NA, 4.374, 0.09288, 0), day_347 = c(NA, NA, 6.42924, 3.2022,
    0), day_348 = c(NA, NA, 0, 16.27668, 0), day_349 = c(NA,
    NA, 0, 0.90072, 9.36684), day_350 = c(NA, NA, 0.135, 1.87272,
    2.49048), day_351 = c(NA, NA, 0, 0, 0), day_352 = c(NA, NA,
    0, 0, 0), day_353 = c(NA, NA, 0, 0, 0), day_354 = c(NA, NA,
    0, 0, 0), day_355 = c(NA, NA, 0, 0, 0), day_356 = c(NA, NA,
    0, 0, 0), day_357 = c(NA, NA, 2.82636, 39.45348, 26.08848
    ), day_358 = c(NA, NA, 0, 0, 22.8582), day_359 = c(NA, NA,
    0, 0, 1.34028), day_360 = c(NA, NA, 30.03804, 0, 3.49704),
    day_361 = c(NA, NA, 0.13392, 4.941, 4.94424), day_362 = c(NA,
    NA, 0.92016, 0, 0.70632), day_363 = c(NA, NA, 0, 0, 0), day_364 = c(NA,
    NA, 0, 0, 0), day_365 = c(NA, NA, 0, 0, 0), day_366 = c(NA,
    NA, 21.42072, 0, 0)), .Names = c("ISO3", "lon", "lat", "day_1",
"day_2", "day_3", "day_4", "day_5", "day_6", "day_7", "day_8",
"day_9", "day_10", "day_11", "day_12", "day_13", "day_14", "day_15",
"day_16", "day_17", "day_18", "day_19", "day_20", "day_21", "day_22",
"day_23", "day_24", "day_25", "day_26", "day_27", "day_28", "day_29",
"day_30", "day_31", "day_32", "day_33", "day_34", "day_35", "day_36",
"day_37", "day_38", "day_39", "day_40", "day_41", "day_42", "day_43",
"day_44", "day_45", "day_46", "day_47", "day_48", "day_49", "day_50",
"day_51", "day_52", "day_53", "day_54", "day_55", "day_56", "day_57",
"day_58", "day_59", "day_60", "day_61", "day_62", "day_63", "day_64",
"day_65", "day_66", "day_67", "day_68", "day_69", "day_70", "day_71",
"day_72", "day_73", "day_74", "day_75", "day_76", "day_77", "day_78",
"day_79", "day_80", "day_81", "day_82", "day_83", "day_84", "day_85",
"day_86", "day_87", "day_88", "day_89", "day_90", "day_91", "day_92",
"day_93", "day_94", "day_95", "day_96", "day_97", "day_98", "day_99",
"day_100", "day_101", "day_102", "day_103", "day_104", "day_105",
"day_106", "day_107", "day_108", "day_109", "day_110", "day_111",
"day_112", "day_113", "day_114", "day_115", "day_116", "day_117",
"day_118", "day_119", "day_120", "day_121", "day_122", "day_123",
"day_124", "day_125", "day_126", "day_127", "day_128", "day_129",
"day_130", "day_131", "day_132", "day_133", "day_134", "day_135",
"day_136", "day_137", "day_138", "day_139", "day_140", "day_141",
"day_142", "day_143", "day_144", "day_145", "day_146", "day_147",
"day_148", "day_149", "day_150", "day_151", "day_152", "day_153",
"day_154", "day_155", "day_156", "day_157", "day_158", "day_159",
"day_160", "day_161", "day_162", "day_163", "day_164", "day_165",
"day_166", "day_167", "day_168", "day_169", "day_170", "day_171",
"day_172", "day_173", "day_174", "day_175", "day_176", "day_177",
"day_178", "day_179", "day_180", "day_181", "day_182", "day_183",
"day_184", "day_185", "day_186", "day_187", "day_188", "day_189",
"day_190", "day_191", "day_192", "day_193", "day_194", "day_195",
"day_196", "day_197", "day_198", "day_199", "day_200", "day_201",
"day_202", "day_203", "day_204", "day_205", "day_206", "day_207",
"day_208", "day_209", "day_210", "day_211", "day_212", "day_213",
"day_214", "day_215", "day_216", "day_217", "day_218", "day_219",
"day_220", "day_221", "day_222", "day_223", "day_224", "day_225",
"day_226", "day_227", "day_228", "day_229", "day_230", "day_231",
"day_232", "day_233", "day_234", "day_235", "day_236", "day_237",
"day_238", "day_239", "day_240", "day_241", "day_242", "day_243",
"day_244", "day_245", "day_246", "day_247", "day_248", "day_249",
"day_250", "day_251", "day_252", "day_253", "day_254", "day_255",
"day_256", "day_257", "day_258", "day_259", "day_260", "day_261",
"day_262", "day_263", "day_264", "day_265", "day_266", "day_267",
"day_268", "day_269", "day_270", "day_271", "day_272", "day_273",
"day_274", "day_275", "day_276", "day_277", "day_278", "day_279",
"day_280", "day_281", "day_282", "day_283", "day_284", "day_285",
"day_286", "day_287", "day_288", "day_289", "day_290", "day_291",
"day_292", "day_293", "day_294", "day_295", "day_296", "day_297",
"day_298", "day_299", "day_300", "day_301", "day_302", "day_303",
"day_304", "day_305", "day_306", "day_307", "day_308", "day_309",
"day_310", "day_311", "day_312", "day_313", "day_314", "day_315",
"day_316", "day_317", "day_318", "day_319", "day_320", "day_321",
"day_322", "day_323", "day_324", "day_325", "day_326", "day_327",
"day_328", "day_329", "day_330", "day_331", "day_332", "day_333",
"day_334", "day_335", "day_336", "day_337", "day_338", "day_339",
"day_340", "day_341", "day_342", "day_343", "day_344", "day_345",
"day_346", "day_347", "day_348", "day_349", "day_350", "day_351",
"day_352", "day_353", "day_354", "day_355", "day_356", "day_357",
"day_358", "day_359", "day_360", "day_361", "day_362", "day_363",
"day_364", "day_365", "day_366"), row.names = c(NA, 5L), class =
"data.frame")

	[[alternative HTML version deleted]]


From ssefick at gmail.com  Fri Nov 18 14:23:06 2016
From: ssefick at gmail.com (Stephen Sefick)
Date: Fri, 18 Nov 2016 07:23:06 -0600
Subject: [R] Melt and compute Max, Mean, Min
In-Reply-To: <CAMLwc7PNUzOtLFv+yemQZEXsf535nsuzMbf0ShZZ_i3BqsRrHw@mail.gmail.com>
References: <CAMLwc7PNUzOtLFv+yemQZEXsf535nsuzMbf0ShZZ_i3BqsRrHw@mail.gmail.com>
Message-ID: <cd0851f7-ee95-b42e-9cae-93f077c6e07c@gmail.com>

Milu,

I am unsure what you are trying to do. Please provide a minimal 
reproducible example, and someone will likely be able to help you with 
the R code. What have you tried so far?
kindest regards,

Stephen

On 11/18/2016 06:49 AM, Miluji Sb wrote:
> Dear all,
>
> I have 51 years of data (1960 - 2010) in csv format, where each file
> represents one year of data. Below is what each file looks like.
>
> These are temperature data by coordinates, my goal is to to compute max,
> min, and mean by year for each of the coordinates and construct a panel
> dataset. Any help will be appreciated, thank you!
>
> Sincerely,
>
> Milu
>
> temp <- dput(head(df,5))
> structure(list(ISO3 = structure(c(28L, 28L, 28L, NA, 28L), .Label =
> c("AFG",
> "AGO", "ALB", "ARE", "ARG", "ARM", "AUS", "AUT", "AZE", "BDI",
> "BEL", "BEN", "BFA", "BGD", "BGR", "BHS", "BIH", "BLR", "BLZ",
> "BOL", "BRA", "BRN", "BTN", "BWA", "CAF", "CAN", "CHE", "CHL",
> "CHN", "CIV", "CMR", "COD", "COG", "COL", "CRI", "CUB", "CYP",
> "CZE", "DEU", "DJI", "DNK", "DOM", "DZA", "ECU", "EGY", "ERI",
> "ESH", "ESP", "EST", "ETH", "FIN", "FJI", "FLK", "FRA", "GAB",
> "GBR", "GEO", "GHA", "GIN", "GNB", "GNQ", "GRC", "GRL", "GTM",
> "GUF", "GUY", "HND", "HRV", "HTI", "HUN", "IDN", "IND", "IRL",
> "IRN", "IRQ", "ISL", "ISR", "ITA", "JAM", "JOR", "JPN", "KAZ",
> "KEN", "KGZ", "KHM", "KIR", "KOR", "KWT", "LAO", "LBN", "LBR",
> "LBY", "LCA", "LKA", "LSO", "LTU", "LUX", "LVA", "MAR", "MDA",
> "MDG", "MEX", "MKD", "MLI", "MMR", "MNE", "MNG", "MOZ", "MRT",
> "MWI", "MYS", "NAM", "NCL", "NER", "NGA", "NIC", "NLD", "NOR",
> "NPL", "NZL", "OMN", "PAK", "PAN", "PER", "PHL", "PNG", "POL",
> "PRI", "PRK", "PRT", "PRY", "QAT", "ROU", "RUS", "RWA", "SAU",
> "SDN", "SEN", "SJM", "SLB", "SLE", "SLV", "SOM", "SRB", "SUR",
> "SVK", "SVN", "SWE", "SWZ", "SYR", "TCD", "TGO", "THA", "TJK",
> "TKM", "TLS", "TUN", "TUR", "TWN", "TZA", "UGA", "UKR", "URY",
> "USA", "UZB", "VEN", "VNM", "VUT", "YEM", "ZAF", "ZMB", "ZWE"
> ), class = "factor"), lon = c(-69L, -68L, -72L, -71L, -70L),
>      lat = c(-55L, -55L, -54L, -54L, -54L), day_1 = c(NA, NA,
>      0, 0, 0), day_2 = c(NA, NA, 0, 0, 0), day_3 = c(NA, NA, 0,
>      23.37984, 0), day_4 = c(NA, NA, 0, 0, 0), day_5 = c(NA, NA,
>      0, 0, 0), day_6 = c(NA, NA, 0, 0, 0), day_7 = c(NA, NA, 2.83824,
>      11.80116, 1.24956), day_8 = c(NA, NA, 0, 1.68588, 14.69448
>      ), day_9 = c(NA, NA, 0, 0, 1.09296), day_10 = c(NA, NA, 0,
>      0, 0), day_11 = c(NA, NA, 3.78, 3.7422, 0), day_12 = c(NA,
>      NA, 0.54, 0, 0), day_13 = c(NA, NA, 0, 0, 0), day_14 = c(NA,
>      NA, 0, 0, 0.39204), day_15 = c(NA, NA, 0, 0, 11.58732), day_16 = c(NA,
>      NA, 0, 0, 0), day_17 = c(NA, NA, 0, 1.14048, 12.26448), day_18 = c(NA,
>      NA, 0, 1.1934, 7.59024), day_19 = c(NA, NA, 9.74268, 0, 0
>      ), day_20 = c(NA, NA, 0, 0, 0), day_21 = c(NA, NA, 1.96776,
>      0, 0), day_22 = c(NA, NA, 0, 0, 0), day_23 = c(NA, NA, 0,
>      0, 0), day_24 = c(NA, NA, 6.21756, 2.74752, 0), day_25 = c(NA,
>      NA, 0, 0, 3.37932), day_26 = c(NA, NA, 4.8384, 0, 0), day_27 = c(NA,
>      NA, 0, 0, 0), day_28 = c(NA, NA, 0, 0, 0), day_29 = c(NA,
>      NA, 22.37328, 0, 0), day_30 = c(NA, NA, 28.97424, 11.25468,
>      0), day_31 = c(NA, NA, 0, 0, 0), day_32 = c(NA, NA, 0, 0,
>      2.00448), day_33 = c(NA, NA, 0, 0, 0), day_34 = c(NA, NA,
>      0, 0, 0), day_35 = c(NA, NA, 0, 0, 0), day_36 = c(NA, NA,
>      0, 0, 0), day_37 = c(NA, NA, 0, 0, 0), day_38 = c(NA, NA,
>      32.7132, 31.71852, 0), day_39 = c(NA, NA, 0, 0, 5.84604),
>      day_40 = c(NA, NA, 0, 0, 0), day_41 = c(NA, NA, 0, 0, 0),
>      day_42 = c(NA, NA, 0, 0, 0), day_43 = c(NA, NA, 0, 0, 0),
>      day_44 = c(NA, NA, 0, 0, 1.78416), day_45 = c(NA, NA, 0,
>      0, 0), day_46 = c(NA, NA, 33.84504, 0, 0), day_47 = c(NA,
>      NA, 0, 0, 0), day_48 = c(NA, NA, 0, 0, 0), day_49 = c(NA,
>      NA, 0, 0, 0), day_50 = c(NA, NA, 0, 0.4752, 0), day_51 = c(NA,
>      NA, 0, 0, 22.02012), day_52 = c(NA, NA, 0, 0, 0), day_53 = c(NA,
>      NA, 0, 0, 3.48084), day_54 = c(NA, NA, 0, 0, 0), day_55 = c(NA,
>      NA, 0.58212, 0, 0), day_56 = c(NA, NA, 0.35316, 0, 0), day_57 = c(NA,
>      NA, 0, 0, 12.65436), day_58 = c(NA, NA, 0, 0, 0), day_59 = c(NA,
>      NA, 0, 0, 0), day_60 = c(NA, NA, 3.03372, 22.05576, 0), day_61 = c(NA,
>      NA, 2.5758, 0, 0), day_62 = c(NA, NA, 0, 0, 0), day_63 = c(NA,
>      NA, 3.67416, 25.22016, 4.21524), day_64 = c(NA, NA, 0.52488,
>      3.60288, 0), day_65 = c(NA, NA, 12.82608, 0, 0), day_66 = c(NA,
>      NA, 0, 0, 0), day_67 = c(NA, NA, 0, 0, 0), day_68 = c(NA,
>      NA, 0, 0, 0), day_69 = c(NA, NA, 0, 0, 0), day_70 = c(NA,
>      NA, 1.11564, 5.17536, 0), day_71 = c(NA, NA, 1.18584, 0,
>      0), day_72 = c(NA, NA, 0, 0, 0.10584), day_73 = c(NA, NA,
>      0.62748, 14.39748, 7.50708), day_74 = c(NA, NA, 7.20252,
>      20.02644, 1.07244), day_75 = c(NA, NA, 1.87488, 0, 0), day_76 = c(NA,
>      NA, 0.26784, 0, 0), day_77 = c(NA, NA, 0, 0, 0), day_78 = c(NA,
>      NA, 0, 0, 2.81664), day_79 = c(NA, NA, 0, 0, 0), day_80 = c(NA,
>      NA, 0, 0, 0), day_81 = c(NA, NA, 0, 0, 0), day_82 = c(NA,
>      NA, 1.29276, 0, 0), day_83 = c(NA, NA, 0.18468, 0, 1.46124
>      ), day_84 = c(NA, NA, 0, 0, 0), day_85 = c(NA, NA, 0, 0,
>      0), day_86 = c(NA, NA, 57.36528, 0, 0), day_87 = c(NA, NA,
>      8.19504, 0, 0), day_88 = c(NA, NA, 0, 0, 0), day_89 = c(NA,
>      NA, 6.45732, 0, 0), day_90 = c(NA, NA, 0, 0, 0), day_91 = c(NA,
>      NA, 0, 0, 44.20332), day_92 = c(NA, NA, 0, 0, 6.31476), day_93 = c(NA,
>      NA, 0, 0, 0.35748), day_94 = c(NA, NA, 16.74972, 30.35988,
>      5.0436), day_95 = c(NA, NA, 4.93992, 1.46556, 19.86768),
>      day_96 = c(NA, NA, 0, 0, 0.88128), day_97 = c(NA, NA, 5.751,
>      19.02096, 0), day_98 = c(NA, NA, 11.5452, 13.37148, 0), day_99 = c(NA,
>      NA, 0, 0, 0), day_100 = c(NA, NA, 0, 0, 0), day_101 = c(NA,
>      NA, 4.70124, 23.80644, 7.61832), day_102 = c(NA, NA, 0, 1.02492,
>      0), day_103 = c(NA, NA, 0, 0, 15.86304), day_104 = c(NA,
>      NA, 0, 0, 0.26352), day_105 = c(NA, NA, 0, 0, 21.60864),
>      day_106 = c(NA, NA, 56.93436, 0, 0.22464), day_107 = c(NA,
>      NA, 8.13348, 0, 0), day_108 = c(NA, NA, 6.83748, 0, 0), day_109 = c(NA,
>      NA, 0, 0, 0), day_110 = c(NA, NA, 14.36724, 0, 0), day_111 = c(NA,
>      NA, 0.63936, 2.43864, 4.0554), day_112 = c(NA, NA, 1.21392,
>      1.15452, 0), day_113 = c(NA, NA, 0.7722, 0, 0), day_114 = c(NA,
>      NA, 0, 0, 1.08864), day_115 = c(NA, NA, 1.47528, 0, 0), day_116 = c(NA,
>      NA, 0, 1.73124, 0), day_117 = c(NA, NA, 0, 0, 0), day_118 = c(NA,
>      NA, 2.4516, 0, 0), day_119 = c(NA, NA, 0, 3.14388, 0), day_120 = c(NA,
>      NA, 1.81872, 0, 0), day_121 = c(NA, NA, 2.77236, 0, 0), day_122 = c(NA,
>      NA, 1.34028, 0.70632, 0), day_123 = c(NA, NA, 0, 0, 0), day_124 = c(NA,
>      NA, 0, 0, 0), day_125 = c(NA, NA, 0.56484, 0.74412, 0), day_126 = c(NA,
>      NA, 1.11888, 0.06264, 0), day_127 = c(NA, NA, 0, 0, 0), day_128 = c(NA,
>      NA, 1.05624, 0, 0), day_129 = c(NA, NA, 26.63928, 34.04268,
>      0), day_130 = c(NA, NA, 6.89796, 0, 0), day_131 = c(NA, NA,
>      1.91592, 2.241, 0), day_132 = c(NA, NA, 0, 2.23668, 45.23904
>      ), day_133 = c(NA, NA, 0, 0, 6.46272), day_134 = c(NA, NA,
>      0, 0, 0), day_135 = c(NA, NA, 0, 0, 0), day_136 = c(NA, NA,
>      0, 0, 0), day_137 = c(NA, NA, 0, 0, 0), day_138 = c(NA, NA,
>      0, 0, 0), day_139 = c(NA, NA, 0, 0, 0), day_140 = c(NA, NA,
>      0, 0, 0), day_141 = c(NA, NA, 0, 0, 0), day_142 = c(NA, NA,
>      0, 0, 0), day_143 = c(NA, NA, 0, 0, 0), day_144 = c(NA, NA,
>      2.943, 5.17536, 0), day_145 = c(NA, NA, 0, 0, 0), day_146 = c(NA,
>      NA, 0, 0, 0), day_147 = c(NA, NA, 0, 0, 0), day_148 = c(NA,
>      NA, 10.96308, 2.98188, 0), day_149 = c(NA, NA, 20.4822, 0.43632,
>      0), day_150 = c(NA, NA, 1.5282, 0, 0), day_151 = c(NA, NA,
>      0, 0, 0), day_152 = c(NA, NA, 0, 0, 0), day_153 = c(NA, NA,
>      0, 0, 0), day_154 = c(NA, NA, 0, 0, 0), day_155 = c(NA, NA,
>      0, 0, 0), day_156 = c(NA, NA, 0, 0, 0), day_157 = c(NA, NA,
>      0, 0, 0), day_158 = c(NA, NA, 0, 0, 0), day_159 = c(NA, NA,
>      0, 0, 0), day_160 = c(NA, NA, 0, 0, 0), day_161 = c(NA, NA,
>      0, 0, 0), day_162 = c(NA, NA, 0, 0, 0), day_163 = c(NA, NA,
>      0, 26.3412, 4.07376), day_164 = c(NA, NA, 0, 4.28328, 3.03156
>      ), day_165 = c(NA, NA, 0, 0, 0), day_166 = c(NA, NA, 0, 0,
>      4.60404), day_167 = c(NA, NA, 0, 0, 0.70848), day_168 = c(NA,
>      NA, 0, 0, 0), day_169 = c(NA, NA, 0, 0, 0), day_170 = c(NA,
>      NA, 0, 0, 0), day_171 = c(NA, NA, 0, 0, 0), day_172 = c(NA,
>      NA, 0, 0, 0), day_173 = c(NA, NA, 0, 0, 3.7854), day_174 = c(NA,
>      NA, 0, 0, 0), day_175 = c(NA, NA, 0, 0, 0), day_176 = c(NA,
>      NA, 0, 0, 0), day_177 = c(NA, NA, 0, 0, 0), day_178 = c(NA,
>      NA, 0, 0, 0), day_179 = c(NA, NA, 0, 0, 0), day_180 = c(NA,
>      NA, 0, 0, 0), day_181 = c(NA, NA, 0, 0, 0), day_182 = c(NA,
>      NA, 0, 0, 0), day_183 = c(NA, NA, 0, 0, 0), day_184 = c(NA,
>      NA, 0, 0, 0), day_185 = c(NA, NA, 0, 0, 0), day_186 = c(NA,
>      NA, 0, 0, 0), day_187 = c(NA, NA, 7.30728, 4.1202, 0), day_188 = c(NA,
>      NA, 2.56608, 0.5886, 0), day_189 = c(NA, NA, 0, 0, 0), day_190 = c(NA,
>      NA, 21.93156, 8.0082, 11.4318), day_191 = c(NA, NA, 3.13308,
>      0, 0), day_192 = c(NA, NA, 0, 0, 0.10692), day_193 = c(NA,
>      NA, 0, 0, 4.65912), day_194 = c(NA, NA, 0, 0, 0), day_195 = c(NA,
>      NA, 0, 0, 0), day_196 = c(NA, NA, 0, 0, 0), day_197 = c(NA,
>      NA, 0, 0, 0), day_198 = c(NA, NA, 0, 0, 0), day_199 = c(NA,
>      NA, 0, 0, 0), day_200 = c(NA, NA, 0, 0, 0), day_201 = c(NA,
>      NA, 0, 0, 0), day_202 = c(NA, NA, 0, 7.77276, 4.6602), day_203 = c(NA,
>      NA, 0, 0.86292, 0), day_204 = c(NA, NA, 0, 0, 0), day_205 = c(NA,
>      NA, 21.45528, 8.69616, 0), day_206 = c(NA, NA, 0, 0, 0),
>      day_207 = c(NA, NA, 0, 0, 0), day_208 = c(NA, NA, 0, 0, 0
>      ), day_209 = c(NA, NA, 0, 0, 0), day_210 = c(NA, NA, 0, 0,
>      0), day_211 = c(NA, NA, 0, 0, 0), day_212 = c(NA, NA, 0,
>      0, 0), day_213 = c(NA, NA, 0, 0, 0), day_214 = c(NA, NA,
>      0, 0, 0), day_215 = c(NA, NA, 0, 0, 0), day_216 = c(NA, NA,
>      0, 0, 0), day_217 = c(NA, NA, 0, 0, 0), day_218 = c(NA, NA,
>      0, 0, 0), day_219 = c(NA, NA, 0, 0, 0), day_220 = c(NA, NA,
>      6.10092, 10.85508, 13.22244), day_221 = c(NA, NA, 0.87156,
>      0, 0), day_222 = c(NA, NA, 0, 0, 15.46452), day_223 = c(NA,
>      NA, 0, 0, 9.83664), day_224 = c(NA, NA, 0, 0, 0), day_225 = c(NA,
>      NA, 0, 0, 0), day_226 = c(NA, NA, 16.46028, 0, 0), day_227 = c(NA,
>      NA, 0, 0, 0), day_228 = c(NA, NA, 0, 0, 0), day_229 = c(NA,
>      NA, 0, 0, 0), day_230 = c(NA, NA, 0, 0, 0), day_231 = c(NA,
>      NA, 2.7108, 0, 0), day_232 = c(NA, NA, 0, 0, 0), day_233 = c(NA,
>      NA, 0, 0, 0), day_234 = c(NA, NA, 0, 0, 0), day_235 = c(NA,
>      NA, 0, 0, 0), day_236 = c(NA, NA, 0, 0, 3.5586), day_237 = c(NA,
>      NA, 0, 0, 0), day_238 = c(NA, NA, 0, 0, 0), day_239 = c(NA,
>      NA, 10.23192, 0, 0), day_240 = c(NA, NA, 0, 0, 0), day_241 = c(NA,
>      NA, 0, 0, 0), day_242 = c(NA, NA, 0, 0, 0), day_243 = c(NA,
>      NA, 0, 0, 0), day_244 = c(NA, NA, 0, 0, 0), day_245 = c(NA,
>      NA, 0, 0, 0), day_246 = c(NA, NA, 0, 0, 0), day_247 = c(NA,
>      NA, 0, 0, 0), day_248 = c(NA, NA, 0, 0, 0), day_249 = c(NA,
>      NA, 0.50544, 0, 0), day_250 = c(NA, NA, 0.12636, 0, 0), day_251 = c(NA,
>      NA, 7.02432, 0, 5.39784), day_252 = c(NA, NA, 3.33828, 8.00064,
>      7.08372), day_253 = c(NA, NA, 0, 0, 0), day_254 = c(NA, NA,
>      0, 0, 0), day_255 = c(NA, NA, 2.5704, 4.71636, 11.99772),
>      day_256 = c(NA, NA, 0.3672, 0.75384, 0), day_257 = c(NA,
>      NA, 0, 0, 0), day_258 = c(NA, NA, 0.50328, 0, 0), day_259 = c(NA,
>      NA, 6.78888, 0, 0), day_260 = c(NA, NA, 0.96984, 0, 0), day_261 = c(NA,
>      NA, 4.62672, 0, 0), day_262 = c(NA, NA, 0, 0, 0), day_263 = c(NA,
>      NA, 3.16224, 0.27864, 0), day_264 = c(NA, NA, 0, 1.31112,
>      0), day_265 = c(NA, NA, 0.37692, 0, 0), day_266 = c(NA, NA,
>      0, 0, 0), day_267 = c(NA, NA, 0.70524, 0.43524, 0), day_268 = c(NA,
>      NA, 0.18792, 0.12744, 0), day_269 = c(NA, NA, 0, 1.79064,
>      0.96012), day_270 = c(NA, NA, 0, 0, 0.58644), day_271 = c(NA,
>      NA, 4.4982, 0, 0), day_272 = c(NA, NA, 0, 0, 0), day_273 = c(NA,
>      NA, 2.04552, 6.56964, 0), day_274 = c(NA, NA, 0.71712, 0.93852,
>      0), day_275 = c(NA, NA, 0, 0, 0), day_276 = c(NA, NA, 0,
>      0, 0), day_277 = c(NA, NA, 3.31452, 0, 0), day_278 = c(NA,
>      NA, 1.20204, 0, 0), day_279 = c(NA, NA, 0, 0, 0), day_280 = c(NA,
>      NA, 0, 0, 0), day_281 = c(NA, NA, 0, 0, 0), day_282 = c(NA,
>      NA, 17.955, 5.7942, 9.93816), day_283 = c(NA, NA, 4.79304,
>      4.8006, 0), day_284 = c(NA, NA, 3.9366, 0.78084, 0), day_285 = c(NA,
>      NA, 0, 0, 0), day_286 = c(NA, NA, 0, 0, 0), day_287 = c(NA,
>      NA, 0, 0, 0), day_288 = c(NA, NA, 0, 0, 0), day_289 = c(NA,
>      NA, 0, 0, 0), day_290 = c(NA, NA, 0, 0, 0), day_291 = c(NA,
>      NA, 0, 0, 0), day_292 = c(NA, NA, 0, 0, 0), day_293 = c(NA,
>      NA, 1.55736, 0, 0), day_294 = c(NA, NA, 4.28328, 0, 0), day_295 = c(NA,
>      NA, 0, 0, 0), day_296 = c(NA, NA, 0, 0, 0), day_297 = c(NA,
>      NA, 1.6362, 0, 0), day_298 = c(NA, NA, 1.28844, 0, 6.14088
>      ), day_299 = c(NA, NA, 0, 0, 0.50112), day_300 = c(NA, NA,
>      0, 0, 0), day_301 = c(NA, NA, 0, 0.13824, 0.03456), day_302 = c(NA,
>      NA, 0, 2.92572, 9.24264), day_303 = c(NA, NA, 2.8188, 0.41796,
>      0), day_304 = c(NA, NA, 2.04876, 11.28384, 0), day_305 = c(NA,
>      NA, 0, 0.3564, 0), day_306 = c(NA, NA, 0, 0, 0), day_307 = c(NA,
>      NA, 0, 2.36736, 0), day_308 = c(NA, NA, 0, 0, 0), day_309 = c(NA,
>      NA, 34.91856, 20.42604, 0), day_310 = c(NA, NA, 0, 0, 0),
>      day_311 = c(NA, NA, 0, 0, 0), day_312 = c(NA, NA, 0, 0.40392,
>      0), day_313 = c(NA, NA, 0, 0.5292, 0), day_314 = c(NA, NA,
>      0, 0, 5.21424), day_315 = c(NA, NA, 0, 0, 0), day_316 = c(NA,
>      NA, 0, 0, 0.4266), day_317 = c(NA, NA, 0, 0, 0), day_318 = c(NA,
>      NA, 0, 0, 0), day_319 = c(NA, NA, 0, 0, 0), day_320 = c(NA,
>      NA, 0.23436, 0.6048, 14.9256), day_321 = c(NA, NA, 0, 0.10908,
>      0), day_322 = c(NA, NA, 7.68096, 6.66036, 4.53924), day_323 = c(NA,
>      NA, 1.09728, 1.59732, 8.51148), day_324 = c(NA, NA, 0, 0,
>      0), day_325 = c(NA, NA, 1.46016, 0, 0), day_326 = c(NA, NA,
>      0, 0, 8.70048), day_327 = c(NA, NA, 0, 0, 0), day_328 = c(NA,
>      NA, 0, 0, 0), day_329 = c(NA, NA, 0, 0, 0), day_330 = c(NA,
>      NA, 5.3082, 0, 0), day_331 = c(NA, NA, 2.5866, 0, 0), day_332 = c(NA,
>      NA, 8.03628, 6.3666, 4.3308), day_333 = c(NA, NA, 0, 0, 0
>      ), day_334 = c(NA, NA, 0, 0, 0), day_335 = c(NA, NA, 0, 0,
>      0), day_336 = c(NA, NA, 0, 0, 5.29632), day_337 = c(NA, NA,
>      0, 1.77444, 2.7216), day_338 = c(NA, NA, 0.40608, 0, 0.83052
>      ), day_339 = c(NA, NA, 0, 0, 0), day_340 = c(NA, NA, 0, 0,
>      0), day_341 = c(NA, NA, 0, 0, 0), day_342 = c(NA, NA, 0,
>      0, 0), day_343 = c(NA, NA, 0, 0, 0), day_344 = c(NA, NA,
>      7.73388, 0, 0), day_345 = c(NA, NA, 4.80384, 0, 0), day_346 = c(NA,
>      NA, 4.374, 0.09288, 0), day_347 = c(NA, NA, 6.42924, 3.2022,
>      0), day_348 = c(NA, NA, 0, 16.27668, 0), day_349 = c(NA,
>      NA, 0, 0.90072, 9.36684), day_350 = c(NA, NA, 0.135, 1.87272,
>      2.49048), day_351 = c(NA, NA, 0, 0, 0), day_352 = c(NA, NA,
>      0, 0, 0), day_353 = c(NA, NA, 0, 0, 0), day_354 = c(NA, NA,
>      0, 0, 0), day_355 = c(NA, NA, 0, 0, 0), day_356 = c(NA, NA,
>      0, 0, 0), day_357 = c(NA, NA, 2.82636, 39.45348, 26.08848
>      ), day_358 = c(NA, NA, 0, 0, 22.8582), day_359 = c(NA, NA,
>      0, 0, 1.34028), day_360 = c(NA, NA, 30.03804, 0, 3.49704),
>      day_361 = c(NA, NA, 0.13392, 4.941, 4.94424), day_362 = c(NA,
>      NA, 0.92016, 0, 0.70632), day_363 = c(NA, NA, 0, 0, 0), day_364 = c(NA,
>      NA, 0, 0, 0), day_365 = c(NA, NA, 0, 0, 0), day_366 = c(NA,
>      NA, 21.42072, 0, 0)), .Names = c("ISO3", "lon", "lat", "day_1",
> "day_2", "day_3", "day_4", "day_5", "day_6", "day_7", "day_8",
> "day_9", "day_10", "day_11", "day_12", "day_13", "day_14", "day_15",
> "day_16", "day_17", "day_18", "day_19", "day_20", "day_21", "day_22",
> "day_23", "day_24", "day_25", "day_26", "day_27", "day_28", "day_29",
> "day_30", "day_31", "day_32", "day_33", "day_34", "day_35", "day_36",
> "day_37", "day_38", "day_39", "day_40", "day_41", "day_42", "day_43",
> "day_44", "day_45", "day_46", "day_47", "day_48", "day_49", "day_50",
> "day_51", "day_52", "day_53", "day_54", "day_55", "day_56", "day_57",
> "day_58", "day_59", "day_60", "day_61", "day_62", "day_63", "day_64",
> "day_65", "day_66", "day_67", "day_68", "day_69", "day_70", "day_71",
> "day_72", "day_73", "day_74", "day_75", "day_76", "day_77", "day_78",
> "day_79", "day_80", "day_81", "day_82", "day_83", "day_84", "day_85",
> "day_86", "day_87", "day_88", "day_89", "day_90", "day_91", "day_92",
> "day_93", "day_94", "day_95", "day_96", "day_97", "day_98", "day_99",
> "day_100", "day_101", "day_102", "day_103", "day_104", "day_105",
> "day_106", "day_107", "day_108", "day_109", "day_110", "day_111",
> "day_112", "day_113", "day_114", "day_115", "day_116", "day_117",
> "day_118", "day_119", "day_120", "day_121", "day_122", "day_123",
> "day_124", "day_125", "day_126", "day_127", "day_128", "day_129",
> "day_130", "day_131", "day_132", "day_133", "day_134", "day_135",
> "day_136", "day_137", "day_138", "day_139", "day_140", "day_141",
> "day_142", "day_143", "day_144", "day_145", "day_146", "day_147",
> "day_148", "day_149", "day_150", "day_151", "day_152", "day_153",
> "day_154", "day_155", "day_156", "day_157", "day_158", "day_159",
> "day_160", "day_161", "day_162", "day_163", "day_164", "day_165",
> "day_166", "day_167", "day_168", "day_169", "day_170", "day_171",
> "day_172", "day_173", "day_174", "day_175", "day_176", "day_177",
> "day_178", "day_179", "day_180", "day_181", "day_182", "day_183",
> "day_184", "day_185", "day_186", "day_187", "day_188", "day_189",
> "day_190", "day_191", "day_192", "day_193", "day_194", "day_195",
> "day_196", "day_197", "day_198", "day_199", "day_200", "day_201",
> "day_202", "day_203", "day_204", "day_205", "day_206", "day_207",
> "day_208", "day_209", "day_210", "day_211", "day_212", "day_213",
> "day_214", "day_215", "day_216", "day_217", "day_218", "day_219",
> "day_220", "day_221", "day_222", "day_223", "day_224", "day_225",
> "day_226", "day_227", "day_228", "day_229", "day_230", "day_231",
> "day_232", "day_233", "day_234", "day_235", "day_236", "day_237",
> "day_238", "day_239", "day_240", "day_241", "day_242", "day_243",
> "day_244", "day_245", "day_246", "day_247", "day_248", "day_249",
> "day_250", "day_251", "day_252", "day_253", "day_254", "day_255",
> "day_256", "day_257", "day_258", "day_259", "day_260", "day_261",
> "day_262", "day_263", "day_264", "day_265", "day_266", "day_267",
> "day_268", "day_269", "day_270", "day_271", "day_272", "day_273",
> "day_274", "day_275", "day_276", "day_277", "day_278", "day_279",
> "day_280", "day_281", "day_282", "day_283", "day_284", "day_285",
> "day_286", "day_287", "day_288", "day_289", "day_290", "day_291",
> "day_292", "day_293", "day_294", "day_295", "day_296", "day_297",
> "day_298", "day_299", "day_300", "day_301", "day_302", "day_303",
> "day_304", "day_305", "day_306", "day_307", "day_308", "day_309",
> "day_310", "day_311", "day_312", "day_313", "day_314", "day_315",
> "day_316", "day_317", "day_318", "day_319", "day_320", "day_321",
> "day_322", "day_323", "day_324", "day_325", "day_326", "day_327",
> "day_328", "day_329", "day_330", "day_331", "day_332", "day_333",
> "day_334", "day_335", "day_336", "day_337", "day_338", "day_339",
> "day_340", "day_341", "day_342", "day_343", "day_344", "day_345",
> "day_346", "day_347", "day_348", "day_349", "day_350", "day_351",
> "day_352", "day_353", "day_354", "day_355", "day_356", "day_357",
> "day_358", "day_359", "day_360", "day_361", "day_362", "day_363",
> "day_364", "day_365", "day_366"), row.names = c(NA, 5L), class =
> "data.frame")
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Let's not spend our time and resources thinking about things that are so little or so large that all they really do for us is puff us up and make us feel like gods.  We are mammals, and have not exhausted the annoying little problems of being mammals.

                                 -K. Mullis

"A big computer, a complex algorithm and a long time does not equal science."

                               -Robert Gentleman


From petr.pikal at precheza.cz  Fri Nov 18 14:46:19 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 18 Nov 2016 13:46:19 +0000
Subject: [R] Melt and compute Max, Mean, Min
In-Reply-To: <CAMLwc7PNUzOtLFv+yemQZEXsf535nsuzMbf0ShZZ_i3BqsRrHw@mail.gmail.com>
References: <CAMLwc7PNUzOtLFv+yemQZEXsf535nsuzMbf0ShZZ_i3BqsRrHw@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5046931@SRVEXCHMBX.precheza.cz>

Hi

I am not completely sure what you want to do but

> apply(temp[,-(1:3)],1, mean, na.rm=T)
       1        2        3        4        5
     NaN      NaN 2.159516 1.519914 1.514007
> apply(temp[,-(1:3)],1, max, na.rm=T)
       1        2        3        4        5
    -Inf     -Inf 57.36528 39.45348 45.23904
Warning messages:
1: In FUN(newX[, i], ...) :
  no non-missing arguments to max; returning -Inf
2: In FUN(newX[, i], ...) :
  no non-missing arguments to max; returning -Inf
> apply(temp[,-(1:3)],1, min, na.rm=T)
  1   2   3   4   5
Inf Inf   0   0   0

gives you mentioned summary for each row. If you have duplicate rows you shall first aggregate them. However, it seems to me that your data are not correct. It is quite strange that for given lat/lon you have one day value 23 and the next day 0.

temp[1:5, 1:10]
  ISO3 lon lat day_1 day_2    day_3 day_4 day_5 day_6    day_7
1  CHL -69 -55    NA    NA       NA    NA    NA    NA       NA
2  CHL -68 -55    NA    NA       NA    NA    NA    NA       NA
3  CHL -72 -54     0     0  0.00000     0     0     0  2.83824
4 <NA> -71 -54     0     0 23.37984     0     0     0 11.80116
5  CHL -70 -54     0     0  0.00000     0     0     0  1.24956

If you want to process all your files you can do it in cycle. The function

list.files()

can be handy for that task.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Miluji Sb
> Sent: Friday, November 18, 2016 1:49 PM
> To: r-help mailing list <r-help at r-project.org>
> Subject: [R] Melt and compute Max, Mean, Min
>
> Dear all,
>
> I have 51 years of data (1960 - 2010) in csv format, where each file represents
> one year of data. Below is what each file looks like.
>
> These are temperature data by coordinates, my goal is to to compute max,
> min, and mean by year for each of the coordinates and construct a panel
> dataset. Any help will be appreciated, thank you!
>
> Sincerely,
>
> Milu
>
> temp <- dput(head(df,5))
> structure(list(ISO3 = structure(c(28L, 28L, 28L, NA, 28L), .Label = c("AFG",
> "AGO", "ALB", "ARE", "ARG", "ARM", "AUS", "AUT", "AZE", "BDI", "BEL",
> "BEN", "BFA", "BGD", "BGR", "BHS", "BIH", "BLR", "BLZ", "BOL", "BRA",
> "BRN", "BTN", "BWA", "CAF", "CAN", "CHE", "CHL", "CHN", "CIV", "CMR",
> "COD", "COG", "COL", "CRI", "CUB", "CYP", "CZE", "DEU", "DJI", "DNK",
> "DOM", "DZA", "ECU", "EGY", "ERI", "ESH", "ESP", "EST", "ETH", "FIN", "FJI",
> "FLK", "FRA", "GAB", "GBR", "GEO", "GHA", "GIN", "GNB", "GNQ", "GRC",
> "GRL", "GTM", "GUF", "GUY", "HND", "HRV", "HTI", "HUN", "IDN", "IND",
> "IRL", "IRN", "IRQ", "ISL", "ISR", "ITA", "JAM", "JOR", "JPN", "KAZ", "KEN",
> "KGZ", "KHM", "KIR", "KOR", "KWT", "LAO", "LBN", "LBR", "LBY", "LCA",
> "LKA", "LSO", "LTU", "LUX", "LVA", "MAR", "MDA", "MDG", "MEX", "MKD",
> "MLI", "MMR", "MNE", "MNG", "MOZ", "MRT", "MWI", "MYS", "NAM",
> "NCL", "NER", "NGA", "NIC", "NLD", "NOR", "NPL", "NZL", "OMN", "PAK",
> "PAN", "PER", "PHL", "PNG", "POL", "PRI", "PRK", "PRT", "PRY", "QAT",
> "ROU", "RUS", "RWA", "SAU", "SDN", "SEN", "SJM", "SLB", "SLE", "SLV",
> "SOM", "SRB", "SUR", "SVK", "SVN", "SWE", "SWZ", "SYR", "TCD", "TGO",
> "THA", "TJK", "TKM", "TLS", "TUN", "TUR", "TWN", "TZA", "UGA", "UKR",
> "URY", "USA", "UZB", "VEN", "VNM", "VUT", "YEM", "ZAF", "ZMB", "ZWE"
> ), class = "factor"), lon = c(-69L, -68L, -72L, -71L, -70L),
>     lat = c(-55L, -55L, -54L, -54L, -54L), day_1 = c(NA, NA,
>     0, 0, 0), day_2 = c(NA, NA, 0, 0, 0), day_3 = c(NA, NA, 0,
>     23.37984, 0), day_4 = c(NA, NA, 0, 0, 0), day_5 = c(NA, NA,
>     0, 0, 0), day_6 = c(NA, NA, 0, 0, 0), day_7 = c(NA, NA, 2.83824,
>     11.80116, 1.24956), day_8 = c(NA, NA, 0, 1.68588, 14.69448
>     ), day_9 = c(NA, NA, 0, 0, 1.09296), day_10 = c(NA, NA, 0,
>     0, 0), day_11 = c(NA, NA, 3.78, 3.7422, 0), day_12 = c(NA,
>     NA, 0.54, 0, 0), day_13 = c(NA, NA, 0, 0, 0), day_14 = c(NA,
>     NA, 0, 0, 0.39204), day_15 = c(NA, NA, 0, 0, 11.58732), day_16 = c(NA,
>     NA, 0, 0, 0), day_17 = c(NA, NA, 0, 1.14048, 12.26448), day_18 = c(NA,
>     NA, 0, 1.1934, 7.59024), day_19 = c(NA, NA, 9.74268, 0, 0
>     ), day_20 = c(NA, NA, 0, 0, 0), day_21 = c(NA, NA, 1.96776,
>     0, 0), day_22 = c(NA, NA, 0, 0, 0), day_23 = c(NA, NA, 0,
>     0, 0), day_24 = c(NA, NA, 6.21756, 2.74752, 0), day_25 = c(NA,
>     NA, 0, 0, 3.37932), day_26 = c(NA, NA, 4.8384, 0, 0), day_27 = c(NA,
>     NA, 0, 0, 0), day_28 = c(NA, NA, 0, 0, 0), day_29 = c(NA,
>     NA, 22.37328, 0, 0), day_30 = c(NA, NA, 28.97424, 11.25468,
>     0), day_31 = c(NA, NA, 0, 0, 0), day_32 = c(NA, NA, 0, 0,
>     2.00448), day_33 = c(NA, NA, 0, 0, 0), day_34 = c(NA, NA,
>     0, 0, 0), day_35 = c(NA, NA, 0, 0, 0), day_36 = c(NA, NA,
>     0, 0, 0), day_37 = c(NA, NA, 0, 0, 0), day_38 = c(NA, NA,
>     32.7132, 31.71852, 0), day_39 = c(NA, NA, 0, 0, 5.84604),
>     day_40 = c(NA, NA, 0, 0, 0), day_41 = c(NA, NA, 0, 0, 0),
>     day_42 = c(NA, NA, 0, 0, 0), day_43 = c(NA, NA, 0, 0, 0),
>     day_44 = c(NA, NA, 0, 0, 1.78416), day_45 = c(NA, NA, 0,
>     0, 0), day_46 = c(NA, NA, 33.84504, 0, 0), day_47 = c(NA,
>     NA, 0, 0, 0), day_48 = c(NA, NA, 0, 0, 0), day_49 = c(NA,
>     NA, 0, 0, 0), day_50 = c(NA, NA, 0, 0.4752, 0), day_51 = c(NA,
>     NA, 0, 0, 22.02012), day_52 = c(NA, NA, 0, 0, 0), day_53 = c(NA,
>     NA, 0, 0, 3.48084), day_54 = c(NA, NA, 0, 0, 0), day_55 = c(NA,
>     NA, 0.58212, 0, 0), day_56 = c(NA, NA, 0.35316, 0, 0), day_57 = c(NA,
>     NA, 0, 0, 12.65436), day_58 = c(NA, NA, 0, 0, 0), day_59 = c(NA,
>     NA, 0, 0, 0), day_60 = c(NA, NA, 3.03372, 22.05576, 0), day_61 = c(NA,
>     NA, 2.5758, 0, 0), day_62 = c(NA, NA, 0, 0, 0), day_63 = c(NA,
>     NA, 3.67416, 25.22016, 4.21524), day_64 = c(NA, NA, 0.52488,
>     3.60288, 0), day_65 = c(NA, NA, 12.82608, 0, 0), day_66 = c(NA,
>     NA, 0, 0, 0), day_67 = c(NA, NA, 0, 0, 0), day_68 = c(NA,
>     NA, 0, 0, 0), day_69 = c(NA, NA, 0, 0, 0), day_70 = c(NA,
>     NA, 1.11564, 5.17536, 0), day_71 = c(NA, NA, 1.18584, 0,
>     0), day_72 = c(NA, NA, 0, 0, 0.10584), day_73 = c(NA, NA,
>     0.62748, 14.39748, 7.50708), day_74 = c(NA, NA, 7.20252,
>     20.02644, 1.07244), day_75 = c(NA, NA, 1.87488, 0, 0), day_76 = c(NA,
>     NA, 0.26784, 0, 0), day_77 = c(NA, NA, 0, 0, 0), day_78 = c(NA,
>     NA, 0, 0, 2.81664), day_79 = c(NA, NA, 0, 0, 0), day_80 = c(NA,
>     NA, 0, 0, 0), day_81 = c(NA, NA, 0, 0, 0), day_82 = c(NA,
>     NA, 1.29276, 0, 0), day_83 = c(NA, NA, 0.18468, 0, 1.46124
>     ), day_84 = c(NA, NA, 0, 0, 0), day_85 = c(NA, NA, 0, 0,
>     0), day_86 = c(NA, NA, 57.36528, 0, 0), day_87 = c(NA, NA,
>     8.19504, 0, 0), day_88 = c(NA, NA, 0, 0, 0), day_89 = c(NA,
>     NA, 6.45732, 0, 0), day_90 = c(NA, NA, 0, 0, 0), day_91 = c(NA,
>     NA, 0, 0, 44.20332), day_92 = c(NA, NA, 0, 0, 6.31476), day_93 = c(NA,
>     NA, 0, 0, 0.35748), day_94 = c(NA, NA, 16.74972, 30.35988,
>     5.0436), day_95 = c(NA, NA, 4.93992, 1.46556, 19.86768),
>     day_96 = c(NA, NA, 0, 0, 0.88128), day_97 = c(NA, NA, 5.751,
>     19.02096, 0), day_98 = c(NA, NA, 11.5452, 13.37148, 0), day_99 = c(NA,
>     NA, 0, 0, 0), day_100 = c(NA, NA, 0, 0, 0), day_101 = c(NA,
>     NA, 4.70124, 23.80644, 7.61832), day_102 = c(NA, NA, 0, 1.02492,
>     0), day_103 = c(NA, NA, 0, 0, 15.86304), day_104 = c(NA,
>     NA, 0, 0, 0.26352), day_105 = c(NA, NA, 0, 0, 21.60864),
>     day_106 = c(NA, NA, 56.93436, 0, 0.22464), day_107 = c(NA,
>     NA, 8.13348, 0, 0), day_108 = c(NA, NA, 6.83748, 0, 0), day_109 = c(NA,
>     NA, 0, 0, 0), day_110 = c(NA, NA, 14.36724, 0, 0), day_111 = c(NA,
>     NA, 0.63936, 2.43864, 4.0554), day_112 = c(NA, NA, 1.21392,
>     1.15452, 0), day_113 = c(NA, NA, 0.7722, 0, 0), day_114 = c(NA,
>     NA, 0, 0, 1.08864), day_115 = c(NA, NA, 1.47528, 0, 0), day_116 = c(NA,
>     NA, 0, 1.73124, 0), day_117 = c(NA, NA, 0, 0, 0), day_118 = c(NA,
>     NA, 2.4516, 0, 0), day_119 = c(NA, NA, 0, 3.14388, 0), day_120 = c(NA,
>     NA, 1.81872, 0, 0), day_121 = c(NA, NA, 2.77236, 0, 0), day_122 = c(NA,
>     NA, 1.34028, 0.70632, 0), day_123 = c(NA, NA, 0, 0, 0), day_124 = c(NA,
>     NA, 0, 0, 0), day_125 = c(NA, NA, 0.56484, 0.74412, 0), day_126 = c(NA,
>     NA, 1.11888, 0.06264, 0), day_127 = c(NA, NA, 0, 0, 0), day_128 = c(NA,
>     NA, 1.05624, 0, 0), day_129 = c(NA, NA, 26.63928, 34.04268,
>     0), day_130 = c(NA, NA, 6.89796, 0, 0), day_131 = c(NA, NA,
>     1.91592, 2.241, 0), day_132 = c(NA, NA, 0, 2.23668, 45.23904
>     ), day_133 = c(NA, NA, 0, 0, 6.46272), day_134 = c(NA, NA,
>     0, 0, 0), day_135 = c(NA, NA, 0, 0, 0), day_136 = c(NA, NA,
>     0, 0, 0), day_137 = c(NA, NA, 0, 0, 0), day_138 = c(NA, NA,
>     0, 0, 0), day_139 = c(NA, NA, 0, 0, 0), day_140 = c(NA, NA,
>     0, 0, 0), day_141 = c(NA, NA, 0, 0, 0), day_142 = c(NA, NA,
>     0, 0, 0), day_143 = c(NA, NA, 0, 0, 0), day_144 = c(NA, NA,
>     2.943, 5.17536, 0), day_145 = c(NA, NA, 0, 0, 0), day_146 = c(NA,
>     NA, 0, 0, 0), day_147 = c(NA, NA, 0, 0, 0), day_148 = c(NA,
>     NA, 10.96308, 2.98188, 0), day_149 = c(NA, NA, 20.4822, 0.43632,
>     0), day_150 = c(NA, NA, 1.5282, 0, 0), day_151 = c(NA, NA,
>     0, 0, 0), day_152 = c(NA, NA, 0, 0, 0), day_153 = c(NA, NA,
>     0, 0, 0), day_154 = c(NA, NA, 0, 0, 0), day_155 = c(NA, NA,
>     0, 0, 0), day_156 = c(NA, NA, 0, 0, 0), day_157 = c(NA, NA,
>     0, 0, 0), day_158 = c(NA, NA, 0, 0, 0), day_159 = c(NA, NA,
>     0, 0, 0), day_160 = c(NA, NA, 0, 0, 0), day_161 = c(NA, NA,
>     0, 0, 0), day_162 = c(NA, NA, 0, 0, 0), day_163 = c(NA, NA,
>     0, 26.3412, 4.07376), day_164 = c(NA, NA, 0, 4.28328, 3.03156
>     ), day_165 = c(NA, NA, 0, 0, 0), day_166 = c(NA, NA, 0, 0,
>     4.60404), day_167 = c(NA, NA, 0, 0, 0.70848), day_168 = c(NA,
>     NA, 0, 0, 0), day_169 = c(NA, NA, 0, 0, 0), day_170 = c(NA,
>     NA, 0, 0, 0), day_171 = c(NA, NA, 0, 0, 0), day_172 = c(NA,
>     NA, 0, 0, 0), day_173 = c(NA, NA, 0, 0, 3.7854), day_174 = c(NA,
>     NA, 0, 0, 0), day_175 = c(NA, NA, 0, 0, 0), day_176 = c(NA,
>     NA, 0, 0, 0), day_177 = c(NA, NA, 0, 0, 0), day_178 = c(NA,
>     NA, 0, 0, 0), day_179 = c(NA, NA, 0, 0, 0), day_180 = c(NA,
>     NA, 0, 0, 0), day_181 = c(NA, NA, 0, 0, 0), day_182 = c(NA,
>     NA, 0, 0, 0), day_183 = c(NA, NA, 0, 0, 0), day_184 = c(NA,
>     NA, 0, 0, 0), day_185 = c(NA, NA, 0, 0, 0), day_186 = c(NA,
>     NA, 0, 0, 0), day_187 = c(NA, NA, 7.30728, 4.1202, 0), day_188 = c(NA,
>     NA, 2.56608, 0.5886, 0), day_189 = c(NA, NA, 0, 0, 0), day_190 = c(NA,
>     NA, 21.93156, 8.0082, 11.4318), day_191 = c(NA, NA, 3.13308,
>     0, 0), day_192 = c(NA, NA, 0, 0, 0.10692), day_193 = c(NA,
>     NA, 0, 0, 4.65912), day_194 = c(NA, NA, 0, 0, 0), day_195 = c(NA,
>     NA, 0, 0, 0), day_196 = c(NA, NA, 0, 0, 0), day_197 = c(NA,
>     NA, 0, 0, 0), day_198 = c(NA, NA, 0, 0, 0), day_199 = c(NA,
>     NA, 0, 0, 0), day_200 = c(NA, NA, 0, 0, 0), day_201 = c(NA,
>     NA, 0, 0, 0), day_202 = c(NA, NA, 0, 7.77276, 4.6602), day_203 = c(NA,
>     NA, 0, 0.86292, 0), day_204 = c(NA, NA, 0, 0, 0), day_205 = c(NA,
>     NA, 21.45528, 8.69616, 0), day_206 = c(NA, NA, 0, 0, 0),
>     day_207 = c(NA, NA, 0, 0, 0), day_208 = c(NA, NA, 0, 0, 0
>     ), day_209 = c(NA, NA, 0, 0, 0), day_210 = c(NA, NA, 0, 0,
>     0), day_211 = c(NA, NA, 0, 0, 0), day_212 = c(NA, NA, 0,
>     0, 0), day_213 = c(NA, NA, 0, 0, 0), day_214 = c(NA, NA,
>     0, 0, 0), day_215 = c(NA, NA, 0, 0, 0), day_216 = c(NA, NA,
>     0, 0, 0), day_217 = c(NA, NA, 0, 0, 0), day_218 = c(NA, NA,
>     0, 0, 0), day_219 = c(NA, NA, 0, 0, 0), day_220 = c(NA, NA,
>     6.10092, 10.85508, 13.22244), day_221 = c(NA, NA, 0.87156,
>     0, 0), day_222 = c(NA, NA, 0, 0, 15.46452), day_223 = c(NA,
>     NA, 0, 0, 9.83664), day_224 = c(NA, NA, 0, 0, 0), day_225 = c(NA,
>     NA, 0, 0, 0), day_226 = c(NA, NA, 16.46028, 0, 0), day_227 = c(NA,
>     NA, 0, 0, 0), day_228 = c(NA, NA, 0, 0, 0), day_229 = c(NA,
>     NA, 0, 0, 0), day_230 = c(NA, NA, 0, 0, 0), day_231 = c(NA,
>     NA, 2.7108, 0, 0), day_232 = c(NA, NA, 0, 0, 0), day_233 = c(NA,
>     NA, 0, 0, 0), day_234 = c(NA, NA, 0, 0, 0), day_235 = c(NA,
>     NA, 0, 0, 0), day_236 = c(NA, NA, 0, 0, 3.5586), day_237 = c(NA,
>     NA, 0, 0, 0), day_238 = c(NA, NA, 0, 0, 0), day_239 = c(NA,
>     NA, 10.23192, 0, 0), day_240 = c(NA, NA, 0, 0, 0), day_241 = c(NA,
>     NA, 0, 0, 0), day_242 = c(NA, NA, 0, 0, 0), day_243 = c(NA,
>     NA, 0, 0, 0), day_244 = c(NA, NA, 0, 0, 0), day_245 = c(NA,
>     NA, 0, 0, 0), day_246 = c(NA, NA, 0, 0, 0), day_247 = c(NA,
>     NA, 0, 0, 0), day_248 = c(NA, NA, 0, 0, 0), day_249 = c(NA,
>     NA, 0.50544, 0, 0), day_250 = c(NA, NA, 0.12636, 0, 0), day_251 = c(NA,
>     NA, 7.02432, 0, 5.39784), day_252 = c(NA, NA, 3.33828, 8.00064,
>     7.08372), day_253 = c(NA, NA, 0, 0, 0), day_254 = c(NA, NA,
>     0, 0, 0), day_255 = c(NA, NA, 2.5704, 4.71636, 11.99772),
>     day_256 = c(NA, NA, 0.3672, 0.75384, 0), day_257 = c(NA,
>     NA, 0, 0, 0), day_258 = c(NA, NA, 0.50328, 0, 0), day_259 = c(NA,
>     NA, 6.78888, 0, 0), day_260 = c(NA, NA, 0.96984, 0, 0), day_261 = c(NA,
>     NA, 4.62672, 0, 0), day_262 = c(NA, NA, 0, 0, 0), day_263 = c(NA,
>     NA, 3.16224, 0.27864, 0), day_264 = c(NA, NA, 0, 1.31112,
>     0), day_265 = c(NA, NA, 0.37692, 0, 0), day_266 = c(NA, NA,
>     0, 0, 0), day_267 = c(NA, NA, 0.70524, 0.43524, 0), day_268 = c(NA,
>     NA, 0.18792, 0.12744, 0), day_269 = c(NA, NA, 0, 1.79064,
>     0.96012), day_270 = c(NA, NA, 0, 0, 0.58644), day_271 = c(NA,
>     NA, 4.4982, 0, 0), day_272 = c(NA, NA, 0, 0, 0), day_273 = c(NA,
>     NA, 2.04552, 6.56964, 0), day_274 = c(NA, NA, 0.71712, 0.93852,
>     0), day_275 = c(NA, NA, 0, 0, 0), day_276 = c(NA, NA, 0,
>     0, 0), day_277 = c(NA, NA, 3.31452, 0, 0), day_278 = c(NA,
>     NA, 1.20204, 0, 0), day_279 = c(NA, NA, 0, 0, 0), day_280 = c(NA,
>     NA, 0, 0, 0), day_281 = c(NA, NA, 0, 0, 0), day_282 = c(NA,
>     NA, 17.955, 5.7942, 9.93816), day_283 = c(NA, NA, 4.79304,
>     4.8006, 0), day_284 = c(NA, NA, 3.9366, 0.78084, 0), day_285 = c(NA,
>     NA, 0, 0, 0), day_286 = c(NA, NA, 0, 0, 0), day_287 = c(NA,
>     NA, 0, 0, 0), day_288 = c(NA, NA, 0, 0, 0), day_289 = c(NA,
>     NA, 0, 0, 0), day_290 = c(NA, NA, 0, 0, 0), day_291 = c(NA,
>     NA, 0, 0, 0), day_292 = c(NA, NA, 0, 0, 0), day_293 = c(NA,
>     NA, 1.55736, 0, 0), day_294 = c(NA, NA, 4.28328, 0, 0), day_295 = c(NA,
>     NA, 0, 0, 0), day_296 = c(NA, NA, 0, 0, 0), day_297 = c(NA,
>     NA, 1.6362, 0, 0), day_298 = c(NA, NA, 1.28844, 0, 6.14088
>     ), day_299 = c(NA, NA, 0, 0, 0.50112), day_300 = c(NA, NA,
>     0, 0, 0), day_301 = c(NA, NA, 0, 0.13824, 0.03456), day_302 = c(NA,
>     NA, 0, 2.92572, 9.24264), day_303 = c(NA, NA, 2.8188, 0.41796,
>     0), day_304 = c(NA, NA, 2.04876, 11.28384, 0), day_305 = c(NA,
>     NA, 0, 0.3564, 0), day_306 = c(NA, NA, 0, 0, 0), day_307 = c(NA,
>     NA, 0, 2.36736, 0), day_308 = c(NA, NA, 0, 0, 0), day_309 = c(NA,
>     NA, 34.91856, 20.42604, 0), day_310 = c(NA, NA, 0, 0, 0),
>     day_311 = c(NA, NA, 0, 0, 0), day_312 = c(NA, NA, 0, 0.40392,
>     0), day_313 = c(NA, NA, 0, 0.5292, 0), day_314 = c(NA, NA,
>     0, 0, 5.21424), day_315 = c(NA, NA, 0, 0, 0), day_316 = c(NA,
>     NA, 0, 0, 0.4266), day_317 = c(NA, NA, 0, 0, 0), day_318 = c(NA,
>     NA, 0, 0, 0), day_319 = c(NA, NA, 0, 0, 0), day_320 = c(NA,
>     NA, 0.23436, 0.6048, 14.9256), day_321 = c(NA, NA, 0, 0.10908,
>     0), day_322 = c(NA, NA, 7.68096, 6.66036, 4.53924), day_323 = c(NA,
>     NA, 1.09728, 1.59732, 8.51148), day_324 = c(NA, NA, 0, 0,
>     0), day_325 = c(NA, NA, 1.46016, 0, 0), day_326 = c(NA, NA,
>     0, 0, 8.70048), day_327 = c(NA, NA, 0, 0, 0), day_328 = c(NA,
>     NA, 0, 0, 0), day_329 = c(NA, NA, 0, 0, 0), day_330 = c(NA,
>     NA, 5.3082, 0, 0), day_331 = c(NA, NA, 2.5866, 0, 0), day_332 = c(NA,
>     NA, 8.03628, 6.3666, 4.3308), day_333 = c(NA, NA, 0, 0, 0
>     ), day_334 = c(NA, NA, 0, 0, 0), day_335 = c(NA, NA, 0, 0,
>     0), day_336 = c(NA, NA, 0, 0, 5.29632), day_337 = c(NA, NA,
>     0, 1.77444, 2.7216), day_338 = c(NA, NA, 0.40608, 0, 0.83052
>     ), day_339 = c(NA, NA, 0, 0, 0), day_340 = c(NA, NA, 0, 0,
>     0), day_341 = c(NA, NA, 0, 0, 0), day_342 = c(NA, NA, 0,
>     0, 0), day_343 = c(NA, NA, 0, 0, 0), day_344 = c(NA, NA,
>     7.73388, 0, 0), day_345 = c(NA, NA, 4.80384, 0, 0), day_346 = c(NA,
>     NA, 4.374, 0.09288, 0), day_347 = c(NA, NA, 6.42924, 3.2022,
>     0), day_348 = c(NA, NA, 0, 16.27668, 0), day_349 = c(NA,
>     NA, 0, 0.90072, 9.36684), day_350 = c(NA, NA, 0.135, 1.87272,
>     2.49048), day_351 = c(NA, NA, 0, 0, 0), day_352 = c(NA, NA,
>     0, 0, 0), day_353 = c(NA, NA, 0, 0, 0), day_354 = c(NA, NA,
>     0, 0, 0), day_355 = c(NA, NA, 0, 0, 0), day_356 = c(NA, NA,
>     0, 0, 0), day_357 = c(NA, NA, 2.82636, 39.45348, 26.08848
>     ), day_358 = c(NA, NA, 0, 0, 22.8582), day_359 = c(NA, NA,
>     0, 0, 1.34028), day_360 = c(NA, NA, 30.03804, 0, 3.49704),
>     day_361 = c(NA, NA, 0.13392, 4.941, 4.94424), day_362 = c(NA,
>     NA, 0.92016, 0, 0.70632), day_363 = c(NA, NA, 0, 0, 0), day_364 = c(NA,
>     NA, 0, 0, 0), day_365 = c(NA, NA, 0, 0, 0), day_366 = c(NA,
>     NA, 21.42072, 0, 0)), .Names = c("ISO3", "lon", "lat", "day_1", "day_2",
> "day_3", "day_4", "day_5", "day_6", "day_7", "day_8", "day_9", "day_10",
> "day_11", "day_12", "day_13", "day_14", "day_15", "day_16", "day_17",
> "day_18", "day_19", "day_20", "day_21", "day_22", "day_23", "day_24",
> "day_25", "day_26", "day_27", "day_28", "day_29", "day_30", "day_31",
> "day_32", "day_33", "day_34", "day_35", "day_36", "day_37", "day_38",
> "day_39", "day_40", "day_41", "day_42", "day_43", "day_44", "day_45",
> "day_46", "day_47", "day_48", "day_49", "day_50", "day_51", "day_52",
> "day_53", "day_54", "day_55", "day_56", "day_57", "day_58", "day_59",
> "day_60", "day_61", "day_62", "day_63", "day_64", "day_65", "day_66",
> "day_67", "day_68", "day_69", "day_70", "day_71", "day_72", "day_73",
> "day_74", "day_75", "day_76", "day_77", "day_78", "day_79", "day_80",
> "day_81", "day_82", "day_83", "day_84", "day_85", "day_86", "day_87",
> "day_88", "day_89", "day_90", "day_91", "day_92", "day_93", "day_94",
> "day_95", "day_96", "day_97", "day_98", "day_99", "day_100", "day_101",
> "day_102", "day_103", "day_104", "day_105", "day_106", "day_107",
> "day_108", "day_109", "day_110", "day_111", "day_112", "day_113",
> "day_114", "day_115", "day_116", "day_117", "day_118", "day_119",
> "day_120", "day_121", "day_122", "day_123", "day_124", "day_125",
> "day_126", "day_127", "day_128", "day_129", "day_130", "day_131",
> "day_132", "day_133", "day_134", "day_135", "day_136", "day_137",
> "day_138", "day_139", "day_140", "day_141", "day_142", "day_143",
> "day_144", "day_145", "day_146", "day_147", "day_148", "day_149",
> "day_150", "day_151", "day_152", "day_153", "day_154", "day_155",
> "day_156", "day_157", "day_158", "day_159", "day_160", "day_161",
> "day_162", "day_163", "day_164", "day_165", "day_166", "day_167",
> "day_168", "day_169", "day_170", "day_171", "day_172", "day_173",
> "day_174", "day_175", "day_176", "day_177", "day_178", "day_179",
> "day_180", "day_181", "day_182", "day_183", "day_184", "day_185",
> "day_186", "day_187", "day_188", "day_189", "day_190", "day_191",
> "day_192", "day_193", "day_194", "day_195", "day_196", "day_197",
> "day_198", "day_199", "day_200", "day_201", "day_202", "day_203",
> "day_204", "day_205", "day_206", "day_207", "day_208", "day_209",
> "day_210", "day_211", "day_212", "day_213", "day_214", "day_215",
> "day_216", "day_217", "day_218", "day_219", "day_220", "day_221",
> "day_222", "day_223", "day_224", "day_225", "day_226", "day_227",
> "day_228", "day_229", "day_230", "day_231", "day_232", "day_233",
> "day_234", "day_235", "day_236", "day_237", "day_238", "day_239",
> "day_240", "day_241", "day_242", "day_243", "day_244", "day_245",
> "day_246", "day_247", "day_248", "day_249", "day_250", "day_251",
> "day_252", "day_253", "day_254", "day_255", "day_256", "day_257",
> "day_258", "day_259", "day_260", "day_261", "day_262", "day_263",
> "day_264", "day_265", "day_266", "day_267", "day_268", "day_269",
> "day_270", "day_271", "day_272", "day_273", "day_274", "day_275",
> "day_276", "day_277", "day_278", "day_279", "day_280", "day_281",
> "day_282", "day_283", "day_284", "day_285", "day_286", "day_287",
> "day_288", "day_289", "day_290", "day_291", "day_292", "day_293",
> "day_294", "day_295", "day_296", "day_297", "day_298", "day_299",
> "day_300", "day_301", "day_302", "day_303", "day_304", "day_305",
> "day_306", "day_307", "day_308", "day_309", "day_310", "day_311",
> "day_312", "day_313", "day_314", "day_315", "day_316", "day_317",
> "day_318", "day_319", "day_320", "day_321", "day_322", "day_323",
> "day_324", "day_325", "day_326", "day_327", "day_328", "day_329",
> "day_330", "day_331", "day_332", "day_333", "day_334", "day_335",
> "day_336", "day_337", "day_338", "day_339", "day_340", "day_341",
> "day_342", "day_343", "day_344", "day_345", "day_346", "day_347",
> "day_348", "day_349", "day_350", "day_351", "day_352", "day_353",
> "day_354", "day_355", "day_356", "day_357", "day_358", "day_359",
> "day_360", "day_361", "day_362", "day_363", "day_364", "day_365",
> "day_366"), row.names = c(NA, 5L), class =
> "data.frame")
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From milujisb at gmail.com  Fri Nov 18 15:10:18 2016
From: milujisb at gmail.com (Miluji Sb)
Date: Fri, 18 Nov 2016 15:10:18 +0100
Subject: [R] Melt and compute Max, Mean, Min
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5046931@SRVEXCHMBX.precheza.cz>
References: <CAMLwc7PNUzOtLFv+yemQZEXsf535nsuzMbf0ShZZ_i3BqsRrHw@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5046931@SRVEXCHMBX.precheza.cz>
Message-ID: <CAMLwc7M8e8_2suuZOVGDKTv=bVMqGiLsPRSGZNXFToMm1WLq8A@mail.gmail.com>

Dear Petr,

Thank you for the code, apologies though as I copied the wrong data, This
is precipitation data and not temperature.

For the loop, could I do something like this?

filelist <- list.files(pattern=".csv")

myDTs <- lapply(filelist, function(.file) {

apply(temp[,-(1:3)],1, mean, na.rm=T)

}

Thanks again!

Sincerely,

Milu


On Fri, Nov 18, 2016 at 2:46 PM, PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hi
>
> I am not completely sure what you want to do but
>
> > apply(temp[,-(1:3)],1, mean, na.rm=T)
>        1        2        3        4        5
>      NaN      NaN 2.159516 1.519914 1.514007
> > apply(temp[,-(1:3)],1, max, na.rm=T)
>        1        2        3        4        5
>     -Inf     -Inf 57.36528 39.45348 45.23904
> Warning messages:
> 1: In FUN(newX[, i], ...) :
>   no non-missing arguments to max; returning -Inf
> 2: In FUN(newX[, i], ...) :
>   no non-missing arguments to max; returning -Inf
> > apply(temp[,-(1:3)],1, min, na.rm=T)
>   1   2   3   4   5
> Inf Inf   0   0   0
>
> gives you mentioned summary for each row. If you have duplicate rows you
> shall first aggregate them. However, it seems to me that your data are not
> correct. It is quite strange that for given lat/lon you have one day value
> 23 and the next day 0.
>
> temp[1:5, 1:10]
>   ISO3 lon lat day_1 day_2    day_3 day_4 day_5 day_6    day_7
> 1  CHL -69 -55    NA    NA       NA    NA    NA    NA       NA
> 2  CHL -68 -55    NA    NA       NA    NA    NA    NA       NA
> 3  CHL -72 -54     0     0  0.00000     0     0     0  2.83824
> 4 <NA> -71 -54     0     0 23.37984     0     0     0 11.80116
> 5  CHL -70 -54     0     0  0.00000     0     0     0  1.24956
>
> If you want to process all your files you can do it in cycle. The function
>
> list.files()
>
> can be handy for that task.
>
> Cheers
> Petr
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Miluji
> Sb
> > Sent: Friday, November 18, 2016 1:49 PM
> > To: r-help mailing list <r-help at r-project.org>
> > Subject: [R] Melt and compute Max, Mean, Min
> >
> > Dear all,
> >
> > I have 51 years of data (1960 - 2010) in csv format, where each file
> represents
> > one year of data. Below is what each file looks like.
> >
> > These are temperature data by coordinates, my goal is to to compute max,
> > min, and mean by year for each of the coordinates and construct a panel
> > dataset. Any help will be appreciated, thank you!
> >
> > Sincerely,
> >
> > Milu
> >
> > temp <- dput(head(df,5))
> > structure(list(ISO3 = structure(c(28L, 28L, 28L, NA, 28L), .Label =
> c("AFG",
> > "AGO", "ALB", "ARE", "ARG", "ARM", "AUS", "AUT", "AZE", "BDI", "BEL",
> > "BEN", "BFA", "BGD", "BGR", "BHS", "BIH", "BLR", "BLZ", "BOL", "BRA",
> > "BRN", "BTN", "BWA", "CAF", "CAN", "CHE", "CHL", "CHN", "CIV", "CMR",
> > "COD", "COG", "COL", "CRI", "CUB", "CYP", "CZE", "DEU", "DJI", "DNK",
> > "DOM", "DZA", "ECU", "EGY", "ERI", "ESH", "ESP", "EST", "ETH", "FIN",
> "FJI",
> > "FLK", "FRA", "GAB", "GBR", "GEO", "GHA", "GIN", "GNB", "GNQ", "GRC",
> > "GRL", "GTM", "GUF", "GUY", "HND", "HRV", "HTI", "HUN", "IDN", "IND",
> > "IRL", "IRN", "IRQ", "ISL", "ISR", "ITA", "JAM", "JOR", "JPN", "KAZ",
> "KEN",
> > "KGZ", "KHM", "KIR", "KOR", "KWT", "LAO", "LBN", "LBR", "LBY", "LCA",
> > "LKA", "LSO", "LTU", "LUX", "LVA", "MAR", "MDA", "MDG", "MEX", "MKD",
> > "MLI", "MMR", "MNE", "MNG", "MOZ", "MRT", "MWI", "MYS", "NAM",
> > "NCL", "NER", "NGA", "NIC", "NLD", "NOR", "NPL", "NZL", "OMN", "PAK",
> > "PAN", "PER", "PHL", "PNG", "POL", "PRI", "PRK", "PRT", "PRY", "QAT",
> > "ROU", "RUS", "RWA", "SAU", "SDN", "SEN", "SJM", "SLB", "SLE", "SLV",
> > "SOM", "SRB", "SUR", "SVK", "SVN", "SWE", "SWZ", "SYR", "TCD", "TGO",
> > "THA", "TJK", "TKM", "TLS", "TUN", "TUR", "TWN", "TZA", "UGA", "UKR",
> > "URY", "USA", "UZB", "VEN", "VNM", "VUT", "YEM", "ZAF", "ZMB", "ZWE"
> > ), class = "factor"), lon = c(-69L, -68L, -72L, -71L, -70L),
> >     lat = c(-55L, -55L, -54L, -54L, -54L), day_1 = c(NA, NA,
> >     0, 0, 0), day_2 = c(NA, NA, 0, 0, 0), day_3 = c(NA, NA, 0,
> >     23.37984, 0), day_4 = c(NA, NA, 0, 0, 0), day_5 = c(NA, NA,
> >     0, 0, 0), day_6 = c(NA, NA, 0, 0, 0), day_7 = c(NA, NA, 2.83824,
> >     11.80116, 1.24956), day_8 = c(NA, NA, 0, 1.68588, 14.69448
> >     ), day_9 = c(NA, NA, 0, 0, 1.09296), day_10 = c(NA, NA, 0,
> >     0, 0), day_11 = c(NA, NA, 3.78, 3.7422, 0), day_12 = c(NA,
> >     NA, 0.54, 0, 0), day_13 = c(NA, NA, 0, 0, 0), day_14 = c(NA,
> >     NA, 0, 0, 0.39204), day_15 = c(NA, NA, 0, 0, 11.58732), day_16 =
> c(NA,
> >     NA, 0, 0, 0), day_17 = c(NA, NA, 0, 1.14048, 12.26448), day_18 =
> c(NA,
> >     NA, 0, 1.1934, 7.59024), day_19 = c(NA, NA, 9.74268, 0, 0
> >     ), day_20 = c(NA, NA, 0, 0, 0), day_21 = c(NA, NA, 1.96776,
> >     0, 0), day_22 = c(NA, NA, 0, 0, 0), day_23 = c(NA, NA, 0,
> >     0, 0), day_24 = c(NA, NA, 6.21756, 2.74752, 0), day_25 = c(NA,
> >     NA, 0, 0, 3.37932), day_26 = c(NA, NA, 4.8384, 0, 0), day_27 = c(NA,
> >     NA, 0, 0, 0), day_28 = c(NA, NA, 0, 0, 0), day_29 = c(NA,
> >     NA, 22.37328, 0, 0), day_30 = c(NA, NA, 28.97424, 11.25468,
> >     0), day_31 = c(NA, NA, 0, 0, 0), day_32 = c(NA, NA, 0, 0,
> >     2.00448), day_33 = c(NA, NA, 0, 0, 0), day_34 = c(NA, NA,
> >     0, 0, 0), day_35 = c(NA, NA, 0, 0, 0), day_36 = c(NA, NA,
> >     0, 0, 0), day_37 = c(NA, NA, 0, 0, 0), day_38 = c(NA, NA,
> >     32.7132, 31.71852, 0), day_39 = c(NA, NA, 0, 0, 5.84604),
> >     day_40 = c(NA, NA, 0, 0, 0), day_41 = c(NA, NA, 0, 0, 0),
> >     day_42 = c(NA, NA, 0, 0, 0), day_43 = c(NA, NA, 0, 0, 0),
> >     day_44 = c(NA, NA, 0, 0, 1.78416), day_45 = c(NA, NA, 0,
> >     0, 0), day_46 = c(NA, NA, 33.84504, 0, 0), day_47 = c(NA,
> >     NA, 0, 0, 0), day_48 = c(NA, NA, 0, 0, 0), day_49 = c(NA,
> >     NA, 0, 0, 0), day_50 = c(NA, NA, 0, 0.4752, 0), day_51 = c(NA,
> >     NA, 0, 0, 22.02012), day_52 = c(NA, NA, 0, 0, 0), day_53 = c(NA,
> >     NA, 0, 0, 3.48084), day_54 = c(NA, NA, 0, 0, 0), day_55 = c(NA,
> >     NA, 0.58212, 0, 0), day_56 = c(NA, NA, 0.35316, 0, 0), day_57 = c(NA,
> >     NA, 0, 0, 12.65436), day_58 = c(NA, NA, 0, 0, 0), day_59 = c(NA,
> >     NA, 0, 0, 0), day_60 = c(NA, NA, 3.03372, 22.05576, 0), day_61 =
> c(NA,
> >     NA, 2.5758, 0, 0), day_62 = c(NA, NA, 0, 0, 0), day_63 = c(NA,
> >     NA, 3.67416, 25.22016, 4.21524), day_64 = c(NA, NA, 0.52488,
> >     3.60288, 0), day_65 = c(NA, NA, 12.82608, 0, 0), day_66 = c(NA,
> >     NA, 0, 0, 0), day_67 = c(NA, NA, 0, 0, 0), day_68 = c(NA,
> >     NA, 0, 0, 0), day_69 = c(NA, NA, 0, 0, 0), day_70 = c(NA,
> >     NA, 1.11564, 5.17536, 0), day_71 = c(NA, NA, 1.18584, 0,
> >     0), day_72 = c(NA, NA, 0, 0, 0.10584), day_73 = c(NA, NA,
> >     0.62748, 14.39748, 7.50708), day_74 = c(NA, NA, 7.20252,
> >     20.02644, 1.07244), day_75 = c(NA, NA, 1.87488, 0, 0), day_76 = c(NA,
> >     NA, 0.26784, 0, 0), day_77 = c(NA, NA, 0, 0, 0), day_78 = c(NA,
> >     NA, 0, 0, 2.81664), day_79 = c(NA, NA, 0, 0, 0), day_80 = c(NA,
> >     NA, 0, 0, 0), day_81 = c(NA, NA, 0, 0, 0), day_82 = c(NA,
> >     NA, 1.29276, 0, 0), day_83 = c(NA, NA, 0.18468, 0, 1.46124
> >     ), day_84 = c(NA, NA, 0, 0, 0), day_85 = c(NA, NA, 0, 0,
> >     0), day_86 = c(NA, NA, 57.36528, 0, 0), day_87 = c(NA, NA,
> >     8.19504, 0, 0), day_88 = c(NA, NA, 0, 0, 0), day_89 = c(NA,
> >     NA, 6.45732, 0, 0), day_90 = c(NA, NA, 0, 0, 0), day_91 = c(NA,
> >     NA, 0, 0, 44.20332), day_92 = c(NA, NA, 0, 0, 6.31476), day_93 =
> c(NA,
> >     NA, 0, 0, 0.35748), day_94 = c(NA, NA, 16.74972, 30.35988,
> >     5.0436), day_95 = c(NA, NA, 4.93992, 1.46556, 19.86768),
> >     day_96 = c(NA, NA, 0, 0, 0.88128), day_97 = c(NA, NA, 5.751,
> >     19.02096, 0), day_98 = c(NA, NA, 11.5452, 13.37148, 0), day_99 =
> c(NA,
> >     NA, 0, 0, 0), day_100 = c(NA, NA, 0, 0, 0), day_101 = c(NA,
> >     NA, 4.70124, 23.80644, 7.61832), day_102 = c(NA, NA, 0, 1.02492,
> >     0), day_103 = c(NA, NA, 0, 0, 15.86304), day_104 = c(NA,
> >     NA, 0, 0, 0.26352), day_105 = c(NA, NA, 0, 0, 21.60864),
> >     day_106 = c(NA, NA, 56.93436, 0, 0.22464), day_107 = c(NA,
> >     NA, 8.13348, 0, 0), day_108 = c(NA, NA, 6.83748, 0, 0), day_109 =
> c(NA,
> >     NA, 0, 0, 0), day_110 = c(NA, NA, 14.36724, 0, 0), day_111 = c(NA,
> >     NA, 0.63936, 2.43864, 4.0554), day_112 = c(NA, NA, 1.21392,
> >     1.15452, 0), day_113 = c(NA, NA, 0.7722, 0, 0), day_114 = c(NA,
> >     NA, 0, 0, 1.08864), day_115 = c(NA, NA, 1.47528, 0, 0), day_116 =
> c(NA,
> >     NA, 0, 1.73124, 0), day_117 = c(NA, NA, 0, 0, 0), day_118 = c(NA,
> >     NA, 2.4516, 0, 0), day_119 = c(NA, NA, 0, 3.14388, 0), day_120 =
> c(NA,
> >     NA, 1.81872, 0, 0), day_121 = c(NA, NA, 2.77236, 0, 0), day_122 =
> c(NA,
> >     NA, 1.34028, 0.70632, 0), day_123 = c(NA, NA, 0, 0, 0), day_124 =
> c(NA,
> >     NA, 0, 0, 0), day_125 = c(NA, NA, 0.56484, 0.74412, 0), day_126 =
> c(NA,
> >     NA, 1.11888, 0.06264, 0), day_127 = c(NA, NA, 0, 0, 0), day_128 =
> c(NA,
> >     NA, 1.05624, 0, 0), day_129 = c(NA, NA, 26.63928, 34.04268,
> >     0), day_130 = c(NA, NA, 6.89796, 0, 0), day_131 = c(NA, NA,
> >     1.91592, 2.241, 0), day_132 = c(NA, NA, 0, 2.23668, 45.23904
> >     ), day_133 = c(NA, NA, 0, 0, 6.46272), day_134 = c(NA, NA,
> >     0, 0, 0), day_135 = c(NA, NA, 0, 0, 0), day_136 = c(NA, NA,
> >     0, 0, 0), day_137 = c(NA, NA, 0, 0, 0), day_138 = c(NA, NA,
> >     0, 0, 0), day_139 = c(NA, NA, 0, 0, 0), day_140 = c(NA, NA,
> >     0, 0, 0), day_141 = c(NA, NA, 0, 0, 0), day_142 = c(NA, NA,
> >     0, 0, 0), day_143 = c(NA, NA, 0, 0, 0), day_144 = c(NA, NA,
> >     2.943, 5.17536, 0), day_145 = c(NA, NA, 0, 0, 0), day_146 = c(NA,
> >     NA, 0, 0, 0), day_147 = c(NA, NA, 0, 0, 0), day_148 = c(NA,
> >     NA, 10.96308, 2.98188, 0), day_149 = c(NA, NA, 20.4822, 0.43632,
> >     0), day_150 = c(NA, NA, 1.5282, 0, 0), day_151 = c(NA, NA,
> >     0, 0, 0), day_152 = c(NA, NA, 0, 0, 0), day_153 = c(NA, NA,
> >     0, 0, 0), day_154 = c(NA, NA, 0, 0, 0), day_155 = c(NA, NA,
> >     0, 0, 0), day_156 = c(NA, NA, 0, 0, 0), day_157 = c(NA, NA,
> >     0, 0, 0), day_158 = c(NA, NA, 0, 0, 0), day_159 = c(NA, NA,
> >     0, 0, 0), day_160 = c(NA, NA, 0, 0, 0), day_161 = c(NA, NA,
> >     0, 0, 0), day_162 = c(NA, NA, 0, 0, 0), day_163 = c(NA, NA,
> >     0, 26.3412, 4.07376), day_164 = c(NA, NA, 0, 4.28328, 3.03156
> >     ), day_165 = c(NA, NA, 0, 0, 0), day_166 = c(NA, NA, 0, 0,
> >     4.60404), day_167 = c(NA, NA, 0, 0, 0.70848), day_168 = c(NA,
> >     NA, 0, 0, 0), day_169 = c(NA, NA, 0, 0, 0), day_170 = c(NA,
> >     NA, 0, 0, 0), day_171 = c(NA, NA, 0, 0, 0), day_172 = c(NA,
> >     NA, 0, 0, 0), day_173 = c(NA, NA, 0, 0, 3.7854), day_174 = c(NA,
> >     NA, 0, 0, 0), day_175 = c(NA, NA, 0, 0, 0), day_176 = c(NA,
> >     NA, 0, 0, 0), day_177 = c(NA, NA, 0, 0, 0), day_178 = c(NA,
> >     NA, 0, 0, 0), day_179 = c(NA, NA, 0, 0, 0), day_180 = c(NA,
> >     NA, 0, 0, 0), day_181 = c(NA, NA, 0, 0, 0), day_182 = c(NA,
> >     NA, 0, 0, 0), day_183 = c(NA, NA, 0, 0, 0), day_184 = c(NA,
> >     NA, 0, 0, 0), day_185 = c(NA, NA, 0, 0, 0), day_186 = c(NA,
> >     NA, 0, 0, 0), day_187 = c(NA, NA, 7.30728, 4.1202, 0), day_188 =
> c(NA,
> >     NA, 2.56608, 0.5886, 0), day_189 = c(NA, NA, 0, 0, 0), day_190 =
> c(NA,
> >     NA, 21.93156, 8.0082, 11.4318), day_191 = c(NA, NA, 3.13308,
> >     0, 0), day_192 = c(NA, NA, 0, 0, 0.10692), day_193 = c(NA,
> >     NA, 0, 0, 4.65912), day_194 = c(NA, NA, 0, 0, 0), day_195 = c(NA,
> >     NA, 0, 0, 0), day_196 = c(NA, NA, 0, 0, 0), day_197 = c(NA,
> >     NA, 0, 0, 0), day_198 = c(NA, NA, 0, 0, 0), day_199 = c(NA,
> >     NA, 0, 0, 0), day_200 = c(NA, NA, 0, 0, 0), day_201 = c(NA,
> >     NA, 0, 0, 0), day_202 = c(NA, NA, 0, 7.77276, 4.6602), day_203 =
> c(NA,
> >     NA, 0, 0.86292, 0), day_204 = c(NA, NA, 0, 0, 0), day_205 = c(NA,
> >     NA, 21.45528, 8.69616, 0), day_206 = c(NA, NA, 0, 0, 0),
> >     day_207 = c(NA, NA, 0, 0, 0), day_208 = c(NA, NA, 0, 0, 0
> >     ), day_209 = c(NA, NA, 0, 0, 0), day_210 = c(NA, NA, 0, 0,
> >     0), day_211 = c(NA, NA, 0, 0, 0), day_212 = c(NA, NA, 0,
> >     0, 0), day_213 = c(NA, NA, 0, 0, 0), day_214 = c(NA, NA,
> >     0, 0, 0), day_215 = c(NA, NA, 0, 0, 0), day_216 = c(NA, NA,
> >     0, 0, 0), day_217 = c(NA, NA, 0, 0, 0), day_218 = c(NA, NA,
> >     0, 0, 0), day_219 = c(NA, NA, 0, 0, 0), day_220 = c(NA, NA,
> >     6.10092, 10.85508, 13.22244), day_221 = c(NA, NA, 0.87156,
> >     0, 0), day_222 = c(NA, NA, 0, 0, 15.46452), day_223 = c(NA,
> >     NA, 0, 0, 9.83664), day_224 = c(NA, NA, 0, 0, 0), day_225 = c(NA,
> >     NA, 0, 0, 0), day_226 = c(NA, NA, 16.46028, 0, 0), day_227 = c(NA,
> >     NA, 0, 0, 0), day_228 = c(NA, NA, 0, 0, 0), day_229 = c(NA,
> >     NA, 0, 0, 0), day_230 = c(NA, NA, 0, 0, 0), day_231 = c(NA,
> >     NA, 2.7108, 0, 0), day_232 = c(NA, NA, 0, 0, 0), day_233 = c(NA,
> >     NA, 0, 0, 0), day_234 = c(NA, NA, 0, 0, 0), day_235 = c(NA,
> >     NA, 0, 0, 0), day_236 = c(NA, NA, 0, 0, 3.5586), day_237 = c(NA,
> >     NA, 0, 0, 0), day_238 = c(NA, NA, 0, 0, 0), day_239 = c(NA,
> >     NA, 10.23192, 0, 0), day_240 = c(NA, NA, 0, 0, 0), day_241 = c(NA,
> >     NA, 0, 0, 0), day_242 = c(NA, NA, 0, 0, 0), day_243 = c(NA,
> >     NA, 0, 0, 0), day_244 = c(NA, NA, 0, 0, 0), day_245 = c(NA,
> >     NA, 0, 0, 0), day_246 = c(NA, NA, 0, 0, 0), day_247 = c(NA,
> >     NA, 0, 0, 0), day_248 = c(NA, NA, 0, 0, 0), day_249 = c(NA,
> >     NA, 0.50544, 0, 0), day_250 = c(NA, NA, 0.12636, 0, 0), day_251 =
> c(NA,
> >     NA, 7.02432, 0, 5.39784), day_252 = c(NA, NA, 3.33828, 8.00064,
> >     7.08372), day_253 = c(NA, NA, 0, 0, 0), day_254 = c(NA, NA,
> >     0, 0, 0), day_255 = c(NA, NA, 2.5704, 4.71636, 11.99772),
> >     day_256 = c(NA, NA, 0.3672, 0.75384, 0), day_257 = c(NA,
> >     NA, 0, 0, 0), day_258 = c(NA, NA, 0.50328, 0, 0), day_259 = c(NA,
> >     NA, 6.78888, 0, 0), day_260 = c(NA, NA, 0.96984, 0, 0), day_261 =
> c(NA,
> >     NA, 4.62672, 0, 0), day_262 = c(NA, NA, 0, 0, 0), day_263 = c(NA,
> >     NA, 3.16224, 0.27864, 0), day_264 = c(NA, NA, 0, 1.31112,
> >     0), day_265 = c(NA, NA, 0.37692, 0, 0), day_266 = c(NA, NA,
> >     0, 0, 0), day_267 = c(NA, NA, 0.70524, 0.43524, 0), day_268 = c(NA,
> >     NA, 0.18792, 0.12744, 0), day_269 = c(NA, NA, 0, 1.79064,
> >     0.96012), day_270 = c(NA, NA, 0, 0, 0.58644), day_271 = c(NA,
> >     NA, 4.4982, 0, 0), day_272 = c(NA, NA, 0, 0, 0), day_273 = c(NA,
> >     NA, 2.04552, 6.56964, 0), day_274 = c(NA, NA, 0.71712, 0.93852,
> >     0), day_275 = c(NA, NA, 0, 0, 0), day_276 = c(NA, NA, 0,
> >     0, 0), day_277 = c(NA, NA, 3.31452, 0, 0), day_278 = c(NA,
> >     NA, 1.20204, 0, 0), day_279 = c(NA, NA, 0, 0, 0), day_280 = c(NA,
> >     NA, 0, 0, 0), day_281 = c(NA, NA, 0, 0, 0), day_282 = c(NA,
> >     NA, 17.955, 5.7942, 9.93816), day_283 = c(NA, NA, 4.79304,
> >     4.8006, 0), day_284 = c(NA, NA, 3.9366, 0.78084, 0), day_285 = c(NA,
> >     NA, 0, 0, 0), day_286 = c(NA, NA, 0, 0, 0), day_287 = c(NA,
> >     NA, 0, 0, 0), day_288 = c(NA, NA, 0, 0, 0), day_289 = c(NA,
> >     NA, 0, 0, 0), day_290 = c(NA, NA, 0, 0, 0), day_291 = c(NA,
> >     NA, 0, 0, 0), day_292 = c(NA, NA, 0, 0, 0), day_293 = c(NA,
> >     NA, 1.55736, 0, 0), day_294 = c(NA, NA, 4.28328, 0, 0), day_295 =
> c(NA,
> >     NA, 0, 0, 0), day_296 = c(NA, NA, 0, 0, 0), day_297 = c(NA,
> >     NA, 1.6362, 0, 0), day_298 = c(NA, NA, 1.28844, 0, 6.14088
> >     ), day_299 = c(NA, NA, 0, 0, 0.50112), day_300 = c(NA, NA,
> >     0, 0, 0), day_301 = c(NA, NA, 0, 0.13824, 0.03456), day_302 = c(NA,
> >     NA, 0, 2.92572, 9.24264), day_303 = c(NA, NA, 2.8188, 0.41796,
> >     0), day_304 = c(NA, NA, 2.04876, 11.28384, 0), day_305 = c(NA,
> >     NA, 0, 0.3564, 0), day_306 = c(NA, NA, 0, 0, 0), day_307 = c(NA,
> >     NA, 0, 2.36736, 0), day_308 = c(NA, NA, 0, 0, 0), day_309 = c(NA,
> >     NA, 34.91856, 20.42604, 0), day_310 = c(NA, NA, 0, 0, 0),
> >     day_311 = c(NA, NA, 0, 0, 0), day_312 = c(NA, NA, 0, 0.40392,
> >     0), day_313 = c(NA, NA, 0, 0.5292, 0), day_314 = c(NA, NA,
> >     0, 0, 5.21424), day_315 = c(NA, NA, 0, 0, 0), day_316 = c(NA,
> >     NA, 0, 0, 0.4266), day_317 = c(NA, NA, 0, 0, 0), day_318 = c(NA,
> >     NA, 0, 0, 0), day_319 = c(NA, NA, 0, 0, 0), day_320 = c(NA,
> >     NA, 0.23436, 0.6048, 14.9256), day_321 = c(NA, NA, 0, 0.10908,
> >     0), day_322 = c(NA, NA, 7.68096, 6.66036, 4.53924), day_323 = c(NA,
> >     NA, 1.09728, 1.59732, 8.51148), day_324 = c(NA, NA, 0, 0,
> >     0), day_325 = c(NA, NA, 1.46016, 0, 0), day_326 = c(NA, NA,
> >     0, 0, 8.70048), day_327 = c(NA, NA, 0, 0, 0), day_328 = c(NA,
> >     NA, 0, 0, 0), day_329 = c(NA, NA, 0, 0, 0), day_330 = c(NA,
> >     NA, 5.3082, 0, 0), day_331 = c(NA, NA, 2.5866, 0, 0), day_332 = c(NA,
> >     NA, 8.03628, 6.3666, 4.3308), day_333 = c(NA, NA, 0, 0, 0
> >     ), day_334 = c(NA, NA, 0, 0, 0), day_335 = c(NA, NA, 0, 0,
> >     0), day_336 = c(NA, NA, 0, 0, 5.29632), day_337 = c(NA, NA,
> >     0, 1.77444, 2.7216), day_338 = c(NA, NA, 0.40608, 0, 0.83052
> >     ), day_339 = c(NA, NA, 0, 0, 0), day_340 = c(NA, NA, 0, 0,
> >     0), day_341 = c(NA, NA, 0, 0, 0), day_342 = c(NA, NA, 0,
> >     0, 0), day_343 = c(NA, NA, 0, 0, 0), day_344 = c(NA, NA,
> >     7.73388, 0, 0), day_345 = c(NA, NA, 4.80384, 0, 0), day_346 = c(NA,
> >     NA, 4.374, 0.09288, 0), day_347 = c(NA, NA, 6.42924, 3.2022,
> >     0), day_348 = c(NA, NA, 0, 16.27668, 0), day_349 = c(NA,
> >     NA, 0, 0.90072, 9.36684), day_350 = c(NA, NA, 0.135, 1.87272,
> >     2.49048), day_351 = c(NA, NA, 0, 0, 0), day_352 = c(NA, NA,
> >     0, 0, 0), day_353 = c(NA, NA, 0, 0, 0), day_354 = c(NA, NA,
> >     0, 0, 0), day_355 = c(NA, NA, 0, 0, 0), day_356 = c(NA, NA,
> >     0, 0, 0), day_357 = c(NA, NA, 2.82636, 39.45348, 26.08848
> >     ), day_358 = c(NA, NA, 0, 0, 22.8582), day_359 = c(NA, NA,
> >     0, 0, 1.34028), day_360 = c(NA, NA, 30.03804, 0, 3.49704),
> >     day_361 = c(NA, NA, 0.13392, 4.941, 4.94424), day_362 = c(NA,
> >     NA, 0.92016, 0, 0.70632), day_363 = c(NA, NA, 0, 0, 0), day_364 =
> c(NA,
> >     NA, 0, 0, 0), day_365 = c(NA, NA, 0, 0, 0), day_366 = c(NA,
> >     NA, 21.42072, 0, 0)), .Names = c("ISO3", "lon", "lat", "day_1",
> "day_2",
> > "day_3", "day_4", "day_5", "day_6", "day_7", "day_8", "day_9", "day_10",
> > "day_11", "day_12", "day_13", "day_14", "day_15", "day_16", "day_17",
> > "day_18", "day_19", "day_20", "day_21", "day_22", "day_23", "day_24",
> > "day_25", "day_26", "day_27", "day_28", "day_29", "day_30", "day_31",
> > "day_32", "day_33", "day_34", "day_35", "day_36", "day_37", "day_38",
> > "day_39", "day_40", "day_41", "day_42", "day_43", "day_44", "day_45",
> > "day_46", "day_47", "day_48", "day_49", "day_50", "day_51", "day_52",
> > "day_53", "day_54", "day_55", "day_56", "day_57", "day_58", "day_59",
> > "day_60", "day_61", "day_62", "day_63", "day_64", "day_65", "day_66",
> > "day_67", "day_68", "day_69", "day_70", "day_71", "day_72", "day_73",
> > "day_74", "day_75", "day_76", "day_77", "day_78", "day_79", "day_80",
> > "day_81", "day_82", "day_83", "day_84", "day_85", "day_86", "day_87",
> > "day_88", "day_89", "day_90", "day_91", "day_92", "day_93", "day_94",
> > "day_95", "day_96", "day_97", "day_98", "day_99", "day_100", "day_101",
> > "day_102", "day_103", "day_104", "day_105", "day_106", "day_107",
> > "day_108", "day_109", "day_110", "day_111", "day_112", "day_113",
> > "day_114", "day_115", "day_116", "day_117", "day_118", "day_119",
> > "day_120", "day_121", "day_122", "day_123", "day_124", "day_125",
> > "day_126", "day_127", "day_128", "day_129", "day_130", "day_131",
> > "day_132", "day_133", "day_134", "day_135", "day_136", "day_137",
> > "day_138", "day_139", "day_140", "day_141", "day_142", "day_143",
> > "day_144", "day_145", "day_146", "day_147", "day_148", "day_149",
> > "day_150", "day_151", "day_152", "day_153", "day_154", "day_155",
> > "day_156", "day_157", "day_158", "day_159", "day_160", "day_161",
> > "day_162", "day_163", "day_164", "day_165", "day_166", "day_167",
> > "day_168", "day_169", "day_170", "day_171", "day_172", "day_173",
> > "day_174", "day_175", "day_176", "day_177", "day_178", "day_179",
> > "day_180", "day_181", "day_182", "day_183", "day_184", "day_185",
> > "day_186", "day_187", "day_188", "day_189", "day_190", "day_191",
> > "day_192", "day_193", "day_194", "day_195", "day_196", "day_197",
> > "day_198", "day_199", "day_200", "day_201", "day_202", "day_203",
> > "day_204", "day_205", "day_206", "day_207", "day_208", "day_209",
> > "day_210", "day_211", "day_212", "day_213", "day_214", "day_215",
> > "day_216", "day_217", "day_218", "day_219", "day_220", "day_221",
> > "day_222", "day_223", "day_224", "day_225", "day_226", "day_227",
> > "day_228", "day_229", "day_230", "day_231", "day_232", "day_233",
> > "day_234", "day_235", "day_236", "day_237", "day_238", "day_239",
> > "day_240", "day_241", "day_242", "day_243", "day_244", "day_245",
> > "day_246", "day_247", "day_248", "day_249", "day_250", "day_251",
> > "day_252", "day_253", "day_254", "day_255", "day_256", "day_257",
> > "day_258", "day_259", "day_260", "day_261", "day_262", "day_263",
> > "day_264", "day_265", "day_266", "day_267", "day_268", "day_269",
> > "day_270", "day_271", "day_272", "day_273", "day_274", "day_275",
> > "day_276", "day_277", "day_278", "day_279", "day_280", "day_281",
> > "day_282", "day_283", "day_284", "day_285", "day_286", "day_287",
> > "day_288", "day_289", "day_290", "day_291", "day_292", "day_293",
> > "day_294", "day_295", "day_296", "day_297", "day_298", "day_299",
> > "day_300", "day_301", "day_302", "day_303", "day_304", "day_305",
> > "day_306", "day_307", "day_308", "day_309", "day_310", "day_311",
> > "day_312", "day_313", "day_314", "day_315", "day_316", "day_317",
> > "day_318", "day_319", "day_320", "day_321", "day_322", "day_323",
> > "day_324", "day_325", "day_326", "day_327", "day_328", "day_329",
> > "day_330", "day_331", "day_332", "day_333", "day_334", "day_335",
> > "day_336", "day_337", "day_338", "day_339", "day_340", "day_341",
> > "day_342", "day_343", "day_344", "day_345", "day_346", "day_347",
> > "day_348", "day_349", "day_350", "day_351", "day_352", "day_353",
> > "day_354", "day_355", "day_356", "day_357", "day_358", "day_359",
> > "day_360", "day_361", "day_362", "day_363", "day_364", "day_365",
> > "day_366"), row.names = c(NA, 5L), class =
> > "data.frame")
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>

	[[alternative HTML version deleted]]


From milujisb at gmail.com  Fri Nov 18 15:56:56 2016
From: milujisb at gmail.com (Miluji Sb)
Date: Fri, 18 Nov 2016 15:56:56 +0100
Subject: [R] Melt and compute Max, Mean, Min
In-Reply-To: <CAMLwc7M8e8_2suuZOVGDKTv=bVMqGiLsPRSGZNXFToMm1WLq8A@mail.gmail.com>
References: <CAMLwc7PNUzOtLFv+yemQZEXsf535nsuzMbf0ShZZ_i3BqsRrHw@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5046931@SRVEXCHMBX.precheza.cz>
	<CAMLwc7M8e8_2suuZOVGDKTv=bVMqGiLsPRSGZNXFToMm1WLq8A@mail.gmail.com>
Message-ID: <CAMLwc7OdFJ2bj3_2TqZpS0HNZrdVc_0=6ud7jv8B1HKN43xr2Q@mail.gmail.com>

If I do:

as.data.frame(apply(df[,-(1:3)],1, mean, na.rm=T))

is it possible to sequentially name the variables as "mean_1960",
"max_1960". "min_1960", "mean_1961", "max_1961". "min_1961", ...?

On Fri, Nov 18, 2016 at 3:10 PM, Miluji Sb <milujisb at gmail.com> wrote:

> Dear Petr,
>
> Thank you for the code, apologies though as I copied the wrong data, This
> is precipitation data and not temperature.
>
> For the loop, could I do something like this?
>
> filelist <- list.files(pattern=".csv")
>
> myDTs <- lapply(filelist, function(.file) {
>
> apply(temp[,-(1:3)],1, mean, na.rm=T)
>
> }
>
> Thanks again!
>
> Sincerely,
>
> Milu
>
>
> On Fri, Nov 18, 2016 at 2:46 PM, PIKAL Petr <petr.pikal at precheza.cz>
> wrote:
>
>> Hi
>>
>> I am not completely sure what you want to do but
>>
>> > apply(temp[,-(1:3)],1, mean, na.rm=T)
>>        1        2        3        4        5
>>      NaN      NaN 2.159516 1.519914 1.514007
>> > apply(temp[,-(1:3)],1, max, na.rm=T)
>>        1        2        3        4        5
>>     -Inf     -Inf 57.36528 39.45348 45.23904
>> Warning messages:
>> 1: In FUN(newX[, i], ...) :
>>   no non-missing arguments to max; returning -Inf
>> 2: In FUN(newX[, i], ...) :
>>   no non-missing arguments to max; returning -Inf
>> > apply(temp[,-(1:3)],1, min, na.rm=T)
>>   1   2   3   4   5
>> Inf Inf   0   0   0
>>
>> gives you mentioned summary for each row. If you have duplicate rows you
>> shall first aggregate them. However, it seems to me that your data are not
>> correct. It is quite strange that for given lat/lon you have one day value
>> 23 and the next day 0.
>>
>> temp[1:5, 1:10]
>>   ISO3 lon lat day_1 day_2    day_3 day_4 day_5 day_6    day_7
>> 1  CHL -69 -55    NA    NA       NA    NA    NA    NA       NA
>> 2  CHL -68 -55    NA    NA       NA    NA    NA    NA       NA
>> 3  CHL -72 -54     0     0  0.00000     0     0     0  2.83824
>> 4 <NA> -71 -54     0     0 23.37984     0     0     0 11.80116
>> 5  CHL -70 -54     0     0  0.00000     0     0     0  1.24956
>>
>> If you want to process all your files you can do it in cycle. The function
>>
>> list.files()
>>
>> can be handy for that task.
>>
>> Cheers
>> Petr
>>
>> > -----Original Message-----
>> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Miluji
>> Sb
>> > Sent: Friday, November 18, 2016 1:49 PM
>> > To: r-help mailing list <r-help at r-project.org>
>> > Subject: [R] Melt and compute Max, Mean, Min
>> >
>> > Dear all,
>> >
>> > I have 51 years of data (1960 - 2010) in csv format, where each file
>> represents
>> > one year of data. Below is what each file looks like.
>> >
>> > These are temperature data by coordinates, my goal is to to compute max,
>> > min, and mean by year for each of the coordinates and construct a panel
>> > dataset. Any help will be appreciated, thank you!
>> >
>> > Sincerely,
>> >
>> > Milu
>> >
>> > temp <- dput(head(df,5))
>> > structure(list(ISO3 = structure(c(28L, 28L, 28L, NA, 28L), .Label =
>> c("AFG",
>> > "AGO", "ALB", "ARE", "ARG", "ARM", "AUS", "AUT", "AZE", "BDI", "BEL",
>> > "BEN", "BFA", "BGD", "BGR", "BHS", "BIH", "BLR", "BLZ", "BOL", "BRA",
>> > "BRN", "BTN", "BWA", "CAF", "CAN", "CHE", "CHL", "CHN", "CIV", "CMR",
>> > "COD", "COG", "COL", "CRI", "CUB", "CYP", "CZE", "DEU", "DJI", "DNK",
>> > "DOM", "DZA", "ECU", "EGY", "ERI", "ESH", "ESP", "EST", "ETH", "FIN",
>> "FJI",
>> > "FLK", "FRA", "GAB", "GBR", "GEO", "GHA", "GIN", "GNB", "GNQ", "GRC",
>> > "GRL", "GTM", "GUF", "GUY", "HND", "HRV", "HTI", "HUN", "IDN", "IND",
>> > "IRL", "IRN", "IRQ", "ISL", "ISR", "ITA", "JAM", "JOR", "JPN", "KAZ",
>> "KEN",
>> > "KGZ", "KHM", "KIR", "KOR", "KWT", "LAO", "LBN", "LBR", "LBY", "LCA",
>> > "LKA", "LSO", "LTU", "LUX", "LVA", "MAR", "MDA", "MDG", "MEX", "MKD",
>> > "MLI", "MMR", "MNE", "MNG", "MOZ", "MRT", "MWI", "MYS", "NAM",
>> > "NCL", "NER", "NGA", "NIC", "NLD", "NOR", "NPL", "NZL", "OMN", "PAK",
>> > "PAN", "PER", "PHL", "PNG", "POL", "PRI", "PRK", "PRT", "PRY", "QAT",
>> > "ROU", "RUS", "RWA", "SAU", "SDN", "SEN", "SJM", "SLB", "SLE", "SLV",
>> > "SOM", "SRB", "SUR", "SVK", "SVN", "SWE", "SWZ", "SYR", "TCD", "TGO",
>> > "THA", "TJK", "TKM", "TLS", "TUN", "TUR", "TWN", "TZA", "UGA", "UKR",
>> > "URY", "USA", "UZB", "VEN", "VNM", "VUT", "YEM", "ZAF", "ZMB", "ZWE"
>> > ), class = "factor"), lon = c(-69L, -68L, -72L, -71L, -70L),
>> >     lat = c(-55L, -55L, -54L, -54L, -54L), day_1 = c(NA, NA,
>> >     0, 0, 0), day_2 = c(NA, NA, 0, 0, 0), day_3 = c(NA, NA, 0,
>> >     23.37984, 0), day_4 = c(NA, NA, 0, 0, 0), day_5 = c(NA, NA,
>> >     0, 0, 0), day_6 = c(NA, NA, 0, 0, 0), day_7 = c(NA, NA, 2.83824,
>> >     11.80116, 1.24956), day_8 = c(NA, NA, 0, 1.68588, 14.69448
>> >     ), day_9 = c(NA, NA, 0, 0, 1.09296), day_10 = c(NA, NA, 0,
>> >     0, 0), day_11 = c(NA, NA, 3.78, 3.7422, 0), day_12 = c(NA,
>> >     NA, 0.54, 0, 0), day_13 = c(NA, NA, 0, 0, 0), day_14 = c(NA,
>> >     NA, 0, 0, 0.39204), day_15 = c(NA, NA, 0, 0, 11.58732), day_16 =
>> c(NA,
>> >     NA, 0, 0, 0), day_17 = c(NA, NA, 0, 1.14048, 12.26448), day_18 =
>> c(NA,
>> >     NA, 0, 1.1934, 7.59024), day_19 = c(NA, NA, 9.74268, 0, 0
>> >     ), day_20 = c(NA, NA, 0, 0, 0), day_21 = c(NA, NA, 1.96776,
>> >     0, 0), day_22 = c(NA, NA, 0, 0, 0), day_23 = c(NA, NA, 0,
>> >     0, 0), day_24 = c(NA, NA, 6.21756, 2.74752, 0), day_25 = c(NA,
>> >     NA, 0, 0, 3.37932), day_26 = c(NA, NA, 4.8384, 0, 0), day_27 = c(NA,
>> >     NA, 0, 0, 0), day_28 = c(NA, NA, 0, 0, 0), day_29 = c(NA,
>> >     NA, 22.37328, 0, 0), day_30 = c(NA, NA, 28.97424, 11.25468,
>> >     0), day_31 = c(NA, NA, 0, 0, 0), day_32 = c(NA, NA, 0, 0,
>> >     2.00448), day_33 = c(NA, NA, 0, 0, 0), day_34 = c(NA, NA,
>> >     0, 0, 0), day_35 = c(NA, NA, 0, 0, 0), day_36 = c(NA, NA,
>> >     0, 0, 0), day_37 = c(NA, NA, 0, 0, 0), day_38 = c(NA, NA,
>> >     32.7132, 31.71852, 0), day_39 = c(NA, NA, 0, 0, 5.84604),
>> >     day_40 = c(NA, NA, 0, 0, 0), day_41 = c(NA, NA, 0, 0, 0),
>> >     day_42 = c(NA, NA, 0, 0, 0), day_43 = c(NA, NA, 0, 0, 0),
>> >     day_44 = c(NA, NA, 0, 0, 1.78416), day_45 = c(NA, NA, 0,
>> >     0, 0), day_46 = c(NA, NA, 33.84504, 0, 0), day_47 = c(NA,
>> >     NA, 0, 0, 0), day_48 = c(NA, NA, 0, 0, 0), day_49 = c(NA,
>> >     NA, 0, 0, 0), day_50 = c(NA, NA, 0, 0.4752, 0), day_51 = c(NA,
>> >     NA, 0, 0, 22.02012), day_52 = c(NA, NA, 0, 0, 0), day_53 = c(NA,
>> >     NA, 0, 0, 3.48084), day_54 = c(NA, NA, 0, 0, 0), day_55 = c(NA,
>> >     NA, 0.58212, 0, 0), day_56 = c(NA, NA, 0.35316, 0, 0), day_57 =
>> c(NA,
>> >     NA, 0, 0, 12.65436), day_58 = c(NA, NA, 0, 0, 0), day_59 = c(NA,
>> >     NA, 0, 0, 0), day_60 = c(NA, NA, 3.03372, 22.05576, 0), day_61 =
>> c(NA,
>> >     NA, 2.5758, 0, 0), day_62 = c(NA, NA, 0, 0, 0), day_63 = c(NA,
>> >     NA, 3.67416, 25.22016, 4.21524), day_64 = c(NA, NA, 0.52488,
>> >     3.60288, 0), day_65 = c(NA, NA, 12.82608, 0, 0), day_66 = c(NA,
>> >     NA, 0, 0, 0), day_67 = c(NA, NA, 0, 0, 0), day_68 = c(NA,
>> >     NA, 0, 0, 0), day_69 = c(NA, NA, 0, 0, 0), day_70 = c(NA,
>> >     NA, 1.11564, 5.17536, 0), day_71 = c(NA, NA, 1.18584, 0,
>> >     0), day_72 = c(NA, NA, 0, 0, 0.10584), day_73 = c(NA, NA,
>> >     0.62748, 14.39748, 7.50708), day_74 = c(NA, NA, 7.20252,
>> >     20.02644, 1.07244), day_75 = c(NA, NA, 1.87488, 0, 0), day_76 =
>> c(NA,
>> >     NA, 0.26784, 0, 0), day_77 = c(NA, NA, 0, 0, 0), day_78 = c(NA,
>> >     NA, 0, 0, 2.81664), day_79 = c(NA, NA, 0, 0, 0), day_80 = c(NA,
>> >     NA, 0, 0, 0), day_81 = c(NA, NA, 0, 0, 0), day_82 = c(NA,
>> >     NA, 1.29276, 0, 0), day_83 = c(NA, NA, 0.18468, 0, 1.46124
>> >     ), day_84 = c(NA, NA, 0, 0, 0), day_85 = c(NA, NA, 0, 0,
>> >     0), day_86 = c(NA, NA, 57.36528, 0, 0), day_87 = c(NA, NA,
>> >     8.19504, 0, 0), day_88 = c(NA, NA, 0, 0, 0), day_89 = c(NA,
>> >     NA, 6.45732, 0, 0), day_90 = c(NA, NA, 0, 0, 0), day_91 = c(NA,
>> >     NA, 0, 0, 44.20332), day_92 = c(NA, NA, 0, 0, 6.31476), day_93 =
>> c(NA,
>> >     NA, 0, 0, 0.35748), day_94 = c(NA, NA, 16.74972, 30.35988,
>> >     5.0436), day_95 = c(NA, NA, 4.93992, 1.46556, 19.86768),
>> >     day_96 = c(NA, NA, 0, 0, 0.88128), day_97 = c(NA, NA, 5.751,
>> >     19.02096, 0), day_98 = c(NA, NA, 11.5452, 13.37148, 0), day_99 =
>> c(NA,
>> >     NA, 0, 0, 0), day_100 = c(NA, NA, 0, 0, 0), day_101 = c(NA,
>> >     NA, 4.70124, 23.80644, 7.61832), day_102 = c(NA, NA, 0, 1.02492,
>> >     0), day_103 = c(NA, NA, 0, 0, 15.86304), day_104 = c(NA,
>> >     NA, 0, 0, 0.26352), day_105 = c(NA, NA, 0, 0, 21.60864),
>> >     day_106 = c(NA, NA, 56.93436, 0, 0.22464), day_107 = c(NA,
>> >     NA, 8.13348, 0, 0), day_108 = c(NA, NA, 6.83748, 0, 0), day_109 =
>> c(NA,
>> >     NA, 0, 0, 0), day_110 = c(NA, NA, 14.36724, 0, 0), day_111 = c(NA,
>> >     NA, 0.63936, 2.43864, 4.0554), day_112 = c(NA, NA, 1.21392,
>> >     1.15452, 0), day_113 = c(NA, NA, 0.7722, 0, 0), day_114 = c(NA,
>> >     NA, 0, 0, 1.08864), day_115 = c(NA, NA, 1.47528, 0, 0), day_116 =
>> c(NA,
>> >     NA, 0, 1.73124, 0), day_117 = c(NA, NA, 0, 0, 0), day_118 = c(NA,
>> >     NA, 2.4516, 0, 0), day_119 = c(NA, NA, 0, 3.14388, 0), day_120 =
>> c(NA,
>> >     NA, 1.81872, 0, 0), day_121 = c(NA, NA, 2.77236, 0, 0), day_122 =
>> c(NA,
>> >     NA, 1.34028, 0.70632, 0), day_123 = c(NA, NA, 0, 0, 0), day_124 =
>> c(NA,
>> >     NA, 0, 0, 0), day_125 = c(NA, NA, 0.56484, 0.74412, 0), day_126 =
>> c(NA,
>> >     NA, 1.11888, 0.06264, 0), day_127 = c(NA, NA, 0, 0, 0), day_128 =
>> c(NA,
>> >     NA, 1.05624, 0, 0), day_129 = c(NA, NA, 26.63928, 34.04268,
>> >     0), day_130 = c(NA, NA, 6.89796, 0, 0), day_131 = c(NA, NA,
>> >     1.91592, 2.241, 0), day_132 = c(NA, NA, 0, 2.23668, 45.23904
>> >     ), day_133 = c(NA, NA, 0, 0, 6.46272), day_134 = c(NA, NA,
>> >     0, 0, 0), day_135 = c(NA, NA, 0, 0, 0), day_136 = c(NA, NA,
>> >     0, 0, 0), day_137 = c(NA, NA, 0, 0, 0), day_138 = c(NA, NA,
>> >     0, 0, 0), day_139 = c(NA, NA, 0, 0, 0), day_140 = c(NA, NA,
>> >     0, 0, 0), day_141 = c(NA, NA, 0, 0, 0), day_142 = c(NA, NA,
>> >     0, 0, 0), day_143 = c(NA, NA, 0, 0, 0), day_144 = c(NA, NA,
>> >     2.943, 5.17536, 0), day_145 = c(NA, NA, 0, 0, 0), day_146 = c(NA,
>> >     NA, 0, 0, 0), day_147 = c(NA, NA, 0, 0, 0), day_148 = c(NA,
>> >     NA, 10.96308, 2.98188, 0), day_149 = c(NA, NA, 20.4822, 0.43632,
>> >     0), day_150 = c(NA, NA, 1.5282, 0, 0), day_151 = c(NA, NA,
>> >     0, 0, 0), day_152 = c(NA, NA, 0, 0, 0), day_153 = c(NA, NA,
>> >     0, 0, 0), day_154 = c(NA, NA, 0, 0, 0), day_155 = c(NA, NA,
>> >     0, 0, 0), day_156 = c(NA, NA, 0, 0, 0), day_157 = c(NA, NA,
>> >     0, 0, 0), day_158 = c(NA, NA, 0, 0, 0), day_159 = c(NA, NA,
>> >     0, 0, 0), day_160 = c(NA, NA, 0, 0, 0), day_161 = c(NA, NA,
>> >     0, 0, 0), day_162 = c(NA, NA, 0, 0, 0), day_163 = c(NA, NA,
>> >     0, 26.3412, 4.07376), day_164 = c(NA, NA, 0, 4.28328, 3.03156
>> >     ), day_165 = c(NA, NA, 0, 0, 0), day_166 = c(NA, NA, 0, 0,
>> >     4.60404), day_167 = c(NA, NA, 0, 0, 0.70848), day_168 = c(NA,
>> >     NA, 0, 0, 0), day_169 = c(NA, NA, 0, 0, 0), day_170 = c(NA,
>> >     NA, 0, 0, 0), day_171 = c(NA, NA, 0, 0, 0), day_172 = c(NA,
>> >     NA, 0, 0, 0), day_173 = c(NA, NA, 0, 0, 3.7854), day_174 = c(NA,
>> >     NA, 0, 0, 0), day_175 = c(NA, NA, 0, 0, 0), day_176 = c(NA,
>> >     NA, 0, 0, 0), day_177 = c(NA, NA, 0, 0, 0), day_178 = c(NA,
>> >     NA, 0, 0, 0), day_179 = c(NA, NA, 0, 0, 0), day_180 = c(NA,
>> >     NA, 0, 0, 0), day_181 = c(NA, NA, 0, 0, 0), day_182 = c(NA,
>> >     NA, 0, 0, 0), day_183 = c(NA, NA, 0, 0, 0), day_184 = c(NA,
>> >     NA, 0, 0, 0), day_185 = c(NA, NA, 0, 0, 0), day_186 = c(NA,
>> >     NA, 0, 0, 0), day_187 = c(NA, NA, 7.30728, 4.1202, 0), day_188 =
>> c(NA,
>> >     NA, 2.56608, 0.5886, 0), day_189 = c(NA, NA, 0, 0, 0), day_190 =
>> c(NA,
>> >     NA, 21.93156, 8.0082, 11.4318), day_191 = c(NA, NA, 3.13308,
>> >     0, 0), day_192 = c(NA, NA, 0, 0, 0.10692), day_193 = c(NA,
>> >     NA, 0, 0, 4.65912), day_194 = c(NA, NA, 0, 0, 0), day_195 = c(NA,
>> >     NA, 0, 0, 0), day_196 = c(NA, NA, 0, 0, 0), day_197 = c(NA,
>> >     NA, 0, 0, 0), day_198 = c(NA, NA, 0, 0, 0), day_199 = c(NA,
>> >     NA, 0, 0, 0), day_200 = c(NA, NA, 0, 0, 0), day_201 = c(NA,
>> >     NA, 0, 0, 0), day_202 = c(NA, NA, 0, 7.77276, 4.6602), day_203 =
>> c(NA,
>> >     NA, 0, 0.86292, 0), day_204 = c(NA, NA, 0, 0, 0), day_205 = c(NA,
>> >     NA, 21.45528, 8.69616, 0), day_206 = c(NA, NA, 0, 0, 0),
>> >     day_207 = c(NA, NA, 0, 0, 0), day_208 = c(NA, NA, 0, 0, 0
>> >     ), day_209 = c(NA, NA, 0, 0, 0), day_210 = c(NA, NA, 0, 0,
>> >     0), day_211 = c(NA, NA, 0, 0, 0), day_212 = c(NA, NA, 0,
>> >     0, 0), day_213 = c(NA, NA, 0, 0, 0), day_214 = c(NA, NA,
>> >     0, 0, 0), day_215 = c(NA, NA, 0, 0, 0), day_216 = c(NA, NA,
>> >     0, 0, 0), day_217 = c(NA, NA, 0, 0, 0), day_218 = c(NA, NA,
>> >     0, 0, 0), day_219 = c(NA, NA, 0, 0, 0), day_220 = c(NA, NA,
>> >     6.10092, 10.85508, 13.22244), day_221 = c(NA, NA, 0.87156,
>> >     0, 0), day_222 = c(NA, NA, 0, 0, 15.46452), day_223 = c(NA,
>> >     NA, 0, 0, 9.83664), day_224 = c(NA, NA, 0, 0, 0), day_225 = c(NA,
>> >     NA, 0, 0, 0), day_226 = c(NA, NA, 16.46028, 0, 0), day_227 = c(NA,
>> >     NA, 0, 0, 0), day_228 = c(NA, NA, 0, 0, 0), day_229 = c(NA,
>> >     NA, 0, 0, 0), day_230 = c(NA, NA, 0, 0, 0), day_231 = c(NA,
>> >     NA, 2.7108, 0, 0), day_232 = c(NA, NA, 0, 0, 0), day_233 = c(NA,
>> >     NA, 0, 0, 0), day_234 = c(NA, NA, 0, 0, 0), day_235 = c(NA,
>> >     NA, 0, 0, 0), day_236 = c(NA, NA, 0, 0, 3.5586), day_237 = c(NA,
>> >     NA, 0, 0, 0), day_238 = c(NA, NA, 0, 0, 0), day_239 = c(NA,
>> >     NA, 10.23192, 0, 0), day_240 = c(NA, NA, 0, 0, 0), day_241 = c(NA,
>> >     NA, 0, 0, 0), day_242 = c(NA, NA, 0, 0, 0), day_243 = c(NA,
>> >     NA, 0, 0, 0), day_244 = c(NA, NA, 0, 0, 0), day_245 = c(NA,
>> >     NA, 0, 0, 0), day_246 = c(NA, NA, 0, 0, 0), day_247 = c(NA,
>> >     NA, 0, 0, 0), day_248 = c(NA, NA, 0, 0, 0), day_249 = c(NA,
>> >     NA, 0.50544, 0, 0), day_250 = c(NA, NA, 0.12636, 0, 0), day_251 =
>> c(NA,
>> >     NA, 7.02432, 0, 5.39784), day_252 = c(NA, NA, 3.33828, 8.00064,
>> >     7.08372), day_253 = c(NA, NA, 0, 0, 0), day_254 = c(NA, NA,
>> >     0, 0, 0), day_255 = c(NA, NA, 2.5704, 4.71636, 11.99772),
>> >     day_256 = c(NA, NA, 0.3672, 0.75384, 0), day_257 = c(NA,
>> >     NA, 0, 0, 0), day_258 = c(NA, NA, 0.50328, 0, 0), day_259 = c(NA,
>> >     NA, 6.78888, 0, 0), day_260 = c(NA, NA, 0.96984, 0, 0), day_261 =
>> c(NA,
>> >     NA, 4.62672, 0, 0), day_262 = c(NA, NA, 0, 0, 0), day_263 = c(NA,
>> >     NA, 3.16224, 0.27864, 0), day_264 = c(NA, NA, 0, 1.31112,
>> >     0), day_265 = c(NA, NA, 0.37692, 0, 0), day_266 = c(NA, NA,
>> >     0, 0, 0), day_267 = c(NA, NA, 0.70524, 0.43524, 0), day_268 = c(NA,
>> >     NA, 0.18792, 0.12744, 0), day_269 = c(NA, NA, 0, 1.79064,
>> >     0.96012), day_270 = c(NA, NA, 0, 0, 0.58644), day_271 = c(NA,
>> >     NA, 4.4982, 0, 0), day_272 = c(NA, NA, 0, 0, 0), day_273 = c(NA,
>> >     NA, 2.04552, 6.56964, 0), day_274 = c(NA, NA, 0.71712, 0.93852,
>> >     0), day_275 = c(NA, NA, 0, 0, 0), day_276 = c(NA, NA, 0,
>> >     0, 0), day_277 = c(NA, NA, 3.31452, 0, 0), day_278 = c(NA,
>> >     NA, 1.20204, 0, 0), day_279 = c(NA, NA, 0, 0, 0), day_280 = c(NA,
>> >     NA, 0, 0, 0), day_281 = c(NA, NA, 0, 0, 0), day_282 = c(NA,
>> >     NA, 17.955, 5.7942, 9.93816), day_283 = c(NA, NA, 4.79304,
>> >     4.8006, 0), day_284 = c(NA, NA, 3.9366, 0.78084, 0), day_285 = c(NA,
>> >     NA, 0, 0, 0), day_286 = c(NA, NA, 0, 0, 0), day_287 = c(NA,
>> >     NA, 0, 0, 0), day_288 = c(NA, NA, 0, 0, 0), day_289 = c(NA,
>> >     NA, 0, 0, 0), day_290 = c(NA, NA, 0, 0, 0), day_291 = c(NA,
>> >     NA, 0, 0, 0), day_292 = c(NA, NA, 0, 0, 0), day_293 = c(NA,
>> >     NA, 1.55736, 0, 0), day_294 = c(NA, NA, 4.28328, 0, 0), day_295 =
>> c(NA,
>> >     NA, 0, 0, 0), day_296 = c(NA, NA, 0, 0, 0), day_297 = c(NA,
>> >     NA, 1.6362, 0, 0), day_298 = c(NA, NA, 1.28844, 0, 6.14088
>> >     ), day_299 = c(NA, NA, 0, 0, 0.50112), day_300 = c(NA, NA,
>> >     0, 0, 0), day_301 = c(NA, NA, 0, 0.13824, 0.03456), day_302 = c(NA,
>> >     NA, 0, 2.92572, 9.24264), day_303 = c(NA, NA, 2.8188, 0.41796,
>> >     0), day_304 = c(NA, NA, 2.04876, 11.28384, 0), day_305 = c(NA,
>> >     NA, 0, 0.3564, 0), day_306 = c(NA, NA, 0, 0, 0), day_307 = c(NA,
>> >     NA, 0, 2.36736, 0), day_308 = c(NA, NA, 0, 0, 0), day_309 = c(NA,
>> >     NA, 34.91856, 20.42604, 0), day_310 = c(NA, NA, 0, 0, 0),
>> >     day_311 = c(NA, NA, 0, 0, 0), day_312 = c(NA, NA, 0, 0.40392,
>> >     0), day_313 = c(NA, NA, 0, 0.5292, 0), day_314 = c(NA, NA,
>> >     0, 0, 5.21424), day_315 = c(NA, NA, 0, 0, 0), day_316 = c(NA,
>> >     NA, 0, 0, 0.4266), day_317 = c(NA, NA, 0, 0, 0), day_318 = c(NA,
>> >     NA, 0, 0, 0), day_319 = c(NA, NA, 0, 0, 0), day_320 = c(NA,
>> >     NA, 0.23436, 0.6048, 14.9256), day_321 = c(NA, NA, 0, 0.10908,
>> >     0), day_322 = c(NA, NA, 7.68096, 6.66036, 4.53924), day_323 = c(NA,
>> >     NA, 1.09728, 1.59732, 8.51148), day_324 = c(NA, NA, 0, 0,
>> >     0), day_325 = c(NA, NA, 1.46016, 0, 0), day_326 = c(NA, NA,
>> >     0, 0, 8.70048), day_327 = c(NA, NA, 0, 0, 0), day_328 = c(NA,
>> >     NA, 0, 0, 0), day_329 = c(NA, NA, 0, 0, 0), day_330 = c(NA,
>> >     NA, 5.3082, 0, 0), day_331 = c(NA, NA, 2.5866, 0, 0), day_332 =
>> c(NA,
>> >     NA, 8.03628, 6.3666, 4.3308), day_333 = c(NA, NA, 0, 0, 0
>> >     ), day_334 = c(NA, NA, 0, 0, 0), day_335 = c(NA, NA, 0, 0,
>> >     0), day_336 = c(NA, NA, 0, 0, 5.29632), day_337 = c(NA, NA,
>> >     0, 1.77444, 2.7216), day_338 = c(NA, NA, 0.40608, 0, 0.83052
>> >     ), day_339 = c(NA, NA, 0, 0, 0), day_340 = c(NA, NA, 0, 0,
>> >     0), day_341 = c(NA, NA, 0, 0, 0), day_342 = c(NA, NA, 0,
>> >     0, 0), day_343 = c(NA, NA, 0, 0, 0), day_344 = c(NA, NA,
>> >     7.73388, 0, 0), day_345 = c(NA, NA, 4.80384, 0, 0), day_346 = c(NA,
>> >     NA, 4.374, 0.09288, 0), day_347 = c(NA, NA, 6.42924, 3.2022,
>> >     0), day_348 = c(NA, NA, 0, 16.27668, 0), day_349 = c(NA,
>> >     NA, 0, 0.90072, 9.36684), day_350 = c(NA, NA, 0.135, 1.87272,
>> >     2.49048), day_351 = c(NA, NA, 0, 0, 0), day_352 = c(NA, NA,
>> >     0, 0, 0), day_353 = c(NA, NA, 0, 0, 0), day_354 = c(NA, NA,
>> >     0, 0, 0), day_355 = c(NA, NA, 0, 0, 0), day_356 = c(NA, NA,
>> >     0, 0, 0), day_357 = c(NA, NA, 2.82636, 39.45348, 26.08848
>> >     ), day_358 = c(NA, NA, 0, 0, 22.8582), day_359 = c(NA, NA,
>> >     0, 0, 1.34028), day_360 = c(NA, NA, 30.03804, 0, 3.49704),
>> >     day_361 = c(NA, NA, 0.13392, 4.941, 4.94424), day_362 = c(NA,
>> >     NA, 0.92016, 0, 0.70632), day_363 = c(NA, NA, 0, 0, 0), day_364 =
>> c(NA,
>> >     NA, 0, 0, 0), day_365 = c(NA, NA, 0, 0, 0), day_366 = c(NA,
>> >     NA, 21.42072, 0, 0)), .Names = c("ISO3", "lon", "lat", "day_1",
>> "day_2",
>> > "day_3", "day_4", "day_5", "day_6", "day_7", "day_8", "day_9", "day_10",
>> > "day_11", "day_12", "day_13", "day_14", "day_15", "day_16", "day_17",
>> > "day_18", "day_19", "day_20", "day_21", "day_22", "day_23", "day_24",
>> > "day_25", "day_26", "day_27", "day_28", "day_29", "day_30", "day_31",
>> > "day_32", "day_33", "day_34", "day_35", "day_36", "day_37", "day_38",
>> > "day_39", "day_40", "day_41", "day_42", "day_43", "day_44", "day_45",
>> > "day_46", "day_47", "day_48", "day_49", "day_50", "day_51", "day_52",
>> > "day_53", "day_54", "day_55", "day_56", "day_57", "day_58", "day_59",
>> > "day_60", "day_61", "day_62", "day_63", "day_64", "day_65", "day_66",
>> > "day_67", "day_68", "day_69", "day_70", "day_71", "day_72", "day_73",
>> > "day_74", "day_75", "day_76", "day_77", "day_78", "day_79", "day_80",
>> > "day_81", "day_82", "day_83", "day_84", "day_85", "day_86", "day_87",
>> > "day_88", "day_89", "day_90", "day_91", "day_92", "day_93", "day_94",
>> > "day_95", "day_96", "day_97", "day_98", "day_99", "day_100", "day_101",
>> > "day_102", "day_103", "day_104", "day_105", "day_106", "day_107",
>> > "day_108", "day_109", "day_110", "day_111", "day_112", "day_113",
>> > "day_114", "day_115", "day_116", "day_117", "day_118", "day_119",
>> > "day_120", "day_121", "day_122", "day_123", "day_124", "day_125",
>> > "day_126", "day_127", "day_128", "day_129", "day_130", "day_131",
>> > "day_132", "day_133", "day_134", "day_135", "day_136", "day_137",
>> > "day_138", "day_139", "day_140", "day_141", "day_142", "day_143",
>> > "day_144", "day_145", "day_146", "day_147", "day_148", "day_149",
>> > "day_150", "day_151", "day_152", "day_153", "day_154", "day_155",
>> > "day_156", "day_157", "day_158", "day_159", "day_160", "day_161",
>> > "day_162", "day_163", "day_164", "day_165", "day_166", "day_167",
>> > "day_168", "day_169", "day_170", "day_171", "day_172", "day_173",
>> > "day_174", "day_175", "day_176", "day_177", "day_178", "day_179",
>> > "day_180", "day_181", "day_182", "day_183", "day_184", "day_185",
>> > "day_186", "day_187", "day_188", "day_189", "day_190", "day_191",
>> > "day_192", "day_193", "day_194", "day_195", "day_196", "day_197",
>> > "day_198", "day_199", "day_200", "day_201", "day_202", "day_203",
>> > "day_204", "day_205", "day_206", "day_207", "day_208", "day_209",
>> > "day_210", "day_211", "day_212", "day_213", "day_214", "day_215",
>> > "day_216", "day_217", "day_218", "day_219", "day_220", "day_221",
>> > "day_222", "day_223", "day_224", "day_225", "day_226", "day_227",
>> > "day_228", "day_229", "day_230", "day_231", "day_232", "day_233",
>> > "day_234", "day_235", "day_236", "day_237", "day_238", "day_239",
>> > "day_240", "day_241", "day_242", "day_243", "day_244", "day_245",
>> > "day_246", "day_247", "day_248", "day_249", "day_250", "day_251",
>> > "day_252", "day_253", "day_254", "day_255", "day_256", "day_257",
>> > "day_258", "day_259", "day_260", "day_261", "day_262", "day_263",
>> > "day_264", "day_265", "day_266", "day_267", "day_268", "day_269",
>> > "day_270", "day_271", "day_272", "day_273", "day_274", "day_275",
>> > "day_276", "day_277", "day_278", "day_279", "day_280", "day_281",
>> > "day_282", "day_283", "day_284", "day_285", "day_286", "day_287",
>> > "day_288", "day_289", "day_290", "day_291", "day_292", "day_293",
>> > "day_294", "day_295", "day_296", "day_297", "day_298", "day_299",
>> > "day_300", "day_301", "day_302", "day_303", "day_304", "day_305",
>> > "day_306", "day_307", "day_308", "day_309", "day_310", "day_311",
>> > "day_312", "day_313", "day_314", "day_315", "day_316", "day_317",
>> > "day_318", "day_319", "day_320", "day_321", "day_322", "day_323",
>> > "day_324", "day_325", "day_326", "day_327", "day_328", "day_329",
>> > "day_330", "day_331", "day_332", "day_333", "day_334", "day_335",
>> > "day_336", "day_337", "day_338", "day_339", "day_340", "day_341",
>> > "day_342", "day_343", "day_344", "day_345", "day_346", "day_347",
>> > "day_348", "day_349", "day_350", "day_351", "day_352", "day_353",
>> > "day_354", "day_355", "day_356", "day_357", "day_358", "day_359",
>> > "day_360", "day_361", "day_362", "day_363", "day_364", "day_365",
>> > "day_366"), row.names = c(NA, 5L), class =
>> > "data.frame")
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-
>> > guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ________________________________
>> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
>> ur?eny pouze jeho adres?t?m.
>> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
>> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
>> vyma?te ze sv?ho syst?mu.
>> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento
>> email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
>> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
>> ?i zpo?d?n?m p?enosu e-mailu.
>>
>> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
>> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
>> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
>> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
>> p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze
>> strany p??jemce s dodatkem ?i odchylkou.
>> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
>> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
>> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
>> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
>> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
>> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
>> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>>
>> This e-mail and any documents attached to it may be confidential and are
>> intended only for its intended recipients.
>> If you received this e-mail by mistake, please immediately inform its
>> sender. Delete the contents of this e-mail with all attachments and its
>> copies from your system.
>> If you are not the intended recipient of this e-mail, you are not
>> authorized to use, disseminate, copy or disclose this e-mail in any manner.
>> The sender of this e-mail shall not be liable for any possible damage
>> caused by modifications of the e-mail or by delay with transfer of the
>> email.
>>
>> In case that this e-mail forms part of business dealings:
>> - the sender reserves the right to end negotiations about entering into a
>> contract in any time, for any reason, and without stating any reasoning.
>> - if the e-mail contains an offer, the recipient is entitled to
>> immediately accept such offer; The sender of this e-mail (offer) excludes
>> any acceptance of the offer on the part of the recipient containing any
>> amendment or variation.
>> - the sender insists on that the respective contract is concluded only
>> upon an express mutual agreement on all its aspects.
>> - the sender of this e-mail informs that he/she is not authorized to
>> enter into any contracts on behalf of the company except for cases in which
>> he/she is expressly authorized to do so in writing, and such authorization
>> or power of attorney is submitted to the recipient or the person
>> represented by the recipient, or the existence of such authorization is
>> known to the recipient of the person represented by the recipient.
>>
>
>

	[[alternative HTML version deleted]]


From Matthias.Weber at fntsoftware.com  Fri Nov 18 16:16:27 2016
From: Matthias.Weber at fntsoftware.com (Matthias Weber)
Date: Fri, 18 Nov 2016 15:16:27 +0000
Subject: [R] summing up a matrix
Message-ID: <HE1PR07MB1212409BCC9587509F41EB6390B00@HE1PR07MB1212.eurprd07.prod.outlook.com>

Hello together,

is it possible, to summing up a matrix?
I have the following matrix at the moment:

                                [,1]         [,2]         [,3]        [,4]         [,5]         [,6]
2016-11                20            200        100         50           100         30
2016-12                100          200        100         50           100         30
2017-01                50            200        100         50           100         30

Now I want to summing up the matrix in a new matrix.

The result should look like the following:

                                [,1]         [,2]         [,3]        [,4]         [,5]         [,6]
2016-11                20            220        320         370         470         500
2016-12                100          300        400         450         550         580
2017-01                50            250        350         400         500         530

Is it possible, to create that?

Thanks for your help.

Best regards.

Mat


	[[alternative HTML version deleted]]


From steinert.janina at gmail.com  Fri Nov 18 13:04:12 2016
From: steinert.janina at gmail.com (Janina Steinert)
Date: Fri, 18 Nov 2016 12:04:12 +0000
Subject: [R] Multi-level Meta-Regression using metafor
Message-ID: <CAFR6YZ4MFbZXoiKtSQ5vD4yrHFA4HRsuwOXZPRKD706YfitPqQ@mail.gmail.com>

Hi!

I am running a multi-level meta-regression in R using the metafor package.
I have specified a univariate multi-level meta-regression as follows:

Test_MR <- rma.mv(yi = effectsize_estimates, V = effsize_sd2, data = Test,
                  mods = ~ x, random = list(~ 1 | coeff, ~ 1 | study))

Can someone help explain what this code specifies exactly? Can I define
this as a mixed-effects model whereby the moderator X is a fixed effect and
random effects denote heterogeneity of effect sizes within study clusters
and between clusters?

Can someone explain what the ~1 at the end of the line specifies?
Thank you!

	[[alternative HTML version deleted]]


From stuartjpatterson at googlemail.com  Fri Nov 18 15:56:18 2016
From: stuartjpatterson at googlemail.com (Stuart Patterson)
Date: Fri, 18 Nov 2016 14:56:18 +0000
Subject: [R] Presenting Hazard ratios for interacting variables in a Cox
	model
Message-ID: <CAJvq0dBT+eeZ39g2RSk7KW4+Mdi4_Le-Kc2X0gM4GgZp=6oMzA@mail.gmail.com>

I have a time-dependent cox model with three variables, each of which
interacts with the other two. So my final model is:

fit12<-coxph(formula = Surv(data$TimeIn, data$Timeout, data$Status) ~ data$
Year+data$Life_Stg+data$prev.tb +data$prev.tb*data$Life_Stg + data$Year*data
$Life_Stg + data$Year*data$prev.tb + frailty(data$Natal_Group), data = data)

For my variables, there are 3 categories of year, three of year, and
prev.tb is a binary variable. Because of the interactions, when I present
the results, I want to present the Hazard ratio, 95% CI, and p value for
each combination of the three variables. How do I get R to give me these
values please?

I think that the contrast function does this for other models but does not
work for coxph?

Grateful for any suggestions

Best wishes

Stuart Patterson, Royal Veterinary College, University of London

	[[alternative HTML version deleted]]


From svdpas at math.leidenuniv.nl  Wed Nov 16 21:15:07 2016
From: svdpas at math.leidenuniv.nl (S.L. van der Pas)
Date: Wed, 16 Nov 2016 14:15:07 -0600
Subject: [R] [R-pkgs] Announcing new package horseshoe
Message-ID: <D8C8E847-8FD6-4BEE-864A-4A07DFBCD877@math.leidenuniv.nl>

Dear R users,

The first version of the package ?horseshoe' is now available on CRAN. It contains various functions for implementing the horseshoe prior for sparse linear regression, and faster versions for the special case of the normal means problem.

The functions output, among other things, the horseshoe estimator (posterior mean), credible intervals, and there is a function to perform variable selection.

The package can be installed by typing install.packages{?horseshoe?)

The page for the package:

https://CRAN.R-project.org/package=horseshoe <https://cran.r-project.org/package=horseshoe>

The manual contains examples for each function:

https://cran.r-project.org/web/packages/horseshoe/horseshoe.pdf <https://cran.r-project.org/web/packages/horseshoe/horseshoe.pdf>

Comments and suggestions are very welcome.

Best wishes,

St?phanie van der Pas, James Scott, Antik Chakraborty and Anirban Bhattacharya



	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages

From lists at dewey.myzen.co.uk  Fri Nov 18 16:40:31 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Fri, 18 Nov 2016 15:40:31 +0000
Subject: [R] Multi-level Meta-Regression using metafor
In-Reply-To: <CAFR6YZ4MFbZXoiKtSQ5vD4yrHFA4HRsuwOXZPRKD706YfitPqQ@mail.gmail.com>
References: <CAFR6YZ4MFbZXoiKtSQ5vD4yrHFA4HRsuwOXZPRKD706YfitPqQ@mail.gmail.com>
Message-ID: <6a775cbd-5a26-27a2-eddf-97c12a0dbaae@dewey.myzen.co.uk>

Dear Janina

On 18/11/2016 12:04, Janina Steinert wrote:
> Hi!
>
> I am running a multi-level meta-regression in R using the metafor package.
> I have specified a univariate multi-level meta-regression as follows:
>
> Test_MR <- rma.mv(yi = effectsize_estimates, V = effsize_sd2, data = Test,
>                   mods = ~ x, random = list(~ 1 | coeff, ~ 1 | study))
>
> Can someone help explain what this code specifies exactly? Can I define
> this as a mixed-effects model whereby the moderator X is a fixed effect and
> random effects denote heterogeneity of effect sizes within study clusters
> and between clusters?
>
> Can someone explain what the ~1 at the end of the line specifies?

The ~ sign is part of a formula and the 1 means the intercept so you are 
saying you want a random intercept for each coeff and for each study. 
You are correct that x is the fixed effect of the moderator.

Have you looked at some of the examples on the metafor website? There is 
a whole range there which cover some of the issues you raise. Point your 
browser at http://www.metafor-project.org/ and then look for the tips 
and for the analyses in the navigation panel.

> Thank you!
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From lists at dewey.myzen.co.uk  Fri Nov 18 16:42:26 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Fri, 18 Nov 2016 15:42:26 +0000
Subject: [R] summing up a matrix
In-Reply-To: <HE1PR07MB1212409BCC9587509F41EB6390B00@HE1PR07MB1212.eurprd07.prod.outlook.com>
References: <HE1PR07MB1212409BCC9587509F41EB6390B00@HE1PR07MB1212.eurprd07.prod.outlook.com>
Message-ID: <208f35ad-8eb8-5ee7-78d6-40e05525012e@dewey.myzen.co.uk>

Dear Matthias

It is rather hard to read that since you posted in HTML which scrambles 
things but I think

?cumsum

may help and possibly

?apply

as you seem to want to work by rows.

On 18/11/2016 15:16, Matthias Weber wrote:
> Hello together,
>
> is it possible, to summing up a matrix?
> I have the following matrix at the moment:
>
>                                 [,1]         [,2]         [,3]        [,4]         [,5]         [,6]
> 2016-11                20            200        100         50           100         30
> 2016-12                100          200        100         50           100         30
> 2017-01                50            200        100         50           100         30
>
> Now I want to summing up the matrix in a new matrix.
>
> The result should look like the following:
>
>                                 [,1]         [,2]         [,3]        [,4]         [,5]         [,6]
> 2016-11                20            220        320         370         470         500
> 2016-12                100          300        400         450         550         580
> 2017-01                50            250        350         400         500         530
>
> Is it possible, to create that?
>
> Thanks for your help.
>
> Best regards.
>
> Mat
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From dcarlson at tamu.edu  Fri Nov 18 16:42:53 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Fri, 18 Nov 2016 15:42:53 +0000
Subject: [R] summing up a matrix
In-Reply-To: <HE1PR07MB1212409BCC9587509F41EB6390B00@HE1PR07MB1212.eurprd07.prod.outlook.com>
References: <HE1PR07MB1212409BCC9587509F41EB6390B00@HE1PR07MB1212.eurprd07.prod.outlook.com>
Message-ID: <0a0bfc8337d24e528542263a998f2966@exch-2p-mbx-w2.ads.tamu.edu>

You should read some of the free tutorials about R. Also use dput() to send your data and plain text (no html) emails.

> dput(mtx)
structure(c(20L, 100L, 50L, 200L, 200L, 200L, 100L, 100L, 100L, 
50L, 50L, 50L, 100L, 100L, 100L, 30L, 30L, 30L), .Dim = c(3L, 
6L), .Dimnames = list(c("2016-11", "2016-12", "2017-01"), NULL))
> t(apply(mtx, 1, cumsum))
        [,1] [,2] [,3] [,4] [,5] [,6]
2016-11   20  220  320  370  470  500
2016-12  100  300  400  450  550  580
2017-01   50  250  350  400  500  530

Use ?dput, ?t, ?apply, and ?cumsum to read the manual pages for these functions.

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Matthias Weber
Sent: Friday, November 18, 2016 9:16 AM
To: 'r-help at r-project.org'
Subject: [R] summing up a matrix

Hello together,

is it possible, to summing up a matrix?
I have the following matrix at the moment:

                                [,1]         [,2]         [,3]        [,4]         [,5]         [,6]
2016-11                20            200        100         50           100         30
2016-12                100          200        100         50           100         30
2017-01                50            200        100         50           100         30

Now I want to summing up the matrix in a new matrix.

The result should look like the following:

                                [,1]         [,2]         [,3]        [,4]         [,5]         [,6]
2016-11                20            220        320         370         470         500
2016-12                100          300        400         450         550         580
2017-01                50            250        350         400         500         530

Is it possible, to create that?

Thanks for your help.

Best regards.

Mat


	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From bran.chri at gmail.com  Fri Nov 18 17:03:42 2016
From: bran.chri at gmail.com (=?UTF-8?Q?Christian_Brandst=c3=a4tter?=)
Date: Fri, 18 Nov 2016 17:03:42 +0100
Subject: [R] summing up a matrix
In-Reply-To: <HE1PR07MB1212409BCC9587509F41EB6390B00@HE1PR07MB1212.eurprd07.prod.outlook.com>
References: <HE1PR07MB1212409BCC9587509F41EB6390B00@HE1PR07MB1212.eurprd07.prod.outlook.com>
Message-ID: <5f9005c5-57d8-21f6-0bbd-341f8f285167@gmail.com>

Hi,

that should do:

mat <- rbind(c(20,200,100,50,100,30),
c(100,200,100,50,100,30),
c(50,200,100,50,100,30))
rownames(mat) <- c("2016-11","2016-12","2017-01")
t(apply(mat,1,cumsum))

Best,
Christian

Am 18.11.2016 um 16:16 schrieb Matthias Weber:
> Hello together,
>
> is it possible, to summing up a matrix?
> I have the following matrix at the moment:
>
>                                 [,1]         [,2]         [,3]        [,4]         [,5]         [,6]
> 2016-11                20            200        100         50           100         30
> 2016-12                100          200        100         50           100         30
> 2017-01                50            200        100         50           100         30
>
> Now I want to summing up the matrix in a new matrix.
>
> The result should look like the following:
>
>                                 [,1]         [,2]         [,3]        [,4]         [,5]         [,6]
> 2016-11                20            220        320         370         470         500
> 2016-12                100          300        400         450         550         580
> 2017-01                50            250        350         400         500         530
>
> Is it possible, to create that?
>
> Thanks for your help.
>
> Best regards.
>
> Mat
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From carlo-giovanni.camarda at ined.fr  Fri Nov 18 17:30:37 2016
From: carlo-giovanni.camarda at ined.fr (Carlo Giovanni Camarda)
Date: Fri, 18 Nov 2016 17:30:37 +0100
Subject: [R] numerical approximation of survival function
Message-ID: <979c04d7-2a3d-cad9-2ab8-bdf49623b9ec@ined.fr>

Dear R-users,

my question concerns numerical approximation and, somehow, survival 
analysis.
Let?s assume we have a density function and we aim to numerically 
compute the hazard, which is, in theory, the ratio between density and 
survival.
In the following example, I take a pdf from a log-normal and proceed as 
I would have known only this function. I numerically approximate the 
survival function by splinefun() and integrate().Finally I compute the 
hazard. It?s easy to see that (1) true hazard is different from the 
numerically approximated one and (2) discrepancy depends upon the 
approximation carried out to compute the survival. Is there any way to 
overcome or attenuate this issue? Or do I just miss something obvious?
In my real study I do not deal with analytically defined pdf, so I would 
like to be sure that approximation is done at its best.

Thanks in advance for your help.
Giancarlo Camarda


## x-values
delta <- 0.1
x <- seq(0, 200, by=delta)
m <- length(x)
## true pdf
fxT <- dlnorm(x, meanlog=3.5, sdlog=0.5)
fxT <- fxT/sum(fxT)
## true survival
SxT <- 1 - plnorm(x, meanlog=3.5, sdlog=0.5)
## true hazard
hxT <- fxT/SxT
## numerical approximation of the pdf
fx <- spline(x, fxT, n=m)$y
## numerical approximation of the survival
Sx <- numeric(m)
fxfun <- splinefun(x, fx/delta)
for(j in 1:m){
   Sx[j] <- 1 - integrate(fxfun, x[1], x[j])$value
}
## hazard: pdf divided by survival
hx <- fx/Sx ## both numerical approx
hx1 <- fx/SxT ## only the survival numerically approx
hx2 <- fxT/Sx ## only the pdf numerically approx
## plotting hazards
plot(x, hxT)
lines(x, hx, col=2, lwd=2)
lines(x, hx1, col=3, lwd=2)
lines(x, hx2, col=4, lty=2, lwd=2)
## it seems that we have got something
## at the survival approx level:
plot(x, Sx/SxT)


	[[alternative HTML version deleted]]


From andrluis at ualberta.ca  Fri Nov 18 20:02:32 2016
From: andrluis at ualberta.ca (=?UTF-8?Q?Andr=C3=A9_Luis_Neves?=)
Date: Fri, 18 Nov 2016 12:02:32 -0700
Subject: [R] Help
Message-ID: <CAHxKz8aWJqHyjRsS-vS++CoQn2cXmGri+mJMbfkTJo+vZgd_6g@mail.gmail.com>

Dear,

I have the following list (mylist), in which I need to pass to the column 2
the name of the list itself.
So, running the follwing commands:

A= data.frame(1:2,3)
B= data.frame(1:4,3)
C= data.frame(c(1:2),c(4:5))
mylist=list(A=A,B=A,C=C)
lapply(mylist, setNames, paste(c("CaZyme")))

The output would be:
> lapply(mylist, setNames, paste(c("CaZyme")))
$A
  CaZyme NA
1      1  3
2      2  3

$B
  CaZyme NA
1      1  3
2      2  3

$C
  CaZyme NA
1      1  4
2      2  5
3      3  6

My question is:
How could I name the second column with the name of each dataframe of the
list, such that NA would be substitute for A, B and C, respectively.

Thank you very much,

Andre


-- 
Andre

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Fri Nov 18 20:21:04 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 18 Nov 2016 11:21:04 -0800
Subject: [R] Presenting Hazard ratios for interacting variables in a Cox
	model
In-Reply-To: <CAJvq0dBT+eeZ39g2RSk7KW4+Mdi4_Le-Kc2X0gM4GgZp=6oMzA@mail.gmail.com>
References: <CAJvq0dBT+eeZ39g2RSk7KW4+Mdi4_Le-Kc2X0gM4GgZp=6oMzA@mail.gmail.com>
Message-ID: <17239E26-76DC-4057-B573-C30288802062@comcast.net>


> On Nov 18, 2016, at 6:56 AM, Stuart Patterson via R-help <r-help at r-project.org> wrote:
> 
> I have a time-dependent cox model with three variables, each of which
> interacts with the other two. So my final model is:
> 
> fit12<-coxph(formula = Surv(data$TimeIn, data$Timeout, data$Status) ~ data$
> Year+data$Life_Stg+data$prev.tb +data$prev.tb*data$Life_Stg + data$Year*data
> $Life_Stg + data$Year*data$prev.tb + frailty(data$Natal_Group), data = data)

It seems fairly likely that you are shooting yourself in the foot by using the `data$variate` inside the formula. It will prevent the regression result from having correctly assembled references to variables. And that will become evident when you try to do any predictions. Try instead:

fit12<-coxph(formula = Surv( TimeIn,  Timeout,  Status) ~  
            prev.tb * Life_Stg +  Year *Life_Stg +  Year * prev.tb + frailty( Natal_Group),      
            data = data)

The `*` in a formula automatically includes the lower order individual variates in the estimates. Your model RHS could have also been written (more clearly in my opinion):

         ~ (prev.tb + Life_Stg +  Year)^2

... since R formulas interpret the `(.)^N` operation as "all base effects and interactions up to order N".

> 
> For my variables, there are 3 categories of year, three of year, and
> prev.tb is a binary variable. Because of the interactions, when I present
> the results, I want to present the Hazard ratio, 95% CI, and p value for
> each combination of the three variables. How do I get R to give me these
> values please?
> 
> I think that the contrast function does this for other models but does not
> work for coxph?

The usual method would be to use `predict` on a 'newdata' dataframe with all the combinations generated from `expand.grid`. The combination of the reference values of all three variables should yield a 1.0 hazard ratio. But time-dependent model predictions need a complete specification of a sequence of values over the time course of the study (as well as specification of the frailty term. So I'm not in a position to comment on feasibility for this situation.

> 
> Grateful for any suggestions
> 
> Best wishes
> 
> Stuart Patterson, Royal Veterinary College, University of London
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From sarah.goslee at gmail.com  Fri Nov 18 20:31:01 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 18 Nov 2016 14:31:01 -0500
Subject: [R] Help
In-Reply-To: <CAHxKz8aWJqHyjRsS-vS++CoQn2cXmGri+mJMbfkTJo+vZgd_6g@mail.gmail.com>
References: <CAHxKz8aWJqHyjRsS-vS++CoQn2cXmGri+mJMbfkTJo+vZgd_6g@mail.gmail.com>
Message-ID: <CAM_vjumwq1_bQPiPsFQDcRbENtPbqaLqC21XZphmS4vu18doyQ@mail.gmail.com>

Thanks for the useful reproducible example.

Here's one of the various ways this can be done:

> lapply(seq_along(mylist), function(i)setNames(mylist[[i]], c("CaZyme", names(mylist)[i])))
[[1]]
  CaZyme A
1      1 3
2      2 3

[[2]]
  CaZyme B
1      1 3
2      2 3

[[3]]
  CaZyme C
1      1 4
2      2 5

On Fri, Nov 18, 2016 at 2:02 PM, Andr? Luis Neves <andrluis at ualberta.ca> wrote:
> Dear,
>
> I have the following list (mylist), in which I need to pass to the column 2
> the name of the list itself.
> So, running the follwing commands:
>
> A= data.frame(1:2,3)
> B= data.frame(1:4,3)
> C= data.frame(c(1:2),c(4:5))
> mylist=list(A=A,B=A,C=C)
> lapply(mylist, setNames, paste(c("CaZyme")))
>
> The output would be:
>> lapply(mylist, setNames, paste(c("CaZyme")))
> $A
>   CaZyme NA
> 1      1  3
> 2      2  3
>
> $B
>   CaZyme NA
> 1      1  3
> 2      2  3
>
> $C
>   CaZyme NA
> 1      1  4
> 2      2  5
> 3      3  6
>
> My question is:
> How could I name the second column with the name of each dataframe of the
> list, such that NA would be substitute for A, B and C, respectively.
>
> Thank you very much,
>
> Andre
>
>


From andrluis at ualberta.ca  Fri Nov 18 21:11:50 2016
From: andrluis at ualberta.ca (=?UTF-8?Q?Andr=C3=A9_Luis_Neves?=)
Date: Fri, 18 Nov 2016 13:11:50 -0700
Subject: [R] Help
In-Reply-To: <CAM_vjumwq1_bQPiPsFQDcRbENtPbqaLqC21XZphmS4vu18doyQ@mail.gmail.com>
References: <CAHxKz8aWJqHyjRsS-vS++CoQn2cXmGri+mJMbfkTJo+vZgd_6g@mail.gmail.com>
	<CAM_vjumwq1_bQPiPsFQDcRbENtPbqaLqC21XZphmS4vu18doyQ@mail.gmail.com>
Message-ID: <CAHxKz8Ywh=icJiZsooJ9ue8t=8mJ0T-bb1SOWXTjNti0830Dkg@mail.gmail.com>

Thank you very much, Sarah!

It worked great in my dataset!

Andre

On Fri, Nov 18, 2016 at 12:31 PM, Sarah Goslee <sarah.goslee at gmail.com>
wrote:

> Thanks for the useful reproducible example.
>
> Here's one of the various ways this can be done:
>
> > lapply(seq_along(mylist), function(i)setNames(mylist[[i]], c("CaZyme",
> names(mylist)[i])))
> [[1]]
>   CaZyme A
> 1      1 3
> 2      2 3
>
> [[2]]
>   CaZyme B
> 1      1 3
> 2      2 3
>
> [[3]]
>   CaZyme C
> 1      1 4
> 2      2 5
>
> On Fri, Nov 18, 2016 at 2:02 PM, Andr? Luis Neves <andrluis at ualberta.ca>
> wrote:
> > Dear,
> >
> > I have the following list (mylist), in which I need to pass to the
> column 2
> > the name of the list itself.
> > So, running the follwing commands:
> >
> > A= data.frame(1:2,3)
> > B= data.frame(1:4,3)
> > C= data.frame(c(1:2),c(4:5))
> > mylist=list(A=A,B=A,C=C)
> > lapply(mylist, setNames, paste(c("CaZyme")))
> >
> > The output would be:
> >> lapply(mylist, setNames, paste(c("CaZyme")))
> > $A
> >   CaZyme NA
> > 1      1  3
> > 2      2  3
> >
> > $B
> >   CaZyme NA
> > 1      1  3
> > 2      2  3
> >
> > $C
> >   CaZyme NA
> > 1      1  4
> > 2      2  5
> > 3      3  6
> >
> > My question is:
> > How could I name the second column with the name of each dataframe of the
> > list, such that NA would be substitute for A, B and C, respectively.
> >
> > Thank you very much,
> >
> > Andre
> >
> >
>



-- 
Andre

	[[alternative HTML version deleted]]


From miaojpm at gmail.com  Sat Nov 19 01:13:27 2016
From: miaojpm at gmail.com (John)
Date: Fri, 18 Nov 2016 16:13:27 -0800
Subject: [R] Replace a dot in a string (".") with a space (" ")
Message-ID: <CABcx46CuprrAYj9QyVqf5_cvbOqhF0tEm=w0Xqh2OtaCGNpXXw@mail.gmail.com>

Hi,

   Is there any function that replaces a dot with a space? I expect "c t"
from the output of the second call of function sub, but it did not do so.

> sub("a", "b", "cat")
[1] "cbt"
> sub(".", " ", "c.t")
[1] " .t"

Thanks!

	[[alternative HTML version deleted]]


From oma.gonzales at gmail.com  Sat Nov 19 01:20:26 2016
From: oma.gonzales at gmail.com (=?UTF-8?B?T21hciBBbmRyw6kgR29uesOhbGVzIETDrWF6?=)
Date: Fri, 18 Nov 2016 19:20:26 -0500
Subject: [R] Replace a dot in a string (".") with a space (" ")
In-Reply-To: <CABcx46CuprrAYj9QyVqf5_cvbOqhF0tEm=w0Xqh2OtaCGNpXXw@mail.gmail.com>
References: <CABcx46CuprrAYj9QyVqf5_cvbOqhF0tEm=w0Xqh2OtaCGNpXXw@mail.gmail.com>
Message-ID: <CAM-xyZj_P+sAKLktMY9KLo=RspzOXr2CjGfa7b877K3oi3Ab2w@mail.gmail.com>

Hi, your were close.

This is the solution:

sub("[.]", " ", "c.t")

You need to scape the point, because point has a especial meaning in
regular expressions. Read more on regex...

2016-11-18 19:13 GMT-05:00 John <miaojpm at gmail.com>:

> Hi,
>
>    Is there any function that replaces a dot with a space? I expect "c t"
> from the output of the second call of function sub, but it did not do so.
>
> > sub("a", "b", "cat")
> [1] "cbt"
> > sub(".", " ", "c.t")
> [1] " .t"
>
> Thanks!
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sat Nov 19 02:35:22 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 18 Nov 2016 17:35:22 -0800
Subject: [R] Replace a dot in a string (".") with a space (" ")
In-Reply-To: <CABcx46CuprrAYj9QyVqf5_cvbOqhF0tEm=w0Xqh2OtaCGNpXXw@mail.gmail.com>
References: <CABcx46CuprrAYj9QyVqf5_cvbOqhF0tEm=w0Xqh2OtaCGNpXXw@mail.gmail.com>
Message-ID: <CAGxFJbQkz+ttS-GCx=4Od9ajs3K0hRbhLii8X1Szr9nBQc31sQ@mail.gmail.com>

Well of course. See ?regexp where it says:

"The period . matches any single character. "

(Always a good idea to read man pages, although this *is* a complex one)

Setting the fixed argument to TRUE  is one way to get what you want
(there are others).

> sub(".", " ", "c.t",fixed=TRUE)
[1] "c t"

Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Nov 18, 2016 at 4:13 PM, John <miaojpm at gmail.com> wrote:
> Hi,
>
>    Is there any function that replaces a dot with a space? I expect "c t"
> from the output of the second call of function sub, but it did not do so.
>
>> sub("a", "b", "cat")
> [1] "cbt"
>> sub(".", " ", "c.t")
> [1] " .t"
>
> Thanks!
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ruipbarradas at sapo.pt  Sat Nov 19 12:37:59 2016
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Sat, 19 Nov 2016 11:37:59 +0000
Subject: [R] Replace a dot in a string (".") with a space (" ")
In-Reply-To: <CAGxFJbQkz+ttS-GCx=4Od9ajs3K0hRbhLii8X1Szr9nBQc31sQ@mail.gmail.com>
References: <CABcx46CuprrAYj9QyVqf5_cvbOqhF0tEm=w0Xqh2OtaCGNpXXw@mail.gmail.com>
	<CAGxFJbQkz+ttS-GCx=4Od9ajs3K0hRbhLii8X1Szr9nBQc31sQ@mail.gmail.com>
Message-ID: <58303997.7080501@sapo.pt>

And here is another.

sub("\\.", " ", "c.t")

Rui Barradas

Em 19-11-2016 01:35, Bert Gunter escreveu:
> Well of course. See ?regexp where it says:
>
> "The period . matches any single character."
>
> (Always a good idea to read man pages, although this *is* a complex one)
>
> Setting the fixed argument to TRUE  is one way to get what you want
> (there are others).
>
>> sub(".", " ", "c.t",fixed=TRUE)
> [1] "c t"
>
> Bert
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Fri, Nov 18, 2016 at 4:13 PM, John <miaojpm at gmail.com> wrote:
>> Hi,
>>
>>     Is there any function that replaces a dot with a space? I expect "c t"
>> from the output of the second call of function sub, but it did not do so.
>>
>>> sub("a", "b", "cat")
>> [1] "cbt"
>>> sub(".", " ", "c.t")
>> [1] " .t"
>>
>> Thanks!
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Guillaume.Tahon at UGent.be  Sat Nov 19 12:50:27 2016
From: Guillaume.Tahon at UGent.be (Guillaume Tahon)
Date: Sat, 19 Nov 2016 11:50:27 +0000
Subject: [R] Average of x normalizations
Message-ID: <1479556229194.13537@UGent.be>

Dear all,

I am currently using R to normalize data (an OTU table). This all happens by using the Vegan package. An example of the script that I'm using is posted below.
What happens is the following:
I first read a text file in which the OTU table is stored. In this table every column represents an OTU, whereas rows represent the samples (see example below). The data from this table is pasted to a txt file from an excel data sheet.
Afterwards, I want to normaize this table, i.e. rarefy it to the sample with the least number of sequences. Since there can be a lot of variation between normalizations, I chose to perform a certain number of iterations (100 in the example below).
The rarefied OTU_TABLE is stored in OTU_sub. This table is identical to the original table (same number of rows and columns, same header), but the numbers will vary (for example, instead of 25 in the Sample1/OTU1 field, this will have changed to 14).
The loop performs the iterations and rarefies the data 100 times. Using these results, a boxplot is generated that gives an overview of the results of all the iterations (in this case the recovered richness/number of OTUs left after normalization).


This works fine, but I would like to step up my game.
What I would like to do is perform x iterations. However, Out of all the iterations, I want to make a final OTU_sub table. This table contains, for each field, the average of all the iterations performed.
Suppose I do 3 iterations and for Sample1/OTU1 I get the numbers 14, 5 and 20 in the different iterations, the final table should give me (14+5+20)/3 = 13.
The same thing should be done for all the other fields. Obviously, nothing should change to the fields belonging to the sample with the least number of reads.

If possible, I would also like to get the standard deviation for each field, preferably in a seperate table. This would allow me to thoroughly analyze the normalization data.

If anyone would be able to assist me with this, I would be very grateful!

Sincerely,





Guillaume Tahon


Example OTU_TABLE:


?       OTU1?   OTU2?   OTU3?
?Sample1        ?25     ?5      0?
?Sample2        ?1      ?18     ?2
?Sample3        ?0      ?1      ?20
?Sample4        ?185    4?      ?25



Currently used script:


library(vegan)
OTU_TABLE<-read.table(file="OTU_TABLE.txt", header=T)
N=100   # No. of iterations

richness_mat <- matrix(nrow = N, ncol = 4) # 4 Samples

colnames(richness_mat)<-c("sample1","sample2", "sample3", "sample4")

for(i in 1:N){
OTU_sub <- rrarefy(OTU_TABLE, min(rowSums(OTU_TABLE))) # rarefy to the sample with the least number of sequences
OTU_vector <- rowSums(OTU_sub > 0)  # Count the number of OTUs per sample

richness_mat[i,1] <- OTU_vector[1]
richness_mat[i,2] <- OTU_vector[2]
richness_mat[i,3] <- OTU_vector[3]
richness_mat[i,4] <- OTU_vector[4]
}

par(mar = c(3,4,2,2))
boxplot(richness_mat, ylab = "number of OTUs")

paste(round(colMeans(as.matrix(richness_mat)),2),"?",
round(apply(richness_mat, 2, sd),2))

	[[alternative HTML version deleted]]


From valkremk at gmail.com  Sat Nov 19 18:14:40 2016
From: valkremk at gmail.com (Val)
Date: Sat, 19 Nov 2016 11:14:40 -0600
Subject: [R] load and
Message-ID: <CAJOiR6ZwMy9R_iYYgOt1K_6Dc6KUoKgY0aqKugCeY3p6vQ0EAw@mail.gmail.com>

Hi all,
I have two *.RData  objects saveds in different time. They do have the
same number of columns but different number of rows. I was trying to
load and compare the two files by the first columns but face problem.

t1=load(file="dat.RData")
t2=load(file="dat256.RData")
Error: object 'dat256' not found

How do I fix this one and compare the two files?

 thank you in advance


From ruipbarradas at sapo.pt  Sat Nov 19 18:53:34 2016
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Sat, 19 Nov 2016 17:53:34 +0000
Subject: [R] load and
In-Reply-To: <CAJOiR6ZwMy9R_iYYgOt1K_6Dc6KUoKgY0aqKugCeY3p6vQ0EAw@mail.gmail.com>
References: <CAJOiR6ZwMy9R_iYYgOt1K_6Dc6KUoKgY0aqKugCeY3p6vQ0EAw@mail.gmail.com>
Message-ID: <5830919E.3090207@sapo.pt>

Hello,

1) You don't need to do t1 = load(...), use just load(...). The objects 
saved in the file will be created with the same name they had when they 
were saved.
2) If the object dat256 was not found try

file_dat256 <- file.choose()
load(file = file_dat256)

Hope this helps,

Rui Barradas

Em 19-11-2016 17:14, Val escreveu:
> Hi all,
> I have two *.RData  objects saveds in different time. They do have the
> same number of columns but different number of rows. I was trying to
> load and compare the two files by the first columns but face problem.
>
> t1=load(file="dat.RData")
> t2=load(file="dat256.RData")
> Error: object 'dat256' not found
>
> How do I fix this one and compare the two files?
>
>   thank you in advance
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jdnewmil at dcn.davis.ca.us  Sat Nov 19 19:19:47 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 19 Nov 2016 10:19:47 -0800
Subject: [R] load and
In-Reply-To: <5830919E.3090207@sapo.pt>
References: <CAJOiR6ZwMy9R_iYYgOt1K_6Dc6KUoKgY0aqKugCeY3p6vQ0EAw@mail.gmail.com>
	<5830919E.3090207@sapo.pt>
Message-ID: <F1526D2D-9564-4285-87E6-B59F34C1F05E@dcn.davis.ca.us>

But if save.image() was used with objects that have the same names then load will overwrite the object making comparing difficult. 

The solution to that is to use the envir argument to load your objects into distinct environments, as in [1].

[1] https://stat.ethz.ch/pipermail/r-help/2016-August/441077.html
-- 
Sent from my phone. Please excuse my brevity.

On November 19, 2016 9:53:34 AM PST, Rui Barradas <ruipbarradas at sapo.pt> wrote:
>Hello,
>
>1) You don't need to do t1 = load(...), use just load(...). The objects
>
>saved in the file will be created with the same name they had when they
>
>were saved.
>2) If the object dat256 was not found try
>
>file_dat256 <- file.choose()
>load(file = file_dat256)
>
>Hope this helps,
>
>Rui Barradas
>
>Em 19-11-2016 17:14, Val escreveu:
>> Hi all,
>> I have two *.RData  objects saveds in different time. They do have
>the
>> same number of columns but different number of rows. I was trying to
>> load and compare the two files by the first columns but face problem.
>>
>> t1=load(file="dat.RData")
>> t2=load(file="dat256.RData")
>> Error: object 'dat256' not found
>>
>> How do I fix this one and compare the two files?
>>
>>   thank you in advance
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From babasaraki at yahoo.co.uk  Sat Nov 19 19:22:41 2016
From: babasaraki at yahoo.co.uk (Umar Ahmad)
Date: Sat, 19 Nov 2016 18:22:41 +0000 (UTC)
Subject: [R] Request for help in R for MacOS
References: <1797283002.490519.1479579761674.ref@mail.yahoo.com>
Message-ID: <1797283002.490519.1479579761674@mail.yahoo.com>

Dear Sir/Madam,
I am pleased to seek for your help or guidance to enable me to fix this problem (see attached?image) with my R for MacOS. I follow all the instructions there in the R for Mac OS X help but I couldn't fix this displayed problem. I wanted to use the R for differential gene expression analysis.
Thanks in anticipation for your kind help and proper guidance to fix this issue.
Pleasant weekend?
Umar.?*UMAR AHMAD?BSc (Hons), M.S(UPM)?Ph.D. Research Fellow?Medical Genetics Laboratory (MGL)Genetics and Regenerative Medicine Research Centre (GRMRC)Level 6, Laboratory Block BFaculty of Medicine & Health Sciences,?Universiti?Putra?Malaysia (UPM)43400 UPM?Serdang, Selangor, Malaysia.T?+601-28959092???E?uahmad at basug.edu.ngThis email may contain confidential information, which also may be legally privileged. Only the intended recipient(s) may access, use, distribute or copy this e-mail. If this e-mail is received in error, please inform the sender by return e-mail and delete the original. If there are doubts about the validity of this message, please contact the sender by telephone. It is the recipient's responsibility to check the e-mail and any attached files for viruses.








-------------- next part --------------
A non-text attachment was scrubbed...
Name: Screen Shot 2016-11-20 at 2.16.11 AM.png
Type: image/png
Size: 138976 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20161119/a9501733/attachment.png>

From jlhooten at eckerd.edu  Sat Nov 19 19:32:46 2016
From: jlhooten at eckerd.edu (Jackson Hooten)
Date: Sat, 19 Nov 2016 13:32:46 -0500
Subject: [R] Help with R
Message-ID: <CAG23Y3sHOJhNW_mzKpsvx-cDdQ6WSWkThULBGOcAEAVQR2S6aA@mail.gmail.com>

Hello R Help,

I am fairly new to R and have been scouring the internet trying to find
useful tips, tricks and advice with how to solve my problems with R. I
finally was able to find you guys, and am now asking for your help. I have
a number of different questions regarding R. My most pressing question is,
how does one get a map of a very specific area into R? I've been following
a tutorial that one of my teachers sent me and been following his code.
However, every time I input this code I get a warning message:

My teacher's code:
> plot(map, col="dimgray", border=F, add=T)

The Error code I get every time I input the above code:
> Error in plot.window(...) : need finite 'ylim' values
In addition: Warning messages:
1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion
2: In min(x) : no non-missing arguments to min; returning Inf
3: In max(x) : no non-missing arguments to max; returning -Inf
4: In plot.window(...) : "border" is not a graphical parameter
5: In plot.window(...) : "add" is not a graphical parameter

So, what I think I figured out was that I didn't possess the "map" that my
teacher had in his R program. So I went to try and figure out how to get my
own map into my R session. I found this line of code to try and input a
good Google Earth map I found that I need for my study. But, alas, this
code didn't work either and I got more Error codes.

The code I used:
> img <-readPNG(system.file("img", "Seal_Island_Map.png", package="png"))

The Error code I get every time I input the above code:
> Error in readPNG(system.file("img", "Seal_Island_Map.png",
package="png")) : unable to open


For the life of me, I have not been able to find anything anywhere that can
help me solve these problems. What am I doing wrong? Can you all help?
Thank you and I hope to talk to you all soon.

Sincerely,
Jackson

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sat Nov 19 20:10:33 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 19 Nov 2016 11:10:33 -0800
Subject: [R] Help with R
In-Reply-To: <CAG23Y3sHOJhNW_mzKpsvx-cDdQ6WSWkThULBGOcAEAVQR2S6aA@mail.gmail.com>
References: <CAG23Y3sHOJhNW_mzKpsvx-cDdQ6WSWkThULBGOcAEAVQR2S6aA@mail.gmail.com>
Message-ID: <CAGxFJbTXDADGwAW+-d2hUcTquEjKVxQrxESMRH=GkxxGUej5Gg@mail.gmail.com>

If you haven't already done so, I strongly recommend that you stop
doing what you're doing until you first spend some time with an R
tutorial or two. You can find some good suggestions here:

https://www.rstudio.com/online-learning/#R

rseek.org is also a good resource for R queries; "plot maps" brought
up lots of relevant hits.


-- Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Nov 19, 2016 at 10:32 AM, Jackson Hooten <jlhooten at eckerd.edu> wrote:
> Hello R Help,
>
> I am fairly new to R and have been scouring the internet trying to find
> useful tips, tricks and advice with how to solve my problems with R. I
> finally was able to find you guys, and am now asking for your help. I have
> a number of different questions regarding R. My most pressing question is,
> how does one get a map of a very specific area into R? I've been following
> a tutorial that one of my teachers sent me and been following his code.
> However, every time I input this code I get a warning message:
>
> My teacher's code:
>> plot(map, col="dimgray", border=F, add=T)
>
> The Error code I get every time I input the above code:
>> Error in plot.window(...) : need finite 'ylim' values
> In addition: Warning messages:
> 1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion
> 2: In min(x) : no non-missing arguments to min; returning Inf
> 3: In max(x) : no non-missing arguments to max; returning -Inf
> 4: In plot.window(...) : "border" is not a graphical parameter
> 5: In plot.window(...) : "add" is not a graphical parameter
>
> So, what I think I figured out was that I didn't possess the "map" that my
> teacher had in his R program. So I went to try and figure out how to get my
> own map into my R session. I found this line of code to try and input a
> good Google Earth map I found that I need for my study. But, alas, this
> code didn't work either and I got more Error codes.
>
> The code I used:
>> img <-readPNG(system.file("img", "Seal_Island_Map.png", package="png"))
>
> The Error code I get every time I input the above code:
>> Error in readPNG(system.file("img", "Seal_Island_Map.png",
> package="png")) : unable to open
>
>
> For the life of me, I have not been able to find anything anywhere that can
> help me solve these problems. What am I doing wrong? Can you all help?
> Thank you and I hope to talk to you all soon.
>
> Sincerely,
> Jackson
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Sat Nov 19 20:15:48 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 19 Nov 2016 11:15:48 -0800
Subject: [R] load and
In-Reply-To: <CAJOiR6ZwMy9R_iYYgOt1K_6Dc6KUoKgY0aqKugCeY3p6vQ0EAw@mail.gmail.com>
References: <CAJOiR6ZwMy9R_iYYgOt1K_6Dc6KUoKgY0aqKugCeY3p6vQ0EAw@mail.gmail.com>
Message-ID: <CAGxFJbQ4DRUL43TjFTRy+QYZ4=v4TgbxH583XrDe4AqaMrsgRg@mail.gmail.com>

... and in addition are the files in your current working directory?
(see ?setwd) . If not you'll need a full path to the file.

-- Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Nov 19, 2016 at 9:14 AM, Val <valkremk at gmail.com> wrote:
> Hi all,
> I have two *.RData  objects saveds in different time. They do have the
> same number of columns but different number of rows. I was trying to
> load and compare the two files by the first columns but face problem.
>
> t1=load(file="dat.RData")
> t2=load(file="dat256.RData")
> Error: object 'dat256' not found
>
> How do I fix this one and compare the two files?
>
>  thank you in advance
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Sat Nov 19 20:39:42 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 19 Nov 2016 11:39:42 -0800
Subject: [R] Help with R
In-Reply-To: <CAG23Y3sHOJhNW_mzKpsvx-cDdQ6WSWkThULBGOcAEAVQR2S6aA@mail.gmail.com>
References: <CAG23Y3sHOJhNW_mzKpsvx-cDdQ6WSWkThULBGOcAEAVQR2S6aA@mail.gmail.com>
Message-ID: <6A750B46-F7B5-4072-B354-5F45C27457F6@dcn.davis.ca.us>

This question is a bit too vague for the R mailing lists.

For general advice on using this list,  read the Posting Guide linked in the footer of this and every post on this list. One key recommendation made there is to send plain text emails because html frequently corrupts your example code so we cannot run it. 

For suggestions about making clear examples... read [1] and/or [2]. This is critical.

For starting points on analyzing spatial data, look at [3]. Keep in mind that for many types of spatial analysis a bitmap image won't be enough, so you need to find vector representations of maps. Also note that there is a dedicated mailing list called R-sig-geo for discussing this kind of analysis.

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

[2] http://adv-r.had.co.nz/Reproducibility.html

[3] https://cran.r-project.org/web/views/Spatial.html


-- 
Sent from my phone. Please excuse my brevity.

On November 19, 2016 10:32:46 AM PST, Jackson Hooten <jlhooten at eckerd.edu> wrote:
>Hello R Help,
>
>I am fairly new to R and have been scouring the internet trying to find
>useful tips, tricks and advice with how to solve my problems with R. I
>finally was able to find you guys, and am now asking for your help. I
>have
>a number of different questions regarding R. My most pressing question
>is,
>how does one get a map of a very specific area into R? I've been
>following
>a tutorial that one of my teachers sent me and been following his code.
>However, every time I input this code I get a warning message:
>
>My teacher's code:
>> plot(map, col="dimgray", border=F, add=T)
>
>The Error code I get every time I input the above code:
>> Error in plot.window(...) : need finite 'ylim' values
>In addition: Warning messages:
>1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion
>2: In min(x) : no non-missing arguments to min; returning Inf
>3: In max(x) : no non-missing arguments to max; returning -Inf
>4: In plot.window(...) : "border" is not a graphical parameter
>5: In plot.window(...) : "add" is not a graphical parameter
>
>So, what I think I figured out was that I didn't possess the "map" that
>my
>teacher had in his R program. So I went to try and figure out how to
>get my
>own map into my R session. I found this line of code to try and input a
>good Google Earth map I found that I need for my study. But, alas, this
>code didn't work either and I got more Error codes.
>
>The code I used:
>> img <-readPNG(system.file("img", "Seal_Island_Map.png",
>package="png"))
>
>The Error code I get every time I input the above code:
>> Error in readPNG(system.file("img", "Seal_Island_Map.png",
>package="png")) : unable to open
>
>
>For the life of me, I have not been able to find anything anywhere that
>can
>help me solve these problems. What am I doing wrong? Can you all help?
>Thank you and I hope to talk to you all soon.
>
>Sincerely,
>Jackson
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Sat Nov 19 21:09:04 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 19 Nov 2016 15:09:04 -0500
Subject: [R] Help with R
In-Reply-To: <CAG23Y3sHOJhNW_mzKpsvx-cDdQ6WSWkThULBGOcAEAVQR2S6aA@mail.gmail.com>
References: <CAG23Y3sHOJhNW_mzKpsvx-cDdQ6WSWkThULBGOcAEAVQR2S6aA@mail.gmail.com>
Message-ID: <79e53124-1acd-2a91-e95f-310a42abe74a@gmail.com>

On 19/11/2016 1:32 PM, Jackson Hooten wrote:
> Hello R Help,
>
> I am fairly new to R and have been scouring the internet trying to find
> useful tips, tricks and advice with how to solve my problems with R. I
> finally was able to find you guys, and am now asking for your help.

You can find a fairly recently edited discussion of how to get help at

https://www.r-project.org/help.html




I have
> a number of different questions regarding R. My most pressing question is,
> how does one get a map of a very specific area into R? I've been following
> a tutorial that one of my teachers sent me and been following his code.
> However, every time I input this code I get a warning message:
>
> My teacher's code:
>> plot(map, col="dimgray", border=F, add=T)
>
> The Error code I get every time I input the above code:
>> Error in plot.window(...) : need finite 'ylim' values
> In addition: Warning messages:
> 1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion
> 2: In min(x) : no non-missing arguments to min; returning Inf
> 3: In max(x) : no non-missing arguments to max; returning -Inf
> 4: In plot.window(...) : "border" is not a graphical parameter
> 5: In plot.window(...) : "add" is not a graphical parameter
>
> So, what I think I figured out was that I didn't possess the "map" that my
> teacher had in his R program. So I went to try and figure out how to get my
> own map into my R session. I found this line of code to try and input a
> good Google Earth map I found that I need for my study. But, alas, this
> code didn't work either and I got more Error codes.
>
> The code I used:
>> img <-readPNG(system.file("img", "Seal_Island_Map.png", package="png"))
>
> The Error code I get every time I input the above code:
>> Error in readPNG(system.file("img", "Seal_Island_Map.png",
> package="png")) : unable to open
>
>
> For the life of me, I have not been able to find anything anywhere that can
> help me solve these problems. What am I doing wrong? Can you all help?
> Thank you and I hope to talk to you all soon.

You need to tell R where the "Seal_Island_Map.png" file is sitting.  The 
"system.file(...)" call you used says it's a file in the "png" package, 
but it's not.  The easiest way to find a file is to use the 
file.choose() function to open a file selection dialog, e.g.

filename <- file.choose()
readPNG(filename)

Duncan Murdoch


From ruipbarradas at sapo.pt  Sat Nov 19 21:33:49 2016
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Sat, 19 Nov 2016 20:33:49 +0000
Subject: [R] Help with R
In-Reply-To: <CAG23Y3sHOJhNW_mzKpsvx-cDdQ6WSWkThULBGOcAEAVQR2S6aA@mail.gmail.com>
References: <CAG23Y3sHOJhNW_mzKpsvx-cDdQ6WSWkThULBGOcAEAVQR2S6aA@mail.gmail.com>
Message-ID: <5830B72D.8010501@sapo.pt>

Hello,

First of all, there's a no homework policy in R-Help, but since it's the 
first time you post I'll try to give some help.

Now, you are trying to call two functions in one statement, system.file 
and readPNG. If you separate the calls, you will see that the call to 
system.file is returning an empty string:

system.file("img", "Seal_Island_Map.png", package="png")
[1] ""

You should then read the help page for function system.file. You do this 
by typing at an R prompt

?system.file

system.file is looking in directory 'img' for a file named 
Seal_Island_Map.png but in that directory of package png there's just 
one file, RLogo.png. So you have to ask your teacher where to find that 
file, and what function does he use to display it [tip: it's not 
function plot()].

Hope this helps,

Rui Barradas

Em 19-11-2016 18:32, Jackson Hooten escreveu:
> Hello R Help,
>
> I am fairly new to R and have been scouring the internet trying to find
> useful tips, tricks and advice with how to solve my problems with R. I
> finally was able to find you guys, and am now asking for your help. I have
> a number of different questions regarding R. My most pressing question is,
> how does one get a map of a very specific area into R? I've been following
> a tutorial that one of my teachers sent me and been following his code.
> However, every time I input this code I get a warning message:
>
> My teacher's code:
>> plot(map, col="dimgray", border=F, add=T)
>
> The Error code I get every time I input the above code:
>> Error in plot.window(...) : need finite 'ylim' values
> In addition: Warning messages:
> 1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion
> 2: In min(x) : no non-missing arguments to min; returning Inf
> 3: In max(x) : no non-missing arguments to max; returning -Inf
> 4: In plot.window(...) : "border" is not a graphical parameter
> 5: In plot.window(...) : "add" is not a graphical parameter
>
> So, what I think I figured out was that I didn't possess the "map" that my
> teacher had in his R program. So I went to try and figure out how to get my
> own map into my R session. I found this line of code to try and input a
> good Google Earth map I found that I need for my study. But, alas, this
> code didn't work either and I got more Error codes.
>
> The code I used:
>> img <-readPNG(system.file("img", "Seal_Island_Map.png", package="png"))
>
> The Error code I get every time I input the above code:
>> Error in readPNG(system.file("img", "Seal_Island_Map.png",
> package="png")) : unable to open
>
>
> For the life of me, I have not been able to find anything anywhere that can
> help me solve these problems. What am I doing wrong? Can you all help?
> Thank you and I hope to talk to you all soon.
>
> Sincerely,
> Jackson
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rbaer at atsu.edu  Sun Nov 20 01:05:13 2016
From: rbaer at atsu.edu (Robert Baer)
Date: Sat, 19 Nov 2016 18:05:13 -0600
Subject: [R] Run a Python code from R
In-Reply-To: <3F13A4EB-7546-4DF8-8515-BE2A2ADDA8D6@comcast.net>
References: <CY1PR05MB273092A72F36BAEB24AA5C9199BE0@CY1PR05MB2730.namprd05.prod.outlook.com>
	<CAKyN3iBNQDUuzNRVvJMvw+HVzcsuqrBTj545ZfM=D8B0+nG9SQ@mail.gmail.com>
	<SN2PR05MB2734537C1D81907B21E8063999B10@SN2PR05MB2734.namprd05.prod.outlook.com>
	<3F13A4EB-7546-4DF8-8515-BE2A2ADDA8D6@comcast.net>
Message-ID: <9d9685bf-b075-94be-356b-923f7de82ca0@atsu.edu>

 From https://www.r-bloggers.com/rpithon-vs-rpython/

<https://www.r-bloggers.com/rpithon-vs-rpython/>"Similar to rPython, the 
rPithon package (http://rpithon.r-forge.r-project.org) allows users to 
execute Python code from R and exchange the data between Python and R. 
However, the underlying mechanisms between these two packages are 
fundamentally different. Wihle rPithon communicates with Python from R 
through pipes, rPython accomplishes the same task with json. A major 
advantage of rPithon over rPython is that multiple Python processes can 
be started within a R session. However, rPithon is not very robust while 
exchanging large data objects between R and Python."


On 11/16/2016 7:10 PM, David Winsemius wrote:
>> On Nov 16, 2016, at 4:53 PM, Nelly Reduan <nell.redu at hotmail.fr> wrote:
>>
>> Thank you very much for your help !
>>
>>
>> I 'm trying to use the package "rPithon" but I obtain this error message:
> Are you sure you are not just misspelling rPython? If that's not the issue than you need to say where you got rPithon,

 From https://www.r-bloggers.com/rpithon-vs-rpython/

"Similar to rPython, the rPithon package 
(http://rpithon.r-forge.r-project.org) allows users to execute Python 
code from R and exchange the data between Python and R. However, the 
underlying mechanisms between these two packages are fundamentally 
different. While rPithon communicates with Python from R through pipes, 
rPython accomplishes the same task with json. A major advantage of 
rPithon over rPython is that multiple Python processes can be started 
within a R session. However, rPithon is not very robust while exchanging 
large data objects between R and Python."


>> On Wed, Nov 16, 2016 at 4:53 PM, Nelly Reduan <nell.redu at hotmail.fr> wrote:
>>> Hello,
>>>
>>>
>>> How can I run this Python code from R ?
>>>
>>>
>>>>>> import nlmpy
>>>>>> nlm = nlmpy.mpd(nRow=50, nCol=50, h=0.75)
>>>>>> nlmpy.exportASCIIGrid("raster.asc", nlm)
>>>
>>> Nlmpy is a Python package to build neutral landscape models
>>>
>>> https://pypi.python.org/pypi/nlmpy . The example comes from this website. I tried to use the function system2 but I don't know how to use it.
>>>
>>>
>>> path_script_python <- "C:/Users/Anaconda2/Lib/site-packages/nlmpy/nlmpy.py"
>>>
>>> test <- system2("python", args = c(path_script_python, as.character(nRow), as.character(nCol), as.character(h)))
>>>
>>> Thanks a lot for your help.
>>> Nell
>>>
>>>
>>> nlmpy 0.1.3 : Python Package Index<https://pypi.python.org/pypi/nlmpy>
>>> pypi.python.org
>>> NLMpy. NLMpy is a Python package for the creation of neutral landscape models that are widely used in the modelling of ecological patterns and processes across ...
>>>
>>>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>>
>>> David Winsemius
>>> Alameda, CA, USA
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From jlhooten at eckerd.edu  Sun Nov 20 05:31:30 2016
From: jlhooten at eckerd.edu (Jackson Hooten)
Date: Sat, 19 Nov 2016 23:31:30 -0500
Subject: [R] Help with R
In-Reply-To: <5830B72D.8010501@sapo.pt>
References: <CAG23Y3sHOJhNW_mzKpsvx-cDdQ6WSWkThULBGOcAEAVQR2S6aA@mail.gmail.com>
	<5830B72D.8010501@sapo.pt>
Message-ID: <ADD348CE-25F2-4A83-A3DD-636E08D6235E@eckerd.edu>

Rui Barradas, 

Thank you very much for your help! This isn't an actual homework assignment, it's an independent project that I'm conducting that my teacher was just giving me a tutorial to help me. Thank you again! 

Sincerely, 
Jackson 

Sent from my iPhone

> On Nov 19, 2016, at 3:33 PM, Rui Barradas <ruipbarradas at sapo.pt> wrote:
> 
> Hello,
> 
> First of all, there's a no homework policy in R-Help, but since it's the first time you post I'll try to give some help.
> 
> Now, you are trying to call two functions in one statement, system.file and readPNG. If you separate the calls, you will see that the call to system.file is returning an empty string:
> 
> system.file("img", "Seal_Island_Map.png", package="png")
> [1] ""
> 
> You should then read the help page for function system.file. You do this by typing at an R prompt
> 
> ?system.file
> 
> system.file is looking in directory 'img' for a file named Seal_Island_Map.png but in that directory of package png there's just one file, RLogo.png. So you have to ask your teacher where to find that file, and what function does he use to display it [tip: it's not function plot()].
> 
> Hope this helps,
> 
> Rui Barradas
> 
> Em 19-11-2016 18:32, Jackson Hooten escreveu:
>> Hello R Help,
>> 
>> I am fairly new to R and have been scouring the internet trying to find
>> useful tips, tricks and advice with how to solve my problems with R. I
>> finally was able to find you guys, and am now asking for your help. I have
>> a number of different questions regarding R. My most pressing question is,
>> how does one get a map of a very specific area into R? I've been following
>> a tutorial that one of my teachers sent me and been following his code.
>> However, every time I input this code I get a warning message:
>> 
>> My teacher's code:
>>> plot(map, col="dimgray", border=F, add=T)
>> 
>> The Error code I get every time I input the above code:
>>> Error in plot.window(...) : need finite 'ylim' values
>> In addition: Warning messages:
>> 1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion
>> 2: In min(x) : no non-missing arguments to min; returning Inf
>> 3: In max(x) : no non-missing arguments to max; returning -Inf
>> 4: In plot.window(...) : "border" is not a graphical parameter
>> 5: In plot.window(...) : "add" is not a graphical parameter
>> 
>> So, what I think I figured out was that I didn't possess the "map" that my
>> teacher had in his R program. So I went to try and figure out how to get my
>> own map into my R session. I found this line of code to try and input a
>> good Google Earth map I found that I need for my study. But, alas, this
>> code didn't work either and I got more Error codes.
>> 
>> The code I used:
>>> img <-readPNG(system.file("img", "Seal_Island_Map.png", package="png"))
>> 
>> The Error code I get every time I input the above code:
>>> Error in readPNG(system.file("img", "Seal_Island_Map.png",
>> package="png")) : unable to open
>> 
>> 
>> For the life of me, I have not been able to find anything anywhere that can
>> help me solve these problems. What am I doing wrong? Can you all help?
>> Thank you and I hope to talk to you all soon.
>> 
>> Sincerely,
>> Jackson
>> 
>>    [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 


From kmezhoud at gmail.com  Sun Nov 20 14:28:22 2016
From: kmezhoud at gmail.com (Karim Mezhoud)
Date: Sun, 20 Nov 2016 14:28:22 +0100
Subject: [R] aggregate dataframe by multiple factors
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C504684B@SRVEXCHMBX.precheza.cz>
References: <CALJKBv9S5=3rYffCbW098eVm8_9XtdE4-FrdWOHTH9AcGWJu4g@mail.gmail.com>
	<89CF0820-5338-4150-A300-78D517CC019B@comcast.net>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C504684B@SRVEXCHMBX.precheza.cz>
Message-ID: <CALJKBv9pDpDKZ5ZKO0hAh3wZXSXiyVPxyqrj2JEsupffC+tN-w@mail.gmail.com>

Sorry for the delay,
Many Thanks for Mr. David and Mr. Petr
I thinked  to use "sort" function to arrange chronologically  value by
'date' (without 'date' is colnames) of each variables (DCE, DP).


The solution of David seems to be simple to understand with "unlist"
function.
The solution of Petr seems to be fancy. I did not find document  about "I"
argument for aggregate function.

How can know which value for which date?

I will save the reshaping/ordering dataframe  for later use.
Many Thanks,
Karim


On Fri, Nov 18, 2016 at 11:34 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hi
>
> same result can be achieved by
>
> dat.ag<-aggregate(dat[ , c("DCE","DP")], by= list(dat$first.Name,
> dat$Name, dat$Department) , "I")
>
> Sorting according to the first row seems to be quite tricky. You could
> probably get closer by using some combination of split and order and
> arranging back chunks  of data
>
> ooo1<-order(split(dat$DCE,interaction(dat$first.Name, dat$Name,
> dat$Department, drop=T))[[1]])
> data.frame(sapply(split(dat$DCE,interaction(dat$first.Name, dat$Name,
> dat$Department, drop=T)), rbind))[ooo1,]
>   Ancient.Nation.QLH Amish.Wives.TAS Auction.Videos.YME
> 2               0.28              NA                 NA
> 4               0.28              NA                 NA
> 1               0.54            0.59               0.57
> 3               0.54            0.59               0.57
>
> however I wonder why the order according to the first row is necessary if
> all NAs are on correct positions?
>
> Cheers
> Petr
>
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of David
> > Winsemius
> > Sent: Friday, November 18, 2016 9:30 AM
> > To: Karim Mezhoud <kmezhoud at gmail.com>
> > Cc: r-help at r-project.org
> > Subject: Re: [R] aggregate dataframe by multiple factors
> >
> >
> > > On Nov 17, 2016, at 11:27 PM, Karim Mezhoud <kmezhoud at gmail.com>
> > wrote:
> > >
> > > Dear all,
> > >
> > > the dat  has missing values NA,
> > >
> > >    first.Name   Name Department  DCE   DP       date
> > > 5      Auction Videos        YME 0.57 0.56 2013-09-30
> > > 18       Amish  Wives        TAS 0.59 0.56 2013-09-30
> > > 34     Ancient Nation        QLH 0.54 0.58 2013-09-30
> > > 53     Auction Videos        YME   NA   NA 2013-12-28
> > > 66       Amish  Wives        TAS   NA   NA 2013-12-28
> > > 82     Ancient Nation        QLH 0.28 0.29 2013-12-28
> > > 102    Auction Videos        YME 0.57 0.56 2014-03-30
> > > 115      Amish  Wives        TAS 0.59 0.56 2014-03-30
> > > 131    Ancient Nation        QLH 0.54 0.58 2014-03-30
> > > 150    Auction Videos        YME   NA   NA 2014-06-28
> > > 163      Amish  Wives        TAS   NA   NA 2014-06-28
> > > 179    Ancient Nation        QLH 0.28 0.29 2014-06-28
> > >
> > >
> > > agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> > > list(dat$first.Name, dat$Name, dat$Department) , "sort"))
> >
> > The closest I could get on a few attempts was:
> >
> > (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> > list(dat$first.Name, dat$Name, dat$Department) , function(d) {
> unlist(d)}))
> >  )
> >
> >   Group.1 Group.2 Group.3 DCE.1 DCE.2 DCE.3 DCE.4 DP.1 DP.2 DP.3 DP.4
> > 1 Ancient  Nation     QLH  0.54  0.28  0.54  0.28 0.58 0.29 0.58 0.29
> > 2   Amish   Wives     TAS  0.59    NA  0.59    NA 0.56   NA 0.56   NA
> > 3 Auction  Videos     YME  0.57    NA  0.57    NA 0.56   NA 0.56   NA
> >
> > I think the sort operation might be somewhat ambiguous in this instance.
> I
> > tried:
> >
> >  (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> > list(dat$first.Name, dat$Name, dat$Department) , function(d) {
> > unlist(lapply(d,sort))}))
> >  )
> >
> > With no success, not even a sorted result.
> >
> > --
> > David.
> > >
> > >
> > > agg has list of value. I would separate value in different columns.
> > >
> > >  Group.1 Group.2 Group.3                    DCE                     DP
> > > 1 Ancient  Nation     QLH 0.28, 0.28, 0.54, 0.54 0.29, 0.29, 0.58, 0.58
> > > 2   Amish   Wives     TAS             0.59, 0.59             0.56, 0.56
> > > 3 Auction  Videos     YME             0.57, 0.57             0.56, 0.56
> > >
> > > The  goal:
> > >
> > > Group.1 Group.2 Group.3  DCE.1 DCE.2 DCE.3  DCE.4  DP.1  DP.2  DP.3
> DP.4
> > > 1 Ancient  Nation     QLH    0.28     0.28    0.54     0.54     0.29,
> 0.29,
> > > 0.58, 0.58
> > > 2   Amish   Wives     TAS        NA     NA     0.59, 0.59           NA
> > > NA  0.56, 0.56
> > > 3 Auction  Videos     YME         NA   NA      0.57, 0.57
>  NA
> > > NA  0.56, 0.56
> > >
> > >
> > >
> > > dat <- structure(list(first.Name = structure(c(3L, 1L, 2L, 3L, 1L, 2L,
> > > 3L, 1L, 2L, 3L, 1L, 2L), .Label = c("Amish", "Ancient", "Auction",
> > > "Ax", "Bachelorette", "Basketball", "BBQ", "Cake", "Celebrity",
> > > "Chef", "Clean", "Colonial", "Comedy", "Comic", "Crocodile", "Dog",
> > > "Empire", "Extreme", "Farm", "Half Pint", "Hollywood", "House", "Ice
> > > Road", "Jersey", "Justice", "Love", "Mega", "Model", "Modern",
> > > "Mountain", "Mystery", "Myth", "New York", "Paradise", "Pioneer",
> > > "Queer", "Restaurant", "Road", "Royal", "Spouse", "Star", "Storage",
> > > "Survival", "The Great American", "Tool", "Treasure", "Wedding",
> > > "Wife"), class = "factor"), Name = structure(c(43L, 47L, 29L, 43L,
> > > 47L, 29L, 43L, 47L, 29L, 43L, 47L, 29L), .Label = c("Aliens",
> > > "Behavior", "Casino", "Casting Call", "Challenge", "Contest",
> > > "Crashers", "Crew", "Dad", "Dancing", "Date", "Disasters", "Dynasty",
> > > "Family", "Garage", "Greenlight", "Gypsies", "Haul", "Hot Rod",
> > > "Inventor", "Jail", "Job", "Justice", "Marvels", "Master", "Mates",
> > > "Model", "Moms", "Nation", "Ninja", "Patrol", "People", "Pitmasters",
> > > "Queens", "Rescue", "Rivals", "Room", "Rooms", "Rules", "Star",
> > > "Stars", "Superhero", "Videos", "VIP", "Wars", "Wishes", "Wives",
> > > "Wrangler"), class = "factor"), Department = structure(c(8L, 6L, 2L,
> > > 8L, 6L, 2L, 8L, 6L, 2L, 8L, 6L, 2L), .Label = c("HXW", "QLH", "RAR",
> > > "RYC", "SYI", "TAS", "VUV", "YME"), class = "factor"),
> > >    DCE = c(0.57, 0.59, 0.54, NA, NA, 0.28, 0.57, 0.59, 0.54,
> > >    NA, NA, 0.28), DP = c(0.56, 0.56, 0.58, NA, NA, 0.29, 0.56,
> > >    0.56, 0.58, NA, NA, 0.29), date = structure(c(15978, 15978,
> > >    15978, 16067, 16067, 16067, 16159, 16159, 16159, 16249, 16249,
> > >    16249), class = "Date")), description = "", row.names = c(5L, 18L,
> > > 34L, 53L, 66L, 82L, 102L, 115L, 131L, 150L, 163L, 179L), class =
> > > "data.frame", .Names = c("first.Name", "Name", "Department", "DCE",
> > > "DP", "date"))
> > >
> > >     [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>

	[[alternative HTML version deleted]]


From nstefi at gmail.com  Sun Nov 20 05:06:36 2016
From: nstefi at gmail.com (Steven Nagy)
Date: Sat, 19 Nov 2016 23:06:36 -0500
Subject: [R] Need some help with regular expression
Message-ID: <00a301d242e3$7d884e20$7898ea60$@gmail.com>

I tried out a regular expression on this website:

http://regexr.com/3en1m

 

So the input text is:

"Name.MEMBER_TYPE:  -> STU"

 

The regular expression is: ((?:\w+|\s) -> STU|STU -> (?:\w+|\s))

And it returns:

"  -> STU"

 

but when I use in R, it doesn't return the same result:

strapply(c, "((?:\\w+|\\s) -> STU|STU -> (?:\\w+|\\s))", c, backref = -1,
perl = TRUE)

returns:
"Name.MEMBER_TYPE: -> STU"

 

 

Here is what I was trying to do:

 

I need to extract some values from a log table, and I created a regular
expression that helps me with that.

The log table has cells with values like:

a = "Name.MEMBER_TYPE: NMA -> STU ; CATEGORY:  -> 1 ; CITY: MISSISSAUGA ->
Mississauga ; ZIP: L5N1H9 -> L5N 1H9 ; COUNTRY: CAN ->  ; MEMBER_STATUS:  ->
N"

or
b = "Name.MEMBER_TYPE: STU -> REG ; CATEGORY: 1 ->" 

so I needed to extract the values that a STU member type is changing from
and to, so I needed NMA, STU in the 1st case or STU, REG in the 2nd case.

I came up with this expression which worked in both cases:

strapply(strapply(a, "(\\w+ -> STU|STU -> \\w+)", c, backref = -1, perl =
TRUE), "(\\w+) -> (\\w+)", c, backref = -2, perl = TRUE)

 

But I had a 3rd case when the source member type was blank:

c = "Name.MEMBER_TYPE: -> STU"

and in that case it returned an error:

strapply(strapply(c, "(\\w+ -> STU|STU -> \\w+)", c, backref = -1, perl =
TRUE), "(\\w+) -> (\\w+)", c, backref = -2, perl = TRUE)

Error: is.character(x) is not TRUE

 

I found that the error is because this returns NULL:

strapply(c, "(\\w+ -> STU|STU -> \\w+)", c, backref = -1, perl = TRUE)

 

 

So I tried to modify the regular expression to match any word or blank
space:

strapply(c, "((?:\\w+|\\s) -> STU|STU -> (?:\\w+|\\s))", c, backref = -1,
perl = TRUE)

 

but this returned me the whole value of "c":

"Name.MEMBER_TYPE:  -> STU"

and I only needed "  -> STU" as it shows on the website regxr.com

 

Is the result wrong on the regxr.com website or strapply returns the wrong
result?

 

Thanks,

Steven


	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sun Nov 20 19:44:24 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 20 Nov 2016 10:44:24 -0800
Subject: [R] aggregate dataframe by multiple factors
In-Reply-To: <CALJKBv9pDpDKZ5ZKO0hAh3wZXSXiyVPxyqrj2JEsupffC+tN-w@mail.gmail.com>
References: <CALJKBv9S5=3rYffCbW098eVm8_9XtdE4-FrdWOHTH9AcGWJu4g@mail.gmail.com>
	<89CF0820-5338-4150-A300-78D517CC019B@comcast.net>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C504684B@SRVEXCHMBX.precheza.cz>
	<CALJKBv9pDpDKZ5ZKO0hAh3wZXSXiyVPxyqrj2JEsupffC+tN-w@mail.gmail.com>
Message-ID: <28273AA6-CE92-4F77-A575-264F41193826@comcast.net>


> On Nov 20, 2016, at 5:28 AM, Karim Mezhoud <kmezhoud at gmail.com> wrote:
> 
> Sorry for the delay,
> Many Thanks for Mr. David and Mr. Petr
> I thinked  to use "sort" function to arrange chronologically  value by  'date' (without 'date' is colnames) of each variables (DCE, DP).
> 
> 
> The solution of David seems to be simple to understand with "unlist" function.
> The solution of Petr seems to be fancy. I did not find document  about "I" argument for aggregate function.
> 
> How can know which value for which date?

You asked for a functional reshaping that did not have the date. Now you want the date? There was no date in the result you indicated was desired.   ....??????

-- 
David.

> 
> I will save the reshaping/ordering dataframe  for later use. 
> Many Thanks,
> Karim
> 
> 
> On Fri, Nov 18, 2016 at 11:34 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:
> Hi
> 
> same result can be achieved by
> 
> dat.ag<-aggregate(dat[ , c("DCE","DP")], by= list(dat$first.Name, dat$Name, dat$Department) , "I")
> 
> Sorting according to the first row seems to be quite tricky. You could probably get closer by using some combination of split and order and arranging back chunks  of data
> 
> ooo1<-order(split(dat$DCE,interaction(dat$first.Name, dat$Name, dat$Department, drop=T))[[1]])
> data.frame(sapply(split(dat$DCE,interaction(dat$first.Name, dat$Name, dat$Department, drop=T)), rbind))[ooo1,]
>   Ancient.Nation.QLH Amish.Wives.TAS Auction.Videos.YME
> 2               0.28              NA                 NA
> 4               0.28              NA                 NA
> 1               0.54            0.59               0.57
> 3               0.54            0.59               0.57
> 
> however I wonder why the order according to the first row is necessary if all NAs are on correct positions?
> 
> Cheers
> Petr
> 
> 
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of David
> > Winsemius
> > Sent: Friday, November 18, 2016 9:30 AM
> > To: Karim Mezhoud <kmezhoud at gmail.com>
> > Cc: r-help at r-project.org
> > Subject: Re: [R] aggregate dataframe by multiple factors
> >
> >
> > > On Nov 17, 2016, at 11:27 PM, Karim Mezhoud <kmezhoud at gmail.com>
> > wrote:
> > >
> > > Dear all,
> > >
> > > the dat  has missing values NA,
> > >
> > >    first.Name   Name Department  DCE   DP       date
> > > 5      Auction Videos        YME 0.57 0.56 2013-09-30
> > > 18       Amish  Wives        TAS 0.59 0.56 2013-09-30
> > > 34     Ancient Nation        QLH 0.54 0.58 2013-09-30
> > > 53     Auction Videos        YME   NA   NA 2013-12-28
> > > 66       Amish  Wives        TAS   NA   NA 2013-12-28
> > > 82     Ancient Nation        QLH 0.28 0.29 2013-12-28
> > > 102    Auction Videos        YME 0.57 0.56 2014-03-30
> > > 115      Amish  Wives        TAS 0.59 0.56 2014-03-30
> > > 131    Ancient Nation        QLH 0.54 0.58 2014-03-30
> > > 150    Auction Videos        YME   NA   NA 2014-06-28
> > > 163      Amish  Wives        TAS   NA   NA 2014-06-28
> > > 179    Ancient Nation        QLH 0.28 0.29 2014-06-28
> > >
> > >
> > > agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> > > list(dat$first.Name, dat$Name, dat$Department) , "sort"))
> >
> > The closest I could get on a few attempts was:
> >
> > (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> > list(dat$first.Name, dat$Name, dat$Department) , function(d) { unlist(d)}))
> >  )
> >
> >   Group.1 Group.2 Group.3 DCE.1 DCE.2 DCE.3 DCE.4 DP.1 DP.2 DP.3 DP.4
> > 1 Ancient  Nation     QLH  0.54  0.28  0.54  0.28 0.58 0.29 0.58 0.29
> > 2   Amish   Wives     TAS  0.59    NA  0.59    NA 0.56   NA 0.56   NA
> > 3 Auction  Videos     YME  0.57    NA  0.57    NA 0.56   NA 0.56   NA
> >
> > I think the sort operation might be somewhat ambiguous in this instance. I
> > tried:
> >
> >  (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> > list(dat$first.Name, dat$Name, dat$Department) , function(d) {
> > unlist(lapply(d,sort))}))
> >  )
> >
> > With no success, not even a sorted result.
> >
> > --
> > David.
> > >
> > >
> > > agg has list of value. I would separate value in different columns.
> > >
> > >  Group.1 Group.2 Group.3                    DCE                     DP
> > > 1 Ancient  Nation     QLH 0.28, 0.28, 0.54, 0.54 0.29, 0.29, 0.58, 0.58
> > > 2   Amish   Wives     TAS             0.59, 0.59             0.56, 0.56
> > > 3 Auction  Videos     YME             0.57, 0.57             0.56, 0.56
> > >
> > > The  goal:
> > >
> > > Group.1 Group.2 Group.3  DCE.1 DCE.2 DCE.3  DCE.4  DP.1  DP.2  DP.3  DP.4
> > > 1 Ancient  Nation     QLH    0.28     0.28    0.54     0.54     0.29, 0.29,
> > > 0.58, 0.58
> > > 2   Amish   Wives     TAS        NA     NA     0.59, 0.59           NA
> > > NA  0.56, 0.56
> > > 3 Auction  Videos     YME         NA   NA      0.57, 0.57             NA
> > > NA  0.56, 0.56
> > >
> > >
> > >
> > > dat <- structure(list(first.Name = structure(c(3L, 1L, 2L, 3L, 1L, 2L,
> > > 3L, 1L, 2L, 3L, 1L, 2L), .Label = c("Amish", "Ancient", "Auction",
> > > "Ax", "Bachelorette", "Basketball", "BBQ", "Cake", "Celebrity",
> > > "Chef", "Clean", "Colonial", "Comedy", "Comic", "Crocodile", "Dog",
> > > "Empire", "Extreme", "Farm", "Half Pint", "Hollywood", "House", "Ice
> > > Road", "Jersey", "Justice", "Love", "Mega", "Model", "Modern",
> > > "Mountain", "Mystery", "Myth", "New York", "Paradise", "Pioneer",
> > > "Queer", "Restaurant", "Road", "Royal", "Spouse", "Star", "Storage",
> > > "Survival", "The Great American", "Tool", "Treasure", "Wedding",
> > > "Wife"), class = "factor"), Name = structure(c(43L, 47L, 29L, 43L,
> > > 47L, 29L, 43L, 47L, 29L, 43L, 47L, 29L), .Label = c("Aliens",
> > > "Behavior", "Casino", "Casting Call", "Challenge", "Contest",
> > > "Crashers", "Crew", "Dad", "Dancing", "Date", "Disasters", "Dynasty",
> > > "Family", "Garage", "Greenlight", "Gypsies", "Haul", "Hot Rod",
> > > "Inventor", "Jail", "Job", "Justice", "Marvels", "Master", "Mates",
> > > "Model", "Moms", "Nation", "Ninja", "Patrol", "People", "Pitmasters",
> > > "Queens", "Rescue", "Rivals", "Room", "Rooms", "Rules", "Star",
> > > "Stars", "Superhero", "Videos", "VIP", "Wars", "Wishes", "Wives",
> > > "Wrangler"), class = "factor"), Department = structure(c(8L, 6L, 2L,
> > > 8L, 6L, 2L, 8L, 6L, 2L, 8L, 6L, 2L), .Label = c("HXW", "QLH", "RAR",
> > > "RYC", "SYI", "TAS", "VUV", "YME"), class = "factor"),
> > >    DCE = c(0.57, 0.59, 0.54, NA, NA, 0.28, 0.57, 0.59, 0.54,
> > >    NA, NA, 0.28), DP = c(0.56, 0.56, 0.58, NA, NA, 0.29, 0.56,
> > >    0.56, 0.58, NA, NA, 0.29), date = structure(c(15978, 15978,
> > >    15978, 16067, 16067, 16067, 16159, 16159, 16159, 16249, 16249,
> > >    16249), class = "Date")), description = "", row.names = c(5L, 18L,
> > > 34L, 53L, 66L, 82L, 102L, 115L, 131L, 150L, 163L, 179L), class =
> > > "data.frame", .Names = c("first.Name", "Name", "Department", "DCE",
> > > "DP", "date"))
> > >
> > >     [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
> 
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
> 
> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
> 
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.
> 

David Winsemius
Alameda, CA, USA


From kmezhoud at gmail.com  Sun Nov 20 19:49:09 2016
From: kmezhoud at gmail.com (Karim Mezhoud)
Date: Sun, 20 Nov 2016 19:49:09 +0100
Subject: [R] aggregate dataframe by multiple factors
In-Reply-To: <28273AA6-CE92-4F77-A575-264F41193826@comcast.net>
References: <CALJKBv9S5=3rYffCbW098eVm8_9XtdE4-FrdWOHTH9AcGWJu4g@mail.gmail.com>
	<89CF0820-5338-4150-A300-78D517CC019B@comcast.net>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C504684B@SRVEXCHMBX.precheza.cz>
	<CALJKBv9pDpDKZ5ZKO0hAh3wZXSXiyVPxyqrj2JEsupffC+tN-w@mail.gmail.com>
	<28273AA6-CE92-4F77-A575-264F41193826@comcast.net>
Message-ID: <CALJKBv8m12aYMhBVLRO9+U=6sZq9Z7qwDMvk_frr7gfBMf5FBA@mail.gmail.com>

Yes the results does not have a date but
successive DCE.1, DCE2, DCE.3 indicates
 DCE.date1 , DCE.date2, DCE.date3
I hope that the chronological order of date is conserved.
Thanks,
Karim


On Sun, Nov 20, 2016 at 7:44 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Nov 20, 2016, at 5:28 AM, Karim Mezhoud <kmezhoud at gmail.com> wrote:
> >
> > Sorry for the delay,
> > Many Thanks for Mr. David and Mr. Petr
> > I thinked  to use "sort" function to arrange chronologically  value by
> 'date' (without 'date' is colnames) of each variables (DCE, DP).
> >
> >
> > The solution of David seems to be simple to understand with "unlist"
> function.
> > The solution of Petr seems to be fancy. I did not find document  about
> "I" argument for aggregate function.
> >
> > How can know which value for which date?
>
> You asked for a functional reshaping that did not have the date. Now you
> want the date? There was no date in the result you indicated was desired.
>  ....??????
>
> --
> David.
>
> >
> > I will save the reshaping/ordering dataframe  for later use.
> > Many Thanks,
> > Karim
> >
> >
> > On Fri, Nov 18, 2016 at 11:34 AM, PIKAL Petr <petr.pikal at precheza.cz>
> wrote:
> > Hi
> >
> > same result can be achieved by
> >
> > dat.ag<-aggregate(dat[ , c("DCE","DP")], by= list(dat$first.Name,
> dat$Name, dat$Department) , "I")
> >
> > Sorting according to the first row seems to be quite tricky. You could
> probably get closer by using some combination of split and order and
> arranging back chunks  of data
> >
> > ooo1<-order(split(dat$DCE,interaction(dat$first.Name, dat$Name,
> dat$Department, drop=T))[[1]])
> > data.frame(sapply(split(dat$DCE,interaction(dat$first.Name, dat$Name,
> dat$Department, drop=T)), rbind))[ooo1,]
> >   Ancient.Nation.QLH Amish.Wives.TAS Auction.Videos.YME
> > 2               0.28              NA                 NA
> > 4               0.28              NA                 NA
> > 1               0.54            0.59               0.57
> > 3               0.54            0.59               0.57
> >
> > however I wonder why the order according to the first row is necessary
> if all NAs are on correct positions?
> >
> > Cheers
> > Petr
> >
> >
> > > -----Original Message-----
> > > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of David
> > > Winsemius
> > > Sent: Friday, November 18, 2016 9:30 AM
> > > To: Karim Mezhoud <kmezhoud at gmail.com>
> > > Cc: r-help at r-project.org
> > > Subject: Re: [R] aggregate dataframe by multiple factors
> > >
> > >
> > > > On Nov 17, 2016, at 11:27 PM, Karim Mezhoud <kmezhoud at gmail.com>
> > > wrote:
> > > >
> > > > Dear all,
> > > >
> > > > the dat  has missing values NA,
> > > >
> > > >    first.Name   Name Department  DCE   DP       date
> > > > 5      Auction Videos        YME 0.57 0.56 2013-09-30
> > > > 18       Amish  Wives        TAS 0.59 0.56 2013-09-30
> > > > 34     Ancient Nation        QLH 0.54 0.58 2013-09-30
> > > > 53     Auction Videos        YME   NA   NA 2013-12-28
> > > > 66       Amish  Wives        TAS   NA   NA 2013-12-28
> > > > 82     Ancient Nation        QLH 0.28 0.29 2013-12-28
> > > > 102    Auction Videos        YME 0.57 0.56 2014-03-30
> > > > 115      Amish  Wives        TAS 0.59 0.56 2014-03-30
> > > > 131    Ancient Nation        QLH 0.54 0.58 2014-03-30
> > > > 150    Auction Videos        YME   NA   NA 2014-06-28
> > > > 163      Amish  Wives        TAS   NA   NA 2014-06-28
> > > > 179    Ancient Nation        QLH 0.28 0.29 2014-06-28
> > > >
> > > >
> > > > agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> > > > list(dat$first.Name, dat$Name, dat$Department) , "sort"))
> > >
> > > The closest I could get on a few attempts was:
> > >
> > > (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> > > list(dat$first.Name, dat$Name, dat$Department) , function(d) {
> unlist(d)}))
> > >  )
> > >
> > >   Group.1 Group.2 Group.3 DCE.1 DCE.2 DCE.3 DCE.4 DP.1 DP.2 DP.3 DP.4
> > > 1 Ancient  Nation     QLH  0.54  0.28  0.54  0.28 0.58 0.29 0.58 0.29
> > > 2   Amish   Wives     TAS  0.59    NA  0.59    NA 0.56   NA 0.56   NA
> > > 3 Auction  Videos     YME  0.57    NA  0.57    NA 0.56   NA 0.56   NA
> > >
> > > I think the sort operation might be somewhat ambiguous in this
> instance. I
> > > tried:
> > >
> > >  (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> > > list(dat$first.Name, dat$Name, dat$Department) , function(d) {
> > > unlist(lapply(d,sort))}))
> > >  )
> > >
> > > With no success, not even a sorted result.
> > >
> > > --
> > > David.
> > > >
> > > >
> > > > agg has list of value. I would separate value in different columns.
> > > >
> > > >  Group.1 Group.2 Group.3                    DCE
>  DP
> > > > 1 Ancient  Nation     QLH 0.28, 0.28, 0.54, 0.54 0.29, 0.29, 0.58,
> 0.58
> > > > 2   Amish   Wives     TAS             0.59, 0.59             0.56,
> 0.56
> > > > 3 Auction  Videos     YME             0.57, 0.57             0.56,
> 0.56
> > > >
> > > > The  goal:
> > > >
> > > > Group.1 Group.2 Group.3  DCE.1 DCE.2 DCE.3  DCE.4  DP.1  DP.2  DP.3
> DP.4
> > > > 1 Ancient  Nation     QLH    0.28     0.28    0.54     0.54
>  0.29, 0.29,
> > > > 0.58, 0.58
> > > > 2   Amish   Wives     TAS        NA     NA     0.59, 0.59
>  NA
> > > > NA  0.56, 0.56
> > > > 3 Auction  Videos     YME         NA   NA      0.57, 0.57
>  NA
> > > > NA  0.56, 0.56
> > > >
> > > >
> > > >
> > > > dat <- structure(list(first.Name = structure(c(3L, 1L, 2L, 3L, 1L,
> 2L,
> > > > 3L, 1L, 2L, 3L, 1L, 2L), .Label = c("Amish", "Ancient", "Auction",
> > > > "Ax", "Bachelorette", "Basketball", "BBQ", "Cake", "Celebrity",
> > > > "Chef", "Clean", "Colonial", "Comedy", "Comic", "Crocodile", "Dog",
> > > > "Empire", "Extreme", "Farm", "Half Pint", "Hollywood", "House", "Ice
> > > > Road", "Jersey", "Justice", "Love", "Mega", "Model", "Modern",
> > > > "Mountain", "Mystery", "Myth", "New York", "Paradise", "Pioneer",
> > > > "Queer", "Restaurant", "Road", "Royal", "Spouse", "Star", "Storage",
> > > > "Survival", "The Great American", "Tool", "Treasure", "Wedding",
> > > > "Wife"), class = "factor"), Name = structure(c(43L, 47L, 29L, 43L,
> > > > 47L, 29L, 43L, 47L, 29L, 43L, 47L, 29L), .Label = c("Aliens",
> > > > "Behavior", "Casino", "Casting Call", "Challenge", "Contest",
> > > > "Crashers", "Crew", "Dad", "Dancing", "Date", "Disasters", "Dynasty",
> > > > "Family", "Garage", "Greenlight", "Gypsies", "Haul", "Hot Rod",
> > > > "Inventor", "Jail", "Job", "Justice", "Marvels", "Master", "Mates",
> > > > "Model", "Moms", "Nation", "Ninja", "Patrol", "People", "Pitmasters",
> > > > "Queens", "Rescue", "Rivals", "Room", "Rooms", "Rules", "Star",
> > > > "Stars", "Superhero", "Videos", "VIP", "Wars", "Wishes", "Wives",
> > > > "Wrangler"), class = "factor"), Department = structure(c(8L, 6L, 2L,
> > > > 8L, 6L, 2L, 8L, 6L, 2L, 8L, 6L, 2L), .Label = c("HXW", "QLH", "RAR",
> > > > "RYC", "SYI", "TAS", "VUV", "YME"), class = "factor"),
> > > >    DCE = c(0.57, 0.59, 0.54, NA, NA, 0.28, 0.57, 0.59, 0.54,
> > > >    NA, NA, 0.28), DP = c(0.56, 0.56, 0.58, NA, NA, 0.29, 0.56,
> > > >    0.56, 0.58, NA, NA, 0.29), date = structure(c(15978, 15978,
> > > >    15978, 16067, 16067, 16067, 16159, 16159, 16159, 16249, 16249,
> > > >    16249), class = "Date")), description = "", row.names = c(5L, 18L,
> > > > 34L, 53L, 66L, 82L, 102L, 115L, 131L, 150L, 163L, 179L), class =
> > > > "data.frame", .Names = c("first.Name", "Name", "Department", "DCE",
> > > > "DP", "date"))
> > > >
> > > >     [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > > David Winsemius
> > > Alameda, CA, USA
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-
> > > guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > ________________________________
> > Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> > Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> > Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento
> email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> > Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou
> modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
> >
> > V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> > - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> > - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
> p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze
> strany p??jemce s dodatkem ?i odchylkou.
> > - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> > - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
> >
> > This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> > If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> > If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> > The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
> >
> > In case that this e-mail forms part of business dealings:
> > - the sender reserves the right to end negotiations about entering into
> a contract in any time, for any reason, and without stating any reasoning.
> > - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> > - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> > - the sender of this e-mail informs that he/she is not authorized to
> enter into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
> >
>
> David Winsemius
> Alameda, CA, USA
>
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sun Nov 20 20:11:29 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 20 Nov 2016 11:11:29 -0800
Subject: [R] aggregate dataframe by multiple factors
In-Reply-To: <CALJKBv8m12aYMhBVLRO9+U=6sZq9Z7qwDMvk_frr7gfBMf5FBA@mail.gmail.com>
References: <CALJKBv9S5=3rYffCbW098eVm8_9XtdE4-FrdWOHTH9AcGWJu4g@mail.gmail.com>
	<89CF0820-5338-4150-A300-78D517CC019B@comcast.net>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C504684B@SRVEXCHMBX.precheza.cz>
	<CALJKBv9pDpDKZ5ZKO0hAh3wZXSXiyVPxyqrj2JEsupffC+tN-w@mail.gmail.com>
	<28273AA6-CE92-4F77-A575-264F41193826@comcast.net>
	<CALJKBv8m12aYMhBVLRO9+U=6sZq9Z7qwDMvk_frr7gfBMf5FBA@mail.gmail.com>
Message-ID: <C69878C3-314C-4911-8C55-5F4190FA0812@comcast.net>


> On Nov 20, 2016, at 10:49 AM, Karim Mezhoud <kmezhoud at gmail.com> wrote:
> 
> Yes the results does not have a date but 
> successive DCE.1, DCE2, DCE.3 indicates
>  DCE.date1 , DCE.date2, DCE.date3 
> I hope that the chronological order of date is conserved.

There are, however, 4 dates. See this pair of results. You can probably do something if you ever figure out what it is that you precisely want. I think the date ordering is automatic here:

> require(reshape2)
Loading required package: reshape2

> dcast(dat, first.Name +  Name + Department ~ date, value.var='DCE')
  first.Name   Name Department 2013-09-30 2013-12-28 2014-03-30 2014-06-28
1      Amish  Wives        TAS       0.59         NA       0.59         NA
2    Ancient Nation        QLH       0.54       0.28       0.54       0.28
3    Auction Videos        YME       0.57         NA       0.57         NA
> dcast(dat, first.Name +  Name + Department ~ date, value.var='DP')
  first.Name   Name Department 2013-09-30 2013-12-28 2014-03-30 2014-06-28
1      Amish  Wives        TAS       0.56         NA       0.56         NA
2    Ancient Nation        QLH       0.58       0.29       0.58       0.29
3    Auction Videos        YME       0.56         NA       0.56         NA



> Thanks,
> Karim
> 
> 
> On Sun, Nov 20, 2016 at 7:44 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> > On Nov 20, 2016, at 5:28 AM, Karim Mezhoud <kmezhoud at gmail.com> wrote:
> >
> > Sorry for the delay,
> > Many Thanks for Mr. David and Mr. Petr
> > I thinked  to use "sort" function to arrange chronologically  value by  'date' (without 'date' is colnames) of each variables (DCE, DP).
> >
> >
> > The solution of David seems to be simple to understand with "unlist" function.
> > The solution of Petr seems to be fancy. I did not find document  about "I" argument for aggregate function.
> >
> > How can know which value for which date?
> 
> You asked for a functional reshaping that did not have the date. Now you want the date? There was no date in the result you indicated was desired.   ....??????
> 
> --
> David.
> 
> >
> > I will save the reshaping/ordering dataframe  for later use.
> > Many Thanks,
> > Karim
> >
> >
> > On Fri, Nov 18, 2016 at 11:34 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:
> > Hi
> >
> > same result can be achieved by
> >
> > dat.ag<-aggregate(dat[ , c("DCE","DP")], by= list(dat$first.Name, dat$Name, dat$Department) , "I")
> >
> > Sorting according to the first row seems to be quite tricky. You could probably get closer by using some combination of split and order and arranging back chunks  of data
> >
> > ooo1<-order(split(dat$DCE,interaction(dat$first.Name, dat$Name, dat$Department, drop=T))[[1]])
> > data.frame(sapply(split(dat$DCE,interaction(dat$first.Name, dat$Name, dat$Department, drop=T)), rbind))[ooo1,]
> >   Ancient.Nation.QLH Amish.Wives.TAS Auction.Videos.YME
> > 2               0.28              NA                 NA
> > 4               0.28              NA                 NA
> > 1               0.54            0.59               0.57
> > 3               0.54            0.59               0.57
> >
> > however I wonder why the order according to the first row is necessary if all NAs are on correct positions?
> >
> > Cheers
> > Petr
> >
> >
> > > -----Original Message-----
> > > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of David
> > > Winsemius
> > > Sent: Friday, November 18, 2016 9:30 AM
> > > To: Karim Mezhoud <kmezhoud at gmail.com>
> > > Cc: r-help at r-project.org
> > > Subject: Re: [R] aggregate dataframe by multiple factors
> > >
> > >
> > > > On Nov 17, 2016, at 11:27 PM, Karim Mezhoud <kmezhoud at gmail.com>
> > > wrote:
> > > >
> > > > Dear all,
> > > >
> > > > the dat  has missing values NA,
> > > >
> > > >    first.Name   Name Department  DCE   DP       date
> > > > 5      Auction Videos        YME 0.57 0.56 2013-09-30
> > > > 18       Amish  Wives        TAS 0.59 0.56 2013-09-30
> > > > 34     Ancient Nation        QLH 0.54 0.58 2013-09-30
> > > > 53     Auction Videos        YME   NA   NA 2013-12-28
> > > > 66       Amish  Wives        TAS   NA   NA 2013-12-28
> > > > 82     Ancient Nation        QLH 0.28 0.29 2013-12-28
> > > > 102    Auction Videos        YME 0.57 0.56 2014-03-30
> > > > 115      Amish  Wives        TAS 0.59 0.56 2014-03-30
> > > > 131    Ancient Nation        QLH 0.54 0.58 2014-03-30
> > > > 150    Auction Videos        YME   NA   NA 2014-06-28
> > > > 163      Amish  Wives        TAS   NA   NA 2014-06-28
> > > > 179    Ancient Nation        QLH 0.28 0.29 2014-06-28
> > > >
> > > >
> > > > agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> > > > list(dat$first.Name, dat$Name, dat$Department) , "sort"))
> > >
> > > The closest I could get on a few attempts was:
> > >
> > > (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> > > list(dat$first.Name, dat$Name, dat$Department) , function(d) { unlist(d)}))
> > >  )
> > >
> > >   Group.1 Group.2 Group.3 DCE.1 DCE.2 DCE.3 DCE.4 DP.1 DP.2 DP.3 DP.4
> > > 1 Ancient  Nation     QLH  0.54  0.28  0.54  0.28 0.58 0.29 0.58 0.29
> > > 2   Amish   Wives     TAS  0.59    NA  0.59    NA 0.56   NA 0.56   NA
> > > 3 Auction  Videos     YME  0.57    NA  0.57    NA 0.56   NA 0.56   NA
> > >
> > > I think the sort operation might be somewhat ambiguous in this instance. I
> > > tried:
> > >
> > >  (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> > > list(dat$first.Name, dat$Name, dat$Department) , function(d) {
> > > unlist(lapply(d,sort))}))
> > >  )
> > >
> > > With no success, not even a sorted result.
> > >
> > > --
> > > David.
> > > >
> > > >
> > > > agg has list of value. I would separate value in different columns.
> > > >
> > > >  Group.1 Group.2 Group.3                    DCE                     DP
> > > > 1 Ancient  Nation     QLH 0.28, 0.28, 0.54, 0.54 0.29, 0.29, 0.58, 0.58
> > > > 2   Amish   Wives     TAS             0.59, 0.59             0.56, 0.56
> > > > 3 Auction  Videos     YME             0.57, 0.57             0.56, 0.56
> > > >
> > > > The  goal:
> > > >
> > > > Group.1 Group.2 Group.3  DCE.1 DCE.2 DCE.3  DCE.4  DP.1  DP.2  DP.3  DP.4
> > > > 1 Ancient  Nation     QLH    0.28     0.28    0.54     0.54     0.29, 0.29,
> > > > 0.58, 0.58
> > > > 2   Amish   Wives     TAS        NA     NA     0.59, 0.59           NA
> > > > NA  0.56, 0.56
> > > > 3 Auction  Videos     YME         NA   NA      0.57, 0.57             NA
> > > > NA  0.56, 0.56
> > > >
> > > >
> > > >
> > > > dat <- structure(list(first.Name = structure(c(3L, 1L, 2L, 3L, 1L, 2L,
> > > > 3L, 1L, 2L, 3L, 1L, 2L), .Label = c("Amish", "Ancient", "Auction",
> > > > "Ax", "Bachelorette", "Basketball", "BBQ", "Cake", "Celebrity",
> > > > "Chef", "Clean", "Colonial", "Comedy", "Comic", "Crocodile", "Dog",
> > > > "Empire", "Extreme", "Farm", "Half Pint", "Hollywood", "House", "Ice
> > > > Road", "Jersey", "Justice", "Love", "Mega", "Model", "Modern",
> > > > "Mountain", "Mystery", "Myth", "New York", "Paradise", "Pioneer",
> > > > "Queer", "Restaurant", "Road", "Royal", "Spouse", "Star", "Storage",
> > > > "Survival", "The Great American", "Tool", "Treasure", "Wedding",
> > > > "Wife"), class = "factor"), Name = structure(c(43L, 47L, 29L, 43L,
> > > > 47L, 29L, 43L, 47L, 29L, 43L, 47L, 29L), .Label = c("Aliens",
> > > > "Behavior", "Casino", "Casting Call", "Challenge", "Contest",
> > > > "Crashers", "Crew", "Dad", "Dancing", "Date", "Disasters", "Dynasty",
> > > > "Family", "Garage", "Greenlight", "Gypsies", "Haul", "Hot Rod",
> > > > "Inventor", "Jail", "Job", "Justice", "Marvels", "Master", "Mates",
> > > > "Model", "Moms", "Nation", "Ninja", "Patrol", "People", "Pitmasters",
> > > > "Queens", "Rescue", "Rivals", "Room", "Rooms", "Rules", "Star",
> > > > "Stars", "Superhero", "Videos", "VIP", "Wars", "Wishes", "Wives",
> > > > "Wrangler"), class = "factor"), Department = structure(c(8L, 6L, 2L,
> > > > 8L, 6L, 2L, 8L, 6L, 2L, 8L, 6L, 2L), .Label = c("HXW", "QLH", "RAR",
> > > > "RYC", "SYI", "TAS", "VUV", "YME"), class = "factor"),
> > > >    DCE = c(0.57, 0.59, 0.54, NA, NA, 0.28, 0.57, 0.59, 0.54,
> > > >    NA, NA, 0.28), DP = c(0.56, 0.56, 0.58, NA, NA, 0.29, 0.56,
> > > >    0.56, 0.58, NA, NA, 0.29), date = structure(c(15978, 15978,
> > > >    15978, 16067, 16067, 16067, 16159, 16159, 16159, 16249, 16249,
> > > >    16249), class = "Date")), description = "", row.names = c(5L, 18L,
> > > > 34L, 53L, 66L, 82L, 102L, 115L, 131L, 150L, 163L, 179L), class =
> > > > "data.frame", .Names = c("first.Name", "Name", "Department", "DCE",
> > > > "DP", "date"))
> > > >
> > > >     [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > > David Winsemius
> > > Alameda, CA, USA
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-
> > > guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > ________________________________
> > Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
> > Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
> > Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> > Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
> >
> > V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> > - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> > - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
> > - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> > - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
> >
> > This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> > If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> > If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> > The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
> >
> > In case that this e-mail forms part of business dealings:
> > - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> > - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> > - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> > - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.
> >
> 
> David Winsemius
> Alameda, CA, USA
> 
> 

David Winsemius
Alameda, CA, USA


From bgunter.4567 at gmail.com  Sun Nov 20 20:15:08 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sun, 20 Nov 2016 11:15:08 -0800
Subject: [R] Need some help with regular expression
In-Reply-To: <00a301d242e3$7d884e20$7898ea60$@gmail.com>
References: <00a301d242e3$7d884e20$7898ea60$@gmail.com>
Message-ID: <CAGxFJbRLyPgD0RJG9Y-ZG4qumPjp4UWi7h1iF4gm1kMYhXeh7g@mail.gmail.com>

If I understand you correctly, I think you are making it more complex
than necessary. Using your example (thanks!!), the following should
get you started:


> x<- c("Name.MEMBER_TYPE: NMA -> STU ; CATEGORY:  -> 1 ; CITY: MISSISSAUGA -> Mississauga ; ZIP: L5N1H9 -> L5N 1H9 ; COUNTRY: CAN ->  ; MEMBER_STATUS:  -> N", "Name.MEMBER_TYPE: STU -> REG ; CATEGORY: 1 ->","Name.MEMBER_TYPE: -> STU")
>
> x
[1] "Name.MEMBER_TYPE: NMA -> STU ; CATEGORY:  -> 1 ; CITY:
MISSISSAUGA -> Mississauga ; ZIP: L5N1H9 -> L5N 1H9 ; COUNTRY: CAN ->
; MEMBER_STATUS:  -> N"

[2] "Name.MEMBER_TYPE: STU -> REG ; CATEGORY: 1 ->"
[3] "Name.MEMBER_TYPE: -> STU"
>
> sub(".*: *([[:alnum:]]* *-> *STU|STU *-> *[[:alnum:]]*).*","\\1",x)
[1] "NMA -> STU" "STU -> REG" "-> STU"


I am sure that you can get things to the form you desire in one go
with some fiddling of the above, but it was easier for me to write the
regex to pick out the pieces you wanted and leave the rest to you.
Others may have slicker ways to do it, of course.

HTH

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Nov 19, 2016 at 8:06 PM, Steven Nagy <nstefi at gmail.com> wrote:
> I tried out a regular expression on this website:
>
> http://regexr.com/3en1m
>
>
>
> So the input text is:
>
> "Name.MEMBER_TYPE:  -> STU"
>
>
>
> The regular expression is: ((?:\w+|\s) -> STU|STU -> (?:\w+|\s))
>
> And it returns:
>
> "  -> STU"
>
>
>
> but when I use in R, it doesn't return the same result:
>
> strapply(c, "((?:\\w+|\\s) -> STU|STU -> (?:\\w+|\\s))", c, backref = -1,
> perl = TRUE)
>
> returns:
> "Name.MEMBER_TYPE: -> STU"
>
>
>
>
>
> Here is what I was trying to do:
>
>
>
> I need to extract some values from a log table, and I created a regular
> expression that helps me with that.
>
> The log table has cells with values like:
>
> a = "Name.MEMBER_TYPE: NMA -> STU ; CATEGORY:  -> 1 ; CITY: MISSISSAUGA ->
> Mississauga ; ZIP: L5N1H9 -> L5N 1H9 ; COUNTRY: CAN ->  ; MEMBER_STATUS:  ->
> N"
>
> or
> b = "Name.MEMBER_TYPE: STU -> REG ; CATEGORY: 1 ->"
>
> so I needed to extract the values that a STU member type is changing from
> and to, so I needed NMA, STU in the 1st case or STU, REG in the 2nd case.
>
> I came up with this expression which worked in both cases:
>
> strapply(strapply(a, "(\\w+ -> STU|STU -> \\w+)", c, backref = -1, perl =
> TRUE), "(\\w+) -> (\\w+)", c, backref = -2, perl = TRUE)
>
>
>
> But I had a 3rd case when the source member type was blank:
>
> c = "Name.MEMBER_TYPE: -> STU"
>
> and in that case it returned an error:
>
> strapply(strapply(c, "(\\w+ -> STU|STU -> \\w+)", c, backref = -1, perl =
> TRUE), "(\\w+) -> (\\w+)", c, backref = -2, perl = TRUE)
>
> Error: is.character(x) is not TRUE
>
>
>
> I found that the error is because this returns NULL:
>
> strapply(c, "(\\w+ -> STU|STU -> \\w+)", c, backref = -1, perl = TRUE)
>
>
>
>
>
> So I tried to modify the regular expression to match any word or blank
> space:
>
> strapply(c, "((?:\\w+|\\s) -> STU|STU -> (?:\\w+|\\s))", c, backref = -1,
> perl = TRUE)
>
>
>
> but this returned me the whole value of "c":
>
> "Name.MEMBER_TYPE:  -> STU"
>
> and I only needed "  -> STU" as it shows on the website regxr.com
>
>
>
> Is the result wrong on the regxr.com website or strapply returns the wrong
> result?
>
>
>
> Thanks,
>
> Steven
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Sun Nov 20 20:17:13 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 20 Nov 2016 11:17:13 -0800
Subject: [R] aggregate dataframe by multiple factors
In-Reply-To: <C69878C3-314C-4911-8C55-5F4190FA0812@comcast.net>
References: <CALJKBv9S5=3rYffCbW098eVm8_9XtdE4-FrdWOHTH9AcGWJu4g@mail.gmail.com>
	<89CF0820-5338-4150-A300-78D517CC019B@comcast.net>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C504684B@SRVEXCHMBX.precheza.cz>
	<CALJKBv9pDpDKZ5ZKO0hAh3wZXSXiyVPxyqrj2JEsupffC+tN-w@mail.gmail.com>
	<28273AA6-CE92-4F77-A575-264F41193826@comcast.net>
	<CALJKBv8m12aYMhBVLRO9+U=6sZq9Z7qwDMvk_frr7gfBMf5FBA@mail.gmail.com>
	<C69878C3-314C-4911-8C55-5F4190FA0812@comcast.net>
Message-ID: <3F3CE841-AA11-463B-828A-D35585460FBD@comcast.net>


> On Nov 20, 2016, at 11:11 AM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
>> On Nov 20, 2016, at 10:49 AM, Karim Mezhoud <kmezhoud at gmail.com> wrote:
>> 
>> Yes the results does not have a date but 
>> successive DCE.1, DCE2, DCE.3 indicates
>> DCE.date1 , DCE.date2, DCE.date3 
>> I hope that the chronological order of date is conserved.
> 
> There are, however, 4 dates. See this pair of results. You can probably do something if you ever figure out what it is that you precisely want. I think the date ordering is automatic here:
> 
>> require(reshape2)
> Loading required package: reshape2
> 
>> dcast(dat, first.Name +  Name + Department ~ date, value.var='DCE')
>  first.Name   Name Department 2013-09-30 2013-12-28 2014-03-30 2014-06-28
> 1      Amish  Wives        TAS       0.59         NA       0.59         NA
> 2    Ancient Nation        QLH       0.54       0.28       0.54       0.28
> 3    Auction Videos        YME       0.57         NA       0.57         NA
>> dcast(dat, first.Name +  Name + Department ~ date, value.var='DP')
>  first.Name   Name Department 2013-09-30 2013-12-28 2014-03-30 2014-06-28
> 1      Amish  Wives        TAS       0.56         NA       0.56         NA
> 2    Ancient Nation        QLH       0.58       0.29       0.58       0.29
> 3    Auction Videos        YME       0.56         NA       0.56         NA

Yjos completes the process in the reshape2 world:

> mdat <- melt(dat, measure.vars=c("DCE", 'DP') )
> str(mdat)
'data.frame':	24 obs. of  6 variables:
 $ first.Name: Factor w/ 48 levels "Amish","Ancient",..: 3 1 2 3 1 2 3 1 2 3 ...
 $ Name      : Factor w/ 48 levels "Aliens","Behavior",..: 43 47 29 43 47 29 43 47 29 43 ...
 $ Department: Factor w/ 8 levels "HXW","QLH","RAR",..: 8 6 2 8 6 2 8 6 2 8 ...
 $ date      : Date, format: "2013-09-30" "2013-09-30" ...
 $ variable  : Factor w/ 2 levels "DCE","DP": 1 1 1 1 1 1 1 1 1 1 ...
 $ value     : num  0.57 0.59 0.54 NA NA 0.28 0.57 0.59 0.54 NA ...

> dcast(mdat, first.Name +  Name + Department ~ date+variable )
  first.Name   Name Department 2013-09-30_DCE 2013-09-30_DP 2013-12-28_DCE
1      Amish  Wives        TAS           0.59          0.56             NA
2    Ancient Nation        QLH           0.54          0.58           0.28
3    Auction Videos        YME           0.57          0.56             NA
  2013-12-28_DP 2014-03-30_DCE 2014-03-30_DP 2014-06-28_DCE 2014-06-28_DP
1            NA           0.59          0.56             NA            NA
2          0.29           0.54          0.58           0.28          0.29
3            NA           0.57          0.56             NA            NA

-- 
David.

> 
> 
> 
>> Thanks,
>> Karim
>> 
>> 
>> On Sun, Nov 20, 2016 at 7:44 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>> 
>>> On Nov 20, 2016, at 5:28 AM, Karim Mezhoud <kmezhoud at gmail.com> wrote:
>>> 
>>> Sorry for the delay,
>>> Many Thanks for Mr. David and Mr. Petr
>>> I thinked  to use "sort" function to arrange chronologically  value by  'date' (without 'date' is colnames) of each variables (DCE, DP).
>>> 
>>> 
>>> The solution of David seems to be simple to understand with "unlist" function.
>>> The solution of Petr seems to be fancy. I did not find document  about "I" argument for aggregate function.
>>> 
>>> How can know which value for which date?
>> 
>> You asked for a functional reshaping that did not have the date. Now you want the date? There was no date in the result you indicated was desired.   ....??????
>> 
>> --
>> David.
>> 
>>> 
>>> I will save the reshaping/ordering dataframe  for later use.
>>> Many Thanks,
>>> Karim
>>> 
>>> 
>>> On Fri, Nov 18, 2016 at 11:34 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:
>>> Hi
>>> 
>>> same result can be achieved by
>>> 
>>> dat.ag<-aggregate(dat[ , c("DCE","DP")], by= list(dat$first.Name, dat$Name, dat$Department) , "I")
>>> 
>>> Sorting according to the first row seems to be quite tricky. You could probably get closer by using some combination of split and order and arranging back chunks  of data
>>> 
>>> ooo1<-order(split(dat$DCE,interaction(dat$first.Name, dat$Name, dat$Department, drop=T))[[1]])
>>> data.frame(sapply(split(dat$DCE,interaction(dat$first.Name, dat$Name, dat$Department, drop=T)), rbind))[ooo1,]
>>>  Ancient.Nation.QLH Amish.Wives.TAS Auction.Videos.YME
>>> 2               0.28              NA                 NA
>>> 4               0.28              NA                 NA
>>> 1               0.54            0.59               0.57
>>> 3               0.54            0.59               0.57
>>> 
>>> however I wonder why the order according to the first row is necessary if all NAs are on correct positions?
>>> 
>>> Cheers
>>> Petr
>>> 
>>> 
>>>> -----Original Message-----
>>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of David
>>>> Winsemius
>>>> Sent: Friday, November 18, 2016 9:30 AM
>>>> To: Karim Mezhoud <kmezhoud at gmail.com>
>>>> Cc: r-help at r-project.org
>>>> Subject: Re: [R] aggregate dataframe by multiple factors
>>>> 
>>>> 
>>>>> On Nov 17, 2016, at 11:27 PM, Karim Mezhoud <kmezhoud at gmail.com>
>>>> wrote:
>>>>> 
>>>>> Dear all,
>>>>> 
>>>>> the dat  has missing values NA,
>>>>> 
>>>>>   first.Name   Name Department  DCE   DP       date
>>>>> 5      Auction Videos        YME 0.57 0.56 2013-09-30
>>>>> 18       Amish  Wives        TAS 0.59 0.56 2013-09-30
>>>>> 34     Ancient Nation        QLH 0.54 0.58 2013-09-30
>>>>> 53     Auction Videos        YME   NA   NA 2013-12-28
>>>>> 66       Amish  Wives        TAS   NA   NA 2013-12-28
>>>>> 82     Ancient Nation        QLH 0.28 0.29 2013-12-28
>>>>> 102    Auction Videos        YME 0.57 0.56 2014-03-30
>>>>> 115      Amish  Wives        TAS 0.59 0.56 2014-03-30
>>>>> 131    Ancient Nation        QLH 0.54 0.58 2014-03-30
>>>>> 150    Auction Videos        YME   NA   NA 2014-06-28
>>>>> 163      Amish  Wives        TAS   NA   NA 2014-06-28
>>>>> 179    Ancient Nation        QLH 0.28 0.29 2014-06-28
>>>>> 
>>>>> 
>>>>> agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
>>>>> list(dat$first.Name, dat$Name, dat$Department) , "sort"))
>>>> 
>>>> The closest I could get on a few attempts was:
>>>> 
>>>> (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
>>>> list(dat$first.Name, dat$Name, dat$Department) , function(d) { unlist(d)}))
>>>> )
>>>> 
>>>>  Group.1 Group.2 Group.3 DCE.1 DCE.2 DCE.3 DCE.4 DP.1 DP.2 DP.3 DP.4
>>>> 1 Ancient  Nation     QLH  0.54  0.28  0.54  0.28 0.58 0.29 0.58 0.29
>>>> 2   Amish   Wives     TAS  0.59    NA  0.59    NA 0.56   NA 0.56   NA
>>>> 3 Auction  Videos     YME  0.57    NA  0.57    NA 0.56   NA 0.56   NA
>>>> 
>>>> I think the sort operation might be somewhat ambiguous in this instance. I
>>>> tried:
>>>> 
>>>> (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
>>>> list(dat$first.Name, dat$Name, dat$Department) , function(d) {
>>>> unlist(lapply(d,sort))}))
>>>> )
>>>> 
>>>> With no success, not even a sorted result.
>>>> 
>>>> --
>>>> David.
>>>>> 
>>>>> 
>>>>> agg has list of value. I would separate value in different columns.
>>>>> 
>>>>> Group.1 Group.2 Group.3                    DCE                     DP
>>>>> 1 Ancient  Nation     QLH 0.28, 0.28, 0.54, 0.54 0.29, 0.29, 0.58, 0.58
>>>>> 2   Amish   Wives     TAS             0.59, 0.59             0.56, 0.56
>>>>> 3 Auction  Videos     YME             0.57, 0.57             0.56, 0.56
>>>>> 
>>>>> The  goal:
>>>>> 
>>>>> Group.1 Group.2 Group.3  DCE.1 DCE.2 DCE.3  DCE.4  DP.1  DP.2  DP.3  DP.4
>>>>> 1 Ancient  Nation     QLH    0.28     0.28    0.54     0.54     0.29, 0.29,
>>>>> 0.58, 0.58
>>>>> 2   Amish   Wives     TAS        NA     NA     0.59, 0.59           NA
>>>>> NA  0.56, 0.56
>>>>> 3 Auction  Videos     YME         NA   NA      0.57, 0.57             NA
>>>>> NA  0.56, 0.56
>>>>> 
>>>>> 
>>>>> 
>>>>> dat <- structure(list(first.Name = structure(c(3L, 1L, 2L, 3L, 1L, 2L,
>>>>> 3L, 1L, 2L, 3L, 1L, 2L), .Label = c("Amish", "Ancient", "Auction",
>>>>> "Ax", "Bachelorette", "Basketball", "BBQ", "Cake", "Celebrity",
>>>>> "Chef", "Clean", "Colonial", "Comedy", "Comic", "Crocodile", "Dog",
>>>>> "Empire", "Extreme", "Farm", "Half Pint", "Hollywood", "House", "Ice
>>>>> Road", "Jersey", "Justice", "Love", "Mega", "Model", "Modern",
>>>>> "Mountain", "Mystery", "Myth", "New York", "Paradise", "Pioneer",
>>>>> "Queer", "Restaurant", "Road", "Royal", "Spouse", "Star", "Storage",
>>>>> "Survival", "The Great American", "Tool", "Treasure", "Wedding",
>>>>> "Wife"), class = "factor"), Name = structure(c(43L, 47L, 29L, 43L,
>>>>> 47L, 29L, 43L, 47L, 29L, 43L, 47L, 29L), .Label = c("Aliens",
>>>>> "Behavior", "Casino", "Casting Call", "Challenge", "Contest",
>>>>> "Crashers", "Crew", "Dad", "Dancing", "Date", "Disasters", "Dynasty",
>>>>> "Family", "Garage", "Greenlight", "Gypsies", "Haul", "Hot Rod",
>>>>> "Inventor", "Jail", "Job", "Justice", "Marvels", "Master", "Mates",
>>>>> "Model", "Moms", "Nation", "Ninja", "Patrol", "People", "Pitmasters",
>>>>> "Queens", "Rescue", "Rivals", "Room", "Rooms", "Rules", "Star",
>>>>> "Stars", "Superhero", "Videos", "VIP", "Wars", "Wishes", "Wives",
>>>>> "Wrangler"), class = "factor"), Department = structure(c(8L, 6L, 2L,
>>>>> 8L, 6L, 2L, 8L, 6L, 2L, 8L, 6L, 2L), .Label = c("HXW", "QLH", "RAR",
>>>>> "RYC", "SYI", "TAS", "VUV", "YME"), class = "factor"),
>>>>>   DCE = c(0.57, 0.59, 0.54, NA, NA, 0.28, 0.57, 0.59, 0.54,
>>>>>   NA, NA, 0.28), DP = c(0.56, 0.56, 0.58, NA, NA, 0.29, 0.56,
>>>>>   0.56, 0.58, NA, NA, 0.29), date = structure(c(15978, 15978,
>>>>>   15978, 16067, 16067, 16067, 16159, 16159, 16159, 16249, 16249,
>>>>>   16249), class = "Date")), description = "", row.names = c(5L, 18L,
>>>>> 34L, 53L, 66L, 82L, 102L, 115L, 131L, 150L, 163L, 179L), class =
>>>>> "data.frame", .Names = c("first.Name", "Name", "Department", "DCE",
>>>>> "DP", "date"))
>>>>> 
>>>>>    [[alternative HTML version deleted]]
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> David Winsemius
>>>> Alameda, CA, USA
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>>> guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> ________________________________
>>> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
>>> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
>>> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
>>> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>>> 
>>> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
>>> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
>>> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
>>> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
>>> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>>> 
>>> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
>>> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
>>> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
>>> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
>>> 
>>> In case that this e-mail forms part of business dealings:
>>> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
>>> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
>>> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
>>> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.
>>> 
>> 
>> David Winsemius
>> Alameda, CA, USA
>> 
>> 
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From kmezhoud at gmail.com  Sun Nov 20 20:26:32 2016
From: kmezhoud at gmail.com (Karim Mezhoud)
Date: Sun, 20 Nov 2016 20:26:32 +0100
Subject: [R] aggregate dataframe by multiple factors
In-Reply-To: <3F3CE841-AA11-463B-828A-D35585460FBD@comcast.net>
References: <CALJKBv9S5=3rYffCbW098eVm8_9XtdE4-FrdWOHTH9AcGWJu4g@mail.gmail.com>
	<89CF0820-5338-4150-A300-78D517CC019B@comcast.net>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C504684B@SRVEXCHMBX.precheza.cz>
	<CALJKBv9pDpDKZ5ZKO0hAh3wZXSXiyVPxyqrj2JEsupffC+tN-w@mail.gmail.com>
	<28273AA6-CE92-4F77-A575-264F41193826@comcast.net>
	<CALJKBv8m12aYMhBVLRO9+U=6sZq9Z7qwDMvk_frr7gfBMf5FBA@mail.gmail.com>
	<C69878C3-314C-4911-8C55-5F4190FA0812@comcast.net>
	<3F3CE841-AA11-463B-828A-D35585460FBD@comcast.net>
Message-ID: <CALJKBv_7hmtXxPtzwdqNrCjrAU_Ne4ChY31QfmExnvMmvSpd5w@mail.gmail.com>

Many Thanks,
What is exactly the solution.
Best,
Karim

On Sun, Nov 20, 2016 at 8:17 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Nov 20, 2016, at 11:11 AM, David Winsemius <dwinsemius at comcast.net>
> wrote:
> >
> >
> >> On Nov 20, 2016, at 10:49 AM, Karim Mezhoud <kmezhoud at gmail.com> wrote:
> >>
> >> Yes the results does not have a date but
> >> successive DCE.1, DCE2, DCE.3 indicates
> >> DCE.date1 , DCE.date2, DCE.date3
> >> I hope that the chronological order of date is conserved.
> >
> > There are, however, 4 dates. See this pair of results. You can probably
> do something if you ever figure out what it is that you precisely want. I
> think the date ordering is automatic here:
> >
> >> require(reshape2)
> > Loading required package: reshape2
> >
> >> dcast(dat, first.Name +  Name + Department ~ date, value.var='DCE')
> >  first.Name   Name Department 2013-09-30 2013-12-28 2014-03-30
> 2014-06-28
> > 1      Amish  Wives        TAS       0.59         NA       0.59
>  NA
> > 2    Ancient Nation        QLH       0.54       0.28       0.54
>  0.28
> > 3    Auction Videos        YME       0.57         NA       0.57
>  NA
> >> dcast(dat, first.Name +  Name + Department ~ date, value.var='DP')
> >  first.Name   Name Department 2013-09-30 2013-12-28 2014-03-30 2014-06-28
> > 1      Amish  Wives        TAS       0.56         NA       0.56
>  NA
> > 2    Ancient Nation        QLH       0.58       0.29       0.58
>  0.29
> > 3    Auction Videos        YME       0.56         NA       0.56
>  NA
>
> Yjos completes the process in the reshape2 world:
>
> > mdat <- melt(dat, measure.vars=c("DCE", 'DP') )
> > str(mdat)
> 'data.frame':   24 obs. of  6 variables:
>  $ first.Name: Factor w/ 48 levels "Amish","Ancient",..: 3 1 2 3 1 2 3 1 2
> 3 ...
>  $ Name      : Factor w/ 48 levels "Aliens","Behavior",..: 43 47 29 43 47
> 29 43 47 29 43 ...
>  $ Department: Factor w/ 8 levels "HXW","QLH","RAR",..: 8 6 2 8 6 2 8 6 2
> 8 ...
>  $ date      : Date, format: "2013-09-30" "2013-09-30" ...
>  $ variable  : Factor w/ 2 levels "DCE","DP": 1 1 1 1 1 1 1 1 1 1 ...
>  $ value     : num  0.57 0.59 0.54 NA NA 0.28 0.57 0.59 0.54 NA ...
>
> > dcast(mdat, first.Name +  Name + Department ~ date+variable )
>   first.Name   Name Department 2013-09-30_DCE 2013-09-30_DP 2013-12-28_DCE
> 1      Amish  Wives        TAS           0.59          0.56             NA
> 2    Ancient Nation        QLH           0.54          0.58           0.28
> 3    Auction Videos        YME           0.57          0.56             NA
>   2013-12-28_DP 2014-03-30_DCE 2014-03-30_DP 2014-06-28_DCE 2014-06-28_DP
> 1            NA           0.59          0.56             NA            NA
> 2          0.29           0.54          0.58           0.28          0.29
> 3            NA           0.57          0.56             NA            NA
>
> --
> David.
>
> >
> >
> >
> >> Thanks,
> >> Karim
> >>
> >>
> >> On Sun, Nov 20, 2016 at 7:44 PM, David Winsemius <
> dwinsemius at comcast.net> wrote:
> >>
> >>> On Nov 20, 2016, at 5:28 AM, Karim Mezhoud <kmezhoud at gmail.com> wrote:
> >>>
> >>> Sorry for the delay,
> >>> Many Thanks for Mr. David and Mr. Petr
> >>> I thinked  to use "sort" function to arrange chronologically  value
> by  'date' (without 'date' is colnames) of each variables (DCE, DP).
> >>>
> >>>
> >>> The solution of David seems to be simple to understand with "unlist"
> function.
> >>> The solution of Petr seems to be fancy. I did not find document  about
> "I" argument for aggregate function.
> >>>
> >>> How can know which value for which date?
> >>
> >> You asked for a functional reshaping that did not have the date. Now
> you want the date? There was no date in the result you indicated was
> desired.   ....??????
> >>
> >> --
> >> David.
> >>
> >>>
> >>> I will save the reshaping/ordering dataframe  for later use.
> >>> Many Thanks,
> >>> Karim
> >>>
> >>>
> >>> On Fri, Nov 18, 2016 at 11:34 AM, PIKAL Petr <petr.pikal at precheza.cz>
> wrote:
> >>> Hi
> >>>
> >>> same result can be achieved by
> >>>
> >>> dat.ag<-aggregate(dat[ , c("DCE","DP")], by= list(dat$first.Name,
> dat$Name, dat$Department) , "I")
> >>>
> >>> Sorting according to the first row seems to be quite tricky. You could
> probably get closer by using some combination of split and order and
> arranging back chunks  of data
> >>>
> >>> ooo1<-order(split(dat$DCE,interaction(dat$first.Name, dat$Name,
> dat$Department, drop=T))[[1]])
> >>> data.frame(sapply(split(dat$DCE,interaction(dat$first.Name, dat$Name,
> dat$Department, drop=T)), rbind))[ooo1,]
> >>>  Ancient.Nation.QLH Amish.Wives.TAS Auction.Videos.YME
> >>> 2               0.28              NA                 NA
> >>> 4               0.28              NA                 NA
> >>> 1               0.54            0.59               0.57
> >>> 3               0.54            0.59               0.57
> >>>
> >>> however I wonder why the order according to the first row is necessary
> if all NAs are on correct positions?
> >>>
> >>> Cheers
> >>> Petr
> >>>
> >>>
> >>>> -----Original Message-----
> >>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of David
> >>>> Winsemius
> >>>> Sent: Friday, November 18, 2016 9:30 AM
> >>>> To: Karim Mezhoud <kmezhoud at gmail.com>
> >>>> Cc: r-help at r-project.org
> >>>> Subject: Re: [R] aggregate dataframe by multiple factors
> >>>>
> >>>>
> >>>>> On Nov 17, 2016, at 11:27 PM, Karim Mezhoud <kmezhoud at gmail.com>
> >>>> wrote:
> >>>>>
> >>>>> Dear all,
> >>>>>
> >>>>> the dat  has missing values NA,
> >>>>>
> >>>>>   first.Name   Name Department  DCE   DP       date
> >>>>> 5      Auction Videos        YME 0.57 0.56 2013-09-30
> >>>>> 18       Amish  Wives        TAS 0.59 0.56 2013-09-30
> >>>>> 34     Ancient Nation        QLH 0.54 0.58 2013-09-30
> >>>>> 53     Auction Videos        YME   NA   NA 2013-12-28
> >>>>> 66       Amish  Wives        TAS   NA   NA 2013-12-28
> >>>>> 82     Ancient Nation        QLH 0.28 0.29 2013-12-28
> >>>>> 102    Auction Videos        YME 0.57 0.56 2014-03-30
> >>>>> 115      Amish  Wives        TAS 0.59 0.56 2014-03-30
> >>>>> 131    Ancient Nation        QLH 0.54 0.58 2014-03-30
> >>>>> 150    Auction Videos        YME   NA   NA 2014-06-28
> >>>>> 163      Amish  Wives        TAS   NA   NA 2014-06-28
> >>>>> 179    Ancient Nation        QLH 0.28 0.29 2014-06-28
> >>>>>
> >>>>>
> >>>>> agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> >>>>> list(dat$first.Name, dat$Name, dat$Department) , "sort"))
> >>>>
> >>>> The closest I could get on a few attempts was:
> >>>>
> >>>> (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> >>>> list(dat$first.Name, dat$Name, dat$Department) , function(d) {
> unlist(d)}))
> >>>> )
> >>>>
> >>>>  Group.1 Group.2 Group.3 DCE.1 DCE.2 DCE.3 DCE.4 DP.1 DP.2 DP.3 DP.4
> >>>> 1 Ancient  Nation     QLH  0.54  0.28  0.54  0.28 0.58 0.29 0.58 0.29
> >>>> 2   Amish   Wives     TAS  0.59    NA  0.59    NA 0.56   NA 0.56   NA
> >>>> 3 Auction  Videos     YME  0.57    NA  0.57    NA 0.56   NA 0.56   NA
> >>>>
> >>>> I think the sort operation might be somewhat ambiguous in this
> instance. I
> >>>> tried:
> >>>>
> >>>> (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> >>>> list(dat$first.Name, dat$Name, dat$Department) , function(d) {
> >>>> unlist(lapply(d,sort))}))
> >>>> )
> >>>>
> >>>> With no success, not even a sorted result.
> >>>>
> >>>> --
> >>>> David.
> >>>>>
> >>>>>
> >>>>> agg has list of value. I would separate value in different columns.
> >>>>>
> >>>>> Group.1 Group.2 Group.3                    DCE                     DP
> >>>>> 1 Ancient  Nation     QLH 0.28, 0.28, 0.54, 0.54 0.29, 0.29, 0.58,
> 0.58
> >>>>> 2   Amish   Wives     TAS             0.59, 0.59             0.56,
> 0.56
> >>>>> 3 Auction  Videos     YME             0.57, 0.57             0.56,
> 0.56
> >>>>>
> >>>>> The  goal:
> >>>>>
> >>>>> Group.1 Group.2 Group.3  DCE.1 DCE.2 DCE.3  DCE.4  DP.1  DP.2  DP.3
> DP.4
> >>>>> 1 Ancient  Nation     QLH    0.28     0.28    0.54     0.54
>  0.29, 0.29,
> >>>>> 0.58, 0.58
> >>>>> 2   Amish   Wives     TAS        NA     NA     0.59, 0.59
>  NA
> >>>>> NA  0.56, 0.56
> >>>>> 3 Auction  Videos     YME         NA   NA      0.57, 0.57
>  NA
> >>>>> NA  0.56, 0.56
> >>>>>
> >>>>>
> >>>>>
> >>>>> dat <- structure(list(first.Name = structure(c(3L, 1L, 2L, 3L, 1L,
> 2L,
> >>>>> 3L, 1L, 2L, 3L, 1L, 2L), .Label = c("Amish", "Ancient", "Auction",
> >>>>> "Ax", "Bachelorette", "Basketball", "BBQ", "Cake", "Celebrity",
> >>>>> "Chef", "Clean", "Colonial", "Comedy", "Comic", "Crocodile", "Dog",
> >>>>> "Empire", "Extreme", "Farm", "Half Pint", "Hollywood", "House", "Ice
> >>>>> Road", "Jersey", "Justice", "Love", "Mega", "Model", "Modern",
> >>>>> "Mountain", "Mystery", "Myth", "New York", "Paradise", "Pioneer",
> >>>>> "Queer", "Restaurant", "Road", "Royal", "Spouse", "Star", "Storage",
> >>>>> "Survival", "The Great American", "Tool", "Treasure", "Wedding",
> >>>>> "Wife"), class = "factor"), Name = structure(c(43L, 47L, 29L, 43L,
> >>>>> 47L, 29L, 43L, 47L, 29L, 43L, 47L, 29L), .Label = c("Aliens",
> >>>>> "Behavior", "Casino", "Casting Call", "Challenge", "Contest",
> >>>>> "Crashers", "Crew", "Dad", "Dancing", "Date", "Disasters", "Dynasty",
> >>>>> "Family", "Garage", "Greenlight", "Gypsies", "Haul", "Hot Rod",
> >>>>> "Inventor", "Jail", "Job", "Justice", "Marvels", "Master", "Mates",
> >>>>> "Model", "Moms", "Nation", "Ninja", "Patrol", "People", "Pitmasters",
> >>>>> "Queens", "Rescue", "Rivals", "Room", "Rooms", "Rules", "Star",
> >>>>> "Stars", "Superhero", "Videos", "VIP", "Wars", "Wishes", "Wives",
> >>>>> "Wrangler"), class = "factor"), Department = structure(c(8L, 6L, 2L,
> >>>>> 8L, 6L, 2L, 8L, 6L, 2L, 8L, 6L, 2L), .Label = c("HXW", "QLH", "RAR",
> >>>>> "RYC", "SYI", "TAS", "VUV", "YME"), class = "factor"),
> >>>>>   DCE = c(0.57, 0.59, 0.54, NA, NA, 0.28, 0.57, 0.59, 0.54,
> >>>>>   NA, NA, 0.28), DP = c(0.56, 0.56, 0.58, NA, NA, 0.29, 0.56,
> >>>>>   0.56, 0.58, NA, NA, 0.29), date = structure(c(15978, 15978,
> >>>>>   15978, 16067, 16067, 16067, 16159, 16159, 16159, 16249, 16249,
> >>>>>   16249), class = "Date")), description = "", row.names = c(5L, 18L,
> >>>>> 34L, 53L, 66L, 82L, 102L, 115L, 131L, 150L, 163L, 179L), class =
> >>>>> "data.frame", .Names = c("first.Name", "Name", "Department", "DCE",
> >>>>> "DP", "date"))
> >>>>>
> >>>>>    [[alternative HTML version deleted]]
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide
> >>>>> http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>> David Winsemius
> >>>> Alameda, CA, USA
> >>>>
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide http://www.R-project.org/posting-
> >>>> guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>> ________________________________
> >>> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a
> jsou ur?eny pouze jeho adres?t?m.
> >>> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> >>> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento
> email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> >>> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou
> modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
> >>>
> >>> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> >>> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> >>> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
> p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze
> strany p??jemce s dodatkem ?i odchylkou.
> >>> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> >>> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
> >>>
> >>> This e-mail and any documents attached to it may be confidential and
> are intended only for its intended recipients.
> >>> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> >>> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> >>> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
> >>>
> >>> In case that this e-mail forms part of business dealings:
> >>> - the sender reserves the right to end negotiations about entering
> into a contract in any time, for any reason, and without stating any
> reasoning.
> >>> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> >>> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> >>> - the sender of this e-mail informs that he/she is not authorized to
> enter into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
> >>>
> >>
> >> David Winsemius
> >> Alameda, CA, USA
> >>
> >>
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
>

	[[alternative HTML version deleted]]


From cg.pettersson at lantmannen.com  Sun Nov 20 20:04:52 2016
From: cg.pettersson at lantmannen.com (CG Pettersson)
Date: Sun, 20 Nov 2016 19:04:52 +0000
Subject: [R] Problems using plsr() in the pls package
Message-ID: <HE1PR02MB09879C9AC17828FAAF5496E58CB20@HE1PR02MB0987.eurprd02.prod.outlook.com>

Dear all,
I am trying to do a PLS regression using plsr() from the pls package.
I have tried to make a dataset according to the methods recommended in "Introduction to the pls Package" (Mevik & Wehrens, 2015) and it looks like I get a structure in the dataset that resembles what is packed with the package:

> str(gasoline)
'data.frame':   60 obs. of  2 variables:
$ octane: num  85.3 85.2 88.5 83.4 87.9 ...
$ NIR   : AsIs [1:60, 1:401] -0.0502 -0.0442 -0.0469 -0.0467 -0.0509 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr  "1" "2" "3" "4" ...
  .. ..$ : chr  "900 nm" "902 nm" "904 nm" "906 nm" ...

> str(lid_allt2)
'data.frame':   223 obs. of  2 variables:
$ DON     : num  55 55 55 55 55 55 55 55 55 55 ...
$ lid_nose: 'AsIs' chr [1:223, 1:192] "215.074835" "203.803736" "197.364614" "192.765756" ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr  "1" "2" "3" "4" ...
  .. ..$ : chr  "n1" "n2" "n3" "n4" ...

Where "gasoline" is a dataset from the package and "lid_allt2" is my own dataset. The reason there are so many "55" in the example row in the str() call is that I have repeated measurments with an electronic nose, normally six repetition for each control measurement.

When I try to explain the DON variable with "lid_nose", the plsr() accepts my coding but returns a error message after on second or so:

> don1 <- plsr(DON ~ lid_nose, ncomp = 10, data = lid_allt2, validation = "LOO")
Error in `[[<-.data.frame`(`*tmp*`, i, value = c(16088L, 14624L, 14080L,  :
  replacement has 42816 rows, data has 223

I have clearly asked plsr() to do something very stupid, but what, and what should I do to come out a little better?

Med v?nlig h?lsning/Best regards
CG Pettersson
Senior Scientific Advisor, PhD
______________________
Lantm?nnen Corporate R&D
Phone:  +46 10 556 19 85
Mobile: + 46 70 330 66 85
Email: cg.pettersson at lantmannen.com<mailto:cg.pettersson at lantmannen.com>
Visiting Address: S:t G?ransgatan 160 A
Address: Box 30192, SE-104 25 Stockholm
Webb: http://www.lantmannen.com<http://www.lantmannen.com/>
Registered Office: Stockholm
Before printing, think about the environment


	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sun Nov 20 21:00:02 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 20 Nov 2016 12:00:02 -0800
Subject: [R] aggregate dataframe by multiple factors
In-Reply-To: <CALJKBv_7hmtXxPtzwdqNrCjrAU_Ne4ChY31QfmExnvMmvSpd5w@mail.gmail.com>
References: <CALJKBv9S5=3rYffCbW098eVm8_9XtdE4-FrdWOHTH9AcGWJu4g@mail.gmail.com>
	<89CF0820-5338-4150-A300-78D517CC019B@comcast.net>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C504684B@SRVEXCHMBX.precheza.cz>
	<CALJKBv9pDpDKZ5ZKO0hAh3wZXSXiyVPxyqrj2JEsupffC+tN-w@mail.gmail.com>
	<28273AA6-CE92-4F77-A575-264F41193826@comcast.net>
	<CALJKBv8m12aYMhBVLRO9+U=6sZq9Z7qwDMvk_frr7gfBMf5FBA@mail.gmail.com>
	<C69878C3-314C-4911-8C55-5F4190FA0812@comcast.net>
	<3F3CE841-AA11-463B-828A-D35585460FBD@comcast.net>
	<CALJKBv_7hmtXxPtzwdqNrCjrAU_Ne4ChY31QfmExnvMmvSpd5w@mail.gmail.com>
Message-ID: <7537BCEB-A9F1-4CF1-9E94-F74D156BEBF2@comcast.net>


> On Nov 20, 2016, at 11:26 AM, Karim Mezhoud <kmezhoud at gmail.com> wrote:
> 
> Many Thanks,
> What is exactly the solution.

Is that a question? If so, you need to be more expansive about the areas of uncertainty, since I _thought_ I had given an "exact" solution.

-- 

David
> Best,
> Karim
> 
> On Sun, Nov 20, 2016 at 8:17 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> > On Nov 20, 2016, at 11:11 AM, David Winsemius <dwinsemius at comcast.net> wrote:
> >
> >
> >> On Nov 20, 2016, at 10:49 AM, Karim Mezhoud <kmezhoud at gmail.com> wrote:
> >>
> >> Yes the results does not have a date but
> >> successive DCE.1, DCE2, DCE.3 indicates
> >> DCE.date1 , DCE.date2, DCE.date3
> >> I hope that the chronological order of date is conserved.
> >
> > There are, however, 4 dates. See this pair of results. You can probably do something if you ever figure out what it is that you precisely want. I think the date ordering is automatic here:
> >
> >> require(reshape2)
> > Loading required package: reshape2
> >
> >> dcast(dat, first.Name +  Name + Department ~ date, value.var='DCE')
> >  first.Name   Name Department 2013-09-30 2013-12-28 2014-03-30 2014-06-28
> > 1      Amish  Wives        TAS       0.59         NA       0.59         NA
> > 2    Ancient Nation        QLH       0.54       0.28       0.54       0.28
> > 3    Auction Videos        YME       0.57         NA       0.57         NA
> >> dcast(dat, first.Name +  Name + Department ~ date, value.var='DP')
> >  first.Name   Name Department 2013-09-30 2013-12-28 2014-03-30 2014-06-28
> > 1      Amish  Wives        TAS       0.56         NA       0.56         NA
> > 2    Ancient Nation        QLH       0.58       0.29       0.58       0.29
> > 3    Auction Videos        YME       0.56         NA       0.56         NA
> 
> Yjos completes the process in the reshape2 world:
> 
> > mdat <- melt(dat, measure.vars=c("DCE", 'DP') )
> > str(mdat)
> 'data.frame':   24 obs. of  6 variables:
>  $ first.Name: Factor w/ 48 levels "Amish","Ancient",..: 3 1 2 3 1 2 3 1 2 3 ...
>  $ Name      : Factor w/ 48 levels "Aliens","Behavior",..: 43 47 29 43 47 29 43 47 29 43 ...
>  $ Department: Factor w/ 8 levels "HXW","QLH","RAR",..: 8 6 2 8 6 2 8 6 2 8 ...
>  $ date      : Date, format: "2013-09-30" "2013-09-30" ...
>  $ variable  : Factor w/ 2 levels "DCE","DP": 1 1 1 1 1 1 1 1 1 1 ...
>  $ value     : num  0.57 0.59 0.54 NA NA 0.28 0.57 0.59 0.54 NA ...
> 
> > dcast(mdat, first.Name +  Name + Department ~ date+variable )
>   first.Name   Name Department 2013-09-30_DCE 2013-09-30_DP 2013-12-28_DCE
> 1      Amish  Wives        TAS           0.59          0.56             NA
> 2    Ancient Nation        QLH           0.54          0.58           0.28
> 3    Auction Videos        YME           0.57          0.56             NA
>   2013-12-28_DP 2014-03-30_DCE 2014-03-30_DP 2014-06-28_DCE 2014-06-28_DP
> 1            NA           0.59          0.56             NA            NA
> 2          0.29           0.54          0.58           0.28          0.29
> 3            NA           0.57          0.56             NA            NA
> 
> --
> David.
> 
> >
> >
> >
> >> Thanks,
> >> Karim
> >>
> >>
> >> On Sun, Nov 20, 2016 at 7:44 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> >>
> >>> On Nov 20, 2016, at 5:28 AM, Karim Mezhoud <kmezhoud at gmail.com> wrote:
> >>>
> >>> Sorry for the delay,
> >>> Many Thanks for Mr. David and Mr. Petr
> >>> I thinked  to use "sort" function to arrange chronologically  value by  'date' (without 'date' is colnames) of each variables (DCE, DP).
> >>>
> >>>
> >>> The solution of David seems to be simple to understand with "unlist" function.
> >>> The solution of Petr seems to be fancy. I did not find document  about "I" argument for aggregate function.
> >>>
> >>> How can know which value for which date?
> >>
> >> You asked for a functional reshaping that did not have the date. Now you want the date? There was no date in the result you indicated was desired.   ....??????
> >>
> >> --
> >> David.
> >>
> >>>
> >>> I will save the reshaping/ordering dataframe  for later use.
> >>> Many Thanks,
> >>> Karim
> >>>
> >>>
> >>> On Fri, Nov 18, 2016 at 11:34 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:
> >>> Hi
> >>>
> >>> same result can be achieved by
> >>>
> >>> dat.ag<-aggregate(dat[ , c("DCE","DP")], by= list(dat$first.Name, dat$Name, dat$Department) , "I")
> >>>
> >>> Sorting according to the first row seems to be quite tricky. You could probably get closer by using some combination of split and order and arranging back chunks  of data
> >>>
> >>> ooo1<-order(split(dat$DCE,interaction(dat$first.Name, dat$Name, dat$Department, drop=T))[[1]])
> >>> data.frame(sapply(split(dat$DCE,interaction(dat$first.Name, dat$Name, dat$Department, drop=T)), rbind))[ooo1,]
> >>>  Ancient.Nation.QLH Amish.Wives.TAS Auction.Videos.YME
> >>> 2               0.28              NA                 NA
> >>> 4               0.28              NA                 NA
> >>> 1               0.54            0.59               0.57
> >>> 3               0.54            0.59               0.57
> >>>
> >>> however I wonder why the order according to the first row is necessary if all NAs are on correct positions?
> >>>
> >>> Cheers
> >>> Petr
> >>>
> >>>
> >>>> -----Original Message-----
> >>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of David
> >>>> Winsemius
> >>>> Sent: Friday, November 18, 2016 9:30 AM
> >>>> To: Karim Mezhoud <kmezhoud at gmail.com>
> >>>> Cc: r-help at r-project.org
> >>>> Subject: Re: [R] aggregate dataframe by multiple factors
> >>>>
> >>>>
> >>>>> On Nov 17, 2016, at 11:27 PM, Karim Mezhoud <kmezhoud at gmail.com>
> >>>> wrote:
> >>>>>
> >>>>> Dear all,
> >>>>>
> >>>>> the dat  has missing values NA,
> >>>>>
> >>>>>   first.Name   Name Department  DCE   DP       date
> >>>>> 5      Auction Videos        YME 0.57 0.56 2013-09-30
> >>>>> 18       Amish  Wives        TAS 0.59 0.56 2013-09-30
> >>>>> 34     Ancient Nation        QLH 0.54 0.58 2013-09-30
> >>>>> 53     Auction Videos        YME   NA   NA 2013-12-28
> >>>>> 66       Amish  Wives        TAS   NA   NA 2013-12-28
> >>>>> 82     Ancient Nation        QLH 0.28 0.29 2013-12-28
> >>>>> 102    Auction Videos        YME 0.57 0.56 2014-03-30
> >>>>> 115      Amish  Wives        TAS 0.59 0.56 2014-03-30
> >>>>> 131    Ancient Nation        QLH 0.54 0.58 2014-03-30
> >>>>> 150    Auction Videos        YME   NA   NA 2014-06-28
> >>>>> 163      Amish  Wives        TAS   NA   NA 2014-06-28
> >>>>> 179    Ancient Nation        QLH 0.28 0.29 2014-06-28
> >>>>>
> >>>>>
> >>>>> agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> >>>>> list(dat$first.Name, dat$Name, dat$Department) , "sort"))
> >>>>
> >>>> The closest I could get on a few attempts was:
> >>>>
> >>>> (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> >>>> list(dat$first.Name, dat$Name, dat$Department) , function(d) { unlist(d)}))
> >>>> )
> >>>>
> >>>>  Group.1 Group.2 Group.3 DCE.1 DCE.2 DCE.3 DCE.4 DP.1 DP.2 DP.3 DP.4
> >>>> 1 Ancient  Nation     QLH  0.54  0.28  0.54  0.28 0.58 0.29 0.58 0.29
> >>>> 2   Amish   Wives     TAS  0.59    NA  0.59    NA 0.56   NA 0.56   NA
> >>>> 3 Auction  Videos     YME  0.57    NA  0.57    NA 0.56   NA 0.56   NA
> >>>>
> >>>> I think the sort operation might be somewhat ambiguous in this instance. I
> >>>> tried:
> >>>>
> >>>> (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> >>>> list(dat$first.Name, dat$Name, dat$Department) , function(d) {
> >>>> unlist(lapply(d,sort))}))
> >>>> )
> >>>>
> >>>> With no success, not even a sorted result.
> >>>>
> >>>> --
> >>>> David.
> >>>>>
> >>>>>
> >>>>> agg has list of value. I would separate value in different columns.
> >>>>>
> >>>>> Group.1 Group.2 Group.3                    DCE                     DP
> >>>>> 1 Ancient  Nation     QLH 0.28, 0.28, 0.54, 0.54 0.29, 0.29, 0.58, 0.58
> >>>>> 2   Amish   Wives     TAS             0.59, 0.59             0.56, 0.56
> >>>>> 3 Auction  Videos     YME             0.57, 0.57             0.56, 0.56
> >>>>>
> >>>>> The  goal:
> >>>>>
> >>>>> Group.1 Group.2 Group.3  DCE.1 DCE.2 DCE.3  DCE.4  DP.1  DP.2  DP.3  DP.4
> >>>>> 1 Ancient  Nation     QLH    0.28     0.28    0.54     0.54     0.29, 0.29,
> >>>>> 0.58, 0.58
> >>>>> 2   Amish   Wives     TAS        NA     NA     0.59, 0.59           NA
> >>>>> NA  0.56, 0.56
> >>>>> 3 Auction  Videos     YME         NA   NA      0.57, 0.57             NA
> >>>>> NA  0.56, 0.56
> >>>>>
> >>>>>
> >>>>>
> >>>>> dat <- structure(list(first.Name = structure(c(3L, 1L, 2L, 3L, 1L, 2L,
> >>>>> 3L, 1L, 2L, 3L, 1L, 2L), .Label = c("Amish", "Ancient", "Auction",
> >>>>> "Ax", "Bachelorette", "Basketball", "BBQ", "Cake", "Celebrity",
> >>>>> "Chef", "Clean", "Colonial", "Comedy", "Comic", "Crocodile", "Dog",
> >>>>> "Empire", "Extreme", "Farm", "Half Pint", "Hollywood", "House", "Ice
> >>>>> Road", "Jersey", "Justice", "Love", "Mega", "Model", "Modern",
> >>>>> "Mountain", "Mystery", "Myth", "New York", "Paradise", "Pioneer",
> >>>>> "Queer", "Restaurant", "Road", "Royal", "Spouse", "Star", "Storage",
> >>>>> "Survival", "The Great American", "Tool", "Treasure", "Wedding",
> >>>>> "Wife"), class = "factor"), Name = structure(c(43L, 47L, 29L, 43L,
> >>>>> 47L, 29L, 43L, 47L, 29L, 43L, 47L, 29L), .Label = c("Aliens",
> >>>>> "Behavior", "Casino", "Casting Call", "Challenge", "Contest",
> >>>>> "Crashers", "Crew", "Dad", "Dancing", "Date", "Disasters", "Dynasty",
> >>>>> "Family", "Garage", "Greenlight", "Gypsies", "Haul", "Hot Rod",
> >>>>> "Inventor", "Jail", "Job", "Justice", "Marvels", "Master", "Mates",
> >>>>> "Model", "Moms", "Nation", "Ninja", "Patrol", "People", "Pitmasters",
> >>>>> "Queens", "Rescue", "Rivals", "Room", "Rooms", "Rules", "Star",
> >>>>> "Stars", "Superhero", "Videos", "VIP", "Wars", "Wishes", "Wives",
> >>>>> "Wrangler"), class = "factor"), Department = structure(c(8L, 6L, 2L,
> >>>>> 8L, 6L, 2L, 8L, 6L, 2L, 8L, 6L, 2L), .Label = c("HXW", "QLH", "RAR",
> >>>>> "RYC", "SYI", "TAS", "VUV", "YME"), class = "factor"),
> >>>>>   DCE = c(0.57, 0.59, 0.54, NA, NA, 0.28, 0.57, 0.59, 0.54,
> >>>>>   NA, NA, 0.28), DP = c(0.56, 0.56, 0.58, NA, NA, 0.29, 0.56,
> >>>>>   0.56, 0.58, NA, NA, 0.29), date = structure(c(15978, 15978,
> >>>>>   15978, 16067, 16067, 16067, 16159, 16159, 16159, 16249, 16249,
> >>>>>   16249), class = "Date")), description = "", row.names = c(5L, 18L,
> >>>>> 34L, 53L, 66L, 82L, 102L, 115L, 131L, 150L, 163L, 179L), class =
> >>>>> "data.frame", .Names = c("first.Name", "Name", "Department", "DCE",
> >>>>> "DP", "date"))
> >>>>>
> >>>>>    [[alternative HTML version deleted]]
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide
> >>>>> http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>> David Winsemius
> >>>> Alameda, CA, USA
> >>>>
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide http://www.R-project.org/posting-
> >>>> guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>> ________________________________
> >>> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
> >>> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
> >>> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> >>> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
> >>>
> >>> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> >>> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> >>> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
> >>> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> >>> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
> >>>
> >>> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> >>> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> >>> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> >>> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
> >>>
> >>> In case that this e-mail forms part of business dealings:
> >>> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> >>> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> >>> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> >>> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.
> >>>
> >>
> >> David Winsemius
> >> Alameda, CA, USA
> >>
> >>
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> 

David Winsemius
Alameda, CA, USA


From ruipbarradas at sapo.pt  Sun Nov 20 21:03:48 2016
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Sun, 20 Nov 2016 20:03:48 +0000
Subject: [R] Problems using plsr() in the pls package
In-Reply-To: <HE1PR02MB09879C9AC17828FAAF5496E58CB20@HE1PR02MB0987.eurprd02.prod.outlook.com>
References: <HE1PR02MB09879C9AC17828FAAF5496E58CB20@HE1PR02MB0987.eurprd02.prod.outlook.com>
Message-ID: <583201A4.4030607@sapo.pt>

Hello,

I know nothing about the plsr package but in the data.frame 'gasoline' 
the column NIR is numeric and in 'lid_allt2' the column lid_nose is 
character. Maybe you need

lid_allt2$lid_nose <- as.numeric(lid_allt2$lid_nose)

but I really don't know if this is the problem.

Hope this helps,

Rui Barradas

Em 20-11-2016 19:04, CG Pettersson escreveu:
> Dear all,
> I am trying to do a PLS regression using plsr() from the pls package.
> I have tried to make a dataset according to the methods recommended in "Introduction to the pls Package" (Mevik & Wehrens, 2015) and it looks like I get a structure in the dataset that resembles what is packed with the package:
>
>> str(gasoline)
> 'data.frame':   60 obs. of  2 variables:
> $ octane: num  85.3 85.2 88.5 83.4 87.9 ...
> $ NIR   : AsIs [1:60, 1:401] -0.0502 -0.0442 -0.0469 -0.0467 -0.0509 ...
>    ..- attr(*, "dimnames")=List of 2
>    .. ..$ : chr  "1" "2" "3" "4" ...
>    .. ..$ : chr  "900 nm" "902 nm" "904 nm" "906 nm" ...
>
>> str(lid_allt2)
> 'data.frame':   223 obs. of  2 variables:
> $ DON     : num  55 55 55 55 55 55 55 55 55 55 ...
> $ lid_nose: 'AsIs' chr [1:223, 1:192] "215.074835" "203.803736" "197.364614" "192.765756" ...
>    ..- attr(*, "dimnames")=List of 2
>    .. ..$ : chr  "1" "2" "3" "4" ...
>    .. ..$ : chr  "n1" "n2" "n3" "n4" ...
>
> Where "gasoline" is a dataset from the package and "lid_allt2" is my own dataset. The reason there are so many "55" in the example row in the str() call is that I have repeated measurments with an electronic nose, normally six repetition for each control measurement.
>
> When I try to explain the DON variable with "lid_nose", the plsr() accepts my coding but returns a error message after on second or so:
>
>> don1 <- plsr(DON ~ lid_nose, ncomp = 10, data = lid_allt2, validation = "LOO")
> Error in `[[<-.data.frame`(`*tmp*`, i, value = c(16088L, 14624L, 14080L,  :
>    replacement has 42816 rows, data has 223
>
> I have clearly asked plsr() to do something very stupid, but what, and what should I do to come out a little better?
>
> Med v?nlig h?lsning/Best regards
> CG Pettersson
> Senior Scientific Advisor, PhD
> ______________________
> Lantm?nnen Corporate R&D
> Phone:  +46 10 556 19 85
> Mobile: + 46 70 330 66 85
> Email: cg.pettersson at lantmannen.com<mailto:cg.pettersson at lantmannen.com>
> Visiting Address: S:t G?ransgatan 160 A
> Address: Box 30192, SE-104 25 Stockholm
> Webb: http://www.lantmannen.com<http://www.lantmannen.com/>
> Registered Office: Stockholm
> Before printing, think about the environment
>
>
> 	[[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From r.turner at auckland.ac.nz  Sun Nov 20 21:40:12 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 21 Nov 2016 09:40:12 +1300
Subject: [R] Problems using plsr() in the pls package
In-Reply-To: <583201A4.4030607@sapo.pt>
References: <HE1PR02MB09879C9AC17828FAAF5496E58CB20@HE1PR02MB0987.eurprd02.prod.outlook.com>
	<583201A4.4030607@sapo.pt>
Message-ID: <0183d1a0-f560-c7e1-971d-d603584de5eb@auckland.ac.nz>

On 21/11/16 09:03, Rui Barradas wrote:
> Hello,
>
> I know nothing about the plsr package but in the data.frame 'gasoline'
> the column NIR is numeric and in 'lid_allt2' the column lid_nose is
> character. Maybe you need
>
> lid_allt2$lid_nose <- as.numeric(lid_allt2$lid_nose)
>
> but I really don't know if this is the problem.


I have no idea either, but surely your suggestion should be:

lid_allt2$lid_nose <- as.numeric(as.character(lid_allt2$lid_nose))

or (better):

lid_allt2$lid_nose <-
     with(lid_allt2,as.numeric(levels(lid_nose))[lid.nose])


cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From ruipbarradas at sapo.pt  Sun Nov 20 22:39:32 2016
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Sun, 20 Nov 2016 21:39:32 +0000
Subject: [R] Problems using plsr() in the pls package
In-Reply-To: <0183d1a0-f560-c7e1-971d-d603584de5eb@auckland.ac.nz>
References: <HE1PR02MB09879C9AC17828FAAF5496E58CB20@HE1PR02MB0987.eurprd02.prod.outlook.com>
	<583201A4.4030607@sapo.pt>
	<0183d1a0-f560-c7e1-971d-d603584de5eb@auckland.ac.nz>
Message-ID: <58321814.60008@sapo.pt>

Hello,

Inline.

Em 20-11-2016 20:40, Rolf Turner escreveu:
> On 21/11/16 09:03, Rui Barradas wrote:
>> Hello,
>>
>> I know nothing about the plsr package but in the data.frame 'gasoline'
>> the column NIR is numeric and in 'lid_allt2' the column lid_nose is
>> character. Maybe you need
>>
>> lid_allt2$lid_nose <- as.numeric(lid_allt2$lid_nose)
>>
>> but I really don't know if this is the problem.
>
>
> I have no idea either, but surely your suggestion should be:
>
> lid_allt2$lid_nose <- as.numeric(as.character(lid_allt2$lid_nose))

Actually no, I also thought of that but str(lid_allt2) doesn't say that 
lid_nose is a factor, it says "chr".

Rui Barradas

>
> or (better):
>
> lid_allt2$lid_nose <-
>      with(lid_allt2,as.numeric(levels(lid_nose))[lid.nose])
>
>
> cheers,
>
> Rolf
>


From kmezhoud at gmail.com  Sun Nov 20 22:43:19 2016
From: kmezhoud at gmail.com (Karim Mezhoud)
Date: Sun, 20 Nov 2016 22:43:19 +0100
Subject: [R] aggregate dataframe by multiple factors
In-Reply-To: <7537BCEB-A9F1-4CF1-9E94-F74D156BEBF2@comcast.net>
References: <CALJKBv9S5=3rYffCbW098eVm8_9XtdE4-FrdWOHTH9AcGWJu4g@mail.gmail.com>
	<89CF0820-5338-4150-A300-78D517CC019B@comcast.net>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C504684B@SRVEXCHMBX.precheza.cz>
	<CALJKBv9pDpDKZ5ZKO0hAh3wZXSXiyVPxyqrj2JEsupffC+tN-w@mail.gmail.com>
	<28273AA6-CE92-4F77-A575-264F41193826@comcast.net>
	<CALJKBv8m12aYMhBVLRO9+U=6sZq9Z7qwDMvk_frr7gfBMf5FBA@mail.gmail.com>
	<C69878C3-314C-4911-8C55-5F4190FA0812@comcast.net>
	<3F3CE841-AA11-463B-828A-D35585460FBD@comcast.net>
	<CALJKBv_7hmtXxPtzwdqNrCjrAU_Ne4ChY31QfmExnvMmvSpd5w@mail.gmail.com>
	<7537BCEB-A9F1-4CF1-9E94-F74D156BEBF2@comcast.net>
Message-ID: <CALJKBv8GgMwAzTj0CqjPOqjsDiQ=MTdFSOrDbWT8G-jJv==G8w@mail.gmail.com>

Sorry,
"That" and not "What".
Best

On Sun, Nov 20, 2016 at 9:00 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Nov 20, 2016, at 11:26 AM, Karim Mezhoud <kmezhoud at gmail.com> wrote:
> >
> > Many Thanks,
> > What is exactly the solution.
>
> Is that a question? If so, you need to be more expansive about the areas
> of uncertainty, since I _thought_ I had given an "exact" solution.
>
> --
>
> David
> > Best,
> > Karim
> >
> > On Sun, Nov 20, 2016 at 8:17 PM, David Winsemius <dwinsemius at comcast.net>
> wrote:
> >
> > > On Nov 20, 2016, at 11:11 AM, David Winsemius <dwinsemius at comcast.net>
> wrote:
> > >
> > >
> > >> On Nov 20, 2016, at 10:49 AM, Karim Mezhoud <kmezhoud at gmail.com>
> wrote:
> > >>
> > >> Yes the results does not have a date but
> > >> successive DCE.1, DCE2, DCE.3 indicates
> > >> DCE.date1 , DCE.date2, DCE.date3
> > >> I hope that the chronological order of date is conserved.
> > >
> > > There are, however, 4 dates. See this pair of results. You can
> probably do something if you ever figure out what it is that you precisely
> want. I think the date ordering is automatic here:
> > >
> > >> require(reshape2)
> > > Loading required package: reshape2
> > >
> > >> dcast(dat, first.Name +  Name + Department ~ date, value.var='DCE')
> > >  first.Name   Name Department 2013-09-30 2013-12-28 2014-03-30
> 2014-06-28
> > > 1      Amish  Wives        TAS       0.59         NA       0.59
>  NA
> > > 2    Ancient Nation        QLH       0.54       0.28       0.54
>  0.28
> > > 3    Auction Videos        YME       0.57         NA       0.57
>  NA
> > >> dcast(dat, first.Name +  Name + Department ~ date, value.var='DP')
> > >  first.Name   Name Department 2013-09-30 2013-12-28 2014-03-30
> 2014-06-28
> > > 1      Amish  Wives        TAS       0.56         NA       0.56
>  NA
> > > 2    Ancient Nation        QLH       0.58       0.29       0.58
>  0.29
> > > 3    Auction Videos        YME       0.56         NA       0.56
>  NA
> >
> > Yjos completes the process in the reshape2 world:
> >
> > > mdat <- melt(dat, measure.vars=c("DCE", 'DP') )
> > > str(mdat)
> > 'data.frame':   24 obs. of  6 variables:
> >  $ first.Name: Factor w/ 48 levels "Amish","Ancient",..: 3 1 2 3 1 2 3 1
> 2 3 ...
> >  $ Name      : Factor w/ 48 levels "Aliens","Behavior",..: 43 47 29 43
> 47 29 43 47 29 43 ...
> >  $ Department: Factor w/ 8 levels "HXW","QLH","RAR",..: 8 6 2 8 6 2 8 6
> 2 8 ...
> >  $ date      : Date, format: "2013-09-30" "2013-09-30" ...
> >  $ variable  : Factor w/ 2 levels "DCE","DP": 1 1 1 1 1 1 1 1 1 1 ...
> >  $ value     : num  0.57 0.59 0.54 NA NA 0.28 0.57 0.59 0.54 NA ...
> >
> > > dcast(mdat, first.Name +  Name + Department ~ date+variable )
> >   first.Name   Name Department 2013-09-30_DCE 2013-09-30_DP
> 2013-12-28_DCE
> > 1      Amish  Wives        TAS           0.59          0.56
>  NA
> > 2    Ancient Nation        QLH           0.54          0.58
>  0.28
> > 3    Auction Videos        YME           0.57          0.56
>  NA
> >   2013-12-28_DP 2014-03-30_DCE 2014-03-30_DP 2014-06-28_DCE 2014-06-28_DP
> > 1            NA           0.59          0.56             NA            NA
> > 2          0.29           0.54          0.58           0.28          0.29
> > 3            NA           0.57          0.56             NA            NA
> >
> > --
> > David.
> >
> > >
> > >
> > >
> > >> Thanks,
> > >> Karim
> > >>
> > >>
> > >> On Sun, Nov 20, 2016 at 7:44 PM, David Winsemius <
> dwinsemius at comcast.net> wrote:
> > >>
> > >>> On Nov 20, 2016, at 5:28 AM, Karim Mezhoud <kmezhoud at gmail.com>
> wrote:
> > >>>
> > >>> Sorry for the delay,
> > >>> Many Thanks for Mr. David and Mr. Petr
> > >>> I thinked  to use "sort" function to arrange chronologically  value
> by  'date' (without 'date' is colnames) of each variables (DCE, DP).
> > >>>
> > >>>
> > >>> The solution of David seems to be simple to understand with "unlist"
> function.
> > >>> The solution of Petr seems to be fancy. I did not find document
> about "I" argument for aggregate function.
> > >>>
> > >>> How can know which value for which date?
> > >>
> > >> You asked for a functional reshaping that did not have the date. Now
> you want the date? There was no date in the result you indicated was
> desired.   ....??????
> > >>
> > >> --
> > >> David.
> > >>
> > >>>
> > >>> I will save the reshaping/ordering dataframe  for later use.
> > >>> Many Thanks,
> > >>> Karim
> > >>>
> > >>>
> > >>> On Fri, Nov 18, 2016 at 11:34 AM, PIKAL Petr <petr.pikal at precheza.cz>
> wrote:
> > >>> Hi
> > >>>
> > >>> same result can be achieved by
> > >>>
> > >>> dat.ag<-aggregate(dat[ , c("DCE","DP")], by= list(dat$first.Name,
> dat$Name, dat$Department) , "I")
> > >>>
> > >>> Sorting according to the first row seems to be quite tricky. You
> could probably get closer by using some combination of split and order and
> arranging back chunks  of data
> > >>>
> > >>> ooo1<-order(split(dat$DCE,interaction(dat$first.Name, dat$Name,
> dat$Department, drop=T))[[1]])
> > >>> data.frame(sapply(split(dat$DCE,interaction(dat$first.Name,
> dat$Name, dat$Department, drop=T)), rbind))[ooo1,]
> > >>>  Ancient.Nation.QLH Amish.Wives.TAS Auction.Videos.YME
> > >>> 2               0.28              NA                 NA
> > >>> 4               0.28              NA                 NA
> > >>> 1               0.54            0.59               0.57
> > >>> 3               0.54            0.59               0.57
> > >>>
> > >>> however I wonder why the order according to the first row is
> necessary if all NAs are on correct positions?
> > >>>
> > >>> Cheers
> > >>> Petr
> > >>>
> > >>>
> > >>>> -----Original Message-----
> > >>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> David
> > >>>> Winsemius
> > >>>> Sent: Friday, November 18, 2016 9:30 AM
> > >>>> To: Karim Mezhoud <kmezhoud at gmail.com>
> > >>>> Cc: r-help at r-project.org
> > >>>> Subject: Re: [R] aggregate dataframe by multiple factors
> > >>>>
> > >>>>
> > >>>>> On Nov 17, 2016, at 11:27 PM, Karim Mezhoud <kmezhoud at gmail.com>
> > >>>> wrote:
> > >>>>>
> > >>>>> Dear all,
> > >>>>>
> > >>>>> the dat  has missing values NA,
> > >>>>>
> > >>>>>   first.Name   Name Department  DCE   DP       date
> > >>>>> 5      Auction Videos        YME 0.57 0.56 2013-09-30
> > >>>>> 18       Amish  Wives        TAS 0.59 0.56 2013-09-30
> > >>>>> 34     Ancient Nation        QLH 0.54 0.58 2013-09-30
> > >>>>> 53     Auction Videos        YME   NA   NA 2013-12-28
> > >>>>> 66       Amish  Wives        TAS   NA   NA 2013-12-28
> > >>>>> 82     Ancient Nation        QLH 0.28 0.29 2013-12-28
> > >>>>> 102    Auction Videos        YME 0.57 0.56 2014-03-30
> > >>>>> 115      Amish  Wives        TAS 0.59 0.56 2014-03-30
> > >>>>> 131    Ancient Nation        QLH 0.54 0.58 2014-03-30
> > >>>>> 150    Auction Videos        YME   NA   NA 2014-06-28
> > >>>>> 163      Amish  Wives        TAS   NA   NA 2014-06-28
> > >>>>> 179    Ancient Nation        QLH 0.28 0.29 2014-06-28
> > >>>>>
> > >>>>>
> > >>>>> agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> > >>>>> list(dat$first.Name, dat$Name, dat$Department) , "sort"))
> > >>>>
> > >>>> The closest I could get on a few attempts was:
> > >>>>
> > >>>> (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> > >>>> list(dat$first.Name, dat$Name, dat$Department) , function(d) {
> unlist(d)}))
> > >>>> )
> > >>>>
> > >>>>  Group.1 Group.2 Group.3 DCE.1 DCE.2 DCE.3 DCE.4 DP.1 DP.2 DP.3 DP.4
> > >>>> 1 Ancient  Nation     QLH  0.54  0.28  0.54  0.28 0.58 0.29 0.58
> 0.29
> > >>>> 2   Amish   Wives     TAS  0.59    NA  0.59    NA 0.56   NA 0.56
>  NA
> > >>>> 3 Auction  Videos     YME  0.57    NA  0.57    NA 0.56   NA 0.56
>  NA
> > >>>>
> > >>>> I think the sort operation might be somewhat ambiguous in this
> instance. I
> > >>>> tried:
> > >>>>
> > >>>> (agg <- as.data.frame(aggregate(dat[ , c("DCE","DP")], by=
> > >>>> list(dat$first.Name, dat$Name, dat$Department) , function(d) {
> > >>>> unlist(lapply(d,sort))}))
> > >>>> )
> > >>>>
> > >>>> With no success, not even a sorted result.
> > >>>>
> > >>>> --
> > >>>> David.
> > >>>>>
> > >>>>>
> > >>>>> agg has list of value. I would separate value in different columns.
> > >>>>>
> > >>>>> Group.1 Group.2 Group.3                    DCE
>  DP
> > >>>>> 1 Ancient  Nation     QLH 0.28, 0.28, 0.54, 0.54 0.29, 0.29, 0.58,
> 0.58
> > >>>>> 2   Amish   Wives     TAS             0.59, 0.59             0.56,
> 0.56
> > >>>>> 3 Auction  Videos     YME             0.57, 0.57             0.56,
> 0.56
> > >>>>>
> > >>>>> The  goal:
> > >>>>>
> > >>>>> Group.1 Group.2 Group.3  DCE.1 DCE.2 DCE.3  DCE.4  DP.1  DP.2
> DP.3  DP.4
> > >>>>> 1 Ancient  Nation     QLH    0.28     0.28    0.54     0.54
>  0.29, 0.29,
> > >>>>> 0.58, 0.58
> > >>>>> 2   Amish   Wives     TAS        NA     NA     0.59, 0.59
>  NA
> > >>>>> NA  0.56, 0.56
> > >>>>> 3 Auction  Videos     YME         NA   NA      0.57, 0.57
>    NA
> > >>>>> NA  0.56, 0.56
> > >>>>>
> > >>>>>
> > >>>>>
> > >>>>> dat <- structure(list(first.Name = structure(c(3L, 1L, 2L, 3L, 1L,
> 2L,
> > >>>>> 3L, 1L, 2L, 3L, 1L, 2L), .Label = c("Amish", "Ancient", "Auction",
> > >>>>> "Ax", "Bachelorette", "Basketball", "BBQ", "Cake", "Celebrity",
> > >>>>> "Chef", "Clean", "Colonial", "Comedy", "Comic", "Crocodile", "Dog",
> > >>>>> "Empire", "Extreme", "Farm", "Half Pint", "Hollywood", "House",
> "Ice
> > >>>>> Road", "Jersey", "Justice", "Love", "Mega", "Model", "Modern",
> > >>>>> "Mountain", "Mystery", "Myth", "New York", "Paradise", "Pioneer",
> > >>>>> "Queer", "Restaurant", "Road", "Royal", "Spouse", "Star",
> "Storage",
> > >>>>> "Survival", "The Great American", "Tool", "Treasure", "Wedding",
> > >>>>> "Wife"), class = "factor"), Name = structure(c(43L, 47L, 29L, 43L,
> > >>>>> 47L, 29L, 43L, 47L, 29L, 43L, 47L, 29L), .Label = c("Aliens",
> > >>>>> "Behavior", "Casino", "Casting Call", "Challenge", "Contest",
> > >>>>> "Crashers", "Crew", "Dad", "Dancing", "Date", "Disasters",
> "Dynasty",
> > >>>>> "Family", "Garage", "Greenlight", "Gypsies", "Haul", "Hot Rod",
> > >>>>> "Inventor", "Jail", "Job", "Justice", "Marvels", "Master", "Mates",
> > >>>>> "Model", "Moms", "Nation", "Ninja", "Patrol", "People",
> "Pitmasters",
> > >>>>> "Queens", "Rescue", "Rivals", "Room", "Rooms", "Rules", "Star",
> > >>>>> "Stars", "Superhero", "Videos", "VIP", "Wars", "Wishes", "Wives",
> > >>>>> "Wrangler"), class = "factor"), Department = structure(c(8L, 6L,
> 2L,
> > >>>>> 8L, 6L, 2L, 8L, 6L, 2L, 8L, 6L, 2L), .Label = c("HXW", "QLH",
> "RAR",
> > >>>>> "RYC", "SYI", "TAS", "VUV", "YME"), class = "factor"),
> > >>>>>   DCE = c(0.57, 0.59, 0.54, NA, NA, 0.28, 0.57, 0.59, 0.54,
> > >>>>>   NA, NA, 0.28), DP = c(0.56, 0.56, 0.58, NA, NA, 0.29, 0.56,
> > >>>>>   0.56, 0.58, NA, NA, 0.29), date = structure(c(15978, 15978,
> > >>>>>   15978, 16067, 16067, 16067, 16159, 16159, 16159, 16249, 16249,
> > >>>>>   16249), class = "Date")), description = "", row.names = c(5L,
> 18L,
> > >>>>> 34L, 53L, 66L, 82L, 102L, 115L, 131L, 150L, 163L, 179L), class =
> > >>>>> "data.frame", .Names = c("first.Name", "Name", "Department", "DCE",
> > >>>>> "DP", "date"))
> > >>>>>
> > >>>>>    [[alternative HTML version deleted]]
> > >>>>>
> > >>>>> ______________________________________________
> > >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>>>> PLEASE do read the posting guide
> > >>>>> http://www.R-project.org/posting-guide.html
> > >>>>> and provide commented, minimal, self-contained, reproducible code.
> > >>>>
> > >>>> David Winsemius
> > >>>> Alameda, CA, USA
> > >>>>
> > >>>> ______________________________________________
> > >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>>> PLEASE do read the posting guide http://www.R-project.org/posting-
> > >>>> guide.html
> > >>>> and provide commented, minimal, self-contained, reproducible code.
> > >>>
> > >>> ________________________________
> > >>> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a
> jsou ur?eny pouze jeho adres?t?m.
> > >>> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> > >>> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento
> email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> > >>> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou
> modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
> > >>>
> > >>> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> > >>> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> > >>> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
> p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze
> strany p??jemce s dodatkem ?i odchylkou.
> > >>> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> > >>> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
> > >>>
> > >>> This e-mail and any documents attached to it may be confidential and
> are intended only for its intended recipients.
> > >>> If you received this e-mail by mistake, please immediately inform
> its sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> > >>> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> > >>> The sender of this e-mail shall not be liable for any possible
> damage caused by modifications of the e-mail or by delay with transfer of
> the email.
> > >>>
> > >>> In case that this e-mail forms part of business dealings:
> > >>> - the sender reserves the right to end negotiations about entering
> into a contract in any time, for any reason, and without stating any
> reasoning.
> > >>> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> > >>> - the sender insists on that the respective contract is concluded
> only upon an express mutual agreement on all its aspects.
> > >>> - the sender of this e-mail informs that he/she is not authorized to
> enter into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
> > >>>
> > >>
> > >> David Winsemius
> > >> Alameda, CA, USA
> > >>
> > >>
> > >
> > > David Winsemius
> > > Alameda, CA, USA
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> >
>
> David Winsemius
> Alameda, CA, USA
>
>

	[[alternative HTML version deleted]]


From nstefi at gmail.com  Mon Nov 21 03:40:44 2016
From: nstefi at gmail.com (Steven Nagy)
Date: Sun, 20 Nov 2016 21:40:44 -0500
Subject: [R] Need some help with regular expression
In-Reply-To: <CAGxFJbRLyPgD0RJG9Y-ZG4qumPjp4UWi7h1iF4gm1kMYhXeh7g@mail.gmail.com>
References: <00a301d242e3$7d884e20$7898ea60$@gmail.com>
	<CAGxFJbRLyPgD0RJG9Y-ZG4qumPjp4UWi7h1iF4gm1kMYhXeh7g@mail.gmail.com>
Message-ID: <001101d243a0$a91e4f90$fb5aeeb0$@gmail.com>

Thanks a lot Bert. That's amazing. I am very new to both R and regular
expressions. I don't really understand the regular expression that you used
below.
And looks like I don't even need any special library, like the "gsubfn" for
the strapply function.
I was trying to use the regexr.com website to analyze your regular
expression, but it doesn't seem to match any text there.
Can you explain me the regular expression that you used?
".*: *([[:alnum:]]* *-> *STU|STU *-> *[[:alnum:]]*).*"
So the dot in the front means any character and the star after that means
that it can repeat 0 or more times, right?
Then followed by a colon character ":" and a space, and what is the next
star after that? It means that the sequence before that again can repeat 0
or more times?
And what are the double square brackets?
Is ":alnum:" specific to R? I don't think "regexr.com" understands that. Or
maybe that site is for regular expressions in Javascript, and the syntax is
different in R?

Thank you,
Steven

-----Original Message-----
From: Bert Gunter [mailto:bgunter.4567 at gmail.com] 
Sent: Sunday, November 20, 2016 2:15 PM
To: Steven Nagy <nstefi at gmail.com>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] Need some help with regular expression

If I understand you correctly, I think you are making it more complex than
necessary. Using your example (thanks!!), the following should get you
started:


> x<- c("Name.MEMBER_TYPE: NMA -> STU ; CATEGORY:  -> 1 ; CITY: 
> MISSISSAUGA -> Mississauga ; ZIP: L5N1H9 -> L5N 1H9 ; COUNTRY: CAN ->  
> ; MEMBER_STATUS:  -> N", "Name.MEMBER_TYPE: STU -> REG ; CATEGORY: 1 
> ->","Name.MEMBER_TYPE: -> STU")
>
> x
[1] "Name.MEMBER_TYPE: NMA -> STU ; CATEGORY:  -> 1 ; CITY:
MISSISSAUGA -> Mississauga ; ZIP: L5N1H9 -> L5N 1H9 ; COUNTRY: CAN -> ;
MEMBER_STATUS:  -> N"

[2] "Name.MEMBER_TYPE: STU -> REG ; CATEGORY: 1 ->"
[3] "Name.MEMBER_TYPE: -> STU"
>
> sub(".*: *([[:alnum:]]* *-> *STU|STU *-> *[[:alnum:]]*).*","\\1",x)
[1] "NMA -> STU" "STU -> REG" "-> STU"


I am sure that you can get things to the form you desire in one go with some
fiddling of the above, but it was easier for me to write the regex to pick
out the pieces you wanted and leave the rest to you.
Others may have slicker ways to do it, of course.

HTH

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Nov 19, 2016 at 8:06 PM, Steven Nagy <nstefi at gmail.com> wrote:
> I tried out a regular expression on this website:
>
> http://regexr.com/3en1m
>
>
>
> So the input text is:
>
> "Name.MEMBER_TYPE:  -> STU"
>
>
>
> The regular expression is: ((?:\w+|\s) -> STU|STU -> (?:\w+|\s))
>
> And it returns:
>
> "  -> STU"
>
>
>
> but when I use in R, it doesn't return the same result:
>
> strapply(c, "((?:\\w+|\\s) -> STU|STU -> (?:\\w+|\\s))", c, backref = 
> -1, perl = TRUE)
>
> returns:
> "Name.MEMBER_TYPE: -> STU"
>
>
>
>
>
> Here is what I was trying to do:
>
>
>
> I need to extract some values from a log table, and I created a 
> regular expression that helps me with that.
>
> The log table has cells with values like:
>
> a = "Name.MEMBER_TYPE: NMA -> STU ; CATEGORY:  -> 1 ; CITY: 
> MISSISSAUGA -> Mississauga ; ZIP: L5N1H9 -> L5N 1H9 ; COUNTRY: CAN ->  
> ; MEMBER_STATUS:  -> N"
>
> or
> b = "Name.MEMBER_TYPE: STU -> REG ; CATEGORY: 1 ->"
>
> so I needed to extract the values that a STU member type is changing 
> from and to, so I needed NMA, STU in the 1st case or STU, REG in the 2nd
case.
>
> I came up with this expression which worked in both cases:
>
> strapply(strapply(a, "(\\w+ -> STU|STU -> \\w+)", c, backref = -1, 
> perl = TRUE), "(\\w+) -> (\\w+)", c, backref = -2, perl = TRUE)
>
>
>
> But I had a 3rd case when the source member type was blank:
>
> c = "Name.MEMBER_TYPE: -> STU"
>
> and in that case it returned an error:
>
> strapply(strapply(c, "(\\w+ -> STU|STU -> \\w+)", c, backref = -1, 
> perl = TRUE), "(\\w+) -> (\\w+)", c, backref = -2, perl = TRUE)
>
> Error: is.character(x) is not TRUE
>
>
>
> I found that the error is because this returns NULL:
>
> strapply(c, "(\\w+ -> STU|STU -> \\w+)", c, backref = -1, perl = TRUE)
>
>
>
>
>
> So I tried to modify the regular expression to match any word or blank
> space:
>
> strapply(c, "((?:\\w+|\\s) -> STU|STU -> (?:\\w+|\\s))", c, backref = 
> -1, perl = TRUE)
>
>
>
> but this returned me the whole value of "c":
>
> "Name.MEMBER_TYPE:  -> STU"
>
> and I only needed "  -> STU" as it shows on the website regxr.com
>
>
>
> Is the result wrong on the regxr.com website or strapply returns the 
> wrong result?
>
>
>
> Thanks,
>
> Steven
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Mon Nov 21 04:05:11 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sun, 20 Nov 2016 19:05:11 -0800
Subject: [R] Need some help with regular expression
In-Reply-To: <001101d243a0$a91e4f90$fb5aeeb0$@gmail.com>
References: <00a301d242e3$7d884e20$7898ea60$@gmail.com>
	<CAGxFJbRLyPgD0RJG9Y-ZG4qumPjp4UWi7h1iF4gm1kMYhXeh7g@mail.gmail.com>
	<001101d243a0$a91e4f90$fb5aeeb0$@gmail.com>
Message-ID: <CAGxFJbTo9p-ixAKFZqqAqH7t_xPg1qtVHejFVQ_=ocr9rBfAKA@mail.gmail.com>

Although others may respond, I think you will do much better studying
?regexp, which will answer all your questions. I believe the effort
you will make figuring it out will pay dividends for your future
R/regular expression usage that you cannot gain from my direct
explanation.

Good luck.

Best,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Nov 20, 2016 at 6:40 PM, Steven Nagy <nstefi at gmail.com> wrote:
> Thanks a lot Bert. That's amazing. I am very new to both R and regular
> expressions. I don't really understand the regular expression that you used
> below.
> And looks like I don't even need any special library, like the "gsubfn" for
> the strapply function.
> I was trying to use the regexr.com website to analyze your regular
> expression, but it doesn't seem to match any text there.
> Can you explain me the regular expression that you used?
> ".*: *([[:alnum:]]* *-> *STU|STU *-> *[[:alnum:]]*).*"
> So the dot in the front means any character and the star after that means
> that it can repeat 0 or more times, right?
> Then followed by a colon character ":" and a space, and what is the next
> star after that? It means that the sequence before that again can repeat 0
> or more times?
> And what are the double square brackets?
> Is ":alnum:" specific to R? I don't think "regexr.com" understands that. Or
> maybe that site is for regular expressions in Javascript, and the syntax is
> different in R?
>
> Thank you,
> Steven
>
> -----Original Message-----
> From: Bert Gunter [mailto:bgunter.4567 at gmail.com]
> Sent: Sunday, November 20, 2016 2:15 PM
> To: Steven Nagy <nstefi at gmail.com>
> Cc: R-help <r-help at r-project.org>
> Subject: Re: [R] Need some help with regular expression
>
> If I understand you correctly, I think you are making it more complex than
> necessary. Using your example (thanks!!), the following should get you
> started:
>
>
>> x<- c("Name.MEMBER_TYPE: NMA -> STU ; CATEGORY:  -> 1 ; CITY:
>> MISSISSAUGA -> Mississauga ; ZIP: L5N1H9 -> L5N 1H9 ; COUNTRY: CAN ->
>> ; MEMBER_STATUS:  -> N", "Name.MEMBER_TYPE: STU -> REG ; CATEGORY: 1
>> ->","Name.MEMBER_TYPE: -> STU")
>>
>> x
> [1] "Name.MEMBER_TYPE: NMA -> STU ; CATEGORY:  -> 1 ; CITY:
> MISSISSAUGA -> Mississauga ; ZIP: L5N1H9 -> L5N 1H9 ; COUNTRY: CAN -> ;
> MEMBER_STATUS:  -> N"
>
> [2] "Name.MEMBER_TYPE: STU -> REG ; CATEGORY: 1 ->"
> [3] "Name.MEMBER_TYPE: -> STU"
>>
>> sub(".*: *([[:alnum:]]* *-> *STU|STU *-> *[[:alnum:]]*).*","\\1",x)
> [1] "NMA -> STU" "STU -> REG" "-> STU"
>
>
> I am sure that you can get things to the form you desire in one go with some
> fiddling of the above, but it was easier for me to write the regex to pick
> out the pieces you wanted and leave the rest to you.
> Others may have slicker ways to do it, of course.
>
> HTH
>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Sat, Nov 19, 2016 at 8:06 PM, Steven Nagy <nstefi at gmail.com> wrote:
>> I tried out a regular expression on this website:
>>
>> http://regexr.com/3en1m
>>
>>
>>
>> So the input text is:
>>
>> "Name.MEMBER_TYPE:  -> STU"
>>
>>
>>
>> The regular expression is: ((?:\w+|\s) -> STU|STU -> (?:\w+|\s))
>>
>> And it returns:
>>
>> "  -> STU"
>>
>>
>>
>> but when I use in R, it doesn't return the same result:
>>
>> strapply(c, "((?:\\w+|\\s) -> STU|STU -> (?:\\w+|\\s))", c, backref =
>> -1, perl = TRUE)
>>
>> returns:
>> "Name.MEMBER_TYPE: -> STU"
>>
>>
>>
>>
>>
>> Here is what I was trying to do:
>>
>>
>>
>> I need to extract some values from a log table, and I created a
>> regular expression that helps me with that.
>>
>> The log table has cells with values like:
>>
>> a = "Name.MEMBER_TYPE: NMA -> STU ; CATEGORY:  -> 1 ; CITY:
>> MISSISSAUGA -> Mississauga ; ZIP: L5N1H9 -> L5N 1H9 ; COUNTRY: CAN ->
>> ; MEMBER_STATUS:  -> N"
>>
>> or
>> b = "Name.MEMBER_TYPE: STU -> REG ; CATEGORY: 1 ->"
>>
>> so I needed to extract the values that a STU member type is changing
>> from and to, so I needed NMA, STU in the 1st case or STU, REG in the 2nd
> case.
>>
>> I came up with this expression which worked in both cases:
>>
>> strapply(strapply(a, "(\\w+ -> STU|STU -> \\w+)", c, backref = -1,
>> perl = TRUE), "(\\w+) -> (\\w+)", c, backref = -2, perl = TRUE)
>>
>>
>>
>> But I had a 3rd case when the source member type was blank:
>>
>> c = "Name.MEMBER_TYPE: -> STU"
>>
>> and in that case it returned an error:
>>
>> strapply(strapply(c, "(\\w+ -> STU|STU -> \\w+)", c, backref = -1,
>> perl = TRUE), "(\\w+) -> (\\w+)", c, backref = -2, perl = TRUE)
>>
>> Error: is.character(x) is not TRUE
>>
>>
>>
>> I found that the error is because this returns NULL:
>>
>> strapply(c, "(\\w+ -> STU|STU -> \\w+)", c, backref = -1, perl = TRUE)
>>
>>
>>
>>
>>
>> So I tried to modify the regular expression to match any word or blank
>> space:
>>
>> strapply(c, "((?:\\w+|\\s) -> STU|STU -> (?:\\w+|\\s))", c, backref =
>> -1, perl = TRUE)
>>
>>
>>
>> but this returned me the whole value of "c":
>>
>> "Name.MEMBER_TYPE:  -> STU"
>>
>> and I only needed "  -> STU" as it shows on the website regxr.com
>>
>>
>>
>> Is the result wrong on the regxr.com website or strapply returns the
>> wrong result?
>>
>>
>>
>> Thanks,
>>
>> Steven
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From petr.pikal at precheza.cz  Mon Nov 21 06:57:58 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Mon, 21 Nov 2016 05:57:58 +0000
Subject: [R] Melt and compute Max, Mean, Min
In-Reply-To: <CAMLwc7OdFJ2bj3_2TqZpS0HNZrdVc_0=6ud7jv8B1HKN43xr2Q@mail.gmail.com>
References: <CAMLwc7PNUzOtLFv+yemQZEXsf535nsuzMbf0ShZZ_i3BqsRrHw@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5046931@SRVEXCHMBX.precheza.cz>
	<CAMLwc7M8e8_2suuZOVGDKTv=bVMqGiLsPRSGZNXFToMm1WLq8A@mail.gmail.com>
	<CAMLwc7OdFJ2bj3_2TqZpS0HNZrdVc_0=6ud7jv8B1HKN43xr2Q@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5046CB9@SRVEXCHMBX.precheza.cz>

Hi

see in line

From: Miluji Sb [mailto:milujisb at gmail.com]
Sent: Friday, November 18, 2016 3:57 PM
To: PIKAL Petr <petr.pikal at precheza.cz>
Cc: r-help mailing list <r-help at r-project.org>
Subject: Re: [R] Melt and compute Max, Mean, Min

If I do:

as.data.frame(apply(df[,-(1:3)],1, mean, na.rm=T))

is it possible to sequentially name the variables as "mean_1960", "max_1960". "min_1960", "mean_1961", "max_1961". "min_1961", ...?

But here you have only mean. How do you want to add min or max?


On Fri, Nov 18, 2016 at 3:10 PM, Miluji Sb <milujisb at gmail.com<mailto:milujisb at gmail.com>> wrote:
Dear Petr,

Thank you for the code, apologies though as I copied the wrong data, This is precipitation data and not temperature.

For the loop, could I do something like this?

filelist <- list.files(pattern=".csv")


Not exactly.

In this case I would use for cycle. It is quite easy to do something like:

for( i in 1:length(filelist)) {

temp<-read.csv(filelist[i]) #you need to read your file in R first

.mean <- apply(temp[,-(1:3)],1, mean, na.rm=T)
.max <- apply(temp[,-(1:3)],1, max, na.rm=T)
.min <- apply(temp[,-(1:3)],1, min, na.rm=T)

# now you can concatenate those results as you wish, name them or anything. It is difficult to suggest any direct code as you did not disclose what do you want to do with summaries further.

}

And BTW, please, do not post in HTML.



myDTs <- lapply(filelist, function(.file) {

apply(temp[,-(1:3)],1, mean, na.rm=T)

}

Thanks again!

Sincerely,

Milu


On Fri, Nov 18, 2016 at 2:46 PM, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
Hi

I am not completely sure what you want to do but

> apply(temp[,-(1:3)],1, mean, na.rm=T)
       1        2        3        4        5
     NaN      NaN 2.159516 1.519914 1.514007
> apply(temp[,-(1:3)],1, max, na.rm=T)
       1        2        3        4        5
    -Inf     -Inf 57.36528 39.45348 45.23904
Warning messages:
1: In FUN(newX[, i], ...) :
  no non-missing arguments to max; returning -Inf
2: In FUN(newX[, i], ...) :
  no non-missing arguments to max; returning -Inf
> apply(temp[,-(1:3)],1, min, na.rm=T)
  1   2   3   4   5
Inf Inf   0   0   0

gives you mentioned summary for each row. If you have duplicate rows you shall first aggregate them. However, it seems to me that your data are not correct. It is quite strange that for given lat/lon you have one day value 23 and the next day 0.

temp[1:5, 1:10]
  ISO3 lon lat day_1 day_2    day_3 day_4 day_5 day_6    day_7
1  CHL -69 -55    NA    NA       NA    NA    NA    NA       NA
2  CHL -68 -55    NA    NA       NA    NA    NA    NA       NA
3  CHL -72 -54     0     0  0.00000     0     0     0  2.83824
4 <NA> -71 -54     0     0 23.37984     0     0     0 11.80116
5  CHL -70 -54     0     0  0.00000     0     0     0  1.24956

If you want to process all your files you can do it in cycle. The function

list.files()

can be handy for that task.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of Miluji Sb
> Sent: Friday, November 18, 2016 1:49 PM
> To: r-help mailing list <r-help at r-project.org<mailto:r-help at r-project.org>>
> Subject: [R] Melt and compute Max, Mean, Min
>
> Dear all,
>
> I have 51 years of data (1960 - 2010) in csv format, where each file represents
> one year of data. Below is what each file looks like.
>
> These are temperature data by coordinates, my goal is to to compute max,
> min, and mean by year for each of the coordinates and construct a panel
> dataset. Any help will be appreciated, thank you!
>
> Sincerely,
>
> Milu
>
> temp <- dput(head(df,5))
> structure(list(ISO3 = structure(c(28L, 28L, 28L, NA, 28L), .Label = c("AFG",
> "AGO", "ALB", "ARE", "ARG", "ARM", "AUS", "AUT", "AZE", "BDI", "BEL",
> "BEN", "BFA", "BGD", "BGR", "BHS", "BIH", "BLR", "BLZ", "BOL", "BRA",
> "BRN", "BTN", "BWA", "CAF", "CAN", "CHE", "CHL", "CHN", "CIV", "CMR",
> "COD", "COG", "COL", "CRI", "CUB", "CYP", "CZE", "DEU", "DJI", "DNK",
> "DOM", "DZA", "ECU", "EGY", "ERI", "ESH", "ESP", "EST", "ETH", "FIN", "FJI",
> "FLK", "FRA", "GAB", "GBR", "GEO", "GHA", "GIN", "GNB", "GNQ", "GRC",
> "GRL", "GTM", "GUF", "GUY", "HND", "HRV", "HTI", "HUN", "IDN", "IND",
> "IRL", "IRN", "IRQ", "ISL", "ISR", "ITA", "JAM", "JOR", "JPN", "KAZ", "KEN",
> "KGZ", "KHM", "KIR", "KOR", "KWT", "LAO", "LBN", "LBR", "LBY", "LCA",
> "LKA", "LSO", "LTU", "LUX", "LVA", "MAR", "MDA", "MDG", "MEX", "MKD",
> "MLI", "MMR", "MNE", "MNG", "MOZ", "MRT", "MWI", "MYS", "NAM",
> "NCL", "NER", "NGA", "NIC", "NLD", "NOR", "NPL", "NZL", "OMN", "PAK",
> "PAN", "PER", "PHL", "PNG", "POL", "PRI", "PRK", "PRT", "PRY", "QAT",
> "ROU", "RUS", "RWA", "SAU", "SDN", "SEN", "SJM", "SLB", "SLE", "SLV",
> "SOM", "SRB", "SUR", "SVK", "SVN", "SWE", "SWZ", "SYR", "TCD", "TGO",
> "THA", "TJK", "TKM", "TLS", "TUN", "TUR", "TWN", "TZA", "UGA", "UKR",
> "URY", "USA", "UZB", "VEN", "VNM", "VUT", "YEM", "ZAF", "ZMB", "ZWE"
> ), class = "factor"), lon = c(-69L, -68L, -72L, -71L, -70L),
>     lat = c(-55L, -55L, -54L, -54L, -54L), day_1 = c(NA, NA,
>     0, 0, 0), day_2 = c(NA, NA, 0, 0, 0), day_3 = c(NA, NA, 0,
>     23.37984, 0), day_4 = c(NA, NA, 0, 0, 0), day_5 = c(NA, NA,
>     0, 0, 0), day_6 = c(NA, NA, 0, 0, 0), day_7 = c(NA, NA, 2.83824,
>     11.80116, 1.24956), day_8 = c(NA, NA, 0, 1.68588, 14.69448
>     ), day_9 = c(NA, NA, 0, 0, 1.09296), day_10 = c(NA, NA, 0,
>     0, 0), day_11 = c(NA, NA, 3.78, 3.7422, 0), day_12 = c(NA,
>     NA, 0.54, 0, 0), day_13 = c(NA, NA, 0, 0, 0), day_14 = c(NA,
>     NA, 0, 0, 0.39204), day_15 = c(NA, NA, 0, 0, 11.58732), day_16 = c(NA,
>     NA, 0, 0, 0), day_17 = c(NA, NA, 0, 1.14048, 12.26448), day_18 = c(NA,
>     NA, 0, 1.1934, 7.59024), day_19 = c(NA, NA, 9.74268, 0, 0
>     ), day_20 = c(NA, NA, 0, 0, 0), day_21 = c(NA, NA, 1.96776,
>     0, 0), day_22 = c(NA, NA, 0, 0, 0), day_23 = c(NA, NA, 0,
>     0, 0), day_24 = c(NA, NA, 6.21756, 2.74752, 0), day_25 = c(NA,
>     NA, 0, 0, 3.37932), day_26 = c(NA, NA, 4.8384, 0, 0), day_27 = c(NA,
>     NA, 0, 0, 0), day_28 = c(NA, NA, 0, 0, 0), day_29 = c(NA,
>     NA, 22.37328, 0, 0), day_30 = c(NA, NA, 28.97424, 11.25468,
>     0), day_31 = c(NA, NA, 0, 0, 0), day_32 = c(NA, NA, 0, 0,
>     2.00448), day_33 = c(NA, NA, 0, 0, 0), day_34 = c(NA, NA,
>     0, 0, 0), day_35 = c(NA, NA, 0, 0, 0), day_36 = c(NA, NA,
>     0, 0, 0), day_37 = c(NA, NA, 0, 0, 0), day_38 = c(NA, NA,
>     32.7132, 31.71852, 0), day_39 = c(NA, NA, 0, 0, 5.84604),
>     day_40 = c(NA, NA, 0, 0, 0), day_41 = c(NA, NA, 0, 0, 0),
>     day_42 = c(NA, NA, 0, 0, 0), day_43 = c(NA, NA, 0, 0, 0),
>     day_44 = c(NA, NA, 0, 0, 1.78416), day_45 = c(NA, NA, 0,
>     0, 0), day_46 = c(NA, NA, 33.84504, 0, 0), day_47 = c(NA,
>     NA, 0, 0, 0), day_48 = c(NA, NA, 0, 0, 0), day_49 = c(NA,
>     NA, 0, 0, 0), day_50 = c(NA, NA, 0, 0.4752, 0), day_51 = c(NA,
>     NA, 0, 0, 22.02012), day_52 = c(NA, NA, 0, 0, 0), day_53 = c(NA,
>     NA, 0, 0, 3.48084), day_54 = c(NA, NA, 0, 0, 0), day_55 = c(NA,
>     NA, 0.58212, 0, 0), day_56 = c(NA, NA, 0.35316, 0, 0), day_57 = c(NA,
>     NA, 0, 0, 12.65436), day_58 = c(NA, NA, 0, 0, 0), day_59 = c(NA,
>     NA, 0, 0, 0), day_60 = c(NA, NA, 3.03372, 22.05576, 0), day_61 = c(NA,
>     NA, 2.5758, 0, 0), day_62 = c(NA, NA, 0, 0, 0), day_63 = c(NA,
>     NA, 3.67416, 25.22016, 4.21524), day_64 = c(NA, NA, 0.52488,
>     3.60288, 0), day_65 = c(NA, NA, 12.82608, 0, 0), day_66 = c(NA,
>     NA, 0, 0, 0), day_67 = c(NA, NA, 0, 0, 0), day_68 = c(NA,
>     NA, 0, 0, 0), day_69 = c(NA, NA, 0, 0, 0), day_70 = c(NA,
>     NA, 1.11564, 5.17536, 0), day_71 = c(NA, NA, 1.18584, 0,
>     0), day_72 = c(NA, NA, 0, 0, 0.10584), day_73 = c(NA, NA,
>     0.62748, 14.39748, 7.50708), day_74 = c(NA, NA, 7.20252,
>     20.02644, 1.07244), day_75 = c(NA, NA, 1.87488, 0, 0), day_76 = c(NA,
>     NA, 0.26784, 0, 0), day_77 = c(NA, NA, 0, 0, 0), day_78 = c(NA,
>     NA, 0, 0, 2.81664), day_79 = c(NA, NA, 0, 0, 0), day_80 = c(NA,
>     NA, 0, 0, 0), day_81 = c(NA, NA, 0, 0, 0), day_82 = c(NA,
>     NA, 1.29276, 0, 0), day_83 = c(NA, NA, 0.18468, 0, 1.46124
>     ), day_84 = c(NA, NA, 0, 0, 0), day_85 = c(NA, NA, 0, 0,
>     0), day_86 = c(NA, NA, 57.36528, 0, 0), day_87 = c(NA, NA,
>     8.19504, 0, 0), day_88 = c(NA, NA, 0, 0, 0), day_89 = c(NA,
>     NA, 6.45732, 0, 0), day_90 = c(NA, NA, 0, 0, 0), day_91 = c(NA,
>     NA, 0, 0, 44.20332), day_92 = c(NA, NA, 0, 0, 6.31476), day_93 = c(NA,
>     NA, 0, 0, 0.35748), day_94 = c(NA, NA, 16.74972, 30.35988,
>     5.0436), day_95 = c(NA, NA, 4.93992, 1.46556, 19.86768),
>     day_96 = c(NA, NA, 0, 0, 0.88128), day_97 = c(NA, NA, 5.751,
>     19.02096, 0), day_98 = c(NA, NA, 11.5452, 13.37148, 0), day_99 = c(NA,
>     NA, 0, 0, 0), day_100 = c(NA, NA, 0, 0, 0), day_101 = c(NA,
>     NA, 4.70124, 23.80644, 7.61832), day_102 = c(NA, NA, 0, 1.02492,
>     0), day_103 = c(NA, NA, 0, 0, 15.86304), day_104 = c(NA,
>     NA, 0, 0, 0.26352), day_105 = c(NA, NA, 0, 0, 21.60864),
>     day_106 = c(NA, NA, 56.93436, 0, 0.22464), day_107 = c(NA,
>     NA, 8.13348, 0, 0), day_108 = c(NA, NA, 6.83748, 0, 0), day_109 = c(NA,
>     NA, 0, 0, 0), day_110 = c(NA, NA, 14.36724, 0, 0), day_111 = c(NA,
>     NA, 0.63936, 2.43864, 4.0554), day_112 = c(NA, NA, 1.21392,
>     1.15452, 0), day_113 = c(NA, NA, 0.7722, 0, 0), day_114 = c(NA,
>     NA, 0, 0, 1.08864), day_115 = c(NA, NA, 1.47528, 0, 0), day_116 = c(NA,
>     NA, 0, 1.73124, 0), day_117 = c(NA, NA, 0, 0, 0), day_118 = c(NA,
>     NA, 2.4516, 0, 0), day_119 = c(NA, NA, 0, 3.14388, 0), day_120 = c(NA,
>     NA, 1.81872, 0, 0), day_121 = c(NA, NA, 2.77236, 0, 0), day_122 = c(NA,
>     NA, 1.34028, 0.70632, 0), day_123 = c(NA, NA, 0, 0, 0), day_124 = c(NA,
>     NA, 0, 0, 0), day_125 = c(NA, NA, 0.56484, 0.74412, 0), day_126 = c(NA,
>     NA, 1.11888, 0.06264, 0), day_127 = c(NA, NA, 0, 0, 0), day_128 = c(NA,
>     NA, 1.05624, 0, 0), day_129 = c(NA, NA, 26.63928, 34.04268,
>     0), day_130 = c(NA, NA, 6.89796, 0, 0), day_131 = c(NA, NA,
>     1.91592, 2.241, 0), day_132 = c(NA, NA, 0, 2.23668, 45.23904
>     ), day_133 = c(NA, NA, 0, 0, 6.46272), day_134 = c(NA, NA,
>     0, 0, 0), day_135 = c(NA, NA, 0, 0, 0), day_136 = c(NA, NA,
>     0, 0, 0), day_137 = c(NA, NA, 0, 0, 0), day_138 = c(NA, NA,
>     0, 0, 0), day_139 = c(NA, NA, 0, 0, 0), day_140 = c(NA, NA,
>     0, 0, 0), day_141 = c(NA, NA, 0, 0, 0), day_142 = c(NA, NA,
>     0, 0, 0), day_143 = c(NA, NA, 0, 0, 0), day_144 = c(NA, NA,
>     2.943, 5.17536, 0), day_145 = c(NA, NA, 0, 0, 0), day_146 = c(NA,
>     NA, 0, 0, 0), day_147 = c(NA, NA, 0, 0, 0), day_148 = c(NA,
>     NA, 10.96308, 2.98188, 0), day_149 = c(NA, NA, 20.4822, 0.43632,
>     0), day_150 = c(NA, NA, 1.5282, 0, 0), day_151 = c(NA, NA,
>     0, 0, 0), day_152 = c(NA, NA, 0, 0, 0), day_153 = c(NA, NA,
>     0, 0, 0), day_154 = c(NA, NA, 0, 0, 0), day_155 = c(NA, NA,
>     0, 0, 0), day_156 = c(NA, NA, 0, 0, 0), day_157 = c(NA, NA,
>     0, 0, 0), day_158 = c(NA, NA, 0, 0, 0), day_159 = c(NA, NA,
>     0, 0, 0), day_160 = c(NA, NA, 0, 0, 0), day_161 = c(NA, NA,
>     0, 0, 0), day_162 = c(NA, NA, 0, 0, 0), day_163 = c(NA, NA,
>     0, 26.3412, 4.07376), day_164 = c(NA, NA, 0, 4.28328, 3.03156
>     ), day_165 = c(NA, NA, 0, 0, 0), day_166 = c(NA, NA, 0, 0,
>     4.60404), day_167 = c(NA, NA, 0, 0, 0.70848), day_168 = c(NA,
>     NA, 0, 0, 0), day_169 = c(NA, NA, 0, 0, 0), day_170 = c(NA,
>     NA, 0, 0, 0), day_171 = c(NA, NA, 0, 0, 0), day_172 = c(NA,
>     NA, 0, 0, 0), day_173 = c(NA, NA, 0, 0, 3.7854), day_174 = c(NA,
>     NA, 0, 0, 0), day_175 = c(NA, NA, 0, 0, 0), day_176 = c(NA,
>     NA, 0, 0, 0), day_177 = c(NA, NA, 0, 0, 0), day_178 = c(NA,
>     NA, 0, 0, 0), day_179 = c(NA, NA, 0, 0, 0), day_180 = c(NA,
>     NA, 0, 0, 0), day_181 = c(NA, NA, 0, 0, 0), day_182 = c(NA,
>     NA, 0, 0, 0), day_183 = c(NA, NA, 0, 0, 0), day_184 = c(NA,
>     NA, 0, 0, 0), day_185 = c(NA, NA, 0, 0, 0), day_186 = c(NA,
>     NA, 0, 0, 0), day_187 = c(NA, NA, 7.30728, 4.1202, 0), day_188 = c(NA,
>     NA, 2.56608, 0.5886, 0), day_189 = c(NA, NA, 0, 0, 0), day_190 = c(NA,
>     NA, 21.93156, 8.0082, 11.4318), day_191 = c(NA, NA, 3.13308,
>     0, 0), day_192 = c(NA, NA, 0, 0, 0.10692), day_193 = c(NA,
>     NA, 0, 0, 4.65912), day_194 = c(NA, NA, 0, 0, 0), day_195 = c(NA,
>     NA, 0, 0, 0), day_196 = c(NA, NA, 0, 0, 0), day_197 = c(NA,
>     NA, 0, 0, 0), day_198 = c(NA, NA, 0, 0, 0), day_199 = c(NA,
>     NA, 0, 0, 0), day_200 = c(NA, NA, 0, 0, 0), day_201 = c(NA,
>     NA, 0, 0, 0), day_202 = c(NA, NA, 0, 7.77276, 4.6602), day_203 = c(NA,
>     NA, 0, 0.86292, 0), day_204 = c(NA, NA, 0, 0, 0), day_205 = c(NA,
>     NA, 21.45528, 8.69616, 0), day_206 = c(NA, NA, 0, 0, 0),
>     day_207 = c(NA, NA, 0, 0, 0), day_208 = c(NA, NA, 0, 0, 0
>     ), day_209 = c(NA, NA, 0, 0, 0), day_210 = c(NA, NA, 0, 0,
>     0), day_211 = c(NA, NA, 0, 0, 0), day_212 = c(NA, NA, 0,
>     0, 0), day_213 = c(NA, NA, 0, 0, 0), day_214 = c(NA, NA,
>     0, 0, 0), day_215 = c(NA, NA, 0, 0, 0), day_216 = c(NA, NA,
>     0, 0, 0), day_217 = c(NA, NA, 0, 0, 0), day_218 = c(NA, NA,
>     0, 0, 0), day_219 = c(NA, NA, 0, 0, 0), day_220 = c(NA, NA,
>     6.10092, 10.85508, 13.22244), day_221 = c(NA, NA, 0.87156,
>     0, 0), day_222 = c(NA, NA, 0, 0, 15.46452), day_223 = c(NA,
>     NA, 0, 0, 9.83664), day_224 = c(NA, NA, 0, 0, 0), day_225 = c(NA,
>     NA, 0, 0, 0), day_226 = c(NA, NA, 16.46028, 0, 0), day_227 = c(NA,
>     NA, 0, 0, 0), day_228 = c(NA, NA, 0, 0, 0), day_229 = c(NA,
>     NA, 0, 0, 0), day_230 = c(NA, NA, 0, 0, 0), day_231 = c(NA,
>     NA, 2.7108, 0, 0), day_232 = c(NA, NA, 0, 0, 0), day_233 = c(NA,
>     NA, 0, 0, 0), day_234 = c(NA, NA, 0, 0, 0), day_235 = c(NA,
>     NA, 0, 0, 0), day_236 = c(NA, NA, 0, 0, 3.5586), day_237 = c(NA,
>     NA, 0, 0, 0), day_238 = c(NA, NA, 0, 0, 0), day_239 = c(NA,
>     NA, 10.23192, 0, 0), day_240 = c(NA, NA, 0, 0, 0), day_241 = c(NA,
>     NA, 0, 0, 0), day_242 = c(NA, NA, 0, 0, 0), day_243 = c(NA,
>     NA, 0, 0, 0), day_244 = c(NA, NA, 0, 0, 0), day_245 = c(NA,
>     NA, 0, 0, 0), day_246 = c(NA, NA, 0, 0, 0), day_247 = c(NA,
>     NA, 0, 0, 0), day_248 = c(NA, NA, 0, 0, 0), day_249 = c(NA,
>     NA, 0.50544, 0, 0), day_250 = c(NA, NA, 0.12636, 0, 0), day_251 = c(NA,
>     NA, 7.02432, 0, 5.39784), day_252 = c(NA, NA, 3.33828, 8.00064,
>     7.08372), day_253 = c(NA, NA, 0, 0, 0), day_254 = c(NA, NA,
>     0, 0, 0), day_255 = c(NA, NA, 2.5704, 4.71636, 11.99772),
>     day_256 = c(NA, NA, 0.3672, 0.75384, 0), day_257 = c(NA,
>     NA, 0, 0, 0), day_258 = c(NA, NA, 0.50328, 0, 0), day_259 = c(NA,
>     NA, 6.78888, 0, 0), day_260 = c(NA, NA, 0.96984, 0, 0), day_261 = c(NA,
>     NA, 4.62672, 0, 0), day_262 = c(NA, NA, 0, 0, 0), day_263 = c(NA,
>     NA, 3.16224, 0.27864, 0), day_264 = c(NA, NA, 0, 1.31112,
>     0), day_265 = c(NA, NA, 0.37692, 0, 0), day_266 = c(NA, NA,
>     0, 0, 0), day_267 = c(NA, NA, 0.70524, 0.43524, 0), day_268 = c(NA,
>     NA, 0.18792, 0.12744, 0), day_269 = c(NA, NA, 0, 1.79064,
>     0.96012), day_270 = c(NA, NA, 0, 0, 0.58644), day_271 = c(NA,
>     NA, 4.4982, 0, 0), day_272 = c(NA, NA, 0, 0, 0), day_273 = c(NA,
>     NA, 2.04552, 6.56964, 0), day_274 = c(NA, NA, 0.71712, 0.93852,
>     0), day_275 = c(NA, NA, 0, 0, 0), day_276 = c(NA, NA, 0,
>     0, 0), day_277 = c(NA, NA, 3.31452, 0, 0), day_278 = c(NA,
>     NA, 1.20204, 0, 0), day_279 = c(NA, NA, 0, 0, 0), day_280 = c(NA,
>     NA, 0, 0, 0), day_281 = c(NA, NA, 0, 0, 0), day_282 = c(NA,
>     NA, 17.955, 5.7942, 9.93816), day_283 = c(NA, NA, 4.79304,
>     4.8006, 0), day_284 = c(NA, NA, 3.9366, 0.78084, 0), day_285 = c(NA,
>     NA, 0, 0, 0), day_286 = c(NA, NA, 0, 0, 0), day_287 = c(NA,
>     NA, 0, 0, 0), day_288 = c(NA, NA, 0, 0, 0), day_289 = c(NA,
>     NA, 0, 0, 0), day_290 = c(NA, NA, 0, 0, 0), day_291 = c(NA,
>     NA, 0, 0, 0), day_292 = c(NA, NA, 0, 0, 0), day_293 = c(NA,
>     NA, 1.55736, 0, 0), day_294 = c(NA, NA, 4.28328, 0, 0), day_295 = c(NA,
>     NA, 0, 0, 0), day_296 = c(NA, NA, 0, 0, 0), day_297 = c(NA,
>     NA, 1.6362, 0, 0), day_298 = c(NA, NA, 1.28844, 0, 6.14088
>     ), day_299 = c(NA, NA, 0, 0, 0.50112), day_300 = c(NA, NA,
>     0, 0, 0), day_301 = c(NA, NA, 0, 0.13824, 0.03456), day_302 = c(NA,
>     NA, 0, 2.92572, 9.24264), day_303 = c(NA, NA, 2.8188, 0.41796,
>     0), day_304 = c(NA, NA, 2.04876, 11.28384, 0), day_305 = c(NA,
>     NA, 0, 0.3564, 0), day_306 = c(NA, NA, 0, 0, 0), day_307 = c(NA,
>     NA, 0, 2.36736, 0), day_308 = c(NA, NA, 0, 0, 0), day_309 = c(NA,
>     NA, 34.91856, 20.42604, 0), day_310 = c(NA, NA, 0, 0, 0),
>     day_311 = c(NA, NA, 0, 0, 0), day_312 = c(NA, NA, 0, 0.40392,
>     0), day_313 = c(NA, NA, 0, 0.5292, 0), day_314 = c(NA, NA,
>     0, 0, 5.21424), day_315 = c(NA, NA, 0, 0, 0), day_316 = c(NA,
>     NA, 0, 0, 0.4266), day_317 = c(NA, NA, 0, 0, 0), day_318 = c(NA,
>     NA, 0, 0, 0), day_319 = c(NA, NA, 0, 0, 0), day_320 = c(NA,
>     NA, 0.23436, 0.6048, 14.9256), day_321 = c(NA, NA, 0, 0.10908,
>     0), day_322 = c(NA, NA, 7.68096, 6.66036, 4.53924), day_323 = c(NA,
>     NA, 1.09728, 1.59732, 8.51148), day_324 = c(NA, NA, 0, 0,
>     0), day_325 = c(NA, NA, 1.46016, 0, 0), day_326 = c(NA, NA,
>     0, 0, 8.70048), day_327 = c(NA, NA, 0, 0, 0), day_328 = c(NA,
>     NA, 0, 0, 0), day_329 = c(NA, NA, 0, 0, 0), day_330 = c(NA,
>     NA, 5.3082, 0, 0), day_331 = c(NA, NA, 2.5866, 0, 0), day_332 = c(NA,
>     NA, 8.03628, 6.3666, 4.3308), day_333 = c(NA, NA, 0, 0, 0
>     ), day_334 = c(NA, NA, 0, 0, 0), day_335 = c(NA, NA, 0, 0,
>     0), day_336 = c(NA, NA, 0, 0, 5.29632), day_337 = c(NA, NA,
>     0, 1.77444, 2.7216), day_338 = c(NA, NA, 0.40608, 0, 0.83052
>     ), day_339 = c(NA, NA, 0, 0, 0), day_340 = c(NA, NA, 0, 0,
>     0), day_341 = c(NA, NA, 0, 0, 0), day_342 = c(NA, NA, 0,
>     0, 0), day_343 = c(NA, NA, 0, 0, 0), day_344 = c(NA, NA,
>     7.73388, 0, 0), day_345 = c(NA, NA, 4.80384, 0, 0), day_346 = c(NA,
>     NA, 4.374, 0.09288, 0), day_347 = c(NA, NA, 6.42924, 3.2022,
>     0), day_348 = c(NA, NA, 0, 16.27668, 0), day_349 = c(NA,
>     NA, 0, 0.90072, 9.36684), day_350 = c(NA, NA, 0.135, 1.87272,
>     2.49048), day_351 = c(NA, NA, 0, 0, 0), day_352 = c(NA, NA,
>     0, 0, 0), day_353 = c(NA, NA, 0, 0, 0), day_354 = c(NA, NA,
>     0, 0, 0), day_355 = c(NA, NA, 0, 0, 0), day_356 = c(NA, NA,
>     0, 0, 0), day_357 = c(NA, NA, 2.82636, 39.45348, 26.08848
>     ), day_358 = c(NA, NA, 0, 0, 22.8582), day_359 = c(NA, NA,
>     0, 0, 1.34028), day_360 = c(NA, NA, 30.03804, 0, 3.49704),
>     day_361 = c(NA, NA, 0.13392, 4.941, 4.94424), day_362 = c(NA,
>     NA, 0.92016, 0, 0.70632), day_363 = c(NA, NA, 0, 0, 0), day_364 = c(NA,
>     NA, 0, 0, 0), day_365 = c(NA, NA, 0, 0, 0), day_366 = c(NA,
>     NA, 21.42072, 0, 0)), .Names = c("ISO3", "lon", "lat", "day_1", "day_2",
> "day_3", "day_4", "day_5", "day_6", "day_7", "day_8", "day_9", "day_10",
> "day_11", "day_12", "day_13", "day_14", "day_15", "day_16", "day_17",
> "day_18", "day_19", "day_20", "day_21", "day_22", "day_23", "day_24",
> "day_25", "day_26", "day_27", "day_28", "day_29", "day_30", "day_31",
> "day_32", "day_33", "day_34", "day_35", "day_36", "day_37", "day_38",
> "day_39", "day_40", "day_41", "day_42", "day_43", "day_44", "day_45",
> "day_46", "day_47", "day_48", "day_49", "day_50", "day_51", "day_52",
> "day_53", "day_54", "day_55", "day_56", "day_57", "day_58", "day_59",
> "day_60", "day_61", "day_62", "day_63", "day_64", "day_65", "day_66",
> "day_67", "day_68", "day_69", "day_70", "day_71", "day_72", "day_73",
> "day_74", "day_75", "day_76", "day_77", "day_78", "day_79", "day_80",
> "day_81", "day_82", "day_83", "day_84", "day_85", "day_86", "day_87",
> "day_88", "day_89", "day_90", "day_91", "day_92", "day_93", "day_94",
> "day_95", "day_96", "day_97", "day_98", "day_99", "day_100", "day_101",
> "day_102", "day_103", "day_104", "day_105", "day_106", "day_107",
> "day_108", "day_109", "day_110", "day_111", "day_112", "day_113",
> "day_114", "day_115", "day_116", "day_117", "day_118", "day_119",
> "day_120", "day_121", "day_122", "day_123", "day_124", "day_125",
> "day_126", "day_127", "day_128", "day_129", "day_130", "day_131",
> "day_132", "day_133", "day_134", "day_135", "day_136", "day_137",
> "day_138", "day_139", "day_140", "day_141", "day_142", "day_143",
> "day_144", "day_145", "day_146", "day_147", "day_148", "day_149",
> "day_150", "day_151", "day_152", "day_153", "day_154", "day_155",
> "day_156", "day_157", "day_158", "day_159", "day_160", "day_161",
> "day_162", "day_163", "day_164", "day_165", "day_166", "day_167",
> "day_168", "day_169", "day_170", "day_171", "day_172", "day_173",
> "day_174", "day_175", "day_176", "day_177", "day_178", "day_179",
> "day_180", "day_181", "day_182", "day_183", "day_184", "day_185",
> "day_186", "day_187", "day_188", "day_189", "day_190", "day_191",
> "day_192", "day_193", "day_194", "day_195", "day_196", "day_197",
> "day_198", "day_199", "day_200", "day_201", "day_202", "day_203",
> "day_204", "day_205", "day_206", "day_207", "day_208", "day_209",
> "day_210", "day_211", "day_212", "day_213", "day_214", "day_215",
> "day_216", "day_217", "day_218", "day_219", "day_220", "day_221",
> "day_222", "day_223", "day_224", "day_225", "day_226", "day_227",
> "day_228", "day_229", "day_230", "day_231", "day_232", "day_233",
> "day_234", "day_235", "day_236", "day_237", "day_238", "day_239",
> "day_240", "day_241", "day_242", "day_243", "day_244", "day_245",
> "day_246", "day_247", "day_248", "day_249", "day_250", "day_251",
> "day_252", "day_253", "day_254", "day_255", "day_256", "day_257",
> "day_258", "day_259", "day_260", "day_261", "day_262", "day_263",
> "day_264", "day_265", "day_266", "day_267", "day_268", "day_269",
> "day_270", "day_271", "day_272", "day_273", "day_274", "day_275",
> "day_276", "day_277", "day_278", "day_279", "day_280", "day_281",
> "day_282", "day_283", "day_284", "day_285", "day_286", "day_287",
> "day_288", "day_289", "day_290", "day_291", "day_292", "day_293",
> "day_294", "day_295", "day_296", "day_297", "day_298", "day_299",
> "day_300", "day_301", "day_302", "day_303", "day_304", "day_305",
> "day_306", "day_307", "day_308", "day_309", "day_310", "day_311",
> "day_312", "day_313", "day_314", "day_315", "day_316", "day_317",
> "day_318", "day_319", "day_320", "day_321", "day_322", "day_323",
> "day_324", "day_325", "day_326", "day_327", "day_328", "day_329",
> "day_330", "day_331", "day_332", "day_333", "day_334", "day_335",
> "day_336", "day_337", "day_338", "day_339", "day_340", "day_341",
> "day_342", "day_343", "day_344", "day_345", "day_346", "day_347",
> "day_348", "day_349", "day_350", "day_351", "day_352", "day_353",
> "day_354", "day_355", "day_356", "day_357", "day_358", "day_359",
> "day_360", "day_361", "day_362", "day_363", "day_364", "day_365",
> "day_366"), row.names = c(NA, 5L), class =
> "data.frame")
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.



________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

	[[alternative HTML version deleted]]


From ramnik.bansal at gmail.com  Mon Nov 21 10:52:58 2016
From: ramnik.bansal at gmail.com (Ramnik Bansal)
Date: Mon, 21 Nov 2016 15:22:58 +0530
Subject: [R] explicit coercion warnings as.numeric Versus as.logical
Message-ID: <CAMLd9E406zPjg1Hge8LkScMhxTPJ1HqSo098Ux729uU9CF9=_w@mail.gmail.com>

Hi,

I am trying to understand under which specific conditions does explicit
coercion produce warnings.

> as.numeric(c(1, F, "b"))
[1]  1 NA NA
Warning message:
NAs introduced by coercion

> as.logical(c(1, F, "b"))
[1]    NA FALSE    NA


In above examples, as.numeric produces warning but as.logical does not.
What is the reason behind this different behaviour. Ideally as.logical
should also have produced the warning message like as.numeric.

Thanks in advance.
Ramnik

	[[alternative HTML version deleted]]


From Marios.BARLAS at cea.fr  Mon Nov 21 12:18:33 2016
From: Marios.BARLAS at cea.fr (BARLAS Marios 247554)
Date: Mon, 21 Nov 2016 11:18:33 +0000
Subject: [R] fdaPDE Documentation and examples
Message-ID: <01BFC0B2B4ABFC4CB432008F852D766101399594@EXDAG0-A1.intra.cea.fr>

Hello,

I'm trying to use R for solving PDEs ( heat and Poisson equations) in R. 

I found fdaPDE as a package implementing FEMs, has any1 used it before ? I find it severely lacks documentation and examples on its usage. 

Any feedback is welcome!

Thanks in advance,

Marios Barlas
PhD Candidate
CMOS & Memory Integration
Advanced Memory Group

Leti, technology research institute 
Commissariat ? l'?nergie atomique et aux ?nergies alternatives
T. +33 4 38 78 11 50 M. +33 6 02 61 83 49
www.leti.fr  | Leti is a member of the Carnot Institutes network



  
          


From stuartjpatterson at googlemail.com  Mon Nov 21 13:21:27 2016
From: stuartjpatterson at googlemail.com (Stuart Patterson)
Date: Mon, 21 Nov 2016 12:21:27 +0000
Subject: [R] Presenting Hazard ratios for interacting variables in a Cox
	model
In-Reply-To: <17239E26-76DC-4057-B573-C30288802062@comcast.net>
References: <CAJvq0dBT+eeZ39g2RSk7KW4+Mdi4_Le-Kc2X0gM4GgZp=6oMzA@mail.gmail.com>
	<17239E26-76DC-4057-B573-C30288802062@comcast.net>
Message-ID: <CAJvq0dBWWOE6zWSn7POrhoqsP-j=JN_xdJYNPJrF4FrJNV21QA@mail.gmail.com>

Dear David,
Thank you for your reply. Your suggestions on how better to write the
command are very useful, and I can see how the simplification would help. I
hadn't realised that the lower order variables would be included if I
simply wrote in teh interaction terms. Thank you

The table below is how I feel that I ought to present my results. If anyone
has any suggestions on how to obtain these values I would be very grateful!

Cheers
Stuart



Variable
Hazard Ratio
95% Confidence Interval
p Value

Year
Prev. TB
Age
2002-2005
No
< 24 months
24-48 months



>48 months



Yes
< 24 months



24-48 months



>48 months



2006-2010
No
< 24 months



24-48 months



>48 months



Yes
< 24 months



24-48 months
a
b
c
>48 months



2011-2015
No
< 24 months



24-48 months



>48 months



Yes
< 24 months



24-48 months



>48 months
















On 18 November 2016 at 19:21, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Nov 18, 2016, at 6:56 AM, Stuart Patterson via R-help <
> r-help at r-project.org> wrote:
> >
> > I have a time-dependent cox model with three variables, each of which
> > interacts with the other two. So my final model is:
> >
> > fit12<-coxph(formula = Surv(data$TimeIn, data$Timeout, data$Status) ~
> data$
> > Year+data$Life_Stg+data$prev.tb +data$prev.tb*data$Life_Stg +
> data$Year*data
> > $Life_Stg + data$Year*data$prev.tb + frailty(data$Natal_Group), data =
> data)
>
> It seems fairly likely that you are shooting yourself in the foot by using
> the `data$variate` inside the formula. It will prevent the regression
> result from having correctly assembled references to variables. And that
> will become evident when you try to do any predictions. Try instead:
>
> fit12<-coxph(formula = Surv( TimeIn,  Timeout,  Status) ~
>             prev.tb * Life_Stg +  Year *Life_Stg +  Year * prev.tb +
> frailty( Natal_Group),
>             data = data)
>
> The `*` in a formula automatically includes the lower order individual
> variates in the estimates. Your model RHS could have also been written
> (more clearly in my opinion):
>
>          ~ (prev.tb + Life_Stg +  Year)^2
>
> ... since R formulas interpret the `(.)^N` operation as "all base effects
> and interactions up to order N".
>
> >
> > For my variables, there are 3 categories of year, three of year, and
> > prev.tb is a binary variable. Because of the interactions, when I present
> > the results, I want to present the Hazard ratio, 95% CI, and p value for
> > each combination of the three variables. How do I get R to give me these
> > values please?
> >
> > I think that the contrast function does this for other models but does
> not
> > work for coxph?
>
> The usual method would be to use `predict` on a 'newdata' dataframe with
> all the combinations generated from `expand.grid`. The combination of the
> reference values of all three variables should yield a 1.0 hazard ratio.
> But time-dependent model predictions need a complete specification of a
> sequence of values over the time course of the study (as well as
> specification of the frailty term. So I'm not in a position to comment on
> feasibility for this situation.
>
> >
> > Grateful for any suggestions
> >
> > Best wishes
> >
> > Stuart Patterson, Royal Veterinary College, University of London
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Mon Nov 21 16:57:18 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 21 Nov 2016 07:57:18 -0800
Subject: [R] explicit coercion warnings as.numeric Versus as.logical
In-Reply-To: <CAMLd9E406zPjg1Hge8LkScMhxTPJ1HqSo098Ux729uU9CF9=_w@mail.gmail.com>
References: <CAMLd9E406zPjg1Hge8LkScMhxTPJ1HqSo098Ux729uU9CF9=_w@mail.gmail.com>
Message-ID: <CAGxFJbTpN6Kuws9WXKJrTm_uPt9pjwaayfpo_x_pGmtsG-571g@mail.gmail.com>

Not an answer, but note that your vectors are all first (silently)
coerced to character, as vectors must be all of one type.

I would hazard a guess that the answer is: it's simply an arbitrary
inconsistency (different folks wrote the functions at different
times). Note that AFAICS, the difference has no effect on the behavior
of the two functions, i.e. the behavior is consistent, which is what
counts. However, I of course defer to real experts.

-- Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Nov 21, 2016 at 1:52 AM, Ramnik Bansal <ramnik.bansal at gmail.com> wrote:
> Hi,
>
> I am trying to understand under which specific conditions does explicit
> coercion produce warnings.
>
>> as.numeric(c(1, F, "b"))
> [1]  1 NA NA
> Warning message:
> NAs introduced by coercion
>
>> as.logical(c(1, F, "b"))
> [1]    NA FALSE    NA
>
>
> In above examples, as.numeric produces warning but as.logical does not.
> What is the reason behind this different behaviour. Ideally as.logical
> should also have produced the warning message like as.numeric.
>
> Thanks in advance.
> Ramnik
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From spatterson at RVC.AC.UK  Mon Nov 21 15:47:48 2016
From: spatterson at RVC.AC.UK (Patterson, Stuart)
Date: Mon, 21 Nov 2016 14:47:48 +0000
Subject: [R] Using contrast with cox models
Message-ID: <VI1PR0202MB2781688AF08D2392C6BA610A86B50@VI1PR0202MB2781.eurprd02.prod.outlook.com>

Hi,

I was wondering if I could get some advice on the following question please:

I have a time-dependent cox model with three variables, each of which interacts with the other two. So my final model is:

fit12<-coxph(formula = Surv(data$TimeIn, data$Timeout, data$Status) ~ data$Year+data$Life_Stg+data$prev.tb +data$prev.tbdata$Life_Stg + data$Yeardata$Life_Stg + data$Year*data$prev.tb + frailty(data$Natal_Group), data = data)

For my variables, there are 3 categories of year, three of year, and prev.tb is a binary variable. Because of the interactions, when I present the results, I want to present the Hazard ratio, 95% CI, and p value for each combination of the three variables. How do I get R to give me these values please?

I think that the contrast function does this for other models but does not work for coxph?



Many thanks

Stuart Patterson

Royal Veterinary College, University of London


[RVC Logo - link to RVC Website]<http://www.rvc.ac.uk>    [Twitter icon - link to RVC (Official) Twitter] <http://twitter.com/RoyalVetCollege>     [Facebook icon - link to RVC (Official) Facebook] <http://www.facebook.com/theRVC>     [YouTube icon - link to RVC YouTube] <http://www.youtube.com/user/RoyalVetsLondon?feature=mhee>     [Pinterest icon - link to RVC Pinterest] <http://pinterest.com/royalvetcollege/>     [Instagram icon - link to RVC Instagram] <http://instagram.com/royalvetcollege>

This message, together with any attachments, is intended for the stated addressee(s) only and may contain privileged or confidential information. Any views or opinions presented are solely those of the author and do not necessarily represent those of the Royal Veterinary College (RVC). If you are not the intended recipient, please notify the sender and be advised that you have received this message in error and that any use, dissemination, forwarding, printing, or copying is strictly prohibited. Unless stated expressly in this email, this email does not create, form part of, or vary any contractual or unilateral obligation. Email communication cannot be guaranteed to be secure or error free as information could be intercepted, corrupted, amended, lost, destroyed, incomplete or contain viruses. Therefore, we do not accept liability for any such matters or their consequences. Communication with us by email will be taken as acceptance of the risks inherent in doing so.

	[[alternative HTML version deleted]]


From macqueen1 at llnl.gov  Mon Nov 21 17:38:45 2016
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Mon, 21 Nov 2016 16:38:45 +0000
Subject: [R] Help with R
In-Reply-To: <CAG23Y3sHOJhNW_mzKpsvx-cDdQ6WSWkThULBGOcAEAVQR2S6aA@mail.gmail.com>
References: <CAG23Y3sHOJhNW_mzKpsvx-cDdQ6WSWkThULBGOcAEAVQR2S6aA@mail.gmail.com>
Message-ID: <D4585CB8.18FBB4%macqueen1@llnl.gov>

To go further with your teacher's code, I would start by finding out what
kind of object "map" is, in R terms. For example, can your teacher give
you the output from the commands:
  class(map)
  str(map)
?

Without that information, I don't think anyone here can give you effective
help.

Also, as one if the others mentioned, R-sig-geo would be a better place to
ask, when you come back with that information. (And mention up front that
this is not homework, to avoid getting that advice again. Folks will
appreciate it.)

As a general rule, loading images, such as a png file, may or may not
result in an object with correct map coordinates; it depends on the
incoming file.

You could also go back to the CRAN webpage, click on "Task Views" on the
left, then click on "Spatial". There is a LOT of material there. The first
two packages to look at, from my perspective, would be sp and rgdal
(especially if you're going to be doing a lot of serious spatial work in
R) but there are plenty of others that could be relevant for your
particular needs. See also the raster package if your starting point is
really going to be an image such as a png file.

-Don

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 11/19/16, 10:32 AM, "R-help on behalf of Jackson Hooten"
<r-help-bounces at r-project.org on behalf of jlhooten at eckerd.edu> wrote:

>Hello R Help,
>
>I am fairly new to R and have been scouring the internet trying to find
>useful tips, tricks and advice with how to solve my problems with R. I
>finally was able to find you guys, and am now asking for your help. I have
>a number of different questions regarding R. My most pressing question is,
>how does one get a map of a very specific area into R? I've been following
>a tutorial that one of my teachers sent me and been following his code.
>However, every time I input this code I get a warning message:
>
>My teacher's code:
>> plot(map, col="dimgray", border=F, add=T)
>
>The Error code I get every time I input the above code:
>> Error in plot.window(...) : need finite 'ylim' values
>In addition: Warning messages:
>1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion
>2: In min(x) : no non-missing arguments to min; returning Inf
>3: In max(x) : no non-missing arguments to max; returning -Inf
>4: In plot.window(...) : "border" is not a graphical parameter
>5: In plot.window(...) : "add" is not a graphical parameter
>
>So, what I think I figured out was that I didn't possess the "map" that my
>teacher had in his R program. So I went to try and figure out how to get
>my
>own map into my R session. I found this line of code to try and input a
>good Google Earth map I found that I need for my study. But, alas, this
>code didn't work either and I got more Error codes.
>
>The code I used:
>> img <-readPNG(system.file("img", "Seal_Island_Map.png", package="png"))
>
>The Error code I get every time I input the above code:
>> Error in readPNG(system.file("img", "Seal_Island_Map.png",
>package="png")) : unable to open
>
>
>For the life of me, I have not been able to find anything anywhere that
>can
>help me solve these problems. What am I doing wrong? Can you all help?
>Thank you and I hope to talk to you all soon.
>
>Sincerely,
>Jackson
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From Jingyi.Huang at warwick.ac.uk  Mon Nov 21 18:47:40 2016
From: Jingyi.Huang at warwick.ac.uk (Huang, Cassie)
Date: Mon, 21 Nov 2016 17:47:40 +0000
Subject: [R] Question on 'predict' in 'markovchain' package
Message-ID: <DB5PR01MB1864A992EED1A390493B152AD5B50@DB5PR01MB1864.eurprd01.prod.exchangelabs.com>

Dear Sir/Madam,


I want to report a problem of 'predict' function in the 'markovchain' package and I will use to examples to explain the problem.


Problem:
-It only follows the path with transition probability greater or equal than 0.5.

Here are two examples.
Example (1)
I created an MC with the transition matrix  (by rows)  is defined as follows:

         0     1
0     0.5   0.5
1     0.6   0.4
-I applied 'predict' on the MC above starting at 0 and 1 respectively and get the transition counts as follows:
(start at 0)
     0  1
0 34  32
1 33  0

(start at 1)
      0  1
0 35 32
1 32  0
In Example (1), we observe that 1-1 has the probability less than 0.5 and it never happens in prediction.

Example (2)
I created an MC with the transition matrix  (by rows)  is defined as follows:

         0     1
0     0.53   0.47
1     0.47  0.53

I get all 0's after applied 'prediction' with starting state 0 while I get all 1's with starting state 1.
This may because only 0-0 and 1-1  have >0.5 probability in the MC so they are the only two path will appear in the prediction.


Code as follows.

statesNames=c("0","1")
mcA<-new("markovchain", transitionMatrix=matrix( c(0.5,0.5,0.6,0.4),byrow=TRUE,
                                                nrow=2, dimnames=list(statesNames,statesNames)))
mcA
mcApred0<-predict(mcA, newdata="0",n.ahead=100)
mcApred1<-predict(mcA, newdata="1",n.ahead=100)
createSequenceMatrix(stringchar = mcApred0)
createSequenceMatrix(stringchar = mcApred1)


statesNames=c("0","1")
mcB<-new("markovchain", transitionMatrix=matrix( c(0.53,0.47,0.47,0.53),byrow=TRUE,
                                                nrow=2, dimnames=list(statesNames,statesNames)))
mcB
predict(mcB, newdata="0",n.ahead=100)
predict(mcB, newdata="1",n.ahead=100)


Best wishes,

Cassie

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Mon Nov 21 20:59:45 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 21 Nov 2016 11:59:45 -0800
Subject: [R] Presenting Hazard ratios for interacting variables in a Cox
	model
In-Reply-To: <CAJvq0dBWWOE6zWSn7POrhoqsP-j=JN_xdJYNPJrF4FrJNV21QA@mail.gmail.com>
References: <CAJvq0dBT+eeZ39g2RSk7KW4+Mdi4_Le-Kc2X0gM4GgZp=6oMzA@mail.gmail.com>
	<17239E26-76DC-4057-B573-C30288802062@comcast.net>
	<CAJvq0dBWWOE6zWSn7POrhoqsP-j=JN_xdJYNPJrF4FrJNV21QA@mail.gmail.com>
Message-ID: <A09216C2-9215-46D5-9606-874389C7D6E1@comcast.net>


> On Nov 21, 2016, at 4:21 AM, Stuart Patterson <stuartjpatterson at googlemail.com> wrote:
> 
> Dear David, 
> Thank you for your reply. Your suggestions on how better to write the command are very useful, and I can see how the simplification would help. I hadn't realised that the lower order variables would be included if I simply wrote in teh interaction terms. Thank you
> 
> The table below is how I feel that I ought to present my results. If anyone has any suggestions on how to obtain these values I would be very grateful!

You should be able to fill in the first empty column using the results of coef(model) and 

expand.grid( Year=levels(Year), Prev_TB=levels(Prev_TB), Age=levels(Age) )

 Calculating the CI's by hand would require knowing the covariance matrix and you should be able to get the needed components from vcov(model). I wondered if using the confint function would be a more economical approach, however, when I look at the code of confint.default, it does not appear that it would handle contrasts involving interactions terms in a manner that I would think was totally correct. It appears to ignore the off-diagonal elements of the vcov matrix. Perhaps using the confint function in the multcomp package would yield more correct estimates. It's documented as "averaging over the interactions", however the code  appears to use only the diagonal elements as well.

I think the "right way" is outlined by Dalgaard and Laviolette in a recent Rhelp thread: 

https://stat.ethz.ch/pipermail/r-help/2016-August/440879.html

You would set up the D matrix to represent levels in the grid of values to match the contrasts being considered. I did find a phia package that might automate this process, but I don't have experience with it.

Your table is probably only readable by me. The rest of the Rhelp world got a single column because you posting in HTML. Do learn the post to Rhelp in plain text.

I dropped it into OpenOffice.org, saved as csv/txt, and read it into R:

> read.csv("~/Documents/Untitled 1.csv")
    Variable        X          X.1 HRs ci.95. p.Value
1       Year Prev. TB          Age                   
2  2002-2005       No  < 24 months                   
3                     24-48 months                   
4                       >48 months                   
5                 Yes  < 24 months                   
6                     24-48 months                   
7                       >48 months                   
8  2006-2010       No  < 24 months                   
9                     24-48 months                   
10                      >48 months                   
11                Yes  < 24 months                   
12                    24-48 months   a      b       c
13                      >48 months                   
14 2011-2015       No  < 24 months                   
15                    24-48 months                   
16                      >48 months                   
17                Yes  < 24 months                   
18                    24-48 months                   
19                      >48 months                   

-- 
David

> 
> On 18 November 2016 at 19:21, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> > On Nov 18, 2016, at 6:56 AM, Stuart Patterson via R-help <r-help at r-project.org> wrote:
> >
> > I have a time-dependent cox model with three variables, each of which
> > interacts with the other two. So my final model is:
> >
> > fit12<-coxph(formula = Surv(data$TimeIn, data$Timeout, data$Status) ~ data$
> > Year+data$Life_Stg+data$prev.tb +data$prev.tb*data$Life_Stg + data$Year*data
> > $Life_Stg + data$Year*data$prev.tb + frailty(data$Natal_Group), data = data)
> 
> It seems fairly likely that you are shooting yourself in the foot by using the `data$variate` inside the formula. It will prevent the regression result from having correctly assembled references to variables. And that will become evident when you try to do any predictions. Try instead:
> 
> fit12<-coxph(formula = Surv( TimeIn,  Timeout,  Status) ~
>             prev.tb * Life_Stg +  Year *Life_Stg +  Year * prev.tb + frailty( Natal_Group),
>             data = data)
> 
> The `*` in a formula automatically includes the lower order individual variates in the estimates. Your model RHS could have also been written (more clearly in my opinion):
> 
>          ~ (prev.tb + Life_Stg +  Year)^2
> 
> ... since R formulas interpret the `(.)^N` operation as "all base effects and interactions up to order N".
> 
> >
> > For my variables, there are 3 categories of year, three of year, and
> > prev.tb is a binary variable. Because of the interactions, when I present
> > the results, I want to present the Hazard ratio, 95% CI, and p value for
> > each combination of the three variables. How do I get R to give me these
> > values please?
> >
> > I think that the contrast function does this for other models but does not
> > work for coxph?
> 
> The usual method would be to use `predict` on a 'newdata' dataframe with all the combinations generated from `expand.grid`. The combination of the reference values of all three variables should yield a 1.0 hazard ratio. But time-dependent model predictions need a complete specification of a sequence of values over the time course of the study (as well as specification of the frailty term. So I'm not in a position to comment on feasibility for this situation.
> 
> >
> > Grateful for any suggestions
> >
> > Best wishes
> >
> > Stuart Patterson, Royal Veterinary College, University of London
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> 

David Winsemius
Alameda, CA, USA


From oluola2011 at yahoo.com  Mon Nov 21 22:26:13 2016
From: oluola2011 at yahoo.com (Olu Ola)
Date: Mon, 21 Nov 2016 21:26:13 +0000 (UTC)
Subject: [R] Combining columns
References: <329582797.1404465.1479763573065.ref@mail.yahoo.com>
Message-ID: <329582797.1404465.1479763573065@mail.yahoo.com>

 Hello,I have the following data
| colA | colB | colC | colD |
| NA | pumpkin | NA | Pumpkin |
| Cassava | NA | NA | Cassava |
| yam | NA | NA | yam |
| NA | Cherry | NA | Cherry |
| NA | NA | Pepper | Pepper |
| NA | NA | Mango | Mango |
| maize | NA | NA | maize |


All I want to do is to combine the first three columns in order to obtain the fourth column.
A way forward?will be greatly appreciated.

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Mon Nov 21 22:35:44 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 22 Nov 2016 08:35:44 +1100
Subject: [R] explicit coercion warnings as.numeric Versus as.logical
In-Reply-To: <CAGxFJbTpN6Kuws9WXKJrTm_uPt9pjwaayfpo_x_pGmtsG-571g@mail.gmail.com>
References: <CAMLd9E406zPjg1Hge8LkScMhxTPJ1HqSo098Ux729uU9CF9=_w@mail.gmail.com>
	<CAGxFJbTpN6Kuws9WXKJrTm_uPt9pjwaayfpo_x_pGmtsG-571g@mail.gmail.com>
Message-ID: <CA+8X3fUQ1MZUOkaSwpwd9dcGMK5-g66-wqU409aKEO=3RUcX0g@mail.gmail.com>

Hi Ramnik,
Bert's answer is correct, and an easy way to see why is to look at:

c(1,F,"b")
[1] "1"     "FALSE" "b"

The reason that "F" is translated to "FALSE" is that is its default
value when R is started. If you change that value:

F<-"foo"
c(1,F,"b")
[1] "1"   "foo" "b"
as.logical(c(1,F,"b"))
[1] NA NA NA

To find out why as.numeric warns you and as.logical doesn't, we will
have to await the response of someone who knows. I suspect that
as.numeric employs a fairly sophisticated analysis of a string to see
if it is a number while as.logical just shoves the value (x) into a
conditional like:

x != 0

Jim


On Tue, Nov 22, 2016 at 2:57 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> Not an answer, but note that your vectors are all first (silently)
> coerced to character, as vectors must be all of one type.
>
> I would hazard a guess that the answer is: it's simply an arbitrary
> inconsistency (different folks wrote the functions at different
> times). Note that AFAICS, the difference has no effect on the behavior
> of the two functions, i.e. the behavior is consistent, which is what
> counts. However, I of course defer to real experts.
>
> -- Bert
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Mon, Nov 21, 2016 at 1:52 AM, Ramnik Bansal <ramnik.bansal at gmail.com> wrote:
>> Hi,
>>
>> I am trying to understand under which specific conditions does explicit
>> coercion produce warnings.
>>
>>> as.numeric(c(1, F, "b"))
>> [1]  1 NA NA
>> Warning message:
>> NAs introduced by coercion
>>
>>> as.logical(c(1, F, "b"))
>> [1]    NA FALSE    NA
>>
>>
>> In above examples, as.numeric produces warning but as.logical does not.
>> What is the reason behind this different behaviour. Ideally as.logical
>> should also have produced the warning message like as.numeric.
>>
>> Thanks in advance.
>> Ramnik
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ruipbarradas at sapo.pt  Mon Nov 21 22:47:12 2016
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Mon, 21 Nov 2016 21:47:12 +0000
Subject: [R] Combining columns
In-Reply-To: <329582797.1404465.1479763573065@mail.yahoo.com>
References: <329582797.1404465.1479763573065.ref@mail.yahoo.com>
	<329582797.1404465.1479763573065@mail.yahoo.com>
Message-ID: <58336B60.7060708@sapo.pt>

Hello,

Try the following.

dat <- read.table(text = "
| colA | colB | colC | colD |
| NA | pumpkin | NA | Pumpkin |
| Cassava | NA | NA | Cassava |
| yam | NA | NA | yam |
| NA | Cherry | NA | Cherry |
| NA | NA | Pepper | Pepper |
| NA | NA | Mango | Mango |
| maize | NA | NA | maize |
", header = TRUE, sep = "|", stringsAsFactors = FALSE, na.strings = " NA ")

dat <- dat[, -c(1, 6)]

dat1 <- dat[, -4]

res <- apply(dat1, 1, function(x) x[!is.na(x)])
res

And please post your data examples using ?dput, it's not the first time 
you post to R-Help.

Hope this helps,

Rui Barradas


Em 21-11-2016 21:26, Olu Ola via R-help escreveu:
>   Hello,I have the following data
> | colA | colB | colC | colD |
> | NA | pumpkin | NA | Pumpkin |
> | Cassava | NA | NA | Cassava |
> | yam | NA | NA | yam |
> | NA | Cherry | NA | Cherry |
> | NA | NA | Pepper | Pepper |
> | NA | NA | Mango | Mango |
> | maize | NA | NA | maize |
>
>
> All I want to do is to combine the first three columns in order to obtain the fourth column.
> A way forward will be greatly appreciated.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From drjimlemon at gmail.com  Mon Nov 21 22:54:36 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 22 Nov 2016 08:54:36 +1100
Subject: [R] Combining columns
In-Reply-To: <329582797.1404465.1479763573065@mail.yahoo.com>
References: <329582797.1404465.1479763573065.ref@mail.yahoo.com>
	<329582797.1404465.1479763573065@mail.yahoo.com>
Message-ID: <CA+8X3fWoNNpedfRYi4w6YnedKSpKa4i7QQLAgxjZZ-2DFRnvXw@mail.gmail.com>

Hi Olu,
If you always have only one non-NA value in the first three columns:

veg_df<-data.frame(col1=c(NA,"cassava","yam",NA,NA,NA,"maize"),
 col2=c("pumpkin",NA,NA,"cherry",NA,NA,NA),
 col3=c(NA,NA,NA,NA,"pepper","mango",NA))

veg_df$col4<-apply(as.matrix(veg_df),1,function(x) x[!is.na(x)])

Jim


On Tue, Nov 22, 2016 at 8:26 AM, Olu Ola via R-help
<r-help at r-project.org> wrote:
>  Hello,I have the following data
> | colA | colB | colC | colD |
> | NA | pumpkin | NA | Pumpkin |
> | Cassava | NA | NA | Cassava |
> | yam | NA | NA | yam |
> | NA | Cherry | NA | Cherry |
> | NA | NA | Pepper | Pepper |
> | NA | NA | Mango | Mango |
> | maize | NA | NA | maize |
>
>
> All I want to do is to combine the first three columns in order to obtain the fourth column.
> A way forward will be greatly appreciated.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From macqueen1 at llnl.gov  Mon Nov 21 22:57:00 2016
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Mon, 21 Nov 2016 21:57:00 +0000
Subject: [R] Combining columns
In-Reply-To: <329582797.1404465.1479763573065@mail.yahoo.com>
References: <329582797.1404465.1479763573065.ref@mail.yahoo.com>
	<329582797.1404465.1479763573065@mail.yahoo.com>
Message-ID: <D458ACB9.18FCBC%macqueen1@llnl.gov>

Something along the lines of

tmpfun <- function(chvec) chvec[!is.na(chvec)]

apply( mydata, 1, tmpfun)

Assuming every row has only one non-NA entry.



-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 11/21/16, 1:26 PM, "R-help on behalf of Olu Ola via R-help"
<r-help-bounces at r-project.org on behalf of r-help at r-project.org> wrote:

> Hello,I have the following data
>| colA | colB | colC | colD |
>| NA | pumpkin | NA | Pumpkin |
>| Cassava | NA | NA | Cassava |
>| yam | NA | NA | yam |
>| NA | Cherry | NA | Cherry |
>| NA | NA | Pepper | Pepper |
>| NA | NA | Mango | Mango |
>| maize | NA | NA | maize |
>
>
>All I want to do is to combine the first three columns in order to obtain
>the fourth column.
>A way forward will be greatly appreciated.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From oluola2011 at yahoo.com  Mon Nov 21 23:58:36 2016
From: oluola2011 at yahoo.com (Olu Ola)
Date: Mon, 21 Nov 2016 22:58:36 +0000 (UTC)
Subject: [R] Combining columns
In-Reply-To: <CA+8X3fWoNNpedfRYi4w6YnedKSpKa4i7QQLAgxjZZ-2DFRnvXw@mail.gmail.com>
References: <329582797.1404465.1479763573065.ref@mail.yahoo.com>
	<329582797.1404465.1479763573065@mail.yahoo.com>
	<CA+8X3fWoNNpedfRYi4w6YnedKSpKa4i7QQLAgxjZZ-2DFRnvXw@mail.gmail.com>
Message-ID: <6148094.1471600.1479769116084@mail.yahoo.com>

Thank you Jim and Don. 

    On Monday, November 21, 2016 4:54 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
 

 Hi Olu,
If you always have only one non-NA value in the first three columns:

veg_df<-data.frame(col1=c(NA,"cassava","yam",NA,NA,NA,"maize"),
 col2=c("pumpkin",NA,NA,"cherry",NA,NA,NA),
 col3=c(NA,NA,NA,NA,"pepper","mango",NA))

veg_df$col4<-apply(as.matrix(veg_df),1,function(x) x[!is.na(x)])

Jim


On Tue, Nov 22, 2016 at 8:26 AM, Olu Ola via R-help
<r-help at r-project.org> wrote:
>? Hello,I have the following data
> | colA | colB | colC | colD |
> | NA | pumpkin | NA | Pumpkin |
> | Cassava | NA | NA | Cassava |
> | yam | NA | NA | yam |
> | NA | Cherry | NA | Cherry |
> | NA | NA | Pepper | Pepper |
> | NA | NA | Mango | Mango |
> | maize | NA | NA | maize |
>
>
> All I want to do is to combine the first three columns in order to obtain the fourth column.
> A way forward will be greatly appreciated.
>
>? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


   
	[[alternative HTML version deleted]]


From acefix at rocketmail.com  Mon Nov 21 21:05:15 2016
From: acefix at rocketmail.com (Fix Ace)
Date: Mon, 21 Nov 2016 20:05:15 +0000 (UTC)
Subject: [R] how to use vector of values to change row order of a heatmap
In-Reply-To: <1596082082.3041347.1474578140499@mail.yahoo.com>
References: <1596082082.3041347.1474578140499.ref@mail.yahoo.com>
	<1596082082.3041347.1474578140499@mail.yahoo.com>
Message-ID: <2066684278.1325071.1479758715149@mail.yahoo.com>

Hello, there,
R document for heatmap says that Rowv could be a vector of values to specify the row order. However, I couldn't figure out how to apply it. A simple example here:> b=as.data.frame(matrix(c(3,4,5,8,9,10,13,14,15,27,19,20),3,4))
> b
? V1 V2 V3 V4
1? 3? 8 13 27
2? 4? 9 14 19
3? 5 10 15 20
> row.names(b)=c("a","b","c")
> b
? V1 V2 V3 V4
a? 3? 8 13 27
b? 4? 9 14 19
c? 5 10 15 20
> heatmap(as.matrix(b))
What I got:
 

Now I would like to put row "a" to the top row, how do I do that?I tried provide a vector of values (all the possible combination of 1,2,3) to Rowv,? "a" is always stay at the bottom
Any input would be very helpful!
Ace
  

   
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image.png
Type: image/png
Size: 10962 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20161121/64a973c3/attachment.png>

From acefix at rocketmail.com  Mon Nov 21 21:14:19 2016
From: acefix at rocketmail.com (Fix Ace)
Date: Mon, 21 Nov 2016 20:14:19 +0000 (UTC)
Subject: [R] how to use vector of values to change row order of a heatmap
References: <1301534071.1349522.1479759259914.ref@mail.yahoo.com>
Message-ID: <1301534071.1349522.1479759259914@mail.yahoo.com>

Hello, there,
R document for heatmap says that Rowv could be a vector of values to specify the row order. However, I couldn't figure out how to apply it. A simple example here:> b=as.data.frame(matrix(c(3,4,5,8,9,10,13,14,15,27,19,20),3,4))
> b
? V1 V2 V3 V4
1? 3? 8 13 27
2? 4? 9 14 19
3? 5 10 15 20
> row.names(b)=c("a","b","c")
> b
? V1 V2 V3 V4
a? 3? 8 13 27
b? 4? 9 14 19
c? 5 10 15 20
> heatmap(as.matrix(b))
What I got: "a" stays at the bottom of the heatmap.
 Now I would like to put row "a" to the top row, how do I do that?I tried provide a vector of values (all the possible combination of 1,2,3) to Rowv,? "a" is always stay at the bottom
Any input would be very helpful!
Thanks.
Ace

	[[alternative HTML version deleted]]


From dkatz at tibco.com  Tue Nov 22 00:35:25 2016
From: dkatz at tibco.com (David Katz)
Date: Mon, 21 Nov 2016 15:35:25 -0800
Subject: [R] gbm question
Message-ID: <CAA+Ab90BZHpH0nm6MQhUGs0jHND0Bz7D6YgUosFpSdYMQGMxxA@mail.gmail.com>

R-Help,

Please help me understand why these models and predictions are different:


library(gbm)

set.seed(32321)
 N <- 1000
     X1 <- runif(N)
     X2 <- 2*runif(N)
     X3 <- ordered(sample(letters[1:4],N,replace=TRUE),levels=letters[4:1])
     X4 <- factor(sample(letters[1:6],N,replace=TRUE))
     X5 <- factor(sample(letters[1:3],N,replace=TRUE))
     X6 <- 3*runif(N)
     mu <- c(-1,0,1,2)[as.numeric(X3)]

     SNR <- 10 # signal-to-noise ratio
     Y <- X1**1.5 + 2 * (X2**.5) + mu
     sigma <- sqrt(var(Y)/SNR)
     Y <- Y + rnorm(N,0,sigma)

     # introduce some missing values
     X1[sample(1:N,size=500)] <- NA
     X4[sample(1:N,size=300)] <- NA

     data <- data.frame(Y=Y,X1=X1,X2=X2,X3=X3,X4=X4,X5=X5,X6=X6)

set.seed(32321)
gbm.formula <-
     gbm(Y~X1+X2+X3+X4+X5+X6,         # formula
         data=data,                   # dataset
         distribution="gaussian",     # see the help for other choices
         n.trees=1000,                # number of trees
         shrinkage=0.05,              # shrinkage or learning rate,
                                      # 0.001 to 0.1 usually work
         interaction.depth=3,         # 1: additive model, 2: two-way
interactions, etc.
         bag.fraction = 0.5,          # subsampling fraction, 0.5 is
probably best
         train.fraction = 1,        # fraction of data for training,
                                      # first train.fraction*N used for
training
         n.minobsinnode = 10,         # minimum total weight needed in each
node
         keep.data=TRUE,              # keep a copy of the dataset with the
object
         verbose=FALSE)               # don't print out progress



set.seed(32321)
gbm.Fit <-
     gbm.fit(x=data[,-1],y=Y,
         distribution="gaussian",     # see the help for other choices
         n.trees=1000,                # number of trees
         shrinkage=0.05,              # shrinkage or learning rate,
                                      # 0.001 to 0.1 usually work
         interaction.depth=3,         # 1: additive model, 2: two-way
interactions, etc.
         bag.fraction = 0.5,          # subsampling fraction, 0.5 is
probably best
         nTrain=length(Y),
                                      # first train.fraction*N used for
training
         n.minobsinnode = 10,         # minimum total weight needed in each
node
         keep.data=TRUE,              # keep a copy of the dataset with the
object
         verbose=FALSE)               # don't print out progress





all.equal(predict(gbm.formula,n.trees=100), predict(gbm.Fit,n.trees=100))

> [1] "Mean relative difference: 0.3585409"

#all.equal(gbm.formula,gbm.Fit) no!

(Based on the package examples)

Thanks

*David Katz*| IAG, TIBCO Spotfire

	[[alternative HTML version deleted]]


From hannah.hlx at gmail.com  Tue Nov 22 06:09:58 2016
From: hannah.hlx at gmail.com (li li)
Date: Tue, 22 Nov 2016 00:09:58 -0500
Subject: [R] ancova for dependent response
Message-ID: <CAHLnndbw3o4PrKsLh6=QgZ6zw3Ex3+5-zCoJx=32MFSjfbUbMA@mail.gmail.com>

Hi all,
  Is there an R function which can handles dependent response in Analysis
of covariance model. The dependence structure is known and is there to
account for it in ANCOVA analysis in R?
  Thanks.
  Hanna

	[[alternative HTML version deleted]]


From suttoncarl at ymail.com  Tue Nov 22 06:38:49 2016
From: suttoncarl at ymail.com (Carl Sutton)
Date: Tue, 22 Nov 2016 05:38:49 +0000 (UTC)
Subject: [R] using conditionals to select rows in data.table
References: <1120250713.31344.1479793129690.ref@mail.yahoo.com>
Message-ID: <1120250713.31344.1479793129690@mail.yahoo.com>

Hopefully the attached is text and not html.? I have not found a text option in firefox.

I have also been informed that a windows cut and paste is not truly text, which is the reason for the attachment from notepad++

If there is any way I can improve the question, please inform of the problem and I will do what I can to correct it.?Carl Sutton 

From jdnewmil at dcn.davis.ca.us  Tue Nov 22 07:09:40 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 21 Nov 2016 22:09:40 -0800
Subject: [R] using conditionals to select rows in data.table
In-Reply-To: <1120250713.31344.1479793129690@mail.yahoo.com>
References: <1120250713.31344.1479793129690.ref@mail.yahoo.com>
	<1120250713.31344.1479793129690@mail.yahoo.com>
Message-ID: <F48460FF-7EFE-4C52-A881-676D1CC3C4DD@dcn.davis.ca.us>

Cut-and-paste is whatever you cut.. cut from a text editor and you will get text. Notepad++ is one such program.

Recommendation: set your mail program to text and put your question on the body of the email. Attachments sometimes get through, but they really are not the preferred way to communicate on this list. You seem to have figured out how to make your email plain text, but the attachment is gone... probably because of the file extension, but it would have been a hassle even if it had got through so no great loss.
-- 
Sent from my phone. Please excuse my brevity.

On November 21, 2016 9:38:49 PM PST, Carl Sutton via R-help <r-help at r-project.org> wrote:
>Hopefully the attached is text and not html.? I have not found a text
>option in firefox.
>
>I have also been informed that a windows cut and paste is not truly
>text, which is the reason for the attachment from notepad++
>
>If there is any way I can improve the question, please inform of the
>problem and I will do what I can to correct it.?Carl Sutton 
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Tue Nov 22 07:12:41 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 21 Nov 2016 22:12:41 -0800
Subject: [R] ancova for dependent response
In-Reply-To: <CAHLnndbw3o4PrKsLh6=QgZ6zw3Ex3+5-zCoJx=32MFSjfbUbMA@mail.gmail.com>
References: <CAHLnndbw3o4PrKsLh6=QgZ6zw3Ex3+5-zCoJx=32MFSjfbUbMA@mail.gmail.com>
Message-ID: <3DA380EF-2A2F-4616-B087-3C26C2F33438@dcn.davis.ca.us>

Yes. https://lmgtfy.com/?q=R+ancova
-- 
Sent from my phone. Please excuse my brevity.

On November 21, 2016 9:09:58 PM PST, li li <hannah.hlx at gmail.com> wrote:
>Hi all,
>Is there an R function which can handles dependent response in Analysis
>of covariance model. The dependence structure is known and is there to
>account for it in ANCOVA analysis in R?
>  Thanks.
>  Hanna
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From maechler at stat.math.ethz.ch  Tue Nov 22 09:59:28 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 22 Nov 2016 09:59:28 +0100
Subject: [R] using conditionals to select rows in data.table
In-Reply-To: <1120250713.31344.1479793129690@mail.yahoo.com>
References: <1120250713.31344.1479793129690.ref@mail.yahoo.com>
	<1120250713.31344.1479793129690@mail.yahoo.com>
Message-ID: <22580.2288.832111.606439@stat.math.ethz.ch>

Dear Carl,

this came through fine, as text only
... but then I did not see any question anymore.

Best regards,
Martin Maechler
(R core and mailing list "operator")

>>>>> Carl Sutton via R-help <r-help at r-project.org>
>>>>>     on Tue, 22 Nov 2016 05:38:49 +0000 writes:

    > Hopefully the attached is text and not html.? I have not
    > found a text option in firefox.  I have also been informed
    > that a windows cut and paste is not truly text, which is
    > the reason for the attachment from notepad++

    > If there is any way I can improve the question, please
    > inform of the problem and I will do what I can to correct
    > it.?Carl Sutton
    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    > more, see https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide
    > http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Tue Nov 22 10:46:13 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 22 Nov 2016 09:46:13 +0000
Subject: [R] Melt and compute Max, Mean, Min
In-Reply-To: <CAMLwc7OQwCeY6b-QJODCY5QLujEL18Zjh7DXgo86WFtWBXYhYg@mail.gmail.com>
References: <CAMLwc7PNUzOtLFv+yemQZEXsf535nsuzMbf0ShZZ_i3BqsRrHw@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5046931@SRVEXCHMBX.precheza.cz>
	<CAMLwc7M8e8_2suuZOVGDKTv=bVMqGiLsPRSGZNXFToMm1WLq8A@mail.gmail.com>
	<CAMLwc7OdFJ2bj3_2TqZpS0HNZrdVc_0=6ud7jv8B1HKN43xr2Q@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5046CB9@SRVEXCHMBX.precheza.cz>
	<CAMLwc7OQwCeY6b-QJODCY5QLujEL18Zjh7DXgo86WFtWBXYhYg@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5046F65@SRVEXCHMBX.precheza.cz>

Hi

Well, thean you could try as I sugested.

.mean <- apply(temp[,-(1:3)],1, mean, na.rm=T)
.max <- apply(temp[,-(1:3)],1, max, na.rm=T)
.min <- apply(temp[,-(1:3)],1, min, na.rm=T)
temp2 <- data.frame(temp[,1:3], maxim = .max, minim = .min, aver = .mean)

You should construct a cycle, read the year file to temp, add above lines, add year and rbind temp2 with previous result(s).

You could do it also manually but if I remember correctly you have plenty of files.

Cheers
Petr

From: Miluji Sb [mailto:milujisb at gmail.com]
Sent: Monday, November 21, 2016 6:15 PM
To: PIKAL Petr <petr.pikal at precheza.cz>
Subject: Re: [R] Melt and compute Max, Mean, Min

Hello Petr,

Thank you so much for your reply. Apologies for the HTML posting, there's something wrong with my email editor. My goal is to compute the maximum, minimum, and mean for each observation by year and then merge them, so they look like the following:

lat | lon | year | max | min | mean

Thanks again.

Sincerely,

Milu

On Mon, Nov 21, 2016 at 6:57 AM, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
Hi

see in line

From: Miluji Sb [mailto:milujisb at gmail.com<mailto:milujisb at gmail.com>]
Sent: Friday, November 18, 2016 3:57 PM
To: PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>>
Cc: r-help mailing list <r-help at r-project.org<mailto:r-help at r-project.org>>
Subject: Re: [R] Melt and compute Max, Mean, Min

If I do:

as.data.frame(apply(df[,-(1:3)],1, mean, na.rm=T))

is it possible to sequentially name the variables as "mean_1960", "max_1960". "min_1960", "mean_1961", "max_1961". "min_1961", ...?

But here you have only mean. How do you want to add min or max?


On Fri, Nov 18, 2016 at 3:10 PM, Miluji Sb <milujisb at gmail.com<mailto:milujisb at gmail.com>> wrote:
Dear Petr,

Thank you for the code, apologies though as I copied the wrong data, This is precipitation data and not temperature.

For the loop, could I do something like this?

filelist <- list.files(pattern=".csv")


Not exactly.

In this case I would use for cycle. It is quite easy to do something like:

for( i in 1:length(filelist)) {

temp<-read.csv(filelist[i]) #you need to read your file in R first

.mean <- apply(temp[,-(1:3)],1, mean, na.rm=T)
.max <- apply(temp[,-(1:3)],1, max, na.rm=T)
.min <- apply(temp[,-(1:3)],1, min, na.rm=T)

# now you can concatenate those results as you wish, name them or anything. It is difficult to suggest any direct code as you did not disclose what do you want to do with summaries further.

}

And BTW, please, do not post in HTML.



myDTs <- lapply(filelist, function(.file) {

apply(temp[,-(1:3)],1, mean, na.rm=T)

}

Thanks again!

Sincerely,

Milu


On Fri, Nov 18, 2016 at 2:46 PM, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
Hi

I am not completely sure what you want to do but

> apply(temp[,-(1:3)],1, mean, na.rm=T)
       1        2        3        4        5
     NaN      NaN 2.159516 1.519914 1.514007
> apply(temp[,-(1:3)],1, max, na.rm=T)
       1        2        3        4        5
    -Inf     -Inf 57.36528 39.45348 45.23904
Warning messages:
1: In FUN(newX[, i], ...) :
  no non-missing arguments to max; returning -Inf
2: In FUN(newX[, i], ...) :
  no non-missing arguments to max; returning -Inf
> apply(temp[,-(1:3)],1, min, na.rm=T)
  1   2   3   4   5
Inf Inf   0   0   0

gives you mentioned summary for each row. If you have duplicate rows you shall first aggregate them. However, it seems to me that your data are not correct. It is quite strange that for given lat/lon you have one day value 23 and the next day 0.

temp[1:5, 1:10]
  ISO3 lon lat day_1 day_2    day_3 day_4 day_5 day_6    day_7
1  CHL -69 -55    NA    NA       NA    NA    NA    NA       NA
2  CHL -68 -55    NA    NA       NA    NA    NA    NA       NA
3  CHL -72 -54     0     0  0.00000     0     0     0  2.83824
4 <NA> -71 -54     0     0 23.37984     0     0     0 11.80116
5  CHL -70 -54     0     0  0.00000     0     0     0  1.24956

If you want to process all your files you can do it in cycle. The function

list.files()

can be handy for that task.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of Miluji Sb
> Sent: Friday, November 18, 2016 1:49 PM
> To: r-help mailing list <r-help at r-project.org<mailto:r-help at r-project.org>>
> Subject: [R] Melt and compute Max, Mean, Min
>
> Dear all,
>
> I have 51 years of data (1960 - 2010) in csv format, where each file represents
> one year of data. Below is what each file looks like.
>
> These are temperature data by coordinates, my goal is to to compute max,
> min, and mean by year for each of the coordinates and construct a panel
> dataset. Any help will be appreciated, thank you!
>
> Sincerely,
>
> Milu
>
> temp <- dput(head(df,5))
> structure(list(ISO3 = structure(c(28L, 28L, 28L, NA, 28L), .Label = c("AFG",
> "AGO", "ALB", "ARE", "ARG", "ARM", "AUS", "AUT", "AZE", "BDI", "BEL",
> "BEN", "BFA", "BGD", "BGR", "BHS", "BIH", "BLR", "BLZ", "BOL", "BRA",
> "BRN", "BTN", "BWA", "CAF", "CAN", "CHE", "CHL", "CHN", "CIV", "CMR",
> "COD", "COG", "COL", "CRI", "CUB", "CYP", "CZE", "DEU", "DJI", "DNK",
> "DOM", "DZA", "ECU", "EGY", "ERI", "ESH", "ESP", "EST", "ETH", "FIN", "FJI",
> "FLK", "FRA", "GAB", "GBR", "GEO", "GHA", "GIN", "GNB", "GNQ", "GRC",
> "GRL", "GTM", "GUF", "GUY", "HND", "HRV", "HTI", "HUN", "IDN", "IND",
> "IRL", "IRN", "IRQ", "ISL", "ISR", "ITA", "JAM", "JOR", "JPN", "KAZ", "KEN",
> "KGZ", "KHM", "KIR", "KOR", "KWT", "LAO", "LBN", "LBR", "LBY", "LCA",
> "LKA", "LSO", "LTU", "LUX", "LVA", "MAR", "MDA", "MDG", "MEX", "MKD",
> "MLI", "MMR", "MNE", "MNG", "MOZ", "MRT", "MWI", "MYS", "NAM",
> "NCL", "NER", "NGA", "NIC", "NLD", "NOR", "NPL", "NZL", "OMN", "PAK",
> "PAN", "PER", "PHL", "PNG", "POL", "PRI", "PRK", "PRT", "PRY", "QAT",
> "ROU", "RUS", "RWA", "SAU", "SDN", "SEN", "SJM", "SLB", "SLE", "SLV",
> "SOM", "SRB", "SUR", "SVK", "SVN", "SWE", "SWZ", "SYR", "TCD", "TGO",
> "THA", "TJK", "TKM", "TLS", "TUN", "TUR", "TWN", "TZA", "UGA", "UKR",
> "URY", "USA", "UZB", "VEN", "VNM", "VUT", "YEM", "ZAF", "ZMB", "ZWE"
> ), class = "factor"), lon = c(-69L, -68L, -72L, -71L, -70L),
>     lat = c(-55L, -55L, -54L, -54L, -54L), day_1 = c(NA, NA,
>     0, 0, 0), day_2 = c(NA, NA, 0, 0, 0), day_3 = c(NA, NA, 0,
>     23.37984, 0), day_4 = c(NA, NA, 0, 0, 0), day_5 = c(NA, NA,
>     0, 0, 0), day_6 = c(NA, NA, 0, 0, 0), day_7 = c(NA, NA, 2.83824,
>     11.80116, 1.24956), day_8 = c(NA, NA, 0, 1.68588, 14.69448
>     ), day_9 = c(NA, NA, 0, 0, 1.09296), day_10 = c(NA, NA, 0,
>     0, 0), day_11 = c(NA, NA, 3.78, 3.7422, 0), day_12 = c(NA,
>     NA, 0.54, 0, 0), day_13 = c(NA, NA, 0, 0, 0), day_14 = c(NA,
>     NA, 0, 0, 0.39204), day_15 = c(NA, NA, 0, 0, 11.58732), day_16 = c(NA,
>     NA, 0, 0, 0), day_17 = c(NA, NA, 0, 1.14048, 12.26448), day_18 = c(NA,
>     NA, 0, 1.1934, 7.59024), day_19 = c(NA, NA, 9.74268, 0, 0
>     ), day_20 = c(NA, NA, 0, 0, 0), day_21 = c(NA, NA, 1.96776,
>     0, 0), day_22 = c(NA, NA, 0, 0, 0), day_23 = c(NA, NA, 0,
>     0, 0), day_24 = c(NA, NA, 6.21756, 2.74752, 0), day_25 = c(NA,
>     NA, 0, 0, 3.37932), day_26 = c(NA, NA, 4.8384, 0, 0), day_27 = c(NA,
>     NA, 0, 0, 0), day_28 = c(NA, NA, 0, 0, 0), day_29 = c(NA,
>     NA, 22.37328, 0, 0), day_30 = c(NA, NA, 28.97424, 11.25468,
>     0), day_31 = c(NA, NA, 0, 0, 0), day_32 = c(NA, NA, 0, 0,
>     2.00448), day_33 = c(NA, NA, 0, 0, 0), day_34 = c(NA, NA,
>     0, 0, 0), day_35 = c(NA, NA, 0, 0, 0), day_36 = c(NA, NA,
>     0, 0, 0), day_37 = c(NA, NA, 0, 0, 0), day_38 = c(NA, NA,
>     32.7132, 31.71852, 0), day_39 = c(NA, NA, 0, 0, 5.84604),
>     day_40 = c(NA, NA, 0, 0, 0), day_41 = c(NA, NA, 0, 0, 0),
>     day_42 = c(NA, NA, 0, 0, 0), day_43 = c(NA, NA, 0, 0, 0),
>     day_44 = c(NA, NA, 0, 0, 1.78416), day_45 = c(NA, NA, 0,
>     0, 0), day_46 = c(NA, NA, 33.84504, 0, 0), day_47 = c(NA,
>     NA, 0, 0, 0), day_48 = c(NA, NA, 0, 0, 0), day_49 = c(NA,
>     NA, 0, 0, 0), day_50 = c(NA, NA, 0, 0.4752, 0), day_51 = c(NA,
>     NA, 0, 0, 22.02012), day_52 = c(NA, NA, 0, 0, 0), day_53 = c(NA,
>     NA, 0, 0, 3.48084), day_54 = c(NA, NA, 0, 0, 0), day_55 = c(NA,
>     NA, 0.58212, 0, 0), day_56 = c(NA, NA, 0.35316, 0, 0), day_57 = c(NA,
>     NA, 0, 0, 12.65436), day_58 = c(NA, NA, 0, 0, 0), day_59 = c(NA,
>     NA, 0, 0, 0), day_60 = c(NA, NA, 3.03372, 22.05576, 0), day_61 = c(NA,
>     NA, 2.5758, 0, 0), day_62 = c(NA, NA, 0, 0, 0), day_63 = c(NA,
>     NA, 3.67416, 25.22016, 4.21524), day_64 = c(NA, NA, 0.52488,
>     3.60288, 0), day_65 = c(NA, NA, 12.82608, 0, 0), day_66 = c(NA,
>     NA, 0, 0, 0), day_67 = c(NA, NA, 0, 0, 0), day_68 = c(NA,
>     NA, 0, 0, 0), day_69 = c(NA, NA, 0, 0, 0), day_70 = c(NA,
>     NA, 1.11564, 5.17536, 0), day_71 = c(NA, NA, 1.18584, 0,
>     0), day_72 = c(NA, NA, 0, 0, 0.10584), day_73 = c(NA, NA,
>     0.62748, 14.39748, 7.50708), day_74 = c(NA, NA, 7.20252,
>     20.02644, 1.07244), day_75 = c(NA, NA, 1.87488, 0, 0), day_76 = c(NA,
>     NA, 0.26784, 0, 0), day_77 = c(NA, NA, 0, 0, 0), day_78 = c(NA,
>     NA, 0, 0, 2.81664), day_79 = c(NA, NA, 0, 0, 0), day_80 = c(NA,
>     NA, 0, 0, 0), day_81 = c(NA, NA, 0, 0, 0), day_82 = c(NA,
>     NA, 1.29276, 0, 0), day_83 = c(NA, NA, 0.18468, 0, 1.46124
>     ), day_84 = c(NA, NA, 0, 0, 0), day_85 = c(NA, NA, 0, 0,
>     0), day_86 = c(NA, NA, 57.36528, 0, 0), day_87 = c(NA, NA,
>     8.19504, 0, 0), day_88 = c(NA, NA, 0, 0, 0), day_89 = c(NA,
>     NA, 6.45732, 0, 0), day_90 = c(NA, NA, 0, 0, 0), day_91 = c(NA,
>     NA, 0, 0, 44.20332), day_92 = c(NA, NA, 0, 0, 6.31476), day_93 = c(NA,
>     NA, 0, 0, 0.35748), day_94 = c(NA, NA, 16.74972, 30.35988,
>     5.0436), day_95 = c(NA, NA, 4.93992, 1.46556, 19.86768),
>     day_96 = c(NA, NA, 0, 0, 0.88128), day_97 = c(NA, NA, 5.751,
>     19.02096, 0), day_98 = c(NA, NA, 11.5452, 13.37148, 0), day_99 = c(NA,
>     NA, 0, 0, 0), day_100 = c(NA, NA, 0, 0, 0), day_101 = c(NA,
>     NA, 4.70124, 23.80644, 7.61832), day_102 = c(NA, NA, 0, 1.02492,
>     0), day_103 = c(NA, NA, 0, 0, 15.86304), day_104 = c(NA,
>     NA, 0, 0, 0.26352), day_105 = c(NA, NA, 0, 0, 21.60864),
>     day_106 = c(NA, NA, 56.93436, 0, 0.22464), day_107 = c(NA,
>     NA, 8.13348, 0, 0), day_108 = c(NA, NA, 6.83748, 0, 0), day_109 = c(NA,
>     NA, 0, 0, 0), day_110 = c(NA, NA, 14.36724, 0, 0), day_111 = c(NA,
>     NA, 0.63936, 2.43864, 4.0554), day_112 = c(NA, NA, 1.21392,
>     1.15452, 0), day_113 = c(NA, NA, 0.7722, 0, 0), day_114 = c(NA,
>     NA, 0, 0, 1.08864), day_115 = c(NA, NA, 1.47528, 0, 0), day_116 = c(NA,
>     NA, 0, 1.73124, 0), day_117 = c(NA, NA, 0, 0, 0), day_118 = c(NA,
>     NA, 2.4516, 0, 0), day_119 = c(NA, NA, 0, 3.14388, 0), day_120 = c(NA,
>     NA, 1.81872, 0, 0), day_121 = c(NA, NA, 2.77236, 0, 0), day_122 = c(NA,
>     NA, 1.34028, 0.70632, 0), day_123 = c(NA, NA, 0, 0, 0), day_124 = c(NA,
>     NA, 0, 0, 0), day_125 = c(NA, NA, 0.56484, 0.74412, 0), day_126 = c(NA,
>     NA, 1.11888, 0.06264, 0), day_127 = c(NA, NA, 0, 0, 0), day_128 = c(NA,
>     NA, 1.05624, 0, 0), day_129 = c(NA, NA, 26.63928, 34.04268,
>     0), day_130 = c(NA, NA, 6.89796, 0, 0), day_131 = c(NA, NA,
>     1.91592, 2.241, 0), day_132 = c(NA, NA, 0, 2.23668, 45.23904
>     ), day_133 = c(NA, NA, 0, 0, 6.46272), day_134 = c(NA, NA,
>     0, 0, 0), day_135 = c(NA, NA, 0, 0, 0), day_136 = c(NA, NA,
>     0, 0, 0), day_137 = c(NA, NA, 0, 0, 0), day_138 = c(NA, NA,
>     0, 0, 0), day_139 = c(NA, NA, 0, 0, 0), day_140 = c(NA, NA,
>     0, 0, 0), day_141 = c(NA, NA, 0, 0, 0), day_142 = c(NA, NA,
>     0, 0, 0), day_143 = c(NA, NA, 0, 0, 0), day_144 = c(NA, NA,
>     2.943, 5.17536, 0), day_145 = c(NA, NA, 0, 0, 0), day_146 = c(NA,
>     NA, 0, 0, 0), day_147 = c(NA, NA, 0, 0, 0), day_148 = c(NA,
>     NA, 10.96308, 2.98188, 0), day_149 = c(NA, NA, 20.4822, 0.43632,
>     0), day_150 = c(NA, NA, 1.5282, 0, 0), day_151 = c(NA, NA,
>     0, 0, 0), day_152 = c(NA, NA, 0, 0, 0), day_153 = c(NA, NA,
>     0, 0, 0), day_154 = c(NA, NA, 0, 0, 0), day_155 = c(NA, NA,
>     0, 0, 0), day_156 = c(NA, NA, 0, 0, 0), day_157 = c(NA, NA,
>     0, 0, 0), day_158 = c(NA, NA, 0, 0, 0), day_159 = c(NA, NA,
>     0, 0, 0), day_160 = c(NA, NA, 0, 0, 0), day_161 = c(NA, NA,
>     0, 0, 0), day_162 = c(NA, NA, 0, 0, 0), day_163 = c(NA, NA,
>     0, 26.3412, 4.07376), day_164 = c(NA, NA, 0, 4.28328, 3.03156
>     ), day_165 = c(NA, NA, 0, 0, 0), day_166 = c(NA, NA, 0, 0,
>     4.60404), day_167 = c(NA, NA, 0, 0, 0.70848), day_168 = c(NA,
>     NA, 0, 0, 0), day_169 = c(NA, NA, 0, 0, 0), day_170 = c(NA,
>     NA, 0, 0, 0), day_171 = c(NA, NA, 0, 0, 0), day_172 = c(NA,
>     NA, 0, 0, 0), day_173 = c(NA, NA, 0, 0, 3.7854), day_174 = c(NA,
>     NA, 0, 0, 0), day_175 = c(NA, NA, 0, 0, 0), day_176 = c(NA,
>     NA, 0, 0, 0), day_177 = c(NA, NA, 0, 0, 0), day_178 = c(NA,
>     NA, 0, 0, 0), day_179 = c(NA, NA, 0, 0, 0), day_180 = c(NA,
>     NA, 0, 0, 0), day_181 = c(NA, NA, 0, 0, 0), day_182 = c(NA,
>     NA, 0, 0, 0), day_183 = c(NA, NA, 0, 0, 0), day_184 = c(NA,
>     NA, 0, 0, 0), day_185 = c(NA, NA, 0, 0, 0), day_186 = c(NA,
>     NA, 0, 0, 0), day_187 = c(NA, NA, 7.30728, 4.1202, 0), day_188 = c(NA,
>     NA, 2.56608, 0.5886, 0), day_189 = c(NA, NA, 0, 0, 0), day_190 = c(NA,
>     NA, 21.93156, 8.0082, 11.4318), day_191 = c(NA, NA, 3.13308,
>     0, 0), day_192 = c(NA, NA, 0, 0, 0.10692), day_193 = c(NA,
>     NA, 0, 0, 4.65912), day_194 = c(NA, NA, 0, 0, 0), day_195 = c(NA,
>     NA, 0, 0, 0), day_196 = c(NA, NA, 0, 0, 0), day_197 = c(NA,
>     NA, 0, 0, 0), day_198 = c(NA, NA, 0, 0, 0), day_199 = c(NA,
>     NA, 0, 0, 0), day_200 = c(NA, NA, 0, 0, 0), day_201 = c(NA,
>     NA, 0, 0, 0), day_202 = c(NA, NA, 0, 7.77276, 4.6602), day_203 = c(NA,
>     NA, 0, 0.86292, 0), day_204 = c(NA, NA, 0, 0, 0), day_205 = c(NA,
>     NA, 21.45528, 8.69616, 0), day_206 = c(NA, NA, 0, 0, 0),
>     day_207 = c(NA, NA, 0, 0, 0), day_208 = c(NA, NA, 0, 0, 0
>     ), day_209 = c(NA, NA, 0, 0, 0), day_210 = c(NA, NA, 0, 0,
>     0), day_211 = c(NA, NA, 0, 0, 0), day_212 = c(NA, NA, 0,
>     0, 0), day_213 = c(NA, NA, 0, 0, 0), day_214 = c(NA, NA,
>     0, 0, 0), day_215 = c(NA, NA, 0, 0, 0), day_216 = c(NA, NA,
>     0, 0, 0), day_217 = c(NA, NA, 0, 0, 0), day_218 = c(NA, NA,
>     0, 0, 0), day_219 = c(NA, NA, 0, 0, 0), day_220 = c(NA, NA,
>     6.10092, 10.85508, 13.22244), day_221 = c(NA, NA, 0.87156,
>     0, 0), day_222 = c(NA, NA, 0, 0, 15.46452), day_223 = c(NA,
>     NA, 0, 0, 9.83664), day_224 = c(NA, NA, 0, 0, 0), day_225 = c(NA,
>     NA, 0, 0, 0), day_226 = c(NA, NA, 16.46028, 0, 0), day_227 = c(NA,
>     NA, 0, 0, 0), day_228 = c(NA, NA, 0, 0, 0), day_229 = c(NA,
>     NA, 0, 0, 0), day_230 = c(NA, NA, 0, 0, 0), day_231 = c(NA,
>     NA, 2.7108, 0, 0), day_232 = c(NA, NA, 0, 0, 0), day_233 = c(NA,
>     NA, 0, 0, 0), day_234 = c(NA, NA, 0, 0, 0), day_235 = c(NA,
>     NA, 0, 0, 0), day_236 = c(NA, NA, 0, 0, 3.5586), day_237 = c(NA,
>     NA, 0, 0, 0), day_238 = c(NA, NA, 0, 0, 0), day_239 = c(NA,
>     NA, 10.23192, 0, 0), day_240 = c(NA, NA, 0, 0, 0), day_241 = c(NA,
>     NA, 0, 0, 0), day_242 = c(NA, NA, 0, 0, 0), day_243 = c(NA,
>     NA, 0, 0, 0), day_244 = c(NA, NA, 0, 0, 0), day_245 = c(NA,
>     NA, 0, 0, 0), day_246 = c(NA, NA, 0, 0, 0), day_247 = c(NA,
>     NA, 0, 0, 0), day_248 = c(NA, NA, 0, 0, 0), day_249 = c(NA,
>     NA, 0.50544, 0, 0), day_250 = c(NA, NA, 0.12636, 0, 0), day_251 = c(NA,
>     NA, 7.02432, 0, 5.39784), day_252 = c(NA, NA, 3.33828, 8.00064,
>     7.08372), day_253 = c(NA, NA, 0, 0, 0), day_254 = c(NA, NA,
>     0, 0, 0), day_255 = c(NA, NA, 2.5704, 4.71636, 11.99772),
>     day_256 = c(NA, NA, 0.3672, 0.75384, 0), day_257 = c(NA,
>     NA, 0, 0, 0), day_258 = c(NA, NA, 0.50328, 0, 0), day_259 = c(NA,
>     NA, 6.78888, 0, 0), day_260 = c(NA, NA, 0.96984, 0, 0), day_261 = c(NA,
>     NA, 4.62672, 0, 0), day_262 = c(NA, NA, 0, 0, 0), day_263 = c(NA,
>     NA, 3.16224, 0.27864, 0), day_264 = c(NA, NA, 0, 1.31112,
>     0), day_265 = c(NA, NA, 0.37692, 0, 0), day_266 = c(NA, NA,
>     0, 0, 0), day_267 = c(NA, NA, 0.70524, 0.43524, 0), day_268 = c(NA,
>     NA, 0.18792, 0.12744, 0), day_269 = c(NA, NA, 0, 1.79064,
>     0.96012), day_270 = c(NA, NA, 0, 0, 0.58644), day_271 = c(NA,
>     NA, 4.4982, 0, 0), day_272 = c(NA, NA, 0, 0, 0), day_273 = c(NA,
>     NA, 2.04552, 6.56964, 0), day_274 = c(NA, NA, 0.71712, 0.93852,
>     0), day_275 = c(NA, NA, 0, 0, 0), day_276 = c(NA, NA, 0,
>     0, 0), day_277 = c(NA, NA, 3.31452, 0, 0), day_278 = c(NA,
>     NA, 1.20204, 0, 0), day_279 = c(NA, NA, 0, 0, 0), day_280 = c(NA,
>     NA, 0, 0, 0), day_281 = c(NA, NA, 0, 0, 0), day_282 = c(NA,
>     NA, 17.955, 5.7942, 9.93816), day_283 = c(NA, NA, 4.79304,
>     4.8006, 0), day_284 = c(NA, NA, 3.9366, 0.78084, 0), day_285 = c(NA,
>     NA, 0, 0, 0), day_286 = c(NA, NA, 0, 0, 0), day_287 = c(NA,
>     NA, 0, 0, 0), day_288 = c(NA, NA, 0, 0, 0), day_289 = c(NA,
>     NA, 0, 0, 0), day_290 = c(NA, NA, 0, 0, 0), day_291 = c(NA,
>     NA, 0, 0, 0), day_292 = c(NA, NA, 0, 0, 0), day_293 = c(NA,
>     NA, 1.55736, 0, 0), day_294 = c(NA, NA, 4.28328, 0, 0), day_295 = c(NA,
>     NA, 0, 0, 0), day_296 = c(NA, NA, 0, 0, 0), day_297 = c(NA,
>     NA, 1.6362, 0, 0), day_298 = c(NA, NA, 1.28844, 0, 6.14088
>     ), day_299 = c(NA, NA, 0, 0, 0.50112), day_300 = c(NA, NA,
>     0, 0, 0), day_301 = c(NA, NA, 0, 0.13824, 0.03456), day_302 = c(NA,
>     NA, 0, 2.92572, 9.24264), day_303 = c(NA, NA, 2.8188, 0.41796,
>     0), day_304 = c(NA, NA, 2.04876, 11.28384, 0), day_305 = c(NA,
>     NA, 0, 0.3564, 0), day_306 = c(NA, NA, 0, 0, 0), day_307 = c(NA,
>     NA, 0, 2.36736, 0), day_308 = c(NA, NA, 0, 0, 0), day_309 = c(NA,
>     NA, 34.91856, 20.42604, 0), day_310 = c(NA, NA, 0, 0, 0),
>     day_311 = c(NA, NA, 0, 0, 0), day_312 = c(NA, NA, 0, 0.40392,
>     0), day_313 = c(NA, NA, 0, 0.5292, 0), day_314 = c(NA, NA,
>     0, 0, 5.21424), day_315 = c(NA, NA, 0, 0, 0), day_316 = c(NA,
>     NA, 0, 0, 0.4266), day_317 = c(NA, NA, 0, 0, 0), day_318 = c(NA,
>     NA, 0, 0, 0), day_319 = c(NA, NA, 0, 0, 0), day_320 = c(NA,
>     NA, 0.23436, 0.6048, 14.9256), day_321 = c(NA, NA, 0, 0.10908,
>     0), day_322 = c(NA, NA, 7.68096, 6.66036, 4.53924), day_323 = c(NA,
>     NA, 1.09728, 1.59732, 8.51148), day_324 = c(NA, NA, 0, 0,
>     0), day_325 = c(NA, NA, 1.46016, 0, 0), day_326 = c(NA, NA,
>     0, 0, 8.70048), day_327 = c(NA, NA, 0, 0, 0), day_328 = c(NA,
>     NA, 0, 0, 0), day_329 = c(NA, NA, 0, 0, 0), day_330 = c(NA,
>     NA, 5.3082, 0, 0), day_331 = c(NA, NA, 2.5866, 0, 0), day_332 = c(NA,
>     NA, 8.03628, 6.3666, 4.3308), day_333 = c(NA, NA, 0, 0, 0
>     ), day_334 = c(NA, NA, 0, 0, 0), day_335 = c(NA, NA, 0, 0,
>     0), day_336 = c(NA, NA, 0, 0, 5.29632), day_337 = c(NA, NA,
>     0, 1.77444, 2.7216), day_338 = c(NA, NA, 0.40608, 0, 0.83052
>     ), day_339 = c(NA, NA, 0, 0, 0), day_340 = c(NA, NA, 0, 0,
>     0), day_341 = c(NA, NA, 0, 0, 0), day_342 = c(NA, NA, 0,
>     0, 0), day_343 = c(NA, NA, 0, 0, 0), day_344 = c(NA, NA,
>     7.73388, 0, 0), day_345 = c(NA, NA, 4.80384, 0, 0), day_346 = c(NA,
>     NA, 4.374, 0.09288, 0), day_347 = c(NA, NA, 6.42924, 3.2022,
>     0), day_348 = c(NA, NA, 0, 16.27668, 0), day_349 = c(NA,
>     NA, 0, 0.90072, 9.36684), day_350 = c(NA, NA, 0.135, 1.87272,
>     2.49048), day_351 = c(NA, NA, 0, 0, 0), day_352 = c(NA, NA,
>     0, 0, 0), day_353 = c(NA, NA, 0, 0, 0), day_354 = c(NA, NA,
>     0, 0, 0), day_355 = c(NA, NA, 0, 0, 0), day_356 = c(NA, NA,
>     0, 0, 0), day_357 = c(NA, NA, 2.82636, 39.45348, 26.08848
>     ), day_358 = c(NA, NA, 0, 0, 22.8582), day_359 = c(NA, NA,
>     0, 0, 1.34028), day_360 = c(NA, NA, 30.03804, 0, 3.49704),
>     day_361 = c(NA, NA, 0.13392, 4.941, 4.94424), day_362 = c(NA,
>     NA, 0.92016, 0, 0.70632), day_363 = c(NA, NA, 0, 0, 0), day_364 = c(NA,
>     NA, 0, 0, 0), day_365 = c(NA, NA, 0, 0, 0), day_366 = c(NA,
>     NA, 21.42072, 0, 0)), .Names = c("ISO3", "lon", "lat", "day_1", "day_2",
> "day_3", "day_4", "day_5", "day_6", "day_7", "day_8", "day_9", "day_10",
> "day_11", "day_12", "day_13", "day_14", "day_15", "day_16", "day_17",
> "day_18", "day_19", "day_20", "day_21", "day_22", "day_23", "day_24",
> "day_25", "day_26", "day_27", "day_28", "day_29", "day_30", "day_31",
> "day_32", "day_33", "day_34", "day_35", "day_36", "day_37", "day_38",
> "day_39", "day_40", "day_41", "day_42", "day_43", "day_44", "day_45",
> "day_46", "day_47", "day_48", "day_49", "day_50", "day_51", "day_52",
> "day_53", "day_54", "day_55", "day_56", "day_57", "day_58", "day_59",
> "day_60", "day_61", "day_62", "day_63", "day_64", "day_65", "day_66",
> "day_67", "day_68", "day_69", "day_70", "day_71", "day_72", "day_73",
> "day_74", "day_75", "day_76", "day_77", "day_78", "day_79", "day_80",
> "day_81", "day_82", "day_83", "day_84", "day_85", "day_86", "day_87",
> "day_88", "day_89", "day_90", "day_91", "day_92", "day_93", "day_94",
> "day_95", "day_96", "day_97", "day_98", "day_99", "day_100", "day_101",
> "day_102", "day_103", "day_104", "day_105", "day_106", "day_107",
> "day_108", "day_109", "day_110", "day_111", "day_112", "day_113",
> "day_114", "day_115", "day_116", "day_117", "day_118", "day_119",
> "day_120", "day_121", "day_122", "day_123", "day_124", "day_125",
> "day_126", "day_127", "day_128", "day_129", "day_130", "day_131",
> "day_132", "day_133", "day_134", "day_135", "day_136", "day_137",
> "day_138", "day_139", "day_140", "day_141", "day_142", "day_143",
> "day_144", "day_145", "day_146", "day_147", "day_148", "day_149",
> "day_150", "day_151", "day_152", "day_153", "day_154", "day_155",
> "day_156", "day_157", "day_158", "day_159", "day_160", "day_161",
> "day_162", "day_163", "day_164", "day_165", "day_166", "day_167",
> "day_168", "day_169", "day_170", "day_171", "day_172", "day_173",
> "day_174", "day_175", "day_176", "day_177", "day_178", "day_179",
> "day_180", "day_181", "day_182", "day_183", "day_184", "day_185",
> "day_186", "day_187", "day_188", "day_189", "day_190", "day_191",
> "day_192", "day_193", "day_194", "day_195", "day_196", "day_197",
> "day_198", "day_199", "day_200", "day_201", "day_202", "day_203",
> "day_204", "day_205", "day_206", "day_207", "day_208", "day_209",
> "day_210", "day_211", "day_212", "day_213", "day_214", "day_215",
> "day_216", "day_217", "day_218", "day_219", "day_220", "day_221",
> "day_222", "day_223", "day_224", "day_225", "day_226", "day_227",
> "day_228", "day_229", "day_230", "day_231", "day_232", "day_233",
> "day_234", "day_235", "day_236", "day_237", "day_238", "day_239",
> "day_240", "day_241", "day_242", "day_243", "day_244", "day_245",
> "day_246", "day_247", "day_248", "day_249", "day_250", "day_251",
> "day_252", "day_253", "day_254", "day_255", "day_256", "day_257",
> "day_258", "day_259", "day_260", "day_261", "day_262", "day_263",
> "day_264", "day_265", "day_266", "day_267", "day_268", "day_269",
> "day_270", "day_271", "day_272", "day_273", "day_274", "day_275",
> "day_276", "day_277", "day_278", "day_279", "day_280", "day_281",
> "day_282", "day_283", "day_284", "day_285", "day_286", "day_287",
> "day_288", "day_289", "day_290", "day_291", "day_292", "day_293",
> "day_294", "day_295", "day_296", "day_297", "day_298", "day_299",
> "day_300", "day_301", "day_302", "day_303", "day_304", "day_305",
> "day_306", "day_307", "day_308", "day_309", "day_310", "day_311",
> "day_312", "day_313", "day_314", "day_315", "day_316", "day_317",
> "day_318", "day_319", "day_320", "day_321", "day_322", "day_323",
> "day_324", "day_325", "day_326", "day_327", "day_328", "day_329",
> "day_330", "day_331", "day_332", "day_333", "day_334", "day_335",
> "day_336", "day_337", "day_338", "day_339", "day_340", "day_341",
> "day_342", "day_343", "day_344", "day_345", "day_346", "day_347",
> "day_348", "day_349", "day_350", "day_351", "day_352", "day_353",
> "day_354", "day_355", "day_356", "day_357", "day_358", "day_359",
> "day_360", "day_361", "day_362", "day_363", "day_364", "day_365",
> "day_366"), row.names = c(NA, 5L), class =
> "data.frame")
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.



________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Tue Nov 22 11:08:15 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 22 Nov 2016 10:08:15 +0000
Subject: [R] how to use vector of values to change row order of a heatmap
In-Reply-To: <1301534071.1349522.1479759259914@mail.yahoo.com>
References: <1301534071.1349522.1479759259914.ref@mail.yahoo.com>
	<1301534071.1349522.1479759259914@mail.yahoo.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5046FAD@SRVEXCHMBX.precheza.cz>

Hi

Heatmap does not work with data frames (at least the version I have)

> b=as.data.frame(matrix(c(3,4,5,8,9,10,13,14,15,27,19,20),3,4))
> heatmap(b)
Error in heatmap(b) : 'x' must be a numeric matrix

With matrix, heatmap works as expected.

> b=matrix(c(3,4,5,8,9,10,13,14,15,27,19,20),3,4)
> heatmap(b)

However for row ordering you need to read Details of help page.

compare

> heatmap(b, Rowv=c(3,2,1))

with

> heatmap(b, Rowv=c(10,2,1))

If either is a vector (of ?weights?) then the appropriate dendrogram is reordered according to the supplied values subject to the constraints imposed by the dendrogram, by reorder(dd, Rowv), in the row case. If either is
                                    ^^^^^^^^^
missing, as by default, then the ordering of the corresponding dendrogram is by the mean value of the rows/columns, i.e., in the case of rows, Rowv <- rowMeans(x, na.rm = na.rm). If either is NA, no reordering will be done for the corresponding side.

Cheers
Petr



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Fix Ace via
> R-help
> Sent: Monday, November 21, 2016 9:14 PM
> To: r-help at r-project.org
> Subject: [R] how to use vector of values to change row order of a heatmap
>
> Hello, there,
> R document for heatmap says that Rowv could be a vector of values to
> specify the row order. However, I couldn't figure out how to apply it. A
> simple example here:>
> b=as.data.frame(matrix(c(3,4,5,8,9,10,13,14,15,27,19,20),3,4))
> > b
>   V1 V2 V3 V4
> 1  3  8 13 27
> 2  4  9 14 19
> 3  5 10 15 20
> > row.names(b)=c("a","b","c")
> > b
>   V1 V2 V3 V4
> a  3  8 13 27
> b  4  9 14 19
> c  5 10 15 20
> > heatmap(as.matrix(b))
> What I got: "a" stays at the bottom of the heatmap.
>  Now I would like to put row "a" to the top row, how do I do that?I tried
> provide a vector of values (all the possible combination of 1,2,3) to Rowv,  "a"
> is always stay at the bottom Any input would be very helpful!
> Thanks.
> Ace
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From matteo.richiardi at gmail.com  Tue Nov 22 13:21:57 2016
From: matteo.richiardi at gmail.com (Matteo Richiardi)
Date: Tue, 22 Nov 2016 12:21:57 +0000
Subject: [R] Getting the index of specific quantiles
Message-ID: <CABSrU1JgbhdO-xCSJyzWcoOi42sOvdhzgyVu9aqWsbxK7awifg@mail.gmail.com>

Dear R-users,
a very easy one for you, I guess. I need to extract the indexes of the
elements corresponding to different quantiles of a vector. When a
quantile is an interpolation between two adjacent values, I need the
index of the value which is closer (the lower value - or the higher
value for what matters - in case the quantile is exactly half way
through).

This is an example.

> x <- c(9,9,1,3,2,7,6,10,5,6)
> quantile(x)
  0%  25%  50%  75% 100%
 1.0  3.5  6.0  8.5 10.0

What I need is a vector 'index' which looks like

> index
3   4   7   1   8

Many thanks for your help ! Matteo


From ferri.leberl at gmx.at  Tue Nov 22 14:46:35 2016
From: ferri.leberl at gmx.at (Ferri Leberl)
Date: Tue, 22 Nov 2016 14:46:35 +0100
Subject: [R] Breaking down a list into a table
Message-ID: <trinity-6038d4d5-154d-4744-babb-418f4b22bf41-1479822395784@3capp-gmx-bs79>



Dear All,
I asked for support to deal with a hirarchy within a character separated list.
I solved the problem crudely but effectively by

- Choosing for a TSV as input, where in columns that may contain several (or as well no) items the items are separated via semicolon
- adding semicolons to the first row to grant that the first row has the maximum number of semicolons of this column
- grasping the column(x<-myarray[,y], where y is some integer value) and saving it as a TSV (with only one column)
- importing it again, defining it semicolumn-separated, with fill option

To all those who feel pain reading this: Is there a shortcut?
Thank you in advance.
Yours, Ferri


From ulrik.stervbo at gmail.com  Tue Nov 22 14:57:18 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Tue, 22 Nov 2016 13:57:18 +0000
Subject: [R] Breaking down a list into a table
In-Reply-To: <trinity-6038d4d5-154d-4744-babb-418f4b22bf41-1479822395784@3capp-gmx-bs79>
References: <trinity-6038d4d5-154d-4744-babb-418f4b22bf41-1479822395784@3capp-gmx-bs79>
Message-ID: <CAKVAULNt9Hm0C2DS0xOqUmjuQwF1a5fqSU_x0RUhzceYTvXMTg@mail.gmail.com>

Hi Ferri,

It sounds like the function 'separate' from the tidyr package is what you
look for,

HTH
Ulrik

On Tue, 22 Nov 2016 at 14:49 Ferri Leberl <ferri.leberl at gmx.at> wrote:



Dear All,
I asked for support to deal with a hirarchy within a character separated
list.
I solved the problem crudely but effectively by

- Choosing for a TSV as input, where in columns that may contain several
(or as well no) items the items are separated via semicolon
- adding semicolons to the first row to grant that the first row has the
maximum number of semicolons of this column
- grasping the column(x<-myarray[,y], where y is some integer value) and
saving it as a TSV (with only one column)
- importing it again, defining it semicolumn-separated, with fill option

To all those who feel pain reading this: Is there a shortcut?
Thank you in advance.
Yours, Ferri

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Tue Nov 22 15:18:15 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 22 Nov 2016 09:18:15 -0500
Subject: [R] Getting the index of specific quantiles
In-Reply-To: <CABSrU1JgbhdO-xCSJyzWcoOi42sOvdhzgyVu9aqWsbxK7awifg@mail.gmail.com>
References: <CABSrU1JgbhdO-xCSJyzWcoOi42sOvdhzgyVu9aqWsbxK7awifg@mail.gmail.com>
Message-ID: <CAM_vjumtmYN_YGuL6GEi55R=6DEzH81Nv2A7W6uTQJTJOkEjwg@mail.gmail.com>

Here's how to get one:

x <- c(9,9,1,3,2,7,6,10,5,6)
> which.min(abs(x - quantile(x, .25)))
[1] 4

And here's one of the various ways to get the entire set:

> xq <- quantile(x)
> sapply(xq, function(y)which.min(abs(x - y)))
  0%  25%  50%  75% 100%
   3    4    7    1    8

Sarah

On Tue, Nov 22, 2016 at 7:21 AM, Matteo Richiardi
<matteo.richiardi at gmail.com> wrote:
> Dear R-users,
> a very easy one for you, I guess. I need to extract the indexes of the
> elements corresponding to different quantiles of a vector. When a
> quantile is an interpolation between two adjacent values, I need the
> index of the value which is closer (the lower value - or the higher
> value for what matters - in case the quantile is exactly half way
> through).
>
> This is an example.
>
>> x <- c(9,9,1,3,2,7,6,10,5,6)
>> quantile(x)
>   0%  25%  50%  75% 100%
>  1.0  3.5  6.0  8.5 10.0
>
> What I need is a vector 'index' which looks like
>
>> index
> 3   4   7   1   8
>
> Many thanks for your help ! Matteo
>
-- 
Sarah Goslee
http://www.functionaldiversity.org


From stvienna at gmail.com  Tue Nov 22 16:18:59 2016
From: stvienna at gmail.com (stvienna wiener)
Date: Tue, 22 Nov 2016 16:18:59 +0100
Subject: [R] Fast continious wavelet transformation (CWT) and plot?
Message-ID: <CAKT4QRJOnJwURJZA_XXb37bATqfK+7c5qoTuG+d19kC2QpLrMA@mail.gmail.com>

Dear all,

This is my test code:

library(wmtsa)
sunspotsLong <- rep(sunspots, times=3000) ## try "times=30" (or 300)

sunspots.cwt <- wavCWT(sunspotsLong)
plot(sunspots.cwt, series=TRUE)

If you adapt times in the second line with "30", the code works. But 300 or
3000 not so much.

Are there more faster ways to calculate the CWT? Different packages that
are better for the continious wavelet transform?

My real time series has around 1 Million observations.


Stefan

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Tue Nov 22 16:48:46 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 22 Nov 2016 07:48:46 -0800
Subject: [R] Getting the index of specific quantiles
In-Reply-To: <CABSrU1JgbhdO-xCSJyzWcoOi42sOvdhzgyVu9aqWsbxK7awifg@mail.gmail.com>
References: <CABSrU1JgbhdO-xCSJyzWcoOi42sOvdhzgyVu9aqWsbxK7awifg@mail.gmail.com>
Message-ID: <BD3F582F-58D0-4FA5-814C-0ECABC1F4099@comcast.net>


> On Nov 22, 2016, at 4:21 AM, Matteo Richiardi <matteo.richiardi at gmail.com> wrote:
> 
> Dear R-users,
> a very easy one for you, I guess. I need to extract the indexes of the
> elements corresponding to different quantiles of a vector. When a
> quantile is an interpolation between two adjacent values, I need the
> index of the value which is closer (the lower value - or the higher
> value for what matters - in case the quantile is exactly half way
> through).
> 
> This is an example.
> 
>> x <- c(9,9,1,3,2,7,6,10,5,6)
>> quantile(x)
>  0%  25%  50%  75% 100%
> 1.0  3.5  6.0  8.5 10.0
> 
> What I need is a vector 'index' which looks like
> 
>> index
> 3   4   7   1   8
> 

Try this (calculate distances with `outer` and then sweep the columns with `apply`)

> dqt <- outer(x, quantile(x), function(x,y) sqrt( (x-y)^2) )
> apply(dqt, 2, which.min)
  0%  25%  50%  75% 100% 
   3    4    7    1    8 

-- 
David.
> Many thanks for your help ! Matteo
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From wdunlap at tibco.com  Tue Nov 22 17:58:24 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 22 Nov 2016 08:58:24 -0800
Subject: [R] Getting the index of specific quantiles
In-Reply-To: <CABSrU1JgbhdO-xCSJyzWcoOi42sOvdhzgyVu9aqWsbxK7awifg@mail.gmail.com>
References: <CABSrU1JgbhdO-xCSJyzWcoOi42sOvdhzgyVu9aqWsbxK7awifg@mail.gmail.com>
Message-ID: <CAF8bMcZT3opcj==ci0p2hQkFv3Xq9C1SyOUeQUdDD42vbrjtGA@mail.gmail.com>

You might try something like
   x <- c(9,9,1,3,2,7,6,10,5,6)
   p <- (0:4)/4
   order(x) [ quantile(seq_along(x),p, type=1) ]
   # [1] 3 4 7 1 8
Selecting which value of 'type' works makes my head hurt.
You could also use 1+(p*(length(x)-1) as the index into order(x).


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Nov 22, 2016 at 4:21 AM, Matteo Richiardi <
matteo.richiardi at gmail.com> wrote:

> Dear R-users,
> a very easy one for you, I guess. I need to extract the indexes of the
> elements corresponding to different quantiles of a vector. When a
> quantile is an interpolation between two adjacent values, I need the
> index of the value which is closer (the lower value - or the higher
> value for what matters - in case the quantile is exactly half way
> through).
>
> This is an example.
>
> > x <- c(9,9,1,3,2,7,6,10,5,6)
> > quantile(x)
>   0%  25%  50%  75% 100%
>  1.0  3.5  6.0  8.5 10.0
>
> What I need is a vector 'index' which looks like
>
> > index
> 3   4   7   1   8
>
> Many thanks for your help ! Matteo
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From matteo.richiardi at gmail.com  Tue Nov 22 18:01:57 2016
From: matteo.richiardi at gmail.com (Matteo Richiardi)
Date: Tue, 22 Nov 2016 17:01:57 +0000
Subject: [R] Getting the index of specific quantiles
In-Reply-To: <CAM_vjumtmYN_YGuL6GEi55R=6DEzH81Nv2A7W6uTQJTJOkEjwg@mail.gmail.com>
References: <CABSrU1JgbhdO-xCSJyzWcoOi42sOvdhzgyVu9aqWsbxK7awifg@mail.gmail.com>
	<CAM_vjumtmYN_YGuL6GEi55R=6DEzH81Nv2A7W6uTQJTJOkEjwg@mail.gmail.com>
Message-ID: <CABSrU1KCn_oRWDCU6vK=JvMYC1of69h4rbzMRyG90ZvRwDjw3Q@mail.gmail.com>

Thanks Sarah (and all the others who replied) for your precious
suggestions! Matteo

On 22 November 2016 at 14:18, Sarah Goslee <sarah.goslee at gmail.com> wrote:
> Here's how to get one:
>
> x <- c(9,9,1,3,2,7,6,10,5,6)
>> which.min(abs(x - quantile(x, .25)))
> [1] 4
>
> And here's one of the various ways to get the entire set:
>
>> xq <- quantile(x)
>> sapply(xq, function(y)which.min(abs(x - y)))
>   0%  25%  50%  75% 100%
>    3    4    7    1    8
>
> Sarah
>
> On Tue, Nov 22, 2016 at 7:21 AM, Matteo Richiardi
> <matteo.richiardi at gmail.com> wrote:
>> Dear R-users,
>> a very easy one for you, I guess. I need to extract the indexes of the
>> elements corresponding to different quantiles of a vector. When a
>> quantile is an interpolation between two adjacent values, I need the
>> index of the value which is closer (the lower value - or the higher
>> value for what matters - in case the quantile is exactly half way
>> through).
>>
>> This is an example.
>>
>>> x <- c(9,9,1,3,2,7,6,10,5,6)
>>> quantile(x)
>>   0%  25%  50%  75% 100%
>>  1.0  3.5  6.0  8.5 10.0
>>
>> What I need is a vector 'index' which looks like
>>
>>> index
>> 3   4   7   1   8
>>
>> Many thanks for your help ! Matteo
>>
> --
> Sarah Goslee
> http://www.functionaldiversity.org


From pnsinha68 at gmail.com  Tue Nov 22 18:16:44 2016
From: pnsinha68 at gmail.com (Partha Sinha)
Date: Tue, 22 Nov 2016 22:46:44 +0530
Subject: [R] Memory problem
Message-ID: <CADcgpJeMB2YcvPwJD5x5hocRmCjyQevq9rtVd1WtL3ooDDOWHg@mail.gmail.com>

 I am using R 3.3.2 on win 7, 32 bit with 2gb Ram. Is it possible to use
more than 2 Gb data set ?

Regards
Partha

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Nov 22 19:00:42 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 22 Nov 2016 10:00:42 -0800
Subject: [R] Memory problem
In-Reply-To: <CADcgpJeMB2YcvPwJD5x5hocRmCjyQevq9rtVd1WtL3ooDDOWHg@mail.gmail.com>
References: <CADcgpJeMB2YcvPwJD5x5hocRmCjyQevq9rtVd1WtL3ooDDOWHg@mail.gmail.com>
Message-ID: <CAGxFJbQc34KhnoZO8mK_chbijWUz=F63Pk-9+kk7VXbG6+tKbg@mail.gmail.com>

Depends how you use it. e.g. it can be stored on disk and worked with
in pieces. Or some packages work with virtual memory, I believe.

However, it is certainly not possible to read it into R. In fact, you
probably won't be able to handle more (and maybe much less) than about
500 mb in R.

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Nov 22, 2016 at 9:16 AM, Partha Sinha <pnsinha68 at gmail.com> wrote:
>  I am using R 3.3.2 on win 7, 32 bit with 2gb Ram. Is it possible to use
> more than 2 Gb data set ?
>
> Regards
> Partha
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marcus.nunes at gmail.com  Tue Nov 22 18:35:35 2016
From: marcus.nunes at gmail.com (Marcus Nunes)
Date: Tue, 22 Nov 2016 14:35:35 -0300
Subject: [R] Memory problem
In-Reply-To: <CADcgpJeMB2YcvPwJD5x5hocRmCjyQevq9rtVd1WtL3ooDDOWHg@mail.gmail.com>
References: <CADcgpJeMB2YcvPwJD5x5hocRmCjyQevq9rtVd1WtL3ooDDOWHg@mail.gmail.com>
Message-ID: <CA+QGQvty1Uk0R-J0vQ7Cu=KrcOD8UvfByfic+bSnRFSnvF+OOw@mail.gmail.com>

Yes.

If you cannot read the dataset with the usual means, using functions like
read.table or read.csv, try the ff package: https://cran.r-
project.org/web/packages/ff/index.html.

Best,

On Tue, Nov 22, 2016 at 2:16 PM, Partha Sinha <pnsinha68 at gmail.com> wrote:

>  I am using R 3.3.2 on win 7, 32 bit with 2gb Ram. Is it possible to use
> more than 2 Gb data set ?
>
> Regards
> Partha
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Marcus Nunes
http://marcusnunes.me/

	[[alternative HTML version deleted]]


From marammagdysalem at gmail.com  Tue Nov 22 19:05:51 2016
From: marammagdysalem at gmail.com (Maram SAlem)
Date: Tue, 22 Nov 2016 20:05:51 +0200
Subject: [R] The code itself disappears after starting to execute the
	for loop
In-Reply-To: <CAAxdm-5FWvfnpwEJtr5eNw0AKv+cjOfgXfQKTLSmYMkk1RgPcA@mail.gmail.com>
References: <59BE1118-9D4B-4298-B605-9BADFBC43E88@gmail.com>
	<CAAxdm-5FWvfnpwEJtr5eNw0AKv+cjOfgXfQKTLSmYMkk1RgPcA@mail.gmail.com>
Message-ID: <2BFA80AA-A18B-4A52-A96B-455AE7BB9063@gmail.com>

Thanks for helping Jim. 

I'm actually using the pbapply function together with the print function within a loop. In earlier versions, the progress bar and the output of the print function used to appear after each iteration of the loop. But with the 3.3.1. Version nothing appears, instead the console turns white and the cursor turns blue ( busy) and I know nothing about the progress of the running code.

I just want to see the bar and the output of the print function as I used to, any help?

Thanks in advance.
Maram Salem



Sent from my iPhone

> On Nov 3, 2016, at 8:30 PM, jim holtman <jholtman at gmail.com> wrote:
> 
> A little more information would help.  How exactly are out creating the output to the console?  Are you using 'print', 'cat' or something else?  Do you have buffered output checked on the GUI (you probably don't want it checked or you output will be delayed till the buffer is full -- this might be the cause of your problem.
> 
> 
> Jim Holtman
> Data Munger Guru
>  
> What is the problem that you are trying to solve?
> Tell me what you want to do, not how you want to do it.
> 
>> On Thu, Nov 3, 2016 at 1:55 PM, Maram SAlem <marammagdysalem at gmail.com> wrote:
>> Hi all,
>> 
>> I've a question concerning the R 3.3.1 version. I have a long code that I used to run on versions earlier to the 3.3.1 version, and when I copied the code to the R console, I can still see the code while the loop is executing , along with the output printed after each iteration of the loop.
>> 
>> Now, on the 3.3.1 version, after I copy the code to the console, it disappears and I only see the printed output of only one iteration at a time, that is, after the first iteration the printed output disappears ( though it's only 6 lines, just giving me some guidance, not a long output).
>> This is causing me some problems, so I don't know if there is a general option for R that enables me to still see the code and the output of all the iterations till the loop is over, as was the case with earlier R versions.
>> 
>> I didn't include the code as it's a long one.
>> 
>> Thanks a lot in advance,
>> 
>> Maram
>> 
>> 
>> Sent from my iPhone
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 

	[[alternative HTML version deleted]]


From istazahn at gmail.com  Tue Nov 22 19:47:29 2016
From: istazahn at gmail.com (Ista Zahn)
Date: Tue, 22 Nov 2016 13:47:29 -0500
Subject: [R] Memory problem
In-Reply-To: <CADcgpJeMB2YcvPwJD5x5hocRmCjyQevq9rtVd1WtL3ooDDOWHg@mail.gmail.com>
References: <CADcgpJeMB2YcvPwJD5x5hocRmCjyQevq9rtVd1WtL3ooDDOWHg@mail.gmail.com>
Message-ID: <CA+vqiLEAydokFTY8GhHc2uSyeW7cfhh4fYZh1A3dLf9E-Y2U-w@mail.gmail.com>

Not conveniently. Memory is cheap, you should buy more.

Best,
Ista

On Nov 22, 2016 12:19 PM, "Partha Sinha" <pnsinha68 at gmail.com> wrote:

>  I am using R 3.3.2 on win 7, 32 bit with 2gb Ram. Is it possible to use
> more than 2 Gb data set ?
>
> Regards
> Partha
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Tue Nov 22 20:36:30 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 22 Nov 2016 11:36:30 -0800
Subject: [R] Memory problem
In-Reply-To: <CA+vqiLEAydokFTY8GhHc2uSyeW7cfhh4fYZh1A3dLf9E-Y2U-w@mail.gmail.com>
References: <CADcgpJeMB2YcvPwJD5x5hocRmCjyQevq9rtVd1WtL3ooDDOWHg@mail.gmail.com>
	<CA+vqiLEAydokFTY8GhHc2uSyeW7cfhh4fYZh1A3dLf9E-Y2U-w@mail.gmail.com>
Message-ID: <0137E87E-C25B-4728-B697-20F74ECC3BE6@dcn.davis.ca.us>

Ah, you also need to use a 64-bit operating system. Depending on the age of your hardware this may also mean you need a new computer. 

There are ways to process data on disk for certain algorithms, but you will be glad to leave them behind once the opportunity arises, so you might as well do so now. 
-- 
Sent from my phone. Please excuse my brevity.

On November 22, 2016 10:47:29 AM PST, Ista Zahn <istazahn at gmail.com> wrote:
>Not conveniently. Memory is cheap, you should buy more.
>
>Best,
>Ista
>
>On Nov 22, 2016 12:19 PM, "Partha Sinha" <pnsinha68 at gmail.com> wrote:
>
>>  I am using R 3.3.2 on win 7, 32 bit with 2gb Ram. Is it possible to
>use
>> more than 2 Gb data set ?
>>
>> Regards
>> Partha
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From henrik.bengtsson at gmail.com  Tue Nov 22 21:14:35 2016
From: henrik.bengtsson at gmail.com (Henrik Bengtsson)
Date: Tue, 22 Nov 2016 21:14:35 +0100
Subject: [R] Memory problem
In-Reply-To: <CAFDcVCSosUVe2+=_89MSnyN5teC68QfPM9mpLTWGN+YwX+Vg0g@mail.gmail.com>
References: <CADcgpJeMB2YcvPwJD5x5hocRmCjyQevq9rtVd1WtL3ooDDOWHg@mail.gmail.com>
	<CA+vqiLEAydokFTY8GhHc2uSyeW7cfhh4fYZh1A3dLf9E-Y2U-w@mail.gmail.com>
	<CAFDcVCSosUVe2+=_89MSnyN5teC68QfPM9mpLTWGN+YwX+Vg0g@mail.gmail.com>
Message-ID: <CAFDcVCTiPWcsWPHRgzUCtqPUxAP8+L__5pjOU2zF2jjp59AkLQ@mail.gmail.com>

On Windows 32-bit I think (it's been a while) you can push it to 3 GB but
to go beyond you need to run R  on 64-bit Windows (same rule for all
software not just R). I'm pretty sure this is already documented in the R
documentation.

Henrik

On Nov 22, 2016 19:49, "Ista Zahn" <istazahn at gmail.com> wrote:

Not conveniently. Memory is cheap, you should buy more.

Best,
Ista

On Nov 22, 2016 12:19 PM, "Partha Sinha" <pnsinha68 at gmail.com> wrote:

>  I am using R 3.3.2 on win 7, 32 bit with 2gb Ram. Is it possible to use
> more than 2 Gb data set ?
>
> Regards
> Partha
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From marine.regis at hotmail.fr  Tue Nov 22 22:29:16 2016
From: marine.regis at hotmail.fr (Marine Regis)
Date: Tue, 22 Nov 2016 21:29:16 +0000
Subject: [R] GAM with the negative binomial distribution: why do predictions
 no match with original values?
Message-ID: <AM5PR0701MB2338A474BE0056E27BD32432E2B40@AM5PR0701MB2338.eurprd07.prod.outlook.com>

Hello,

>From capture data, I would like to assess the effect of longitudinal changes in proportion of forests on abundance of skunks. To test this, I built this GAM where the dependent variable is the number of unique skunks and the independent variables are the X coordinates of the centroids of trapping sites (called "X" in the GAM) and the proportion of forests within the trapping sites (called "prop_forest" in the GAM):

    mod <- gam(nb_unique ~ s(x,prop_forest), offset=log_trap_eff, family=nb(theta=NULL, link="log"), data=succ_capt_skunk, method = "REML", select = TRUE)
    summary(mod)

    Family: Negative Binomial(13.446)
    Link function: log

    Formula:
    nb_unique ~ s(x, prop_forest)

    Parametric coefficients:
                Estimate Std. Error z value Pr(>|z|)
    (Intercept) -2.02095    0.03896  -51.87   <2e-16 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    Approximate significance of smooth terms:
                       edf Ref.df Chi.sq  p-value
    s(x,prop_forest) 3.182     29  17.76 0.000102 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    R-sq.(adj) =   0.37   Deviance explained =   49%
    -REML = 268.61  Scale est. = 1         n = 58


I built a GAM  for the negative binomial family. When I use the function `predict.gam`, the predictions of capture success from the GAM and the values of capture success from original data are very different. What is the reason for differences occur?

**With GAM:**

    modPred <- predict.gam(mod, se.fit=TRUE,type="response")
    summary(modPred$fit)
       Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
     0.1026  0.1187  0.1333  0.1338  0.1419  0.1795

 **With original data:**

    summary(succ_capt_skunk$nb_unique)
       Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
      17.00   59.00   82.00   81.83  106.80  147.00

The question has already been posted on Cross validated (http://stats.stackexchange.com/questions/247347/gam-with-the-negative-binomial-distribution-why-do-predictions-no-match-with-or) without success.

Thanks a lot for your time.
Have a nice day
Marine


	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Nov 22 23:07:22 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 22 Nov 2016 14:07:22 -0800
Subject: [R] GAM with the negative binomial distribution: why do
 predictions no match with original values?
In-Reply-To: <AM5PR0701MB2338A474BE0056E27BD32432E2B40@AM5PR0701MB2338.eurprd07.prod.outlook.com>
References: <AM5PR0701MB2338A474BE0056E27BD32432E2B40@AM5PR0701MB2338.eurprd07.prod.outlook.com>
Message-ID: <CAGxFJbTT8MQvU+y5iMGBznD85ATgDa=4fqeh71Y+aJPN+LVqzA@mail.gmail.com>

Define "very different."  Sounds like a subjective opinion to me, for
which I have no response. Apparently others are similarly flummoxed.
Of course they would not in general be identical.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Nov 22, 2016 at 1:29 PM, Marine Regis <marine.regis at hotmail.fr> wrote:
> Hello,
>
> >From capture data, I would like to assess the effect of longitudinal changes in proportion of forests on abundance of skunks. To test this, I built this GAM where the dependent variable is the number of unique skunks and the independent variables are the X coordinates of the centroids of trapping sites (called "X" in the GAM) and the proportion of forests within the trapping sites (called "prop_forest" in the GAM):
>
>     mod <- gam(nb_unique ~ s(x,prop_forest), offset=log_trap_eff, family=nb(theta=NULL, link="log"), data=succ_capt_skunk, method = "REML", select = TRUE)
>     summary(mod)
>
>     Family: Negative Binomial(13.446)
>     Link function: log
>
>     Formula:
>     nb_unique ~ s(x, prop_forest)
>
>     Parametric coefficients:
>                 Estimate Std. Error z value Pr(>|z|)
>     (Intercept) -2.02095    0.03896  -51.87   <2e-16 ***
>     ---
>     Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
>     Approximate significance of smooth terms:
>                        edf Ref.df Chi.sq  p-value
>     s(x,prop_forest) 3.182     29  17.76 0.000102 ***
>     ---
>     Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
>     R-sq.(adj) =   0.37   Deviance explained =   49%
>     -REML = 268.61  Scale est. = 1         n = 58
>
>
> I built a GAM  for the negative binomial family. When I use the function `predict.gam`, the predictions of capture success from the GAM and the values of capture success from original data are very different. What is the reason for differences occur?
>
> **With GAM:**
>
>     modPred <- predict.gam(mod, se.fit=TRUE,type="response")
>     summary(modPred$fit)
>        Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>      0.1026  0.1187  0.1333  0.1338  0.1419  0.1795
>
>  **With original data:**
>
>     summary(succ_capt_skunk$nb_unique)
>        Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>       17.00   59.00   82.00   81.83  106.80  147.00
>
> The question has already been posted on Cross validated (http://stats.stackexchange.com/questions/247347/gam-with-the-negative-binomial-distribution-why-do-predictions-no-match-with-or) without success.
>
> Thanks a lot for your time.
> Have a nice day
> Marine
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Tue Nov 22 23:33:13 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 22 Nov 2016 14:33:13 -0800
Subject: [R] GAM with the negative binomial distribution: why do
	predictions no match with original values?
In-Reply-To: <AM5PR0701MB2338A474BE0056E27BD32432E2B40@AM5PR0701MB2338.eurprd07.prod.outlook.com>
References: <AM5PR0701MB2338A474BE0056E27BD32432E2B40@AM5PR0701MB2338.eurprd07.prod.outlook.com>
Message-ID: <0E3684A3-F300-4699-B4D1-09B188186403@comcast.net>


> On Nov 22, 2016, at 1:29 PM, Marine Regis <marine.regis at hotmail.fr> wrote:
> 
> Hello,
> 
>> From capture data, I would like to assess the effect of longitudinal changes in proportion of forests on abundance of skunks. To test this, I built this GAM where the dependent variable is the number of unique skunks and the independent variables are the X coordinates of the centroids of trapping sites (called "X" in the GAM) and the proportion of forests within the trapping sites (called "prop_forest" in the GAM):
> 
>    mod <- gam(nb_unique ~ s(x,prop_forest), offset=log_trap_eff, family=nb(theta=NULL, link="log"), data=succ_capt_skunk, method = "REML", select = TRUE)
>    summary(mod)
> 
>    Family: Negative Binomial(13.446)
>    Link function: log
> 
>    Formula:
>    nb_unique ~ s(x, prop_forest)
> 
>    Parametric coefficients:
>                Estimate Std. Error z value Pr(>|z|)
>    (Intercept) -2.02095    0.03896  -51.87   <2e-16 ***
>    ---
>    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
>    Approximate significance of smooth terms:
>                       edf Ref.df Chi.sq  p-value
>    s(x,prop_forest) 3.182     29  17.76 0.000102 ***
>    ---
>    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
>    R-sq.(adj) =   0.37   Deviance explained =   49%
>    -REML = 268.61  Scale est. = 1         n = 58
> 
> 
> I built a GAM  for the negative binomial family. When I use the function `predict.gam`, the predictions of capture success from the GAM and the values of capture success from original data are very different. What is the reason for differences occur?

You have an offset that is not described. And `gam` suppresses the Intercept. These would seem to be likely sources of confusion. For the best answers either on Rhelp or on CrossValidated.com you should be offering a working example. It's not our responsibility to build these for you.

I found that others had included offsets and then had questions about prediction. I haven't reviewed these candidates but perhaps you can find one in this modest listing that comes up from the MarkMail search engine:

http://markmail.org/search/?q=list%3Aorg.r-project.r-help+mgcv+gam+offset+predict

library(mgcv) 
x<-seq(0,10,length=100) 
y<-x^2+rnorm(100) 
m1<-gam(y~s(x,k=10,bs='cs')) 
m2<-gam(y~s(x,k=10,bs='cs'), offset= rep(10,100) ) 
x1<-seq(0,10,0.1) 
y1<-predict(m1,newdata=list(x=x1)) 
y2<-predict(m2,newdata=list(x=x1))

plot(x,y,ylim=c(0,100)) 
lines(x1,y1,lwd=4,col='red') 
lines(x1,y2,lwd=4,col='blue')


-- 
David.


> 
> **With GAM:**
> 
>    modPred <- predict.gam(mod, se.fit=TRUE,type="response")
>    summary(modPred$fit)
>       Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>     0.1026  0.1187  0.1333  0.1338  0.1419  0.1795
> 
> **With original data:**
> 
>    summary(succ_capt_skunk$nb_unique)
>       Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>      17.00   59.00   82.00   81.83  106.80  147.00
> 
> The question has already been posted on Cross validated (http://stats.stackexchange.com/questions/247347/gam-with-the-negative-binomial-distribution-why-do-predictions-no-match-with-or) without success.
> 
> Thanks a lot for your time.
> Have a nice day
> Marine
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From cadeb at usgs.gov  Tue Nov 22 23:39:18 2016
From: cadeb at usgs.gov (Cade, Brian)
Date: Tue, 22 Nov 2016 15:39:18 -0700
Subject: [R] GAM with the negative binomial distribution: why do
 predictions no match with original values?
In-Reply-To: <AM5PR0701MB2338A474BE0056E27BD32432E2B40@AM5PR0701MB2338.eurprd07.prod.outlook.com>
References: <AM5PR0701MB2338A474BE0056E27BD32432E2B40@AM5PR0701MB2338.eurprd07.prod.outlook.com>
Message-ID: <CAM5M9BRMbBxHPXH9yeDHYdiaHuDkFxRShWwyiA9rEV5Zjo7qHQ@mail.gmail.com>

Well part of the issue is that the negative binomial estimates are for
means and they can differ a fair bit from the raw counts, but I'm also
guessing that part of the issue is that the offset may not be accounted for
with the predict.gam() function.

Brian

Brian S. Cade, PhD

U. S. Geological Survey
Fort Collins Science Center
2150 Centre Ave., Bldg. C
Fort Collins, CO  80526-8818

email:  cadeb at usgs.gov <brian_cade at usgs.gov>
tel:  970 226-9326


On Tue, Nov 22, 2016 at 2:29 PM, Marine Regis <marine.regis at hotmail.fr>
wrote:

> Hello,
>
> >From capture data, I would like to assess the effect of longitudinal
> changes in proportion of forests on abundance of skunks. To test this, I
> built this GAM where the dependent variable is the number of unique skunks
> and the independent variables are the X coordinates of the centroids of
> trapping sites (called "X" in the GAM) and the proportion of forests within
> the trapping sites (called "prop_forest" in the GAM):
>
>     mod <- gam(nb_unique ~ s(x,prop_forest), offset=log_trap_eff,
> family=nb(theta=NULL, link="log"), data=succ_capt_skunk, method = "REML",
> select = TRUE)
>     summary(mod)
>
>     Family: Negative Binomial(13.446)
>     Link function: log
>
>     Formula:
>     nb_unique ~ s(x, prop_forest)
>
>     Parametric coefficients:
>                 Estimate Std. Error z value Pr(>|z|)
>     (Intercept) -2.02095    0.03896  -51.87   <2e-16 ***
>     ---
>     Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
>     Approximate significance of smooth terms:
>                        edf Ref.df Chi.sq  p-value
>     s(x,prop_forest) 3.182     29  17.76 0.000102 ***
>     ---
>     Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
>     R-sq.(adj) =   0.37   Deviance explained =   49%
>     -REML = 268.61  Scale est. = 1         n = 58
>
>
> I built a GAM  for the negative binomial family. When I use the function
> `predict.gam`, the predictions of capture success from the GAM and the
> values of capture success from original data are very different. What is
> the reason for differences occur?
>
> **With GAM:**
>
>     modPred <- predict.gam(mod, se.fit=TRUE,type="response")
>     summary(modPred$fit)
>        Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>      0.1026  0.1187  0.1333  0.1338  0.1419  0.1795
>
>  **With original data:**
>
>     summary(succ_capt_skunk$nb_unique)
>        Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>       17.00   59.00   82.00   81.83  106.80  147.00
>
> The question has already been posted on Cross validated (
> http://stats.stackexchange.com/questions/247347/gam-with-
> the-negative-binomial-distribution-why-do-predictions-no-match-with-or)
> without success.
>
> Thanks a lot for your time.
> Have a nice day
> Marine
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Tue Nov 22 23:52:45 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 22 Nov 2016 23:52:45 +0100
Subject: [R] GAM with the negative binomial distribution: why do
	predictions no match with original values?
In-Reply-To: <CAGxFJbTT8MQvU+y5iMGBznD85ATgDa=4fqeh71Y+aJPN+LVqzA@mail.gmail.com>
References: <AM5PR0701MB2338A474BE0056E27BD32432E2B40@AM5PR0701MB2338.eurprd07.prod.outlook.com>
	<CAGxFJbTT8MQvU+y5iMGBznD85ATgDa=4fqeh71Y+aJPN+LVqzA@mail.gmail.com>
Message-ID: <B87E98BB-8D00-4933-8586-B6CF7638C74E@gmail.com>


> On 22 Nov 2016, at 23:07 , Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> Define "very different."  Sounds like a subjective opinion to me, for
> which I have no response. Apparently others are similarly flummoxed.
> Of course they would not in general be identical.

Er? I don't see much reason to disagree that a range 0.10-0.18 is different from 17-147.

However, other bits of information are missing: We don't know which gam() function is being used (to my knowledge there is one in package gam but also one in mgcv). We don't have the data, so we cannot reproduce and try to find the root of the problem. 

Offhand, it looks like the predict.gam() function is misbehaving, which could have something to do with the offset term and/or the nb dispersion parameter. On a hunch, does anything change if you use

nb_unique ~ s(x,prop_forest) + offset(log_trap_eff)

instead of the offset= argument? And, by the way, does fitted(mod,...) change anything?

-pd 

> 
> Cheers,
> Bert
> 
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
> On Tue, Nov 22, 2016 at 1:29 PM, Marine Regis <marine.regis at hotmail.fr> wrote:
>> Hello,
>> 
>>> From capture data, I would like to assess the effect of longitudinal changes in proportion of forests on abundance of skunks. To test this, I built this GAM where the dependent variable is the number of unique skunks and the independent variables are the X coordinates of the centroids of trapping sites (called "X" in the GAM) and the proportion of forests within the trapping sites (called "prop_forest" in the GAM):
>> 
>>    mod <- gam(nb_unique ~ s(x,prop_forest), offset=log_trap_eff, family=nb(theta=NULL, link="log"), data=succ_capt_skunk, method = "REML", select = TRUE)
>>    summary(mod)
>> 
>>    Family: Negative Binomial(13.446)
>>    Link function: log
>> 
>>    Formula:
>>    nb_unique ~ s(x, prop_forest)
>> 
>>    Parametric coefficients:
>>                Estimate Std. Error z value Pr(>|z|)
>>    (Intercept) -2.02095    0.03896  -51.87   <2e-16 ***
>>    ---
>>    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>> 
>>    Approximate significance of smooth terms:
>>                       edf Ref.df Chi.sq  p-value
>>    s(x,prop_forest) 3.182     29  17.76 0.000102 ***
>>    ---
>>    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>> 
>>    R-sq.(adj) =   0.37   Deviance explained =   49%
>>    -REML = 268.61  Scale est. = 1         n = 58
>> 
>> 
>> I built a GAM  for the negative binomial family. When I use the function `predict.gam`, the predictions of capture success from the GAM and the values of capture success from original data are very different. What is the reason for differences occur?
>> 
>> **With GAM:**
>> 
>>    modPred <- predict.gam(mod, se.fit=TRUE,type="response")
>>    summary(modPred$fit)
>>       Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>>     0.1026  0.1187  0.1333  0.1338  0.1419  0.1795
>> 
>> **With original data:**
>> 
>>    summary(succ_capt_skunk$nb_unique)
>>       Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>>      17.00   59.00   82.00   81.83  106.80  147.00
>> 
>> The question has already been posted on Cross validated (http://stats.stackexchange.com/questions/247347/gam-with-the-negative-binomial-distribution-why-do-predictions-no-match-with-or) without success.
>> 
>> Thanks a lot for your time.
>> Have a nice day
>> Marine
>> 
>> 
>>        [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From marine.regis at hotmail.fr  Wed Nov 23 01:24:18 2016
From: marine.regis at hotmail.fr (Marine Regis)
Date: Wed, 23 Nov 2016 00:24:18 +0000
Subject: [R] GAM with the negative binomial distribution: why do
 predictions no match with original values?
In-Reply-To: <B87E98BB-8D00-4933-8586-B6CF7638C74E@gmail.com>
References: <AM5PR0701MB2338A474BE0056E27BD32432E2B40@AM5PR0701MB2338.eurprd07.prod.outlook.com>
	<CAGxFJbTT8MQvU+y5iMGBznD85ATgDa=4fqeh71Y+aJPN+LVqzA@mail.gmail.com>,
	<B87E98BB-8D00-4933-8586-B6CF7638C74E@gmail.com>
Message-ID: <AM5PR0701MB2338C813C8EE72DBBA5982CFE2B70@AM5PR0701MB2338.eurprd07.prod.outlook.com>

Thanks a lot for your answers.
Peter: sorry, here is the missing information:

  *   I use the function gam() of the package ?mgcv?
  *   Yes, the output changes when I use offset(log_trap_eff) instead of offset=log_trap_eff. By using offset(log_trap_eff), the output is more coherent with the observed values. Here are the new predictions:
> summary(mod$fit)
Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
10.01   68.14   85.71   83.16  101.00  130.20


  *   I have tried to create a reproductive example to show the difference between offset(log_trap_eff) and offset=log_trap_eff.
nb_unique <- rnegbin(58, mu=82, theta=13.446)
x <- runif(58,min=-465300,max=435200)
prop_forest <- runif(58,min=0,max=1)
log_trap_eff <- runif(58,min=4,max=6)

With offset=log_trap_eff:

mod1 <- gam(nb_unique ~ s(x,prop_forest), offset=log_trap_eff, family=nb(theta=NULL, link="log"), method = "REML", select = TRUE)

> mod1Pred <- predict.gam(mod1, se.fit=TRUE, type="response")

> summary(mod1Pred$fit)

   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.

 0.5852  0.5852  0.5852  0.5852  0.5852  0.5852


With offset(log_trap_eff):
mod2 <- gam(nb_unique ~ s(x,prop_forest) + offset(log_trap_eff), family=nb(theta=NULL, link="log"), method = "REML", select = TRUE)

> mod2Pred <- predict.gam(mod2, se.fit=TRUE, type="response")

> summary(mod2Pred$fit)

   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.

  32.03   61.18   97.20  112.20  165.00  226.00



Value range of observed data:

> summary(nb_unique)

   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.

  43.00   67.00   81.00   84.16   92.75  153.00


  *   By using fitted(mod), I obtain NULL.
I am a novice in GAMs. So, I don?t know why the results are different between models with offset=argument and offset().
Thanks a lot for your help.
Have a nice day
Marine



________________________________
De : peter dalgaard <pdalgd at gmail.com>
Envoy? : mardi 22 novembre 2016 23:52
? : Bert Gunter
Cc : Marine Regis; r-help at r-project.org
Objet : Re: [R] GAM with the negative binomial distribution: why do predictions no match with original values?


> On 22 Nov 2016, at 23:07 , Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
> Define "very different."  Sounds like a subjective opinion to me, for
> which I have no response. Apparently others are similarly flummoxed.
> Of course they would not in general be identical.

Er? I don't see much reason to disagree that a range 0.10-0.18 is different from 17-147.

However, other bits of information are missing: We don't know which gam() function is being used (to my knowledge there is one in package gam but also one in mgcv). We don't have the data, so we cannot reproduce and try to find the root of the problem.

Offhand, it looks like the predict.gam() function is misbehaving, which could have something to do with the offset term and/or the nb dispersion parameter. On a hunch, does anything change if you use

nb_unique ~ s(x,prop_forest) + offset(log_trap_eff)

instead of the offset= argument? And, by the way, does fitted(mod,...) change anything?

-pd

>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Tue, Nov 22, 2016 at 1:29 PM, Marine Regis <marine.regis at hotmail.fr> wrote:
>> Hello,
>>
>>> From capture data, I would like to assess the effect of longitudinal changes in proportion of forests on abundance of skunks. To test this, I built this GAM where the dependent variable is the number of unique skunks and the independent variables are the X coordinates of the centroids of trapping sites (called "X" in the GAM) and the proportion of forests within the trapping sites (called "prop_forest" in the GAM):
>>
>>    mod <- gam(nb_unique ~ s(x,prop_forest), offset=log_trap_eff, family=nb(theta=NULL, link="log"), data=succ_capt_skunk, method = "REML", select = TRUE)
>>    summary(mod)
>>
>>    Family: Negative Binomial(13.446)
>>    Link function: log
>>
>>    Formula:
>>    nb_unique ~ s(x, prop_forest)
>>
>>    Parametric coefficients:
>>                Estimate Std. Error z value Pr(>|z|)
>>    (Intercept) -2.02095    0.03896  -51.87   <2e-16 ***
>>    ---
>>    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>
>>    Approximate significance of smooth terms:
>>                       edf Ref.df Chi.sq  p-value
>>    s(x,prop_forest) 3.182     29  17.76 0.000102 ***
>>    ---
>>    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>
>>    R-sq.(adj) =   0.37   Deviance explained =   49%
>>    -REML = 268.61  Scale est. = 1         n = 58
>>
>>
>> I built a GAM  for the negative binomial family. When I use the function `predict.gam`, the predictions of capture success from the GAM and the values of capture success from original data are very different. What is the reason for differences occur?
>>
>> **With GAM:**
>>
>>    modPred <- predict.gam(mod, se.fit=TRUE,type="response")
>>    summary(modPred$fit)
>>       Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>>     0.1026  0.1187  0.1333  0.1338  0.1419  0.1795
>>
>> **With original data:**
>>
>>    summary(succ_capt_skunk$nb_unique)
>>       Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>>      17.00   59.00   82.00   81.83  106.80  147.00
>>
>> The question has already been posted on Cross validated (http://stats.stackexchange.com/questions/247347/gam-with-the-negative-binomial-distribution-why-do-predictions-no-match-with-or) without success.
[http://cdn.sstatic.net/Sites/stats/img/apple-touch-icon at 2.png?v=344f57aa10cc&a]<http://stats.stackexchange.com/questions/247347/gam-with-the-negative-binomial-distribution-why-do-predictions-no-match-with-or>

GAM with the negative binomial distribution: why do predictions no match with original values?<http://stats.stackexchange.com/questions/247347/gam-with-the-negative-binomial-distribution-why-do-predictions-no-match-with-or>
stats.stackexchange.com
>From capture data, I would like to assess the effect of longitudinal changes in proportion of forests on abundance of skunks. To test this, I built this GAM where the dependent variable is the numb...



>>
>> Thanks a lot for your time.
>> Have a nice day
>> Marine
>>
>>
>>        [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help

thz.ch/mailman/listinfo/r-help>
stat.ethz.ch
The main R mailing list, for announcements about the development of R and the availability of new code, questions and answers about problems and solutions using R ...



>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help

thz.ch/mailman/listinfo/r-help>
stat.ethz.ch
The main R mailing list, for announcements about the development of R and the availability of new code, questions and answers about problems and solutions using R ...



> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com










	[[alternative HTML version deleted]]


From cbenjami at BTBOCES.ORG  Wed Nov 23 12:54:02 2016
From: cbenjami at BTBOCES.ORG (Courtney Benjamin)
Date: Wed, 23 Nov 2016 11:54:02 +0000
Subject: [R] Further Subsetting of Data for Log. Reg. Results in qr.default
	Error
Message-ID: <1479902034098.6655@BTBOCES.ORG>

Hello R Experts,

In further subsetting data within a logistic regression in the survey package, I am getting a qr.default error.

Below is an example of the original model that runs correctly, but when I subset the data further to look at students of a particular curriculum concentration, the qr.default error occurs.  I thought it may have been related to converting the F1RTRCC variable into a factor for use in the original model; I went back and restored that variable to its original form and it didn't help.

Any guidance is greatly appreciated.


library(RCurl)
library(survey)

data <- getURL("https://raw.githubusercontent.com/cbenjamin1821/careertech-ed/master/elsq1adj.csv")
elsq1ch <- read.csv(text = data)

#Specifying the svyrepdesign object which applies the BRR weights
elsq1ch_brr<-svrepdesign(variables = elsq1ch[,1:16], repweights = elsq1ch[,18:217], weights = elsq1ch[,17], combined.weights = TRUE, type = "BRR")
elsq1ch_brr

##Resetting baseline levels for predictors
elsq1ch_brr <- update( elsq1ch_brr , F1HIMATH = relevel(F1HIMATH,"PreAlg or Less") )
elsq1ch_brr <- update( elsq1ch_brr , BYINCOME = relevel(BYINCOME,"0-25K") )
elsq1ch_brr <- update( elsq1ch_brr , F1RACE = relevel(F1RACE,"White") )
elsq1ch_brr <- update( elsq1ch_brr , F1SEX = relevel(F1SEX,"Male") )
elsq1ch_brr <- update( elsq1ch_brr , F1RTRCC = relevel(F1RTRCC,"Academic") )

#Log. Reg. model-all curric. concentrations including F1RTRCC as a predictor
allCC <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+F1RGPP2+F1HIMATH+F1RTRCC,family="binomial",design=elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1,na.action=na.omit)
summary(allCC)

##CTE Log. Reg. model that is resulting in the qr.default error
CTE <- svyglm(formula=F3ATTAINB~F1PARED+BYINCOME+F1RACE+F1SEX+F1RGPP2+F1HIMATH,family="binomial",design=elsq1ch_brr,subset=BYSCTRL==1&G10COHRT==1&F1RTRCC=="Academic",na.action=na.omit)
summary(CTE)



Courtney Benjamin

Broome-Tioga BOCES

Automotive Technology II Teacher

Located at Gault Toyota

Doctoral Candidate-Educational Theory & Practice

State University of New York at Binghamton

cbenjami at btboces.org<mailto:cbenjami at btboces.org>

607-763-8633

	[[alternative HTML version deleted]]


From mickyleo at gmail.com  Wed Nov 23 12:54:12 2016
From: mickyleo at gmail.com (Michela Leone)
Date: Wed, 23 Nov 2016 12:54:12 +0100
Subject: [R] How to extract parameters from train in caret package
Message-ID: <CAMht388=z9HgebpJFk-M4L5vdVC_yHRchkAf1LhgjV3B1Bjrjg@mail.gmail.com>

 Dear R-users,
   I need to extract coefficients  from the  final model after running
train function of caret package.

I need to have something like "summary" used after running a regression
model
in this case  from the best model.

below my  script   as explanation

data(Sonar)
set.seed(107)
 inTrain <- createDataPartition(y = Sonar$Class,  p = .75,  list = FALSE)

training <- Sonar[ inTrain,]
testing <- Sonar[-inTrain,]


 plsFit <- train(Class ~ .,
                   data = training,
                  method = "pls",
                  tuneLength = 15,
                  preProc = c("center", "scale"))

What I need is a summary  which looks like


Call:
glm(formula = Class ~ ., family = "binomial", data = training)

Deviance Residuals:
       Min          1Q      Median          3Q         Max
-2.418e-05  -2.110e-08  -2.110e-08   2.110e-08   2.697e-05

Coefficients:
              Estimate Std. Error z value Pr(>|z|)
(Intercept)  1.081e+02  2.668e+05   0.000    1.000
V1           7.235e+02  4.390e+06   0.000    1.000
V2          -1.024e+03  2.282e+06   0.000    1.000
V3           1.113e+03  2.735e+06   0.000    1.000
V4          -7.072e+02  1.828e+06   0.000    1.000
V5          -1.542e+01  1.534e+06   0.000    1.000
V6          -1.983e+02  1.438e+06   0.000    1.000
V7           2.543e+02  1.465e+06   0.000    1.000
V8           3.507e+02  8.199e+05   0.000    1.000
V9          -4.010e+02  8.517e+05   0.000    1.000
V10          4.317e+01  9.660e+05   0.000    1.000
V11          1.364e+02  1.800e+06   0.000    1.000
V12         -4.606e+02  2.350e+06   0.000    1.000
V13          1.746e+02  1.246e+06   0.000    1.000


    Null deviance: 2.1688e+02  on 156  degrees of freedom
Residual deviance: 1.0630e-08  on  96  degrees of freedom
AIC: 122

Number of Fisher Scoring iterations: 25



Many thanks for your help ! Michela Leone

	[[alternative HTML version deleted]]


From f_j_rod at hotmail.com  Wed Nov 23 14:36:13 2016
From: f_j_rod at hotmail.com (Frank S.)
Date: Wed, 23 Nov 2016 13:36:13 +0000
Subject: [R] Checking specific pair of codes within subject
Message-ID: <AM5PR0402MB2689BC1B312B5827B5D4CC18BAB70@AM5PR0402MB2689.eurprd04.prod.outlook.com>

Dear all,
I come with an old question I had, which I have also solved but the last point. I've a data.table as follows:

dt <- data.table(id = rep(1:2, c(5, 2)),
     date = as.Date(rep(c("2005-07-25", "2006-09-17", "2001-10-19"), c(3, 2, 2))),
      sex = as.factor( rep(c(0, 1), c(5, 2))),
      hosp = as.factor(c('A11', 'C11', 'R1000', 'W1000', 'C11', 'B22', 'D22')),
      check = as.factor(rep(c(T, F, T), c(3, 2, 2))))

By avoiding splitting data table, I would like to do: If check = T, keep all rows by unique pairs (id, date) excepting those
where hosp %in% c("C11", "D22"). If check = F, keep all rows but changing "C11" to "A11". So desired otput is:

    id             date  sex  hosp
1:  1 2005-07-25   0   A11
2:  1 2005-07-25   0 R1000
3:  1 2006-09-17   0 W1000
4:  1 2006-09-17   0   A11
5:  2 2001-10-19   1   B22


I've tried (without result):

dtnew <- dt[, {
  if (check == TRUE) {
     idx <-  which(!hosp %in% c("C11", "D22"))
     res <- list(hosp = hosp[idx], sex = sex[idx], check = check[idx])
  } else {
     if (levels(hosp) == "C11"){
         idx <- which(id)
         res <- list(hosp = "A22", sex = sex[idx], check = check[idx])
     } else {
         idx <- which(id)
         res <- list(hosp = hosp[idx], sex = sex[idx], check = check[idx])
     }
  }
  res
}, by = list(id, date)]

I'm aware it may not be the most efficient way. Thanks for any help!!


Frank S.

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Wed Nov 23 16:55:45 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 23 Nov 2016 07:55:45 -0800
Subject: [R] The code itself disappears after starting to execute the
	for loop
In-Reply-To: <2BFA80AA-A18B-4A52-A96B-455AE7BB9063@gmail.com>
References: <59BE1118-9D4B-4298-B605-9BADFBC43E88@gmail.com>
	<CAAxdm-5FWvfnpwEJtr5eNw0AKv+cjOfgXfQKTLSmYMkk1RgPcA@mail.gmail.com>
	<2BFA80AA-A18B-4A52-A96B-455AE7BB9063@gmail.com>
Message-ID: <CAGxFJbTxXbS-+Rki-143izvQEo1JZjSy+8jY2qNFj_RKHppCOQ@mail.gmail.com>

In addition to Jim's comments, which you have not yet satisfactorily
addressed (buffering in GUI??),

1. Show your code!

2. Show ouput of sessionInfo()

3. Upgrade to the latest R version maybe

4. Perhaps write to package maintainer (see ?maintainer) if nothing or
no one helps.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Nov 22, 2016 at 10:05 AM, Maram SAlem <marammagdysalem at gmail.com> wrote:
> Thanks for helping Jim.
>
> I'm actually using the pbapply function together with the print function within a loop. In earlier versions, the progress bar and the output of the print function used to appear after each iteration of the loop. But with the 3.3.1. Version nothing appears, instead the console turns white and the cursor turns blue ( busy) and I know nothing about the progress of the running code.
>
> I just want to see the bar and the output of the print function as I used to, any help?
>
> Thanks in advance.
> Maram Salem
>
>
>
> Sent from my iPhone
>
>> On Nov 3, 2016, at 8:30 PM, jim holtman <jholtman at gmail.com> wrote:
>>
>> A little more information would help.  How exactly are out creating the output to the console?  Are you using 'print', 'cat' or something else?  Do you have buffered output checked on the GUI (you probably don't want it checked or you output will be delayed till the buffer is full -- this might be the cause of your problem.
>>
>>
>> Jim Holtman
>> Data Munger Guru
>>
>> What is the problem that you are trying to solve?
>> Tell me what you want to do, not how you want to do it.
>>
>>> On Thu, Nov 3, 2016 at 1:55 PM, Maram SAlem <marammagdysalem at gmail.com> wrote:
>>> Hi all,
>>>
>>> I've a question concerning the R 3.3.1 version. I have a long code that I used to run on versions earlier to the 3.3.1 version, and when I copied the code to the R console, I can still see the code while the loop is executing , along with the output printed after each iteration of the loop.
>>>
>>> Now, on the 3.3.1 version, after I copy the code to the console, it disappears and I only see the printed output of only one iteration at a time, that is, after the first iteration the printed output disappears ( though it's only 6 lines, just giving me some guidance, not a long output).
>>> This is causing me some problems, so I don't know if there is a general option for R that enables me to still see the code and the output of all the iterations till the loop is over, as was the case with earlier R versions.
>>>
>>> I didn't include the code as it's a long one.
>>>
>>> Thanks a lot in advance,
>>>
>>> Maram
>>>
>>>
>>> Sent from my iPhone
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marammagdysalem at gmail.com  Wed Nov 23 17:22:26 2016
From: marammagdysalem at gmail.com (Maram SAlem)
Date: Wed, 23 Nov 2016 18:22:26 +0200
Subject: [R] The code itself disappears after starting to execute the
	for loop
In-Reply-To: <CAGxFJbTxXbS-+Rki-143izvQEo1JZjSy+8jY2qNFj_RKHppCOQ@mail.gmail.com>
References: <59BE1118-9D4B-4298-B605-9BADFBC43E88@gmail.com>
	<CAAxdm-5FWvfnpwEJtr5eNw0AKv+cjOfgXfQKTLSmYMkk1RgPcA@mail.gmail.com>
	<2BFA80AA-A18B-4A52-A96B-455AE7BB9063@gmail.com>
	<CAGxFJbTxXbS-+Rki-143izvQEo1JZjSy+8jY2qNFj_RKHppCOQ@mail.gmail.com>
Message-ID: <2FC08618-B485-43D0-9103-A816172D41A6@gmail.com>

Thanks a lot Bert , will check out your suggestions.

I've unchecked the buffer output option in GUI but still have the same problem.

Thanks for your time and concern.

Maram Salem 

Sent from my iPhone

> On Nov 23, 2016, at 5:55 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> In addition to Jim's comments, which you have not yet satisfactorily
> addressed (buffering in GUI??),
> 
> 1. Show your code!
> 
> 2. Show ouput of sessionInfo()
> 
> 3. Upgrade to the latest R version maybe
> 
> 4. Perhaps write to package maintainer (see ?maintainer) if nothing or
> no one helps.
> 
> Cheers,
> Bert
> 
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
>> On Tue, Nov 22, 2016 at 10:05 AM, Maram SAlem <marammagdysalem at gmail.com> wrote:
>> Thanks for helping Jim.
>> 
>> I'm actually using the pbapply function together with the print function within a loop. In earlier versions, the progress bar and the output of the print function used to appear after each iteration of the loop. But with the 3.3.1. Version nothing appears, instead the console turns white and the cursor turns blue ( busy) and I know nothing about the progress of the running code.
>> 
>> I just want to see the bar and the output of the print function as I used to, any help?
>> 
>> Thanks in advance.
>> Maram Salem
>> 
>> 
>> 
>> Sent from my iPhone
>> 
>>> On Nov 3, 2016, at 8:30 PM, jim holtman <jholtman at gmail.com> wrote:
>>> 
>>> A little more information would help.  How exactly are out creating the output to the console?  Are you using 'print', 'cat' or something else?  Do you have buffered output checked on the GUI (you probably don't want it checked or you output will be delayed till the buffer is full -- this might be the cause of your problem.
>>> 
>>> 
>>> Jim Holtman
>>> Data Munger Guru
>>> 
>>> What is the problem that you are trying to solve?
>>> Tell me what you want to do, not how you want to do it.
>>> 
>>>> On Thu, Nov 3, 2016 at 1:55 PM, Maram SAlem <marammagdysalem at gmail.com> wrote:
>>>> Hi all,
>>>> 
>>>> I've a question concerning the R 3.3.1 version. I have a long code that I used to run on versions earlier to the 3.3.1 version, and when I copied the code to the R console, I can still see the code while the loop is executing , along with the output printed after each iteration of the loop.
>>>> 
>>>> Now, on the 3.3.1 version, after I copy the code to the console, it disappears and I only see the printed output of only one iteration at a time, that is, after the first iteration the printed output disappears ( though it's only 6 lines, just giving me some guidance, not a long output).
>>>> This is causing me some problems, so I don't know if there is a general option for R that enables me to still see the code and the output of all the iterations till the loop is over, as was the case with earlier R versions.
>>>> 
>>>> I didn't include the code as it's a long one.
>>>> 
>>>> Thanks a lot in advance,
>>>> 
>>>> Maram
>>>> 
>>>> 
>>>> Sent from my iPhone
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>>        [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From andersonaed at gmail.com  Wed Nov 23 15:09:52 2016
From: andersonaed at gmail.com (Anderson Eduardo)
Date: Wed, 23 Nov 2016 11:09:52 -0300
Subject: [R] variable selection problem
Message-ID: <CAEPvLrwS9q3RZ9bkC0dQzyfGB5+qMJoHtmWPQPWSpNX1uUnPbw@mail.gmail.com>

Hello

I am trying to run vignette
<https://cran.r-project.org/web/packages/MaxentVariableSelection/vignettes/MaxentVariableSelection.pdf>
example for the MaxentVariableSelection package, but something wrong is
happening. I can't figure out.

Here is the code:

maxentPath = ("/home/anderson/R/x86_64-pc-linux-gnu-library/3.3/dismo/
java/maxent.jar")
gridfolder <- ("/home/anderson/Downloads/BioOracle_9090RV")
occurrencelocations <- system.file("extdata", "Occurrencedata.csv",package="
MaxentVariableSelection")
backgroundlocations <- system.file("extdata", "Backgrounddata.csv",package="
MaxentVariableSelection")
additionalargs="nolinear noquadratic noproduct nothreshold noautofeature"
contributionthreshold <- 5
correlationthreshold <- 0.9
betamultiplier=seq(2,6,0.5)

VariableSelection(maxent,
                  outdir,
                  gridfolder,
                  occurrencelocations,
                  backgroundlocations,
                  additionalargs,
                  contributionthreshold,
                  correlationthreshold,
                  betamultiplier
                  )

Aand the error message:

> VariableSelection(maxent,
+                   outdir,
+                   gridfolder,
+                   occurrencelocations,
+                   backgroundlocations,
+                   additionalargs,
+                   contributionthreshold,
+                   correlationthreshold,
+                   betamultiplier
+                   )
-----------------------
Choosing betamultiplier  2

 Number of remaining variables 4
Testing variable contributions...
Calculating average AUC values from 10 maxent models...
arguments 'show.output.on.console', 'minimized' and 'invisible' are for
Windows only
Error in as.vector(x, "character") :
  cannot coerce type 'closure' to vector of type 'character'


I have not found the solution in blogs and online forums and I would ask
earnestly the help of the members of this forum.

Thanks in advance.

Anderson A. Eduardo
------------------------------------------------------------
------------------
Lattes <http://lattes.cnpq.br/3826166230581311> | Researcher ID
<http://orcid.org/0000-0001-8045-8043> | Google Acad?mico
<https://scholar.google.com.br/citations?user=oOUjq9IAAAAJ&hl=pt-BR> | Site
<http://andersonaireseduardo.xpg.uol.com.br/>
------------------------------------------------------------
------------------

	[[alternative HTML version deleted]]


From hagen804 at yahoo.de  Wed Nov 23 12:51:35 2016
From: hagen804 at yahoo.de (Falk Hildebrand)
Date: Wed, 23 Nov 2016 11:51:35 +0000 (UTC)
Subject: [R] browser() pre 3.1 behaviour
References: <1696348476.491175.1479901895455.ref@mail.yahoo.com>
Message-ID: <1696348476.491175.1479901895455@mail.yahoo.com>

Dear R help list,I have been using the "browser()" function for a long time now to debug my function code. However, since 3.1 (I think) the behaviour has changed that browser() (or the debug interface) is being called also when a loop is being executed (copy pasted to the R interface). So I have to cancel this browser or confirm each iteration of the loop. And this I believe also extends to various other calls. In the end I would like to set up my R to behave like it did in R <= 3.0. This has been so bothersome, that I was sticking with old R versions, but now some packages are no longer supported in these old versions.
How to set up browser() to follow the old behaviour??
I have been looking for some time now to find an answer to this on the internet, but was so far unsuccessful, I would be grateful for any help.best,?Falk Hildebrand
PS: I think this is the change that occured in 3.1:
[R] R 3.1.0 is released

  
|  
|   |  
[R] R 3.1.0 is released
   |  |

  |

 
 DEBUGGING:

   * The behaviour of the code browser has been made more consistent,
     in part following the suggestions in PR#14985.

   * Calls to browser() are now consistent with calls to the browser
     triggered by debug(), in that Enter will default to n rather than
     c.

   * A new browser command s has been added, to "step into" function
     calls.

   * A new browser command f has been added, to "finish" the current
     loop or function.

   * Within the browser, the command help will display a short list of
     available commands.
	[[alternative HTML version deleted]]


From lordpreetam at gmail.com  Wed Nov 23 13:09:48 2016
From: lordpreetam at gmail.com (Preetam Pal)
Date: Wed, 23 Nov 2016 17:39:48 +0530
Subject: [R] Unable to Install POT Package for R 3.1.0
Message-ID: <CAHVFrXHs46YyT6YNB=bxVa61LwOt3s1NufpWrYxjog_zy1pUFg@mail.gmail.com>

Hi, I am trying to install the package POT for R* version 3.1.0* (spring
dance), using:

*install.packages("POT", repos="http://R-Forge.R-project.org
<http://R-Forge.R-project.org>")*
*( link <https://r-forge.r-project.org/R/?group_id=76> )*

*But I am getting the following error:*


*package ?POT? is available as a source package but not as a binaryWarning
in install.packages :  package ?POT? is not available (for R version 3.1.0)*


Can anyone suggest how I can get it working please?
I need it for Peaks-Over-Threshold analysis under extreme value theory. I
am trying to make use of functions mentioned in this link ( link2
<http://benz.nchu.edu.tw/~finmyc/POT_guide.pdf> ).Thanks.

Regards,
Preetam

	[[alternative HTML version deleted]]


From mattered91 at gmail.com  Wed Nov 23 17:08:38 2016
From: mattered91 at gmail.com (Luke Skywalker)
Date: Wed, 23 Nov 2016 17:08:38 +0100
Subject: [R] Chi2 algorithm - R
Message-ID: <CAH0KQ1A9L7qR+HRERUjZARnYYkpHx_3Yo5unK6fFAvRha=KM8w@mail.gmail.com>

Good evening,

I'm encountering a different kind of discretization with respect to the
1997 Liu and Setiono's one descripted in their papers, using Chi2 algorithm
for feature selection with discretization.

As stated in R documentation (discretization - R (from CRAN)
<https://cran.r-project.org/web/packages/discretization/discretization.pdf>),
R package discretizion offers the function Chi2, which comes to life in the
following papers:

Liu, H. and Setiono, R. (1995). Chi2: Feature selection and discretization
of numeric attributes, Tools with Artificial Intelligence, 388?391.

Liu, H. and Setiono, R. (1997). Feature selection and discretization, IEEE
transactions on knowledge and data engineering, Vol.9, no.4, 642?645.

I wrote the following R programming language code, in which I have set
alpha and delta equal to the ones set in the papers above. Finally, the
following code prints out the discretized dataframe. I used Iris dataframe,
as in one of the examples in the two papers. The first paper above states
that alfa = 0.5 and delta = 5%, and that "the originally odd numbered data
are selected for training (75 patterns) and rest for testing (75
patterns)". With this asset, Sepal attributes should be removed.

library(discretization)
data(iris)
df1 <- iris[FALSE,]for(i in 1:nrow(iris)){
    if(i %% 2 != 0){
        df1 <- rbind(df1, iris[i,])
    }}
chi2(df1, alp=0.5, del=0.05)$Disc.data

The point is that, observing the dataframe printed out by the last
instruction, you can see that no attribute is removed. The discretized data
frame still have 4 attributes discretized: if I correctly understood the
above papers, Sepal Length and Sepal Width should have been both
discretized in just one interval by Chi2 algorithm.

I have posted a question here: http://stats.stackexchange.com/questions/
247499/why-does-not-r-chi2-algorithm-discretize-in-the-
same-manner-as-in-the-paper-by-l?noredirect=1#comment470974_247499.


Moreover, it's really hard to understand the cut points that Chi2 algorithm
implemented in R makes. For example:

res <- chi2(iris, 0.5, 0.05)

cut(iris$Sepal.Length, res$cutp, labels=FALSE) is different from
res$Disc.data$Sepal.Length

Help me understand, please

Best regards

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Wed Nov 23 21:29:59 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 23 Nov 2016 15:29:59 -0500
Subject: [R] browser() pre 3.1 behaviour
In-Reply-To: <1696348476.491175.1479901895455@mail.yahoo.com>
References: <1696348476.491175.1479901895455.ref@mail.yahoo.com>
	<1696348476.491175.1479901895455@mail.yahoo.com>
Message-ID: <f9841ea8-045f-9143-fb54-bfd959781670@gmail.com>

On 23/11/2016 6:51 AM, Falk Hildebrand via R-help wrote:
> Dear R help list,I have been using the "browser()" function for a long time now to debug my function code. However, since 3.1 (I think) the behaviour has changed that browser() (or the debug interface) is being called also when a loop is being executed (copy pasted to the R interface).

This doesn't really parse for me.  Could you please post detailed 
instructions for what you're doing and describe what you're seeing?

Duncan Murdoch
>   So I have to cancel this browser or confirm each iteration of the loop. And this I believe also extends to various other calls. In the end I would like to set up my R to behave like it did in R <= 3.0. This has been so bothersome, that I was sticking with old R versions, but now some packages are no longer supported in these old versions.
> How to set up browser() to follow the old behaviour??
> I have been looking for some time now to find an answer to this on the internet, but was so far unsuccessful, I would be grateful for any help.best, Falk Hildebrand
> PS: I think this is the change that occured in 3.1:
> [R] R 3.1.0 is released
>
>    
> |
> |   |
> [R] R 3.1.0 is released
>     |  |
>
>    |
>
>   
>   DEBUGGING:
>
>     * The behaviour of the code browser has been made more consistent,
>       in part following the suggestions in PR#14985.
>
>     * Calls to browser() are now consistent with calls to the browser
>       triggered by debug(), in that Enter will default to n rather than
>       c.
>
>     * A new browser command s has been added, to "step into" function
>       calls.
>
>     * A new browser command f has been added, to "finish" the current
>       loop or function.
>
>     * Within the browser, the command help will display a short list of
>       available commands.
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Wed Nov 23 22:21:36 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 23 Nov 2016 22:21:36 +0100
Subject: [R] Chi2 algorithm - R
In-Reply-To: <CAH0KQ1A9L7qR+HRERUjZARnYYkpHx_3Yo5unK6fFAvRha=KM8w@mail.gmail.com>
References: <CAH0KQ1A9L7qR+HRERUjZARnYYkpHx_3Yo5unK6fFAvRha=KM8w@mail.gmail.com>
Message-ID: <19496A01-7105-4C44-A14A-F1699C8167E5@gmail.com>

Notice that this relates to an R _package_, which has a maintainer. You cannot expect general R users or developers to know about the details of the package. It doesn't look like there is dcoumentation beyond the help pages, so you may need to contact the maintainer or study the actual code.

-pd 

> On 23 Nov 2016, at 17:08 , Luke Skywalker <mattered91 at gmail.com> wrote:
> 
> Good evening,
> 
> I'm encountering a different kind of discretization with respect to the
> 1997 Liu and Setiono's one descripted in their papers, using Chi2 algorithm
> for feature selection with discretization.
> 
> As stated in R documentation (discretization - R (from CRAN)
> <https://cran.r-project.org/web/packages/discretization/discretization.pdf>),
> R package discretizion offers the function Chi2, which comes to life in the
> following papers:
> 
> Liu, H. and Setiono, R. (1995). Chi2: Feature selection and discretization
> of numeric attributes, Tools with Artificial Intelligence, 388?391.
> 
> Liu, H. and Setiono, R. (1997). Feature selection and discretization, IEEE
> transactions on knowledge and data engineering, Vol.9, no.4, 642?645.
> 
> I wrote the following R programming language code, in which I have set
> alpha and delta equal to the ones set in the papers above. Finally, the
> following code prints out the discretized dataframe. I used Iris dataframe,
> as in one of the examples in the two papers. The first paper above states
> that alfa = 0.5 and delta = 5%, and that "the originally odd numbered data
> are selected for training (75 patterns) and rest for testing (75
> patterns)". With this asset, Sepal attributes should be removed.
> 
> library(discretization)
> data(iris)
> df1 <- iris[FALSE,]for(i in 1:nrow(iris)){
>    if(i %% 2 != 0){
>        df1 <- rbind(df1, iris[i,])
>    }}
> chi2(df1, alp=0.5, del=0.05)$Disc.data
> 
> The point is that, observing the dataframe printed out by the last
> instruction, you can see that no attribute is removed. The discretized data
> frame still have 4 attributes discretized: if I correctly understood the
> above papers, Sepal Length and Sepal Width should have been both
> discretized in just one interval by Chi2 algorithm.
> 
> I have posted a question here: http://stats.stackexchange.com/questions/
> 247499/why-does-not-r-chi2-algorithm-discretize-in-the-
> same-manner-as-in-the-paper-by-l?noredirect=1#comment470974_247499.
> 
> 
> Moreover, it's really hard to understand the cut points that Chi2 algorithm
> implemented in R makes. For example:
> 
> res <- chi2(iris, 0.5, 0.05)
> 
> cut(iris$Sepal.Length, res$cutp, labels=FALSE) is different from
> res$Disc.data$Sepal.Length
> 
> Help me understand, please
> 
> Best regards
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From mattered91 at gmail.com  Wed Nov 23 22:26:45 2016
From: mattered91 at gmail.com (Luke Skywalker)
Date: Wed, 23 Nov 2016 22:26:45 +0100
Subject: [R] Chi2 algorithm - R
In-Reply-To: <19496A01-7105-4C44-A14A-F1699C8167E5@gmail.com>
References: <CAH0KQ1A9L7qR+HRERUjZARnYYkpHx_3Yo5unK6fFAvRha=KM8w@mail.gmail.com>
	<19496A01-7105-4C44-A14A-F1699C8167E5@gmail.com>
Message-ID: <CAH0KQ1DC8-PUm4=-FUkRvOUg9fFtyk5LCvkY3byVSuBRZgUkgA@mail.gmail.com>

What does it mean to "have a mantainer"? Is he a third party? Is he an
individual developer and you can install whose package on your risk? Are
the package created by maintainers not tested?

Anyway, I wrote him. I'm waiting for response.

Regards

Il 23/Nov/2016 22:21, "peter dalgaard" <pdalgd at gmail.com> ha scritto:

> Notice that this relates to an R _package_, which has a maintainer. You
> cannot expect general R users or developers to know about the details of
> the package. It doesn't look like there is dcoumentation beyond the help
> pages, so you may need to contact the maintainer or study the actual code.
>
> -pd
>
> > On 23 Nov 2016, at 17:08 , Luke Skywalker <mattered91 at gmail.com> wrote:
> >
> > Good evening,
> >
> > I'm encountering a different kind of discretization with respect to the
> > 1997 Liu and Setiono's one descripted in their papers, using Chi2
> algorithm
> > for feature selection with discretization.
> >
> > As stated in R documentation (discretization - R (from CRAN)
> > <https://cran.r-project.org/web/packages/discretization/
> discretization.pdf>),
> > R package discretizion offers the function Chi2, which comes to life in
> the
> > following papers:
> >
> > Liu, H. and Setiono, R. (1995). Chi2: Feature selection and
> discretization
> > of numeric attributes, Tools with Artificial Intelligence, 388?391.
> >
> > Liu, H. and Setiono, R. (1997). Feature selection and discretization,
> IEEE
> > transactions on knowledge and data engineering, Vol.9, no.4, 642?645.
> >
> > I wrote the following R programming language code, in which I have set
> > alpha and delta equal to the ones set in the papers above. Finally, the
> > following code prints out the discretized dataframe. I used Iris
> dataframe,
> > as in one of the examples in the two papers. The first paper above states
> > that alfa = 0.5 and delta = 5%, and that "the originally odd numbered
> data
> > are selected for training (75 patterns) and rest for testing (75
> > patterns)". With this asset, Sepal attributes should be removed.
> >
> > library(discretization)
> > data(iris)
> > df1 <- iris[FALSE,]for(i in 1:nrow(iris)){
> >    if(i %% 2 != 0){
> >        df1 <- rbind(df1, iris[i,])
> >    }}
> > chi2(df1, alp=0.5, del=0.05)$Disc.data
> >
> > The point is that, observing the dataframe printed out by the last
> > instruction, you can see that no attribute is removed. The discretized
> data
> > frame still have 4 attributes discretized: if I correctly understood the
> > above papers, Sepal Length and Sepal Width should have been both
> > discretized in just one interval by Chi2 algorithm.
> >
> > I have posted a question here: http://stats.stackexchange.com/questions/
> > 247499/why-does-not-r-chi2-algorithm-discretize-in-the-
> > same-manner-as-in-the-paper-by-l?noredirect=1#comment470974_247499.
> >
> >
> > Moreover, it's really hard to understand the cut points that Chi2
> algorithm
> > implemented in R makes. For example:
> >
> > res <- chi2(iris, 0.5, 0.05)
> >
> > cut(iris$Sepal.Length, res$cutp, labels=FALSE) is different from
> > res$Disc.data$Sepal.Length
> >
> > Help me understand, please
> >
> > Best regards
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From olivier.crouzet at univ-nantes.fr  Wed Nov 23 22:56:54 2016
From: olivier.crouzet at univ-nantes.fr (Olivier Crouzet)
Date: Wed, 23 Nov 2016 21:56:54 +0000
Subject: [R] Chi2 algorithm - R
In-Reply-To: <CAH0KQ1DC8-PUm4=-FUkRvOUg9fFtyk5LCvkY3byVSuBRZgUkgA@mail.gmail.com>
References: <CAH0KQ1A9L7qR+HRERUjZARnYYkpHx_3Yo5unK6fFAvRha=KM8w@mail.gmail.com>
	<19496A01-7105-4C44-A14A-F1699C8167E5@gmail.com>
	<CAH0KQ1DC8-PUm4=-FUkRvOUg9fFtyk5LCvkY3byVSuBRZgUkgA@mail.gmail.com>
Message-ID: <1800963324-1479938214-cardhu_decombobulator_blackberry.rim.net-2068869590-@b15.c1.bise7.blackberry>

Hi,

(1) If the package has been installed from CRAN then it's been tested (which does not imply that it's'exempt of any bugs), any other source (e.g. Github) has not been tested... according to my understanding;

(2) The GNU GPL explicitly states that you install and use the software "at your own risk"... This is the case for any GPL licensed software, even for base-R... so this is no surprise!

Olivier.



--
Olivier Crouzet
LLING - Laboratoire de Linguistique de Nantes
UMR 6310 CNRS / Universit? de Nantes

-----Original Message-----
From: Luke Skywalker <mattered91 at gmail.com>
Sender: "R-help" <r-help-bounces at r-project.org>Date: Wed, 23 Nov 2016 22:26:45 
To: peter dalgaard<pdalgd at gmail.com>
Cc: <r-help at r-project.org>
Subject: Re: [R] Chi2 algorithm - R

What does it mean to "have a mantainer"? Is he a third party? Is he an
individual developer and you can install whose package on your risk? Are
the package created by maintainers not tested?

Anyway, I wrote him. I'm waiting for response.

Regards

Il 23/Nov/2016 22:21, "peter dalgaard" <pdalgd at gmail.com> ha scritto:

> Notice that this relates to an R _package_, which has a maintainer. You
> cannot expect general R users or developers to know about the details of
> the package. It doesn't look like there is dcoumentation beyond the help
> pages, so you may need to contact the maintainer or study the actual code.
>
> -pd
>
> > On 23 Nov 2016, at 17:08 , Luke Skywalker <mattered91 at gmail.com> wrote:
> >
> > Good evening,
> >
> > I'm encountering a different kind of discretization with respect to the
> > 1997 Liu and Setiono's one descripted in their papers, using Chi2
> algorithm
> > for feature selection with discretization.
> >
> > As stated in R documentation (discretization - R (from CRAN)
> > <https://cran.r-project.org/web/packages/discretization/
> discretization.pdf>),
> > R package discretizion offers the function Chi2, which comes to life in
> the
> > following papers:
> >
> > Liu, H. and Setiono, R. (1995). Chi2: Feature selection and
> discretization
> > of numeric attributes, Tools with Artificial Intelligence, 388?391.
> >
> > Liu, H. and Setiono, R. (1997). Feature selection and discretization,
> IEEE
> > transactions on knowledge and data engineering, Vol.9, no.4, 642?645.
> >
> > I wrote the following R programming language code, in which I have set
> > alpha and delta equal to the ones set in the papers above. Finally, the
> > following code prints out the discretized dataframe. I used Iris
> dataframe,
> > as in one of the examples in the two papers. The first paper above states
> > that alfa = 0.5 and delta = 5%, and that "the originally odd numbered
> data
> > are selected for training (75 patterns) and rest for testing (75
> > patterns)". With this asset, Sepal attributes should be removed.
> >
> > library(discretization)
> > data(iris)
> > df1 <- iris[FALSE,]for(i in 1:nrow(iris)){
> >    if(i %% 2 != 0){
> >        df1 <- rbind(df1, iris[i,])
> >    }}
> > chi2(df1, alp=0.5, del=0.05)$Disc.data
> >
> > The point is that, observing the dataframe printed out by the last
> > instruction, you can see that no attribute is removed. The discretized
> data
> > frame still have 4 attributes discretized: if I correctly understood the
> > above papers, Sepal Length and Sepal Width should have been both
> > discretized in just one interval by Chi2 algorithm.
> >
> > I have posted a question here: http://stats.stackexchange.com/questions/
> > 247499/why-does-not-r-chi2-algorithm-discretize-in-the-
> > same-manner-as-in-the-paper-by-l?noredirect=1#comment470974_247499.
> >
> >
> > Moreover, it's really hard to understand the cut points that Chi2
> algorithm
> > implemented in R makes. For example:
> >
> > res <- chi2(iris, 0.5, 0.05)
> >
> > cut(iris$Sepal.Length, res$cutp, labels=FALSE) is different from
> > res$Disc.data$Sepal.Length
> >
> > Help me understand, please
> >
> > Best regards
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From simon.wood at bath.edu  Wed Nov 23 23:32:25 2016
From: simon.wood at bath.edu (Simon Wood)
Date: Wed, 23 Nov 2016 22:32:25 +0000
Subject: [R] GAM with the negative binomial distribution: why do
 predictions no match with original values?
In-Reply-To: <AM5PR0701MB2338C813C8EE72DBBA5982CFE2B70@AM5PR0701MB2338.eurprd07.prod.outlook.com>
References: <AM5PR0701MB2338A474BE0056E27BD32432E2B40@AM5PR0701MB2338.eurprd07.prod.outlook.com>
	<CAGxFJbTT8MQvU+y5iMGBznD85ATgDa=4fqeh71Y+aJPN+LVqzA@mail.gmail.com>
	<B87E98BB-8D00-4933-8586-B6CF7638C74E@gmail.com>
	<AM5PR0701MB2338C813C8EE72DBBA5982CFE2B70@AM5PR0701MB2338.eurprd07.prod.outlook.com>
Message-ID: <cf00ba2e-47d4-964d-e903-e48435e647fc@bath.edu>

?predict.gam (mgcv) says....

    "Note that, in common with other prediction functions, any offset
      supplied to ?gam? as an argument is always ignored when
      predicting, unlike offsets specified in the gam model formula."

.... which was originally implemented to prevent surprises to people 
familiar with predict.lm (which used to behave that way, and was 
documented to behave that way).... the problem is that predict.lm 
doesn't behave like that any more, so I guess at some point I should 
remove this feature...

best,
Simon




On 23/11/16 00:24, Marine Regis wrote:
> Thanks a lot for your answers.
> Peter: sorry, here is the missing information:
>
>    *   I use the function gam() of the package ?mgcv?
>    *   Yes, the output changes when I use offset(log_trap_eff) instead of offset=log_trap_eff. By using offset(log_trap_eff), the output is more coherent with the observed values. Here are the new predictions:
>> summary(mod$fit)
> Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> 10.01   68.14   85.71   83.16  101.00  130.20
>
>
>    *   I have tried to create a reproductive example to show the difference between offset(log_trap_eff) and offset=log_trap_eff.
> nb_unique <- rnegbin(58, mu=82, theta=13.446)
> x <- runif(58,min=-465300,max=435200)
> prop_forest <- runif(58,min=0,max=1)
> log_trap_eff <- runif(58,min=4,max=6)
>
> With offset=log_trap_eff:
>
> mod1 <- gam(nb_unique ~ s(x,prop_forest), offset=log_trap_eff, family=nb(theta=NULL, link="log"), method = "REML", select = TRUE)
>
>> mod1Pred <- predict.gam(mod1, se.fit=TRUE, type="response")
>> summary(mod1Pred$fit)
>     Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>
>   0.5852  0.5852  0.5852  0.5852  0.5852  0.5852
>
>
> With offset(log_trap_eff):
> mod2 <- gam(nb_unique ~ s(x,prop_forest) + offset(log_trap_eff), family=nb(theta=NULL, link="log"), method = "REML", select = TRUE)
>
>> mod2Pred <- predict.gam(mod2, se.fit=TRUE, type="response")
>> summary(mod2Pred$fit)
>     Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>
>    32.03   61.18   97.20  112.20  165.00  226.00
>
>
>
> Value range of observed data:
>
>> summary(nb_unique)
>     Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>
>    43.00   67.00   81.00   84.16   92.75  153.00
>
>
>    *   By using fitted(mod), I obtain NULL.
> I am a novice in GAMs. So, I don?t know why the results are different between models with offset=argument and offset().
> Thanks a lot for your help.
> Have a nice day
> Marine
>
>
>
> ________________________________
> De : peter dalgaard <pdalgd at gmail.com>
> Envoy? : mardi 22 novembre 2016 23:52
> ? : Bert Gunter
> Cc : Marine Regis; r-help at r-project.org
> Objet : Re: [R] GAM with the negative binomial distribution: why do predictions no match with original values?
>
>
>> On 22 Nov 2016, at 23:07 , Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>
>> Define "very different."  Sounds like a subjective opinion to me, for
>> which I have no response. Apparently others are similarly flummoxed.
>> Of course they would not in general be identical.
> Er? I don't see much reason to disagree that a range 0.10-0.18 is different from 17-147.
>
> However, other bits of information are missing: We don't know which gam() function is being used (to my knowledge there is one in package gam but also one in mgcv). We don't have the data, so we cannot reproduce and try to find the root of the problem.
>
> Offhand, it looks like the predict.gam() function is misbehaving, which could have something to do with the offset term and/or the nb dispersion parameter. On a hunch, does anything change if you use
>
> nb_unique ~ s(x,prop_forest) + offset(log_trap_eff)
>
> instead of the offset= argument? And, by the way, does fitted(mod,...) change anything?
>
> -pd
>
>> Cheers,
>> Bert
>>
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Tue, Nov 22, 2016 at 1:29 PM, Marine Regis <marine.regis at hotmail.fr> wrote:
>>> Hello,
>>>
>>>>  From capture data, I would like to assess the effect of longitudinal changes in proportion of forests on abundance of skunks. To test this, I built this GAM where the dependent variable is the number of unique skunks and the independent variables are the X coordinates of the centroids of trapping sites (called "X" in the GAM) and the proportion of forests within the trapping sites (called "prop_forest" in the GAM):
>>>     mod <- gam(nb_unique ~ s(x,prop_forest), offset=log_trap_eff, family=nb(theta=NULL, link="log"), data=succ_capt_skunk, method = "REML", select = TRUE)
>>>     summary(mod)
>>>
>>>     Family: Negative Binomial(13.446)
>>>     Link function: log
>>>
>>>     Formula:
>>>     nb_unique ~ s(x, prop_forest)
>>>
>>>     Parametric coefficients:
>>>                 Estimate Std. Error z value Pr(>|z|)
>>>     (Intercept) -2.02095    0.03896  -51.87   <2e-16 ***
>>>     ---
>>>     Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>>
>>>     Approximate significance of smooth terms:
>>>                        edf Ref.df Chi.sq  p-value
>>>     s(x,prop_forest) 3.182     29  17.76 0.000102 ***
>>>     ---
>>>     Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>>
>>>     R-sq.(adj) =   0.37   Deviance explained =   49%
>>>     -REML = 268.61  Scale est. = 1         n = 58
>>>
>>>
>>> I built a GAM  for the negative binomial family. When I use the function `predict.gam`, the predictions of capture success from the GAM and the values of capture success from original data are very different. What is the reason for differences occur?
>>>
>>> **With GAM:**
>>>
>>>     modPred <- predict.gam(mod, se.fit=TRUE,type="response")
>>>     summary(modPred$fit)
>>>        Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>>>      0.1026  0.1187  0.1333  0.1338  0.1419  0.1795
>>>
>>> **With original data:**
>>>
>>>     summary(succ_capt_skunk$nb_unique)
>>>        Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>>>       17.00   59.00   82.00   81.83  106.80  147.00
>>>
>>> The question has already been posted on Cross validated (http://stats.stackexchange.com/questions/247347/gam-with-the-negative-binomial-distribution-why-do-predictions-no-match-with-or) without success.
> [http://cdn.sstatic.net/Sites/stats/img/apple-touch-icon at 2.png?v=344f57aa10cc&a]<http://stats.stackexchange.com/questions/247347/gam-with-the-negative-binomial-distribution-why-do-predictions-no-match-with-or>
>
> GAM with the negative binomial distribution: why do predictions no match with original values?<http://stats.stackexchange.com/questions/247347/gam-with-the-negative-binomial-distribution-why-do-predictions-no-match-with-or>
> stats.stackexchange.com
> >From capture data, I would like to assess the effect of longitudinal changes in proportion of forests on abundance of skunks. To test this, I built this GAM where the dependent variable is the numb...
>
>
>
>>> Thanks a lot for your time.
>>> Have a nice day
>>> Marine
>>>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
> thz.ch/mailman/listinfo/r-help>
> stat.ethz.ch
> The main R mailing list, for announcements about the development of R and the availability of new code, questions and answers about problems and solutions using R ...
>
>
>
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
> thz.ch/mailman/listinfo/r-help>
> stat.ethz.ch
> The main R mailing list, for announcements about the development of R and the availability of new code, questions and answers about problems and solutions using R ...
>
>
>
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
Simon Wood, School of Mathematics, University of Bristol BS8 1TW UK
+44 (0)117 33 18273     http://www.maths.bris.ac.uk/~sw15190


	[[alternative HTML version deleted]]


From babasaraki at yahoo.co.uk  Thu Nov 24 04:59:16 2016
From: babasaraki at yahoo.co.uk (Umar Ahmad)
Date: Thu, 24 Nov 2016 03:59:16 +0000 (UTC)
Subject: [R] Fixing non-UTF8 locale and other warning in R for Mac OS X
References: <1842209445.1471599.1479959956585.ref@mail.yahoo.com>
Message-ID: <1842209445.1471599.1479959956585@mail.yahoo.com>

Hello R Experts,
Please can anyone help or guide me through on how I can fix this problem below that is usual showed whenever I open R statitsics in Mac OS but I never have seen this to exist on Window computer. It seems to always refer me to R for Mac OS X FAQ (see Help) of which I went there many times but couldn't know how to fix this issue.?

"During startup - Warning messages:1: Setting LC_CTYPE failed, using "C"?2: Setting LC_COLLATE failed, using "C"?3: Setting LC_TIME failed, using "C"?4: Setting LC_MESSAGES failed, using "C"?5: Setting LC_MONETARY failed, using "C"?[R.app GUI 1.68 (7288) x86_64-apple-darwin13.4.0]
WARNING: You're using a non-UTF8 locale, therefore only ASCII characters will work.Please read R for Mac OS X FAQ (see Help) section 9 and adjust your system preferences accordingly."


Thanks in anticipation for your guidance.
Umar.?
	[[alternative HTML version deleted]]


From ranjanagirish30 at gmail.com  Thu Nov 24 13:19:35 2016
From: ranjanagirish30 at gmail.com (Ranjana Girish)
Date: Thu, 24 Nov 2016 17:49:35 +0530
Subject: [R] Regarding socket exception when using Rserve
Message-ID: <CAF5P65mmXfbop9Bg6eYdzwVhHopT3mt-1YnnnhAEmg5fKKy3bQ@mail.gmail.com>

Hi all,

we are connecting to R language from Java using Rserve().

while doing so,Got one issue at hand

Some of the command in R is taking more than 20 min. During this time we
are receiving

java.net.SocketException: Connection reset
        at java.net.SocketInputStream.read(SocketInputStream.java:196)
        at java.net.SocketInputStream.read(SocketInputStream.java:122)
        at java.net.SocketInputStream.read(SocketInputStream.java:108)
        at org.rosuda.REngine.Rserve.protocol.RTalk.request(RTalk.java:213)
        at org.rosuda.REngine.Rserve.protocol.RTalk.request(RTalk.java:180)
        at org.rosuda.REngine.Rserve.protocol.RTalk.request(RTalk.java:250)
        at org.rosuda.REngine.Rserve.RConnection.eval(RConnection.java:258)
        at
org.rosuda.REngine.Rserve.RConnection.parseAndEval(RConnection.java:497)
        at org.rosuda.REngine.REngine.parseAndEval(REngine.java:108)



The R command which is getting executed is
 DocumentTermMatrix(documents)

which is a huge matrix creation operation,

The above command works fine, if we execute in R environment directly.

We tried,

1. Both the eval function: *VoidEval* and *serverEval*
*2. Also increased the java.paramters both at Apachae tomcat*
  options( java.parameters = "-Xss4096m -Xms8192m -Xmx16384m
-XX:PermSize=10240m -XX:MaxPermSize=10240m")
3.Also the same in setenv of apachae tomcat file



Please tell what are the other solutions ,that can be try out

Thanks in advance


Ranjana

	[[alternative HTML version deleted]]


From jon.skoien at jrc.ec.europa.eu  Thu Nov 24 16:07:53 2016
From: jon.skoien at jrc.ec.europa.eu (Jon Skoien)
Date: Thu, 24 Nov 2016 16:07:53 +0100
Subject: [R] The code itself disappears after starting to execute the
 for loop
In-Reply-To: <2FC08618-B485-43D0-9103-A816172D41A6@gmail.com>
References: <59BE1118-9D4B-4298-B605-9BADFBC43E88@gmail.com>
	<CAAxdm-5FWvfnpwEJtr5eNw0AKv+cjOfgXfQKTLSmYMkk1RgPcA@mail.gmail.com>
	<2BFA80AA-A18B-4A52-A96B-455AE7BB9063@gmail.com>
	<CAGxFJbTxXbS-+Rki-143izvQEo1JZjSy+8jY2qNFj_RKHppCOQ@mail.gmail.com>
	<2FC08618-B485-43D0-9103-A816172D41A6@gmail.com>
Message-ID: <54b8d998-9299-4034-be0a-9c9ad7169974@jrc.ec.europa.eu>

I don't have a solution to this problem, but as I have also struggled 
with what I think is the same problem, I tried to find a small 
reproducible example.
The problem seems indeed to be with the progress bar, which will clear 
the console after x iterations when the progress bar is called in a 
function which is wrapped in a loop. In the example below, this happened 
for me every ~44th iteration. Interestingly, it seems that reduction of 
the sleep times in this function increases the number of iterations 
before clearing.

testit <- function(x = sort(runif(20)), ...)
{
   pb <- txtProgressBar(...)
   for(i in c(0, x, 1)) {Sys.sleep(0.2); setTxtProgressBar(pb, i)}
   Sys.sleep(1)
   close(pb)
}

iter = 0
while (TRUE) {testit(style = 3); iter = iter + 1; print(paste("done", 
iter))}

Is this only a problem for a few, or is it reproducible? Any hints to 
what the problem could be? I have seen this in some versions of R, and 
could also reproduce on a freshly installed 3.3.2.

Best wishes,
Jon

R version 3.3.2 (2016-10-31)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 8.1 x64 (build 9600)

locale:
[1] LC_COLLATE=English_United States.1252
[2] LC_CTYPE=English_United States.1252
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C
[5] LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods base





On 11/23/2016 5:22 PM, Maram SAlem wrote:
> Thanks a lot Bert , will check out your suggestions.
>
> I've unchecked the buffer output option in GUI but still have the same problem.
>
> Thanks for your time and concern.
>
> Maram Salem
>
> Sent from my iPhone
>
>> On Nov 23, 2016, at 5:55 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>
>> In addition to Jim's comments, which you have not yet satisfactorily
>> addressed (buffering in GUI??),
>>
>> 1. Show your code!
>>
>> 2. Show ouput of sessionInfo()
>>
>> 3. Upgrade to the latest R version maybe
>>
>> 4. Perhaps write to package maintainer (see ?maintainer) if nothing or
>> no one helps.
>>
>> Cheers,
>> Bert
>>
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>>> On Tue, Nov 22, 2016 at 10:05 AM, Maram SAlem <marammagdysalem at gmail.com> wrote:
>>> Thanks for helping Jim.
>>>
>>> I'm actually using the pbapply function together with the print function within a loop. In earlier versions, the progress bar and the output of the print function used to appear after each iteration of the loop. But with the 3.3.1. Version nothing appears, instead the console turns white and the cursor turns blue ( busy) and I know nothing about the progress of the running code.
>>>
>>> I just want to see the bar and the output of the print function as I used to, any help?
>>>
>>> Thanks in advance.
>>> Maram Salem
>>>
>>>
>>>
>>> Sent from my iPhone
>>>
>>>> On Nov 3, 2016, at 8:30 PM, jim holtman <jholtman at gmail.com> wrote:
>>>>
>>>> A little more information would help.  How exactly are out creating the output to the console?  Are you using 'print', 'cat' or something else?  Do you have buffered output checked on the GUI (you probably don't want it checked or you output will be delayed till the buffer is full -- this might be the cause of your problem.
>>>>
>>>>
>>>> Jim Holtman
>>>> Data Munger Guru
>>>>
>>>> What is the problem that you are trying to solve?
>>>> Tell me what you want to do, not how you want to do it.
>>>>
>>>>> On Thu, Nov 3, 2016 at 1:55 PM, Maram SAlem <marammagdysalem at gmail.com> wrote:
>>>>> Hi all,
>>>>>
>>>>> I've a question concerning the R 3.3.1 version. I have a long code that I used to run on versions earlier to the 3.3.1 version, and when I copied the code to the R console, I can still see the code while the loop is executing , along with the output printed after each iteration of the loop.
>>>>>
>>>>> Now, on the 3.3.1 version, after I copy the code to the console, it disappears and I only see the printed output of only one iteration at a time, that is, after the first iteration the printed output disappears ( though it's only 6 lines, just giving me some guidance, not a long output).
>>>>> This is causing me some problems, so I don't know if there is a general option for R that enables me to still see the code and the output of all the iterations till the loop is over, as was the case with earlier R versions.
>>>>>
>>>>> I didn't include the code as it's a long one.
>>>>>
>>>>> Thanks a lot in advance,
>>>>>
>>>>> Maram
>>>>>
>>>>>
>>>>> Sent from my iPhone
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Jon Olav Sk?ien
Joint Research Centre - European Commission
Institute for Space, Security & Migration
Disaster Risk Management Unit

Via E. Fermi 2749, TP 122,  I-21027 Ispra (VA), ITALY

jon.skoien at jrc.ec.europa.eu
Tel:  +39 0332 789205

Disclaimer: Views expressed in this email are those of the individual and do not necessarily represent official views of the European Commission.


From jdnewmil at dcn.davis.ca.us  Thu Nov 24 16:39:12 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 24 Nov 2016 07:39:12 -0800
Subject: [R] Chi2 algorithm - R
In-Reply-To: <CAH0KQ1DC8-PUm4=-FUkRvOUg9fFtyk5LCvkY3byVSuBRZgUkgA@mail.gmail.com>
References: <CAH0KQ1A9L7qR+HRERUjZARnYYkpHx_3Yo5unK6fFAvRha=KM8w@mail.gmail.com>
	<19496A01-7105-4C44-A14A-F1699C8167E5@gmail.com>
	<CAH0KQ1DC8-PUm4=-FUkRvOUg9fFtyk5LCvkY3byVSuBRZgUkgA@mail.gmail.com>
Message-ID: <CE79C9F2-14B9-48EA-B92F-DE109C87C474@dcn.davis.ca.us>

As I understand it,  you or I could write a package and if the automated testing designed to ferret out basic R language compatibility and operating system independence passes (and the maintainer accepts responsibility for it and releases the code as open source), then it will usually be accepted for distribution through CRAN. Peer review (in particular re the algorithms employed) by anyone involved with the R language development team is not part of this... this is why the distinction between R and contributed packages is important.

So yes, while most maintainers are trying to do the right thing, they are only human (and volunteers), using their code is very much at your own risk and not the responsibility of "R".
-- 
Sent from my phone. Please excuse my brevity.

On November 23, 2016 1:26:45 PM PST, Luke Skywalker <mattered91 at gmail.com> wrote:
>What does it mean to "have a mantainer"? Is he a third party? Is he an
>individual developer and you can install whose package on your risk?
>Are
>the package created by maintainers not tested?
>
>Anyway, I wrote him. I'm waiting for response.
>
>Regards
>
>Il 23/Nov/2016 22:21, "peter dalgaard" <pdalgd at gmail.com> ha scritto:
>
>> Notice that this relates to an R _package_, which has a maintainer.
>You
>> cannot expect general R users or developers to know about the details
>of
>> the package. It doesn't look like there is dcoumentation beyond the
>help
>> pages, so you may need to contact the maintainer or study the actual
>code.
>>
>> -pd
>>
>> > On 23 Nov 2016, at 17:08 , Luke Skywalker <mattered91 at gmail.com>
>wrote:
>> >
>> > Good evening,
>> >
>> > I'm encountering a different kind of discretization with respect to
>the
>> > 1997 Liu and Setiono's one descripted in their papers, using Chi2
>> algorithm
>> > for feature selection with discretization.
>> >
>> > As stated in R documentation (discretization - R (from CRAN)
>> > <https://cran.r-project.org/web/packages/discretization/
>> discretization.pdf>),
>> > R package discretizion offers the function Chi2, which comes to
>life in
>> the
>> > following papers:
>> >
>> > Liu, H. and Setiono, R. (1995). Chi2: Feature selection and
>> discretization
>> > of numeric attributes, Tools with Artificial Intelligence, 388?391.
>> >
>> > Liu, H. and Setiono, R. (1997). Feature selection and
>discretization,
>> IEEE
>> > transactions on knowledge and data engineering, Vol.9, no.4,
>642?645.
>> >
>> > I wrote the following R programming language code, in which I have
>set
>> > alpha and delta equal to the ones set in the papers above. Finally,
>the
>> > following code prints out the discretized dataframe. I used Iris
>> dataframe,
>> > as in one of the examples in the two papers. The first paper above
>states
>> > that alfa = 0.5 and delta = 5%, and that "the originally odd
>numbered
>> data
>> > are selected for training (75 patterns) and rest for testing (75
>> > patterns)". With this asset, Sepal attributes should be removed.
>> >
>> > library(discretization)
>> > data(iris)
>> > df1 <- iris[FALSE,]for(i in 1:nrow(iris)){
>> >    if(i %% 2 != 0){
>> >        df1 <- rbind(df1, iris[i,])
>> >    }}
>> > chi2(df1, alp=0.5, del=0.05)$Disc.data
>> >
>> > The point is that, observing the dataframe printed out by the last
>> > instruction, you can see that no attribute is removed. The
>discretized
>> data
>> > frame still have 4 attributes discretized: if I correctly
>understood the
>> > above papers, Sepal Length and Sepal Width should have been both
>> > discretized in just one interval by Chi2 algorithm.
>> >
>> > I have posted a question here:
>http://stats.stackexchange.com/questions/
>> > 247499/why-does-not-r-chi2-algorithm-discretize-in-the-
>> > same-manner-as-in-the-paper-by-l?noredirect=1#comment470974_247499.
>> >
>> >
>> > Moreover, it's really hard to understand the cut points that Chi2
>> algorithm
>> > implemented in R makes. For example:
>> >
>> > res <- chi2(iris, 0.5, 0.05)
>> >
>> > cut(iris$Sepal.Length, res$cutp, labels=FALSE) is different from
>> > res$Disc.data$Sepal.Length
>> >
>> > Help me understand, please
>> >
>> > Best regards
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From stuti.verma284 at gmail.com  Thu Nov 24 08:43:48 2016
From: stuti.verma284 at gmail.com (Stuti Verma)
Date: Thu, 24 Nov 2016 13:13:48 +0530
Subject: [R] Error occurring
Message-ID: <CAND-_-Y+LBLuVibmMUTq_YfZd=jbgf7TxOhLQZhtFk-tyV2pzQ@mail.gmail.com>

> j <- function() {
+ if(!exists ("a")){
+ a <- 1
+ } else{
+ a <- a+1
+ } print(a)}

Error: unexpected symbol in:
"a <- a+1
} print"

> j <- function() {
+ if(!exists ("a")){
+ a <- 1
+ } else{
+ a <- a+1
+ } print("a")}

Error: unexpected symbol in:
"a <- a+1
} print"

	[[alternative HTML version deleted]]


From cg.pettersson at lantmannen.com  Thu Nov 24 17:34:35 2016
From: cg.pettersson at lantmannen.com (CG Pettersson)
Date: Thu, 24 Nov 2016 16:34:35 +0000
Subject: [R] Datapoint tracking in scatterplot from plot() used on plsr()
	objects
Message-ID: <DB5PR02MB09832323F7D58BF519AE38378CB60@DB5PR02MB0983.eurprd02.prod.outlook.com>

Dear all,
I am working with PLS-regression using plsr() from the pls package. I have a medium sized dataset with 223 rows, ten reference columns and 192 columns with measurement data from an "electronic nose".

There is a convenient way of inspecting what you have done in the calls to plsr() by typing plot(object) which results in a scatterplot predicted/measured. How do I track what row in the dataset is what point in the scatterplot? I have tried to read the documentation for plot() but don?t find the solution, probably because I don?t understand it :(. I am quite sure it is possible as I have seen something like that in other uses of plot() but I can?t find where.

Med v?nlig h?lsning/Best regards
CG Pettersson
Senior Scientific Advisor, PhD
______________________
Lantm?nnen Corporate R&D
Phone:  +46 10 556 19 85
Mobile: + 46 70 330 66 85
Email: cg.pettersson at lantmannen.com<mailto:cg.pettersson at lantmannen.com>
Visiting Address: S:t G?ransgatan 160 A
Address: Box 30192, SE-104 25 Stockholm
Webb: http://www.lantmannen.com<http://www.lantmannen.com/>
Registered Office: Stockholm
Before printing, think about the environment


	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Thu Nov 24 21:44:11 2016
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Thu, 24 Nov 2016 20:44:11 +0000
Subject: [R] Error occurring
In-Reply-To: <CAND-_-Y+LBLuVibmMUTq_YfZd=jbgf7TxOhLQZhtFk-tyV2pzQ@mail.gmail.com>
References: <CAND-_-Y+LBLuVibmMUTq_YfZd=jbgf7TxOhLQZhtFk-tyV2pzQ@mail.gmail.com>
Message-ID: <5837511B.7080906@sapo.pt>

Hello,

I don't see the error you mention.

j <- function() {
	if(!exists ("a")){
		a <- 1
	} else{
		a <- a+1
	}
	 print(a)
}

j()
[1] 1


sessionInfo()
R version 3.3.2 (2016-10-31)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

locale:
[1] LC_COLLATE=Portuguese_Portugal.1252 
LC_CTYPE=Portuguese_Portugal.1252
[3] LC_MONETARY=Portuguese_Portugal.1252 LC_NUMERIC=C 

[5] LC_TIME=Portuguese_Portugal.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] tools_3.3.2

Rui Barradas

Em 24-11-2016 07:43, Stuti Verma escreveu:
>> j <- function() {
> + if(!exists ("a")){
> + a <- 1
> + } else{
> + a <- a+1
> + } print(a)}
>
> Error: unexpected symbol in:
> "a <- a+1
> } print"
>
>> j <- function() {
> + if(!exists ("a")){
> + a <- 1
> + } else{
> + a <- a+1
> + } print("a")}
>
> Error: unexpected symbol in:
> "a <- a+1
> } print"
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From drjimlemon at gmail.com  Thu Nov 24 22:14:19 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 25 Nov 2016 08:14:19 +1100
Subject: [R] Error occurring
In-Reply-To: <CAND-_-Y+LBLuVibmMUTq_YfZd=jbgf7TxOhLQZhtFk-tyV2pzQ@mail.gmail.com>
References: <CAND-_-Y+LBLuVibmMUTq_YfZd=jbgf7TxOhLQZhtFk-tyV2pzQ@mail.gmail.com>
Message-ID: <CA+8X3fUjwNZmr7uYZnxnanUxRXMHAVAp3fMvjDOOc1mL6xX1FQ@mail.gmail.com>

`Hi Stuti,
Your problem is that if you want to have more than one command on a
single line, you must separate them with a semicolon.

j <- function() {
 if(!exists ("a")){
 a <- 1
 } else{
 a <- a+1
}; print(a)}

The above will work, but is usually considered bad form. What follows
is usually easier to read and avoids that sort of error.

j <- function() {
 if(!exists ("a")) {
 a <- 1
 } else {
 a <- a+1
 }
 print(a)
}

Notice how the opening and closing braces ({}) are "lined up" to make
the code easier to read. Most people recommend using at least two
spaces indent, but far be it from me to demand conformity in these
matters.

Jim


On Thu, Nov 24, 2016 at 6:43 PM, Stuti Verma <stuti.verma284 at gmail.com> wrote:
>> j <- function() {
> + if(!exists ("a")){
> + a <- 1
> + } else{
> + a <- a+1
> + } print(a)}
>
> Error: unexpected symbol in:
> "a <- a+1
> } print"
>
>> j <- function() {
> + if(!exists ("a")){
> + a <- 1
> + } else{
> + a <- a+1
> + } print("a")}
>
> Error: unexpected symbol in:
> "a <- a+1
> } print"
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Thu Nov 24 22:18:42 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 24 Nov 2016 13:18:42 -0800
Subject: [R] Error occurring
In-Reply-To: <5837511B.7080906@sapo.pt>
References: <CAND-_-Y+LBLuVibmMUTq_YfZd=jbgf7TxOhLQZhtFk-tyV2pzQ@mail.gmail.com>
	<5837511B.7080906@sapo.pt>
Message-ID: <AD6681A0-7334-4258-9A9B-9BB85D80607A@dcn.davis.ca.us>

If I was to make a totally wild leap,  I would say it looks like someone using an inappropriate text editor such as Microsoft Word to edit R code.

Stuti, please carefully read and follow the recommendations in the Posting Guide mentioned at the bottom of this and every post on this mailing list before posting again. R is a plain text language, and this list is a plain text mailing list. 
-- 
Sent from my phone. Please excuse my brevity.

On November 24, 2016 12:44:11 PM PST, Rui Barradas <ruipbarradas at sapo.pt> wrote:
>Hello,
>
>I don't see the error you mention.
>
>j <- function() {
>	if(!exists ("a")){
>		a <- 1
>	} else{
>		a <- a+1
>	}
>	 print(a)
>}
>
>j()
>[1] 1
>
>
>sessionInfo()
>R version 3.3.2 (2016-10-31)
>Platform: x86_64-w64-mingw32/x64 (64-bit)
>Running under: Windows 7 x64 (build 7601) Service Pack 1
>
>locale:
>[1] LC_COLLATE=Portuguese_Portugal.1252 
>LC_CTYPE=Portuguese_Portugal.1252
>[3] LC_MONETARY=Portuguese_Portugal.1252 LC_NUMERIC=C 
>
>[5] LC_TIME=Portuguese_Portugal.1252
>
>attached base packages:
>[1] stats     graphics  grDevices utils     datasets  methods   base
>
>loaded via a namespace (and not attached):
>[1] tools_3.3.2
>
>Rui Barradas
>
>Em 24-11-2016 07:43, Stuti Verma escreveu:
>>> j <- function() {
>> + if(!exists ("a")){
>> + a <- 1
>> + } else{
>> + a <- a+1
>> + } print(a)}
>>
>> Error: unexpected symbol in:
>> "a <- a+1
>> } print"
>>
>>> j <- function() {
>> + if(!exists ("a")){
>> + a <- 1
>> + } else{
>> + a <- a+1
>> + } print("a")}
>>
>> Error: unexpected symbol in:
>> "a <- a+1
>> } print"
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Thu Nov 24 22:22:00 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 24 Nov 2016 13:22:00 -0800
Subject: [R] Datapoint tracking in scatterplot from plot() used on
	plsr() objects
In-Reply-To: <DB5PR02MB09832323F7D58BF519AE38378CB60@DB5PR02MB0983.eurprd02.prod.outlook.com>
References: <DB5PR02MB09832323F7D58BF519AE38378CB60@DB5PR02MB0983.eurprd02.prod.outlook.com>
Message-ID: <65912167-5DB7-432C-A6D5-4FC773178294@comcast.net>


> On Nov 24, 2016, at 8:34 AM, CG Pettersson <cg.pettersson at lantmannen.com> wrote:
> 
> Dear all,
> I am working with PLS-regression using plsr() from the pls package. I have a medium sized dataset with 223 rows, ten reference columns and 192 columns with measurement data from an "electronic nose".
> 
> There is a convenient way of inspecting what you have done in the calls to plsr() by typing plot(object) which results in a scatterplot predicted/measured. How do I track what row in the dataset is what point in the scatterplot? I have tried to read the documentation for plot() but don?t find the solution, probably because I don?t understand it :(. I am quite sure it is possible as I have seen something like that in other uses of plot() but I can?t find where.

Have you looked at matplot? Generally connects along columns. Might need to be:

 matplot( t(object). type="b")

> 
> Med v?nlig h?lsning/Best regards
> CG Pettersson
> Senior Scientific Advisor, PhD
> ______________________
> Lantm?nnen Corporate R&D
> Phone:  +46 10 556 19 85
> Mobile: + 46 70 330 66 85
> Email: cg.pettersson at lantmannen.com<mailto:cg.pettersson at lantmannen.com>
> Visiting Address: S:t G?ransgatan 160 A
> Address: Box 30192, SE-104 25 Stockholm
> Webb: http://www.lantmannen.com<http://www.lantmannen.com/>
> Registered Office: Stockholm
> Before printing, think about the environment
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From klebyn at yahoo.com.br  Thu Nov 24 22:38:54 2016
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Thu, 24 Nov 2016 19:38:54 -0200
Subject: [R] [tcltk] tktable: bindings doesn't triggers the
 ValidationCommand and Command
Message-ID: <b54d33f9-d4c2-2b07-8b2a-1ee1a2379d5b@yahoo.com.br>

Dears,

I'm trying to create an Data Editor like Rgui.exe's FIX (windows)...

The code is below.

The problem is that I can not get the binds to trigger the validation
commands and the main table command

Control-C, Control-V, and Control-X work only in the visual without my R
data being changed.

Any help, tip or example is welcome and I thank you in advance for your
attention.

Thank you very much


Cleber

################### R 3.4  Tcl  8.6


library( tcltk )
tclRequire("Tktable")

ncol <- 6
nrow <- 6

x <- matrix( rnorm( nrow*ncol ), nrow, ncol )

rownames(x) <- paste0( "Sam ", 1:nrow )
colnames(x) <- paste0( "Var ", 1:ncol )

###############################################################

showdigits <- 6

tablecmd <- function(r,c,S){
r <- as.integer( r )
c <- as.integer( c )
showNA <- is.na( x[ r,c ] )
if( r == 0 && c > 0 ) return( tcl("expr", '{', colnames(x)[c], '}' ) )
if( c == 0 && r > 0 ) return( tcl("expr", '{', rownames(x)[r], '}' ) )
if( r >  0 && c > 0 &&  showNA ) return( tcl("expr", "{}" ) )
if( r >  0 && c > 0 && !showNA ) return( tcl("expr", round( x[r,c],
digits=showdigits ) ) )# signif
if( r == 0 && c == 0 ) return( tcl("expr", "{}" ) )
}

tablevcmd <- function( S,s,r,c ){
# s : current value # S : potential new value
if( grepl("\n", S ) ){
     tcl('::tk::table::MoveCell', .Tk.ID( tableData ), 1, 0 )
     return( tcl( 'expr', 0 ) )
     }
r <- as.integer( r )
c <- as.integer( c )
if( r == 0 && c > 0 ) {
     colnames(x)[c] <<- S
     return( tcl( 'expr', 1 ) )
     }
if( c == 0 && r > 0 ) {
     rownames(x)[r] <<- S
     return( tcl( 'expr', 1 ) )
     }
if( grepl(" ", S ) ){
     tcl('::tk::table::MoveCell', .Tk.ID( tableData ), 1, 0 )
     return( tcl( 'expr', 0 ) )
     }
if( S == "" ) {
     x[r,c] <<- NA
     return( tcl( 'expr', 1 ) )
     }
if( S != s ){
     x[r,c] <<- as.numeric( S )
     return( tcl( 'expr', 1 ) )
     }
}

# make the GUI
top <- tktoplevel()
tcl( 'wm', 'title',     top, 'DataFix' )

fmTableData <- ttkframe( top, borderwidth=2 )
tcl( 'pack', fmTableData, fill="both", expand=TRUE, padx=15, pady=15  )

fxscroll <- function(...){ tcl( scrX, 'set', ... ) }
fyscroll <- function(...){ tcl( scrY, 'set', ... ) }

tableData <- tkwidget( fmTableData, 'table', rows=nrow+1, cols=ncol+1,
height=-1, width=-1,
ellipsis='............', insertofftime=0, flashmode=TRUE, flashtime=1,
anchor='e',
resizeborders='col', wrap=FALSE, font='{Courier} 10', padx=5, pady=2,#
ipadx=3, ipady=1,
rowstretchmode='unset', colstretchmode='unset', multiline=FALSE, cache=TRUE,
background="white", selectmode="extended", selecttitle=TRUE,
relief='groove',
borderwidth=c(0,1,0,1), drawmode='compatible', colwidth=12,
highlightcolor="gray", highlightbackground="white", highlightthickness=1,
xscrollcommand=fxscroll, yscrollcommand=fyscroll, rowseparator='\n',
colseparator='\t',
validate=TRUE, vcmd=tablevcmd, usecommand=TRUE, command=tablecmd )

scrX <- ttkscrollbar( fmTableData, orient="horizont",
command=function(...) tcl( tableData,'xview',...) )
scrY <- ttkscrollbar( fmTableData, orient="vertical",
command=function(...) tcl( tableData,'yview',...) )
#### empacotando os scrollbars
tcl( "pack", scrY, side = "right",  fill = "y", expand = FALSE, pady =
c(0,18) )
tcl( "pack", scrX, side = "bottom", fill = "x", expand = FALSE )

tcl( tableData, "tag", "celltag", "ZeroZero", "0,0" )
tcl( tableData, "tag", "rowtag",  "rowtitle", "0" )
tcl( tableData, "tag", "coltag",  "coltitle", "0" )

tcl( tableData, "tag", "configure", "ZeroZero", bg='SystemButtonFace',
fg='SystemButtonFace', state='disabled' )

tcl( tableData, "tag", "configure", "rowtitle", bg='lightgray',
relief='groove', anchor='center')#, borderwidth=c(1,1,1,1) )
tcl( tableData, "tag", "configure", "coltitle", bg='lightgray',
relief='groove', anchor='w')#, borderwidth=c(1,1,1,1) )

tcl( tableData, "tag", "configure", "active", fg='green', bg='gray90',
relief='solid', borderwidth=c(1,1,1,1) )
tcl( tableData, "width", "0", "15" )

tcl( 'pack', tableData, side='left', anchor='n', fill="both", expand=TRUE )

#################################################################
# stay here to serve as an example
#################################################################
# bind Table <Up> {::tk::table::MoveCell %W -1  0}
tcl( 'bind', .Tk.ID( tableData ), '<F1>', paste('::tk::table::MoveCell',
.Tk.ID( tableData ), 1, 0 )  )
# bind Table <$cut>    {tk_tableCut %W}
tcl( 'bind', .Tk.ID( tableData ), '<F2>', paste('tk_tableCut', .Tk.ID(
tableData ) ) )












---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus

	[[alternative HTML version deleted]]


From erinm.hodgess at gmail.com  Fri Nov 25 04:03:24 2016
From: erinm.hodgess at gmail.com (Erin Hodgess)
Date: Thu, 24 Nov 2016 21:03:24 -0600
Subject: [R] abline with zoo series
Message-ID: <CACxE24=gQ9yO1Hqx0u2473maDkAUBOeJRBqK788BKW+sg1_U8A@mail.gmail.com>

Hello!  Happy Thanksgiving to those who are celebrating.

I have a zoo series that I am plotting, and I would like to have some
vertical lines at certain points, to indicate US business cycles.  Here is
an example:

app1 <- get.hist.quote(instrument="appl",
start="1985-01-01",end="2016-08-31", quote="AdjClose", compression="m")
#Fine
plot(app1,main="Historical Stock Prices: Apple Corporation")
#Still Fine
#Now I want to use abline at July 1990 and March 1991 (as a start) for
business cycles.  I tried v=67 and v="1990-07", no good.

I have a feeling that it's really simple and I'm just not seeing it.

Any help much appreciated.

Thanks,
Erin


-- 
Erin Hodgess
Associate Professor
Department of Mathematical and Statistics
University of Houston - Downtown
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Fri Nov 25 04:32:01 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 25 Nov 2016 14:32:01 +1100
Subject: [R] abline with zoo series
In-Reply-To: <CACxE24=gQ9yO1Hqx0u2473maDkAUBOeJRBqK788BKW+sg1_U8A@mail.gmail.com>
References: <CACxE24=gQ9yO1Hqx0u2473maDkAUBOeJRBqK788BKW+sg1_U8A@mail.gmail.com>
Message-ID: <CA+8X3fUhBmEV1EF+Xb-TTqXFrHFkfaKVMncWyWL8qVwNCRMtYA@mail.gmail.com>

Hi Erin,
I would look at:

par("usr")

to see what the range of the abscissa might be.

Jim


On Fri, Nov 25, 2016 at 2:03 PM, Erin Hodgess <erinm.hodgess at gmail.com> wrote:
> Hello!  Happy Thanksgiving to those who are celebrating.
>
> I have a zoo series that I am plotting, and I would like to have some
> vertical lines at certain points, to indicate US business cycles.  Here is
> an example:
>
> app1 <- get.hist.quote(instrument="appl",
> start="1985-01-01",end="2016-08-31", quote="AdjClose", compression="m")
> #Fine
> plot(app1,main="Historical Stock Prices: Apple Corporation")
> #Still Fine
> #Now I want to use abline at July 1990 and March 1991 (as a start) for
> business cycles.  I tried v=67 and v="1990-07", no good.
>
> I have a feeling that it's really simple and I'm just not seeing it.
>
> Any help much appreciated.
>
> Thanks,
> Erin
>
>
> --
> Erin Hodgess
> Associate Professor
> Department of Mathematical and Statistics
> University of Houston - Downtown
> mailto: erinm.hodgess at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Fri Nov 25 05:14:55 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 24 Nov 2016 20:14:55 -0800
Subject: [R] abline with zoo series
In-Reply-To: <CACxE24=gQ9yO1Hqx0u2473maDkAUBOeJRBqK788BKW+sg1_U8A@mail.gmail.com>
References: <CACxE24=gQ9yO1Hqx0u2473maDkAUBOeJRBqK788BKW+sg1_U8A@mail.gmail.com>
Message-ID: <EF5CCC93-1AC6-4E03-BD23-10CDC6C29FBC@comcast.net>


> On Nov 24, 2016, at 7:03 PM, Erin Hodgess <erinm.hodgess at gmail.com> wrote:
> 
> Hello!  Happy Thanksgiving to those who are celebrating.
> 
> I have a zoo series that I am plotting, and I would like to have some
> vertical lines at certain points, to indicate US business cycles.  Here is
> an example:
> 

library(tseries)
> app1 <- get.hist.quote(instrument="aapl",  # note correction
> start="1985-01-01",end="2016-08-31", quote="AdjClose", compression="m")
> #Fine
> plot(app1,main="Historical Stock Prices: Apple Corporation")
> #Still Fine
> #Now I want to use abline at July 1990 and March 1991 (as a start) for
> business cycles.  I tried v=67 and v="1990-07", no good.

#Try this:

abline( v= as.Date("1990-07-01") )

The x-axis is being created using the Date class values and formatting.


> 
> I have a feeling that it's really simple and I'm just not seeing it.
> 
> Any help much appreciated.
> 
> Thanks,
> Erin
> 
> 
> -- 
> Erin Hodgess
> Associate Professor
> Department of Mathematical and Statistics
> University of Houston - Downtown
> mailto: erinm.hodgess at gmail.com
> 
> 	[[alternative HTML version deleted]]

Sigh.

> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From erinm.hodgess at gmail.com  Fri Nov 25 05:37:52 2016
From: erinm.hodgess at gmail.com (Erin Hodgess)
Date: Thu, 24 Nov 2016 22:37:52 -0600
Subject: [R] abline with zoo series
In-Reply-To: <EF5CCC93-1AC6-4E03-BD23-10CDC6C29FBC@comcast.net>
References: <CACxE24=gQ9yO1Hqx0u2473maDkAUBOeJRBqK788BKW+sg1_U8A@mail.gmail.com>
	<EF5CCC93-1AC6-4E03-BD23-10CDC6C29FBC@comcast.net>
Message-ID: <CACxE24nNhOz8o4E4ewP8tZFkBYoOVoigWz3q+ee9hUcM+T2sgw@mail.gmail.com>

Awesome!!!!
Thanks!!!!!

Sincerely,
Erin


On Thu, Nov 24, 2016 at 10:14 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Nov 24, 2016, at 7:03 PM, Erin Hodgess <erinm.hodgess at gmail.com>
> wrote:
> >
> > Hello!  Happy Thanksgiving to those who are celebrating.
> >
> > I have a zoo series that I am plotting, and I would like to have some
> > vertical lines at certain points, to indicate US business cycles.  Here
> is
> > an example:
> >
>
> library(tseries)
> > app1 <- get.hist.quote(instrument="aapl",  # note correction
> > start="1985-01-01",end="2016-08-31", quote="AdjClose", compression="m")
> > #Fine
> > plot(app1,main="Historical Stock Prices: Apple Corporation")
> > #Still Fine
> > #Now I want to use abline at July 1990 and March 1991 (as a start) for
> > business cycles.  I tried v=67 and v="1990-07", no good.
>
> #Try this:
>
> abline( v= as.Date("1990-07-01") )
>
> The x-axis is being created using the Date class values and formatting.
>
>
> >
> > I have a feeling that it's really simple and I'm just not seeing it.
> >
> > Any help much appreciated.
> >
> > Thanks,
> > Erin
> >
> >
> > --
> > Erin Hodgess
> > Associate Professor
> > Department of Mathematical and Statistics
> > University of Houston - Downtown
> > mailto: erinm.hodgess at gmail.com
> >
> >       [[alternative HTML version deleted]]
>
> Sigh.
>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
>


-- 
Erin Hodgess
Associate Professor
Department of Mathematical and Statistics
University of Houston - Downtown
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From valkremk at gmail.com  Fri Nov 25 05:50:29 2016
From: valkremk at gmail.com (Val)
Date: Thu, 24 Nov 2016 22:50:29 -0600
Subject: [R] Variable
Message-ID: <CAJOiR6a-BKMXKJ5WdEC=TbcXv+w-=Mh7RWemwC0DdTKwNLLdQQ@mail.gmail.com>

Hi all,

I am trying to get shell variable(s) into my R script  in Linux . How
do I get them?

my shell script is
t1.sh
 #!bin/bash
       Name=Alex; export Name
       Age=25; export Age


How do get the Name and Age variables in my R script?

My R script is

test.R
print " Your Name is $Name and  you are $Age  years old"

My another shell script that call the R script is

test.sh
         #!bin/bash
         source  t1.sh
         Rscript test.R
So by running this script  ./test.sh

I want get:  Your Name is Alex and  you are 25  years old

I can define those variables in R  but that is not my intention.

Thank you in advance


From jdnewmil at dcn.davis.ca.us  Fri Nov 25 05:59:10 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 24 Nov 2016 20:59:10 -0800
Subject: [R] Variable
In-Reply-To: <CAJOiR6a-BKMXKJ5WdEC=TbcXv+w-=Mh7RWemwC0DdTKwNLLdQQ@mail.gmail.com>
References: <CAJOiR6a-BKMXKJ5WdEC=TbcXv+w-=Mh7RWemwC0DdTKwNLLdQQ@mail.gmail.com>
Message-ID: <B3B2E1F9-09FA-4C08-BF55-26CC176B264E@dcn.davis.ca.us>

?commandArgs
-- 
Sent from my phone. Please excuse my brevity.

On November 24, 2016 8:50:29 PM PST, Val <valkremk at gmail.com> wrote:
>Hi all,
>
>I am trying to get shell variable(s) into my R script  in Linux . How
>do I get them?
>
>my shell script is
>t1.sh
> #!bin/bash
>       Name=Alex; export Name
>       Age=25; export Age
>
>
>How do get the Name and Age variables in my R script?
>
>My R script is
>
>test.R
>print " Your Name is $Name and  you are $Age  years old"
>
>My another shell script that call the R script is
>
>test.sh
>         #!bin/bash
>         source  t1.sh
>         Rscript test.R
>So by running this script  ./test.sh
>
>I want get:  Your Name is Alex and  you are 25  years old
>
>I can define those variables in R  but that is not my intention.
>
>Thank you in advance
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Fri Nov 25 06:27:05 2016
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 25 Nov 2016 00:27:05 -0500
Subject: [R] abline with zoo series
In-Reply-To: <CACxE24=gQ9yO1Hqx0u2473maDkAUBOeJRBqK788BKW+sg1_U8A@mail.gmail.com>
References: <CACxE24=gQ9yO1Hqx0u2473maDkAUBOeJRBqK788BKW+sg1_U8A@mail.gmail.com>
Message-ID: <CAP01uRk1Y9a8pV0qwg=6RksPZj6xcUB-z_xjR4nRbbYEXpPrRQ@mail.gmail.com>

Recessions are typically shown by shading.  The zoo package has
xblocks for this purpose.  If app1 is your zoo object then:

plot(app1)
tt <- time(app1)
xblocks(tt, tt >= "1990-07-01" & tt <= "1991-03-31",
   col = rgb(0.7, 0.7, 0.7, 0.5)) # transparent grey

See ?xblocks for more info.



On Thu, Nov 24, 2016 at 10:03 PM, Erin Hodgess <erinm.hodgess at gmail.com> wrote:
> Hello!  Happy Thanksgiving to those who are celebrating.
>
> I have a zoo series that I am plotting, and I would like to have some
> vertical lines at certain points, to indicate US business cycles.  Here is
> an example:
>
> app1 <- get.hist.quote(instrument="appl",
> start="1985-01-01",end="2016-08-31", quote="AdjClose", compression="m")
> #Fine
> plot(app1,main="Historical Stock Prices: Apple Corporation")
> #Still Fine
> #Now I want to use abline at July 1990 and March 1991 (as a start) for
> business cycles.  I tried v=67 and v="1990-07", no good.
>
> I have a feeling that it's really simple and I'm just not seeing it.
>
> Any help much appreciated.
>
> Thanks,
> Erin
>
>
> --
> Erin Hodgess
> Associate Professor
> Department of Mathematical and Statistics
> University of Houston - Downtown
> mailto: erinm.hodgess at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From dwinsemius at comcast.net  Fri Nov 25 06:29:15 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 24 Nov 2016 21:29:15 -0800
Subject: [R] Variable
In-Reply-To: <CAJOiR6a-BKMXKJ5WdEC=TbcXv+w-=Mh7RWemwC0DdTKwNLLdQQ@mail.gmail.com>
References: <CAJOiR6a-BKMXKJ5WdEC=TbcXv+w-=Mh7RWemwC0DdTKwNLLdQQ@mail.gmail.com>
Message-ID: <ADC6A753-0EEC-4532-9A41-0507ECEC7A96@comcast.net>


> On Nov 24, 2016, at 8:50 PM, Val <valkremk at gmail.com> wrote:
> 
> Hi all,
> 
> I am trying to get shell variable(s) into my R script  in Linux . How
> do I get them?
> 
> my shell script is
> t1.sh
> #!bin/bash
>       Name=Alex; export Name
>       Age=25; export Age
> 
> 
> How do get the Name and Age variables in my R script?
> 
> My R script is
> 
> test.R
> print " Your Name is $Name and  you are $Age  years old"

You might want to look at `?system` and `?Sys.getenv`. Do note that for an R function the arguments need to be flanked by parentheses.

> 
> My another shell script that call the R script is
> 
> test.sh
>         #!bin/bash
>         source  t1.sh
>         Rscript test.R
> So by running this script  ./test.sh
> 
> I want get:  Your Name is Alex and  you are 25  years old
> 
> I can define those variables in R  but that is not my intention.


David Winsemius
Alameda, CA, USA


From erinm.hodgess at gmail.com  Fri Nov 25 07:53:35 2016
From: erinm.hodgess at gmail.com (Erin Hodgess)
Date: Fri, 25 Nov 2016 00:53:35 -0600
Subject: [R] abline with zoo series
In-Reply-To: <CAP01uRk1Y9a8pV0qwg=6RksPZj6xcUB-z_xjR4nRbbYEXpPrRQ@mail.gmail.com>
References: <CACxE24=gQ9yO1Hqx0u2473maDkAUBOeJRBqK788BKW+sg1_U8A@mail.gmail.com>
	<CAP01uRk1Y9a8pV0qwg=6RksPZj6xcUB-z_xjR4nRbbYEXpPrRQ@mail.gmail.com>
Message-ID: <CACxE24=q9NN05GX-98fJree5zzBvDNa_F_qgjmdF9bTLCQQZOA@mail.gmail.com>

Nice!  That's perfect!

Thanks very much!

Sincerely,
Erin

On Thu, Nov 24, 2016 at 11:27 PM, Gabor Grothendieck <
ggrothendieck at gmail.com> wrote:

> Recessions are typically shown by shading.  The zoo package has
> xblocks for this purpose.  If app1 is your zoo object then:
>
> plot(app1)
> tt <- time(app1)
> xblocks(tt, tt >= "1990-07-01" & tt <= "1991-03-31",
>    col = rgb(0.7, 0.7, 0.7, 0.5)) # transparent grey
>
> See ?xblocks for more info.
>
>
>
> On Thu, Nov 24, 2016 at 10:03 PM, Erin Hodgess <erinm.hodgess at gmail.com>
> wrote:
> > Hello!  Happy Thanksgiving to those who are celebrating.
> >
> > I have a zoo series that I am plotting, and I would like to have some
> > vertical lines at certain points, to indicate US business cycles.  Here
> is
> > an example:
> >
> > app1 <- get.hist.quote(instrument="appl",
> > start="1985-01-01",end="2016-08-31", quote="AdjClose", compression="m")
> > #Fine
> > plot(app1,main="Historical Stock Prices: Apple Corporation")
> > #Still Fine
> > #Now I want to use abline at July 1990 and March 1991 (as a start) for
> > business cycles.  I tried v=67 and v="1990-07", no good.
> >
> > I have a feeling that it's really simple and I'm just not seeing it.
> >
> > Any help much appreciated.
> >
> > Thanks,
> > Erin
> >
> >
> > --
> > Erin Hodgess
> > Associate Professor
> > Department of Mathematical and Statistics
> > University of Houston - Downtown
> > mailto: erinm.hodgess at gmail.com
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com
>



-- 
Erin Hodgess
Associate Professor
Department of Mathematical and Statistics
University of Houston - Downtown
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From Z.Alharbi at uea.ac.uk  Fri Nov 25 15:47:44 2016
From: Z.Alharbi at uea.ac.uk (Zahyah Alharbi (CMP))
Date: Fri, 25 Nov 2016 14:47:44 +0000
Subject: [R] error with caretEnsmble of different training datasets of SVM -
 need help
Message-ID: <AM4PR0401MB19220DFE62D1C8AA2CFB3D56BE890@AM4PR0401MB1922.eurprd04.prod.outlook.com>

Hi,
The following is a reproducible example , what basically I am trying to do , is creating five imputed datasets then apply SVM to each imputed dataset using the train function in caret, then ensemble the resulted training model using caretEnsemble. Lastly, I am predicting each test set using the ensemble model.

 However, I have this error (Error: { .... is not TRUE) occur with caretEnsemble although I converted the list of the resulted model to caretlist.

Any help is truly appreciated.

        library(mice)
        library(e1071)
        library(caret)
        library("caretEnsemble")

    data <- iris
    #Generate 10% missing values at Random
    iris.mis <- prodNA(iris, noNA = 0.1)
    #remove categorical variables
    iris.mis <- subset(iris.mis, select = -c(Species))

    # 5 Imputation using mice pmm

    imp <- mice(iris.mis, m=5, maxit = 10, method = 'pmm', seed = 500)

    # save 5 imputed dataset.
    x1 <- complete(imp, action = 1, include = FALSE)
    x2 <- complete(imp, action = 2, include = FALSE)
    x3 <- complete(imp, action = 3, include = FALSE)
    x4 <- complete(imp, action = 4, include = FALSE)
    x5 <- complete(imp, action = 5, include = FALSE)

    ## Apply the following method with 10 fold across validation for each imputed set and Compute rmse for each imputed set
    avg.rmse <- NULL
    avg.foldrmse <- matrix(data = NA,nrow=5, ncol=1)
    SDofMean.rmse <- NULL
    form <- iris$Sepal.Width # target coloumn
    fold <- 10  # number of fold for cross validation
    n <- nrow(x1)  # since all data sample are the same length
    prop <- n%/%fold
    set.seed(7)
    newseq <- rank(runif(n))
    k <- as.factor((newseq - 1)%/%prop + 1)
    y <- unlist(strsplit(as.character(form), " "))[2]
    vec.error <- vector(length = fold)

    for (i in seq(fold))
    {
      avg.foldrmse <- NULL
      # Perfrom SVM method on each imputed dataset
      fit1 <- train(Sepal.Width ~., data = x1[k != i, ],method='svmLinear2')
      fit2 <- train(Sepal.Width ~., data = x2[k != i, ],method='svmLinear2')
      fit3 <- train(Sepal.Width ~., data = x3[k != i, ],method='svmLinear2')
      fit4 <- train(Sepal.Width ~., data = x4[k != i, ],method='svmLinear2')
      fit5 <- train(Sepal.Width ~., data = x5[k != i, ],method='svmLinear2')


      #combine in the created model to a list
      svm.fit <- list(svmLinear1 = fit1, svmLinear2 = fit2, svmLinear3 = fit3, svmLinear4 = fit4, svmLinear5 = fit5)

      # convert the list to cartlist
      class(svm.fit) <- "caretList"

      #create the ensemble where the error occur.
      svm.all <- caretEnsemble(svm.fit,method='svmLinear2')


      # predict the 5 test set using the ensemble model and compute the RMSE
      fcast1 <- predict(svm.all, newdata = x1[k == i, ])
      rmse1 <-  sqrt(mean((x1[k == i, ]$Sepal.Width - fcast1)^2))
      avg.foldrmse[1] <- rmse1
      # predict using test set of the Second imputed dataset
      fcast2 <- predict(svm.all, newdata = x2[k == i, ])
      rmse2 <-  sqrt(mean((x2[k == i, ]$Sepal.Width- fcast2)^2))
      avg.foldrmse[2] <- rmse2
      # predict using test set of the Third imputed dataset
      fcast3 <- predict(svm.all, newdata = x3[k == i, ])
      rmse3 <-  sqrt(mean((x3[k == i, ]$Sepal.Width- fcast3)^2))
      avg.foldrmse[3] <- rmse3
      # predict using test set of the fourth imputed dataset
      fcast4 <- predict(svm.all, newdata = x4[k == i, ])
      rmse4 <-  sqrt(mean((x4[k == i, ]$Sepal.Width - fcast4)^2))
      avg.foldrmse[4] <- rmse4
      # predict using test set of the fifth imputed dataset
      fcast5 <- predict(svm.all, newdata = x5[k == i, ])
      rmse5 <-  sqrt(mean((x5[k == i, ]$Sepal.Width - fcast5)^2))
      avg.foldrmse[5] <- rmse5

    }# end loop



Regards,
Zawahy

	[[alternative HTML version deleted]]


From nell.redu at hotmail.fr  Fri Nov 25 19:04:52 2016
From: nell.redu at hotmail.fr (Nelly Reduan)
Date: Fri, 25 Nov 2016 18:04:52 +0000
Subject: [R] GAMs: predictions with the number of live traps as an offset
	variable
Message-ID: <CY1PR05MB27301CEAE1290E80337B46B399890@CY1PR05MB2730.namprd05.prod.outlook.com>

Hello,

I built a Generalized Additive Model (GAM) using a negative binomial distribution (my response variable represents an animal count). In the GAM, I included the log(trapping effort) as an offset variable to account for variations in the sampling effort across trapping sites. Trapping effort was calculated as the total number of live traps per night. Is there a way to fit the GAM at each raster pixel defining a landscape when the trapping effort in the pixel is unknown (some landscape areas do not cover the trapping sites)? In particular, I would like to use the GAM in order to build a predictive map of abundance.

Thanks a lot for your help.
Nell


	[[alternative HTML version deleted]]


From ticor.vaakav at gmail.com  Fri Nov 25 18:49:24 2016
From: ticor.vaakav at gmail.com (TicoR)
Date: Fri, 25 Nov 2016 23:19:24 +0530
Subject: [R] How to find coefficient of determination for pareto
	distribution in R?
Message-ID: <CAPZYZO4bL3rmy84wz=Jefn7=qbke23gvpGGJ_Xskx0GGGVmfsA@mail.gmail.com>

How to find coefficient of determination for pareto distribution in R?

I have made model using generalized Pareto distribution. How can I find the
coefficient of distribution for model in R

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Fri Nov 25 21:23:10 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 25 Nov 2016 21:23:10 +0100
Subject: [R] GAM with the negative binomial distribution: why do
	predictions no match with original values?
In-Reply-To: <cf00ba2e-47d4-964d-e903-e48435e647fc@bath.edu>
References: <AM5PR0701MB2338A474BE0056E27BD32432E2B40@AM5PR0701MB2338.eurprd07.prod.outlook.com>
	<CAGxFJbTT8MQvU+y5iMGBznD85ATgDa=4fqeh71Y+aJPN+LVqzA@mail.gmail.com>
	<B87E98BB-8D00-4933-8586-B6CF7638C74E@gmail.com>
	<AM5PR0701MB2338C813C8EE72DBBA5982CFE2B70@AM5PR0701MB2338.eurprd07.prod.outlook.com>
	<cf00ba2e-47d4-964d-e903-e48435e647fc@bath.edu>
Message-ID: <7464B4A0-09DD-45D1-97B3-674361E575F5@gmail.com>


> On 23 Nov 2016, at 23:32 , Simon Wood <simon.wood at bath.edu> wrote:
> 
> ?predict.gam (mgcv) says....
>  
>    "Note that, in common with other prediction functions, any offset
>      supplied to ?gam? as an argument is always ignored when
>      predicting, unlike offsets specified in the gam model formula."
> 


> .... which was originally implemented to prevent surprises to people familiar with predict.lm (which used to behave that way, and was documented to behave that way).... the problem is that predict.lm doesn't behave like that any more, so I guess at some point I should remove this feature...
> 

Fortune candidate...

-pd

> best,
> Simon 
>  
> 
> 
> 
> On 23/11/16 00:24, Marine Regis wrote:
>> Thanks a lot for your answers.
>> Peter: sorry, here is the missing information:
>> 
>>   *   I use the function gam() of the package ?mgcv?
>>   *   Yes, the output changes when I use offset(log_trap_eff) instead of offset=log_trap_eff. By using offset(log_trap_eff), the output is more coherent with the observed values. Here are the new predictions:
>> 
>>> summary(mod$fit)
>>> 
>> Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>> 10.01   68.14   85.71   83.16  101.00  130.20
>> 
>> 
>>   *   I have tried to create a reproductive example to show the difference between offset(log_trap_eff) and offset=log_trap_eff.
>> nb_unique <- rnegbin(58, mu=82, theta=13.446)
>> x <- runif(58,min=-465300,max=435200)
>> prop_forest <- runif(58,min=0,max=1)
>> log_trap_eff <- runif(58,min=4,max=6)
>> 
>> With offset=log_trap_eff:
>> 
>> mod1 <- gam(nb_unique ~ s(x,prop_forest), offset=log_trap_eff, family=nb(theta=NULL, link="log"), method = "REML", select = TRUE)
>> 
>> 
>>> mod1Pred <- predict.gam(mod1, se.fit=TRUE, type="response")
>>> 
>>> summary(mod1Pred$fit)
>>> 
>>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>> 
>>  0.5852  0.5852  0.5852  0.5852  0.5852  0.5852
>> 
>> 
>> With offset(log_trap_eff):
>> mod2 <- gam(nb_unique ~ s(x,prop_forest) + offset(log_trap_eff), family=nb(theta=NULL, link="log"), method = "REML", select = TRUE)
>> 
>> 
>>> mod2Pred <- predict.gam(mod2, se.fit=TRUE, type="response")
>>> 
>>> summary(mod2Pred$fit)
>>> 
>>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>> 
>>   32.03   61.18   97.20  112.20  165.00  226.00
>> 
>> 
>> 
>> Value range of observed data:
>> 
>> 
>>> summary(nb_unique)
>>> 
>>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>> 
>>   43.00   67.00   81.00   84.16   92.75  153.00
>> 
>> 
>>   *   By using fitted(mod), I obtain NULL.
>> I am a novice in GAMs. So, I don?t know why the results are different between models with offset=argument and offset().
>> Thanks a lot for your help.
>> Have a nice day
>> Marine
>> 
>> 
>> 
>> ________________________________
>> De : peter dalgaard 
>> <pdalgd at gmail.com>
>> 
>> Envoy? : mardi 22 novembre 2016 23:52
>> ? : Bert Gunter
>> Cc : Marine Regis; 
>> r-help at r-project.org
>> 
>> Objet : Re: [R] GAM with the negative binomial distribution: why do predictions no match with original values?
>> 
>> 
>> 
>>> On 22 Nov 2016, at 23:07 , Bert Gunter <bgunter.4567 at gmail.com>
>>>  wrote:
>>> 
>>> Define "very different."  Sounds like a subjective opinion to me, for
>>> which I have no response. Apparently others are similarly flummoxed.
>>> Of course they would not in general be identical.
>>> 
>> Er? I don't see much reason to disagree that a range 0.10-0.18 is different from 17-147.
>> 
>> However, other bits of information are missing: We don't know which gam() function is being used (to my knowledge there is one in package gam but also one in mgcv). We don't have the data, so we cannot reproduce and try to find the root of the problem.
>> 
>> Offhand, it looks like the predict.gam() function is misbehaving, which could have something to do with the offset term and/or the nb dispersion parameter. On a hunch, does anything change if you use
>> 
>> nb_unique ~ s(x,prop_forest) + offset(log_trap_eff)
>> 
>> instead of the offset= argument? And, by the way, does fitted(mod,...) change anything?
>> 
>> -pd
>> 
>> 
>>> Cheers,
>>> Bert
>>> 
>>> 
>>> Bert Gunter
>>> 
>>> "The trouble with having an open mind is that people keep coming along
>>> and sticking things into it."
>>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>> 
>>> 
>>> On Tue, Nov 22, 2016 at 1:29 PM, Marine Regis 
>>> <marine.regis at hotmail.fr>
>>>  wrote:
>>> 
>>>> Hello,
>>>> 
>>>> 
>>>>> From capture data, I would like to assess the effect of longitudinal changes in proportion of forests on abundance of skunks. To test this, I built this GAM where the dependent variable is the number of unique skunks and the independent variables are the X coordinates of the centroids of trapping sites (called "X" in the GAM) and the proportion of forests within the trapping sites (called "prop_forest" in the GAM):
>>>>> 
>>>>    mod <- gam(nb_unique ~ s(x,prop_forest), offset=log_trap_eff, family=nb(theta=NULL, link="log"), data=succ_capt_skunk, method = "REML", select = TRUE)
>>>>    summary(mod)
>>>> 
>>>>    Family: Negative Binomial(13.446)
>>>>    Link function: log
>>>> 
>>>>    Formula:
>>>>    nb_unique ~ s(x, prop_forest)
>>>> 
>>>>    Parametric coefficients:
>>>>                Estimate Std. Error z value Pr(>|z|)
>>>>    (Intercept) -2.02095    0.03896  -51.87   <2e-16 ***
>>>>    ---
>>>>    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>>> 
>>>>    Approximate significance of smooth terms:
>>>>                       edf Ref.df Chi.sq  p-value
>>>>    s(x,prop_forest) 3.182     29  17.76 0.000102 ***
>>>>    ---
>>>>    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>>> 
>>>>    R-sq.(adj) =   0.37   Deviance explained =   49%
>>>>    -REML = 268.61  Scale est. = 1         n = 58
>>>> 
>>>> 
>>>> I built a GAM  for the negative binomial family. When I use the function `predict.gam`, the predictions of capture success from the GAM and the values of capture success from original data are very different. What is the reason for differences occur?
>>>> 
>>>> **With GAM:**
>>>> 
>>>>    modPred <- predict.gam(mod, se.fit=TRUE,type="response")
>>>>    summary(modPred$fit)
>>>>       Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>>>>     0.1026  0.1187  0.1333  0.1338  0.1419  0.1795
>>>> 
>>>> **With original data:**
>>>> 
>>>>    summary(succ_capt_skunk$nb_unique)
>>>>       Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>>>>      17.00   59.00   82.00   81.83  106.80  147.00
>>>> 
>>>> The question has already been posted on Cross validated (
>>>> http://stats.stackexchange.com/questions/247347/gam-with-the-negative-binomial-distribution-why-do-predictions-no-match-with-or
>>>> ) without success.
>>>> 
>> [http://cdn.sstatic.net/Sites/stats/img/apple-touch-icon at 2.png?v=344f57aa10cc&a]<http://stats.stackexchange.com/questions/247347/gam-with-the-negative-binomial-distribution-why-do-predictions-no-match-with-or>
>> 
>> 
>> GAM with the negative binomial distribution: why do predictions no match with original values?
>> <http://stats.stackexchange.com/questions/247347/gam-with-the-negative-binomial-distribution-why-do-predictions-no-match-with-or>
>> 
>> stats.stackexchange.com
>> >From capture data, I would like to assess the effect of longitudinal changes in proportion of forests on abundance of skunks. To test this, I built this GAM where the dependent variable is the numb...
>> 
>> 
>> 
>> 
>>>> Thanks a lot for your time.
>>>> Have a nice day
>>>> Marine
>>>> 
>>>> 
>>>>        [[alternative HTML version deleted]]
>>>> 
>>>> ______________________________________________
>>>> 
>>>> R-help at r-project.org
>>>>  mailing list -- To UNSUBSCRIBE and more, see
>>>> 
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> thz.ch/mailman/listinfo/r-help>
>> stat.ethz.ch
>> The main R mailing list, for announcements about the development of R and the availability of new code, questions and answers about problems and solutions using R ...
>> 
>> 
>> 
>> 
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> 
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>> ______________________________________________
>>> 
>>> R-help at r-project.org
>>>  mailing list -- To UNSUBSCRIBE and more, see
>>> 
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> thz.ch/mailman/listinfo/r-help>
>> stat.ethz.ch
>> The main R mailing list, for announcements about the development of R and the availability of new code, questions and answers about problems and solutions using R ...
>> 
>> 
>> 
>> 
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> 
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: 
>> pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> 
>> 
>> 
>> ______________________________________________
>> 
>> R-help at r-project.org
>>  mailing list -- To UNSUBSCRIBE and more, see
>> 
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> 
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> 
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> 
> -- 
> Simon Wood, School of Mathematics, University of Bristol BS8 1TW UK
> +44 (0)117 33 18273     
> http://www.maths.bris.ac.uk/~sw15190

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From chocold12 at gmail.com  Sat Nov 26 17:11:35 2016
From: chocold12 at gmail.com (lily li)
Date: Sat, 26 Nov 2016 09:11:35 -0700
Subject: [R] About data manipulation
Message-ID: <CAN5afy-tQDYjEm5ZObphkbap5D6nJXMPk6BTTBTmhZL0LvggfQ@mail.gmail.com>

Hi R users,

I'm trying to manipulate a dataframe and have some difficulties.

The original dataset is like this:

DF
year   month   total   id     note
2000     1         98    GA   1
2001     1        100   GA   1
2002     2         99    GA   1
2002     2         80    GB   1
...
2012     1         78    GA   2
...

The structure is like this: when year is between 2000-2005, note is 1; when
year is between 2006-2010, note is 2; GA, GB, etc represent different
groups, but they all have years 2000-2005, 2006-2010, 2011-2015.
I want to calculate one average value for each month in each time slice.
For example, between 2000-2005, when note is 1, for GA, there is one value
in month 1, one value in month 2, etc; for GB, there is one value in month
1, one value in month 2, between this time period. So later, there is no
'year' column, but other columns.
I tried the script: DF_GA = aggregate(total~year+month,data=subset(DF,
id==GA&note==1)), but it did not give me the ideal dataframe. How to do
then?
Thanks for your help.

	[[alternative HTML version deleted]]


From philipt900 at iinet.net.au  Sun Nov 27 00:42:17 2016
From: philipt900 at iinet.net.au (P Tennant)
Date: Sun, 27 Nov 2016 10:42:17 +1100
Subject: [R] About data manipulation
In-Reply-To: <CAN5afy-tQDYjEm5ZObphkbap5D6nJXMPk6BTTBTmhZL0LvggfQ@mail.gmail.com>
References: <CAN5afy-tQDYjEm5ZObphkbap5D6nJXMPk6BTTBTmhZL0LvggfQ@mail.gmail.com>
Message-ID: <583A1DD9.3040000@iinet.net.au>

Hi,

It may help that:

aggregate(DF$total, list(DF$note, DF$id, DF$month), mean)

should give you means broken down by time slice (note), id and month. 
You could then subset means for GA or GB from the aggregated dataframe.

Philip

On 27/11/2016 3:11 AM, lily li wrote:
> Hi R users,
>
> I'm trying to manipulate a dataframe and have some difficulties.
>
> The original dataset is like this:
>
> DF
> year   month   total   id     note
> 2000     1         98    GA   1
> 2001     1        100   GA   1
> 2002     2         99    GA   1
> 2002     2         80    GB   1
> ...
> 2012     1         78    GA   2
> ...
>
> The structure is like this: when year is between 2000-2005, note is 1; when
> year is between 2006-2010, note is 2; GA, GB, etc represent different
> groups, but they all have years 2000-2005, 2006-2010, 2011-2015.
> I want to calculate one average value for each month in each time slice.
> For example, between 2000-2005, when note is 1, for GA, there is one value
> in month 1, one value in month 2, etc; for GB, there is one value in month
> 1, one value in month 2, between this time period. So later, there is no
> 'year' column, but other columns.
> I tried the script: DF_GA = aggregate(total~year+month,data=subset(DF,
> id==GA&note==1)), but it did not give me the ideal dataframe. How to do
> then?
> Thanks for your help.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Sun Nov 27 01:10:28 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 26 Nov 2016 16:10:28 -0800
Subject: [R] About data manipulation
In-Reply-To: <583A1DD9.3040000@iinet.net.au>
References: <CAN5afy-tQDYjEm5ZObphkbap5D6nJXMPk6BTTBTmhZL0LvggfQ@mail.gmail.com>
	<583A1DD9.3040000@iinet.net.au>
Message-ID: <CAGxFJbQrBK=0mfNVGzXPduqsNLYu3yZK5AFAtksyb6xQgC-BhQ@mail.gmail.com>

A reproducible example was not provided, but I think what is wanted is
either ?tapply or ?ave; e.g.

within(DF, means <- ave(total, note, month, FUN = mean))


Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Nov 26, 2016 at 3:42 PM, P Tennant <philipt900 at iinet.net.au> wrote:
> Hi,
>
> It may help that:
>
> aggregate(DF$total, list(DF$note, DF$id, DF$month), mean)
>
> should give you means broken down by time slice (note), id and month. You
> could then subset means for GA or GB from the aggregated dataframe.
>
> Philip
>
> On 27/11/2016 3:11 AM, lily li wrote:
>>
>> Hi R users,
>>
>> I'm trying to manipulate a dataframe and have some difficulties.
>>
>> The original dataset is like this:
>>
>> DF
>> year   month   total   id     note
>> 2000     1         98    GA   1
>> 2001     1        100   GA   1
>> 2002     2         99    GA   1
>> 2002     2         80    GB   1
>> ...
>> 2012     1         78    GA   2
>> ...
>>
>> The structure is like this: when year is between 2000-2005, note is 1;
>> when
>> year is between 2006-2010, note is 2; GA, GB, etc represent different
>> groups, but they all have years 2000-2005, 2006-2010, 2011-2015.
>> I want to calculate one average value for each month in each time slice.
>> For example, between 2000-2005, when note is 1, for GA, there is one value
>> in month 1, one value in month 2, etc; for GB, there is one value in month
>> 1, one value in month 2, between this time period. So later, there is no
>> 'year' column, but other columns.
>> I tried the script: DF_GA = aggregate(total~year+month,data=subset(DF,
>> id==GA&note==1)), but it did not give me the ideal dataframe. How to do
>> then?
>> Thanks for your help.
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jholtman at gmail.com  Sun Nov 27 01:55:08 2016
From: jholtman at gmail.com (jim holtman)
Date: Sat, 26 Nov 2016 19:55:08 -0500
Subject: [R] About data manipulation
In-Reply-To: <CAN5afy-tQDYjEm5ZObphkbap5D6nJXMPk6BTTBTmhZL0LvggfQ@mail.gmail.com>
References: <CAN5afy-tQDYjEm5ZObphkbap5D6nJXMPk6BTTBTmhZL0LvggfQ@mail.gmail.com>
Message-ID: <CAAxdm-74RHFchxM+SE+SrnG0hubCLDGzn3o4HgqRv0Qqzych0Q@mail.gmail.com>

You did not provide any data, but I will take a stab at it using the
"dplyr" package

library(dplyr)
DT %>%
    group_by(month, id, note) %>%
    summarise(avg = mean(total))



Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Sat, Nov 26, 2016 at 11:11 AM, lily li <chocold12 at gmail.com> wrote:

> Hi R users,
>
> I'm trying to manipulate a dataframe and have some difficulties.
>
> The original dataset is like this:
>
> DF
> year   month   total   id     note
> 2000     1         98    GA   1
> 2001     1        100   GA   1
> 2002     2         99    GA   1
> 2002     2         80    GB   1
> ...
> 2012     1         78    GA   2
> ...
>
> The structure is like this: when year is between 2000-2005, note is 1; when
> year is between 2006-2010, note is 2; GA, GB, etc represent different
> groups, but they all have years 2000-2005, 2006-2010, 2011-2015.
> I want to calculate one average value for each month in each time slice.
> For example, between 2000-2005, when note is 1, for GA, there is one value
> in month 1, one value in month 2, etc; for GB, there is one value in month
> 1, one value in month 2, between this time period. So later, there is no
> 'year' column, but other columns.
> I tried the script: DF_GA = aggregate(total~year+month,data=subset(DF,
> id==GA&note==1)), but it did not give me the ideal dataframe. How to do
> then?
> Thanks for your help.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From alemu.tadesse at gmail.com  Sun Nov 27 02:04:26 2016
From: alemu.tadesse at gmail.com (Alemu Tadesse)
Date: Sat, 26 Nov 2016 17:04:26 -0800
Subject: [R] Merra2 files
Message-ID: <CACGkHRO_qYDNKSL-u0fEC+Wv0Th8rb_n6Bm7FbWzzLsCRtc7MA@mail.gmail.com>

Dear R users,

I am wondering if someone has a script to download and read Merra 2 files.

I really appreciate your help.

Best,

Alemu

	[[alternative HTML version deleted]]


From tom at vims.edu  Sun Nov 27 00:18:56 2016
From: tom at vims.edu (Tom Mosca)
Date: Sat, 26 Nov 2016 23:18:56 +0000
Subject: [R] Partial Fraction Decomposition
Message-ID: <CB5791F5EA3D82408B277900997482D27F7E3B32@mboxes2.campus.vims.edu>

Hello Folks,

As a neophyte R user I frequently have questions, and I?m sorry to bother experienced users with what may appear to be trivial questions to which I should be able to find answers without help.  I?ve searched everywhere I know to look, and can?t find any reference to this question.  Perhaps I just haven't guessed a correct keyword for my searches.  I apologize in advance.

Given a rational expression P/Q with P and Q being polynomials that are prime in relation to each other, and with Q factored, is there an R function or package that will return the partial fraction decomposition of P/Q?

For example:
Given (3x^3+x^2-8x+6)/(x^2)(x-1)^2
Return 4/x + 6/x^2 ? 1/(x-1) + 2/(x-1)^2

Thank you, Tom

	[[alternative HTML version deleted]]


From chocold12 at gmail.com  Sun Nov 27 08:03:30 2016
From: chocold12 at gmail.com (lily li)
Date: Sun, 27 Nov 2016 00:03:30 -0700
Subject: [R] About data manipulation
In-Reply-To: <CAAxdm-74RHFchxM+SE+SrnG0hubCLDGzn3o4HgqRv0Qqzych0Q@mail.gmail.com>
References: <CAN5afy-tQDYjEm5ZObphkbap5D6nJXMPk6BTTBTmhZL0LvggfQ@mail.gmail.com>
	<CAAxdm-74RHFchxM+SE+SrnG0hubCLDGzn3o4HgqRv0Qqzych0Q@mail.gmail.com>
Message-ID: <CAN5afy94AxGM7pgQpLUXb5FVajGfut4WgAa_isqLAFr2umzwvA@mail.gmail.com>

Thanks Jim, this method is very convenient and is what I want. Could I know
how to save the resulted dataframe? It printed in the console directly.

On Sat, Nov 26, 2016 at 5:55 PM, jim holtman <jholtman at gmail.com> wrote:

> You did not provide any data, but I will take a stab at it using the
> "dplyr" package
>
> library(dplyr)
> DT %>%
>     group_by(month, id, note) %>%
>     summarise(avg = mean(total))
>
>
>
> Jim Holtman
> Data Munger Guru
>
> What is the problem that you are trying to solve?
> Tell me what you want to do, not how you want to do it.
>
> On Sat, Nov 26, 2016 at 11:11 AM, lily li <chocold12 at gmail.com> wrote:
>
>> Hi R users,
>>
>> I'm trying to manipulate a dataframe and have some difficulties.
>>
>> The original dataset is like this:
>>
>> DF
>> year   month   total   id     note
>> 2000     1         98    GA   1
>> 2001     1        100   GA   1
>> 2002     2         99    GA   1
>> 2002     2         80    GB   1
>> ...
>> 2012     1         78    GA   2
>> ...
>>
>> The structure is like this: when year is between 2000-2005, note is 1;
>> when
>> year is between 2006-2010, note is 2; GA, GB, etc represent different
>> groups, but they all have years 2000-2005, 2006-2010, 2011-2015.
>> I want to calculate one average value for each month in each time slice.
>> For example, between 2000-2005, when note is 1, for GA, there is one value
>> in month 1, one value in month 2, etc; for GB, there is one value in month
>> 1, one value in month 2, between this time period. So later, there is no
>> 'year' column, but other columns.
>> I tried the script: DF_GA = aggregate(total~year+month,data=subset(DF,
>> id==GA&note==1)), but it did not give me the ideal dataframe. How to do
>> then?
>> Thanks for your help.
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From jholtman at gmail.com  Sun Nov 27 08:33:58 2016
From: jholtman at gmail.com (jim holtman)
Date: Sun, 27 Nov 2016 02:33:58 -0500
Subject: [R] About data manipulation
In-Reply-To: <CAN5afy94AxGM7pgQpLUXb5FVajGfut4WgAa_isqLAFr2umzwvA@mail.gmail.com>
References: <CAN5afy-tQDYjEm5ZObphkbap5D6nJXMPk6BTTBTmhZL0LvggfQ@mail.gmail.com>
	<CAAxdm-74RHFchxM+SE+SrnG0hubCLDGzn3o4HgqRv0Qqzych0Q@mail.gmail.com>
	<CAN5afy94AxGM7pgQpLUXb5FVajGfut4WgAa_isqLAFr2umzwvA@mail.gmail.com>
Message-ID: <CAAxdm-6Rfzx5rQninaDUn-v08uraPjbNTE8MdanFV7Lw6swhfQ@mail.gmail.com>

just assign it to an object

x<- DT .....


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Sun, Nov 27, 2016 at 2:03 AM, lily li <chocold12 at gmail.com> wrote:

> Thanks Jim, this method is very convenient and is what I want. Could I
> know how to save the resulted dataframe? It printed in the console directly.
>
> On Sat, Nov 26, 2016 at 5:55 PM, jim holtman <jholtman at gmail.com> wrote:
>
>> You did not provide any data, but I will take a stab at it using the
>> "dplyr" package
>>
>> library(dplyr)
>> DT %>%
>>     group_by(month, id, note) %>%
>>     summarise(avg = mean(total))
>>
>>
>>
>> Jim Holtman
>> Data Munger Guru
>>
>> What is the problem that you are trying to solve?
>> Tell me what you want to do, not how you want to do it.
>>
>> On Sat, Nov 26, 2016 at 11:11 AM, lily li <chocold12 at gmail.com> wrote:
>>
>>> Hi R users,
>>>
>>> I'm trying to manipulate a dataframe and have some difficulties.
>>>
>>> The original dataset is like this:
>>>
>>> DF
>>> year   month   total   id     note
>>> 2000     1         98    GA   1
>>> 2001     1        100   GA   1
>>> 2002     2         99    GA   1
>>> 2002     2         80    GB   1
>>> ...
>>> 2012     1         78    GA   2
>>> ...
>>>
>>> The structure is like this: when year is between 2000-2005, note is 1;
>>> when
>>> year is between 2006-2010, note is 2; GA, GB, etc represent different
>>> groups, but they all have years 2000-2005, 2006-2010, 2011-2015.
>>> I want to calculate one average value for each month in each time slice.
>>> For example, between 2000-2005, when note is 1, for GA, there is one
>>> value
>>> in month 1, one value in month 2, etc; for GB, there is one value in
>>> month
>>> 1, one value in month 2, between this time period. So later, there is no
>>> 'year' column, but other columns.
>>> I tried the script: DF_GA = aggregate(total~year+month,data=subset(DF,
>>> id==GA&note==1)), but it did not give me the ideal dataframe. How to do
>>> then?
>>> Thanks for your help.
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posti
>>> ng-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From nicholas.wray at ntlworld.com  Sun Nov 27 10:09:57 2016
From: nicholas.wray at ntlworld.com (WRAY NICHOLAS)
Date: Sun, 27 Nov 2016 09:09:57 +0000 (GMT)
Subject: [R] Partial Fraction Decomposition
In-Reply-To: <CB5791F5EA3D82408B277900997482D27F7E3B32@mboxes2.campus.vims.edu>
References: <CB5791F5EA3D82408B277900997482D27F7E3B32@mboxes2.campus.vims.edu>
Message-ID: <215490342.2271590.1480237797450.JavaMail.open-xchange@oxbe8.tb.ukmail.iss.as9143.net>

I am not aware that R really does symbolic manipulation in the way which you
want, but I would have thought that this process would make a very pretty little
problem to solve for you yourself, in that you could write a programme to
analyse the original expression as a string of characters, whose particular
nature means that you have to make certain choices.  Start with a simple
example, say (x^2 + x +1)/(x+1) (i haven't actually worked this out) and
gradually work up

Nick

> 
>     On 26 November 2016 at 23:18 Tom Mosca <tom at vims.edu> wrote:
> 
> 
>     Hello Folks,
> 
>     As a neophyte R user I frequently have questions, and I?m sorry to bother
> experienced users with what may appear to be trivial questions to which I
> should be able to find answers without help. I?ve searched everywhere I know
> to look, and can?t find any reference to this question. Perhaps I just haven't
> guessed a correct keyword for my searches. I apologize in advance.
> 
>     Given a rational expression P/Q with P and Q being polynomials that are
> prime in relation to each other, and with Q factored, is there an R function
> or package that will return the partial fraction decomposition of P/Q?
> 
>     For example:
>     Given (3x^3+x^2-8x+6)/(x^2)(x-1)^2
>     Return 4/x + 6/x^2 ? 1/(x-1) + 2/(x-1)^2
> 
>     Thank you, Tom
> 
>     [[alternative HTML version deleted]]
> 
>     ______________________________________________
>     R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
>     and provide commented, minimal, self-contained, reproducible code.
>
	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Sun Nov 27 10:35:41 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sun, 27 Nov 2016 22:35:41 +1300
Subject: [R] Partial Fraction Decomposition
In-Reply-To: <215490342.2271590.1480237797450.JavaMail.open-xchange@oxbe8.tb.ukmail.iss.as9143.net>
References: <CB5791F5EA3D82408B277900997482D27F7E3B32@mboxes2.campus.vims.edu>
	<215490342.2271590.1480237797450.JavaMail.open-xchange@oxbe8.tb.ukmail.iss.as9143.net>
Message-ID: <0e59ba93-d7e2-1008-4f57-c2e539d6324c@auckland.ac.nz>

On 27/11/16 22:09, WRAY NICHOLAS wrote:
> I am not aware that R really does symbolic manipulation in the way which you
> want, but I would have thought that this process would make a very pretty little
> problem to solve for you yourself, in that you could write a programme to
> analyse the original expression as a string of characters, whose particular
> nature means that you have to make certain choices.  Start with a simple
> example, say (x^2 + x +1)/(x+1) (i haven't actually worked this out) and
> gradually work up

A quick Google search reveals Ryacas and rSymPy as (at least) two 
packages that do symbolic manipulations.  I have never used either, and 
don't know if they do the partial fraction calculations that the OP 
wanted.  But they'd be worth checking out.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From unwin at math.uni-augsburg.de  Sun Nov 27 13:19:47 2016
From: unwin at math.uni-augsburg.de (Antony Unwin)
Date: Sun, 27 Nov 2016 13:19:47 +0100
Subject: [R] R Course in Dublin (January 30th-February 1st,
 2017) Intoductory -> Modern
Message-ID: <BC8CF0C7-8964-4DD7-98AF-F5545A26B8BF@math.uni-augsburg.de>

An R course from introductory to modern will be given by

Louis Aslett (Oxford University, author of the packages PhaseType and ReliabilityTheory)
and
Antony Unwin (author of the book ?Graphical Data Analysis with R? CRC Press 2015  http://www.gradaanwr.net <http://www.gradaanwr.net/>).

The course will be held in Dublin from January30th to February 1st, 2017.

Details at  

http://insightsc.ie/training/r-statistical-software/ <http://insightsc.ie/training/r-statistical-software/>


Antony Unwin
Insight Statistical Consulting, Dublin, Ireland
University of Augsburg, Germany
	[[alternative HTML version deleted]]


From ticor.vaakav at gmail.com  Sun Nov 27 16:04:15 2016
From: ticor.vaakav at gmail.com (TicoR)
Date: Sun, 27 Nov 2016 20:34:15 +0530
Subject: [R] Fitting data using Generalized Pareto Distribution
Message-ID: <CAPZYZO4Zxeob9HA0TigwAY1fsv2WVWuZv-NkqSeFdG_+MvVMkA@mail.gmail.com>

I am trying to fit some data using Generalized Pareto Distribution in R
using extRemes package(https://cran.r-project.org/web/packages/extRemes) I
am able to get the parameters for the distribution. How would I get the
simulated values for the model using the parameters?

	[[alternative HTML version deleted]]


From chrishold at psyctc.org  Sun Nov 27 16:12:27 2016
From: chrishold at psyctc.org (Chris Evans)
Date: Sun, 27 Nov 2016 15:12:27 +0000 (GMT)
Subject: [R] In praise of "options(warnPartialMatchDollar = TRUE)"
Message-ID: <1994152409.14154685.1480259547678.JavaMail.zimbra@psyctc.org>

I am just posting this to the list because someone else may one day waste an hour or so because s/he has unknowingly hit a partial match failure using "$". It's my folly that I did but I am surprised that options(warnPartialMatchDollar = TRUE) isn't the default setting. 

Here's a bit of reproducible code that shows the challenge. 

#rm(list=ls()) ### BEWARE: me making sure environment was clean 
set.seed(12345) # get fully reproducible example 
nRows <- 100 
Sample <- sample(0:1,nRows,replace=TRUE) 
data2 <- data.frame(cbind(1:nRows,Sample)) # create data frame 
table(data2$Samp) # call which silently achieves partial match 
data2$innoccuousname <- factor(data2$Samp,labels=c("Non-clinical","Clinical"),levels=0:1) 
str(data2$Samp) # all fine, no apparent destruction of the non-existent vector data2$Samp 
data2$SampFac <- factor(data2$Samp,labels=c("Non-clinical","Clinical"),levels=0:1) 
str(data2$Samp) # returns NULL because there is no longer a single partial match to "Samp" but no warning! 
str(data2$Sample) # but of course, data2$Sample is still there 

Because I had used "data2$Samp" all the way through a large file of R (markup) code and hadn't noticed that the variable names in the SPSS file I was reading in had changed from "Samp" to "Sample" I appeared to be destroying data2$Samp. 

I have now set options(warnPartialMatchDollar = TRUE) in my Rprofile.site file and am just posting this here in case it helps someone some day. 

Very best all, 


Chris 


	[[alternative HTML version deleted]]


From btupper at bigelow.org  Sun Nov 27 16:23:16 2016
From: btupper at bigelow.org (Ben Tupper)
Date: Sun, 27 Nov 2016 10:23:16 -0500
Subject: [R] Merra2 files
In-Reply-To: <CACGkHRO_qYDNKSL-u0fEC+Wv0Th8rb_n6Bm7FbWzzLsCRtc7MA@mail.gmail.com>
References: <CACGkHRO_qYDNKSL-u0fEC+Wv0Th8rb_n6Bm7FbWzzLsCRtc7MA@mail.gmail.com>
Message-ID: <ED4D1FA4-29C4-45AC-8A9F-0930B219FD4D@bigelow.org>

Hi,

One of the best places to start a search for information is RSeek.org

http://rseek.org/?q=Merra

Ben

> On Nov 26, 2016, at 8:04 PM, Alemu Tadesse <alemu.tadesse at gmail.com> wrote:
> 
> Dear R users,
> 
> I am wondering if someone has a script to download and read Merra 2 files.
> 
> I really appreciate your help.
> 
> Best,
> 
> Alemu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org


From ruipbarradas at sapo.pt  Sun Nov 27 16:38:02 2016
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Sun, 27 Nov 2016 15:38:02 +0000
Subject: [R] Fitting data using Generalized Pareto Distribution
In-Reply-To: <CAPZYZO4Zxeob9HA0TigwAY1fsv2WVWuZv-NkqSeFdG_+MvVMkA@mail.gmail.com>
References: <CAPZYZO4Zxeob9HA0TigwAY1fsv2WVWuZv-NkqSeFdG_+MvVMkA@mail.gmail.com>
Message-ID: <583AFDDA.7040008@sapo.pt>

Hello,

Please provide us with a reproducible example. A data exampla would be 
nice and some working code, the code you are using to fit the data.

Rui Barradas

Em 27-11-2016 15:04, TicoR escreveu:
> I am trying to fit some data using Generalized Pareto Distribution in R
> using extRemes package(https://cran.r-project.org/web/packages/extRemes) I
> am able to get the parameters for the distribution. How would I get the
> simulated values for the model using the parameters?
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ticor.vaakav at gmail.com  Sun Nov 27 17:01:11 2016
From: ticor.vaakav at gmail.com (TicoR)
Date: Sun, 27 Nov 2016 21:31:11 +0530
Subject: [R] Fitting data using Generalized Pareto Distribution
In-Reply-To: <583AFDDA.7040008@sapo.pt>
References: <CAPZYZO4Zxeob9HA0TigwAY1fsv2WVWuZv-NkqSeFdG_+MvVMkA@mail.gmail.com>
	<583AFDDA.7040008@sapo.pt>
Message-ID: <CAPZYZO4JPpwY2d-PrFtWnQESYUVSvh_6rKLGecOV6H1fKw9E3A@mail.gmail.com>

Let the train be the data set consisting of numbers that I need to fit.
Code is as follows:
library(extRemes)
thresh90 <- quantile(train, 0.90)
model<-fevd(train,threshold =thresh90,type="GP")

Model returns the following :
Negative Log-Likelihood Value:  317.7561


 Estimated parameters:
     scale      shape
 0.8858284 -0.3596079

 Standard Error Estimates:
     scale      shape
0.04377744 0.03194536

 Estimated parameter covariance matrix.
             scale        shape
scale  0.001916464 -0.001236952
shape -0.001236952  0.001020506



On Sun, Nov 27, 2016 at 9:08 PM, Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> Please provide us with a reproducible example. A data exampla would be
> nice and some working code, the code you are using to fit the data.
>
> Rui Barradas
>
>
> Em 27-11-2016 15:04, TicoR escreveu:
>
>> I am trying to fit some data using Generalized Pareto Distribution in R
>> using extRemes package(https://cran.r-project.org/web/packages/extRemes)
>> I
>> am able to get the parameters for the distribution. How would I get the
>> simulated values for the model using the parameters?
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posti
>> ng-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sun Nov 27 19:25:18 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 27 Nov 2016 10:25:18 -0800
Subject: [R] In praise of "options(warnPartialMatchDollar = TRUE)"
In-Reply-To: <1994152409.14154685.1480259547678.JavaMail.zimbra@psyctc.org>
References: <1994152409.14154685.1480259547678.JavaMail.zimbra@psyctc.org>
Message-ID: <FD12E8CE-75FC-4E9B-9F09-2B8DCEE9C164@comcast.net>


> On Nov 27, 2016, at 7:12 AM, Chris Evans <chrishold at psyctc.org> wrote:
> 
> I am just posting this to the list because someone else may one day waste an hour or so because s/he has unknowingly hit a partial match failure using "$". It's my folly that I did but I am surprised that options(warnPartialMatchDollar = TRUE) isn't the default setting. 
> 
> Here's a bit of reproducible code that shows the challenge. 
> 
> #rm(list=ls()) ### BEWARE: me making sure environment was clean 
> set.seed(12345) # get fully reproducible example 
> nRows <- 100 
> Sample <- sample(0:1,nRows,replace=TRUE) 
> data2 <- data.frame(cbind(1:nRows,Sample)) # create data frame 

Using dataframe( cbind( ...) ) is a predictable method for creating later headaches. and there is no options-warning available. cbind coerces an argument list of vectors to matrix class, thus dropping all attributes (dates, times and factors are all destroyed.)

> table(data2$Samp) # call which silently achieves partial match 
> data2$innoccuousname <- factor(data2$Samp,labels=c("Non-clinical","Clinical"),levels=0:1) 
> str(data2$Samp) # all fine, no apparent destruction of the non-existent vector data2$Samp 
> data2$SampFac <- factor(data2$Samp,labels=c("Non-clinical","Clinical"),levels=0:1) 
> str(data2$Samp) # returns NULL because there is no longer a single partial match to "Samp" but no warning! 
> str(data2$Sample) # but of course, data2$Sample is still there 
> 
> Because I had used "data2$Samp" all the way through a large file of R (markup) code and hadn't noticed that the variable names in the SPSS file I was reading in had changed from "Samp" to "Sample" I appeared to be destroying data2$Samp. 
> 
> I have now set options(warnPartialMatchDollar = TRUE) in my Rprofile.site file and am just posting this here in case it helps someone some day. 

This is one of the reasons many experienced R programmers eschew the use of the "$" function in programming.

The preferred use would be :

data2[['Samp']]

(No partial match.)

> 
> 	[[alternative HTML version deleted]]

Plain text is generally preferred on Rhelp but there does not appear to have been a problem in this posting instance.

> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From chrishold at psyctc.org  Sun Nov 27 20:21:17 2016
From: chrishold at psyctc.org (Chris Evans)
Date: Sun, 27 Nov 2016 19:21:17 +0000 (GMT)
Subject: [R] In praise of "options(warnPartialMatchDollar = TRUE)"
In-Reply-To: <FD12E8CE-75FC-4E9B-9F09-2B8DCEE9C164@comcast.net>
References: <1994152409.14154685.1480259547678.JavaMail.zimbra@psyctc.org>
	<FD12E8CE-75FC-4E9B-9F09-2B8DCEE9C164@comcast.net>
Message-ID: <516616191.14169478.1480274477700.JavaMail.zimbra@psyctc.org>

Was about to reply just to sender but thought there were good byproducts of this so, to all ...

Many thanks Dr. Winsemius, and apologies for the HTML: tired sloppiness.  My bad.

Aha. For the first time I can really see a logic for dataFrame[['variable']] -- thanks for that.  I will have to break myself of the "$" habit.  Pity as it's a lot more keystrokes!  W

I understand about the nasty asset stripped side effects of cbind but thought in this situation it would cause no problems. Is the preferred route to create turn a first vector to data frame and then add the others using dataframe[['newVariable']] <- nextVector ?

Very best wishes and thanks again: this is an amazing list,

Chris


----- Original Message -----
> From: "David Winsemius" <dwinsemius at comcast.net>
> To: "Chris Evans" <chrishold at psyctc.org>
> Cc: r-help at r-project.org
> Sent: Sunday, 27 November, 2016 18:25:18
> Subject: Re: [R] In praise of "options(warnPartialMatchDollar = TRUE)"

>> On Nov 27, 2016, at 7:12 AM, Chris Evans <chrishold at psyctc.org> wrote:
>> 
>> I am just posting this to the list because someone else may one day waste an
>> hour or so because s/he has unknowingly hit a partial match failure using "$".
>> It's my folly that I did but I am surprised that options(warnPartialMatchDollar
>> = TRUE) isn't the default setting.
>> 
>> Here's a bit of reproducible code that shows the challenge.
>> 
>> #rm(list=ls()) ### BEWARE: me making sure environment was clean
>> set.seed(12345) # get fully reproducible example
>> nRows <- 100
>> Sample <- sample(0:1,nRows,replace=TRUE)
>> data2 <- data.frame(cbind(1:nRows,Sample)) # create data frame
> 
> Using dataframe( cbind( ...) ) is a predictable method for creating later
> headaches. and there is no options-warning available. cbind coerces an argument
> list of vectors to matrix class, thus dropping all attributes (dates, times and
> factors are all destroyed.)
> 
>> table(data2$Samp) # call which silently achieves partial match
>> data2$innoccuousname <-
>> factor(data2$Samp,labels=c("Non-clinical","Clinical"),levels=0:1)
>> str(data2$Samp) # all fine, no apparent destruction of the non-existent vector
>> data2$Samp
>> data2$SampFac <-
>> factor(data2$Samp,labels=c("Non-clinical","Clinical"),levels=0:1)
>> str(data2$Samp) # returns NULL because there is no longer a single partial match
>> to "Samp" but no warning!
>> str(data2$Sample) # but of course, data2$Sample is still there
>> 
>> Because I had used "data2$Samp" all the way through a large file of R (markup)
>> code and hadn't noticed that the variable names in the SPSS file I was reading
>> in had changed from "Samp" to "Sample" I appeared to be destroying data2$Samp.
>> 
>> I have now set options(warnPartialMatchDollar = TRUE) in my Rprofile.site file
>> and am just posting this here in case it helps someone some day.
> 
> This is one of the reasons many experienced R programmers eschew the use of the
> "$" function in programming.
> 
> The preferred use would be :
> 
> data2[['Samp']]
> 
> (No partial match.)
> 
>> 
>> 	[[alternative HTML version deleted]]
> 
> Plain text is generally preferred on Rhelp but there does not appear to have
> been a problem in this posting instance.
> 
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA


From dwinsemius at comcast.net  Sun Nov 27 21:12:44 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 27 Nov 2016 12:12:44 -0800
Subject: [R] In praise of "options(warnPartialMatchDollar = TRUE)"
In-Reply-To: <516616191.14169478.1480274477700.JavaMail.zimbra@psyctc.org>
References: <1994152409.14154685.1480259547678.JavaMail.zimbra@psyctc.org>
	<FD12E8CE-75FC-4E9B-9F09-2B8DCEE9C164@comcast.net>
	<516616191.14169478.1480274477700.JavaMail.zimbra@psyctc.org>
Message-ID: <6A98215D-5A4B-4C1E-ABC8-0E050C2396FD@comcast.net>


> On Nov 27, 2016, at 11:21 AM, Chris Evans <chrishold at psyctc.org> wrote:
> 
> Was about to reply just to sender but thought there were good byproducts of this so, to all ...
> 
> Many thanks Dr. Winsemius, and apologies for the HTML: tired sloppiness.  My bad.
> 
> Aha. For the first time I can really see a logic for dataFrame[['variable']] -- thanks for that.  I will have to break myself of the "$" habit.  Pity as it's a lot more keystrokes!  W
> 
> I understand about the nasty asset stripped side effects of cbind but thought in this situation it would cause no problems. Is the preferred route to create turn a first vector to data frame and then add the others using dataframe[['newVariable']] <- nextVector ?

With an existing dataframe the use of cbind is safe, because the cbind data.frame function will not coerce arguments to matrix class. It is the use of cbind with vectors that is the source of danger.

This would be the preferred method of constructing a data.frame from objects with attributes:

dt <- as.Date( 1:10, origin="1970-01-01")
fac <- factor(letters[1:10])
nums <-10:1

dfrm <- data.frame( dt, fac, nums)

#OR skip the preliminary vector creation and use a named argument list:

dfrm <- data.frame( dt=as.Date( 1:10, origin="1970-01-01"),
                    fac=factor(letters[1:10]),
                    nums=10:1)

The second method lets you avoid leaving loose vectors that might later get used inappropriately if you happened to later write a function with a parameter or object that matched a named vector in the global environment. 


After either method, you can `cbind` to that dfrm object to your heart's content, because the cbind.data.frame method is dispatched.

-- 
David.

> 
> Very best wishes and thanks again: this is an amazing list,
> 
> Chris
> 
> 
> ----- Original Message -----
>> From: "David Winsemius" <dwinsemius at comcast.net>
>> To: "Chris Evans" <chrishold at psyctc.org>
>> Cc: r-help at r-project.org
>> Sent: Sunday, 27 November, 2016 18:25:18
>> Subject: Re: [R] In praise of "options(warnPartialMatchDollar = TRUE)"
> 
>>> On Nov 27, 2016, at 7:12 AM, Chris Evans <chrishold at psyctc.org> wrote:
>>> 
>>> I am just posting this to the list because someone else may one day waste an
>>> hour or so because s/he has unknowingly hit a partial match failure using "$".
>>> It's my folly that I did but I am surprised that options(warnPartialMatchDollar
>>> = TRUE) isn't the default setting.
>>> 
>>> Here's a bit of reproducible code that shows the challenge.
>>> 
>>> #rm(list=ls()) ### BEWARE: me making sure environment was clean
>>> set.seed(12345) # get fully reproducible example
>>> nRows <- 100
>>> Sample <- sample(0:1,nRows,replace=TRUE)
>>> data2 <- data.frame(cbind(1:nRows,Sample)) # create data frame
>> 
>> Using dataframe( cbind( ...) ) is a predictable method for creating later
>> headaches. and there is no options-warning available. cbind coerces an argument
>> list of vectors to matrix class, thus dropping all attributes (dates, times and
>> factors are all destroyed.)
>> 
>>> table(data2$Samp) # call which silently achieves partial match
>>> data2$innoccuousname <-
>>> factor(data2$Samp,labels=c("Non-clinical","Clinical"),levels=0:1)
>>> str(data2$Samp) # all fine, no apparent destruction of the non-existent vector
>>> data2$Samp
>>> data2$SampFac <-
>>> factor(data2$Samp,labels=c("Non-clinical","Clinical"),levels=0:1)
>>> str(data2$Samp) # returns NULL because there is no longer a single partial match
>>> to "Samp" but no warning!
>>> str(data2$Sample) # but of course, data2$Sample is still there
>>> 
>>> Because I had used "data2$Samp" all the way through a large file of R (markup)
>>> code and hadn't noticed that the variable names in the SPSS file I was reading
>>> in had changed from "Samp" to "Sample" I appeared to be destroying data2$Samp.
>>> 
>>> I have now set options(warnPartialMatchDollar = TRUE) in my Rprofile.site file
>>> and am just posting this here in case it helps someone some day.
>> 
>> This is one of the reasons many experienced R programmers eschew the use of the
>> "$" function in programming.
>> 
>> The preferred use would be :
>> 
>> data2[['Samp']]
>> 
>> (No partial match.)
>> 
>>> 
>>> 	[[alternative HTML version deleted]]
>> 
>> Plain text is generally preferred on Rhelp but there does not appear to have
>> been a problem in this posting instance.
>> 
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> David Winsemius
>> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From ruipbarradas at sapo.pt  Sun Nov 27 22:26:07 2016
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Sun, 27 Nov 2016 21:26:07 +0000
Subject: [R] Fitting data using Generalized Pareto Distribution
In-Reply-To: <CAPZYZO4JPpwY2d-PrFtWnQESYUVSvh_6rKLGecOV6H1fKw9E3A@mail.gmail.com>
References: <CAPZYZO4Zxeob9HA0TigwAY1fsv2WVWuZv-NkqSeFdG_+MvVMkA@mail.gmail.com>
	<583AFDDA.7040008@sapo.pt>
	<CAPZYZO4JPpwY2d-PrFtWnQESYUVSvh_6rKLGecOV6H1fKw9E3A@mail.gmail.com>
Message-ID: <583B4F6F.1000501@sapo.pt>

Hello,

Why quantile(train, 0.9) ?  If you use quantile(train) it seems to fit 
the data much better. You haven't posted a data example so I've made up one.


library(eva)  # needed for rgpd()
library(extRemes)

set.seed(1)
train <- rgpd(1e3, scale = 0.9, shape = -0.4)
thresh90 <- quantile(train)
model <- fevd(train,threshold =thresh90,type="GP")
model

x <- seq(0, 3, length.out = 1e3)

hist(train, prob = TRUE)
lines(x, dgpd(x, scale = 0.7595442, shape = -0.3707967), col = "red")


Hope this helps,

Rui Barradas

Em 27-11-2016 16:01, TicoR escreveu:
> Let the train be the data set consisting of numbers that I need to fit.
> Code is as follows:
> library(extRemes)
> thresh90 <- quantile(train, 0.90)
> model<-fevd(train,threshold =thresh90,type="GP")
>
> Model returns the following :
> Negative Log-Likelihood Value:  317.7561
>
>
>   Estimated parameters:
>       scale      shape
>   0.8858284 -0.3596079
>
>   Standard Error Estimates:
>       scale      shape
> 0.04377744 0.03194536
>
>   Estimated parameter covariance matrix.
>               scale        shape
> scale  0.001916464 -0.001236952
> shape -0.001236952  0.001020506
>
>
>
> On Sun, Nov 27, 2016 at 9:08 PM, Rui Barradas <ruipbarradas at sapo.pt
> <mailto:ruipbarradas at sapo.pt>> wrote:
>
>     Hello,
>
>     Please provide us with a reproducible example. A data exampla would
>     be nice and some working code, the code you are using to fit the data.
>
>     Rui Barradas
>
>
>     Em 27-11-2016 15:04, TicoR escreveu:
>
>         I am trying to fit some data using Generalized Pareto
>         Distribution in R
>         using extRemes
>         package(https://cran.r-project.org/web/packages/extRemes
>         <https://cran.r-project.org/web/packages/extRemes>) I
>         am able to get the parameters for the distribution. How would I
>         get the
>         simulated values for the model using the parameters?
>
>                  [[alternative HTML version deleted]]
>
>         ______________________________________________
>         R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>         -- To UNSUBSCRIBE and more, see
>         https://stat.ethz.ch/mailman/listinfo/r-help
>         <https://stat.ethz.ch/mailman/listinfo/r-help>
>         PLEASE do read the posting guide
>         http://www.R-project.org/posting-guide.html
>         <http://www.R-project.org/posting-guide.html>
>         and provide commented, minimal, self-contained, reproducible code.
>
>


From bgunter.4567 at gmail.com  Sun Nov 27 23:03:48 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sun, 27 Nov 2016 14:03:48 -0800
Subject: [R] In praise of "options(warnPartialMatchDollar = TRUE)"
In-Reply-To: <6A98215D-5A4B-4C1E-ABC8-0E050C2396FD@comcast.net>
References: <1994152409.14154685.1480259547678.JavaMail.zimbra@psyctc.org>
	<FD12E8CE-75FC-4E9B-9F09-2B8DCEE9C164@comcast.net>
	<516616191.14169478.1480274477700.JavaMail.zimbra@psyctc.org>
	<6A98215D-5A4B-4C1E-ABC8-0E050C2396FD@comcast.net>
Message-ID: <CAGxFJbSdv37b4niOi6t_ZQiFxGzk3u=TMVrURy0SDjrjTHVtXA@mail.gmail.com>

...
>
> After either method, you can `cbind` to that dfrm object to your heart's content, because the cbind.data.frame method is dispatched.
>
> --
> David.
>

... But of course, this is unnecessary anyway, as:

"The cbind data frame method is just a wrapper for data.frame(...,
check.names = FALSE). This means that it will split matrix columns in
data frame arguments, and convert character columns to factors unless
stringsAsFactors = FALSE is specified."


Cheers,
Bert


From dwinsemius at comcast.net  Sun Nov 27 23:15:47 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 27 Nov 2016 14:15:47 -0800
Subject: [R] In praise of "options(warnPartialMatchDollar = TRUE)"
In-Reply-To: <CAGxFJbSdv37b4niOi6t_ZQiFxGzk3u=TMVrURy0SDjrjTHVtXA@mail.gmail.com>
References: <1994152409.14154685.1480259547678.JavaMail.zimbra@psyctc.org>
	<FD12E8CE-75FC-4E9B-9F09-2B8DCEE9C164@comcast.net>
	<516616191.14169478.1480274477700.JavaMail.zimbra@psyctc.org>
	<6A98215D-5A4B-4C1E-ABC8-0E050C2396FD@comcast.net>
	<CAGxFJbSdv37b4niOi6t_ZQiFxGzk3u=TMVrURy0SDjrjTHVtXA@mail.gmail.com>
Message-ID: <27B940A0-7A27-43C9-AACB-49563193105D@comcast.net>


> On Nov 27, 2016, at 2:03 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> ...
>> 
>> After either method, you can `cbind` to that dfrm object to your heart's content, because the cbind.data.frame method is dispatched.
>> 
>> --
>> David.
>> 
> 
> ... But of course, this is unnecessary anyway, as:
> 
> "The cbind data frame method is just a wrapper for data.frame(...,
> check.names = FALSE). This means that it will split matrix columns in
> data frame arguments, and convert character columns to factors unless
> stringsAsFactors = FALSE is specified."

Good point especially the reminder about the stringsAsFactors pitfall. The factor call to create my example was superfluous, but if I had included a character column the print method for dataframes would h=not have warned me anyway. Neither does the print method for data.tables. Only the print method for the dplyr-tibble objects gives a clear indication of factor-classed columns.

--
Best;

David Winsemius
Alameda, CA, USA


From yzhlinscau at 163.com  Mon Nov 28 07:03:33 2016
From: yzhlinscau at 163.com (yz)
Date: Mon, 28 Nov 2016 14:03:33 +0800 (CST)
Subject: [R] About MC simulation of AR1 model in R
Message-ID: <615400.7ff2.158a9880402.Coremail.yzhlinscau@163.com>

I want to run MC simulation of AR1(auto-regression) matrix in R, which would be as residuals in linear mixed model. 

AR1 matrix with the following character: 

??r,??c is the auto-correlation parameter in the row and column direction. 

And I wrote one function in R ( under following). But I run the function, it seems only work  for the first Row auto-corr. When setting different a set of Row auto-corr values, the simulated dataset would change with the same value. But it did not work for the second column auto-corr parameter, even if setting different col atuo-corr, the simulated dataset seemd no changed in col auto-corr value that nearly is zero all the time. Would someone please help me  to find the questions that the R function codes somewhere got wrong? Thanks a lots. 

####### simulation codes for AR1 model 
multi_norm <- function(data_num,Pr,Pc) { 
  require(MASS) 
 # data_num for row/col number; Pr for row auto-corr; Pc for colum auto-corr. 

  V <- matrix(data=NA, nrow=data_num, ncol=data_num) 
  R.mat=diag(data_num) 
  C.mat=diag(data_num) 
  
  set.seed(2016) 
  means <- runif(1, min=0, max=1) 
  means1=rep(means,data_num*data_num) 

  # variance 
  set.seed(2016) 
  var <- runif(1, min=0, max=1) 
  
  for (i in 1:data_num) { 
    # a two-level nested loop to generate AR matrix 
    for (j in 1:data_num) { 
      if (i == j) { 
        # covariances on the diagonal 
        V[i,j] <- 1 #varsmodule[i] 
      } else if(i<j){ 
        # covariances 
        R.mat[i,j]<-  V[i,i]*(Pr^(j-i)) 
        C.mat[i,j]<-  V[i,i]*(Pc^(j-i)) 
      }else {R.mat[i,j]=R.mat[j,i];C.mat[i,j]=C.mat[j,i]} 
    } 
  } 
  
  V=var*kronecker(C.mat,R.mat) 
  
  # simulate multivariate normal distribution 
  # given means and covariance matrix 
  X <- t(mvrnorm(n = data_num, means1, V))   
  aam=X[1:data_num,] 

  aad=data.frame()   
  for(i in 1:data_num){ 
    for(j in 1:data_num){ 
      aad[j+data_num*(i-1),1]=i 
      aad[j+data_num*(i-1),2]=j 
      aad[j+data_num*(i-1),3]=aam[i,j] 
    } 
  } 
  names(aad)=c('Row','Col','y') 
  for(i in 1:2) aad[,i]=factor(aad[,i]) 
  
  return(aad) 
} 

The simulation results as following: 

> aam=multi_norm(30,0.6,0.01) 
> mm2=asreml(y~1,rcov=~ar1(Row):ar1(Col),data=aam,trace=F,maxit=30) 
> summary(mm2)$varcomp 
                gamma  component  std.error    z.ratio    constraint 
R!variance 1.00000000 0.16267855 0.01038210 15.6691353      Positive 
R!Row.cor  0.55811722 0.55811722 0.02734902 20.4072085 Unconstrained 
R!Col.cor  0.01735573 0.01735573 0.03368048  0.5153055 Unconstrained 
> aam=multi_norm(30,0.6,0.3) 
> mm2=asreml(y~1,rcov=~ar1(Row):ar1(Col),data=aam,trace=F,maxit=30) 
> summary(mm2)$varcomp 
                 gamma   component  std.error   z.ratio    constraint 
R!variance  1.00000000  0.17491494 0.01199393 14.583624      Positive 
R!Row.cor   0.62097328  0.62097328 0.02534858 24.497358 Unconstrained 
R!Col.cor  -0.03744104 -0.03744104 0.03380648 -1.107511 Unconstrained 
> aam=multi_norm(30,0.6,0.6) 
> mm2=asreml(y~1,rcov=~ar1(Row):ar1(Col),data=aam,trace=F,maxit=30) 
> summary(mm2)$varcomp 
                 gamma   component  std.error    z.ratio    constraint 
R!variance 1.000000000 0.180804271 0.01227539 14.7289994      Positive 
R!Row.cor  0.581797580 0.581797580 0.02861663 20.3307541 Unconstrained 
R!Col.cor  0.007598536 0.007598536 0.03448510  0.2203426 Unconstrained 

> aam=multi_norm(30,0.3,0.6) 
> mm2=asreml(y~1,rcov=~ar1(Row):ar1(Col),data=aam,trace=F,maxit=30) 
> summary(mm2)$varcomp 
                  gamma    component   std.error    z.ratio    constraint 
R!variance  1.000000000  0.177888691 0.008979462 19.8106171      Positive 
R!Row.cor   0.269572147  0.269572147 0.031823892  8.4707474 Unconstrained 
R!Col.cor  -0.004159379 -0.004159379 0.035830577 -0.1160846 Unconstrained 
> aam=multi_norm(30,0.9,0.6) 
> mm2=asreml(y~1,rcov=~ar1(Row):ar1(Col),data=aam,trace=F,maxit=30) 
> summary(mm2)$varcomp 
                gamma  component  std.error    z.ratio    constraint 
R!variance 1.00000000 0.19194479 0.02674158  7.1777654      Positive 
R!Row.cor  0.91213667 0.91213667 0.01247011 73.1458677 Unconstrained 
R!Col.cor  0.01203907 0.01203907 0.03474589  0.3464891 Unconstrained 
	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Sat Nov 26 17:18:21 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 26 Nov 2016 08:18:21 -0800
Subject: [R] Unable to Install POT Package for R 3.1.0
In-Reply-To: <CAHVFrXHs46YyT6YNB=bxVa61LwOt3s1NufpWrYxjog_zy1pUFg@mail.gmail.com>
References: <CAHVFrXHs46YyT6YNB=bxVa61LwOt3s1NufpWrYxjog_zy1pUFg@mail.gmail.com>
Message-ID: <85EFD917-E594-4147-833F-F1EA64814BAD@dcn.davis.ca.us>

You probably need to upgrade your R software to the current version. Many CRAN mirrors don't keep binary package repositories for old versions of R online, and the Posting Guide warns that old versions of R are effectively off-topic on the mailing lists.

You could also try downloading the zip file and installing it, but you are on your own then since there are many possible hard-to-diagnose problems with doing that. 
-- 
Sent from my phone. Please excuse my brevity.

On November 23, 2016 4:09:48 AM PST, Preetam Pal <lordpreetam at gmail.com> wrote:
>Hi, I am trying to install the package POT for R* version 3.1.0*
>(spring
>dance), using:
>
>*install.packages("POT", repos="http://R-Forge.R-project.org
><http://R-Forge.R-project.org>")*
>*( link <https://r-forge.r-project.org/R/?group_id=76> )*
>
>*But I am getting the following error:*
>
>
>*package ?POT? is available as a source package but not as a
>binaryWarning
>in install.packages :  package ?POT? is not available (for R version
>3.1.0)*
>
>
>Can anyone suggest how I can get it working please?
>I need it for Peaks-Over-Threshold analysis under extreme value theory.
>I
>am trying to make use of functions mentioned in this link ( link2
><http://benz.nchu.edu.tw/~finmyc/POT_guide.pdf> ).Thanks.
>
>Regards,
>Preetam
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From emorway at usgs.gov  Mon Nov 28 18:38:02 2016
From: emorway at usgs.gov (Morway, Eric)
Date: Mon, 28 Nov 2016 09:38:02 -0800
Subject: [R] Manipulating groups of boolean data subject to group size and
 distance from other groups
Message-ID: <CAPoqHzq5gLpwDAs6Vb8v8KTdPKz-5ZJcqZChnywqKgA+iU-AhQ@mail.gmail.com>

The example below is a pared-down version of a much larger dataset.  My
goal is to use the binary data contained in DF$col2 to guide manipulation
of the binary data itself, subject to the following:

   - Groups of '1' that are separated from other, larger groups of "1's" in
   'col2' by 2 or more years should be converted to "0"
   - Groups of '1' need to be at least 2 consecutive years to be preserved

So in the example provided below, DF$col2 would be manipulated such that
its values are overrided to:

c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1)

That is, the first group of 1's in positions 2 through 6 are separated from
other groups of 1's by 2 (or more) years, and the second group of 1's
(positions 11 & 12) span only a single year and do not meet the criteria of
being at least 2 years long.

The example R script below shows a small example I'm working with, called
"DF".  The code that comes after the first line is my attempt to go through
some R-gymnastics to append a column to DF called "isl2" that reflects the
number of consecutive years in the 0/1 groups, where the +/- sign acts as
(or denotes) the original binary condition: 0 = negative, 1 = positive.
However, I'm stuck with how to proceed further.  Could someone please help
me come up with script that modifies DF$col2 shown below to be like that
shown above?

DF <- data.frame(col1=rep(1991:2004,
each=2),col2=c(0,0,1,1,1,1,0,0,0,0,1,1,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1))

DF$inc <- c(0, abs(diff(DF$col2)))
DF$cum <- cumsum(DF$inc)

ex1 <- aggregate(col1 ~ cum, data=DF, function(x) length(unique(x)))
names(ex1) <- c('cum','isl')

tmp1a <- merge(DF, ex1, by="cum", all.x=TRUE)
tmp1a$isl2 <- (-1*tmp1a$col2) * tmp1a$isl
tmp1a$isl2[tmp1a$isl2==0] <- tmp1a$isl[tmp1a$isl2==0]

DF$grpng <- tmp1a$isl2

At this point I was thinking I could use DF$grpng to sweep through col2 and
make adjustments, but I didn't know how to proceed.

For debugging purposes, a slightly different example would go from:

DF <- data.frame(col1=rep(1991:2004, each=2),col2=c(1,1,1,1,
1,1,0,0,0,0,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1))

to 'col2' looking like:

c(0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1)

That is, even though the first group of 1's is greater than two consecutive
years, it is separated from a larger group of 1's by 2 (or more years).

	[[alternative HTML version deleted]]


From jeanphilippe.fontaine at gssi.infn.it  Mon Nov 28 10:06:56 2016
From: jeanphilippe.fontaine at gssi.infn.it (jean-philippe)
Date: Mon, 28 Nov 2016 10:06:56 +0100
Subject: [R] reshaping a large dataframe in R
Message-ID: <583BF3B0.9080403@gssi.infn.it>

dear all,

I have a dataframe of 500 rows and 4004 columns that I would like to 
reshape to a dataframe of 500500 rows and 4 columns. That is from this 
dataframe:

V1 V2 V3 V4 ... V4001 V4002 V4003 V4004

1 2 3 4 ... 4001 4002 4003 4004

1 2 3 4 ... 4001 4002 4003 4004

1 2 3 4 ... 4001 4002 4003 4004

... ... ... ... ... ... ... ... ... ... ... ... ...

1 2 3 4 ... 4001 4002 4003 4004

I would like :


V1 V2 V3 V4

1 2 3 4

1 2 3 4

1 2 3 4

1 2 3 4

... ... ... ... ... ... ... ... ...

4001 4002 4003 4004

4001 4002 4003 4004

4001 4002 4003 4004

... ... ... ... ...

4001 4002 4003 4004

I tried already to use y=matrix(as.matrix(dataGaus[[1]]),500500,4) 
(where dataGaus is my dataframe) but it doesn't give the expected 
result. I tried also to use reshape but I can't manage to use it to 
reproduce the result (and I have been through lot of posts on 
StackOverflow and on the net). In python, we can do this with a simple 
command numpy.array(dataGaus[[1]]).reshape(-1,4). For some reasons, I am 
doing my analysis in R, and I would like to know if there is a function 
which does the same thing as the reshape(-1,4) of numpy in Python?

Thanks in advance, best


Jean-Philippe

-- 
Jean-Philippe Fontaine
PhD Student in Astroparticle Physics,
Gran Sasso Science Institute (GSSI),
Viale Francesco Crispi 7,
67100 L'Aquila, Italy
Mobile: +393487128593, +33615653774


From dcarlson at tamu.edu  Mon Nov 28 20:27:17 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Mon, 28 Nov 2016 19:27:17 +0000
Subject: [R] reshaping a large dataframe in R
In-Reply-To: <583BF3B0.9080403@gssi.infn.it>
References: <583BF3B0.9080403@gssi.infn.it>
Message-ID: <058240d5c7ab4fe185d37fa99a5a459c@exch-2p-mbx-w2.ads.tamu.edu>

There may be a simpler way of getting there, but this works:

> rows <- 500
> cols <- 4004
> dat <- as.data.frame(t(replicate(rows, 1:cols)))
> dat[c(1:3, 500), c(1:4, 4001:4004)]
    V1 V2 V3 V4 V4001 V4002 V4003 V4004
1    1  2  3  4  4001  4002  4003  4004
2    1  2  3  4  4001  4002  4003  4004
3    1  2  3  4  4001  4002  4003  4004
500  1  2  3  4  4001  4002  4003  4004
> dat2 <- array(as.matrix(dat), dim=c(rows, 4, cols/4))
> dat3 <- as.data.frame(matrix(aperm(dat2, c(1, 3, 2)), rows*cols/4, 4))
> head(dat3)
  V1 V2 V3 V4
1  1  2  3  4
2  1  2  3  4
3  1  2  3  4
4  1  2  3  4
5  1  2  3  4
6  1  2  3  4
> tail(dat3)
         V1   V2   V3   V4
500495 4001 4002 4003 4004
500496 4001 4002 4003 4004
500497 4001 4002 4003 4004
500498 4001 4002 4003 4004
500499 4001 4002 4003 4004
500500 4001 4002 4003 4004

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of jean-philippe
Sent: Monday, November 28, 2016 3:07 AM
To: r-help at r-project.org
Subject: [R] reshaping a large dataframe in R

dear all,

I have a dataframe of 500 rows and 4004 columns that I would like to 
reshape to a dataframe of 500500 rows and 4 columns. That is from this 
dataframe:

V1 V2 V3 V4 ... V4001 V4002 V4003 V4004

1 2 3 4 ... 4001 4002 4003 4004

1 2 3 4 ... 4001 4002 4003 4004

1 2 3 4 ... 4001 4002 4003 4004

... ... ... ... ... ... ... ... ... ... ... ... ...

1 2 3 4 ... 4001 4002 4003 4004

I would like :


V1 V2 V3 V4

1 2 3 4

1 2 3 4

1 2 3 4

1 2 3 4

... ... ... ... ... ... ... ... ...

4001 4002 4003 4004

4001 4002 4003 4004

4001 4002 4003 4004

... ... ... ... ...

4001 4002 4003 4004

I tried already to use y=matrix(as.matrix(dataGaus[[1]]),500500,4) 
(where dataGaus is my dataframe) but it doesn't give the expected 
result. I tried also to use reshape but I can't manage to use it to 
reproduce the result (and I have been through lot of posts on 
StackOverflow and on the net). In python, we can do this with a simple 
command numpy.array(dataGaus[[1]]).reshape(-1,4). For some reasons, I am 
doing my analysis in R, and I would like to know if there is a function 
which does the same thing as the reshape(-1,4) of numpy in Python?

Thanks in advance, best


Jean-Philippe

-- 
Jean-Philippe Fontaine
PhD Student in Astroparticle Physics,
Gran Sasso Science Institute (GSSI),
Viale Francesco Crispi 7,
67100 L'Aquila, Italy
Mobile: +393487128593, +33615653774

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Mon Nov 28 21:25:14 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 28 Nov 2016 12:25:14 -0800
Subject: [R] Manipulating groups of boolean data subject to group size
	and distance from other groups
In-Reply-To: <CAPoqHzq5gLpwDAs6Vb8v8KTdPKz-5ZJcqZChnywqKgA+iU-AhQ@mail.gmail.com>
References: <CAPoqHzq5gLpwDAs6Vb8v8KTdPKz-5ZJcqZChnywqKgA+iU-AhQ@mail.gmail.com>
Message-ID: <6FA2D04C-E0F5-45CF-B059-8E3E89757FB4@comcast.net>


> On Nov 28, 2016, at 9:38 AM, Morway, Eric <emorway at usgs.gov> wrote:
> 
> The example below is a pared-down version of a much larger dataset.  My
> goal is to use the binary data contained in DF$col2 to guide manipulation
> of the binary data itself, subject to the following:
> 
>   - Groups of '1' that are separated from other, larger groups of "1's" in
>   'col2' by 2 or more years should be converted to "0"
>   - Groups of '1' need to be at least 2 consecutive years to be preserved
> 
> So in the example provided below, DF$col2 would be manipulated such that
> its values are overrided to:
> 
> c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1)
> 
> That is, the first group of 1's in positions 2 through 6 are separated from
> other groups of 1's by 2 (or more) years, and the second group of 1's
> (positions 11 & 12) span only a single year and do not meet the criteria of
> being at least 2 years long.
> 
> The example R script below shows a small example I'm working with, called
> "DF".  The code that comes after the first line is my attempt to go through
> some R-gymnastics to append a column to DF called "isl2" that reflects the
> number of consecutive years in the 0/1 groups, where the +/- sign acts as
> (or denotes) the original binary condition: 0 = negative, 1 = positive.
> However, I'm stuck with how to proceed further.  Could someone please help
> me come up with script that modifies DF$col2 shown below to be like that
> shown above?
> 
> DF <- data.frame(col1=rep(1991:2004,
> each=2),col2=c(0,0,1,1,1,1,0,0,0,0,1,1,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1))

It's not clear from you verbal description why the first group pf 1's with length 4 is discarded while the second group of ones also of length 4 is preserved. There's ambiguity in the rules about "how large" a run must be in order to be "safe" from removal.

In any case the answer will almost surely involve the use of the rle function which if you have not encountered it should be your next visit to the help pages.

-- 
David,
> 
> DF$inc <- c(0, abs(diff(DF$col2)))
> DF$cum <- cumsum(DF$inc)
> 
> ex1 <- aggregate(col1 ~ cum, data=DF, function(x) length(unique(x)))
> names(ex1) <- c('cum','isl')
> 
> tmp1a <- merge(DF, ex1, by="cum", all.x=TRUE)
> tmp1a$isl2 <- (-1*tmp1a$col2) * tmp1a$isl
> tmp1a$isl2[tmp1a$isl2==0] <- tmp1a$isl[tmp1a$isl2==0]
> 
> DF$grpng <- tmp1a$isl2
> 
> At this point I was thinking I could use DF$grpng to sweep through col2 and
> make adjustments, but I didn't know how to proceed.
> 
> For debugging purposes, a slightly different example would go from:
> 
> DF <- data.frame(col1=rep(1991:2004, each=2),col2=c(1,1,1,1,
> 1,1,0,0,0,0,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1))
> 
> to 'col2' looking like:
> 
> c(0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1)
> 
> That is, even though the first group of 1's is greater than two consecutive
> years, it is separated from a larger group of 1's by 2 (or more years).


> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From djnordlund at gmail.com  Mon Nov 28 22:58:28 2016
From: djnordlund at gmail.com (Daniel Nordlund)
Date: Mon, 28 Nov 2016 13:58:28 -0800
Subject: [R] reshaping a large dataframe in R
In-Reply-To: <583BF3B0.9080403@gssi.infn.it>
References: <583BF3B0.9080403@gssi.infn.it>
Message-ID: <73738946-b7a5-89b8-3683-dc6a6b4e34c2@gmail.com>

On 11/28/2016 1:06 AM, jean-philippe wrote:
> dear all,
>
> I have a dataframe of 500 rows and 4004 columns that I would like to
> reshape to a dataframe of 500500 rows and 4 columns. That is from this
> dataframe:
>
> V1 V2 V3 V4 ... V4001 V4002 V4003 V4004
>
> 1 2 3 4 ... 4001 4002 4003 4004
>
> 1 2 3 4 ... 4001 4002 4003 4004
>
> 1 2 3 4 ... 4001 4002 4003 4004
>
> ... ... ... ... ... ... ... ... ... ... ... ... ...
>
> 1 2 3 4 ... 4001 4002 4003 4004
>
> I would like :
>
>
> V1 V2 V3 V4
>
> 1 2 3 4
>
> 1 2 3 4
>
> 1 2 3 4
>
> 1 2 3 4
>
> ... ... ... ... ... ... ... ... ...
>
> 4001 4002 4003 4004
>
> 4001 4002 4003 4004
>
> 4001 4002 4003 4004
>
> ... ... ... ... ...
>
> 4001 4002 4003 4004
>
> I tried already to use y=matrix(as.matrix(dataGaus[[1]]),500500,4)
> (where dataGaus is my dataframe) but it doesn't give the expected
> result. I tried also to use reshape but I can't manage to use it to
> reproduce the result (and I have been through lot of posts on
> StackOverflow and on the net). In python, we can do this with a simple
> command numpy.array(dataGaus[[1]]).reshape(-1,4). For some reasons, I am
> doing my analysis in R, and I would like to know if there is a function
> which does the same thing as the reshape(-1,4) of numpy in Python?
>
> Thanks in advance, best
>
>
> Jean-Philippe
>

I don't know about efficiency, but it looks like you could do something 
like this:

y <- t(matrix(t(dataGaus),4))


Maybe someone will come along with something better,

Dan

-- 
Daniel Nordlund
Port Townsend, WA  USA


From emorway at usgs.gov  Mon Nov 28 23:20:49 2016
From: emorway at usgs.gov (Morway, Eric)
Date: Mon, 28 Nov 2016 14:20:49 -0800
Subject: [R] Manipulating groups of boolean data subject to group size
 and distance from other groups
In-Reply-To: <6FA2D04C-E0F5-45CF-B059-8E3E89757FB4@comcast.net>
References: <CAPoqHzq5gLpwDAs6Vb8v8KTdPKz-5ZJcqZChnywqKgA+iU-AhQ@mail.gmail.com>
	<6FA2D04C-E0F5-45CF-B059-8E3E89757FB4@comcast.net>
Message-ID: <CAPoqHzqPH4iV-XjH4=vPyG_kEwg+3aNZeqJu0cgpDJJAOMpyTQ@mail.gmail.com>

To help with the clarification, I renamed 'col1' to 'year' and 'col2' to
'origDat'.  With that said...

The reason the second 'block' of 1's (four consecutive 1's appearing in
DF$origDat[11:14]) is preserved is because they are only separated by a
total of 1 year (1998 in DF$year) from a larger group of consecutive 1's
(years 1999 through 2002).  Because the first block of 1's are separated
from from any other block of ones by at least 2 years, which I have deemed
to be too large of a gap in data (0's are a surrogate for missing data),
the 1's appearing in DF$year[3:6] should be reset to 0.

I modified the script based on David's suggestion of rle (I was previously
unaware of it) to that shown below, and it works for all three example DF's
provided at the top of the script. That is, after running the script with
any of the first 3 DF's provided, the data in DF$finalDat (as compared to
DF$origDat) is reflective of what I'm after.

HOWEVER, the use of nested while loops and if statements strikes me as
antithetical to elegant R scripting.  Second, my script, as currently
constituted, has a significant bug in that the rules I've set forth are not
completely satisfied.  If DF4 is used (uncomment the line: "DF <- DF4") the
blocks of 1's at the beginning and end of DF$origDat are preserved, whereas
the middle (and largest continuous) block of 1's appearing in the middle of
DF$origDat are reset to 0.  Thus, I think I'm in need of a more elegant way
of pursuing this problem...should anyone be so inclined to offer of
additional thoughts.

The (semi-) working script using rle is:

DF <- data.frame(year=rep(1991:2004, each=2),

 origDat=c(0,0,1,1,1,1,0,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1))

#DF <- data.frame(year=rep(1991:2004, each=2),
#
origDat=c(1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1))

#DF <- data.frame(year=rep(1991:2004, each=2),
#
origDat=c(1,1,1,1,1,1,0,0,0,0,1,1,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1))

# An example that doesn't work
DF4 <- data.frame(year=rep(1991:2004, each=2),

 origDat=c(1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1))
#DF <- DF4

DF$inc <- c(1, abs(diff(DF$origDat)))
DF$cumsum <- cumsum(DF$inc)

ex1 <- aggregate(year ~ cumsum, data=DF, function(x) length(unique(x)))
names(ex1) <- c('cumsum','isl')

tmp1a <- merge(DF, ex1, by="cumsum", all.x=TRUE)
tmp1a$isl2 <- (-1*tmp1a$origDat) * tmp1a$isl
tmp1a$isl2[tmp1a$isl2==0] <- tmp1a$isl[tmp1a$isl2==0]
tmp1a$isl2 <- -1 * tmp1a$isl2

DF$grpng <- tmp1a$isl2

runlen <- data.frame(cumsum = seq(1:length(rle(DF$grpng)$lengths)),
                     len = rle(DF$grpng)$lengths,
                     val = rle(DF$grpng)$values)

i <- 1
while(i <= nrow(runlen)){
  if(runlen[i,'val'] >= 2){  # As long as a '-2' or smaller doesn't follow,
                            # then the current group of data is NOT
                            # too 'distant' from other data and should be
                            # preserved.  Otherwise, the current grp of
                            # 1's should be reset to 0
    j <- i + 1
    while(j <= nrow(runlen)){
      if(runlen[j,'val'] <= -2){
        # If code enters here, then swich the sign of 'val' to
        # effectively inactivate this block of 1's
        runlen[i,'val'] <- -1 * runlen[i,'val']
      }
      #print(paste0("j: ",as.character(j)))
      j <- j + 1
    }
  } else if (runlen[i,'val'] > 0 & runlen[i,'val'] < 2){
    # If the script enters here, then the current group of data
    # doesn't meet the minimum continuous length requirement of
    # 2 or more years (in this example a check of >0 & <2 seems
    # silly, but in the real-world dataset 2 will be replaced with
    # a much larger example.
    runlen[i,'val'] <- -1 * runlen[i,'val']
  }
  #print(paste0("i: ",as.character(i)))
  i <- i + 1
}

runlen$finalDat <- ifelse(runlen$val < 0, 0, 1)
DF <- merge(DF, runlen, by = 'cumsum', all.x = TRUE)
DF

	[[alternative HTML version deleted]]


From sewashm at gmail.com  Mon Nov 28 23:23:23 2016
From: sewashm at gmail.com (Ashta)
Date: Mon, 28 Nov 2016 16:23:23 -0600
Subject: [R] read
Message-ID: <CADDFq31_sF2KOPcUGfmH3tccHWYx9xXB7jfr4SQeFnc1iz6yCg@mail.gmail.com>

Hi all,

I have a script that  reads a file (dat.csv)  from several folders.
However, in some folders the file name is (dat) with out csv  and in
other folders it is dat.csv.  The format of data is the same(only the
file name differs  with and without "csv".

Is it possible to read these files  depending on their name in one?
like read.csv("dat.csv"). How can I read both type of file names?

Thank you in advance


From chris.barker at barkerstats.com  Tue Nov 29 00:36:46 2016
From: chris.barker at barkerstats.com (Chris)
Date: Mon, 28 Nov 2016 23:36:46 +0000 (UTC)
Subject: [R] errors when installing packages
References: <364767725.1896328.1480376206377.ref@mail.yahoo.com>
Message-ID: <364767725.1896328.1480376206377@mail.yahoo.com>

I'm using R 3.3.1?
when installing/ updating a library module, for example "Hmisc" I get an error message about "unable to move..."

cutting/pasting
survival? successfully unpacked and MD5 sums checkedWarning: unable to move temporary installation ?C:\Users\Chris\Documents\R\win-library\3.3\file4681d2a5a2a\survival? to ?C:\Users\Chris\Documents\R\win-library\3.3\survival?

?Chris Barker, Ph.D.
Adjunct Associate Professor of Biostatistics - UIC-SPH
and

skype: barkerstats


	[[alternative HTML version deleted]]


From john.archie.mckown at gmail.com  Tue Nov 29 02:09:33 2016
From: john.archie.mckown at gmail.com (John McKown)
Date: Mon, 28 Nov 2016 19:09:33 -0600
Subject: [R] read
In-Reply-To: <CADDFq31_sF2KOPcUGfmH3tccHWYx9xXB7jfr4SQeFnc1iz6yCg@mail.gmail.com>
References: <CADDFq31_sF2KOPcUGfmH3tccHWYx9xXB7jfr4SQeFnc1iz6yCg@mail.gmail.com>
Message-ID: <CAAJSdjhtQ2-n8Mx7Y9TyT0r+9QxtA-chw5vr0EFHQQR7Abs0Cw@mail.gmail.com>

On Mon, Nov 28, 2016 at 4:23 PM, Ashta <sewashm at gmail.com> wrote:

> Hi all,
>
> I have a script that  reads a file (dat.csv)  from several folders.
> However, in some folders the file name is (dat) with out csv  and in
> other folders it is dat.csv.  The format of data is the same(only the
> file name differs  with and without "csv".
>
> Is it possible to read these files  depending on their name in one?
> like read.csv("dat.csv"). How can I read both type of file names?
>
> Thank you in advance
>
>
?I'd do something like this:

> files=c('dat.csv','dat')
> file2read=files[file.exists(files)][1]
> file2read
[1] "dat.csv"

You put the possible file names into the variable in the order of
preference. E.g. I prefer "dat.csv" over "dat" if by chance both exist.

> files=c('not.csv','not')
> file2read=files[file.exists(files)][1]
> file2read
[1] NA

?The above shows the result should none of the files exist. So if
"file2read" has an NA, then you go on to the next directory.?


-- 
Heisenberg may have been here.

Unicode: http://xkcd.com/1726/

Maranatha! <><
John McKown?

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Tue Nov 29 02:20:11 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 28 Nov 2016 17:20:11 -0800
Subject: [R] read
In-Reply-To: <CADDFq31_sF2KOPcUGfmH3tccHWYx9xXB7jfr4SQeFnc1iz6yCg@mail.gmail.com>
References: <CADDFq31_sF2KOPcUGfmH3tccHWYx9xXB7jfr4SQeFnc1iz6yCg@mail.gmail.com>
Message-ID: <79249AB2-FB58-42B9-935D-1DBECB231246@dcn.davis.ca.us>

No, and yes, depending what you mean. 

No, because you have to supply the file name to open it... you cannot directly use wildcards to open files.

Yes,  because the list.files function can be used to match all file names fitting a regex pattern, and you can use those filenames to open the files.

E.g.

fns  <- list.files( pattern="dat(\\.[^.]+)$" )
dtaL <- lapply( fns, function(fn){ read.csv( fn, stringsAsFactors=FALSE ) } )

If you only expect one file to be in any given directory, you can skip the lapply and just read the file, or you can extract the data frame from the list using dtaL[[ 1 ]].

?list.files
?regex for help on patterns
-- 
Sent from my phone. Please excuse my brevity.

On November 28, 2016 2:23:23 PM PST, Ashta <sewashm at gmail.com> wrote:
>Hi all,
>
>I have a script that  reads a file (dat.csv)  from several folders.
>However, in some folders the file name is (dat) with out csv  and in
>other folders it is dat.csv.  The format of data is the same(only the
>file name differs  with and without "csv".
>
>Is it possible to read these files  depending on their name in one?
>like read.csv("dat.csv"). How can I read both type of file names?
>
>Thank you in advance
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From valkremk at gmail.com  Tue Nov 29 02:56:04 2016
From: valkremk at gmail.com (Val)
Date: Mon, 28 Nov 2016 19:56:04 -0600
Subject: [R] read
In-Reply-To: <79249AB2-FB58-42B9-935D-1DBECB231246@dcn.davis.ca.us>
References: <CADDFq31_sF2KOPcUGfmH3tccHWYx9xXB7jfr4SQeFnc1iz6yCg@mail.gmail.com>
	<79249AB2-FB58-42B9-935D-1DBECB231246@dcn.davis.ca.us>
Message-ID: <CAJOiR6bCzqhCyf_OB26ZiaLqF5SzD1=y+DrHL5kDz5M9ECOBzw@mail.gmail.com>

Hi Jeff  and John,

Thank you for your response.
In each folder, I am expecting a single file name (either dat or
dat.csv).v  so will this work?


Is the following correct?
fns  <- list.files(mydir)
    if (is.element(pattern="dat(\\.[^.]+)$",fns ))

Thank you again.

On Mon, Nov 28, 2016 at 7:20 PM, Jeff Newmiller
<jdnewmil at dcn.davis.ca.us> wrote:
> No, and yes, depending what you mean.
>
> No, because you have to supply the file name to open it... you cannot directly use wildcards to open files.
>
> Yes,  because the list.files function can be used to match all file names fitting a regex pattern, and you can use those filenames to open the files.
>
> E.g.
>
> fns  <- list.files( pattern="dat(\\.[^.]+)$" )
> dtaL <- lapply( fns, function(fn){ read.csv( fn, stringsAsFactors=FALSE ) } )
>
> If you only expect one file to be in any given directory, you can skip the lapply and just read the file, or you can extract the data frame from the list using dtaL[[ 1 ]].
>
> ?list.files
> ?regex for help on patterns
> --
> Sent from my phone. Please excuse my brevity.
>
> On November 28, 2016 2:23:23 PM PST, Ashta <sewashm at gmail.com> wrote:
>>Hi all,
>>
>>I have a script that  reads a file (dat.csv)  from several folders.
>>However, in some folders the file name is (dat) with out csv  and in
>>other folders it is dat.csv.  The format of data is the same(only the
>>file name differs  with and without "csv".
>>
>>Is it possible to read these files  depending on their name in one?
>>like read.csv("dat.csv"). How can I read both type of file names?
>>
>>Thank you in advance
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Tue Nov 29 03:29:44 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 28 Nov 2016 18:29:44 -0800
Subject: [R] read
In-Reply-To: <CAJOiR6bCzqhCyf_OB26ZiaLqF5SzD1=y+DrHL5kDz5M9ECOBzw@mail.gmail.com>
References: <CADDFq31_sF2KOPcUGfmH3tccHWYx9xXB7jfr4SQeFnc1iz6yCg@mail.gmail.com>
	<79249AB2-FB58-42B9-935D-1DBECB231246@dcn.davis.ca.us>
	<CAJOiR6bCzqhCyf_OB26ZiaLqF5SzD1=y+DrHL5kDz5M9ECOBzw@mail.gmail.com>
Message-ID: <C9991CE8-B21A-4117-9E26-F3D32ABAE5B4@dcn.davis.ca.us>

Doesn't look right to me... you are likely to need to change something to handle the multiple directories thing somehow, but I don't know why you made the changes you did make to my suggestion.
-- 
Sent from my phone. Please excuse my brevity.

On November 28, 2016 5:56:04 PM PST, Val <valkremk at gmail.com> wrote:
>Hi Jeff  and John,
>
>Thank you for your response.
>In each folder, I am expecting a single file name (either dat or
>dat.csv).v  so will this work?
>
>
>Is the following correct?
>fns  <- list.files(mydir)
>    if (is.element(pattern="dat(\\.[^.]+)$",fns ))
>
>Thank you again.
>
>On Mon, Nov 28, 2016 at 7:20 PM, Jeff Newmiller
><jdnewmil at dcn.davis.ca.us> wrote:
>> No, and yes, depending what you mean.
>>
>> No, because you have to supply the file name to open it... you cannot
>directly use wildcards to open files.
>>
>> Yes,  because the list.files function can be used to match all file
>names fitting a regex pattern, and you can use those filenames to open
>the files.
>>
>> E.g.
>>
>> fns  <- list.files( pattern="dat(\\.[^.]+)$" )
>> dtaL <- lapply( fns, function(fn){ read.csv( fn,
>stringsAsFactors=FALSE ) } )
>>
>> If you only expect one file to be in any given directory, you can
>skip the lapply and just read the file, or you can extract the data
>frame from the list using dtaL[[ 1 ]].
>>
>> ?list.files
>> ?regex for help on patterns
>> --
>> Sent from my phone. Please excuse my brevity.
>>
>> On November 28, 2016 2:23:23 PM PST, Ashta <sewashm at gmail.com> wrote:
>>>Hi all,
>>>
>>>I have a script that  reads a file (dat.csv)  from several folders.
>>>However, in some folders the file name is (dat) with out csv  and in
>>>other folders it is dat.csv.  The format of data is the same(only the
>>>file name differs  with and without "csv".
>>>
>>>Is it possible to read these files  depending on their name in one?
>>>like read.csv("dat.csv"). How can I read both type of file names?
>>>
>>>Thank you in advance
>>>
>>>______________________________________________
>>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide
>>>http://www.R-project.org/posting-guide.html
>>>and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From dkrstajic at hotmail.com  Tue Nov 29 08:43:05 2016
From: dkrstajic at hotmail.com (Damjan Krstajic)
Date: Tue, 29 Nov 2016 07:43:05 +0000
Subject: [R] independent censoring
Message-ID: <VI1PR04MB10857BB0AD4CC09A00FFFD5BB48D0@VI1PR04MB1085.eurprd04.prod.outlook.com>

Dear All,


Independent censoring is one of the fundamental assumptions in the survival analysis. However, I cannot find any test for it or any paper which discusses how real that assumption is.


I would be grateful if anybody could point me to some useful references. I have found the following paper as an interesting reference but it is not freely available.


Leung, Kwan-Moon, Robert M. Elashoff, and Abdelmonem A. Afifi. "Censoring issues in survival analysis." Annual review of public health 18.1 (1997): 83-104.


Any feedback would be much appreciated.


Kind regards

DK


	[[alternative HTML version deleted]]


From tring at gvdnet.dk  Tue Nov 29 09:30:07 2016
From: tring at gvdnet.dk (Troels Ring)
Date: Tue, 29 Nov 2016 09:30:07 +0100
Subject: [R] nls for R version 3.3.2
Message-ID: <5c247a04-4a0d-9104-18ff-4cf8c5d1bf6b@gvdnet.dk>

Dear friends - updated to R 3.3.2 - tried to install nls - got this sad 
response

package ?nls? is not available (as a binary package for R version 3.3.2)

I'm on windows 7

Did I do something wrong? Will a binary appear eventually? Would I have 
to make it myself?

Best wishes

Troels Ring

Aalborg, Denmark


From bhh at xs4all.nl  Tue Nov 29 09:51:57 2016
From: bhh at xs4all.nl (Berend Hasselman)
Date: Tue, 29 Nov 2016 09:51:57 +0100
Subject: [R] nls for R version 3.3.2
In-Reply-To: <5c247a04-4a0d-9104-18ff-4cf8c5d1bf6b@gvdnet.dk>
References: <5c247a04-4a0d-9104-18ff-4cf8c5d1bf6b@gvdnet.dk>
Message-ID: <2A917BD1-8007-4B16-8A3D-4A48823862DE@xs4all.nl>


> On 29 Nov 2016, at 09:30, Troels Ring <tring at gvdnet.dk> wrote:
> 
> Dear friends - updated to R 3.3.2 - tried to install nls - got this sad response
> 
> package ?nls? is not available (as a binary package for R version 3.3.2)
> 
> I'm on windows 7
> 
> Did I do something wrong? Will a binary appear eventually? Would I have to make it myself?
> 

Indeed you did something wrong.
nls is part of the stats package in R.
Just do ?nls to see the help.

Berend Hasselman

> Best wishes
> 
> Troels Ring
> 
> Aalborg, Denmark
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Tue Nov 29 09:53:46 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 29 Nov 2016 08:53:46 +0000
Subject: [R] reshaping a large dataframe in R
In-Reply-To: <058240d5c7ab4fe185d37fa99a5a459c@exch-2p-mbx-w2.ads.tamu.edu>
References: <583BF3B0.9080403@gssi.infn.it>
	<058240d5c7ab4fe185d37fa99a5a459c@exch-2p-mbx-w2.ads.tamu.edu>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5047CBE@SRVEXCHMBX.precheza.cz>

Hi

probably not at all simpler

> dat2.p<-split(t(dat), rep(1:(ncol(dat)/4), each=4))
> dat3.p<-as.data.frame(do.call(rbind, lapply(dat2.p, function(x) t(matrix(x, 4,nrow(dat))))))
>
> all.equal(dat3.p, dat3)
[1] TRUE

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of David L
> Carlson
> Sent: Monday, November 28, 2016 8:27 PM
> To: jean-philippe <jeanphilippe.fontaine at gssi.infn.it>; r-help at r-project.org
> Subject: Re: [R] reshaping a large dataframe in R
>
> There may be a simpler way of getting there, but this works:
>
> > rows <- 500
> > cols <- 4004
> > dat <- as.data.frame(t(replicate(rows, 1:cols))) dat[c(1:3, 500),
> > c(1:4, 4001:4004)]
>     V1 V2 V3 V4 V4001 V4002 V4003 V4004
> 1    1  2  3  4  4001  4002  4003  4004
> 2    1  2  3  4  4001  4002  4003  4004
> 3    1  2  3  4  4001  4002  4003  4004
> 500  1  2  3  4  4001  4002  4003  4004
> > dat2 <- array(as.matrix(dat), dim=c(rows, 4, cols/4))
> > dat3 <- as.data.frame(matrix(aperm(dat2, c(1, 3, 2)), rows*cols/4, 4))
> > head(dat3)
>   V1 V2 V3 V4
> 1  1  2  3  4
> 2  1  2  3  4
> 3  1  2  3  4
> 4  1  2  3  4
> 5  1  2  3  4
> 6  1  2  3  4
> > tail(dat3)
>          V1   V2   V3   V4
> 500495 4001 4002 4003 4004
> 500496 4001 4002 4003 4004
> 500497 4001 4002 4003 4004
> 500498 4001 4002 4003 4004
> 500499 4001 4002 4003 4004
> 500500 4001 4002 4003 4004
>
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of jean-
> philippe
> Sent: Monday, November 28, 2016 3:07 AM
> To: r-help at r-project.org
> Subject: [R] reshaping a large dataframe in R
>
> dear all,
>
> I have a dataframe of 500 rows and 4004 columns that I would like to reshape
> to a dataframe of 500500 rows and 4 columns. That is from this
> dataframe:
>
> V1 V2 V3 V4 ... V4001 V4002 V4003 V4004
>
> 1 2 3 4 ... 4001 4002 4003 4004
>
> 1 2 3 4 ... 4001 4002 4003 4004
>
> 1 2 3 4 ... 4001 4002 4003 4004
>
> ... ... ... ... ... ... ... ... ... ... ... ... ...
>
> 1 2 3 4 ... 4001 4002 4003 4004
>
> I would like :
>
>
> V1 V2 V3 V4
>
> 1 2 3 4
>
> 1 2 3 4
>
> 1 2 3 4
>
> 1 2 3 4
>
> ... ... ... ... ... ... ... ... ...
>
> 4001 4002 4003 4004
>
> 4001 4002 4003 4004
>
> 4001 4002 4003 4004
>
> ... ... ... ... ...
>
> 4001 4002 4003 4004
>
> I tried already to use y=matrix(as.matrix(dataGaus[[1]]),500500,4)
> (where dataGaus is my dataframe) but it doesn't give the expected result. I
> tried also to use reshape but I can't manage to use it to reproduce the result
> (and I have been through lot of posts on StackOverflow and on the net). In
> python, we can do this with a simple command
> numpy.array(dataGaus[[1]]).reshape(-1,4). For some reasons, I am doing my
> analysis in R, and I would like to know if there is a function which does the
> same thing as the reshape(-1,4) of numpy in Python?
>
> Thanks in advance, best
>
>
> Jean-Philippe
>
> --
> Jean-Philippe Fontaine
> PhD Student in Astroparticle Physics,
> Gran Sasso Science Institute (GSSI),
> Viale Francesco Crispi 7,
> 67100 L'Aquila, Italy
> Mobile: +393487128593, +33615653774
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From dwinsemius at comcast.net  Tue Nov 29 09:54:32 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 29 Nov 2016 00:54:32 -0800
Subject: [R] nls for R version 3.3.2
In-Reply-To: <5c247a04-4a0d-9104-18ff-4cf8c5d1bf6b@gvdnet.dk>
References: <5c247a04-4a0d-9104-18ff-4cf8c5d1bf6b@gvdnet.dk>
Message-ID: <AE7D9E00-B39B-456A-9E2B-870D754E12DB@comcast.net>


> On Nov 29, 2016, at 12:30 AM, Troels Ring <tring at gvdnet.dk> wrote:
> 
> Dear friends - updated to R 3.3.2 - tried to install nls - got this sad response
> 
> package ?nls? is not available (as a binary package for R version 3.3.2)
> 
> I'm on windows 7

I don't see an `nls` package on CRAN. Perhaps it has been replaced by nls2. On the other hand, you may be looking for the functions `nls` in the stats package. (Hard to tell from the available information.)

packageDescription('nls2')

Package: nls2
Version: 0.2
Date: 2013-03-07
Title: Non-linear regression with brute force
Author: G. Grothendieck
Maintainer: G. Grothendieck <ggrothendieck at gmail.com>
Description: Adds brute force and multiple starting values to nls.
Depends: proto
Suggests: nlstools
License: GPL-2
BugReports: http://groups.google.com/group/sqldf
URL: http://nls2.googlecode.com


> 
> Did I do something wrong? Will a binary appear eventually? Would I have to make it myself?
> 
> Best wishes
> 
> Troels Ring
> 
> Aalborg, Denmark
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From pdalgd at gmail.com  Tue Nov 29 10:53:20 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 29 Nov 2016 10:53:20 +0100
Subject: [R] errors when installing packages
In-Reply-To: <364767725.1896328.1480376206377@mail.yahoo.com>
References: <364767725.1896328.1480376206377.ref@mail.yahoo.com>
	<364767725.1896328.1480376206377@mail.yahoo.com>
Message-ID: <90F593E1-44D9-4B07-9278-99305F2F570C@gmail.com>

Presumably a problem at your end. Suspicion points to permission settings on the target directory, or virus checkers "helpfully" moving recently created files to a safe place for scrutiny.

(Do you _really_ get errors for "survival" when installing "Hmisc"? It could happen via a dependency, I suppose. If it is just the survival folder that has incorrect owner/permission, you could try removing the folder and reinstalling survival.) 

-pd

On 29 Nov 2016, at 00:36 , Chris <chris.barker at barkerstats.com> wrote:

> I'm using R 3.3.1 
> when installing/ updating a library module, for example "Hmisc" I get an error message about "unable to move..."
> 
> cutting/pasting
> survival? successfully unpacked and MD5 sums checkedWarning: unable to move temporary installation ?C:\Users\Chris\Documents\R\win-library\3.3\file4681d2a5a2a\survival? to ?C:\Users\Chris\Documents\R\win-library\3.3\survival?
> 
>  Chris Barker, Ph.D.
> Adjunct Associate Professor of Biostatistics - UIC-SPH
> and
> 
> skype: barkerstats
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From therneau at mayo.edu  Tue Nov 29 14:20:38 2016
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Tue, 29 Nov 2016 07:20:38 -0600
Subject: [R] independent censoring
In-Reply-To: <mailman.7.1480417202.1400.r-help@r-project.org>
References: <mailman.7.1480417202.1400.r-help@r-project.org>
Message-ID: <021cdb$4vk4jg@ironport10.mayo.edu>



On 11/29/2016 05:00 AM, r-help-request at r-project.org wrote:
> Independent censoring is one of the fundamental assumptions in the survival analysis. However, I cannot find any test for it or any paper which discusses how real that assumption is.
>
>
> I would be grateful if anybody could point me to some useful references. I have found the following paper as an interesting reference but it is not freely available.
>
>
> Leung, Kwan-Moon, Robert M. Elashoff, and Abdelmonem A. Afifi. "Censoring issues in survival analysis." Annual review of public health 18.1 (1997): 83-104.
>
>

This is because there is no test for independent censoring.  Say I am following a cohort 
of older gentlemen (65 years old) who were diagnoses with condition "x", and after 8 years 
there a a dozen who no longer answer my letters.  Why not?
  a. Perhaps because they are in a nursing home, with dementia.
  b. Perhaps because they have moved to another city to be interact and be near to 
grandchildren.

In case a, those lost to follow-up are much sicker than the average subject, and in case b 
they are most likely the most healthy and active of the group.   In a) the KM will 
over-estimate survival and in b it will underestimate.

The main point is that there is absolutely no way to know, other than actually tracking 
the subjects down.  Any study which has a substantial fraction with incomplete follow-up 
is making a guess.  The more accurate phrase would be "a blind hope for independent 
censoring" than "assume".   There are cases where simple reasoning or experience tells me 
that this hope is futile, but mostly we just hope.  The alternative is proactive 
follow-up, i.e., devote enough staff and resources to actively contact all of the study 
subjects on a regular schedule.   Even then you will lose a few.  (In one study several 
years ago, long term follow-up of cancer, there was a new Mrs Smith who refused to 
acknowledge the existence of the prior wife, even to forward letters.)

Terry Therneau


From profjcnash at gmail.com  Tue Nov 29 14:21:47 2016
From: profjcnash at gmail.com (J C Nash)
Date: Tue, 29 Nov 2016 08:21:47 -0500
Subject: [R] nls for R version 3.3.2
In-Reply-To: <5c247a04-4a0d-9104-18ff-4cf8c5d1bf6b@gvdnet.dk>
References: <5c247a04-4a0d-9104-18ff-4cf8c5d1bf6b@gvdnet.dk>
Message-ID: <ebfbd040-5583-aad2-cfe4-690c8d5168f7@gmail.com>

You may also want to use tools that are more robust.

Package nlmrt uses analytic Jacobian where possible and a Marquardt solver.

Package minpack.lm uses a Marquardt solver, but the forward difference derivatives of
nls() for its Jacobian.

alpha level work in https://r-forge.r-project.org/R/?group_id=395 package nlsr is aimed at cleaning up some of the work
in nlmrt and extending its capabilities.

JN


On 16-11-29 03:30 AM, Troels Ring wrote:
> Dear friends - updated to R 3.3.2 - tried to install nls - got this sad response
> 
> package ?nls? is not available (as a binary package for R version 3.3.2)
> 
> I'm on windows 7
> 
> Did I do something wrong? Will a binary appear eventually? Would I have to make it myself?
> 
> Best wishes
> 
> Troels Ring
> 
> Aalborg, Denmark
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dbastos at toledo.com  Tue Nov 29 15:06:17 2016
From: dbastos at toledo.com (Daniel Bastos)
Date: Tue, 29 Nov 2016 12:06:17 -0200
Subject: [R] on ``unfolding'' a json into data frame columns
Message-ID: <86mvgipehy.fsf@toledo.com>

Greetings!

In an SQL table, I have a column that contains a JSON.  I'd like easy
access to all (in an ideal world) of these JSON fields.  I started out
trying to get all fields from the JSON and so I wrote this function.

unfold.json <- function (df, column)
{
    library(jsonlite)
    ret <- data.frame()
    
    for (i in 1:nrow(df)) {
        js <- fromJSON(df[i, ][[column]])
        ret <- rbind(ret, cbind(df[i, ], js))
    }

    ret
}

It takes a data frame and a column-string where the JSON is to be
found.  It produces a new RET data frame with all the rows of DF but
with new columns --- extracted from every field in the JSON.

(The performance is horrible.)

fromJSON sometimes produces a list that sometimes contains a data frame.
As a result, I end up getting a RET data frame with duplicated rows.
Here's what happens.

> nrow(df)
[1] 1

> nrow(unfold.json(df, "response"))
[1] 3
Warning messages:
1: In data.frame(CreateUTC = "2016-11-29 02:00:43", Payload = list( :
  row names were found from a short variable and have been discarded
2: In data.frame(..., check.names = FALSE) :
  row names were found from a short variable and have been discarded
> 

I expected a data frame with 1 row.  The reason 3 rows is produced is
because in the JSON there's an array with 3 rows.

> fromJSON(df$response)$RawPayload
[1] 200   1 128

I have also cases where fromJSON(df$response)$Payload$Fields is a data
frame containing various rows.  So unfold.json produces a data frame
with these various rows.

So I gave up on this general approach.

(*) My humble approach

For the moment I'm not interested in RawPayload nor Payload$Fields, so I
nullified them in this new approach.  To improve performance, I guessed
perhaps merge() would help and I think it did, but this was not at all a
decision thought out.

unfold.json.fast <- function (df, column)
{
    library(jsonlite)
    ret <- data.frame()
    if (nrow(df) > 0) {
        for (i in 1:nrow(df)) {
            ls <- fromJSON(df[i, ][[column]])
            ls$RawPayload <- NULL
            ls$Payload$Fields <- NULL
            js <- data.frame(ls)
            ret <- rbind(ret, merge(df[i, ], js))
        }
    }

    ret
}

I'm looking for advice.  How would you approach this problem?  

Thank you!


From h.wickham at gmail.com  Tue Nov 29 15:26:44 2016
From: h.wickham at gmail.com (Hadley Wickham)
Date: Tue, 29 Nov 2016 08:26:44 -0600
Subject: [R] on ``unfolding'' a json into data frame columns
In-Reply-To: <86mvgipehy.fsf@toledo.com>
References: <86mvgipehy.fsf@toledo.com>
Message-ID: <CABdHhvEOMSU5yVERtyzttGmFTDwi7h60c0+dCY3jHKOkoN2xng@mail.gmail.com>

Two quick hints:

* use simplifyDataFrame = FALSE in fromJSON()

* read https://jennybc.github.io/purrr-tutorial/ls02_map-extraction-advanced.html
(and https://jennybc.github.io/purrr-tutorial/)

Hadley

On Tue, Nov 29, 2016 at 8:06 AM, Daniel Bastos <dbastos at toledo.com> wrote:
> Greetings!
>
> In an SQL table, I have a column that contains a JSON.  I'd like easy
> access to all (in an ideal world) of these JSON fields.  I started out
> trying to get all fields from the JSON and so I wrote this function.
>
> unfold.json <- function (df, column)
> {
>     library(jsonlite)
>     ret <- data.frame()
>
>     for (i in 1:nrow(df)) {
>         js <- fromJSON(df[i, ][[column]])
>         ret <- rbind(ret, cbind(df[i, ], js))
>     }
>
>     ret
> }
>
> It takes a data frame and a column-string where the JSON is to be
> found.  It produces a new RET data frame with all the rows of DF but
> with new columns --- extracted from every field in the JSON.
>
> (The performance is horrible.)
>
> fromJSON sometimes produces a list that sometimes contains a data frame.
> As a result, I end up getting a RET data frame with duplicated rows.
> Here's what happens.
>
>> nrow(df)
> [1] 1
>
>> nrow(unfold.json(df, "response"))
> [1] 3
> Warning messages:
> 1: In data.frame(CreateUTC = "2016-11-29 02:00:43", Payload = list( :
>   row names were found from a short variable and have been discarded
> 2: In data.frame(..., check.names = FALSE) :
>   row names were found from a short variable and have been discarded
>>
>
> I expected a data frame with 1 row.  The reason 3 rows is produced is
> because in the JSON there's an array with 3 rows.
>
>> fromJSON(df$response)$RawPayload
> [1] 200   1 128
>
> I have also cases where fromJSON(df$response)$Payload$Fields is a data
> frame containing various rows.  So unfold.json produces a data frame
> with these various rows.
>
> So I gave up on this general approach.
>
> (*) My humble approach
>
> For the moment I'm not interested in RawPayload nor Payload$Fields, so I
> nullified them in this new approach.  To improve performance, I guessed
> perhaps merge() would help and I think it did, but this was not at all a
> decision thought out.
>
> unfold.json.fast <- function (df, column)
> {
>     library(jsonlite)
>     ret <- data.frame()
>     if (nrow(df) > 0) {
>         for (i in 1:nrow(df)) {
>             ls <- fromJSON(df[i, ][[column]])
>             ls$RawPayload <- NULL
>             ls$Payload$Fields <- NULL
>             js <- data.frame(ls)
>             ret <- rbind(ret, merge(df[i, ], js))
>         }
>     }
>
>     ret
> }
>
> I'm looking for advice.  How would you approach this problem?
>
> Thank you!
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
http://hadley.nz


From weiserc at hhu.de  Tue Nov 29 15:36:03 2016
From: weiserc at hhu.de (Constantin Weiser)
Date: Tue, 29 Nov 2016 15:36:03 +0100
Subject: [R] Unexpected interference between dplyr and plm
Message-ID: <d5fc847a-005a-468c-4152-ae58643eb332@hhu.de>

Hello,

I'm struggling with an unexpected interference between the two packages 
dplyr and plm, or to be more concrete with the "lag(x, ...)" function of 
both packages.

If dplyr is in the namespace the plm function uses no longer the 
appropriate lag()-function which accounts for the panel structure.

The following code demonstrates the unexpected behaviour:

## starting from a new R-Session (plm and dplyr unloaded) ##

   ## generate dataset
   set.seed(4711)
   df <- data.frame(
           i = rep(1:10, each = 4),
           t = rep(1:4, times = 10),
           y = rnorm(40),
           x = rnorm(40)
   )
   ## manually generated laged variable
   df$lagx <- c(NA, df$x[-40])
   df$lagx[df$t == 1] <- NA


require(plm)
summary(plm(y~lagx, data = df, index = c("i", "t")))
summary(plm(y~lag(x, 1), data = df, index = c("i", "t")))
# > this result is expected

require(dplyr)
summary(plm(y~lagx, data = df, index = c("i", "t")))
summary(plm(y~lag(x, 1), data = df, index = c("i", "t")))
# > this result is unexpected

Is there a way to force R to use the "correct" lag-function? (or at the 
devel-level to harmonise both functions)

Thank you very much in advance for your answer

Yours
Constantin

-- 
^
|                X
|               /eiser, Dr. Constantin (weiserc at hhu.de)
|              /Chair of Statistics and Econometrics
|             / Heinrich Heine-University of D?sseldorf
| *    /\    /  Universit?tsstra?e 1, 40225 D?sseldorf, Germany
|  \  /  \  /   Oeconomicum (Building 24.31), Room 01.22
|   \/    \/    Tel: 0049 211 81-15307
+----------------------------------------------------------->


From sarah.goslee at gmail.com  Tue Nov 29 15:52:47 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 29 Nov 2016 09:52:47 -0500
Subject: [R] Unexpected interference between dplyr and plm
In-Reply-To: <d5fc847a-005a-468c-4152-ae58643eb332@hhu.de>
References: <d5fc847a-005a-468c-4152-ae58643eb332@hhu.de>
Message-ID: <CAM_vjumwQrxP8N4S-hcLBPy0N9_DPuyP-_qJh1KwFBjqyZtgqw@mail.gmail.com>

Hi,

It shouldn't be entirely unexpected: when I load dplyr, I get a series
of messages telling me that certain functions are masked.


The following object is masked from ?package:plm?:

    between

The following objects are masked from ?package:stats?:

    filter, lag

The following objects are masked from ?package:base?:

    intersect, setdiff, setequal, union


You can see the search path that R uses when looking for a function or
other object here:

In your example, it should look like this:

> search()
 [1] ".GlobalEnv"        "package:dplyr"     "package:plm"
"package:Formula"
 [5] "package:stats"     "package:graphics"  "package:grDevices"
"package:utils"
 [9] "package:datasets"  "package:vimcom"    "package:setwidth"
"package:colorout"
[13] "package:methods"   "Autoloads"         "package:base"


So R is searching the local environment, then dplyr, and then farther
down the list, stats, which is where the lag function comes from (see
above warning).

Once you know where the desired function comes from you can specify
its namespace:


summary(plm(y~lagx, data = df, index = c("i", "t")))
summary(plm(y~stats::lag(x, 1), data = df, index = c("i", "t")))

If you weren't paying attention to the warning messages at package
load, you can also use the getAnywhere function to find out:

> getAnywhere(lag)
2 differing objects matching ?lag? were found
in the following places
  package:dplyr
  package:stats
  namespace:dplyr
  namespace:stats


Sarah


On Tue, Nov 29, 2016 at 9:36 AM, Constantin Weiser <weiserc at hhu.de> wrote:
> Hello,
>
> I'm struggling with an unexpected interference between the two packages
> dplyr and plm, or to be more concrete with the "lag(x, ...)" function of
> both packages.
>
> If dplyr is in the namespace the plm function uses no longer the appropriate
> lag()-function which accounts for the panel structure.
>
> The following code demonstrates the unexpected behaviour:
>
> ## starting from a new R-Session (plm and dplyr unloaded) ##
>
>   ## generate dataset
>   set.seed(4711)
>   df <- data.frame(
>           i = rep(1:10, each = 4),
>           t = rep(1:4, times = 10),
>           y = rnorm(40),
>           x = rnorm(40)
>   )
>   ## manually generated laged variable
>   df$lagx <- c(NA, df$x[-40])
>   df$lagx[df$t == 1] <- NA
>
>
> require(plm)
> summary(plm(y~lagx, data = df, index = c("i", "t")))
> summary(plm(y~lag(x, 1), data = df, index = c("i", "t")))
> # > this result is expected
>
> require(dplyr)
> summary(plm(y~lagx, data = df, index = c("i", "t")))
> summary(plm(y~lag(x, 1), data = df, index = c("i", "t")))
> # > this result is unexpected
>
> Is there a way to force R to use the "correct" lag-function? (or at the
> devel-level to harmonise both functions)
>
> Thank you very much in advance for your answer
>
> Yours
> Constantin
>
> --
> ^

-- 
Sarah Goslee
http://www.functionaldiversity.org


From ed_isfahani at yahoo.com  Tue Nov 29 18:22:47 2016
From: ed_isfahani at yahoo.com (Elham -)
Date: Tue, 29 Nov 2016 17:22:47 +0000 (UTC)
Subject: [R] transpose rows and columns for large data
References: <1367996161.3261271.1480440167187.ref@mail.yahoo.com>
Message-ID: <1367996161.3261271.1480440167187@mail.yahoo.com>

Hi,

I am trying to transpose large datasets inexcel (44 columns and 57774 rows) but it keeps giving me the message we can'tpaste because copy area and paste area aren't the same size. Is there a way totranspose all the data at one time instead of piece by piece? One dataset has agreat amount of rows and columns.?

I tried this R function to transpose the datamatrix:

data <- read.table("your_file_location", sep ="\t", comment.char = "", stringAsFactors = F, header = T)


?
transpose_data <- t(data)

But I received tis error:

unused argument (stringAsFactors = F)


?

?
Is there another way (I prefer a way with Excel)?


	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Tue Nov 29 18:39:42 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 29 Nov 2016 09:39:42 -0800
Subject: [R] Unexpected interference between dplyr and plm
In-Reply-To: <CAM_vjumwQrxP8N4S-hcLBPy0N9_DPuyP-_qJh1KwFBjqyZtgqw@mail.gmail.com>
References: <d5fc847a-005a-468c-4152-ae58643eb332@hhu.de>
	<CAM_vjumwQrxP8N4S-hcLBPy0N9_DPuyP-_qJh1KwFBjqyZtgqw@mail.gmail.com>
Message-ID: <7DF89AF6-9338-4AE0-9EC9-FF24D7D7B18D@comcast.net>


> On Nov 29, 2016, at 6:52 AM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
> 
> Hi,
> 
> It shouldn't be entirely unexpected: when I load dplyr, I get a series
> of messages telling me that certain functions are masked.
> 
> 
> The following object is masked from ?package:plm?:
> 
>    between
> 
> The following objects are masked from ?package:stats?:
> 
>    filter, lag
> 
> The following objects are masked from ?package:base?:
> 
>    intersect, setdiff, setequal, union
> 
> 
> You can see the search path that R uses when looking for a function or
> other object here:
> 
> In your example, it should look like this:
> 
>> search()
> [1] ".GlobalEnv"        "package:dplyr"     "package:plm"
> "package:Formula"
> [5] "package:stats"     "package:graphics"  "package:grDevices"
> "package:utils"
> [9] "package:datasets"  "package:vimcom"    "package:setwidth"
> "package:colorout"
> [13] "package:methods"   "Autoloads"         "package:base"
> 
> 
> So R is searching the local environment, then dplyr, and then farther
> down the list, stats, which is where the lag function comes from (see
> above warning).
> 
> Once you know where the desired function comes from you can specify
> its namespace:

The other option would be to load dplyr first (which would give the waring that stats::lag was masked) and then later load plm (which should give a further warning that dplyr::lag is masked). Then the plm::lag function will be found first.

-- 
David.
> 
> 
> summary(plm(y~lagx, data = df, index = c("i", "t")))
> summary(plm(y~stats::lag(x, 1), data = df, index = c("i", "t")))
> 
> If you weren't paying attention to the warning messages at package
> load, you can also use the getAnywhere function to find out:
> 
>> getAnywhere(lag)
> 2 differing objects matching ?lag? were found
> in the following places
>  package:dplyr
>  package:stats
>  namespace:dplyr
>  namespace:stats
> 
> 
> Sarah
> 
> 
> On Tue, Nov 29, 2016 at 9:36 AM, Constantin Weiser <weiserc at hhu.de> wrote:
>> Hello,
>> 
>> I'm struggling with an unexpected interference between the two packages
>> dplyr and plm, or to be more concrete with the "lag(x, ...)" function of
>> both packages.
>> 
>> If dplyr is in the namespace the plm function uses no longer the appropriate
>> lag()-function which accounts for the panel structure.
>> 
>> The following code demonstrates the unexpected behaviour:
>> 
>> ## starting from a new R-Session (plm and dplyr unloaded) ##
>> 
>>  ## generate dataset
>>  set.seed(4711)
>>  df <- data.frame(
>>          i = rep(1:10, each = 4),
>>          t = rep(1:4, times = 10),
>>          y = rnorm(40),
>>          x = rnorm(40)
>>  )
>>  ## manually generated laged variable
>>  df$lagx <- c(NA, df$x[-40])
>>  df$lagx[df$t == 1] <- NA
>> 
>> 
>> require(plm)
>> summary(plm(y~lagx, data = df, index = c("i", "t")))
>> summary(plm(y~lag(x, 1), data = df, index = c("i", "t")))
>> # > this result is expected
>> 
>> require(dplyr)
>> summary(plm(y~lagx, data = df, index = c("i", "t")))
>> summary(plm(y~lag(x, 1), data = df, index = c("i", "t")))
>> # > this result is unexpected
>> 
>> Is there a way to force R to use the "correct" lag-function? (or at the
>> devel-level to harmonise both functions)
>> 
>> Thank you very much in advance for your answer
>> 
>> Yours
>> Constantin
>> 
>> --
>> ^
> 
> -- 
> Sarah Goslee
> http://www.functionaldiversity.org
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Tue Nov 29 18:43:08 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 29 Nov 2016 09:43:08 -0800
Subject: [R] transpose rows and columns for large data
In-Reply-To: <1367996161.3261271.1480440167187@mail.yahoo.com>
References: <1367996161.3261271.1480440167187.ref@mail.yahoo.com>
	<1367996161.3261271.1480440167187@mail.yahoo.com>
Message-ID: <821A51F2-38BE-480E-A88A-7044906F8832@comcast.net>


> On Nov 29, 2016, at 9:22 AM, Elham - via R-help <r-help at r-project.org> wrote:
> 
> Hi,
> 
> I am trying to transpose large datasets inexcel (44 columns and 57774 rows) but it keeps giving me the message we can'tpaste because copy area and paste area aren't the same size. Is there a way totranspose all the data at one time instead of piece by piece? One dataset has agreat amount of rows and columns. 
> 
> I tried this R function to transpose the datamatrix:
> 
> data <- read.table("your_file_location", sep ="\t", comment.char = "", stringAsFactors = F, header = T)
> 
> 
>  
> transpose_data <- t(data)
> 
> But I received tis error:
> 
> unused argument (stringAsFactors = F)
> 

You misspelled that argument's name. And do learn to use FALSE and TRUE.

>  
> Is there another way (I prefer a way with Excel)?

This is not a help list for Excel.


-- 

David Winsemius
Alameda, CA, USA


From wdunlap at tibco.com  Tue Nov 29 18:52:41 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 29 Nov 2016 09:52:41 -0800
Subject: [R] Unexpected interference between dplyr and plm
In-Reply-To: <7DF89AF6-9338-4AE0-9EC9-FF24D7D7B18D@comcast.net>
References: <d5fc847a-005a-468c-4152-ae58643eb332@hhu.de>
	<CAM_vjumwQrxP8N4S-hcLBPy0N9_DPuyP-_qJh1KwFBjqyZtgqw@mail.gmail.com>
	<7DF89AF6-9338-4AE0-9EC9-FF24D7D7B18D@comcast.net>
Message-ID: <CAF8bMcZXhAutpOUHcAD=xaLY-92ceBR9UryFQE0P6kAFk+27Yw@mail.gmail.com>

>The other option would be to load dplyr first (which would give the waring
that >stats::lag was masked) and then later load plm (which should give a
further >warning that dplyr::lag is masked). Then the plm::lag function
will be found
>first.

Another option is to write the package maintainers and complain
that masking core functions is painful for users.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Nov 29, 2016 at 9:39 AM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Nov 29, 2016, at 6:52 AM, Sarah Goslee <sarah.goslee at gmail.com>
> wrote:
> >
> > Hi,
> >
> > It shouldn't be entirely unexpected: when I load dplyr, I get a series
> > of messages telling me that certain functions are masked.
> >
> >
> > The following object is masked from ?package:plm?:
> >
> >    between
> >
> > The following objects are masked from ?package:stats?:
> >
> >    filter, lag
> >
> > The following objects are masked from ?package:base?:
> >
> >    intersect, setdiff, setequal, union
> >
> >
> > You can see the search path that R uses when looking for a function or
> > other object here:
> >
> > In your example, it should look like this:
> >
> >> search()
> > [1] ".GlobalEnv"        "package:dplyr"     "package:plm"
> > "package:Formula"
> > [5] "package:stats"     "package:graphics"  "package:grDevices"
> > "package:utils"
> > [9] "package:datasets"  "package:vimcom"    "package:setwidth"
> > "package:colorout"
> > [13] "package:methods"   "Autoloads"         "package:base"
> >
> >
> > So R is searching the local environment, then dplyr, and then farther
> > down the list, stats, which is where the lag function comes from (see
> > above warning).
> >
> > Once you know where the desired function comes from you can specify
> > its namespace:
>
> The other option would be to load dplyr first (which would give the waring
> that stats::lag was masked) and then later load plm (which should give a
> further warning that dplyr::lag is masked). Then the plm::lag function will
> be found first.
>
> --
> David.
> >
> >
> > summary(plm(y~lagx, data = df, index = c("i", "t")))
> > summary(plm(y~stats::lag(x, 1), data = df, index = c("i", "t")))
> >
> > If you weren't paying attention to the warning messages at package
> > load, you can also use the getAnywhere function to find out:
> >
> >> getAnywhere(lag)
> > 2 differing objects matching ?lag? were found
> > in the following places
> >  package:dplyr
> >  package:stats
> >  namespace:dplyr
> >  namespace:stats
> >
> >
> > Sarah
> >
> >
> > On Tue, Nov 29, 2016 at 9:36 AM, Constantin Weiser <weiserc at hhu.de>
> wrote:
> >> Hello,
> >>
> >> I'm struggling with an unexpected interference between the two packages
> >> dplyr and plm, or to be more concrete with the "lag(x, ...)" function of
> >> both packages.
> >>
> >> If dplyr is in the namespace the plm function uses no longer the
> appropriate
> >> lag()-function which accounts for the panel structure.
> >>
> >> The following code demonstrates the unexpected behaviour:
> >>
> >> ## starting from a new R-Session (plm and dplyr unloaded) ##
> >>
> >>  ## generate dataset
> >>  set.seed(4711)
> >>  df <- data.frame(
> >>          i = rep(1:10, each = 4),
> >>          t = rep(1:4, times = 10),
> >>          y = rnorm(40),
> >>          x = rnorm(40)
> >>  )
> >>  ## manually generated laged variable
> >>  df$lagx <- c(NA, df$x[-40])
> >>  df$lagx[df$t == 1] <- NA
> >>
> >>
> >> require(plm)
> >> summary(plm(y~lagx, data = df, index = c("i", "t")))
> >> summary(plm(y~lag(x, 1), data = df, index = c("i", "t")))
> >> # > this result is expected
> >>
> >> require(dplyr)
> >> summary(plm(y~lagx, data = df, index = c("i", "t")))
> >> summary(plm(y~lag(x, 1), data = df, index = c("i", "t")))
> >> # > this result is unexpected
> >>
> >> Is there a way to force R to use the "correct" lag-function? (or at the
> >> devel-level to harmonise both functions)
> >>
> >> Thank you very much in advance for your answer
> >>
> >> Yours
> >> Constantin
> >>
> >> --
> >> ^
> >
> > --
> > Sarah Goslee
> > http://www.functionaldiversity.org
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From ed_isfahani at yahoo.com  Tue Nov 29 18:56:32 2016
From: ed_isfahani at yahoo.com (Elham -)
Date: Tue, 29 Nov 2016 17:56:32 +0000 (UTC)
Subject: [R] transpose rows and columns for large data
In-Reply-To: <821A51F2-38BE-480E-A88A-7044906F8832@comcast.net>
References: <1367996161.3261271.1480440167187.ref@mail.yahoo.com>
	<1367996161.3261271.1480440167187@mail.yahoo.com>
	<821A51F2-38BE-480E-A88A-7044906F8832@comcast.net>
Message-ID: <685014488.3323928.1480442192752@mail.yahoo.com>

yes you have right about excel.by R,what should I do for transposing row and column? 

    On Tuesday, November 29, 2016 9:13 PM, David Winsemius <dwinsemius at comcast.net> wrote:
 

 
> On Nov 29, 2016, at 9:22 AM, Elham - via R-help <r-help at r-project.org> wrote:
> 
> Hi,
> 
> I am trying to transpose large datasets inexcel (44 columns and 57774 rows) but it keeps giving me the message we can'tpaste because copy area and paste area aren't the same size. Is there a way totranspose all the data at one time instead of piece by piece? One dataset has agreat amount of rows and columns. 
> 
> I tried this R function to transpose the datamatrix:
> 
> data <- read.table("your_file_location", sep ="\t", comment.char = "", stringAsFactors = F, header = T)
> 
> 
>? 
> transpose_data <- t(data)
> 
> But I received tis error:
> 
> unused argument (stringAsFactors = F)
> 

You misspelled that argument's name. And do learn to use FALSE and TRUE.

>? 
> Is there another way (I prefer a way with Excel)?

This is not a help list for Excel.


-- 

David Winsemius
Alameda, CA, USA


   
	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Nov 29 19:00:30 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 29 Nov 2016 10:00:30 -0800
Subject: [R] transpose rows and columns for large data
In-Reply-To: <1367996161.3261271.1480440167187@mail.yahoo.com>
References: <1367996161.3261271.1480440167187.ref@mail.yahoo.com>
	<1367996161.3261271.1480440167187@mail.yahoo.com>
Message-ID: <CAGxFJbT7QfMttugABFzVFz0zjDaOuxBg8sA=bQoA1i3nd_4GZw@mail.gmail.com>

It's 'stringsAsFactors' = FALSE (without my added quotes) with an 's'
at the end of 'strings' .

-- Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Nov 29, 2016 at 9:22 AM, Elham - via R-help
<r-help at r-project.org> wrote:
> Hi,
>
> I am trying to transpose large datasets inexcel (44 columns and 57774 rows) but it keeps giving me the message we can'tpaste because copy area and paste area aren't the same size. Is there a way totranspose all the data at one time instead of piece by piece? One dataset has agreat amount of rows and columns.
>
> I tried this R function to transpose the datamatrix:
>
> data <- read.table("your_file_location", sep ="\t", comment.char = "", stringAsFactors = F, header = T)
>
>
>
> transpose_data <- t(data)
>
> But I received tis error:
>
> unused argument (stringAsFactors = F)
>
>
>
>
>
> Is there another way (I prefer a way with Excel)?
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ddalthorp at usgs.gov  Tue Nov 29 19:09:53 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Tue, 29 Nov 2016 10:09:53 -0800
Subject: [R] transpose rows and columns for large data
In-Reply-To: <685014488.3323928.1480442192752@mail.yahoo.com>
References: <1367996161.3261271.1480440167187.ref@mail.yahoo.com>
	<1367996161.3261271.1480440167187@mail.yahoo.com>
	<821A51F2-38BE-480E-A88A-7044906F8832@comcast.net>
	<685014488.3323928.1480442192752@mail.yahoo.com>
Message-ID: <CAJeYpE-LpY-c3oGmEEu3crGzmZ2bJ__7Y=D-sYm6w73NZg7RUQ@mail.gmail.com>

Try David's suggestion to spell the argument "stringsAsFactors" correctly.
Then:

data <- read.table("your_file_location", sep ="\t", comment.char = "",
stringsAsFactors = F, header = T)
transpose_data <- t(data)

-Dan

On Tue, Nov 29, 2016 at 9:56 AM, Elham - via R-help <r-help at r-project.org>
wrote:

> yes you have right about excel.by R,what should I do for transposing row
> and column?
>
>     On Tuesday, November 29, 2016 9:13 PM, David Winsemius <
> dwinsemius at comcast.net> wrote:
>
>
>
> > On Nov 29, 2016, at 9:22 AM, Elham - via R-help <r-help at r-project.org>
> wrote:
> >
> > Hi,
> >
> > I am trying to transpose large datasets inexcel (44 columns and 57774
> rows) but it keeps giving me the message we can'tpaste because copy area
> and paste area aren't the same size. Is there a way totranspose all the
> data at one time instead of piece by piece? One dataset has agreat amount
> of rows and columns.
> >
> > I tried this R function to transpose the datamatrix:
> >
> > data <- read.table("your_file_location", sep ="\t", comment.char = "",
> stringAsFactors = F, header = T)
> >
> >
> >
> > transpose_data <- t(data)
> >
> > But I received tis error:
> >
> > unused argument (stringAsFactors = F)
> >
>
> You misspelled that argument's name. And do learn to use FALSE and TRUE.
>
> >
> > Is there another way (I prefer a way with Excel)?
>
> This is not a help list for Excel.
>
>
> --
>
> David Winsemius
> Alameda, CA, USA
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.




-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From ed_isfahani at yahoo.com  Tue Nov 29 19:37:32 2016
From: ed_isfahani at yahoo.com (Elham -)
Date: Tue, 29 Nov 2016 18:37:32 +0000 (UTC)
Subject: [R] transpose rows and columns for large data
In-Reply-To: <CAJeYpE-LpY-c3oGmEEu3crGzmZ2bJ__7Y=D-sYm6w73NZg7RUQ@mail.gmail.com>
References: <1367996161.3261271.1480440167187.ref@mail.yahoo.com>
	<1367996161.3261271.1480440167187@mail.yahoo.com>
	<821A51F2-38BE-480E-A88A-7044906F8832@comcast.net>
	<685014488.3323928.1480442192752@mail.yahoo.com>
	<CAJeYpE-LpY-c3oGmEEu3crGzmZ2bJ__7Y=D-sYm6w73NZg7RUQ@mail.gmail.com>
Message-ID: <1630484246.3359558.1480444652744@mail.yahoo.com>

thank you all,it worked 

    On Tuesday, November 29, 2016 9:49 PM, "Dalthorp, Daniel" <ddalthorp at usgs.gov> wrote:
 

 Try David's suggestion to spell the argument "stringsAsFactors" correctly. Then:

data <- read.table("your_file_location", sep ="\t", comment.char = "", stringsAsFactors = F, header = T)
transpose_data <- t(data)

-Dan
On Tue, Nov 29, 2016 at 9:56 AM, Elham - via R-help <r-help at r-project.org> wrote:

yes you have right about excel.by R,what should I do for transposing row and column?

? ? On Tuesday, November 29, 2016 9:13 PM, David Winsemius <dwinsemius at comcast.net> wrote:



> On Nov 29, 2016, at 9:22 AM, Elham - via R-help <r-help at r-project.org> wrote:
>
> Hi,
>
> I am trying to transpose large datasets inexcel (44 columns and 57774 rows) but it keeps giving me the message we can'tpaste because copy area and paste area aren't the same size. Is there a way totranspose all the data at one time instead of piece by piece? One dataset has agreat amount of rows and columns.
>
> I tried this R function to transpose the datamatrix:
>
> data <- read.table("your_file_ location", sep ="\t", comment.char = "", stringAsFactors = F, header = T)
>
>
>?
> transpose_data <- t(data)
>
> But I received tis error:
>
> unused argument (stringAsFactors = F)
>

You misspelled that argument's name. And do learn to use FALSE and TRUE.

>?
> Is there another way (I prefer a way with Excel)?

This is not a help list for Excel.


--

David Winsemius
Alameda, CA, USA



? ? ? ? [[alternative HTML version deleted]]

______________________________ ________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/ listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/ posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



-- 
Dan Dalthorp, PhDUSGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way 
Corvallis, OR 97331 
ph: 541-750-0953
ddalthorp at usgs.gov



   
	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Nov 29 19:54:40 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 29 Nov 2016 10:54:40 -0800
Subject: [R] transpose rows and columns for large data
In-Reply-To: <1630484246.3359558.1480444652744@mail.yahoo.com>
References: <1367996161.3261271.1480440167187.ref@mail.yahoo.com>
	<1367996161.3261271.1480440167187@mail.yahoo.com>
	<821A51F2-38BE-480E-A88A-7044906F8832@comcast.net>
	<685014488.3323928.1480442192752@mail.yahoo.com>
	<CAJeYpE-LpY-c3oGmEEu3crGzmZ2bJ__7Y=D-sYm6w73NZg7RUQ@mail.gmail.com>
	<1630484246.3359558.1480444652744@mail.yahoo.com>
Message-ID: <CAGxFJbRNe7sch5YoewjSbJCW1TbNJ7+6cw1MLaJZ8cHoa=Aazg@mail.gmail.com>

It is probably worth mentioning that this (i.e. transposing a data
frame) can be a potentially disastrous thing to do in R, though the
explanation is probably more than you want to know at this point (see
?t  and follow the 'as.matrix' link for details).  But if you start
getting weird results and error/warning messages when working with
your transposed data, at least you'll know why.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Nov 29, 2016 at 10:37 AM, Elham - via R-help
<r-help at r-project.org> wrote:
> thank you all,it worked
>
>     On Tuesday, November 29, 2016 9:49 PM, "Dalthorp, Daniel" <ddalthorp at usgs.gov> wrote:
>
>
>  Try David's suggestion to spell the argument "stringsAsFactors" correctly. Then:
>
> data <- read.table("your_file_location", sep ="\t", comment.char = "", stringsAsFactors = F, header = T)
> transpose_data <- t(data)
>
> -Dan
> On Tue, Nov 29, 2016 at 9:56 AM, Elham - via R-help <r-help at r-project.org> wrote:
>
> yes you have right about excel.by R,what should I do for transposing row and column?
>
>     On Tuesday, November 29, 2016 9:13 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>
>
>
>> On Nov 29, 2016, at 9:22 AM, Elham - via R-help <r-help at r-project.org> wrote:
>>
>> Hi,
>>
>> I am trying to transpose large datasets inexcel (44 columns and 57774 rows) but it keeps giving me the message we can'tpaste because copy area and paste area aren't the same size. Is there a way totranspose all the data at one time instead of piece by piece? One dataset has agreat amount of rows and columns.
>>
>> I tried this R function to transpose the datamatrix:
>>
>> data <- read.table("your_file_ location", sep ="\t", comment.char = "", stringAsFactors = F, header = T)
>>
>>
>>
>> transpose_data <- t(data)
>>
>> But I received tis error:
>>
>> unused argument (stringAsFactors = F)
>>
>
> You misspelled that argument's name. And do learn to use FALSE and TRUE.
>
>>
>> Is there another way (I prefer a way with Excel)?
>
> This is not a help list for Excel.
>
>
> --
>
> David Winsemius
> Alameda, CA, USA
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________ ________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/ listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/ posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Dan Dalthorp, PhDUSGS Forest and Rangeland Ecosystem Science Center
> Forest Sciences Lab, Rm 189
> 3200 SW Jefferson Way
> Corvallis, OR 97331
> ph: 541-750-0953
> ddalthorp at usgs.gov
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From hadley at rstudio.com  Tue Nov 29 20:26:34 2016
From: hadley at rstudio.com (Hadley Wickham)
Date: Tue, 29 Nov 2016 13:26:34 -0600
Subject: [R] Unexpected interference between dplyr and plm
In-Reply-To: <CAF8bMcZXhAutpOUHcAD=xaLY-92ceBR9UryFQE0P6kAFk+27Yw@mail.gmail.com>
References: <d5fc847a-005a-468c-4152-ae58643eb332@hhu.de>
	<CAM_vjumwQrxP8N4S-hcLBPy0N9_DPuyP-_qJh1KwFBjqyZtgqw@mail.gmail.com>
	<7DF89AF6-9338-4AE0-9EC9-FF24D7D7B18D@comcast.net>
	<CAF8bMcZXhAutpOUHcAD=xaLY-92ceBR9UryFQE0P6kAFk+27Yw@mail.gmail.com>
Message-ID: <CABdHhvFYGmNcZzWXrs=u4hWwYd3=uGT-bpRiiJhmqnvzOn9kzQ@mail.gmail.com>

On Tue, Nov 29, 2016 at 11:52 AM, William Dunlap <wdunlap at tibco.com> wrote:
>>The other option would be to load dplyr first (which would give the waring
>> that >stats::lag was masked) and then later load plm (which should give a
>> further >warning that dplyr::lag is masked). Then the plm::lag function will
>> be found
>>first.
>
> Another option is to write the package maintainers and complain
> that masking core functions is painful for users.

Don't worry; many people have done that.

Hadley

-- 
http://hadley.nz


From sarah.goslee at gmail.com  Tue Nov 29 20:31:28 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 29 Nov 2016 14:31:28 -0500
Subject: [R] Unexpected interference between dplyr and plm
In-Reply-To: <7DF89AF6-9338-4AE0-9EC9-FF24D7D7B18D@comcast.net>
References: <d5fc847a-005a-468c-4152-ae58643eb332@hhu.de>
	<CAM_vjumwQrxP8N4S-hcLBPy0N9_DPuyP-_qJh1KwFBjqyZtgqw@mail.gmail.com>
	<7DF89AF6-9338-4AE0-9EC9-FF24D7D7B18D@comcast.net>
Message-ID: <CAM_vjum5K5NpXL6JMLpw3pMiE0qQ7HCiYp=ba2sNNrUMKxEiag@mail.gmail.com>

On Tue, Nov 29, 2016 at 12:39 PM, David Winsemius
<dwinsemius at comcast.net> wrote:
>
>
> The other option would be to load dplyr first (which would give the waring that stats::lag was masked) and then later load plm (which should give a further warning that dplyr::lag is masked). Then the plm::lag function will be found first.

There isn't a plm::lag function; the desired function is stats::lag

It matters whether dplyr is loaded because that masks stats::lag().

It only matters whether plm is loaded because that package provides
the function that the original querent wanted to use lag() in the
context of.

Sarah




-- 
Sarah Goslee
http://www.functionaldiversity.org


From bgunter.4567 at gmail.com  Tue Nov 29 20:33:56 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 29 Nov 2016 11:33:56 -0800
Subject: [R] transpose rows and columns for large data
In-Reply-To: <1833999438.1544821.1480446087527@mail.yahoo.com>
References: <1367996161.3261271.1480440167187.ref@mail.yahoo.com>
	<1367996161.3261271.1480440167187@mail.yahoo.com>
	<821A51F2-38BE-480E-A88A-7044906F8832@comcast.net>
	<685014488.3323928.1480442192752@mail.yahoo.com>
	<CAJeYpE-LpY-c3oGmEEu3crGzmZ2bJ__7Y=D-sYm6w73NZg7RUQ@mail.gmail.com>
	<1630484246.3359558.1480444652744@mail.yahoo.com>
	<CAGxFJbRNe7sch5YoewjSbJCW1TbNJ7+6cw1MLaJZ8cHoa=Aazg@mail.gmail.com>
	<1833999438.1544821.1480446087527@mail.yahoo.com>
Message-ID: <CAGxFJbSwZMNjz79LE5_18PetzntXp3_5UbGWc+UV5=wrQTbtfQ@mail.gmail.com>

No, no. It *is* for transposing. But it is *what* you are transposing
-- a data frame -- that may lead to the problems. You will have to
read what I referred you to and perhaps spend time with an R tutorial
or two (there are many good ones on the web) if your R learning is not
yet sufficient to understand what they say.

-- Bert




Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Nov 29, 2016 at 11:01 AM, Elham - <ed_isfahani at yahoo.com> wrote:
> excuse me I did not understand,you mean this function is not for
> transposing? what function do you suggest?
>
>
> On Tuesday, November 29, 2016 10:24 PM, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
>
>
> It is probably worth mentioning that this (i.e. transposing a data
> frame) can be a potentially disastrous thing to do in R, though the
> explanation is probably more than you want to know at this point (see
> ?t  and follow the 'as.matrix' link for details).  But if you start
> getting weird results and error/warning messages when working with
> your transposed data, at least you'll know why.
>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Tue, Nov 29, 2016 at 10:37 AM, Elham - via R-help
> <r-help at r-project.org> wrote:
>> thank you all,it worked
>>
>>    On Tuesday, November 29, 2016 9:49 PM, "Dalthorp, Daniel"
>> <ddalthorp at usgs.gov> wrote:
>>
>>
>>  Try David's suggestion to spell the argument "stringsAsFactors"
>> correctly. Then:
>>
>> data <- read.table("your_file_location", sep ="\t", comment.char = "",
>> stringsAsFactors = F, header = T)
>> transpose_data <- t(data)
>>
>> -Dan
>> On Tue, Nov 29, 2016 at 9:56 AM, Elham - via R-help <r-help at r-project.org>
>> wrote:
>>
>> yes you have right about excel.by R,what should I do for transposing row
>> and column?
>>
>>    On Tuesday, November 29, 2016 9:13 PM, David Winsemius
>> <dwinsemius at comcast.net> wrote:
>>
>>
>>
>>> On Nov 29, 2016, at 9:22 AM, Elham - via R-help <r-help at r-project.org>
>>> wrote:
>>>
>>> Hi,
>>>
>>> I am trying to transpose large datasets inexcel (44 columns and 57774
>>> rows) but it keeps giving me the message we can'tpaste because copy area and
>>> paste area aren't the same size. Is there a way totranspose all the data at
>>> one time instead of piece by piece? One dataset has agreat amount of rows
>>> and columns.
>>>
>>> I tried this R function to transpose the datamatrix:
>>>
>>> data <- read.table("your_file_ location", sep ="\t", comment.char = "",
>>> stringAsFactors = F, header = T)
>>>
>>>
>>>
>>> transpose_data <- t(data)
>>>
>>> But I received tis error:
>>>
>>> unused argument (stringAsFactors = F)
>>>
>>
>> You misspelled that argument's name. And do learn to use FALSE and TRUE.
>>
>>>
>>> Is there another way (I prefer a way with Excel)?
>>
>> This is not a help list for Excel.
>>
>>
>> --
>>
>> David Winsemius
>> Alameda, CA, USA
>>
>>
>>
>>        [[alternative HTML version deleted]]
>>
>> ______________________________ ________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/ listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>> --
>> Dan Dalthorp, PhDUSGS Forest and Rangeland Ecosystem Science Center
>> Forest Sciences Lab, Rm 189
>> 3200 SW Jefferson Way
>> Corvallis, OR 97331
>> ph: 541-750-0953
>
>> ddalthorp at usgs.gov
>>
>>
>>
>>
>>        [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>


From marc_schwartz at me.com  Tue Nov 29 20:54:11 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Tue, 29 Nov 2016 13:54:11 -0600
Subject: [R] transpose rows and columns for large data
In-Reply-To: <CAGxFJbSwZMNjz79LE5_18PetzntXp3_5UbGWc+UV5=wrQTbtfQ@mail.gmail.com>
References: <1367996161.3261271.1480440167187.ref@mail.yahoo.com>
	<1367996161.3261271.1480440167187@mail.yahoo.com>
	<821A51F2-38BE-480E-A88A-7044906F8832@comcast.net>
	<685014488.3323928.1480442192752@mail.yahoo.com>
	<CAJeYpE-LpY-c3oGmEEu3crGzmZ2bJ__7Y=D-sYm6w73NZg7RUQ@mail.gmail.com>
	<1630484246.3359558.1480444652744@mail.yahoo.com>
	<CAGxFJbRNe7sch5YoewjSbJCW1TbNJ7+6cw1MLaJZ8cHoa=Aazg@mail.gmail.com>
	<1833999438.1544821.1480446087527@mail.yahoo.com>
	<CAGxFJbSwZMNjz79LE5_18PetzntXp3_5UbGWc+UV5=wrQTbtfQ@mail.gmail.com>
Message-ID: <58DA9100-13BE-4A4E-AFE6-08477BD2FAF3@me.com>

Hi,

To provide a [very] small example of what Bert is referring to:

DF <- data.frame(Letters = letters[1:4], int = 1:4)

> str(DF)
'data.frame':	4 obs. of  2 variables:
 $ Letters: Factor w/ 4 levels "a","b","c","d": 1 2 3 4
 $ int    : int  1 2 3 4

> DF
  Letters int
1       a   1
2       b   2
3       c   3
4       d   4


DFt <- t(DF)

> str(DFt)
 chr [1:2, 1:4] "a" "1" "b" "2" "c" "3" "d" "4"
 - attr(*, "dimnames")=List of 2
  ..$ : chr [1:2] "Letters" "int"
  ..$ : NULL

> DFt
        [,1] [,2] [,3] [,4]
Letters "a"  "b"  "c"  "d" 
int     "1"  "2"  "3"  "4" 


Note the change in the structure  and the data types of the data frame after the transposition...

A data frame, which is a special type of 'list', may contain multiple data types, one per column. It is designed, more or less, specifically to be able to handle multiple data types across the columns, as you might have in a database (character, numeric, date, etc.), but at the expense of some operations.

A matrix, which is in reality a vector with dimensions, can only contain a single data type and in this example, the numeric column is coerced to character. Note also that the "Letters" column in "DF" is changed from being a factor to character as well. So both columns are affected by the transposition.

Regards,

Marc Schwartz


> On Nov 29, 2016, at 1:33 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> No, no. It *is* for transposing. But it is *what* you are transposing
> -- a data frame -- that may lead to the problems. You will have to
> read what I referred you to and perhaps spend time with an R tutorial
> or two (there are many good ones on the web) if your R learning is not
> yet sufficient to understand what they say.
> 
> -- Bert
> 
> 
> 
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
> On Tue, Nov 29, 2016 at 11:01 AM, Elham - <ed_isfahani at yahoo.com> wrote:
>> excuse me I did not understand,you mean this function is not for
>> transposing? what function do you suggest?
>> 
>> 
>> On Tuesday, November 29, 2016 10:24 PM, Bert Gunter <bgunter.4567 at gmail.com>
>> wrote:
>> 
>> 
>> It is probably worth mentioning that this (i.e. transposing a data
>> frame) can be a potentially disastrous thing to do in R, though the
>> explanation is probably more than you want to know at this point (see
>> ?t  and follow the 'as.matrix' link for details).  But if you start
>> getting weird results and error/warning messages when working with
>> your transposed data, at least you'll know why.
>> 
>> Cheers,
>> Bert
>> 
>> 
>> Bert Gunter
>> 
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>> 
>> 
>> On Tue, Nov 29, 2016 at 10:37 AM, Elham - via R-help
>> <r-help at r-project.org> wrote:
>>> thank you all,it worked
>>> 
>>>   On Tuesday, November 29, 2016 9:49 PM, "Dalthorp, Daniel"
>>> <ddalthorp at usgs.gov> wrote:
>>> 
>>> 
>>> Try David's suggestion to spell the argument "stringsAsFactors"
>>> correctly. Then:
>>> 
>>> data <- read.table("your_file_location", sep ="\t", comment.char = "",
>>> stringsAsFactors = F, header = T)
>>> transpose_data <- t(data)
>>> 
>>> -Dan
>>> On Tue, Nov 29, 2016 at 9:56 AM, Elham - via R-help <r-help at r-project.org>
>>> wrote:
>>> 
>>> yes you have right about excel.by R,what should I do for transposing row
>>> and column?
>>> 
>>>   On Tuesday, November 29, 2016 9:13 PM, David Winsemius
>>> <dwinsemius at comcast.net> wrote:
>>> 
>>> 
>>> 
>>>> On Nov 29, 2016, at 9:22 AM, Elham - via R-help <r-help at r-project.org>
>>>> wrote:
>>>> 
>>>> Hi,
>>>> 
>>>> I am trying to transpose large datasets inexcel (44 columns and 57774
>>>> rows) but it keeps giving me the message we can'tpaste because copy area and
>>>> paste area aren't the same size. Is there a way totranspose all the data at
>>>> one time instead of piece by piece? One dataset has agreat amount of rows
>>>> and columns.
>>>> 
>>>> I tried this R function to transpose the datamatrix:
>>>> 
>>>> data <- read.table("your_file_ location", sep ="\t", comment.char = "",
>>>> stringAsFactors = F, header = T)
>>>> 
>>>> 
>>>> 
>>>> transpose_data <- t(data)
>>>> 
>>>> But I received tis error:
>>>> 
>>>> unused argument (stringAsFactors = F)
>>>> 
>>> 
>>> You misspelled that argument's name. And do learn to use FALSE and TRUE.
>>> 
>>>> 
>>>> Is there another way (I prefer a way with Excel)?
>>> 
>>> This is not a help list for Excel.
>>> 
>>> 
>>> --
>>> 
>>> David Winsemius
>>> Alameda, CA, USA
>>> 
>>> 
>>> 
>>>       [[alternative HTML version deleted]]
>>> 
>>> ______________________________ ________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/ listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/
>>> posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> 
>>> 
>>> --
>>> Dan Dalthorp, PhDUSGS Forest and Rangeland Ecosystem Science Center
>>> Forest Sciences Lab, Rm 189
>>> 3200 SW Jefferson Way
>>> Corvallis, OR 97331
>>> ph: 541-750-0953
>> 
>>> ddalthorp at usgs.gov
>>> 
>>> 
>>> 
>>> 
>>>       [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From milujisb at gmail.com  Tue Nov 29 23:45:49 2016
From: milujisb at gmail.com (Miluji Sb)
Date: Tue, 29 Nov 2016 23:45:49 +0100
Subject: [R] Convert arc-second to degree
Message-ID: <CAMLwc7OwKQyRWvGFc-Z1yVPxSrqNsCe+TZ_qgSb=RV_j7AityA@mail.gmail.com>

Dear all,

I am using the Gridded Population of the World (v4) for the year 2010. The
data is in GeoTiFF format.

Source:
http://sedac.ciesin.columbia.edu/data/set/gpw-v4-population-count-adjusted-to-2015-unwpp-country-totals/data-download

I imported the data using:

library(raster)
library(maptools)
library(ncdf4)
library(rgdal)

population <-
raster("gpw-v4-population-count-adjusted-to-2015-unwpp-country-totals_2010.tif")
population1 <- stack(population )
extent(population1 ) <- c(-180, 180,-58,85)

### Information
class       : RasterStack
dimensions  : 17400, 43200, 751680000, 1  (nrow, ncol, ncell, nlayers)
resolution  : 0.008333333, 0.008333333  (x, y)
extent      : -180, 180, -60, 85  (xmin, xmax, ymin, ymax)
coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84
+towgs84=0,0,0
names       :
gpw.v4.population.count.adjusted.to.2015.unwpp.country.totals_2010
min values  :
   0
max values  :
141715.3
###

I need to extract population by a set of coordinates which are at 1? x 1?,
how can I convert from arc-second to degree in R? The information also
shows that resolution is 0.008333333? x 0.008333333?, is it enough to do
something like this?

pop_agg <- aggregate(population1 , fact=120, fun=sum)

Thank you very much.

Sincerely,

Milu

	[[alternative HTML version deleted]]


From btupper at bigelow.org  Wed Nov 30 00:09:31 2016
From: btupper at bigelow.org (Ben Tupper)
Date: Tue, 29 Nov 2016 18:09:31 -0500
Subject: [R] Convert arc-second to degree
In-Reply-To: <CAMLwc7OwKQyRWvGFc-Z1yVPxSrqNsCe+TZ_qgSb=RV_j7AityA@mail.gmail.com>
References: <CAMLwc7OwKQyRWvGFc-Z1yVPxSrqNsCe+TZ_qgSb=RV_j7AityA@mail.gmail.com>
Message-ID: <730CC9A3-ABF6-48E9-AFFA-D1371F727B1A@bigelow.org>

Hello,

I haven't downloaded the data, but a mock-up of your steps below does as you ask. You can see the resolution of y is 1 x 1 and each is filled with the sum of 120 x 120 original cells each of which had a value of 1.

In this case, the raster package faithfully interprets the fractional degree spatial units from the get-go.  So you needn't worry about the arc-second to degree issue.

Ben

P.S.  This question is about spatial data; your best results will be had by subscribing to and posting to the spatial mailing list for R.  https://stat.ethz.ch/mailman/listinfo/r-sig-geo

library(raster)
x <- raster(
     nrows = 17400, ncols = 43200,
     xmn = -180, xmx = 180, ymn = -60, ymx = 85, 
     crs = '+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0')
x[] <- 1
x
  class       : RasterLayer 
  dimensions  : 17400, 43200, 751680000  (nrow, ncol, ncell)
  resolution  : 0.008333333, 0.008333333  (x, y)
  extent      : -180, 180, -60, 85  (xmin, xmax, ymin, ymax)
  coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 
  data source : in memory
  names       : layer 
  values      : 1, 1  (min, max)

y <- aggregate(x, fact = 120, fun = sum)
y
  class       : RasterLayer 
  dimensions  : 145, 360, 52200  (nrow, ncol, ncell)
  resolution  : 1, 1  (x, y)
  extent      : -180, 180, -60, 85  (xmin, xmax, ymin, ymax)
  coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 
  data source : /private/var/folders/xx/nnm6q33102z059rfg4rh2y900000gn/T/RtmpgsByoE/raster/r_tmp_2016-11-29_180022_89972_05480.grd 
  names       : layer 
  values      : 14400, 14400  (min, max)




> On Nov 29, 2016, at 5:45 PM, Miluji Sb <milujisb at gmail.com> wrote:
> 
> Dear all,
> 
> I am using the Gridded Population of the World (v4) for the year 2010. The
> data is in GeoTiFF format.
> 
> Source:
> http://sedac.ciesin.columbia.edu/data/set/gpw-v4-population-count-adjusted-to-2015-unwpp-country-totals/data-download
> 
> I imported the data using:
> 
> library(raster)
> library(maptools)
> library(ncdf4)
> library(rgdal)
> 
> population <-
> raster("gpw-v4-population-count-adjusted-to-2015-unwpp-country-totals_2010.tif")
> population1 <- stack(population )
> extent(population1 ) <- c(-180, 180,-58,85)
> 
> ### Information
> class       : RasterStack
> dimensions  : 17400, 43200, 751680000, 1  (nrow, ncol, ncell, nlayers)
> resolution  : 0.008333333, 0.008333333  (x, y)
> extent      : -180, 180, -60, 85  (xmin, xmax, ymin, ymax)
> coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84
> +towgs84=0,0,0
> names       :
> gpw.v4.population.count.adjusted.to.2015.unwpp.country.totals_2010
> min values  :
>   0
> max values  :
> 141715.3
> ###
> 
> I need to extract population by a set of coordinates which are at 1? x 1?,
> how can I convert from arc-second to degree in R? The information also
> shows that resolution is 0.008333333? x 0.008333333?, is it enough to do
> something like this?
> 
> pop_agg <- aggregate(population1 , fact=120, fun=sum)
> 
> Thank you very much.
> 
> Sincerely,
> 
> Milu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org


From glennmschultz at me.com  Tue Nov 29 19:37:29 2016
From: glennmschultz at me.com (Glenn Schultz)
Date: Tue, 29 Nov 2016 18:37:29 +0000 (GMT)
Subject: [R] using grepl in dplyr
Message-ID: <00db84c8-b19b-4347-831a-6af567e5784e@me.com>

Hello All,

I have a dataframe of about 1.5 million rows from this dataframe I need to filter out identifiers. ?An example would be 070000-07099, AD0000-AD0999, and AL0000-AL9999, FN0000-FN9999. ?I am using grepl to identify those of interest as follows:

?grepl("^[FN]|[AD]{2}", Identifier)

The above seems to work in the case of FN and AD. ?However, there are 20 such identifiers and there must be a better way to do this than a long "or" statement. ?Ultimately, I would like to filter these out using dplyr which I think the first step is to create a vector of TRUE/FALSE then filter on TRUE

Any Ideas are appreciated,
Glenn



From sinkyh at oregonstate.edu  Tue Nov 29 21:17:16 2016
From: sinkyh at oregonstate.edu (Hassan Sinky)
Date: Tue, 29 Nov 2016 12:17:16 -0800
Subject: [R] Dendrogram branches and igraph edges
Message-ID: <CAO1+r1PKPVLhDg-=FgZqOEqk4UPaSELO=vf1uAxQ+u8BZPA9iQ@mail.gmail.com>

Hello everyone,

I have generated a dendrogram by applying a hierarchical clustering
technique to a graph. Given this dendrogram I am trying to efficiently
find/ map/ label the dendrogram branches to their corresponding graph
edges. Using dendextend I am able to partition the leaves, obtain
subgraphs, perform depth first searches, etc. However, the issue is finding
which graph edge (endpoints) is represented by which dendrogram branch.

Any help is greatly appreciated.

Thanks!

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Nov 30 01:09:47 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 29 Nov 2016 16:09:47 -0800
Subject: [R] Unexpected interference between dplyr and plm
In-Reply-To: <CABdHhvFYGmNcZzWXrs=u4hWwYd3=uGT-bpRiiJhmqnvzOn9kzQ@mail.gmail.com>
References: <d5fc847a-005a-468c-4152-ae58643eb332@hhu.de>
	<CAM_vjumwQrxP8N4S-hcLBPy0N9_DPuyP-_qJh1KwFBjqyZtgqw@mail.gmail.com>
	<7DF89AF6-9338-4AE0-9EC9-FF24D7D7B18D@comcast.net>
	<CAF8bMcZXhAutpOUHcAD=xaLY-92ceBR9UryFQE0P6kAFk+27Yw@mail.gmail.com>
	<CABdHhvFYGmNcZzWXrs=u4hWwYd3=uGT-bpRiiJhmqnvzOn9kzQ@mail.gmail.com>
Message-ID: <7A9F7387-C24C-4044-BB26-AD667EDC4216@comcast.net>


> On Nov 29, 2016, at 11:26 AM, Hadley Wickham <hadley at rstudio.com> wrote:
> 
> On Tue, Nov 29, 2016 at 11:52 AM, William Dunlap <wdunlap at tibco.com> wrote:
>>> The other option would be to load dplyr first (which would give the waring
>>> that >stats::lag was masked) and then later load plm (which should give a
>>> further >warning that dplyr::lag is masked). Then the plm::lag function will
>>> be found
>>> first.
>> 
>> Another option is to write the package maintainers and complain
>> that masking core functions is painful for users.
> 
> Don't worry; many people have done that.

Is it possible that the maintainer could add an explicit importation of  the lag function from pkg:stats?


> 
> Hadley
> 
> -- 
> http://hadley.nz

David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Wed Nov 30 01:19:38 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 29 Nov 2016 16:19:38 -0800
Subject: [R] Unexpected interference between dplyr and plm
In-Reply-To: <7A9F7387-C24C-4044-BB26-AD667EDC4216@comcast.net>
References: <d5fc847a-005a-468c-4152-ae58643eb332@hhu.de>
	<CAM_vjumwQrxP8N4S-hcLBPy0N9_DPuyP-_qJh1KwFBjqyZtgqw@mail.gmail.com>
	<7DF89AF6-9338-4AE0-9EC9-FF24D7D7B18D@comcast.net>
	<CAF8bMcZXhAutpOUHcAD=xaLY-92ceBR9UryFQE0P6kAFk+27Yw@mail.gmail.com>
	<CABdHhvFYGmNcZzWXrs=u4hWwYd3=uGT-bpRiiJhmqnvzOn9kzQ@mail.gmail.com>
	<7A9F7387-C24C-4044-BB26-AD667EDC4216@comcast.net>
Message-ID: <2D35B762-5BA1-42FD-8F1A-A20AEE5122B6@comcast.net>


> On Nov 29, 2016, at 4:09 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
>> On Nov 29, 2016, at 11:26 AM, Hadley Wickham <hadley at rstudio.com> wrote:
>> 
>> On Tue, Nov 29, 2016 at 11:52 AM, William Dunlap <wdunlap at tibco.com> wrote:
>>>> The other option would be to load dplyr first (which would give the waring
>>>> that >stats::lag was masked) and then later load plm (which should give a
>>>> further >warning that dplyr::lag is masked). Then the plm::lag function will
>>>> be found
>>>> first.
>>> 
>>> Another option is to write the package maintainers and complain
>>> that masking core functions is painful for users.
>> 
>> Don't worry; many people have done that.
> 
> Is it possible that the maintainer could add an explicit importation of  the lag function from pkg:stats?

Sorry. Meant to ask explicitly whether the plm maintainer could do that for plm users. Seems clear that dplyr is holding its ground.
> 
> 
>> 
>> Hadley
>> 
>> -- 
>> http://hadley.nz
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jdnewmil at dcn.davis.ca.us  Wed Nov 30 01:56:12 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 29 Nov 2016 16:56:12 -0800
Subject: [R] using grepl in dplyr
In-Reply-To: <00db84c8-b19b-4347-831a-6af567e5784e@me.com>
References: <00db84c8-b19b-4347-831a-6af567e5784e@me.com>
Message-ID: <3B6B7406-4D92-4566-8358-81D52C0B9A01@dcn.davis.ca.us>

That is not a very selective regex.

Actually, a long "or" probably is best, but you don't have to type it in directly. 

prefixes <- c( "AD", "FN" )
pat <- paste0( "^(", paste( prefixes, collapse="|" ), ")[0-9]{4}$" )
grepl( pat, Identifier )

-- 
Sent from my phone. Please excuse my brevity.

On November 29, 2016 10:37:29 AM PST, Glenn Schultz <glennmschultz at me.com> wrote:
>Hello All,
>
>I have a dataframe of about 1.5 million rows from this dataframe I need
>to filter out identifiers. ?An example would be 070000-07099,
>AD0000-AD0999, and AL0000-AL9999, FN0000-FN9999. ?I am using grepl to
>identify those of interest as follows:
>
>?grepl("^[FN]|[AD]{2}", Identifier)
>
>The above seems to work in the case of FN and AD. ?However, there are
>20 such identifiers and there must be a better way to do this than a
>long "or" statement. ?Ultimately, I would like to filter these out
>using dplyr which I think the first step is to create a vector of
>TRUE/FALSE then filter on TRUE
>
>Any Ideas are appreciated,
>Glenn
>
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From valkremk at gmail.com  Wed Nov 30 03:11:10 2016
From: valkremk at gmail.com (Val)
Date: Tue, 29 Nov 2016 20:11:10 -0600
Subject: [R] files
Message-ID: <CAJOiR6ZsmMW9ONE-WdvarH7w7M2BUmBpeHk=QftKW8RtKt8qaw@mail.gmail.com>

Hi all,

In one folder  I have several files  and  I want
combine/concatenate(rbind) based on some condition .
Here is  the sample of the files in one folder
   test.csv
   test123.csv
   test456.csv
   Adat.csv
   Adat123.csv
   Adat456.csv

I want to create 2  files as follows

test_all  = rbind(test.csv, test123.csv,test456.csv)
Adat_al l= rbind(Adat.csv, Adat123.csv,Adat456.csv)

The actual number of  of files are many and  is there an efficient way
of doing it?

Thank you


From sarah.goslee at gmail.com  Wed Nov 30 03:28:52 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 29 Nov 2016 21:28:52 -0500
Subject: [R] files
In-Reply-To: <CAJOiR6ZsmMW9ONE-WdvarH7w7M2BUmBpeHk=QftKW8RtKt8qaw@mail.gmail.com>
References: <CAJOiR6ZsmMW9ONE-WdvarH7w7M2BUmBpeHk=QftKW8RtKt8qaw@mail.gmail.com>
Message-ID: <CAM_vjum1g-oa644eouSVN3PAvtMFBEDnbpsdu7T3q6q+iQP7gg@mail.gmail.com>

Something like this:

filelist <- list.files(pattern="^test")
myfiles <- lapply(filelist, read.csv)
myfiles <- do.call(rbind, myfiles)



On Tue, Nov 29, 2016 at 9:11 PM, Val <valkremk at gmail.com> wrote:
> Hi all,
>
> In one folder  I have several files  and  I want
> combine/concatenate(rbind) based on some condition .
> Here is  the sample of the files in one folder
>    test.csv
>    test123.csv
>    test456.csv
>    Adat.csv
>    Adat123.csv
>    Adat456.csv
>
> I want to create 2  files as follows
>
> test_all  = rbind(test.csv, test123.csv,test456.csv)
> Adat_al l= rbind(Adat.csv, Adat123.csv,Adat456.csv)
>
> The actual number of  of files are many and  is there an efficient way
> of doing it?
>
> Thank you
>


-- 
Sarah Goslee
http://www.functionaldiversity.org


From valkremk at gmail.com  Wed Nov 30 03:41:51 2016
From: valkremk at gmail.com (Val)
Date: Tue, 29 Nov 2016 20:41:51 -0600
Subject: [R] files
In-Reply-To: <CAM_vjum1g-oa644eouSVN3PAvtMFBEDnbpsdu7T3q6q+iQP7gg@mail.gmail.com>
References: <CAJOiR6ZsmMW9ONE-WdvarH7w7M2BUmBpeHk=QftKW8RtKt8qaw@mail.gmail.com>
	<CAM_vjum1g-oa644eouSVN3PAvtMFBEDnbpsdu7T3q6q+iQP7gg@mail.gmail.com>
Message-ID: <CAJOiR6ZvwHc2JcyguTUzdFj7th0xpsLgacJTKfr0Pzt-m+k7cg@mail.gmail.com>

Thank you Sarah,

Some of the files are not csv but  some are *txt with  space delimited.
   Bdat.txt
   Bdat123.txt
   Bdat456.txt
How do I do that?



On Tue, Nov 29, 2016 at 8:28 PM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
> Something like this:
>
> filelist <- list.files(pattern="^test")
> myfiles <- lapply(filelist, read.csv)
> myfiles <- do.call(rbind, myfiles)
>
>
>
> On Tue, Nov 29, 2016 at 9:11 PM, Val <valkremk at gmail.com> wrote:
>> Hi all,
>>
>> In one folder  I have several files  and  I want
>> combine/concatenate(rbind) based on some condition .
>> Here is  the sample of the files in one folder
>>    test.csv
>>    test123.csv
>>    test456.csv
>>    Adat.csv
>>    Adat123.csv
>>    Adat456.csv
>>
>> I want to create 2  files as follows
>>
>> test_all  = rbind(test.csv, test123.csv,test456.csv)
>> Adat_al l= rbind(Adat.csv, Adat123.csv,Adat456.csv)
>>
>> The actual number of  of files are many and  is there an efficient way
>> of doing it?
>>
>> Thank you
>>
>
>
> --
> Sarah Goslee
> http://www.functionaldiversity.org


From jdnewmil at dcn.davis.ca.us  Wed Nov 30 04:32:45 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 29 Nov 2016 19:32:45 -0800 (PST)
Subject: [R] files
In-Reply-To: <CAJOiR6ZvwHc2JcyguTUzdFj7th0xpsLgacJTKfr0Pzt-m+k7cg@mail.gmail.com>
References: <CAJOiR6ZsmMW9ONE-WdvarH7w7M2BUmBpeHk=QftKW8RtKt8qaw@mail.gmail.com>
	<CAM_vjum1g-oa644eouSVN3PAvtMFBEDnbpsdu7T3q6q+iQP7gg@mail.gmail.com>
	<CAJOiR6ZvwHc2JcyguTUzdFj7th0xpsLgacJTKfr0Pzt-m+k7cg@mail.gmail.com>
Message-ID: <alpine.BSF.2.00.1611291926020.45862@pedal.dcn.davis.ca.us>

Presumably using read.table instead of read.csv.

Get the import working with one sample file first, then do whatever you 
had to do with one file over and over.

You still need to go read up on regex patterns... to get you started the 
pattern for matching the csv files would be something like 
"Bdat[0-9]*\\.csv$", and the pattern for matching txt files would be 
"Bdat[0-9]*\\.txt$", but understanding regular expressions is a topic in 
its own right, separate from R. Google is your friend.

On Tue, 29 Nov 2016, Val wrote:

> Thank you Sarah,
>
> Some of the files are not csv but  some are *txt with  space delimited.
>   Bdat.txt
>   Bdat123.txt
>   Bdat456.txt
> How do I do that?
>
>
>
> On Tue, Nov 29, 2016 at 8:28 PM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
>> Something like this:
>>
>> filelist <- list.files(pattern="^test")
>> myfiles <- lapply(filelist, read.csv)
>> myfiles <- do.call(rbind, myfiles)
>>
>>
>>
>> On Tue, Nov 29, 2016 at 9:11 PM, Val <valkremk at gmail.com> wrote:
>>> Hi all,
>>>
>>> In one folder  I have several files  and  I want
>>> combine/concatenate(rbind) based on some condition .
>>> Here is  the sample of the files in one folder
>>>    test.csv
>>>    test123.csv
>>>    test456.csv
>>>    Adat.csv
>>>    Adat123.csv
>>>    Adat456.csv
>>>
>>> I want to create 2  files as follows
>>>
>>> test_all  = rbind(test.csv, test123.csv,test456.csv)
>>> Adat_al l= rbind(Adat.csv, Adat123.csv,Adat456.csv)
>>>
>>> The actual number of  of files are many and  is there an efficient way
>>> of doing it?
>>>
>>> Thank you
>>>
>>
>>
>> --
>> Sarah Goslee
>> http://www.functionaldiversity.org
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From cbenjami at BTBOCES.ORG  Wed Nov 30 06:45:01 2016
From: cbenjami at BTBOCES.ORG (Courtney Benjamin)
Date: Wed, 30 Nov 2016 05:45:01 +0000
Subject: [R] Svyolr-Properly Specifying Start Values
Message-ID: <1480484688763.5932@BTBOCES.ORG>

Hello R Users,

I am trying to use the svyolr command and coming up with the following error:

Error in MASS::polr(formula, data = df, ..., Hess = TRUE, model = FALSE,  :
  attempt to find suitable starting values failed
>From what I have read online, a possible solution is to specify a value in the start argument of svyolr; unfortunately, I have not been able to find any detailed/clear descriptions about how to go about specifying values for the start argument.  Any help in explaining how to go about establishing reasonable start values would be greatly appreciated.

library (survey)
library(RCurl)

data <- getURL("https://raw.githubusercontent.com/cbenjamin1821/careertech-ed/master/elsq1adj2.csv")
elsq1ch <- read.csv(text = data)


#Specifying the svyrepdesign object which applies the BRR weights
elsq1ch_brr<-svrepdesign(variables = elsq1ch[,1:16], repweights = elsq1ch[,18:217], weights = elsq1ch[,17], combined.weights = TRUE, type = "BRR")
elsq1ch_brr

allCColr <- svyolr(F3ATTAINMENT~F1PARED+BYINCOME+F1RACE+F1SEX+F1RGPP2+F1HIMATH+F1RTRCC,design=subset(elsq1ch_brr,BYSCTRL==1&G10COHRT==1),na.action=na.omit)

?





Courtney Benjamin

Broome-Tioga BOCES

Automotive Technology II Teacher

Located at Gault Toyota

Doctoral Candidate-Educational Theory & Practice

State University of New York at Binghamton

cbenjami at btboces.org<mailto:cbenjami at btboces.org>

607-763-8633

	[[alternative HTML version deleted]]


From ferri.leberl at gmx.at  Wed Nov 30 16:18:19 2016
From: ferri.leberl at gmx.at (Ferri Leberl)
Date: Wed, 30 Nov 2016 16:18:19 +0100
Subject: [R] Breaking down a list into a table
In-Reply-To: <CAKVAULNt9Hm0C2DS0xOqUmjuQwF1a5fqSU_x0RUhzceYTvXMTg@mail.gmail.com>
References: <trinity-6038d4d5-154d-4744-babb-418f4b22bf41-1479822395784@3capp-gmx-bs79>,
	<CAKVAULNt9Hm0C2DS0xOqUmjuQwF1a5fqSU_x0RUhzceYTvXMTg@mail.gmail.com>
Message-ID: <trinity-f61a49ed-793e-4da3-9b4f-94e0d55830e3-1480519099878@3capp-gmx-bs36>


Thank to Ulrik for the hint.
However, I don't comprehend the function until now:

For example I made up an array "input":

input
     kop      
[1,] "w;d;e;f"
[2,] "w;d;e;f"
[3,] "w;d;e;f"
[4,] "w;d;e;f"
[5,] "w;d;e;f"

and tried to break it into four cols with commmand:

output<-separate(into,kop,into=c("a","b","c","d"),sep=";")

R returned:

Fehler in UseMethod("separate_") : 
  nicht anwendbare Methode f?r 'separate_' auf Objekt der Klasse "c('matrix', 'character')" angewendet

Could you please explain me my mistake?

Thank you in advance!

Yours, Ferri





?

Gesendet:?Dienstag, 22. November 2016 um 14:57 Uhr
Von:?"Ulrik Stervbo" <ulrik.stervbo at gmail.com>
An:?"Ferri Leberl" <ferri.leberl at gmx.at>, "r-helpr-project.org" <r-help at r-project.org>
Betreff:?Re: [R] Breaking down a list into a table

Hi Ferri,
?It sounds like the function 'separate' from the tidyr package is what you look for,
?
HTH
Ulrik?

On Tue, 22 Nov 2016 at 14:49 Ferri Leberl <ferri.leberl at gmx.at[mailto:ferri.leberl at gmx.at]> wrote:

Dear All,
I asked for support to deal with a hirarchy within a character separated list.
I solved the problem crudely but effectively by

- Choosing for a TSV as input, where in columns that may contain several (or as well no) items the items are separated via semicolon
- adding semicolons to the first row to grant that the first row has the maximum number of semicolons of this column
- grasping the column(x<-myarray[,y], where y is some integer value) and saving it as a TSV (with only one column)
- importing it again, defining it semicolumn-separated, with fill option

To all those who feel pain reading this: Is there a shortcut?
Thank you in advance.
Yours, Ferri

______________________________________________
R-help at r-project.org[mailto:R-help at r-project.org] mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help[https://stat.ethz.ch/mailman/listinfo/r-help]
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html[http://www.R-project.org/posting-guide.html]
and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Wed Nov 30 17:08:08 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 30 Nov 2016 08:08:08 -0800
Subject: [R] Breaking down a list into a table
In-Reply-To: <trinity-f61a49ed-793e-4da3-9b4f-94e0d55830e3-1480519099878@3capp-gmx-bs36>
References: <trinity-6038d4d5-154d-4744-babb-418f4b22bf41-1479822395784@3capp-gmx-bs79>
	<CAKVAULNt9Hm0C2DS0xOqUmjuQwF1a5fqSU_x0RUhzceYTvXMTg@mail.gmail.com>
	<trinity-f61a49ed-793e-4da3-9b4f-94e0d55830e3-1480519099878@3capp-gmx-bs36>
Message-ID: <CAGxFJbRxzVsOzt7M0rbK-9Sams+MM2HE14uhsx5sy4ijvfTYqQ@mail.gmail.com>

Well, if i understand correctly, there's really no need to use special
packages for such simple tasks. The strsplit() function in base R does
nicely (see ?strplit):

Here is a reproducible example, --which you did not provide; please do
so in future:

> testdat <- c("abc;def;hij","qq;sdff;kuqw","oprrt;lbas;rw")

> testdat
[1] "abc;def;hij"   "qq;sdff;kuqw"  "oprrt;lbas;rw"

## note that this is a vector, not an array/data frame with 1 column.
You can subscript it in the call below -- e.g. testdat[,1] -- if it is
an array.

> out <- strsplit(testdat,split= ";")
> out
[[1]]
[1] "abc" "def" "hij"

[[2]]
[1] "qq"   "sdff" "kuqw"

[[3]]
[1] "oprrt" "lbas"  "rw"

## Note that the output of strsplit() is a list because it is designed
to handle the more general case of unequal numbers of pieces in each
row. To reform it to a matrix (i.e. 2d array) use rbind in a do.call
(?do.call) construct:


> do.call(rbind,out)
     [,1]    [,2]   [,3]
[1,] "abc"   "def"  "hij"
[2,] "qq"    "sdff" "kuqw"
[3,] "oprrt" "lbas" "rw"

All of this presupposes familiarity with basic R data structures and
constructs. If you do not have such familiarity yet, then I think you
would do well to go through some R tutorials to gain it (if you wish
to continue to use R). Packages are nice, with many helpful features,
but you still need to know the basics of the language -- and R *is* a
language.

As far as your error message is concerned, separate() is apparently an
S3 generic function (more reading!) but has no method for matrices. If
you had done the call as:

output<-separate(into,input[,1] ,into=c("a","b","c","d"),sep=";")

## giving the data as a vector rather than an array of 1 column
it probably would have worked (but you need to check, as I don't use
the tidyr package).

Cheers,

-- Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Nov 30, 2016 at 7:18 AM, Ferri Leberl <ferri.leberl at gmx.at> wrote:
>
> Thank to Ulrik for the hint.
> However, I don't comprehend the function until now:
>
> For example I made up an array "input":
>
> input
>      kop
> [1,] "w;d;e;f"
> [2,] "w;d;e;f"
> [3,] "w;d;e;f"
> [4,] "w;d;e;f"
> [5,] "w;d;e;f"
>
> and tried to break it into four cols with commmand:
>
> output<-separate(into,kop,into=c("a","b","c","d"),sep=";")
>
> R returned:
>
> Fehler in UseMethod("separate_") :
>   nicht anwendbare Methode f?r 'separate_' auf Objekt der Klasse "c('matrix', 'character')" angewendet
>
> Could you please explain me my mistake?
>
> Thank you in advance!
>
> Yours, Ferri
>
>
>
>
>
>
>
> Gesendet: Dienstag, 22. November 2016 um 14:57 Uhr
> Von: "Ulrik Stervbo" <ulrik.stervbo at gmail.com>
> An: "Ferri Leberl" <ferri.leberl at gmx.at>, "r-helpr-project.org" <r-help at r-project.org>
> Betreff: Re: [R] Breaking down a list into a table
>
> Hi Ferri,
>  It sounds like the function 'separate' from the tidyr package is what you look for,
>
> HTH
> Ulrik
>
> On Tue, 22 Nov 2016 at 14:49 Ferri Leberl <ferri.leberl at gmx.at[mailto:ferri.leberl at gmx.at]> wrote:
>
> Dear All,
> I asked for support to deal with a hirarchy within a character separated list.
> I solved the problem crudely but effectively by
>
> - Choosing for a TSV as input, where in columns that may contain several (or as well no) items the items are separated via semicolon
> - adding semicolons to the first row to grant that the first row has the maximum number of semicolons of this column
> - grasping the column(x<-myarray[,y], where y is some integer value) and saving it as a TSV (with only one column)
> - importing it again, defining it semicolumn-separated, with fill option
>
> To all those who feel pain reading this: Is there a shortcut?
> Thank you in advance.
> Yours, Ferri
>
> ______________________________________________
> R-help at r-project.org[mailto:R-help at r-project.org] mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help[https://stat.ethz.ch/mailman/listinfo/r-help]
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html[http://www.R-project.org/posting-guide.html]
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Wed Nov 30 17:18:25 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 30 Nov 2016 08:18:25 -0800
Subject: [R] Breaking down a list into a table
In-Reply-To: <trinity-f61a49ed-793e-4da3-9b4f-94e0d55830e3-1480519099878@3capp-gmx-bs36>
References: <trinity-6038d4d5-154d-4744-babb-418f4b22bf41-1479822395784@3capp-gmx-bs79>,
	<CAKVAULNt9Hm0C2DS0xOqUmjuQwF1a5fqSU_x0RUhzceYTvXMTg@mail.gmail.com>
	<trinity-f61a49ed-793e-4da3-9b4f-94e0d55830e3-1480519099878@3capp-gmx-bs36>
Message-ID: <D9CA2FB5-FFDD-46E8-8821-EDE695AC217E@dcn.davis.ca.us>

The tidyr::separate function modifies data frames.  You did not give it a data frame. Re-read your preferred R introductory material on what a data frame is (R comes with "Introduction to R").

You also did not give a reproducible example... e.g. does not execute as-is in a clean R work space. 

library(tidyr)
input <- data.frame( kop=rep( "w;d;e;f", 5 ) )
output <- separate( input, kop, into=c( "C1", "C2", "C3", "C4" ), sep=";" )

You really should read the documentation for any contributed package you use. (This presumes that you keep track of the fact that you are using them... which is a good idea as there are thousands of them). I recommend

vignette("tidy-data")

and

?separate

in this case. 
-- 
Sent from my phone. Please excuse my brevity.

On November 30, 2016 7:18:19 AM PST, Ferri Leberl <ferri.leberl at gmx.at> wrote:
>
>Thank to Ulrik for the hint.
>However, I don't comprehend the function until now:
>
>For example I made up an array "input":
>
>input
>     kop      
>[1,] "w;d;e;f"
>[2,] "w;d;e;f"
>[3,] "w;d;e;f"
>[4,] "w;d;e;f"
>[5,] "w;d;e;f"
>
>and tried to break it into four cols with commmand:
>
>output<-separate(into,kop,into=c("a","b","c","d"),sep=";")
>
>R returned:
>
>Fehler in UseMethod("separate_") : 
>nicht anwendbare Methode f?r 'separate_' auf Objekt der Klasse
>"c('matrix', 'character')" angewendet
>
>Could you please explain me my mistake?
>
>Thank you in advance!
>
>Yours, Ferri
>
>
>
>
>
>?
>
>Gesendet:?Dienstag, 22. November 2016 um 14:57 Uhr
>Von:?"Ulrik Stervbo" <ulrik.stervbo at gmail.com>
>An:?"Ferri Leberl" <ferri.leberl at gmx.at>, "r-helpr-project.org"
><r-help at r-project.org>
>Betreff:?Re: [R] Breaking down a list into a table
>
>Hi Ferri,
>?It sounds like the function 'separate' from the tidyr package is what
>you look for,
>?
>HTH
>Ulrik?
>
>On Tue, 22 Nov 2016 at 14:49 Ferri Leberl
><ferri.leberl at gmx.at[mailto:ferri.leberl at gmx.at]> wrote:
>
>Dear All,
>I asked for support to deal with a hirarchy within a character
>separated list.
>I solved the problem crudely but effectively by
>
>- Choosing for a TSV as input, where in columns that may contain
>several (or as well no) items the items are separated via semicolon
>- adding semicolons to the first row to grant that the first row has
>the maximum number of semicolons of this column
>- grasping the column(x<-myarray[,y], where y is some integer value)
>and saving it as a TSV (with only one column)
>- importing it again, defining it semicolumn-separated, with fill
>option
>
>To all those who feel pain reading this: Is there a shortcut?
>Thank you in advance.
>Yours, Ferri
>
>______________________________________________
>R-help at r-project.org[mailto:R-help at r-project.org] mailing list -- To
>UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help[https://stat.ethz.ch/mailman/listinfo/r-help]
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html[http://www.R-project.org/posting-guide.html]
>and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From ranjanagirish30 at gmail.com  Wed Nov 30 18:18:27 2016
From: ranjanagirish30 at gmail.com (Ranjana Girish)
Date: Wed, 30 Nov 2016 22:48:27 +0530
Subject: [R] Improve performance of RConnection and Rserve
In-Reply-To: <CAF5P65=+rbJZSF8r=4RjKK-erM4OU9sOW9Q0QzzzFftPgair=w@mail.gmail.com>
References: <CAF5P65=+rbJZSF8r=4RjKK-erM4OU9sOW9Q0QzzzFftPgair=w@mail.gmail.com>
Message-ID: <CAF5P65mJw2RqOVDChaaewfrn2JUPqDkHvE5AFPTw4m2Fj0RRuw@mail.gmail.com>

Hi all,

If we execute an R script directly in R prompt in Linux environment, it
takes around 30 mins to complete.

The same R script if we execute from java using RConnection, eval and
Rserve for each line it takes thrice the time.

Basically the performance is very bad. And in most of the occasion it never
ends up in result.

Can anyone please tell how to improve the performance using Rconnection.

Thanks in advance

Ranjana

	[[alternative HTML version deleted]]


From steinert.janina at gmail.com  Wed Nov 30 10:55:18 2016
From: steinert.janina at gmail.com (Janina Steinert)
Date: Wed, 30 Nov 2016 09:55:18 +0000
Subject: [R] Meta-Analysis using RVE (robumeta)
Message-ID: <CAFR6YZ71V1HisHhuw7yA_-8kwA+GGm-4iBOxtim_5fbmnsTmFw@mail.gmail.com>

Hello,

I'm runnig a meta-analysis, correcting for clustering of standard errors
with the robumetacommand. R gives the following warning:

If df < 4, do not trust the results

In some cases my df is in fact <4. What does this tell me then? Does it
simply mean that the number of included studies is likely too small (often
aroung 5-6, but with more than 10 single effect sizes)?

Is there anything else I could do for these cases?
Thank you

	[[alternative HTML version deleted]]


From philipgladwin at btinternet.com  Wed Nov 30 17:59:46 2016
From: philipgladwin at btinternet.com (PHILIP GLADWIN)
Date: Wed, 30 Nov 2016 16:59:46 +0000 (UTC)
Subject: [R] Question regarding Naive Bayes
References: <1645764271.6264773.1480525186074.ref@mail.yahoo.com>
Message-ID: <1645764271.6264773.1480525186074@mail.yahoo.com>

Hello,


?
I am working with the na?ve bayes function inlibrary(e1071).


?
The function calls are:

transactions.train.nb = naiveBayes(as.factor(DealerID) ~

???????????????????????????????????as.factor(Manufacturer) 

??????????????????????????????????? + as.factor(RangeDesc)

??????????????????????????????????? +as.factor(BodyType)? 

??????????????????????????????????? +as.factor(FuelType) 

??????????????????????????????????? +as.factor(PaintColour)

??????????????????????????????????? +as.factor(TransmissionType) 

??????????????????????????????????? +as.factor(Mileage)

??????????????????????????????????? +as.factor(Registration),

?????????????????????????????????????data=transactions.train, 

?????????????????????????????????????na.action=na.omit)


?
where transactions.train is a dataframe with dimension 2032rows by 14 columns.


?
and


?
transactions.test.nb = predict(transactions.train.nb,transactions.test[,-1], type='raw')


?
An example of the result are

View(transactions.test.nb)


?
Reduced results shown:

??????????????? 188???????? ????????????225???????? ????????????????229???????? ????????????????270???????? ????????????273

??????????????? ??????????????? ??????????????? ??????????????? ??????????????? ?

1????????????? 0.000984????????????? 0.000492????????????? 0.000492????????????? 0.000492????????????? 0.001476

2????????????? 0.000984????????????? 0.000492????????????? 0.000492????????????? 0.000492????????????? 0.001476

3????????????? 0.000984????????????? 0.000492????????????? 0.000492????????????? 0.000492????????????? 0.001476

4????????????? 0.000984????????????? 0.000492????????????? 0.000492????????????? 0.000492????????????? 0.001476

5????????????? 0.000984????????????? 0.000492????????????? 0.000492????????????? 0.000492????????????? 0.001476


?
I was struggling to understand why the returnedprobabilities are the same for each column as I was hoping for them to bedifferent.

Dealer ID should have a different probability to row 1 than row 2.Each row does sum to 1.


?
Transactions.train represents 67% of the full set of data.

I?ve tried introducing laplace smoothing, and experimentedwith increasing and decreasing the number of parameters used to generate thetraining naivebayes object

But as of yet I can?t figure it out.? Could anybody help?


?
Kind regards,

Phil,


	[[alternative HTML version deleted]]


From dimitri.liakhovitski at gmail.com  Wed Nov 30 21:51:38 2016
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Wed, 30 Nov 2016 15:51:38 -0500
Subject: [R] stringi behaves differently in 2 similar situations
Message-ID: <CAN2xGJZcrP0i75tWuAMcG2FvScnH2E+JF-6OPp_s_30UK6Oo_A@mail.gmail.com>

Hello!

library(stringi)

stri_extract_all_words("me.com", simplify = TRUE)         # returns with a dot
stri_extract_all_words("watch32.com", simplify = TRUE)  # removes the dot

Why is the dot removed only in the second case?
How is it possible to ask it NOT to remove the dot in the second case?

Thanks a lot!


-- 
Dimitri Liakhovitski


From sarah.goslee at gmail.com  Wed Nov 30 22:27:30 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 30 Nov 2016 16:27:30 -0500
Subject: [R] stringi behaves differently in 2 similar situations
In-Reply-To: <CAN2xGJZcrP0i75tWuAMcG2FvScnH2E+JF-6OPp_s_30UK6Oo_A@mail.gmail.com>
References: <CAN2xGJZcrP0i75tWuAMcG2FvScnH2E+JF-6OPp_s_30UK6Oo_A@mail.gmail.com>
Message-ID: <CAM_vjunE0A_BLV_FTZJRL2g+YbzS4YUoyt3Hq=wLFxfDXMoiSA@mail.gmail.com>

A dot is treated differently if it has a number on no, one, or both sides.

> stri_extract_all_words("me.com", simplify = TRUE)
     [,1]
[1,] "me.com"
> stri_extract_all_words("me1.com", simplify = TRUE)
     [,1]  [,2]
[1,] "me1" "com"
> stri_extract_all_words("me1.2com", simplify = TRUE)
     [,1]
[1,] "me1.2com"

?stri_extract_all_words

sent me to

?"stringi-search-boundaries"

which suggests that you should spend some time with the user guide:

     _Boundary Analysis_ - ICU User Guide, <URL:
     http://userguide.icu-project.org/boundaryanalysis>


Depending on your objective, you might be better off with strsplit()
separating on whitespace.

Sarah

On Wed, Nov 30, 2016 at 3:51 PM, Dimitri Liakhovitski
<dimitri.liakhovitski at gmail.com> wrote:
> Hello!
>
> library(stringi)
>
> stri_extract_all_words("me.com", simplify = TRUE)         # returns with a dot
> stri_extract_all_words("watch32.com", simplify = TRUE)  # removes the dot
>
> Why is the dot removed only in the second case?
> How is it possible to ask it NOT to remove the dot in the second case?
>
> Thanks a lot!
>

-- 
Sarah Goslee
http://www.functionaldiversity.org


From anthonyebert at gmail.com  Tue Nov 29 09:17:20 2016
From: anthonyebert at gmail.com (Anthony Ebert)
Date: Tue, 29 Nov 2016 18:17:20 +1000
Subject: [R] [R-pkgs] New package queuecomputer
Message-ID: <CAOR1wai8w9wZjUQJHCD6qpiGz8jats5zoaRvpD6e5nUmuh4+AQ@mail.gmail.com>

Dear R users,

queuecomputer is a new R package now available on CRAN. It is a very
fast method for simulating queueing networks.

The user supplies the arrival and service times and the departure
times are computed deterministically. The name queuecomputer is meant
in the sense that the package 'computes queues'.

The page for the package:

https://CRAN.R-project.org/package=queuecomputer

The github repo is:

https://github.com/AnthonyEbert/queuecomputer

All feedback is welcome anthonyebert+CRAN at gmail.com

Please send any reports of bugs to anthonyebert+CRAN at gmail.com or
create an issue on github.

Kind regards,

Anthony Ebert

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


