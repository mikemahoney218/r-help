From h.wickham at gmail.com  Mon Jan  1 01:33:13 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 31 Dec 2006 16:33:13 -0800
Subject: [R] Does SQL group by have a heavy duty equivalent in R
In-Reply-To: <en99cb$o63$1@sea.gmane.org>
References: <bd93cdad0612302317j674e7e4dk257ac5707964f1@mail.gmail.com>
	<f8e6ff050612310758p11f96c0dl256ac5b15d11dc2c@mail.gmail.com>
	<en99cb$o63$1@sea.gmane.org>
Message-ID: <f8e6ff050612311633l635303dbm9717bc6802727a78@mail.gmail.com>

> I converted the whole data frame to character by using
> as.matrix

You shouldn't need to do that.

> And then using a posting that explained how to get the naming conventions
> back (which had been lost when converting to matrix)
>
> Anything that I did not list with the id's it insisted in including them
> with the measured variables. In other words it would not let me drop.
>
> despite
>
> melted<-melt(BigDF, id=c("SAMPLE_ID","ASSAY_ID"),
> measured=c("GENOTYPE_ID","DESCRIPTION"))

That should be measure=c(...)

Hadley


From fjbuch at gmail.com  Mon Jan  1 01:43:44 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Sun, 31 Dec 2006 19:43:44 -0500
Subject: [R] Does SQL group by have a heavy duty equivalent in R
In-Reply-To: <f8e6ff050612311633l635303dbm9717bc6802727a78@mail.gmail.com>
References: <bd93cdad0612302317j674e7e4dk257ac5707964f1@mail.gmail.com>
	<f8e6ff050612310758p11f96c0dl256ac5b15d11dc2c@mail.gmail.com>
	<en99cb$o63$1@sea.gmane.org>
	<f8e6ff050612311633l635303dbm9717bc6802727a78@mail.gmail.com>
Message-ID: <bd93cdad0612311643v42a302a0kf06a7943af83b395@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20061231/70c12810/attachment.pl 

From fjbuch at gmail.com  Mon Jan  1 02:16:45 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Sun, 31 Dec 2006 20:16:45 -0500
Subject: [R] Subset by using multiple values
Message-ID: <bd93cdad0612311716o19b74d15k7d6174ead6bf5403@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20061231/c6752413/attachment.pl 

From fjbuch at gmail.com  Mon Jan  1 02:36:00 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Sun, 31 Dec 2006 20:36:00 -0500
Subject: [R] Subset by using multiple values
References: <bd93cdad0612311716o19b74d15k7d6174ead6bf5403@mail.gmail.com>
Message-ID: <en9oi0$mf3$1@sea.gmane.org>

I found a solution to my problem. I thought I would post it here. That will 
help me in 3 months when I have forgotten it or some other poor soul who 
stumbles across the same problem.

RawSeqBig<-RawSeqBig[RawSeqBig$ASSAY_ID %in% rejectrs$rs==FALSE,]

"Farrel Buchinsky" <fjbuch at gmail.com> wrote in message 
news:bd93cdad0612311716o19b74d15k7d6174ead6bf5403 at mail.gmail.com...
>I have a vector containg about 20 unique values. It is called rejectrs$rs.
> It is a factor
> I have a data frame with about 100000 rows.
> I want to exclude all rows where in variable rs the value is one of the 20
> on the exclude list. I thought this would work but none did.
>
> RawSeqBig<-subset(RawSeqBig,ASSAY_ID!=rejectrs$rs)
>
> RawSeqBig<-subset(RawSeqBig,ASSAY_ID!=list(rejectrs$rs))
>
>
> -- 
> Farrel Buchinsky
> Mobile: (412) 779-1073
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From cberry at tajo.ucsd.edu  Mon Jan  1 03:02:55 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Sun, 31 Dec 2006 18:02:55 -0800
Subject: [R] Does SQL group by have a heavy duty equivalent in R
In-Reply-To: <bd93cdad0612302317j674e7e4dk257ac5707964f1@mail.gmail.com>
References: <bd93cdad0612302317j674e7e4dk257ac5707964f1@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0612311739430.30473@tajo.ucsd.edu>

On Sun, 31 Dec 2006, Farrel Buchinsky wrote:

> I have hundreds of humans who have undergone SNP genotyping at hundreds of
> loci. Some have even undergone the procedure twice or thrice (kind of an
> internal control).
>
> So obviously I need to find those replications, and confirm that the results
> are the same. If there is discordance then I need to address it.

Why not use  duplicated() ?

For a data.frame with 200 rows of which about 50 are duplicates and 201 
columns finding the (non) duplicates takes little time on my year old AMD 
64 running Windows XP:

> my.dat <- data.frame(ID=rep(1:100, sample(1:3,100,repl=T)))
> snp.dat <- lapply(1:200,function(x) 0:1 )
> snp.frame <- as.data.frame(do.call(cbind,snp.dat))
> my.dat <- cbind( my.dat,snp.frame[sample(nrow(my.dat))%%2+1,])
> system.time( table(duplicated(my.dat)) )
[1] 0.03 0.00 0.03   NA   NA
>

Finding the non-duplicated rows for which there is at least one 
replication:

> system.time( which( (!duplicated(my.dat)) & (my.dat$ID %in% names(which(table(my.dat$ID)>1)) ) ))
[1] 0.05 0.00 0.05   NA   NA


> 
>
> I tried to use the aggregate function
>
> nr.attempts
> <-aggregate(RawSeq$GENOTYPE_ID,list(sample=RawSeq$SAMPLE_ID,assay=RawSeq$ASSAY_ID),length)
> This was simply to figure out how many times the same piece of information
> had been obtained. I ran out of patience. It took beyond forever and tapply
> did not perform much better. The reshape package did not help - it implied
> one was out of luck if the data was not numeric. All of my data is character
> or factor.
>
> Instead I used RODBC
>
> sqlSave(channel,RawSeq)
> to push the table into a Microsoft Access database
> Then a sql query, courtesy of the Microsoft Access Query Wizard a la design
> mode.
>
> SELECT RawSeq.SAMPLE_ID, RawSeq.ASSAY_ID, Min(RawSeq.GENOTYPE_ID) AS
> MinOfGENOTYPE_ID, Max(RawSeq.GENOTYPE_ID) AS MaxOfGENOTYPE_ID, Count(
> RawSeq.rownames) AS CountOfrownames
> FROM RawSeq
> WHERE (((RawSeq.GENOTYPE_ID)<>""))
> GROUP BY RawSeq.SAMPLE_ID, RawSeq.ASSAY_ID
> ORDER BY Count(RawSeq.rownames) DESC;
>
> This way I could easily use the minimum and maximum values to see if they
> were discordant.
> Microsoft Access handled it with aplomb. I plan to use RODBC to bring the
> result of the SQL query back into R.
>
> This is the first time I have seen Microsoft Access outpace R.
> Is my observation correct or am I missing something. I would much rather
> perform all data manipulation and analyses in R.
>
>
>
> -- 
> Farrel Buchinsky
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0717


From richard.rowe at jcu.edu.au  Mon Jan  1 03:35:41 2007
From: richard.rowe at jcu.edu.au (Richard Rowe)
Date: Mon, 01 Jan 2007 12:35:41 +1000
Subject: [R] if ... problem with compound instructions
Message-ID: <6.0.3.0.1.20070101112735.03159df8@mail.jcu.edu.au>

I am having problems with the 'if' syntax.

I have an n x 4 matrix, X say.  The first two columns hold x, y values and 
I am attempting to fill the second two columns with the quadrant in which 
the datapoint (x, y) is and with the heading angle.

So I have two problems
1) how to do this elegantly (at which I've failed as I can't seem to 
vectorize the problem) and
2) how to accomplish the task in a for loop ...

for (i in 1: length(X[,1])) (
  if ((X[i,1] ==0) & (X[i,2] ==0)) (X[i,3] <- NA; X[i,4] <-NA) else (
removing the pathological case ... then a series of nested if statements 
assigning quadrant and calculating heading

e.g.
if ((X[i,1] < 0) & (X[i,2] >= 0)) (X[i,3] <- 4; X[i,4] <- 
atan(X[i,1]/X[i,2]) + 2*pi) else (


In the first instance the ';'  seems to the source of a syntax 
error.  Removing the second elements of the compound statement solves the 
syntax problem and the code runs.

As the R syntax is supposed to be 'Algol-like' I had thought

if <A> then <B> else <C>
should work for compound <B> ... ie that the bracket (X[i,3] <- NA; X[i,4] 
<-NA) should be actioned

1) any elegant solutions to what must be a common task?

2) any explanations for the ';' effect ?

thanks

Richard Rowe

Dr Richard Rowe
Zoology & Tropical Ecology
School of Tropical Biology
James Cook University
Townsville 4811
AUSTRALIA

ph +61 7 47 81 4851
fax +61 7 47 25 1570
JCU has CRICOS Provider Code 00117J


From murdoch at stats.uwo.ca  Mon Jan  1 03:52:07 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 31 Dec 2006 21:52:07 -0500
Subject: [R] if ... problem with compound instructions
In-Reply-To: <6.0.3.0.1.20070101112735.03159df8@mail.jcu.edu.au>
References: <6.0.3.0.1.20070101112735.03159df8@mail.jcu.edu.au>
Message-ID: <45987757.70900@stats.uwo.ca>

On 12/31/2006 9:35 PM, Richard Rowe wrote:
> I am having problems with the 'if' syntax.
> 
> I have an n x 4 matrix, X say.  The first two columns hold x, y values and 
> I am attempting to fill the second two columns with the quadrant in which 
> the datapoint (x, y) is and with the heading angle.
> 
> So I have two problems
> 1) how to do this elegantly (at which I've failed as I can't seem to 
> vectorize the problem) and

You would use the ifelse() function for a vectorized solution.

> 2) how to accomplish the task in a for loop ...
> 
> for (i in 1: length(X[,1])) (
>   if ((X[i,1] ==0) & (X[i,2] ==0)) (X[i,3] <- NA; X[i,4] <-NA) else (
> removing the pathological case ... then a series of nested if statements 
> assigning quadrant and calculating heading
> 
> e.g.
> if ((X[i,1] < 0) & (X[i,2] >= 0)) (X[i,3] <- 4; X[i,4] <- 
> atan(X[i,1]/X[i,2]) + 2*pi) else (

Use {} rather than () around blocks of statements, and &&
rather than & for most logical constructions, e.g.

if ((X[i,1] < 0) && (X[i,2] >= 0)) {
   X[i,3] <- 4; X[i,4] <- atan(X[i,1]/X[i,2]) + 2*pi
} else {

...

Duncan Murdoch

> 
> 
> In the first instance the ';'  seems to the source of a syntax 
> error.  Removing the second elements of the compound statement solves the 
> syntax problem and the code runs.
> 
> As the R syntax is supposed to be 'Algol-like' I had thought
> 
> if <A> then <B> else <C>
> should work for compound <B> ... ie that the bracket (X[i,3] <- NA; X[i,4] 
> <-NA) should be actioned
> 
> 1) any elegant solutions to what must be a common task?
> 
> 2) any explanations for the ';' effect ?
> 
> thanks
> 
> Richard Rowe
> 
> Dr Richard Rowe
> Zoology & Tropical Ecology
> School of Tropical Biology
> James Cook University
> Townsville 4811
> AUSTRALIA
> 
> ph +61 7 47 81 4851
> fax +61 7 47 25 1570
> JCU has CRICOS Provider Code 00117J
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From duru at pangea.Stanford.EDU  Mon Jan  1 04:15:54 2007
From: duru at pangea.Stanford.EDU (Obinna Duru)
Date: Sun, 31 Dec 2006 19:15:54 -0800
Subject: [R] New to R
Message-ID: <6.2.5.6.2.20061231191117.03bd0170@pangea.Stanford.EDU>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20061231/40c16a5c/attachment.pl 

From liuwensui at gmail.com  Mon Jan  1 04:21:04 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Sun, 31 Dec 2006 22:21:04 -0500
Subject: [R] New to R
In-Reply-To: <6.2.5.6.2.20061231191117.03bd0170@pangea.Stanford.EDU>
References: <6.2.5.6.2.20061231191117.03bd0170@pangea.Stanford.EDU>
Message-ID: <1115a2b00612311921p795c8fb0j57faca7f7d8a549a@mail.gmail.com>

what is the format of your data files, txt/csv/mdb/xls? the syntax is
very different.
could you please give more info?

thanks.


On 12/31/06, Obinna Duru <duru at pangea.stanford.edu> wrote:
> Hey, I am very new to R and I need to use it (and the ACEPACK
> package) to do some statistical analysis.
>
> I have installed acepack but efforts to get started has been
> unsuccessful. I can't seem to be able to load my data files because I
> am yet to figure the syntax to use. Is there a work directory in R
> where I can put my files and call them anytime, like in Matlab? My
> files are on my C drive and I just can't figure the syntax to get them into R.
>
> Any help?
>
> ....Best Regards
>
> Obinna Duru
>
> Energy Resources Engineering Department,
> Green Earth Sciences Building,
> 367 Panama Street,
> Stanford, CA 94305-2220
> cell:   (650) 814 6079
> fax:    (659) 725 2099
> email:  duru at stanford.edu
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From cberry at tajo.ucsd.edu  Mon Jan  1 04:46:06 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Sun, 31 Dec 2006 19:46:06 -0800
Subject: [R] Does SQL group by have a heavy duty equivalent in R
In-Reply-To: <Pine.LNX.4.64.0612311739430.30473@tajo.ucsd.edu>
References: <bd93cdad0612302317j674e7e4dk257ac5707964f1@mail.gmail.com>
	<Pine.LNX.4.64.0612311739430.30473@tajo.ucsd.edu>
Message-ID: <Pine.LNX.4.64.0612311942320.31545@tajo.ucsd.edu>

On Sun, 31 Dec 2006, Charles C. Berry wrote:

> On Sun, 31 Dec 2006, Farrel Buchinsky wrote:
>
>>  I have hundreds of humans who have undergone SNP genotyping at hundreds of
>>  loci. Some have even undergone the procedure twice or thrice (kind of an
>>  internal control).
>>
>>  So obviously I need to find those replications, and confirm that the
>>  results
>>  are the same. If there is discordance then I need to address it.
>
> Why not use  duplicated() ?

More specifically:

 	unique( IDs[ duplicated( IDs ) & ! duplicated ( cbind (IDs, SNPs ) ) ] )

gives a list of those IDs for which the SNPs in all replicates of an ID 
are not the same.


>
> For a data.frame with 200 rows of which about 50 are duplicates and 201 
> columns finding the (non) duplicates takes little time on my year old AMD 64 
> running Windows XP:
>
>>  my.dat <- data.frame(ID=rep(1:100, sample(1:3,100,repl=T)))
>>  snp.dat <- lapply(1:200,function(x) 0:1 )
>>  snp.frame <- as.data.frame(do.call(cbind,snp.dat))
>>  my.dat <- cbind( my.dat,snp.frame[sample(nrow(my.dat))%%2+1,])
>>  system.time( table(duplicated(my.dat)) )
> [1] 0.03 0.00 0.03   NA   NA
>> 
>
> Finding the non-duplicated rows for which there is at least one replication:
>
>>  system.time( which( (!duplicated(my.dat)) & (my.dat$ID %in%
>>  names(which(table(my.dat$ID)>1)) ) ))
> [1] 0.05 0.00 0.05   NA   NA
>
>
>> 
>>
>>  I tried to use the aggregate function
>>
>>  nr.attempts
>>  <-aggregate(RawSeq$GENOTYPE_ID,list(sample=RawSeq$SAMPLE_ID,assay=RawSeq$ASSAY_ID),length)
>>  This was simply to figure out how many times the same piece of information
>>  had been obtained. I ran out of patience. It took beyond forever and
>>  tapply
>>  did not perform much better. The reshape package did not help - it implied
>>  one was out of luck if the data was not numeric. All of my data is
>>  character
>>  or factor.
>>
>>  Instead I used RODBC
>>
>>  sqlSave(channel,RawSeq)
>>  to push the table into a Microsoft Access database
>>  Then a sql query, courtesy of the Microsoft Access Query Wizard a la
>>  design
>>  mode.
>>
>>  SELECT RawSeq.SAMPLE_ID, RawSeq.ASSAY_ID, Min(RawSeq.GENOTYPE_ID) AS
>>  MinOfGENOTYPE_ID, Max(RawSeq.GENOTYPE_ID) AS MaxOfGENOTYPE_ID, Count(
>>  RawSeq.rownames) AS CountOfrownames
>>  FROM RawSeq
>>  WHERE (((RawSeq.GENOTYPE_ID)<>""))
>>  GROUP BY RawSeq.SAMPLE_ID, RawSeq.ASSAY_ID
>>  ORDER BY Count(RawSeq.rownames) DESC;
>>
>>  This way I could easily use the minimum and maximum values to see if they
>>  were discordant.
>>  Microsoft Access handled it with aplomb. I plan to use RODBC to bring the
>>  result of the SQL query back into R.
>>
>>  This is the first time I have seen Microsoft Access outpace R.
>>  Is my observation correct or am I missing something. I would much rather
>>  perform all data manipulation and analyses in R.
>> 
>> 
>>
>>  --
>>  Farrel Buchinsky
>>
>>   [[alternative HTML version deleted]]
>>
>>  ______________________________________________
>>  R-help at stat.math.ethz.ch mailing list
>>  https://stat.ethz.ch/mailman/listinfo/r-help
>>  PLEASE do read the posting guide
>>  http://www.R-project.org/posting-guide.html
>>  and provide commented, minimal, self-contained, reproducible code.
>> 
>
> Charles C. Berry                        (858) 534-2098
>                                         Dept of Family/Preventive Medicine
> E mailto:cberry at tajo.ucsd.edu	         UC San Diego
> http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0717
>
>
>
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0717


From Bill.Venables at csiro.au  Mon Jan  1 07:09:24 2007
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Mon, 1 Jan 2007 16:09:24 +1000
Subject: [R] if ... problem with compound instructions
Message-ID: <B998A44C8986644EA8029CFE6396A924840A9A@exqld2-bne.qld.csiro.au>

Step 1:

quadrant <- 1 + (X[, 1] < 0) + 2*(X[, 2] > 0)

This is not the usual labelling of the quadrants as '3' and '4' are interchanged. If you want to be picky about it

quadrant <- ifelse(quadrant > 2, 7 - quadrant, quadrant)

Step 2:

angle <- atan2(X[,2], X[,1]) %% (2*pi)  # I think this is what you want

(why did you want to know the quadrant?)

Oh, then you might do

X[, 3:4] <- cbind(quadrant, angle)

---

Bill Venables
CMIS, CSIRO Laboratories,
PO Box 120, Cleveland, Qld. 4163
AUSTRALIA
Office Phone (email preferred): +61 7 3826 7251
Fax (if absolutely necessary):??? +61 7 3826 7304
Mobile (rarely used):??????????????? +61 4 1963 4642
Home Phone:????????????????????????? +61 7 3286 7700
mailto:Bill.Venables at csiro.au
http://www.cmis.csiro.au/bill.venables/ 
?

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Richard Rowe
Sent: Monday, 1 January 2007 12:36 PM
To: r-help at stat.math.ethz.ch
Subject: [R] if ... problem with compound instructions

I am having problems with the 'if' syntax.

I have an n x 4 matrix, X say.  The first two columns hold x, y values and 
I am attempting to fill the second two columns with the quadrant in which 
the datapoint (x, y) is and with the heading angle.

So I have two problems
1) how to do this elegantly (at which I've failed as I can't seem to 
vectorize the problem) and
2) how to accomplish the task in a for loop ...

for (i in 1: length(X[,1])) (
  if ((X[i,1] ==0) & (X[i,2] ==0)) (X[i,3] <- NA; X[i,4] <-NA) else (
removing the pathological case ... then a series of nested if statements 
assigning quadrant and calculating heading

e.g.
if ((X[i,1] < 0) & (X[i,2] >= 0)) (X[i,3] <- 4; X[i,4] <- 
atan(X[i,1]/X[i,2]) + 2*pi) else (


In the first instance the ';'  seems to the source of a syntax 
error.  Removing the second elements of the compound statement solves the 
syntax problem and the code runs.

As the R syntax is supposed to be 'Algol-like' I had thought

if <A> then <B> else <C>
should work for compound <B> ... ie that the bracket (X[i,3] <- NA; X[i,4] 
<-NA) should be actioned

1) any elegant solutions to what must be a common task?

2) any explanations for the ';' effect ?

thanks

Richard Rowe

Dr Richard Rowe
Zoology & Tropical Ecology
School of Tropical Biology
James Cook University
Townsville 4811
AUSTRALIA

ph +61 7 47 81 4851
fax +61 7 47 25 1570
JCU has CRICOS Provider Code 00117J

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From duru at pangea.Stanford.EDU  Mon Jan  1 10:09:06 2007
From: duru at pangea.Stanford.EDU (Obinna Duru)
Date: Mon, 01 Jan 2007 01:09:06 -0800
Subject: [R] Problems with R
Message-ID: <6.2.5.6.2.20070101010137.046f2b50@pangea.Stanford.EDU>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070101/f56d9312/attachment.pl 

From gregor.gorjanc at bfro.uni-lj.si  Mon Jan  1 11:51:06 2007
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Mon, 1 Jan 2007 10:51:06 +0000 (UTC)
Subject: [R] zero random effect sizes with binomial lmer
References: <Pine.LNX.4.63.0612311744310.3632@babel.ling.upenn.edu>
Message-ID: <loom.20070101T114646-983@post.gmane.org>

Daniel Ezra Johnson <johnson4 <at> babel.ling.upenn.edu> writes:
> 1) Yes, I have tweaked the data to show as clearly as I can that this is a 
> bug, that a tiny change in initial conditions causes the collapse of a 
> reasonable 'parameter' estimate.

I would not call this a bug, since this is related to data and not to the
software. I might be wrong!

> 2) mcmcsamp() does not work (currently) for binomial fitted models.

Sorry, for wrong pointer. You could try with some other packages if they
have support for binomial models with "random" effects. I would just try
in BUGS --> take a look at R2WinBUGS or Brugs.

> 3) This is an issue of what happens when the sample is too small. For all 
> larger data sets I have gotten a ranef variance between 0.05 and 1.00 or 
> so.
> 
> It makes no sense to say that as the data set gets smaller, the systematic 
> variation between Items goes away. It doesn't, as I've shown. In the data 

I believe that when data gets smaller such parameters are harder to estimate
and you can easily get 0 as MLE.

> above, certain Items were still 10+ times as likely (log-odds wise) to 
> have Response==1 as others.

Gregor


From kubovy at virginia.edu  Mon Jan  1 12:48:07 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Mon, 1 Jan 2007 06:48:07 -0500
Subject: [R] New to R
In-Reply-To: <6.2.5.6.2.20061231191117.03bd0170@pangea.Stanford.EDU>
References: <6.2.5.6.2.20061231191117.03bd0170@pangea.Stanford.EDU>
Message-ID: <128FB59C-6D2F-4B30-A11B-794FBEA29C7B@virginia.edu>

On Dec 31, 2006, at 10:15 PM, Obinna Duru wrote:

> I have installed acepack but efforts to get started has been
> unsuccessful. I can't seem to be able to load my data files because I
> am yet to figure the syntax to use. Is there a work directory in R
> where I can put my files and call them anytime, like in Matlab? My
> files are on my C drive and I just can't figure the syntax to get  
> them into R.

Start by looking at
?getwd
?read.table
intall.packages('foreign') # if not installed
?read.spss
?read.ssd
etc.

and then
http://www.bioconductor.org/developers/progRef/fileHandling.pdf

followed by the more general
http://cran.r-project.org/doc/manuals/R-intro.pdf
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From murdoch at stats.uwo.ca  Mon Jan  1 16:07:22 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 01 Jan 2007 10:07:22 -0500
Subject: [R] Problems with R
In-Reply-To: <6.2.5.6.2.20070101010137.046f2b50@pangea.Stanford.EDU>
References: <6.2.5.6.2.20070101010137.046f2b50@pangea.Stanford.EDU>
Message-ID: <459923AA.2060008@stats.uwo.ca>

On 1/1/2007 4:09 AM, Obinna Duru wrote:
> Hello,
> 
> Please I have problems with R. I downloaded R from UC Berkeley CRAN 
> mirror, but each time I try to read a data file using the command 
> 'moi.data <- read.csv(choose.files())' or 'my.data <- choose.files() 
> #' , the program becomes unstable and exits itself.

This must be some local problem with your system; I think you'll have to 
get local help to diagnose it.

As a workaround, you can specify the name of the file rather than using 
choose.files(), but you really should try to find what's messing up your 
system.

Duncan Murdoch


> 
> However, I just noticed from my Windows Task manager that the 
> R-program process tree is still active and using up about 30% of the 
> processor activity.
> 
> Is this a problem with my download? I have tried reinstalling from UC 
> Los Angeles, but still have the same issues.
> 
> Any help will be appreciated. I need to use the acepack package in R.
> 
> Thank you
> 
> 
> ....Best Regards
> 
> Obinna Duru
> 
> Energy Resources Engineering Department,
> Green Earth Sciences Building,
> 367 Panama Street,
> Stanford, CA 94305-2220
> cell:   (650) 814 6079
> fax:    (659) 725 2099
> email:  duru at stanford.edu
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From kubovy at virginia.edu  Mon Jan  1 16:10:58 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Mon, 1 Jan 2007 10:10:58 -0500
Subject: [R] Help with filled.contour()
Message-ID: <758246BF-9625-4F20-948A-CFC120689A83@virginia.edu>

The following plot is a first approximation to what I need:

***********************************
mu1 <- 0
mu2 <- 5
s <- 1
x <- seq(-2.5, 7.5, length = 41)
y <- seq(-2.5, 2.5, length = 41)
f <- function(x,y){
term1 <- 1/(2*pi*sqrt(s*s))
term2 <- -1/2
term3 <- (x - mu1)^2/s
term4 <- (y - mu1)^2/s
term5 <- (x - mu2)^2/s
term1*(.5 * exp(term2*(term3 + term4)) + .5 * exp(term2*(term5 +  
term4)))
}
z <- outer(x, y, f)
persp(x, y, z)
require(grDevices)
filled.contour(x, y, z, axes = F, frame.plot = F, asp = 1,
     col = palette(gray(seq(0, 0.9, len = 25))), nlevels = 25)
***********************************
It has four drawbacks. I would like:
(1) to remove the white grid;
(2) to remove the white contours;
(3) its appearance to be smooth;
(4) to remove the key.
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From hustqiufeng at sohu.com  Mon Jan  1 16:17:30 2007
From: hustqiufeng at sohu.com (Feng Qiu)
Date: Mon, 1 Jan 2007 10:17:30 -0500
Subject: [R] Any container in R?
References: <6.0.3.0.1.20070101112735.03159df8@mail.jcu.edu.au>
Message-ID: <000a01c72db7$f7065180$6401a8c0@Aglog>

R has list and array to contain elements. But does R have more powerful 
container, such as "map" as in C++ STL?  or is there such a package?

Thanks and Happy 2007!

Best,

Feng


From murdoch at stats.uwo.ca  Mon Jan  1 16:37:24 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 01 Jan 2007 10:37:24 -0500
Subject: [R] Any container in R?
In-Reply-To: <000a01c72db7$f7065180$6401a8c0@Aglog>
References: <6.0.3.0.1.20070101112735.03159df8@mail.jcu.edu.au>
	<000a01c72db7$f7065180$6401a8c0@Aglog>
Message-ID: <45992AB4.3070604@stats.uwo.ca>

On 1/1/2007 10:17 AM, Feng Qiu wrote:
> R has list and array to contain elements. But does R have more powerful 
> container, such as "map" as in C++ STL?  or is there such a package?

In what way are maps more powerful than lists?  You can use names to 
index lists.

The other container in R is the environment; they have fairly strange 
semantics, though.

Duncan Murdoch


From e.pebesma at geo.uu.nl  Mon Jan  1 16:47:24 2007
From: e.pebesma at geo.uu.nl (Edzer J. Pebesma)
Date: Mon, 01 Jan 2007 16:47:24 +0100
Subject: [R] [R-sig-Geo] plot methods in sp
In-Reply-To: <45961C45.4040306@univ-fcomte.fr>
References: <45961C45.4040306@univ-fcomte.fr>
Message-ID: <45992D0C.9000105@geo.uu.nl>

Patrick, I can see this is confusing.

Besides methods(plot), try after library(sp) a

showMethods(plot)

to get an overview of the S4-style plot methods; methods(plot) only 
shows the S3-style plot methods. Then,

class?SpatialPolygons

gives methods available for this class, among which plot, and the html 
listing of plot methods has entries like

plot.SpatialPolygons,missing-method

that points to the same help page (which is rather brief).

getMethod("plot", c("SpatialPolygons", "missing"))

shows you the arguments this method takes, and shows that it actually 
calls plot.SpatialPolygons. This method is not exported from sp, so to 
view it you need to access it as

 sp:::plot.SpatialPolygons
 
the other sp functions it calls can also be accessed by preceding them 
with sp:::

I agree that more elaborate documentation along with examples would be 
useful.

Hope this helps,
--
Edzer

Patrick Giraudoux wrote:
> Dear listers,
>
> I am working since a while with the sp package and still wonder how the 
> plot methods are managed with sp spatial objects. For instance, 
> SpatialPolygonsDataFrame objects have obviously a plot method. However 
> it cannot be found in the list provided by methods(plot) . Furthermore 
> ?plot.SpatialPolygonsDataFrame, nor ?plot.SpatialPolygons, etc.. provide 
> a help, though the lattice function spplot is adequately documented.
>
> On the other hand, plot(myobject, border="grey"), with myobject a 
> SpatialPolygonsDataframe is well interpreted and recalls the syntax of 
> plot.polylist of matools (though myobject is far from being a polylist...).
>
> Can anybody (especially the package's authors...) comment on this? Where 
> a help with the list of the plot function arguments can be found?
>
> Thanks for any hint,
>
> Patrick
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


From hustqiufeng at sohu.com  Mon Jan  1 17:22:57 2007
From: hustqiufeng at sohu.com (Feng Qiu)
Date: Mon, 1 Jan 2007 11:22:57 -0500
Subject: [R] Any container in R?
References: <6.0.3.0.1.20070101112735.03159df8@mail.jcu.edu.au>
	<000a01c72db7$f7065180$6401a8c0@Aglog>
	<45992AB4.3070604@stats.uwo.ca>
Message-ID: <003601c72dc1$1b68f5b0$6401a8c0@Aglog>

Hi Duncan:
         Thanks for your hints.
         I'm trying to collect distinct elements in one column in a matrix. 
If there is a map, I can easily build up such a collection. While if using 
list, I have to check by myself if this element already exists in the 
collection every time I examine a new entry in the column.

Best,

Feng

----- Original Message ----- 
From: "Duncan Murdoch" <murdoch at stats.uwo.ca>
To: "Feng Qiu" <hustqiufeng at sohu.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Monday, January 01, 2007 10:37 AM
Subject: Re: [R] Any container in R?


> On 1/1/2007 10:17 AM, Feng Qiu wrote:
>> R has list and array to contain elements. But does R have more powerful 
>> container, such as "map" as in C++ STL?  or is there such a package?
>
> In what way are maps more powerful than lists?  You can use names to index 
> lists.
>
> The other container in R is the environment; they have fairly strange 
> semantics, though.
>
> Duncan Murdoch
>
>


From adrian_d at eskimo.com  Mon Jan  1 17:54:26 2007
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Mon, 1 Jan 2007 08:54:26 -0800 (PST)
Subject: [R] autofill for a tkentry field
Message-ID: <Pine.SUN.4.58.0701010853010.112@eskimo.com>


Hello all,

I'm experimenting with tlk/tk and I find it very useful. I would like to
have the text of a tkentry be automatically filled when the user starts
typing, according to a  predetermined list.  I've found a solution on the
tcl wiki pages, but I don't quite know how to translate it in R... Here is
the tcl code: http://wiki.tcl.tk/13267

And here is my code that does not work.
 autofill <- function(){
    cat("I was here!")
    entry <- tclvalue(f1)
    ind <- grep(entry, fruitList)
    if (length(ind)>0){f1 <- tclVar(fruitList[ind[1]])}
   }

  fruitList <<- c("Apple", "Banana", "Peach", "Pear", "Plum", "Mango")
  tkdestroy(tt)
  tt  <- tktoplevel()
  pw  <- tkframe(tt)
  ppw <- tk2labelframe(pw, text="Fruits:")
  f1  <<- tclVar("")
  entry1.f <- tkentry(ppw, width=30, textvariable=f1, validate="key",
                      validatecommand=autofill)
  tkgrid(entry1.f)
  tkgrid(ppw)
  tkgrid(pw)

I don't know how to pass arguments to the autofill function, so I can
manipulate the textvariable.  Any help is much appreciated.  If we get a
working solution, maybe we should add this example to
http://www.sciviews.org/_rgui/tcltk/index.html.

Thank you,
Adrian


From philip at sctkjeld.dk  Mon Jan  1 14:12:34 2007
From: philip at sctkjeld.dk (Philip)
Date: Mon, 01 Jan 2007 14:12:34 +0100
Subject: [R] arima.sim with a periodic model
Message-ID: <459908C2.80607@sctkjeld.dk>

Hi all.

I have a periodiv arma model and I want to simulate it. In S-plus, the 
following works for me:

phi <- 0.9
theta <- 0
p <- 1 # period
model <- list(ar=phi, ma=theta, period=p)
Yt <- arima.sim(model, n=250)

How do I do something like this "period=12" in R? I read help(arima.sim) 
but it doesn't tell.


Thanks! Philip.


From iteachtyping at gmail.com  Mon Jan  1 03:29:42 2007
From: iteachtyping at gmail.com (Raymond Balise)
Date: Sun, 31 Dec 2006 18:29:42 -0800
Subject: [R] Types of objects?
Message-ID: <3f2154540612311829i5f59637bx1738a4310ec5f189@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20061231/4d1e6cbb/attachment.pl 

From murdoch at stats.uwo.ca  Mon Jan  1 18:36:12 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 01 Jan 2007 12:36:12 -0500
Subject: [R] Any container in R?
In-Reply-To: <003601c72dc1$1b68f5b0$6401a8c0@Aglog>
References: <6.0.3.0.1.20070101112735.03159df8@mail.jcu.edu.au>
	<000a01c72db7$f7065180$6401a8c0@Aglog>
	<45992AB4.3070604@stats.uwo.ca>
	<003601c72dc1$1b68f5b0$6401a8c0@Aglog>
Message-ID: <4599468C.2080206@stats.uwo.ca>

On 1/1/2007 11:22 AM, Feng Qiu wrote:
> Hi Duncan:
>          Thanks for your hints.
>          I'm trying to collect distinct elements in one column in a matrix. 
> If there is a map, I can easily build up such a collection. While if using 
> list, I have to check by myself if this element already exists in the 
> collection every time I examine a new entry in the column.

You might want to use the unique() function, or duplicated(), rather 
than doing this yourself.

Duncan Murdoch

> 
> Best,
> 
> Feng
> 
> ----- Original Message ----- 
> From: "Duncan Murdoch" <murdoch at stats.uwo.ca>
> To: "Feng Qiu" <hustqiufeng at sohu.com>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Monday, January 01, 2007 10:37 AM
> Subject: Re: [R] Any container in R?
> 
> 
>> On 1/1/2007 10:17 AM, Feng Qiu wrote:
>>> R has list and array to contain elements. But does R have more powerful 
>>> container, such as "map" as in C++ STL?  or is there such a package?
>> In what way are maps more powerful than lists?  You can use names to index 
>> lists.
>>
>> The other container in R is the environment; they have fairly strange 
>> semantics, though.
>>
>> Duncan Murdoch
>>
>>


From dieter.menne at menne-biomed.de  Mon Jan  1 18:55:08 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 1 Jan 2007 17:55:08 +0000 (UTC)
Subject: [R] Types of objects?
References: <3f2154540612311829i5f59637bx1738a4310ec5f189@mail.gmail.com>
Message-ID: <loom.20070101T185442-330@post.gmane.org>

Raymond Balise <iteachtyping <at> gmail.com> writes:

> 
> Is there a list of object types used by R?  Is there a function that I can
> use to tell the type of an object?
typeof()

Dieter


From dieter.menne at menne-biomed.de  Mon Jan  1 19:09:12 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 1 Jan 2007 18:09:12 +0000 (UTC)
Subject: [R] zero random effect sizes with binomial lmer
References: <Pine.LNX.4.63.0612310424090.2213@babel.ling.upenn.edu>
Message-ID: <loom.20070101T185745-10@post.gmane.org>

Daniel Ezra Johnson <johnson4 <at> babel.ling.upenn.edu> writes:

> 
> I am fitting models to the responses to a questionnaire that has
> seven yes/no questions (Item). For each combination of Subject and
> Item, the variable Response is coded as 0 or 1.
> 
> I want to include random effects for both Subject and Item. While I
> understand that the datasets are fairly small, and there are a lot of
> invariant subjects, I do not understand something that is happening
> here, and in comparing other subsets of the data.
> 
> In the data below, which has been adjusted to show this phenomenon
> clearly, the Subject random effect variance is comparable for A
> (1.63) and B (1.712), but the Item random effect variance comes out
> as 0.109 for B and essentially zero for A (5.00e-10).
...

Check the list archives for quite a few postings of Professor Brian Ripley on
the subject of Hauk-Donner.


Dieter


From bbands at gmail.com  Mon Jan  1 19:41:02 2007
From: bbands at gmail.com (BBands)
Date: Mon, 1 Jan 2007 10:41:02 -0800
Subject: [R] Does SQL group by have a heavy duty equivalent in R
In-Reply-To: <bd93cdad0612302317j674e7e4dk257ac5707964f1@mail.gmail.com>
References: <bd93cdad0612302317j674e7e4dk257ac5707964f1@mail.gmail.com>
Message-ID: <6e8360ad0701011041m13519144tc6287b6a2899d9f@mail.gmail.com>

On 12/30/06, Farrel Buchinsky <fjbuch at gmail.com> wrote:
> Instead I used RODBC
>
> sqlSave(channel,RawSeq)
> to push the table into a Microsoft Access database
> Then a sql query, courtesy of the Microsoft Access Query Wizard a la design
> mode.

If SQL does prove to be part of your approach you might want to have a
look at SQLite. Dirk introduced it to me back when we worked on
Crusher. We use it for development and some production work. Main
attributes: simple, fast and light weight.

Everything you for R need here:
http://cran.r-project.org/src/contrib/Descriptions/RSQLite.html

SQLite's home page:
http://www.sqlite.org/

And a Python connector:
http://initd.org/tracker/pysqlite

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From hustqiufeng at sohu.com  Mon Jan  1 19:45:11 2007
From: hustqiufeng at sohu.com (Feng Qiu)
Date: Mon, 1 Jan 2007 13:45:11 -0500
Subject: [R] Any container in R?
References: <6.0.3.0.1.20070101112735.03159df8@mail.jcu.edu.au>
	<000a01c72db7$f7065180$6401a8c0@Aglog>
	<45992AB4.3070604@stats.uwo.ca>
	<003601c72dc1$1b68f5b0$6401a8c0@Aglog>
	<4599468C.2080206@stats.uwo.ca>
Message-ID: <007301c72dd4$fa3a9b50$6401a8c0@Aglog>

Hi Duncan:
         Thank you very much! I checked out unique(), it does exactly what I 
want. But I'm still curious about if R provides "STL"(standard template 
library).

Best,

Feng


----- Original Message ----- 
From: "Duncan Murdoch" <murdoch at stats.uwo.ca>
To: "Feng Qiu" <hustqiufeng at sohu.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Monday, January 01, 2007 12:36 PM
Subject: Re: [R] Any container in R?


> On 1/1/2007 11:22 AM, Feng Qiu wrote:
>> Hi Duncan:
>>          Thanks for your hints.
>>          I'm trying to collect distinct elements in one column in a 
>> matrix. If there is a map, I can easily build up such a collection. While 
>> if using list, I have to check by myself if this element already exists 
>> in the collection every time I examine a new entry in the column.
>
> You might want to use the unique() function, or duplicated(), rather than 
> doing this yourself.
>
> Duncan Murdoch
>
>>
>> Best,
>>
>> Feng
>>
>> ----- Original Message ----- 
>> From: "Duncan Murdoch" <murdoch at stats.uwo.ca>
>> To: "Feng Qiu" <hustqiufeng at sohu.com>
>> Cc: <r-help at stat.math.ethz.ch>
>> Sent: Monday, January 01, 2007 10:37 AM
>> Subject: Re: [R] Any container in R?
>>
>>
>>> On 1/1/2007 10:17 AM, Feng Qiu wrote:
>>>> R has list and array to contain elements. But does R have more powerful 
>>>> container, such as "map" as in C++ STL?  or is there such a package?
>>> In what way are maps more powerful than lists?  You can use names to 
>>> index lists.
>>>
>>> The other container in R is the environment; they have fairly strange 
>>> semantics, though.
>>>
>>> Duncan Murdoch
>>>
>>>
>
>
>


From hustqiufeng at sohu.com  Mon Jan  1 19:56:46 2007
From: hustqiufeng at sohu.com (Feng Qiu)
Date: Mon, 1 Jan 2007 13:56:46 -0500
Subject: [R] Problems with R
References: <6.2.5.6.2.20070101010137.046f2b50@pangea.Stanford.EDU>
Message-ID: <008b01c72dd6$98e8e6c0$6401a8c0@Aglog>

You got to supply the absolute path of your files and the seperating symbol 
in your csv file.
Please read the help file by typing help(read.csv),  you will have no 
problem.


----- Original Message ----- 
From: "Obinna Duru" <duru at pangea.Stanford.EDU>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, January 01, 2007 4:09 AM
Subject: [R] Problems with R


>
> Hello,
>
> Please I have problems with R. I downloaded R from UC Berkeley CRAN
> mirror, but each time I try to read a data file using the command
> 'moi.data <- read.csv(choose.files())' or 'my.data <- choose.files()
> #' , the program becomes unstable and exits itself.
>
> However, I just noticed from my Windows Task manager that the
> R-program process tree is still active and using up about 30% of the
> processor activity.
>
> Is this a problem with my download? I have tried reinstalling from UC
> Los Angeles, but still have the same issues.
>
> Any help will be appreciated. I need to use the acepack package in R.
>
> Thank you
>
>
> ....Best Regards
>
> Obinna Duru
>
> Energy Resources Engineering Department,
> Green Earth Sciences Building,
> 367 Panama Street,
> Stanford, CA 94305-2220
> cell:   (650) 814 6079
> fax:    (659) 725 2099
> email:  duru at stanford.edu
>
>
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From iteachtyping at gmail.com  Mon Jan  1 20:13:07 2007
From: iteachtyping at gmail.com (Raymond Balise)
Date: Mon, 1 Jan 2007 11:13:07 -0800
Subject: [R] Types of objects?
In-Reply-To: <loom.20070101T185442-330@post.gmane.org>
References: <3f2154540612311829i5f59637bx1738a4310ec5f189@mail.gmail.com>
	<loom.20070101T185442-330@post.gmane.org>
Message-ID: <3f2154540701011113o5cae928en6927ce6b38da5fb0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070101/9d0716a2/attachment.pl 

From dieter.menne at menne-biomed.de  Mon Jan  1 20:13:07 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 1 Jan 2007 19:13:07 +0000 (UTC)
Subject: [R] Help with filled.contour()
References: <758246BF-9625-4F20-948A-CFC120689A83@virginia.edu>
Message-ID: <loom.20070101T200956-783@post.gmane.org>

Michael Kubovy <kubovy <at> virginia.edu> writes:

> 
> The following plot is a first approximation to what I need:
.. see below 

> ***********************************
> It has four drawbacks. I would like:
> (1) to remove the white grid;
> (2) to remove the white contours;
> (3) its appearance to be smooth;
> (4) to remove the key.


In partial fulfillment of your requirements, increasing the levels gives a nice
smooth plot.

mu1 <- 0
mu2 <- 5
s <- 1
x <- seq(-2.5, 7.5, length = 41)
y <- seq(-2.5, 2.5, length = 41)
f <- function(x,y){
term1 <- 1/(2*pi*sqrt(s*s))
term2 <- -1/2
term3 <- (x - mu1)^2/s
term4 <- (y - mu1)^2/s
term5 <- (x - mu2)^2/s
term1*(.5 * exp(term2*(term3 + term4)) + .5 * exp(term2*(term5 +  
term4)))
}

z <- outer(x, y, f)
persp(x, y, z)
require(grDevices)
filled.contour(x, y, z, axes = F, frame.plot = F, asp = 1,
     col = palette(gray(seq(0, 0.9, len = 500))), nlevels = 500)

I thought that lattice would be more flexible, and getting rid of the keys was
easy, only I could not get the smooth plot. Deepayan, are we missing something?

library(lattice)
dt <- expand.grid(x=x,y=y)
dt$z <- apply(dt, 1,function(xy)  f(xy[1],xy[2]))
col.regions = palette(gray(seq(0, 0.9, len = 500)))
contourplot(z~x*y,data=dt,cuts=20,col.regions=col.regions,colorkey=FALSE,
  contour=TRUE,region=TRUE)


Dieter


From A.Robinson at ms.unimelb.edu.au  Mon Jan  1 20:19:29 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Tue, 2 Jan 2007 06:19:29 +1100
Subject: [R] zero random effect sizes with binomial lmer
In-Reply-To: <loom.20070101T185745-10@post.gmane.org>
References: <Pine.LNX.4.63.0612310424090.2213@babel.ling.upenn.edu>
	<loom.20070101T185745-10@post.gmane.org>
Message-ID: <20070101191929.GC56246@ms.unimelb.edu.au>

According to pp 197-198 of MASS 4, the Hauck-Donner phenomenon refers
to cases when the Wald approximations and the likelihood ratio tests
have different p values because of the former underestimating the the
change in log-likelihood on setting \beta_i = 0.

This seems quite different to me than the phenomenon that Daniel is
reporting, and that I believe I saw previously (post:
http://tolstoy.newcastle.edu.au/R/e2/help/06/12/6903.html
)

I tried an earlier version of R, on a different platform, and got
quite different results.  Sadly, the *earlier* results are the ones
that make sense.

eg:

> sessionInfo()
R version 2.4.1 Patched (2006-12-30 r40330) 
i386-unknown-freebsd6.1 

locale:
C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"   "datasets"  "methods"  
[7] "base"     

other attached packages:
       lme4      Matrix     lattice 
"0.9975-10"  "0.9975-6"   "0.14-16" 
> ranef(lmer(Reaction ~ Days + (1|Subject), sleepstudy,
       family=Gamma(link="log")))
An object of class "ranef.lmer"
[[1]]
      (Intercept)
308  6.817268e-10
309 -1.369242e-09
310 -1.122033e-09
330  1.164825e-10
331  2.096848e-10
332  1.494418e-10
333  3.042078e-10
334 -6.276876e-11
335 -7.556428e-10
337  1.263863e-09
349 -3.984973e-10
350  2.107439e-10
351 -1.230185e-10
352  6.409427e-10
369  1.224258e-10
370 -1.528146e-10
371 -5.310404e-11
372  3.228682e-10



> sessionInfo()
Version 2.3.1 (2006-06-01) 
i386-pc-mingw32 

attached base packages:
[1] "methods"   "stats"   "graphics"  "grDevices" "utils" "datasets" 
[7] "base"     

other attached packages:
      lme4     Matrix    lattice 
 "0.995-2" "0.995-20"   "0.13-8" 
> ranef(lmer(Reaction ~ Days + (1|Subject), sleepstudy,
+        family=Gamma(link="log")))
An object of class "ranef.lmer"
[[1]]
     (Intercept)
308  0.128473227
309 -0.294234827
310 -0.232009186
330  0.029091372
331  0.046196655
332  0.035400265
333  0.063273674
334 -0.004238362
335 -0.147285458
337  0.220381662
349 -0.070565390
350  0.046822487
351 -0.015994000
352  0.121461879
369  0.030457370
370 -0.021387277
371 -0.002494534
372  0.066650443

> 



Cheers,

Andrew




On Mon, Jan 01, 2007 at 06:09:12PM +0000, Dieter Menne wrote:
> Daniel Ezra Johnson <johnson4 <at> babel.ling.upenn.edu> writes:
> 
> > 
> > I am fitting models to the responses to a questionnaire that has
> > seven yes/no questions (Item). For each combination of Subject and
> > Item, the variable Response is coded as 0 or 1.
> > 
> > I want to include random effects for both Subject and Item. While I
> > understand that the datasets are fairly small, and there are a lot of
> > invariant subjects, I do not understand something that is happening
> > here, and in comparing other subsets of the data.
> > 
> > In the data below, which has been adjusted to show this phenomenon
> > clearly, the Subject random effect variance is comparable for A
> > (1.63) and B (1.712), but the Item random effect variance comes out
> > as 0.109 for B and essentially zero for A (5.00e-10).
> ...
> 
> Check the list archives for quite a few postings of Professor Brian Ripley on
> the subject of Hauk-Donner.
> 
> 
> Dieter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From ggrothendieck at gmail.com  Mon Jan  1 20:26:15 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 1 Jan 2007 14:26:15 -0500
Subject: [R] Types of objects?
In-Reply-To: <3f2154540701011113o5cae928en6927ce6b38da5fb0@mail.gmail.com>
References: <3f2154540612311829i5f59637bx1738a4310ec5f189@mail.gmail.com>
	<loom.20070101T185442-330@post.gmane.org>
	<3f2154540701011113o5cae928en6927ce6b38da5fb0@mail.gmail.com>
Message-ID: <971536df0701011126x51842964m1ad7a400353e4cb3@mail.gmail.com>

Try class, typeof and mode.

On 1/1/07, Raymond Balise <iteachtyping at gmail.com> wrote:
> What I am looking for is a function that goes with section 2.8 of the
> "Introduction to R" PDF that ships with R.  It says that the R objects are
> vectors, matrices, factors, lists, data frames and functions.  Is there a
> function that returns this level of information or is that taxonomy just
> something artificial for the write up?
> Ray
>
>
> On 1/1/07, Dieter Menne <dieter.menne at menne-biomed.de> wrote:
> >
> > Raymond Balise <iteachtyping <at> gmail.com> writes:
> >
> > >
> > > Is there a list of object types used by R?  Is there a function that I
> > can
> > > use to tell the type of an object?
> > typeof()
> >
> > Dieter
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From kubovy at virginia.edu  Mon Jan  1 20:47:55 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Mon, 1 Jan 2007 14:47:55 -0500
Subject: [R] Help with filled.contour()
In-Reply-To: <loom.20070101T200956-783@post.gmane.org>
References: <758246BF-9625-4F20-948A-CFC120689A83@virginia.edu>
	<loom.20070101T200956-783@post.gmane.org>
Message-ID: <990E1DC3-4FAD-4297-B3FF-D12F6710B46F@virginia.edu>

On Jan 1, 2007, at 2:13 PM, Dieter Menne wrote:

> Michael Kubovy <kubovy <at> virginia.edu> writes:
>
>>
>> The following plot is a first approximation to what I need:
> .. see below
>
>> ***********************************
>> It has four drawbacks. I would like:
>> (1) to remove the white grid;
>> (2) to remove the white contours;
>> (3) its appearance to be smooth;
>> (4) to remove the key.
>
>
> In partial fulfillment of your requirements, increasing the levels  
> gives a nice
> smooth plot.
>
> mu1 <- 0
> mu2 <- 5
> s <- 1
> x <- seq(-2.5, 7.5, length = 41)
> y <- seq(-2.5, 2.5, length = 41)
> f <- function(x,y){
> term1 <- 1/(2*pi*sqrt(s*s))
> term2 <- -1/2
> term3 <- (x - mu1)^2/s
> term4 <- (y - mu1)^2/s
> term5 <- (x - mu2)^2/s
> term1*(.5 * exp(term2*(term3 + term4)) + .5 * exp(term2*(term5 +
> term4)))
> }
>
> z <- outer(x, y, f)
> persp(x, y, z)
> require(grDevices)
> filled.contour(x, y, z, axes = F, frame.plot = F, asp = 1,
>      col = palette(gray(seq(0, 0.9, len = 500))), nlevels = 500)

Thanks, Dieter.

I tried and it gave a strange result. See
http://people.virginia.edu/~mk9y/mySite/twoGaussian.R
and
http://people.virginia.edu/~mk9y/mySite/twoGaussian.pdf

*********************************
Session Info
*********************************
 > sessionInfo()
R version 2.4.1 (2006-12-18)
powerpc-apple-darwin8.8.0

locale:
C

attached base packages:
[1] "datasets"  "grid"      "graphics"  "grDevices" "stats"      
"utils"     "methods"
[8] "base"

other attached packages:
          JGR       iplots       JavaGD         MASS      
gridBase      lattice
     "1.4-14"      "1.0-5"      "0.3-5"     "7.2-30"      "0.4-3"     
"0.14-16"
latticeExtra        rJava       xtable
      "0.1-4"     "0.4-12"      "1.4-2"

_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From tlumley at u.washington.edu  Mon Jan  1 21:21:51 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 1 Jan 2007 12:21:51 -0800 (PST)
Subject: [R] Any container in R?
In-Reply-To: <007301c72dd4$fa3a9b50$6401a8c0@Aglog>
References: <6.0.3.0.1.20070101112735.03159df8@mail.jcu.edu.au>
	<000a01c72db7$f7065180$6401a8c0@Aglog> <45992AB4.3070604@stats.uwo.ca>
	<003601c72dc1$1b68f5b0$6401a8c0@Aglog> <4599468C.2080206@stats.uwo.ca>
	<007301c72dd4$fa3a9b50$6401a8c0@Aglog>
Message-ID: <Pine.LNX.4.64.0701011145580.30755@homer24.u.washington.edu>

On Mon, 1 Jan 2007, Feng Qiu wrote:

> Hi Duncan:
>         Thank you very much! I checked out unique(), it does exactly what I
> want. But I'm still curious about if R provides "STL"(standard template
> library).

No.

Some things the STL does aren't needed in R, others are implemented 
differently, and others aren't implemented.

One particularly important example is iterators, which will often either 
happen invisibly due to vectorized operations or will be done with the 
*apply family of functions.

Your example could have been done either way. Using duplicated() is the 
vectorized approach; the apply approach would use tapply().

C++ is not terribly similar to R. A lot of the effort in STL is expended 
on allowing a piece of code to be used on different types (where 
appropriate). In R you have to expend effort on stopping a piece of code 
being used on different types (where inappropriate).


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From ivowel at gmail.com  Mon Jan  1 22:03:50 2007
From: ivowel at gmail.com (ivo welch)
Date: Mon, 1 Jan 2007 16:03:50 -0500
Subject: [R] advice on semi-serious attempt to extend summary
Message-ID: <50d1c22d0701011303jd9527f0p80f92a85ec5c8921@mail.gmail.com>

Dear R wizards:

I am trying (finally) to build a function that might be useful to
others.  In particular, I want to create a summary.lme (extended lm)
method that [a] adds normalized coefficients and [b] white
heteroskedasticity adjusted se's and T's.  I believe I already know
how to do the programming to do these two, at least in simple
unweighted cases.  Now my challenges are just [1] to trap weird cases
(e.g., hccm dies because the standard errors cannot be computed), and
[2] to follow "proper R rules and regulations."

I started my experiments by copying summary.lm() to summary.lme.  But
there is some magic that I do not understand.  Apparently, the
  class(ans) <- "summary.lm"
signals to R that it should not produce an unlisted summary of the
components of ans, but that it should print something that is nicely
formatted.
  example:  y=rnorm(5); x=rnorm(5); m=lm(y~x);  summary.lm(m);
does exactly this nice output.  alas, just replacing the name at the end with
    class(ans) <- "summary.lme"
and typing summary.lme(m), having renamed summary.lm to summary, then
loses the nice printout.

so, it seems to me that somewhere R knows that a "summary.lm" object
gets special printing.  if I have not misunderstood this, then where
and how does this magic happen?

advice appreciated.

regards,

/iaw


From ggrothendieck at gmail.com  Mon Jan  1 22:12:14 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 1 Jan 2007 16:12:14 -0500
Subject: [R] advice on semi-serious attempt to extend summary
In-Reply-To: <50d1c22d0701011303jd9527f0p80f92a85ec5c8921@mail.gmail.com>
References: <50d1c22d0701011303jd9527f0p80f92a85ec5c8921@mail.gmail.com>
Message-ID: <971536df0701011312ve3bcde3l35a0072bce60ab55@mail.gmail.com>

Try:

methods(print)
stats:::print.summary.lm

On 1/1/07, ivo welch <ivowel at gmail.com> wrote:
> Dear R wizards:
>
> I am trying (finally) to build a function that might be useful to
> others.  In particular, I want to create a summary.lme (extended lm)
> method that [a] adds normalized coefficients and [b] white
> heteroskedasticity adjusted se's and T's.  I believe I already know
> how to do the programming to do these two, at least in simple
> unweighted cases.  Now my challenges are just [1] to trap weird cases
> (e.g., hccm dies because the standard errors cannot be computed), and
> [2] to follow "proper R rules and regulations."
>
> I started my experiments by copying summary.lm() to summary.lme.  But
> there is some magic that I do not understand.  Apparently, the
>  class(ans) <- "summary.lm"
> signals to R that it should not produce an unlisted summary of the
> components of ans, but that it should print something that is nicely
> formatted.
>  example:  y=rnorm(5); x=rnorm(5); m=lm(y~x);  summary.lm(m);
> does exactly this nice output.  alas, just replacing the name at the end with
>    class(ans) <- "summary.lme"
> and typing summary.lme(m), having renamed summary.lm to summary, then
> loses the nice printout.
>
> so, it seems to me that somewhere R knows that a "summary.lm" object
> gets special printing.  if I have not misunderstood this, then where
> and how does this magic happen?
>
> advice appreciated.
>
> regards,
>
> /iaw
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From DOCGArmelini at iese.edu  Mon Jan  1 23:43:31 2007
From: DOCGArmelini at iese.edu (Armelini, Guillermo)
Date: Mon, 1 Jan 2007 23:43:31 +0100
Subject: [R] matrix size
Message-ID: <2DA68F10815D6E49956E3DC2CE18603E88BFA4@SRVSTAFF.iese.org>

Hello everyone
Could anybody tell me how to set the following matrix?
 
n2<-matrix(nrow=10185,ncol=10185,seq(0,0,length=103734225))
 
R answer was
Error: cannot allocate vector of size 810423 Kb
 
Are there any solution? I tried to increase the memory size but it didn't work
G



This message has been scanned for viruses by TRENDMICRO,\ an...{{dropped}}


From maechler at stat.math.ethz.ch  Tue Jan  2 00:18:45 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 2 Jan 2007 00:18:45 +0100
Subject: [R] matrix size
In-Reply-To: <2DA68F10815D6E49956E3DC2CE18603E88BFA4@SRVSTAFF.iese.org>
References: <2DA68F10815D6E49956E3DC2CE18603E88BFA4@SRVSTAFF.iese.org>
Message-ID: <17817.38613.669233.601335@stat.math.ethz.ch>

>>>>> "GArmelini" == Armelini, Guillermo <DOCGArmelini at iese.edu>
>>>>>     on Mon, 1 Jan 2007 23:43:31 +0100 writes:

    GArmelini> Hello everyone Could anybody tell me how to set
    GArmelini> the following matrix?
 
    GArmelini> n2<-matrix(nrow=10185,ncol=10185,seq(0,0,length=103734225))
 
    GArmelini> R answer was Error: cannot allocate vector of
    GArmelini> size 810423 Kb

of course.
 
    GArmelini> Are there any solution? I tried to increase the
    GArmelini> memory size but it didn't work G

You might consider using *sparse* matrices
such as available by R's "Matrix" package.

 install.packages("Matrix")  # once only
 library(Matrix)             # every time you use it
 n2 <- Matrix(0, nrow=10185, ncol=10185)

will produce a sparse Matrix (of class "dsCMatrix")
that does not need a lot of memory.

If you want to do anything reasonable, with 'n2' I assume you'll
want to add some non-zero entries.

To do that (and similary things) a bit more efficiently in the 
current implementations of "Matrix", 
I'd recommend to work with a "TsparseMatrix"
(instead of the `default' Csparse*), i.e. a sparse matrix
representation working with so called triplets, e.g.

m2 <- as(n2, "TsparseMatrix")
m2[1,3] <- 10
m2[2, 1:10] <- 0:9

m2[1:10, 1:20]

etc,.. 
BTW: Such sub-assignments should now work, but some or still
slow. Till now the main emphasis for such matrices was general
good organization and speed in matrix operations rather than
"index operations".

I'm quite interested to hear what you want to do with your
matrix.  Use R-help if you think it could be of general
interest, or reply privately if you prefer.

Regards,
Martin Maechler, ETH Zurich


From roger at ysidro.econ.uiuc.edu  Tue Jan  2 00:24:10 2007
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Mon, 1 Jan 2007 17:24:10 -0600
Subject: [R] matrix size
In-Reply-To: <2DA68F10815D6E49956E3DC2CE18603E88BFA4@SRVSTAFF.iese.org>
References: <2DA68F10815D6E49956E3DC2CE18603E88BFA4@SRVSTAFF.iese.org>
Message-ID: <4BCD45E2-C3A8-46AB-808A-1A3A38DA1F5C@ysidro.econ.uiuc.edu>


On Jan 1, 2007, at 4:43 PM, Armelini, Guillermo wrote:

> Hello everyone
> Could anybody tell me how to set the following matrix?
>
> n2<-matrix(nrow=10185,ncol=10185,seq(0,0,length=103734225))

You can use:

library(SparseM)
as.matrix.coo(0,10185,10185)

but then you need to find something interesting to do with such a
boring matrix...


>
> R answer was
> Error: cannot allocate vector of size 810423 Kb
>
> Are there any solution? I tried to increase the memory size but it  
> didn't work
> G
>
>
>
> This message has been scanned for viruses by TRENDMICRO,\ an... 
> {{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From john.kornak at ucsf.edu  Tue Jan  2 00:26:45 2007
From: john.kornak at ucsf.edu (John Kornak)
Date: Mon, 01 Jan 2007 15:26:45 -0800
Subject: [R] rimage package broken with fedora upgrade
In-Reply-To: <20061231153249.GA1890@psych.upenn.edu>
References: <459726A2.9080202@ucsf.edu>
	<399fc6f10612301951v34c77ee4uf70886054369c1e8@mail.gmail.com>
	<20061231153249.GA1890@psych.upenn.edu>
Message-ID: <459998B5.2020909@ucsf.edu>


Thanks to Jonathan and Oleg for their suggestions/comments. Their
responses are added at the end of this email.

I tried Oleg's suggestion of giving me write permissions on
/usr/lib/R/library. This didn't seem to work (I tried various
combinations of changing permissions and ownership but nothing worked).
Anyway, before changing permissions the library would not work when R
was fired up as root. I am by no means a linux/unix expert and maybe
there is some method of giving me write permissions that would have
solved the problem.

Jonathan was right that his was essentially a different problem. I
already had fftw2 installed.

My (heavy-handed) solution was to bite the bullet and re-install fedora 
core 6 from scratch. (Apparently the upgrade from fc3 does not work as 
well as back up + fresh install.) After running yum update, I then 
installed R by clicking the relevant R RPM in CRAN. I subsequently 
installed fftw2 and fftw2-devel (using "yum install package_name") (the 
development package is needed) and installed rimage using 
install.packages("rimage") within R and everything is now fine. Note 
that it is also necessary to install libjpeg and libjpeg-devel if you do 
not already have it.

I hope that this helps others that want

I am looking forward to playing with Oleg's EBimage package.

Thanks again and Happy New Year to all!

Cheers

John


-- 
John Kornak,PhD
Assistant Professor
Departments of Radiology, and Epidemiology & Biostatistics
University of California, San Francisco
Box 0946
San Francisco, CA 94143
Tel: (415) 353-4740
fax: (415) 353-9423
Email: john.kornak at ucsf.edu



Jonathan Baron wrote:
> Responding to the original post, which I did not save.
> 
>> On 31/12/06, John Kornak <john.kornak at ucsf.edu> wrote:
>>>
>>> Dear R list members
>>>
>>> I would be grateful if anyone could guide me to a solution for fixing my
>>> rimage package problem described below.
>>>
>>> I recently upgraded my machine from fedora core 3 to fedora core 6 and
>>> then upgraded R from version 2.3.1 from version 2.4.1.
>>>
>>> I then fired up R, tried to load the rimage library and received the
>>> following messages:
>>>
>>>   > library(rimage)
>>> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>>>          unable to load shared library
>>> '/usr/lib/R/library/rimage/libs/rimage.so':
>>>    /usr/lib/R/library/rimage/libs/rimage.so: cannot restore segment prot
>>> after reloc: Permission denied
>>> Error in library(rimage) : .First.lib failed for 'rimage'

Jonathan Baron wrote:

> 
> I had the same problem, but, unfortunately, a slightly different error 
> message.  My error message said:
> 
>> library(rimage)
> Error in dyn.load(x, as.logical(local), as.logical(now)) : 
>         unable to load shared library
> '/usr/lib/R/library/rimage/libs/rimage.so':
>   libfftw.so.2: cannot open shared object file: No such file or
> directory
> Error in library(rimage) : .First.lib failed for 'rimage'
> 
> So I took the hint and said
> 
> yum install libfftw.so.2
> 
> and this installed libfftw,
> which cured the problem.
> 
> But you did not get this error message about libfftw.
> 
> Jon
> 

Oleg Sklyar wrote:

Cannot help you much with that as the thing seems to compile normally.
As you use a global R installation, try giving yourself write
permissions in /usr/lib/R/library, this might help. Otherwise, try
installing R from source in your home or elsewhere where you have all
write permissions and check it again.

Anyway, I wanted also to advertise my package because you use rimage:

http://bioconductor.org/packages/2.0/bioc/html/EBImage.html

The difference is - it supports up to 95 image formats and implements
many image processing methods. In fact you can combine this with
functions from rimage. If interested, go for a development version above
and it works on Fedora 6 Zod and 2.4.1 - did try just today.

Best regards,
Oleg


From fjbuch at gmail.com  Tue Jan  2 02:02:01 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Mon, 1 Jan 2007 20:02:01 -0500
Subject: [R] Does SQL group by have a heavy duty equivalent in R
In-Reply-To: <Pine.LNX.4.64.0612311942320.31545@tajo.ucsd.edu>
References: <bd93cdad0612302317j674e7e4dk257ac5707964f1@mail.gmail.com>
	<Pine.LNX.4.64.0612311739430.30473@tajo.ucsd.edu>
	<Pine.LNX.4.64.0612311942320.31545@tajo.ucsd.edu>
Message-ID: <bd93cdad0701011702q14f9f5dfn37a13d53413f5cb2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070101/af6d0ed0/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Jan  2 02:22:00 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Jan 2007 01:22:00 +0000 (GMT)
Subject: [R] Does SQL group by have a heavy duty equivalent in R
In-Reply-To: <6e8360ad0701011041m13519144tc6287b6a2899d9f@mail.gmail.com>
References: <bd93cdad0612302317j674e7e4dk257ac5707964f1@mail.gmail.com>
	<6e8360ad0701011041m13519144tc6287b6a2899d9f@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701020119280.6443@gannet.stats.ox.ac.uk>

On Mon, 1 Jan 2007, BBands wrote:

> On 12/30/06, Farrel Buchinsky <fjbuch at gmail.com> wrote:
>> Instead I used RODBC
>>
>> sqlSave(channel,RawSeq)
>> to push the table into a Microsoft Access database
>> Then a sql query, courtesy of the Microsoft Access Query Wizard a la design
>> mode.
>
> If SQL does prove to be part of your approach you might want to have a
> look at SQLite. Dirk introduced it to me back when we worked on
> Crusher. We use it for development and some production work. Main
> attributes: simple, fast and light weight.

And it works perfectly well with RODBC, so you don't need yet another 
package.  RODBC even comes with SQLite examples.

BTW, 'fast' is rather misleading: SQLite is often fast for small tasks, 
but can be very slow compared to a more serious DBMS.

> Everything you for R need here:
> http://cran.r-project.org/src/contrib/Descriptions/RSQLite.html
>
> SQLite's home page:
> http://www.sqlite.org/
>
> And a Python connector:
> http://initd.org/tracker/pysqlite
>
>    jab
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From johnson4 at babel.ling.upenn.edu  Tue Jan  2 02:59:55 2007
From: johnson4 at babel.ling.upenn.edu (Daniel Ezra Johnson)
Date: Mon, 1 Jan 2007 20:59:55 -0500 (EST)
Subject: [R] zero random effect sizes with binomial lmer
Message-ID: <Pine.LNX.4.63.0701012053440.5970@babel.ling.upenn.edu>

> From: Andrew Robinson <A.Robinson_at_ms.unimelb.edu.au>
> Date: Mon 01 Jan 2007 - 19:19:29 GMT
>
> I tried an earlier version of R, on a different platform, and got quite 
> different results. Sadly, the *earlier* results are the ones that make 
> sense.

Andrew, I tried installing R 2.3.1, it seemed to work as you said, but I 
realized it was because the default method was PQL instead of Laplace. 
Switching the method to Laplace, the zero effect came back.

It seems like there's no easy way around this. I would like to compare 
random effect sizes between different subsets of my data, but this may not 
be possible using the current functions...

Daniel


From ivowel at gmail.com  Tue Jan  2 03:00:50 2007
From: ivowel at gmail.com (ivo welch)
Date: Mon, 1 Jan 2007 21:00:50 -0500
Subject: [R] slightly extended lm class
Message-ID: <50d1c22d0701011800x627fb502q8849380bc165d47@mail.gmail.com>

Dear R readers:

I have written a short lme.R function, which adds normalized
coefficients and White heteroskedasticity-adjusted statistics to the
standard output.  Otherwise, it behaves like lm.  This is of course
trivial for experts, but for me and other amateur users perhaps
helpful.

  y= rnorm(15);  x= rnorm(15);  z= rnorm(15);
  m= lme( y ~ x + z);  print(summary(m));

produces something like

Call:
lm(formula = ..1)

Residuals:
   Min     1Q Median     3Q    Max
-26.04 -10.61   1.55  13.84  28.84

Coefficients:
            Estimate NormEst Std. Error t value Pr(>|t|) t-htsk Pr(>|th|)
(Intercept)   6.1343  0.0000     5.8886  1.0417   0.3181   1.41     0.183
x             0.6981  0.1109     1.5922  0.4385   0.6688   0.37     0.716
z            -0.7720 -0.4735     0.4123 -1.8727   0.0857  -2.06     0.062 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 18.7 on 12 degrees of freedom
Multiple R-Squared: 0.233,      Adjusted R-squared: 0.106
F-statistic: 1.83 on 2 and 12 DF,  p-value: 0.203


If anyone is interested, it is available at
"http://welch.econ.brown.edu/computers/lme.R".  I didn't even get the
formatting on the coefficient matrix right, but this is cosmetic.
maybe other errors in it, too.  of course, it would be nice if someone
made something industrial strength out of this---for use by casual
amateurs such as myself.

hope it helps someone.

regards,

/iaw


From kubovy at virginia.edu  Tue Jan  2 03:12:15 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Mon, 1 Jan 2007 21:12:15 -0500
Subject: [R] slightly extended lm class
In-Reply-To: <50d1c22d0701011800x627fb502q8849380bc165d47@mail.gmail.com>
References: <50d1c22d0701011800x627fb502q8849380bc165d47@mail.gmail.com>
Message-ID: <8B88D0BE-6EA0-4797-AAE8-C8E41A4CD5A6@virginia.edu>

On Jan 1, 2007, at 9:00 PM, ivo welch wrote:

> I have written a short lme.R function, which adds normalized
> coefficients and White heteroskedasticity-adjusted statistics to the
> standard output.  Otherwise, it behaves like lm.

Is it a good idea to call it lme, since there's a widely used lme()  
in the nlme package?
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From A.Robinson at ms.unimelb.edu.au  Tue Jan  2 03:14:30 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Tue, 2 Jan 2007 13:14:30 +1100
Subject: [R] zero random effect sizes with binomial lmer
In-Reply-To: <Pine.LNX.4.63.0701012053440.5970@babel.ling.upenn.edu>
References: <Pine.LNX.4.63.0701012053440.5970@babel.ling.upenn.edu>
Message-ID: <20070102021430.GH56246@ms.unimelb.edu.au>

Good observation, Daniel.  I had not picked up the different default
fitting method. 

Cheers

Andrew

On Mon, Jan 01, 2007 at 08:59:55PM -0500, Daniel Ezra Johnson wrote:
> > From: Andrew Robinson <A.Robinson_at_ms.unimelb.edu.au>
> > Date: Mon 01 Jan 2007 - 19:19:29 GMT
> >
> > I tried an earlier version of R, on a different platform, and got quite 
> > different results. Sadly, the *earlier* results are the ones that make 
> > sense.
> 
> Andrew, I tried installing R 2.3.1, it seemed to work as you said, but I 
> realized it was because the default method was PQL instead of Laplace. 
> Switching the method to Laplace, the zero effect came back.
> 
> It seems like there's no easy way around this. I would like to compare 
> random effect sizes between different subsets of my data, but this may not 
> be possible using the current functions...
> 
> Daniel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From ivowel at gmail.com  Tue Jan  2 03:23:27 2007
From: ivowel at gmail.com (ivo welch)
Date: Mon, 1 Jan 2007 21:23:27 -0500
Subject: [R] slightly extended lm class
In-Reply-To: <8B88D0BE-6EA0-4797-AAE8-C8E41A4CD5A6@virginia.edu>
References: <50d1c22d0701011800x627fb502q8849380bc165d47@mail.gmail.com>
	<8B88D0BE-6EA0-4797-AAE8-C8E41A4CD5A6@virginia.edu>
Message-ID: <50d1c22d0701011823j654301d9u669c8ccc009ba097@mail.gmail.com>

obviously not.  any other suggestion?  of course, it would be even
better if someone who knew what he is doing were to add this
functionality on demand (e.g., optional parameters) to the standard lm
classes.  just a thought...

On 1/1/07, Michael Kubovy <kubovy at virginia.edu> wrote:
> On Jan 1, 2007, at 9:00 PM, ivo welch wrote:
>
> > I have written a short lme.R function, which adds normalized
> > coefficients and White heteroskedasticity-adjusted statistics to the
> > standard output.  Otherwise, it behaves like lm.
>
> Is it a good idea to call it lme, since there's a widely used lme()
> in the nlme package?
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
> Parcels:    Room 102        Gilmer Hall
>          McCormick Road    Charlottesville, VA 22903
> Office:    B011    +1-434-982-4729
> Lab:        B019    +1-434-982-4751
> Fax:        +1-434-982-4766
> WWW:    http://www.people.virginia.edu/~mk9y/
>
>
>


From kbeath at efs.mq.edu.au  Tue Jan  2 03:36:52 2007
From: kbeath at efs.mq.edu.au (Ken Beath)
Date: Tue, 2 Jan 2007 13:36:52 +1100
Subject: [R] zero random effect sizes with binomial lmer
In-Reply-To: <s5998651.032@mail.efs.mq.edu.au>
References: <s5998651.032@mail.efs.mq.edu.au>
Message-ID: <753607BF-B98D-461A-A99B-0F78FFB71D5F@efs.mq.edu.au>

On Sun, 31 Dec 2006, at 05:50 PM, Daniel Ezra Johnson  
<johnson4 at babel.ling.upenn.edu> wrote:

>
> Gregor,
>
> Thanks for your replies.
>
> 1) Yes, I have tweaked the data to show as clearly as I can that  
> this is a
> bug, that a tiny change in initial conditions causes the collapse of a
> reasonable 'parameter' estimate.
>
> 2) mcmcsamp() does not work (currently) for binomial fitted models.
>
> 3) This is an issue of what happens when the sample is too small.  
> For all
> larger data sets I have gotten a ranef variance between 0.05 and  
> 1.00 or
> so.
>
> It makes no sense to say that as the data set gets smaller, the  
> systematic
> variation between Items goes away. It doesn't, as I've shown. In  
> the data
> above, certain Items were still 10+ times as likely (log-odds wise) to
> have Response==1 as others.
>
> It may make sense to say that the effect becomes unestimable, due  
> to its
> small size. But my understanding is not that this should make the
> algorithm return zero as an estimated value.
>
> D
>
>

There is always the possibility that the Laplace approximation is  
proving too inaccurate for this problem but that seems unlikely, as  
there should be enough observations. The only way to check is to use  
a package that uses adaptive Gauss-Hermite for the integration,  
gllamm in Stata or NLMIXED in SAS may do it. PQL is even worse, so it  
is not an option.

The real problem is that there is not enough variation across your  
items,  and so the estimate of the random effect is close to zero.  
The only difference between your datasets is that one results in an  
estimate closer to zero. Fit the model without (Item|1), and the fit  
hardly worsens in either case, with resulting better AIC and BIC.

The responses by item can be checked with
 > apply(aa,2,sum)
[1] 2 2 2 4 1 5 4
 > apply(bb,2,sum)
[1] 2 2 2 4 1 6 4

This is not perfect as it ignores the subject variation, but they  
don't have a lot of variation.


From ripley at stats.ox.ac.uk  Tue Jan  2 03:38:41 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Jan 2007 02:38:41 +0000 (GMT)
Subject: [R] slightly extended lm class
In-Reply-To: <50d1c22d0701011823j654301d9u669c8ccc009ba097@mail.gmail.com>
References: <50d1c22d0701011800x627fb502q8849380bc165d47@mail.gmail.com>
	<8B88D0BE-6EA0-4797-AAE8-C8E41A4CD5A6@virginia.edu>
	<50d1c22d0701011823j654301d9u669c8ccc009ba097@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701020236370.8763@gannet.stats.ox.ac.uk>

Have you looked at
https://stat.ethz.ch/pipermail/r-devel/2006-December/044111.html
?

That seems the appropriate mailing list if you want to continue this 
discussion.

On Mon, 1 Jan 2007, ivo welch wrote:

> obviously not.  any other suggestion?  of course, it would be even
> better if someone who knew what he is doing were to add this
> functionality on demand (e.g., optional parameters) to the standard lm
> classes.  just a thought...
>
> On 1/1/07, Michael Kubovy <kubovy at virginia.edu> wrote:
>> On Jan 1, 2007, at 9:00 PM, ivo welch wrote:
>>
>>> I have written a short lme.R function, which adds normalized
>>> coefficients and White heteroskedasticity-adjusted statistics to the
>>> standard output.  Otherwise, it behaves like lm.
>>
>> Is it a good idea to call it lme, since there's a widely used lme()
>> in the nlme package?
>> _____________________________
>> Professor Michael Kubovy
>> University of Virginia
>> Department of Psychology
>> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
>> Parcels:    Room 102        Gilmer Hall
>>          McCormick Road    Charlottesville, VA 22903
>> Office:    B011    +1-434-982-4729
>> Lab:        B019    +1-434-982-4751
>> Fax:        +1-434-982-4766
>> WWW:    http://www.people.virginia.edu/~mk9y/
>>
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From dieter.menne at menne-biomed.de  Tue Jan  2 09:15:57 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 2 Jan 2007 08:15:57 +0000 (UTC)
Subject: [R] Help with filled.contour()
References: <758246BF-9625-4F20-948A-CFC120689A83@virginia.edu>
	<loom.20070101T200956-783@post.gmane.org>
	<990E1DC3-4FAD-4297-B3FF-D12F6710B46F@virginia.edu>
Message-ID: <loom.20070102T090808-68@post.gmane.org>

Michael Kubovy <kubovy <at> virginia.edu> writes:

> 
> I tried and it gave a strange result. See
> http://people.virginia.edu/~mk9y/mySite/twoGaussian.R
> and
> http://people.virginia.edu/~mk9y/mySite/twoGaussian.pdf
> 
> *********************************
> Session Info
> *********************************
>  > sessionInfo()
> R version 2.4.1 (2006-12-18)
> powerpc-apple-darwin8.8.0

Hmm, strange, I can reproduce your problem on Windows (otherwise same config)
with pdf, but it looks beautifully on screen for me if I mentally remove the
ugly legend.

Dieter


From docesalvaj at iese.edu  Tue Jan  2 10:16:23 2007
From: docesalvaj at iese.edu (Salvaj, Erica)
Date: Tue, 2 Jan 2007 10:16:23 +0100
Subject: [R] SNA Matrix
Message-ID: <2DA68F10815D6E49956E3DC2CE18603E91666D@SRVSTAFF.iese.org>

Hello
 
I export a one mode network from Pajek to R, and the former made an .r file  called PajekR.r, that is actually an script to be run in R
The problem is that what the file actually does is to set a 0 martrix and then assing to each pair of nodes the corresponding values. Since the matrix is huge (10.000 nodes) a copy/paste procedure is very time consuming, and I guess pretty inefficient. So the question is: are there any way to read directly the .r file in R, without copy and paste the sentences of the script in the R console?
 
Erica H. Salvaj
PhD Candidate
IESE Business School
docesalvaj at iese.edu
 


This message has been scanned for viruses by TRENDMICRO,\ an...{{dropped}}


From tobias.verbeke at telenet.be  Tue Jan  2 10:36:01 2007
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Tue, 02 Jan 2007 10:36:01 +0100
Subject: [R] SNA Matrix
In-Reply-To: <2DA68F10815D6E49956E3DC2CE18603E91666D@SRVSTAFF.iese.org>
References: <2DA68F10815D6E49956E3DC2CE18603E91666D@SRVSTAFF.iese.org>
Message-ID: <459A2781.9040202@telenet.be>

Salvaj, Erica wrote:
> Hello
>  
> I export a one mode network from Pajek to R, and the former made an .r file  called PajekR.r, that is actually an script to be run in R
> The problem is that what the file actually does is to set a 0 martrix and then assing to each pair of nodes the corresponding values. Since the matrix is huge (10.000 nodes) a copy/paste procedure is very time consuming, and I guess pretty inefficient. So the question is: are there any way to read directly the .r file in R, without copy and paste the sentences of the script in the R console?
>   

The command you are looking for is source.

If PajekR.r resides in your working directory, it
is as easy as

source("PajekR.r")

Otherwise replace "PajekR.r" with the full path to this file.

An other option would be

source(choose.files())

which fires up a GUI that lets you browse for the file.

HTH,
Tobias
>  
> Erica H. Salvaj
> PhD Candidate
> IESE Business School
> docesalvaj at iese.edu
>  
>
>
> This message has been scanned for viruses by TRENDMICRO,\ an...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From bxc at steno.dk  Tue Jan  2 12:50:05 2007
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Tue, 2 Jan 2007 12:50:05 +0100
Subject: [R] How to extract the variance componets from lme
Message-ID: <40D3930AC1C8EA469E39536E5BC80835038E2F11@EXDKBA021.corp.novocorp.net>

Here is a piece of code fitting a model to a (part) of a dataset, just
for
illustration. I can extract the random interaction and the residual
variance
in group meth==1 using VarCorr, but how do I get the other residual
variance?

Is there any way to get the other variances in numerical form directly -
it 
seems a litte contraintuitive to use "as.numeric" when extracting
estimates, it's 
a bit like good old days writing SAS-programs that reads the SAS output
files...

Bendix
------------------------------------------------------------------------
-------

library( nlme )

dfr <-
structure(list(meth = structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2), .Label = c("CO",
"pulse"), class = "factor"), item = c(1, 1, 1, 2, 2, 2, 3, 3,
3, 4, 4, 4, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4), repl = c(1,
2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1,
2, 3), y = c(78, 76.4, 77.2, 68.7, 67.6, 68.3, 82.9, 80.1, 80.7,
62.3, 65.8, 67.5, 71, 72, 73, 68, 67, 68, 82, 77, 77, 43, 69,
77)), .Names = c("meth", "item", "repl", "y"), row.names = c("1",
"2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "184",
"185", "186", "187", "188", "189", "190", "191", "192", "193",
"194", "195"), class = "data.frame")

m1 <-
lme( y ~ factor( meth ) + factor( item ),
     random = ~1 | MI,
     weights = varIdent( form = ~1 | meth ),
     method ="REML",
     data = cbind( dfr, MI=interaction( dfr$meth, dfr$item ) ) )

m1

# The MI std and the residual std for meth==1
as.numeric(VarCorr(m1)[,2])

______________________________________________

Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2-4
DK-2820 Gentofte
Denmark
+45 44 43 87 38 (direct)
+45 30 75 87 38 (mobile)
+45 44 43 73 13 (fax)
bxc at steno.dk   http://www.biostat.ku.dk/~bxc


From richard.gott at dur.ac.uk  Tue Jan  2 14:46:38 2007
From: richard.gott at dur.ac.uk (R Gott)
Date: Tue, 02 Jan 2007 13:46:38 +0000
Subject: [R] RMySQL adn FC6 - install fail
Message-ID: <1167745598.3427.34.camel@home>

Dear anybody

I am running Fedora Core 6 with the latest version of R.

When I try to install the package RMySQL I get this message:

        * Installing *source* package 'RMySQL' ...
        creating cache ./config.cache
        checking how to run the C preprocessor... /lib/cpp
        checking for compress in -lz... no
        checking for getopt_long in -lc... no
        checking for mysql_init in -lmysqlclient... no
        checking for mysql.h... no
        checking for mysql_init in -lmysqlclient... no
        checking for mysql_init in -lmysqlclient... no
        checking for mysql_init in -lmysqlclient... no
        checking for mysql_init in -lmysqlclient... no
        checking for mysql_init in -lmysqlclient... no
        
        Configuration error:
           Could not locate the library "libz" required by MySQL.
        
        INSTRUCTIONS:
        
           The "libz" library is required by the MySQL client library
           in order to compress/uncompress connections between clients
           and the MySQL engine.
        
           Make sure you have "libz" installed properly and/or included
           in your $LD_LIBRARY_PATH.  Perhaps it is not in any of the
           standard directories (e.g., /usr/lib/, /usr/local/lib)?
        
        Aborting the installation of RMySQL.

I haev checked my installation adn foudn teh follwing files:

        /usr/lib/libz.so (whcih points to)
        /usr/lib/libz.so.1.2
        
Any help woudl be appreciated.  I couldn't find anytign in teh archives.

Richard


From joris.dewolf at cropdesign.com  Tue Jan  2 15:12:04 2007
From: joris.dewolf at cropdesign.com (joris.dewolf at cropdesign.com)
Date: Tue, 2 Jan 2007 15:12:04 +0100
Subject: [R] How to extract the variance componets from lme
In-Reply-To: <40D3930AC1C8EA469E39536E5BC80835038E2F11@EXDKBA021.corp.novocorp.net>
Message-ID: <OFAB62D618.E6A346C5-ONC1257257.004DA568-C1257257.004DE9A5@basf-c-s.be>

I advice you strongly to use VarCorr(), but if you insist

tmp <- as.matrix(m1$modelStruct$reStruct$MI)
c(sqrt(diag(tmp)), Residual = 1) * m1$sigma


Joris

r-help-bounces at stat.math.ethz.ch wrote on 02/01/2007 12:50:05:

> Here is a piece of code fitting a model to a (part) of a dataset, just
> for
> illustration. I can extract the random interaction and the residual
> variance
> in group meth==1 using VarCorr, but how do I get the other residual
> variance?
>
> Is there any way to get the other variances in numerical form directly -
> it
> seems a litte contraintuitive to use "as.numeric" when extracting
> estimates, it's
> a bit like good old days writing SAS-programs that reads the SAS output
> files...
>
> Bendix
> ------------------------------------------------------------------------
> -------
>
> library( nlme )
>
> dfr <-
> structure(list(meth = structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2), .Label = c("CO",
> "pulse"), class = "factor"), item = c(1, 1, 1, 2, 2, 2, 3, 3,
> 3, 4, 4, 4, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4), repl = c(1,
> 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1,
> 2, 3), y = c(78, 76.4, 77.2, 68.7, 67.6, 68.3, 82.9, 80.1, 80.7,
> 62.3, 65.8, 67.5, 71, 72, 73, 68, 67, 68, 82, 77, 77, 43, 69,
> 77)), .Names = c("meth", "item", "repl", "y"), row.names = c("1",
> "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "184",
> "185", "186", "187", "188", "189", "190", "191", "192", "193",
> "194", "195"), class = "data.frame")
>
> m1 <-
> lme( y ~ factor( meth ) + factor( item ),
>      random = ~1 | MI,
>      weights = varIdent( form = ~1 | meth ),
>      method ="REML",
>      data = cbind( dfr, MI=interaction( dfr$meth, dfr$item ) ) )
>
> m1
>
> # The MI std and the residual std for meth==1
> as.numeric(VarCorr(m1)[,2])
>
> ______________________________________________
>
> Bendix Carstensen
> Senior Statistician
> Steno Diabetes Center
> Niels Steensens Vej 2-4
> DK-2820 Gentofte
> Denmark
> +45 44 43 87 38 (direct)
> +45 30 75 87 38 (mobile)
> +45 44 43 73 13 (fax)
> bxc at steno.dk   http://www.biostat.ku.dk/~bxc
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pls at mevik.net  Tue Jan  2 13:36:32 2007
From: pls at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik_and_Ron_Wehrens?=)
Date: Tue, 02 Jan 2007 13:36:32 +0100
Subject: [R] [R-pkgs] pls version 2.0-0
Message-ID: <m0sleta9m7.fsf@bar.nemo-project.org>

Version 2.0-0 of the pls package is now available on CRAN.

The pls package implements partial least squares regression (PLSR) and
principal component regression (PCR).  Features of the package include

- Several plsr algorithms: orthogonal scores, kernel pls and simpls
- Flexible cross-validation
- A formula interface, with traditional methods like predict, coef,
  plot and summary
- Functions for extraction of scores and loadings, and calculation of
  (R)MSEP and R2
- Functions for plotting predictions, validation statistics,
  coefficients, scores, loadings, biplots and correlation loadings.

The main changes since 1.2-0 are

- There is now an options mechanism for selecting default fit algorithms.
  See ?pls.options.
- loadingplot() and coefplot() now try to be more intelligent when plotting
  x axis labels.
- The handling of factors in X has been improved, by changing the way the
  intercept is removed from the model matrix.
- All PLSR and PCR algorithms, as well as mvrCv(), have been optimised.
  Depending on the algorithm used, the size of the matrices, and the number
  of components used, one can expect from 5% to 65% reduction in
  computation time.
- Scaling of scores and loadings of kernel PLS and svd PCR algorithm has
  changed.  They are now scaled using the `classic' scaling found in
  oscorespls.
- The arguments `ncomp' now always means "number of components", and `comps'
  always means "component number".  The argument `cumulative' has been
  removed.
- A new data set 'gasoline' has been included.
- The 'NIR' and 'sensory' data sets have been renamed to 'yarn' and 'oliveoil'.


See the file CHANGES in the sources for all changes.

-- 
Bj?rn-Helge Mevik

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From bcutayar at lfdj.com  Tue Jan  2 16:01:28 2007
From: bcutayar at lfdj.com (bcutayar at lfdj.com)
Date: Tue, 02 Jan 2007 16:01:28 +0100
Subject: [R] =?iso-8859-1?q?en_cong=E9?=
Message-ID: <OF53A3666B.3D2D40CF-ONC1257257.00528845-C1257257.00528846@lfdj.com>




Je serai absent(e) ? partir du  02/01/2007 de retour le 08/01/2007.

Vous pouvez contacter M.Bidart en mon absence.

>>>>>>>>>>>>>    Bonne ann?e 2007  <<<<<<<<<<


Si vous n'etes pas destinataires de ce message, merci d'aver...{{dropped}}


From bbands at gmail.com  Tue Jan  2 16:15:58 2007
From: bbands at gmail.com (BBands)
Date: Tue, 2 Jan 2007 07:15:58 -0800
Subject: [R] Does SQL group by have a heavy duty equivalent in R
In-Reply-To: <Pine.LNX.4.64.0701020119280.6443@gannet.stats.ox.ac.uk>
References: <bd93cdad0612302317j674e7e4dk257ac5707964f1@mail.gmail.com>
	<6e8360ad0701011041m13519144tc6287b6a2899d9f@mail.gmail.com>
	<Pine.LNX.4.64.0701020119280.6443@gannet.stats.ox.ac.uk>
Message-ID: <6e8360ad0701020715x2966c1faod9284186c23c9e9e@mail.gmail.com>

On 1/1/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> And it works perfectly well with RODBC, so you don't need yet
> another package.  RODBC even comes with SQLite examples.

Thanks, I hadn't thought of that and it should simplify things as we
are already using RODBC for MySQL.

> BTW, 'fast' is rather misleading: SQLite is often fast for small tasks,
> but can be very slow compared to a more serious DBMS.

Agreed. We switch to MySQL for larger jobs.

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From P.Dalgaard at biostat.ku.dk  Tue Jan  2 16:18:25 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 02 Jan 2007 16:18:25 +0100
Subject: [R] RMySQL adn FC6 - install fail
In-Reply-To: <1167745598.3427.34.camel@home>
References: <1167745598.3427.34.camel@home>
Message-ID: <459A77C1.6020406@biostat.ku.dk>

R Gott wrote:

(With a name like that you'd better get good at this....)  ;-)
> Dear anybody
>
> I am running Fedora Core 6 with the latest version of R.
>
> When I try to install the package RMySQL I get this message:
>
>         * Installing *source* package 'RMySQL' ...
>         creating cache ./config.cache
>         checking how to run the C preprocessor... /lib/cpp
>         checking for compress in -lz... no
>         checking for getopt_long in -lc... no
>         checking for mysql_init in -lmysqlclient... no
>         checking for mysql.h... no
>         checking for mysql_init in -lmysqlclient... no
>         checking for mysql_init in -lmysqlclient... no
>         checking for mysql_init in -lmysqlclient... no
>         checking for mysql_init in -lmysqlclient... no
>         checking for mysql_init in -lmysqlclient... no
>         
>         Configuration error:
>            Could not locate the library "libz" required by MySQL.
>         
>         INSTRUCTIONS:
>         
>            The "libz" library is required by the MySQL client library
>            in order to compress/uncompress connections between clients
>            and the MySQL engine.
>         
>            Make sure you have "libz" installed properly and/or included
>            in your $LD_LIBRARY_PATH.  Perhaps it is not in any of the
>            standard directories (e.g., /usr/lib/, /usr/local/lib)?
>         
>         Aborting the installation of RMySQL.
>
> I haev checked my installation adn foudn teh follwing files:
>
>         /usr/lib/libz.so (whcih points to)
>         /usr/lib/libz.so.1.2
>         
>   
(not  .so.1.2.3 ?)
> Any help woudl be appreciated.  I couldn't find anytign in teh archives.
>
> Richard
>
>   

Looks like I have

[pd at janus ~]$ rpm -qf /usr/lib64/libz.so
zlib-devel-1.2.3-3
[pd at janus ~]$ rpm -q zlib
zlib-1.2.3-3


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From s-walker at ti.com  Tue Jan  2 16:28:20 2007
From: s-walker at ti.com (Walker, Sam)
Date: Tue, 2 Jan 2007 09:28:20 -0600
Subject: [R] lattice panel.linejoin type question
Message-ID: <76E525F192FF454A9712272E207B714B3AF6E7@dlee12.ent.ti.com>

Hello R Users!

I'm trying to use the panel.linejoin function to draw points and lines
(type="o") but it wouldn't do it.  Modifying the panel.linejoin function
as such (adding type argument to the panel.lines call).
 
*original*
        panel.lines(vals[xx], yy, col = col.line, lty = lty, 
            lwd = lwd, ...)
*modified
        panel.lines(vals[xx], yy, col = col.line, lty = lty, 
            lwd = lwd, type, ...)


Then passing the type="o" in the panel.linejoin call as such works.
xyplot(
	...
	,panel.groups=function(x,y,subscripts,...) {
 		panel.linejoin(x,y,fun=mean,type="o",...)
	}
)

However, if I don't add the "type" in the panel.lines call, I get a
"formal argument "type" matched by multiple actual arguments" Error.

It seems like overkill to redefine panel.lines, is there a simpler way?


R 2.4.1 Windows XP
Lattice 0.14-16

Best Regards,
Sam


From webmaster at xen.net  Tue Jan  2 17:02:26 2007
From: webmaster at xen.net (Ricardo =?ISO-8859-1?Q?Rodr=EDguez=20-=20Your=20XEN=20ICT=20Team?=)
Date: Tue, 2 Jan 2007 17:02:26 +0100
Subject: [R] graphical parameters: margins
Message-ID: <20070102T170226Z_C5AC00020000@xen.net>

Hi all,
 
Please, while using image() which is the graphical parameter which control the space between ylab and the y axis? I do need to write a number of relatively long y labels and I am not able the control, if possible, this space.

See the effect I need to avoid...

http://nvx.environmentalchange.net/@rrodriguez/images/overlapping.jpg

Thanks for your help,

Ricardo

--
Ricardo Rodr?guez
Your XEN ICT Team


From Max.Kuhn at pfizer.com  Tue Jan  2 17:26:31 2007
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Tue, 2 Jan 2007 11:26:31 -0500
Subject: [R] bug in odfWeave
In-Reply-To: <458CB886.30409@mail.jci.tju.edu>
Message-ID: <71257D09F114DA4A8E134DEAC70F25D307120277@groamrexm03.amer.pfizer.com>

I've verified this bug and will be releasing a new version shortly.

Max 

-----Original Message-----
From: Abhijit Dasgupta [mailto:Abhijit.Dasgupta at mail.jci.tju.edu] 
Sent: Saturday, December 23, 2006 12:03 AM
To: r-help at stat.math.ethz.ch; Kuhn, Max
Subject: bug in odfWeave

Hi,

I think there is a minor bug in odfWeave. In the function odfStyleGen, 
the following line has an extra "=":

           if(length(grep("italic", thisStyle$fontType)))
               fontText <- c(fontText, tagattr("fo:font-style=",
"italic"))
 
This is creating an error if some text needs to be formatted as italic, 
since the corresponding entry in style.xml is "fo:font-style==italic". 
For the windows version, which I'm using, I can't correct since the R 
files are packaged in odfWeave.rdb (or at least I don't know how). 
Hopefully Max Kuhn or someone can update the windows binary to correct 
this.

Abhijit Dasgupta

PS: I found the bug after downloading the source files.
----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From therneau at mayo.edu  Tue Jan  2 17:29:32 2007
From: therneau at mayo.edu (Terry Therneau)
Date: Tue, 2 Jan 2007 10:29:32 -0600 (CST)
Subject: [R] Survfit with a coxph object
Message-ID: <200701021629.l02GTWi15273@hsrnfs-101.mayo.edu>


> When I run coxph I get the coxph object back fairly quickly,
> however when I try to run survfit  it does not come back.

 If you are very, very patient the routine will come back eventually.  
Unfortunately, for some very large data sets this could be months...

   The reason is that the algorithms for coxph have been carefully optimized
over the years, but survfit is used so much less frequently that I have not
propogated many of these improvements forward to that routine.  In particular,
there is a computation which is O(d*n) if done in the obvious way, but O(2n)
when approached more cleverly; where d=number of events and n= number of
observations in the data set.  Your example has d ~ 50,000 and n~ 100,000, so
I would expect survfit.coxph to be roughly 20000 times slower than coxph.

  The long term solution is for me to fix this.  It's a couple of week's work,
if I can only find the weeks to do it.  The mid term one is to take Frank
Harrell's suggestion.  If standard errors are not needed, there is an O(n)
algorithm, which he has implemented as part of his additions to the coxph
suite.

	Terry Therneau


From gavin.simpson at ucl.ac.uk  Tue Jan  2 17:44:11 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 02 Jan 2007 16:44:11 +0000
Subject: [R] graphical parameters: margins
In-Reply-To: <20070102T170226Z_C5AC00020000@xen.net>
References: <20070102T170226Z_C5AC00020000@xen.net>
Message-ID: <1167756251.8577.45.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2007-01-02 at 17:02 +0100, Ricardo Rodr?guez - Your XEN ICT Team
wrote:
> Hi all,
>  
> Please, while using image() which is the graphical parameter which
> control the space between ylab and the y axis? I do need to write a
> number of relatively long y labels and I am not able the control, if
> possible, this space.
> 
> See the effect I need to avoid...
> 
> http://nvx.environmentalchange.net/@rrodriguez/images/overlapping.jpg
> 
> Thanks for your help,
> 
> Ricardo

Either of these two gives you the answer

> help.search("graphical parameters")
> RSiteSearch("graphical parameters margin")

more specifically, read ?par and in particular, the entry for parameter
'mar' and it's relatives.

You might also need to add the axis label separately from the figure:

opar <- par(mar = c(5,7,4,2) +0.1)
plot(1:10, ann = FALSE) # or plot(1:10, ylab = "")
mtext("label", side = 2, line = 6)
par(opar)

1) opar <- par(mar = c(5,7,4,2) +0.1) creates 7.1 lines on the left of
the plot and saves defaults
2) mtext("label", side = 2, line = 6) displays the axis label on line 6
to push it away from the plot axis. Repeat for other sides...
3) par(opar) resets to the defaults.

HTH

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From minya.pu at gmail.com  Tue Jan  2 18:20:32 2007
From: minya.pu at gmail.com (Minya Pu)
Date: Tue, 2 Jan 2007 09:20:32 -0800
Subject: [R] user-specified random effects design matrix in glmmPQL?
Message-ID: <c1352e4a0701020920j67fd51adj3b30f40f8cd17132@mail.gmail.com>

Hi,
 I want to do a logistic regression model with random effects but I
need to sepcify my own design matrix for the random effects. Can I do
it in glmmPQL or anything similar? If so, how?

Thanks,
Minya


From rkoenker at uiuc.edu  Tue Jan  2 18:20:10 2007
From: rkoenker at uiuc.edu (roger koenker)
Date: Tue, 2 Jan 2007 11:20:10 -0600
Subject: [R] package dependency tree
Message-ID: <FF501BA1-95AE-48D5-BF42-FDF1CA71E187@uiuc.edu>

Is there a painless way to find the names of all packages on CRAN
that "Depend" on a specified package?


url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


From gzhu at peak6.com  Tue Jan  2 18:46:48 2007
From: gzhu at peak6.com (Geoffrey Zhu)
Date: Tue, 2 Jan 2007 11:46:48 -0600
Subject: [R] R modules
Message-ID: <99F81FFD0EA54E4DA8D4F1BFE272F34103043457@ppi-mail1.chicago.peak6.net>

Hi All,

I'd like to know what is the best way to organize R code in multiple modules=
 and files. The R code we are writing is too much for a single file. Besides=
, there are a lot of reusable functions we'd like to factor out. But writing=
 a package for that is quite an over-kill and might be too inflexible. 

So what is the best way to organize R code into multiple files and reusable=
 modules?

Thanks,
Geoffrey



_______________________________________________________=0A=
=0A=
=0A=
The information in this email or in any file attached hereto is=0A=
intended only for the personal and confidential use of the individual=0A=
or entity to which it is addressed and may contain information that is=0A=
proprietary and confidential. If you are not the intended recipient of=0A=
this message you are hereby notified that any review, dissemination,=0A=
distribution or copying of this message is strictly prohibited. This communi=
cation is for information purposes only and should not be regarded as an off=
er to sell or as a solicitation of an offer to buy any financial product. Em=
ail transmission cannot be guaranteed to be secure or error-free.


From ggrothendieck at gmail.com  Tue Jan  2 18:49:01 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 2 Jan 2007 12:49:01 -0500
Subject: [R] package dependency tree
In-Reply-To: <FF501BA1-95AE-48D5-BF42-FDF1CA71E187@uiuc.edu>
References: <FF501BA1-95AE-48D5-BF42-FDF1CA71E187@uiuc.edu>
Message-ID: <971536df0701020949v33708c39p918df22f8f41993e@mail.gmail.com>

Try this, noting that available.packages() returns a matrix whose columns
include "Depends" and "Suggests" and whose rownames are the package
names:

> AP <- available.packages()
> rownames(AP)[grep("quantreg", AP[, "Depends"])]
[1] "cobs"    "emplik"  "lss"     "rankreg" "rqmcmb2"
> rownames(AP)[grep("quantreg", AP[, "Suggests"])]
[1] "apTreeshape" "diveMove"    "dyn"         "ggplot"      "gsubfn"
[6] "np"

On 1/2/07, roger koenker <rkoenker at uiuc.edu> wrote:
> Is there a painless way to find the names of all packages on CRAN
> that "Depend" on a specified package?
>
>
> url:    www.econ.uiuc.edu/~roger            Roger Koenker
> email    rkoenker at uiuc.edu            Department of Economics
> vox:     217-333-4558                University of Illinois
> fax:       217-244-6678                Champaign, IL 61820
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Tue Jan  2 18:57:05 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 2 Jan 2007 12:57:05 -0500
Subject: [R] R modules
In-Reply-To: <99F81FFD0EA54E4DA8D4F1BFE272F34103043457@ppi-mail1.chicago.peak6.net>
References: <99F81FFD0EA54E4DA8D4F1BFE272F34103043457@ppi-mail1.chicago.peak6.net>
Message-ID: <971536df0701020957s6e37c0cfx8cd5ade7edb95a32@mail.gmail.com>

You could have a master file that sources the rest:

source("A.R")
source("B.R")

or you could save the function in a workspace and just load it back in;
however, at some point I would move that all to a package.  I typically
will create a package for myself even if I have no intention of distributing
it since it nicely puts together all the source and documentation and the
checking tools are helpful to improve the QA.

Its a bit of work to figure it all out the first time but once you
are over that hump its worth it.

On 1/2/07, Geoffrey Zhu <gzhu at peak6.com> wrote:
> Hi All,
>
> I'd like to know what is the best way to organize R code in multiple modules=
>  and files. The R code we are writing is too much for a single file. Besides=
> , there are a lot of reusable functions we'd like to factor out. But writing=
>  a package for that is quite an over-kill and might be too inflexible.
>
> So what is the best way to organize R code into multiple files and reusable=
>  modules?
>
> Thanks,
> Geoffrey
>
>
>
> _______________________________________________________=0A=
> =0A=
> =0A=
> The information in this email or in any file attached hereto is=0A=
> intended only for the personal and confidential use of the individual=0A=
> or entity to which it is addressed and may contain information that is=0A=
> proprietary and confidential. If you are not the intended recipient of=0A=
> this message you are hereby notified that any review, dissemination,=0A=
> distribution or copying of this message is strictly prohibited. This communi=
> cation is for information purposes only and should not be regarded as an off=
> er to sell or as a solicitation of an offer to buy any financial product. Em=
> ail transmission cannot be guaranteed to be secure or error-free.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Matthias.Kohl at stamats.de  Tue Jan  2 18:58:47 2007
From: Matthias.Kohl at stamats.de (Matthias Kohl)
Date: Tue, 2 Jan 2007 18:58:47 +0100 (MET)
Subject: [R] package dependency tree
In-Reply-To: <971536df0701020949v33708c39p918df22f8f41993e@mail.gmail.com>
References: <FF501BA1-95AE-48D5-BF42-FDF1CA71E187@uiuc.edu>
	<971536df0701020949v33708c39p918df22f8f41993e@mail.gmail.com>
Message-ID: <459A9D55.7080303@stamats.de>

Hello,

http://bioconductor.org/packages/1.9/bioc/html/pkgDepTools.html
resp.
http://bioconductor.org/packages/2.0/bioc/html/pkgDepTools.html
may help you.

Best regards,
Matthias

Gabor Grothendieck schrieb:
> Try this, noting that available.packages() returns a matrix whose columns
> include "Depends" and "Suggests" and whose rownames are the package
> names:
>
>   
>> AP <- available.packages()
>> rownames(AP)[grep("quantreg", AP[, "Depends"])]
>>     
> [1] "cobs"    "emplik"  "lss"     "rankreg" "rqmcmb2"
>   
>> rownames(AP)[grep("quantreg", AP[, "Suggests"])]
>>     
> [1] "apTreeshape" "diveMove"    "dyn"         "ggplot"      "gsubfn"
> [6] "np"
>
> On 1/2/07, roger koenker <rkoenker at uiuc.edu> wrote:
>   
>> Is there a painless way to find the names of all packages on CRAN
>> that "Depend" on a specified package?
>>
>>
>> url:    www.econ.uiuc.edu/~roger            Roger Koenker
>> email    rkoenker at uiuc.edu            Department of Economics
>> vox:     217-333-4558                University of Illinois
>> fax:       217-244-6678                Champaign, IL 61820
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
Dr. rer. nat. Matthias Kohl
E-Mail: matthias.kohl at stamats.de
Home: www.stamats.de


From spluque at gmail.com  Tue Jan  2 19:00:19 2007
From: spluque at gmail.com (Sebastian P. Luque)
Date: Tue, 02 Jan 2007 12:00:19 -0600
Subject: [R] package dependency tree
References: <FF501BA1-95AE-48D5-BF42-FDF1CA71E187@uiuc.edu>
Message-ID: <877iw548cs.fsf@patagonia.sebmags.homelinux.org>

On Tue, 2 Jan 2007 11:20:10 -0600,
roger koenker <rkoenker at uiuc.edu> wrote:

> Is there a painless way to find the names of all packages on CRAN that
> "Depend" on a specified package?

Maybe this is not too painful:


pkgs <- available.packages()            # repos arg may be useful here
pkgs.dpnd <- pkgs[grep("quantreg", pkgs[, "Depends"]), ]
names(pkgs.dpnd)


-- 
Seb


From P.Dalgaard at biostat.ku.dk  Tue Jan  2 19:07:07 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 02 Jan 2007 19:07:07 +0100
Subject: [R] package dependency tree
In-Reply-To: <FF501BA1-95AE-48D5-BF42-FDF1CA71E187@uiuc.edu>
References: <FF501BA1-95AE-48D5-BF42-FDF1CA71E187@uiuc.edu>
Message-ID: <459A9F4B.2040101@biostat.ku.dk>

roger koenker wrote:
> Is there a painless way to find the names of all packages on CRAN
> that "Depend" on a specified package?
>   
Depends on how accurately you need them. These *probably* depend on "boot"

> x <- available.packages()

> rownames(x)[grep("boot",x[,"Depends"])]

 [1] "circular"     "cramer"       "DCluster"     "equivalence"  "np"

 [6] "pastecs"      "relaimpo"     "sensitivity"  "simpleboot"   "spdep"

[11] "survrec"      "titan"        "verification" "Zelig"

This allows visual inspection of the Depends fields too:

x[grep("boot",x[,"Depends"]), "Depends", drop=F]    

(There could have been dependencies on, say, simpleboot). For increased precision, 
you'll need to grep more carefully, for "\\<boot\\>". Finally, catching indirect 
dependencies requires iterative application, something like this:

> f <- function(s) rownames(x)[grep(paste("\\<",s,"\\>",sep=""),x[,"Depends"])]
> (y <- f("boot"))
 [1] "circular"     "cramer"       "DCluster"     "equivalence"  "np"
 [6] "pastecs"      "relaimpo"     "sensitivity"  "simpleboot"   "spdep"
[11] "survrec"      "titan"        "verification" "Zelig"
> (y0 <- unlist(lapply(y,f)))
[1] "wle"      "DCluster" "svcR"     "gcmrec"   "VDCutil"
> (y0 <- setdiff(y0,y))
[1] "wle"     "svcR"    "gcmrec"  "VDCutil"
> (y <- union(y, y0))
 [1] "circular"     "cramer"       "DCluster"     "equivalence"  "np"
 [6] "pastecs"      "relaimpo"     "sensitivity"  "simpleboot"   "spdep"
[11] "survrec"      "titan"        "verification" "Zelig"        "wle"
[16] "svcR"         "gcmrec"       "VDCutil"
> (y0 <- unlist(lapply(y0,f)))
character(0)

...and stop at this point since there are no more dependents. Otherwise, repeat.  
 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From deepayan.sarkar at gmail.com  Tue Jan  2 19:09:16 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 2 Jan 2007 10:09:16 -0800
Subject: [R] lattice panel.linejoin type question
In-Reply-To: <76E525F192FF454A9712272E207B714B3AF6E7@dlee12.ent.ti.com>
References: <76E525F192FF454A9712272E207B714B3AF6E7@dlee12.ent.ti.com>
Message-ID: <eb555e660701021009m3ab5bf8aw3cdc3bfe8b3e3f6d@mail.gmail.com>

On 1/2/07, Walker, Sam <s-walker at ti.com> wrote:
> Hello R Users!
>
> I'm trying to use the panel.linejoin function to draw points and lines
> (type="o") but it wouldn't do it.  Modifying the panel.linejoin function
> as such (adding type argument to the panel.lines call).
>
> *original*
>         panel.lines(vals[xx], yy, col = col.line, lty = lty,
>             lwd = lwd, ...)
> *modified
>         panel.lines(vals[xx], yy, col = col.line, lty = lty,
>             lwd = lwd, type, ...)

I think I'll just remove 'type' from the argument list of
'panel.linejoin', unless I see any problems with that change.

Deepayan

>
>
> Then passing the type="o" in the panel.linejoin call as such works.
> xyplot(
>         ...
>         ,panel.groups=function(x,y,subscripts,...) {
>                 panel.linejoin(x,y,fun=mean,type="o",...)
>         }
> )
>
> However, if I don't add the "type" in the panel.lines call, I get a
> "formal argument "type" matched by multiple actual arguments" Error.
>
> It seems like overkill to redefine panel.lines, is there a simpler way?
>
>
> R 2.4.1 Windows XP
> Lattice 0.14-16
>
> Best Regards,
> Sam
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Tue Jan  2 19:13:13 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Jan 2007 18:13:13 +0000 (GMT)
Subject: [R] package dependency tree
In-Reply-To: <FF501BA1-95AE-48D5-BF42-FDF1CA71E187@uiuc.edu>
References: <FF501BA1-95AE-48D5-BF42-FDF1CA71E187@uiuc.edu>
Message-ID: <Pine.LNX.4.64.0701021802380.28558@gannet.stats.ox.ac.uk>

On Tue, 2 Jan 2007, roger koenker wrote:

> Is there a painless way to find the names of all packages on CRAN
> that "Depend" on a specified package?

Assuming you have just CRAN in your selected repositories:

> foo <- available.packages()
> deps <- strsplit(foo[, "Depends"], ",[[:space:]]*")
> names(deps)[sapply(deps, function(x) "quantreg" %in% x)]
[1] "cobs"    "emplik"  "lss"     "pheno"   "rankreg" "rqmcmb2"

seems painless enough.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From murdoch at stats.uwo.ca  Tue Jan  2 19:37:02 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 02 Jan 2007 13:37:02 -0500
Subject: [R] R modules
In-Reply-To: <99F81FFD0EA54E4DA8D4F1BFE272F34103043457@ppi-mail1.chicago.peak6.net>
References: <99F81FFD0EA54E4DA8D4F1BFE272F34103043457@ppi-mail1.chicago.peak6.net>
Message-ID: <459AA64E.5070406@stats.uwo.ca>

On 1/2/2007 12:46 PM, Geoffrey Zhu wrote:
> Hi All,
> 
> I'd like to know what is the best way to organize R code in multiple modules=
>  and files. The R code we are writing is too much for a single file. Besides=
> , there are a lot of reusable functions we'd like to factor out. But writing=
>  a package for that is quite an over-kill and might be too inflexible. 
> 
> So what is the best way to organize R code into multiple files and reusable=
>  modules?

Write a package for it.  It's not overkill, it's using the tools the way 
they were designed to be used.

Not all packages need to be submitted to CRAN, you can have a package 
for your own use.  It gives you a way to organize your code and 
documentation, numerous code checking tools, namespaces, all sorts of 
good stuff.

Duncan Murdoch


From sfalcon at fhcrc.org  Tue Jan  2 19:55:18 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 02 Jan 2007 10:55:18 -0800
Subject: [R] package dependency tree
In-Reply-To: <459A9D55.7080303@stamats.de> (Matthias Kohl's message of "Tue,
	2 Jan 2007 18:58:47 +0100 (MET)")
References: <FF501BA1-95AE-48D5-BF42-FDF1CA71E187@uiuc.edu>
	<971536df0701020949v33708c39p918df22f8f41993e@mail.gmail.com>
	<459A9D55.7080303@stamats.de>
Message-ID: <m2k605uull.fsf@fhcrc.org>

Matthias Kohl <Matthias.Kohl at stamats.de> writes:

> Hello,
>
> http://bioconductor.org/packages/1.9/bioc/html/pkgDepTools.html
> resp.
> http://bioconductor.org/packages/2.0/bioc/html/pkgDepTools.html
> may help you.

[shameless plug]

Last Month's RNews has an article that demonstrates the pkgDepTools
package:

[144] Seth Falcon. Modeling package dependencies using graphs. R News,
6(5):8-12, December 2006.

+ seth


From fissell+ at pitt.edu  Tue Jan  2 20:46:29 2007
From: fissell+ at pitt.edu (fissell+ at pitt.edu)
Date: Tue, 02 Jan 2007 14:46:29 -0500 (EST)
Subject: [R] arith-true mean() fails make check on IRIX
Message-ID: <ML-3.4.1167767189.5627.fissell@kraepelin.wpic.pitt.edu>

Hello,

I am building R-2.4.1 on an SGI IRIX platform, using gcc 3.3.
gmake check failed, and the arith-true.Rout.fail file indicated:
> is.na(mean(c(1,NA,NA)[-1], trim = .1, na.rm = TRUE))
[1] FALSE
>

I tried the mean() command in R and got:
> mean(c(1,NA,NA)[-1], trim = .1, na.rm = TRUE) 
[1] Inf

I think the problem is in the na.rm = TRUE, because I get:
> mean(c(1,NA,NA)[-1])
[1] NA

How serious is this problem, and is there a workaround ?

thank you
Kate





------------------------------
Kate Fissell
Systems Analyst
University of Pittsburgh
3939 O'Hara St.
Pittsburgh PA 15260
(412) 624-5279
fax: (412) 624-9149
fissell+ at pitt.edu


From michael_graber at gmx.de  Tue Jan  2 21:43:26 2007
From: michael_graber at gmx.de (Michael Graber)
Date: Tue, 02 Jan 2007 21:43:26 +0100
Subject: [R] Geometric Brownian Process
Message-ID: <459AC3EE.2050207@gmx.de>

Dear R People,

Consider I have 3 realizations of an Geometric Brownian process.
Now i want to overlay  the plot of these realizations. In a future point 
in time a probability density curve  in this specific point of time 
should overlay this plot ( view rotated 90?).
I am sorry for not providing any source code. Can anybody point mo to an 
package, or has anybody an idea how to simulate an geometric brownian 
process in R?

Thanks in advance,

Michael Graber


From blomsp at ozemail.com.au  Tue Jan  2 23:24:42 2007
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Wed, 03 Jan 2007 09:24:42 +1100
Subject: [R] SNA Matrix
In-Reply-To: <2DA68F10815D6E49956E3DC2CE18603E91666D@SRVSTAFF.iese.org>
References: <2DA68F10815D6E49956E3DC2CE18603E91666D@SRVSTAFF.iese.org>
Message-ID: <459ADBAA.5050206@ozemail.com.au>

?source

Salvaj, Erica wrote:
> Hello
>  
> I export a one mode network from Pajek to R, and the former made an .r file  called PajekR.r, that is actually an script to be run in R
> The problem is that what the file actually does is to set a 0 martrix and then assing to each pair of nodes the corresponding values. Since the matrix is huge (10.000 nodes) a copy/paste procedure is very time consuming, and I guess pretty inefficient. So the question is: are there any way to read directly the .r file in R, without copy and paste the sentences of the script in the R console?
>  
> Erica H. Salvaj
> PhD Candidate
> IESE Business School
> docesalvaj at iese.edu
>  
>
>
> This message has been scanned for viruses by TRENDMICRO,\ an...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>   
-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat. 
Centre for Resource and Environmental Studies
The Australian National University              
Canberra ACT 0200                               
Australia                                       
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C

The combination of some data and an aching desire for 
an answer does not ensure that a reasonable answer 
can be extracted from a given body of data.
- John Tukey.


From larsfromspace at web.de  Tue Jan  2 23:50:30 2007
From: larsfromspace at web.de (downunder03)
Date: Tue, 2 Jan 2007 14:50:30 -0800 (PST)
Subject: [R]  Problem labeling arrays
Message-ID: <8132366.post@talk.nabble.com>


Hi all. I'am sitting for hours. I have a problem labelling arrays. I am
loosing labels for higher dimensions than matrices. Has anyone already faced
that problem? There must be some direct way without writing complicated
functions. Thanks for any hint. greetings lars


x<-matrix(sin(1:90),nrow=100,ncol=9)
dimnames(x)[[2]]<-c("F1_5", "F2_5", "F3_5","F4_5", "F5_5","F6_5", "F7_5",
"F8_5", "F9_5")

partialCorr <- function (cond, mat)
        cor(qr.resid(qr(cbind(1, mat[, cond])), mat[, -cond])) 

# matrix keep label
d=partialCorr(c(3,2),x)
d

# array looses label
bsp1 <- array(0, c(7,7,36))
k <- 0
for(i in 1:8) for(j in (i+1):9) {
    k <- k+1
    bsp1[, , k] <- partialCorr(c(i, j), x)
} 
bsp1
# I tried something to label inside the loop
bsp1 <- array(0, c(7,7,36))
k <- 0
for(i in 1:8) for(j in (i+1):9) {
    k <- k+1
    bsp1[, , k] <- partialCorr(c(i, j), x)
    dimnames(bsp1[,,k])=dimnames(partialCorr(c(i,j), x))
} 
bsp1
# I tried something to label outside the loop but can't directy adress the
array
labels <- array(0, c(7,1,36))
k <- 0
for(i in 1:8) for(j in (i+1):9) {
    k <- k+1
    labels[, , k] <- as.vector(rownames(partialCorr(c(i,j), x)))
}
labels
for (k in 1:36){
dimnames(bsp1[,,k])=list(c(labels[,1,k]),c(labels[,1,k]))
}
bsp1

-- 
View this message in context: http://www.nabble.com/-R--Problem-labeling-arrays-tf2910585.html#a8132366
Sent from the R help mailing list archive at Nabble.com.


From larsfromspace at web.de  Tue Jan  2 23:50:30 2007
From: larsfromspace at web.de (downunder03)
Date: Tue, 2 Jan 2007 14:50:30 -0800 (PST)
Subject: [R]  Problem labelling arrays
Message-ID: <8132366.post@talk.nabble.com>


Hi all. I'am sitting for hours. I have a problem labelling arrays. I am
loosing labels for higher dimensions than matrices. Has anyone already faced
that problem? There must be some direct way without writing complicated
functions. Thanks for any hint. greetings lars


x<-matrix(sin(1:90),nrow=100,ncol=9)
dimnames(x)[[2]]<-c("F1_5", "F2_5", "F3_5","F4_5", "F5_5","F6_5", "F7_5",
"F8_5", "F9_5")

partialCorr <- function (cond, mat)
        cor(qr.resid(qr(cbind(1, mat[, cond])), mat[, -cond])) 

# matrix keep label
d=partialCorr(c(3,2),x)
d

# array looses label
bsp1 <- array(0, c(7,7,36))
k <- 0
for(i in 1:8) for(j in (i+1):9) {
    k <- k+1
    bsp1[, , k] <- partialCorr(c(i, j), x)
} 
bsp1
# I tried something to label inside the loop
bsp1 <- array(0, c(7,7,36))
k <- 0
for(i in 1:8) for(j in (i+1):9) {
    k <- k+1
    bsp1[, , k] <- partialCorr(c(i, j), x)
    dimnames(bsp1[,,k])=dimnames(partialCorr(c(i,j), x))
} 
bsp1
# I tried something to label outside the loop but can't directy adress the
array
labels <- array(0, c(7,1,36))
k <- 0
for(i in 1:8) for(j in (i+1):9) {
    k <- k+1
    labels[, , k] <- as.vector(rownames(partialCorr(c(i,j), x)))
}
labels
for (k in 1:36){
dimnames(bsp1[,,k])=list(c(labels[,1,k]),c(labels[,1,k]))
}
bsp1

-- 
View this message in context: http://www.nabble.com/-R--Problem-labelling-arrays-tf2910585.html#a8132366
Sent from the R help mailing list archive at Nabble.com.


From blomsp at ozemail.com.au  Wed Jan  3 01:37:36 2007
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Wed, 03 Jan 2007 11:37:36 +1100
Subject: [R] Geometric Brownian Process
In-Reply-To: <459AC3EE.2050207@gmx.de>
References: <459AC3EE.2050207@gmx.de>
Message-ID: <459AFAD0.8030201@ozemail.com.au>

This function samples from the solution to the stochastic differential 
equation of the geometric wiener process:

rgwiener <- function (n=1, t=1, S0=1000, mu=0, sigma=1) {
    S0 * exp((mu - 1/2 * sigma^2) * t + sigma * sqrt(t) * rnorm(n))
}

To test this, note that the geometric wiener process has log(S_t/S0) ~ 
N((mu -1/2*sigma^2)* t, sigma^2 * t)

So if we let mu = 0, sigma = 1, t =1, S0 = 1000, log(S_t/S0) should be ~ 
Normal(-0.5, 1).

 > samp <- log(rgwiener(100000)/1000)
 > hist(samp)
 > library(MASS)
 > fitdistr(samp, "normal")

       mean            sd    
  -0.500553189    1.000032814
 ( 0.003162381) ( 0.002236141)

which looks good to me.

Because the geometric wiener process is a transformation of the ordinary 
wiener process, we can simulate it easily enough (apologies to the 
authors of package e1071, whose function rwiener I hacked):

gwiener <- function (mu=0, sigma=1, S0=1000, frequency=1000) {
z <- S0 * exp((mu - 1/2 * sigma^2) * seq(0, 1, length=frequency) +
    sigma * cumsum(rnorm(frequency)/sqrt(frequency)))
ts(z, start = 1/frequency, frequency=frequency)
}

 > plot(gwiener(), type="l")

Looks ok.

Cheers,

Simon.

Michael Graber wrote:
> Dear R People,
>
> Consider I have 3 realizations of an Geometric Brownian process.
> Now i want to overlay  the plot of these realizations. In a future point 
> in time a probability density curve  in this specific point of time 
> should overlay this plot ( view rotated 90?).
> I am sorry for not providing any source code. Can anybody point mo to an 
> package, or has anybody an idea how to simulate an geometric brownian 
> process in R?
>
> Thanks in advance,
>
> Michael Graber
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>   


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat. 
Centre for Resource and Environmental Studies
The Australian National University              
Canberra ACT 0200                               
Australia                                       
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C

The combination of some data and an aching desire for 
an answer does not ensure that a reasonable answer 
can be extracted from a given body of data.
- John Tukey.


From Soren.Hojsgaard at agrsci.dk  Wed Jan  3 02:23:29 2007
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Wed, 3 Jan 2007 02:23:29 +0100
Subject: [R] SQLite: When reading a table,
	a "\r" is padded onto the last column. Why?
References: <C83C5E3DEEE97E498B74729A33F6EAEC038785AA@DJFPOST01.djf.agrsci.dk>
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC038785AB@DJFPOST01.djf.agrsci.dk>

Hi,
 
I put the iris data into a SQLite database with 
 
dbWriteTable(con, "iris", iris, row.names=F, overwrite = T)

Then I retrieve data from the database with 
 
rs  <- dbSendQuery(con, "select * from iris")
d1  <- fetch(rs)
dbClearResult(rs)

Then I get 
> head(d1)
  Sepal_Length Sepal_Width Petal_Length Petal_Width  Species
1          5.1         3.5          1.4         0.2 setosa\r
2          4.9         3.0          1.4         0.2 setosa\r
3          4.7         3.2          1.3         0.2 setosa\r
4          4.6         3.1          1.5         0.2 setosa\r
5          5.0         3.6          1.4         0.2 setosa\r
6          5.4         3.9          1.7         0.4 setosa\r

Can anyone explain the extra "\r" at the end?  I am on Windows XP using R 2.4.1
Thanks in advance
S?ren


From hustqiufeng at sohu.com  Wed Jan  3 07:13:24 2007
From: hustqiufeng at sohu.com (Feng Qiu)
Date: Wed, 3 Jan 2007 01:13:24 -0500
Subject: [R] Any container in R?
References: <6.0.3.0.1.20070101112735.03159df8@mail.jcu.edu.au>
	<000a01c72db7$f7065180$6401a8c0@Aglog>
	<45992AB4.3070604@stats.uwo.ca>
	<003601c72dc1$1b68f5b0$6401a8c0@Aglog>
	<4599468C.2080206@stats.uwo.ca>
	<007301c72dd4$fa3a9b50$6401a8c0@Aglog>
	<Pine.LNX.4.64.0701011145580.30755@homer24.u.washington.edu>
Message-ID: <009b01c72efe$4c1a4770$6401a8c0@Aglog>

Hi Thomas:
           Thanks for your explanation.
           You are right that in C++, we try to use the same piece of codes 
on different type of objects. While in R, most operations are done in a 
"vector" way automatically, which reduced the need for containers.

Best,
Feng
----- Original Message ----- 
From: "Thomas Lumley" <tlumley at u.washington.edu>
To: "Feng Qiu" <hustqiufeng at sohu.com>
Cc: "Duncan Murdoch" <murdoch at stats.uwo.ca>; <r-help at stat.math.ethz.ch>
Sent: Monday, January 01, 2007 3:21 PM
Subject: Re: [R] Any container in R?


> On Mon, 1 Jan 2007, Feng Qiu wrote:
>
>> Hi Duncan:
>>         Thank you very much! I checked out unique(), it does exactly what 
>> I
>> want. But I'm still curious about if R provides "STL"(standard template
>> library).
>
> No.
>
> Some things the STL does aren't needed in R, others are implemented 
> differently, and others aren't implemented.
>
> One particularly important example is iterators, which will often either 
> happen invisibly due to vectorized operations or will be done with the 
> *apply family of functions.
>
> Your example could have been done either way. Using duplicated() is the 
> vectorized approach; the apply approach would use tapply().
>
> C++ is not terribly similar to R. A lot of the effort in STL is expended 
> on allowing a piece of code to be used on different types (where 
> appropriate). In R you have to expend effort on stopping a piece of code 
> being used on different types (where inappropriate).
>
>
>  -thomas
>
> Thomas Lumley Assoc. Professor, Biostatistics
> tlumley at u.washington.edu University of Washington, Seattle
>
>
>
>


From webmaster at xen.net  Tue Jan  2 23:37:45 2007
From: webmaster at xen.net (Ricardo =?ISO-8859-1?Q?Rodr=EDguez=20-=20Your=20XEN=20ICT=20Team?=)
Date: Tue, 2 Jan 2007 23:37:45 +0100
Subject: [R] graphical parameters: margins
References: 7191ff0f404b5f9b830a60cd1cd432ba
Message-ID: <20070102T233745Z_C5AC00020000@xen.net>



--
Ricardo Rodr?guez
Your XEN ICT Team

>>> Gavin Simpson<gavin.simpson at ucl.ac.uk> 2/1/2007 17:44 >>>

>Either of these two gives you the answer

> help.search("graphical parameters")
> RSiteSearch("graphical parameters margin")

>more specifically, read ?par and in particular, the entry for parameter
>'mar' and it's relatives.

>You might also need to add the axis label separately from the figure:

>opar <- par(mar = c(5,7,4,2) +0.1)
>plot(1:10, ann = FALSE) # or plot(1:10, ylab = "")
>mtext("label", side = 2, line = 6)
>par(opar)

>1) opar <- par(mar = c(5,7,4,2) +0.1) creates 7.1 lines on the left of
>the plot and saves defaults
>2) mtext("label", side = 2, line = 6) displays the axis label on line 6
>to push it away from the plot axis. Repeat for other sides...
>3) par(opar) resets to the defaults.

>HTH

Thanks Gavin,

I frequently reach the help page or any other document concerning the doubt, but at least for me it is by no means easy to correctly interpret their contents without the help of more experienced people. I do hope  I will catch up ASAP!

Cheers,

Ricardo

From ripley at stats.ox.ac.uk  Wed Jan  3 08:43:05 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 3 Jan 2007 07:43:05 +0000 (GMT)
Subject: [R] SQLite: When reading a table,
 a "\r" is padded onto the last column. Why?
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC038785AB@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC038785AA@DJFPOST01.djf.agrsci.dk>
	<C83C5E3DEEE97E498B74729A33F6EAEC038785AB@DJFPOST01.djf.agrsci.dk>
Message-ID: <Pine.LNX.4.64.0701030719120.25219@gannet.stats.ox.ac.uk>

I guess you are using package RSQLite without telling us (or telling us 
the version), and that your example is incomplete?

Using RSiteSearch("RSQLite Windows") quickly shows that this is a 
previously reported problem with the package, e.g.:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/72515.html

I believe the issue is that RSQLite actually writes out a CRLF-terminated 
text file and imports that into SQLite.  (I checked version 0.4-15.) It 
seems function safe.write() needs to be modified to write to a binary-mode 
connection since SQLite appears to require LF-terminated files.

Using RODBC to work with SQLite databases works correctly even under
Windows (and is much more efficient at writing to the database).

[I am not sure who is actually maintaining RSQLite, so am Cc: both the 
stated maintainer and the person who prepared the package for 
distribution. The posting guide asked you to contact the maintainer: what 
response did _you_ get?]


On Wed, 3 Jan 2007, S?ren H?jsgaard wrote:

> Hi,
>
> I put the iris data into a SQLite database with
>
> dbWriteTable(con, "iris", iris, row.names=F, overwrite = T)
>
> Then I retrieve data from the database with
>
> rs  <- dbSendQuery(con, "select * from iris")
> d1  <- fetch(rs)
> dbClearResult(rs)
>
> Then I get
>> head(d1)
>  Sepal_Length Sepal_Width Petal_Length Petal_Width  Species
> 1          5.1         3.5          1.4         0.2 setosa\r
> 2          4.9         3.0          1.4         0.2 setosa\r
> 3          4.7         3.2          1.3         0.2 setosa\r
> 4          4.6         3.1          1.5         0.2 setosa\r
> 5          5.0         3.6          1.4         0.2 setosa\r
> 6          5.4         3.9          1.7         0.4 setosa\r
>
> Can anyone explain the extra "\r" at the end?  I am on Windows XP using R 2.4.1
> Thanks in advance
> S?ren
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Wed Jan  3 08:45:02 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 3 Jan 2007 07:45:02 +0000 (GMT)
Subject: [R] arith-true mean() fails make check on IRIX
In-Reply-To: <ML-3.4.1167767189.5627.fissell@kraepelin.wpic.pitt.edu>
References: <ML-3.4.1167767189.5627.fissell@kraepelin.wpic.pitt.edu>
Message-ID: <Pine.LNX.4.64.0701022029040.927@gannet.stats.ox.ac.uk>

On Tue, 2 Jan 2007, fissell+ at pitt.edu wrote:

> Hello,
>
> I am building R-2.4.1 on an SGI IRIX platform, using gcc 3.3.
> gmake check failed, and the arith-true.Rout.fail file indicated:
>> is.na(mean(c(1,NA,NA)[-1], trim = .1, na.rm = TRUE))
> [1] FALSE
>>
>
> I tried the mean() command in R and got:
>> mean(c(1,NA,NA)[-1], trim = .1, na.rm = TRUE)
> [1] Inf
>
> I think the problem is in the na.rm = TRUE, because I get:
>> mean(c(1,NA,NA)[-1])
> [1] NA
>
> How serious is this problem, and is there a workaround ?

It indicates that your compiler is getting the wrong answer when
computing 0./0, and that does look quite serious.  You should get the 
same answer as mean(numeric(0)), and that should be NaN.

As the compiler is very old, you might like to try updating it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From deepayan.sarkar at r-project.org  Tue Jan  2 20:03:09 2007
From: deepayan.sarkar at r-project.org (Deepayan Sarkar)
Date: Tue, 2 Jan 2007 11:03:09 -0800
Subject: [R] [R-pkgs] rcompletion update
Message-ID: <eb555e660701021103w7b62ce07y9e452ecac2f65052@mail.gmail.com>

Hi,

The rcompletion package, originally intended to provide completion for
readline-based R interfaces, has undergone a number of changes.  These
changes are summarised below:

  o Reorganisation:

    - The package has been split into two.  All the completion code
      has been moved to a pure R package called 'rcompgen'.
      'rcompletion' now requires 'rcompgen' and simply provides
      readline bindings that uses 'rcompgen' to generate possible
      completions.  (Source packages are available on CRAN, binaries
      should be available soon.)

    - The purpose of this reorganisation is to allow other backends to
      use the completion facilities provided by 'rcompgen', hopefully
      avoiding duplication of effort.  I'm happy to add further
      infrastructure to 'rcompgen' if that is helpful.

    - as a proof of concept, .../examples/altesscomp.el contains code
      that provides an alternative (using 'rcompgen') to ESS's
      built-in completion mechanism.  It should be enough to include
      the contents of this file in ~/.emacs (please read the comments at
      the end before doing so).  The file is also available at

      http://rcompletion.googlecode.com/svn/trunk/rcompgen/inst/examples/altesscomp.el

      I am particularly interested in feedback from ESS users
      regarding how this compares with the default mechanism in terms
      of speed (especially in older machines).

  o Hosting:

    - The project is now hosted on Google Code, at
      http://code.google.com/p/rcompletion/

    - (For those interested, this now also hosts my R bash_completion
      script)


  o New completion features:

    - when the token is determined to be the first argument of
      library() or require(), completion is done on _installed_
      package names.  This is disabled by default since the first call
      to installed.packages() can be slow (especially when using
      remote file systems).

    - when the token is determined to be the first argument of data(),
      completion is done on available data sets.

    - tokens after a question mark (?) match aliases in help topics
      rather than object names.  So, for example, ?INST will complete
      to ?INSTALL even though there is no object named INSTALL.

    - the old behaviour of appending a left-parenthesis to function
      names has been disabled by default, since this requires
      evaluation of the mode of _all_ matches, which is undesirable
      for lazy-loaded symbols.

As always, comments and suggestions are most welcome.

-Deepayan

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From attenka at utu.fi  Wed Jan  3 09:17:06 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Wed, 03 Jan 2007 10:17:06 +0200
Subject: [R] Hershey fonts for musical notation?
Message-ID: <f737c20326671.459b82a2@utu.fi>

Hi,

I'd like to know if it is possible to use Hershey vector fonts to create very primitive musical notation. 
If I can hang some whole notes on these lines

X11()
plot(0,0, xlim=c(0,10), ylim=c(0,10))
# Staves:
for (i in c(seq(from=2,to=2.8,by=0.2),seq(from=4,to=4.8,by=0.2)))
{
	abline(h=i)
}


it is enough.

Best wishes,

Atte Tenkanen
University of Turku, Finland
___________________________________________________________________
P.S.
By the way, right now the demo(Hershey) seems not to work in OSX version R 2.4.1. ...
I get a message

> i <- i + 1
Error in deparse(ei, control = c("showAttributes", "useSource")) : 
	invalid multibyte string


From ripley at stats.ox.ac.uk  Wed Jan  3 09:22:28 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 3 Jan 2007 08:22:28 +0000 (GMT)
Subject: [R] graphical parameters: margins
In-Reply-To: <20070102T233745Z_C5AC00020000@xen.net>
References: 7191ff0f404b5f9b830a60cd1cd432ba
	<20070102T233745Z_C5AC00020000@xen.net>
Message-ID: <Pine.LNX.4.64.0701030818480.27367@gannet.stats.ox.ac.uk>

In this particular case 'An Introduction to R' has a comprehensive 
description of graphical parameters with figures (as do all good books on 
S/R e.g. MASS4 - since it has the same first author).


On Tue, 2 Jan 2007, Ricardo Rodr?guez - Your XEN ICT Team wrote:

> --
> Ricardo Rodr?guez
> Your XEN ICT Team
>
>>>> Gavin Simpson<gavin.simpson at ucl.ac.uk> 2/1/2007 17:44 >>>
>
>> Either of these two gives you the answer
>
>> help.search("graphical parameters")
>> RSiteSearch("graphical parameters margin")
>
>> more specifically, read ?par and in particular, the entry for parameter
>> 'mar' and it's relatives.
>
>> You might also need to add the axis label separately from the figure:
>
>> opar <- par(mar = c(5,7,4,2) +0.1)
>> plot(1:10, ann = FALSE) # or plot(1:10, ylab = "")
>> mtext("label", side = 2, line = 6)
>> par(opar)
>
>> 1) opar <- par(mar = c(5,7,4,2) +0.1) creates 7.1 lines on the left of
>> the plot and saves defaults
>> 2) mtext("label", side = 2, line = 6) displays the axis label on line 6
>> to push it away from the plot axis. Repeat for other sides...
>> 3) par(opar) resets to the defaults.
>
>> HTH
>
> Thanks Gavin,
>
> I frequently reach the help page or any other document concerning the 
> doubt, but at least for me it is by no means easy to correctly interpret 
> their contents without the help of more experienced people. I do hope I 
> will catch up ASAP!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From c.declercq at orsnpdc.org  Wed Jan  3 09:40:01 2007
From: c.declercq at orsnpdc.org (Christophe Declercq)
Date: Wed, 3 Jan 2007 09:40:01 +0100
Subject: [R] Hershey fonts for musical notation?
In-Reply-To: <f737c20326671.459b82a2@utu.fi>
Message-ID: <20070103083958.D80BA342AA@smtp.nordnet.fr>

Hi, Atte 

> De : r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] De la part de Atte Tenkanen
> Envoy? : mercredi 3 janvier 2007 09:17
> Hi,
> 
> I'd like to know if it is possible to use Hershey vector 
> fonts to create very primitive musical notation. 
> [...]

There is an example of a music score produced with R and the Hershey fonts
in the book 'R Graphics', by Paul Murrell (Chapman & Hall/CRC, 2005), page
15.

The R Code is on the web page for the book:
http://www.stat.auckland.ac.nz/~paul/RGraphics/examples-stevemiller.R

BTW, I do a lot of things with R but for music scores I use the ABC language
(http://www.walshaw.plus.com/abc/). You can output PostScript from it with
Jef Moine's abcm2ps (http://moinejf.free.fr/).

Happy new year!

Christophe
-- 
Christophe Declercq, MD
Observatoire r?gional de la sant?
Nord-Pas-de-Calais
235, avenue de la recherche
BP 86 F-59373 LOOS CEDEX
Phone +33 3 20 15 49 24
Fax + 33 3 20 15 10 46
E-mail c.declercq at orsnpdc.org


From webmaster at xen.net  Wed Jan  3 10:41:30 2007
From: webmaster at xen.net (Ricardo =?ISO-8859-1?Q?Rodr=EDguez=20-=20Your=20XEN=20ICT=20Team?=)
Date: Wed, 3 Jan 2007 10:41:30 +0100
Subject: [R] graphical parameters: margins
References: 783cafa19b7dbf3cfcb7d545e0c5b30a
Message-ID: <20070103T104130Z_C5AC00020000@xen.net>

>>> Prof Brian Ripley<ripley at stats.ox.ac.uk> 3/1/2007 09:22 >>>

>In this particular case 'An Introduction to R' has a comprehensive 
>description of graphical parameters with figures (as do all good books on 
>S/R e.g. MASS4 - since it has the same first author).
 
Thanks, Brian,
 
I've reached both "Introduction to R", particularly http://cran.r-project.org/doc/manuals/R-intro.html#Figure-margins, and MASS4 at Springer website. I think it is worth I ask the Three Wise Men for MASS4 as a present!
 
Greetings,
 
Ricardo


From attenka at utu.fi  Wed Jan  3 10:42:22 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Wed, 03 Jan 2007 11:42:22 +0200
Subject: [R] Hershey fonts for musical notation?
In-Reply-To: <20070103083958.D80BA342AA@smtp.nordnet.fr>
References: <f737c20326671.459b82a2@utu.fi>
	<20070103083958.D80BA342AA@smtp.nordnet.fr>
Message-ID: <f7319ed227a95.459b969e@utu.fi>

Hello Christophe,

Thanks a lot! This is what I need. My purpose is to generate chords and I need interactive responses straight in R. I can output midi event lists as csv-files and convert them with a nice midicsv-program in linux- or OSX-console. Then it is easy to convert midi files to notes with Finale or other music notation program. 

Atte



> Hi, Atte 
> 
> > De : r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] De la part de Atte 
> Tenkanen> Envoy? : mercredi 3 janvier 2007 09:17
> > Hi,
> > 
> > I'd like to know if it is possible to use Hershey vector 
> > fonts to create very primitive musical notation. 
> > [...]
> 
> There is an example of a music score produced with R and the 
> Hershey fonts
> in the book 'R Graphics', by Paul Murrell (Chapman & Hall/CRC, 
> 2005), page
> 15.
> 
> The R Code is on the web page for the book:
> http://www.stat.auckland.ac.nz/~paul/RGraphics/examples-stevemiller.R
> 
> BTW, I do a lot of things with R but for music scores I use the ABC 
> language(http://www.walshaw.plus.com/abc/). You can output 
> PostScript from it with
> Jef Moine's abcm2ps (http://moinejf.free.fr/).
> 
> Happy new year!
> 
> Christophe
> -- 
> Christophe Declercq, MD
> Observatoire r?gional de la sant?
> Nord-Pas-de-Calais
> 235, avenue de la recherche
> BP 86 F-59373 LOOS CEDEX
> Phone +33 3 20 15 49 24
> Fax + 33 3 20 15 10 46
> E-mail c.declercq at orsnpdc.org
> 
>


From sgerster at student.ethz.ch  Wed Jan  3 11:22:33 2007
From: sgerster at student.ethz.ch (Gerster  Sarah)
Date: Wed, 3 Jan 2007 11:22:33 +0100
Subject: [R] optim
Message-ID: <E9D421793368D44FB81D3C438D300574034780F3@EX5.d.ethz.ch>

Hi!

I'm trying to figure out how to use optim... I get some really strange results, so I guess I got something wrong.

I defined the following function which should be minimized:
errorFunction <- function(localShifts,globalShift,fileName,experimentalPI,lambda)
{
  lambda <- 1/sqrt(147)
  # error <- abs(errHuber(localShifts,globalShift,
  #    "/home/sarah/Semesterarbeit/Sequences/R/R1593_filtered.data",3.48)) +
  #    sum(abs(localShifts))*lambda

  error <- sum(abs(localShifts))*lambda
  error # return the error to be minimized
}

Then I call optim:
par <- seq(length=9, from=0, by=0)
lambda <- 1/sqrt(147)
optim(par, errorFunction, gr=NULL, method="Nelder-Mead", hessian=FALSE,
        globalShift,
        "/home/sarah/Semesterarbeit/Sequences/R/R1593_filtered.data",
        experimentalPI=3.48, lambda = lambda)

The output is:
$par
[1]  0.56350964  0.56350964  0.56350964  0.56350964  0.00000000 -0.29515957
[7]  0.00569937  0.32543297  0.18615880

$value
[1] 0.2529198

$counts
function gradient 
      31       31 

$convergence
[1] 0

$message
[1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"

Warning messages:
1: bounds can only be used with method L-BFGS-B in: optim(par, errorFunction, gr = NULL, method = "Nelder-Mead",  
2: NAs introduced by coercion 



If I change my "error-function" to
errorFunction <- function(localShifts,globalShift,fileName,experimentalPI,lambda)
{
  error <- sum(abs(localShifts*lambda))
  error # return the error to be minimized
}

or to:
errorFunction <- function(localShifts,globalShift,fileName,experimentalPI,lambda)
{
  error <- sum(abs(localShifts))/sqrt(147)
  error # return the error to be minimized
}

The output is:
$par
[1]  6.018101e-20  6.018101e-20  6.018101e-20  6.018101e-20  0.000000e+00
[6]  5.176245e-21 -4.002183e-21 -8.254019e-20  3.231412e-21

$value
[1] 2.768593e-20

$counts
function gradient 
      76       76 

$convergence
[1] 0

$message
[1] "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL"

Warning messages:
1: bounds can only be used with method L-BFGS-B in: optim(par, errorFunction, gr = NULL, method = "Nelder-Mead",  
2: NAs introduced by coercion 


- What is wrong with the first version?

Thanks for the help!

Sarah


From derek.eder at lungall.gu.se  Wed Jan  3 12:16:12 2007
From: derek.eder at lungall.gu.se (Derek Eder)
Date: Wed, 03 Jan 2007 12:16:12 +0100
Subject: [R] Lattice / Trellis analog of axis(graphics) ?
Message-ID: <459B907C.2010402@lungall.gu.se>

My question is so basic that I am (almost too) embarrassed to admit that 
I could not find an answer after an hour's worth of homework.

What is the Trellis / Lattice analog for the axis(graphics) function 
that enables the creation of axes in locations other than the default 
(i.e., bottom for X axis and right for Y axis) ?

For example when plotting mileage against weight (in American units), 
one might want to also include a second X axis on the top margin (e.g., 
axis() pos = 3) with fuel mileage in metric units.

xyplot(Mileage ? Weight, data = fuel.frame)


Thank you,

humbly yours,

Derek Eder





platform i386-pc-mingw32
arch i386
os mingw32
system i386, mingw32
status
major 2
minor 4.0
year 2006
month 10
day 03
svn rev 39566
language R
version.string R version 2.4.0 (2006-10-03)

-- 
Derek N. Eder

Gothenburg University 
VINKLA - Vigilance and Neurocognition laboratory 

SU/Sahlgrenska
Utvecklingslab 1, Med
Gr?na str?ket 8
SE 413 45 G?teborg (Gothenburg)
Sverige (Sweden)

+46 (031)* 342 8261 (28261 inom Sahlgrenska)
+46 0704 915 714 (mobile)
+46 (031) 25 97 07 (home)

* omit the 0 when calling from outside Sweden


From s.p.pears at hotmail.co.uk  Wed Jan  3 12:16:55 2007
From: s.p.pears at hotmail.co.uk (Simon Pears)
Date: Wed, 03 Jan 2007 11:16:55 +0000
Subject: [R] R and threading
Message-ID: <BAY108-F19030BFD4ED37A74125033E4B90@phx.gbl>

Hi,

I am considering using R to integrate with a Java application. However, 
before deciding upon R I need to understand if R is capable of dealing with 
multiple requests simulataneously.

Is a single instance of R capable of dealing with multiple simulataneous 
requests or does a new instance of R have to be started for each request?

I have read Luke Tierney's 2001 notes on threading at 
http://www.stat.uiowa.edu/~luke/R/thrgui/thrgui.pdf.
Have these concepts been introduced into the R engine?

Regards

Dr.Simon Pears

_________________________________________________________________
Be the first to hear what's new at MSN - sign up to our free newsletters!


From mdowle at concordiafunds.com  Wed Jan  3 12:49:22 2007
From: mdowle at concordiafunds.com (Matthew Dowle)
Date: Wed, 3 Jan 2007 11:49:22 -0000
Subject: [R] RODBC compile error with R 2.4.1
Message-ID: <946928E281E8FD4B91D5B2CC3C91967DAA952E@lnex01.CONCORDIAFUNDS.COM>


Hi All,

I'm getting the following error,  could anyone help please?

$ R CMD INSTALL RODBC_1.1-7.tar.gz
* Installing *source* package 'RODBC' ...
checking for gcc... gcc -std=gnu99
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables... 
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc -std=gnu99 accepts -g... yes
checking for gcc -std=gnu99 option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -std=gnu99 -E
checking for egrep... grep -E
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking sql.h usability... yes
checking sql.h presence... yes
checking for sql.h... yes
checking sqlext.h usability... yes
checking sqlext.h presence... yes
checking for sqlext.h... yes
checking for library containing SQLTables... -lodbc
checking for SQLLEN... yes
checking for SQLULEN... yes
checking for long... yes
checking size of long... configure: error: cannot compute sizeof (long),
77
See `config.log' for more details.
ERROR: configuration failed for package 'RODBC'
** Removing '/usr/local/lib/R/library/RODBC'
** Restoring previous '/usr/local/lib/R/library/RODBC'

> version
               _                           
platform       x86_64-unknown-linux-gnu    
arch           x86_64                      
os             linux-gnu                   
system         x86_64, linux-gnu           
status                                     
major          2                           
minor          4.1                         
year           2006                        
month          12                          
day            18                          
svn rev        40228                       
language       R                           
version.string R version 2.4.1 (2006-12-18)
>


Regards,
Matthew


From Thomas.Petzoldt at TU-Dresden.de  Wed Jan  3 12:50:51 2007
From: Thomas.Petzoldt at TU-Dresden.de (Thomas Petzoldt)
Date: Wed, 03 Jan 2007 12:50:51 +0100
Subject: [R] Lattice / Trellis analog of axis(graphics) ?
In-Reply-To: <459B907C.2010402@lungall.gu.se>
References: <459B907C.2010402@lungall.gu.se>
Message-ID: <459B989B.5000705@TU-Dresden.de>

Hi Derek,

see ?xyplot and ?panel.axis

Hint: RSiteSearch("panel.axis") will point you to examples.

Thomas

Derek Eder wrote:
> My question is so basic that I am (almost too) embarrassed to admit that 
> I could not find an answer after an hour's worth of homework.
> 
> What is the Trellis / Lattice analog for the axis(graphics) function 
> that enables the creation of axes in locations other than the default 
> (i.e., bottom for X axis and right for Y axis) ?
> 
> For example when plotting mileage against weight (in American units), 
> one might want to also include a second X axis on the top margin (e.g., 
> axis() pos = 3) with fuel mileage in metric units.
> 
> xyplot(Mileage ? Weight, data = fuel.frame)

[...]


From jenny197806 at yahoo.se  Wed Jan  3 12:54:50 2007
From: jenny197806 at yahoo.se (Jenny persson)
Date: Wed, 3 Jan 2007 12:54:50 +0100 (CET)
Subject: [R] How to add characters on graph ?
Message-ID: <20070103115451.17578.qmail@web28003.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: inte tillg?nglig
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070103/d0f3260e/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Jan  3 12:54:58 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 3 Jan 2007 11:54:58 +0000 (GMT)
Subject: [R] Hershey fonts for musical notation?
In-Reply-To: <f737c20326671.459b82a2@utu.fi>
References: <f737c20326671.459b82a2@utu.fi>
Message-ID: <Pine.LNX.4.64.0701031145090.2401@gannet.stats.ox.ac.uk>

On Wed, 3 Jan 2007, Atte Tenkanen wrote:

[...]

> By the way, right now the demo(Hershey) seems not to work in OSX version R 2.4.1. ...
> I get a message
>
>> i <- i + 1
> Error in deparse(ei, control = c("showAttributes", "useSource")) :
> 	invalid multibyte string


But it does should work in R-devel, which deparses differently.  The 
issue is the use of a UTF-8 locale and the Hershey byte codes are indeed 
an invalid string in such a locale. Here is a simple example in UTF-8.

> x <- "\301"
> x
[1]Error: invalid multibyte string


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From s.p.pears at hotmail.co.uk  Wed Jan  3 11:26:37 2007
From: s.p.pears at hotmail.co.uk (Simon Pears)
Date: Wed, 03 Jan 2007 10:26:37 +0000
Subject: [R] R's capaability of dealing with multiple requests
Message-ID: <BAY108-F36EBD5A003D22220899319E4B90@phx.gbl>

Hi,

I am interested in integrating R with a Java front end. Before deciding to 
use R I am concerned about multi-threading.

I have been investigating R's capability of dealing with multiple requests 
simultaneously (multi-threading)
and have looked at Luke Tierney's 2001 notes for ideas for future releases 
of R at http://www.stat.uiowa.edu/~luke/R/thrgui/thrgui.pdf

Is a single instance of R capable of dealing with multiple requests 
simultanouesly or does a new instance of R have to be created for each 
request?

Regards,

Dr. Simon Pears


From kbeath at efs.mq.edu.au  Wed Jan  3 12:04:15 2007
From: kbeath at efs.mq.edu.au (Ken Beath)
Date: Wed, 03 Jan 2007 22:04:15 +1100
Subject: [R] optim
Message-ID: <s59c2866.004@mail.efs.mq.edu.au>

The function needs to have a single parameter. Then extract each
parameter.
For example, or see first example in help for optim.
errorFunction <- function(params) {
   localShifts <- params[1]
etc

>>> "Gerster  Sarah" <sgerster at student.ethz.ch> 01/03/07 9:22 PM >>>
Hi!

I'm trying to figure out how to use optim... I get some really strange
results, so I guess I got something wrong.

I defined the following function which should be minimized:
errorFunction <-
function(localShifts,globalShift,fileName,experimentalPI,lambda)
{
  lambda <- 1/sqrt(147)
  # error <- abs(errHuber(localShifts,globalShift,
  #   
"/home/sarah/Semesterarbeit/Sequences/R/R1593_filtered.data",3.48)) +
  #    sum(abs(localShifts))*lambda

  error <- sum(abs(localShifts))*lambda
  error # return the error to be minimized
}

Then I call optim:
par <- seq(length=9, from=0, by=0)
lambda <- 1/sqrt(147)
optim(par, errorFunction, gr=NULL, method="Nelder-Mead", hessian=FALSE,
        globalShift,
        "/home/sarah/Semesterarbeit/Sequences/R/R1593_filtered.data",
        experimentalPI=3.48, lambda = lambda)

The output is:
$par
[1]  0.56350964  0.56350964  0.56350964  0.56350964  0.00000000
-0.29515957
[7]  0.00569937  0.32543297  0.18615880

$value
[1] 0.2529198

$counts
function gradient 
      31       31 

$convergence
[1] 0

$message
[1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"

Warning messages:
1: bounds can only be used with method L-BFGS-B in: optim(par,
errorFunction, gr = NULL, method = "Nelder-Mead",  
2: NAs introduced by coercion 



If I change my "error-function" to
errorFunction <-
function(localShifts,globalShift,fileName,experimentalPI,lambda)
{
  error <- sum(abs(localShifts*lambda))
  error # return the error to be minimized
}

or to:
errorFunction <-
function(localShifts,globalShift,fileName,experimentalPI,lambda)
{
  error <- sum(abs(localShifts))/sqrt(147)
  error # return the error to be minimized
}

The output is:
$par
[1]  6.018101e-20  6.018101e-20  6.018101e-20  6.018101e-20 
0.000000e+00
[6]  5.176245e-21 -4.002183e-21 -8.254019e-20  3.231412e-21

$value
[1] 2.768593e-20

$counts
function gradient 
      76       76 

$convergence
[1] 0

$message
[1] "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL"

Warning messages:
1: bounds can only be used with method L-BFGS-B in: optim(par,
errorFunction, gr = NULL, method = "Nelder-Mead",  
2: NAs introduced by coercion 


- What is wrong with the first version?

Thanks for the help!

Sarah

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Wed Jan  3 13:37:56 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 3 Jan 2007 12:37:56 +0000 (GMT)
Subject: [R] RODBC compile error with R 2.4.1
In-Reply-To: <946928E281E8FD4B91D5B2CC3C91967DAA952E@lnex01.CONCORDIAFUNDS.COM>
References: <946928E281E8FD4B91D5B2CC3C91967DAA952E@lnex01.CONCORDIAFUNDS.COM>
Message-ID: <Pine.LNX.4.64.0701031222020.3976@gannet.stats.ox.ac.uk>

This is not a 'compile error' but an error from the configure script, 
probably a run error.  As it says

> See `config.log' for more details.

and then ask your local Linux guru what is broken locally.

This is neither an R nor an RODBC problem, and for what it is worth 
RODBC_1.1-7.tar.gz configures correctly on x86_64 Linux on FC5 and 
FC3, as well as various Debian versions (even with Debian's modified
unixODBC).


On Wed, 3 Jan 2007, Matthew Dowle wrote:

>
> Hi All,
>
> I'm getting the following error,  could anyone help please?
>
> $ R CMD INSTALL RODBC_1.1-7.tar.gz
> * Installing *source* package 'RODBC' ...
> checking for gcc... gcc -std=gnu99
> checking for C compiler default output file name... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc -std=gnu99 accepts -g... yes
> checking for gcc -std=gnu99 option to accept ANSI C... none needed
> checking how to run the C preprocessor... gcc -std=gnu99 -E
> checking for egrep... grep -E
> checking for ANSI C header files... yes
> checking for sys/types.h... yes
> checking for sys/stat.h... yes
> checking for stdlib.h... yes
> checking for string.h... yes
> checking for memory.h... yes
> checking for strings.h... yes
> checking for inttypes.h... yes
> checking for stdint.h... yes
> checking for unistd.h... yes
> checking sql.h usability... yes
> checking sql.h presence... yes
> checking for sql.h... yes
> checking sqlext.h usability... yes
> checking sqlext.h presence... yes
> checking for sqlext.h... yes
> checking for library containing SQLTables... -lodbc
> checking for SQLLEN... yes
> checking for SQLULEN... yes
> checking for long... yes
> checking size of long... configure: error: cannot compute sizeof (long),
> 77
> See `config.log' for more details.
> ERROR: configuration failed for package 'RODBC'
> ** Removing '/usr/local/lib/R/library/RODBC'
> ** Restoring previous '/usr/local/lib/R/library/RODBC'
>
>> version
>               _
> platform       x86_64-unknown-linux-gnu
> arch           x86_64
> os             linux-gnu
> system         x86_64, linux-gnu
> status
> major          2
> minor          4.1
> year           2006
> month          12
> day            18
> svn rev        40228
> language       R
> version.string R version 2.4.1 (2006-12-18)
>>
>
>
> Regards,
> Matthew
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jholtman at gmail.com  Wed Jan  3 13:44:37 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 3 Jan 2007 07:44:37 -0500
Subject: [R] How to add characters on graph ?
In-Reply-To: <20070103115451.17578.qmail@web28003.mail.ukl.yahoo.com>
References: <20070103115451.17578.qmail@web28003.mail.ukl.yahoo.com>
Message-ID: <644e1f320701030444v16943c6brda2040fd6888ae7e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070103/0366e8f5/attachment.pl 

From petr.pikal at precheza.cz  Wed Jan  3 13:45:21 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 03 Jan 2007 13:45:21 +0100
Subject: [R] How to add characters on graph ?
In-Reply-To: <20070103115451.17578.qmail@web28003.mail.ukl.yahoo.com>
Message-ID: <459BB371.30749.1533BC0@localhost>

Hi

On 3 Jan 2007 at 12:54, Jenny persson wrote:

Date sent:      	Wed, 3 Jan 2007 12:54:50 +0100 (CET)
From:           	Jenny persson <jenny197806 at yahoo.se>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] How to add characters on graph ?

> Dear R-users,
> 
>   I have following data 
> 
> 
>   # Plot coloured scatter plot
> 
>   c<-dat[100:110,c(5,7,8)]
> 
>   par(mfrow=c(3,2))
>   plot(c$lb,c$index, pch=1, col=5,cex=1, lwd=2,
>   xlab="LB", ylab="Index",cex.main =1,font.main= 1,
>    main="scatterplot")
> 
>               ID       index       lb
>   100 FLINDYTHNIPLI  1.84770221 9.087463
>   101          none  0.06657547 8.927778
>   102 GDDKVYSANGFTT -0.22922544 8.599913
>   103 GDFTQGPQSAKTR  0.01203925 8.483816
>   104 GDKEFSDALGYLQ -0.06264494 8.463524
>   105 GDPTETLRQCFDD -0.10011148 8.483816
>   106 GDSGGSFQNGHAQ -0.13460447 8.442943
>   107 GDVYSFAIIMQEV  1.91504700 8.413628
>   108 GLRSLYPPPPPPQ -0.11224126 8.383704
>   109 GLWVTYKAQDAKT  0.03723291 8.257388
>   110 GMSQPLLDRTVPD -0.06580206 8.294621
> 
>   When I plotted a scatter plot of index against lb, there are two
>   extreme values. How can I plot so that these values are replaced by
>   their ID or the IDs are next to these values on the graph? I want to
>   do something like: if index > 1.5 then  plot the IDs instead of the
>   indexes greater than 1.5 or place the Ids next to their indexes. The

I would use such construction

plot(x,y, ..., type="n")
points(x,y, pch=c(NA,1)[(index>1.5)+1])
sel<-(index>1.5)
text(x[sel],y[sel], ID[sel])

see ?points, ?text, ?plot

HTH
Petr


>   data above is a little part of my real data (which might have more
>   than two extreme outliers).
> 
>   Thanks for your help,
> 
>   Jenny
> 
>  __________________________________________________
> 
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From bagatti.davide at gmail.com  Wed Jan  3 14:05:08 2007
From: bagatti.davide at gmail.com (Bagatti Davide)
Date: Wed, 3 Jan 2007 14:05:08 +0100
Subject: [R] loading data and executing queries with R and Mysql
Message-ID: <51f845600701030505v3b8c2bb6i1171f1eef2e6b1b7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070103/c3f10941/attachment.pl 

From jprisbrey at yahoo.com  Wed Jan  3 14:20:40 2007
From: jprisbrey at yahoo.com (Jeffrey Prisbrey)
Date: Wed, 3 Jan 2007 05:20:40 -0800 (PST)
Subject: [R] understanding integer divide (%/%)
Message-ID: <946221.85257.qm@web58408.mail.re3.yahoo.com>

I am confused about why the following occurs:

> version
               _                           
platform       i386-pc-mingw32             
arch           i386                        
os             mingw32                     
system         i386, mingw32               
status                                     
major          2                           
minor          4.0                         
year           2006                        
month          10                          
day            03                          
svn rev        39566                       
language       R                           
version.string R version 2.4.0 (2006-10-03)
> 1 %/% 0.1
[1] 9
> 10 %/% 1
[1] 10
> 

This effect led me into an trap when I tried to
classify a set of proportions based on the first
decimal place by integer dividing by 0.1.  Can someone
explain why this behavior occurs and give me an
insight into how to predict it?

Thanks,
-- Jeff


From br44114 at gmail.com  Wed Jan  3 14:30:01 2007
From: br44114 at gmail.com (bogdan romocea)
Date: Wed, 3 Jan 2007 08:30:01 -0500
Subject: [R] loading data and executing queries with R and Mysql
Message-ID: <8d5a36350701030530p1a3466f3k6fb5dce13fa1c386@mail.gmail.com>

Nevermind the CPU usage, the likely problem is that your queries are
inefficient in one or more ways (i.e., you don't use indexes when you
really should - it's impossible to guess without knowing how the data
and the queries look like, which somehow you've decided are not
important enough to describe in your email). One nice reference is
http://highperformancemysql.com/


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bagatti Davide
> Sent: Wednesday, January 03, 2007 8:05 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] loading data and executing queries with R and Mysql
>
> Hello everyone,
>
> I have a problem when I execute queries using R 2.3.1 and
> MySql server 5.0.
> What I do: I load data in different csv files (every file represents a
> particular temporal step of a simulation) using Mysql query
> "load data" with
> RMySQL command DbSendQuery (but the same problem there is also using
> DbWritetable). Then I use a function where I have a lot of
> queries that
> interact with the database.
> Well, while loading data is very fast, query execution is very slow
> ...looking to Windows task manager I see that my cpu doesn't
> go to 100% of
> usage, but only at 30-45%. Looking at the processes I see Rgui.exe use
> between 0-23% and mysql-nt.exe use between 8-20% and so it is
> very slow. I
> am sure my cpu has no particular problems.
> Could you help me?
>
> Thanks in advance
>
> Davide
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Thierry.ONKELINX at inbo.be  Wed Jan  3 14:29:14 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 3 Jan 2007 14:29:14 +0100
Subject: [R] understanding integer divide (%/%)
In-Reply-To: <946221.85257.qm@web58408.mail.re3.yahoo.com>
Message-ID: <2E9C414912813E4EB981326983E0A104026EB6B6@inboexch.inbo.be>

This is due to the internal representation of 0.1, which is not exactly
0.1 but very close to it. If you want to do an integer divide, you
should only use integers to divide with.

Cheers,

Thierry

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens Jeffrey Prisbrey
Verzonden: woensdag 3 januari 2007 14:21
Aan: r-help op stat.math.ethz.ch
Onderwerp: [R] understanding integer divide (%/%)

I am confused about why the following occurs:

> version
               _                           
platform       i386-pc-mingw32             
arch           i386                        
os             mingw32                     
system         i386, mingw32               
status                                     
major          2                           
minor          4.0                         
year           2006                        
month          10                          
day            03                          
svn rev        39566                       
language       R                           
version.string R version 2.4.0 (2006-10-03)
> 1 %/% 0.1
[1] 9
> 10 %/% 1
[1] 10
> 

This effect led me into an trap when I tried to
classify a set of proportions based on the first
decimal place by integer dividing by 0.1.  Can someone
explain why this behavior occurs and give me an
insight into how to predict it?

Thanks,
-- Jeff

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From wraff at titus.u-strasbg.fr  Wed Jan  3 15:20:35 2007
From: wraff at titus.u-strasbg.fr (Wolfgang Raffelsberger)
Date: Wed, 03 Jan 2007 15:20:35 +0100
Subject: [R] RODBC : first line of data from query omitted
Message-ID: <459BBBB3.8090408@igbmc.u-strasbg.fr>

Dear List,

when reading MS Excel files in R using package RODBC I encountered the 
problem of having the first line of data getting omitted.

I read the data as :
 > library(RODBC)
 > channel1 <- odbcConnectExcel("myFile.xls")
 > sheet1 <- sqlQuery(channel1, "SELECT * FROM [Cell measures (1)$]")     
 # I use sqlQuery() instead of sqlFetch() since the sheet I want to 
extract is called : "Cell measures (1)"

The first line of data is missing in the resulting object (sheet1) in 
the case where the corresponding sheet contains a list of data without a 
line serving as header.  And the original first line appears as 
column-name (in the case of strings while any numeric content is 
transformed to incrementing column-names).
The 2nd line from my input appears then as 1st line of data in the 
resulting R-object and, no surprise, the total number of lines is 1 too few.

 >  sheet1[1:3,1:5]
  D - 5(fld 10)  F2      F3    F4      F5
1 D - 5(fld 11) 162 182.110 0.042 184.695
2 D - 5(fld 12) 163 198.154 0.086 201.932
3 D - 5(fld 13) 164 182.403 0.034 182.816

However, the 1st line in the original reads as :
  D - 5(fld 10) 161 182.929 0.045 188.819

Do you have an idea how to formulate the query that I can read the 1st 
line of data ?
Is there some argument like the "col.names=FALSE" in read.table() ?
Or is there a way to add an additional line unsing the SQL coomand 
"INSERT" (so that the real data would start in line 2) ?

 > sessionInfo()
R version 2.4.0 (2006-10-03)
i386-pc-mingw32

locale:
LC_COLLATE=French_France.1252;LC_CTYPE=French_France.1252;LC_MONETARY=French_France.1252;LC_NUMERIC=C;LC_TIME=French_France.1252

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     
"datasets"  "tcltk"     "base"    

other attached packages:
   RODBC     svIO   R2HTML   svMisc svSocket    svIDE
 "1.1-7"  "0.9-5"   "1.58"  "0.9-5"  "0.9-5"  "0.9-5"


Thank's in advance,
Wolfgang

 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . .

Wolfgang Raffelsberger, PhD
Laboratoire de BioInformatique et G?nomique Int?gratives
IGBMC
1 rue Laurent Fries,  67404 Illkirch  Strasbourg,  France
Tel (+33) 388 65 3314         Fax (+33) 388 65 3276
wolfgang.raffelsberger at igbmc.u-strasbg.fr


From ripley at stats.ox.ac.uk  Wed Jan  3 15:54:55 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 3 Jan 2007 14:54:55 +0000 (GMT)
Subject: [R] loading data and executing queries with R and Mysql
In-Reply-To: <51f845600701030505v3b8c2bb6i1171f1eef2e6b1b7@mail.gmail.com>
References: <51f845600701030505v3b8c2bb6i1171f1eef2e6b1b7@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701031319300.5467@gannet.stats.ox.ac.uk>

Without any actual example Ias requested in the footer of this message) I 
can only guess, but the most common cause of slow queries is the lack of 
indices in the database, so did you create any?

You haven't told us your actual OS (beyond 'Windows'), but a guess is that 
your processes are I/O bound, and that your file system could well do with 
a tune.  For example, if this is NTFS, is there lots (at least 30%) of 
free space and did you defragment it after saving the data?

On Wed, 3 Jan 2007, Bagatti Davide wrote:

> Hello everyone,
>
> I have a problem when I execute queries using R 2.3.1 and MySql server 5.0.
> What I do: I load data in different csv files (every file represents a
> particular temporal step of a simulation) using Mysql query "load data" with
> RMySQL command DbSendQuery (but the same problem there is also using
> DbWritetable). Then I use a function where I have a lot of queries that
> interact with the database.
> Well, while loading data is very fast, query execution is very slow
> ...looking to Windows task manager I see that my cpu doesn't go to 100% of
> usage, but only at 30-45%. Looking at the processes I see Rgui.exe use
> between 0-23% and mysql-nt.exe use between 8-20% and so it is very slow. I
> am sure my cpu has no particular problems.
> Could you help me?
>
> Thanks in advance
>
> Davide
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From larsfromspace at web.de  Wed Jan  3 15:55:04 2007
From: larsfromspace at web.de (downunder03)
Date: Wed, 3 Jan 2007 06:55:04 -0800 (PST)
Subject: [R] label array
Message-ID: <8142071.post@talk.nabble.com>


hi all. how can i adress a array directly. for example i wanna give array 1
other labels than array 2. How can I overcome this problem?

...this doesn't work

tab <- array(1:8, c(2, 2, 2))
dimnames(tab[,,1]) <- list(c("No","Yes"), c("No","Yes"),c("ARRAY1"))
dimnames(tab[,,2]) <- list(c("big","small"), c("small","big"),c("ARRAY2"))



-- 
View this message in context: http://www.nabble.com/label-array-tf2913929.html#a8142071
Sent from the R help mailing list archive at Nabble.com.


From larsfromspace at web.de  Wed Jan  3 15:55:04 2007
From: larsfromspace at web.de (downunder03)
Date: Wed, 3 Jan 2007 06:55:04 -0800 (PST)
Subject: [R]  label array
Message-ID: <8142071.post@talk.nabble.com>


hi all. how can i adress a array directly. for example i wanna give array 1
other labels than array 2. How can I overcome this problem?

...this doesn't work

tab <- array(1:8, c(2, 2, 2))
dimnames(tab[,,1]) <- list(c("No","Yes"), c("No","Yes"),c("ARRAY1"))
dimnames(tab[,,2]) <- list(c("big","small"), c("small","big"),c("ARRAY2"))



-- 
View this message in context: http://www.nabble.com/-R--label-array-tf2913929.html#a8142071
Sent from the R help mailing list archive at Nabble.com.


From rolf at math.unb.ca  Wed Jan  3 15:56:26 2007
From: rolf at math.unb.ca (rolf at math.unb.ca)
Date: Wed, 3 Jan 2007 10:56:26 -0400 (AST)
Subject: [R] understanding integer divide (%/%)
Message-ID: <200701031456.l03EuQ73001402@weisner.math.unb.ca>


Thierry Onkelinx wrote:

> If you want to do an integer divide, you should only use integers to
> divide with.

	I think this should go into ``fortunes''.

				cheers,

					Rolf Turner
					rolf at math.unb.ca


From larsfromspace at web.de  Wed Jan  3 15:55:04 2007
From: larsfromspace at web.de (downunder03)
Date: Wed, 3 Jan 2007 06:55:04 -0800 (PST)
Subject: [R]  accessing arrays
Message-ID: <8142071.post@talk.nabble.com>


hi all. how can i adress a array directly. for example i wanna give array 1
other labels than array 2. How can I overcome this problem?

...this doesn't work

tab <- array(1:8, c(2, 2, 2))
dimnames(tab[,,1]) <- list(c("No","Yes"), c("No","Yes"),c("ARRAY1"))
dimnames(tab[,,2]) <- list(c("big","small"), c("small","big"),c("ARRAY2"))



-- 
View this message in context: http://www.nabble.com/-R--accessing-arrays-tf2913929.html#a8142071
Sent from the R help mailing list archive at Nabble.com.


From hustqiufeng at sohu.com  Wed Jan  3 14:40:58 2007
From: hustqiufeng at sohu.com (Feng Qiu)
Date: Wed, 3 Jan 2007 08:40:58 -0500
Subject: [R]  Is there a function for this?
References: <FF501BA1-95AE-48D5-BF42-FDF1CA71E187@uiuc.edu>
Message-ID: <003e01c72f3c$d31dd780$6401a8c0@Aglog>

Hi everybody, I'm trying to do a statistic on the error rate of a prediction 
algorithm.

suppose this is the real category
[good, good, bad, bad, good, good, bad, bad]
this is the predicted category
[good, bad, bad, bad, good, good, good, bad]

I'm trying to do a statistic on the error rate for each group("good","bad"): 
what percentage of instances are predicted incorrectly for each group ?
Of course I can write a loop to do that, but is there a easy way to do that?

Thank you!

Best,

Feng


From ggrothendieck at gmail.com  Wed Jan  3 16:24:02 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 3 Jan 2007 10:24:02 -0500
Subject: [R] Is there a function for this?
In-Reply-To: <003e01c72f3c$d31dd780$6401a8c0@Aglog>
References: <FF501BA1-95AE-48D5-BF42-FDF1CA71E187@uiuc.edu>
	<003e01c72f3c$d31dd780$6401a8c0@Aglog>
Message-ID: <971536df0701030724m636803eex1b1f2729ab4a2656@mail.gmail.com>

Try this:

> actual <- factor(c("good", "good", "bad", "bad", "good", "good", "bad", "bad"))
> pred <- factor(c("good", "bad", "bad", "bad", "good", "good", "good", "bad"))

> table(actual, pred)
      pred
actual bad good
  bad    3    1
  good   1    3

> prop.table(table(actual, pred), 1)
      pred
actual  bad good
  bad  0.75 0.25
  good 0.25 0.75

> prop.table(table(actual, pred), 2)
      pred
actual  bad good
  bad  0.75 0.25
  good 0.25 0.75

> library(gmodels)
> CrossTable(actual, pred)


   Cell Contents
|-------------------------|
|                       N |
| Chi-square contribution |
|           N / Row Total |
|           N / Col Total |
|         N / Table Total |
|-------------------------|


Total Observations in Table:  8


             | pred
      actual |       bad |      good | Row Total |
-------------|-----------|-----------|-----------|
         bad |         3 |         1 |         4 |
             |     0.500 |     0.500 |           |
             |     0.750 |     0.250 |     0.500 |
             |     0.750 |     0.250 |           |
             |     0.375 |     0.125 |           |
-------------|-----------|-----------|-----------|
        good |         1 |         3 |         4 |
             |     0.500 |     0.500 |           |
             |     0.250 |     0.750 |     0.500 |
             |     0.250 |     0.750 |           |
             |     0.125 |     0.375 |           |
-------------|-----------|-----------|-----------|
Column Total |         4 |         4 |         8 |
             |     0.500 |     0.500 |           |
-------------|-----------|-----------|-----------|





On 1/3/07, Feng Qiu <hustqiufeng at sohu.com> wrote:
> Hi everybody, I'm trying to do a statistic on the error rate of a prediction
> algorithm.
>
> suppose this is the real category
> [good, good, bad, bad, good, good, bad, bad]
> this is the predicted category
> [good, bad, bad, bad, good, good, good, bad]
>
> I'm trying to do a statistic on the error rate for each group("good","bad"):
> what percentage of instances are predicted incorrectly for each group ?
> Of course I can write a loop to do that, but is there a easy way to do that?
>
> Thank you!
>
> Best,
>
> Feng
>


From andreashary at googlemail.com  Wed Jan  3 16:30:45 2007
From: andreashary at googlemail.com (Andreas Hary)
Date: Wed, 3 Jan 2007 15:30:45 +0000
Subject: [R] Is there a function for this?
In-Reply-To: <003e01c72f3c$d31dd780$6401a8c0@Aglog>
References: <FF501BA1-95AE-48D5-BF42-FDF1CA71E187@uiuc.edu>
	<003e01c72f3c$d31dd780$6401a8c0@Aglog>
Message-ID: <8a17c70701030730g5516a6desf890951294036e1b@mail.gmail.com>

Try the following:

act <- c('good','good','bad','bad','good','good','bad','bad')
pred <- c('good','bad','bad','bad','good','good','good','bad')
table(pred,act)
table(pred,act)/apply(table(pred,act),1,sum)

Cheers,

Andreas


On 1/3/07, Feng Qiu <hustqiufeng at sohu.com> wrote:
> Hi everybody, I'm trying to do a statistic on the error rate of a prediction
> algorithm.
>
> suppose this is the real category
> [good, good, bad, bad, good, good, bad, bad]
> this is the predicted category
> [good, bad, bad, bad, good, good, good, bad]
>
> I'm trying to do a statistic on the error rate for each group("good","bad"):
> what percentage of instances are predicted incorrectly for each group ?
> Of course I can write a loop to do that, but is there a easy way to do that?
>
> Thank you!
>
> Best,
>
> Feng
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sebastian.weber at physik.tu-darmstadt.de  Wed Jan  3 16:36:51 2007
From: sebastian.weber at physik.tu-darmstadt.de (Sebastian Weber)
Date: Wed, 03 Jan 2007 15:36:51 +0000
Subject: [R] mathematical symbols in plots
Message-ID: <1167838611.7280.16.camel@rock.kraft.de>

Hello everyone!

I'm trying to plot some mathematical expression along my axis, but
demo(plotmath) did not have the symbol I was looking for. In particular,
I would like to denote the mean of an observable by writing

<k>

which I tried to enter with

expression(group("<", k, ">"))

However, my naive try doesn't work and the help doesn't want to tell me,
does someone know?

And here another one: How can I sepcify which fonts get used with which
R prints those mathematical symbols? Since I finally include my plots in
latex-documents as eps, I would love to use the same font-encoding for
all postscript stuff. A problem in the past has been, that R embedded
it's own font within the ps-files generated. These were not compatible
with the fonts used at the magazine where I published my document. This
lead to quite some confusion as \gamma became g and so on. Any solution
to this problem? Any hint? As I'm not too much into font-encoding, I
have actually no real clue where to even start searching.

Thank you very much for any help.

Greetings,

Sebastian Weber


From tlumley at u.washington.edu  Wed Jan  3 16:45:33 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 3 Jan 2007 07:45:33 -0800 (PST)
Subject: [R] understanding integer divide (%/%)
In-Reply-To: <2E9C414912813E4EB981326983E0A104026EB6B6@inboexch.inbo.be>
References: <2E9C414912813E4EB981326983E0A104026EB6B6@inboexch.inbo.be>
Message-ID: <Pine.LNX.4.64.0701030743370.30561@homer21.u.washington.edu>

On Wed, 3 Jan 2007, ONKELINX, Thierry wrote:

> This is due to the internal representation of 0.1, which is not exactly
> 0.1 but very close to it. If you want to do an integer divide, you
> should only use integers to divide with.

This must be more-or-less correct, but it is worth noting that
> 0.1*10==1
[1] TRUE
> 1/0.1==10
[1] TRUE
> 1%/%0.1==10
[1] FALSE
so it isn't quite that simple.

Interestingly, the results seem to vary by system -- on a G4 Mac I get
1 %/% (1/x) == x for all x from 1 to 50

 	-thomas



> Cheers,
>
> Thierry
>
> ------------------------------------------------------------------------
> ----
>
> ir. Thierry Onkelinx
>
> Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
> and Forest
>
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
>
> Gaverstraat 4
>
> 9500 Geraardsbergen
>
> Belgium
>
> tel. + 32 54/436 185
>
> Thierry.Onkelinx at inbo.be
>
> www.inbo.be
>
>
>
> Do not put your faith in what statistics say until you have carefully
> considered what they do not say.  ~William W. Watt
>
> A statistical analysis, properly conducted, is a delicate dissection of
> uncertainties, a surgery of suppositions. ~M.J.Moroney
>
> -----Oorspronkelijk bericht-----
> Van: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] Namens Jeffrey Prisbrey
> Verzonden: woensdag 3 januari 2007 14:21
> Aan: r-help at stat.math.ethz.ch
> Onderwerp: [R] understanding integer divide (%/%)
>
> I am confused about why the following occurs:
>
>> version
>               _
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          4.0
> year           2006
> month          10
> day            03
> svn rev        39566
> language       R
> version.string R version 2.4.0 (2006-10-03)
>> 1 %/% 0.1
> [1] 9
>> 10 %/% 1
> [1] 10
>>
>
> This effect led me into an trap when I tried to
> classify a set of proportions based on the first
> decimal place by integer dividing by 0.1.  Can someone
> explain why this behavior occurs and give me an
> insight into how to predict it?
>
> Thanks,
> -- Jeff
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From luke at stat.uiowa.edu  Wed Jan  3 16:49:20 2007
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Wed, 3 Jan 2007 09:49:20 -0600 (CST)
Subject: [R] wrapping mle()
In-Reply-To: <874prdhuot.fsf@patagonia.sebmags.homelinux.org>
References: <8764buj738.fsf@patagonia.sebmags.homelinux.org>
	<971536df0612292130g26ac8286k4df7c5499c97e609@mail.gmail.com>
	<Pine.LNX.4.64.0612301545060.4532@itasca2.wildberry.org>
	<874prdhuot.fsf@patagonia.sebmags.homelinux.org>
Message-ID: <Pine.LNX.4.64.0701030938320.15218@nokomis.stat.uiowa.edu>

On Sat, 30 Dec 2006, Sebastian P. Luque wrote:

> On Sat, 30 Dec 2006 15:46:01 -0600 (CST),
> Luke Tierney <luke at stat.uiowa.edu> wrote:
>
>> It is much cleaner to do this sort of thing with lexical scope.  For
>> example,
>
>>     mkll <- function(x, y) {
>>        function(ymax=15, xhalf=6) {
>>           -sum(stats::dpois(y, lambda=ymax/(1+x/xhalf), log=TRUE))
>>        }
>>     }
>
>> creates a log-likelihood likelyhood function for data x,y that can
>> then be used by
>
>>     fit.mle <- function(mkfun, x, y) {
>>         loglik.fun <- mkfun(x, y)
>>         mle(loglik.fun, method="L-BFGS-B", lower=c(0, 0))
>>     }
>
>> as in
>
>
>>    > fit.mle(mkll, x=0:10, y=c(26, 17, 13, 12, 20, 5, 9, 8, 5, 4, 8))
>
>>     Call:
>>     mle(minuslogl = loglik.fun, method = "L-BFGS-B", lower = c(0,
>>         0))
>
>>     Coefficients:
>>          ymax     xhalf
>>     24.999420  3.055779
>
> Thanks Luke, this looks excellent.
>
>
>> It is not clear why you want to be able to pass ll as a character string
>> or why you want to assume that the thing passed in will refer to
>> variables named 'x' and 'y', both usually bad ideas, so this specific
>> approach may not apply, but something variant should.
>
> In the real case, I need to provide two different log likelihood
> functions, and then tell fit.mle() which one to use in a given call.  I
> was actually defining 'x' and 'y' as formal arguments to fit.mle().
> Wouldn't that ensure that the original ll() would refer to the correct
> variables?

No, since ll was defined at top level lexical scoping rules mean that
free variables in the definition of ll are top level variables,
regardless of where ll is called.

>  In any case, it was easy to use your suggestion almost by
> direct analogy, which makes the code much more readable.  Thanks a lot.
>
> In the case I describe though, why would it be a bad idea to use a string
> to refer to the function, and then use match.fun()?  I actually picked up
> the idea from functions such as apply() and friends.

Not bad just not necessary. Calling fit.mle(ll ...) will do and you
then don't need match.call, which makes your code simpler.

Best,

luke

>
>> The ability to use environment(f)<-env to change the environment of a
>> function is one of the most dubious language features of R (maybe the
>> most dubious, though there are a couple of other strong contenders) and
>> should not be used except in very rare circumstances.
>
> Keeping the lexical scoping technique you showed in mind should help stay
> away from that.
>
>
> Cheers,
>
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From luke at stat.uiowa.edu  Wed Jan  3 17:03:23 2007
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Wed, 3 Jan 2007 10:03:23 -0600 (CST)
Subject: [R] wrapping mle()
In-Reply-To: <971536df0612301759q42454370obee8a87fde2db7ac@mail.gmail.com>
References: <8764buj738.fsf@patagonia.sebmags.homelinux.org> 
	<971536df0612292130g26ac8286k4df7c5499c97e609@mail.gmail.com> 
	<Pine.LNX.4.64.0612301545060.4532@itasca2.wildberry.org>
	<971536df0612301759q42454370obee8a87fde2db7ac@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701030949321.15218@nokomis.stat.uiowa.edu>

On Sat, 30 Dec 2006, Gabor Grothendieck wrote:

> That has two disadvantages:
>
> (1) it only works if the user is defining ll himself; however, if the
> user is getting
> ll from somewhere else then its not applicable since the user no
> longer controls its
> scope whereas resetting the environment method still works

In that case surgery on environments is even worse: There is no
reliable way to know which variables in an arbitrary function body
might benefit from having values provided though environment surgery
and which ones definitely should not be messed with this in this
way. Brian's reply in this thread is related to this.  The reliable
way to provide data for variables in a function body is to make those
variables arguments in an appropriate function -- that is what the
nested function approach does.  In situations like these careful use
of ... arguments is another option but this can be tricky to get
right.  It is far superior to environment surgery though.

> (2) its "cleaner" for the developer but harder for the user who is now
> forced into
> a more complicated construct, i.e. the nested double function construct

Aside from the fact that the S language philosophy is to blur the
user/developer distinction this really does not make sense. Nested
functions like these are a simple idea that is basic to all modern
function-oriented languages.  (It even exists in languages like Pascal
for downward-only function arguments.) It is an idea that can be useful
for anyone who writes functions in R.  Environment surgery in contrast
is messy and complex and essentially impossible to get right.  If you
want to do this in the privacy of your own code that is fine, but
please don't encourage others to go down this path.

Best,

luke


>
> By the way, here is one additional solution using the proto package that
> avoids explicitly resetting of the environment in favor implicitly setting 
> it.
> A new proto object is created which to hold FUN and since proto methods have
> their object as their scope, their environment is implicitly reset:
>
> library(proto)
> library(stats4)
> ll <- function(ymax=15, xhalf=6) {
> -sum(stats::dpois(y, lambda=ymax/(1+x/xhalf), log=TRUE))
> }
> fit.mle <- function(FUN, x, y)
> mle(proto(FUN = match.fun(FUN))[["FUN"]], method="L-BFGS-B", lower=c(0, 0))
> fit.mle("ll", x=0:10, y=c(26, 17, 13, 12, 20, 5, 9, 8, 5, 4, 8))
>
>
>
>
>
>
> On 12/30/06, Luke Tierney <luke at stat.uiowa.edu> wrote:
>> It is much cleaner to do this sort of thing with lexical scope.  For
>> example,
>>
>>     mkll <- function(x, y) {
>>        function(ymax=15, xhalf=6) {
>>           -sum(stats::dpois(y, lambda=ymax/(1+x/xhalf), log=TRUE))
>>        }
>>     }
>> 
>> creates a log-likelihood likelyhood function for data x,y that can
>> then be used by
>>
>>     fit.mle <- function(mkfun, x, y) {
>>         loglik.fun <- mkfun(x, y)
>>         mle(loglik.fun, method="L-BFGS-B", lower=c(0, 0))
>>     }
>> 
>> as in
>> 
>>
>>     > fit.mle(mkll, x=0:10, y=c(26, 17, 13, 12, 20, 5, 9, 8, 5, 4, 8))
>>
>>     Call:
>>     mle(minuslogl = loglik.fun, method = "L-BFGS-B", lower = c(0,
>>         0))
>>
>>     Coefficients:
>>          ymax     xhalf
>>     24.999420  3.055779
>> 
>> It is not clear why you want to be able to pass ll as a character
>> string or why you want to assume that the thing passed in will refer
>> to variables named 'x' and 'y', both usually bad ideas, so this
>> specific approach may not apply, but something variant should.
>> 
>> The ability to use environment(f)<-env to change the environment of a
>> function is one of the most dubious language features of R (maybe the
>> most dubious, though there are a couple of other strong contenders)
>> and should not be used except in very rare circumstances.
>> 
>> Best,
>> 
>> luke
>> 
>> On Sat, 30 Dec 2006, Gabor Grothendieck wrote:
>> 
>> > Add the line marked ### so that the environment of loglik.fun is reset to
>> > the environment within fit.mle so that it can find y there:
>> >
>> > library(stats4)
>> > ll <- function(ymax=15, xhalf=6) {
>> >   -sum(stats::dpois(y, lambda=ymax/(1+x/xhalf), log=TRUE))
>> > }
>> > fit.mle <- function(FUN, x, y) {
>> >   loglik.fun <- match.fun(FUN)
>> >   environment(loglik.fun) <- environment() ###
>> >   mle(loglik.fun, method="L-BFGS-B", lower=c(0, 0))
>> > }
>> > fit.mle("ll", x=0:10, y=c(26, 17, 13, 12, 20, 5, 9, 8, 5, 4, 8))
>> >
>> >
>> >
>> > On 12/30/06, Sebastian P. Luque <spluque at gmail.com> wrote:
>> >> Hi,
>> >>
>> >> How can we set the environment for the minuslog function in mle()?  The
>> >> call in this code fails because the "ll" function cannot find the object
>> >> 'y'.  Modifying from the example in ?mle:
>> >>
>> >>
>> >> library(stats4)
>> >> ll <- function(ymax=15, xhalf=6) {
>> >>    -sum(stats::dpois(y, lambda=ymax/(1+x/xhalf), log=TRUE))
>> >> }
>> >> fit.mle <- function(FUN, x, y) {
>> >>    loglik.fun <- match.fun(FUN)
>> >>    mle(loglik.fun, method="L-BFGS-B", lower=c(0, 0))
>> >> }
>> >> fit.mle("ll", x=0:10, y=c(26, 17, 13, 12, 20, 5, 9, 8, 5, 4, 8))
>> >>
>> >>
>> >> How should "fit.mle" be constructed so that "ll" works on the 
>> appropriate
>> >> environment?  Thanks in advance for any advice on this.
>> >>
>> >>
>> >> --
>> >> Seb
>> >>
>> >> ______________________________________________
>> >> R-help at stat.math.ethz.ch mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>> 
>> --
>> Luke Tierney
>> Chair, Statistics and Actuarial Science
>> Ralph E. Wareham Professor of Mathematical Sciences
>> University of Iowa                  Phone:             319-335-3386
>> Department of Statistics and        Fax:               319-335-3017
>>    Actuarial Science
>> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
>> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
>> 
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From Greg.Snow at intermountainmail.org  Wed Jan  3 17:21:42 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Wed, 3 Jan 2007 09:21:42 -0700
Subject: [R] How to add characters on graph ?
Message-ID: <07E228A5BE53C24CAD490193A7381BBB77C351@LP-EXCHVS07.CO.IHC.COM>

If you just want to label and identify outliers after creating a plot
then look at the identify function.  In your case you could just run the
following command after creating your plot:

> out.index <- identify( c$lb, c$index )

Then click on (or near) the outliers or other interesting points.  The
points will be labelled on the plot and their position in the dataset
will be saved in out.index.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jenny persson
> Sent: Wednesday, January 03, 2007 4:55 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How to add characters on graph ?
> 
> Dear R-users,
>    
>   I have following data 
>    
>    
>   # Plot coloured scatter plot
>    
>   c<-dat[100:110,c(5,7,8)]
>    
>   par(mfrow=c(3,2))
>   plot(c$lb,c$index, pch=1, col=5,cex=1, lwd=2,
>   xlab="LB", ylab="Index",cex.main =1,font.main= 1,
>    main="scatterplot")
>    
>               ID       index       lb
>   100 FLINDYTHNIPLI  1.84770221 9.087463
>   101          none  0.06657547 8.927778
>   102 GDDKVYSANGFTT -0.22922544 8.599913
>   103 GDFTQGPQSAKTR  0.01203925 8.483816
>   104 GDKEFSDALGYLQ -0.06264494 8.463524
>   105 GDPTETLRQCFDD -0.10011148 8.483816
>   106 GDSGGSFQNGHAQ -0.13460447 8.442943
>   107 GDVYSFAIIMQEV  1.91504700 8.413628
>   108 GLRSLYPPPPPPQ -0.11224126 8.383704
>   109 GLWVTYKAQDAKT  0.03723291 8.257388
>   110 GMSQPLLDRTVPD -0.06580206 8.294621
>    
>   When I plotted a scatter plot of index against lb, there 
> are two extreme values. How can I plot so that these values 
> are replaced by their ID or the IDs are next to these values 
> on the graph? I want to do something like: if index > 1.5 
> then  plot the IDs instead of the indexes greater than 1.5 or 
> place the Ids next to their indexes.
>   The data above is a little part of my real data (which 
> might have more than two extreme outliers).
>    
>   Thanks for your help,
>    
>   Jenny
> 
>  __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ronggui.huang at gmail.com  Wed Jan  3 17:22:33 2007
From: ronggui.huang at gmail.com (ronggui)
Date: Thu, 4 Jan 2007 00:22:33 +0800
Subject: [R] SQLite: When reading a table,
	a "\r" is padded onto the last column. Why?
In-Reply-To: <Pine.LNX.4.64.0701030719120.25219@gannet.stats.ox.ac.uk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC038785AA@DJFPOST01.djf.agrsci.dk>
	<C83C5E3DEEE97E498B74729A33F6EAEC038785AB@DJFPOST01.djf.agrsci.dk>
	<Pine.LNX.4.64.0701030719120.25219@gannet.stats.ox.ac.uk>
Message-ID: <38b9f0350701030822y79b9b4c4n1454bdfb653f3675@mail.gmail.com>

RSQLite can import data from a large file directly (via
"dbWriteTable"). This future is quite appealing.


On 1/3/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> I guess you are using package RSQLite without telling us (or telling us
> the version), and that your example is incomplete?
>
> Using RSiteSearch("RSQLite Windows") quickly shows that this is a
> previously reported problem with the package, e.g.:
>
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/72515.html
>
> I believe the issue is that RSQLite actually writes out a CRLF-terminated
> text file and imports that into SQLite.  (I checked version 0.4-15.) It
> seems function safe.write() needs to be modified to write to a binary-mode
> connection since SQLite appears to require LF-terminated files.
>
> Using RODBC to work with SQLite databases works correctly even under
> Windows (and is much more efficient at writing to the database).
>
> [I am not sure who is actually maintaining RSQLite, so am Cc: both the
> stated maintainer and the person who prepared the package for
> distribution. The posting guide asked you to contact the maintainer: what
> response did _you_ get?]
>
>
> On Wed, 3 Jan 2007, S?ren H?jsgaard wrote:
>
> > Hi,
> >
> > I put the iris data into a SQLite database with
> >
> > dbWriteTable(con, "iris", iris, row.names=F, overwrite = T)
> >
> > Then I retrieve data from the database with
> >
> > rs  <- dbSendQuery(con, "select * from iris")
> > d1  <- fetch(rs)
> > dbClearResult(rs)
> >
> > Then I get
> >> head(d1)
> >  Sepal_Length Sepal_Width Petal_Length Petal_Width  Species
> > 1          5.1         3.5          1.4         0.2 setosa\r
> > 2          4.9         3.0          1.4         0.2 setosa\r
> > 3          4.7         3.2          1.3         0.2 setosa\r
> > 4          4.6         3.1          1.5         0.2 setosa\r
> > 5          5.0         3.6          1.4         0.2 setosa\r
> > 6          5.4         3.9          1.7         0.4 setosa\r
> >
> > Can anyone explain the extra "\r" at the end?  I am on Windows XP using R 2.4.1
> > Thanks in advance
> > S?ren
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


-- 
Ronggui Huang
Department of Sociology
Fudan University, Shanghai, China
???
????????


From paolo.radaelli at unimib.it  Wed Jan  3 17:56:18 2007
From: paolo.radaelli at unimib.it (Paolo Radaelli)
Date: Wed, 3 Jan 2007 17:56:18 +0100
Subject: [R] User defined split function in Rpart
References: <28429921.42921164895427764.JavaMail.nabble@jubjub.nabble.com>
Message-ID: <00ee01c72f58$17068800$90788495@radaelli>

Dear all,
 I'm trying to manage with user defined split function in rpart
(file rpart\tests\usersplits.R in 
http://cran.r-project.org/src/contrib/rpart_3.1-34.tar.gz - see bottom of 
the email).
Suppose to have the following data.frame (note that x's values are already 
sorted)
> D
y x
1 7 0.428
2 3 0.876
3 1 1.467
4 6 1.492
5 3 1.703
6 4 2.406
7 8 2.628
8 6 2.879
9 5 3.025
10 3 3.494
11 2 3.496
12 6 4.623
13 4 4.824
14 6 4.847
15 2 6.234
16 7 7.041
17 2 8.600
18 4 9.225
19 5 9.381
20 8 9.986

Running rpart and setting minbucket=1 and maxdepth=1 we get the following 
tree (which uses, by default, deviance):
> rpart(D$y~D$x,control=rpart.control(minbucket=1,maxdepth=1))
    n= 20
    node), split, n, deviance, yval * denotes terminal node
1) root 20 84.80000 4.600000
2) D$x< 9.6835 19 72.63158 4.421053 *
3) D$x>=9.6835 1 0.00000 8.000000 *

This means that the first 19 observation has been sent to the left side of 
the tree and one observation to the right.
This is correct when we observe goodness (the maximum is the last element of 
the vector).

The thing i really don't understand is the direction vector.
# direction= -1 = send "y< cutpoint" to the left side of the tree
# 1 = send "y< cutpoint" to the right

What does it mean ?
In the example here considered we have
> sign(lmean)
[1] 1 1 -1 -1 -1 -1 -1 1 1 1 -1 -1 -1 -1 -1 -1 -1 -1 -1

Which is the criterion used ?
In my opinion we should have all the values equal to -1 given that they have 
to be sent to left side of the tree.
Does someone can help me ?
Thank you

#######################################################
# The split function, where most of the work occurs.
# Called once per split variable per node.
# If continuous=T (the case here considered)
# The actual x variable is ordered
# y is supplied in the sort order of x, with no missings,
# return two vectors of length (n-1):
# goodness = goodness of the split, larger numbers are better.
# 0 = couldn't find any worthwhile split
# the ith value of goodness evaluates splitting obs 1:i vs (i+1):n
# direction= -1 = send "y< cutpoint" to the left side of the tree
# 1 = send "y< cutpoint" to the right
# this is not a big deal, but making larger "mean y's" move towards
# the right of the tree, as we do here, seems to make it easier to
# read
# If continuos=F, x is a set of integers defining the groups for an
# unordered predictor. In this case:
# direction = a vector of length m= "# groups". It asserts that the
# best split can be found by lining the groups up in this order
# and going from left to right, so that only m-1 splits need to
# be evaluated rather than 2^(m-1)
# goodness = m-1 values, as before.
#
# The reason for returning a vector of goodness is that the C routine
# enforces the "minbucket" constraint. It selects the best return value
# that is not too close to an edge.
The vector wt of weights in our case is:
> wt
[1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

temp2 <- function(y, wt, x, parms, continuous) {
# Center y
n <- length(y)
y <- y- sum(y*wt)/sum(wt)
if (continuous) {
# continuous x variable
temp <- cumsum(y*wt)[-n]
left.wt <- cumsum(wt)[-n]
right.wt <- sum(wt) - left.wt
lmean <- temp/left.wt
rmean <- -temp/right.wt
goodness <- (left.wt*lmean^2 + right.wt*rmean^2)/sum(wt*y^2)
list(goodness= goodness, direction=sign(lmean))
}
}

Paolo Radaelli
Dipartimento di Metodi Quantitativi per le Scienze Economiche ed Aziendali
Facolt? di Economia
Universit? degli Studi di Milano-Bicocca
P.zza dell'Ateneo Nuovo, 1
20126 Milano
Italy
e-mail paolo.radaelli a unimib.it


From wraff at titus.u-strasbg.fr  Wed Jan  3 17:59:02 2007
From: wraff at titus.u-strasbg.fr (Wolfgang Raffelsberger)
Date: Wed, 03 Jan 2007 17:59:02 +0100
Subject: [R] accessing arrays
In-Reply-To: <8142071.post@talk.nabble.com>
References: <8142071.post@talk.nabble.com>
Message-ID: <459BE0D6.4000807@igbmc.u-strasbg.fr>

try this :
x <- 
array(1:24,dim=c(2,3,4),dimnames=list(letters[1:2],LETTERS[1:3],letters[23:26]))

Cheers,
Wolfgang

downunder03 a ?crit :
> hi all. how can i adress a array directly. for example i wanna give array 1
> other labels than array 2. How can I overcome this problem?
>
> ...this doesn't work
>
> tab <- array(1:8, c(2, 2, 2))
> dimnames(tab[,,1]) <- list(c("No","Yes"), c("No","Yes"),c("ARRAY1"))
> dimnames(tab[,,2]) <- list(c("big","small"), c("small","big"),c("ARRAY2"))
>
>
>
>   


 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . .

Wolfgang Raffelsberger, PhD
Laboratoire de BioInformatique et G?nomique Int?gratives
IGBMC
1 rue Laurent Fries,  67404 Illkirch  Strasbourg,  France
Tel (+33) 388 65 3314         Fax (+33) 388 65 3276
wolfgang.raffelsberger at igbmc.u-strasbg.fr


From jwd at surewest.net  Wed Jan  3 18:24:28 2007
From: jwd at surewest.net (J Dougherty)
Date: Wed, 3 Jan 2007 09:24:28 -0800
Subject: [R] RODBC compile error with R 2.4.1
In-Reply-To: <946928E281E8FD4B91D5B2CC3C91967DAA952E@lnex01.CONCORDIAFUNDS.COM>
References: <946928E281E8FD4B91D5B2CC3C91967DAA952E@lnex01.CONCORDIAFUNDS.COM>
Message-ID: <200701030924.28577.jwd@surewest.net>

Matthew,

You don't seem to say what linux release you are using,  They can't very well 
help you without that information.  Not all releases are equal.

JWD

On Wednesday 03 January 2007 03:49, Matthew Dowle wrote:
> Hi All,
>
> I'm getting the following error,  could anyone help please?
>
> $ R CMD INSTALL RODBC_1.1-7.tar.gz
> * Installing *source* package 'RODBC' ...
> checking for gcc... gcc -std=gnu99
> checking for C compiler default output file name... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc -std=gnu99 accepts -g... yes
> checking for gcc -std=gnu99 option to accept ANSI C... none needed
> checking how to run the C preprocessor... gcc -std=gnu99 -E
> checking for egrep... grep -E
> checking for ANSI C header files... yes
> checking for sys/types.h... yes
> checking for sys/stat.h... yes
> checking for stdlib.h... yes
> checking for string.h... yes
> checking for memory.h... yes
> checking for strings.h... yes
> checking for inttypes.h... yes
> checking for stdint.h... yes
> checking for unistd.h... yes
> checking sql.h usability... yes
> checking sql.h presence... yes
> checking for sql.h... yes
> checking sqlext.h usability... yes
> checking sqlext.h presence... yes
> checking for sqlext.h... yes
> checking for library containing SQLTables... -lodbc
> checking for SQLLEN... yes
> checking for SQLULEN... yes
> checking for long... yes
> checking size of long... configure: error: cannot compute sizeof (long),
> 77
> See `config.log' for more details.
> ERROR: configuration failed for package 'RODBC'
> ** Removing '/usr/local/lib/R/library/RODBC'
> ** Restoring previous '/usr/local/lib/R/library/RODBC'
>
> > version
>
>                _
> platform       x86_64-unknown-linux-gnu
> arch           x86_64
> os             linux-gnu
> system         x86_64, linux-gnu
> status
> major          2
> minor          4.1
> year           2006
> month          12
> day            18
> svn rev        40228
> language       R
> version.string R version 2.4.1 (2006-12-18)
>
>
>
> Regards,
> Matthew
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.


From hu-student at web.de  Wed Jan  3 18:25:35 2007
From: hu-student at web.de (Lars Rohrschneider)
Date: Wed, 3 Jan 2007 09:25:35 -0800 (PST)
Subject: [R] accessing arrays
In-Reply-To: <459BE0D6.4000807@igbmc.u-strasbg.fr>
References: <8142071.post@talk.nabble.com>
	<459BE0D6.4000807@igbmc.u-strasbg.fr>
Message-ID: <8144807.post@talk.nabble.com>


Hi Wolfgang,

thanks for your hint. But I am desperating. I have an 3 dim array of say 10
matrices where every matrix has to stick an other label. I found no way to
direct assign the labels.

for example 

tab <- array(1:8, c(2, 2, 2))
dimnames(tab[,,1]) <- list(c("No","Yes"), c("No","Yes"))
dimnames(tab[,,2]) <- list(c("big","small"), c("small","big"))

should look like this
, , 1

    No Yes
No   1   3
Yes  2   4

, , 2

        big  small
small   5   7
big     6   8







Wolfgang Raffelsberger wrote:
> 
> try this :
> x <- 
> array(1:24,dim=c(2,3,4),dimnames=list(letters[1:2],LETTERS[1:3],letters[23:26]))
> 
> Cheers,
> Wolfgang
> 
> downunder03 a ?crit :
>> hi all. how can i adress a array directly. for example i wanna give array
>> 1
>> other labels than array 2. How can I overcome this problem?
>>
>> ...this doesn't work
>>
>> tab <- array(1:8, c(2, 2, 2))
>> dimnames(tab[,,1]) <- list(c("No","Yes"), c("No","Yes"),c("ARRAY1"))
>> dimnames(tab[,,2]) <- list(c("big","small"),
>> c("small","big"),c("ARRAY2"))
>>
>>
>>
>>   
> 
> 
>  
> 
> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
> . . . . .
> 
> Wolfgang Raffelsberger, PhD
> Laboratoire de BioInformatique et G?nomique Int?gratives
> IGBMC
> 1 rue Laurent Fries,  67404 Illkirch  Strasbourg,  France
> Tel (+33) 388 65 3314         Fax (+33) 388 65 3276
> wolfgang.raffelsberger at igbmc.u-strasbg.fr
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/-R--accessing-arrays-tf2913929.html#a8144807
Sent from the R help mailing list archive at Nabble.com.


From pburns at pburns.seanet.com  Wed Jan  3 18:33:27 2007
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Wed, 03 Jan 2007 17:33:27 +0000
Subject: [R] accessing arrays
In-Reply-To: <8142071.post@talk.nabble.com>
References: <8142071.post@talk.nabble.com>
Message-ID: <459BE8E7.6020100@pburns.seanet.com>

You can't do that.  If you want to have different labels
on the first two dimensions, then a 3-dimensional array
doesn't seem to be the natural data structure.

I would suggest two matrices held in a list.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

downunder03 wrote:

>hi all. how can i adress a array directly. for example i wanna give array 1
>other labels than array 2. How can I overcome this problem?
>
>...this doesn't work
>
>tab <- array(1:8, c(2, 2, 2))
>dimnames(tab[,,1]) <- list(c("No","Yes"), c("No","Yes"),c("ARRAY1"))
>dimnames(tab[,,2]) <- list(c("big","small"), c("small","big"),c("ARRAY2"))
>
>
>
>  
>


From ggrothendieck at gmail.com  Wed Jan  3 18:47:36 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 3 Jan 2007 12:47:36 -0500
Subject: [R] accessing arrays
In-Reply-To: <459BE8E7.6020100@pburns.seanet.com>
References: <8142071.post@talk.nabble.com> <459BE8E7.6020100@pburns.seanet.com>
Message-ID: <971536df0701030947n13c02cc6q5d9913f4f58d026c@mail.gmail.com>

Or you could define it as your own class and define your own print and
other methods, e.g.

> X <- structure(array(1:8, c(2,2,2)), class = "twomats")
> attr(X, "DIMNAMES") <- list(list(c("No", "Yes"), c("No", "Yes")),
+    dimnames = list(c("No", "Yes"), c("big", "small")))
>
> print.twomats <- function(x, ...) {
+         Y <- list(X[,,1], X[,,2])
+ for(i in 1:2) {
+ dimnames(Y[[i]]) <- attr(x, "DIMNAMES")[[i]]
+ print(Y[[i]])
+ }
+ }
>
> X
    No Yes
No   1   3
Yes  2   4
    big small
No    5     7
Yes   6     8




On 1/3/07, Patrick Burns <pburns at pburns.seanet.com> wrote:
> You can't do that.  If you want to have different labels
> on the first two dimensions, then a 3-dimensional array
> doesn't seem to be the natural data structure.
>
> I would suggest two matrices held in a list.
>
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> downunder03 wrote:
>
> >hi all. how can i adress a array directly. for example i wanna give array 1
> >other labels than array 2. How can I overcome this problem?
> >
> >...this doesn't work
> >
> >tab <- array(1:8, c(2, 2, 2))
> >dimnames(tab[,,1]) <- list(c("No","Yes"), c("No","Yes"),c("ARRAY1"))
> >dimnames(tab[,,2]) <- list(c("big","small"), c("small","big"),c("ARRAY2"))
> >
> >
> >
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wraff at titus.u-strasbg.fr  Wed Jan  3 18:49:34 2007
From: wraff at titus.u-strasbg.fr (Wolfgang Raffelsberger)
Date: Wed, 03 Jan 2007 18:49:34 +0100
Subject: [R] accessing arrays
In-Reply-To: <8144807.post@talk.nabble.com>
References: <8142071.post@talk.nabble.com>	<459BE0D6.4000807@igbmc.u-strasbg.fr>
	<8144807.post@talk.nabble.com>
Message-ID: <459BECAE.4060900@igbmc.u-strasbg.fr>

Hi Lars,

in a 3-dim array you have 3 axes, day x, y and z (which I named 
xNo/xYes, yNo/yYes and zSmall/zBig)
to assign directly the labels use :

 > tab <- array(1:8, c(2, 2, 2),dimnames= list(c("xNo","xYes"), 
c("yNo","yYes"),c("zBig","zSmall")))
 > tab
, , zBig

     yNo yYes
xNo    1    3
xYes   2    4

, , zSmall

     yNo yYes
xNo    5    7
xYes   6    8

If you wanted to have an array allowing to capture xNo/xYes vs yNo/yYes 
AND zBig","zSmall" vs aaBig","aaSmall" you need to go one dimension 
higher .. but I'm not sure if this is really what you wanted.

Hope this helps,
Wolfgang

Lars Rohrschneider a ?crit :
> Hi Wolfgang,
> thanks for your hint. But I am desperating. I have an 3 dim array of say 10matrices where every matrix has to stick an other label. I found no way todirect assign the labels.
> for example 
> tab <- array(1:8, c(2, 2, 2))dimnames(tab[,,1]) <- list(c("No","Yes"), c("No","Yes"))dimnames(tab[,,2]) <- list(c("big","small"), c("small","big"))
> should look like this, , 1
>     No YesNo   1   3Yes  2   4
> , , 2
>         big  smallsmall   5   7big     6   8
>
>
>
>
>
>
> Wolfgang Raffelsberger wrote:> > try this :> x <- > array(1:24,dim=c(2,3,4),dimnames=list(letters[1:2],LETTERS[1:3],letters[23:26]))> > Cheers,> Wolfgang> > downunder03 a ?crit :>> hi all. how can i adress a array directly. for example i wanna give array>> 1>> other labels than array 2. How can I overcome this problem?>>>> ...this doesn't work>>>> tab <- array(1:8, c(2, 2, 2))>> dimnames(tab[,,1]) <- list(c("No","Yes"), c("No","Yes"),c("ARRAY1"))>> dimnames(tab[,,2]) <- list(c("big","small"),>> c("small","big"),c("ARRAY2"))>>>>>>>>   > > >  > > . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . > . . . . .> > Wolfgang Raffelsberger, PhD> Laboratoire de BioInformatique et G?nomique Int?gratives> IGBMC> 1 rue Laurent Fries,  67404 Illkirch  Strasbourg,  France> Tel (+33) 388 65 3314         Fax (+33) 388 65 3276> wolfgang.raffelsberger at igbmc.u-strasbg.fr> > ______________________________________________> R-help at stat.math.ethz.ch mailing list> https://stat.ethz.ch/mailman/listinfo/r-help> PLEASE do read the posting guide> http://www.R-project.org/posting-guide.html> and provide commented, minimal, self-contained, reproducible code.> > 
> -- View this message in context: http://www.nabble.com/-R--accessing-arrays-tf2913929.html#a8144807Sent from the R help mailing list archive at Nabble.com.
> ______________________________________________R-help at stat.math.ethz.ch mailing listhttps://stat.ethz.ch/mailman/listinfo/r-helpPLEASE do read the posting guide http://www.R-project.org/posting-guide.htmland provide commented, minimal, self-contained, reproducible code.
>
>
>
>   


-- 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . .

Wolfgang Raffelsberger, PhD
Laboratoire de BioInformatique et G?nomique Int?gratives
IGBMC
1 rue Laurent Fries,  67404 Illkirch  Strasbourg,  France
Tel (+33) 388 65 3314         Fax (+33) 388 65 3276
http://www-bio3d-igbmc.u-strasbg.fr/~wraff
wolfgang.raffelsberger at igbmc.u-strasbg.fr


From jose.cortinas at uhasselt.be  Wed Jan  3 13:57:07 2007
From: jose.cortinas at uhasselt.be (Jose Cortinas)
Date: Wed, 3 Jan 2007 13:57:07 +0100
Subject: [R] question about regression forest
Message-ID: <007c01c72f36$acaa7460$6d0ebec1@ibis.luc.ac.be>

Dear All,

My name is Jos? Corti?as Abrahantes, I am statistician and work at the
university in Belgium. I started working recently with machine learning
techniques and I finding a fascinating field. The reason of my email is to
ask you a question related to regression forest. I am interested to compare
the fit of linear regression, regression trees, bagging trees and regression
forest for the case in which we have only one predictor variable. In all the
articles that I have found related to regression forest they reported the
advantages of the use of a random subsets of predictors used to grow the
tree with respect to bagging, in my case I have only one, thus it is not 
really contributing. I was
expecting then to see a similar behaviour than bagging, the rsquared values
produced by both methods are very similar indeed, but what I find strange is
that if I take the 2.5 and 97.5 percentile of all rsquared from each tree
grow the interval obtained for regression forest is much narrower than the
one obtained for bagging. Do anyone know why is this? Thanks in advance.

Best regards and best wishes for 2007,
Jos? Corti?as Abrahantes


From sfalcon at fhcrc.org  Wed Jan  3 17:43:21 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 03 Jan 2007 08:43:21 -0800
Subject: [R] SQLite: When reading a table,
	a "\r" is padded onto the last column. Why?
In-Reply-To: <Pine.LNX.4.64.0701030719120.25219@gannet.stats.ox.ac.uk> (Brian
	Ripley's message of "Wed, 3 Jan 2007 07:43:05 +0000 (GMT)")
References: <C83C5E3DEEE97E498B74729A33F6EAEC038785AA@DJFPOST01.djf.agrsci.dk>
	<C83C5E3DEEE97E498B74729A33F6EAEC038785AB@DJFPOST01.djf.agrsci.dk>
	<Pine.LNX.4.64.0701030719120.25219@gannet.stats.ox.ac.uk>
Message-ID: <m2zm90jc2e.fsf@fhcrc.org>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
> [I am not sure who is actually maintaining RSQLite, so am Cc: both the
> stated maintainer and the person who prepared the package for
> distribution. The posting guide asked you to contact the maintainer:
> what response did _you_ get?]

For the record, I will be (have been) taking on the maintainer role
for RSQLite.  The Maintainer field will be updated in the next
version.

As to the '\r' problem, a release candidate RSQLite 0.4-17 is
available here:

    http://bioconductor.org/packages/misc/

This version uses prepared queries to implement dbWriteTable.  This
should resolve the '\r' issue on Windows and should also be
considerably more efficient.  Soren, can you give this one a try and
let me know if it works for you?

Recent work on RSQLite has focused on integrating SQLite3's type
system into the interface.  We now rely on the column type in the DB
when retrieving results.  Previously, type.convert was used.  I'm
fairly certain these changes will result in changes in behavior -- in
most cases, I think the changes are for the better.

+ seth


From ripley at stats.ox.ac.uk  Wed Jan  3 18:57:46 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 3 Jan 2007 17:57:46 +0000 (GMT)
Subject: [R] understanding integer divide (%/%)
In-Reply-To: <Pine.LNX.4.64.0701030743370.30561@homer21.u.washington.edu>
References: <2E9C414912813E4EB981326983E0A104026EB6B6@inboexch.inbo.be>
	<Pine.LNX.4.64.0701030743370.30561@homer21.u.washington.edu>
Message-ID: <Pine.LNX.4.64.0701031716240.21438@gannet.stats.ox.ac.uk>

On Wed, 3 Jan 2007, Thomas Lumley wrote:

> On Wed, 3 Jan 2007, ONKELINX, Thierry wrote:
>
>> This is due to the internal representation of 0.1, which is not exactly
>> 0.1 but very close to it. If you want to do an integer divide, you
>> should only use integers to divide with.
>
> This must be more-or-less correct, but it is worth noting that
>> 0.1*10==1
> [1] TRUE
>> 1/0.1==10
> [1] TRUE
>> 1%/%0.1==10
> [1] FALSE
> so it isn't quite that simple.
>
> Interestingly, the results seem to vary by system -- on a G4 Mac I get
> 1 %/% (1/x) == x for all x from 1 to 50

And even 1 %/% 0.1 == 10 on my Linux boxes.

Other things which are going on are the use of extra-precision registers 
(and potentially the system floor() function).

%/% (but not / or *) makes use of a round of iterative refinement.
It does

1/0.1 (10)
rounds down (10)
tmp = 1 - 0.1 *10 (slightly negative)
Oops, the answer must be 10 - 1.

This is needed for consistency since

> 1 %% 0.1
[1] 0.1

on MinGW.

I think the Windows answer is correct, as 0.1 will be stored as 1/8 * 
53-bit binary fraction with leading 1, and according to package gmp

> as.bigq(0.1)
[1] "3602879701896397/36028797018963968"

the denominator being 2^55.  So 1 - 10 * 0.1 is -2/2^55 < 0.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From j.hadfield at sheffield.ac.uk  Wed Jan  3 19:39:18 2007
From: j.hadfield at sheffield.ac.uk (Jarrod Hadfield)
Date: Wed, 03 Jan 2007 18:39:18 +0000
Subject: [R] problem with logLik and offsets
Message-ID: <459BF856.4070200@shef.ac.uk>

Hi,

I'm trying to compare models, one of which has all parameters fixed 
using offsets.  The log-likelihoods seem reasonble in all cases except 
the model in which there are no free parameters (model3 in the toy 
example below).  Any help would be appreciated.

Cheers,

Jarrod

x<-rnorm(100)
y<-rnorm(100, 1+x)

model1<-lm(y~x)
logLik(model1)
sum(dnorm(y, predict(model1), summary(model1)$sigma,log=TRUE))

# no offset - in agreement

model2<-lm(y~offset(rep(1,100))+x-1)
logLik(model2)
sum(dnorm(y, predict(model2),summary(model2)$sigma,log=TRUE))

# offset and free parameters - in agreement

model3<-lm(y~offset(rep(1,100))+offset(x)-1)
logLik(model3)
sum(dnorm(y, predict(model3),summary(model3)$sigma,log=TRUE))

# offset only - discrepancy

sum(predict(model3)-c(1+x))

# yet predict is correct


From hstevens at muohio.edu  Wed Jan  3 19:44:09 2007
From: hstevens at muohio.edu (Martin Henry H. Stevens)
Date: Wed, 3 Jan 2007 13:44:09 -0500
Subject: [R] mcmcsamp and variance ratios
Message-ID: <ED68137A-6EEE-40D6-BB97-CBF6D0E40355@muohio.edu>

Hi folks,
I have assumed that ratios of variance components (Fst and Qst in  
population genetics) could be estimated using the output of mcmcsamp  
(the series on mcmc sample estimates of variance components).

What I have started to do is to use the matrix output that included  
the log(variances), exponentiate, calculate the relevant ratio, and  
apply either quantile or or HPDinterval to get confidence intervals.

This seems too simple but I can't think of what is wrong with it.

All thoughts appreciated.

-Hank


Dr. Hank Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/

"E Pluribus Unum"


From Mark.Leeds at morganstanley.com  Wed Jan  3 20:20:17 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Wed, 3 Jan 2007 14:20:17 -0500
Subject: [R] ewma help
Message-ID: <D3AEEDA31E57474B840BEBC25A8A8344F39364@NYWEXMB23.msad.ms.com>

I wrote my own ewma function to deal with the somewhat odd way that
filter handles missing values.
The function I wrote works as long as the NA isn't first but when it is
first I still get a zero in the output.
I'm not expert enough to look at filter and undeerstand what it is
doing.

# 1)  THE FIRST CASE DOESN'T WORK
# I WOULD PREFER A 1 IN THE SECOND ELEMENT OF THE OUTPUT
# BECAUSE THAT IS THE INITIAL NON NA VALUE OF THE SERIES
# I DON"T KNOW WHY A ZERO GETS THERE AND THAT
# MAKES THE REST OF THE SERIES WRONG BECAUSE IT'S
#A RECURSIVE RELATIONSHIP. BASICALLY
#I PREFER THE OUTPUT TO BE THE SAME AS THE SECOND SET OF OUTPUT :
# 1 NA
# 2 1.00
# 3 1.50
# 4 2.25
# WITH THE NA JUST IN A DIFFERENT PLACE

x<-zoo(matrix(c(NA,1,2,3),nc=1))
ewma(x,lambda=0.5)

1 NA
2  0   ( 
3  1
4  2

#=======================================================================
==================================

# 2) THIS CASE DOES WORK

x<-zoo(matrix(c(1,NA,2,3),nc=1))
ewma(x,lambda=0.5)

1 1.00
2   NA
3 1.50
4 2.25



ewma<-function(x,lambda = 1, init = x[1]) {
    # work with 'non-zoo' data for speed and then recombine
    .raw <- coredata(x)
    good.ind <- !is.na(.raw)  # determine good values

    .raw[good.ind] <- filter(lambda * .raw[good.ind], filter=(1-lambda),
        method='recursive', init=coredata(init))
    zoo(.raw, index(x)) # create zoo object for return    
}
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}


From bagatti.davide at gmail.com  Wed Jan  3 20:44:44 2007
From: bagatti.davide at gmail.com (Bagatti Davide)
Date: Wed, 3 Jan 2007 20:44:44 +0100
Subject: [R] loading data and executing queries with R and Mysql
In-Reply-To: <Pine.LNX.4.64.0701031319300.5467@gannet.stats.ox.ac.uk>
References: <51f845600701030505v3b8c2bb6i1171f1eef2e6b1b7@mail.gmail.com>
	<Pine.LNX.4.64.0701031319300.5467@gannet.stats.ox.ac.uk>
Message-ID: <51f845600701031144y26ec0364w50f4d5c58fd27598@mail.gmail.com>

Hello,
thanks for your help.
 I tried with index in database (primary keys and index) but nothing
changed.
My hard disk has two partition FAT 32: in the first there is ubuntu 6.10, in
the second (19 GB with 6GB of free space, never defragmented) there is
Windows XP HE. I have the problem under Windows.
In attach (csv file) you find an example of data I am using: it is only one
step, so I load (with "load data") one file of this type for each step of
the simulation (these are about 1000 steps, but the problem happens when I
load more than 80-100 files ).
The queries are similar to the following:

dbGetQuery(con, paste("select ID,IDAgentPartner,Vote from
TabAgentRelationships where step= ",passo," and Vote > ",min_voto," and
IDAgentPartner!=0",sep=""));

Thanks

Davide

2007/1/3, Prof Brian Ripley <ripley at stats.ox.ac.uk>:
>
> Without any actual example Ias requested in the footer of this message) I
> can only guess, but the most common cause of slow queries is the lack of
> indices in the database, so did you create any?
>
> You haven't told us your actual OS (beyond 'Windows'), but a guess is that
> your processes are I/O bound, and that your file system could well do with
> a tune.  For example, if this is NTFS, is there lots (at least 30%) of
> free space and did you defragment it after saving the data?
>
> On Wed, 3 Jan 2007, Bagatti Davide wrote:
>
> > Hello everyone,
> >
> > I have a problem when I execute queries using R 2.3.1 and MySql server
> 5.0.
> > What I do: I load data in different csv files (every file represents a
> > particular temporal step of a simulation) using Mysql query "load data"
> with
> > RMySQL command DbSendQuery (but the same problem there is also using
> > DbWritetable). Then I use a function where I have a lot of queries that
> > interact with the database.
> > Well, while loading data is very fast, query execution is very slow
> > ...looking to Windows task manager I see that my cpu doesn't go to 100%
> of
> > usage, but only at 30-45%. Looking at the processes I see Rgui.exe use
> > between 0-23% and mysql-nt.exe use between 8-20% and so it is very slow.
> I
> > am sure my cpu has no particular problems.
> > Could you help me?
> >
> > Thanks in advance
> >
> > Davide
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>

From topkatz at msn.com  Wed Jan  3 20:56:21 2007
From: topkatz at msn.com (Talbot Katz)
Date: Wed, 03 Jan 2007 14:56:21 -0500
Subject: [R] na.action and simultaneous regressions
Message-ID: <BAY132-F26F042518BC14B337172CEAAB90@phx.gbl>

Hi.

I am running regressions of several dependent variables using the same set 
of independent variables.  The independent variable values are complete, but 
each dependent variable has some missing values for some observations; by 
default, lm(y1~x) will carry out the regressions using only the observations 
without missing values of y1.  If I do lm(cbind(y1,y2)~x), the default will 
be to use only the observations for which neither y1 nor y2 is missing.  I'd 
like to have the regression for each separate dependent variable use all the 
non-missing cases for that variable.  I would think that there should be a 
way to do that using the na.action option, but I haven't seen this in the 
documentation or figured out how to do it on my own.  Can it be done this 
way, or do I have to code the regressions in a loop?  (By the way, since it 
restricts to non-missing values in all the variables simultaneously, is this 
because it's doing some sort of SUR or other simultaneous equation 
estimation behind the scenes?)

Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell


From deepayan.sarkar at gmail.com  Wed Jan  3 22:30:20 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 3 Jan 2007 13:30:20 -0800
Subject: [R] Lattice / Trellis analog of axis(graphics) ?
In-Reply-To: <459B907C.2010402@lungall.gu.se>
References: <459B907C.2010402@lungall.gu.se>
Message-ID: <eb555e660701031330u31c9424fu300093a1cfc5e6e8@mail.gmail.com>

On 1/3/07, Derek Eder <derek.eder at lungall.gu.se> wrote:
> My question is so basic that I am (almost too) embarrassed to admit that
> I could not find an answer after an hour's worth of homework.
>
> What is the Trellis / Lattice analog for the axis(graphics) function
> that enables the creation of axes in locations other than the default
> (i.e., bottom for X axis and right for Y axis) ?
>
> For example when plotting mileage against weight (in American units),
> one might want to also include a second X axis on the top margin (e.g.,
> axis() pos = 3) with fuel mileage in metric units.
>
> xyplot(Mileage ? Weight, data = fuel.frame)

You might find the help page ?axis.default (and its example) useful.

-Deepayan


From gunter.berton at gene.com  Wed Jan  3 22:46:03 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Wed, 3 Jan 2007 13:46:03 -0800
Subject: [R] na.action and simultaneous regressions
In-Reply-To: <BAY132-F26F042518BC14B337172CEAAB90@phx.gbl>
Message-ID: <00d601c72f80$91450f10$4d908980@gne.windows.gene.com>

As the Help page says:

If response is a matrix a linear model is fitted separately by least-squares
to each column of the matrix

So there's nothing hidden going on "behind the scenes," and
apply(cbind(y1,y2),2,function(z)lm(z~x)) (or an explicit loop, of course)
will produce a list each of whose components is a separate fit using all the
nonmissing data in the column. 

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Talbot Katz
Sent: Wednesday, January 03, 2007 11:56 AM
To: r-help at stat.math.ethz.ch
Subject: [R] na.action and simultaneous regressions

Hi.

I am running regressions of several dependent variables using the same set 
of independent variables.  The independent variable values are complete, but

each dependent variable has some missing values for some observations; by 
default, lm(y1~x) will carry out the regressions using only the observations

without missing values of y1.  If I do lm(cbind(y1,y2)~x), the default will 
be to use only the observations for which neither y1 nor y2 is missing.  I'd

like to have the regression for each separate dependent variable use all the

non-missing cases for that variable.  I would think that there should be a 
way to do that using the na.action option, but I haven't seen this in the 
documentation or figured out how to do it on my own.  Can it be done this 
way, or do I have to code the regressions in a loop?  (By the way, since it 
restricts to non-missing values in all the variables simultaneously, is this

because it's doing some sort of SUR or other simultaneous equation 
estimation behind the scenes?)

Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From hu-student at web.de  Wed Jan  3 22:57:28 2007
From: hu-student at web.de (Lars Rohrschneider)
Date: Wed, 3 Jan 2007 13:57:28 -0800 (PST)
Subject: [R] accessing arrays
In-Reply-To: <971536df0701030947n13c02cc6q5d9913f4f58d026c@mail.gmail.com>
References: <8142071.post@talk.nabble.com> <459BE8E7.6020100@pburns.seanet.com>
	<971536df0701030947n13c02cc6q5d9913f4f58d026c@mail.gmail.com>
Message-ID: <8149519.post@talk.nabble.com>


Thanks all for your hints and extensive codes. With list it seemed to work.
lars



Gabor Grothendieck wrote:
> 
> Or you could define it as your own class and define your own print and
> other methods, e.g.
> 
>> X <- structure(array(1:8, c(2,2,2)), class = "twomats")
>> attr(X, "DIMNAMES") <- list(list(c("No", "Yes"), c("No", "Yes")),
> +    dimnames = list(c("No", "Yes"), c("big", "small")))
>>
>> print.twomats <- function(x, ...) {
> +         Y <- list(X[,,1], X[,,2])
> + for(i in 1:2) {
> + dimnames(Y[[i]]) <- attr(x, "DIMNAMES")[[i]]
> + print(Y[[i]])
> + }
> + }
>>
>> X
>     No Yes
> No   1   3
> Yes  2   4
>     big small
> No    5     7
> Yes   6     8
> 
> 
> 
> 
> On 1/3/07, Patrick Burns <pburns at pburns.seanet.com> wrote:
>> You can't do that.  If you want to have different labels
>> on the first two dimensions, then a 3-dimensional array
>> doesn't seem to be the natural data structure.
>>
>> I would suggest two matrices held in a list.
>>
>> Patrick Burns
>> patrick at burns-stat.com
>> +44 (0)20 8525 0696
>> http://www.burns-stat.com
>> (home of S Poetry and "A Guide for the Unwilling S User")
>>
>> downunder03 wrote:
>>
>> >hi all. how can i adress a array directly. for example i wanna give
>> array 1
>> >other labels than array 2. How can I overcome this problem?
>> >
>> >...this doesn't work
>> >
>> >tab <- array(1:8, c(2, 2, 2))
>> >dimnames(tab[,,1]) <- list(c("No","Yes"), c("No","Yes"),c("ARRAY1"))
>> >dimnames(tab[,,2]) <- list(c("big","small"),
>> c("small","big"),c("ARRAY2"))
>> >
>> >
>> >
>> >
>> >
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/-R--accessing-arrays-tf2913929.html#a8149519
Sent from the R help mailing list archive at Nabble.com.


From jeff.horner at vanderbilt.edu  Wed Jan  3 23:09:32 2007
From: jeff.horner at vanderbilt.edu (Jeffrey Horner)
Date: Wed, 03 Jan 2007 16:09:32 -0600
Subject: [R] Help with filled.contour()
In-Reply-To: <loom.20070102T090808-68@post.gmane.org>
References: <758246BF-9625-4F20-948A-CFC120689A83@virginia.edu>	<loom.20070101T200956-783@post.gmane.org>	<990E1DC3-4FAD-4297-B3FF-D12F6710B46F@virginia.edu>
	<loom.20070102T090808-68@post.gmane.org>
Message-ID: <459C299C.1020002@vanderbilt.edu>

Dieter Menne wrote:
> Michael Kubovy <kubovy <at> virginia.edu> writes:
> 
>> I tried and it gave a strange result. See
>> http://people.virginia.edu/~mk9y/mySite/twoGaussian.R
>> and
>> http://people.virginia.edu/~mk9y/mySite/twoGaussian.pdf
>>
>> *********************************
>> Session Info
>> *********************************
>>  > sessionInfo()
>> R version 2.4.1 (2006-12-18)
>> powerpc-apple-darwin8.8.0
> 
> Hmm, strange, I can reproduce your problem on Windows (otherwise same config)
> with pdf, but it looks beautifully on screen for me if I mentally remove the
> ugly legend.


Try the image function. The smoothness of the plot will be proportional 
to the length of x and y. For instance 200 isn't bad:

mu1 <- 0
mu2 <- 5
s <- 1
x <- seq(-2.5, 7.5, length = 200)
y <- seq(-2.5, 2.5, length = 200)
f <- function(x,y){
     term1 <- 1/(2*pi*sqrt(s*s))
     term2 <- -1/2
     term3 <- (x - mu1)^2/s
     term4 <- (y - mu1)^2/s
     term5 <- (x - mu2)^2/s
     term1*(.5 * exp(term2*(term3 + term4)) + .5 * exp(term2*(term5 + 
term4)))
} # setting up the function of the multivariate normal density
z <- outer(x, y, f)
# persp(x, y, z)
require(grDevices)
#pdf('twoGaussian.pdf')
#filled.contour(x, y, z, axes = F, frame.plot = F, asp = 1,
#               col = gray(seq(0, 0.9, len = 25)), nlevels = 25)
image(x,y,z,col=gray(seq(0,0.9,len=200)))

Cheers,

Jeff
-- 
http://biostat.mc.vanderbilt.edu/JeffreyHorner


From rvaradhan at jhmi.edu  Wed Jan  3 23:14:35 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Wed, 3 Jan 2007 17:14:35 -0500
Subject: [R] na.action and simultaneous regressions
In-Reply-To: <00d601c72f80$91450f10$4d908980@gne.windows.gene.com>
Message-ID: <000601c72f84$8daf4150$7c94100a@win.ad.jhu.edu>

No, Bert, lm doesn't produce a list each of whose components is a separate
fit using "all" the nonmissing data in the column.  It is true that the
regressions are independently performed, but when the response matrix is
passed from "lm" on to "lm.fit", only the complete rows are passed, i.e.
rows with no missing values.  I looked at "lm" function, but it was not
obvious to me how to fix it.  

In the following toy example, the degrees of freedom for y1 regression
should be 18 and that for y2 should be 15, but both degrees of freedom are
only 15.

> y1 <- runif(20)
> y2 <- c(runif(17), rep(NA,3))
> x <- rnorm(20)
> summary(lm(cbind(y1,y2) ~ x))
Response y1 :

Call:
lm(formula = y1 ~ x)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.52592 -0.22632 -0.00964  0.25117  0.31227 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.56989    0.06902   8.257 5.82e-07 ***
x           -0.12325    0.06516  -1.891    0.078 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.2798 on 15 degrees of freedom
Multiple R-Squared: 0.1926,     Adjusted R-squared: 0.1387 
F-statistic: 3.577 on 1 and 15 DF,  p-value: 0.07804 


Response y2 :

Call:
lm(formula = y2 ~ x)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.48880 -0.28552 -0.06022  0.23167  0.54425 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.43712    0.07686   5.687 4.31e-05 ***
x            0.10278    0.07257   1.416    0.177    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.3115 on 15 degrees of freedom
Multiple R-Squared: 0.118,      Adjusted R-squared: 0.05915 
F-statistic: 2.006 on 1 and 15 DF,  p-value: 0.1771 


Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bert Gunter
Sent: Wednesday, January 03, 2007 4:46 PM
To: 'Talbot Katz'; r-help at stat.math.ethz.ch
Subject: Re: [R] na.action and simultaneous regressions

As the Help page says:

If response is a matrix a linear model is fitted separately by least-squares
to each column of the matrix

So there's nothing hidden going on "behind the scenes," and
apply(cbind(y1,y2),2,function(z)lm(z~x)) (or an explicit loop, of course)
will produce a list each of whose components is a separate fit using all the
nonmissing data in the column. 

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Talbot Katz
Sent: Wednesday, January 03, 2007 11:56 AM
To: r-help at stat.math.ethz.ch
Subject: [R] na.action and simultaneous regressions

Hi.

I am running regressions of several dependent variables using the same set 
of independent variables.  The independent variable values are complete, but

each dependent variable has some missing values for some observations; by 
default, lm(y1~x) will carry out the regressions using only the observations

without missing values of y1.  If I do lm(cbind(y1,y2)~x), the default will 
be to use only the observations for which neither y1 nor y2 is missing.  I'd

like to have the regression for each separate dependent variable use all the

non-missing cases for that variable.  I would think that there should be a 
way to do that using the na.action option, but I haven't seen this in the 
documentation or figured out how to do it on my own.  Can it be done this 
way, or do I have to code the regressions in a loop?  (By the way, since it 
restricts to non-missing values in all the variables simultaneously, is this

because it's doing some sort of SUR or other simultaneous equation 
estimation behind the scenes?)

Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From gunter.berton at gene.com  Wed Jan  3 23:19:39 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Wed, 3 Jan 2007 14:19:39 -0800
Subject: [R] na.action and simultaneous regressions
In-Reply-To: <000601c72f84$8daf4150$7c94100a@win.ad.jhu.edu>
Message-ID: <00e001c72f85$42af5090$4d908980@gne.windows.gene.com>

Ravi:

You misinterpreted my reply -- perhaps I was unclear. I did **not** say that
lm() with a matrix response would do it, but that the apply construction or
an explicit loop would. As you and the poster noted, lm() produces a
separate fit to each column of only the rowwise complete data.


Bert Gunter


-----Original Message-----
From: Ravi Varadhan [mailto:rvaradhan at jhmi.edu] 
Sent: Wednesday, January 03, 2007 2:15 PM
To: 'Bert Gunter'; 'Talbot Katz'; r-help at stat.math.ethz.ch
Subject: RE: [R] na.action and simultaneous regressions

No, Bert, lm doesn't produce a list each of whose components is a separate
fit using "all" the nonmissing data in the column.  It is true that the
regressions are independently performed, but when the response matrix is
passed from "lm" on to "lm.fit", only the complete rows are passed, i.e.
rows with no missing values.  I looked at "lm" function, but it was not
obvious to me how to fix it.  

In the following toy example, the degrees of freedom for y1 regression
should be 18 and that for y2 should be 15, but both degrees of freedom are
only 15.

> y1 <- runif(20)
> y2 <- c(runif(17), rep(NA,3))
> x <- rnorm(20)
> summary(lm(cbind(y1,y2) ~ x))
Response y1 :

Call:
lm(formula = y1 ~ x)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.52592 -0.22632 -0.00964  0.25117  0.31227 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.56989    0.06902   8.257 5.82e-07 ***
x           -0.12325    0.06516  -1.891    0.078 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.2798 on 15 degrees of freedom
Multiple R-Squared: 0.1926,     Adjusted R-squared: 0.1387 
F-statistic: 3.577 on 1 and 15 DF,  p-value: 0.07804 


Response y2 :

Call:
lm(formula = y2 ~ x)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.48880 -0.28552 -0.06022  0.23167  0.54425 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.43712    0.07686   5.687 4.31e-05 ***
x            0.10278    0.07257   1.416    0.177    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.3115 on 15 degrees of freedom
Multiple R-Squared: 0.118,      Adjusted R-squared: 0.05915 
F-statistic: 2.006 on 1 and 15 DF,  p-value: 0.1771 


Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bert Gunter
Sent: Wednesday, January 03, 2007 4:46 PM
To: 'Talbot Katz'; r-help at stat.math.ethz.ch
Subject: Re: [R] na.action and simultaneous regressions

As the Help page says:

If response is a matrix a linear model is fitted separately by least-squares
to each column of the matrix

So there's nothing hidden going on "behind the scenes," and
apply(cbind(y1,y2),2,function(z)lm(z~x)) (or an explicit loop, of course)
will produce a list each of whose components is a separate fit using all the
nonmissing data in the column. 

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Talbot Katz
Sent: Wednesday, January 03, 2007 11:56 AM
To: r-help at stat.math.ethz.ch
Subject: [R] na.action and simultaneous regressions

Hi.

I am running regressions of several dependent variables using the same set 
of independent variables.  The independent variable values are complete, but

each dependent variable has some missing values for some observations; by 
default, lm(y1~x) will carry out the regressions using only the observations

without missing values of y1.  If I do lm(cbind(y1,y2)~x), the default will 
be to use only the observations for which neither y1 nor y2 is missing.  I'd

like to have the regression for each separate dependent variable use all the

non-missing cases for that variable.  I would think that there should be a 
way to do that using the na.action option, but I haven't seen this in the 
documentation or figured out how to do it on my own.  Can it be done this 
way, or do I have to code the regressions in a loop?  (By the way, since it 
restricts to non-missing values in all the variables simultaneously, is this

because it's doing some sort of SUR or other simultaneous equation 
estimation behind the scenes?)

Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From rvaradhan at jhmi.edu  Wed Jan  3 23:21:01 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Wed, 3 Jan 2007 17:21:01 -0500
Subject: [R] na.action and simultaneous regressions
In-Reply-To: <00d601c72f80$91450f10$4d908980@gne.windows.gene.com>
Message-ID: <000a01c72f85$7375e310$7c94100a@win.ad.jhu.edu>

Sorry, Bert.  I didn't notice your use of "apply", which will indeed give
you separate regression results using all available data.  But I was
wondering, if there was a way to modify "lm" to be able to accomplish this,
since it is doing separate regressions anyway.

Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bert Gunter
Sent: Wednesday, January 03, 2007 4:46 PM
To: 'Talbot Katz'; r-help at stat.math.ethz.ch
Subject: Re: [R] na.action and simultaneous regressions

As the Help page says:

If response is a matrix a linear model is fitted separately by least-squares
to each column of the matrix

So there's nothing hidden going on "behind the scenes," and
apply(cbind(y1,y2),2,function(z)lm(z~x)) (or an explicit loop, of course)
will produce a list each of whose components is a separate fit using all the
nonmissing data in the column. 

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Talbot Katz
Sent: Wednesday, January 03, 2007 11:56 AM
To: r-help at stat.math.ethz.ch
Subject: [R] na.action and simultaneous regressions

Hi.

I am running regressions of several dependent variables using the same set 
of independent variables.  The independent variable values are complete, but

each dependent variable has some missing values for some observations; by 
default, lm(y1~x) will carry out the regressions using only the observations

without missing values of y1.  If I do lm(cbind(y1,y2)~x), the default will 
be to use only the observations for which neither y1 nor y2 is missing.  I'd

like to have the regression for each separate dependent variable use all the

non-missing cases for that variable.  I would think that there should be a 
way to do that using the na.action option, but I haven't seen this in the 
documentation or figured out how to do it on my own.  Can it be done this 
way, or do I have to code the regressions in a loop?  (By the way, since it 
restricts to non-missing values in all the variables simultaneously, is this

because it's doing some sort of SUR or other simultaneous equation 
estimation behind the scenes?)

Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From topkatz at msn.com  Wed Jan  3 23:23:04 2007
From: topkatz at msn.com (Talbot Katz)
Date: Wed, 03 Jan 2007 17:23:04 -0500
Subject: [R] na.action and simultaneous regressions
In-Reply-To: <00d601c72f80$91450f10$4d908980@gne.windows.gene.com>
Message-ID: <BAY132-F4D2CA7FC1A86AD832C3D7AAB90@phx.gbl>

Hi Bert.

Thank you so much, your solution with "apply" works perfectly.  Sorry, I 
know this was an elementary question, and I saw the statement you referred 
to on the Help page.  I just wasn't sure why, considering that there is a 
facility for na options, the option of treating the dependent variables 
separately with respect to missing values wasn't included.  Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell



>From: Bert Gunter <gunter.berton at gene.com>
>To: "'Talbot Katz'" <topkatz at msn.com>, <r-help at stat.math.ethz.ch>
>Subject: RE: [R] na.action and simultaneous regressions
>Date: Wed, 3 Jan 2007 13:46:03 -0800
>
>As the Help page says:
>
>If response is a matrix a linear model is fitted separately by 
>least-squares
>to each column of the matrix
>
>So there's nothing hidden going on "behind the scenes," and
>apply(cbind(y1,y2),2,function(z)lm(z~x)) (or an explicit loop, of course)
>will produce a list each of whose components is a separate fit using all 
>the
>nonmissing data in the column.
>
>Bert Gunter
>Genentech Nonclinical Statistics
>South San Francisco, CA 94404
>
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Talbot Katz
>Sent: Wednesday, January 03, 2007 11:56 AM
>To: r-help at stat.math.ethz.ch
>Subject: [R] na.action and simultaneous regressions
>
>Hi.
>
>I am running regressions of several dependent variables using the same set
>of independent variables.  The independent variable values are complete, 
>but
>
>each dependent variable has some missing values for some observations; by
>default, lm(y1~x) will carry out the regressions using only the 
>observations
>
>without missing values of y1.  If I do lm(cbind(y1,y2)~x), the default will
>be to use only the observations for which neither y1 nor y2 is missing.  
>I'd
>
>like to have the regression for each separate dependent variable use all 
>the
>
>non-missing cases for that variable.  I would think that there should be a
>way to do that using the na.action option, but I haven't seen this in the
>documentation or figured out how to do it on my own.  Can it be done this
>way, or do I have to code the regressions in a loop?  (By the way, since it
>restricts to non-missing values in all the variables simultaneously, is 
>this
>
>because it's doing some sort of SUR or other simultaneous equation
>estimation behind the scenes?)
>
>Thanks!
>
>--  TMK  --
>212-460-5430	home
>917-656-5351	cell
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>


From karl.sommer at dpi.vic.gov.au  Wed Jan  3 23:30:48 2007
From: karl.sommer at dpi.vic.gov.au (karl.sommer at dpi.vic.gov.au)
Date: Thu, 4 Jan 2007 09:30:48 +1100
Subject: [R] R grahics: Save as hangs computer
Message-ID: <OF85BB75BE.88D54447-ONCA257258.00788F8B-CA257258.007BAB6C@nre.vic.gov.au>

Hello list,

I have encountered a problem trying to save graphs using the R-graphics
menu:  File|Save as.  The menu suggests that files may be saved as either
Metafile, Postscript, pdf, png, bmp, jpeg.
When I specify any of those file formats a menu comes up requesting a file
name.  After providing a name R invariably hangs and has to be restarted.

I am able to save files under the various formats using the command line
without problems.  However, sometimes it would be convenient to use the
menus.

I was wondering if anyone else had encountered a similar behaviour and had
found a remedy.

I am running are under GNU-Emacs ESS 5.3.3.

> sessionInfo()
Version 2.3.0 (2006-04-24)
i386-pc-mingw32

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
 lattice
"0.13-8"

Regards

Karl

_________________________________
Dr Karl J Sommer,
Department of Primary Industries,
Catchment & Agriculture Services,
PO Box 905
Mildura, VIC, 3502
Australia

Tel: +61 (0)3 5051 4390
Fax +61 (0)3 5051 4534

Email:     karl.sommer at dpi.vic.gov.au


From murdoch at stats.uwo.ca  Thu Jan  4 00:10:03 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 03 Jan 2007 18:10:03 -0500
Subject: [R] R grahics: Save as hangs computer
In-Reply-To: <OF85BB75BE.88D54447-ONCA257258.00788F8B-CA257258.007BAB6C@nre.vic.gov.au>
References: <OF85BB75BE.88D54447-ONCA257258.00788F8B-CA257258.007BAB6C@nre.vic.gov.au>
Message-ID: <459C37CB.5020404@stats.uwo.ca>

On 1/3/2007 5:30 PM, karl.sommer at dpi.vic.gov.au wrote:
> Hello list,
> 
> I have encountered a problem trying to save graphs using the R-graphics
> menu:  File|Save as.  The menu suggests that files may be saved as either
> Metafile, Postscript, pdf, png, bmp, jpeg.
> When I specify any of those file formats a menu comes up requesting a file
> name.  After providing a name R invariably hangs and has to be restarted.
> 
> I am able to save files under the various formats using the command line
> without problems.  However, sometimes it would be convenient to use the
> menus.
> 
> I was wondering if anyone else had encountered a similar behaviour and had
> found a remedy.
> 
> I am running are under GNU-Emacs ESS 5.3.3.
> 
>> sessionInfo()
> Version 2.3.0 (2006-04-24)

That version is out of date.  Could you please update to the current 
version (2.4.1), and see if the problem persists?  If so, could you 
please try it when running Rterm or Rgui on its own, rather than running 
under Emacs?

Thanks.

Duncan Murdoch

> i386-pc-mingw32
> 
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
> 
> other attached packages:
>  lattice
> "0.13-8"
> 
> Regards
> 
> Karl
> 
> _________________________________
> Dr Karl J Sommer,
> Department of Primary Industries,
> Catchment & Agriculture Services,
> PO Box 905
> Mildura, VIC, 3502
> Australia
> 
> Tel: +61 (0)3 5051 4390
> Fax +61 (0)3 5051 4534
> 
> Email:     karl.sommer at dpi.vic.gov.au
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From kubovy at virginia.edu  Thu Jan  4 01:37:17 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Wed, 3 Jan 2007 19:37:17 -0500
Subject: [R] Help with filled.contour()
In-Reply-To: <459C299C.1020002@vanderbilt.edu>
References: <758246BF-9625-4F20-948A-CFC120689A83@virginia.edu>	<loom.20070101T200956-783@post.gmane.org>	<990E1DC3-4FAD-4297-B3FF-D12F6710B46F@virginia.edu>
	<loom.20070102T090808-68@post.gmane.org>
	<459C299C.1020002@vanderbilt.edu>
Message-ID: <10BCDE2A-EDCF-4F4D-BBA5-BB796191899D@virginia.edu>

To Jeff: a tip of the hat.

I have another question after Jeff's solution:

On Jan 3, 2007, at 5:09 PM, Jeffrey Horner wrote:

> Michael Kubovy <kubovy <at> virginia.edu> writes:
>> I tried and it gave a strange result. See
>> http://people.virginia.edu/~mk9y/mySite/twoGaussian.R
>> and
>> http://people.virginia.edu/~mk9y/mySite/twoGaussian.pdf
>
>
> Try the image function. The smoothness of the plot will be  
> proportional to the length of x and y. For instance 200 isn't bad:
>
> mu1 <- 0
> mu2 <- 5
> s <- 1
> x <- seq(-2.5, 7.5, length = 200)
> y <- seq(-2.5, 2.5, length = 200)
> f <- function(x,y){
>     term1 <- 1/(2*pi*sqrt(s*s))
>     term2 <- -1/2
>     term3 <- (x - mu1)^2/s
>     term4 <- (y - mu1)^2/s
>     term5 <- (x - mu2)^2/s
>     term1*(.5 * exp(term2*(term3 + term4)) + .5 * exp(term2*(term5  
> + term4)))
> } # setting up the function of the multivariate normal density
> z <- outer(x, y, f)
> # persp(x, y, z)
> require(grDevices)
> #pdf('twoGaussian.pdf')
> #filled.contour(x, y, z, axes = F, frame.plot = F, asp = 1,
> #               col = gray(seq(0, 0.9, len = 25)), nlevels = 25)
> image(x,y,z,col=gray(seq(0,0.9,len=200)))

Is there a simpler way to get rid of axes, frame, and axis labels than
image(x, y, z, col = gray(seq(0, 0.9, len = 200)), asp = 1, xaxt =  
'n', yaxt = 'n', bty = 'n', ann = F)
?

_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From ronggui.huang at gmail.com  Thu Jan  4 02:22:23 2007
From: ronggui.huang at gmail.com (ronggui)
Date: Thu, 4 Jan 2007 09:22:23 +0800
Subject: [R] SQLite: When reading a table,
	a "\r" is padded onto the last column. Why?
In-Reply-To: <m2zm90jc2e.fsf@fhcrc.org>
References: <C83C5E3DEEE97E498B74729A33F6EAEC038785AA@DJFPOST01.djf.agrsci.dk>
	<C83C5E3DEEE97E498B74729A33F6EAEC038785AB@DJFPOST01.djf.agrsci.dk>
	<Pine.LNX.4.64.0701030719120.25219@gannet.stats.ox.ac.uk>
	<m2zm90jc2e.fsf@fhcrc.org>
Message-ID: <38b9f0350701031722h2099128fld57807a1e33965b7@mail.gmail.com>

On 1/4/07, Seth Falcon <sfalcon at fhcrc.org> wrote:
> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
> > [I am not sure who is actually maintaining RSQLite, so am Cc: both the
> > stated maintainer and the person who prepared the package for
> > distribution. The posting guide asked you to contact the maintainer:
> > what response did _you_ get?]
>
> For the record, I will be (have been) taking on the maintainer role
> for RSQLite.  The Maintainer field will be updated in the next
> version.
>
> As to the '\r' problem, a release candidate RSQLite 0.4-17 is
> available here:
>
>     http://bioconductor.org/packages/misc/
>
> This version uses prepared queries to implement dbWriteTable.  This
> should resolve the '\r' issue on Windows and should also be
> considerably more efficient.  Soren, can you give this one a try and
> let me know if it works for you?

When write a data frame to db table, the problem of "\r" is fixed. But
for importing data frome file, the problem is still there. When if the
final line lacks the eol sign "\n", "\001x\001(" comes up.

> dbWriteTable(con,"test","c:/test.txt",sep="\t",head=T,over=T,eol="\n")
[1] TRUE
Warning message:
incomplete final line found by readTableHeader on 'c://test.txt'
> dbReadTable(con,"test")
  a           b
1 1         2\r
2 3          \r
3 1         3\r
4 0 5\001x\001(
> dbWriteTable(con,"test","c:/test.txt",sep="\t",head=T,over=T,eol="\n")
[1] TRUE
> dbReadTable(con,"test")
  a   b
1 1 2\r
2 3  \r
3 1 3\r
4 0 5\r
>   data(USArrests)
>         dbWriteTable(con, "USArrests", USArrests, overwrite = T)
[1] TRUE
> dbReadTable(con, "USArrests")
               Murder Assault UrbanPop Rape
Alabama          13.2     236       58 21.2
Alaska           10.0     263       48 44.5
Arizona           8.1     294       80 31.0
Arkansas          8.8     190       50 19.5
California        9.0     276       91 40.6
Colorado          7.9     204       78 38.7


> sessionInfo()
R version 2.4.0 Patched (2006-11-21 r39949)
i386-pc-mingw32

locale:
LC_COLLATE=Chinese_People's Republic of
China.936;LC_CTYPE=Chinese_People's Republic of
China.936;LC_MONETARY=Chinese_People's Republic of
China.936;LC_NUMERIC=C;LC_TIME=Chinese_People's Republic of China.936

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"

other attached packages:
 RSQLite      DBI
"0.4-17" "0.1-11"

> Recent work on RSQLite has focused on integrating SQLite3's type
> system into the interface.  We now rely on the column type in the DB
> when retrieving results.  Previously, type.convert was used.  I'm
> fairly certain these changes will result in changes in behavior -- in
> most cases, I think the changes are for the better.
>
> + seth
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ronggui Huang
Department of Sociology
Fudan University, Shanghai, China
??????
????????????????


From lacher at psy.uq.edu.au  Thu Jan  4 03:35:01 2007
From: lacher at psy.uq.edu.au (Philippe Lacherez)
Date: Thu, 4 Jan 2007 12:35:01 +1000 (EST)
Subject: [R] Help re zinb model
Message-ID: <Pine.GSO.4.64.0701041233070.6817@psych.psy.uq.edu.au>


Hi all,

I am hoping someone can help with a problem I have. I want to do a 
zero-inflated negative binomial model on some count data.  I have found 
how to get the model (using zicounts), and the test of each predictor on 
both the negative binomial and zero-inflated parts of the distribution. 
Can anyone tell me how I can also get an omnibus test of significance for 
the fit of the model?  Stata I think gives a likelihood ratio chi-square 
test of the full versus null model for the zinb model.  Is there a way to 
get this in R?  Alternatively, is there a way I use the deviance, or 
maximum likelihood value to derive this?

Cheers
Philippe Lacherez


From murdoch at stats.uwo.ca  Thu Jan  4 05:10:46 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 03 Jan 2007 23:10:46 -0500
Subject: [R] rJava help
Message-ID: <459C7E46.3080701@stats.uwo.ca>

I've just bought a couple of "iButton Thermochrons" (logging 
thermometers), and I'd like to access them through their Java interface 
from R.  But I've never really used Java, so I'm running into a problem, 
and I hope there's a very simple solution.

I've managed to use rJava to create an object with

 > provider <- .jnew("com/dalsemi/onewire/OneWireAccessProvider")
 > .jcall(provider,"Ljava/lang/Class;", "getClass")
[1] "Java-Object{class com.dalsemi.onewire.OneWireAccessProvider}"

and can see that it has a "getDefaultAdapter" method:

 > .jmethods(provider,"getDefaultAdapter")
[1] "public static com.dalsemi.onewire.adapter.DSPortAdapter 
com.dalsemi.onewire.OneWireAccessProvider.getDefaultAdapter() throws 
com.dalsemi.onewire.adapter.OneWireIOException,com.dalsemi.onewire.OneWireException"

but when I call that, I get an error:

 > adapter <- .jcall(provider, 
"Lcom/dalsemi/onewire/adapter/DSPortAdapter;", "getDefaultAdapter")
Error in .jcall(provider, "Lcom/dalsemi/onewire/adapter/DSPortAdapter;",  :
         RcallMethod: method not found

Is it obvious what I am doing wrong?

Duncan Murdoch


From Augusto.Sanabria at ga.gov.au  Thu Jan  4 05:56:59 2007
From: Augusto.Sanabria at ga.gov.au (Augusto.Sanabria at ga.gov.au)
Date: Thu, 4 Jan 2007 15:56:59 +1100
Subject: [R] Software for kriging
Message-ID: <9B2962F493D17F4F81A9211B1F9C56FB9B1A69@mail.ga.gov.au>


Dear R-list members,

I wish everyone a happy and successful 2007!

Does anyone know of R-based software for
optimal spatial prediction (kriging)?

We are working on a seismic event characterisation
technique and need to do some kriging.

Any help would be greatly appreciated.

Augusto



--------------------------------------------
Augusto Sanabria. MSc, PhD.
Mathematical Modeller
Risk Research Group
Geospatial & Earth Monitoring Division
Geoscience Australia (www.ga.gov.au)
Cnr. Jerrabomberra Av. & Hindmarsh Dr.
Symonston ACT 2601
Ph. (02) 6249-9155


From blomsp at ozemail.com.au  Thu Jan  4 06:20:58 2007
From: blomsp at ozemail.com.au (blomsp at ozemail.com.au)
Date: Thu, 04 Jan 2007 14:20:58 +0900
Subject: [R] Software for kriging
Message-ID: <20070104142058.14895cgvoam848og@mymail.com.au>

packages gstat and geoR both have kriging functions. There are probably others.
Have a look at the spatial task view on CRAN.

HTH,

Simon.

Augusto.Sanabria at ga.gov.au wrote:
> Dear R-list members,
>
> I wish everyone a happy and successful 2007!
>
> Does anyone know of R-based software for
> optimal spatial prediction (kriging)?
>
> We are working on a seismic event characterisation
> technique and need to do some kriging.
>
> Any help would be greatly appreciated.
>
> Augusto
>
>
>
> --------------------------------------------
> Augusto Sanabria. MSc, PhD.
> Mathematical Modeller
> Risk Research Group
> Geospatial & Earth Monitoring Division
> Geoscience Australia (www.ga.gov.au)
> Cnr. Jerrabomberra Av. & Hindmarsh Dr.
> Symonston ACT 2601
> Ph. (02) 6249-9155
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C

The combination of some data and an aching desire for
an answer does not ensure that a reasonable answer
can be extracted from a given body of data.
- John Tukey.



----------------------------------------------------------------
This message was sent using MyMail


From wangtong at usc.edu  Thu Jan  4 06:30:59 2007
From: wangtong at usc.edu (Tong Wang)
Date: Wed, 03 Jan 2007 21:30:59 -0800
Subject: [R] need help with debug package
Message-ID: <dde0d92f3e6f.459c2093@usc.edu>

Hi all,     
      I met a problem while using the debug package,  I have the following program: 

mainfun<- function(){
           beta<-1
           result<-subfun(beta+x)
}
                     
subfun<-function(expr){
           y <- eval(expr, envir=list(x=c(1,2)),enclos = parent.frame())
          return(y)
}

I have no problem using this program without calling the debug package.   but once 
I mtrace(subfun),  the debugger can't find all the beta after entering subfun , and give 
the message :     "Error in beta : non-numeric argument to binary operator"

Is there anyway to get around ?

thanks a lot 
happy new year


From arun.kumar.saha at gmail.com  Thu Jan  4 06:36:09 2007
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Thu, 4 Jan 2007 11:06:09 +0530
Subject: [R] Time series plot
Message-ID: <d4c57560701032136t3d6e032fm9b0d2368ecdb9389@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070104/78a8f035/attachment.pl 

From ggrothendieck at gmail.com  Thu Jan  4 06:48:03 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 4 Jan 2007 00:48:03 -0500
Subject: [R] Time series plot
In-Reply-To: <d4c57560701032136t3d6e032fm9b0d2368ecdb9389@mail.gmail.com>
References: <d4c57560701032136t3d6e032fm9b0d2368ecdb9389@mail.gmail.com>
Message-ID: <971536df0701032148v3036e1b7i4d8d192e5bacdf@mail.gmail.com>

You can use read.zoo in the zoo package to read in the data
and then see:

https://www.stat.math.ethz.ch/pipermail/r-help/2006-December/122742.html

See ?axis for creating additional axes with classic graphics and

library(lattice)
?panel.axis

in lattice graphics.  Search the archives for examples.

On 1/4/07, Arun Kumar Saha <arun.kumar.saha at gmail.com> wrote:
> Dear all R users,
>
> Suppose I have a data set like this:
>
> date              price
>
> 1-Jan-02     4.8803747
> 2-Jan-02     4.8798430
> 3-Jan-02     4.8840133
> 4-Jan-02     4.8803747
> 5-Jan-02     4.8749683
> 6-Jan-02     4.8754263
> 7-Jan-02     4.8746628
> 8-Jan-02     4.8753500
> 9-Jan-02     4.8882416
> 10-Jan-02     4.8895217
> 11-Jan-02     4.8871108
>
> I want to get a time series plot of that dataset. But in x-axis I want to
> see the first day, and last day, and other day in between them  i.e.
> 1-Jan-02,  6-Jan-02, and  11-Jan-02 only. Can anyone tell me how to do that?
>
> My second question is that is there any way to define a secondary axis like
> Microsoft Excel in the same plot window?
>
> Thanks and regards,
> Arun
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From blomsp at ozemail.com.au  Thu Jan  4 07:13:51 2007
From: blomsp at ozemail.com.au (blomsp at ozemail.com.au)
Date: Thu, 04 Jan 2007 15:13:51 +0900
Subject: [R] Help re zinb model
Message-ID: <20070104151351.1yr9rakbu4w8gsc8@mymail.com.au>

Remember that -2 * the difference in the likelihoods between the two models is
asymptotically chi-squared distributed, with degrees of freedom equal to the
difference in number of parameters between the models. So you can just
calculate that for your preferred and null models, then use the pchisq function
to test significance. Get the likelihoods from obj$maxlike.

HTH,

Simon.

Philippe Lacherez wrote:
[Hide Quoted Text]
> Hi all,
>
> I am hoping someone can help with a problem I have. I want to do a
> zero-inflated negative binomial model on some count data.  I have found
> how to get the model (using zicounts), and the test of each predictor on
> both the negative binomial and zero-inflated parts of the distribution.
> Can anyone tell me how I can also get an omnibus test of significance for
> the fit of the model?  Stata I think gives a likelihood ratio chi-square
> test of the full versus null model for the zinb model.  Is there a way to
> get this in R?  Alternatively, is there a way I use the deviance, or
> maximum likelihood value to derive this?
>
> Cheers
> Philippe Lacherez
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C

The combination of some data and an aching desire for
an answer does not ensure that a reasonable answer
can be extracted from a given body of data.
- John Tukey.


----------------------------------------------------------------
This message was sent using MyMail


From blomsp at ozemail.com.au  Thu Jan  4 07:23:38 2007
From: blomsp at ozemail.com.au (blomsp at ozemail.com.au)
Date: Thu, 04 Jan 2007 15:23:38 +0900
Subject: [R]  Help re zinb model
Message-ID: <20070104152338.d8lqd5eh0ll444g4@mymail.com.au>

Of course that should have been differences in the log-likelihoods in my
previous post. Aaargh.

Simon.
-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C

The combination of some data and an aching desire for
an answer does not ensure that a reasonable answer
can be extracted from a given body of data.
- John Tukey.


----------------------------------------------------------------
This message was sent using MyMail


From buser at stat.math.ethz.ch  Thu Jan  4 08:50:32 2007
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Thu, 4 Jan 2007 08:50:32 +0100
Subject: [R] mathematical symbols in plots
In-Reply-To: <1167838611.7280.16.camel@rock.kraft.de>
References: <1167838611.7280.16.camel@rock.kraft.de>
Message-ID: <17820.45512.740658.238850@stat.math.ethz.ch>

Dear Sebastian

plot(1:10, 1:10)
text(4, 9, expression(paste("<", k, ">")))

should work here.

Best regards,

Christoph

--------------------------------------------------------------

Credit and Surety PML study: visit our web page www.cs-pml.org

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH Zurich	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------



Sebastian Weber writes:
 > Hello everyone!
 > 
 > I'm trying to plot some mathematical expression along my axis, but
 > demo(plotmath) did not have the symbol I was looking for. In particular,
 > I would like to denote the mean of an observable by writing
 > 
 > <k>
 > 
 > which I tried to enter with
 > 
 > expression(group("<", k, ">"))
 > 
 > However, my naive try doesn't work and the help doesn't want to tell me,
 > does someone know?
 > 
 > And here another one: How can I sepcify which fonts get used with which
 > R prints those mathematical symbols? Since I finally include my plots in
 > latex-documents as eps, I would love to use the same font-encoding for
 > all postscript stuff. A problem in the past has been, that R embedded
 > it's own font within the ps-files generated. These were not compatible
 > with the fonts used at the magazine where I published my document. This
 > lead to quite some confusion as \gamma became g and so on. Any solution
 > to this problem? Any hint? As I'm not too much into font-encoding, I
 > have actually no real clue where to even start searching.
 > 
 > Thank you very much for any help.
 > 
 > Greetings,
 > 
 > Sebastian Weber
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
 > and provide commented, minimal, self-contained, reproducible code.


From r.hankin at noc.soton.ac.uk  Thu Jan  4 09:08:28 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 4 Jan 2007 08:08:28 +0000
Subject: [R] Software for kriging
In-Reply-To: <9B2962F493D17F4F81A9211B1F9C56FB9B1A69@mail.ga.gov.au>
References: <9B2962F493D17F4F81A9211B1F9C56FB9B1A69@mail.ga.gov.au>
Message-ID: <F4B6ABE4-53A9-442C-906A-B7D41745C0EC@soc.soton.ac.uk>

Hi

the "emulator" package of the the BACCO bundle
includes kriging as a special case.

HTH

Robin


On 4 Jan 2007, at 04:56, <Augusto.Sanabria at ga.gov.au> wrote:

>
> Dear R-list members,
>
> I wish everyone a happy and successful 2007!
>
> Does anyone know of R-based software for
> optimal spatial prediction (kriging)?
>
> We are working on a seismic event characterisation
> technique and need to do some kriging.
>
> Any help would be greatly appreciated.
>
> Augusto
>
>
>
> --------------------------------------------
> Augusto Sanabria. MSc, PhD.
> Mathematical Modeller
> Risk Research Group
> Geospatial & Earth Monitoring Division
> Geoscience Australia (www.ga.gov.au)
> Cnr. Jerrabomberra Av. & Hindmarsh Dr.
> Symonston ACT 2601
> Ph. (02) 6249-9155
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From arun.kumar.saha at gmail.com  Thu Jan  4 09:48:11 2007
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Thu, 4 Jan 2007 14:18:11 +0530
Subject: [R] Time series plot
In-Reply-To: <971536df0701032148v3036e1b7i4d8d192e5bacdf@mail.gmail.com>
References: <d4c57560701032136t3d6e032fm9b0d2368ecdb9389@mail.gmail.com>
	<971536df0701032148v3036e1b7i4d8d192e5bacdf@mail.gmail.com>
Message-ID: <d4c57560701040048n6085d25fuf47ada3c8f05a0d7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070104/df2da08e/attachment.pl 

From wangtong at usc.edu  Thu Jan  4 10:04:48 2007
From: wangtong at usc.edu (Tong Wang)
Date: Thu, 04 Jan 2007 01:04:48 -0800
Subject: [R] need help with debug package
In-Reply-To: <dde0d92f3e6f.459c2093@usc.edu>
References: <dde0d92f3e6f.459c2093@usc.edu>
Message-ID: <de699e153856.459c52b0@usc.edu>

I think I have figured out part of the answer,  when I entered a debugger, the calling environment (parent frame) 
refers to the debugger program, instead of the mainfun( ).   But is there anyway to solve this problem ? 

thanks 

----- Original Message -----
From: Tong Wang <wangtong at usc.edu>
Date: Wednesday, January 3, 2007 9:30 pm
Subject: need help with debug package
To: R help <r-help at stat.math.ethz.ch>

> Hi all,     
>      I met a problem while using the debug package,  I have the 
> following program: 
> 
> mainfun<- function(){
>           beta<-1
>           result<-subfun(beta+x)
> }
>                     
> subfun<-function(expr){
>           y <- eval(expr, envir=list(x=c(1,2)),enclos = 
> parent.frame())          return(y)
> }
> 
> I have no problem using this program without calling the debug 
> package.   but once 
> I mtrace(subfun),  the debugger can't find all the beta after 
> entering subfun , and give 
> the message :     "Error in beta : non-numeric argument to binary 
> operator"
> Is there anyway to get around ?
> 
> thanks a lot 
> happy new year
>


From Achim.Zeileis at wu-wien.ac.at  Thu Jan  4 11:41:55 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 4 Jan 2007 11:41:55 +0100 (CET)
Subject: [R] Help re zinb model
In-Reply-To: <20070104151351.1yr9rakbu4w8gsc8@mymail.com.au>
References: <20070104151351.1yr9rakbu4w8gsc8@mymail.com.au>
Message-ID: <Pine.LNX.4.64.0701041134440.29474@eowyn>

On Thu, 4 Jan 2007, blomsp at ozemail.com.au wrote:

> Remember that -2 * the difference in the likelihoods between the two 
> models is asymptotically chi-squared distributed, with degrees of 
> freedom equal to the difference in number of parameters between the 
> models. So you can just calculate that for your preferred and null 
> models, then use the pchisq function
> to test significance. Get the likelihoods from obj$maxlike.

The function lrtest() in package "lmtest" offers a flexible implementation 
of this which works for fitted models that provide a logLik() method. The 
zicounts() implementation does not, but zeroinfl() in package "pscl". E.g. 
you can do:

library("pscl")
data("teeth", package = "zicounts")
fm1 <- zeroinfl(dmft ~ gender + age | gender + age, data = teeth,
   dist = "negbin")
summary(fm1)
fm2 <- zeroinfl(dmft ~ 1, data = teeth, dist = "negbin")
summary(fm2)

library("lmtest")
lrtest(fm1, fm2)

hth,
Z


From dusa.adrian at gmail.com  Thu Jan  4 12:13:11 2007
From: dusa.adrian at gmail.com (Adrian DUSA)
Date: Thu, 4 Jan 2007 13:13:11 +0200
Subject: [R] dashed lines and SVG files
	devSVG("/folderul/unde/salvez/myplot.svg", width=10,
	height=10) plot(1:10, 1:10) dev.off()
Message-ID: <be5487e70701040313t318d969fraa0a3fb9618116a1@mail.gmail.com>

Dear helpers,

I have a question about the SVG device. It works fine, the SVG file is
indeed produced, only the graphic differs from the R window.
In the SVG file the dashed line is just a regular plain one. My toy example is:

library(RSvgDevice)
devSVG("myplot.svg", width=10, height=10)
plot(1:10)
abline(v=5, lty=?dashed?)
dev.off()

Is there anything more (or different) I should do?
Many thanks in advance,
Adrian


From dusa.adrian at gmail.com  Thu Jan  4 12:17:58 2007
From: dusa.adrian at gmail.com (Adrian DUSA)
Date: Thu, 4 Jan 2007 13:17:58 +0200
Subject: [R] dashed lines and SVG files
Message-ID: <be5487e70701040317h4ee201dcrcb58b8cdc486bb2b@mail.gmail.com>

Sorry for duplicating the message, the previous had an unintended
subject line...

Dear helpers,

I have a question about the SVG device. It works fine, the SVG file is
indeed produced, only the graphic differs from the R window.
In the SVG file the dashed line is just a regular plain one. My toy example is:

library(RSvgDevice)
devSVG("myplot.svg", width=10, height=10)
plot(1:10)
abline(v=5, lty=?dashed?)
dev.off()

Is there anything more (or different) I should do?
Many thanks in advance,
Adrian


From mir.k at gmx.at  Thu Jan  4 12:21:10 2007
From: mir.k at gmx.at (mirca heli)
Date: Thu, 04 Jan 2007 12:21:10 +0100
Subject: [R] pretended size postscript and size of the graphic device window
Message-ID: <20070104112110.21320@gmx.net>

Dear list members!

I've two questions concerning graphic export:

a) I want to export my graphics as PostScript files. in this way I use the postscript() function. The tricky part is that they must have a pretended size (7 x 7 cm) and an absoulte font size (10pt).
b) how can i (permanent) change the size of the graphic device window?

Best regards
mirca heli
--


From ggrothendieck at gmail.com  Thu Jan  4 12:26:44 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 4 Jan 2007 06:26:44 -0500
Subject: [R] Time series plot
In-Reply-To: <d4c57560701040048n6085d25fuf47ada3c8f05a0d7@mail.gmail.com>
References: <d4c57560701032136t3d6e032fm9b0d2368ecdb9389@mail.gmail.com>
	<971536df0701032148v3036e1b7i4d8d192e5bacdf@mail.gmail.com>
	<d4c57560701040048n6085d25fuf47ada3c8f05a0d7@mail.gmail.com>
Message-ID: <971536df0701040326m194dcd6eob2845915a0097151@mail.gmail.com>

Check out #2 in:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/85801.html

and RSiteSearch("axis(4") to find additional examples.

On 1/4/07, Arun Kumar Saha <arun.kumar.saha at gmail.com> wrote:
> Dear Gabor,
>
> Thank you very much for your letter. Actually I got partial solution from
> your suggestion. Still I am fighting with defining a secondary axis. More
> pecisely, suppose I have following two dataset:
>
> x = c(1:10)
> y = x*10
>
> To plot x I can simply write plot(x, type='l'), here the"y-axis" takes value
> from 1:10. Now I want to plot y on a Secondary "Y-axis" on same graphics
> window. Secondary y-axis will take value from 1:100 and plot y accordingly,
> just like Microsoft Excel. Is there any solution?
>
> Thanks and regards,
>
>
>
>
>
> On 1/4/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > You can use read.zoo in the zoo package to read in the data
> > and then see:
> >
> >
> https://www.stat.math.ethz.ch/pipermail/r-help/2006-December/122742.html
> >
> > See ?axis for creating additional axes with classic graphics and
> >
> > library(lattice)
> > ?panel.axis
> >
> > in lattice graphics.  Search the archives for examples.
> >
> > On 1/4/07, Arun Kumar Saha <arun.kumar.saha at gmail.com> wrote:
> > > Dear all R users,
> > >
> > > Suppose I have a data set like this:
> > >
> > > date              price
> > >
> > > 1-Jan-02     4.8803747
> > > 2-Jan-02     4.8798430
> > > 3-Jan-02     4.8840133
> > > 4-Jan-02     4.8803747
> > > 5-Jan-02     4.8749683
> > > 6-Jan-02     4.8754263
> > > 7-Jan-02     4.8746628
> > > 8-Jan-02     4.8753500
> > > 9-Jan-02     4.8882416
> > > 10-Jan-02     4.8895217
> > > 11-Jan-02     4.8871108
> > >
> > > I want to get a time series plot of that dataset. But in x-axis I want
> to
> > > see the first day, and last day, and other day in between them   i.e.
> > > 1-Jan-02,  6-Jan-02, and  11-Jan-02 only. Can anyone tell me how to do
> that?
> > >
> > > My second question is that is there any way to define a secondary axis
> like
> > > Microsoft Excel in the same plot window?
> > >
> > > Thanks and regards,
> > > Arun
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
>
>


From petr.pikal at precheza.cz  Thu Jan  4 12:49:55 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 04 Jan 2007 12:49:55 +0100
Subject: [R] Time series plot
In-Reply-To: <d4c57560701040048n6085d25fuf47ada3c8f05a0d7@mail.gmail.com>
References: <971536df0701032148v3036e1b7i4d8d192e5bacdf@mail.gmail.com>
Message-ID: <459CF7F3.7083.F9453B@localhost>

Hi

On 4 Jan 2007 at 14:18, Arun Kumar Saha wrote:

Date sent:      	Thu, 4 Jan 2007 14:18:11 +0530
From:           	"Arun Kumar Saha" <arun.kumar.saha at gmail.com>
To:             	"Gabor Grothendieck" <ggrothendieck at gmail.com>
Copies to:      	"r-help at stat.math.ethz.ch" <R-help at stat.math.ethz.ch>
Subject:        	Re: [R] Time series plot

> Dear Gabor,
> 
> Thank you very much for your letter. Actually I got partial solution
> from your suggestion. Still I am fighting with defining a secondary
> axis. More pecisely, suppose I have following two dataset:
> 
> x = c(1:10)
> y = x*10
> 
> To plot x I can simply write plot(x, type='l'), here the"y-axis" takes
> value from 1:10. Now I want to plot y on a Secondary "Y-axis" on same

I think you are lost a bit in plotting
let's
x be
x<-rnorm(10)
and
y<-x*10

then
gives you x values on y axis and x axis is 1:10. You can put another 
values y on the same plot but only if you give them space before

plot(x, type="l", ylim=range(y))
points(y) 
but it is probably not what you want.

If you want to plot time series to get time labels on x axis then 
Gabor's solution would be perfectly valid.

Or you can convert your date (which is probably factor) to real time.

> tab=read.table("clipboard", header=T)
> tab
        date    price
1   1-Jan-02 4.880375
2   2-Jan-02 4.879843
3   3-Jan-02 4.884013
4   4-Jan-02 4.880375
5   5-Jan-02 4.874968
6   6-Jan-02 4.875426
7   7-Jan-02 4.874663
8   8-Jan-02 4.875350
9   9-Jan-02 4.888242
10 10-Jan-02 4.889522
11 11-Jan-02 4.887111

tab$newdate <- as.POSIXct(strptime(tab$date, format="%d-%b-%y"))

> str(tab)
'data.frame':   11 obs. of  3 variables:
 $ date   : Factor w/ 11 levels "1-Jan-02","10-Jan-02",..: 1 4 5 6 7 
8 9 10 11 2 ...
 $ price  : num  4.88 4.88 4.88 4.88 4.87 ...
 $ newdate:'POSIXct', format: chr  "2002-01-01" "2002-01-02" "2002-01-
03" "2002-01-04" .

plot(tab$newdate, tab$price)

what is what you may want without explicit need for secondary y axis.

HTH
Petr

> graphics window. Secondary y-axis will take value from 1:100 and plot
> y accordingly, just like Microsoft Excel. Is there any solution?
> 
> Thanks and regards,
> 
> 
> 
> 
> 
> On 1/4/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >
> > You can use read.zoo in the zoo package to read in the data
> > and then see:
> >
> > https://www.stat.math.ethz.ch/pipermail/r-help/2006-December/122742.
> > html
> >
> > See ?axis for creating additional axes with classic graphics and
> >
> > library(lattice)
> > ?panel.axis
> >
> > in lattice graphics.  Search the archives for examples.
> >
> > On 1/4/07, Arun Kumar Saha <arun.kumar.saha at gmail.com> wrote:
> > > Dear all R users,
> > >
> > > Suppose I have a data set like this:
> > >
> > > date              price
> > >
> > > 1-Jan-02     4.8803747
> > > 2-Jan-02     4.8798430
> > > 3-Jan-02     4.8840133
> > > 4-Jan-02     4.8803747
> > > 5-Jan-02     4.8749683
> > > 6-Jan-02     4.8754263
> > > 7-Jan-02     4.8746628
> > > 8-Jan-02     4.8753500
> > > 9-Jan-02     4.8882416
> > > 10-Jan-02     4.8895217
> > > 11-Jan-02     4.8871108
> > >
> > > I want to get a time series plot of that dataset. But in x-axis I
> > > want
> > to
> > > see the first day, and last day, and other day in between them 
> > > i.e. 1-Jan-02,  6-Jan-02, and  11-Jan-02 only. Can anyone tell me
> > > how to do
> > that?
> > >
> > > My second question is that is there any way to define a secondary
> > > axis
> > like
> > > Microsoft Excel in the same plot window?
> > >
> > > Thanks and regards,
> > > Arun
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From gavin.simpson at ucl.ac.uk  Thu Jan  4 13:19:36 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 04 Jan 2007 12:19:36 +0000
Subject: [R] pretended size postscript and size of the graphic
	device	window
In-Reply-To: <20070104112110.21320@gmx.net>
References: <20070104112110.21320@gmx.net>
Message-ID: <1167913176.25474.36.camel@gsimpson.geog.ucl.ac.uk>

On Thu, 2007-01-04 at 12:21 +0100, mirca heli wrote:
> Dear list members!
> 
> I've two questions concerning graphic export:
> 
> a) I want to export my graphics as PostScript files. in this way I use
> the postscript() function. The tricky part is that they must have a
> pretended size (7 x 7 cm) and an absoulte font size (10pt).

If I understand you correctly, ?postscript contains all you need to
know, eg:

postscript(file="foo.eps", paper="special", onefile=FALSE, 
           width=7/2.54, height=7/2.54, pointsize=10, 
           horizontal=FALSE)
plot(rnorm(100), rnorm(100), main = "foo")
dev.off()

Is this what you wanted?

> b) how can i (permanent) change the size of the graphic device window?

This may well depend on your OS (unstated). I was looking for this the
other day as the window is too big on my laptop - I didn't look to hard
though so it is no surprise that I did not find a solution.

HTH

G

> 
> Best regards
> mirca heli
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From ligges at statistik.uni-dortmund.de  Thu Jan  4 13:23:49 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 04 Jan 2007 13:23:49 +0100
Subject: [R] pretended size postscript and size of the graphic device
 window
In-Reply-To: <20070104112110.21320@gmx.net>
References: <20070104112110.21320@gmx.net>
Message-ID: <459CF1D5.2040403@statistik.uni-dortmund.de>



mirca heli wrote:
> Dear list members!
> 
> I've two questions concerning graphic export:
> 
> a) I want to export my graphics as PostScript files. in this way I use the postscript() function. The tricky part is that they must have a pretended size (7 x 7 cm) and an absoulte font size (10pt).


See ?postscript and its arguments width, height, paper and pointsize.

> b) how can i (permanent) change the size of the graphic device window?

For the postscript device, see ?ps.options.

Uwe Ligges



> Best regards
> mirca heli
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Thu Jan  4 14:31:44 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 04 Jan 2007 08:31:44 -0500
Subject: [R] pretended size postscript and size of the graphic device
 window
In-Reply-To: <20070104112110.21320@gmx.net>
References: <20070104112110.21320@gmx.net>
Message-ID: <459D01C0.2070703@stats.uwo.ca>

On 1/4/2007 6:21 AM, mirca heli wrote:
> Dear list members!
> 
> I've two questions concerning graphic export:
> 
> a) I want to export my graphics as PostScript files. in this way I use the postscript() function. The tricky part is that they must have a pretended size (7 x 7 cm) and an absoulte font size (10pt).

> b) how can i (permanent) change the size of the graphic device window?

Most of the graphics device functions take args to set their size.  You 
can create your own function and use it instead of the standard one, 
with different defaults:  e.g.

mywin <- function() windows(2,2)
options(device="mywin")


Now I'll get really tiny windows.  You can put these lines in your 
.Rprofile (see ?Startup) if you want them to happen in all sessions.

Duncan Murdoch


From bill.shipley at usherbrooke.ca  Thu Jan  4 15:41:35 2007
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Thu, 4 Jan 2007 09:41:35 -0500
Subject: [R] setting new working directories
Message-ID: <001001c7300e$7016b440$bb1ad284@BIO041>

Hello, and Happy New Year.  My default working directory is getting very
cluttered.  I know that I should be using a different working directory for
each project (I work in Windows), but do not know how to go about creating
different ones and moving back and forth between them.  I have read Venables
& Ripley (Modern Applied Statistics with S-PLUS, 1994) but this seems out of
date with respect to this topic and have searched through the documentation
but cannot find a clear explanation for doing this.  Can someone point me to
the proper documentation for creating and using different working
directories from within Windows (please, no comments about switching to
UNIX...).
Thanks.

Bill Shipley


From Abhijit.Dasgupta at mail.jci.tju.edu  Thu Jan  4 15:48:24 2007
From: Abhijit.Dasgupta at mail.jci.tju.edu (Abhijit Dasgupta)
Date: Thu, 04 Jan 2007 09:48:24 -0500
Subject: [R] setting new working directories
In-Reply-To: <001001c7300e$7016b440$bb1ad284@BIO041>
References: <001001c7300e$7016b440$bb1ad284@BIO041>
Message-ID: <459D13B8.9060003@mail.jci.tju.edu>

The basic command for this is setwd() (for "set working directory")

You can save your workspace after you use the setwd command, and this 
will create a .RData file in that directory. In windows you can click on 
the .RData directory to open R using that particular working directory. 
You can always check on the current working directory using getwd().

Another useful command in this context is save.image (see ?save.image 
for details)

Abhijit
Bill Shipley wrote:
> Hello, and Happy New Year.  My default working directory is getting very
> cluttered.  I know that I should be using a different working directory for
> each project (I work in Windows), but do not know how to go about creating
> different ones and moving back and forth between them.  I have read Venables
> & Ripley (Modern Applied Statistics with S-PLUS, 1994) but this seems out of
> date with respect to this topic and have searched through the documentation
> but cannot find a clear explanation for doing this.  Can someone point me to
> the proper documentation for creating and using different working
> directories from within Windows (please, no comments about switching to
> UNIX...).
> Thanks.
>
> Bill Shipley
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ccleland at optonline.net  Thu Jan  4 15:48:54 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 04 Jan 2007 09:48:54 -0500
Subject: [R] setting new working directories
In-Reply-To: <001001c7300e$7016b440$bb1ad284@BIO041>
References: <001001c7300e$7016b440$bb1ad284@BIO041>
Message-ID: <459D13D6.6070807@optonline.net>

Bill Shipley wrote:
> Hello, and Happy New Year.  My default working directory is getting very
> cluttered.  I know that I should be using a different working directory for
> each project (I work in Windows), but do not know how to go about creating
> different ones and moving back and forth between them.  I have read Venables
> & Ripley (Modern Applied Statistics with S-PLUS, 1994) but this seems out of
> date with respect to this topic and have searched through the documentation
> but cannot find a clear explanation for doing this.  Can someone point me to
> the proper documentation for creating and using different working
> directories from within Windows (please, no comments about switching to
> UNIX...).
> Thanks.

  RSiteSearch("working directory") is very helpful.  In particular, look
at the help pages for getwd() and setwd().

> Bill Shipley
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From Thierry.ONKELINX at inbo.be  Thu Jan  4 15:50:13 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Thu, 4 Jan 2007 15:50:13 +0100
Subject: [R] setting new working directories
In-Reply-To: <001001c7300e$7016b440$bb1ad284@BIO041>
Message-ID: <2E9C414912813E4EB981326983E0A104026EBB17@inboexch.inbo.be>

See ?getwd and ?setwd to set the working directory
See ?load and ?save to read the workspace.

Cheers,

Thierry

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens Bill Shipley
Verzonden: donderdag 4 januari 2007 15:42
Aan: R help list
Onderwerp: [R] setting new working directories

Hello, and Happy New Year.  My default working directory is getting very
cluttered.  I know that I should be using a different working directory
for
each project (I work in Windows), but do not know how to go about
creating
different ones and moving back and forth between them.  I have read
Venables
& Ripley (Modern Applied Statistics with S-PLUS, 1994) but this seems
out of
date with respect to this topic and have searched through the
documentation
but cannot find a clear explanation for doing this.  Can someone point
me to
the proper documentation for creating and using different working
directories from within Windows (please, no comments about switching to
UNIX...).
Thanks.

Bill Shipley

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From aajakh at yahoo.com  Thu Jan  4 15:52:05 2007
From: aajakh at yahoo.com (AA)
Date: Thu, 4 Jan 2007 06:52:05 -0800 (PST)
Subject: [R] setting new working directories
Message-ID: <20070104145205.27153.qmail@web37907.mail.mud.yahoo.com>

use setwd() and getwd(). see ?setwd
or on the shortcut properties tab, set the target directory to your working dir and have
as many shortcuts as working directories.
working with R commands does it for me.
all the best.
A.

----- Original Message ----
From: Bill Shipley <bill.shipley at usherbrooke.ca>
To: R help list <r-help at stat.math.ethz.ch>
Sent: Thursday, January 4, 2007 9:41:35 AM
Subject: [R] setting new working directories

Hello, and Happy New Year.  My default working directory is getting very
cluttered.  I know that I should be using a different working directory for
each project (I work in Windows), but do not know how to go about creating
different ones and moving back and forth between them.  I have read Venables
& Ripley (Modern Applied Statistics with S-PLUS, 1994) but this seems out of
date with respect to this topic and have searched through the documentation
but cannot find a clear explanation for doing this.  Can someone point me to
the proper documentation for creating and using different working
directories from within Windows (please, no comments about switching to
UNIX...).
Thanks.

Bill Shipley

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Abhijit.Dasgupta at mail.jci.tju.edu  Thu Jan  4 16:07:20 2007
From: Abhijit.Dasgupta at mail.jci.tju.edu (Abhijit Dasgupta)
Date: Thu, 04 Jan 2007 10:07:20 -0500
Subject: [R] setting new working directories
In-Reply-To: <20070104145205.27153.qmail@web37907.mail.mud.yahoo.com>
References: <20070104145205.27153.qmail@web37907.mail.mud.yahoo.com>
Message-ID: <459D1828.4020000@mail.jci.tju.edu>

An additional note for Windows. The directory name needs to be written 
as "C:/Project/Working directory" or "C:\\Project\\Working directory" as 
opposed to the usual way of referencing directories in Windows.

AA wrote:
> use setwd() and getwd(). see ?setwd
> or on the shortcut properties tab, set the target directory to your working dir and have
> as many shortcuts as working directories.
> working with R commands does it for me.
> all the best.
> A.
>
> ----- Original Message ----
> From: Bill Shipley <bill.shipley at usherbrooke.ca>
> To: R help list <r-help at stat.math.ethz.ch>
> Sent: Thursday, January 4, 2007 9:41:35 AM
> Subject: [R] setting new working directories
>
> Hello, and Happy New Year.  My default working directory is getting very
> cluttered.  I know that I should be using a different working directory for
> each project (I work in Windows), but do not know how to go about creating
> different ones and moving back and forth between them.  I have read Venables
> & Ripley (Modern Applied Statistics with S-PLUS, 1994) but this seems out of
> date with respect to this topic and have searched through the documentation
> but cannot find a clear explanation for doing this.  Can someone point me to
> the proper documentation for creating and using different working
> directories from within Windows (please, no comments about switching to
> UNIX...).
> Thanks.
>
> Bill Shipley
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Thu Jan  4 16:25:06 2007
From: ripley at stats.ox.ac.uk (Professor Brian Ripley)
Date: Thu, 04 Jan 2007 15:25:06 +0000
Subject: [R] setting new working directories
In-Reply-To: <459D1828.4020000@mail.jci.tju.edu>
References: <20070104145205.27153.qmail@web37907.mail.mud.yahoo.com>
	<459D1828.4020000@mail.jci.tju.edu>
Message-ID: <459D1C52.3080502@stats.ox.ac.uk>

Abhijit Dasgupta wrote:
> An additional note for Windows. The directory name needs to be written 
> as "C:/Project/Working directory" or "C:\\Project\\Working directory" as 
> opposed to the usual way of referencing directories in Windows.

Which are?  Those are the forms given in the Microsoft documentation.

The 2002 edition of MASS is not at all out of date: the 1994 edition is
explicitly about S-PLUS (and the 'Windows' version of S-PLUS in 1994 was 
in fact a DOS program running under an extender).


> AA wrote:
>> use setwd() and getwd(). see ?setwd
>> or on the shortcut properties tab, set the target directory to your working dir and have
>> as many shortcuts as working directories.
>> working with R commands does it for me.
>> all the best.
>> A.
>>
>> ----- Original Message ----
>> From: Bill Shipley <bill.shipley at usherbrooke.ca>
>> To: R help list <r-help at stat.math.ethz.ch>
>> Sent: Thursday, January 4, 2007 9:41:35 AM
>> Subject: [R] setting new working directories
>>
>> Hello, and Happy New Year.  My default working directory is getting very
>> cluttered.  I know that I should be using a different working directory for
>> each project (I work in Windows), but do not know how to go about creating
>> different ones and moving back and forth between them.  I have read Venables
>> & Ripley (Modern Applied Statistics with S-PLUS, 1994) but this seems out of
>> date with respect to this topic and have searched through the documentation
>> but cannot find a clear explanation for doing this.  Can someone point me to
>> the proper documentation for creating and using different working
>> directories from within Windows (please, no comments about switching to
>> UNIX...).
>> Thanks.
>>
>> Bill Shipley
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From b.rowlingson at lancaster.ac.uk  Thu Jan  4 16:40:40 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 04 Jan 2007 15:40:40 +0000
Subject: [R] setting new working directories
In-Reply-To: <459D1828.4020000@mail.jci.tju.edu>
References: <20070104145205.27153.qmail@web37907.mail.mud.yahoo.com>
	<459D1828.4020000@mail.jci.tju.edu>
Message-ID: <459D1FF8.7060707@lancaster.ac.uk>

Bill Shipley wrote:

>> Hello, and Happy New Year.  My default working directory is getting very
>> cluttered.  I know that I should be using a different working directory for
>> each project (I work in Windows), but do not know how to go about creating
>> different ones and moving back and forth between them.  


  If you make a new directory, then in it put a copy of a shortcut to R 
(actually to Rgui.exe in R's bin directory) then right click on the 
shortcut, select 'Properties', and set the 'start in' to your new 
working directory, you can browse to that directory in windows, double 
click the R shortcut, and be working in that new working directory with 
no setwd() needed.

Barry

PS oh, I mean 'folder' not 'directory' of course, this is Windows...


From bates at stat.wisc.edu  Thu Jan  4 17:18:06 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 4 Jan 2007 10:18:06 -0600
Subject: [R] mcmcsamp and variance ratios
In-Reply-To: <ED68137A-6EEE-40D6-BB97-CBF6D0E40355@muohio.edu>
References: <ED68137A-6EEE-40D6-BB97-CBF6D0E40355@muohio.edu>
Message-ID: <40e66e0b0701040818x39901b02s67b0b90b7bcb393d@mail.gmail.com>

On 1/3/07, Martin Henry H. Stevens <hstevens at muohio.edu> wrote:
> Hi folks,
> I have assumed that ratios of variance components (Fst and Qst in
> population genetics) could be estimated using the output of mcmcsamp
> (the series on mcmc sample estimates of variance components).

> What I have started to do is to use the matrix output that included
> the log(variances), exponentiate, calculate the relevant ratio, and
> apply either quantile or or HPDinterval to get confidence intervals.

> This seems too simple but I can't think of what is wrong with it.

Why bother exponentiating?  I'm not sure what ratios you want but if
they are ratios of two of the variances that are columns of the matrix
then you just need to take the difference of the logarithms.  I expect
that the quantiles and HPDintervals would be better behaved, in the
sense of being based on a distribution that is close to symmetric, on
the scale of the logarithm of the ratio instead of the ratio itself.

Quantiles calculated for the logarithm of the ratio will map to
quantiles of the ratio.  However, if you really do feel that you must
report an HPDinterval on the ratio then you would need to exponentiate
the logarithm of the ratio before calculating the interval.
Technically the HPD interval of the ratio is not the same as
exponentiating the end points of the HPDinterval of the logarithm of
the ratio but I doubt that the differences would be substantial.


From br44114 at gmail.com  Thu Jan  4 17:35:27 2007
From: br44114 at gmail.com (bogdan romocea)
Date: Thu, 4 Jan 2007 11:35:27 -0500
Subject: [R] export many plots to one file
Message-ID: <8d5a36350701040835p1d0c4bbbqb447cfe521c2970d@mail.gmail.com>

Dear useRs,

I have a few hundred plots that I'd like to export to one document.
pdf() isn't an option, because the file created is prohibitively huge
(due to scatter plots with many points). So I have to use png()
instead, but then I end up with a lot of files (would prefer just
one).

1. Is there a way to have pdf() embed images, instead of vector
instructions? (What would have to be changed/added, and where? I'd
consider that a very useful feature.)

2. Does anyone have a script for importing many images (png, bitmap,
jpg) into one PDF file? I'd prefer something that works both on
Windows and GNU.

Thank you,
b.


From bbands at gmail.com  Thu Jan  4 18:05:19 2007
From: bbands at gmail.com (BBands)
Date: Thu, 4 Jan 2007 09:05:19 -0800
Subject: [R] get.hist.quote
Message-ID: <6e8360ad0701040905hbaee639n8b8ce58d5900df25@mail.gmail.com>

Odd behavior from get.hist.quote this AM.

"""
> get.hist.quote('sunw')
trying URL 'http://chart.yahoo.com/table.csv?s=sunw&a=0&b=02&c=1991&d=0&e=03&f=2007&g=d&q=q&y=0&z=sunw&x=.csv'
Content type 'text/csv' length unknown
opened URL
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .......... .......... .........
downloaded 189Kb

Error in if (!quiet && dat[n] != start) cat(format(dat[n], "time
series starts %Y-%m-%d\n")) :
        missing value where TRUE/FALSE needed
>
"""
Indentical for 2.4.0 on SuSE 10.1 and 2.4.1 on Win XP.

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From Thierry.ONKELINX at inbo.be  Thu Jan  4 18:16:55 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Thu, 4 Jan 2007 18:16:55 +0100
Subject: [R] export many plots to one file
In-Reply-To: <8d5a36350701040835p1d0c4bbbqb447cfe521c2970d@mail.gmail.com>
Message-ID: <2E9C414912813E4EB981326983E0A104026EBBBB@inboexch.inbo.be>

I can think of two options:
1. Use R2HTML and save the html output as PDF
2. Use Sweave and compile the LaTeX file to PDF. Search the mailing list
archive on how to save the graphs as png or jpeg (as Sweave will
standard generate eps or pdf graphs).

Cheers,

Thierry

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens bogdan romocea
Verzonden: donderdag 4 januari 2007 17:35
Aan: r-help
Onderwerp: [R] export many plots to one file

Dear useRs,

I have a few hundred plots that I'd like to export to one document.
pdf() isn't an option, because the file created is prohibitively huge
(due to scatter plots with many points). So I have to use png()
instead, but then I end up with a lot of files (would prefer just
one).

1. Is there a way to have pdf() embed images, instead of vector
instructions? (What would have to be changed/added, and where? I'd
consider that a very useful feature.)

2. Does anyone have a script for importing many images (png, bitmap,
jpg) into one PDF file? I'd prefer something that works both on
Windows and GNU.

Thank you,
b.

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From macq at llnl.gov  Thu Jan  4 18:19:19 2007
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 4 Jan 2007 09:19:19 -0800
Subject: [R] Time series plot
In-Reply-To: <d4c57560701040048n6085d25fuf47ada3c8f05a0d7@mail.gmail.com>
References: <d4c57560701032136t3d6e032fm9b0d2368ecdb9389@mail.gmail.com>
	<971536df0701032148v3036e1b7i4d8d192e5bacdf@mail.gmail.com>
	<d4c57560701040048n6085d25fuf47ada3c8f05a0d7@mail.gmail.com>
Message-ID: <p06230902c1c2e73fd9fd@[128.115.153.6]>

Here's an example illustrating a way to get a second y axis that has 
a different range:

x <- 1:10
y1 <- 2*x
y2 <- 100-3*x+rnorm(10)

par(mar=c(5.1,4.1,4.1,4.1))

plot(x,y1)
par(new=TRUE)
plot(x,y2,xaxt='n',yaxt='n',xlab='',ylab='',pch=3)
axis(4)
mtext('y2',side=4,line=2.5)

-Don

At 2:18 PM +0530 1/4/07, Arun Kumar Saha wrote:
>Dear Gabor,
>
>Thank you very much for your letter. Actually I got partial solution from
>your suggestion. Still I am fighting with defining a secondary axis. More
>pecisely, suppose I have following two dataset:
>
>x = c(1:10)
>y = x*10
>
>To plot x I can simply write plot(x, type='l'), here the"y-axis" takes value
>from 1:10. Now I want to plot y on a Secondary "Y-axis" on same graphics
>window. Secondary y-axis will take value from 1:100 and plot y accordingly,
>just like Microsoft Excel. Is there any solution?
>
>Thanks and regards,
>
>
>
>
>
>On 1/4/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>>
>>  You can use read.zoo in the zoo package to read in the data
>>  and then see:
>>
>>  https://www.stat.math.ethz.ch/pipermail/r-help/2006-December/122742.html
>>
>>  See ?axis for creating additional axes with classic graphics and
>>
>>  library(lattice)
>>  ?panel.axis
>>
>>  in lattice graphics.  Search the archives for examples.
>>
>>  On 1/4/07, Arun Kumar Saha <arun.kumar.saha at gmail.com> wrote:
>>  > Dear all R users,
>>  >
>>  > Suppose I have a data set like this:
>>  >
>>  > date              price
>>  >
>>  > 1-Jan-02     4.8803747
>>  > 2-Jan-02     4.8798430
>>  > 3-Jan-02     4.8840133
>>  > 4-Jan-02     4.8803747
>>  > 5-Jan-02     4.8749683
>>  > 6-Jan-02     4.8754263
>>  > 7-Jan-02     4.8746628
>>  > 8-Jan-02     4.8753500
>>  > 9-Jan-02     4.8882416
>>  > 10-Jan-02     4.8895217
>>  > 11-Jan-02     4.8871108
>>  >
>>  > I want to get a time series plot of that dataset. But in x-axis I want
>>  to
>>  > see the first day, and last day, and other day in between them  i.e.
>>  > 1-Jan-02,  6-Jan-02, and  11-Jan-02 only. Can anyone tell me how to do
>>  that?
>>  >
>>  > My second question is that is there any way to define a secondary axis
>>  like
>>  > Microsoft Excel in the same plot window?
>>  >
>>  > Thanks and regards,
>>  > Arun
>>  >
>>  >        [[alternative HTML version deleted]]
>>  >
>>  > ______________________________________________
>>  > R-help at stat.math.ethz.ch mailing list
>>  > https://stat.ethz.ch/mailman/listinfo/r-help
>>  > PLEASE do read the posting guide
>>  http://www.R-project.org/posting-guide.html
>>  > and provide commented, minimal, self-contained, reproducible code.
>>  >
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA


From bbands at gmail.com  Thu Jan  4 18:22:42 2007
From: bbands at gmail.com (BBands)
Date: Thu, 4 Jan 2007 09:22:42 -0800
Subject: [R] get.hist.quote
In-Reply-To: <6e8360ad0701040905hbaee639n8b8ce58d5900df25@mail.gmail.com>
References: <6e8360ad0701040905hbaee639n8b8ce58d5900df25@mail.gmail.com>
Message-ID: <6e8360ad0701040922w108c9841q3b0473a9bbd36b67@mail.gmail.com>

On 1/4/07, BBands <bbands at gmail.com> wrote:
> Odd behavior from get.hist.quote this AM.

Now working, must have been a Yahoo! issue.

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From efg at stowers-institute.org  Thu Jan  4 18:42:50 2007
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Thu, 4 Jan 2007 11:42:50 -0600
Subject: [R] setting new working directories
References: <001001c7300e$7016b440$bb1ad284@BIO041>
Message-ID: <enjeb0$6q3$1@sea.gmane.org>

Bill,

I like to use Windows Explorer to find folders and then launch R with the 
selected folder as the working directory.

I put some notes online about this:
http://research.stowers-institute.org/efg/R/TechNote/WindowsExplorerWorkingDirectory/index.htm

efg

"Bill Shipley" <bill.shipley at usherbrooke.ca> wrote in message 
news:001001c7300e$7016b440$bb1ad284 at BIO041...
> Hello, and Happy New Year.  My default working directory is getting very
> cluttered.  I know that I should be using a different working directory 
> for
> each project (I work in Windows), but do not know how to go about creating
> different ones and moving back and forth between them.


From BEN at SSANET.COM  Thu Jan  4 18:42:56 2007
From: BEN at SSANET.COM (Ben Fairbank)
Date: Thu, 4 Jan 2007 11:42:56 -0600
Subject: [R] Seek general information about time/date storage and functions
	in R
Message-ID: <CA612484A337C6479EA341DF9EEE14AC05C45859@hercules.ssainfo>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070104/9d1b8cda/attachment.pl 

From murdoch at stats.uwo.ca  Thu Jan  4 18:48:01 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 04 Jan 2007 12:48:01 -0500
Subject: [R] setting new working directories
In-Reply-To: <001001c7300e$7016b440$bb1ad284@BIO041>
References: <001001c7300e$7016b440$bb1ad284@BIO041>
Message-ID: <459D3DD1.9000308@stats.uwo.ca>

On 1/4/2007 9:41 AM, Bill Shipley wrote:
> Hello, and Happy New Year.  My default working directory is getting very
> cluttered.  I know that I should be using a different working directory for
> each project (I work in Windows), but do not know how to go about creating
> different ones and moving back and forth between them.  I have read Venables
> & Ripley (Modern Applied Statistics with S-PLUS, 1994) but this seems out of
> date with respect to this topic and have searched through the documentation
> but cannot find a clear explanation for doing this.  Can someone point me to
> the proper documentation for creating and using different working
> directories from within Windows (please, no comments about switching to
> UNIX...).

I don't think R has facilities for creating directories:  you would do 
that in the OS, e.g. in Windows Explorer, right click and ask for "New | 
Folder".

In Windows the easiest way to set a directory as the current working 
directory is

setwd(choose.dir())

In the choose.dir dialog you can type the directory name in standard 
Windows format (don't worry about escaping \), or you can use the 
directory browser to choose it.

By the way, if you do decide to switch to Unix, you'll have to do 
without choose.dir() (unless it's in a contributed package somewhere).

Duncan Murdoch


From Mark.Leeds at morganstanley.com  Thu Jan  4 18:53:16 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Thu, 4 Jan 2007 12:53:16 -0500
Subject: [R] Seek general information about time/date storage and
	functionsin R
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05C45859@hercules.ssainfo>
Message-ID: <D3AEEDA31E57474B840BEBC25A8A8344F393A1@NYWEXMB23.msad.ms.com>

I think the best explanation of dates and times is in r-news 2.4.1 but
2.4.1 might be off so someone will hopefully correct me if I'm wrong.


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ben Fairbank
Sent: Thursday, January 04, 2007 12:43 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Seek general information about time/date storage and
functionsin R

Hello R List -

 

I have to import Excel files (either as .csv files or using RODBC) into
R (2.4.1, Windows) and operate on dates and times (e.g. find minutes
between times, change dates to days of week or analyze by weeks of
year).  The help files for format.Date, strptime, as.POSIX,
DateTimeClasses, etc. etc. are informative but perhaps a little terse.
I have googled unsuccessfully for a more general description of how R
represents times and dates, and the methods (e.g. date arithmetic) for
working with them.  Can a reader point out such an introduction,
preferably on-line?

 

Thank you,

 

Ben Fairbank

 

 


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}


From murdoch at stats.uwo.ca  Thu Jan  4 18:51:26 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 04 Jan 2007 12:51:26 -0500
Subject: [R] Seek general information about time/date storage and
 functions in R
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05C45859@hercules.ssainfo>
References: <CA612484A337C6479EA341DF9EEE14AC05C45859@hercules.ssainfo>
Message-ID: <459D3E9E.2010903@stats.uwo.ca>

On 1/4/2007 12:42 PM, Ben Fairbank wrote:
> Hello R List -
> 
>  
> 
> I have to import Excel files (either as .csv files or using RODBC) into
> R (2.4.1, Windows) and operate on dates and times (e.g. find minutes
> between times, change dates to days of week or analyze by weeks of
> year).  The help files for format.Date, strptime, as.POSIX,
> DateTimeClasses, etc. etc. are informative but perhaps a little terse.
> I have googled unsuccessfully for a more general description of how R
> represents times and dates, and the methods (e.g. date arithmetic) for
> working with them.  Can a reader point out such an introduction,
> preferably on-line?
> 

There was an article in R-News on just this topic, in the R Help Desk in
http://cran.r-project.org/doc/Rnews/Rnews_2004-1.pdf .

Duncan Murdoch


From Greg.Snow at intermountainmail.org  Thu Jan  4 19:09:22 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Thu, 4 Jan 2007 11:09:22 -0700
Subject: [R] export many plots to one file
Message-ID: <07E228A5BE53C24CAD490193A7381BBB77C498@LP-EXCHVS07.CO.IHC.COM>

The approach I would take (possibly due to ignorance of a better option)
is to export to the multiple .png files, then use a tool like
imagemagick to combine them into a single pdf file.

For a quick test I exported 3 graphs from R and called them test1.png,
test2.png, and test3.png.  The imagemagick command is then just:

convert test*.png test.pdf

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
> Sent: Thursday, January 04, 2007 9:35 AM
> To: r-help
> Subject: [R] export many plots to one file
> 
> Dear useRs,
> 
> I have a few hundred plots that I'd like to export to one document.
> pdf() isn't an option, because the file created is 
> prohibitively huge (due to scatter plots with many points). 
> So I have to use png() instead, but then I end up with a lot 
> of files (would prefer just one).
> 
> 1. Is there a way to have pdf() embed images, instead of 
> vector instructions? (What would have to be changed/added, 
> and where? I'd consider that a very useful feature.)
> 
> 2. Does anyone have a script for importing many images (png, bitmap,
> jpg) into one PDF file? I'd prefer something that works both 
> on Windows and GNU.
> 
> Thank you,
> b.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From hstevens at muohio.edu  Thu Jan  4 19:28:26 2007
From: hstevens at muohio.edu (Martin Henry H. Stevens)
Date: Thu, 4 Jan 2007 13:28:26 -0500
Subject: [R] mcmcsamp and variance ratios
In-Reply-To: <40e66e0b0701040818x39901b02s67b0b90b7bcb393d@mail.gmail.com>
References: <ED68137A-6EEE-40D6-BB97-CBF6D0E40355@muohio.edu>
	<40e66e0b0701040818x39901b02s67b0b90b7bcb393d@mail.gmail.com>
Message-ID: <3882CEB2-4D70-487B-95FA-6B3ED3484626@muohio.edu>


On Jan 4, 2007, at 11:18 AM, Douglas Bates wrote:

> On 1/3/07, Martin Henry H. Stevens <hstevens at muohio.edu> wrote:
>> Hi folks,
>> I have assumed that ratios of variance components (Fst and Qst in
>> population genetics) could be estimated using the output of mcmcsamp
>> (the series on mcmc sample estimates of variance components).
>
>> What I have started to do is to use the matrix output that included
>> the log(variances), exponentiate, calculate the relevant ratio, and
>> apply either quantile or or HPDinterval to get confidence intervals.
>
>> This seems too simple but I can't think of what is wrong with it.
>
> Why bother exponentiating?  I'm not sure what ratios you want but if
> they are ratios of two of the variances that are columns of the matrix
> then you just need to take the difference of the logarithms.  I expect
> that the quantiles and HPDintervals would be better behaved, in the
> sense of being based on a distribution that is close to symmetric, on
> the scale of the logarithm of the ratio instead of the ratio itself.
>
> Quantiles calculated for the logarithm of the ratio will map to
> quantiles of the ratio.  However, if you really do feel that you must
> report an HPDinterval on the ratio then you would need to exponentiate
> the logarithm of the ratio before calculating the interval.
> Technically the HPD interval of the ratio is not the same as
> exponentiating the end points of the HPDinterval of the logarithm of
> the ratio but I doubt that the differences would be substantial.

My collaborator (the evolutionary biologist on this project) is very  
skeptical of the results I have been providing. Most of the Qst ratios,

Qst = Var[population] / ( Var[population] + Var[genotype] )

have values close to 0.5 (0.45--0.55) and wider confidence intervals  
(e.g. 0.2--0.8) than they have tended to see in the literature.

I suspect that this derives from our tiny sample sizes: 24 genotypes  
total, distributed among 9 populations (2-3 genotypes within each  
population).

Our variances (shrinkage estimates) frequently do not differ from  
zero. My model building using AIC results in the removal of most of  
the variance components. We only stuck the terms back in the model in  
order to get SOME number for these.

The biologist (supported by a biometrician) wants to bootstrap or  
jackknife the models. I will be very skeptical if all of a sudden  
they get qualitatively different estimates and intervals.

Does my perspective make sense? All comments appreciated.

-Hank

Dr. Hank Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/

"E Pluribus Unum"


From ggrothendieck at gmail.com  Thu Jan  4 19:30:10 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 4 Jan 2007 13:30:10 -0500
Subject: [R] setting new working directories
In-Reply-To: <001001c7300e$7016b440$bb1ad284@BIO041>
References: <001001c7300e$7016b440$bb1ad284@BIO041>
Message-ID: <971536df0701041030x7bf03acw70ee6e0369bab570@mail.gmail.com>

If you are working on Windows XP then downlaod the batchfiles
distribution whose home page is at:

http://code.google.com/p/batchfiles/

This will provide a bunch of batch files that are useful in connection
with using R.

At the Windows console this command shows you your path

   path

so place Rgui.bat from the batchfiles distribution in any of
the folders listed in the path or in the folder you want to be
your working directory.  Then issue this command at the
Windows console:

  Rgui.bat

That will start up R with the working directory set to the current
directory.

It also automatically finds which version of R you are using from the
registry so you don't have to do anything each time you install
a new version of R (unlike other methods).

On 1/4/07, Bill Shipley <bill.shipley at usherbrooke.ca> wrote:
> Hello, and Happy New Year.  My default working directory is getting very
> cluttered.  I know that I should be using a different working directory for
> each project (I work in Windows), but do not know how to go about creating
> different ones and moving back and forth between them.  I have read Venables
> & Ripley (Modern Applied Statistics with S-PLUS, 1994) but this seems out of
> date with respect to this topic and have searched through the documentation
> but cannot find a clear explanation for doing this.  Can someone point me to
> the proper documentation for creating and using different working
> directories from within Windows (please, no comments about switching to
> UNIX...).
> Thanks.
>
> Bill Shipley
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From joe-byers at utulsa.edu  Thu Jan  4 20:06:18 2007
From: joe-byers at utulsa.edu (Joe Byers)
Date: Thu, 04 Jan 2007 13:06:18 -0600
Subject: [R] Install RMySQL with R 2.4.0
In-Reply-To: <45525FC9.1040504@cs.odu.edu>
References: <mailman.9.1162724403.23649.r-help@stat.math.ethz.ch>	<001c01c700e9$60b875f0$0ac0a8c0@MightyMini>	<eit4k8$7j3$1@sea.gmane.org>	<eit7hn$l73$1@sea.gmane.org>
	<45525FC9.1040504@cs.odu.edu>
Message-ID: <459D502A.5020103@utulsa.edu>

All,

I am glad all of you have benefited from the posting of the RMySQL 5-10 
zip file on my university website.  I am asking for some help from the 
group, I am leaving the university at the end of the semester and I need 
a place to post this file until I get settled in my new position. 
Anyone that can help me or us out with this. It would be greatly 
appreciated.

Thank you
Joe



Frank McCown wrote:
> Joe Byers wrote:
>> All,
>>
>> After staring at this error message for an hour or so yesterday and this 
>> morning.  I decided to try something else.  Low and behold trying to 
>> build the package in cygwin causes R to try and build under linux/unix 
>> not windows.  I went to the command prompt and was able to build the 
>> package.
>>
>> Download the RMySQL...tar.gz file and unzip somewhere like drive:/projects
>>
>> Several notes
>> 1.  make sure you have mysql directories on your computer somewhere with 
>>   the subdirs of include, bin, and lib.  You can just copy these from 
>> you actual server unless you want to install them.  I used d:/mysql/...
>> 2.  Modify configure.win in RMySQL and Makevars.win ins RMySQL/src to 
>> have the mysql directories from (1)
>> 3.  Copy and paste this script to a batch file and execute
>> **************
>> Rem build without --docs=normal tries to build chm help on windows this 
>> bombs
>> Rem if a zip program not installed the zip file will not be built
>> Rem go find the temp directory where R built the package and copy to 
>> ../R/library
>> Rem temp directory will look something like C:\Temp\Rinst32098657\RMySQL
>>
>> Rem if R bin directory in the path this will run otherwise add the 
>> drive:\Dir1\R\bin to the command
>> Rcmd build --binary \projects\RMySQL --docs=normal
>> ***********************
>> 4. Note that I have --docs=normal in the command line. This is needed to 
>>   get the package built.  Windows packages now default to chm files and 
>> RMySQL does not have any windows chm help files.  All txt, html, and 
>> latex help are built with this option.
>> 5.  I am not sure where the RMySQL...zip file is stored, I think in 
>> ...R\Bin.  I just copied the files from the temp\RinstXXXXXX\to the 
>> ...\R\library to install.
>>
>> This may or may  not work for you, it did for me.
>>
>> I will try and update my website www.cba.utulsa.edu/byersj Research and 
>> Analytics section to include a link to the RMySQL zip file for others to 
>> download.
>>
>> Good Luck
>> Joe
> 
> 
> Joe,
> 
> Thanks for telling us how you got RMySQL installed.  Would you mind 
> posting the dll files so the rest of us wouldn't have to recompile anything?
> 
> Thanks,
> Frank
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch at stats.uwo.ca  Thu Jan  4 20:24:43 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 04 Jan 2007 14:24:43 -0500
Subject: [R] setting new working directories
In-Reply-To: <459D3DD1.9000308@stats.uwo.ca>
References: <001001c7300e$7016b440$bb1ad284@BIO041>
	<459D3DD1.9000308@stats.uwo.ca>
Message-ID: <459D547B.6030009@stats.uwo.ca>

On 1/4/2007 12:48 PM, Duncan Murdoch wrote:
> On 1/4/2007 9:41 AM, Bill Shipley wrote:
>> Hello, and Happy New Year.  My default working directory is getting very
>> cluttered.  I know that I should be using a different working directory for
>> each project (I work in Windows), but do not know how to go about creating
>> different ones and moving back and forth between them.  I have read Venables
>> & Ripley (Modern Applied Statistics with S-PLUS, 1994) but this seems out of
>> date with respect to this topic and have searched through the documentation
>> but cannot find a clear explanation for doing this.  Can someone point me to
>> the proper documentation for creating and using different working
>> directories from within Windows (please, no comments about switching to
>> UNIX...).
> 
> I don't think R has facilities for creating directories:  you would do 
> that in the OS, e.g. in Windows Explorer, right click and ask for "New | 
> Folder".

A couple of people have pointed out dir.create() to me.  I would have 
found it with

help.search('directory')

Duncan Murdoch

> 
> In Windows the easiest way to set a directory as the current working 
> directory is
> 
> setwd(choose.dir())
> 
> In the choose.dir dialog you can type the directory name in standard 
> Windows format (don't worry about escaping \), or you can use the 
> directory browser to choose it.
> 
> By the way, if you do decide to switch to Unix, you'll have to do 
> without choose.dir() (unless it's in a contributed package somewhere).
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From news at aspden.com  Thu Jan  4 20:45:36 2007
From: news at aspden.com (John Lawrence Aspden)
Date: Thu, 04 Jan 2007 19:45:36 +0000
Subject: [R] littler+dget+stdin -> segmentation fault
Message-ID: <enjlh0$4uc$1@sea.gmane.org>

Hi, I'm trying to write a series of pipes using littler, and I get the
following behaviour: Sorry if I'm just doing something witless, I'm new to
R. I'm using the latest versions from debian testing (2.4.0 and 0.0.8).


$ r -e 'a<-dget(file=stdin()); print(a)'
?list(a=2)
Segmentation fault

In R itself this works:

> dget(file=stdin())
?list(a=2)
$a
[1] 2

As do (from the command line):

$ cat >foo
list(a=2)
$ r -e 'a<-dget(file="foo"); print(a)'
$a
[1] 2

and (using littler and scan instead of dget)

$ r -e 'a<-scan(file=stdin()); print(a)'

Thanks in advance,

John.

-- 
Contractor in Cambridge UK -- http://www.aspden.com


From jukka.nyblom at jyu.fi  Thu Jan  4 16:39:18 2007
From: jukka.nyblom at jyu.fi (Jukka Nyblom)
Date: Thu, 04 Jan 2007 17:39:18 +0200
Subject: [R] loess
Message-ID: <459D1FA6.5040804@jyu.fi>

Hi,

I have tried

 > for (i in 1:100) L[,i] <- loess((i = =(1:100))~I(1:100), span=.5, 
degree=1)$fit

to create a matrix which gives me the smoothing weights (correctly as 
far as I have experienced), eg.

 > yhat <- loess(y~I(1:100), span=.5,degree=1)$fit
 > yhat[30]
[1] -0.2131983
 > L[30,]%*%y
           [,1]
[1,] -0.2131983

But,  L[30,] has 56 nonzero coefficients, not 50 that I expect with span 
= 0.5. Actually the number of nonzero elements on rows varies being 49, 
50, 55 or 56.

Does anyone know why?

Jukka Nyblom


From steven.gorle at telenet.be  Wed Jan  3 22:29:05 2007
From: steven.gorle at telenet.be (=?iso-8859-1?Q?Steven_Gorl=E9?=)
Date: Wed, 3 Jan 2007 22:29:05 +0100
Subject: [R] stratified sampling from known population datafile
Message-ID: <006601c72f7e$32a83e20$bd00a8c0@pcx>

Dear R-wizards,

I have a population from which I want to draw a stratified sample by region.

In  Venables and Ripley "Modern Applied statistics with S" I found some 
great procedures for Simple Random Sampling (with and without replacement) 
and for Systematic sampling and it works!

For stratified sampling I referred to the manual of the survey package.Are 
there any other papers available on this subject?
Is the output correct? And how can I draw a random (stratified by region) 
sample from my (population) datafile bmi?




 dstrat<-svydesign(id=~1,strata=~REGIONCH, data=bmi)
Warning in svydesign(id = ~1, strata = ~REGIONCH, data = bmi) :
         No weights or probabilities supplied, assuming equal probability
> summary(dstrat)
Stratified Independent Sampling design (with replacement)
svydesign(id = ~1, strata = ~REGIONCH, data = bmi)
Probabilities:
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
      1       1       1       1       1       1
Stratum Sizes:
           Brussels  Flanders  Walloonia
obs             2571      2987      3006
design.PSU      2571      2987      3006
actual.PSU      2571      2987      3006
Data variables:
 [1] "ID"       "WFIN"     "HH"       "REGION"   "EDU3"     "FA3"
 [7] "TA2"      "AGE7"     "SEX"      "VOEG"     "BMI"      "LNBMI"
[13] "LNVOEG"   "FLA"      "BRU"      "WAL"      "AGEGR1"   "AGEGR2"
[19] "AGEGR3"   "AGEGR4"   "AGEGR5"   "AGEGR6"   "AGEGR7"   "EDUPRIM"
[25] "EDUSEC"   "EDUHIGH"  "INCLOW"   "INCMED"   "INCHIG"   "REGIONCH"
[31] "PROVINCE" "SGP"      "GHQ12"    "GHQBIN"
> svymean(~BMI, dstrat)
    mean SE
BMI   NA NA


Thanks in advance!!


Kind regards,

Steven Gorle


From statadat at gmail.com  Thu Jan  4 17:30:26 2007
From: statadat at gmail.com (domenico pestalozzi)
Date: Thu, 4 Jan 2007 17:30:26 +0100
Subject: [R] memory limits in R loading a dataset and using the package tree
Message-ID: <e591a95b0701040830j37ac495arc72ff9dc74987e0d@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070104/6e94ce08/attachment.pl 

From england at cs.umn.edu  Thu Jan  4 22:13:14 2007
From: england at cs.umn.edu (Darin A. England)
Date: Thu, 4 Jan 2007 15:13:14 -0600
Subject: [R] randomForest and missing data
Message-ID: <20070104211314.GA27922@cs.umn.edu>


Does anyone know a reason why, in principle, a call to randomForest
cannot accept a data frame with missing predictor values? If each
individual tree is built using CART, then it seems like this
should be possible. (I understand that one may impute missing values
using rfImpute or some other method, but I would like to avoid doing
that.) 

If this functionality were available, then when the trees are being
constructed and when subsequent data are put through the forest, one
would also specify an argument for the use of surrogate rules, just
like in rpart. 

I realize this question is very specific to randomForest, as opposed
to R in general, but any comments are appreciated. I suppose I am
looking for someone to say "It's not appropriate, and here's why
..." or "Good idea. Please implement and post your code."

Thanks,

Darin England, Senior Scientist
Ingenix


From sfalcon at fhcrc.org  Thu Jan  4 22:20:23 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 04 Jan 2007 13:20:23 -0800
Subject: [R] [R-pkgs] RSQLite 0.4-18 sent to CRAN
Message-ID: <m2odpe32w8.fsf@fhcrc.org>

A new version of RSQLite has been pushed to CRAN.

In this version...

* Further integration of the manifest type system available since
  SQLite 3.  We now obtain the column type from the DB instead of
  pulling everything across as a character vector and calling
  type.convert.  This should improve performance and provide a more
  reliable interface to build on top of.  Note, however, that since
  type.convert is no longer called, return values will be different.
  In particular, text columns will come across as text, not factor.

* dbWriteTable has been refactored and no longer uses temp files.
  This resolves performance issues and line ending quandries on
  Windows.

* Fix for a bug in dbWriteTable when used to import text files; files
  lacking a trailing end of line marker can now be used.

Questions?  Send them to the r-sig-db mailing list.

Best Wishes,

+ seth

-- 
Seth Falcon | Computational Biology | Fred Hutchinson Cancer Research Center
http://bioconductor.org

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From g.comte at alliance-ir.net  Thu Jan  4 22:41:43 2007
From: g.comte at alliance-ir.net (COMTE Guillaume)
Date: Thu, 4 Jan 2007 22:41:43 +0100
Subject: [R] problem with plot() and POSIXt dates
Message-ID: <15C100200A5F4E45AF8CFB45A926EF3418AEED@allexch01.alliance.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070104/fb4fb4ff/attachment.pl 

From pratap_stat at yahoo.co.in  Thu Jan  4 22:37:42 2007
From: pratap_stat at yahoo.co.in (nalluri pratap)
Date: Thu, 4 Jan 2007 21:37:42 +0000 (GMT)
Subject: [R] Weighting Data
Message-ID: <491494.47469.qm@web8604.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070104/fcfae03e/attachment.pl 

From ssj1364 at gmail.com  Thu Jan  4 22:39:27 2007
From: ssj1364 at gmail.com (sj)
Date: Thu, 4 Jan 2007 14:39:27 -0700
Subject: [R]  importing timestamp data into R
Message-ID: <1c6126db0701041339p483e3587j34139027ab2018a8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070104/ddfb52b9/attachment.pl 

From Sicotte.Hugues at mayo.edu  Thu Jan  4 22:44:27 2007
From: Sicotte.Hugues at mayo.edu (Sicotte, Hugues   Ph.D.)
Date: Thu, 4 Jan 2007 15:44:27 -0600
Subject: [R] randomForest and missing data
Message-ID: <2E17292A64E6ED418A60BE89326B1AAB3224C4@msgebe11.mfad.mfroot.org>

I don't know about this module, but a general answer is that if you have
missing data, it may affect your model. If your data is missing at
random, then you might be lucky in your model building.

If however your data was not missing at random (e.g. censoring) , you
might build a wrong predictor.

Missing at random or not, that is a question you should answer and deal
with before modeling.

I refer you to a book like
"Analysis of Incomplete Multivariate data". By Schafer

If there is a way around that with randomForest, I'd be interested to
know too.

Hugues Sicotte


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Darin A. England
Sent: Thursday, January 04, 2007 3:13 PM
To: r-help at stat.math.ethz.ch
Subject: [R] randomForest and missing data


Does anyone know a reason why, in principle, a call to randomForest
cannot accept a data frame with missing predictor values? If each
individual tree is built using CART, then it seems like this
should be possible. (I understand that one may impute missing values
using rfImpute or some other method, but I would like to avoid doing
that.) 

If this functionality were available, then when the trees are being
constructed and when subsequent data are put through the forest, one
would also specify an argument for the use of surrogate rules, just
like in rpart. 

I realize this question is very specific to randomForest, as opposed
to R in general, but any comments are appreciated. I suppose I am
looking for someone to say "It's not appropriate, and here's why
..." or "Good idea. Please implement and post your code."

Thanks,

Darin England, Senior Scientist
Ingenix

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From gunter.berton at gene.com  Thu Jan  4 22:54:53 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Thu, 4 Jan 2007 13:54:53 -0800
Subject: [R] Some Windows code for GUI-izing workspace loading
In-Reply-To: <459D547B.6030009@stats.uwo.ca>
Message-ID: <001801c7304a$f7966410$4d908980@gne.windows.gene.com>

 
Folks:

Motivated by the recent thread on setting working directories, below are a
couple of functions for GUI-izing saving and loading files **in Windows
only** that sort of takes care of this automatically.  The simple strategy
is just to maintain a file consisting of the filenames of recently saved
workspace (.Rdata, etc.)files. Whenever I save a workspace via the function
mySave() below, the filename is chosen via a standard Windows file browser,
and the filename where the workspace was saved is added to the list if it
isn't already there. The recent() function then reads this file and brings
up a GUI standard Windows list box (via select.list()) of the first k
filenames (default k = 10) to load into the workspace **and** sets the
working directory to that of the first file loaded (several can be brought
in at once).

I offer these functions with some trepidation: they are extremely simple and
unsophisticated, and you definitely use them at your own risk. There is no
checking nor warning for whether object names in one loaded file duplicate
and hence overwrite those in another when more than one is loaded, for
example. Nevertheless, I have found the functions handy, as I use the
"recently used files" options on all my software all the time and wanted to
emulate this for R.

Suggestions for improvement (or better yet, code!) or information about bugs
or other stupidities gratefully appreciated.

Cheers,

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404


#### Code Follows  #####

mySave<-
function(recentlistFile=paste("c:/Program
Files/R","recentFiles.txt",sep="/"),
		savePlots=FALSE)
{
## DESCRIPTION:
## Use a windows GUI to save current workspace

## ARGUMENTS:
##    recentlistFile: a quoted character string giving the full
pathname/filename to
##		the file containing the listof recent files.
##		This must be the same as the "filename" argument of recent()
##    The default saves the file in the global R program directory, which
means it does not
##    have to be changed when updating to new versions of R which I store
under
##    the global R directory. You may need to change this if you have a
different
##    way of doing things.
##
##
##    savePlots: logical. Should the .SavedPlots plot history be saved? This
object can
##    be quite large and not saving it often makes saving and loading much
faster,
##    as well as avoiding memory problems. The default is not to save.

if(!savePlots) if(exists(".SavedPlots",where=1))rm(.SavedPlots,pos=1)

fname<-choose.files(caption='Save
As...',filters=Filters['RData',],multi=FALSE)
if(fname!=""){
	save.image(fname)
  if(!file.exists(recentlistFile))write(fname,recentlistFile,ncol=1)
  else{
	      nm<-scan(recentlistFile,what="",quiet=TRUE,sep="\n")
##      remove duplicate filenames and list in LIFO order
	      write(unique(c(fname,nm)),recentlistFile,ncol=1)
        }
	}
	else cat('\nWorkspace not saved\n')
}



 recent<-
function(filename=paste("c:/Program
Files/R","recentFiles.txt",sep="/"),nshow=10,
	setwork=TRUE)
{

## DESCRIPTION:
## GUI-izes workspace loading by bringing up a select box of files
containing
##	recently saved workspaces to load into R.

## ARGUMENTS:
        ## file: character. The full path name to the file containing the
file list, 
        ## which is a text file with the filenames, one per line.
        ##
        ##
        ## nshow: The maximum number of paths to show in the list
        ##
        ## setwork: logical. Should the working directory be set to that of
the first file
        ##  loaded?
        
        ## find the file containing the filenames if it exists
        if(!file.exists(filename))
                stop("File containing recent files list cannot be found.")
        filelist<-scan(filename,what=character(),quiet=TRUE,sep='\n')
        len<-length(filelist)
        if(!len)stop("No recent files")
        recentFiles<-select.list(filelist[1:min(nshow,len)],multiple=TRUE)
        if(!length(recentFiles))stop("No files selected")
        i<-0
        for(nm in recentFiles){
                if(file.exists(nm)){
                        load(nm,env=.GlobalEnv)
                        i<-i+1
                        if(i==1 &&setwork)setwd(dirname(nm))
                }
                else cat('\nFile',nm,'not found.\n')
        }
        cat('\n\n',i,paste(' file',ifelse(i==1,'','s'),' loaded\n',sep=""))
}


From england at cs.umn.edu  Thu Jan  4 23:07:10 2007
From: england at cs.umn.edu (Darin A. England)
Date: Thu, 4 Jan 2007 16:07:10 -0600
Subject: [R] randomForest and missing data
In-Reply-To: <2E17292A64E6ED418A60BE89326B1AAB3224C4@msgebe11.mfad.mfroot.org>
References: <2E17292A64E6ED418A60BE89326B1AAB3224C4@msgebe11.mfad.mfroot.org>
Message-ID: <20070104220709.GA28969@cs.umn.edu>

Yes I completely agree with your statements. As far as a way around
it, I would say that CART has some facilities for dealing with
missing data. e.g. when an observation is dropped into the tree and
encounters a split at which the variable is missing, then one option
is to simply not send it further down the tree. One may then obtain
a prediction for that interior node, albeit probably not a very good
one, but it is one way to handle cases with missing values. So, my
thought is that why can't we simply have that capability with
randomForest as well?

Darin

On Thu, Jan 04, 2007 at 03:44:27PM -0600, Sicotte, Hugues   Ph.D. wrote:
> I don't know about this module, but a general answer is that if you have
> missing data, it may affect your model. If your data is missing at
> random, then you might be lucky in your model building.
> 
> If however your data was not missing at random (e.g. censoring) , you
> might build a wrong predictor.
> 
> Missing at random or not, that is a question you should answer and deal
> with before modeling.
> 
> I refer you to a book like
> "Analysis of Incomplete Multivariate data". By Schafer
> 
> If there is a way around that with randomForest, I'd be interested to
> know too.
> 
> Hugues Sicotte
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Darin A. England
> Sent: Thursday, January 04, 2007 3:13 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] randomForest and missing data
> 
> 
> Does anyone know a reason why, in principle, a call to randomForest
> cannot accept a data frame with missing predictor values? If each
> individual tree is built using CART, then it seems like this
> should be possible. (I understand that one may impute missing values
> using rfImpute or some other method, but I would like to avoid doing
> that.) 
> 
> If this functionality were available, then when the trees are being
> constructed and when subsequent data are put through the forest, one
> would also specify an argument for the use of surrogate rules, just
> like in rpart. 
> 
> I realize this question is very specific to randomForest, as opposed
> to R in general, but any comments are appreciated. I suppose I am
> looking for someone to say "It's not appropriate, and here's why
> ..." or "Good idea. Please implement and post your code."
> 
> Thanks,
> 
> Darin England, Senior Scientist
> Ingenix
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Thu Jan  4 23:19:37 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 4 Jan 2007 17:19:37 -0500
Subject: [R] importing timestamp data into R
In-Reply-To: <1c6126db0701041339p483e3587j34139027ab2018a8@mail.gmail.com>
References: <1c6126db0701041339p483e3587j34139027ab2018a8@mail.gmail.com>
Message-ID: <971536df0701041419o3db4b405lcf91f15bfe6eae80@mail.gmail.com>

Try this:

Lines <- "DT_1,DT_2
[2006/08/10 21:12:14 ],[2006/08/10 21:54:00 ]
[2006/08/10 20:42:00 ],[2006/08/10 22:48:00 ]
[2006/08/10 20:58:00 ],[2006/08/10 21:39:00 ]
[2006/08/04 12:15:24 ],[2006/08/04 12:20:00 ]
[2006/08/04 12:02:00 ],[2006/08/04 14:20:00 ]
"

Lines2 <- gsub("\\[|\\]", "", readLines(textConnection(Lines)))

# using chron
library(chron)
DT <- read.csv(textConnection(Lines2))
DT[] <- lapply(DT, function(x)
   chron(substring(x, 1, 10), substring(x, 12), format = c("Y/M/D", "h:m:s")))
DT

# using POSIXct
DT <- read.csv(textConnection(Lines2))
DT[] <- lapply(DT, as.POSIXct)
DT


On 1/4/07, sj <ssj1364 at gmail.com> wrote:
> I have a set of timestamp data that I have in a text file that I would like
> to import into R for analysis.
> The timestamps are formated as follows:
>
> DT_1,DT_2
> [2006/08/10 21:12:14 ],[2006/08/10 21:54:00 ]
> [2006/08/10 20:42:00 ],[2006/08/10 22:48:00 ]
> [2006/08/10 20:58:00 ],[2006/08/10 21:39:00 ]
> [2006/08/04 12:15:24 ],[2006/08/04 12:20:00 ]
> [2006/08/04 12:02:00 ],[2006/08/04 14:20:00 ]
>
> I can get them into R but I cannot figure out how to convert them into
> something R will recognize as a date/time. I have tried using "as.Date",
> strptime, and chron. Any help would be appreciated?
>
> best,
>
> Spencer
>
>
>
> On 1/4/07, Darin A. England <england at cs.umn.edu> wrote:
> >
> >
> > Does anyone know a reason why, in principle, a call to randomForest
> > cannot accept a data frame with missing predictor values? If each
> > individual tree is built using CART, then it seems like this
> > should be possible. (I understand that one may impute missing values
> > using rfImpute or some other method, but I would like to avoid doing
> > that.)
> >
> > If this functionality were available, then when the trees are being
> > constructed and when subsequent data are put through the forest, one
> > would also specify an argument for the use of surrogate rules, just
> > like in rpart.
> >
> > I realize this question is very specific to randomForest, as opposed
> > to R in general, but any comments are appreciated. I suppose I am
> > looking for someone to say "It's not appropriate, and here's why
> > ..." or "Good idea. Please implement and post your code."
> >
> > Thanks,
> >
> > Darin England, Senior Scientist
> > Ingenix
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From karl.sommer at dpi.vic.gov.au  Thu Jan  4 23:33:54 2007
From: karl.sommer at dpi.vic.gov.au (karl.sommer at dpi.vic.gov.au)
Date: Fri, 5 Jan 2007 09:33:54 +1100
Subject: [R] R grahics: Save as hangs computer
Message-ID: <OF6EC4AEB8.E3D2B5D6-ONCA257259.00795AC1-CA257259.007BF448@nre.vic.gov.au>


Hello,

thanks for the advice.  I was aware that the version was out of date and
your message prompted me to finally upgrade to the latest version 2.4.1.
Unfortunately, running R under Emacs ESS, the problem I described earlier
persists.

I have also tried the Save as option under the standard RGui interface and
this worked in both the old and the new versions of R, 2.3.0 and 2.4.1
respectively.  The problem seems to be associated with Emacs ESS.  However
I don't have a clue where to start in order to find a solution.

In general I quite like using Emacs ESS.  It provides syntax highlighting
and this makes scripts far easier to read than with the standard editor
that comes with RGui.

I would be grateful for any further hints from someone who has encountered
similar problems in running R 2.4.1 through Emacs ESS 5.3.3 under Windows
2000.

Regards

Karl



|---------+---------------------------->
|         |           murdoch at stats.uwo|
|         |           .ca              |
|         |                            |
|         |           04/01/2007 10:10 |
|         |                            |
|---------+---------------------------->
  >------------------------------------------------------------------------------------------------------------------------------|
  |                                                                                                                              |
  |       To:       karl.sommer at dpi.vic.gov.au                                                                                   |
  |       cc:       r-help at stat.math.ethz.ch                                                                                     |
  |       Subject:  Re: [R] R grahics: Save as hangs computer                                                                    |
  >------------------------------------------------------------------------------------------------------------------------------|




On 1/3/2007 5:30 PM, karl.sommer at dpi.vic.gov.au wrote:
> Hello list,
>
> I have encountered a problem trying to save graphs using the R-graphics
> menu:  File|Save as.  The menu suggests that files may be saved as either
> Metafile, Postscript, pdf, png, bmp, jpeg.
> When I specify any of those file formats a menu comes up requesting a
file
> name.  After providing a name R invariably hangs and has to be restarted.
>
> I am able to save files under the various formats using the command line
> without problems.  However, sometimes it would be convenient to use the
> menus.
>
> I was wondering if anyone else had encountered a similar behaviour and
had
> found a remedy.
>
> I am running are under GNU-Emacs ESS 5.3.3.
>
>> sessionInfo()
> Version 2.3.0 (2006-04-24)

That version is out of date.  Could you please update to the current
version (2.4.1), and see if the problem persists?  If so, could you
please try it when running Rterm or Rgui on its own, rather than running
under Emacs?

Thanks.

Duncan Murdoch

> i386-pc-mingw32
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"
"datasets"
> [7] "base"
>
> other attached packages:
>  lattice
> "0.13-8"
>
> Regards
>
> Karl
>
> _________________________________
> Dr Karl J Sommer,
> Department of Primary Industries,
> Catchment & Agriculture Services,
> PO Box 905
> Mildura, VIC, 3502
> Australia
>
> Tel: +61 (0)3 5051 4390
> Fax +61 (0)3 5051 4534
>
> Email:     karl.sommer at dpi.vic.gov.au
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From charmyss at hotmail.com  Thu Jan  4 23:06:40 2007
From: charmyss at hotmail.com (Shuangshuang Jin ^_^)
Date: Thu, 04 Jan 2007 14:06:40 -0800
Subject: [R] A question on REML in R
In-Reply-To: <BAY119-F178C0A7B7C17EF5B6CDDB1C3BA0@phx.gbl>
Message-ID: <BAY119-F6C4F4711C2CE64DDFE3AEC3B80@phx.gbl>

Hello, everyone, I'm using R to deal with a REML problem. I found "lmer" is 
the right function for this. But I got stuck because I couldn't interpret 
the result. I'm attaching a short example of my executing log. Please have a 
look and give me some advice on it. Thanks a lot!

  Plot  Block   Treatment   Data
    1     1        2         7.8
    2     1        1         5.9
    3     1        3        10.3
    4     2        3        10.9
    5     2        2         8.9
    6     2        1         7.2
    7     3        2        11.1
    8     3        3        12.8
    9     3        1         9.1
   10     4        1         9.8
   11     4        2        12.2
   12     4        3        14.0

>anova(lm(Data~as.factor(Treatment)+as.factor(Block)))
Analysis of Variance Table

Response: Data
                     Df Sum Sq Mean Sq F value    Pr(>F)
as.factor(Treatment)  2 32.000  16.000  282.35 1.162e-06 ***
as.factor(Block)      3 30.000  10.000  176.47 3.066e-06 ***
Residuals             6  0.340   0.057
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

>lmer(Data~Treatment+(Treatment|Block))
Linear mixed-effects model fit by REML
Formula: Data ~ Treatment + (Treatment | Block)
   AIC  BIC logLik MLdeviance REMLdeviance
28.68 31.1 -9.339      16.88        18.68
Random effects:
Groups   Name        Variance   Std.Dev.   Corr
Block    (Intercept) 3.3116e+00 1.8198e+00
          Treatment   2.4303e-11 4.9298e-06 0.000
Residual             4.8606e-02 2.2047e-01
number of obs: 12, groups: Block, 4

Fixed effects:
            Estimate Std. Error t value
(Intercept)  6.00000    0.92533   6.484
Treatment    2.00000    0.07795  25.658

Correlation of Fixed Effects:
          (Intr)
Treatment -0.168
Warning message:
Estimated variance-covariance for factor 'Block' is singular
in: `LMEoptimize<-`(`*tmp*`, value = list(maxIter = 200, tolerance = 
1.49011611938477e-08,

Charmy

_________________________________________________________________
Communicate instantly! Use your Hotmail address to sign into Windows Live 
Messenger now. http://get.live.com/messenger/overview


From jeff.horner at vanderbilt.edu  Thu Jan  4 23:43:52 2007
From: jeff.horner at vanderbilt.edu (Jeffrey Horner)
Date: Thu, 04 Jan 2007 16:43:52 -0600
Subject: [R] littler+dget+stdin -> segmentation fault
In-Reply-To: <enjlh0$4uc$1@sea.gmane.org>
References: <enjlh0$4uc$1@sea.gmane.org>
Message-ID: <459D8328.5080900@vanderbilt.edu>

John Lawrence Aspden wrote:
> Hi, I'm trying to write a series of pipes using littler, and I get the
> following behaviour: Sorry if I'm just doing something witless, I'm new to
> R. I'm using the latest versions from debian testing (2.4.0 and 0.0.8).
> 
> 
> $ r -e 'a<-dget(file=stdin()); print(a)'
> ?list(a=2)
> Segmentation fault

You've found a bug which has been fixed. Expect a new version 0.0.9 of 
littler later tonight from here:

http://dirk.eddelbuettel.com/code/littler/

and soon after than in debian.

What exactly are you trying to accomplish?

Jeff
-- 
http://biostat.mc.vanderbilt.edu/JeffreyHorner


From helprhelp at gmail.com  Thu Jan  4 23:50:20 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Thu, 4 Jan 2007 17:50:20 -0500
Subject: [R] randomForest and missing data
In-Reply-To: <20070104211314.GA27922@cs.umn.edu>
References: <20070104211314.GA27922@cs.umn.edu>
Message-ID: <cdf817830701041450o706eb7c8nb1ba4985033f2fae@mail.gmail.com>

You can try randomForest in Fortran codes, which has that function
doing missing replacement automatically. There are two ways of
imputations (one is fast and the other is time-consuming) to do that.
I did it long time ago.

the link is below. If you have any question, just let me know.
http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm

In principle, each individual tree is NOT a cart tree since each
splitting predictor is randomly selected. In my impression, rf is more
like nearest neighbor algorithm. The surrogation is NOT used in rf
implementation. That's "why" you have to impute it before using it;
while the imputation is not implemented in r-version, in my best
knowledge.
You can check that from reading the original technical report or some
presentation by original authors. I remember there was some slide
comparing rf and CART somewhere.


HTH,

weiwei

On 1/4/07, Darin A. England <england at cs.umn.edu> wrote:
>
> Does anyone know a reason why, in principle, a call to randomForest
> cannot accept a data frame with missing predictor values? If each
> individual tree is built using CART, then it seems like this
> should be possible. (I understand that one may impute missing values
> using rfImpute or some other method, but I would like to avoid doing
> that.)
>
> If this functionality were available, then when the trees are being
> constructed and when subsequent data are put through the forest, one
> would also specify an argument for the use of surrogate rules, just
> like in rpart.
>
> I realize this question is very specific to randomForest, as opposed
> to R in general, but any comments are appreciated. I suppose I am
> looking for someone to say "It's not appropriate, and here's why
> ..." or "Good idea. Please implement and post your code."
>
> Thanks,
>
> Darin England, Senior Scientist
> Ingenix
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From rmh at temple.edu  Fri Jan  5 00:03:16 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu,  4 Jan 2007 18:03:16 -0500 (EST)
Subject: [R] R grahics: Save as hangs computer
Message-ID: <20070104180316.BRJ29671@po-d.temple.edu>

The good news, you don't have to shut down R.  Several control-G in
the *R* buffer in emacs will recover control.  A second attempt in the
same GUI graphics device did get the postscript file saved.

A workaround for this problem is to use the command line, rather than
the GUI menu, to save the file.  This line
    dev.copy2eps()
works.

I discovered that it is necessary to set
   options(chmhelp=FALSE)
when running R from emacs as the chmhelp is also freezing R
and the help system.  I am using  R-2.4.1 on Windows.

Use
   ?dev.copy2eps
for details on the command line dev.* commands.

Follow-up should go to the ess-bugs mailing list.

Rich


From ripley at stats.ox.ac.uk  Fri Jan  5 00:25:47 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Jan 2007 23:25:47 +0000 (GMT)
Subject: [R] memory limits in R loading a dataset and using the package
 tree
In-Reply-To: <e591a95b0701040830j37ac495arc72ff9dc74987e0d@mail.gmail.com>
References: <e591a95b0701040830j37ac495arc72ff9dc74987e0d@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701042319510.8318@gannet.stats.ox.ac.uk>

Please read the rw-FAQ Q2.9.  There are ways to raise the limit, and you 
have not told us that you used them (nor the version of R you used, which 
matters as the limits are version-specific).

Beyond that, there are ways to use read.table more efficiently: see its 
help page and the 'R Data Import/Export' manual.  In particular, did you 
set nrows and colClasses?

But for the size of problem you have I would use a 64-bit build of R.


On Thu, 4 Jan 2007, domenico pestalozzi wrote:

> I think the question is discussed in other thread, but I don't exactly find
> what I want .
> I'm working in Windows XP with 2GB of memory and a Pentium 4 - 3.00Ghx.
> I have the necessity of working with large dataset, generally from 300,000
> records to 800,000 (according to the project), and about 300 variables
> (...but a dataset with 800,000 records could not be "large" in your
> opinion...). Because of we are deciding if R will be the official software
> in our company, I'd like to say if the possibility of using R with these
> datasets depends only by the characteristics of the "engine" (memory and
> processor).
> In this case we can improve the machine (for example, what memory you
> reccomend?).
>
> For example, I have a dataset of 200,000 records and 211 variables but I
> can't load the dataset because R doesn't work : I control the loading
> procedure (read.table in R) by using the windows task-manager and R is
> blocked when the file paging is 1.10 GB.
> After this I try with a sample of 100,000 records and I can correctly load
> tha dataset, but I'd like to use the package tree, but after some seconds (
> I use this tree(variable1~., myDataset) )   I obtain the message "Reached
> total allocation of 1014Mb".
>
> I'd like your opinion and suggestion, considering that I could improve (in
> memory) my computer.
>
> pestalozzi
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Jan  5 00:29:54 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Jan 2007 23:29:54 +0000 (GMT)
Subject: [R] problem with plot() and POSIXt dates
In-Reply-To: <15C100200A5F4E45AF8CFB45A926EF3418AEED@allexch01.alliance.com>
References: <15C100200A5F4E45AF8CFB45A926EF3418AEED@allexch01.alliance.com>
Message-ID: <Pine.LNX.4.64.0701042327090.8318@gannet.stats.ox.ac.uk>

As the footer says

   PLEASE do read the posting guide
   http://www.R-project.org/posting-guide.html
   and provide commented, minimal, self-contained, reproducible code.

We can't help you with a problem we cannot reproduce, and random guessing 
is not going to be productive.

On Thu, 4 Jan 2007, COMTE Guillaume wrote:

> Hy all,
>
> I'm plotting graphs using plot() function, they are on X axes POSIX dates:
> "POSIXt"   "oldClass" "POSIXct"  "POSIXlt"
> I can't figure out why sometimes it prints the month and days and sometimes it prints the unix timestamp.
> It appens usually when the xlim is short like only some days.
> xlim is settled as a POSIXt like this
> "2006-12-30 17:25:44 CET" "2007-01-02 03:16:51 CET"
> On the graph it prints : 1167500000 and 1167700000 instead of dates.
> And the result gives a x axes in unix timestamps as if the plot function didn't recognize that it is a timestamp but just an integer.
> What am i missing, since R sees itself that it is time stamps and not integer when xlim is enougth large, how do i tell the plot function to see these numbers as POSIXt?
>
> thks.
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From tebaldi at rap.ucar.edu  Fri Jan  5 00:30:15 2007
From: tebaldi at rap.ucar.edu (Claudia Tebaldi)
Date: Thu, 4 Jan 2007 15:30:15 -0800 (PST)
Subject: [R] color of opposite sign values in filled.contour
Message-ID: <56323.171.64.241.204.1167953415.squirrel@mail.rap.ucar.edu>

Dear R-helpers,

I'm plotting geophysical data in the form of contours using
"filled.contour". The display would be much more effective if the areas
with negative values could be color coded
by -- say -- "cold colors" in the range of blue to green, and conversely
the areas with positive values got plotted with "warm colors", from yellow
to red.
Right now if I use a palette spanning the spectrum I need the entire range
is associated with the actual range of the data, which can be positively
or negatively skewed, and as a result the position of the zero is totally
arbitrary.
I'm wondering if someone out there has come up with a clever way to set
the color scale accordingly, as a function of the actual range of the
values in the matrix that is being plotted. Ideally, it would be neat to
still use the entire spectrum, but sampling differently the cold and warm
subsets accordingly to the extent of the negative and positive values in
the data.

Also, when I try to play around in an ad hoc fashion with the palette I
often get funny results in the legend, with color-scale wrapping or blank
cells at one of the extremes. I cannot hack effectively the code of the
filled.contour function, obviously...


Thank you in advance for your help
& happy new year

claudia tebaldi











-- 
Claudia Tebaldi
ISSE/CGD/IMAGe
http://www.image.ucar.edu/~tebaldi

currently visiting
Center for Environmental Science and Policy
Stanford University
tel:   (650) 724-9261
skype: claudia.tebaldi


From ripley at stats.ox.ac.uk  Fri Jan  5 00:45:15 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Jan 2007 23:45:15 +0000 (GMT)
Subject: [R] loess
In-Reply-To: <459D1FA6.5040804@jyu.fi>
References: <459D1FA6.5040804@jyu.fi>
Message-ID: <Pine.LNX.4.64.0701042334580.8318@gannet.stats.ox.ac.uk>

On Thu, 4 Jan 2007, Jukka Nyblom wrote:

> Hi,
>
> I have tried
>
> > for (i in 1:100) L[,i] <- loess((i = =(1:100))~I(1:100), span=.5,
> degree=1)$fit
>
> to create a matrix which gives me the smoothing weights (correctly as
> far as I have experienced), eg.
>
> > yhat <- loess(y~I(1:100), span=.5,degree=1)$fit
> > yhat[30]
> [1] -0.2131983
> > L[30,]%*%y
>           [,1]
> [1,] -0.2131983
>
> But,  L[30,] has 56 nonzero coefficients, not 50 that I expect with span
> = 0.5. Actually the number of nonzero elements on rows varies being 49,
> 50, 55 or 56.
>
> Does anyone know why?

loess is a complicated algorithm, and you need to study the background 
references in depth to fully understand it.  In particular, the default is 
not to do direct fitting (as I guess you are assuming) but interpolation. 
See ?loess.control.

Most descriptions, including the help page, are simplifications.

> Jukka Nyblom

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From news at aspden.com  Fri Jan  5 00:55:58 2007
From: news at aspden.com (John Lawrence Aspden)
Date: Thu, 04 Jan 2007 23:55:58 +0000
Subject: [R] littler+dget+stdin -> segmentation fault
References: <enjlh0$4uc$1@sea.gmane.org> <459D8328.5080900@vanderbilt.edu>
Message-ID: <enk46e$etm$1@sea.gmane.org>

Jeffrey Horner wrote:

> John Lawrence Aspden wrote:
>> Hi, I'm trying to write a series of pipes using littler, and I get the
>> following behaviour: 
>> 
>> $ r -e 'a<-dget(file=stdin()); print(a)'
>> ?list(a=2)
>> Segmentation fault
> 
> You've found a bug which has been fixed. Expect a new version 0.0.9 of
> littler later tonight from here:
> 
> http://dirk.eddelbuettel.com/code/littler/
> 
> What exactly are you trying to accomplish?


Hi Jeff, that's gratifying! Thanks.

I'm trying to write some scripts to process the output from a brain scanner, 
using some routines which someone else wrote to use interactively in R.

The idea is to hide R from the end-user so that as far as they're concerned 
it's just a normal unix filter. Indeed the end-user is probably going to be 
another program eventually.

However the first process is a time consuming wavelet transform and 
correlation step, and then there are loads of possibilities for what to do 
with the result.

so I want to be able to say:

process1 <brain.data >processed.data (time consuming)

to do the hard bit, and then e.g.

process2 <processed.data
process3 <processed.data | graphing-tool

to produce various visualizations and statistics

I'm using dput/dget to read and write the intermediate structure (a list of 
matrices). All the other methods seem to remember the structure's original 
name and then stick it in the global namespace, which I find annoying. I
also like the fact that dput/dget work with text files, which means that
one day I'll be able to drop in a replacement for process1 written in C to
deal with anticipated huge data sets.

This seems to be working well so far (I'm only prototyping to see if it's 
feasible), but I'm having to work around the bug by having the file as a 
command line argument rather than piping it in.

Cheers, John.

-- 
Contractor in Cambridge UK -- http://www.aspden.com


From h.wickham at gmail.com  Fri Jan  5 01:01:33 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 4 Jan 2007 16:01:33 -0800
Subject: [R] color of opposite sign values in filled.contour
In-Reply-To: <56323.171.64.241.204.1167953415.squirrel@mail.rap.ucar.edu>
References: <56323.171.64.241.204.1167953415.squirrel@mail.rap.ucar.edu>
Message-ID: <f8e6ff050701041601o566c3912p9e5f39f8daf9b5dd@mail.gmail.com>

Hi Claudia,

It's quite easy to do this using ggplot, although you get exactly the
same appearance as filled.contour (hopefully in the next version).
Have a look at ggtile and scgradient.

Regards,

Hadley

On 1/4/07, Claudia Tebaldi <tebaldi at rap.ucar.edu> wrote:
> Dear R-helpers,
>
> I'm plotting geophysical data in the form of contours using
> "filled.contour". The display would be much more effective if the areas
> with negative values could be color coded
> by -- say -- "cold colors" in the range of blue to green, and conversely
> the areas with positive values got plotted with "warm colors", from yellow
> to red.
> Right now if I use a palette spanning the spectrum I need the entire range
> is associated with the actual range of the data, which can be positively
> or negatively skewed, and as a result the position of the zero is totally
> arbitrary.
> I'm wondering if someone out there has come up with a clever way to set
> the color scale accordingly, as a function of the actual range of the
> values in the matrix that is being plotted. Ideally, it would be neat to
> still use the entire spectrum, but sampling differently the cold and warm
> subsets accordingly to the extent of the negative and positive values in
> the data.
>
> Also, when I try to play around in an ad hoc fashion with the palette I
> often get funny results in the legend, with color-scale wrapping or blank
> cells at one of the extremes. I cannot hack effectively the code of the
> filled.contour function, obviously...
>
>
> Thank you in advance for your help
> & happy new year
>
> claudia tebaldi
>
>
>
>
>
>
>
>
>
>
>
> --
> Claudia Tebaldi
> ISSE/CGD/IMAGe
> http://www.image.ucar.edu/~tebaldi
>
> currently visiting
> Center for Environmental Science and Policy
> Stanford University
> tel:   (650) 724-9261
> skype: claudia.tebaldi
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch at stats.uwo.ca  Fri Jan  5 01:35:48 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 04 Jan 2007 19:35:48 -0500
Subject: [R] R grahics: Save as hangs computer
In-Reply-To: <OF6EC4AEB8.E3D2B5D6-ONCA257259.00795AC1-CA257259.007BF448@nre.vic.gov.au>
References: <OF6EC4AEB8.E3D2B5D6-ONCA257259.00795AC1-CA257259.007BF448@nre.vic.gov.au>
Message-ID: <459D9D64.3090906@stats.uwo.ca>

On 1/4/2007 5:33 PM, karl.sommer at dpi.vic.gov.au wrote:
> Hello,
> 
> thanks for the advice.  I was aware that the version was out of date and
> your message prompted me to finally upgrade to the latest version 2.4.1.
> Unfortunately, running R under Emacs ESS, the problem I described earlier
> persists.
> 
> I have also tried the Save as option under the standard RGui interface and
> this worked in both the old and the new versions of R, 2.3.0 and 2.4.1
> respectively.  The problem seems to be associated with Emacs ESS.  However
> I don't have a clue where to start in order to find a solution.

There may be other Emacs implementations available on Windows; if so, 
I'd try one of those.  If that doesn't work, you could try filing this 
as an Emacs bug report, but I suspect that it won't get fixed.  Your 
best choice may be to abandon Windows or Emacs.

Duncan Murdoch
> 
> In general I quite like using Emacs ESS.  It provides syntax highlighting
> and this makes scripts far easier to read than with the standard editor
> that comes with RGui.
> 
> I would be grateful for any further hints from someone who has encountered
> similar problems in running R 2.4.1 through Emacs ESS 5.3.3 under Windows
> 2000.
> 
> Regards
> 
> Karl
> 
> 
> 
> |---------+---------------------------->
> |         |           murdoch at stats.uwo|
> |         |           .ca              |
> |         |                            |
> |         |           04/01/2007 10:10 |
> |         |                            |
> |---------+---------------------------->
>   >------------------------------------------------------------------------------------------------------------------------------|
>   |                                                                                                                              |
>   |       To:       karl.sommer at dpi.vic.gov.au                                                                                   |
>   |       cc:       r-help at stat.math.ethz.ch                                                                                     |
>   |       Subject:  Re: [R] R grahics: Save as hangs computer                                                                    |
>   >------------------------------------------------------------------------------------------------------------------------------|
> 
> 
> 
> 
> On 1/3/2007 5:30 PM, karl.sommer at dpi.vic.gov.au wrote:
>> Hello list,
>>
>> I have encountered a problem trying to save graphs using the R-graphics
>> menu:  File|Save as.  The menu suggests that files may be saved as either
>> Metafile, Postscript, pdf, png, bmp, jpeg.
>> When I specify any of those file formats a menu comes up requesting a
> file
>> name.  After providing a name R invariably hangs and has to be restarted.
>>
>> I am able to save files under the various formats using the command line
>> without problems.  However, sometimes it would be convenient to use the
>> menus.
>>
>> I was wondering if anyone else had encountered a similar behaviour and
> had
>> found a remedy.
>>
>> I am running are under GNU-Emacs ESS 5.3.3.
>>
>>> sessionInfo()
>> Version 2.3.0 (2006-04-24)
> 
> That version is out of date.  Could you please update to the current
> version (2.4.1), and see if the problem persists?  If so, could you
> please try it when running Rterm or Rgui on its own, rather than running
> under Emacs?
> 
> Thanks.
> 
> Duncan Murdoch
> 
>> i386-pc-mingw32
>>
>> attached base packages:
>> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"
> "datasets"
>> [7] "base"
>>
>> other attached packages:
>>  lattice
>> "0.13-8"
>>
>> Regards
>>
>> Karl
>>
>> _________________________________
>> Dr Karl J Sommer,
>> Department of Primary Industries,
>> Catchment & Agriculture Services,
>> PO Box 905
>> Mildura, VIC, 3502
>> Australia
>>
>> Tel: +61 (0)3 5051 4390
>> Fax +61 (0)3 5051 4534
>>
>> Email:     karl.sommer at dpi.vic.gov.au
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ssk2031 at columbia.edu  Fri Jan  5 03:02:17 2007
From: ssk2031 at columbia.edu (ssk2031 at columbia.edu)
Date: Thu, 04 Jan 2007 21:02:17 -0500
Subject: [R] R grahics: Save as hangs computer
In-Reply-To: <459D9D64.3090906@stats.uwo.ca>
References: <OF6EC4AEB8.E3D2B5D6-ONCA257259.00795AC1-CA257259.007BF448@nre.vic.gov.au>
	<459D9D64.3090906@stats.uwo.ca>
Message-ID: <1167962537.459db1a981cd4@cubmail.cc.columbia.edu>


Just in case you were not already aware, you could try Tinn-R
(http://www.sciviews.org/Tinn-R/) or JGR (http://rosuda.org/JGR/)
or one of the editors described at this site
(http://www.sciviews.org/_rgui/projects/Editors.html) as Emacs
alternatives. I love Emacs and miss the speed of using Emacs
keystrokes to navigate the files, but the other editors do offer
easier access to command syntax help among other things. I found
that I ended up crashing Emacs-ESS-R a lot on Windows XP and so now
I use Tinn-R.

-S.

Quoting Duncan Murdoch <murdoch at stats.uwo.ca>:

> On 1/4/2007 5:33 PM, karl.sommer at dpi.vic.gov.au wrote:
> > Hello,
> >
> > thanks for the advice.  I was aware that the version was out of
> date and
> > your message prompted me to finally upgrade to the latest
> version 2.4.1.
> > Unfortunately, running R under Emacs ESS, the problem I
> described earlier
> > persists.
> >
> > I have also tried the Save as option under the standard RGui
> interface and
> > this worked in both the old and the new versions of R, 2.3.0
> and 2.4.1
> > respectively.  The problem seems to be associated with Emacs
> ESS.  However
> > I don't have a clue where to start in order to find a solution.
>
> There may be other Emacs implementations available on Windows; if
> so,
> I'd try one of those.  If that doesn't work, you could try filing
> this
> as an Emacs bug report, but I suspect that it won't get fixed.
> Your
> best choice may be to abandon Windows or Emacs.
>
> Duncan Murdoch
> >
> > In general I quite like using Emacs ESS.  It provides syntax
> highlighting
> > and this makes scripts far easier to read than with the
> standard editor
> > that comes with RGui.
> >
> > I would be grateful for any further hints from someone who has
> encountered
> > similar problems in running R 2.4.1 through Emacs ESS 5.3.3
> under Windows
> > 2000.
> >
> > Regards
> >
> > Karl
> >
> >
> >
> > |---------+---------------------------->
> > |         |           murdoch at stats.uwo|
> > |         |           .ca              |
> > |         |                            |
> > |         |           04/01/2007 10:10 |
> > |         |                            |
> > |---------+---------------------------->
> >
>
>------------------------------------------------------------------------------------------------------------------------------|
> >   |
>
> |
> >   |       To:       karl.sommer at dpi.vic.gov.au
>
> |
> >   |       cc:       r-help at stat.math.ethz.ch
>
> |
> >   |       Subject:  Re: [R] R grahics: Save as hangs computer
>
> |
> >
>
>------------------------------------------------------------------------------------------------------------------------------|
> >
> >
> >
> >
> > On 1/3/2007 5:30 PM, karl.sommer at dpi.vic.gov.au wrote:
> >> Hello list,
> >>
> >> I have encountered a problem trying to save graphs using the
> R-graphics
> >> menu:  File|Save as.  The menu suggests that files may be
> saved as either
> >> Metafile, Postscript, pdf, png, bmp, jpeg.
> >> When I specify any of those file formats a menu comes up
> requesting a
> > file
> >> name.  After providing a name R invariably hangs and has to be
> restarted.
> >>
> >> I am able to save files under the various formats using the
> command line
> >> without problems.  However, sometimes it would be convenient
> to use the
> >> menus.
> >>
> >> I was wondering if anyone else had encountered a similar
> behaviour and
> > had
> >> found a remedy.
> >>
> >> I am running are under GNU-Emacs ESS 5.3.3.
> >>
> >>> sessionInfo()
> >> Version 2.3.0 (2006-04-24)
> >
> > That version is out of date.  Could you please update to the
> current
> > version (2.4.1), and see if the problem persists?  If so, could
> you
> > please try it when running Rterm or Rgui on its own, rather
> than running
> > under Emacs?
> >
> > Thanks.
> >
> > Duncan Murdoch
> >
> >> i386-pc-mingw32
> >>
> >> attached base packages:
> >> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"
> > "datasets"
> >> [7] "base"
> >>
> >> other attached packages:
> >>  lattice
> >> "0.13-8"
> >>
> >> Regards
> >>
> >> Karl
> >>
> >> _________________________________
> >> Dr Karl J Sommer,
> >> Department of Primary Industries,
> >> Catchment & Agriculture Services,
> >> PO Box 905
> >> Mildura, VIC, 3502
> >> Australia
> >>
> >> Tel: +61 (0)3 5051 4390
> >> Fax +61 (0)3 5051 4534
> >>
> >> Email:     karl.sommer at dpi.vic.gov.au
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible
> code.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible
> code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible
> code.
>


From liud2 at mail.nih.gov  Fri Jan  5 03:08:20 2007
From: liud2 at mail.nih.gov (Liu, Delong (NIH/CIT) [C])
Date: Thu, 4 Jan 2007 21:08:20 -0500
Subject: [R] coefficients of each local polynomial from loess() or locfit()
Message-ID: <08BFEF2D7CC3104FA411B6E1991200C21D85C2@NIHCESMLBX3.nih.gov>

I want to extract estimated coeffiicents of each local polynomial at given x from loess(),  locfit(), or KernSmooth().  Can some experts provide me with suggestions?  Thanks.
 
Delong Liu


From maj at stats.waikato.ac.nz  Fri Jan  5 04:18:00 2007
From: maj at stats.waikato.ac.nz (maj at stats.waikato.ac.nz)
Date: Fri, 5 Jan 2007 16:18:00 +1300 (NZDT)
Subject: [R] ifelse on data frames
Message-ID: <2213.203.109.182.160.1167967080.squirrel@webmail.scms.waikato.ac.nz>

[Using R 2.2.0 on Windows XP; OK, OK, I will update soon!]

I have noticed some undesirable behaviour when applying
ifelse to a data frame. Here is my code:

A <- scan()
 1.000000 0.000000 0.000000  0 0.00000
 0.027702 0.972045 0.000253  0 0.00000

A <- matrix(A,nrow=2,ncol=5,byrow=T)
A == 0
ifelse(A==0,0,-A*log(A))

A <- as.data.frame(A)
ifelse(A==0,0,-A*log(A))

and this is the output:

> A <- scan()
1:  1.000000 0.000000 0.000000  0 0.00000
6:  0.027702 0.972045 0.000253  0 0.00000
11:
Read 10 items
> A <- matrix(A,nrow=2,ncol=5,byrow=T)
> A == 0
      [,1]  [,2]  [,3] [,4] [,5]
[1,] FALSE  TRUE  TRUE TRUE TRUE
[2,] FALSE FALSE FALSE TRUE TRUE
> ifelse(A==0,0,-A*log(A))
           [,1]       [,2]        [,3] [,4] [,5]
[1,] 0.00000000 0.00000000 0.000000000    0    0
[2,] 0.09934632 0.02756057 0.002095377    0    0
>
> A <- as.data.frame(A)
> ifelse(A==0,0,-A*log(A))
[[1]]
[1] 0.00000000 0.09934632

[[2]]
[1]        NaN 0.02756057

[[3]]
[1] 0

[[4]]
[1] NaN NaN

[[5]]
[1] 0

[[6]]
[1] 0.00000000 0.09934632

[[7]]
[1] 0

[[8]]
[1] 0

[[9]]
[1] 0

[[10]]
[1] 0

>

Is this a bug or a feature? Can the behaviour be explained?

Regards,  Murray Jorgensen
-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021 1395 862


From rmh at temple.edu  Fri Jan  5 04:35:32 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu,  4 Jan 2007 22:35:32 -0500 (EST)
Subject: [R] Fwd: Re:  R grahics: Save as hangs computer
Message-ID: <20070104223532.BRJ58896@po-d.temple.edu>

Can you send ess-bugs some reproducible examples of what isn't working?
These are things that are probably easily fixed.

Rich


---- Original message ----
>Date: Thu, 04 Jan 2007 21:02:17 -0500
>From: ssk2031 at columbia.edu  
>Subject: Re: [R] R grahics: Save as hangs computer  
>To: karl.sommer at dpi.vic.gov.au, r-help at stat.math.ethz.ch
>
>
>Just in case you were not already aware, you could try Tinn-R
>(http://www.sciviews.org/Tinn-R/) or JGR (http://rosuda.org/JGR/)
>or one of the editors described at this site
>(http://www.sciviews.org/_rgui/projects/Editors.html) as Emacs
>alternatives. I love Emacs and miss the speed of using Emacs
>keystrokes to navigate the files, but the other editors do offer
>easier access to command syntax help among other things. I found
>that I ended up crashing Emacs-ESS-R a lot on Windows XP and so now
>I use Tinn-R.
>
>-S.


From rmh at temple.edu  Fri Jan  5 04:39:06 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu,  4 Jan 2007 22:39:06 -0500 (EST)
Subject: [R] color of opposite sign values in filled.contour
Message-ID: <20070104223906.BRJ59189@po-d.temple.edu>

Get the RColorBrewer package from CRAN

Description: The packages provides palettes for drawing nice maps
        shaded according to a variable.


From karl.sommer at dpi.vic.gov.au  Fri Jan  5 04:48:30 2007
From: karl.sommer at dpi.vic.gov.au (karl.sommer at dpi.vic.gov.au)
Date: Fri, 5 Jan 2007 14:48:30 +1100
Subject: [R] R grahics: Save as hangs computer
Message-ID: <OFD39B9D80.2EC64287-ONCA257259.00814412-CA25725A.0014EB9C@nre.vic.gov.au>


Hello list

thanks for that advice Rich.  You are right a few control-G allowed me to
recover and it had saved an eps file.

The dev.copy2eps appears to take exactly what is visible in the graphics
window and convert it to an eps file. This does the job for the moment.

Opening the eps file in GSview I noticed that there is still a lot of white
space in the bounding box which creates empty space when I want to include
the image into a tex document.  Is there a way to crop the image to remove
the white space before including it in a document?

Cheers

Karl



|---------+---------------------------->
|         |           rmh at temple.edu   |
|         |                            |
|         |           05/01/2007 10:03 |
|         |                            |
|---------+---------------------------->
  >------------------------------------------------------------------------------------------------------------------------------|
  |                                                                                                                              |
  |       To:       karl.sommer at dpi.vic.gov.au, murdoch at stats.uwo.ca                                                             |
  |       cc:       r-help at stat.math.ethz.ch, ess-bugs at stat.math.ethz.ch                                                         |
  |       Subject:  Re: [R] R grahics: Save as hangs computer                                                                    |
  >------------------------------------------------------------------------------------------------------------------------------|




The good news, you don't have to shut down R.  Several control-G in
the *R* buffer in emacs will recover control.  A second attempt in the
same GUI graphics device did get the postscript file saved.

A workaround for this problem is to use the command line, rather than
the GUI menu, to save the file.  This line
    dev.copy2eps()
works.

I discovered that it is necessary to set
   options(chmhelp=FALSE)
when running R from emacs as the chmhelp is also freezing R
and the help system.  I am using  R-2.4.1 on Windows.

Use
   ?dev.copy2eps
for details on the command line dev.* commands.

Follow-up should go to the ess-bugs mailing list.

Rich


From rmh at temple.edu  Fri Jan  5 05:30:56 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu,  4 Jan 2007 23:30:56 -0500 (EST)
Subject: [R] Fwd: Re:  R grahics: Save as hangs computer
Message-ID: <20070104233056.BRJ64931@po-d.temple.edu>

R is actually smarter than that.  The bounding box is square.
>From 

plot(1:10)
dev.copy2eps(file="tmp.eps")


The bounding box (which you can see by opening tmp.eps in emacs) is
%%BoundingBox: 0 0 517 517
and LaTeX will therefore interpret it correctly.  GSview is misleading
you since it puts that square region at one end of the rectangle
implied by the GSview setting of media at "letter" or "A4".

In addition to most likely not needing to crop, there are many ways to crop
the image.

If cropping is just to get rid of white space, I usually do it in LaTeX by
adding some negative space
\height{-2in}
inside the figure environment.


Rich

---- Original message ----
>Date: Fri, 5 Jan 2007 14:48:30 +1100
>From: karl.sommer at dpi.vic.gov.au  
>Subject: Re: [R] R grahics: Save as hangs computer  
>To: rmh at temple.edu
>Cc: ess-bugs at stat.math.ethz.ch, murdoch at stats.uwo.ca, r-help at stat.math.ethz.ch
>

>Opening the eps file in GSview I noticed that there is still a lot of white
>space in the bounding box which creates empty space when I want to include
>the image into a tex document.  Is there a way to crop the image to remove
>the white space before including it in a document?


From rmh at temple.edu  Fri Jan  5 06:15:03 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri,  5 Jan 2007 00:15:03 -0500 (EST)
Subject: [R] R grahics: Save as hangs computer
Message-ID: <20070105001503.BRJ69225@po-d.temple.edu>

An additional note.

You can see the bounding box in the GSview display by clicking
Options/Show Bounding Box

Rich


From rmh at temple.edu  Fri Jan  5 06:16:31 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri,  5 Jan 2007 00:16:31 -0500 (EST)
Subject: [R] R grahics: Save as hangs computer
Message-ID: <20070105001631.BRJ69331@po-d.temple.edu>

another note
you can clip the GSview to just the bounding box with
Options/EPS CCip

Rich


From FredeA.Togersen at agrsci.dk  Fri Jan  5 08:46:41 2007
From: FredeA.Togersen at agrsci.dk (=?iso-8859-1?Q?Frede_Aakmann_T=F8gersen?=)
Date: Fri, 5 Jan 2007 08:46:41 +0100
Subject: [R] importing timestamp data into R
In-Reply-To: <1c6126db0701041339p483e3587j34139027ab2018a8@mail.gmail.com>
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC04C49256@DJFPOST01.djf.agrsci.dk>

Suppose that you have the timestamps in a comma seperated text file, "times.dat" then do

tid <- read.csv("times.dat", header = TRUE, colClasses = "character")

# Note the colClasses argument! If not used the columns of tid are factors by default.

time.conv <- function(x) as.POSIXct(strptime(x, format = "[%Y/%m/%d %H:%M:%S ]"))

# The value of strptime has class "POSIXlt" and needs to be converted to "POSIXct"

tid2 <- tid
for (i in 1:2) tid2[,i] <- time.conv(tid2[,i])

print(tid2)
str(tid2)
class(tid2[,1])

Med venlig hilsen
Frede Aakmann T?gersen
 

 

> -----Oprindelig meddelelse-----
> Fra: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] P? vegne af sj
> Sendt: 4. januar 2007 22:39
> Til: r-help at stat.math.ethz.ch
> Emne: [R] importing timestamp data into R
> 
> I have a set of timestamp data that I have in a text file 
> that I would like to import into R for analysis.
> The timestamps are formated as follows:
> 
> DT_1,DT_2
> [2006/08/10 21:12:14 ],[2006/08/10 21:54:00 ] [2006/08/10 
> 20:42:00 ],[2006/08/10 22:48:00 ] [2006/08/10 20:58:00 
> ],[2006/08/10 21:39:00 ]
> [2006/08/04 12:15:24 ],[2006/08/04 12:20:00 ]
> [2006/08/04 12:02:00 ],[2006/08/04 14:20:00 ]
> 
> I can get them into R but I cannot figure out how to convert 
> them into something R will recognize as a date/time. I have 
> tried using "as.Date", strptime, and chron. Any help would be 
> appreciated?
> 
> best,
> 
> Spencer
> 
> 
> 
> On 1/4/07, Darin A. England <england at cs.umn.edu> wrote:
> >
> >
> > Does anyone know a reason why, in principle, a call to randomForest 
> > cannot accept a data frame with missing predictor values? If each 
> > individual tree is built using CART, then it seems like 
> this should be 
> > possible. (I understand that one may impute missing values using 
> > rfImpute or some other method, but I would like to avoid doing
> > that.)
> >
> > If this functionality were available, then when the trees are being 
> > constructed and when subsequent data are put through the 
> forest, one 
> > would also specify an argument for the use of surrogate rules, just 
> > like in rpart.
> >
> > I realize this question is very specific to randomForest, 
> as opposed 
> > to R in general, but any comments are appreciated. I suppose I am 
> > looking for someone to say "It's not appropriate, and 
> here's why ..." 
> > or "Good idea. Please implement and post your code."
> >
> > Thanks,
> >
> > Darin England, Senior Scientist
> > Ingenix
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jim at bitwrit.com.au  Fri Jan  5 09:02:20 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Fri, 05 Jan 2007 19:02:20 +1100
Subject: [R] setting new working directories
In-Reply-To: <001001c7300e$7016b440$bb1ad284@BIO041>
References: <001001c7300e$7016b440$bb1ad284@BIO041>
Message-ID: <459E060C.4060800@bitwrit.com.au>

Bill Shipley wrote:
> Hello, and Happy New Year.  My default working directory is getting very
> cluttered.  I know that I should be using a different working directory for
> each project (I work in Windows), but do not know how to go about creating
> different ones and moving back and forth between them.  I have read Venables
> & Ripley (Modern Applied Statistics with S-PLUS, 1994) but this seems out of
> date with respect to this topic and have searched through the documentation
> but cannot find a clear explanation for doing this.  Can someone point me to
> the proper documentation for creating and using different working
> directories from within Windows (please, no comments about switching to
> UNIX...).
> Thanks.
> 
I'm not sure to which New Year you refer, but thanks and the same to you.

I think you may want to automatically start up R in a directory for each 
project. There is a discussion of that in Kickstarting R at:

http://cran.r-project.org/doc/contrib/Lemon-kickstart/kr_start.html

Basically, you can create icons on the desktop that will start R in a 
number of directories. In Windows, don't worry about adding the line to 
your "R" startup file, just set the "Start Program in" to the desired 
directory in the Properties dialog of the shortcut.

You can also set up an R file that gives you an interactive choice when 
the program starts. Create a file like this:

cat("R programming\n")
cat("Animals\n")
cat("Vegetables\n")
cat("Minerals\n")
cat("Type in the first letter of the project -")
answer<-toupper(strsplit(readline(""))[1])
if(answer == "R") setwd("c:/jim/R/programs")
if(answer == "A") setwd("c:/jim/things/animals")
...

and say you name it c:\jim\R\SelectProject.R
(notice that Windows uses backslashes but you use slashes in R)

put the following line in your .First function:

source("c:/jim/R/SelectProject.R")

and you should be able to select your project and directory on startup.

Jim


From daj025 at gmail.com  Fri Jan  5 04:38:34 2007
From: daj025 at gmail.com (David James)
Date: Thu, 4 Jan 2007 22:38:34 -0500
Subject: [R] [R-pkgs] RMySQL 0.5-11 uploaded to CRAN
Message-ID: <74c69e370701041938g50c2147fn3cfb767fe219487b@mail.gmail.com>

Hello,

I've uploaded  version 0.5-11 of RMySQL into CRAN, and it should be available
soon.

>From the NEWS file:

Version 0.5-11

* Fixed a bug that would crash R with a failed mysql_real_connect().

* dbApply() is now working again (but still experimentally).

* Re-formatted the C code.

[0.5-9 through 0.5-10 were maintanance releases that Seth Falcon
kindly put out.]

Regards,

--
David

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From parrinel at med.unibs.it  Fri Jan  5 09:21:50 2007
From: parrinel at med.unibs.it (Giovanni Parrinello)
Date: Fri, 05 Jan 2007 09:21:50 +0100
Subject: [R] Error in load
Message-ID: <459E0A9E.5000109@med.unibs.it>

Dear All,
can anyone explain this message
Error in load("matched2.RData") : Value of SET_STRING_ELT() must be a 
'CHARSXP' not a 'builtin'?
Have I the possibility to retrieve the data?
TIA
Giovanni

-- 
dr. Giovanni Parrinello
Department of Biotecnologies
Medical Statistics Unit
University of Brescia
Viale Europa, 11 25123 Brescia
email: parrinel at med.unibs.it
Phone: +390303717528
Fax: +390303717488


From jim at bitwrit.com.au  Fri Jan  5 09:25:51 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Fri, 05 Jan 2007 19:25:51 +1100
Subject: [R] color of opposite sign values in filled.contour
In-Reply-To: <56323.171.64.241.204.1167953415.squirrel@mail.rap.ucar.edu>
References: <56323.171.64.241.204.1167953415.squirrel@mail.rap.ucar.edu>
Message-ID: <459E0B8F.9000008@bitwrit.com.au>

Claudia Tebaldi wrote:
> Dear R-helpers,
> 
> I'm plotting geophysical data in the form of contours using
> "filled.contour". The display would be much more effective if the areas
> with negative values could be color coded
> by -- say -- "cold colors" in the range of blue to green, and conversely
> the areas with positive values got plotted with "warm colors", from yellow
> to red.
> Right now if I use a palette spanning the spectrum I need the entire range
> is associated with the actual range of the data, which can be positively
> or negatively skewed, and as a result the position of the zero is totally
> arbitrary.
> I'm wondering if someone out there has come up with a clever way to set
> the color scale accordingly, as a function of the actual range of the
> values in the matrix that is being plotted. Ideally, it would be neat to
> still use the entire spectrum, but sampling differently the cold and warm
> subsets accordingly to the extent of the negative and positive values in
> the data.
> 
> Also, when I try to play around in an ad hoc fashion with the palette I
> often get funny results in the legend, with color-scale wrapping or blank
> cells at one of the extremes. I cannot hack effectively the code of the
> filled.contour function, obviously...
> 
> 
Hi Claudia,

Have a look at color.scale in the plotrix package. You can specify quite 
a variety of different color ranges into which your numeric values will 
be transformed. If you want different color ranges for positive and 
negative values, you can calculate them separately. Here's an example 
using blue to green for negative values and yellow to red for positive:

testval<-c(-3,6,-1,8,0,-2,4,10,12)
testcol<-rep(0,length(testval))
testcol[testval<0]<-color.scale(testval[testval<0],0,c(0,1),c(1,0))
testcol[testval>=0]<-color.scale(testval[testval>=0],1,c(1,0),0)
plot(testval,col=testcol,pch=19)

You might also be interested in color.scale.lines

Jim


From ripley at stats.ox.ac.uk  Fri Jan  5 09:47:29 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Jan 2007 08:47:29 +0000 (GMT)
Subject: [R] Error in load
In-Reply-To: <459E0A9E.5000109@med.unibs.it>
References: <459E0A9E.5000109@med.unibs.it>
Message-ID: <Pine.LNX.4.64.0701050842490.30200@gannet.stats.ox.ac.uk>

You appear to have a corrupted character vector in your saved image.
Either R was internally corrupted when the image was saved or the file has 
been corrupted.  In neither case would the the data be intact.

On Fri, 5 Jan 2007, Giovanni Parrinello wrote:

> Dear All,
> can anyone explain this message
> Error in load("matched2.RData") : Value of SET_STRING_ELT() must be a
> 'CHARSXP' not a 'builtin'?
> Have I the possibility to retrieve the data?
> TIA
> Giovanni
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From b.jacobs at pandora.be  Fri Jan  5 09:54:17 2007
From: b.jacobs at pandora.be (Bert Jacobs)
Date: Fri, 5 Jan 2007 09:54:17 +0100
Subject: [R] Fast Removing Duplicates from Every Column
In-Reply-To: AAAAAOXvfE/tUWVMujI1J0cWacvEYCYA
Message-ID: <20070105085418.8440E124050@hoboe2bl1.telenet-ops.be>

Hi,

I'm looking for some lines of code that does the following:
I have a dataframe with 160 Columns and a number of rows (max 30):

		Col1 Col2 Col3 ... Col 159 Col 160 
Row 1 	0 	0 	LD ... 0	   VD 
Row 2 	HD 	0 	0 	 0 	   MD 
Row 3 	0 	HD 	HD 	 0       LD 
Row 4 	LD 	HD 	HD 	 0 	   LD 
...		...
LastRow	HD    HD    LD     0       MD


Now I want a dataframe that looks like this. As you see all duplicates are
removed. Can this dataframe be constructed in a fast way?

		Col1 Col2 Col3 ... Col 159 Col 160 
Row 1       0    0    LD       0	    VD
Row 2     	HD   HD   0        0        MD
Row 3     	LD   0    HD       0        LD

Thx for helping me out.
Bert


From g.comte at alliance-ir.net  Fri Jan  5 11:04:54 2007
From: g.comte at alliance-ir.net (COMTE Guillaume)
Date: Fri, 5 Jan 2007 11:04:54 +0100
Subject: [R] Problem with plot() and POSIXt dates
Message-ID: <15C100200A5F4E45AF8CFB45A926EF3423E0AB@allexch01.alliance.com>

 

 Hy all,

 

 I'm plotting graphs using plot() function, they are on X axes POSIX dates:

 "POSIXt"   "oldClass" "POSIXct"  "POSIXlt"

 I can't figure out why sometimes it prints the month and days and sometimes it prints the unix timestamp.

 It appens usually when the xlim is short like only some days.

 xlim is settled as a POSIXt like this

 "2006-12-30 17:25:44 CET" "2007-01-02 03:16:51 CET"

 On the graph it prints : 1167500000 and 1167700000 instead of dates.

 And the result gives a x axes in unix timestamps as if the plot function didn't recognize that it is a timestamp but just an integer.

 What am i missing, since R sees itself that it is time stamps and not integer when xlim is enougth large, how do i tell the plot function to see these numbers as POSIXt?

I put an attachement, inside there is an example that demonstrate what i experience (hope it won't be deleted, it is a tar.gz archive made using debian under

R version 2.1.0, 2005-04-18, i386-pc-linux-gnu

attached base packages:

[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"

[7] "base").

 

 thks.

COMTE Guillaume

Ing?nieur Projet

Alliance Technologies

Projet Philharmonie

24 rue Martre

92110 Clichy

Tel : 01 40 87 48 06

Fax : 01 40 87 48 14

 


From g.comte at alliance-ir.net  Fri Jan  5 11:28:37 2007
From: g.comte at alliance-ir.net (COMTE Guillaume)
Date: Fri, 5 Jan 2007 11:28:37 +0100
Subject: [R] =?iso-8859-1?q?the_Rscript_is_missing_in_my_pr=E9vious_mail?=
	=?iso-8859-1?q?=2E?=
Message-ID: <15C100200A5F4E45AF8CFB45A926EF3423E0AD@allexch01.alliance.com>

 

 

Sorry i forgot to give the Rscript to execute with the datas in the archive attached :

 

limite_x<-structure(as.numeric(read.table("limite_x")), class=c("POSIXt","POSIXct"))

limite_y<-as.numeric(read.table("limite_y"))

test<-as.numeric(read.table("y_values"))

abscisse_test<-structure(as.numeric(read.table("x_values")), class=c("POSIXt","POSIXct"))

plot((test/10)~abscisse_test,type="s",col="lightgreen",xlab="",ylab="",ylim=limite_y,xlim=limite_x)

length_test<-length(test)

#if i cut the var "test" to be closer to xlim values, i know that xlim is at the end of "test" :

test_short<-test[(length_test-10):length_test]

abscisse_test_short<-abscisse_test[(length_test-10):length_test]

plot((test_short/10)~abscisse_test_short,type="s",col="lightgreen",xlab="",ylab="",ylim=limite_y,xlim=limite_x)

dev.off()

 

COMTE Guillaume

Ing?nieur Projet

Alliance Technologies

Projet Philharmonie

24 rue Martre

92110 Clichy

Tel : 01 40 87 48 06

Fax : 01 40 87 48 14

 


From petr.pikal at precheza.cz  Fri Jan  5 11:34:58 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 05 Jan 2007 11:34:58 +0100
Subject: [R] ifelse on data frames
In-Reply-To: <3948d9e50701042338r5147dfa9kc5522e61f7f967e9@mail.gmail.com>
References: <2213.203.109.182.160.1167967080.squirrel@webmail.scms.waikato.ac.nz>
Message-ID: <459E37E2.22989.D674F0@localhost>

Hi

you could use also another approach in case of data frames

A <- as.data.frame(A)
A0 <- -A*log(A)
A0[is.na(A0)] <- 0

which changes NaN's to zeroes

HTH
Petr


On 5 Jan 2007 at 16:38, talepanda wrote:

Date sent:      	Fri, 5 Jan 2007 16:38:05 +0900
From:           	talepanda <talepanda at gmail.com>
To:             	maj at stats.waikato.ac.nz
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] ifelse on data frames

> It can be explained.
> 
> > class(A)
> [1] "data.frame"
> > length(A)
> [1] 5
> > class(A==0)
> [1] "matrix"
> > length(A==0)
> [1] 10
> > class(-A*log(A))
> [1] "data.frame"
> > length(-A*log(A))
> [1] 5
> 
> as you can see, the result of A==0 is matrix with length=10, while the
> result of -A*log(A) is still data.frame with length=5.
> 
> then, when calling ifelse( [length=10], 0, [length=5] ), internally,
> the NO(3rd) argument was repeated by rep(-A*log(A),length.out=10) (try
> this). the result is "list" with length=10 and each element has 2
> sub-elements.
> 
> So, the return value of A[(A==0)==FALSE] has 2 sub-elements as you
> get.
> 
> I think what confusing you is the behavior of A==0.
> 
> However, when using 'ifelse', I think you should use matrix as the
> arguments because data.frame is not consistent with the purpose of
> 'ifelse'.
> 
> On 1/5/07, maj at stats.waikato.ac.nz <maj at stats.waikato.ac.nz> wrote: >
> [Using R 2.2.0 on Windows XP; OK, OK, I will update soon!] > > I have
> noticed some undesirable behaviour when applying > ifelse to a data
> frame. Here is my code: > > A <- scan() >  1.000000 0.000000 0.000000 
> 0 0.00000 >  0.027702 0.972045 0.000253  0 0.00000 > > A <-
> matrix(A,nrow=2,ncol=5,byrow=T) > A == 0 > ifelse(A==0,0,-A*log(A)) >
> > A <- as.data.frame(A) > ifelse(A==0,0,-A*log(A)) > > and this is the
> output: > > > A <- scan() > 1:  1.000000 0.000000 0.000000  0 0.00000
> > 6:  0.027702 0.972045 0.000253  0 0.00000 > 11: > Read 10 items > >
> A <- matrix(A,nrow=2,ncol=5,byrow=T) > > A == 0 >       [,1]  [,2] 
> [,3] [,4] [,5] > [1,] FALSE  TRUE  TRUE TRUE TRUE > [2,] FALSE FALSE
> FALSE TRUE TRUE > > ifelse(A==0,0,-A*log(A)) >            [,1]      
> [,2]        [,3] [,4] [,5] > [1,] 0.00000000 0.00000000 0.000000000   
> 0    0 > [2,] 0.09934632 0.02756057 0.002095377    0    0 > > > > A <-
> as.data.frame(A) > > ifelse(A==0,0,-A*log(A)) > [[1]] > [1] 0.00000000
> 0.09934632 > > [[2]] > [1]        NaN 0.02756057 > > [[3]] > [1] 0 > >
> [[4]] > [1] NaN NaN > > [[5]] > [1] 0 > > [[6]] > [1] 0.00000000
> 0.09934632 > > [[7]] > [1] 0 > > [[8]] > [1] 0 > > [[9]] > [1] 0 > >
> [[10]] > [1] 0 > > > > > Is this a bug or a feature? Can the behaviour
> be explained? > > Regards,  Murray Jorgensen > -- > Dr Murray
> Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html >
> Department of Statistics, University of Waikato, Hamilton, New Zealand
> > Email: maj at waikato.ac.nz                                Fax 7 838
> 4155 > Phone  +64 7 838 4773 wk    Home +64 7 825 0441    Mobile 021
> 1395 862 > > ______________________________________________ >
> R-help at stat.math.ethz.ch mailing list >
> https://stat.ethz.ch/mailman/listinfo/r-help > PLEASE do read the
> posting guide http://www.R-project.org/posting-guide.html > and
> provide commented, minimal, self-contained, reproducible code. >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From I.Visser at uva.nl  Fri Jan  5 11:41:41 2007
From: I.Visser at uva.nl (Ingmar Visser)
Date: Fri, 05 Jan 2007 11:41:41 +0100
Subject: [R] Efficient multinom probs
Message-ID: <C1C3E9F5.C47A%I.Visser@uva.nl>

Dear R-helpers,

I need to compute probabilties of multinomial observations, eg by doing the
following:

y=sample(1:3,15,1)
prob=matrix(runif(45),15)
prob=prob/rowSums(prob)
diag(prob[,y])

However, my question is whether this is the most efficient way to do this.
In the call prob[,y] a whole matrix is computed which seems a bit of a
waste. 

Is there maybe a vectorized version of dmultinom which does this?

Best, Ingmar


From petr.pikal at precheza.cz  Fri Jan  5 11:51:14 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 05 Jan 2007 11:51:14 +0100
Subject: [R] Fast Removing Duplicates from Every Column
In-Reply-To: <20070105085418.8440E124050@hoboe2bl1.telenet-ops.be>
References: AAAAAOXvfE/tUWVMujI1J0cWacvEYCYA
Message-ID: <459E3BB2.3648.E558B5@localhost>

Hi

I am not sure if I understand how do you want to select unique items.

with
 sapply(DF, function(x) !duplicated(x))
you can get data frame with TRUE when an item in particular column is 
unique and FALSE in opposite. However then you need to choose which 
rows to keep or discard

e.g.

DF[rowSums(sapply(comp, function(x) !duplicated(x)))>1,]

selects all rows in which are 2 or more unique values.

HTH
Petr


On 5 Jan 2007 at 9:54, Bert Jacobs wrote:

From:           	"Bert Jacobs" <b.jacobs at pandora.be>
To:             	"'R help list'" <r-help at stat.math.ethz.ch>
Date sent:      	Fri, 5 Jan 2007 09:54:17 +0100
Subject:        	Re: [R] Fast Removing Duplicates from Every Column

> Hi,
> 
> I'm looking for some lines of code that does the following:
> I have a dataframe with 160 Columns and a number of rows (max 30):
> 
>   Col1 Col2 Col3 ... Col 159 Col 160 
> Row 1 	0 	0 	LD ... 0	   VD 
> Row 2 	HD 	0 	0 	 0 	   MD 
> Row 3 	0 	HD 	HD 	 0       LD 
> Row 4 	LD 	HD 	HD 	 0 	   LD 
> ...		...
> LastRow	HD    HD    LD     0       MD
> 
> 
> Now I want a dataframe that looks like this. As you see all duplicates
> are removed. Can this dataframe be constructed in a fast way?
> 
>   Col1 Col2 Col3 ... Col 159 Col 160 
> Row 1       0    0    LD       0	    VD
> Row 2     	HD   HD   0        0        MD
> Row 3     	LD   0    HD       0        LD
> 
> Thx for helping me out.
> Bert
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From dimitris.rizopoulos at med.kuleuven.be  Fri Jan  5 11:53:06 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 5 Jan 2007 11:53:06 +0100
Subject: [R] Efficient multinom probs
References: <C1C3E9F5.C47A%I.Visser@uva.nl>
Message-ID: <015701c730b7$ae826cc0$0540210a@www.domain>

maybe

prob[cbind(1:length(y), y)]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Ingmar Visser" <I.Visser at uva.nl>
To: <R-help at stat.math.ethz.ch>
Sent: Friday, January 05, 2007 11:41 AM
Subject: [R] Efficient multinom probs


> Dear R-helpers,
>
> I need to compute probabilties of multinomial observations, eg by 
> doing the
> following:
>
> y=sample(1:3,15,1)
> prob=matrix(runif(45),15)
> prob=prob/rowSums(prob)
> diag(prob[,y])
>
> However, my question is whether this is the most efficient way to do 
> this.
> In the call prob[,y] a whole matrix is computed which seems a bit of 
> a
> waste.
>
> Is there maybe a vectorized version of dmultinom which does this?
>
> Best, Ingmar
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From jgarcia at ija.csic.es  Fri Jan  5 12:06:00 2007
From: jgarcia at ija.csic.es (javier garcia-pintado)
Date: Fri, 05 Jan 2007 12:06:00 +0100
Subject: [R] gstat package. "singular" attibute
Message-ID: <459E3118.4050504@ija.csic.es>

Hello,
I'm using the gstat package within R for an automated procedure that
uses ordinary kriging.
I can see that there is a logical ("singular") atrtibute of some
adjusted model semivariograms:

.- attr(*, "singular")= logi TRUE

I cannot find documentation about the exact meaning and the implications
of this attribute, and I dont know anything about the inner calculations
of model semivariograms.

I guess that the inverse of some matrix need to be  calculated , and
this matrix is singular, but I also see that the model semivariogram is
calculated anyway.

Could you briefly tell me something about the significance of this
attribute and if I should not use these model semivariograms when the
"singular" attibute is true?

Thank you very much and best regards,

Javier


-- 
Javier Garc?a-Pintado
Institute of Earth Sciences Jaume Almera (CSIC)
Lluis Sole Sabaris s/n, 08028 Barcelona
Phone: +34 934095410
Fax:   +34 934110012
e-mail:jgarcia at ija.csic.es 


From ripley at stats.ox.ac.uk  Fri Jan  5 12:12:56 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Jan 2007 11:12:56 +0000 (GMT)
Subject: [R] Problem with plot() and POSIXt dates
In-Reply-To: <15C100200A5F4E45AF8CFB45A926EF3423E0AB@allexch01.alliance.com>
References: <15C100200A5F4E45AF8CFB45A926EF3423E0AB@allexch01.alliance.com>
Message-ID: <Pine.LNX.4.64.0701051107410.5225@gannet.stats.ox.ac.uk>

No attachment arrived: see the posting guide for handling attachments.
But in any case the posting guide says

   If you are using an old version of R and think it does not work
   properly, upgrade to the latest version and try that, before posting.

and your version of R is ancient.  Note, 'before posting' ....


On Fri, 5 Jan 2007, COMTE Guillaume wrote:

>
>
> Hy all,
>
>
>
> I'm plotting graphs using plot() function, they are on X axes POSIX dates:
>
> "POSIXt"   "oldClass" "POSIXct"  "POSIXlt"
>
> I can't figure out why sometimes it prints the month and days and sometimes it prints the unix timestamp.
>
> It appens usually when the xlim is short like only some days.
>
> xlim is settled as a POSIXt like this
>
> "2006-12-30 17:25:44 CET" "2007-01-02 03:16:51 CET"
>
> On the graph it prints : 1167500000 and 1167700000 instead of dates.
>
> And the result gives a x axes in unix timestamps as if the plot function didn't recognize that it is a timestamp but just an integer.
>
> What am i missing, since R sees itself that it is time stamps and not integer when xlim is enougth large, how do i tell the plot function to see these numbers as POSIXt?
>
> I put an attachement, inside there is an example that demonstrate what i experience (hope it won't be deleted, it is a tar.gz archive made using debian under
>
> R version 2.1.0, 2005-04-18, i386-pc-linux-gnu
>
> attached base packages:
>
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
>
> [7] "base").
>
>
>
> thks.
>
> COMTE Guillaume
>
> Ing?nieur Projet
>
> Alliance Technologies
>
> Projet Philharmonie
>
> 24 rue Martre
>
> 92110 Clichy
>
> Tel : 01 40 87 48 06
>
> Fax : 01 40 87 48 14
>
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From Achim.Zeileis at wu-wien.ac.at  Fri Jan  5 12:16:22 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 5 Jan 2007 12:16:22 +0100 (CET)
Subject: [R] color of opposite sign values in filled.contour
In-Reply-To: <20070104223906.BRJ59189@po-d.temple.edu>
References: <20070104223906.BRJ59189@po-d.temple.edu>
Message-ID: <Pine.LNX.4.64.0701051213440.6295@eowyn>

On Thu, 4 Jan 2007, Richard M. Heiberger wrote:

> Get the RColorBrewer package from CRAN
>
> Description: The packages provides palettes for drawing nice maps
>        shaded according to a variable.

The package "vcd" also offers the function diverge_hcl() that constructs 
diverging palettes (based on HCL colors) particularly aimed at 
filled.contour() or image() plots. See
   http://epub.wu-wien.ac.at/dyn/openURL?id=oai:epub.wu-wien.ac.at:epub-wu-01_abd
for some more background information.
Z


From e.pebesma at geo.uu.nl  Fri Jan  5 13:12:08 2007
From: e.pebesma at geo.uu.nl (Edzer J. Pebesma)
Date: Fri, 05 Jan 2007 13:12:08 +0100
Subject: [R] gstat package. "singular" attibute
In-Reply-To: <459E3118.4050504@ija.csic.es>
References: <459E3118.4050504@ija.csic.es>
Message-ID: <459E4098.4080409@geo.uu.nl>

Javier, consider two examples. First:

 > library(gstat)
Loading required package: sp
 > data(meuse)
 > coordinates(meuse)=~x+y
 > variogram(log(zinc)~1,meuse,width=100,cutoff=200)
   np      dist     gamma dir.hor dir.ver   id
1  52  77.01898 0.1299659       0       0 var1
2 263 156.23373 0.2091154       0       0 var1
 > v = variogram(log(zinc)~1,meuse,width=100,cutoff=200)
 > vm = fit.variogram(v, vgm(1, "Exp", 100, 1))
Warning: singular model in variogram fit
 > attr(vm, "singular")
[1] TRUE

Here I try to fit a three-parameter model to two data (semivariance) 
points. Can't be done, infinite number of solutions, indicated by the 
singularity flag. Second example: bad initial value for range:

 > v = variogram(log(zinc)~1,meuse,width=100,cutoff=1000)
 > vm = fit.variogram(v, vgm(1, "Sph", 10, 1))
Warning: singular model in variogram fit
 > attr(vm, "singular")
[1] TRUE

Starting with a range of 10, any combination of nugget and partial sill 
that fit the total sill improve the fit equally, indicated by the 
singularity. A larger value of the range (try 800) will lead to a good, 
non-singular fit.

fit.variogram does usually a non-linear regression, so any problem in 
that area is potentially present. You may want to consider fixing 
certain parameters to avoid certain problems; look at the fit.sills and 
fit.ranges arguments of fit.variogram.

In some cases, a singular model does fit the sample variogram nicely, 
e.g. where you use spherical or exponential models to effectively fit a 
linear semivariogram model: two parameters can be identified (nugget, 
slope) but three are fitted. The problem is to tell such a case from the 
two above, without looking at plots (i.e., automatically).
--
Edzer

javier garcia-pintado wrote:
> Hello,
> I'm using the gstat package within R for an automated procedure that
> uses ordinary kriging.
> I can see that there is a logical ("singular") atrtibute of some
> adjusted model semivariograms:
>
> .- attr(*, "singular")= logi TRUE
>
> I cannot find documentation about the exact meaning and the implications
> of this attribute, and I dont know anything about the inner calculations
> of model semivariograms.
>
> I guess that the inverse of some matrix need to be  calculated , and
> this matrix is singular, but I also see that the model semivariogram is
> calculated anyway.
>
> Could you briefly tell me something about the significance of this
> attribute and if I should not use these model semivariograms when the
> "singular" attibute is true?
>
> Thank you very much and best regards,
>
> Javier
>
>
>


From francogrex at mail.com  Fri Jan  5 13:54:50 2007
From: francogrex at mail.com (francogrex)
Date: Fri, 5 Jan 2007 04:54:50 -0800 (PST)
Subject: [R] maximum likelihood estimation of 5 parameters
Message-ID: <8177473.post@talk.nabble.com>


Hi Guys, it would be great if you could help me with a MLE problem in R.

I am trying to evaluate  the maximum likelihood estimates of theta = (a1,
b1, a2, b2, P) which defines a mixture of a Poisson distribution and two
gamma prior distributions (where the Poisson means have a gamma
distribution, actually 2 gammas and P is the mixing factor). The likelihood
function for theta is L(theta) = Pi,j{P f(Nij; a1, b1, Eij) + (1 ? P) f(Nij;
a2, b2, Eij),} 
The maximum likelihood estimate of theta is the vector that maximizes the
above equation (the values of N and E are given). The authors of the article
I read say that the maximization involves an iterative search in the five
dimensional parameter space, where each iteration involves computing
log[L(theta)] and its first and second-order derivatives. In test runs it is
suggested that the maximization typically takes between 5 and 15 iterations
from the starting point theta = (a1 = 0.2, b1 = 0.1, a2 = 2, b2 = 4, P =
1/3). 

Now I have done maximization of a gamma-poisson mixture before (1 poisson, 1
gamma) successfully and I could determine correctly alpha (a) and beta(a).
But this one above is giving me ridiculously large unusable values (for
example P should not be above 1 and sometimes I get values of 500!) or even
negative values! I know the values I should be obtaining with my samples
shouldn't be far from the staring points. Is there a way to help me solve
this issue? Thanks.
-- 
View this message in context: http://www.nabble.com/maximum-likelihood-estimation-of-5-parameters-tf2925364.html#a8177473
Sent from the R help mailing list archive at Nabble.com.


From g.comte at alliance-ir.net  Fri Jan  5 14:48:45 2007
From: g.comte at alliance-ir.net (COMTE Guillaume)
Date: Fri, 5 Jan 2007 14:48:45 +0100
Subject: [R] Problem with plot() and POSIXt dates
Message-ID: <15C100200A5F4E45AF8CFB45A926EF3423E0B3@allexch01.alliance.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070105/28fac9f6/attachment.pl 

From I.Visser at uva.nl  Fri Jan  5 14:57:45 2007
From: I.Visser at uva.nl (Ingmar Visser)
Date: Fri, 05 Jan 2007 14:57:45 +0100
Subject: [R] maximum likelihood estimation of 5 parameters
In-Reply-To: <8177473.post@talk.nabble.com>
Message-ID: <C1C417E9.C48C%I.Visser@uva.nl>

Franco,
You can provide lower and upper bounds on the parameters if you use optim
with method="L-BFGS-B".
Hth, Ingmar


> From: francogrex <francogrex at mail.com>
> Date: Fri, 5 Jan 2007 04:54:50 -0800 (PST)
> To: <r-help at stat.math.ethz.ch>
> Subject: [R] maximum likelihood estimation of 5 parameters
> 
> 
Hi Guys, it would be great if you could help me with a MLE problem in R.

I
> am trying to evaluate  the maximum likelihood estimates of theta = (a1,
b1,
> a2, b2, P) which defines a mixture of a Poisson distribution and two
gamma
> prior distributions (where the Poisson means have a gamma
distribution,
> actually 2 gammas and P is the mixing factor). The likelihood
function for
> theta is L(theta) = Pi,j{P f(Nij; a1, b1, Eij) + (1 ? P) f(Nij;
a2, b2, Eij),}
> 
The maximum likelihood estimate of theta is the vector that maximizes
> the
above equation (the values of N and E are given). The authors of the
> article
I read say that the maximization involves an iterative search in the
> five
dimensional parameter space, where each iteration involves
> computing
log[L(theta)] and its first and second-order derivatives. In test
> runs it is
suggested that the maximization typically takes between 5 and 15
> iterations
from the starting point theta = (a1 = 0.2, b1 = 0.1, a2 = 2, b2 =
> 4, P =
1/3). 

Now I have done maximization of a gamma-poisson mixture before
> (1 poisson, 1
gamma) successfully and I could determine correctly alpha (a)
> and beta(a).
But this one above is giving me ridiculously large unusable
> values (for
example P should not be above 1 and sometimes I get values of
> 500!) or even
negative values! I know the values I should be obtaining with my
> samples
shouldn't be far from the staring points. Is there a way to help me
> solve
this issue? Thanks.
-- 
View this message in context:
> http://www.nabble.com/maximum-likelihood-estimation-of-5-parameters-tf2925364.
> html#a8177473
Sent from the R help mailing list archive at
> Nabble.com.

______________________________________________
R-help at stat.math.e
> thz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do
> read the posting guide http://www.R-project.org/posting-guide.html
and provide
> commented, minimal, self-contained, reproducible code.


From dieter.menne at menne-biomed.de  Fri Jan  5 15:01:46 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 5 Jan 2007 14:01:46 +0000 (UTC)
Subject: [R] coefficients of each local polynomial from loess() or
	locfit()
References: <08BFEF2D7CC3104FA411B6E1991200C21D85C2@NIHCESMLBX3.nih.gov>
Message-ID: <loom.20070105T145441-714@post.gmane.org>

Liu, Delong (NIH/CIT) [C] <liud2 <at> mail.nih.gov> writes:

>>> 
I want to extract estimated coeffiicents of each local polynomial at 
given x from loess(),  locfit(), or KernSmooth().  Can some experts 
provide me with suggestions?  Thanks.
>>

Try 

cars.lo <- loess(dist ~ speed, cars)
str(cars.lo)

List of 17
 $ n        : int 50
 $ fitted   : num [1:50]  5.89  5.89 12.57 12.57 15.37 ...
 $ residuals: Named num [1:50] -3.894  4.106 -8.568  9.432  0.631 ...
... omitted
  ..$ cell       : num 0.2
  ..$ family     : chr "gaussian"
  ..$ iterations : num 1
 $ kd       :List of 5
  ..$ parameter: Named int [1:7] 1 50 2 19 11 1049 849
  .. ..- attr(*, "names")= chr [1:7] "d" "n" "vc" "nc" ...
  ..$ a        : int [1:19] 1 1 1 1 1 1 1 0 0 0 ...
  ..$ xi       : num [1:19] 15 12 19 9 13 17 20 0 0 0 ...
  ..$ vert     : num [1:2]  3.90 25.11
  ..$ vval     : num [1:22]  5.71  1.72 96.46 10.88 41.21 ...
 $ call     : language loess(formula = dist ~ speed, data = cars)

Looks like kd holds information about the polynomials. Then, try

getAnywhere(predict.loess)

which will show you that the real work is done in function predLoess. 
Trying again

getAnywhere(predLoess)

you get an idea how the parameters are used for prediction.

fit[inside] <- .C(R_loess_ifit, as.integer(kd$parameter), 
     as.integer(kd$a), as.double(kd$xi), as.double(kd$vert), 
        as.double(kd$vval), as.integer(M1), as.double(x.evaluate[inside, 
      ]), fit = double(M1))$fit

Dieter


From tebaldi at rap.ucar.edu  Fri Jan  5 15:08:07 2007
From: tebaldi at rap.ucar.edu (Claudia Tebaldi)
Date: Fri, 5 Jan 2007 06:08:07 -0800 (PST)
Subject: [R] color of opposite sign values in filled.contour
Message-ID: <57176.171.66.157.249.1168006087.squirrel@mail.rap.ucar.edu>


Dear all,

especially those of you that kindly provided suggestions yesterday,
I was not asking for cool palettes -- even if I now appreciate the
pointers -- but I was asking for a way to make the 0 level of a filled
contour plot correspond to the neutral color in the color scale
when the range of my values is not symmetrical around zero...without
hacking into the filled.contour function (which I'm not able to do
succesfully). I couldn't get the ggplot package suggested because I'm not
running MAC OSX 10.4, unfortunately...all the other suggestions didn't
provide a way out...at least that I could recognize.

Thanks again -- last time I bother you I promise!


claudia tebaldi











-- 
Claudia Tebaldi
ISSE/CGD/IMAGe
http://www.image.ucar.edu/~tebaldi

currently visiting
Center for Environmental Science and Policy
Stanford University
tel:   (650) 724-9261
skype: claudia.tebaldi



-- 
Claudia Tebaldi
ISSE/CGD/IMAGe
http://www.image.ucar.edu/~tebaldi

currently visiting
Center for Environmental Science and Policy
Stanford University
tel:   (650) 724-9261
skype: claudia.tebaldi


From dieter.menne at menne-biomed.de  Fri Jan  5 15:05:03 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 5 Jan 2007 14:05:03 +0000 (UTC)
Subject: [R] coefficients of each local polynomial from loess() or
	locfit()
References: <08BFEF2D7CC3104FA411B6E1991200C21D85C2@NIHCESMLBX3.nih.gov>
Message-ID: <loom.20070105T150315-153@post.gmane.org>

Liu, Delong (NIH/CIT) [C] <liud2 <at> mail.nih.gov> writes:

> 
> I want to extract estimated coeffiicents of each local polynomial at given x
from loess(),  locfit(), or
> KernSmooth().  Can some experts provide me with suggestions?  Thanks.

Before you start on your own, also note Brian Ripleys recent posting on

http://article.gmane.org/gmane.comp.lang.r.general/76625


Dieter


From ggrothendieck at gmail.com  Fri Jan  5 15:13:28 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 5 Jan 2007 09:13:28 -0500
Subject: [R] Problem with plot() and POSIXt dates
In-Reply-To: <15C100200A5F4E45AF8CFB45A926EF3423E0B3@allexch01.alliance.com>
References: <15C100200A5F4E45AF8CFB45A926EF3423E0B3@allexch01.alliance.com>
Message-ID: <971536df0701050613h5bb0a6bfp98f1f43eaf64cd3b@mail.gmail.com>

We can force the X axis into the format of our choice by
suppressing it in the plot and explicitly calling
axis.POSIXct with our choice of format
(see ?axis.POSIXct):

x <- seq(from = as.POSIXct("2005-10-14"),
       to = as.POSIXct("2007-01-02"),
       by = "day"
y <- as.numeric(x)

plot(y ~ x, xaxt = "n")
at <- x[seq(1, length(x), length = 5)]
axis.POSIXct(1, at, at, "%y-%b-%d")

Actually your data appear to be dates rather than date times
so I would use Date class rather than POSIXct.  See the Help
Desk article on dates and times in R News 4/1.

[Try making your examples smaller and more self contained
such as shown above.]

On 1/5/07, COMTE Guillaume <g.comte at alliance-ir.net> wrote:
> So maybe i will finaly succeed asking the right way,
>
>
>
> Hy all,
>
> I'm plotting graphs using plot() function, they are on X axes POSIX dates:
>
> "POSIXt"   "oldClass" "POSIXct"  "POSIXlt"
>
> I can't figure out why sometimes it prints the month and days and sometimes it prints the unix timestamps.
>
> Here is an example that reproduce my problem the data's to use with it can be downloaded from http://www.alliance-ir.net/r-project/demo.tar.gz <http://www.alliance-ir.net/r-project/demo.tar.gz>  :
>
> limite_x<-structure(as.numeric(read.table("limite_x")), class=c("POSIXt","POSIXct"))
>
> limite_y<-as.numeric(read.table("limite_y"))
>
> test<-as.numeric(read.table("y_values"))
>
> abscisse_test<-structure(as.numeric(read.table("x_values")), class=c("POSIXt","POSIXct"))
>
> plot((test/10)~abscisse_test,type="s",col="lightgreen",xlab="",ylab="",ylim=limite_y,xlim=limite_x)
>
> length_test<-length(test)
>
> #if i cut the var "test" to be closer to xlim values, i know that xlim is at the end of "test" :
>
> test_short<-test[(length_test-10):length_test]
>
> abscisse_test_short<-abscisse_test[(length_test-10):length_test]
>
> plot((test_short/10)~abscisse_test_short,type="s",col="lightgreen",xlab="",ylab="",ylim=limite_y,xlim=limite_x)
>
> dev.off()
>
> > sessionInfo()
>
> R version 2.4.0 (2006-10-03)
>
> i386-pc-linux-gnu
>
>
>
> locale:
>
> LC_CTYPE=fr_FR at euro;LC_NUMERIC=C;LC_TIME=fr_FR at euro;LC_COLLATE=fr_FR at euro;LC_MONETARY=fr_FR at euro;LC_MESSAGES=fr_FR at euro;LC_PAPER=fr_FR at euro;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=fr_FR at euro;LC_IDENTIFICATION=C
>
>
>
> attached base packages:
>
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
>
> [7] "base"
>
>
>
> COMTE Guillaume
>
> Ing?nieur Projet
>
> Alliance Technologies
>
> Projet Philharmonie
>
> 24 rue Martre
>
> 92110 Clichy
>
> Tel : 01 40 87 48 06
>
> Fax : 01 40 87 48 14
>
>
>
>
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From ggrothendieck at gmail.com  Fri Jan  5 15:44:01 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 5 Jan 2007 09:44:01 -0500
Subject: [R] Problem with plot() and POSIXt dates
In-Reply-To: <971536df0701050613h5bb0a6bfp98f1f43eaf64cd3b@mail.gmail.com>
References: <15C100200A5F4E45AF8CFB45A926EF3423E0B3@allexch01.alliance.com>
	<971536df0701050613h5bb0a6bfp98f1f43eaf64cd3b@mail.gmail.com>
Message-ID: <971536df0701050644l32e1943m68e52aafbf075a68@mail.gmail.com>

Sorry, there was a missing parenthesis after the first statement.
Here is the code again:

x <- seq(from = as.POSIXct("2005-10-14"),
      to = as.POSIXct("2007-01-02"),
      by = "day")
y <- as.numeric(x)

plot(y ~ x, xaxt = "n")
at <- x[seq(1, length(x), length = 5)]
axis.POSIXct(1, at, at, "%y-%b-%d")

On 1/5/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> We can force the X axis into the format of our choice by
> suppressing it in the plot and explicitly calling
> axis.POSIXct with our choice of format
> (see ?axis.POSIXct):
>
> x <- seq(from = as.POSIXct("2005-10-14"),
>       to = as.POSIXct("2007-01-02"),
>       by = "day"
> y <- as.numeric(x)
>
> plot(y ~ x, xaxt = "n")
> at <- x[seq(1, length(x), length = 5)]
> axis.POSIXct(1, at, at, "%y-%b-%d")
>
> Actually your data appear to be dates rather than date times
> so I would use Date class rather than POSIXct.  See the Help
> Desk article on dates and times in R News 4/1.
>
> [Try making your examples smaller and more self contained
> such as shown above.]
>
> On 1/5/07, COMTE Guillaume <g.comte at alliance-ir.net> wrote:
> > So maybe i will finaly succeed asking the right way,
> >
> >
> >
> > Hy all,
> >
> > I'm plotting graphs using plot() function, they are on X axes POSIX dates:
> >
> > "POSIXt"   "oldClass" "POSIXct"  "POSIXlt"
> >
> > I can't figure out why sometimes it prints the month and days and sometimes it prints the unix timestamps.
> >
> > Here is an example that reproduce my problem the data's to use with it can be downloaded from http://www.alliance-ir.net/r-project/demo.tar.gz <http://www.alliance-ir.net/r-project/demo.tar.gz>  :
> >
> > limite_x<-structure(as.numeric(read.table("limite_x")), class=c("POSIXt","POSIXct"))
> >
> > limite_y<-as.numeric(read.table("limite_y"))
> >
> > test<-as.numeric(read.table("y_values"))
> >
> > abscisse_test<-structure(as.numeric(read.table("x_values")), class=c("POSIXt","POSIXct"))
> >
> > plot((test/10)~abscisse_test,type="s",col="lightgreen",xlab="",ylab="",ylim=limite_y,xlim=limite_x)
> >
> > length_test<-length(test)
> >
> > #if i cut the var "test" to be closer to xlim values, i know that xlim is at the end of "test" :
> >
> > test_short<-test[(length_test-10):length_test]
> >
> > abscisse_test_short<-abscisse_test[(length_test-10):length_test]
> >
> > plot((test_short/10)~abscisse_test_short,type="s",col="lightgreen",xlab="",ylab="",ylim=limite_y,xlim=limite_x)
> >
> > dev.off()
> >
> > > sessionInfo()
> >
> > R version 2.4.0 (2006-10-03)
> >
> > i386-pc-linux-gnu
> >
> >
> >
> > locale:
> >
> > LC_CTYPE=fr_FR at euro;LC_NUMERIC=C;LC_TIME=fr_FR at euro;LC_COLLATE=fr_FR at euro;LC_MONETARY=fr_FR at euro;LC_MESSAGES=fr_FR at euro;LC_PAPER=fr_FR at euro;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=fr_FR at euro;LC_IDENTIFICATION=C
> >
> >
> >
> > attached base packages:
> >
> > [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> >
> > [7] "base"
> >
> >
> >
> > COMTE Guillaume
> >
> > Ing?nieur Projet
> >
> > Alliance Technologies
> >
> > Projet Philharmonie
> >
> > 24 rue Martre
> >
> > 92110 Clichy
> >
> > Tel : 01 40 87 48 06
> >
> > Fax : 01 40 87 48 14
> >
> >
> >
> >
> >        [[alternative HTML version deleted]]
> >
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
>


From murdoch at stats.uwo.ca  Fri Jan  5 15:46:00 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 05 Jan 2007 09:46:00 -0500
Subject: [R] color of opposite sign values in filled.contour
In-Reply-To: <57176.171.66.157.249.1168006087.squirrel@mail.rap.ucar.edu>
References: <57176.171.66.157.249.1168006087.squirrel@mail.rap.ucar.edu>
Message-ID: <459E64A8.9020103@stats.uwo.ca>

On 1/5/2007 9:08 AM, Claudia Tebaldi wrote:
> Dear all,
> 
> especially those of you that kindly provided suggestions yesterday,
> I was not asking for cool palettes -- even if I now appreciate the
> pointers -- but I was asking for a way to make the 0 level of a filled
> contour plot correspond to the neutral color in the color scale
> when the range of my values is not symmetrical around zero...without
> hacking into the filled.contour function (which I'm not able to do
> succesfully). I couldn't get the ggplot package suggested because I'm not
> running MAC OSX 10.4, unfortunately...all the other suggestions didn't
> provide a way out...at least that I could recognize.
> 
> Thanks again -- last time I bother you I promise!

The easiest way would be to use the "levels" parameter to create 
breakpoints which aren't linear in the original scale.  For example,

x <- y <- seq(-10,10,len=20)
z <- outer(x, y, function(x,y) x^2+y^2)
filled.contour(x,y,z, levels=seq(0,15)^2)

If you don't want to transform the spacing between the colors, you just 
want to change the sequence you get, then you should write your own 
palette function.  For example,

mypalette <- function(n) {
   n1 <- n %/% 3
   n2 <- n - n1 + 1
   c( colorRampPalette(c("red", "white"))(n1),
      colorRampPalette(c("white", "blue"))(n2)[-1] )
}

filled.contour(x,y,z, color=mypalette)

Duncan Murdoch


From cressonim at nhlbi.nih.gov  Fri Jan  5 16:05:39 2007
From: cressonim at nhlbi.nih.gov (cressonim)
Date: Fri, 05 Jan 2007 10:05:39 -0500
Subject: [R] aov - glm - lmer
Message-ID: <1168009540.16658.0.camel@PCCMB-01492535.nhlbi.nih.gov>

I am sorry to ask a trivial question but I am not a statistician.
When I need to compare more than two groups in a unbalanced design with
SAS system I use PROC GLM (like the example in data5.csv from Cody R.
"Applied statistics and SAS programming language" p.223). R glm gives
different results.

Thanks

Massimo Cressoni
   

From stevenmh at muohio.edu  Fri Jan  5 16:34:07 2007
From: stevenmh at muohio.edu (Martin Henry H. Stevens)
Date: Fri, 5 Jan 2007 10:34:07 -0500
Subject: [R] aov - glm - lmer
In-Reply-To: <1168009540.16658.0.camel@PCCMB-01492535.nhlbi.nih.gov>
References: <1168009540.16658.0.camel@PCCMB-01492535.nhlbi.nih.gov>
Message-ID: <E1A5261B-CB53-4B2B-B1D5-1F2FD910A40A@muohio.edu>

In R,
-- lm fits ordinary least squares linear  models; this is likely to  
be what you want if you were using PROC GLM.
-- glm fits generalized linear models (e.g. logit, Poisson, Gaussian,  
etc.).
-- lmer and lme fit mixed models, similar to PROC MIXED
Cheers,
Hank
On Jan 5, 2007, at 10:05 AM, cressonim wrote:

> I am sorry to ask a trivial question but I am not a statistician.
> When I need to compare more than two groups in a unbalanced design  
> with
> SAS system I use PROC GLM (like the example in data5.csv from Cody R.
> "Applied statistics and SAS programming language" p.223). R glm gives
> different results.
>
> Thanks
>
> Massimo Cressoni
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.



Dr. Martin Henry H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"

If you send an attachment, please try to send it in a format anyone  
can read, such as PDF, text, Open Document Format, HTML, or RTF.  
Please try not to send me MS Word or PowerPoint attachments-
Why? See:  http://www.gnu.org/philosophy/no-word-attachments.html


From francogrex at mail.com  Fri Jan  5 16:42:00 2007
From: francogrex at mail.com (francogrex)
Date: Fri, 5 Jan 2007 07:42:00 -0800 (PST)
Subject: [R] maximum likelihood estimation of 5 parameters
In-Reply-To: <C1C417E9.C48C%I.Visser@uva.nl>
References: <8177473.post@talk.nabble.com> <C1C417E9.C48C%I.Visser@uva.nl>
Message-ID: <8180120.post@talk.nabble.com>



Franco,
You can provide lower and upper bounds on the parameters if you use optim
with method="L-BFGS-B".
Hth, Ingmar

Thanks, but when I use L-BFGS-B it tells me that there is an  error in
optim(start, f, method = method, hessian = TRUE, ...) : L-BFGS-B needs
finite values of 'fn'

-- 
View this message in context: http://www.nabble.com/maximum-likelihood-estimation-of-5-parameters-tf2925364.html#a8180120
Sent from the R help mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Fri Jan  5 16:53:42 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Jan 2007 15:53:42 +0000 (GMT)
Subject: [R] maximum likelihood estimation of 5 parameters
In-Reply-To: <8180120.post@talk.nabble.com>
References: <8177473.post@talk.nabble.com> <C1C417E9.C48C%I.Visser@uva.nl>
	<8180120.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.64.0701051549090.2494@gannet.stats.ox.ac.uk>

On Fri, 5 Jan 2007, francogrex wrote:

[quoting Ingmar Vissar without attribution, contrary to the posting 
guide.]

> Franco,
> You can provide lower and upper bounds on the parameters if you use optim
> with method="L-BFGS-B".
> Hth, Ingmar
>
> Thanks, but when I use L-BFGS-B it tells me that there is an  error in
> optim(start, f, method = method, hessian = TRUE, ...) : L-BFGS-B needs
> finite values of 'fn'

It sounds as if you have ignored the advice to scale the problem via 
control options 'fnscale' and 'parscale'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rvaradhan at jhmi.edu  Fri Jan  5 17:42:59 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Fri, 5 Jan 2007 11:42:59 -0500
Subject: [R] maximum likelihood estimation of 5 parameters
In-Reply-To: <8180120.post@talk.nabble.com>
Message-ID: <000801c730e8$8eeb26f0$7c94100a@win.ad.jhu.edu>

Franco,
Is it possible that you have failed to provide the negative of loglikelihood
to "optim", since optim, by default, minimizes a function?  If you want to
do this withput redefining the log-likelihood, you should set fnscale= -1
(as hinted by Prof. Ripley).  This would turn the problem into a
maximization problem.  

If this doesn't work, you should provide more details (a reproducible code
with actual error message).

Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of francogrex
Sent: Friday, January 05, 2007 10:42 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] maximum likelihood estimation of 5 parameters



Franco,
You can provide lower and upper bounds on the parameters if you use optim
with method="L-BFGS-B".
Hth, Ingmar

Thanks, but when I use L-BFGS-B it tells me that there is an  error in
optim(start, f, method = method, hessian = TRUE, ...) : L-BFGS-B needs
finite values of 'fn'

-- 
View this message in context:
http://www.nabble.com/maximum-likelihood-estimation-of-5-parameters-tf292536
4.html#a8180120
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From auxsvr at yahoo.com  Fri Jan  5 18:12:01 2007
From: auxsvr at yahoo.com (MrJ Man)
Date: Fri, 5 Jan 2007 09:12:01 -0800 (PST)
Subject: [R] ifelse on data frames
Message-ID: <20070105171201.36548.qmail@web56404.mail.re3.yahoo.com>

On Friday 05 January 2007 12:34, Petr Pikal wrote:
> Hi
>
> you could use also another approach in case of data
frames
>
> A <- as.data.frame(A)
> A0 <- -A*log(A)
> A0[is.na(A0)] <- 0
I think you meant A0[which(is.na(A0))] <- 0
>
> which changes NaN's to zeroes
>
> HTH
> Petr
Regards


From rdiaz at cnio.es  Fri Jan  5 19:02:07 2007
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Fri, 5 Jan 2007 19:02:07 +0100
Subject: [R] eval(parse(text vs. get when accessing a function
Message-ID: <200701051902.07294.rdiaz@cnio.es>

Dear All,

I've read Thomas Lumley's fortune "If the answer is parse() you should usually 
rethink the question.". But I am not sure it that also applies (and why) to 
other situations (Lumley's comment 
http://tolstoy.newcastle.edu.au/R/help/05/02/12204.html
was in reply to accessing a list).

Suppose I have similarly called functions, except for a postfix. E.g.

f.1 <- function(x) {x + 1}
f.2 <- function(x) {x + 2}

And sometimes I want to call f.1 and some other times f.2 inside another 
function. I can either do:

g <- function(x, fpost) {
    calledf <- eval(parse(text = paste("f.", fpost, sep = "")))
    calledf(x)
    ## do more stuff
}


Or:

h <- function(x, fpost) {
    calledf <- get(paste("f.", fpost, sep = ""))
    calledf(x)
    ## do more stuff
}


Two questions:
1) Why is the second better? 

2) By changing g or h I could use "do.call" instead; why would that be better? 
Because I can handle differences in argument lists?



Thanks,


R.



-- 
Ram?n D?az-Uriarte
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}


From rolf at math.unb.ca  Fri Jan  5 19:13:57 2007
From: rolf at math.unb.ca (rolf at math.unb.ca)
Date: Fri, 5 Jan 2007 14:13:57 -0400 (AST)
Subject: [R] ifelse on data frames
Message-ID: <200701051813.l05IDv7V027430@weisner.math.unb.ca>

``MrJ Man'' wrote: 

> On Friday 05 January 2007 12:34, Petr Pikal wrote:
> > Hi
> >
> > you could use also another approach in case of data
> frames
> >
> > A <- as.data.frame(A)
> > A0 <- -A*log(A)
> > A0[is.na(A0)] <- 0
> I think you meant A0[which(is.na(A0))] <- 0

	He most certainly DOES NOT mean this!

	You should try things out before offering gratuitous
	advice.

	(a) A0[is.na(A0)] <- 0
		works perfectly.

	(b) A0[which(is.na(A0))] <- 0
		gets it wrong!!!

	I would have thought that

	A0[which(is.na(A0),arr.ind=TRUE)] <- 0

	would work and get it right, but it gives the
	error message

		Error in `[<-.data.frame`(`*tmp*`, which(is.na(A0),
                   	   arr.ind = TRUE), value = 0) : 
        	only logical matrix subscripts are allowed in replacement
> >
> > which changes NaN's to zeroes
> >
> > HTH
> > Petr

				cheers,

					Rolf Turner
					rolf at math.unb.ca


From P.Dalgaard at biostat.ku.dk  Fri Jan  5 19:21:34 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 05 Jan 2007 19:21:34 +0100
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <200701051902.07294.rdiaz@cnio.es>
References: <200701051902.07294.rdiaz@cnio.es>
Message-ID: <459E972E.6070807@biostat.ku.dk>

Ramon Diaz-Uriarte wrote:
> Dear All,
>
> I've read Thomas Lumley's fortune "If the answer is parse() you should usually 
> rethink the question.". But I am not sure it that also applies (and why) to 
> other situations (Lumley's comment 
> http://tolstoy.newcastle.edu.au/R/help/05/02/12204.html
> was in reply to accessing a list).
>
> Suppose I have similarly called functions, except for a postfix. E.g.
>
> f.1 <- function(x) {x + 1}
> f.2 <- function(x) {x + 2}
>
> And sometimes I want to call f.1 and some other times f.2 inside another 
> function. I can either do:
>
> g <- function(x, fpost) {
>     calledf <- eval(parse(text = paste("f.", fpost, sep = "")))
>     calledf(x)
>     ## do more stuff
> }
>
>
> Or:
>
> h <- function(x, fpost) {
>     calledf <- get(paste("f.", fpost, sep = ""))
>     calledf(x)
>     ## do more stuff
> }
>
>
> Two questions:
> 1) Why is the second better? 
>
> 2) By changing g or h I could use "do.call" instead; why would that be better? 
> Because I can handle differences in argument lists?
>
>   
Who says that they are better?  If the question is how to call a
function specified by half of its name, the answer could well be to use
parse(), the point is that you should rethink whether that was really
the right question.

Why not instead, e.g.

f <- list("1"=function(x) {x + 1} , "2"=function(x) {x + 2})
h <- function(x, fpost) f[[fpost]](x)

> h(2,"2")
[1] 4
> h(2,"1")
[1] 3

> Thanks,
>
>
> R.
>
>
>
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From gunter.berton at gene.com  Fri Jan  5 19:35:42 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Fri, 5 Jan 2007 10:35:42 -0800
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <200701051902.07294.rdiaz@cnio.es>
Message-ID: <008f01c730f8$4eafb870$4d908980@gne.windows.gene.com>

??

Or to add to what Peter Dalgaard said... (perhaps for the case of many more
functions)

Why eval(parse())? What's wrong with if then? 

g <- function(fpost,x){if(fpost==1)f.1 else f.2 }(x)

or switch() if you have more than 2 possible arguments? I think your remarks
reinforce the wisdom of Thomas's "axiom" . 

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ramon Diaz-Uriarte
Sent: Friday, January 05, 2007 10:02 AM
To: r-help; rdiaz02 at gmail.com
Subject: [R] eval(parse(text vs. get when accessing a function

Dear All,

I've read Thomas Lumley's fortune "If the answer is parse() you should
usually 
rethink the question.". But I am not sure it that also applies (and why) to 
other situations (Lumley's comment 
http://tolstoy.newcastle.edu.au/R/help/05/02/12204.html
was in reply to accessing a list).

Suppose I have similarly called functions, except for a postfix. E.g.

f.1 <- function(x) {x + 1}
f.2 <- function(x) {x + 2}

And sometimes I want to call f.1 and some other times f.2 inside another 
function. I can either do:

g <- function(x, fpost) {
    calledf <- eval(parse(text = paste("f.", fpost, sep = "")))
    calledf(x)
    ## do more stuff
}


Or:

h <- function(x, fpost) {
    calledf <- get(paste("f.", fpost, sep = ""))
    calledf(x)
    ## do more stuff
}


Two questions:
1) Why is the second better? 

2) By changing g or h I could use "do.call" instead; why would that be
better? 
Because I can handle differences in argument lists?



Thanks,


R.



-- 
Ram?n D?az-Uriarte
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From rdiaz at cnio.es  Fri Jan  5 19:40:57 2007
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Fri, 5 Jan 2007 19:40:57 +0100
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <459E972E.6070807@biostat.ku.dk>
References: <200701051902.07294.rdiaz@cnio.es> <459E972E.6070807@biostat.ku.dk>
Message-ID: <200701051940.57136.rdiaz@cnio.es>

On Friday 05 January 2007 19:21, Peter Dalgaard wrote:
> Ramon Diaz-Uriarte wrote:
> > Dear All,
> >
> > I've read Thomas Lumley's fortune "If the answer is parse() you should
> > usually rethink the question.". But I am not sure it that also applies
> > (and why) to other situations (Lumley's comment
> > http://tolstoy.newcastle.edu.au/R/help/05/02/12204.html
> > was in reply to accessing a list).
> >
> > Suppose I have similarly called functions, except for a postfix. E.g.
> >
> > f.1 <- function(x) {x + 1}
> > f.2 <- function(x) {x + 2}
> >
> > And sometimes I want to call f.1 and some other times f.2 inside another
> > function. I can either do:
> >
> > g <- function(x, fpost) {
> >     calledf <- eval(parse(text = paste("f.", fpost, sep = "")))
> >     calledf(x)
> >     ## do more stuff
> > }
> >
> >
> > Or:
> >
> > h <- function(x, fpost) {
> >     calledf <- get(paste("f.", fpost, sep = ""))
> >     calledf(x)
> >     ## do more stuff
> > }
> >
> >
> > Two questions:
> > 1) Why is the second better?
> >
> > 2) By changing g or h I could use "do.call" instead; why would that be
> > better? Because I can handle differences in argument lists?

Dear Peter,

Thanks for your answer.

>
> Who says that they are better?  If the question is how to call a
> function specified by half of its name, the answer could well be to use
> parse(), the point is that you should rethink whether that was really
> the right question.
>
> Why not instead, e.g.
>
> f <- list("1"=function(x) {x + 1} , "2"=function(x) {x + 2})
> h <- function(x, fpost) f[[fpost]](x)
>
> > h(2,"2")
>
> [1] 4
>
> > h(2,"1")
>
> [1] 3
>

I see, this is direct way of dealing with the problem. However, you first need 
to build the f list, and you might not know about that ahead of time. For 
instance, if I build a function so that the only thing that you need to do to 
use my function g is to call your function "f.something", and then pass 
the "something". 

I am still under the impression that, given your answer, 
using "eval(parse(text" is not your preferred way.  What are the possible 
problems (if there are any, that is). I guess I am puzzled by "rethink 
whether that was really the right question". 


Thanks,

R.







> > Thanks,
> >
> >
> > R.

-- 
Ram?n D?az-Uriarte
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}


From rdiaz at cnio.es  Fri Jan  5 19:43:17 2007
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Fri, 5 Jan 2007 19:43:17 +0100
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <008f01c730f8$4eafb870$4d908980@gne.windows.gene.com>
References: <008f01c730f8$4eafb870$4d908980@gne.windows.gene.com>
Message-ID: <200701051943.17822.rdiaz@cnio.es>

On Friday 05 January 2007 19:35, Bert Gunter wrote:
> ??
>
> Or to add to what Peter Dalgaard said... (perhaps for the case of many more
> functions)
>
> Why eval(parse())? What's wrong with if then?
>
> g <- function(fpost,x){if(fpost==1)f.1 else f.2 }(x)
>
> or switch() if you have more than 2 possible arguments? I think your
> remarks reinforce the wisdom of Thomas's "axiom" .

Thanks, Bert, but as with Peter's solution, your solution forces me to build g 
ahead of time. And again, I am not sure I see why the attempt to avoid 
eval(parse(text.

Best,

R.


>
> Bert Gunter
> Genentech Nonclinical Statistics
> South San Francisco, CA 94404
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ramon Diaz-Uriarte
> Sent: Friday, January 05, 2007 10:02 AM
> To: r-help; rdiaz02 at gmail.com
> Subject: [R] eval(parse(text vs. get when accessing a function
>
> Dear All,
>
> I've read Thomas Lumley's fortune "If the answer is parse() you should
> usually
> rethink the question.". But I am not sure it that also applies (and why) to
> other situations (Lumley's comment
> http://tolstoy.newcastle.edu.au/R/help/05/02/12204.html
> was in reply to accessing a list).
>
> Suppose I have similarly called functions, except for a postfix. E.g.
>
> f.1 <- function(x) {x + 1}
> f.2 <- function(x) {x + 2}
>
> And sometimes I want to call f.1 and some other times f.2 inside another
> function. I can either do:
>
> g <- function(x, fpost) {
>     calledf <- eval(parse(text = paste("f.", fpost, sep = "")))
>     calledf(x)
>     ## do more stuff
> }
>
>
> Or:
>
> h <- function(x, fpost) {
>     calledf <- get(paste("f.", fpost, sep = ""))
>     calledf(x)
>     ## do more stuff
> }
>
>
> Two questions:
> 1) Why is the second better?
>
> 2) By changing g or h I could use "do.call" instead; why would that be
> better?
> Because I can handle differences in argument lists?
>
>
>
> Thanks,
>
>
> R.

-- 
Ram?n D?az-Uriarte
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}


From jholtman at gmail.com  Fri Jan  5 20:01:24 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 5 Jan 2007 14:01:24 -0500
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <200701051943.17822.rdiaz@cnio.es>
References: <008f01c730f8$4eafb870$4d908980@gne.windows.gene.com>
	<200701051943.17822.rdiaz@cnio.es>
Message-ID: <644e1f320701051101n22b29616t393a3161ebbd8d4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070105/4cfbf1d4/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Jan  5 20:10:30 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Jan 2007 19:10:30 +0000 (GMT)
Subject: [R] problem with logLik and offsets
In-Reply-To: <459BF856.4070200@shef.ac.uk>
References: <459BF856.4070200@shef.ac.uk>
Message-ID: <Pine.LNX.4.64.0701051909570.14900@gannet.stats.ox.ac.uk>

The problem is with residuals in model3.

Fixed in 2.4.1 patched and later.

On Wed, 3 Jan 2007, Jarrod Hadfield wrote:

> Hi,
>
> I'm trying to compare models, one of which has all parameters fixed
> using offsets.  The log-likelihoods seem reasonble in all cases except
> the model in which there are no free parameters (model3 in the toy
> example below).  Any help would be appreciated.
>
> Cheers,
>
> Jarrod
>
> x<-rnorm(100)
> y<-rnorm(100, 1+x)
>
> model1<-lm(y~x)
> logLik(model1)
> sum(dnorm(y, predict(model1), summary(model1)$sigma,log=TRUE))
>
> # no offset - in agreement
>
> model2<-lm(y~offset(rep(1,100))+x-1)
> logLik(model2)
> sum(dnorm(y, predict(model2),summary(model2)$sigma,log=TRUE))
>
> # offset and free parameters - in agreement
>
> model3<-lm(y~offset(rep(1,100))+offset(x)-1)
> logLik(model3)
> sum(dnorm(y, predict(model3),summary(model3)$sigma,log=TRUE))
>
> # offset only - discrepancy
>
> sum(predict(model3)-c(1+x))
>
> # yet predict is correct
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rmh at temple.edu  Fri Jan  5 21:09:08 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri,  5 Jan 2007 15:09:08 -0500 (EST)
Subject: [R] split-plot multiple comparisons
Message-ID: <20070105150908.BRK89209@po-d.temple.edu>

Thank you for your example.  I am now using it an example in the MMC
Mean--mean Multiple Comparisons plot in the HH package on CRAN.  The
source for HH_1.17 is on CRAN in Austria as of this morning.  The
Windows and MacOS binaries will be on CRAN in a few days and will then
propagate to the mirrors.

Until then, you can get the R Windows binary directly from me at

http://astro.ocis.temple.edu/~rmh/HH/HH_1.17.zip          ## R Windows binary
http://astro.ocis.temple.edu/~rmh/HH/HH_1.17.tar.gz       ## source
http://astro.ocis.temple.edu/~rmh/HH/HH_1.17_S_WIN386.zip ## S-Plus 8 Windows binary

The S-Plus 8 package is also available at
http://csan.insightful.com/

After you install and load HH with

library(HH)
?MMC

will give the complete example.


Some comments on the example.

The first interaction2wt figure shows parallel traces in the blocks,
visually confirming that the blocks appear to be orthogonal to the
treatments.

The second interaction2wt figure shows a hint of the hibrido:nitrogeno
interaction since the P3732 trace is monotone increasing in nitrogen
and the others bend back.

The MMC plot shows very clearly that the hybrids fall into two
different groups, with LH74, P3747, and P3732 in one group and with
Mol17 and A632 in the other group.  There is a non-significant
distinction between LH74 and the two P37** varieties.

The MMC plot needs the tiebreaker in this example because observed
means for several of the groups are almost identical.

The MMC plot is described in

Journal of Computational & Graphical Statistics 
2006, vol. 15, no. 4, pp. 937 - 955 
Mean-Mean Multiple Comparison Displays for Families of Linear Contrasts
Richard M. Heiberger; Burt Holland 

Abstract
Traditional tabular and graphical displays of results of simultaneous
confidence intervals or hypothesis tests are deficient in several
respects. Expanding on earlier work, we present new mean-mean multiple
comparison graphs that succinctly and compactly display the results of
traditional procedures for multiple comparisons of population means or
linear contrasts involving means. The MMC plot can be used with
unbalanced, multifactor designs with covariates. After reviewing the
construction of these displays in the S language (S-Plus and R), we
demonstrate their application to four multiple comparison scenarios.


From helprhelp at gmail.com  Fri Jan  5 21:11:37 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Fri, 5 Jan 2007 15:11:37 -0500
Subject: [R] memory limits in R loading a dataset and using the package
	tree
In-Reply-To: <e591a95b0701040830j37ac495arc72ff9dc74987e0d@mail.gmail.com>
References: <e591a95b0701040830j37ac495arc72ff9dc74987e0d@mail.gmail.com>
Message-ID: <cdf817830701051211y590a76dfxa8be459b3001b1c9@mail.gmail.com>

IMHO, R is not good at really large-scale data mining, esp. when the
algorithm is complicated. The alternatives are
1. sampling your data; sometimes you really do not need that large
number of records and the accuracy might already be good enough when
you load less.

2. find an alternative (commercial software) to do this job if you
really need to load all.

3. make a wrapper function, sampling your data and load it into R and
build model and repeat this process until you get n models. Then you
can do like meta-learning or simply majority-win if your problem is
classification.

HTH,

On 1/4/07, domenico pestalozzi <statadat at gmail.com> wrote:
> I think the question is discussed in other thread, but I don't exactly find
> what I want .
> I'm working in Windows XP with 2GB of memory and a Pentium 4 - 3.00Ghx.
> I have the necessity of working with large dataset, generally from 300,000
> records to 800,000 (according to the project), and about 300 variables
> (...but a dataset with 800,000 records could not be "large" in your
> opinion...). Because of we are deciding if R will be the official software
> in our company, I'd like to say if the possibility of using R with these
> datasets depends only by the characteristics of the "engine" (memory and
> processor).
> In this case we can improve the machine (for example, what memory you
> reccomend?).
>
> For example, I have a dataset of 200,000 records and 211 variables but I
> can't load the dataset because R doesn't work : I control the loading
> procedure (read.table in R) by using the windows task-manager and R is
> blocked when the file paging is 1.10 GB.
> After this I try with a sample of 100,000 records and I can correctly load
> tha dataset, but I'd like to use the package tree, but after some seconds (
> I use this tree(variable1~., myDataset) )   I obtain the message "Reached
> total allocation of 1014Mb".
>
> I'd like your opinion and suggestion, considering that I could improve (in
> memory) my computer.
>
> pestalozzi
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From fdoespin at gmail.com  Fri Jan  5 18:29:20 2007
From: fdoespin at gmail.com (fernando espindola)
Date: Fri, 05 Jan 2007 17:29:20 +0000
Subject: [R] Error in save function
Message-ID: <459E8AF0.7010608@gmail.com>

Hi r-user,

I am trying to save same objects in workspace, but the next error 
message is show in line command,

save(area,cent,larg,mran,eddy,dia,vort,slan,file="stat.Rdata")
Erro en save.default(area, cent, larg, mran, eddy, dia, vort, slan, file 
= "stat.Rdata") :
        5 arguments passed to 'saveToConn' which requires 6

Is there a way to help me solve this problems?

Thank

Fernando


From kbeath at efs.mq.edu.au  Fri Jan  5 21:48:42 2007
From: kbeath at efs.mq.edu.au (Ken Beath)
Date: Sat, 06 Jan 2007 07:48:42 +1100
Subject: [R] maximum likelihood estimation of 5 parameters
Message-ID: <s59f5478.045@mail.efs.mq.edu.au>

Using the inverse logistic transform to replace p by exp(xp)/(1+exp(xp)) allows unconstrained fitting of xp. There may still be problems where xp tends to + or - infinity depending on starting values.

>>> francogrex <francogrex at mail.com> 01/05/07 11:54 PM >>>

Hi Guys, it would be great if you could help me with a MLE problem in R.

I am trying to evaluate  the maximum likelihood estimates of theta = (a1,
b1, a2, b2, P) which defines a mixture of a Poisson distribution and two
gamma prior distributions (where the Poisson means have a gamma
distribution, actually 2 gammas and P is the mixing factor). The likelihood
function for theta is L(theta) = Pi,j{P f(Nij; a1, b1, Eij) + (1 * P) f(Nij;
a2, b2, Eij),} 
The maximum likelihood estimate of theta is the vector that maximizes the
above equation (the values of N and E are given). The authors of the article
I read say that the maximization involves an iterative search in the five
dimensional parameter space, where each iteration involves computing
log[L(theta)] and its first and second-order derivatives. In test runs it is
suggested that the maximization typically takes between 5 and 15 iterations
from the starting point theta = (a1 = 0.2, b1 = 0.1, a2 = 2, b2 = 4, P =
1/3). 

Now I have done maximization of a gamma-poisson mixture before (1 poisson, 1
gamma) successfully and I could determine correctly alpha (a) and beta(a).
But this one above is giving me ridiculously large unusable values (for
example P should not be above 1 and sometimes I get values of 500!) or even
negative values! I know the values I should be obtaining with my samples
shouldn't be far from the staring points. Is there a way to help me solve
this issue? Thanks.
-- 
View this message in context: http://www.nabble.com/maximum-likelihood-estimation-of-5-parameters-tf2925364.html#a8177473
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Fri Jan  5 22:00:40 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Jan 2007 21:00:40 +0000 (GMT)
Subject: [R] Error in save function
In-Reply-To: <459E8AF0.7010608@gmail.com>
References: <459E8AF0.7010608@gmail.com>
Message-ID: <Pine.LNX.4.64.0701052050270.6717@gannet.stats.ox.ac.uk>

There is no 'save.default' function in R.

You were asked in the posting guide to report the results of 
sessionInfo().  (It looks like an old version of R.oo might be the 
culprit, and you might need to update.packages().)

On Fri, 5 Jan 2007, fernando espindola wrote:

> Hi r-user,
>
> I am trying to save same objects in workspace, but the next error
> message is show in line command,
>
> save(area,cent,larg,mran,eddy,dia,vort,slan,file="stat.Rdata")
> Erro en save.default(area, cent, larg, mran, eddy, dia, vort, slan, file
> = "stat.Rdata") :
>        5 arguments passed to 'saveToConn' which requires 6
>
> Is there a way to help me solve this problems?
>
> Thank
>
> Fernando
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Greg.Snow at intermountainmail.org  Fri Jan  5 22:18:02 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 5 Jan 2007 14:18:02 -0700
Subject: [R] eval(parse(text vs. get when accessing a function
Message-ID: <07E228A5BE53C24CAD490193A7381BBB77C654@LP-EXCHVS07.CO.IHC.COM>

Ramon,

I prefer to use the list method for this type of thing, here are a couple of reasons why (maybe you are more organized than me and would never do some of the stupid things that I have, so these don't apply to you, but you can see that the general suggestion applys to some of the rest of us).

Using the list forces you to think about what functions may be called and thinking about things before doing them is usually a good idea.  Personally I don't trust the user of my functions (usually my future self who has forgotten something that seemed obvious at the time) to not do something stupid with them.

With list elements you can have names for the functions and access them either by the name or by a number, I find that a lot easier when I go back to edit/update than to remember which function f.1 or f.2 did what.

With your function, what if the user runs:

> g(5,3)

What should it do?  (you have only shown definitions for f.1 and f.2).  With my luck I would accidentily type that and just happen to have a f.3 function sitting around from a previous project that does something that I really don't want it to do now.  If I use the list approach then I will get a subscript out of bounds error rather than running something unintended.


2nd, If I used the eval-parse approach then I would probably at some point redefine f.1 or f.2 to the output of a regression analysis or something, then go back and run the g function at a later time and wonder why I am getting an error, then once I have finally figured it out, now I need to remember what f.1 did and rewrite it again.  I am much less likely to accidentally replace an element of a list, and if the list is well named I am unlikely to replace the whole list by accident.


3rd, If I ever want to use this code somewhere else (new version of R, on the laptop, give to coworker, ...), it is a lot easier to save and load a single list than to try to think of all the functions that need to be saved.


Personally I have never regretted trying not to underestimate my own future stupidity.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ramon 
> Diaz-Uriarte
> Sent: Friday, January 05, 2007 11:41 AM
> To: Peter Dalgaard
> Cc: r-help; rdiaz02 at gmail.com
> Subject: Re: [R] eval(parse(text vs. get when accessing a function
> 
> On Friday 05 January 2007 19:21, Peter Dalgaard wrote:
> > Ramon Diaz-Uriarte wrote:
> > > Dear All,
> > >
> > > I've read Thomas Lumley's fortune "If the answer is parse() you 
> > > should usually rethink the question.". But I am not sure it that 
> > > also applies (and why) to other situations (Lumley's comment 
> > > http://tolstoy.newcastle.edu.au/R/help/05/02/12204.html
> > > was in reply to accessing a list).
> > >
> > > Suppose I have similarly called functions, except for a 
> postfix. E.g.
> > >
> > > f.1 <- function(x) {x + 1}
> > > f.2 <- function(x) {x + 2}
> > >
> > > And sometimes I want to call f.1 and some other times f.2 inside 
> > > another function. I can either do:
> > >
> > > g <- function(x, fpost) {
> > >     calledf <- eval(parse(text = paste("f.", fpost, sep = "")))
> > >     calledf(x)
> > >     ## do more stuff
> > > }
> > >
> > >
> > > Or:
> > >
> > > h <- function(x, fpost) {
> > >     calledf <- get(paste("f.", fpost, sep = ""))
> > >     calledf(x)
> > >     ## do more stuff
> > > }
> > >
> > >
> > > Two questions:
> > > 1) Why is the second better?
> > >
> > > 2) By changing g or h I could use "do.call" instead; why 
> would that 
> > > be better? Because I can handle differences in argument lists?
> 
> Dear Peter,
> 
> Thanks for your answer.
> 
> >
> > Who says that they are better?  If the question is how to call a 
> > function specified by half of its name, the answer could well be to 
> > use parse(), the point is that you should rethink whether that was 
> > really the right question.
> >
> > Why not instead, e.g.
> >
> > f <- list("1"=function(x) {x + 1} , "2"=function(x) {x + 2}) h <- 
> > function(x, fpost) f[[fpost]](x)
> >
> > > h(2,"2")
> >
> > [1] 4
> >
> > > h(2,"1")
> >
> > [1] 3
> >
> 
> I see, this is direct way of dealing with the problem. 
> However, you first need to build the f list, and you might 
> not know about that ahead of time. For instance, if I build a 
> function so that the only thing that you need to do to use my 
> function g is to call your function "f.something", and then 
> pass the "something". 
> 
> I am still under the impression that, given your answer, 
> using "eval(parse(text" is not your preferred way.  What are 
> the possible problems (if there are any, that is). I guess I 
> am puzzled by "rethink whether that was really the right question". 
> 
> 
> Thanks,
> 
> R.
> 
> 
> 
> 
> 
> 
> 
> > > Thanks,
> > >
> > >
> > > R.
> 
> --
> Ram?n D?az-Uriarte
> Centro Nacional de Investigaciones Oncol?gicas (CNIO) 
> (Spanish National Cancer Center) Melchor Fern?ndez Almagro, 3
> 28029 Madrid (Spain)
> Fax: +-34-91-224-6972
> Phone: +-34-91-224-6900
> 
> http://ligarto.org/rdiaz
> PGP KeyID: 0xE89B3462
> (http://ligarto.org/rdiaz/0xE89B3462.asc)
> 
> 
> 
> **NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en 
> s...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From p.dalgaard at biostat.ku.dk  Fri Jan  5 22:33:45 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 05 Jan 2007 22:33:45 +0100
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB77C654@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBB77C654@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <459EC439.5010706@biostat.ku.dk>

Greg Snow wrote:
> Personally I have never regretted trying not to underestimate my own future stupidity.
>
>   
Fortune!


From Sicotte.Hugues at mayo.edu  Fri Jan  5 22:48:28 2007
From: Sicotte.Hugues at mayo.edu (Sicotte, Hugues   Ph.D.)
Date: Fri, 5 Jan 2007 15:48:28 -0600
Subject: [R] memory limits in R loading a dataset and using the
	packagetree
Message-ID: <2E17292A64E6ED418A60BE89326B1AAB3224CC@msgebe11.mfad.mfroot.org>

I agree about sampling, but.. You can go a little further with your
hardware.
The defaults in R is to play nice and limit your allocation to half
the available RAM. Make sure you have a lot of disk swap space (at least
1G with 2G of RAM) and you can set your memory limit to 2G for R.

See help(memory.size)  and use the memory.limit function

Hugues 


P.s. Someone let me use their 16Gig of RAM linux
And I was able to run R-64 bits with "top" showing 6Gigs of RAM
allocated (with suitable --max-mem-size command line parameters at
startup for R). 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Weiwei Shi
Sent: Friday, January 05, 2007 2:12 PM
To: domenico pestalozzi
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] memory limits in R loading a dataset and using the
packagetree

IMHO, R is not good at really large-scale data mining, esp. when the
algorithm is complicated. The alternatives are
1. sampling your data; sometimes you really do not need that large
number of records and the accuracy might already be good enough when
you load less.

2. find an alternative (commercial software) to do this job if you
really need to load all.

3. make a wrapper function, sampling your data and load it into R and
build model and repeat this process until you get n models. Then you
can do like meta-learning or simply majority-win if your problem is
classification.

HTH,

On 1/4/07, domenico pestalozzi <statadat at gmail.com> wrote:
> I think the question is discussed in other thread, but I don't exactly
find
> what I want .
> I'm working in Windows XP with 2GB of memory and a Pentium 4 -
3.00Ghx.
> I have the necessity of working with large dataset, generally from
300,000
> records to 800,000 (according to the project), and about 300 variables
> (...but a dataset with 800,000 records could not be "large" in your
> opinion...). Because of we are deciding if R will be the official
software
> in our company, I'd like to say if the possibility of using R with
these
> datasets depends only by the characteristics of the "engine" (memory
and
> processor).
> In this case we can improve the machine (for example, what memory you
> reccomend?).
>
> For example, I have a dataset of 200,000 records and 211 variables but
I
> can't load the dataset because R doesn't work : I control the loading
> procedure (read.table in R) by using the windows task-manager and R is
> blocked when the file paging is 1.10 GB.
> After this I try with a sample of 100,000 records and I can correctly
load
> tha dataset, but I'd like to use the package tree, but after some
seconds (
> I use this tree(variable1~., myDataset) )   I obtain the message
"Reached
> total allocation of 1014Mb".
>
> I'd like your opinion and suggestion, considering that I could improve
(in
> memory) my computer.
>
> pestalozzi
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From francogrex at mail.com  Fri Jan  5 23:00:51 2007
From: francogrex at mail.com (francogrex)
Date: Fri, 5 Jan 2007 14:00:51 -0800 (PST)
Subject: [R] maximum likelihood estimation of 5 parameters
In-Reply-To: <000801c730e8$8eeb26f0$7c94100a@win.ad.jhu.edu>
References: <8177473.post@talk.nabble.com> <C1C417E9.C48C%I.Visser@uva.nl>
	<8180120.post@talk.nabble.com>
	<000801c730e8$8eeb26f0$7c94100a@win.ad.jhu.edu>
Message-ID: <8186869.post@talk.nabble.com>


No I have not forgotten to use a negative fnscale to optimize, so as you
suggest I will post some parts of the code I am running to show you the
errors:

> n
 [1]   3   1   4  54   6  58  20  14   3  14   4  65   1   7   9  10   2   4 
66
[20]   5   9   7  12   7  55 105   2   5  10  55   5  28   1   1   6   2   1 
30
[39]   6  49   7  21   8   7
> e
 [1] 21.763201  1.209070  4.836270 32.644798 19.546600 24.584400 30.226700
 [8]  6.045340 14.010100  3.113350 21.015100 12.583100 15.826200 19.458401
[15]  3.891690  1.329970  0.241814  3.143580 13.057900  0.725441 18.136000
[22]  2.187660  6.319900  1.701510 29.654900 36.460999  7.292190  1.215370
[29]  3.209070 19.995001 11.972300  3.455920  0.138539  0.113350  1.360200
[36]  1.889170  1.518890 18.226700  4.050380 27.340099  1.181360 16.370300
[43] 20.589399 25.314899

> fr<-function(a1,b1,a2,b2,p){
+ 
+ w<-((gamma(a1+n)))/((gamma(a1)*factorial(n))*(1+(e/b1)^a1)*(1+(b1/e)^n))
+ z<-((gamma(a2+n)))/((gamma(a2)*factorial(n))*(1+(e/b2)^a2)*(1+(b2/e)^n))
+ 
+ sum (log( (p*w)+ ((1-p)*z) ))
+ 
+ }
> 
> mle((fr),
> start=list(a1=0.2,b1=0.1,a2=2,b2=4,p=0.33),method="BFGS",control=list(fnscale=-1))
Error in optim(start, f, method = method, hessian = TRUE, ...) : 
        non-finite finite-difference value [2]

And with the L-BFGS-B:

Error in optim(start, f, method = method, hessian = TRUE, ...) : 
        L-BFGS-B needs finite values of 'fn'

AND WITH NELDER-MEAD it doesn't work either (same error), but when I change
intial parameters (though I shouldn't, it gives something very weird
(negatives or sometimes huge values).

Call:
mle(minuslogl = (fr), start = list(a1 = 1, b1 = 1, a2 = 10, b2 = 10, 
    p = 0.9), method = "Nelder-Mead", control = list(fnscale = -1))

Coefficients:
        a1         b1         a2         b2          p 
-2.5035823  0.6236359 26.5562988 12.9604112 -0.1383767 

Thanks



Ravi Varadhan wrote:
> 
> Franco,
> Is it possible that you have failed to provide the negative of
> loglikelihood
> to "optim", since optim, by default, minimizes a function?  If you want to
> do this withput redefining the log-likelihood, you should set fnscale= -1
> (as hinted by Prof. Ripley).  This would turn the problem into a
> maximization problem.  
> 
> If this doesn't work, you should provide more details (a reproducible code
> with actual error message).
> 
> Ravi.
> 
> ----------------------------------------------------------------------------
> -------
> 
> Ravi Varadhan, Ph.D.
> 
> Assistant Professor, The Center on Aging and Health
> 
> Division of Geriatric Medicine and Gerontology 
> 
> Johns Hopkins University
> 
> Ph: (410) 502-2619
> 
> Fax: (410) 614-9625
> 
> Email: rvaradhan at jhmi.edu
> 
> Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
> 
>  
> 
> ----------------------------------------------------------------------------
> --------
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of francogrex
> Sent: Friday, January 05, 2007 10:42 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] maximum likelihood estimation of 5 parameters
> 
> 
> 
> Franco,
> You can provide lower and upper bounds on the parameters if you use optim
> with method="L-BFGS-B".
> Hth, Ingmar
> 
> Thanks, but when I use L-BFGS-B it tells me that there is an  error in
> optim(start, f, method = method, hessian = TRUE, ...) : L-BFGS-B needs
> finite values of 'fn'
> 
> -- 
> View this message in context:
> http://www.nabble.com/maximum-likelihood-estimation-of-5-parameters-tf292536
> 4.html#a8180120
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/maximum-likelihood-estimation-of-5-parameters-tf2925364.html#a8186869
Sent from the R help mailing list archive at Nabble.com.


From pburns at pburns.seanet.com  Fri Jan  5 23:17:25 2007
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 05 Jan 2007 22:17:25 +0000
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <459EC439.5010706@biostat.ku.dk>
References: <07E228A5BE53C24CAD490193A7381BBB77C654@LP-EXCHVS07.CO.IHC.COM>
	<459EC439.5010706@biostat.ku.dk>
Message-ID: <459ECE75.9030900@pburns.seanet.com>

Peter Dalgaard wrote:

>Greg Snow wrote:
>  
>
>>Personally I have never regretted trying not to underestimate my own future stupidity.
>>
>>  
>>    
>>
>Fortune!
>  
>

Seconded.

>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From Inman.Brant at mayo.edu  Sat Jan  6 00:01:43 2007
From: Inman.Brant at mayo.edu (Inman, Brant A.   M.D.)
Date: Fri, 5 Jan 2007 17:01:43 -0600
Subject: [R] Calling "confint.glm" from within another function
Message-ID: <6021CA6EF4C8374084D4F5A141F1CBBB6649DB@msgebe23.mfad.mfroot.org>


On July 12, 2004 Spencer Graves wrote an email describing essentially
the same issue that I would like help on: calling the confint function
from within another homemade function.  Because he provided many good
examples of the problem, I will not reproduce them here but will instead
refer readers to the original posting:

http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg23826.html

It is not clear from the replies to his posting how he managed to solve
the problem, so I am reposting a similar note again in hopes of
guidance.  In particular, I would like to know if and how I need to
modify the confint function call so that I can access it from within
another function that I have written.

Thanks,

Brant Inman


From tlumley at u.washington.edu  Sat Jan  6 00:06:14 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 5 Jan 2007 15:06:14 -0800 (PST)
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <200701051940.57136.rdiaz@cnio.es>
Message-ID: <Pine.LNX.4.43.0701051506140.3919@hymn04.u.washington.edu>

On Fri, 5 Jan 2007, Ramon Diaz-Uriarte wrote:

> I see, this is direct way of dealing with the problem. However, you first need
> to build the f list, and you might not know about that ahead of time. For
> instance, if I build a function so that the only thing that you need to do to
> use my function g is to call your function "f.something", and then pass
> the "something".
>
> I am still under the impression that, given your answer,
> using "eval(parse(text" is not your preferred way.  What are the possible
> problems (if there are any, that is). I guess I am puzzled by "rethink
> whether that was really the right question".
>

There are definitely situations where parse() is necessary or convenient,
or we wouldn't provide it. For example, there are some formula-manipulation problems where it really does seem to be the best solution.

The point of my observation was that it is relatively common for people to ask about parse() solutions to problems, but relatively rare to see them in code by experienced R programmers.  The 'rethink the question' point is that a narrowly-posed programming problem may suggest parse() as the answer, when thinking more broadly about what you are trying to do may allow a completely different approach [the example of lists is a common one].

The problem with eval(parse()) is not primarily one of speed.  A problem with parse() is than manipulating text strings is easy to mess up, since text has so much less structure than code. A problem with eval() is that it is too powerful -- since it can do anything, it is harder to keep track of what it is doing.

In one sense this is just a style issue, but I still think my comment is good advice. If you find yourself wanting to use parse() it is a good idea to stop and think about whether there is a better way to do it. Often, there is. Sometimes, there isn't.


        -thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From marc_schwartz at comcast.net  Sat Jan  6 00:13:36 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 05 Jan 2007 17:13:36 -0600
Subject: [R] Calling "confint.glm" from within another function
In-Reply-To: <6021CA6EF4C8374084D4F5A141F1CBBB6649DB@msgebe23.mfad.mfroot.org>
References: <6021CA6EF4C8374084D4F5A141F1CBBB6649DB@msgebe23.mfad.mfroot.org>
Message-ID: <1168038817.4698.192.camel@localhost.localdomain>

On Fri, 2007-01-05 at 17:01 -0600, Inman, Brant A. M.D. wrote:
> On July 12, 2004 Spencer Graves wrote an email describing essentially
> the same issue that I would like help on: calling the confint function
> from within another homemade function.  Because he provided many good
> examples of the problem, I will not reproduce them here but will instead
> refer readers to the original posting:
> 
> http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg23826.html
> 
> It is not clear from the replies to his posting how he managed to solve
> the problem, so I am reposting a similar note again in hopes of
> guidance.  In particular, I would like to know if and how I need to
> modify the confint function call so that I can access it from within
> another function that I have written.
> 
> Thanks,
> 
> Brant Inman

I'll see your thread and raise you one:-)

  https://stat.ethz.ch/pipermail/r-devel/2006-December/043952.html

Bottom line, it has been fixed in a recent update to the VR bundle.

HTH,

Marc Schwartz


From cberry at tajo.ucsd.edu  Sat Jan  6 02:06:54 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Fri, 5 Jan 2007 17:06:54 -0800
Subject: [R] maximum likelihood estimation of 5 parameters
In-Reply-To: <8186869.post@talk.nabble.com>
References: <8177473.post@talk.nabble.com> <C1C417E9.C48C%I.Visser@uva.nl>
	<8180120.post@talk.nabble.com>
	<000801c730e8$8eeb26f0$7c94100a@win.ad.jhu.edu>
	<8186869.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.64.0701051540001.10424@tajo.ucsd.edu>


Franco,

It is up to the user of mle() to define a function that is numerically 
well behaved - even near the boundaries - and/or to provide suitable 
boundary constraints.

Using gamma() in situations in which the argument could be 500 (say) will 
cause problems. It appears that the maximum of fr is attained for values 
for which gamma(a2+n) cannot be evaluated using machine arithmetic (and 
for very small values of 'p', too). Does this function truly have a finite 
maximum for these data?

Dividing by very small numbers is also a potential killer.

Trying to exponentiate potentially negative numbers is also asking for 
trouble.

So, you need to write a more robust version of 'fr'.

Some suggestions:

 	Use lgamma() or choose() or lchoose() rather than gamma() or
 	factorial()

 	Try to add and subtract logarithms in preference to
 	multiplying and dividing as cancellations usually are more
 	accurate and you are less likely to run into machine accuracy
 	issues.

 	Replace p with plogis(p.x) and consider bounding p.x to keep
 	plogis(x) away from 0 and 1

 	Establish boundary constraints to keep
 	things like (1+(e/b1)^a1)*(1+(b1/e)^n)
 	from causing problems. Consider log1p(), etc or writing this
 	out longhand and doing some cancellations.

 	Worry about fnscale. It seems unlikely that all args are indeed on
 	the same scale.

 	Use stingy boundary constrants to get a better set of starting
 	values. Progressively relax the constraints and watch how the
 	parameter changes.

If after all that, your new fr function still hands you non-finite values, 
then you need to investigate to find out where they are coming from.

There are many ways to do this, but broadly speaking you are 'debugging' 
your code, so use the manuals and RSiteSearch to figure out what you need 
to do to become skilled in debugging.

On Fri, 5 Jan 2007, francogrex wrote:

>
> No I have not forgotten to use a negative fnscale to optimize, so as you
> suggest I will post some parts of the code I am running to show you the
> errors:
>
>> n
> [1]   3   1   4  54   6  58  20  14   3  14   4  65   1   7   9  10   2   4
> 66
> [20]   5   9   7  12   7  55 105   2   5  10  55   5  28   1   1   6   2   1
> 30
> [39]   6  49   7  21   8   7
>> e
> [1] 21.763201  1.209070  4.836270 32.644798 19.546600 24.584400 30.226700
> [8]  6.045340 14.010100  3.113350 21.015100 12.583100 15.826200 19.458401
> [15]  3.891690  1.329970  0.241814  3.143580 13.057900  0.725441 18.136000
> [22]  2.187660  6.319900  1.701510 29.654900 36.460999  7.292190  1.215370
> [29]  3.209070 19.995001 11.972300  3.455920  0.138539  0.113350  1.360200
> [36]  1.889170  1.518890 18.226700  4.050380 27.340099  1.181360 16.370300
> [43] 20.589399 25.314899
>
>> fr<-function(a1,b1,a2,b2,p){
> +
> + w<-((gamma(a1+n)))/((gamma(a1)*factorial(n))*(1+(e/b1)^a1)*(1+(b1/e)^n))
> + z<-((gamma(a2+n)))/((gamma(a2)*factorial(n))*(1+(e/b2)^a2)*(1+(b2/e)^n))
> +
> + sum (log( (p*w)+ ((1-p)*z) ))
> +
> + }
>>
>> mle((fr),
>> start=list(a1=0.2,b1=0.1,a2=2,b2=4,p=0.33),method="BFGS",control=list(fnscale=-1))
> Error in optim(start, f, method = method, hessian = TRUE, ...) :
>        non-finite finite-difference value [2]
>
> And with the L-BFGS-B:
>
> Error in optim(start, f, method = method, hessian = TRUE, ...) :
>        L-BFGS-B needs finite values of 'fn'
>
> AND WITH NELDER-MEAD it doesn't work either (same error), but when I change
> intial parameters (though I shouldn't, it gives something very weird
> (negatives or sometimes huge values).
>
> Call:
> mle(minuslogl = (fr), start = list(a1 = 1, b1 = 1, a2 = 10, b2 = 10,
>    p = 0.9), method = "Nelder-Mead", control = list(fnscale = -1))
>
> Coefficients:
>        a1         b1         a2         b2          p
> -2.5035823  0.6236359 26.5562988 12.9604112 -0.1383767
>
> Thanks
>
>
>
> Ravi Varadhan wrote:
>>
>> Franco,
>> Is it possible that you have failed to provide the negative of
>> loglikelihood
>> to "optim", since optim, by default, minimizes a function?  If you want to
>> do this withput redefining the log-likelihood, you should set fnscale= -1
>> (as hinted by Prof. Ripley).  This would turn the problem into a
>> maximization problem.
>>
>> If this doesn't work, you should provide more details (a reproducible code
>> with actual error message).
>>
>> Ravi.
>>
>> ----------------------------------------------------------------------------
>> -------
>>
>> Ravi Varadhan, Ph.D.
>>
>> Assistant Professor, The Center on Aging and Health
>>
>> Division of Geriatric Medicine and Gerontology
>>
>> Johns Hopkins University
>>
>> Ph: (410) 502-2619
>>
>> Fax: (410) 614-9625
>>
>> Email: rvaradhan at jhmi.edu
>>
>> Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
>>
>>
>>
>> ----------------------------------------------------------------------------
>> --------
>>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of francogrex
>> Sent: Friday, January 05, 2007 10:42 AM
>> To: r-help at stat.math.ethz.ch
>> Subject: Re: [R] maximum likelihood estimation of 5 parameters
>>
>>
>>
>> Franco,
>> You can provide lower and upper bounds on the parameters if you use optim
>> with method="L-BFGS-B".
>> Hth, Ingmar
>>
>> Thanks, but when I use L-BFGS-B it tells me that there is an  error in
>> optim(start, f, method = method, hessian = TRUE, ...) : L-BFGS-B needs
>> finite values of 'fn'
>>
>> --
>> View this message in context:
>> http://www.nabble.com/maximum-likelihood-estimation-of-5-parameters-tf292536
>> 4.html#a8180120
>> Sent from the R help mailing list archive at Nabble.com.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
> -- 
> View this message in context: http://www.nabble.com/maximum-likelihood-estimation-of-5-parameters-tf2925364.html#a8186869
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0717


From gilbertg at musc.edu  Sat Jan  6 06:19:10 2007
From: gilbertg at musc.edu (gilbertg at musc.edu)
Date: Sat,  6 Jan 2007 00:19:10 -0500
Subject: [R] Bootstrapping Confidence Intervals for Medians
Message-ID: <20070106001910.3u6k0cyqs4844c0g@webmail.musc.edu>

I apologize for this post. I am new to R (two days) and I have tried and tried
to calculated confidence intervals for medians. Can someone help me?

Here is my data:

institution1
0.21
0.16
0.32
0.69
1.15
0.9
0.87
0.87
0.73

The first four observations compose group 1 and observations 5 through 9 compose
group 2. I would like to create a bootstrapped 90% confidence interval on the
difference of the medians (n2-n1). I have successfully calculated a permutation
test.

This shouldn't be as difficult as I am making it, would someone please enlighten
me?

Please reply to gilbertg at musc.edu as I have not subscribed to this list yet.

TIA,

Greg Gilbert, Faculty Research Associate
Department of Biostatistics, Bioinformatics, & Epidemiology
Medical University of South Carolina


From lu25 at stat.purdue.edu  Sat Jan  6 07:21:29 2007
From: lu25 at stat.purdue.edu (Zhenqiang Lu)
Date: Sat, 6 Jan 2007 01:21:29 -0500 (EST)
Subject: [R] help with gls
Message-ID: <Pine.A41.4.58.0701060117100.61942@odds.stat.purdue.edu>


Hello R-users,

I am using gls function in R to fit a model with certain correlation
structure.

The medol as:
fit.a<-gls(y~1,data=test.data,correlation=corAR1(form=~1|aa),method="ML")
mu<-summary(fit.a)$coefficient

With the toy data I made to test, the estimate of  mu is exactly equal to
the overall mean of y which can not be true.

But, if I make a toy data with y more than two replicates (for each level
of aa we have more than 2 abservations, for example N=4), the estimates of
mu will not be as the same as the overall mean of y.

Would you please tell me why this happens?
The following is my testing code.
Thanks.
James



require(mvtnorm)
rho=-0.5
N=2
sigma.a=2

Rho.a<-matrix(c(1,rho^(1:(N-1)))[outer(X=1:N,Y=1:N,function(x,y)
1+abs(x-y))],ncol=N)
Sigma.a<-sigma.a^2 * Rho/(1-rho.a^2)
y<-rep(0,N*20)
for (ii in 1:20)
 {y[((ii-1)*N+1):(ii*N)]<-rmvnorm(1, mean=rep(0,N), sigma=Sigma.a)
 }
test.data<-data.frame(y=y,aa=rep(1:20,each=N,len=N*20))
fit.a<-gls(y~1,data=test.data,correlation=corAR1(form=~1|aa),method="ML")
mu.a<-summary(fit.a)$coefficient
rho.a<-getVarCov(fit.a)[1,2]/getVarCov(fit.a)[1,1]
print(c(mean(y),mu.a))

______________________________________________________
Zhenqiang (James) Lu

Department of Statistics
Purdue University
West Lafayette, IN 47906
TEL: (765) 494-0027
FAX: (765) 494-0558


From ripley at stats.ox.ac.uk  Sat Jan  6 09:59:12 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 6 Jan 2007 08:59:12 +0000 (GMT)
Subject: [R] Bootstrapping Confidence Intervals for Medians
In-Reply-To: <20070106001910.3u6k0cyqs4844c0g@webmail.musc.edu>
References: <20070106001910.3u6k0cyqs4844c0g@webmail.musc.edu>
Message-ID: <Pine.LNX.4.64.0701060826350.1307@gannet.stats.ox.ac.uk>

On Sat, 6 Jan 2007, gilbertg at musc.edu wrote:

> I apologize for this post. I am new to R (two days) and I have tried and 
> tried to calculated confidence intervals for medians. Can someone help 
> me?

Later, you say you want a confidence interval for a difference in medians, 
not the same thing.

For medians, see MASS4 section 5.7 for worked examples and discussion of 
the pitfalls.

> Here is my data:
>
> institution1
> 0.21
> 0.16
> 0.32
> 0.69
> 1.15
> 0.9
> 0.87
> 0.87
> 0.73
>
> The first four observations compose group 1 and observations 5 through 9 
> compose group 2. I would like to create a bootstrapped 90% confidence 
> interval on the difference of the medians (n2-n1). I have successfully 
> calculated a permutation test.
>
> This shouldn't be as difficult as I am making it, would someone please 
> enlighten me?


It seems to me to be much more difficult than you have made it.  We need 
to know exactly what you mean by

> a bootstrapped 90% confidence interval on the difference of the medians

The 'standard' theory of bootstrap confidence intervals as implemented in 
e.g. package 'boot' is for a single-sample problem (and it would be 
pushing its justification very hard to use this for n=9).  But you have 
two samples, and haven't told us how you intend to bootstrap.  I guess you 
mean a stratified bootstrap, sampling with replacement independently from 
observations 1-4 and 5-9.  I don't know of theory for bootstrap confidence 
intervals from that scenario: do you?

Beyond this, there are considerable problems with bootstrapping medians in 
small samples as the median is a non-smooth function of the data and the 
bootstrap samples take very few values.  See for example the galaxies 
dataset as discussed in MASS4.  For the stratified bootstrapping I 
referred to, there are only a handful of possible values of each of the 
medians and so the bootstrap distribution is a highly non-uniform one on a 
few values.  E.g.

x <- c(0.21, 0.16, 0.32, 0.69)
y <- c(1.15, 0.9, 0.87, 0.87, 0.73)
z <- numeric(10000)
for(i in seq_len(10000))
z[i] <- median(sample(x, replace=TRUE)) - median(sample(y, replace=TRUE))

  -0.99 -0.965  -0.94  -0.91 -0.885  -0.83  -0.74 -0.725 -0.715  -0.71
     33     70     83     27    134     64    129     16    259    317
   -0.7  -0.69 -0.685  -0.66 -0.645 -0.635  -0.63 -0.605  -0.58  -0.57
     43    370    711   1064     70    538    455   1388    424     29
  -0.55 -0.545  -0.52  -0.49 -0.475 -0.465  -0.46  -0.45 -0.445  -0.42
    905     57     79     41     54    119     28    183    146    436
  -0.41 -0.395 -0.365 -0.305  -0.28 -0.225  -0.21  -0.18  -0.04
    100    290    759     13     34     64    117    323     28

You could use that table to give 'basic' or 'percentile' confidence 
intervals, if you have reason to believe in them.


> Greg Gilbert, Faculty Research Associate
> Department of Biostatistics, Bioinformatics, & Epidemiology
> Medical University of South Carolina

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From patrick.giraudoux at univ-fcomte.fr  Sat Jan  6 12:00:26 2007
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sat, 06 Jan 2007 12:00:26 +0100
Subject: [R] negative binomial family glm R and STATA
Message-ID: <459F814A.60501@univ-fcomte.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070106/0f203adf/attachment.pl 

From jholtman at gmail.com  Sat Jan  6 01:22:21 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 5 Jan 2007 19:22:21 -0500
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <Pine.LNX.4.43.0701051506140.3919@hymn04.u.washington.edu>
References: <200701051940.57136.rdiaz@cnio.es>
	<Pine.LNX.4.43.0701051506140.3919@hymn04.u.washington.edu>
Message-ID: <644e1f320701051622q5f5ae28cu8fecf4e538165c10@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070105/5a1e232e/attachment.pl 

From markus.jantti at iki.fi  Sat Jan  6 13:16:08 2007
From: markus.jantti at iki.fi (Markus =?ISO-8859-1?Q?J=E4ntti?=)
Date: Sat, 06 Jan 2007 14:16:08 +0200
Subject: [R] help with gls
In-Reply-To: <Pine.A41.4.58.0701060117100.61942@odds.stat.purdue.edu>
References: <Pine.A41.4.58.0701060117100.61942@odds.stat.purdue.edu>
Message-ID: <1168085768.4060.47.camel@pallas.pp.htv.fi>

On Sat, 2007-01-06 at 01:21 -0500, Zhenqiang Lu wrote:
> Hello R-users,
> 
> I am using gls function in R to fit a model with certain correlation
> structure.
> 
> The medol as:
> fit.a<-gls(y~1,data=test.data,correlation=corAR1(form=~1|aa),method="ML")
> mu<-summary(fit.a)$coefficient
> 
> With the toy data I made to test, the estimate of  mu is exactly equal to
> the overall mean of y which can not be true.
> 

On the contrary, this has to be true. 

Due to a well-known result (a recent is is Greene, 2003, sec 14.2.2, pp
343-344) when the covariates are the same in every equation, a
"seemingly unrelated regression" of this sort has GLS and OLS produce
identical results.   The OLS estimate is exactly the arithmetic
average. 
 
@Book{greene2003,
  Author         = {William H Greene},
  Title          = {Econometric Analysis},
  Publisher      = {Prentice-Hall International Ltd},
  Address        = {London},
  Edition        = {Fifth},
  year           = 2003,
}

> But, if I make a toy data with y more than two replicates (for each level
> of aa we have more than 2 abservations, for example N=4), the estimates of
> mu will not be as the same as the overall mean of y.

This is what is strange. You should probably provide example code of
this too. 

> 
> Would you please tell me why this happens?
> The following is my testing code.

The code had some typos in it (and you did not load nlme). I believe
this would be correct:

require(mvtnorm)
library(nlme)
rho=-0.5
N=2
sigma.a=2

Rho.a<-matrix(c(1,rho^(1:(N-1)))[outer(X=1:N,Y=1:N,function(x,y)
1+abs(x-y))],ncol=N)
Sigma.a<-sigma.a^2 * Rho.a/(1-rho^2)
y<-rep(0,N*20)
for (ii in 1:20)
 {y[((ii-1)*N+1):(ii*N)]<-rmvnorm(1, mean=rep(0,N), sigma=Sigma.a)
 }
test.data<-data.frame(y=y,aa=rep(1:20,each=N,len=N*20))
fit.a<-gls(y~1,data=test.data,correlation=corAR1(form=~1|
aa),method="ML")
mu.a<-summary(fit.a)$coefficient
rho.a<-getVarCov(fit.a)[1,2]/getVarCov(fit.a)[1,1]
print(c(mean(y),mu.a))

Regards,

Markus

> Thanks.
> James
> 
> 
> 
> require(mvtnorm)
> rho=-0.5
> N=2
> sigma.a=2
> 
> Rho.a<-matrix(c(1,rho^(1:(N-1)))[outer(X=1:N,Y=1:N,function(x,y)
> 1+abs(x-y))],ncol=N)
> Sigma.a<-sigma.a^2 * Rho/(1-rho.a^2)
> y<-rep(0,N*20)
> for (ii in 1:20)
>  {y[((ii-1)*N+1):(ii*N)]<-rmvnorm(1, mean=rep(0,N), sigma=Sigma.a)
>  }
> test.data<-data.frame(y=y,aa=rep(1:20,each=N,len=N*20))
> fit.a<-gls(y~1,data=test.data,correlation=corAR1(form=~1|aa),method="ML")
> mu.a<-summary(fit.a)$coefficient
> rho.a<-getVarCov(fit.a)[1,2]/getVarCov(fit.a)[1,1]
> print(c(mean(y),mu.a))
> 
> ______________________________________________________
> Zhenqiang (James) Lu
> 
> Department of Statistics
> Purdue University
> West Lafayette, IN 47906
> TEL: (765) 494-0027
> FAX: (765) 494-0558
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
-- 
Markus Jantti
Abo Akademi University
markus.jantti at iki.fi
http://www.iki.fi/~mjantti


From gavin.simpson at ucl.ac.uk  Sat Jan  6 13:55:47 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Sat, 06 Jan 2007 12:55:47 +0000
Subject: [R] listing all functions in R
Message-ID: <1168088147.3149.6.camel@dhcppc2.my.nat.localnet>

Dear List,

I'm building an R syntax highlighting file for GeSHi [*] for a website I
am currently putting together. The syntax file needs a list of keywords
to highlight. How can I generate a list of all the functions in a base R
installation?

Ideally the list would be formatted like this:

"'fun1', 'fun2', 'fun3'" 

when printed to the screen so I can copy and paste it into the syntax
file.

I'm sure this has been asked before, but I stupidly didn't save that
email and I couldn't come up with a suitable query parameter for
Jonathan Baron's search site to return results before timing out.

Thanks in advance,

Gav
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT                          [w] http://www.freshwaters.org.uk/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From rdiaz02 at gmail.com  Sat Jan  6 14:29:59 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Sat, 6 Jan 2007 14:29:59 +0100
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <644e1f320701051101n22b29616t393a3161ebbd8d4@mail.gmail.com>
References: <008f01c730f8$4eafb870$4d908980@gne.windows.gene.com>
	<200701051943.17822.rdiaz@cnio.es>
	<644e1f320701051101n22b29616t393a3161ebbd8d4@mail.gmail.com>
Message-ID: <624934630701060529m65aad5dcwfbeca39e1bc019e9@mail.gmail.com>

On 1/5/07, jim holtman <jholtman at gmail.com> wrote:
> The other reason for considering which of the different approaches to use
> would be performance:
>
> > f.1 <- function(x) x+1
> > f.2 <- function(x) x+2
> >
> > system.time({
> +     for (i in 1:100000){
> +         eval(parse(text=paste('f.', i%%2+1, sep='')))(i)
> +     }
>  + })
> [1] 6.96 0.00 8.32   NA   NA
> >
> > system.time({
> +     for (i in 1:100000){
> +         {if(i %% 2 == 0) f.1 else f.2}(i)
> +     }
> + })
> [1] 0.52 0.00 0.61   NA   NA
> >
> >
>
> eval(parse...) seems to be an order of magnitude slower.  It would make a
> difference if you were calling it several thousand times; so it depends on
> your application.

Yes, that is true, thanks. Note, though, that in my case I am more
likely to do the "eval(parse(" and pasting only once, and then call
the new function thousands of times; something more like your second
version than the first.


g <- function(x, fpost) {
   calledf <- eval(parse(text = paste("f.", fpost, sep = "")))
   calledf(x)
   ## the thousands of calls to calledf go here
}


R.

>
>
> On 1/5/07, Ramon Diaz-Uriarte <rdiaz at cnio.es> wrote:
> >
> > On Friday 05 January 2007 19:35, Bert Gunter wrote:
> > > ??
> > >
> > > Or to add to what Peter Dalgaard said... (perhaps for the case of many
> more
> > > functions)
> > >
> > > Why eval(parse())? What's wrong with if then?
> > >
> > > g <- function(fpost,x){if(fpost==1)f.1 else f.2 }(x)
> > >
> > > or switch() if you have more than 2 possible arguments? I think your
> > > remarks reinforce the wisdom of Thomas's "axiom" .
> >
> > Thanks, Bert, but as with Peter's solution, your solution forces me to
> build g
> > ahead of time. And again, I am not sure I see why the attempt to avoid
> > eval(parse(text.
> >
> > Best,
> >
> > R.
> >
> >
> > >
> > > Bert Gunter
> > > Genentech Nonclinical Statistics
> > > South San Francisco, CA 94404
> > >
> > >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> Ramon Diaz-Uriarte
> > > Sent: Friday, January 05, 2007 10:02 AM
> > > To: r-help; rdiaz02 at gmail.com
> > > Subject: [R] eval(parse(text vs. get when accessing a function
> > >
> > > Dear All,
> > >
> > > I've read Thomas Lumley's fortune "If the answer is parse() you should
> > > usually
> > > rethink the question.". But I am not sure it that also applies (and why)
> to
> > > other situations (Lumley's comment
> > > http://tolstoy.newcastle.edu.au/R/help/05/02/12204.html
> > > was in reply to accessing a list).
> > >
> > > Suppose I have similarly called functions, except for a postfix. E.g.
> > >
> > > f.1 <- function(x) {x + 1}
> > > f.2 <- function(x) {x + 2}
> > >
> > > And sometimes I want to call f.1 and some other times f.2 inside another
> > > function. I can either do:
> > >
> > > g <- function(x, fpost) {
> > >     calledf <- eval(parse(text = paste("f.", fpost, sep = "")))
> > >     calledf(x)
> > >     ## do more stuff
> > > }
> > >
> > >
> > > Or:
> > >
> > > h <- function(x, fpost) {
> > >     calledf <- get(paste("f.", fpost, sep = ""))
> > >     calledf(x)
> > >     ## do more stuff
> > > }
> > >
> > >
> > > Two questions:
> > > 1) Why is the second better?
> > >
> > > 2) By changing g or h I could use "do.call" instead; why would that be
> > > better?
> > > Because I can handle differences in argument lists?
> > >
> > >
> > >
> > > Thanks,
> > >
> > >
> > > R.
> >
> > --
> > Ram?n D?az-Uriarte
> > Centro Nacional de Investigaciones Oncol?gicas (CNIO)
> > (Spanish National Cancer Center)
> > Melchor Fern?ndez Almagro, 3
> > 28029 Madrid (Spain)
> > Fax: +-34-91-224-6972
> > Phone: +-34-91-224-6900
> >
> > http://ligarto.org/rdiaz
> > PGP KeyID: 0xE89B3462
> > (http://ligarto.org/rdiaz/0xE89B3462.asc )
> >
> >
> >
> > **NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From jajcardinale at gmail.com  Sat Jan  6 14:34:07 2007
From: jajcardinale at gmail.com (John Cardinale)
Date: Sat, 6 Jan 2007 08:34:07 -0500
Subject: [R] ANCOVA
Message-ID: <6d74b8030701060534m5dfbb6bdi2349c4f5de6cbfb1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070106/d4c5ee98/attachment.pl 

From ripley at stats.ox.ac.uk  Sat Jan  6 14:48:15 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 6 Jan 2007 13:48:15 +0000 (GMT)
Subject: [R] listing all functions in R
In-Reply-To: <1168088147.3149.6.camel@dhcppc2.my.nat.localnet>
References: <1168088147.3149.6.camel@dhcppc2.my.nat.localnet>
Message-ID: <Pine.LNX.4.64.0701061331420.8651@gannet.stats.ox.ac.uk>

Could you tell us what you mean by

- 'function'  (if() and + are functions in R, so do you want those?)

- 'a base R installation'?   What is 'base R' (standard + recommended 
packages?)  And on what platform: the list is platform-specific?

Here is a reasonable shot:

findfuns <- function(x) {
     if(require(x, character.only=TRUE)) {
        env <- paste("package", x, sep=":")
        nm <- ls(env, all=TRUE)
        nm[unlist(lapply(nm, function(n) exists(n, where=env,
                                               mode="function",
                                               inherits=FALSE)))]
     } else character(0)
}
pkgs <- dir(.Library)
z <-  lapply(pkgs, findfuns)
names(z) <- pkgs

I don't understand your desired format, but

write(sQuote(sort(unique(unlist(z)))), "")

gives a single-column quoted list.  It does include internal functions, 
operators, S3 methods ... so you probably want to edit it.


On Sat, 6 Jan 2007, Gavin Simpson wrote:

> Dear List,
>
> I'm building an R syntax highlighting file for GeSHi [*] for a website I
> am currently putting together. The syntax file needs a list of keywords
> to highlight. How can I generate a list of all the functions in a base R
> installation?
>
> Ideally the list would be formatted like this:
>
> "'fun1', 'fun2', 'fun3'"
>
> when printed to the screen so I can copy and paste it into the syntax
> file.
>
> I'm sure this has been asked before, but I stupidly didn't save that
> email and I couldn't come up with a suitable query parameter for
> Jonathan Baron's search site to return results before timing out.
>
> Thanks in advance,
>
> Gav
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ggrothendieck at gmail.com  Sat Jan  6 15:03:56 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 6 Jan 2007 09:03:56 -0500
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <644e1f320701051101n22b29616t393a3161ebbd8d4@mail.gmail.com>
References: <008f01c730f8$4eafb870$4d908980@gne.windows.gene.com>
	<200701051943.17822.rdiaz@cnio.es>
	<644e1f320701051101n22b29616t393a3161ebbd8d4@mail.gmail.com>
Message-ID: <971536df0701060603o60587f55ud8ade265a2679446@mail.gmail.com>

Here are a few more timing alternatives from slowest to
fastest.  Of the ones that dynamically construct the name
of the frunction (all except last example), get seems to be
the fastest although its still 6x slower than the if statement.

> f.1 <- function(x) x+1
> f.2 <- function(x) x+2
>
> n <- 10^4; i <- 1; x <- 10
>
> system.time(
+     replicate(n, eval(parse(text=paste('f.', i, sep='')))(x))
+ )
[1] 1.27 0.00 1.31   NA   NA
>
> system.time(
+     replicate(n, match.fun(paste("f.", i, sep=''))(x) )
+ )
[1] 1.15 0.01 1.20   NA   NA
>
> system.time(
+     replicate(n, do.call(paste("f.", i, sep=''), list(x)) )
+ )
[1] 0.99 0.00 1.02   NA   NA
>
> system.time(
+     replicate(n, get(paste("f.", i, sep=''))(x) )
+ )
[1] 0.93 0.00 0.94   NA   NA
>
> system.time(
+     replicate(n, {if(i %% 2 == 0) f.1 else f.2}(x) )
+ )
[1] 0.15 0.00 0.15   NA   NA



On 1/5/07, jim holtman <jholtman at gmail.com> wrote:
> The other reason for considering which of the different approaches to use
> would be performance:
>
> > f.1 <- function(x) x+1
> > f.2 <- function(x) x+2
> >
> > system.time({
> +     for (i in 1:100000){
> +         eval(parse(text=paste('f.', i%%2+1, sep='')))(i)
> +     }
> + })
> [1] 6.96 0.00 8.32   NA   NA
> >
> > system.time({
> +     for (i in 1:100000){
> +         {if(i %% 2 == 0) f.1 else f.2}(i)
> +     }
> + })
> [1] 0.52 0.00 0.61   NA   NA
> >
> >
>
> eval(parse...) seems to be an order of magnitude slower.  It would make a
> difference if you were calling it several thousand times; so it depends on
> your application.
>
>
> On 1/5/07, Ramon Diaz-Uriarte <rdiaz at cnio.es> wrote:
> >
> > On Friday 05 January 2007 19:35, Bert Gunter wrote:
> > > ??
> > >
> > > Or to add to what Peter Dalgaard said... (perhaps for the case of many
> > more
> > > functions)
> > >
> > > Why eval(parse())? What's wrong with if then?
> > >
> > > g <- function(fpost,x){if(fpost==1)f.1 else f.2 }(x)
> > >
> > > or switch() if you have more than 2 possible arguments? I think your
> > > remarks reinforce the wisdom of Thomas's "axiom" .
> >
> > Thanks, Bert, but as with Peter's solution, your solution forces me to
> > build g
> > ahead of time. And again, I am not sure I see why the attempt to avoid
> > eval(parse(text.
> >
> > Best,
> >
> > R.
> >
> >
> > >
> > > Bert Gunter
> > > Genentech Nonclinical Statistics
> > > South San Francisco, CA 94404
> > >
> > >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ramon
> > Diaz-Uriarte
> > > Sent: Friday, January 05, 2007 10:02 AM
> > > To: r-help; rdiaz02 at gmail.com
> > > Subject: [R] eval(parse(text vs. get when accessing a function
> > >
> > > Dear All,
> > >
> > > I've read Thomas Lumley's fortune "If the answer is parse() you should
> > > usually
> > > rethink the question.". But I am not sure it that also applies (and why)
> > to
> > > other situations (Lumley's comment
> > > http://tolstoy.newcastle.edu.au/R/help/05/02/12204.html
> > > was in reply to accessing a list).
> > >
> > > Suppose I have similarly called functions, except for a postfix. E.g.
> > >
> > > f.1 <- function(x) {x + 1}
> > > f.2 <- function(x) {x + 2}
> > >
> > > And sometimes I want to call f.1 and some other times f.2 inside another
> > > function. I can either do:
> > >
> > > g <- function(x, fpost) {
> > >     calledf <- eval(parse(text = paste("f.", fpost, sep = "")))
> > >     calledf(x)
> > >     ## do more stuff
> > > }
> > >
> > >
> > > Or:
> > >
> > > h <- function(x, fpost) {
> > >     calledf <- get(paste("f.", fpost, sep = ""))
> > >     calledf(x)
> > >     ## do more stuff
> > > }
> > >
> > >
> > > Two questions:
> > > 1) Why is the second better?
> > >
> > > 2) By changing g or h I could use "do.call" instead; why would that be
> > > better?
> > > Because I can handle differences in argument lists?
> > >
> > >
> > >
> > > Thanks,
> > >
> > >
> > > R.
> >
> > --
> > Ram?n D?az-Uriarte
> > Centro Nacional de Investigaciones Oncol?gicas (CNIO)
> > (Spanish National Cancer Center)
> > Melchor Fern?ndez Almagro, 3
> > 28029 Madrid (Spain)
> > Fax: +-34-91-224-6972
> > Phone: +-34-91-224-6900
> >
> > http://ligarto.org/rdiaz
> > PGP KeyID: 0xE89B3462
> > (http://ligarto.org/rdiaz/0xE89B3462.asc)
> >
> >
> >
> > **NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?
>
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From rdiaz02 at gmail.com  Sat Jan  6 15:16:51 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Sat, 6 Jan 2007 15:16:51 +0100
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB77C654@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBB77C654@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <624934630701060616i2718ab71xb981fbca39c0c910@mail.gmail.com>

Dear Greg,


On 1/5/07, Greg Snow <Greg.Snow at intermountainmail.org> wrote:
> Ramon,
>
> I prefer to use the list method for this type of thing, here are a couple of reasons why (maybe you are more organized than me and would never do some of the stupid things that I have, so these don't apply to you, but you can see that the general suggestion applys to some of the rest of us).
>


Those suggestions do apply to me of course (no claim to being
organized nor beyond idiocy here). And actually the suggestions on
this thread are being very useful. I think, though, that I was not
very clear on the context and my examples were too dumbed down. So
I'll try to give more detail (nothing here is secret, I am just trying
not to bore people).

The code is part of a web-based application, so there is no
interactive user. The R code is passed the arguments (and optional
user functions) from the CGI.

There is one "core" function (call it cvFunct) that, among other
things, does cross-validation. So this is one way to do things:

cvFunct <- function(whatever, genefiltertype, whateverelse) {
      internalGeneSelect <- eval(parse(text = paste("geneSelect",
                                             genefiltertype, sep = ".")))

      ## do things calling internalGeneSelect,
}

and now define all possible functions as

geneSelect.Fratio <- function(x, y, z) {##something}
geneSelect.Wilcoxon <- function(x, y, z) {## something else}

If I want more geneSelect functions, adding them is simple. And I can
even allow the user to pass her/his own functions, with the only
restriction that it takes three args, x, y, z, and that the function
is to be called: "geneSelect." and a user choosen string. (Yes, I need
to make sure no calls to "system", etc, are in the user code, etc,
etc, but that is another issue).

The general idea is not new of course. For instance, in package
"e1071", a somewhat similar thing is done in function "tune", and
David Meyer there uses "do.call". However, tune is a lot more general
than what I had in mind. For instance, "tune" deals with arbitrary
functions, with arbitrary numbers and names of parameters, whereas my
functions above all take only three arguments (x: a matrix, y: a
vector; z: an integer), so the neat functionality provided by
"do.call", and passing the args as a list is not really needed.

So, given that my situation is so structured, and I do not need
"do.call", I think the approach via eval(parse(paste makes my life
simple:

a) the central function (cvFunct) uses something I can easily
recognize: "internalGeneSelect"

b) after the initial eval(parse(text I do not need to worry anymore
about what the "true" gene selection function is called

c) adding new functions and calling them is simple: function naming
follows a simple pattern ("geneSelect." + postfix) and calling the
user function only requires passing the postfix to cvFunct.

d) notice also that, at least the functs. I define, will of course not
be named "f.1", etc, but rather things like "geneSelect.Fratio" or
"geneSelect.namesThatStartWithCuteLetters";

I hope this makes things more clear. I did not include this detail
because this is probably boring (I guess most of you have stopped
reading by now :-).


> Using the list forces you to think about what functions may be called and thinking about things before doing them is usually a good idea.  Personally I don't trust the user of my functions (usually my future self who has forgotten something that seemed obvious at the time) to not do something stupid with them.
>
> With list elements you can have names for the functions and access them either by the name or by a number, I find that a lot easier when I go back to edit/update than to remember which function f.1 or f.2 did what.
>

But I don't see how having your functions as list elements is easier
(specially if the function is longer than 2 to 3 lines) than having
all functions systematically named things such as:

geneSelect.Fratio
geneSelect.Random
geneSelect.LetterA
etc

Of course, I could have a list with the components named "Fratio"
"Random", "LetterA". But I fail to see what it adds. And it forces me
to build the list, and probably rebuild it whe (or not build it until)
the user enters her/his own selection function. But the later I do not
need to do with the scheme above.


> With your function, what if the user runs:
>
> > g(5,3)
>
> What should it do?  (you have only shown definitions for f.1 and f.2).  With my luck I would accidentily type that and just happen to have a f.3 function sitting around from a previous project that does something that I really don't want it to do now.  If I use the list approach then I will get a subscript out of bounds error rather than running something unintended.
>
>

I see the general concern, but not how it applies here. If I pass
argument "Fratio" then either I use geneSelect.Fratio or I get an
error if "geneSelect.Fratio" does not exist. Similar to what would
happen if I do

g1(2, 8)

when f.8 is not defined:

Error in eval(expr, envir, enclos) : object "f.8" not found
So even in more general cases, except for function redefinitions, etc,
you are not able to call non-existent stuff.

> 2nd, If I used the eval-parse approach then I would probably at some point redefine f.1 or f.2 to the output of a regression analysis or something, then go back and run the g function at a later time and wonder why I am getting an error, then once I have finally figured it out, now I need to remember what f.1 did and rewrite it again.  I am much less likely to accidentally replace an element of a list, and if the list is well named I am unlikely to replace the whole list by accident.
>
>

Yes, that is true. Again, it does not apply to the actual case I have
in mind, but of course, without the detailed info on context I just
gave, you could not know that.


> 3rd, If I ever want to use this code somewhere else (new version of R, on the laptop, give to coworker, ...), it is a lot easier to save and load a single list than to try to think of all the functions that need to be saved.
>

Oh, sure. But all the functions above live in a single file (actually,
a minipackage) except for the optional use function (which is read
from a file).


>
> Personally I have never regretted trying not to underestimate my own future stupidity.
>

Neither do I. And actually, that is why I asked: if Thomas Lumley
said, in the fortune, that I better rethink about it, then I should
try rethinking about it. But I asked because I failed to see what the
problem is.


> Hope this helps,
>

It certainly does.


Best,

R.


> --
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at intermountainmail.org
> (801) 408-8111
>
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ramon
> > Diaz-Uriarte
> > Sent: Friday, January 05, 2007 11:41 AM
> > To: Peter Dalgaard
> > Cc: r-help; rdiaz02 at gmail.com
> > Subject: Re: [R] eval(parse(text vs. get when accessing a function
> >
> > On Friday 05 January 2007 19:21, Peter Dalgaard wrote:
> > > Ramon Diaz-Uriarte wrote:
> > > > Dear All,
> > > >
> > > > I've read Thomas Lumley's fortune "If the answer is parse() you
> > > > should usually rethink the question.". But I am not sure it that
> > > > also applies (and why) to other situations (Lumley's comment
> > > > http://tolstoy.newcastle.edu.au/R/help/05/02/12204.html
> > > > was in reply to accessing a list).
> > > >
> > > > Suppose I have similarly called functions, except for a
> > postfix. E.g.
> > > >
> > > > f.1 <- function(x) {x + 1}
> > > > f.2 <- function(x) {x + 2}
> > > >
> > > > And sometimes I want to call f.1 and some other times f.2 inside
> > > > another function. I can either do:
> > > >
> > > > g <- function(x, fpost) {
> > > >     calledf <- eval(parse(text = paste("f.", fpost, sep = "")))
> > > >     calledf(x)
> > > >     ## do more stuff
> > > > }
> > > >
> > > >
> > > > Or:
> > > >
> > > > h <- function(x, fpost) {
> > > >     calledf <- get(paste("f.", fpost, sep = ""))
> > > >     calledf(x)
> > > >     ## do more stuff
> > > > }
> > > >
> > > >
> > > > Two questions:
> > > > 1) Why is the second better?
> > > >
> > > > 2) By changing g or h I could use "do.call" instead; why
> > would that
> > > > be better? Because I can handle differences in argument lists?
> >
> > Dear Peter,
> >
> > Thanks for your answer.
> >
> > >
> > > Who says that they are better?  If the question is how to call a
> > > function specified by half of its name, the answer could well be to
> > > use parse(), the point is that you should rethink whether that was
> > > really the right question.
> > >
> > > Why not instead, e.g.
> > >
> > > f <- list("1"=function(x) {x + 1} , "2"=function(x) {x + 2}) h <-
> > > function(x, fpost) f[[fpost]](x)
> > >
> > > > h(2,"2")
> > >
> > > [1] 4
> > >
> > > > h(2,"1")
> > >
> > > [1] 3
> > >
> >
> > I see, this is direct way of dealing with the problem.
> > However, you first need to build the f list, and you might
> > not know about that ahead of time. For instance, if I build a
> > function so that the only thing that you need to do to use my
> > function g is to call your function "f.something", and then
> > pass the "something".
> >
> > I am still under the impression that, given your answer,
> > using "eval(parse(text" is not your preferred way.  What are
> > the possible problems (if there are any, that is). I guess I
> > am puzzled by "rethink whether that was really the right question".
> >
> >
> > Thanks,
> >
> > R.
> >
> >
> >
> >
> >
> >
> >
> > > > Thanks,
> > > >
> > > >
> > > > R.
> >
> > --
> > Ram?n D?az-Uriarte
> > Centro Nacional de Investigaciones Oncol?gicas (CNIO)
> > (Spanish National Cancer Center) Melchor Fern?ndez Almagro, 3
> > 28029 Madrid (Spain)
> > Fax: +-34-91-224-6972
> > Phone: +-34-91-224-6900
> >
> > http://ligarto.org/rdiaz
> > PGP KeyID: 0xE89B3462
> > (http://ligarto.org/rdiaz/0xE89B3462.asc)
> >
> >
> >
> > **NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en
> > s...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From gavin.simpson at ucl.ac.uk  Sat Jan  6 15:25:36 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Sat, 06 Jan 2007 14:25:36 +0000
Subject: [R] listing all functions in R
In-Reply-To: <Pine.LNX.4.64.0701061331420.8651@gannet.stats.ox.ac.uk>
References: <1168088147.3149.6.camel@dhcppc2.my.nat.localnet>
	<Pine.LNX.4.64.0701061331420.8651@gannet.stats.ox.ac.uk>
Message-ID: <1168093536.3149.25.camel@dhcppc2.my.nat.localnet>

On Sat, 2007-01-06 at 13:48 +0000, Prof Brian Ripley wrote:
> Could you tell us what you mean by

Thank you for your reply, Prof. Ripley.

> 
> - 'function'  (if() and + are functions in R, so do you want those?)

I was thinking about functions that are used like this: foo()
So I don't need things like "names<-". I don't need functions like +. -,
$, as I can highlight the separately if desired, though I'm not doing
this at the moment.

Functions like for() while(), if() function() are handled separately.

> 
> - 'a base R installation'?   What is 'base R' (standard + recommended 
> packages?)  And on what platform: the list is platform-specific?

Yes, I mean standard + recommended packages. As for platform, most of my
intended audience will be MS Windows users, though I am using Linux
(Fedora) to generate this list (i.e. my R installation is on Linux).

> 
> Here is a reasonable shot:
> 
> findfuns <- function(x) {
>      if(require(x, character.only=TRUE)) {
>         env <- paste("package", x, sep=":")
>         nm <- ls(env, all=TRUE)
>         nm[unlist(lapply(nm, function(n) exists(n, where=env,
>                                                mode="function",
>                                                inherits=FALSE)))]
>      } else character(0)
> }
> pkgs <- dir(.Library)
> z <-  lapply(pkgs, findfuns)
> names(z) <- pkgs

Excellent, that works just fine for me. I can edit out certain packages
that I don't expect to use, before formatting as desired. I can also use
this function on a library of packages that I use regularly and will be
using in the web pages.

> 
> I don't understand your desired format, but
> 
> write(sQuote(sort(unique(unlist(z)))), "")

I wanted a single string "...", with entries enclosed in "''" and
separated by "," (this is to go in a PHP array). I can generate such a
string from your z, above, as follows:

paste(sQuote(sort(unique(unlist(z)), decreasing = TRUE)), 
      collapse = ", ")

> 
> gives a single-column quoted list.  It does include internal functions, 
> operators, S3 methods ... so you probably want to edit it.

Once again, thank you.

All the best

Gav

> 
> 
> On Sat, 6 Jan 2007, Gavin Simpson wrote:
> 
> > Dear List,
> >
> > I'm building an R syntax highlighting file for GeSHi [*] for a website I
> > am currently putting together. The syntax file needs a list of keywords
> > to highlight. How can I generate a list of all the functions in a base R
> > installation?
> >
> > Ideally the list would be formatted like this:
> >
> > "'fun1', 'fun2', 'fun3'"
> >
> > when printed to the screen so I can copy and paste it into the syntax
> > file.
> >
> > I'm sure this has been asked before, but I stupidly didn't save that
> > email and I couldn't come up with a suitable query parameter for
> > Jonathan Baron's search site to return results before timing out.
> >
> > Thanks in advance,
> >
> > Gav
> >
> 
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT                          [w] http://www.freshwaters.org.uk/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From rdiaz02 at gmail.com  Sat Jan  6 15:30:16 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Sat, 6 Jan 2007 15:30:16 +0100
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <Pine.LNX.4.43.0701051506140.3919@hymn04.u.washington.edu>
References: <200701051940.57136.rdiaz@cnio.es>
	<Pine.LNX.4.43.0701051506140.3919@hymn04.u.washington.edu>
Message-ID: <624934630701060630j7bf7d517wa5e9cb79762573c7@mail.gmail.com>

On 1/6/07, Thomas Lumley <tlumley at u.washington.edu> wrote:
> On Fri, 5 Jan 2007, Ramon Diaz-Uriarte wrote:
>
> > I see, this is direct way of dealing with the problem. However, you first need
> > to build the f list, and you might not know about that ahead of time. For
> > instance, if I build a function so that the only thing that you need to do to
> > use my function g is to call your function "f.something", and then pass
> > the "something".
> >
> > I am still under the impression that, given your answer,
> > using "eval(parse(text" is not your preferred way.  What are the possible
> > problems (if there are any, that is). I guess I am puzzled by "rethink
> > whether that was really the right question".
> >
>
> There are definitely situations where parse() is necessary or convenient,
> or we wouldn't provide it. For example, there are some formula-manipulation problems where it really does seem to be the best solution.
>
> The point of my observation was that it is relatively common for people to ask about parse() solutions to problems, but relatively rare to see them in code by experienced R programmers.  The 'rethink the question' point is that a narrowly-posed programming problem may suggest parse() as the answer, when thinking more broadly about what you are trying to do may allow a completely different approach [the example of lists is a common one].
>

Yes, the general thing I am trying to do do ---see my response to Greg
Snow for details--- has been done before. And I looked at code from
more experienced programmers, such as David Meyer's tune in e1071. I
think one of the reasons David is using do.call is that he allows to
use arbitrary functions, whereas I do not (currently) need that
functionality.

Thus, instead of calling "do.call(whatever)" I can call
"internalGeneSelect". And, when reading my code, or debugging, it is
easier for me to quickly decode "internalGeneSelect" ("oh, yes,
calling the geneSelection function") than decode "do.call".

But my "internalGeneSelect" depends on "eval(parse(text = " and that
is where my doubts started.

Because of this thread, though, I am actually starting to think I
should go ahead and use "do.call", because it will make life simpler
if someone (including myself) decides to extend the code.  I guess
this can be a case of "thinking more broadly".


> The problem with eval(parse()) is not primarily one of speed.  A problem with parse() is than manipulating text strings is easy to mess up, since text has so much less structure than code. A problem with eval() is that it is too powerful -- since it can do anything, it is harder to keep track of what it is doing.
>

Yes, I understand that. In my specific case, though, there is quite a
high degree of structure on the text used. And I felt that do.call was
also very powerful (and I've messed with "..." in similar situations
in the past).


> In one sense this is just a style issue, but I still think my comment is good advice. If you find yourself wanting to use parse() it is a good idea to stop and think about whether there is a better way to do it. Often, there is. Sometimes, there isn't.
>

Thanks for your comments. I think here "do.call" might actually be the
way to go.


Best,

R.





>
>         -thomas
>
> Thomas Lumley                   Assoc. Professor, Biostatistics
> tlumley at u.washington.edu        University of Washington, Seattle
>
>
>


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From ggrothendieck at gmail.com  Sat Jan  6 15:31:20 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 6 Jan 2007 09:31:20 -0500
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <624934630701060616i2718ab71xb981fbca39c0c910@mail.gmail.com>
References: <07E228A5BE53C24CAD490193A7381BBB77C654@LP-EXCHVS07.CO.IHC.COM>
	<624934630701060616i2718ab71xb981fbca39c0c910@mail.gmail.com>
Message-ID: <971536df0701060631q43e8c073p55d96884c68e56c1@mail.gmail.com>

I guess the problems with eval(parse(...)) are:

- speed
- verbosity
- security
- length of string to be parsed

get is faster, less verbose and safer than eval(parse(...)) in this case.

The length of string problem seems not applicable here but I have enountered
one situation where it was.  gsubfn in package gsubfn does string replacement.
I found that one user wanted to use it on strings that were ~ 25000 characters
long and parse could not handle that length.  I removed parse replacing it with
other constructs and now they can be handled.


On 1/6/07, Ramon Diaz-Uriarte <rdiaz02 at gmail.com> wrote:
> Dear Greg,
>
>
> On 1/5/07, Greg Snow <Greg.Snow at intermountainmail.org> wrote:
> > Ramon,
> >
> > I prefer to use the list method for this type of thing, here are a couple of reasons why (maybe you are more organized than me and would never do some of the stupid things that I have, so these don't apply to you, but you can see that the general suggestion applys to some of the rest of us).
> >
>
>
> Those suggestions do apply to me of course (no claim to being
> organized nor beyond idiocy here). And actually the suggestions on
> this thread are being very useful. I think, though, that I was not
> very clear on the context and my examples were too dumbed down. So
> I'll try to give more detail (nothing here is secret, I am just trying
> not to bore people).
>
> The code is part of a web-based application, so there is no
> interactive user. The R code is passed the arguments (and optional
> user functions) from the CGI.
>
> There is one "core" function (call it cvFunct) that, among other
> things, does cross-validation. So this is one way to do things:
>
> cvFunct <- function(whatever, genefiltertype, whateverelse) {
>      internalGeneSelect <- eval(parse(text = paste("geneSelect",
>                                             genefiltertype, sep = ".")))
>
>      ## do things calling internalGeneSelect,
> }
>
> and now define all possible functions as
>
> geneSelect.Fratio <- function(x, y, z) {##something}
> geneSelect.Wilcoxon <- function(x, y, z) {## something else}
>
> If I want more geneSelect functions, adding them is simple. And I can
> even allow the user to pass her/his own functions, with the only
> restriction that it takes three args, x, y, z, and that the function
> is to be called: "geneSelect." and a user choosen string. (Yes, I need
> to make sure no calls to "system", etc, are in the user code, etc,
> etc, but that is another issue).
>
> The general idea is not new of course. For instance, in package
> "e1071", a somewhat similar thing is done in function "tune", and
> David Meyer there uses "do.call". However, tune is a lot more general
> than what I had in mind. For instance, "tune" deals with arbitrary
> functions, with arbitrary numbers and names of parameters, whereas my
> functions above all take only three arguments (x: a matrix, y: a
> vector; z: an integer), so the neat functionality provided by
> "do.call", and passing the args as a list is not really needed.
>
> So, given that my situation is so structured, and I do not need
> "do.call", I think the approach via eval(parse(paste makes my life
> simple:
>
> a) the central function (cvFunct) uses something I can easily
> recognize: "internalGeneSelect"
>
> b) after the initial eval(parse(text I do not need to worry anymore
> about what the "true" gene selection function is called
>
> c) adding new functions and calling them is simple: function naming
> follows a simple pattern ("geneSelect." + postfix) and calling the
> user function only requires passing the postfix to cvFunct.
>
> d) notice also that, at least the functs. I define, will of course not
> be named "f.1", etc, but rather things like "geneSelect.Fratio" or
> "geneSelect.namesThatStartWithCuteLetters";
>
> I hope this makes things more clear. I did not include this detail
> because this is probably boring (I guess most of you have stopped
> reading by now :-).
>
>
> > Using the list forces you to think about what functions may be called and thinking about things before doing them is usually a good idea.  Personally I don't trust the user of my functions (usually my future self who has forgotten something that seemed obvious at the time) to not do something stupid with them.
> >
> > With list elements you can have names for the functions and access them either by the name or by a number, I find that a lot easier when I go back to edit/update than to remember which function f.1 or f.2 did what.
> >
>
> But I don't see how having your functions as list elements is easier
> (specially if the function is longer than 2 to 3 lines) than having
> all functions systematically named things such as:
>
> geneSelect.Fratio
> geneSelect.Random
> geneSelect.LetterA
> etc
>
> Of course, I could have a list with the components named "Fratio"
> "Random", "LetterA". But I fail to see what it adds. And it forces me
> to build the list, and probably rebuild it whe (or not build it until)
> the user enters her/his own selection function. But the later I do not
> need to do with the scheme above.
>
>
> > With your function, what if the user runs:
> >
> > > g(5,3)
> >
> > What should it do?  (you have only shown definitions for f.1 and f.2).  With my luck I would accidentily type that and just happen to have a f.3 function sitting around from a previous project that does something that I really don't want it to do now.  If I use the list approach then I will get a subscript out of bounds error rather than running something unintended.
> >
> >
>
> I see the general concern, but not how it applies here. If I pass
> argument "Fratio" then either I use geneSelect.Fratio or I get an
> error if "geneSelect.Fratio" does not exist. Similar to what would
> happen if I do
>
> g1(2, 8)
>
> when f.8 is not defined:
>
> Error in eval(expr, envir, enclos) : object "f.8" not found
> So even in more general cases, except for function redefinitions, etc,
> you are not able to call non-existent stuff.
>
> > 2nd, If I used the eval-parse approach then I would probably at some point redefine f.1 or f.2 to the output of a regression analysis or something, then go back and run the g function at a later time and wonder why I am getting an error, then once I have finally figured it out, now I need to remember what f.1 did and rewrite it again.  I am much less likely to accidentally replace an element of a list, and if the list is well named I am unlikely to replace the whole list by accident.
> >
> >
>
> Yes, that is true. Again, it does not apply to the actual case I have
> in mind, but of course, without the detailed info on context I just
> gave, you could not know that.
>
>
> > 3rd, If I ever want to use this code somewhere else (new version of R, on the laptop, give to coworker, ...), it is a lot easier to save and load a single list than to try to think of all the functions that need to be saved.
> >
>
> Oh, sure. But all the functions above live in a single file (actually,
> a minipackage) except for the optional use function (which is read
> from a file).
>
>
> >
> > Personally I have never regretted trying not to underestimate my own future stupidity.
> >
>
> Neither do I. And actually, that is why I asked: if Thomas Lumley
> said, in the fortune, that I better rethink about it, then I should
> try rethinking about it. But I asked because I failed to see what the
> problem is.
>
>
> > Hope this helps,
> >
>
> It certainly does.
>
>
> Best,
>
> R.
>
>
> > --
> > Gregory (Greg) L. Snow Ph.D.
> > Statistical Data Center
> > Intermountain Healthcare
> > greg.snow at intermountainmail.org
> > (801) 408-8111
> >
> >
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ramon
> > > Diaz-Uriarte
> > > Sent: Friday, January 05, 2007 11:41 AM
> > > To: Peter Dalgaard
> > > Cc: r-help; rdiaz02 at gmail.com
> > > Subject: Re: [R] eval(parse(text vs. get when accessing a function
> > >
> > > On Friday 05 January 2007 19:21, Peter Dalgaard wrote:
> > > > Ramon Diaz-Uriarte wrote:
> > > > > Dear All,
> > > > >
> > > > > I've read Thomas Lumley's fortune "If the answer is parse() you
> > > > > should usually rethink the question.". But I am not sure it that
> > > > > also applies (and why) to other situations (Lumley's comment
> > > > > http://tolstoy.newcastle.edu.au/R/help/05/02/12204.html
> > > > > was in reply to accessing a list).
> > > > >
> > > > > Suppose I have similarly called functions, except for a
> > > postfix. E.g.
> > > > >
> > > > > f.1 <- function(x) {x + 1}
> > > > > f.2 <- function(x) {x + 2}
> > > > >
> > > > > And sometimes I want to call f.1 and some other times f.2 inside
> > > > > another function. I can either do:
> > > > >
> > > > > g <- function(x, fpost) {
> > > > >     calledf <- eval(parse(text = paste("f.", fpost, sep = "")))
> > > > >     calledf(x)
> > > > >     ## do more stuff
> > > > > }
> > > > >
> > > > >
> > > > > Or:
> > > > >
> > > > > h <- function(x, fpost) {
> > > > >     calledf <- get(paste("f.", fpost, sep = ""))
> > > > >     calledf(x)
> > > > >     ## do more stuff
> > > > > }
> > > > >
> > > > >
> > > > > Two questions:
> > > > > 1) Why is the second better?
> > > > >
> > > > > 2) By changing g or h I could use "do.call" instead; why
> > > would that
> > > > > be better? Because I can handle differences in argument lists?
> > >
> > > Dear Peter,
> > >
> > > Thanks for your answer.
> > >
> > > >
> > > > Who says that they are better?  If the question is how to call a
> > > > function specified by half of its name, the answer could well be to
> > > > use parse(), the point is that you should rethink whether that was
> > > > really the right question.
> > > >
> > > > Why not instead, e.g.
> > > >
> > > > f <- list("1"=function(x) {x + 1} , "2"=function(x) {x + 2}) h <-
> > > > function(x, fpost) f[[fpost]](x)
> > > >
> > > > > h(2,"2")
> > > >
> > > > [1] 4
> > > >
> > > > > h(2,"1")
> > > >
> > > > [1] 3
> > > >
> > >
> > > I see, this is direct way of dealing with the problem.
> > > However, you first need to build the f list, and you might
> > > not know about that ahead of time. For instance, if I build a
> > > function so that the only thing that you need to do to use my
> > > function g is to call your function "f.something", and then
> > > pass the "something".
> > >
> > > I am still under the impression that, given your answer,
> > > using "eval(parse(text" is not your preferred way.  What are
> > > the possible problems (if there are any, that is). I guess I
> > > am puzzled by "rethink whether that was really the right question".
> > >
> > >
> > > Thanks,
> > >
> > > R.
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > > > > Thanks,
> > > > >
> > > > >
> > > > > R.
> > >
> > > --
> > > Ram?n D?az-Uriarte
> > > Centro Nacional de Investigaciones Oncol?gicas (CNIO)
> > > (Spanish National Cancer Center) Melchor Fern?ndez Almagro, 3
> > > 28029 Madrid (Spain)
> > > Fax: +-34-91-224-6972
> > > Phone: +-34-91-224-6900
> > >
> > > http://ligarto.org/rdiaz
> > > PGP KeyID: 0xE89B3462
> > > (http://ligarto.org/rdiaz/0xE89B3462.asc)
> > >
> > >
> > >
> > > **NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en
> > > s...{{dropped}}
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
>
>
> --
> Ramon Diaz-Uriarte
> Statistical Computing Team
> Structural Biology and Biocomputing Programme
> Spanish National Cancer Centre (CNIO)
> http://ligarto.org/rdiaz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Sat Jan  6 16:30:24 2007
From: ripley at stats.ox.ac.uk (Brian Ripley)
Date: Sat, 6 Jan 2007 15:30:24 +0000 (GMT)
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <624934630701060616i2718ab71xb981fbca39c0c910@mail.gmail.com>
References: <07E228A5BE53C24CAD490193A7381BBB77C654@LP-EXCHVS07.CO.IHC.COM>
	<624934630701060616i2718ab71xb981fbca39c0c910@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701061523220.19955@auk.stats>

On Sat, 6 Jan 2007, Ramon Diaz-Uriarte wrote:

> Dear Greg,
>
>
> On 1/5/07, Greg Snow <Greg.Snow at intermountainmail.org> wrote:
>> Ramon,
>>
>> I prefer to use the list method for this type of thing, here are a couple of reasons why (maybe you are more organized than me and would never do some of the stupid things that I have, so these don't apply to you, but you can see that the general suggestion applys to some of the rest of us).
>>
>
>
> Those suggestions do apply to me of course (no claim to being
> organized nor beyond idiocy here). And actually the suggestions on
> this thread are being very useful. I think, though, that I was not
> very clear on the context and my examples were too dumbed down. So
> I'll try to give more detail (nothing here is secret, I am just trying
> not to bore people).
>
> The code is part of a web-based application, so there is no
> interactive user. The R code is passed the arguments (and optional
> user functions) from the CGI.
>
> There is one "core" function (call it cvFunct) that, among other
> things, does cross-validation. So this is one way to do things:
>
> cvFunct <- function(whatever, genefiltertype, whateverelse) {
>      internalGeneSelect <- eval(parse(text = paste("geneSelect",
>                                             genefiltertype, sep = ".")))
>
>      ## do things calling internalGeneSelect,
> }

That looks like a more complicated alternative to

    get(paste("geneSelect", genefiltertype, sep = "."))

I would worry about scope in both cases: I think you most likely want 
eval.parent in yours, and to pick an environment for use in get() (but 
the view you have shown is still too narrow for us to know).

> and now define all possible functions as
>
> geneSelect.Fratio <- function(x, y, z) {##something}
> geneSelect.Wilcoxon <- function(x, y, z) {## something else}
>
> If I want more geneSelect functions, adding them is simple. And I can
> even allow the user to pass her/his own functions, with the only
> restriction that it takes three args, x, y, z, and that the function
> is to be called: "geneSelect." and a user choosen string. (Yes, I need
> to make sure no calls to "system", etc, are in the user code, etc,
> etc, but that is another issue).

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rdiaz02 at gmail.com  Sat Jan  6 15:32:43 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Sat, 6 Jan 2007 15:32:43 +0100
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <644e1f320701051622q5f5ae28cu8fecf4e538165c10@mail.gmail.com>
References: <200701051940.57136.rdiaz@cnio.es>
	<Pine.LNX.4.43.0701051506140.3919@hymn04.u.washington.edu>
	<644e1f320701051622q5f5ae28cu8fecf4e538165c10@mail.gmail.com>
Message-ID: <624934630701060632m48733c8ay6355c03bc150e246@mail.gmail.com>

On 1/6/07, jim holtman <jholtman at gmail.com> wrote:
> I agree with what you are saying.  That is the reason I have "What is the
> problem you are trying to solve" on my signature.
>
> The other way of saying that is "Tell me what you want to do, not how you
> want to do it."
>

Point taken: I did not provide enough detail. (But then, with the
detail I provided in my reply to Greg Snow's answer, I think most
people will have not looked at the long email, which means I would not
have been getting as much useful feedback :-).

R.




>
> On 1/5/07, Thomas Lumley <tlumley at u.washington.edu> wrote:
> >
> > On Fri, 5 Jan 2007, Ramon Diaz-Uriarte wrote:
> >
> > > I see, this is direct way of dealing with the problem. However, you
> first need
> > > to build the f list, and you might not know about that ahead of time.
> For
> > > instance, if I build a function so that the only thing that you need to
> do to
> > > use my function g is to call your function " f.something", and then pass
> > > the "something".
> > >
> > > I am still under the impression that, given your answer,
> > > using "eval(parse(text" is not your preferred way.  What are the
> possible
> > > problems (if there are any, that is). I guess I am puzzled by "rethink
> > > whether that was really the right question".
> > >
> >
> > There are definitely situations where parse() is necessary or convenient,
> > or we wouldn't provide it. For example, there are some
> formula-manipulation problems where it really does seem to be the best
> solution.
> >
> > The point of my observation was that it is relatively common for people to
> ask about parse() solutions to problems, but relatively rare to see them in
> code by experienced R programmers.  The 'rethink the question' point is that
> a narrowly-posed programming problem may suggest parse() as the answer, when
> thinking more broadly about what you are trying to do may allow a completely
> different approach [the example of lists is a common one].
> >
> > The problem with eval(parse()) is not primarily one of speed.  A problem
> with parse() is than manipulating text strings is easy to mess up, since
> text has so much less structure than code. A problem with eval() is that it
> is too powerful -- since it can do anything, it is harder to keep track of
> what it is doing.
> >
> > In one sense this is just a style issue, but I still think my comment is
> good advice. If you find yourself wanting to use parse() it is a good idea
> to stop and think about whether there is a better way to do it. Often, there
> is. Sometimes, there isn't.
> >
> >
> >        -thomas
> >
> > Thomas Lumley                   Assoc. Professor, Biostatistics
> > tlumley at u.washington.edu        University of Washington, Seattle
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From murdoch at stats.uwo.ca  Sat Jan  6 16:43:02 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 06 Jan 2007 10:43:02 -0500
Subject: [R] listing all functions in R
In-Reply-To: <1168093536.3149.25.camel@dhcppc2.my.nat.localnet>
References: <1168088147.3149.6.camel@dhcppc2.my.nat.localnet>	<Pine.LNX.4.64.0701061331420.8651@gannet.stats.ox.ac.uk>
	<1168093536.3149.25.camel@dhcppc2.my.nat.localnet>
Message-ID: <459FC386.6030004@stats.uwo.ca>

On 1/6/2007 9:25 AM, Gavin Simpson wrote:
> On Sat, 2007-01-06 at 13:48 +0000, Prof Brian Ripley wrote:
>> Could you tell us what you mean by
> 
> Thank you for your reply, Prof. Ripley.
> 
>> - 'function'  (if() and + are functions in R, so do you want those?)
> 
> I was thinking about functions that are used like this: foo()
> So I don't need things like "names<-". I don't need functions like +. -,
> $, as I can highlight the separately if desired, though I'm not doing
> this at the moment.
> 
> Functions like for() while(), if() function() are handled separately.
> 
>> - 'a base R installation'?   What is 'base R' (standard + recommended 
>> packages?)  And on what platform: the list is platform-specific?
> 
> Yes, I mean standard + recommended packages. As for platform, most of my
> intended audience will be MS Windows users, though I am using Linux
> (Fedora) to generate this list (i.e. my R installation is on Linux).

Be careful:  the installed list of functions differs slightly from 
platform to platform.  For example, on Windows there's a function 
choose.dir in the utils package, but I don't think this exists on Unix.

The list also varies from version to version, so if you could manage to 
run some code in the user's installed R to generate the list on the fly, 
you'd get the most accurate list.

Duncan Murdoch

> 
>> Here is a reasonable shot:
>>
>> findfuns <- function(x) {
>>      if(require(x, character.only=TRUE)) {
>>         env <- paste("package", x, sep=":")
>>         nm <- ls(env, all=TRUE)
>>         nm[unlist(lapply(nm, function(n) exists(n, where=env,
>>                                                mode="function",
>>                                                inherits=FALSE)))]
>>      } else character(0)
>> }
>> pkgs <- dir(.Library)
>> z <-  lapply(pkgs, findfuns)
>> names(z) <- pkgs
> 
> Excellent, that works just fine for me. I can edit out certain packages
> that I don't expect to use, before formatting as desired. I can also use
> this function on a library of packages that I use regularly and will be
> using in the web pages.
> 
>> I don't understand your desired format, but
>>
>> write(sQuote(sort(unique(unlist(z)))), "")
> 
> I wanted a single string "...", with entries enclosed in "''" and
> separated by "," (this is to go in a PHP array). I can generate such a
> string from your z, above, as follows:
> 
> paste(sQuote(sort(unique(unlist(z)), decreasing = TRUE)), 
>       collapse = ", ")
> 
>> gives a single-column quoted list.  It does include internal functions, 
>> operators, S3 methods ... so you probably want to edit it.
> 
> Once again, thank you.
> 
> All the best
> 
> Gav
> 
>>
>> On Sat, 6 Jan 2007, Gavin Simpson wrote:
>>
>>> Dear List,
>>>
>>> I'm building an R syntax highlighting file for GeSHi [*] for a website I
>>> am currently putting together. The syntax file needs a list of keywords
>>> to highlight. How can I generate a list of all the functions in a base R
>>> installation?
>>>
>>> Ideally the list would be formatted like this:
>>>
>>> "'fun1', 'fun2', 'fun3'"
>>>
>>> when printed to the screen so I can copy and paste it into the syntax
>>> file.
>>>
>>> I'm sure this has been asked before, but I stupidly didn't save that
>>> email and I couldn't come up with a suitable query parameter for
>>> Jonathan Baron's search site to return results before timing out.
>>>
>>> Thanks in advance,
>>>
>>> Gav
>>>


From ggrothendieck at gmail.com  Sat Jan  6 16:58:11 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 6 Jan 2007 10:58:11 -0500
Subject: [R] listing all functions in R
In-Reply-To: <459FC386.6030004@stats.uwo.ca>
References: <1168088147.3149.6.camel@dhcppc2.my.nat.localnet>
	<Pine.LNX.4.64.0701061331420.8651@gannet.stats.ox.ac.uk>
	<1168093536.3149.25.camel@dhcppc2.my.nat.localnet>
	<459FC386.6030004@stats.uwo.ca>
Message-ID: <971536df0701060758y4b23e8dal881807031a8d1b9b@mail.gmail.com>

The arguments to the functions can differ too even if they
exist on multiple platforms.   system() on Windows has the
input= argument but not on UNIX.

On 1/6/07, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 1/6/2007 9:25 AM, Gavin Simpson wrote:
> > On Sat, 2007-01-06 at 13:48 +0000, Prof Brian Ripley wrote:
> >> Could you tell us what you mean by
> >
> > Thank you for your reply, Prof. Ripley.
> >
> >> - 'function'  (if() and + are functions in R, so do you want those?)
> >
> > I was thinking about functions that are used like this: foo()
> > So I don't need things like "names<-". I don't need functions like +. -,
> > $, as I can highlight the separately if desired, though I'm not doing
> > this at the moment.
> >
> > Functions like for() while(), if() function() are handled separately.
> >
> >> - 'a base R installation'?   What is 'base R' (standard + recommended
> >> packages?)  And on what platform: the list is platform-specific?
> >
> > Yes, I mean standard + recommended packages. As for platform, most of my
> > intended audience will be MS Windows users, though I am using Linux
> > (Fedora) to generate this list (i.e. my R installation is on Linux).
>
> Be careful:  the installed list of functions differs slightly from
> platform to platform.  For example, on Windows there's a function
> choose.dir in the utils package, but I don't think this exists on Unix.
>
> The list also varies from version to version, so if you could manage to
> run some code in the user's installed R to generate the list on the fly,
> you'd get the most accurate list.
>
> Duncan Murdoch
>
> >
> >> Here is a reasonable shot:
> >>
> >> findfuns <- function(x) {
> >>      if(require(x, character.only=TRUE)) {
> >>         env <- paste("package", x, sep=":")
> >>         nm <- ls(env, all=TRUE)
> >>         nm[unlist(lapply(nm, function(n) exists(n, where=env,
> >>                                                mode="function",
> >>                                                inherits=FALSE)))]
> >>      } else character(0)
> >> }
> >> pkgs <- dir(.Library)
> >> z <-  lapply(pkgs, findfuns)
> >> names(z) <- pkgs
> >
> > Excellent, that works just fine for me. I can edit out certain packages
> > that I don't expect to use, before formatting as desired. I can also use
> > this function on a library of packages that I use regularly and will be
> > using in the web pages.
> >
> >> I don't understand your desired format, but
> >>
> >> write(sQuote(sort(unique(unlist(z)))), "")
> >
> > I wanted a single string "...", with entries enclosed in "''" and
> > separated by "," (this is to go in a PHP array). I can generate such a
> > string from your z, above, as follows:
> >
> > paste(sQuote(sort(unique(unlist(z)), decreasing = TRUE)),
> >       collapse = ", ")
> >
> >> gives a single-column quoted list.  It does include internal functions,
> >> operators, S3 methods ... so you probably want to edit it.
> >
> > Once again, thank you.
> >
> > All the best
> >
> > Gav
> >
> >>
> >> On Sat, 6 Jan 2007, Gavin Simpson wrote:
> >>
> >>> Dear List,
> >>>
> >>> I'm building an R syntax highlighting file for GeSHi [*] for a website I
> >>> am currently putting together. The syntax file needs a list of keywords
> >>> to highlight. How can I generate a list of all the functions in a base R
> >>> installation?
> >>>
> >>> Ideally the list would be formatted like this:
> >>>
> >>> "'fun1', 'fun2', 'fun3'"
> >>>
> >>> when printed to the screen so I can copy and paste it into the syntax
> >>> file.
> >>>
> >>> I'm sure this has been asked before, but I stupidly didn't save that
> >>> email and I couldn't come up with a suitable query parameter for
> >>> Jonathan Baron's search site to return results before timing out.
> >>>
> >>> Thanks in advance,
> >>>
> >>> Gav
> >>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gavin.simpson at ucl.ac.uk  Sat Jan  6 17:02:40 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Sat, 06 Jan 2007 16:02:40 +0000
Subject: [R] listing all functions in R
In-Reply-To: <459FC386.6030004@stats.uwo.ca>
References: <1168088147.3149.6.camel@dhcppc2.my.nat.localnet>
	<Pine.LNX.4.64.0701061331420.8651@gannet.stats.ox.ac.uk>
	<1168093536.3149.25.camel@dhcppc2.my.nat.localnet>
	<459FC386.6030004@stats.uwo.ca>
Message-ID: <1168099360.3149.37.camel@dhcppc2.my.nat.localnet>

On Sat, 2007-01-06 at 10:43 -0500, Duncan Murdoch wrote:
> On 1/6/2007 9:25 AM, Gavin Simpson wrote:
> > On Sat, 2007-01-06 at 13:48 +0000, Prof Brian Ripley wrote:
> >> Could you tell us what you mean by
> > 
> > Thank you for your reply, Prof. Ripley.
> > 
> >> - 'function'  (if() and + are functions in R, so do you want those?)
> > 
> > I was thinking about functions that are used like this: foo()
> > So I don't need things like "names<-". I don't need functions like +. -,
> > $, as I can highlight the separately if desired, though I'm not doing
> > this at the moment.
> > 
> > Functions like for() while(), if() function() are handled separately.
> > 
> >> - 'a base R installation'?   What is 'base R' (standard + recommended 
> >> packages?)  And on what platform: the list is platform-specific?
> > 
> > Yes, I mean standard + recommended packages. As for platform, most of my
> > intended audience will be MS Windows users, though I am using Linux
> > (Fedora) to generate this list (i.e. my R installation is on Linux).

Cheers Duncan.

> Be careful:  the installed list of functions differs slightly from 
> platform to platform.  For example, on Windows there's a function 
> choose.dir in the utils package, but I don't think this exists on Unix.

Good point. However as I am in control of the R snippets I display on
the web pages and the highlighting file/list, I can add the odd thing
that Brian Ripley's findfuns function doesn't list because of platform
differences.

What I wanted to avoid was having to add functions to my key word list
each time I wrote another page on the site that used a new R snippet. As
it is early days, I'd probably spend about as much time adding functions
to the keyword list as writing pages for the site - which would put me
of a bit and slow me down. At least now I only have to add the odd
function missed.

> 
> The list also varies from version to version, so if you could manage to 
> run some code in the user's installed R to generate the list on the fly, 
> you'd get the most accurate list.

Yes. I'm planning on wrapping findfuns into a little R script that
searches additional packages that I'll use, and that will update the
packages before compiling the list. I can then run this script
periodically in R to update the list, as R is updated etc.

> 
> Duncan Murdoch

Many thanks for your reply,

All the best,

G

> 
> > 
> >> Here is a reasonable shot:
> >>
> >> findfuns <- function(x) {
> >>      if(require(x, character.only=TRUE)) {
> >>         env <- paste("package", x, sep=":")
> >>         nm <- ls(env, all=TRUE)
> >>         nm[unlist(lapply(nm, function(n) exists(n, where=env,
> >>                                                mode="function",
> >>                                                inherits=FALSE)))]
> >>      } else character(0)
> >> }
> >> pkgs <- dir(.Library)
> >> z <-  lapply(pkgs, findfuns)
> >> names(z) <- pkgs
> > 
> > Excellent, that works just fine for me. I can edit out certain packages
> > that I don't expect to use, before formatting as desired. I can also use
> > this function on a library of packages that I use regularly and will be
> > using in the web pages.
> > 
> >> I don't understand your desired format, but
> >>
> >> write(sQuote(sort(unique(unlist(z)))), "")
> > 
> > I wanted a single string "...", with entries enclosed in "''" and
> > separated by "," (this is to go in a PHP array). I can generate such a
> > string from your z, above, as follows:
> > 
> > paste(sQuote(sort(unique(unlist(z)), decreasing = TRUE)), 
> >       collapse = ", ")
> > 
> >> gives a single-column quoted list.  It does include internal functions, 
> >> operators, S3 methods ... so you probably want to edit it.
> > 
> > Once again, thank you.
> > 
> > All the best
> > 
> > Gav
> > 
> >>
> >> On Sat, 6 Jan 2007, Gavin Simpson wrote:
> >>
> >>> Dear List,
> >>>
> >>> I'm building an R syntax highlighting file for GeSHi [*] for a website I
> >>> am currently putting together. The syntax file needs a list of keywords
> >>> to highlight. How can I generate a list of all the functions in a base R
> >>> installation?
> >>>
> >>> Ideally the list would be formatted like this:
> >>>
> >>> "'fun1', 'fun2', 'fun3'"
> >>>
> >>> when printed to the screen so I can copy and paste it into the syntax
> >>> file.
> >>>
> >>> I'm sure this has been asked before, but I stupidly didn't save that
> >>> email and I couldn't come up with a suitable query parameter for
> >>> Jonathan Baron's search site to return results before timing out.
> >>>
> >>> Thanks in advance,
> >>>
> >>> Gav
> >>>
> 
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT                          [w] http://www.freshwaters.org.uk/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From gavin.simpson at ucl.ac.uk  Sat Jan  6 17:16:01 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Sat, 06 Jan 2007 16:16:01 +0000
Subject: [R] listing all functions in R
In-Reply-To: <971536df0701060758y4b23e8dal881807031a8d1b9b@mail.gmail.com>
References: <1168088147.3149.6.camel@dhcppc2.my.nat.localnet>
	<Pine.LNX.4.64.0701061331420.8651@gannet.stats.ox.ac.uk>
	<1168093536.3149.25.camel@dhcppc2.my.nat.localnet>
	<459FC386.6030004@stats.uwo.ca>
	<971536df0701060758y4b23e8dal881807031a8d1b9b@mail.gmail.com>
Message-ID: <1168100161.3149.49.camel@dhcppc2.my.nat.localnet>

On Sat, 2007-01-06 at 10:58 -0500, Gabor Grothendieck wrote:
> The arguments to the functions can differ too even if they
> exist on multiple platforms.   system() on Windows has the
> input= argument but not on UNIX.

That's a good point Gabor, and one I hadn't considered as yet. As I'm
only just setting out on the road to providing R help resources for the
wider world (rather than the limited environs of the courses I have
run), I tend to not have thought about these things much - though I
guess I have a few gotchas waiting to bite me in the ass before too
long.

I am just starting to think about the best way to organise the snippets
of code to allow me to keep them up-to-date with current R and changes
in package code that the snippets use. Dropping the code verbatim into
PHP scripts isn't a good idea. At the moment I intend to store all
snippets in individual *.R files and read them into to variables within
the PHP scripts, from where they will be highlighted and formatted for
display.

It would be reasonably easy to write an R script to source all *.R files
in a directory to look for errors and problems. And having them all as
separate files means I can still use Emacs/ESS to prepare, format, and
run the code through R, which is my preferred environment.

All the best,

G

> 
> On 1/6/07, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> > On 1/6/2007 9:25 AM, Gavin Simpson wrote:
> > > On Sat, 2007-01-06 at 13:48 +0000, Prof Brian Ripley wrote:
> > >> Could you tell us what you mean by
> > >
> > > Thank you for your reply, Prof. Ripley.
> > >
> > >> - 'function'  (if() and + are functions in R, so do you want those?)
> > >
> > > I was thinking about functions that are used like this: foo()
> > > So I don't need things like "names<-". I don't need functions like +. -,
> > > $, as I can highlight the separately if desired, though I'm not doing
> > > this at the moment.
> > >
> > > Functions like for() while(), if() function() are handled separately.
> > >
> > >> - 'a base R installation'?   What is 'base R' (standard + recommended
> > >> packages?)  And on what platform: the list is platform-specific?
> > >
> > > Yes, I mean standard + recommended packages. As for platform, most of my
> > > intended audience will be MS Windows users, though I am using Linux
> > > (Fedora) to generate this list (i.e. my R installation is on Linux).
> >
> > Be careful:  the installed list of functions differs slightly from
> > platform to platform.  For example, on Windows there's a function
> > choose.dir in the utils package, but I don't think this exists on Unix.
> >
> > The list also varies from version to version, so if you could manage to
> > run some code in the user's installed R to generate the list on the fly,
> > you'd get the most accurate list.
> >
> > Duncan Murdoch
> >
> > >
> > >> Here is a reasonable shot:
> > >>
> > >> findfuns <- function(x) {
> > >>      if(require(x, character.only=TRUE)) {
> > >>         env <- paste("package", x, sep=":")
> > >>         nm <- ls(env, all=TRUE)
> > >>         nm[unlist(lapply(nm, function(n) exists(n, where=env,
> > >>                                                mode="function",
> > >>                                                inherits=FALSE)))]
> > >>      } else character(0)
> > >> }
> > >> pkgs <- dir(.Library)
> > >> z <-  lapply(pkgs, findfuns)
> > >> names(z) <- pkgs
> > >
> > > Excellent, that works just fine for me. I can edit out certain packages
> > > that I don't expect to use, before formatting as desired. I can also use
> > > this function on a library of packages that I use regularly and will be
> > > using in the web pages.
> > >
> > >> I don't understand your desired format, but
> > >>
> > >> write(sQuote(sort(unique(unlist(z)))), "")
> > >
> > > I wanted a single string "...", with entries enclosed in "''" and
> > > separated by "," (this is to go in a PHP array). I can generate such a
> > > string from your z, above, as follows:
> > >
> > > paste(sQuote(sort(unique(unlist(z)), decreasing = TRUE)),
> > >       collapse = ", ")
> > >
> > >> gives a single-column quoted list.  It does include internal functions,
> > >> operators, S3 methods ... so you probably want to edit it.
> > >
> > > Once again, thank you.
> > >
> > > All the best
> > >
> > > Gav
> > >
> > >>
> > >> On Sat, 6 Jan 2007, Gavin Simpson wrote:
> > >>
> > >>> Dear List,
> > >>>
> > >>> I'm building an R syntax highlighting file for GeSHi [*] for a website I
> > >>> am currently putting together. The syntax file needs a list of keywords
> > >>> to highlight. How can I generate a list of all the functions in a base R
> > >>> installation?
> > >>>
> > >>> Ideally the list would be formatted like this:
> > >>>
> > >>> "'fun1', 'fun2', 'fun3'"
> > >>>
> > >>> when printed to the screen so I can copy and paste it into the syntax
> > >>> file.
> > >>>
> > >>> I'm sure this has been asked before, but I stupidly didn't save that
> > >>> email and I couldn't come up with a suitable query parameter for
> > >>> Jonathan Baron's search site to return results before timing out.
> > >>>
> > >>> Thanks in advance,
> > >>>
> > >>> Gav
> > >>>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT                          [w] http://www.freshwaters.org.uk/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From kubovy at virginia.edu  Sat Jan  6 17:46:45 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sat, 6 Jan 2007 11:46:45 -0500
Subject: [R] ANCOVA
In-Reply-To: <6d74b8030701060534m5dfbb6bdi2349c4f5de6cbfb1@mail.gmail.com>
References: <6d74b8030701060534m5dfbb6bdi2349c4f5de6cbfb1@mail.gmail.com>
Message-ID: <15FE1317-AD4D-48DF-968E-112FD6DA6B6B@virginia.edu>

On Jan 6, 2007, at 8:34 AM, John Cardinale wrote:

> Are there any R function which can do analysis of covariance?

?lm
RSiteSearch('ancova')
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From rdiaz02 at gmail.com  Sat Jan  6 20:08:45 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Sat, 6 Jan 2007 20:08:45 +0100
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <Pine.LNX.4.64.0701061523220.19955@auk.stats>
References: <07E228A5BE53C24CAD490193A7381BBB77C654@LP-EXCHVS07.CO.IHC.COM>
	<624934630701060616i2718ab71xb981fbca39c0c910@mail.gmail.com>
	<Pine.LNX.4.64.0701061523220.19955@auk.stats>
Message-ID: <624934630701061108u1248f5admaff332172e97e23b@mail.gmail.com>

On 1/6/07, Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Sat, 6 Jan 2007, Ramon Diaz-Uriarte wrote:
(...)

> >
> > cvFunct <- function(whatever, genefiltertype, whateverelse) {
> >      internalGeneSelect <- eval(parse(text = paste("geneSelect",
> >                                             genefiltertype, sep = ".")))
> >
> >      ## do things calling internalGeneSelect,
> > }
>
> That looks like a more complicated alternative to
>
>     get(paste("geneSelect", genefiltertype, sep = "."))
>

Yes, you are right, thanks. Actually, now that I think of it, the
eval(parse(text looks _a lot_ more verbose.


> I would worry about scope in both cases: I think you most likely want
> eval.parent in yours, and to pick an environment for use in get() (but
> the view you have shown is still too narrow for us to know).
>

The function where get (or eval) are called from is defined in a
package. The other  functions (the ones with the postfix) are either
in the same package or in the global environment (read from a file). I
think with both solutions (get and eval) and defining the other
functions both ways (in a package and in the global env) I should be
OK, but I probably want to make this explicit.


Thanks,

R.


> > and now define all possible functions as
> >
> > geneSelect.Fratio <- function(x, y, z) {##something}
> > geneSelect.Wilcoxon <- function(x, y, z) {## something else}
> >
> > If I want more geneSelect functions, adding them is simple. And I can
> > even allow the user to pass her/his own functions, with the only
> > restriction that it takes three args, x, y, z, and that the function
> > is to be called: "geneSelect." and a user choosen string. (Yes, I need
> > to make sure no calls to "system", etc, are in the user code, etc,
> > etc, but that is another issue).
>
> [...]
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From rdiaz02 at gmail.com  Sat Jan  6 20:35:22 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Sat, 6 Jan 2007 20:35:22 +0100
Subject: [R] ANCOVA
In-Reply-To: <15FE1317-AD4D-48DF-968E-112FD6DA6B6B@virginia.edu>
References: <6d74b8030701060534m5dfbb6bdi2349c4f5de6cbfb1@mail.gmail.com>
	<15FE1317-AD4D-48DF-968E-112FD6DA6B6B@virginia.edu>
Message-ID: <624934630701061135q30dc6a7r4894d2e4d420f3e3@mail.gmail.com>

On 1/6/07, Michael Kubovy <kubovy at virginia.edu> wrote:
> On Jan 6, 2007, at 8:34 AM, John Cardinale wrote:
>
> > Are there any R function which can do analysis of covariance?
>
> ?lm
> RSiteSearch('ancova')



Given the question, you'll probably need to find "how to do an ancova
with lm". Several documents in
http://cran.r-project.org/other-docs.html will show you how (and why
ancova is just one special case of linear model). In particular, I
think Faraway's "Practical regression and Anova using R" has explicit
chapters/sections for Ancova. Many of other standard texts on R/S do
too.

R.






> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
> Parcels:    Room 102        Gilmer Hall
>          McCormick Road    Charlottesville, VA 22903
> Office:    B011    +1-434-982-4729
> Lab:        B019    +1-434-982-4751
> Fax:        +1-434-982-4766
> WWW:    http://www.people.virginia.edu/~mk9y/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From zkmetty at gmail.com  Sat Jan  6 21:25:56 2007
From: zkmetty at gmail.com (Zoltan Kmetty)
Date: Sat, 6 Jan 2007 21:25:56 +0100
Subject: [R] memory problem
Message-ID: <c1055ec10701061225o18e66bc9r7b315970dc7d4c36@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070106/c93b2915/attachment.pl 

From rmh at temple.edu  Sat Jan  6 23:21:20 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Sat,  6 Jan 2007 17:21:20 -0500 (EST)
Subject: [R] ANCOVA
Message-ID: <20070106172120.BRM42484@po-d.temple.edu>

Please look at the help file
?ancova
in the HH package.

Please get HH_1.17 which was up on CRAN yesterday.  The source
file has propagated to the mirrors.  As of a few minutes ago,
the Windows binary is on cran.at.r-project.org but not yet at the mirrors.

Be sure to have history recording on in the graphics window
because it will be very helpful to toggle between the displays
for the different models.

Rich


From Inman.Brant at mayo.edu  Sat Jan  6 23:56:43 2007
From: Inman.Brant at mayo.edu (Inman, Brant A.   M.D.)
Date: Sat, 6 Jan 2007 16:56:43 -0600
Subject: [R] Using VGAM's vglm function for ordinal logistic regression
Message-ID: <6021CA6EF4C8374084D4F5A141F1CBBB6649E1@msgebe23.mfad.mfroot.org>


R-Experts:

I am using the vglm function of the VGAM library to perform proportional
odds ordinal logistic regression.  The issue that I would like help with
concerns the format in which the response variable must be provided for
this function to work correctly. Consider the following example:

------

library(VGAM)
library(MASS)

attach(pneumo)
pneumo	# Inspect the format of the original dataset

# Restructure the pneumo dataset into a different format
pneumo2 <- data.frame(matrix(ncol=3, nrow=24))
colnames(pneumo2) <- c('exposure.time', 'severity', 'freq')
pneumo2[,1] <- rep(pneumo[,1],3)	
pneumo2[,2] <-
as.ordered(c(rep('normal',8),rep('mild',8),rep('severe',8)))
pneumo2[1:8,3]   <- pneumo[,2]
pneumo2[9:16,3]  <- pneumo[,3]
pneumo2[17:24,3] <- pneumo[,4]
pneumo2	# Inspect the format of the new modified dataset

------

The problem occurs when I try to analyze these two datasets, which are
identical in content, with the proportional odds model using vglm:

------

# Analyze the original dataset with vglm, get one set of results 

> vglm(vglm(cbind(normal, mild, severe) ~ log(exposure.time),
data=pneumo, 
+  family=cumulative(parallel=T))

Coefficients:
     (Intercept):1      (Intercept):2 log(exposure.time) 
          9.676093          10.581725          -2.596807 

Degrees of Freedom: 16 Total; 13 Residual
Residual Deviance: 5.026826 
Log-likelihood: -204.2742

# Analyzing the modified dataset with vglm gives another set of results

> vglm(severity ~ log(exposure.time), weights=freq, data=pneumo2, 
+ family=cumulative(parallel=T))

Coefficients:
     (Intercept):1      (Intercept):2 log(exposure.time) 
        -1.6306499          2.5630170         -0.1937956 

Degrees of Freedom: 48 Total; 45 Residual
Residual Deviance: 503.7251 
Log-likelihood: -251.8626 
Warning messages:
1: 4 elements replaced by 1.819e-12 in: checkwz(wz, M = M, trace =
trace, wzeps = control$wzepsilon) 
2: 4 elements replaced by 1.819e-12 in: checkwz(wz, M = M, trace =
trace, wzeps = control$wzepsilon) 
3: 4 elements replaced by 1.819e-12 in: checkwz(wz, M = M, trace =
trace, wzeps = control$wzepsilon) 
4: 4 elements replaced by 1.819e-12 in: checkwz(wz, M = M, trace =
trace, wzeps = control$wzepsilon) 
5: 4 elements replaced by 1.819e-12 in: checkwz(wz, M = M, trace =
trace, wzeps = control$wzepsilon) 

# Analyzing the modified dataset with polr reproduces these second
results

> polr(severity ~ log(exposure.time), weights=freq, data=pneumo2)

Coefficients:
log(exposure.time) 
         0.1938154 

Intercepts:
  mild|normal normal|severe 
    -1.630612      2.563055 

Residual Deviance: 503.7251 
AIC: 509.7251

------

Can someone explain what I am doing wrong when using vglm and polr with
the modified dataset?  I do not understand why these two formulations
should give different results. 

Brant Inman


From dreiss at systemsbiology.org  Sun Jan  7 00:25:43 2007
From: dreiss at systemsbiology.org (David Reiss)
Date: Sat, 6 Jan 2007 15:25:43 -0800
Subject: [R] has anyone implemented LARS with the "positive lasso"?
Message-ID: <fd913b0d0701061525x4dc6c7c6we84b0fef268626b6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070106/73e9e706/attachment.pl 

From ligges at statistik.uni-dortmund.de  Sun Jan  7 09:42:08 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 07 Jan 2007 09:42:08 +0100
Subject: [R] memory problem
In-Reply-To: <c1055ec10701061225o18e66bc9r7b315970dc7d4c36@mail.gmail.com>
References: <c1055ec10701061225o18e66bc9r7b315970dc7d4c36@mail.gmail.com>
Message-ID: <45A0B260.7030302@statistik.uni-dortmund.de>



Zoltan Kmetty wrote:
> Hi!
> 
> I had some memory problem with R - hope somebody could tell me a solution.
> 
> I work with very large datasets, but R cannot allocate enough memoty to
> handle these datasets.
> 
> I want work a matrix with row= 100 000 000 and column=10
> 
> A know this is 1 milliard cases, but i thought R could handle it (other
> commercial software like spss could do), but R wrote out everytime: not
> enough memory..
> 
> any good idea?


Buy a machine that has at least 8Gb (better 16Gb) of RAM and proceed ...

Uwe Ligges



> Thanks, Zoltan
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Sun Jan  7 09:52:06 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 7 Jan 2007 08:52:06 +0000 (GMT)
Subject: [R] Using VGAM's vglm function for ordinal logistic regression
In-Reply-To: <6021CA6EF4C8374084D4F5A141F1CBBB6649E1@msgebe23.mfad.mfroot.org>
References: <6021CA6EF4C8374084D4F5A141F1CBBB6649E1@msgebe23.mfad.mfroot.org>
Message-ID: <Pine.LNX.4.64.0701070834280.25847@gannet.stats.ox.ac.uk>

On Sat, 6 Jan 2007, Inman, Brant A.   M.D. wrote:

>
> R-Experts:
>
> I am using the vglm function of the VGAM library to perform proportional
> odds ordinal logistic regression.  The issue that I would like help with
> concerns the format in which the response variable must be provided for
> this function to work correctly.

> pneumo2$severity
  [1] normal normal normal normal normal normal normal normal mild   mild
[11] mild   mild   mild   mild   mild   mild   severe severe severe severe
[21] severe severe severe severe
Levels: mild < normal < severe

is different from the ordering in the first example.

The difference between vglm and polr is that the latter uses the 
conventional sign for the coefficients: see MASS4 p.204.

I would never use as.ordered on a character vector, as this leaves it to R 
to choose the ordering.  (Even if you think you intended alphabetical 
order, that depends on the locale: see the warnings on the help page.)


> Consider the following example:
>
> ------
>
> library(VGAM)
> library(MASS)
>
> attach(pneumo)
> pneumo	# Inspect the format of the original dataset
>
> # Restructure the pneumo dataset into a different format
> pneumo2 <- data.frame(matrix(ncol=3, nrow=24))
> colnames(pneumo2) <- c('exposure.time', 'severity', 'freq')
> pneumo2[,1] <- rep(pneumo[,1],3)
> pneumo2[,2] <-
> as.ordered(c(rep('normal',8),rep('mild',8),rep('severe',8)))
> pneumo2[1:8,3]   <- pneumo[,2]
> pneumo2[9:16,3]  <- pneumo[,3]
> pneumo2[17:24,3] <- pneumo[,4]
> pneumo2	# Inspect the format of the new modified dataset
>
> ------
>
> The problem occurs when I try to analyze these two datasets, which are
> identical in content, with the proportional odds model using vglm:
>
> ------
>
> # Analyze the original dataset with vglm, get one set of results
>
>> vglm(vglm(cbind(normal, mild, severe) ~ log(exposure.time),
> data=pneumo,
> +  family=cumulative(parallel=T))
>
> Coefficients:
>     (Intercept):1      (Intercept):2 log(exposure.time)
>          9.676093          10.581725          -2.596807
>
> Degrees of Freedom: 16 Total; 13 Residual
> Residual Deviance: 5.026826
> Log-likelihood: -204.2742
>
> # Analyzing the modified dataset with vglm gives another set of results
>
>> vglm(severity ~ log(exposure.time), weights=freq, data=pneumo2,
> + family=cumulative(parallel=T))
>
> Coefficients:
>     (Intercept):1      (Intercept):2 log(exposure.time)
>        -1.6306499          2.5630170         -0.1937956
>
> Degrees of Freedom: 48 Total; 45 Residual
> Residual Deviance: 503.7251
> Log-likelihood: -251.8626
> Warning messages:
> 1: 4 elements replaced by 1.819e-12 in: checkwz(wz, M = M, trace =
> trace, wzeps = control$wzepsilon)
> 2: 4 elements replaced by 1.819e-12 in: checkwz(wz, M = M, trace =
> trace, wzeps = control$wzepsilon)
> 3: 4 elements replaced by 1.819e-12 in: checkwz(wz, M = M, trace =
> trace, wzeps = control$wzepsilon)
> 4: 4 elements replaced by 1.819e-12 in: checkwz(wz, M = M, trace =
> trace, wzeps = control$wzepsilon)
> 5: 4 elements replaced by 1.819e-12 in: checkwz(wz, M = M, trace =
> trace, wzeps = control$wzepsilon)
>
> # Analyzing the modified dataset with polr reproduces these second
> results
>
>> polr(severity ~ log(exposure.time), weights=freq, data=pneumo2)
>
> Coefficients:
> log(exposure.time)
>         0.1938154
>
> Intercepts:
>  mild|normal normal|severe
>    -1.630612      2.563055
>
> Residual Deviance: 503.7251
> AIC: 509.7251
>
> ------
>
> Can someone explain what I am doing wrong when using vglm and polr with
> the modified dataset?  I do not understand why these two formulations
> should give different results.
>
> Brant Inman
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From j.zutt at tudelft.nl  Sun Jan  7 12:43:11 2007
From: j.zutt at tudelft.nl (Jonne Zutt)
Date: Sun, 07 Jan 2007 12:43:11 +0100
Subject: [R] export many plots to one file
In-Reply-To: <8d5a36350701040835p1d0c4bbbqb447cfe521c2970d@mail.gmail.com>
References: <8d5a36350701040835p1d0c4bbbqb447cfe521c2970d@mail.gmail.com>
Message-ID: <45A0DCCF.2000906@tudelft.nl>

Another thing to think about is about each individual scatter plot.
Aren't you plotting too much duplicate (x,y) values?
You could try to plot only unique values, or even try to filter out 
points that are very close.


From mark at wardle.org  Sun Jan  7 13:01:32 2007
From: mark at wardle.org (Mark Wardle)
Date: Sun, 07 Jan 2007 12:01:32 +0000
Subject: [R] as.Date() results depend on order of data within vector?
Message-ID: <45A0E11C.1020906@wardle.org>

Dear all,

The as.Date() function appears to give different results depending on
the order of the vector passed into it.

d1 = c("1900-01-01", "2007-01-01","","2001-05-03")
d2 = c("", "1900-01-01", "2007-01-01","2001-05-03")
as.Date(d1)	# gives correct results
as.Date(d2)	# fails with error (* see below)

This problem does not arise if the dates are NA rather than an empty
string, but my data is coming via RODBC and I still don't have NAs
passed across properly.

I might add that I initially noticed this behaviour when using RODBC's
sqlQuery() function call, and I initially had difficulty explaining why
one column of dates was passed correctly, but another failed. The
failing column was a "date of death" column where it was NA ("") for
most patients.

I've come up with two workarounds that work. The first is to sort the
data at the SQL level, ensuring the initial record is not null. The
second is to use sqlQuery() with as.is=T option, and then do the sorting
and conversion afterwards.

Is the behaviour of as.Date() shown above as expected/designed?

Many thanks,

Mark


(*) "Error in fromchar(x) : character string is not in a standard
unambiguous format"

sessionInfo():
R version 2.4.0 (2006-10-03) powerpc-apple-darwin8.7.0 locale:
C/en_GB.UTF-8/C/C/C/C
attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"
"datasets" "base"

other attached packages:
rcompletion       RODBC
   "0.0-12"     "1.1-7"


From gavin.simpson at ucl.ac.uk  Sun Jan  7 13:35:13 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Sun, 07 Jan 2007 12:35:13 +0000
Subject: [R] as.Date() results depend on order of data within vector?
In-Reply-To: <45A0E11C.1020906@wardle.org>
References: <45A0E11C.1020906@wardle.org>
Message-ID: <1168173313.3153.3.camel@dhcppc2.my.nat.localnet>

On Sun, 2007-01-07 at 12:01 +0000, Mark Wardle wrote:
> Dear all,
> 
> The as.Date() function appears to give different results depending on
> the order of the vector passed into it.
> 
> d1 = c("1900-01-01", "2007-01-01","","2001-05-03")
> d2 = c("", "1900-01-01", "2007-01-01","2001-05-03")
> as.Date(d1)	# gives correct results
> as.Date(d2)	# fails with error (* see below)
> 
> This problem does not arise if the dates are NA rather than an empty
> string, but my data is coming via RODBC and I still don't have NAs
> passed across properly.
> 
> I might add that I initially noticed this behaviour when using RODBC's
> sqlQuery() function call, and I initially had difficulty explaining why
> one column of dates was passed correctly, but another failed. The
> failing column was a "date of death" column where it was NA ("") for
> most patients.
> 
> I've come up with two workarounds that work. The first is to sort the
> data at the SQL level, ensuring the initial record is not null. The
> second is to use sqlQuery() with as.is=T option, and then do the sorting
> and conversion afterwards.

Why not just tell R what the format the dates are in, using the "format"
argument to as.Date?

> d1 = c("1900-01-01", "2007-01-01","","2001-05-03")
> d2 = c("", "1900-01-01", "2007-01-01","2001-05-03")
> as.Date(d1, "%Y-%m-%d")
[1] "1900-01-01" "2007-01-01" NA           "2001-05-03"
> as.Date(d2, "%Y-%m-%d")
[1] NA           "1900-01-01" "2007-01-01" "2001-05-03"

> 
> Is the behaviour of as.Date() shown above as expected/designed?

I don't know about expected/designed, but I would have thought
explicitly stating the date format would be the most fool-proof way of
making sure R did what you wanted, and the easiest way to work around
your "problem".

HTH

G

> 
> Many thanks,
> 
> Mark
> 
> 
> (*) "Error in fromchar(x) : character string is not in a standard
> unambiguous format"
> 
> sessionInfo():
> R version 2.4.0 (2006-10-03) powerpc-apple-darwin8.7.0 locale:
> C/en_GB.UTF-8/C/C/C/C
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"
> "datasets" "base"
> 
> other attached packages:
> rcompletion       RODBC
>    "0.0-12"     "1.1-7"
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT                          [w] http://www.freshwaters.org.uk/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From ripley at stats.ox.ac.uk  Sun Jan  7 13:36:48 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 7 Jan 2007 12:36:48 +0000 (GMT)
Subject: [R] as.Date() results depend on order of data within vector?
In-Reply-To: <45A0E11C.1020906@wardle.org>
References: <45A0E11C.1020906@wardle.org>
Message-ID: <Pine.LNX.4.64.0701071232450.9041@gannet.stats.ox.ac.uk>

On Sun, 7 Jan 2007, Mark Wardle wrote:

> Dear all,
>
> The as.Date() function appears to give different results depending on
> the order of the vector passed into it.
>
> d1 = c("1900-01-01", "2007-01-01","","2001-05-03")
> d2 = c("", "1900-01-01", "2007-01-01","2001-05-03")
> as.Date(d1)	# gives correct results
> as.Date(d2)	# fails with error (* see below)
>
> This problem does not arise if the dates are NA rather than an empty
> string, but my data is coming via RODBC and I still don't have NAs
> passed across properly.
>
> I might add that I initially noticed this behaviour when using RODBC's
> sqlQuery() function call, and I initially had difficulty explaining why
> one column of dates was passed correctly, but another failed. The
> failing column was a "date of death" column where it was NA ("") for
> most patients.
>
> I've come up with two workarounds that work. The first is to sort the
> data at the SQL level, ensuring the initial record is not null. The
> second is to use sqlQuery() with as.is=T option, and then do the sorting
> and conversion afterwards.
>
> Is the behaviour of as.Date() shown above as expected/designed?

Yes.  It uses the first non-NA string to choose the format *if you do not 
specify it*.

The correct work-around is to get non-valid strings returned as NA, not 
"".  That is argument 'na.strings' in RODBC (and elsewhere: read.table 
behaves in the same way).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mark at wardle.org  Sun Jan  7 14:11:38 2007
From: mark at wardle.org (Mark Wardle)
Date: Sun, 07 Jan 2007 13:11:38 +0000
Subject: [R] as.Date() results depend on order of data within vector?
In-Reply-To: <Pine.LNX.4.64.0701071232450.9041@gannet.stats.ox.ac.uk>
References: <45A0E11C.1020906@wardle.org>
	<Pine.LNX.4.64.0701071232450.9041@gannet.stats.ox.ac.uk>
Message-ID: <45A0F18A.90803@wardle.org>

Prof Brian Ripley wrote:

> The correct work-around is to get non-valid strings returned as NA, not
> "".  That is argument 'na.strings' in RODBC (and elsewhere: read.table
> behaves in the same way).
> 


Thanks for these replies.

As I have mentioned before, my peculiar combination of PostgreSQL,
Actual's ODBC driver on Mac OS X, and RODBC means that for numbers and
dates, NULL values are passed to R as empty strings rather than NAs (2).
This does not occur with PostgreSQL's "text" column type.

For the benefit of others who in the future may use this combination(1),
my workaround for numbers/integers/boolean values is to essentially have
temporary intermediate tables with columns of type "text" whatever the
format, and let R/RODBC parse the strings into the correct resulting
format (which it then does faultlessly). This does not work for dates
however, and so I must use one of the two workarounds I mentioned in my
post.


Best wishes,

Mark

(1) unlikely as it may be
(2) I still cannot fathom why integers and dates are not handled correctly.


From antoniababe at yahoo.se  Sun Jan  7 15:08:52 2007
From: antoniababe at yahoo.se (antoniababe at yahoo.se)
Date: Sun, 7 Jan 2007 15:08:52 +0100 (CET)
Subject: [R] different points and lines on the same plot
Message-ID: <540854.92447.qm@web26005.mail.ukl.yahoo.com>

Dear all,

I have following data called "paitent"

day    patient1    patient4    patient5    patient6   
0   -0.27842688 -0.04080808 -0.41948398 -0.04508318 
56  -0.22275425 -0.01767067 -0.30977249 -0.03168185  
112 -0.08217659 -0.26209243 -0.29141451 -0.09876170 
252  0.08044537 -0.26701769  0.05727087 -0.09663701 

where each patient have response values at four time
points. I want to plot each patient's values over time
on the same plot where the value points are connected
by line. That is, the graph will have four lines for
the four patients. I tried the program below but
couldn't make it work correctly. I'm new beginner and
haven't yet learned how functions line and points work
together. Hope you can help me out.

Thanks for your help,

Antonia

par(mfrow=c(1,1))
        plot(patient[,1],patient[,2], pch=1,
type="l",col=1,cex=1,lwd=2,
             xlab="Days", ylab="Patient
response",cex.main =1,font.main= 1,
             main=NULL)
               
                 
points(patient[,1],patient[,3],col=2,pch=1,cex=1)
lines(patient[,1],patient[,3],col=2,lty=1,cex=1)
           
points(patient[,1],patient[,4],col=3,pch=1,cex=1)
lines(patient[,1],patient[,4],col=2,lty=2,cex=1)
       
points(patient[,1],patient[,5],col=4,pch=1,cex=1)
lines(patient[,1],patient[,5],col=2,lty=1,cex=1)
  points(patient[,1],patient[,6],col=5,pch=1,cex=1)
 lines(patient[,1],patient[,6],col=2,lty=1,cex=1)


From M.LESNOFF at CGIAR.ORG  Sun Jan  7 15:16:37 2007
From: M.LESNOFF at CGIAR.ORG (Lesnoff, Matthieu (ILRI))
Date: Sun, 7 Jan 2007 17:16:37 +0300
Subject: [R] negative binomial family glm R and STATA
In-Reply-To: <459F814A.60501@univ-fcomte.fr>
Message-ID: <6BEABCD5CA640A44A848448A42A03B730480D4F8@ilrikeadx1.ILRI.CGIARAD.ORG>

Dear Patrick

below are some comments.

For ML estimation of negative binomial glim, there is also the function
negbin in the package aod (CRAN). This function uses optim(stats). Based
on your data, we have just detected a small bug in negbin, when the
Hessian matrix (that we use for computing the variances of the ML
estimates) is singular, which seems to be the case in the model you
proposed. We will soon fix this bug and update the package. At the end
of my message, I've provided a corrected (and simplified) version of the
function, negbin0, that you can source for reproducing the code below.
Note that we don't estimate theta but phi = 1 / theta (with E[y] = mu
and Var[y] = mu + phi * mu^2).

#=== FIT OF YOUR MODEL

# The data you provided

mydata <- zonesdb4 

# Remove the unused level "0" of "axe_routier" 

mydata$axe_routier <- factor(as.character(mydata$axe_routier), levels =
c(1, 2))

# Your model

>negbin0(
>    formula = nbcas ~ pop + Area + V_100kHab + gares + ports +
axe_routier + lacs,
>    random = ~ 1,
>    control = list(maxit = 2000),
>    data = mydata,
>    )

$param
  (Intercept)           pop          Area    V_100kHab1        gares1
ports1  axe_routier2         lacs1 
 6.008098e+00  1.015842e-05 -3.019320e-06  1.556476e+00  1.267495e+00
-4.549933e+00 -3.156201e+00  4.677113e+00 

8.287353e+00 

$H.singularity
[1] TRUE

$varparam
[1] NA

$logL
[1] -418.5078

$logL.max
[1] -167.6718

$dev
[1] 501.672

$code
[1] 0

#=== END

Here phi = 8.287353 (i.e. theta = 0.1206658), log-likelihood = -418.5078
and deviance = 501.672.

Few remarks:

- our results seem not to be similar to the STATA results you provided.
If I well understood, with STATA, log-likelihood = -597.1477759 (which
is lower than ours) and theta = 1. With R, I considered all the
covariables as factors, except pop and Area (continuous). Did you the
same with STATA?

- In the optimization process used in the example, the Hessian matrix is
singular. That often occurs when the model is overparametrized (and
therefore very instable) compared to the number of data you have (I
think your model is).

- I am not sure that the type of model you proposed is the most adapted.
Why not a model such as "log(nbcas / pop) = X b", which is commonly used
(see Agresti, 1990. Categorical data analysis. Wiley) for analysing
rates of occurence of events, for example in epidemiology? With negbin,
this model is (with only considering axe_routier): 
 
> negbin(
+     formula = nbcas ~ axe_routier + offset(log(pop)),
+     random = ~ 1,
+     data = mydata
+     )

Negative-binomial model
-----------------------
negbin(formula = nbcas ~ axe_routier + offset(log(pop)), random = ~1, 
    data = mydata)

Convergence was obtained after 82 iterations.

Fixed-effect coefficients:
             Estimate Std. Error  z value Pr(> |z|)
(Intercept)   -6.5072     0.4888 -13.3114    < 1e-4
axe_routier2   1.0234     0.6839   1.4964    0.1346

Overdispersion coefficients:
                Estimate Std. Error z value Pr(> z)
phi.(Intercept)  10.7611     1.7936  5.9997  < 1e-4

Log-likelihood statistics
 Log-lik    nbpar  df res. Deviance      AIC     AICc 
-411.192        3       89  487.040  828.384  828.656 

- Finally, the response variable "nbcas" has a lot of values 0. A
zero-inflated model could perhaps much better fit the data.

Best wishes

Matthieu


#==============================================
#==== FUNCTION negbin0 (TO SOURCE)
#==============================================

negbin0 <- function(formula, random, data, phi.ini = NULL, fixpar =
list(), hessian = TRUE,...){
    link <- "log"
    f <- formula
    mf <- model.frame(formula = f, data = data)
    y <- model.response(mf)
    fam <- poisson(link = "log")
    fm <- glm(formula = f, family = fam, data = data)
    offset <- model.offset(mf)
    # Model matrices
    modmatrix.b <- model.matrix(object = f, data = data)
    if(random != ~ 1)
        random <- update.formula(random, ~ . - 1)
    modmatrix.phi <- model.matrix(object = random, data = data)
    nb.b <- ncol(modmatrix.b) ; nb.phi <- ncol(modmatrix.phi) ; nbpar <-
nb.b + nb.phi
    # Initial values
    if(is.null(phi.ini)) phi.ini <- rep(0.1, nb.phi)
    b <- fm$coefficients
    param.ini <- c(b, phi.ini)
    if(is.null(unlist(fixpar)) == FALSE) param.ini[fixpar[[1]]] <-
fixpar[[2]]  
    # minuslogL
    minuslogL <- function(param){
        if(!is.null(unlist(fixpar))) param[fixpar[[1]]] <- fixpar[[2]]  
        b <- param[1:nb.b]
        eta <- as.vector(modmatrix.b %*% b)
        mu <- if(is.null(offset)) exp(eta) else exp(offset + eta)
        phi <- as.vector(modmatrix.phi %*% param[(nb.b + 1):(nb.b +
nb.phi)])
        k <- 1 / phi 
        cnd <- phi == 0
        f1 <- dpois(x = y[cnd], lambda = mu[cnd], log = TRUE) 
        y2 <- y[!cnd]; k2 <- k[!cnd]; mu2 <- mu[!cnd]
        f2 <- lgamma(y2 + k2) - lfactorial(y2) - lgamma(k2) + k2 *
log(k2 / (k2 + mu2)) + y2 * log(mu2 / (k2 + mu2))
        fn <- sum(c(f1, f2))
        if(!is.finite(fn)) fn <- -1e20
        -fn
        }
    # Fit
    res <- optim(par = param.ini, fn = minuslogL, hessian = hessian,
...)
    ## Results
    param <- res$par
    if(is.null(unlist(fixpar)) == FALSE) param[fixpar[[1]]] <-
fixpar[[2]]
    H <- NA ; H.singularity <- NA ; varparam <- NA    
    if(hessian == TRUE){
        H <- res$hessian
        if(is.null(unlist(fixpar))) {
            H.singularity <- ifelse(qr(H)$rank < nrow(H), TRUE, FALSE)
            if(!H.singularity) varparam <- qr.solve(H)
            }
        else{
            idparam <- 1:(nb.b + nb.phi)
            idestim <- idparam[-fixpar[[1]]]
            Hr <- H[-fixpar[[1]], -fixpar[[1]]]
            H.singularity <- ifelse(qr(Hr)$rank < nrow(Hr), TRUE, FALSE)
            if(!H.singularity) {
                Vr <- solve(Hr) ; dimnames(Vr) <- list(idestim, idestim)
                varparam <- matrix(0, nrow = nrow(H), ncol = ncol(H)) ;
varparam[idestim, idestim] <- Vr
                }
            }
        }
    if(is.null(unlist(fixpar))) nbpar <- length(param) else nbpar <-
length(param[-fixpar[[1]]])
    logL.max <- sum(dpois(x = y, lambda = y, log = TRUE))
    logL <- -res$value
    dev <- -2 * (logL - logL.max)
    df.residual <- length(y) - nbpar 
    iterations <- res$counts
    code <- res$convergence
    res <- list(
        link = link, formula = formula, random = random, param = param, 
        H = H, H.singularity = H.singularity, varparam = varparam, logL
= logL, logL.max = logL.max, 
        dev = dev, nbpar = nbpar, df.residual = df.residual, iterations
= iterations, 
        code = code, param.ini = param.ini
        )
    res
    }
  

--------------------------------------------------
Matthieu Lesnoff
International Livestock Research Institute (ILRI)
Lab. 8
Old Naivasha Road
PO BOX 30709
00100 GPO Nairobi, Kenya
Tel:   Off: (+254) 20 422 3000 (ext. 4801)
       Res: (+254) 20 422 3134
       Mob: (+254) 725 785 570
       Sec: (+254) 20 422 3013 
Fax: (+254) 20 422 3001
Email: m.lesnoff at cgiar.org 
--------------------------------------------------     



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Patrick Giraudoux
> Sent: samedi 6 janvier 2007 14:00
> To: r-help at stat.math.ethz.ch
> Cc: Bertrand SUDRE
> Subject: [R] negative binomial family glm R and STATA
> 
> Dear Lister,
> 
> I am facing a strange problem fitting a GLM of the negative 
> binomial family. Actually, I tried to estimate theta (the 
> scale parameter) through glm.nb from MASS and could get 
> convergence only relaxing the convergence tolerance to 1e-3. 
> With warning messages:
> 
>  
> glm1<-glm.nb(nbcas~.,data=zonesdb4,control=glm.control(epsilon
>  = 1e-3)) There were 25 warnings (use warnings() to see them) 
>  > warnings() Warning messages:
> 1: iteration limit reached in: theta.ml(Y, mu, n, w, limit = 
> control$maxit, trace = control$trace >   ...
> 2: NaNs produced in: sqrt(1/i)
> 
> etc....
> 
> The estimate of theta was: 0.0939. When trying to compute 
> confidence interval then, I got this message:
> 
>  > confint(glm1a)
> Waiting for profiling to be done...
> Error in profile.glm(object, which = parm, alpha = (1 - 
> level)/4, trace = trace) :
>         profiling has found a better solution, so original 
> fit had not converged
> 
> Moreover, trying some other solutions "by hand" (without 
> warnings produced, here) with glm(.... 
> family=negative.binomial(1)....),  I found that theta = 0.7 
> lead to a much lower AIC (AIC= 1073) than theta = 1 (AIC=1211).
> 
> Facing such unstable results my first reaction has been to 
> forget about fitting a negative binomial model on this data 
> set. The reader will find the dataset in a dumped format 
> below for trials.
> 
> A friend of mine tried the same with STATA and got the 
> following result without any warning  from STATA :
> 
> . glm nbcas pop area v_100khab gares ports axe_routier lacs,
> family(nbinomial) link(log) eform
>  
> Iteration 0:   log likelihood = -616.84031 
> Iteration 1:   log likelihood = -599.77767 
> Iteration 2:   log likelihood = -597.22486 
> Iteration 3:   log likelihood = -597.14784 
> Iteration 4:   log likelihood = -597.14778 
> Iteration 5:   log likelihood = -597.14778 
>  
> Generalized linear models                          No. of obs      
> =        92
> Optimization     : ML                              Residual df     
> =        84
>                                                    Scale parameter 
> =         1
> Deviance         =  597.0007978                    (1/df) Deviance =  
> 7.107152
> Pearson          =  335.6135273                    (1/df) Pearson  =  
> 3.995399
>  
> Variance function: V(u) = u+(1)u^2                 [Neg. Binomial]
> Link function    : g(u) = ln(u)                    [Log]
>  
>                                                    AIC             =  
> 13.15539
> Log likelihood   = -597.1477759                    BIC             =  
> 217.1706
>  
> --------------------------------------------------------------
> ---------------- 
> 
>              |                 OIM
>        nbcas |        IRR   Std. Err.      z    P>|z|     [95% Conf. 
> Interval]
> -------------+------------------------------------------------
> ----------
> -------------+------
> 
>          pop |   1.000011   1.82e-06     6.02   0.000     1.000007    
> 1.000014
>         area |   1.000014   .0000244     0.57   0.569     .9999661    
> 1.000062
>    v_100khab |   2.485394   .7924087     2.86   0.004     1.330485    
> 4.642806
>        gares |   2.185483   .7648255     2.23   0.025     1.100686    
> 4.339418
>        ports |   .1805793    .100423    -3.08   0.002     .0607158    
> .5370744
>  axe_routier |    .828243   .2258397    -0.69   0.489     .4853532    
> 1.413376
>         lacs |   20.50183   12.17126     5.09   0.000     6.404161    
> 65.63311
> 
> 
> Has somebody an idea about (1) why the AIC values given are 
> so different between softwares (R = 1211, STATA= 13.15) for 
> the same model and (2) what can explain so different 
> behaviour between R and STATA ?
> 
> Here below the data.frame:
> 
> 
> zonesdb4 <-
> structure(list(nbcas = as.integer(c(318, 0, 42, 3011, 6, 911, 
> 45, 273, 0, 0, 89, 122, 407, 83, 0, 1844, 58, 0, 0, 0, 0, 
> 8926, 0, 0, 0, 0, 108, 0, 13, 1884, 0, 0, 0, 0, 963, 0, 199, 
> 735, 0, 2182, 971, 0, 65, 0, 7927, 30, 0, 186, 0, 1363, 808, 
> 0, 0, 0, 0, 135, 0, 1338, 0, 0, 488, 0, 344, 0, 0, 454, 4808, 
> 0, 692, 0, 0, 24, 1301, 0, 0, 474, 228, 0, 0, 98, 44, 0, 0, 
> 0, 1562, 375, 327, 0, 0, 0, 0, 0)), pop = 
> as.integer(c(247215, 55709, 63625, 253153, 51789, 142806, 
> 129839, 95799, 129996, 66668, 76043, 267232, 153200, 136333, 
> 264888, 198244, 233600, 89152, 128085, 71803, 81911, 122523, 
> 149806, 122470, 50979, 160773, 80700, 56146, 226965, 245322, 
> 165768, 215129, 46843, 108471, 108690, 188724, 161794, 
> 226965, 95850, 156326, 145291, 51789, 218808, 53189, 245854, 
> 152047, 146509, 243765, 65012, 226830, 66742, 144762, 93858, 
> 73793, 123107, 189793, 91013, 135212, 67487, 105050, 194903, 
> 206606, 62169, 96832, 145570, 167062, 1598576, 146509, 
> 103928, 118334, 91509, 295644, 139650, 106980, 66529, 126126, 
> 257341, 56973, 33793, 164072, 145225, 155638, 131100, 100880, 
> 245482, 166213, 127365, 113713, 57540, 78571, 62499, 81916)), 
> Area = c(10027.1, 9525.3, 638.646, 861.486, 4966.32, 11973, 
> 1823.89, 1327.45, 789.595, 4892.38, 638.908, 15959.8, 
> 2036.56, 7397.62, 4626.03, 10237.5, 9823.24, 4253.59, 
> 2448.78, 12280.2, 910.972, 16365, 2041.92, 4343.46, 1081.42, 
> 1601.11, 4664.47, 335.865, 2818.68, 7348.1, 1148.41, 265.158, 
> 14883.6, 3698.58, 12444.6, 1711.45, 15462, 10419.5, 13119.2, 
> 1276.14, 872.91, 19291.4, 6719.82, 8505.53, 13219.8, 13069, 
> 5212.03, 3924.42, 791.219, 881.281, 10038.5, 9101.94, 
> 7925.71, 943.062, 9888, 20585.3, 4600.35, 3258.27, 11813.4, 
> 130.184, 10644.3, 1925.89, 1892.88, 3833.6, 350.3, 7154.79, 
> 2800.63, 559.986, 3152, 7095.39, 6058.3, 113.225, 5067.66, 
> 1293.05, 15109.8, 4111.54, 94.5213, 4012.91, 1468.02, 
> 10651.3, 8541.69, 1806.28, 20166.3, 1110.75, 2026.98, 
> 21114.4, 2041.51, 17740.9, 16627.5, 15266.1, 525.467, 
> 371.132), V_100kHab = structure(as.integer(c(1, 1, 1, 1, 1, 
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 
> 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 
> 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 
> 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 
> 1, 2, 1, 1, 1, 1, 2)), .Label = c("0", "1"), class = "factor"),
>     gares = structure(as.integer(c(2, 2, 1, 1, 1, 1, 1, 1, 1,
>     2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1,
>     1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
>     1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,
>     1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1,
>     1, 1, 1, 1, 1, 1, 1)), .Label = c("0", "1"), class = "factor"),
>     ports = structure(as.integer(c(2, 1, 1, 1, 1, 1, 1, 1, 1,
>     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1,
>     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,
>     2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,
>     1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1,
>     2, 1, 1, 1, 1, 1, 1)), .Label = c("0", "1"), class = "factor"),
>     axe_routier = structure(as.integer(c(2, 3, 3, 3, 2, 2, 2,
>     3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2,
>     3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3,
>     2, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2,
>     3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 2, 3, 3,
>     3, 2, 3, 3, 3, 2, 2, 3, 3)), .Label = c("0", "1", "2"), 
> class = "factor"),
>     lacs = structure(as.integer(c(1, 1, 1, 1, 1, 1, 1, 1, 1,
>     1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1,
>     1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2,
>     2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,
>     1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1,
>     2, 1, 1, 1, 1, 1, 1)), .Label = c("0", "1"), class = 
> "factor")), .Names = c("nbcas", "pop", "Area", "V_100kHab", 
> "gares", "ports", "axe_routier", "lacs"), row.names = 
> c("Ankoro", "Dilolo", "Tshitenge", "Tshitshimbi", "Tshofa", 
> "Tshudi Loto", "Vanga Kete", "Wikong", "Djalo Djeka", 
> "Fungurume", "Gandajika", "Kabalo", "Kabeya Kamuanga", 
> "Kabinda", "Kabondo Dianda", "Kabongo", "Kafakumba", "Bena 
> Dibele", "Kafubu", "Kalamba", "Kalambayi Kabanga", "Kalemie", 
> "Kalenda", "Kalonda Est", "Kamalondo", "Kamana", "Kambove", 
> "Kamiji", "Bibanga", "Kamina", "Kampemba", "Kanda Kanda", 
> "Kansimba", "Kanzenze", "Kapanga", "Kapolowe", "Kasaji", 
> "Kasenga", "Katako Kombe", "Katuba", "Kenya", "Kiambi", 
> "Kikula", "Kilela Balanda", "Kilwa", "Kinda", "Bukenya", 
> "Kinkondja", "Kipushi", "Kisanga", "Kitenge", "Kole", 
> "Kongolo", "Likasi", "Lodja", "Lomela", "Lualaba", "Butumba", 
> "Lubao", "Lubilanji", "Lubudi", "Lubumbashi", "Ludimbi 
> Lukula", "Lukafu", "Lukelenge", "Lusambo", "Cilundu", 
> "Makota", "Malemba Nkulu", "Manika", "Manono", "Miabi", 
> "Minga", "Muene Ditu", "Mufunga Sampwe", "Mukanga", 
> "Mukumbi", "Mulongo", "Mulumba", "Mutshatsha", "Nyemba", 
> "Dilala", "Nyunzu", "Panda", "Pania Mutombo", "Pweto", 
> "Rwashi", "Sakania", "Sandoa", "Songa", "Tshamilemba", 
> "Tshilenge"), class = "data.frame", na.action = 
> structure(as.integer(c(8, 34, 40, 41, 45, 71, 73, 79, 80, 83, 
> 84, 85, 86, 93)), .Names = c("Wembo Nyama", "Kaniama", 
> "Kasansa", "Bukama", "Kayamba", "Luputa", "Lwamba", "Mbuji 
> mayi", "Mbulula", "Mitwaba", "Moba", "Dikungu Tshumbe", 
> "Mpokolo", "Mumbunda"), class = "omit"))
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Sun Jan  7 15:32:54 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 7 Jan 2007 09:32:54 -0500
Subject: [R] different points and lines on the same plot
In-Reply-To: <540854.92447.qm@web26005.mail.ukl.yahoo.com>
References: <540854.92447.qm@web26005.mail.ukl.yahoo.com>
Message-ID: <971536df0701070632u3ca99cd7la631ce637bac7ae@mail.gmail.com>

Try this:

matplot(patient[,1], patient[,-1], type = "o")


On 1/7/07, antoniababe at yahoo.se <antoniababe at yahoo.se> wrote:
> Dear all,
>
> I have following data called "paitent"
>
> day    patient1    patient4    patient5    patient6
> 0   -0.27842688 -0.04080808 -0.41948398 -0.04508318
> 56  -0.22275425 -0.01767067 -0.30977249 -0.03168185
> 112 -0.08217659 -0.26209243 -0.29141451 -0.09876170
> 252  0.08044537 -0.26701769  0.05727087 -0.09663701
>
> where each patient have response values at four time
> points. I want to plot each patient's values over time
> on the same plot where the value points are connected
> by line. That is, the graph will have four lines for
> the four patients. I tried the program below but
> couldn't make it work correctly. I'm new beginner and
> haven't yet learned how functions line and points work
> together. Hope you can help me out.
>
> Thanks for your help,
>
> Antonia
>
> par(mfrow=c(1,1))
>        plot(patient[,1],patient[,2], pch=1,
> type="l",col=1,cex=1,lwd=2,
>             xlab="Days", ylab="Patient
> response",cex.main =1,font.main= 1,
>             main=NULL)
>
>
> points(patient[,1],patient[,3],col=2,pch=1,cex=1)
> lines(patient[,1],patient[,3],col=2,lty=1,cex=1)
>
> points(patient[,1],patient[,4],col=3,pch=1,cex=1)
> lines(patient[,1],patient[,4],col=2,lty=2,cex=1)
>
> points(patient[,1],patient[,5],col=4,pch=1,cex=1)
> lines(patient[,1],patient[,5],col=2,lty=1,cex=1)
>  points(patient[,1],patient[,6],col=5,pch=1,cex=1)
>  lines(patient[,1],patient[,6],col=2,lty=1,cex=1)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Inman.Brant at mayo.edu  Sun Jan  7 17:27:34 2007
From: Inman.Brant at mayo.edu (Inman, Brant A.   M.D.)
Date: Sun, 7 Jan 2007 10:27:34 -0600
Subject: [R] Using VGAM's vglm function for ordinal logistic regression
Message-ID: <6021CA6EF4C8374084D4F5A141F1CBBB6649E2@msgebe23.mfad.mfroot.org>

Thank you for the help.  Indeed, the differences in the results that I
noted were due to the incorrect ordering of the response variable that
resulted from my unattentive use of as.ordered on a character vector.

Brant


-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Sunday, January 07, 2007 2:52 AM
To: Inman, Brant A. M.D.
Cc: r-help at stat.math.ethz.ch; yee at stat.auckland.ac.nz
Subject: Re: [R] Using VGAM's vglm function for ordinal logistic
regression

On Sat, 6 Jan 2007, Inman, Brant A.   M.D. wrote:

>
> R-Experts:
>
> I am using the vglm function of the VGAM library to perform
proportional
> odds ordinal logistic regression.  The issue that I would like help
with
> concerns the format in which the response variable must be provided
for
> this function to work correctly.

> pneumo2$severity
  [1] normal normal normal normal normal normal normal normal mild
mild
[11] mild   mild   mild   mild   mild   mild   severe severe severe
severe
[21] severe severe severe severe
Levels: mild < normal < severe

is different from the ordering in the first example.

The difference between vglm and polr is that the latter uses the 
conventional sign for the coefficients: see MASS4 p.204.

I would never use as.ordered on a character vector, as this leaves it to
R 
to choose the ordering.  (Even if you think you intended alphabetical 
order, that depends on the locale: see the warnings on the help page.)


> Consider the following example:
>
> ------
>
> library(VGAM)
> library(MASS)
>
> attach(pneumo)
> pneumo	# Inspect the format of the original dataset
>
> # Restructure the pneumo dataset into a different format
> pneumo2 <- data.frame(matrix(ncol=3, nrow=24))
> colnames(pneumo2) <- c('exposure.time', 'severity', 'freq')
> pneumo2[,1] <- rep(pneumo[,1],3)
> pneumo2[,2] <-
> as.ordered(c(rep('normal',8),rep('mild',8),rep('severe',8)))
> pneumo2[1:8,3]   <- pneumo[,2]
> pneumo2[9:16,3]  <- pneumo[,3]
> pneumo2[17:24,3] <- pneumo[,4]
> pneumo2	# Inspect the format of the new modified dataset
>
> ------
>
> The problem occurs when I try to analyze these two datasets, which are
> identical in content, with the proportional odds model using vglm:
>
> ------
>
> # Analyze the original dataset with vglm, get one set of results
>
>> vglm(vglm(cbind(normal, mild, severe) ~ log(exposure.time),
> data=pneumo,
> +  family=cumulative(parallel=T))
>
> Coefficients:
>     (Intercept):1      (Intercept):2 log(exposure.time)
>          9.676093          10.581725          -2.596807
>
> Degrees of Freedom: 16 Total; 13 Residual
> Residual Deviance: 5.026826
> Log-likelihood: -204.2742
>
> # Analyzing the modified dataset with vglm gives another set of
results
>
>> vglm(severity ~ log(exposure.time), weights=freq, data=pneumo2,
> + family=cumulative(parallel=T))
>
> Coefficients:
>     (Intercept):1      (Intercept):2 log(exposure.time)
>        -1.6306499          2.5630170         -0.1937956
>
> Degrees of Freedom: 48 Total; 45 Residual
> Residual Deviance: 503.7251
> Log-likelihood: -251.8626
> Warning messages:
> 1: 4 elements replaced by 1.819e-12 in: checkwz(wz, M = M, trace =
> trace, wzeps = control$wzepsilon)
> 2: 4 elements replaced by 1.819e-12 in: checkwz(wz, M = M, trace =
> trace, wzeps = control$wzepsilon)
> 3: 4 elements replaced by 1.819e-12 in: checkwz(wz, M = M, trace =
> trace, wzeps = control$wzepsilon)
> 4: 4 elements replaced by 1.819e-12 in: checkwz(wz, M = M, trace =
> trace, wzeps = control$wzepsilon)
> 5: 4 elements replaced by 1.819e-12 in: checkwz(wz, M = M, trace =
> trace, wzeps = control$wzepsilon)
>
> # Analyzing the modified dataset with polr reproduces these second
> results
>
>> polr(severity ~ log(exposure.time), weights=freq, data=pneumo2)
>
> Coefficients:
> log(exposure.time)
>         0.1938154
>
> Intercepts:
>  mild|normal normal|severe
>    -1.630612      2.563055
>
> Residual Deviance: 503.7251
> AIC: 509.7251
>
> ------
>
> Can someone explain what I am doing wrong when using vglm and polr
with
> the modified dataset?  I do not understand why these two formulations
> should give different results.
>
> Brant Inman
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Inman.Brant at mayo.edu  Sun Jan  7 18:05:22 2007
From: Inman.Brant at mayo.edu (Inman, Brant A.   M.D.)
Date: Sun, 7 Jan 2007 11:05:22 -0600
Subject: [R] Partial proportional odds logistic regression
Message-ID: <6021CA6EF4C8374084D4F5A141F1CBBB6649E3@msgebe23.mfad.mfroot.org>


R-experts:

I would like to explore the partial proportional odds models of Peterson
and Harrell (Applied Statistics 1990, 39(2): 205-217) for a dataset that
I am analyzing.  I have not been able to locate a R package that
implements these models.  Is anyone aware of existing R functions,
packages, etc... that might be used to implement the partial
proportional odds models?

Brant Inman


From niederlein-rstat at yahoo.de  Sun Jan  7 18:11:27 2007
From: niederlein-rstat at yahoo.de (Antje)
Date: Sun, 07 Jan 2007 18:11:27 +0100
Subject: [R] Multiple plots via sapply or lapply?
Message-ID: <45A129BF.7070205@yahoo.de>

Hi all,

I've got the following problem. I have a vector containing file names. I 
want to read these files as csv and calculate the density-function for 
each file (has just one column with data). Then, I'd like to plot all 
density functions into one window. I did the following to calculate the 
density data:

s <- sapply(filelist, function(x) {
		if(file.exists(x))
		{
			file <- read.csv(x, sep="\t", header=F)
			return( list(density(file$V1)$x, density(file$V1)$y))
		}
	})

Now I would like to plot these x,y data in a similar way but my result 
"s" is a matrix containing lists...

      File1.csv 		File2.csv 	File3.csv
[1,] Numeric,512  	Numeric,512  	Numeric,512
[2,] Numeric,512  	Numeric,512  	Numeric,512

Now I don't know how to handle the x,y values for each plot into an 
sapply (or lapply, I don't know)

Any idea? Maybe, I should somehow change the return type?

Antje


From laurentRhelp at free.fr  Sun Jan  7 19:33:49 2007
From: laurentRhelp at free.fr (Laurent Rhelp)
Date: Sun, 07 Jan 2007 19:33:49 +0100
Subject: [R] odfWeave and figures in MS Word Format
Message-ID: <45A13D0D.90004@free.fr>

Dear R-List,

   I try to use the package odfWeave but I missed something. I use the 
command odfWeave("examples.odt","out.odt") to generate the file that I 
can open with OpenOffice : it works fine. Then, I used the "Save As"  to 
export the document to HTML format : it works fine and creates the .png 
files for every figure from  the out.odt document. But when I export the 
document to MS Word format (OS : XP) there is no figure in the word 
document .doc.

What did I do wrong ?

Thanks

Laurent

##
R  version
               _                        
platform       i386-pc-mingw32          
arch           i386                     
os             mingw32                  
system         i386, mingw32            
status                                  
major          2                        
minor          3.1                      
year           2006                     
month          06                       
day            01                       
svn rev        38247                    
language       R                        
version.string Version 2.3.1 (2006-06-01)

##

OpenOffice Version

2.1

##


From roger.bos at us.rothschild.com  Sun Jan  7 20:34:27 2007
From: roger.bos at us.rothschild.com (Bos, Roger)
Date: Sun, 7 Jan 2007 14:34:27 -0500
Subject: [R] memory problem
Message-ID: <D8C95B444AD6EE4AAD638D818A9CFD341DC18D@RINNYCSE000.rth.ad.rothschild.com>

Doing so will also require using the Linux version of R as there is no
open source 64bit compiler to create a windows version (so I am told).
Anyway, memory is getting cheap now.  I got a 64bit machine from Dell
with 16Gb od DDR2 for around $4000.  Good luck.
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
Sent: Sunday, January 07, 2007 3:42 AM
To: Zoltan Kmetty
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] memory problem



Zoltan Kmetty wrote:
> Hi!
> 
> I had some memory problem with R - hope somebody could tell me a
solution.
> 
> I work with very large datasets, but R cannot allocate enough memoty 
> to handle these datasets.
> 
> I want work a matrix with row= 100 000 000 and column=10
> 
> A know this is 1 milliard cases, but i thought R could handle it 
> (other commercial software like spss could do), but R wrote out 
> everytime: not enough memory..
> 
> any good idea?


Buy a machine that has at least 8Gb (better 16Gb) of RAM and proceed ...

Uwe Ligges



> Thanks, Zoltan
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

********************************************************************** * 
This message is for the named person's use only. It may 
contain confidential, proprietary or legally privileged 
information. No right to confidential or privileged treatment 
of this message is waived or lost by any error in 
transmission. If you have received this message in error, 
please immediately notify the sender by e-mail, 
delete the message and all copies from your system and destroy 
any hard copies. You must not, directly or indirectly, use, 
disclose, distribute, print or copy any part of this message 
if you are not the intended recipient.


From p_connolly at ihug.co.nz  Sun Jan  7 20:42:32 2007
From: p_connolly at ihug.co.nz (Patrick Connolly)
Date: Mon, 8 Jan 2007 08:42:32 +1300
Subject: [R] as.Date() results depend on order of data within vector?
In-Reply-To: <45A0E11C.1020906@wardle.org>
References: <45A0E11C.1020906@wardle.org>
Message-ID: <20070107194231.GD4736@ihug.co.nz>

On Sun, 07-Jan-2007 at 12:01PM +0000, Mark Wardle wrote:

|> Dear all,
|> 
|> The as.Date() function appears to give different results depending on
|> the order of the vector passed into it.
|> 
|> d1 = c("1900-01-01", "2007-01-01","","2001-05-03")
|> d2 = c("", "1900-01-01", "2007-01-01","2001-05-03")
|> as.Date(d1)	# gives correct results
|> as.Date(d2)	# fails with error (* see below)
|> 
|> This problem does not arise if the dates are NA rather than an empty
|> string, but my data is coming via RODBC and I still don't have NAs
|> passed across properly.
|> 
|> I might add that I initially noticed this behaviour when using RODBC's
|> sqlQuery() function call, and I initially had difficulty explaining why
|> one column of dates was passed correctly, but another failed. The
|> failing column was a "date of death" column where it was NA ("") for
|> most patients.
|> 
|> I've come up with two workarounds that work. The first is to sort the
|> data at the SQL level, ensuring the initial record is not null. The
|> second is to use sqlQuery() with as.is=T option, and then do the sorting
|> and conversion afterwards.

Simpler, I think, is to add one line
d2[d2 == ""] <- NA

I've not tested the idea extensively, so there might be occasions
where it falls down.  If you're working with a dataframe, you can use
one of the apply functions to effect all columns.


HTH

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From paul.boutros at utoronto.ca  Sun Jan  7 05:22:15 2007
From: paul.boutros at utoronto.ca (Paul Boutros)
Date: Sat, 06 Jan 2007 23:22:15 -0500
Subject: [R] creating a list of lists
Message-ID: <20070106232215.vmionf6pl7sos8w8@webmail.utoronto.ca>

Hello,

I'm trying to create a series of randomForest objects, basically in a  
loop like this:

forests <- list();

for (level in 1:10) {

          # do some other things here

	# create a random forest
	forest <- randomForest(
		x = x.level,
		y = z.level,
		ntree = trees
		);

	forests <- c(forests, forest);

	}


But instead of creating a list of 10 forests, this creates a list of  
180 elements, 18 for each forest.  Is there a way to create a list of  
randomForest objects for use later on in code like:

for (forest in forests) {
         values <- predict(forest, data);
         # do things with these predicted values
         }

Sincerely,
Paul


From mtmorgan at fhcrc.org  Sat Jan  6 17:51:23 2007
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Sat, 06 Jan 2007 08:51:23 -0800
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <624934630701060616i2718ab71xb981fbca39c0c910@mail.gmail.com>
	(Ramon Diaz-Uriarte's message of "Sat, 6 Jan 2007 15:16:51 +0100")
References: <07E228A5BE53C24CAD490193A7381BBB77C654@LP-EXCHVS07.CO.IHC.COM>
	<624934630701060616i2718ab71xb981fbca39c0c910@mail.gmail.com>
Message-ID: <6phps9sm73o.fsf@gopher4.fhcrc.org>

Hi Ramon,

It seems like a naming convention (f.xxx) and eval(parse(...)) are
standing in for objects (of class 'GeneSelector', say, representing a
function with a particular form and doing a particular operation) and
dispatch (a function 'geneConverter' might handle a converter of class
'GeneSelector' one way, user supplied ad-hoc functions more carefully;
inside geneConverter the only real concern is that the converter
argument is in fact a callable function).

eval(parse(...)) brings scoping rules to the fore as an explicit
programming concern; here scope is implicit, but that's probably better
-- R will get its own rules right.

Martin

Here's an S4 sketch:

setClass("GeneSelector",
         contains="function",
         representation=representation(description="character"),
         validity=function(object) {
             msg <- NULL
             argNames <- names(formals(object))
             if (argNames[1]!="x")
               msg <- c(msg, "\n  GeneSelector requires a first argument named 'x'")
             if (!"..." %in% argNames)
               msg <- c(msg, "\n  GeneSelector requires '...' in its signature")
             if (0==length(object at description))
               msg <- c(msg, "\n  Please describe your GeneSelector")
             if (is.null(msg)) TRUE else msg
         })

setGeneric("geneConverter",
           function(converter, x, ...) standardGeneric("geneConverter"),
           signature=c("converter"))

setMethod("geneConverter",
          signature(converter="GeneSelector"),
          function(converter, x, ...) {
              ## important stuff here
              converter(x, ...)
          })

setMethod("geneConverter",
          signature(converter="function"),
          function(converter, x, ...) {
              message("ad-hoc converter; hope it works!")
              converter(x, ...)
          })

and then...

> c1 <- new("GeneSelector",
+           function(x, ...) prod(x, ...),
+           description="Product of x")
> 
> c2 <- new("GeneSelector",
+           function(x, ...) sum(x, ...),
+           description="Sum of x")
> 
> geneConverter(c1, 1:4)
[1] 24
> geneConverter(c2, 1:4)
[1] 10
> geneConverter(mean, 1:4)
ad-hoc converter; hope it works!
[1] 2.5
> 
> cvterr <- new("GeneSelector", function(y) {})
Error in validObject(.Object) : invalid class "GeneSelector" object: 1: 
  GeneSelector requires a first argument named 'x'
invalid class "GeneSelector" object: 2: 
  GeneSelector requires '...' in its signature
invalid class "GeneSelector" object: 3: 
  Please describe your GeneSelector
> xxx <- 10
> geneConverter(xxx, 1:4)
Error in function (classes, fdef, mtable)  : 
	unable to find an inherited method for function "geneConverter", for signature "numeric"


"Ramon Diaz-Uriarte" <rdiaz02 at gmail.com> writes:

> Dear Greg,
>
>
> On 1/5/07, Greg Snow <Greg.Snow at intermountainmail.org> wrote:
>> Ramon,
>>
>> I prefer to use the list method for this type of thing, here are a couple of reasons why (maybe you are more organized than me and would never do some of the stupid things that I have, so these don't apply to you, but you can see that the general suggestion applys to some of the rest of us).
>>
>
>
> Those suggestions do apply to me of course (no claim to being
> organized nor beyond idiocy here). And actually the suggestions on
> this thread are being very useful. I think, though, that I was not
> very clear on the context and my examples were too dumbed down. So
> I'll try to give more detail (nothing here is secret, I am just trying
> not to bore people).
>
> The code is part of a web-based application, so there is no
> interactive user. The R code is passed the arguments (and optional
> user functions) from the CGI.
>
> There is one "core" function (call it cvFunct) that, among other
> things, does cross-validation. So this is one way to do things:
>
> cvFunct <- function(whatever, genefiltertype, whateverelse) {
>       internalGeneSelect <- eval(parse(text = paste("geneSelect",
>                                              genefiltertype, sep = ".")))
>
>       ## do things calling internalGeneSelect,
> }
>
> and now define all possible functions as
>
> geneSelect.Fratio <- function(x, y, z) {##something}
> geneSelect.Wilcoxon <- function(x, y, z) {## something else}
>
> If I want more geneSelect functions, adding them is simple. And I can
> even allow the user to pass her/his own functions, with the only
> restriction that it takes three args, x, y, z, and that the function
> is to be called: "geneSelect." and a user choosen string. (Yes, I need
> to make sure no calls to "system", etc, are in the user code, etc,
> etc, but that is another issue).
>
> The general idea is not new of course. For instance, in package
> "e1071", a somewhat similar thing is done in function "tune", and
> David Meyer there uses "do.call". However, tune is a lot more general
> than what I had in mind. For instance, "tune" deals with arbitrary
> functions, with arbitrary numbers and names of parameters, whereas my
> functions above all take only three arguments (x: a matrix, y: a
> vector; z: an integer), so the neat functionality provided by
> "do.call", and passing the args as a list is not really needed.
>
> So, given that my situation is so structured, and I do not need
> "do.call", I think the approach via eval(parse(paste makes my life
> simple:
>
> a) the central function (cvFunct) uses something I can easily
> recognize: "internalGeneSelect"
>
> b) after the initial eval(parse(text I do not need to worry anymore
> about what the "true" gene selection function is called
>
> c) adding new functions and calling them is simple: function naming
> follows a simple pattern ("geneSelect." + postfix) and calling the
> user function only requires passing the postfix to cvFunct.
>
> d) notice also that, at least the functs. I define, will of course not
> be named "f.1", etc, but rather things like "geneSelect.Fratio" or
> "geneSelect.namesThatStartWithCuteLetters";
>
> I hope this makes things more clear. I did not include this detail
> because this is probably boring (I guess most of you have stopped
> reading by now :-).
>
>
>> Using the list forces you to think about what functions may be called and thinking about things before doing them is usually a good idea.  Personally I don't trust the user of my functions (usually my future self who has forgotten something that seemed obvious at the time) to not do something stupid with them.
>>
>> With list elements you can have names for the functions and access them either by the name or by a number, I find that a lot easier when I go back to edit/update than to remember which function f.1 or f.2 did what.
>>
>
> But I don't see how having your functions as list elements is easier
> (specially if the function is longer than 2 to 3 lines) than having
> all functions systematically named things such as:
>
> geneSelect.Fratio
> geneSelect.Random
> geneSelect.LetterA
> etc
>
> Of course, I could have a list with the components named "Fratio"
> "Random", "LetterA". But I fail to see what it adds. And it forces me
> to build the list, and probably rebuild it whe (or not build it until)
> the user enters her/his own selection function. But the later I do not
> need to do with the scheme above.
>
>
>> With your function, what if the user runs:
>>
>> > g(5,3)
>>
>> What should it do?  (you have only shown definitions for f.1 and f.2).  With my luck I would accidentily type that and just happen to have a f.3 function sitting around from a previous project that does something that I really don't want it to do now.  If I use the list approach then I will get a subscript out of bounds error rather than running something unintended.
>>
>>
>
> I see the general concern, but not how it applies here. If I pass
> argument "Fratio" then either I use geneSelect.Fratio or I get an
> error if "geneSelect.Fratio" does not exist. Similar to what would
> happen if I do
>
> g1(2, 8)
>
> when f.8 is not defined:
>
> Error in eval(expr, envir, enclos) : object "f.8" not found
> So even in more general cases, except for function redefinitions, etc,
> you are not able to call non-existent stuff.
>
>> 2nd, If I used the eval-parse approach then I would probably at some point redefine f.1 or f.2 to the output of a regression analysis or something, then go back and run the g function at a later time and wonder why I am getting an error, then once I have finally figured it out, now I need to remember what f.1 did and rewrite it again.  I am much less likely to accidentally replace an element of a list, and if the list is well named I am unlikely to replace the whole list by accident.
>>
>>
>
> Yes, that is true. Again, it does not apply to the actual case I have
> in mind, but of course, without the detailed info on context I just
> gave, you could not know that.
>
>
>> 3rd, If I ever want to use this code somewhere else (new version of R, on the laptop, give to coworker, ...), it is a lot easier to save and load a single list than to try to think of all the functions that need to be saved.
>>
>
> Oh, sure. But all the functions above live in a single file (actually,
> a minipackage) except for the optional use function (which is read
> from a file).
>
>
>>
>> Personally I have never regretted trying not to underestimate my own future stupidity.
>>
>
> Neither do I. And actually, that is why I asked: if Thomas Lumley
> said, in the fortune, that I better rethink about it, then I should
> try rethinking about it. But I asked because I failed to see what the
> problem is.
>
>
>> Hope this helps,
>>
>
> It certainly does.
>
>
> Best,
>
> R.
>
>
>> --
>> Gregory (Greg) L. Snow Ph.D.
>> Statistical Data Center
>> Intermountain Healthcare
>> greg.snow at intermountainmail.org
>> (801) 408-8111
>>
>>
>>
>> > -----Original Message-----
>> > From: r-help-bounces at stat.math.ethz.ch
>> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ramon
>> > Diaz-Uriarte
>> > Sent: Friday, January 05, 2007 11:41 AM
>> > To: Peter Dalgaard
>> > Cc: r-help; rdiaz02 at gmail.com
>> > Subject: Re: [R] eval(parse(text vs. get when accessing a function
>> >
>> > On Friday 05 January 2007 19:21, Peter Dalgaard wrote:
>> > > Ramon Diaz-Uriarte wrote:
>> > > > Dear All,
>> > > >
>> > > > I've read Thomas Lumley's fortune "If the answer is parse() you
>> > > > should usually rethink the question.". But I am not sure it that
>> > > > also applies (and why) to other situations (Lumley's comment
>> > > > http://tolstoy.newcastle.edu.au/R/help/05/02/12204.html
>> > > > was in reply to accessing a list).
>> > > >
>> > > > Suppose I have similarly called functions, except for a
>> > postfix. E.g.
>> > > >
>> > > > f.1 <- function(x) {x + 1}
>> > > > f.2 <- function(x) {x + 2}
>> > > >
>> > > > And sometimes I want to call f.1 and some other times f.2 inside
>> > > > another function. I can either do:
>> > > >
>> > > > g <- function(x, fpost) {
>> > > >     calledf <- eval(parse(text = paste("f.", fpost, sep = "")))
>> > > >     calledf(x)
>> > > >     ## do more stuff
>> > > > }
>> > > >
>> > > >
>> > > > Or:
>> > > >
>> > > > h <- function(x, fpost) {
>> > > >     calledf <- get(paste("f.", fpost, sep = ""))
>> > > >     calledf(x)
>> > > >     ## do more stuff
>> > > > }
>> > > >
>> > > >
>> > > > Two questions:
>> > > > 1) Why is the second better?
>> > > >
>> > > > 2) By changing g or h I could use "do.call" instead; why
>> > would that
>> > > > be better? Because I can handle differences in argument lists?
>> >
>> > Dear Peter,
>> >
>> > Thanks for your answer.
>> >
>> > >
>> > > Who says that they are better?  If the question is how to call a
>> > > function specified by half of its name, the answer could well be to
>> > > use parse(), the point is that you should rethink whether that was
>> > > really the right question.
>> > >
>> > > Why not instead, e.g.
>> > >
>> > > f <- list("1"=function(x) {x + 1} , "2"=function(x) {x + 2}) h <-
>> > > function(x, fpost) f[[fpost]](x)
>> > >
>> > > > h(2,"2")
>> > >
>> > > [1] 4
>> > >
>> > > > h(2,"1")
>> > >
>> > > [1] 3
>> > >
>> >
>> > I see, this is direct way of dealing with the problem.
>> > However, you first need to build the f list, and you might
>> > not know about that ahead of time. For instance, if I build a
>> > function so that the only thing that you need to do to use my
>> > function g is to call your function "f.something", and then
>> > pass the "something".
>> >
>> > I am still under the impression that, given your answer,
>> > using "eval(parse(text" is not your preferred way.  What are
>> > the possible problems (if there are any, that is). I guess I
>> > am puzzled by "rethink whether that was really the right question".
>> >
>> >
>> > Thanks,
>> >
>> > R.
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> > > > Thanks,
>> > > >
>> > > >
>> > > > R.
>> >
>> > --
>> > Ram?n D?az-Uriarte
>> > Centro Nacional de Investigaciones Oncol?gicas (CNIO)
>> > (Spanish National Cancer Center) Melchor Fern?ndez Almagro, 3
>> > 28029 Madrid (Spain)
>> > Fax: +-34-91-224-6972
>> > Phone: +-34-91-224-6900
>> >
>> > http://ligarto.org/rdiaz
>> > PGP KeyID: 0xE89B3462
>> > (http://ligarto.org/rdiaz/0xE89B3462.asc)
>> >
>> >
>> >
>> > **NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en
>> > s...{{dropped}}
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>>
>
>
> -- 
> Ramon Diaz-Uriarte
> Statistical Computing Team
> Structural Biology and Biocomputing Programme
> Spanish National Cancer Centre (CNIO)
> http://ligarto.org/rdiaz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Martin T. Morgan
Bioconductor / Computational Biology
http://bioconductor.org


From rdiaz02 at gmail.com  Sat Jan  6 20:23:27 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Sat, 6 Jan 2007 20:23:27 +0100
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <6phps9sm73o.fsf@gopher4.fhcrc.org>
References: <07E228A5BE53C24CAD490193A7381BBB77C654@LP-EXCHVS07.CO.IHC.COM>
	<624934630701060616i2718ab71xb981fbca39c0c910@mail.gmail.com>
	<6phps9sm73o.fsf@gopher4.fhcrc.org>
Message-ID: <624934630701061123j1f779e5bma21728f8cbf4bf09@mail.gmail.com>

Hi Martin,



On 1/6/07, Martin Morgan <mtmorgan at fhcrc.org> wrote:
> Hi Ramon,
>
> It seems like a naming convention (f.xxx) and eval(parse(...)) are
> standing in for objects (of class 'GeneSelector', say, representing a
> function with a particular form and doing a particular operation) and
> dispatch (a function 'geneConverter' might handle a converter of class
> 'GeneSelector' one way, user supplied ad-hoc functions more carefully;
> inside geneConverter the only real concern is that the converter
> argument is in fact a callable function).
>
> eval(parse(...)) brings scoping rules to the fore as an explicit
> programming concern; here scope is implicit, but that's probably better
> -- R will get its own rules right.
>
> Martin
>
> Here's an S4 sketch:
>
> setClass("GeneSelector",
>          contains="function",
>          representation=representation(description="character"),
>          validity=function(object) {
>              msg <- NULL
>              argNames <- names(formals(object))
>              if (argNames[1]!="x")
>                msg <- c(msg, "\n  GeneSelector requires a first argument named 'x'")
>              if (!"..." %in% argNames)
>                msg <- c(msg, "\n  GeneSelector requires '...' in its signature")
>              if (0==length(object at description))
>                msg <- c(msg, "\n  Please describe your GeneSelector")
>              if (is.null(msg)) TRUE else msg
>          })
>
> setGeneric("geneConverter",
>            function(converter, x, ...) standardGeneric("geneConverter"),
>            signature=c("converter"))
>
> setMethod("geneConverter",
>           signature(converter="GeneSelector"),
>           function(converter, x, ...) {
>               ## important stuff here
>               converter(x, ...)
>           })
>
> setMethod("geneConverter",
>           signature(converter="function"),
>           function(converter, x, ...) {
>               message("ad-hoc converter; hope it works!")
>               converter(x, ...)
>           })
>
> and then...
>
> > c1 <- new("GeneSelector",
> +           function(x, ...) prod(x, ...),
> +           description="Product of x")
> >
> > c2 <- new("GeneSelector",
> +           function(x, ...) sum(x, ...),
> +           description="Sum of x")
> >
> > geneConverter(c1, 1:4)
> [1] 24
> > geneConverter(c2, 1:4)
> [1] 10
> > geneConverter(mean, 1:4)
> ad-hoc converter; hope it works!
> [1] 2.5
> >
> > cvterr <- new("GeneSelector", function(y) {})
> Error in validObject(.Object) : invalid class "GeneSelector" object: 1:
>   GeneSelector requires a first argument named 'x'
> invalid class "GeneSelector" object: 2:
>   GeneSelector requires '...' in its signature
> invalid class "GeneSelector" object: 3:
>   Please describe your GeneSelector
> > xxx <- 10
> > geneConverter(xxx, 1:4)
> Error in function (classes, fdef, mtable)  :
>         unable to find an inherited method for function "geneConverter", for signature "numeric"
>



Thanks!! That is actually a rather interesting alternative approach
and I can see it also adds a lot of structure to the problem. I have
to confess, though, that I am not a fan of OOP (nor of S4 classes); in
this case, in particular, it seems there is a lot of scaffolding in
the code above (the counterpoint to the structure?) and, regarding
scoping rules, I prefer to think about them explicitly (I find it much
simpler than inheritance).

Best,

R.


>
> "Ramon Diaz-Uriarte" <rdiaz02 at gmail.com> writes:
>
> > Dear Greg,
> >
> >
> > On 1/5/07, Greg Snow <Greg.Snow at intermountainmail.org> wrote:
> >> Ramon,
> >>
> >> I prefer to use the list method for this type of thing, here are a couple of reasons why (maybe you are more organized than me and would never do some of the stupid things that I have, so these don't apply to you, but you can see that the general suggestion applys to some of the rest of us).
> >>
> >
> >
> > Those suggestions do apply to me of course (no claim to being
> > organized nor beyond idiocy here). And actually the suggestions on
> > this thread are being very useful. I think, though, that I was not
> > very clear on the context and my examples were too dumbed down. So
> > I'll try to give more detail (nothing here is secret, I am just trying
> > not to bore people).
> >
> > The code is part of a web-based application, so there is no
> > interactive user. The R code is passed the arguments (and optional
> > user functions) from the CGI.
> >
> > There is one "core" function (call it cvFunct) that, among other
> > things, does cross-validation. So this is one way to do things:
> >
> > cvFunct <- function(whatever, genefiltertype, whateverelse) {
> >       internalGeneSelect <- eval(parse(text = paste("geneSelect",
> >                                              genefiltertype, sep = ".")))
> >
> >       ## do things calling internalGeneSelect,
> > }
> >
> > and now define all possible functions as
> >
> > geneSelect.Fratio <- function(x, y, z) {##something}
> > geneSelect.Wilcoxon <- function(x, y, z) {## something else}
> >
> > If I want more geneSelect functions, adding them is simple. And I can
> > even allow the user to pass her/his own functions, with the only
> > restriction that it takes three args, x, y, z, and that the function
> > is to be called: "geneSelect." and a user choosen string. (Yes, I need
> > to make sure no calls to "system", etc, are in the user code, etc,
> > etc, but that is another issue).
> >
> > The general idea is not new of course. For instance, in package
> > "e1071", a somewhat similar thing is done in function "tune", and
> > David Meyer there uses "do.call". However, tune is a lot more general
> > than what I had in mind. For instance, "tune" deals with arbitrary
> > functions, with arbitrary numbers and names of parameters, whereas my
> > functions above all take only three arguments (x: a matrix, y: a
> > vector; z: an integer), so the neat functionality provided by
> > "do.call", and passing the args as a list is not really needed.
> >
> > So, given that my situation is so structured, and I do not need
> > "do.call", I think the approach via eval(parse(paste makes my life
> > simple:
> >
> > a) the central function (cvFunct) uses something I can easily
> > recognize: "internalGeneSelect"
> >
> > b) after the initial eval(parse(text I do not need to worry anymore
> > about what the "true" gene selection function is called
> >
> > c) adding new functions and calling them is simple: function naming
> > follows a simple pattern ("geneSelect." + postfix) and calling the
> > user function only requires passing the postfix to cvFunct.
> >
> > d) notice also that, at least the functs. I define, will of course not
> > be named "f.1", etc, but rather things like "geneSelect.Fratio" or
> > "geneSelect.namesThatStartWithCuteLetters";
> >
> > I hope this makes things more clear. I did not include this detail
> > because this is probably boring (I guess most of you have stopped
> > reading by now :-).
> >
> >
> >> Using the list forces you to think about what functions may be called and thinking about things before doing them is usually a good idea.  Personally I don't trust the user of my functions (usually my future self who has forgotten something that seemed obvious at the time) to not do something stupid with them.
> >>
> >> With list elements you can have names for the functions and access them either by the name or by a number, I find that a lot easier when I go back to edit/update than to remember which function f.1 or f.2 did what.
> >>
> >
> > But I don't see how having your functions as list elements is easier
> > (specially if the function is longer than 2 to 3 lines) than having
> > all functions systematically named things such as:
> >
> > geneSelect.Fratio
> > geneSelect.Random
> > geneSelect.LetterA
> > etc
> >
> > Of course, I could have a list with the components named "Fratio"
> > "Random", "LetterA". But I fail to see what it adds. And it forces me
> > to build the list, and probably rebuild it whe (or not build it until)
> > the user enters her/his own selection function. But the later I do not
> > need to do with the scheme above.
> >
> >
> >> With your function, what if the user runs:
> >>
> >> > g(5,3)
> >>
> >> What should it do?  (you have only shown definitions for f.1 and f.2).  With my luck I would accidentily type that and just happen to have a f.3 function sitting around from a previous project that does something that I really don't want it to do now.  If I use the list approach then I will get a subscript out of bounds error rather than running something unintended.
> >>
> >>
> >
> > I see the general concern, but not how it applies here. If I pass
> > argument "Fratio" then either I use geneSelect.Fratio or I get an
> > error if "geneSelect.Fratio" does not exist. Similar to what would
> > happen if I do
> >
> > g1(2, 8)
> >
> > when f.8 is not defined:
> >
> > Error in eval(expr, envir, enclos) : object "f.8" not found
> > So even in more general cases, except for function redefinitions, etc,
> > you are not able to call non-existent stuff.
> >
> >> 2nd, If I used the eval-parse approach then I would probably at some point redefine f.1 or f.2 to the output of a regression analysis or something, then go back and run the g function at a later time and wonder why I am getting an error, then once I have finally figured it out, now I need to remember what f.1 did and rewrite it again.  I am much less likely to accidentally replace an element of a list, and if the list is well named I am unlikely to replace the whole list by accident.
> >>
> >>
> >
> > Yes, that is true. Again, it does not apply to the actual case I have
> > in mind, but of course, without the detailed info on context I just
> > gave, you could not know that.
> >
> >
> >> 3rd, If I ever want to use this code somewhere else (new version of R, on the laptop, give to coworker, ...), it is a lot easier to save and load a single list than to try to think of all the functions that need to be saved.
> >>
> >
> > Oh, sure. But all the functions above live in a single file (actually,
> > a minipackage) except for the optional use function (which is read
> > from a file).
> >
> >
> >>
> >> Personally I have never regretted trying not to underestimate my own future stupidity.
> >>
> >
> > Neither do I. And actually, that is why I asked: if Thomas Lumley
> > said, in the fortune, that I better rethink about it, then I should
> > try rethinking about it. But I asked because I failed to see what the
> > problem is.
> >
> >
> >> Hope this helps,
> >>
> >
> > It certainly does.
> >
> >
> > Best,
> >
> > R.
> >
> >
> >> --
> >> Gregory (Greg) L. Snow Ph.D.
> >> Statistical Data Center
> >> Intermountain Healthcare
> >> greg.snow at intermountainmail.org
> >> (801) 408-8111
> >>
> >>
> >>
> >> > -----Original Message-----
> >> > From: r-help-bounces at stat.math.ethz.ch
> >> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ramon
> >> > Diaz-Uriarte
> >> > Sent: Friday, January 05, 2007 11:41 AM
> >> > To: Peter Dalgaard
> >> > Cc: r-help; rdiaz02 at gmail.com
> >> > Subject: Re: [R] eval(parse(text vs. get when accessing a function
> >> >
> >> > On Friday 05 January 2007 19:21, Peter Dalgaard wrote:
> >> > > Ramon Diaz-Uriarte wrote:
> >> > > > Dear All,
> >> > > >
> >> > > > I've read Thomas Lumley's fortune "If the answer is parse() you
> >> > > > should usually rethink the question.". But I am not sure it that
> >> > > > also applies (and why) to other situations (Lumley's comment
> >> > > > http://tolstoy.newcastle.edu.au/R/help/05/02/12204.html
> >> > > > was in reply to accessing a list).
> >> > > >
> >> > > > Suppose I have similarly called functions, except for a
> >> > postfix. E.g.
> >> > > >
> >> > > > f.1 <- function(x) {x + 1}
> >> > > > f.2 <- function(x) {x + 2}
> >> > > >
> >> > > > And sometimes I want to call f.1 and some other times f.2 inside
> >> > > > another function. I can either do:
> >> > > >
> >> > > > g <- function(x, fpost) {
> >> > > >     calledf <- eval(parse(text = paste("f.", fpost, sep = "")))
> >> > > >     calledf(x)
> >> > > >     ## do more stuff
> >> > > > }
> >> > > >
> >> > > >
> >> > > > Or:
> >> > > >
> >> > > > h <- function(x, fpost) {
> >> > > >     calledf <- get(paste("f.", fpost, sep = ""))
> >> > > >     calledf(x)
> >> > > >     ## do more stuff
> >> > > > }
> >> > > >
> >> > > >
> >> > > > Two questions:
> >> > > > 1) Why is the second better?
> >> > > >
> >> > > > 2) By changing g or h I could use "do.call" instead; why
> >> > would that
> >> > > > be better? Because I can handle differences in argument lists?
> >> >
> >> > Dear Peter,
> >> >
> >> > Thanks for your answer.
> >> >
> >> > >
> >> > > Who says that they are better?  If the question is how to call a
> >> > > function specified by half of its name, the answer could well be to
> >> > > use parse(), the point is that you should rethink whether that was
> >> > > really the right question.
> >> > >
> >> > > Why not instead, e.g.
> >> > >
> >> > > f <- list("1"=function(x) {x + 1} , "2"=function(x) {x + 2}) h <-
> >> > > function(x, fpost) f[[fpost]](x)
> >> > >
> >> > > > h(2,"2")
> >> > >
> >> > > [1] 4
> >> > >
> >> > > > h(2,"1")
> >> > >
> >> > > [1] 3
> >> > >
> >> >
> >> > I see, this is direct way of dealing with the problem.
> >> > However, you first need to build the f list, and you might
> >> > not know about that ahead of time. For instance, if I build a
> >> > function so that the only thing that you need to do to use my
> >> > function g is to call your function "f.something", and then
> >> > pass the "something".
> >> >
> >> > I am still under the impression that, given your answer,
> >> > using "eval(parse(text" is not your preferred way.  What are
> >> > the possible problems (if there are any, that is). I guess I
> >> > am puzzled by "rethink whether that was really the right question".
> >> >
> >> >
> >> > Thanks,
> >> >
> >> > R.
> >> >
> >> >
> >> >
> >> >
> >> >
> >> >
> >> >
> >> > > > Thanks,
> >> > > >
> >> > > >
> >> > > > R.
> >> >
> >> > --
> >> > Ram?n D?az-Uriarte
> >> > Centro Nacional de Investigaciones Oncol?gicas (CNIO)
> >> > (Spanish National Cancer Center) Melchor Fern?ndez Almagro, 3
> >> > 28029 Madrid (Spain)
> >> > Fax: +-34-91-224-6972
> >> > Phone: +-34-91-224-6900
> >> >
> >> > http://ligarto.org/rdiaz
> >> > PGP KeyID: 0xE89B3462
> >> > (http://ligarto.org/rdiaz/0xE89B3462.asc)
> >> >
> >> >
> >> >
> >> > **NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en
> >> > s...{{dropped}}
> >> >
> >> > ______________________________________________
> >> > R-help at stat.math.ethz.ch mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> >> > http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible code.
> >> >
> >>
> >>
> >
> >
> > --
> > Ramon Diaz-Uriarte
> > Statistical Computing Team
> > Structural Biology and Biocomputing Programme
> > Spanish National Cancer Centre (CNIO)
> > http://ligarto.org/rdiaz
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Martin T. Morgan
> Bioconductor / Computational Biology
> http://bioconductor.org
>


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From helprhelp at gmail.com  Sun Jan  7 21:30:21 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Sun, 7 Jan 2007 15:30:21 -0500
Subject: [R] creating a list of lists
In-Reply-To: <20070106232215.vmionf6pl7sos8w8@webmail.utoronto.ca>
References: <20070106232215.vmionf6pl7sos8w8@webmail.utoronto.ca>
Message-ID: <cdf817830701071230k6435e27clda5c0378fb99e639@mail.gmail.com>

change
 forests <- c(forests, forest);
into
 forests[[level]] <- forest



On 1/6/07, Paul Boutros <paul.boutros at utoronto.ca> wrote:
> Hello,
>
> I'm trying to create a series of randomForest objects, basically in a
> loop like this:
>
> forests <- list();
>
> for (level in 1:10) {
>
>           # do some other things here
>
>         # create a random forest
>         forest <- randomForest(
>                 x = x.level,
>                 y = z.level,
>                 ntree = trees
>                 );
>
>         forests <- c(forests, forest);
>
>         }
>
>
> But instead of creating a list of 10 forests, this creates a list of
> 180 elements, 18 for each forest.  Is there a way to create a list of
> randomForest objects for use later on in code like:
>
> for (forest in forests) {
>          values <- predict(forest, data);
>          # do things with these predicted values
>          }
>
> Sincerely,
> Paul
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From eric.archer at noaa.gov  Sun Jan  7 22:49:41 2007
From: eric.archer at noaa.gov (Eric Archer)
Date: Sun, 07 Jan 2007 13:49:41 -0800
Subject: [R] creating a list of lists
In-Reply-To: <20070106232215.vmionf6pl7sos8w8@webmail.utoronto.ca>
References: <20070106232215.vmionf6pl7sos8w8@webmail.utoronto.ca>
Message-ID: <45A16AF5.4000108@noaa.gov>

Howabout using 'lapply'?

forest <- lapply(1:10, function(level) {
  # do some other things here
  # create and return a random forest
   randomForest(x = x.level, y = z.level, ntree = trees)
})

# give meaningful names
names(forest) <- paste("level", 1:10, sep = "_")
 
Paul Boutros wrote:
> Hello,
>
> I'm trying to create a series of randomForest objects, basically in a  
> loop like this:
>
> forests <- list();
>
> for (level in 1:10) {
>
>           # do some other things here
>
> 	# create a random forest
> 	forest <- randomForest(
> 		x = x.level,
> 		y = z.level,
> 		ntree = trees
> 		);
>
> 	forests <- c(forests, forest);
>
> 	}
>
>
> But instead of creating a list of 10 forests, this creates a list of  
> 180 elements, 18 for each forest.  Is there a way to create a list of  
> randomForest objects for use later on in code like:
>
> for (forest in forests) {
>          values <- predict(forest, data);
>          # do things with these predicted values
>          }
>
> Sincerely,
> Paul
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 

Eric Archer, Ph.D.
NOAA-SWFSC
8604 La Jolla Shores Dr.
La Jolla, CA 92037
858-546-7121,7003(FAX)
eric.archer at noaa.gov


"Lighthouses are more helpful than churches."
    - Benjamin Franklin

"...but I'll take a GPS over either one."
    - Craig George


From attenka at utu.fi  Sun Jan  7 23:50:22 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Mon, 08 Jan 2007 00:50:22 +0200
Subject: [R] MDS in 3D
Message-ID: <f76eb3cf306c.45a1954e@utu.fi>

Hi,

I have tried to develop multidimensional scaling for 3D space using PCA without success, yet;-) Is there some application ready in R?

Cheers,

Atte


From attenka at utu.fi  Mon Jan  8 00:03:36 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Mon, 08 Jan 2007 01:03:36 +0200
Subject: [R] MDS in 3D
In-Reply-To: <f76eb3cf306c.45a1954e@utu.fi>
References: <f76eb3cf306c.45a1954e@utu.fi>
Message-ID: <fd6ca9895998.45a19868@utu.fi>


> Hi,
> 
> I have tried to develop multidimensional scaling for 3D space using 
> PCA without success, yet;-) Is there some application ready in R?
> 
> Cheers,
> 
> Atte
> 

I found xgobi, but when I try to run example I get some command not found -errors.

Atte


> data(morsecodes) ## from the XGobi/XGvis data, see  ?morsecodes
> mc.row <- paste(morsecodes.row[,1],morsecodes.row[,2])
> 
> xgvis(dmat = morsecodes.dist,
+       pos = morsecodes.pos,
+       rowlab = mc.row,
+       colors = morsecodes.colors,
+       glyphs = morsecodes.glyphs,
+       lines = morsecodes.lines,
+       linecolors = morsecodes.linecolors)
xgvis /tmp/RtmpDaR3cT/xgvis-6058ed8 & 
> 
> ##>   2) Show lines by hitting "l" with the mouse over the plot.
> ##>   3) Examine morsecode labels by hitting "i" and mousing around on the plot.
> ##>   3b) Press "r" (on the plot) to switch 3D rotation in xgobi.
> ##>   4) Run MDS in 3D by clicking "Run MDS" (in xgvis).
> ##>   5) Speed up the optimization by increasing the "Stepsize" with the slider.
> ##>      The "Stress function" value may go as low as 0.1925 (MM).
> ##>   6) When the optimization calms down, click "Run MDS" to toggle MDS off.
> ##>   7) Rotate the MDS configuration in 3D {by "r" with mouse over plot}.
> ##>   8) Increase the rotation speed with the slider in the top left and
> ##>      control the rotation direction by dragging the mouse on the plot.
> ##>   9) You can check out the initial configuration by
> 
> ## In order to have no color warning :
> Mcolors <- unique(morsecodes.colors)
/bin/sh: line 1: xgvis: command not found
> (Mcolors <- paste("*brushColor", 0:(length(Mcolors)-1),": ", Mcolors, sep=""))
[1] "*brushColor0: SkyBlue" "*brushColor1: Green"  
[3] "*brushColor2: Yellow"  "*brushColor3: HotPink"
[5] "*brushColor4: Red"    
> 
> xgobi(morsecodes.pos, collab = morsecodes.col, rowlab = mc.row,
+       colors = morsecodes.colors,
+       glyphs = morsecodes.glyphs,
+       lines  = morsecodes.lines,
+       linecolors = morsecodes.linecolors,
+       resources= c("*showLines: True", Mcolors))
xgobi -title 'morsecodes.pos' -std mmx /tmp/RtmpDaR3cT/xgobi-mrscd56e509fe & 
/bin/sh: line 1: xgobi: command not found
> 
> ##>      This XGobi window will be linked with
> ##>      the XGvis window for glyph-color brushing and labeling.


From attenka at utu.fi  Mon Jan  8 00:04:47 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Mon, 08 Jan 2007 01:04:47 +0200
Subject: [R] MDS in 3D
In-Reply-To: <fd6ca9895998.45a19868@utu.fi>
References: <f76eb3cf306c.45a1954e@utu.fi> <fd6ca9895998.45a19868@utu.fi>
Message-ID: <f77efcb06780.45a198af@utu.fi>

...and my system is OSX 10.4.

Atte

> 
> > Hi,
> > 
> > I have tried to develop multidimensional scaling for 3D space 
> using 
> > PCA without success, yet;-) Is there some application ready in R?
> > 
> > Cheers,
> > 
> > Atte
> > 
> 
> I found xgobi, but when I try to run example I get some command not 
> found -errors.
> 
> Atte
> 
> 
> > data(morsecodes) ## from the XGobi/XGvis data, see  ?morsecodes
> > mc.row <- paste(morsecodes.row[,1],morsecodes.row[,2])
> > 
> > xgvis(dmat = morsecodes.dist,
> +       pos = morsecodes.pos,
> +       rowlab = mc.row,
> +       colors = morsecodes.colors,
> +       glyphs = morsecodes.glyphs,
> +       lines = morsecodes.lines,
> +       linecolors = morsecodes.linecolors)
> xgvis /tmp/RtmpDaR3cT/xgvis-6058ed8 & 
> > 
> > ##>   2) Show lines by hitting "l" with the mouse over the plot.
> > ##>   3) Examine morsecode labels by hitting "i" and mousing 
> around on the plot.
> > ##>   3b) Press "r" (on the plot) to switch 3D rotation in xgobi.
> > ##>   4) Run MDS in 3D by clicking "Run MDS" (in xgvis).
> > ##>   5) Speed up the optimization by increasing the "Stepsize" 
> with the slider.
> > ##>      The "Stress function" value may go as low as 0.1925 (MM).
> > ##>   6) When the optimization calms down, click "Run MDS" to 
> toggle MDS off.
> > ##>   7) Rotate the MDS configuration in 3D {by "r" with mouse 
> over plot}.
> > ##>   8) Increase the rotation speed with the slider in the top 
> left and
> > ##>      control the rotation direction by dragging the mouse on 
> the plot.
> > ##>   9) You can check out the initial configuration by
> > 
> > ## In order to have no color warning :
> > Mcolors <- unique(morsecodes.colors)
> /bin/sh: line 1: xgvis: command not found
> > (Mcolors <- paste("*brushColor", 0:(length(Mcolors)-1),": ", 
> Mcolors, sep=""))
> [1] "*brushColor0: SkyBlue" "*brushColor1: Green"  
> [3] "*brushColor2: Yellow"  "*brushColor3: HotPink"
> [5] "*brushColor4: Red"    
> > 
> > xgobi(morsecodes.pos, collab = morsecodes.col, rowlab = mc.row,
> +       colors = morsecodes.colors,
> +       glyphs = morsecodes.glyphs,
> +       lines  = morsecodes.lines,
> +       linecolors = morsecodes.linecolors,
> +       resources= c("*showLines: True", Mcolors))
> xgobi -title 'morsecodes.pos' -std mmx /tmp/RtmpDaR3cT/xgobi-
> mrscd56e509fe & 
> /bin/sh: line 1: xgobi: command not found
> > 
> > ##>      This XGobi window will be linked with
> > ##>      the XGvis window for glyph-color brushing and labeling.
>


From d.stasinopoulos at londonmet.ac.uk  Mon Jan  8 00:14:05 2007
From: d.stasinopoulos at londonmet.ac.uk (Mikis Stasinopoulos)
Date: Sun, 07 Jan 2007 23:14:05 +0000
Subject: [R] negative binomial family glm R and STATA
Message-ID: <45A17EBD.7030108@londonmet.ac.uk>

Dear  Patrick

Try the package gamlss which allow to fit the negative binomial 
distrbution. For example  with your data I am getting
#---------------------------------------------------
ga1<-gamlss(nbcas~.,data=zonesdb4,family=NBI)
GAMLSS-RS iteration 1: Global Deviance = 817.9027
GAMLSS-RS iteration 2: Global Deviance = 817.9025
 ga1

Family:  c("NBI", "Negative Binomial type I")
Fitting method: RS()

Call:  gamlss(formula = nbcas ~ ., family = NBI, data = zonesdb4)

Mu Coefficients:
 (Intercept)           pop          Area    V_100kHab1        gares1 
   3.204e+00     1.114e-05     1.354e-05     9.144e-01     7.946e-01 
      ports1  axe_routier1  axe_routier2         lacs1 
  -1.730e+00     1.989e-01            NA     3.042e+00 
Sigma Coefficients:
(Intercept) 
      2.313 

 Degrees of Freedom for the fit: 9 Residual Deg. of Freedom   83
Global Deviance:     817.902
            AIC:     835.902
            SBC:     858.599
#--------------------------------------------------------------

Note that the AIC: 835.902 is similar to your fitted model using  glm.nb 
which is  AIC: 836.2.

The coefficients are not identical but this is not suprissing when you 
are using x-variables with extreme values as pop and Area.
The profile function for sigma can be found using

prof.dev(ga1,"sigma", min=7, max=16, step=0.1, type="l")

Your discrepancy with STATA come from the fact that in STATA you are 
fitting the model with sigma fixed to 1.
You can see that by fitting the same model in  GAMLSS.

 >  ga2<-gamlss(nbcas~.,data=zonesdb4,family=NBI, sigma.fix=T, 
sigma.start=1)
GAMLSS-RS iteration 1: Global Deviance = 1194.299
GAMLSS-RS iteration 2: Global Deviance = 1194.298

This  is similar  to  the  log likelihod you are  getting in STATA. i.e. 
-2*-597.14778= 1194.296.

You can also use the stepGAIC() function to simplify your model. For  
example

 ga2<-stepGAIC(ga1)

will result to a model with only pop and lacs in the mdel.

Note also the the Negative binomial type II fits better to you data.

 >  ga3<-gamlss(nbcas~.,data=zonesdb4,family=NBII)
GAMLSS-RS iteration 1: Global Deviance = 804.5682
...
GAMLSS-RS iteration 10: Global Deviance = 804.4995


Thanks

Mikis


From ggrothendieck at gmail.com  Mon Jan  8 00:40:22 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 7 Jan 2007 18:40:22 -0500
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <624934630701061123j1f779e5bma21728f8cbf4bf09@mail.gmail.com>
References: <07E228A5BE53C24CAD490193A7381BBB77C654@LP-EXCHVS07.CO.IHC.COM>
	<624934630701060616i2718ab71xb981fbca39c0c910@mail.gmail.com>
	<6phps9sm73o.fsf@gopher4.fhcrc.org>
	<624934630701061123j1f779e5bma21728f8cbf4bf09@mail.gmail.com>
Message-ID: <971536df0701071540r4d75cb8cm64d9002573442cda@mail.gmail.com>

The S4 is not essential.  You could do it in S3 too:

> f.a <- function(x) x+1
> f.b <- function(x) x+2
> f <- function(x) UseMethod("f")
>
> f(structure(10, class = "a"))
[1] 11
attr(,"class")
[1] "a"

On 1/6/07, Ramon Diaz-Uriarte <rdiaz02 at gmail.com> wrote:
> Hi Martin,
>
>
>
> On 1/6/07, Martin Morgan <mtmorgan at fhcrc.org> wrote:
> > Hi Ramon,
> >
> > It seems like a naming convention (f.xxx) and eval(parse(...)) are
> > standing in for objects (of class 'GeneSelector', say, representing a
> > function with a particular form and doing a particular operation) and
> > dispatch (a function 'geneConverter' might handle a converter of class
> > 'GeneSelector' one way, user supplied ad-hoc functions more carefully;
> > inside geneConverter the only real concern is that the converter
> > argument is in fact a callable function).
> >
> > eval(parse(...)) brings scoping rules to the fore as an explicit
> > programming concern; here scope is implicit, but that's probably better
> > -- R will get its own rules right.
> >
> > Martin
> >
> > Here's an S4 sketch:
> >
> > setClass("GeneSelector",
> >          contains="function",
> >          representation=representation(description="character"),
> >          validity=function(object) {
> >              msg <- NULL
> >              argNames <- names(formals(object))
> >              if (argNames[1]!="x")
> >                msg <- c(msg, "\n  GeneSelector requires a first argument named 'x'")
> >              if (!"..." %in% argNames)
> >                msg <- c(msg, "\n  GeneSelector requires '...' in its signature")
> >              if (0==length(object at description))
> >                msg <- c(msg, "\n  Please describe your GeneSelector")
> >              if (is.null(msg)) TRUE else msg
> >          })
> >
> > setGeneric("geneConverter",
> >            function(converter, x, ...) standardGeneric("geneConverter"),
> >            signature=c("converter"))
> >
> > setMethod("geneConverter",
> >           signature(converter="GeneSelector"),
> >           function(converter, x, ...) {
> >               ## important stuff here
> >               converter(x, ...)
> >           })
> >
> > setMethod("geneConverter",
> >           signature(converter="function"),
> >           function(converter, x, ...) {
> >               message("ad-hoc converter; hope it works!")
> >               converter(x, ...)
> >           })
> >
> > and then...
> >
> > > c1 <- new("GeneSelector",
> > +           function(x, ...) prod(x, ...),
> > +           description="Product of x")
> > >
> > > c2 <- new("GeneSelector",
> > +           function(x, ...) sum(x, ...),
> > +           description="Sum of x")
> > >
> > > geneConverter(c1, 1:4)
> > [1] 24
> > > geneConverter(c2, 1:4)
> > [1] 10
> > > geneConverter(mean, 1:4)
> > ad-hoc converter; hope it works!
> > [1] 2.5
> > >
> > > cvterr <- new("GeneSelector", function(y) {})
> > Error in validObject(.Object) : invalid class "GeneSelector" object: 1:
> >   GeneSelector requires a first argument named 'x'
> > invalid class "GeneSelector" object: 2:
> >   GeneSelector requires '...' in its signature
> > invalid class "GeneSelector" object: 3:
> >   Please describe your GeneSelector
> > > xxx <- 10
> > > geneConverter(xxx, 1:4)
> > Error in function (classes, fdef, mtable)  :
> >         unable to find an inherited method for function "geneConverter", for signature "numeric"
> >
>
>
>
> Thanks!! That is actually a rather interesting alternative approach
> and I can see it also adds a lot of structure to the problem. I have
> to confess, though, that I am not a fan of OOP (nor of S4 classes); in
> this case, in particular, it seems there is a lot of scaffolding in
> the code above (the counterpoint to the structure?) and, regarding
> scoping rules, I prefer to think about them explicitly (I find it much
> simpler than inheritance).
>
> Best,
>
> R.
>
>
> >
> > "Ramon Diaz-Uriarte" <rdiaz02 at gmail.com> writes:
> >
> > > Dear Greg,
> > >
> > >
> > > On 1/5/07, Greg Snow <Greg.Snow at intermountainmail.org> wrote:
> > >> Ramon,
> > >>
> > >> I prefer to use the list method for this type of thing, here are a couple of reasons why (maybe you are more organized than me and would never do some of the stupid things that I have, so these don't apply to you, but you can see that the general suggestion applys to some of the rest of us).
> > >>
> > >
> > >
> > > Those suggestions do apply to me of course (no claim to being
> > > organized nor beyond idiocy here). And actually the suggestions on
> > > this thread are being very useful. I think, though, that I was not
> > > very clear on the context and my examples were too dumbed down. So
> > > I'll try to give more detail (nothing here is secret, I am just trying
> > > not to bore people).
> > >
> > > The code is part of a web-based application, so there is no
> > > interactive user. The R code is passed the arguments (and optional
> > > user functions) from the CGI.
> > >
> > > There is one "core" function (call it cvFunct) that, among other
> > > things, does cross-validation. So this is one way to do things:
> > >
> > > cvFunct <- function(whatever, genefiltertype, whateverelse) {
> > >       internalGeneSelect <- eval(parse(text = paste("geneSelect",
> > >                                              genefiltertype, sep = ".")))
> > >
> > >       ## do things calling internalGeneSelect,
> > > }
> > >
> > > and now define all possible functions as
> > >
> > > geneSelect.Fratio <- function(x, y, z) {##something}
> > > geneSelect.Wilcoxon <- function(x, y, z) {## something else}
> > >
> > > If I want more geneSelect functions, adding them is simple. And I can
> > > even allow the user to pass her/his own functions, with the only
> > > restriction that it takes three args, x, y, z, and that the function
> > > is to be called: "geneSelect." and a user choosen string. (Yes, I need
> > > to make sure no calls to "system", etc, are in the user code, etc,
> > > etc, but that is another issue).
> > >
> > > The general idea is not new of course. For instance, in package
> > > "e1071", a somewhat similar thing is done in function "tune", and
> > > David Meyer there uses "do.call". However, tune is a lot more general
> > > than what I had in mind. For instance, "tune" deals with arbitrary
> > > functions, with arbitrary numbers and names of parameters, whereas my
> > > functions above all take only three arguments (x: a matrix, y: a
> > > vector; z: an integer), so the neat functionality provided by
> > > "do.call", and passing the args as a list is not really needed.
> > >
> > > So, given that my situation is so structured, and I do not need
> > > "do.call", I think the approach via eval(parse(paste makes my life
> > > simple:
> > >
> > > a) the central function (cvFunct) uses something I can easily
> > > recognize: "internalGeneSelect"
> > >
> > > b) after the initial eval(parse(text I do not need to worry anymore
> > > about what the "true" gene selection function is called
> > >
> > > c) adding new functions and calling them is simple: function naming
> > > follows a simple pattern ("geneSelect." + postfix) and calling the
> > > user function only requires passing the postfix to cvFunct.
> > >
> > > d) notice also that, at least the functs. I define, will of course not
> > > be named "f.1", etc, but rather things like "geneSelect.Fratio" or
> > > "geneSelect.namesThatStartWithCuteLetters";
> > >
> > > I hope this makes things more clear. I did not include this detail
> > > because this is probably boring (I guess most of you have stopped
> > > reading by now :-).
> > >
> > >
> > >> Using the list forces you to think about what functions may be called and thinking about things before doing them is usually a good idea.  Personally I don't trust the user of my functions (usually my future self who has forgotten something that seemed obvious at the time) to not do something stupid with them.
> > >>
> > >> With list elements you can have names for the functions and access them either by the name or by a number, I find that a lot easier when I go back to edit/update than to remember which function f.1 or f.2 did what.
> > >>
> > >
> > > But I don't see how having your functions as list elements is easier
> > > (specially if the function is longer than 2 to 3 lines) than having
> > > all functions systematically named things such as:
> > >
> > > geneSelect.Fratio
> > > geneSelect.Random
> > > geneSelect.LetterA
> > > etc
> > >
> > > Of course, I could have a list with the components named "Fratio"
> > > "Random", "LetterA". But I fail to see what it adds. And it forces me
> > > to build the list, and probably rebuild it whe (or not build it until)
> > > the user enters her/his own selection function. But the later I do not
> > > need to do with the scheme above.
> > >
> > >
> > >> With your function, what if the user runs:
> > >>
> > >> > g(5,3)
> > >>
> > >> What should it do?  (you have only shown definitions for f.1 and f.2).  With my luck I would accidentily type that and just happen to have a f.3 function sitting around from a previous project that does something that I really don't want it to do now.  If I use the list approach then I will get a subscript out of bounds error rather than running something unintended.
> > >>
> > >>
> > >
> > > I see the general concern, but not how it applies here. If I pass
> > > argument "Fratio" then either I use geneSelect.Fratio or I get an
> > > error if "geneSelect.Fratio" does not exist. Similar to what would
> > > happen if I do
> > >
> > > g1(2, 8)
> > >
> > > when f.8 is not defined:
> > >
> > > Error in eval(expr, envir, enclos) : object "f.8" not found
> > > So even in more general cases, except for function redefinitions, etc,
> > > you are not able to call non-existent stuff.
> > >
> > >> 2nd, If I used the eval-parse approach then I would probably at some point redefine f.1 or f.2 to the output of a regression analysis or something, then go back and run the g function at a later time and wonder why I am getting an error, then once I have finally figured it out, now I need to remember what f.1 did and rewrite it again.  I am much less likely to accidentally replace an element of a list, and if the list is well named I am unlikely to replace the whole list by accident.
> > >>
> > >>
> > >
> > > Yes, that is true. Again, it does not apply to the actual case I have
> > > in mind, but of course, without the detailed info on context I just
> > > gave, you could not know that.
> > >
> > >
> > >> 3rd, If I ever want to use this code somewhere else (new version of R, on the laptop, give to coworker, ...), it is a lot easier to save and load a single list than to try to think of all the functions that need to be saved.
> > >>
> > >
> > > Oh, sure. But all the functions above live in a single file (actually,
> > > a minipackage) except for the optional use function (which is read
> > > from a file).
> > >
> > >
> > >>
> > >> Personally I have never regretted trying not to underestimate my own future stupidity.
> > >>
> > >
> > > Neither do I. And actually, that is why I asked: if Thomas Lumley
> > > said, in the fortune, that I better rethink about it, then I should
> > > try rethinking about it. But I asked because I failed to see what the
> > > problem is.
> > >
> > >
> > >> Hope this helps,
> > >>
> > >
> > > It certainly does.
> > >
> > >
> > > Best,
> > >
> > > R.
> > >
> > >
> > >> --
> > >> Gregory (Greg) L. Snow Ph.D.
> > >> Statistical Data Center
> > >> Intermountain Healthcare
> > >> greg.snow at intermountainmail.org
> > >> (801) 408-8111
> > >>
> > >>
> > >>
> > >> > -----Original Message-----
> > >> > From: r-help-bounces at stat.math.ethz.ch
> > >> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ramon
> > >> > Diaz-Uriarte
> > >> > Sent: Friday, January 05, 2007 11:41 AM
> > >> > To: Peter Dalgaard
> > >> > Cc: r-help; rdiaz02 at gmail.com
> > >> > Subject: Re: [R] eval(parse(text vs. get when accessing a function
> > >> >
> > >> > On Friday 05 January 2007 19:21, Peter Dalgaard wrote:
> > >> > > Ramon Diaz-Uriarte wrote:
> > >> > > > Dear All,
> > >> > > >
> > >> > > > I've read Thomas Lumley's fortune "If the answer is parse() you
> > >> > > > should usually rethink the question.". But I am not sure it that
> > >> > > > also applies (and why) to other situations (Lumley's comment
> > >> > > > http://tolstoy.newcastle.edu.au/R/help/05/02/12204.html
> > >> > > > was in reply to accessing a list).
> > >> > > >
> > >> > > > Suppose I have similarly called functions, except for a
> > >> > postfix. E.g.
> > >> > > >
> > >> > > > f.1 <- function(x) {x + 1}
> > >> > > > f.2 <- function(x) {x + 2}
> > >> > > >
> > >> > > > And sometimes I want to call f.1 and some other times f.2 inside
> > >> > > > another function. I can either do:
> > >> > > >
> > >> > > > g <- function(x, fpost) {
> > >> > > >     calledf <- eval(parse(text = paste("f.", fpost, sep = "")))
> > >> > > >     calledf(x)
> > >> > > >     ## do more stuff
> > >> > > > }
> > >> > > >
> > >> > > >
> > >> > > > Or:
> > >> > > >
> > >> > > > h <- function(x, fpost) {
> > >> > > >     calledf <- get(paste("f.", fpost, sep = ""))
> > >> > > >     calledf(x)
> > >> > > >     ## do more stuff
> > >> > > > }
> > >> > > >
> > >> > > >
> > >> > > > Two questions:
> > >> > > > 1) Why is the second better?
> > >> > > >
> > >> > > > 2) By changing g or h I could use "do.call" instead; why
> > >> > would that
> > >> > > > be better? Because I can handle differences in argument lists?
> > >> >
> > >> > Dear Peter,
> > >> >
> > >> > Thanks for your answer.
> > >> >
> > >> > >
> > >> > > Who says that they are better?  If the question is how to call a
> > >> > > function specified by half of its name, the answer could well be to
> > >> > > use parse(), the point is that you should rethink whether that was
> > >> > > really the right question.
> > >> > >
> > >> > > Why not instead, e.g.
> > >> > >
> > >> > > f <- list("1"=function(x) {x + 1} , "2"=function(x) {x + 2}) h <-
> > >> > > function(x, fpost) f[[fpost]](x)
> > >> > >
> > >> > > > h(2,"2")
> > >> > >
> > >> > > [1] 4
> > >> > >
> > >> > > > h(2,"1")
> > >> > >
> > >> > > [1] 3
> > >> > >
> > >> >
> > >> > I see, this is direct way of dealing with the problem.
> > >> > However, you first need to build the f list, and you might
> > >> > not know about that ahead of time. For instance, if I build a
> > >> > function so that the only thing that you need to do to use my
> > >> > function g is to call your function "f.something", and then
> > >> > pass the "something".
> > >> >
> > >> > I am still under the impression that, given your answer,
> > >> > using "eval(parse(text" is not your preferred way.  What are
> > >> > the possible problems (if there are any, that is). I guess I
> > >> > am puzzled by "rethink whether that was really the right question".
> > >> >
> > >> >
> > >> > Thanks,
> > >> >
> > >> > R.
> > >> >
> > >> >
> > >> >
> > >> >
> > >> >
> > >> >
> > >> >
> > >> > > > Thanks,
> > >> > > >
> > >> > > >
> > >> > > > R.
> > >> >
> > >> > --
> > >> > Ram?n D?az-Uriarte
> > >> > Centro Nacional de Investigaciones Oncol?gicas (CNIO)
> > >> > (Spanish National Cancer Center) Melchor Fern?ndez Almagro, 3
> > >> > 28029 Madrid (Spain)
> > >> > Fax: +-34-91-224-6972
> > >> > Phone: +-34-91-224-6900
> > >> >
> > >> > http://ligarto.org/rdiaz
> > >> > PGP KeyID: 0xE89B3462
> > >> > (http://ligarto.org/rdiaz/0xE89B3462.asc)
> > >> >
> > >> >
> > >> >
> > >> > **NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en
> > >> > s...{{dropped}}
> > >> >
> > >> > ______________________________________________
> > >> > R-help at stat.math.ethz.ch mailing list
> > >> > https://stat.ethz.ch/mailman/listinfo/r-help
> > >> > PLEASE do read the posting guide
> > >> > http://www.R-project.org/posting-guide.html
> > >> > and provide commented, minimal, self-contained, reproducible code.
> > >> >
> > >>
> > >>
> > >
> > >
> > > --
> > > Ramon Diaz-Uriarte
> > > Statistical Computing Team
> > > Structural Biology and Biocomputing Programme
> > > Spanish National Cancer Centre (CNIO)
> > > http://ligarto.org/rdiaz
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > --
> > Martin T. Morgan
> > Bioconductor / Computational Biology
> > http://bioconductor.org
> >
>
>
> --
> Ramon Diaz-Uriarte
> Statistical Computing Team
> Structural Biology and Biocomputing Programme
> Spanish National Cancer Centre (CNIO)
> http://ligarto.org/rdiaz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From freezeman06 at gmail.com  Mon Jan  8 03:59:43 2007
From: freezeman06 at gmail.com (freezemanx)
Date: Sun, 7 Jan 2007 18:59:43 -0800 (PST)
Subject: [R] Using JRI Calling  R function from Java
Message-ID: <8212021.post@talk.nabble.com>


Hey guys.

I have Installed and running JRI from Java
and it works. Using : java -cp jri.jar; MainApp.java

The problem is How do I calling R function that need R library 
I have tried these with my Java program : 
               x =  re.eval("glm( y ~ x1 + x2, family = poisson)");
               String resultString = x.asString();
but it did'n't work, resultString variable did't give any result it's null

questions : 
Is there any other way to call this R function from Java using JRI?

and  I have heard omegahat package can be use to calling R function
are there any links, examples , tutorials for using omegahat?

Thanks 
-- 
View this message in context: http://www.nabble.com/Using-JRI-Calling--R-function-from-Java-tf2937179.html#a8212021
Sent from the R help mailing list archive at Nabble.com.


From lsmithingm at gmail.com  Mon Jan  8 04:21:42 2007
From: lsmithingm at gmail.com (Linda Smith)
Date: Sun, 7 Jan 2007 19:21:42 -0800
Subject: [R] R scripts to plot Taylor Diagram
Message-ID: <f22a33d30701071921u3d021695xf408641f98a6894e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070107/a5ea151e/attachment.pl 

From ripley at stats.ox.ac.uk  Mon Jan  8 04:51:43 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Jan 2007 03:51:43 +0000 (GMT)
Subject: [R] MDS in 3D
In-Reply-To: <f76eb3cf306c.45a1954e@utu.fi>
References: <f76eb3cf306c.45a1954e@utu.fi>
Message-ID: <Pine.LNX.4.64.0701080348470.21410@gannet.stats.ox.ac.uk>

On Mon, 8 Jan 2007, Atte Tenkanen wrote:

> I have tried to develop multidimensional scaling for 3D space using PCA 
> without success, yet;-) Is there some application ready in R?

Yes.

stats has cmdscale. MASS has sammon and isoMDS.  All three have a 'k'
argument to determine the output dimension.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From h.wickham at gmail.com  Mon Jan  8 05:40:04 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 7 Jan 2007 20:40:04 -0800
Subject: [R] MDS in 3D
In-Reply-To: <fd6ca9895998.45a19868@utu.fi>
References: <f76eb3cf306c.45a1954e@utu.fi> <fd6ca9895998.45a19868@utu.fi>
Message-ID: <f8e6ff050701072040j2eb6ca7bvb0d7420baf407e8d@mail.gmail.com>

> I found xgobi, but when I try to run example I get some command not found -errors.

Try the more moden ggobi and ggvis, http://www.ggobi.org.  Installing
on the mac is still a bit of a trial, but we hope to have an installer
soon.

Hadley


From attenka at utu.fi  Mon Jan  8 09:10:55 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Mon, 08 Jan 2007 10:10:55 +0200
Subject: [R] MDS in 3D
In-Reply-To: <Pine.LNX.4.64.0701080348470.21410@gannet.stats.ox.ac.uk>
References: <f76eb3cf306c.45a1954e@utu.fi>
	<Pine.LNX.4.64.0701080348470.21410@gannet.stats.ox.ac.uk>
Message-ID: <fd6fc8db518.45a218af@utu.fi>

Oh, it was nearer than I have thought! Thanks.

Atte

> 
> > I have tried to develop multidimensional scaling for 3D space 
> using PCA 
> > without success, yet;-) Is there some application ready in R?
> 
> Yes.
> 
> stats has cmdscale. MASS has sammon and isoMDS.  All three have a 'k'
> argument to determine the output dimension.
> 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From henric.nilsson at statisticon.se  Mon Jan  8 09:33:26 2007
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Mon, 08 Jan 2007 09:33:26 +0100
Subject: [R] ifelse on data frames
In-Reply-To: <2213.203.109.182.160.1167967080.squirrel@webmail.scms.waikato.ac.nz>
References: <2213.203.109.182.160.1167967080.squirrel@webmail.scms.waikato.ac.nz>
Message-ID: <45A201D6.2010404@statisticon.se>

maj at stats.waikato.ac.nz said the following on 2007-01-05 04:18:

> [Using R 2.2.0 on Windows XP; OK, OK, I will update soon!]
> 
> I have noticed some undesirable behaviour when applying
> ifelse to a data frame. Here is my code:
> 
> A <- scan()
>  1.000000 0.000000 0.000000  0 0.00000
>  0.027702 0.972045 0.000253  0 0.00000
> 
> A <- matrix(A,nrow=2,ncol=5,byrow=T)
> A == 0
> ifelse(A==0,0,-A*log(A))
> 
> A <- as.data.frame(A)
> ifelse(A==0,0,-A*log(A))

How about using

sapply(A, function(x) ifelse(x == 0, 0, -x*log(x)))

?


HTH,
Henric



> 
> and this is the output:
> 
>> A <- scan()
> 1:  1.000000 0.000000 0.000000  0 0.00000
> 6:  0.027702 0.972045 0.000253  0 0.00000
> 11:
> Read 10 items
>> A <- matrix(A,nrow=2,ncol=5,byrow=T)
>> A == 0
>       [,1]  [,2]  [,3] [,4] [,5]
> [1,] FALSE  TRUE  TRUE TRUE TRUE
> [2,] FALSE FALSE FALSE TRUE TRUE
>> ifelse(A==0,0,-A*log(A))
>            [,1]       [,2]        [,3] [,4] [,5]
> [1,] 0.00000000 0.00000000 0.000000000    0    0
> [2,] 0.09934632 0.02756057 0.002095377    0    0
>> A <- as.data.frame(A)
>> ifelse(A==0,0,-A*log(A))
> [[1]]
> [1] 0.00000000 0.09934632
> 
> [[2]]
> [1]        NaN 0.02756057
> 
> [[3]]
> [1] 0
> 
> [[4]]
> [1] NaN NaN
> 
> [[5]]
> [1] 0
> 
> [[6]]
> [1] 0.00000000 0.09934632
> 
> [[7]]
> [1] 0
> 
> [[8]]
> [1] 0
> 
> [[9]]
> [1] 0
> 
> [[10]]
> [1] 0
> 
> 
> Is this a bug or a feature? Can the behaviour be explained?
> 
> Regards,  Murray Jorgensen


From karileigh at gmail.com  Mon Jan  8 06:09:44 2007
From: karileigh at gmail.com (Kari)
Date: Sun, 7 Jan 2007 21:09:44 -0800
Subject: [R] Plot .jpeg image in margins?
Message-ID: <95e45f50701072109o2bd8a351y29d5fbaaefe94f07@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070107/78b414ac/attachment.pl 

From lsmithingm at gmail.com  Mon Jan  8 02:09:51 2007
From: lsmithingm at gmail.com (Linda Smith)
Date: Sun, 7 Jan 2007 17:09:51 -0800
Subject: [R] Scripts to plot Taylor Diagram in R
Message-ID: <f22a33d30701071709s539525f2uaac004c2d288ad67@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070107/a1a3153d/attachment.pl 

From lorenz.gygax at art.admin.ch  Mon Jan  8 10:13:39 2007
From: lorenz.gygax at art.admin.ch (lorenz.gygax at art.admin.ch)
Date: Mon, 8 Jan 2007 10:13:39 +0100
Subject: [R] Contrasts for ordered factors
Message-ID: <145C63777EF3ED41A5A99035845F7DD9D31163@EVD-C8001.bk.evdad.admin.ch>

Dear all,

I do not seem to grasp how contrasts are set for ordered factors. Perhaps someone can elighten me?

When I work with ordered factors, I would often like to be able to reduce the used polynomial to a simpler one (where possible). Thus, I would like to explicetly code the polynomial but ideally, the intial model (thus, the full polynomial) would be identical to one with an ordered factor.

Here is a toy example with an explanatory variable (EV) with three distinct values (1 to 3) and a continuous response variable (RV):

options (contrasts= c ('contr.treatment', 'contr.poly'))
example.df <- data.frame (EV= rep (1:3, 5))
set.seed (298)
example.df$RV <- 2 * example.df$EV + rnorm (15)

I evaluate this data using either an ordered factor or a polynomial with a linear and a quadratic term:

lm.ord <- lm (RV ~ ordered (EV), example.df)
lm.pol <- lm (RV ~ EV + I(EV^2), example.df)

I then see that the estimated coefficients differ (and in other examples that I have come across, it is often even more extreme):

coef (lm.ord)
(Intercept) ordered(EV).L ordered(EV).Q 
  3.9497767     2.9740535    -0.1580798 
coef (lm.pol)
(Intercept)            EV       I(EV^2) 
 -0.9015283     2.8774032    -0.1936074 

but the predictions are the same (except for some rounding):

table (round (predict (lm.ord), 6) == round (predict (lm.pol), 6))
TRUE 
  15 

I thus conclude that the two models are the same and are just using a different parametrisation. I can easily interprete the parameters of the explicit polynomial but I started to wonder about the parametrisation of the ordered factor. In search of an answer, I did check the contrasts:

contr.poly (levels (ordered (example.df$EV)))
                .L         .Q
[1,] -7.071068e-01  0.4082483
[2,] -9.073264e-17 -0.8164966
[3,]  7.071068e-01  0.4082483

The linear part basically seems to be -0.707, 0 (apart for numerical rounding) and 0.707. I can understand that any even-spaced parametrisation is possible for the linear part. But does someone know where the value of 0.707 comes from (it seems to be the square-root of 0.5, but why?) and why the middle term is not exactly 0?

I do not understand the quadratic part at all. Wouldn't that need the be the linear part to the power of 2?

Thank you for your thoughts! Lorenz
- 
Lorenz Gygax
Swiss Federal Veterinary Office
Centre for proper housing of ruminants and pigs
T?nikon, CH-8356 Ettenhausen / Switzerland


From niederlein-rstat at yahoo.de  Mon Jan  8 10:26:29 2007
From: niederlein-rstat at yahoo.de (Antje)
Date: Mon, 08 Jan 2007 10:26:29 +0100
Subject: [R] tapply, how to get level information
Message-ID: <45A20E45.4060300@yahoo.de>


Hello,

I'm applying a self-written function to a matrix on basis of different 
levels.
Is there any way, to get the level information within the self-written 
function???

t <- tapply(mat, levels, plotDensity)

plotDensity <- function(x) {
	??? print(level(x)) ???
}

Antje


From dickgiesser at gmail.com  Mon Jan  8 11:00:30 2007
From: dickgiesser at gmail.com (Benjamin Dickgiesser)
Date: Mon, 8 Jan 2007 10:00:30 +0000
Subject: [R] Speeding things up
Message-ID: <b75d67340701080200s5540e26ar2c21cb5abb67a13a@mail.gmail.com>

Hi,

is it possible to do this operation faster? I am going over 35k data
entries and this takes quite some time.

	for(cnt in 2:length(sdata$date))
	{

		if(sdata$value[cnt] < sdata$value[cnt - 1])	{
			sdata$ddtd[cnt] <- sdata$ddtd[cnt - 1] + sdata$value[cnt - 1] -
sdata$value[cnt]
		}
		else sdata$ddtd[cnt] <- 0
	
	}
	return(sdata)

Thank you,
Benjamin


From ripley at stats.ox.ac.uk  Mon Jan  8 11:37:11 2007
From: ripley at stats.ox.ac.uk (Professor Brian Ripley)
Date: Mon, 08 Jan 2007 10:37:11 +0000
Subject: [R] Contrasts for ordered factors
In-Reply-To: <145C63777EF3ED41A5A99035845F7DD9D31163@EVD-C8001.bk.evdad.admin.ch>
References: <145C63777EF3ED41A5A99035845F7DD9D31163@EVD-C8001.bk.evdad.admin.ch>
Message-ID: <45A21ED7.9090008@stats.ox.ac.uk>

contr.poly is using orthoogonal polynomials: look at poly() for
further information.  It seems to me that you did not realize that,
or did not realize what they are, or ... and that may be enough of
a hint for you or you may need more help, in which case please ask
again.

lorenz.gygax at art.admin.ch wrote:
> Dear all,
> 
> I do not seem to grasp how contrasts are set for ordered factors. Perhaps someone can elighten me?
> 
> When I work with ordered factors, I would often like to be able to reduce the used polynomial to a simpler one (where possible). Thus, I would like to explicetly code the polynomial but ideally, the intial model (thus, the full polynomial) would be identical to one with an ordered factor.
> 
> Here is a toy example with an explanatory variable (EV) with three distinct values (1 to 3) and a continuous response variable (RV):
> 
> options (contrasts= c ('contr.treatment', 'contr.poly'))
> example.df <- data.frame (EV= rep (1:3, 5))
> set.seed (298)
> example.df$RV <- 2 * example.df$EV + rnorm (15)
> 
> I evaluate this data using either an ordered factor or a polynomial with a linear and a quadratic term:
> 
> lm.ord <- lm (RV ~ ordered (EV), example.df)
> lm.pol <- lm (RV ~ EV + I(EV^2), example.df)
> 
> I then see that the estimated coefficients differ (and in other examples that I have come across, it is often even more extreme):
> 
> coef (lm.ord)
> (Intercept) ordered(EV).L ordered(EV).Q 
>   3.9497767     2.9740535    -0.1580798 
> coef (lm.pol)
> (Intercept)            EV       I(EV^2) 
>  -0.9015283     2.8774032    -0.1936074 
> 
> but the predictions are the same (except for some rounding):
> 
> table (round (predict (lm.ord), 6) == round (predict (lm.pol), 6))
> TRUE 
>   15 
> 
> I thus conclude that the two models are the same and are just using a different parametrisation. I can easily interprete the parameters of the explicit polynomial but I started to wonder about the parametrisation of the ordered factor. In search of an answer, I did check the contrasts:
> 
> contr.poly (levels (ordered (example.df$EV)))
>                 .L         .Q
> [1,] -7.071068e-01  0.4082483
> [2,] -9.073264e-17 -0.8164966
> [3,]  7.071068e-01  0.4082483
> 
> The linear part basically seems to be -0.707, 0 (apart for numerical rounding) and 0.707. I can understand that any even-spaced parametrisation is possible for the linear part. But does someone know where the value of 0.707 comes from (it seems to be the square-root of 0.5, but why?) and why the middle term is not exactly 0?
> 
> I do not understand the quadratic part at all. Wouldn't that need the be the linear part to the power of 2?
> 
> Thank you for your thoughts! Lorenz
> - 
> Lorenz Gygax
> Swiss Federal Veterinary Office
> Centre for proper housing of ruminants and pigs
> T?nikon, CH-8356 Ettenhausen / Switzerland
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Mon Jan  8 12:02:40 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 08 Jan 2007 12:02:40 +0100
Subject: [R] different points and lines on the same plot
In-Reply-To: <971536df0701070632u3ca99cd7la631ce637bac7ae@mail.gmail.com>
References: <540854.92447.qm@web26005.mail.ukl.yahoo.com>
Message-ID: <45A232E0.1588.E0591B@localhost>

Hi

On 7 Jan 2007 at 9:32, Gabor Grothendieck wrote:

Date sent:      	Sun, 7 Jan 2007 09:32:54 -0500
From:           	"Gabor Grothendieck" <ggrothendieck at gmail.com>
To:             	"antoniababe at yahoo.se" <antoniababe at yahoo.se>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] different points and lines on the same plot

> Try this:
> 
> matplot(patient[,1], patient[,-1], type = "o")

Another option is to use plot and lines.

first initialise plotting region without any actual plotting

> plot(patient[,1], patient[,2], type="n", ylim=range(patient[,-1]))

then add lines in a cycle

> for(i in 1:4) lines(patient[,1], patient[,i+1], col=i, lwd=2, 
lty=i)

HTH
Petr



> 
> 
> On 1/7/07, antoniababe at yahoo.se <antoniababe at yahoo.se> wrote:
> > Dear all,
> >
> > I have following data called "paitent"
> >
> > day    patient1    patient4    patient5    patient6
> > 0   -0.27842688 -0.04080808 -0.41948398 -0.04508318
> > 56  -0.22275425 -0.01767067 -0.30977249 -0.03168185
> > 112 -0.08217659 -0.26209243 -0.29141451 -0.09876170
> > 252  0.08044537 -0.26701769  0.05727087 -0.09663701
> >
> > where each patient have response values at four time
> > points. I want to plot each patient's values over time
> > on the same plot where the value points are connected
> > by line. That is, the graph will have four lines for
> > the four patients. I tried the program below but
> > couldn't make it work correctly. I'm new beginner and
> > haven't yet learned how functions line and points work
> > together. Hope you can help me out.
> >
> > Thanks for your help,
> >
> > Antonia
> >
> > par(mfrow=c(1,1))
> >        plot(patient[,1],patient[,2], pch=1,
> > type="l",col=1,cex=1,lwd=2,
> >             xlab="Days", ylab="Patient
> > response",cex.main =1,font.main= 1,
> >             main=NULL)
> >
> >
> > points(patient[,1],patient[,3],col=2,pch=1,cex=1)
> > lines(patient[,1],patient[,3],col=2,lty=1,cex=1)
> >
> > points(patient[,1],patient[,4],col=3,pch=1,cex=1)
> > lines(patient[,1],patient[,4],col=2,lty=2,cex=1)
> >
> > points(patient[,1],patient[,5],col=4,pch=1,cex=1)
> > lines(patient[,1],patient[,5],col=2,lty=1,cex=1)
> >  points(patient[,1],patient[,6],col=5,pch=1,cex=1)
> >  lines(patient[,1],patient[,6],col=2,lty=1,cex=1)
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From P.Dalgaard at biostat.ku.dk  Mon Jan  8 12:08:00 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 08 Jan 2007 12:08:00 +0100
Subject: [R] Contrasts for ordered factors
In-Reply-To: <145C63777EF3ED41A5A99035845F7DD9D31163@EVD-C8001.bk.evdad.admin.ch>
References: <145C63777EF3ED41A5A99035845F7DD9D31163@EVD-C8001.bk.evdad.admin.ch>
Message-ID: <45A22610.6060905@biostat.ku.dk>

lorenz.gygax at art.admin.ch wrote:
> Dear all,
>
> I do not seem to grasp how contrasts are set for ordered factors. Perhaps someone can elighten me?
>
> When I work with ordered factors, I would often like to be able to reduce the used polynomial to a simpler one (where possible). Thus, I would like to explicetly code the polynomial but ideally, the intial model (thus, the full polynomial) would be identical to one with an ordered factor.
>
> Here is a toy example with an explanatory variable (EV) with three distinct values (1 to 3) and a continuous response variable (RV):
>
> options (contrasts= c ('contr.treatment', 'contr.poly'))
> example.df <- data.frame (EV= rep (1:3, 5))
> set.seed (298)
> example.df$RV <- 2 * example.df$EV + rnorm (15)
>
> I evaluate this data using either an ordered factor or a polynomial with a linear and a quadratic term:
>
> lm.ord <- lm (RV ~ ordered (EV), example.df)
> lm.pol <- lm (RV ~ EV + I(EV^2), example.df)
>
> I then see that the estimated coefficients differ (and in other examples that I have come across, it is often even more extreme):
>
> coef (lm.ord)
> (Intercept) ordered(EV).L ordered(EV).Q 
>   3.9497767     2.9740535    -0.1580798 
> coef (lm.pol)
> (Intercept)            EV       I(EV^2) 
>  -0.9015283     2.8774032    -0.1936074 
>
> but the predictions are the same (except for some rounding):
>
> table (round (predict (lm.ord), 6) == round (predict (lm.pol), 6))
> TRUE 
>   15 
>
> I thus conclude that the two models are the same and are just using a different parametrisation. I can easily interprete the parameters of the explicit polynomial but I started to wonder about the parametrisation of the ordered factor. In search of an answer, I did check the contrasts:
>
> contr.poly (levels (ordered (example.df$EV)))
>                 .L         .Q
> [1,] -7.071068e-01  0.4082483
> [2,] -9.073264e-17 -0.8164966
> [3,]  7.071068e-01  0.4082483
>
> The linear part basically seems to be -0.707, 0 (apart for numerical rounding) and 0.707. I can understand that any even-spaced parametrisation is possible for the linear part. But does someone know where the value of 0.707 comes from (it seems to be the square-root of 0.5, but why?) and why the middle term is not exactly 0?
>
> I do not understand the quadratic part at all. Wouldn't that need the be the linear part to the power of 2?
>
>   
These are orthogonal polynomials. 

To see the main point, try

> M <- cbind(1,contr.poly (3))

> M

                  .L         .Q

[1,] 1 -7.071068e-01  0.4082483

[2,] 1 -7.850462e-17 -0.8164966

[3,] 1  7.071068e-01  0.4082483

> zapsmall(crossprod(M))

     .L .Q

   3  0  0

.L 0  1  0

.Q 0  0  1


This parametrization has better numerical properties than the
straightforward 1,x,x^2,... , especially in balanced designs.

(SOAPBOX: Some, including me, feel that  having polynomials as default
contrasts for ordered factors is a bit of a design misfeature - It was
inherited from S, but assigning equidistant numerical values to ordered
groups isn't really well-founded, and does become plainly wrong when the
levels are really something like 0, 3, 6, 12, 18, 24 months.)
 

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From dickgiesser at gmail.com  Mon Jan  8 12:17:24 2007
From: dickgiesser at gmail.com (Benjamin Dickgiesser)
Date: Mon, 8 Jan 2007 11:17:24 +0000
Subject: [R] Export dataframe to txt
Message-ID: <b75d67340701080317i31b05de1l67c03b9eac49d382@mail.gmail.com>

Hi all,

Is there a function to export a dataframe to a text file?
I want to store a large set of data which I have saved in a dataframe
in my workspace and copy and past doesn't cut it.

Thank you,
Benjamin


From milton_ruser at yahoo.com.br  Mon Jan  8 12:22:04 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Mon, 8 Jan 2007 11:22:04 +0000 (GMT)
Subject: [R] ACCESS/Office : connecting
Message-ID: <377449.24974.qm@web56602.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070108/848ef12f/attachment.pl 

From news at aspden.com  Mon Jan  8 12:25:41 2007
From: news at aspden.com (John Lawrence Aspden)
Date: Mon, 08 Jan 2007 11:25:41 +0000
Subject: [R] scripts with littler
Message-ID: <ent9nm$rc6$1@sea.gmane.org>

Hi,

I'm trying to write R scripts using littler (under Debian), and was
originally using the shebang line:

#!/usr/bin/env r

However this picks up any .RData file that happens to be lying around, which
I find a little disturbing, because it means that the script may not behave
the same way on successive invocations.

If you drop the /usr/bin/env trick then 

#!/usr/bin/r --vanilla

seems to work, but it also prevents the loading of the libraries in my home
directory, some of which I'd like to use.

#!/usr/bin/r --no-restore

doesn't work at all.

Ideally I'd like #!/usr/bin/env r --no-restore

Has anyone else been round this loop and can offer advice?

Cheers, John.

-- 
Contractor in Cambridge UK -- http://www.aspden.com


From hb at stat.berkeley.edu  Mon Jan  8 12:27:18 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Mon, 8 Jan 2007 22:27:18 +1100
Subject: [R] Speeding things up
In-Reply-To: <b75d67340701080200s5540e26ar2c21cb5abb67a13a@mail.gmail.com>
References: <b75d67340701080200s5540e26ar2c21cb5abb67a13a@mail.gmail.com>
Message-ID: <59d7961d0701080327n525dbc80h5df17122afe40fc1@mail.gmail.com>

First, since you only update the 'ddtd' conditioned on 'value', you
should be able to vectorize removing the loop.  I let you figure out
how to do that yourself.

Second, you apply the "$" operator multiple times in the loop that
will definitely add some overhead.  It should be faster to extract
'value' and 'ddtd' and work with those instead.

/Henrik

On 1/8/07, Benjamin Dickgiesser <dickgiesser at gmail.com> wrote:
> Hi,
>
> is it possible to do this operation faster? I am going over 35k data
> entries and this takes quite some time.
>
>         for(cnt in 2:length(sdata$date))
>         {
>
>                 if(sdata$value[cnt] < sdata$value[cnt - 1])     {
>                         sdata$ddtd[cnt] <- sdata$ddtd[cnt - 1] + sdata$value[cnt - 1] -
> sdata$value[cnt]
>                 }
>                 else sdata$ddtd[cnt] <- 0
>
>         }
>         return(sdata)
>
> Thank you,
> Benjamin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From zkmetty at gmail.com  Mon Jan  8 12:32:41 2007
From: zkmetty at gmail.com (Zoltan Kmetty)
Date: Mon, 8 Jan 2007 12:32:41 +0100
Subject: [R] Speeding things up
In-Reply-To: <b75d67340701080200s5540e26ar2c21cb5abb67a13a@mail.gmail.com>
References: <b75d67340701080200s5540e26ar2c21cb5abb67a13a@mail.gmail.com>
Message-ID: <c1055ec10701080332k3b78ec02xedd3fc399a43254b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070108/10e6e8cd/attachment.pl 

From damjan.krstajic at rcc.org.yu  Fri Jan  5 12:54:38 2007
From: damjan.krstajic at rcc.org.yu (damjan.krstajic at rcc.org.yu)
Date: Fri, 5 Jan 2007 12:54:38 +0100 (CET)
Subject: [R] outsourcing R work
Message-ID: <58841.217.24.31.88.1167998078.squirrel@mail.rcc.org.yu>

Dear Colleagues,

If you have a need to outsource any coding in R please check the web site
of the Research Centre for Cheminformatics in Belgrade, Serbia
http://www.rcc.org.yu . We have skilled statisticians experienced in R,
fluent in English and with work experience in the West.

For more information please check our website
http://www.rcc.org.yu/outsourcing.htm or contact me.

Kind regards,
DK
-----------------
Damjan Krstajic
Director
Research Centre for Cheminformatics
e-mail: Damjan.Krstajic at rcc.org.yu
www.rcc.org.yu


From wwwhsd at gmail.com  Mon Jan  8 12:37:28 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Mon, 8 Jan 2007 09:37:28 -0200
Subject: [R] Export dataframe to txt
In-Reply-To: <b75d67340701080317i31b05de1l67c03b9eac49d382@mail.gmail.com>
References: <b75d67340701080317i31b05de1l67c03b9eac49d382@mail.gmail.com>
Message-ID: <da79af330701080337q34ab0259q5eb08855ac2e109f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070108/cef5975e/attachment.pl 

From dickgiesser at gmail.com  Mon Jan  8 12:39:34 2007
From: dickgiesser at gmail.com (Benjamin Dickgiesser)
Date: Mon, 8 Jan 2007 11:39:34 +0000
Subject: [R] Speeding things up
In-Reply-To: <c1055ec10701080332k3b78ec02xedd3fc399a43254b@mail.gmail.com>
References: <b75d67340701080200s5540e26ar2c21cb5abb67a13a@mail.gmail.com>
	<c1055ec10701080332k3b78ec02xedd3fc399a43254b@mail.gmail.com>
Message-ID: <b75d67340701080339u891c846pc4666148eaca3e7a@mail.gmail.com>

Thanks for your answers, your help is highly appreciated!

On 1/8/07, Zoltan Kmetty <zkmetty at gmail.com> wrote:
> Hi Benjamin!
>
>
> ##TRY THIS: THIS MAYBE MUCH FASTER, BECAUSE ONLY WORK WITH THE IMPORTANAT
> ROWS - HOPE NO ERROR IN IT:D
>
> puffer1 <- as.matrix(sdata$value)
> puffer2 <- rbind(as.matrix(puffer1[2:nrow(puffer1),1]),0)
>
> speedy <- puffer1 > puffer2
> speedy <- (as.matrix(which(puffer1==TRUE)))+1
> sdata$ddtd[]=0
>
> for(cnt in 1:nrow(speedy))
>        {
>
> sdata$ddtd[speedy[cnt,1]] <- sdata$ddtd[(speedy[cnt,1]) - 1] +
> sdata$value[(speedy[cnt,1]) - 1] -sdata$value[speedy[cnt,1]]
>        }
>        return(sdata)
>
>
>
>
> #Zoltan
>
>
>
> 2007/1/8, Benjamin Dickgiesser <dickgiesser at gmail.com>:
> >
> > Hi,
> >
> > is it possible to do this operation faster? I am going over 35k data
> > entries and this takes quite some time.
> >
> >        for(cnt in 2:length(sdata$date))
> >        {
> >
> >                if(sdata$value[cnt] < sdata$value[cnt - 1])     {
> >                        sdata$ddtd[cnt] <- sdata$ddtd[cnt - 1] +
> sdata$value[cnt - 1] -
> > sdata$value[cnt]
> >                }
> >                else sdata$ddtd[cnt] <- 0
> >
> >        }
> >        return(sdata)
> >
> > Thank you,
> > Benjamin
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>


From detlef.steuer at hsu-hamburg.de  Mon Jan  8 12:40:32 2007
From: detlef.steuer at hsu-hamburg.de (Detlef Steuer)
Date: Mon, 8 Jan 2007 12:40:32 +0100
Subject: [R] Export dataframe to txt
In-Reply-To: <b75d67340701080317i31b05de1l67c03b9eac49d382@mail.gmail.com>
References: <b75d67340701080317i31b05de1l67c03b9eac49d382@mail.gmail.com>
Message-ID: <20070108124032.a9ec23f5.detlef.steuer@hsu-hamburg.de>

On Mon, 8 Jan 2007 11:17:24 +0000
"Benjamin Dickgiesser" <dickgiesser at gmail.com> wrote:

> Hi all,
> 
> Is there a function to export a dataframe to a text file?
> I want to store a large set of data which I have saved in a dataframe
> in my workspace and copy and past doesn't cut it.

see ?write.table

Hth
Detlef

> 
> Thank you,
> Benjamin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From news at aspden.com  Mon Jan  8 12:43:48 2007
From: news at aspden.com (John Lawrence Aspden)
Date: Mon, 08 Jan 2007 11:43:48 +0000
Subject: [R] scripts with littler / subroutines
Message-ID: <entapk$uo3$1@sea.gmane.org>

Hi (again),

Another difficulty I'm having is creating a common function (foo, say) to
share between two scripts.

I've tried making a third file containing the function and then sourcing it
with source ("foo.R"), but that only works if you run the script in the
directory where "foo.R" is. (or if the scripts know where they're
installed)

The other solutions that occur are copy-and-paste, a preprocessor, or some
sort of special-purpose library. I think I like the preprocessor best, but
it's still kind of nasty.

I have the feeling that I'm probably missing something obvious here! Can
anyone help?

Cheers, John.



-- 
Contractor in Cambridge UK -- http://www.aspden.com


From zkmetty at gmail.com  Mon Jan  8 12:52:59 2007
From: zkmetty at gmail.com (Zoltan Kmetty)
Date: Mon, 8 Jan 2007 12:52:59 +0100
Subject: [R] Export dataframe to txt
In-Reply-To: <b75d67340701080317i31b05de1l67c03b9eac49d382@mail.gmail.com>
References: <b75d67340701080317i31b05de1l67c03b9eac49d382@mail.gmail.com>
Message-ID: <c1055ec10701080352s52818ccbr43dcb29230235499@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070108/9e01eab1/attachment.pl 

From petr.pikal at precheza.cz  Mon Jan  8 12:54:31 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 08 Jan 2007 12:54:31 +0100
Subject: [R] Export dataframe to txt
In-Reply-To: <b75d67340701080317i31b05de1l67c03b9eac49d382@mail.gmail.com>
Message-ID: <45A23F07.625.10FD1A1@localhost>

Hi
AFAIK 
?sink
?write.table
are possible options.

HTH
Petr



On 8 Jan 2007 at 11:17, Benjamin Dickgiesser wrote:

Date sent:      	Mon, 8 Jan 2007 11:17:24 +0000
From:           	"Benjamin Dickgiesser" <dickgiesser at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Export dataframe to txt

> Hi all,
> 
> Is there a function to export a dataframe to a text file?
> I want to store a large set of data which I have saved in a dataframe
> in my workspace and copy and past doesn't cut it.
> 
> Thank you,
> Benjamin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From magepost at googlemail.com  Mon Jan  8 13:14:07 2007
From: magepost at googlemail.com (Oleg Sklyar)
Date: Mon, 8 Jan 2007 12:14:07 +0000
Subject: [R] Plot .jpeg image in margins?
In-Reply-To: <95e45f50701072109o2bd8a351y29d5fbaaefe94f07@mail.gmail.com>
References: <95e45f50701072109o2bd8a351y29d5fbaaefe94f07@mail.gmail.com>
Message-ID: <399fc6f10701080414t4008e3a3x168bd6e4676a2bc5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070108/ff2e4ed2/attachment.pl 

From HEINRICH.RINNER at tirol.gv.at  Mon Jan  8 13:20:59 2007
From: HEINRICH.RINNER at tirol.gv.at (RINNER Heinrich)
Date: Mon, 8 Jan 2007 13:20:59 +0100
Subject: [R] ACCESS/Office : connecting
Message-ID: <D9D0AED7B0265A4E8AC4B9715163E90612912C@mxs02.tirol.local>

Hi,
you can use the RODBC pakage for that; see:

library(RODBC)
?RODBC
?odbcConnectAccess
?sqlSave

Regards,
Heinrich.

> -----Urspr?ngliche Nachricht-----
> Von: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] Im Auftrag von 
> Milton Cezar Ribeiro
> Gesendet: Montag, 08. J?nner 2007 12:22
> An: R-help
> Betreff: [R] ACCESS/Office : connecting
> 
> 
> Hi there, 
>    
>   How can I connect to a ACCESS (.mdb) file? In fact, I would 
> like to connect to a blank file, write a data.frame as table 
> and after that INSERT records using some "insert" command.
>    
>   Kind regards,
>    
>   Miltinho
>   Brazil
> 
>  __________________________________________________
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide 
> commented, minimal, self-contained, reproducible code.
>


From rfrancois at mango-solutions.com  Mon Jan  8 13:22:03 2007
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Mon, 08 Jan 2007 12:22:03 +0000
Subject: [R] R Training Course in Paris
Message-ID: <45A2376B.6040105@mango-solutions.com>

Mango Solutions are pleased to announce the above course in Paris as
part of our schedule for Q1 2007.

-----------------------------------------------------------------------
Introduction to R and R Programming - 12th February 2008-14th February
-----------------------------------------------------------------------

(Please find a french version of this announcement from
http://www.mango-solutions.com/services/rtraining/r_paris_training_07_french.html)

* Who Should Attend ?

This is a course suitable for beginners and improvers in the R language
and is ideal for people wanting an all round introduction to R

* Course Goals

- To allow attendees to understand the technology behind the R package
- Improve attendees programming style and confidence
- To enable users to access a wide range of available functionality
- To enable attendees to program in R within their own environment
- To understand how to embed R routines within other applications

* Course Outline

1. Introduction to the R language and the R community
2. The R Environment
3. R data objects
4. Using R functions
5. The "apply" family of functions
6. Writing R functions
7. Standard Graphics
8. Advanced Graphics
9. R Statistics
10. R Applications


The cost of these courses is ?1800 for commercial attendees and ?850 for
academic attendees. A ?100 discount will be offered to members of the R
Foundation (http://www.r-project.org/foundation/main.html).

Should your organization have more than 3 possible attendees why not
talk to us about hosting a customized and focused course delivered at
your premises? Details of further courses in alternative locations are
available at http://www.mango-solutions.com/services/training.html

More details about this course :
- in french :
http://www.mango-solutions.com/services/rtraining/r_paris_training_07_french.html
- in english :
http://www.mango-solutions.com/services/rtraining/r_paris_training_07.html

Should you want to book a place on this course or have any questions
please contact training at mango-solutions.com


Cordialement,

Romain Francois

--
Mango Solutions
Tel +44 (0)1249 467 467
Mob +44 (0)7813 526 123
Fax +44 (0)1249 467 468

data analysis that delivers


From rfrancois at mango-solutions.com  Mon Jan  8 13:35:07 2007
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Mon, 08 Jan 2007 12:35:07 +0000
Subject: [R] R Training Course in Paris
In-Reply-To: <45A2376B.6040105@mango-solutions.com>
References: <45A2376B.6040105@mango-solutions.com>
Message-ID: <45A23A7B.7080304@mango-solutions.com>

Romain Francois wrote:
> Mango Solutions are pleased to announce the above course in Paris as
> part of our schedule for Q1 2007.
>
> -----------------------------------------------------------------------
> Introduction to R and R Programming - 12th February 2008-14th February
> -----------------------------------------------------------------------
>   
Small typo, the correct date is : 12th February 2007-14th February.

Cheers,

Romain

> (Please find a french version of this announcement from
> http://www.mango-solutions.com/services/rtraining/r_paris_training_07_french.html)
>
> * Who Should Attend ?
>
> This is a course suitable for beginners and improvers in the R language
> and is ideal for people wanting an all round introduction to R
>
> * Course Goals
>
> - To allow attendees to understand the technology behind the R package
> - Improve attendees programming style and confidence
> - To enable users to access a wide range of available functionality
> - To enable attendees to program in R within their own environment
> - To understand how to embed R routines within other applications
>
> * Course Outline
>
> 1. Introduction to the R language and the R community
> 2. The R Environment
> 3. R data objects
> 4. Using R functions
> 5. The "apply" family of functions
> 6. Writing R functions
> 7. Standard Graphics
> 8. Advanced Graphics
> 9. R Statistics
> 10. R Applications
>
>
> The cost of these courses is ?1800 for commercial attendees and ?850 for
> academic attendees. A ?100 discount will be offered to members of the R
> Foundation (http://www.r-project.org/foundation/main.html).
>
> Should your organization have more than 3 possible attendees why not
> talk to us about hosting a customized and focused course delivered at
> your premises? Details of further courses in alternative locations are
> available at http://www.mango-solutions.com/services/training.html
>
> More details about this course :
> - in french :
> http://www.mango-solutions.com/services/rtraining/r_paris_training_07_french.html
> - in english :
> http://www.mango-solutions.com/services/rtraining/r_paris_training_07.html
>
> Should you want to book a place on this course or have any questions
> please contact training at mango-solutions.com
>
>
> Cordialement,
>
> Romain Francois
>   
-- 
Mango Solutions
Tel  +44 1249 467 467
Fax  +44 1249 467 468
Mob  +44 7813 526 123
data analysis that delivers


From weird_fritz at yahoo.com  Sat Jan  6 18:16:50 2007
From: weird_fritz at yahoo.com (Melina Chen)
Date: Sat, 6 Jan 2007 09:16:50 -0800 (PST)
Subject: [R] garchFit in R
Message-ID: <785184.62112.qm@web33013.mail.mud.yahoo.com>

Dear all,
   
  I have problem here : 
  I'm using garchFit from fSeries package, here is part of the script : 
  > data <- read.table("d:/data.txt")
  > a <- garchFit(~garch(1,1),ts(data))
   
  I also attached the file here. In my experience, I got my R not responding.
  I also tried with 
  > a <- garchFit(~garch(1,1),ts(data*10))
  and it's worked. 
   
  I wonder if something wrong with the first
   
  Thanks 

 __________________________________________________


-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: data.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070106/877f92aa/attachment.txt 

From rfluss at netvision.net.il  Mon Jan  8 14:17:38 2007
From: rfluss at netvision.net.il (Fluss)
Date: Mon, 08 Jan 2007 15:17:38 +0200
Subject: [R] query
Message-ID: <003301c73327$5e51d1b0$ff4317ac@homeronen>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070108/ff29c8a7/attachment.pl 

From kalyansikha at yahoo.com  Mon Jan  8 14:20:38 2007
From: kalyansikha at yahoo.com (Bhanu Kalyan.K)
Date: Mon, 8 Jan 2007 05:20:38 -0800 (PST)
Subject: [R] How to use Rattle for Data mining through R?
Message-ID: <907370.79084.qm@web34311.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070108/d45b03bd/attachment.pl 

From kalyansikha at yahoo.com  Mon Jan  8 14:20:38 2007
From: kalyansikha at yahoo.com (Bhanu Kalyan.K)
Date: Mon, 8 Jan 2007 05:20:38 -0800 (PST)
Subject: [R] How to use Rattle for Data mining through R?
Message-ID: <907370.79084.qm@web34311.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070108/d45b03bd/attachment-0001.pl 

From pinard at iro.umontreal.ca  Mon Jan  8 14:29:20 2007
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Mon, 8 Jan 2007 08:29:20 -0500
Subject: [R] scripts with littler
In-Reply-To: <ent9nm$rc6$1@sea.gmane.org>
References: <ent9nm$rc6$1@sea.gmane.org>
Message-ID: <20070108132920.GA4640@phenix.progiciels-bpi.ca>

[John Lawrence Aspden]

>I'm trying to write R scripts using littler (under Debian), and was
>originally using the shebang line:

>#!/usr/bin/env r

>However this picks up any .RData file that happens to be lying around, which
>I find a little disturbing, because it means that the script may not behave
>the same way on successive invocations.

>If you drop the /usr/bin/env trick then 

>#!/usr/bin/r --vanilla

>seems to work, but it also prevents the loading of the libraries in my home
>directory, some of which I'd like to use.

>#!/usr/bin/r --no-restore

>doesn't work at all.

>Ideally I'd like #!/usr/bin/env r --no-restore

>Has anyone else been round this loop and can offer advice?

I usually do something like:


#!/bin/sh
R --slave --vanilla <<EOF

   R script goes here...

EOF

# vim: ft=r


If you need to search special places for packages, you may tweak 
exported environment variables between the first and second line.




-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca


From pinard at iro.umontreal.ca  Mon Jan  8 14:32:01 2007
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Mon, 8 Jan 2007 08:32:01 -0500
Subject: [R] scripts with littler / subroutines
In-Reply-To: <entapk$uo3$1@sea.gmane.org>
References: <entapk$uo3$1@sea.gmane.org>
Message-ID: <20070108133201.GB4640@phenix.progiciels-bpi.ca>

[John Lawrence Aspden]

>Another difficulty I'm having is creating a common function (foo, say) to
>share between two scripts.

In your previous message, you were telling us that you want to load from 
your home directory.  You might put the common functions there, maybe?

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca


From ggrothendieck at gmail.com  Mon Jan  8 14:54:04 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 8 Jan 2007 08:54:04 -0500
Subject: [R] garchFit in R
In-Reply-To: <785184.62112.qm@web33013.mail.mud.yahoo.com>
References: <785184.62112.qm@web33013.mail.mud.yahoo.com>
Message-ID: <971536df0701080554t5e1f4718kc640c8f97349d411@mail.gmail.com>

See ?garchFit .  The data argument is supposed to be of class "timeSeries" or
a data frame, not of class "ts".   Furthermore even if it did accept a variable
of class "ts" the ts function does not take a data frame as its
argument.  See ?ts
Follow the examples of using garchFit here:

https://svn.r-project.org/Rmetrics/trunk/fSeries/test/runit4C.R

and also review all your code doing a ? on each function that you use,
performing a class(x) on each argument, x, ensuring that each function
is called with the variables of the appropriate class.

On 1/6/07, Melina Chen <weird_fritz at yahoo.com> wrote:
> Dear all,
>
>  I have problem here :
>  I'm using garchFit from fSeries package, here is part of the script :
>  > data <- read.table("d:/data.txt")
>  > a <- garchFit(~garch(1,1),ts(data))
>
>  I also attached the file here. In my experience, I got my R not responding.
>  I also tried with
>  > a <- garchFit(~garch(1,1),ts(data*10))
>  and it's worked.
>
>  I wonder if something wrong with the first
>
>  Thanks
>
>  __________________________________________________
>
>
>
>
> 0.0026668996
> -0.0019157865
> -0.0001120967
> 0.0005845345
> 0.0003697987
> -0.0010323306
> -0.0023072483
> -0.0005885481
> 0.0012301371
> 0.0003082133
> -0.0002494891
> -0.0002006737
> -0.0004800350
> -0.0002843525
> 0.0027182299
> -0.0004827581
> 0.0007702074
> -0.0018742761
> -0.0012589320
> -0.0001864557
> 0.0019782314
> 0.0000000000
> -0.0016593427
> 0.0014883173
> -0.0019642834
> -0.0001129345
> 0.0003289013
> 0.0047631296
> -0.0003933189
> -0.0000777348
> -0.0001992589
> -0.0019439182
> -0.0017467015
> -0.0001716240
> -0.0003041862
> 0.0028812028
> -0.0006391600
> -0.0004934305
> 0.0002638840
> 0.0004931309
> 0.0002341631
> -0.0007321793
> -0.0021696036
> 0.0011669581
> -0.0003526987
> 0.0011745500
> -0.0001808695
> -0.0006997451
> 0.0000587632
> 0.0011198772
> 0.0017887369
> 0.0030052082
> 0.0012059188
> 0.0031819752
> -0.0032446005
> 0.0009576615
> -0.0030971545
> -0.0033876838
> 0.0007118065
> -0.0008289275
> -0.0001171526
> 0.0004440365
> -0.0016515719
> 0.0010317472
> -0.0002638158
> -0.0009735958
> 0.0011201799
> 0.0003223110
> -0.0026539675
> -0.0005848754
> 0.0010266956
> 0.0004805825
> -0.0005100232
> -0.0012333941
> -0.0046007250
> -0.0037661601
> 0.0030195219
> 0.0011691642
> -0.0005518513
> 0.0012071954
> -0.0009286963
> -0.0013194775
> -0.0010534771
> -0.0018384612
> 0.0010729640
> -0.0001352279
> -0.0041928562
> -0.0048981960
> 0.0035253602
> -0.0007769906
> -0.0065450379
> 0.0003404330
> 0.0018162840
> -0.0008223038
> -0.0003036179
> -0.0016866227
> -0.0038151118
> 0.0006511983
> -0.0049316481
> -0.0035685861
> 0.0000583904
> 0.0037786345
> 0.0047777192
> -0.0019298763
> -0.0030373119
> -0.0006163457
> -0.0058485648
> 0.0076319879
> -0.0061118610
> 0.0014086934
> 0.0020437674
> 0.0014607365
> 0.0013718807
> -0.0041446062
> 0.0010107937
> -0.0025287507
> 0.0035741117
> -0.0003428967
> 0.0004378049
> -0.0009605837
> -0.0010898236
> 0.0020398630
> 0.0009532263
> 0.0010508613
> -0.0006039345
> -0.0034717995
> 0.0027354396
> -0.0039024422
> -0.0002178340
> 0.0013848366
> 0.0018343314
> -0.0003219009
> -0.0018887114
> 0.0018939904
> 0.0007963918
> 0.0004529223
> 0.0008571518
> 0.0044218012
> -0.0019126781
> 0.0045515451
> 0.0092559993
> -0.0009015621
> -0.0020175552
> 0.0002240705
> 0.0038168506
> -0.0035878071
> -0.0002901422
> -0.0007389559
> 0.0051061545
> -0.0021933090
> 0.0014213553
> -0.0025780541
> -0.0004574410
> 0.0005336344
> -0.0004675298
> 0.0003608554
> 0.0007360280
> -0.0015191088
> -0.0021837991
> -0.0047678519
> -0.0010459318
> 0.0033618169
> -0.0011022447
> 0.0011588266
> 0.0001439934
> 0.0036812865
> -0.0010617579
> -0.0018386819
> 0.0006154677
> 0.0001178651
> -0.0006306943
> -0.0001283020
> -0.0011409873
> -0.0003809962
> 0.0016194965
> 0.0001436634
> 0.0009275365
> -0.0017079955
> 0.0017694197
> -0.0006351358
> 0.0000153772
> -0.0000358809
> -0.0024417258
> -0.0006706615
> 0.0004231532
> -0.0003250688
> 0.0000000000
> -0.0013493117
> -0.0007721806
> -0.0004618912
> 0.0011615764
> -0.0002227404
> -0.0010842518
> 0.0013691318
> -0.0006736543
> -0.0008721193
> 0.0036790147
> 0.0002935971
> -0.0001545000
> -0.0006959312
> -0.0006298763
> 0.0014236639
> 0.0016909739
> 0.0015975386
> -0.0005625643
> 0.0039227325
> 0.0001673279
> -0.0031647730
> -0.0008280651
> 0.0013231273
> 0.0013343643
> -0.0028724331
> -0.0002304126
> 0.0014980535
> -0.0012062182
> -0.0015279076
> 0.0022489747
> 0.0025778462
> -0.0014449700
> -0.0023558165
> -0.0009387271
> 0.0003644465
> 0.0052782725
> -0.0023175981
> -0.0001121273
> 0.0003362951
> -0.0000916910
> -0.0014952546
> -0.0008391870
> 0.0016716533
> -0.0013695639
> 0.0004655229
> 0.0013630021
> -0.0025559437
> 0.0000922755
> 0.0014686387
> -0.0004293316
> 0.0004599820
> -0.0009204516
> -0.0005327070
> 0.0016216967
> -0.0005620481
> 0.0013425983
> -0.0021153235
> 0.0000921851
> 0.0003224940
> 0.0012161541
> -0.0029080728
> 0.0004518232
> -0.0008167053
> -0.0008079421
> -0.0032780219
> -0.0022896000
> 0.0021286819
> -0.0003479945
> -0.0004262835
> -0.0013439693
> -0.0002400599
> 0.0019219713
> 0.0008826071
> -0.0000311203
> 0.0014344017
> 0.0003049113
> -0.0007031687
> 0.0014052008
> -0.0005419102
> 0.0033541187
> -0.0028844242
> -0.0000103175
> 0.0012312089
> 0.0003085428
> 0.0011550870
> -0.0003231148
> -0.0015316507
> -0.0002626703
> 0.0018456252
> -0.0023353370
> -0.0004851011
> -0.0008528198
> 0.0003930220
> 0.0014243068
> -0.0028740447
> 0.0012842983
> -0.0001965414
> 0.0031186304
> -0.0001335643
> 0.0000513757
> 0.0002465190
> 0.0001745330
> -0.0007293975
> 0.0017341634
> -0.0003227070
> 0.0012945009
> 0.0042097711
> -0.0021506299
> 0.0039128858
> -0.0014132766
> -0.0012860343
> -0.0001825811
> 0.0042303056
> 0.0020048020
> -0.0016583125
> -0.0024867810
> -0.0030701626
> -0.0010944863
> 0.0033814397
> -0.0011090121
> -0.0015187180
> 0.0023393556
> 0.0027394008
> -0.0004125788
> -0.0007557330
> -0.0003985487
> -0.0019830371
> -0.0001369206
> 0.0018220686
> -0.0002980935
> -0.0017877958
> 0.0012264205
> 0.0014499989
> -0.0018550441
> 0.0022734809
> -0.0001663181
> 0.0010321589
> -0.0008708797
> -0.0020962515
> 0.0022171716
> 0.0017294980
> -0.0001254584
> -0.0019265679
> 0.0009970473
> -0.0012189246
> 0.0022237246
> 0.0062328191
> 0.0003461302
> 0.0010170142
> -0.0041073144
> -0.0017758092
> 0.0016413812
> -0.0003487109
> 0.0134060935
> 0.0007820953
> -0.0024378250
> 0.0028090684
> 0.0014722024
> -0.0023115682
> 0.0047350981
> -0.0017324519
> -0.0019609177
> 0.0008565874
> 0.0095713406
> 0.0011553390
> -0.0011036118
> 0.0025878844
> -0.0007672553
> 0.0045879638
> 0.0037965748
> -0.0002894546
> -0.0003356413
> -0.0047682710
> -0.0053434738
> 0.0023476196
> 0.0038460414
> 0.0025221633
> -0.0018404295
> 0.0032087517
> -0.0003589401
> -0.0006311651
> -0.0004705149
> -0.0001292502
> 0.0014104374
> 0.0008321273
> -0.0029495048
> 0.0018411819
> -0.0017440828
> 0.0008729169
> -0.0018124527
> -0.0109842171
> 0.0001330341
> -0.0086313361
> 0.0007117559
> 0.0018778818
> -0.0043960443
> -0.0065551243
> 0.0081967409
> -0.0012476457
> 0.0004616097
> -0.0019762154
> 0.0027331643
> 0.0006297845
> 0.0017874811
> 0.0022935983
> 0.0028345839
> 0.0003857538
> -0.0020517587
> 0.0015611731
> 0.0041264571
> -0.0045508080
> 0.0026774187
> 0.0018403439
> 0.0007594088
> -0.0026471653
> 0.0022180974
> -0.0008594096
> 0.0002646149
> 0.0028485530
> -0.0014242952
> 0.0023431397
> 0.0000655574
> -0.0001685962
> -0.0005765314
> -0.0026251388
> -0.0001982347
> 0.0026357185
> -0.0003004166
> 0.0025936211
> -0.0005324412
> 0.0023118427
> 0.0028956934
> -0.0032770451
> -0.0023980828
> 0.0003320420
> -0.0012499847
> 0.0008758348
> 0.0022261150
> -0.0030691327
> 0.0011189845
> -0.0090432070
> 0.0023806149
> -0.0062025019
> 0.0004284574
> 0.0011532883
> -0.0019190319
> 0.0069431980
> -0.0011875711
> -0.0008141729
> 0.0023953007
> 0.0037516504
> -0.0041594387
> 0.0001897166
> -0.0003083316
> -0.0029950641
> -0.0003824151
> 0.0021038875
> -0.0006239009
> -0.0021452347
> 0.0052368863
> -0.0009901612
> -0.0038302275
> -0.0002105926
> 0.0010089590
> -0.0001385332
> -0.0007364005
> 0.0030379539
> -0.0017142850
> -0.0019462791
> 0.0006369676
> 0.0002153040
> 0.0010940075
> 0.0003577012
> -0.0024094334
> 0.0014166835
> 0.0010308873
> -0.0023325323
> 0.0007804982
> -0.0020668283
> -0.0020815414
> 0.0040431083
> -0.0052764949
> 0.0005905279
> -0.0002661222
> -0.0003631569
> 0.0021792202
> -0.0024699645
> 0.0006442210
> 0.0007689069
> -0.0018690191
> -0.0003689430
> 0.0021993075
> -0.0005076613
> 0.0019741241
> -0.0016839593
> 0.0015009220
> 0.0030916237
> -0.0021916309
> 0.0025598129
> 0.0059242960
> -0.0030754847
> 0.0034996097
> 0.0041952994
> -0.0002099720
> 0.0014258120
> -0.0021638521
> -0.0000046751
> 0.0011299180
> -0.0008121233
> 0.0017204605
> -0.0019634590
> -0.0021837312
> 0.0052207679
> -0.0002693069
> -0.0042895590
> 0.0010915753
> 0.0028076567
> -0.0018168420
> -0.0011218462
> 0.0015138088
> -0.0017104302
> -0.0025359511
> -0.0027498747
> 0.0007009334
> -0.0014599741
> 0.0021030792
> -0.0015147071
> -0.0013249600
> 0.0024141894
> -0.0015494203
> 0.0011045871
> -0.0026738466
> 0.0021100523
> -0.0013104480
> 0.0017700666
> 0.0038238738
> -0.0029486279
> 0.0008027278
> -0.0007649190
> 0.0023565366
> 0.0034503853
> -0.0021831524
> -0.0002953571
> 0.0022032986
> -0.0011914808
> -0.0003182811
> -0.0001685962
> 0.0021773408
> -0.0033123840
> 0.0013738685
> -0.0046265316
> 0.0029519897
> 0.0011592645
> -0.0002813183
> -0.0000515948
> 0.0007873259
> -0.0009796878
> 0.0021299888
> 0.0017662702
> -0.0011642791
> 0.0022280267
> -0.0003759623
> -0.0006133776
> -0.0002884009
> 0.0016950953
> -0.0003245744
> -0.0015147932
> 0.0028763787
> 0.0002172732
> -0.0001802829
> -0.0001479803
> 0.0008317342
> 0.0006043177
> 0.0029586773
> 0.0032205251
> -0.0023697383
> -0.0024700495
> 0.0018252511
> -0.0013796960
> -0.0000734622
> 0.0006423729
> -0.0003898988
> 0.0013471053
> -0.0003478310
> -0.0006964989
> 0.0019994314
> 0.0017129978
> 0.0000727439
> 0.0021630989
> -0.0024768938
> 0.0027572618
> 0.0033099986
> -0.0017216539
> 0.0045655969
> -0.0036746992
> -0.0032846564
> -0.0018654179
> 0.0021144412
> -0.0037869539
> -0.0002786179
> -0.0014232799
> 0.0013227509
> -0.0005945106
> -0.0018895360
> -0.0002114787
> 0.0002987986
> 0.0006612224
> 0.0012142082
> -0.0005081797
> -0.0013350848
> -0.0016158400
> 0.0001752247
> 0.0002719194
> 0.0025679082
> 0.0009973374
> -0.0018500911
> -0.0000780234
> 0.0023757137
> -0.0016602593
> 0.0052743434
> 0.0001719978
> 0.0003483132
> -0.0013223575
> -0.0024926061
> 0.0022113126
> 0.0038226656
> -0.0019749095
> -0.0004159649
> -0.0007968708
> 0.0013212829
> 0.0025362504
> -0.0010883677
> 0.0025278335
> -0.0004255133
> -0.0017512260
> -0.0015460389
> 0.0005279848
> 0.0027692801
> 0.0001433774
> 0.0016989974
> -0.0002454965
> 0.0051088575
> 0.0010841502
> -0.0024542212
> 0.0008181430
> -0.0028013377
> 0.0001333842
> 0.0012518130
> 0.0007528995
> 0.0000353984
> -0.0022446502
> 0.0014918121
> 0.0013895170
> 0.0010678763
> -0.0014568469
> 0.0000353754
> 0.0004286971
> 0.0002340579
> 0.0016216876
> -0.0000703824
> -0.0028601147
> -0.0012861161
> 0.0001465433
> -0.0003242379
> -0.0010944232
> 0.0015252071
> 0.0003549644
> -0.0009635156
> 0.0003110451
> 0.0006835161
> 0.0044124339
> 0.0003598386
> -0.0001052878
> 0.0037917681
> -0.0015336873
> 0.0036596723
> 0.0015725933
> 0.0101443419
> 0.0013586502
> 0.0023664137
> 0.0158470726
> -0.0132567483
> -0.0012765733
> -0.0052537277
> 0.0012080288
> 0.0110393067
> -0.0079441158
> -0.0014296741
> -0.0061100649
> -0.0041045143
> -0.0018731100
> -0.0029635943
> 0.0012855554
> 0.0027058128
> 0.0024670292
> 0.0003330021
> 0.0034219186
> -0.0007797791
> -0.0023818925
> 0.0036821574
> -0.0010585531
> 0.0039416367
> -0.0022154062
> -0.0004182347
> 0.0017926122
> -0.0053149589
> -0.0084503792
> 0.0046886741
> -0.0027292650
> 0.0002594352
> 0.0057113757
> -0.0026917353
> 0.0006948576
> -0.0020922234
> -0.0001076761
> -0.0007027037
> -0.0003668921
> -0.0007563435
> 0.0014639518
> -0.0042935245
> 0.0012174132
> -0.0002258331
> 0.0027411302
> 0.0011036824
> -0.0012504764
> 0.0012892268
> -0.0031066690
> 0.0034380584
> -0.0004002814
> -0.0005472174
> -0.0046162089
> 0.0006226882
> -0.0005747574
> 0.0028705391
> -0.0008014934
> 0.0016576724
> 0.0014232247
> -0.0030028490
> 0.0018948608
> 0.0018049590
> -0.0001848894
> -0.0008178968
> -0.0013032038
> 0.0005959889
> 0.0000388404
> 0.0004830575
> -0.0022343257
> -0.0022458802
> -0.0001916761
> -0.0066695947
> -0.0003009817
> 0.0002257558
> -0.0027480200
> 0.0004896118
> 0.0004135143
> 0.0017298174
> 0.0032106731
> -0.0010778672
> 0.0000528558
> -0.0014470467
> -0.0010042927
> 0.0029575487
> -0.0003696948
> 0.0002200943
> 0.0005233763
> -0.0069563040
> -0.0021043176
> -0.0034108782
> -0.0024177557
> -0.0054240313
> 0.0011361807
> 0.0017467622
> -0.0053773023
> -0.0004495826
> 0.0057536713
> -0.0011134528
> 0.0030813477
> -0.0060784466
> 0.0024553784
> -0.0008276570
> 0.0013923168
> -0.0060333122
> 0.0007529589
> 0.0024496840
> -0.0011744944
> -0.0001528252
> -0.0014057327
> 0.0011416352
> -0.0020673539
> -0.0040469413
> -0.0022006582
> 0.0051845459
> -0.0031343269
> -0.0001269736
> 0.0002257052
> 0.0000000000
> -0.0002633342
> -0.0020699028
> 0.0013543326
> 0.0019040096
> -0.0006055568
> 0.0041980117
> -0.0023464466
> 0.0001870608
> -0.0037094530
> -0.0015258226
> -0.0004166349
> 0.0017160487
> -0.0023702138
> 0.0019123188
> 0.0025805529
> 0.0039214110
> -0.0027024037
> -0.0030869636
> 0.0004854059
> -0.0025461639
> -0.0033195977
> 0.0028313361
> -0.0015204409
> 0.0014967249
> -0.0026978298
> 0.0002242677
> 0.0002241520
> 0.0005574934
> -0.0028953073
> 0.0004072731
> 0.0043459355
> -0.0036090239
> 0.0016415862
> -0.0049531271
> -0.0021152864
> 0.0017489884
> 0.0001880073
> -0.0025280242
> -0.0000387841
> 0.0031014610
> -0.0026459666
> 0.0015856533
> 0.0008725374
> -0.0012925584
> -0.0055024559
> 0.0009186767
> -0.0005715012
> -0.0033808481
> 0.0011953811
> -0.0017671780
> -0.0013536206
> 0.0021716497
> -0.0027212173
> 0.0009254474
> 0.0001186313
> -0.0001581823
> 0.0001581823
> -0.0016687561
> -0.0003375012
> -0.0005514851
> -0.0006716703
> 0.0037381712
> 0.0161588870
> 0.0024851313
> -0.0066140638
> 0.0069685996
> 0.0007742447
> 0.0013657219
> 0.0056571032
> 0.0012235353
> -0.0072993172
> -0.0009045940
> -0.0021275546
> 0.0048966306
> 0.0028772465
> -0.0038202281
> 0.0003145548
> 0.0054843663
> -0.0014018046
> 0.0045095120
> -0.0025658379
> 0.0004672246
> 0.0034997200
> 0.0016845205
> -0.0052212301
> -0.0053362546
> 0.0037195105
> 0.0010754114
> -0.0010707703
> 0.0015381144
> -0.0010788895
> -0.0000788223
> -0.0046104030
> 0.0048421923
> 0.0003011414
> -0.0039826309
> -0.0106603036
> 0.0021931336
> 0.0000000000
> -0.0028834641
> 0.0008340113
> 0.0000478831
> 0.0006506859
> 0.0021080214
> 0.0017852296
> 0.0053916391
> -0.0030054126
> 0.0006498032
> -0.0039752092
> 0.0012991340
> -0.0019263839
> -0.0000760919
> -0.0013719388
> -0.0012614292
> 0.0016858593
> 0.0004240157
> 0.0002285109
> -0.0021565907
> 0.0001578131
> 0.0002676729
> 0.0009641658
> -0.0001907540
> -0.0010410847
> 0.0012318387
> -0.0021220810
> 0.0027843006
> -0.0023293790
> 0.0001531295
> -0.0008764389
> 0.0016173969
> -0.0011669833
> 0.0014773318
> 0.0039624557
> -0.0012124524
> -0.0020108353
> -0.0005816913
> -0.0001956615
> 0.0022424279
> -0.0039690190
> 0.0021178259
> 0.0013380279
> -0.0017722247
> 0.0035467168
> -0.0024596569
> -0.0012398476
> 0.0037421170
> -0.0012469585
> -0.0021466899
> -0.0009314656
> 0.0023653559
> 0.0015808339
> 0.0005304014
> 0.0026846686
> -0.0003905801
> -0.0000800417
> -0.0003485888
> -0.0016383413
> 0.0017137355
> 0.0000800921
> -0.0014013850
> 0.0029344174
> -0.0006906055
> -0.0001928139
> -0.0014323212
> 0.0014934666
> -0.0000376269
> 0.0003855214
> -0.0028288293
> -0.0001939776
> -0.0000378594
> -0.0023347181
> 0.0009410914
> 0.0011948108
> -0.0003884270
> -0.0009250961
> 0.0002326455
> -0.0025564310
> 0.0018200849
> -0.0003091623
> -0.0001237266
> -0.0016881012
> 0.0007256384
> 0.0001192324
> -0.0006155908
> 0.0000000000
> 0.0005774400
> 0.0005766733
> -0.0001238487
> 0.0032984850
> -0.0029556059
> 0.0020895108
> 0.0005302742
> 0.0000378520
> -0.0023150283
> 0.0001902214
> 0.0007410684
> -0.0006127160
> 0.0027858619
> -0.0016371350
> -0.0013912022
> 0.0011398812
> 0.0003413818
> -0.0014051777
> -0.0014860716
> 0.0004530284
> 0.0007571708
> -0.0009860103
> -0.0006776904
> -0.0001098663
> -0.0025442383
> 0.0036321222
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From news at aspden.com  Mon Jan  8 15:54:12 2007
From: news at aspden.com (John Lawrence Aspden)
Date: Mon, 08 Jan 2007 14:54:12 +0000
Subject: [R] scripts with littler
References: <ent9nm$rc6$1@sea.gmane.org>
	<20070108132920.GA4640@phenix.progiciels-bpi.ca>
Message-ID: <entlul$ng$1@sea.gmane.org>

Thanks, that's a really neat mechanism, ( I especially like the note to vim,
which will save all my scripts having to end .R )

Is there any way to get at the command line and stdio though?

With littler I can do things like:

#!/usr/bin/env r

print(argv)
t=read.table(file=stdin())

so that I can write unix-style filters.

Cheers, John.



Fran?ois Pinard wrote:


> I usually do something like:
> 
> 
> #!/bin/sh
> R --slave --vanilla <<EOF
> 
>    R script goes here...
> 
> EOF
> 
> # vim: ft=r
> 
> 
> If you need to search special places for packages, you may tweak
> exported environment variables between the first and second line.


-- 
Contractor in Cambridge UK -- http://www.aspden.com


From news at aspden.com  Mon Jan  8 16:02:27 2007
From: news at aspden.com (John Lawrence Aspden)
Date: Mon, 08 Jan 2007 15:02:27 +0000
Subject: [R] scripts with littler / subroutines
References: <entapk$uo3$1@sea.gmane.org>
	<20070108133201.GB4640@phenix.progiciels-bpi.ca>
Message-ID: <entme4$ah2$1@sea.gmane.org>

Fran?ois Pinard wrote:

> [John Lawrence Aspden]
> 
>>Another difficulty I'm having is creating a common function (foo, say) to
>>share between two scripts.
> 
> In your previous message, you were telling us that you want to load from
> your home directory.  You might put the common functions there, maybe?
> 

I am doing at the moment, and using source to load them in. I'm worried
about what happens when I come to distribute the code though.

After a couple of e-mails off-list (thanks Dirk!) it sounds as though the
solution is to create a library for R with the common subroutines in, and
then load that from the scripts.

Cheers, John.





-- 
Contractor in Cambridge UK -- http://www.aspden.com


From ggrothendieck at gmail.com  Mon Jan  8 16:03:52 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 8 Jan 2007 10:03:52 -0500
Subject: [R] scripts with littler
In-Reply-To: <entlul$ng$1@sea.gmane.org>
References: <ent9nm$rc6$1@sea.gmane.org>
	<20070108132920.GA4640@phenix.progiciels-bpi.ca>
	<entlul$ng$1@sea.gmane.org>
Message-ID: <971536df0701080703q48e04a5cv96720798b8b30c11@mail.gmail.com>

Looks like it will be possible to write scripts with R 2.5.0 using the
new -f flag and file("stdin").  From https://svn.r-project.org/R/trunk/NEWS :

    o	Command-line R (and Rterm.exe under Windows) accepts the options
	'-f filename', '--file=filename' and '-e expression' to follow
	other script interpreters.  These imply --no-save unless
	--save is specified.

[..]

    o	file("stdin") is now recognized, and refers to the process's
	'stdin' file stream whereas stdin() refers to the console.
	These may differ, for example for a GUI console, an embedded
	application of R or if --file= has been used.



On 1/8/07, John Lawrence Aspden <news at aspden.com> wrote:
> Thanks, that's a really neat mechanism, ( I especially like the note to vim,
> which will save all my scripts having to end .R )
>
> Is there any way to get at the command line and stdio though?
>
> With littler I can do things like:
>
> #!/usr/bin/env r
>
> print(argv)
> t=read.table(file=stdin())
>
> so that I can write unix-style filters.
>
> Cheers, John.
>
>
>
> Fran?ois Pinard wrote:
>
>
> > I usually do something like:
> >
> >
> > #!/bin/sh
> > R --slave --vanilla <<EOF
> >
> >    R script goes here...
> >
> > EOF
> >
> > # vim: ft=r
> >
> >
> > If you need to search special places for packages, you may tweak
> > exported environment variables between the first and second line.
>
>
> --
> Contractor in Cambridge UK -- http://www.aspden.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bxc at steno.dk  Mon Jan  8 16:05:59 2007
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Mon, 8 Jan 2007 16:05:59 +0100
Subject: [R] Course announcement: STATISTICAL PRACTICE IN EPIDEMIOLOGY USING
	R
Message-ID: <40D3930AC1C8EA469E39536E5BC80835039A70CD@EXDKBA021.corp.novocorp.net>

Course in

STATISTICAL PRACTICE IN EPIDEMIOLOGY USING R 
============================================
Tartu, Estonia, 25 to 30 May 2007

The course is aimed at epidemiologists and statisticians who wish to use R for statistical modelling and analysis of epidemiological data.
The course requires basic knowledge of epidemiological concepts and study types. These will only be briefly reviewed, whereas the more advanced epidemiological and statistical concepts will be treated in depth.

Contents:
---------
    * History of R. Language. Objects. Functions.
    * Interface to other dataformats. Dataframes.
    * Graphical methods for exploring data.
    * Classical methods and tabulation of data.
    * Logistic regression for case-control-studies.
    * Poisson regression for follow-up studies.
    * Parametrization of models.
    * Graphics in R.
    * Graphical reporting of results.
    * Causal inference.
    * Time-splitting & SMR.
    * Survival analysis in continuous time.
    * Interval censoring.
    * Nested and matched case-control studies.
    * Case-cohort studies.
    * Competing risk models.
    * Multistage models.
    * Bootstrap and simulation

The methods will be illustrated using R in practical exercises.

The Epi package which is under development for epidemiological analysis in R will be introduced.

Participants are required to have a fairly good understanding of statistical principles and some familiarity with epidemiological concepts. The course will be mainly practically oriented with more than half the time at the computer.

Price: 600 EUR. (300 EUR for countries outside EU-2003 and the like).

Application: Deadline 1st April 2007. Send and e-mail which briefly states your qualifications in epidemiology and statistics, to the organizers Krista Fischer, Esa L??r? & Bendix Carstensen. 

Further information at: www.pubhealth.ku.dk/~bxc/SPE

-------------------------------------------------------------------
Organizers:
Krista Fischer, University of Tartu, Estonia <krista.fischer at ut.ee> 
Esa L??r?, University of Oulu, Finland <esa.laara at oulu.fi> 
Bendix Carstensen, Steno Diabetes Center, Denmark <bxc at steno.dk>


From tlumley at u.washington.edu  Mon Jan  8 16:44:29 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 8 Jan 2007 07:44:29 -0800 (PST)
Subject: [R] memory problem
In-Reply-To: <c1055ec10701061225o18e66bc9r7b315970dc7d4c36@mail.gmail.com>
References: <c1055ec10701061225o18e66bc9r7b315970dc7d4c36@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701080744001.7194@homer21.u.washington.edu>

On Sat, 6 Jan 2007, Zoltan Kmetty wrote:

> Hi!
>
> I had some memory problem with R - hope somebody could tell me a solution.
>
> I work with very large datasets, but R cannot allocate enough memoty to
> handle these datasets.

You haven't said what you want to do with these datasets.

 	-thomas


From fararooei at yahoo.com  Mon Jan  8 16:49:38 2007
From: fararooei at yahoo.com (mohammad frarouei)
Date: Mon, 8 Jan 2007 07:49:38 -0800 (PST)
Subject: [R] limitation in the number of covariates in nlme
Message-ID: <502218.41061.qm@web56808.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070108/423a6aa6/attachment.pl 

From edzabc at gmail.com  Mon Jan  8 17:02:12 2007
From: edzabc at gmail.com (Ed Zhang)
Date: Mon, 8 Jan 2007 11:02:12 -0500
Subject: [R] fSeries Package
Message-ID: <5b42b3f80701080802u152a425cp6ae739f1374a409d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070108/6975cc37/attachment.pl 

From georg.hoermann at gmx.de  Mon Jan  8 17:12:34 2007
From: georg.hoermann at gmx.de (Georg Hoermann)
Date: Mon, 08 Jan 2007 17:12:34 +0100
Subject: [R] Simple spectral analysis
Message-ID: <45A26D72.1040404@gmx.de>

Hello world,

I am actually trying to transfer a lecture from Statistica to
R and I ran into problems with spectral analysis, I think I
just don't get it 8-(
(The posting from "FFT, frequs, magnitudes, phases" from 2005
did not enlighten me)

As a starter for the students I have a 10year data set of air 
temperature with daily values  and I try to
get a periodogram where the annual period (365 days) should be clearly
visible (in statistica I can get the frequencies and the period).
I tried the spectrum() and pgram() functions, but
did not find a way through... The final aim would be to
get the periodogram (and the residuals from the reassembled data set...)

Thanks and greetings,
Georg

The data set:

air = read.csv("http://www.hydrology.uni-kiel.de/~schorsch/air_temp.csv")
airtemp = ts(T_air, start=c(1989,1), freq = 365)
plot(airtemp)


-- 
Georg Hoermann, Dep. of Hydrology, Ecology, Kiel University, Germany
+49/431/23761412, mo: +49/171/4995884, icq:348340729, skype: ghoermann


From edzabc at gmail.com  Mon Jan  8 17:18:07 2007
From: edzabc at gmail.com (ED)
Date: Mon, 8 Jan 2007 11:18:07 -0500
Subject: [R] fSeries Package
Message-ID: <5b42b3f80701080818i4c287269u9a413c0d576f1c2b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070108/f907a5e8/attachment.pl 

From megh700004 at yahoo.com  Mon Jan  8 17:32:43 2007
From: megh700004 at yahoo.com (Megh Dal)
Date: Mon, 8 Jan 2007 08:32:43 -0800 (PST)
Subject: [R] OLS in S+
Message-ID: <306431.5316.qm@web58107.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070108/7f397fd0/attachment.pl 

From ripley at stats.ox.ac.uk  Mon Jan  8 17:37:08 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Jan 2007 16:37:08 +0000 (GMT)
Subject: [R] limitation in the number of covariates in nlme
In-Reply-To: <502218.41061.qm@web56808.mail.re3.yahoo.com>
References: <502218.41061.qm@web56808.mail.re3.yahoo.com>
Message-ID: <Pine.LNX.4.64.0701081633480.27068@gannet.stats.ox.ac.uk>

Please check you have the latest version of nlme (3.1-79), as some 
restrictions of this sort were lifted a few weeks ago.

If you have, please note the advice about a minimal reproducible example 
in the footer of every help message as we will need one to be able to 
help you.


On Mon, 8 Jan 2007, mohammad frarouei wrote:

> Dear All
>
>  I am fitting a nlme model in which I have 7 covariates. Adding one more 
> variable to the model, R gives me an error message:
>  ?Error in parse(file, n, text, prompt) : syntax error in "list(
..?
>  This does not depend on which variables are in the model and seems to 
> depend strictly on the number of covariates.
>
>  Any suggestion would be appreciated.
>
>  M.Fararooei
>  PhD candidate
>
> __________________________________________________
>
>
>
> 	[[alternative HTML version deleted]]
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From dray at biomserv.univ-lyon1.fr  Mon Jan  8 18:01:53 2007
From: dray at biomserv.univ-lyon1.fr (=?ISO-8859-1?Q?St=E9phane_Dray?=)
Date: Mon, 08 Jan 2007 18:01:53 +0100
Subject: [R] Cross-compilation of R and ld bug ?
Message-ID: <45A27901.2030000@biomserv.univ-lyon1.fr>

Hello list,

I would like to cross-compile R packages using R 2.4.0. I am working on 
Linux Debian and cross-compiled (windows binaries) without problems with 
older R version.
I have used the doc of Yan and Rossini in the contributed section of the 
R documentation (same version of MinGW...).
When I try to cross-compile R (make R), the procedure stopped and returns :
************************************
i586-mingw32-windres  --include-dir ../include -i dllversion.rc -o 
dllversion.o
i586-mingw32-gcc  -shared -s -mwindows -o R.dll R.def console.o 
dataentry.o dynl oad.o edit.o editor.o embeddedR.o extra.o opt.o pager.o 
preferences.o psignal.o rhome.o rui.o run.o shext.o sys-win32.o system.o 
e_pow.o malloc.o ../main/libmai n.a ../appl/libappl.a 
../nmath/libnmath.a graphapp/ga.a getline/gl.a ../extra/xd r/libxdr.a 
../extra/zlib/libz.a ../extra/pcre/libpcre.a ../extra/bzip2/libbz2.a 
../extra/intl/libintl.a ../extra/trio/libtrio.a dllversion.o -L. -lg2c 
-lRblas - lcomctl32 -lversion
../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1703): 
undefin ed reference to `__pcre_ucp_findprop'
../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1740): 
undefin ed reference to `__pcre_ucp_findprop'
../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1848): 
undefin ed reference to `__pcre_ucp_findprop'
../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x187f): 
undefin ed reference to `__pcre_ucp_findprop'
../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1966): 
undefin ed reference to `__pcre_ucp_findprop'
../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1f2a): 
more un defined references to `__pcre_ucp_findprop' follow
collect2: ld returned 1 exit status
make[4]: *** [R.dll] Erreur 1
make[3]: *** [../../bin/R.dll] Erreur 2
make[2]: *** [rbuild] Erreur 2
make[1]: *** [all] Erreur 2
make[1]: quittant le r?pertoire ? 
/home/stephane/Rdev/CrossCompileBuild/WinR/R-2 .4.0/src/gnuwin32 ?
****************************************
I have read (http://www.murdoch-sutherland.com/Rtools/) that a bug 
exists for ld version 2.16.91 20050827 and that Prof Ripley produced a 
patched version. It seems that my problem is also related to ld (and I 
have the same version). So I wonder if the same bug could be responsible 
of the error in the cross-compiler.
Two questions:
- Are they other people which have the same problem when cross-compiling 
R on Linux
- Is it possible that the problem is related to ld. If yes, is it 
possible to obtain a patched version ?

Thanks in advance.
Sincerely,

-- 
St?phane DRAY (dray at biomserv.univ-lyon1.fr )
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - Lyon I
43, Bd du 11 Novembre 1918, 69622 Villeurbanne Cedex, France
Tel: 33 4 72 43 27 57       Fax: 33 4 72 43 13 88
http://biomserv.univ-lyon1.fr/~dray/


From gtg757i at mail.gatech.edu  Mon Jan  8 17:53:06 2007
From: gtg757i at mail.gatech.edu (Tudor Bodea)
Date: Mon, 08 Jan 2007 11:53:06 -0500
Subject: [R] Access, Process and Read Information from Web Sites
Message-ID: <1168275186.45a276f21bd10@webmail.mail.gatech.edu>

Dear R useRs,

Does any of you know if it is possible to access a web site (e.g.,
www.marriott.com), fill in the requested information (e.g., city, check-in
date, etc), and save the results (e.g., room availability and room rates) in
text files through R? I started with url and url.show but it seems that this
does not do what I would like to do. Any lead would be greatly appreciated.
Thanks.

Tudor

--
Tudor Dan Bodea
Ph.D. Candidate
Georgia Institute of Technology
School of Civil and Environmental Engineering


From f.harrell at vanderbilt.edu  Mon Jan  8 18:06:23 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 08 Jan 2007 11:06:23 -0600
Subject: [R] OLS in S+
In-Reply-To: <306431.5316.qm@web58107.mail.re3.yahoo.com>
References: <306431.5316.qm@web58107.mail.re3.yahoo.com>
Message-ID: <45A27A0F.9060901@vanderbilt.edu>

Megh Dal wrote:
> Dear all,
> 
> Is there any equivalent of the function OLS in S+ in R?
> 
> Thanks and regards,
> megh
> 

If you are asking about the ols function in the Design library, it's in 
the Design package in R.  ols is a wrapper for lm that makes certain 
plots and penalized estimations easier to do.

Frank

> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From antoniababe at yahoo.se  Mon Jan  8 18:25:10 2007
From: antoniababe at yahoo.se (antoniababe at yahoo.se)
Date: Mon, 8 Jan 2007 18:25:10 +0100 (CET)
Subject: [R] Boxplot issue
Message-ID: <20070108172510.46495.qmail@web26008.mail.ukl.yahoo.com>

Dear R-users,

I have a data frame containing 2 colums: column 1 is
the patient numbers (totally 36 patients), column 2 is
patient's response values (each patient has 100
response values). If I produce a boxplot for each
patient on the same graph in order to compare them
against each other then the boxplots are very small. 

How can I instead of creating one graph containing 36
boxplots, create four different graphs where three of
them have 10 boxplots each representing data of 10
patients and the fourth graph has boxplots of
remaining patients ?

Thanks alot for any suggestion,
Greetings,
Antonia


From ripley at stats.ox.ac.uk  Mon Jan  8 18:39:26 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Jan 2007 17:39:26 +0000 (GMT)
Subject: [R] Cross-compilation of R and ld bug ?
In-Reply-To: <45A27901.2030000@biomserv.univ-lyon1.fr>
References: <45A27901.2030000@biomserv.univ-lyon1.fr>
Message-ID: <Pine.LNX.4.64.0701081731100.319@gannet.stats.ox.ac.uk>

There are two problems here:

1) your binutils is not current.
2) your R is not current.

Updating either will solve this, but please update both.
Almost up-to-date cross-compilers are (as ever) available from

http://www.stats.ox.ac.uk/pub/Rtools/

(If you want to compile R-devel you will need to update the mingw, which 
can be done from the i386 distribution.  I will rebuild the 
cross-compilers in due course.)

On Mon, 8 Jan 2007, St?phane Dray wrote:

> Hello list,
>
> I would like to cross-compile R packages using R 2.4.0. I am working on
> Linux Debian and cross-compiled (windows binaries) without problems with
> older R version.
> I have used the doc of Yan and Rossini in the contributed section of the
> R documentation (same version of MinGW...).
> When I try to cross-compile R (make R), the procedure stopped and returns :
> ************************************
> i586-mingw32-windres  --include-dir ../include -i dllversion.rc -o
> dllversion.o
> i586-mingw32-gcc  -shared -s -mwindows -o R.dll R.def console.o
> dataentry.o dynl oad.o edit.o editor.o embeddedR.o extra.o opt.o pager.o
> preferences.o psignal.o rhome.o rui.o run.o shext.o sys-win32.o system.o
> e_pow.o malloc.o ../main/libmai n.a ../appl/libappl.a
> ../nmath/libnmath.a graphapp/ga.a getline/gl.a ../extra/xd r/libxdr.a
> ../extra/zlib/libz.a ../extra/pcre/libpcre.a ../extra/bzip2/libbz2.a
> ../extra/intl/libintl.a ../extra/trio/libtrio.a dllversion.o -L. -lg2c
> -lRblas - lcomctl32 -lversion
> ../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1703):
> undefin ed reference to `__pcre_ucp_findprop'
> ../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1740):
> undefin ed reference to `__pcre_ucp_findprop'
> ../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1848):
> undefin ed reference to `__pcre_ucp_findprop'
> ../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x187f):
> undefin ed reference to `__pcre_ucp_findprop'
> ../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1966):
> undefin ed reference to `__pcre_ucp_findprop'
> ../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1f2a):
> more un defined references to `__pcre_ucp_findprop' follow
> collect2: ld returned 1 exit status
> make[4]: *** [R.dll] Erreur 1
> make[3]: *** [../../bin/R.dll] Erreur 2
> make[2]: *** [rbuild] Erreur 2
> make[1]: *** [all] Erreur 2
> make[1]: quittant le r?pertoire ?
> /home/stephane/Rdev/CrossCompileBuild/WinR/R-2 .4.0/src/gnuwin32 ?
> ****************************************
> I have read (http://www.murdoch-sutherland.com/Rtools/) that a bug
> exists for ld version 2.16.91 20050827 and that Prof Ripley produced a
> patched version. It seems that my problem is also related to ld (and I
> have the same version). So I wonder if the same bug could be responsible
> of the error in the cross-compiler.
> Two questions:
> - Are they other people which have the same problem when cross-compiling
> R on Linux
> - Is it possible that the problem is related to ld. If yes, is it
> possible to obtain a patched version ?
>
> Thanks in advance.
> Sincerely,
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From jholtman at gmail.com  Mon Jan  8 19:09:56 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 8 Jan 2007 13:09:56 -0500
Subject: [R] Boxplot issue
In-Reply-To: <20070108172510.46495.qmail@web26008.mail.ukl.yahoo.com>
References: <20070108172510.46495.qmail@web26008.mail.ukl.yahoo.com>
Message-ID: <644e1f320701081009i6b87d223t2a65631a8c3d99d2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070108/0f71f7fd/attachment.pl 

From maechler at stat.math.ethz.ch  Mon Jan  8 19:18:26 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 8 Jan 2007 19:18:26 +0100
Subject: [R] memory problem --- use sparse matrices
In-Reply-To: <45A0B260.7030302@statistik.uni-dortmund.de>
References: <c1055ec10701061225o18e66bc9r7b315970dc7d4c36@mail.gmail.com>
	<45A0B260.7030302@statistik.uni-dortmund.de>
Message-ID: <17826.35570.767005.207748@stat.math.ethz.ch>

>>>>> "UweL" == Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>>>>     on Sun, 07 Jan 2007 09:42:08 +0100 writes:

    UweL> Zoltan Kmetty wrote:
    >> Hi!
    >> 
    >> I had some memory problem with R - hope somebody could
    >> tell me a solution.
    >> 
    >> I work with very large datasets, but R cannot allocate
    >> enough memoty to handle these datasets.
    >> 
    >> I want work a matrix with row= 100 000 000 and column=10
    >> 
    >> A know this is 1 milliard cases, but i thought R could
    >> handle it (other commercial software like spss could do),
    >> but R wrote out everytime: not enough memory..
    >> 
    >> any good idea?

    UweL> Buy a machine that has at least 8Gb (better 16Gb) of
    UweL> RAM and proceed ...

Well, I doubt that Zoltan wants to *fill* his matrix with all
non-zeros. If he does, Uwe and Roger are right.

Otherwise, working with a *sparse* matrix, using the 'Matrix'
(my recommendation, but I am biased) or 'SparseM' package, might
well be feasible:

install.packages("Matrix") # if needed; only once for your R

library(Matrix) # each time you need it


TsparseMatrix <- function(nrow, ncol, i,j,x) 
{
    ## Purpose: User friendly construction of sparse "Matrix" from triple
    ## ----------------------------------------------------------------------
    ## Arguments: (i,j,x): 2 integer and 1 numeric vector of the same length:
    ##
    ##	The matrix M will have
    ##       M[i[k], j[k]] == x[k] , for k = 1,2,..., length(i)
    ##    and M[ i', j' ]  ==  0  for `` all other pairs (i',j')
    ## ----------------------------------------------------------------------
    ## Author: Martin Maechler, Date:  8 Jan 2007, 18:46
    nnz <- length(i)
    stopifnot(length(j) == nnz, length(x) == nnz,
              is.numeric(x), is.numeric(i), is.numeric(j))
    dim <- c(as.integer(nrow), as.integer(ncol))
    ## The conformability of (i,j) with 'dim' will be checked automatically
    ## by an internal "validObject()" that is part of new(.):
    new("dgTMatrix", x = x, Dim = dim,
        ## our "Tsparse" Matrices use  0-based indices :
        i = as.integer(i - 1:1),
        j = as.integer(j - 1:1))
}

For example :

> TsparseMatrix(10,20, c(1,3:8), c(2,9,6:10), 7 * (1:7))
10 x 20 sparse Matrix of class "dgTMatrix"
                                                  
 [1,] . 7 . . .  .  .  .  .  . . . . . . . . . . .
 [2,] . . . . .  .  .  .  .  . . . . . . . . . . .
 [3,] . . . . .  .  .  . 14  . . . . . . . . . . .
 [4,] . . . . . 21  .  .  .  . . . . . . . . . . .
 [5,] . . . . .  . 28  .  .  . . . . . . . . . . .
 [6,] . . . . .  .  . 35  .  . . . . . . . . . . .
 [7,] . . . . .  .  .  . 42  . . . . . . . . . . .
 [8,] . . . . .  .  .  .  . 49 . . . . . . . . . .
 [9,] . . . . .  .  .  .  .  . . . . . . . . . . .
[10,] . . . . .  .  .  .  .  . . . . . . . . . . .

But

nr <- 1e8
nc <- 10
set.seed(1)
i <- sample(nr, 10000)
j <- sample(nc, 10000)
x <- round(rnorm(10000), 2)

M <- TsparseMatrix(nr, nc, i=i, j=j, x=x)

works,
e.g. you can

x <- 1:10
system.time(y <- M %*% x)  # needs around 4 sec on one of our better machines
y <- as.vector(y)

## but you can become even more efficient, translating from the
## so-called "triplet" to the (recommended) "Csparse"
## representation:
M. <- as(M, "CsparseMatrix")

object.size(M) / object.size(M.)
## 1.328921; i.e. we saved 33%

## and

system.time(y. <- M. %*% x) # much faster (1 sec)

identical(as.vector(y.), y)


--- --- ---

I hope this is useful to you.

Martin Maechler,
ETH Zurich


From Greg.Snow at intermountainmail.org  Mon Jan  8 19:25:50 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 8 Jan 2007 11:25:50 -0700
Subject: [R] Plot .jpeg image in margins?
Message-ID: <07E228A5BE53C24CAD490193A7381BBB77C75C@LP-EXCHVS07.CO.IHC.COM>

Here is one example of a way to do it:

library(rimage) # for the read.jpeg function
library(TeachingDemos) # for the subplot function

par(xpd=NA,mar=c(5,4,4,8)+0.1)

plot(1:10,10:1)

x <- read.jpeg(system.file("data", "cat.jpg", package="rimage"))

subplot( plot(x), 12, 5 )


Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kari
> Sent: Sunday, January 07, 2007 10:10 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Plot .jpeg image in margins?
> 
> Is it possible to plot an image (currently a jpeg) in the margins?
> 
> 
> Thanks,
> Kari
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From laurentRhelp at free.fr  Mon Jan  8 19:42:09 2007
From: laurentRhelp at free.fr (Laurent Rhelp)
Date: Mon, 08 Jan 2007 19:42:09 +0100
Subject: [R] odfWeave and figures in MS Word Format
Message-ID: <45A29081.2060004@free.fr>

I answer to myself.
I understood my error : first of all, we have to save the file in the 
.rtf format !
Then, from the rtf file, we can generate the file in the .doc format.

I am sorry for my question.

Thanks

Laurent


From Greg.Snow at intermountainmail.org  Mon Jan  8 19:36:05 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 8 Jan 2007 11:36:05 -0700
Subject: [R] query
Message-ID: <07E228A5BE53C24CAD490193A7381BBB77C764@LP-EXCHVS07.CO.IHC.COM>

Look at :

> myccf * c(17,18,19,18,17)/19

Do those numbers match with the result of ccf?

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Fluss
> Sent: Monday, January 08, 2007 6:18 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] query
> 
> Hello!
> I found the ccf function gives different estimates than the 
> simple lag correlations.
> Why is that?
> This is my code:
> 
> set.seed(20)
> x<-rnorm(20)
> y<-x+rnorm(20,sd=0.3)
> print("R CCF:")
> print(ccf(x,y,lag.max=2,plot=F))
> myccf<- c( cor(y[-(1:2)],x[-(19:20)]) ,  cor(y[-1],x[-20]),
>                    cor(y,x),
>                    
> cor(x[-1],y[-20]),cor(x[-(1:2)],y[-(19:20)]) ) print("My CCF:")
> print(myccf)
> 
> Thank You!
> Ron
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From liud2 at mail.nih.gov  Mon Jan  8 19:45:55 2007
From: liud2 at mail.nih.gov (Liu, Delong (NIH/CIT) [C])
Date: Mon, 8 Jan 2007 13:45:55 -0500
Subject: [R] coefficients of each local polynomial from loess() or locfit()
In-Reply-To: <mailman.17.1168081205.8010.r-help@stat.math.ethz.ch>
Message-ID: <08BFEF2D7CC3104FA411B6E1991200C21D9D7C@NIHCESMLBX3.nih.gov>

Dieter,

Thanks for your suggestions and help.  I am not an expert with R
programming. In my current application, I am not interested in point
prediction from loess().  Instead, I am more interested in obtaining the
coefficient estimates of local polynomial from loess().  Is it
straightforward to modify loess() so that the coefficient estimates can
be put into the return list of loess()?  

Delong
------------------------------------------------------------------------
--

Message: 8
Date: Fri, 5 Jan 2007 14:01:46 +0000 (UTC)
From: Dieter Menne <dieter.menne at menne-biomed.de>
Subject: Re: [R] coefficients of each local polynomial from loess() or
	locfit()
To: r-help at stat.math.ethz.ch
Message-ID: <loom.20070105T145441-714 at post.gmane.org>
Content-Type: text/plain; charset=us-ascii

Liu, Delong (NIH/CIT) [C] <liud2 <at> mail.nih.gov> writes:

>>> 
I want to extract estimated coeffiicents of each local polynomial at 
given x from loess(),  locfit(), or KernSmooth().  Can some experts 
provide me with suggestions?  Thanks.
>>

Try 

cars.lo <- loess(dist ~ speed, cars)
str(cars.lo)

List of 17
 $ n        : int 50
 $ fitted   : num [1:50]  5.89  5.89 12.57 12.57 15.37 ...
 $ residuals: Named num [1:50] -3.894  4.106 -8.568  9.432  0.631 ...
... omitted
  ..$ cell       : num 0.2
  ..$ family     : chr "gaussian"
  ..$ iterations : num 1
 $ kd       :List of 5
  ..$ parameter: Named int [1:7] 1 50 2 19 11 1049 849
  .. ..- attr(*, "names")= chr [1:7] "d" "n" "vc" "nc" ...
  ..$ a        : int [1:19] 1 1 1 1 1 1 1 0 0 0 ...
  ..$ xi       : num [1:19] 15 12 19 9 13 17 20 0 0 0 ...
  ..$ vert     : num [1:2]  3.90 25.11
  ..$ vval     : num [1:22]  5.71  1.72 96.46 10.88 41.21 ...
 $ call     : language loess(formula = dist ~ speed, data = cars)

Looks like kd holds information about the polynomials. Then, try

getAnywhere(predict.loess)

which will show you that the real work is done in function predLoess. 
Trying again

getAnywhere(predLoess)

you get an idea how the parameters are used for prediction.

fit[inside] <- .C(R_loess_ifit, as.integer(kd$parameter), 
     as.integer(kd$a), as.double(kd$xi), as.double(kd$vert), 
        as.double(kd$vval), as.integer(M1), as.double(x.evaluate[inside,

      ]), fit = double(M1))$fit

Dieter


From ecartis at unicode.org  Mon Jan  8 20:06:04 2007
From: ecartis at unicode.org (Ecartis)
Date: Mon, 08 Jan 2007 13:06:04 -0600 (CST)
Subject: [R] Ecartis command results: -- Binary/unsupported file stripped by
	Ecartis --
Message-ID: <ecartis-01082007130604.14672.1@sarasvati.unicode.org>

Request received for list 'unicode' via request address.

>> The original message was received at Mon, 8 Jan 2007 21:05:47 +0200
>> from 2.178.6.230
Unknown command.

>> ----- The following addresses had permanent fatal errors -----
Unknown command.

>> unicode-request at unicode.org
Unknown command.

---
Ecartis v1.0.0 - job execution complete.


From michela.cameletti at unibg.it  Mon Jan  8 20:19:33 2007
From: michela.cameletti at unibg.it (Michela Cameletti)
Date: Mon, 8 Jan 2007 20:19:33 +0100 (CET)
Subject: [R] spatio temporal plot
Message-ID: <3333.87.5.92.64.1168283973.squirrel@mailserver.unibg.it>

Dear R-users,
I have a matrix of data (air pollution) with n rows and T columns, where n
is the number of spatial locations and T is the number of time points
(days of the year).
I would like to use a 3d plot for plotting the n time series I have: so
x-axis and y-axis are for the spatial coordinates and the z-axis is for
the T air pollution data. But the final result should be like this: see
the attached figure. It's like doing n vertical plots of the n time series
and put them nearby using the spatial coordinates.
Can you help me please?
Thanks in advance,
best regards
Michela
-------------- next part --------------
A non-text attachment was scrubbed...
Name: timeseriesplot.PNG
Type: image/png
Size: 18947 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070108/01840503/attachment.png 

From dieter.menne at menne-biomed.de  Mon Jan  8 20:29:51 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 8 Jan 2007 19:29:51 +0000 (UTC)
Subject: [R] coefficients of each local polynomial from loess() or
	locfit()
References: <mailman.17.1168081205.8010.r-help@stat.math.ethz.ch>
	<08BFEF2D7CC3104FA411B6E1991200C21D9D7C@NIHCESMLBX3.nih.gov>
Message-ID: <loom.20070108T202605-580@post.gmane.org>

Liu, Delong (NIH/CIT) [C] <liud2 <at> mail.nih.gov> writes:

> Instead, I am more interested in obtaining the
> coefficient estimates of local polynomial from loess().  Is it
> straightforward to modify loess() so that the coefficient estimates can
> be put into the return list of loess()?  

No need to change loess.
> 
> cars.lo <- loess(dist ~ speed, cars)
> str(cars.lo)
> 
> List of 17
...
>  $ kd       :List of 5
>   ..$ parameter: Named int [1:7] 1 50 2 19 11 1049 849
>   .. ..- attr(*, "names")= chr [1:7] "d" "n" "vc" "nc" ...
>   ..$ a        : int [1:19] 1 1 1 1 1 1 1 0 0 0 ...
>   ..$ xi       : num [1:19] 15 12 19 9 13 17 20 0 0 0 ...
>   ..$ vert     : num [1:2]  3.90 25.11
>   ..$ vval     : num [1:22]  5.71  1.72 96.46 10.88 41.21 ...

As I showed you, they are in cars.lo$kd, but you must dig into the source code
to find out how they are used. And after reading Brian Ripley's warning, I would
recommend that you don't try it if you are not sure how "extract" these.

Dieter


From scionforbai at gmail.com  Mon Jan  8 20:32:23 2007
From: scionforbai at gmail.com (Scionforbai)
Date: Mon, 8 Jan 2007 20:32:23 +0100
Subject: [R] spatio temporal plot
In-Reply-To: <3333.87.5.92.64.1168283973.squirrel@mailserver.unibg.it>
References: <3333.87.5.92.64.1168283973.squirrel@mailserver.unibg.it>
Message-ID: <e9ee1f0a0701081132u7c2f397dm6f48f3840eb45e28@mail.gmail.com>

Altough I didn't test it, I think "rgl" package should do this.

Regards,

Scionforbai


From btyner at stat.purdue.edu  Mon Jan  8 20:41:09 2007
From: btyner at stat.purdue.edu (Benjamin Tyner)
Date: Mon, 08 Jan 2007 14:41:09 -0500
Subject: [R] finer control of scales in xyplot
Message-ID: <45A29E55.7060703@stat.purdue.edu>

When plotting over multiple pages in lattice, I'd like to be able to 
have "same" scales within a page, but "free" scales between pages. In 
other words, something like:

z<-data.frame(x=1:100,
              y=runif(100),
              d=rep(1:2,50),
              p=rep(1:2,each=50))

plot<-xyplot(y~x|d*p,
             data=z,
             scales=list(x=list(relation="free")),
             layout=c(1,2))
            
but within a page, to have common x-axes. As long as 'x' is sorted, I 
can get the desired effect by transforming x to a relative scale:

plot<-xyplot(y~unlist(with(z,tapply(x,p,function(x) 
(x-min(x))/diff(range(x)))))|d*p,
             data=z,
             layout=c(1,2))

except that I'd like the tickmark labels to be in the original units of 
'x'. I've started looking at xscale.components.default to see whether it 
can work on a per-page (instead of per-panel) basis, but I get the 
nagging suspicion that I'm making this harder than it needs to be. Any 
assistance greatly appreciated.

Thanks,
Ben


From efg at stowers-institute.org  Mon Jan  8 21:20:09 2007
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Mon, 8 Jan 2007 14:20:09 -0600
Subject: [R] listing all functions in R
References: <1168088147.3149.6.camel@dhcppc2.my.nat.localnet>
	<Pine.LNX.4.64.0701061331420.8651@gannet.stats.ox.ac.uk>
Message-ID: <enu91s$ep7$1@sea.gmane.org>

"Prof Brian Ripley" <ripley at stats.ox.ac.uk> wrote in message 
news:Pine.LNX.4.64.0701061331420.8651 at gannet.stats.ox.ac.uk...
> Here is a reasonable shot:
>
> findfuns <- function(x) {
>     if(require(x, character.only=TRUE)) {
>        env <- paste("package", x, sep=":")
>        nm <- ls(env, all=TRUE)
>        nm[unlist(lapply(nm, function(n) exists(n, where=env,
>                                               mode="function",
>                                               inherits=FALSE)))]
>     } else character(0)
> }
> pkgs <- dir(.Library)
> z <-  lapply(pkgs, findfuns)
> names(z) <- pkgs

Any recommendations on how to trap problems with "require" when using 
findfuns?  One bad package and the lapply above doesn't return anything.

For example:

> findfuns("bcp")
Loading required package: bcp
Loading required package: DNAcopy
Error: package 'DNAcopy' could not be loaded
In addition: Warning message:
there is no package called 'DNAcopy' in: library(pkg, character.only = TRUE, 
logical = TRUE, lib.loc = lib.loc)

> require("bcp", character.only=TRUE)
Loading required package: bcp
Loading required package: DNAcopy
Error: package 'DNAcopy' could not be loaded
In addition: Warning message:
there is no package called 'DNAcopy' in: library(pkg, character.only = TRUE, 
logical = TRUE, lib.loc = lib.loc)


I used "try" around the "require" call with "options(error=recover)" with 
"recover" defined to be a do nothing function to avoid the stop, but then 
there were other problems (e.g., "unable to load shared library ... 
LoadLibrary Failure: The specified module could not be found" and "Maximal 
number of DLLs reached")

Besides "bcp" I saw problems with other packages, e.g., cairoDevice, 
caMassClass, ... several others.

I'm using R 2.4.1with all CRAN packages installed that existed last week and 
at least several Bioconductor packages installed by the biocLite procedure.

FWIW:
> length(pkgs)
[1] 957


Thanks for any suggestions.

efg

Earl F. Glynn
Scientific Programmer
Stower Institute


From efg at stowers-institute.org  Mon Jan  8 23:07:29 2007
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Mon, 8 Jan 2007 16:07:29 -0600
Subject: [R] Simple spectral analysis
References: <45A26D72.1040404@gmx.de>
Message-ID: <enufb3$702$1@sea.gmane.org>

"Georg Hoermann" <georg.hoermann at gmx.de> wrote in message 
news:45A26D72.1040404 at gmx.de...
> The data set:
>
> air = read.csv("http://www.hydrology.uni-kiel.de/~schorsch/air_temp.csv")
> airtemp = ts(T_air, start=c(1989,1), freq = 365)
> plot(airtemp)

Maybe this will get you started using fft or spectrum -- I'm not sure why 
the spectrum answer is only close:

air = read.csv("http://www.hydrology.uni-kiel.de/~schorsch/air_temp.csv")

TempAirC <- air$T_air
Time     <- as.Date(air$Date, "%d.%m.%Y")
N <- length(Time)

oldpar <- par(mfrow=c(4,1))
plot(TempAirC ~ Time)

# Using fft
transform <- fft(TempAirC)

# Extract DC component from transform
dc <- Mod(transform[1])/N

periodogram <- round( Mod(transform)^2/N, 3)

# Drop first element, which is the mean
periodogram <- periodogram[-1]

# keep first half up to Nyquist limit
periodogram <- periodogram[1:(N/2)]

# Approximate number of data points in single cycle:
print( N / which(max(periodogram) == periodogram) )

# plot spectrum against Fourier Frequency index
plot(periodogram, col="red", type="o",
     xlab="Fourier Frequency Index", xlim=c(0,25),
     ylab="Periodogram",
     main="Periodogram derived from 'fft'")

# Using spectrum
s <- spectrum(TempAirC, taper=0, detrend=FALSE, col="red",
              main="Spectral Density")

plot(log(s$spec) ~ s$freq, col="red", type="o",
     xlab="Fourier Frequency", xlim=c(0.0, 0.005),
     ylab="Log(Periodogram)",
     main="Periodogram from 'spectrum'")

cat("Max frequency\n")
maxfreq <- s$freq[ which(max(s$spec) == s$spec) ]

# Period will be 1/frequency:
cat("Corresponding period\n")
print(1/maxfreq)

par(oldpar)



efg

Earl F. Glynn
Scientific Programmer
Stowers Institute


From drf5n at maplepark.com  Mon Jan  8 23:48:32 2007
From: drf5n at maplepark.com (David Forrest)
Date: Mon, 8 Jan 2007 16:48:32 -0600 (CST)
Subject: [R] Does strptime(...,tz="GMT") do anything?
Message-ID: <Pine.LNX.4.64.0701081633510.2367@maplepark.com>

Hi All

In trying to correlate some tide gauge data I need to deal with varying 
timezones.  From the documentation on strptime, it seemed that the tz 
variable might have some effect on the conversion, but I'm not seeing an 
effect.

> strptime("20061201 1:02 PST",format="%Y%m%d %H:%M",tz="PST")+0
[1] "2006-12-01 01:02:00 EST"
> strptime("20061201 1:02 PST",format="%Y%m%d %H:%M",tz="")+0
[1] "2006-12-01 01:02:00 EST"
> strptime("20061201 1:02 PST",format="%Y%m%d %H:%M",tz="GMT")+0
[1] "2006-12-01 01:02:00 EST"
> strptime("20061201 1:02 PST",format="%Y%m%d %H:%M",tz="UTC")+0
[1] "2006-12-01 01:02:00 EST"
> strptime("20061201 1:02 PST",format="%Y%m%d %H:%M",tz="EST")+0
[1] "2006-12-01 01:02:00 EST"

What is the recommended way of handling this?  Computing and adding 
offsets manually?

> strptime("20061201 1:02 PST",format="%Y%m%d %H:%M",tz="UTC")+3600*3
[1] "2006-12-01 04:02:00 EST"

Or am I doing something wrong with the strptime(..., tz="EST") function?

Thanks for your time,
Dave
-- 
  Dr. David Forrest
  drf at vims.edu                                    (804)684-7900w
  drf5n at maplepark.com                             (804)642-0662h
                                    http://maplepark.com/~drf5n/


From p.dalgaard at biostat.ku.dk  Tue Jan  9 00:37:58 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 09 Jan 2007 00:37:58 +0100
Subject: [R] Simple spectral analysis
In-Reply-To: <enufb3$702$1@sea.gmane.org>
References: <45A26D72.1040404@gmx.de> <enufb3$702$1@sea.gmane.org>
Message-ID: <45A2D5D6.1020002@biostat.ku.dk>

Earl F. Glynn wrote:
> "Georg Hoermann" <georg.hoermann at gmx.de> wrote in message 
> news:45A26D72.1040404 at gmx.de...
>   
>> The data set:
>>
>> air = read.csv("http://www.hydrology.uni-kiel.de/~schorsch/air_temp.csv")
>> airtemp = ts(T_air, start=c(1989,1), freq = 365)
>> plot(airtemp)
>>     
>
> Maybe this will get you started using fft or spectrum -- I'm not sure why 
> the spectrum answer is only close:
>   
The defaults for detrending and tapering could be involved. Putting, 
e.g., detrend=F gives me a spectrum with substantially higher 
low-frequency components.

But what was the problem in the first place?

spec.pgram(airtemp,xlim=c(0,10))

abline(v=1:10,col="red")


shows a strong peak at 1 and maybe a weak peak at 2, and the other 
integer frequencies less pronounced. This seems reasonably in tune with

> x <- (1:3652)/365

> summary(lm(air$T_air ~ sin(2*pi*x)+cos(2*pi*x)+  sin(4*pi*x)+cos(4*pi*x) +  sin(6*pi*x)+cos(6*pi*x)+x))

Call:

lm(formula = air$T_air ~ sin(2 * pi * x) + cos(2 * pi * x) + 

    sin(4 * pi * x) + cos(4 * pi * x) + sin(6 * pi * x) + cos(6 * 

    pi * x) + x)

Residuals:

     Min       1Q   Median       3Q      Max 

-16.3109  -2.3317  -0.1080   2.2063  10.6249 

Coefficients:

                Estimate Std. Error t value Pr(>|t|)    

(Intercept)      9.67904    0.11267  85.909   <2e-16 ***

sin(2 * pi * x) -2.64554    0.07967 -33.208   <2e-16 ***

cos(2 * pi * x) -7.73520    0.07938 -97.443   <2e-16 ***

sin(4 * pi * x)  0.92967    0.07948  11.696   <2e-16 ***

cos(4 * pi * x)  0.13982    0.07938   1.761   0.0783 .  

sin(6 * pi * x)  0.13320    0.07945   1.676   0.0937 .  

cos(6 * pi * x)  0.14480    0.07938   1.824   0.0682 .  

x               -0.23773    0.01952 -12.179   <2e-16 ***

---

Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 


Residual standard error: 3.393 on 3644 degrees of freedom

Multiple R-Squared: 0.7486,     Adjusted R-squared: 0.7482 

F-statistic:  1550 on 7 and 3644 DF,  p-value: < 2.2e-16 





> air = read.csv("http://www.hydrology.uni-kiel.de/~schorsch/air_temp.csv")
>
> TempAirC <- air$T_air
> Time     <- as.Date(air$Date, "%d.%m.%Y")
> N <- length(Time)
>
> oldpar <- par(mfrow=c(4,1))
> plot(TempAirC ~ Time)
>
> # Using fft
> transform <- fft(TempAirC)
>
> # Extract DC component from transform
> dc <- Mod(transform[1])/N
>
> periodogram <- round( Mod(transform)^2/N, 3)
>
> # Drop first element, which is the mean
> periodogram <- periodogram[-1]
>
> # keep first half up to Nyquist limit
> periodogram <- periodogram[1:(N/2)]
>
> # Approximate number of data points in single cycle:
> print( N / which(max(periodogram) == periodogram) )
>
> # plot spectrum against Fourier Frequency index
> plot(periodogram, col="red", type="o",
>      xlab="Fourier Frequency Index", xlim=c(0,25),
>      ylab="Periodogram",
>      main="Periodogram derived from 'fft'")
>
> # Using spectrum
> s <- spectrum(TempAirC, taper=0, detrend=FALSE, col="red",
>               main="Spectral Density")
>
> plot(log(s$spec) ~ s$freq, col="red", type="o",
>      xlab="Fourier Frequency", xlim=c(0.0, 0.005),
>      ylab="Log(Periodogram)",
>      main="Periodogram from 'spectrum'")
>
> cat("Max frequency\n")
> maxfreq <- s$freq[ which(max(s$spec) == s$spec) ]
>
> # Period will be 1/frequency:
> cat("Corresponding period\n")
> print(1/maxfreq)
>
> par(oldpar)
>
>
>
> efg
>
> Earl F. Glynn
> Scientific Programmer
> Stowers Institute
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Tue Jan  9 00:43:35 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Jan 2007 23:43:35 +0000 (GMT)
Subject: [R] Does strptime(...,tz="GMT") do anything?
In-Reply-To: <Pine.LNX.4.64.0701081633510.2367@maplepark.com>
References: <Pine.LNX.4.64.0701081633510.2367@maplepark.com>
Message-ID: <Pine.LNX.4.64.0701082325570.9254@gannet.stats.ox.ac.uk>

Is EST a timezone on your system?  You have not told us your system, but 
on most the timezone in the Eastern US is EST5EDT, not EST.  Similarly 
with PST.  (See ?as.POSIXct.)

Remember that strptime returns an object of class "POSIXlt" and that has a 
'isdst' field.  The timezone controls that, but none of your examples are 
in a timezone that is on DST, nor would they be in EST5EDT or PST8PDT.

Rather than add 0, I think you want to convert to POSIXct, specifying the 
timezone (a valid one).

as.POSIXct(strptime("20061201 1:02",format="%Y%m%d %H:%M", tz="PST8PDT"),
            tz="PST8PDT")
[1] "2006-12-01 01:02:00 PST"


On Mon, 8 Jan 2007, David Forrest wrote:

> Hi All
>
> In trying to correlate some tide gauge data I need to deal with varying
> timezones.  From the documentation on strptime, it seemed that the tz
> variable might have some effect on the conversion, but I'm not seeing an
> effect.
>
>> strptime("20061201 1:02 PST",format="%Y%m%d %H:%M",tz="PST")+0
> [1] "2006-12-01 01:02:00 EST"
>> strptime("20061201 1:02 PST",format="%Y%m%d %H:%M",tz="")+0
> [1] "2006-12-01 01:02:00 EST"
>> strptime("20061201 1:02 PST",format="%Y%m%d %H:%M",tz="GMT")+0
> [1] "2006-12-01 01:02:00 EST"
>> strptime("20061201 1:02 PST",format="%Y%m%d %H:%M",tz="UTC")+0
> [1] "2006-12-01 01:02:00 EST"
>> strptime("20061201 1:02 PST",format="%Y%m%d %H:%M",tz="EST")+0
> [1] "2006-12-01 01:02:00 EST"
>
> What is the recommended way of handling this?  Computing and adding
> offsets manually?
>
>> strptime("20061201 1:02 PST",format="%Y%m%d %H:%M",tz="UTC")+3600*3
> [1] "2006-12-01 04:02:00 EST"
>
> Or am I doing something wrong with the strptime(..., tz="EST") function?
>
> Thanks for your time,
> Dave
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Tue Jan  9 00:46:23 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Jan 2007 23:46:23 +0000 (GMT)
Subject: [R] scripts with littler
In-Reply-To: <971536df0701080703q48e04a5cv96720798b8b30c11@mail.gmail.com>
References: <ent9nm$rc6$1@sea.gmane.org>
	<20070108132920.GA4640@phenix.progiciels-bpi.ca>
	<entlul$ng$1@sea.gmane.org>
	<971536df0701080703q48e04a5cv96720798b8b30c11@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701082235040.9254@gannet.stats.ox.ac.uk>

He missed

     o	There is a new front-end Rscript which can be used for #!
 	scripts and similar tasks.  See help("Rscript") and 'An
 	Introduction to R' for further details.

and that is needed for #! scripts.  (You cannot write #!/path/to/R -f as R 
is a shell script and so disallowed on most OSes.)

As I understand the earlier question, if you have

#! /usr/env cmd arg1 arg2

/usr/env is passed 'cmd arg1 arg2' as the name of the utility, at least 
under bash which says

    If  the program is a file beginning with #!, the remainder of the first
    line specifies an interpreter for the program.  The shell executes  the
    specified interpreter on operating systems that do not handle this exe-
    cutable format themselves.  The arguments to the interpreter consist of
    a  single optional argument following the interpreter name ...

Note the 'single'.  This is detailed as part of the description of Rscript 
referred to in the NEWS item.  I don't know how universal this is, but the 
Solaris Bourne shell does the same thing.

Fran?ois Pinard's idea of here documents is nice until you want to read 
from the script's stdin rather than the script itself.



On Mon, 8 Jan 2007, Gabor Grothendieck wrote:

> Looks like it will be possible to write scripts with R 2.5.0 using the
> new -f flag and file("stdin").  From https://svn.r-project.org/R/trunk/NEWS :
>
>    o	Command-line R (and Rterm.exe under Windows) accepts the options
> 	'-f filename', '--file=filename' and '-e expression' to follow
> 	other script interpreters.  These imply --no-save unless
> 	--save is specified.
>
> [..]
>
>    o	file("stdin") is now recognized, and refers to the process's
> 	'stdin' file stream whereas stdin() refers to the console.
> 	These may differ, for example for a GUI console, an embedded
> 	application of R or if --file= has been used.
>
>
>
> On 1/8/07, John Lawrence Aspden <news at aspden.com> wrote:
>> Thanks, that's a really neat mechanism, ( I especially like the note to vim,
>> which will save all my scripts having to end .R )
>>
>> Is there any way to get at the command line and stdio though?
>>
>> With littler I can do things like:
>>
>> #!/usr/bin/env r
>>
>> print(argv)
>> t=read.table(file=stdin())
>>
>> so that I can write unix-style filters.
>>
>> Cheers, John.
>>
>>
>>
>> Fran?ois Pinard wrote:
>>
>>
>>> I usually do something like:
>>>
>>>
>>> #!/bin/sh
>>> R --slave --vanilla <<EOF
>>>
>>>    R script goes here...
>>>
>>> EOF
>>>
>>> # vim: ft=r
>>>
>>>
>>> If you need to search special places for packages, you may tweak
>>> exported environment variables between the first and second line.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From deepayan.sarkar at gmail.com  Tue Jan  9 02:53:53 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 8 Jan 2007 17:53:53 -0800
Subject: [R] finer control of scales in xyplot
In-Reply-To: <45A29E55.7060703@stat.purdue.edu>
References: <45A29E55.7060703@stat.purdue.edu>
Message-ID: <eb555e660701081753n7ac184a4j2814324a70eeadbc@mail.gmail.com>

On 1/8/07, Benjamin Tyner <btyner at stat.purdue.edu> wrote:
> When plotting over multiple pages in lattice, I'd like to be able to
> have "same" scales within a page, but "free" scales between pages. In
> other words, something like:
>
> z<-data.frame(x=1:100,
>               y=runif(100),
>               d=rep(1:2,50),
>               p=rep(1:2,each=50))
>
> plot<-xyplot(y~x|d*p,
>              data=z,
>              scales=list(x=list(relation="free")),
>              layout=c(1,2))
>
> but within a page, to have common x-axes. As long as 'x' is sorted, I
> can get the desired effect by transforming x to a relative scale:
>
> plot<-xyplot(y~unlist(with(z,tapply(x,p,function(x)
> (x-min(x))/diff(range(x)))))|d*p,
>              data=z,
>              layout=c(1,2))
>
> except that I'd like the tickmark labels to be in the original units of
> 'x'. I've started looking at xscale.components.default to see whether it
> can work on a per-page (instead of per-panel) basis, but I get the
> nagging suspicion that I'm making this harder than it needs to be. Any
> assistance greatly appreciated.

A "nice" solution is unlikely. As a general rule, the trellis object
doesn't know anything about what layout it's going to be plotted with,
and unfortunately the panel-specific scales are determined when the
object is created, not when it is plotted (I don't remember all the
reasons for this choice, but changing it would require a major
overhaul). A similar task, row-specific or column-specific "free"
scales, is difficult for the same reason.

If I need to do this, I usually have in scales

1. relation="free"

2. limits = list(lim1, lim2, ...) where lim1, lim2, etc are
packet-specific limits (they have to be supplied explicitly). You can
omit this if you can make sure your prepanel function gives you the
right things.

This will almost do what you want, but will repeat the ticks/labels
for every panel. To suppress that you can additionally have

3. at = list(TRUE, NULL, ...) etc where NULL means ticks/labels won't be drawn.

This doesn't really help, because the space will still be wasted, so finally,

4. add par.settings = list(layout.heights = list(axis.panel = c(1, 0, ...)))

-Deepayan


From wangtong at usc.edu  Tue Jan  9 03:09:28 2007
From: wangtong at usc.edu (Tong Wang)
Date: Mon, 08 Jan 2007 18:09:28 -0800
Subject: [R] A question about R environment
Message-ID: <dc87d92f2693.45a288d8@usc.edu>

Hi  all,
     
I created environment  "mytoolbox" by :   mytoolbox <- new.env(parent=baseenv())
Is there anyway I put it in the search path ?    

If you need some background :
  In a project, I often write some small functions,  and load them into my workspace directly,   so when I list the objects 
  with ls(), it looks pretty messy.  So I am wondering if it is possible to creat an environment,  and put these tools into 
  this environment.  For example,  I have functions    fun1(), fun2() ......    and creat an environment  mytoolbox  which 
  contains all these functions.  And it should be somewhere in the search path:   ".GlobalEnv"        "mytoolbox"                        "package:methods"  ........


From ggrothendieck at gmail.com  Tue Jan  9 03:24:40 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 8 Jan 2007 21:24:40 -0500
Subject: [R] A question about R environment
In-Reply-To: <dc87d92f2693.45a288d8@usc.edu>
References: <dc87d92f2693.45a288d8@usc.edu>
Message-ID: <971536df0701081824p7f7ab7f5xf4b4c0f1e4ef6856@mail.gmail.com>

Try this:

> e <- new.env()
> e$f <- function(x)x
> attach(e)
> search()
 [1] ".GlobalEnv"        "e"                 "package:stats"
 [4] "package:graphics"  "package:grDevices" "package:utils"
 [7] "package:datasets"  "package:methods"   "Autoloads"
[10] "package:base"
> f
function(x)x

On 1/8/07, Tong Wang <wangtong at usc.edu> wrote:
> Hi  all,
>
> I created environment  "mytoolbox" by :   mytoolbox <- new.env(parent=baseenv())
> Is there anyway I put it in the search path ?
>
> If you need some background :
>  In a project, I often write some small functions,  and load them into my workspace directly,   so when I list the objects
>  with ls(), it looks pretty messy.  So I am wondering if it is possible to creat an environment,  and put these tools into
>  this environment.  For example,  I have functions    fun1(), fun2() ......    and creat an environment  mytoolbox  which
>  contains all these functions.  And it should be somewhere in the search path:   ".GlobalEnv"        "mytoolbox"                        "package:methods"  ........
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Inman.Brant at mayo.edu  Tue Jan  9 04:10:03 2007
From: Inman.Brant at mayo.edu (Inman, Brant A.   M.D.)
Date: Mon, 8 Jan 2007 21:10:03 -0600
Subject: [R]  Partial proportional odds logistic regression
Message-ID: <6021CA6EF4C8374084D4F5A141F1CBBB6649F7@msgebe23.mfad.mfroot.org>


Just a follow-up note on my last posting.  I still have not had any
replies from the R-experts our there that use partial proportional odds
regression (and I have to hope that there are some of you!) but I do
think that I have figured out how to perform the unconstrained partial
proportional odds model using vglm.  I show this code below for the
benefit of others that may want to try it (or point out my errors) using
one of the datasets in Petersen and Harrell's paper (Appl Stat 1990).
However, I remain open for suggestions on how to implement the
unconstrained partial proportional odds model.

--------------

library(VGAM)
library(MASS)
library(Design)

#######################################################################
# Nausea dataset
# Peterson and Harrell. Applied Statistics 1990, 39(2): 205-217

nausea.short <- data.frame(matrix(nrow=12, ncol=3))	#Table 2
	colnames(nausea.short) <- c('nausea', 'cisplatin', 'freq')
	nausea.short[,1] <- ordered(rep(seq(0,5,1),2),
labels=seq(0,5,1))
	nausea.short[,2] <- c(rep(0,6), rep(1,6))
	nausea.short[,3] <- c(43,39,13,22,15,29,7,7,3,12,15,14)

# Proportional odds ordinal logistic regression: 3 options
polr(nausea ~ cisplatin, weights=freq, data=nausea.short,
method='logistic')
lrm(nausea ~ cisplatin, weights=freq, data=nausea.short)
vglm(nausea ~ cisplatin, weights=freq, data=nausea.short,
family=cumulative(parallel=T, reverse=T))


# Unconstrained partial proportional odds ordinal logistic regression
vglm(nausea ~ cisplatin, weights=freq, data=nausea.short,
family=cumulative(parallel=F, reverse=T))

--------------

The results obtained with this approach appear consistent with those
presented in Table 3 of the paper.  However, the code for the
unconstrained partial proportional odds model is so simple (just one
letter is different than in the proportional odds model!) that I wonder
if there is not room for error here that I am too inexperienced to
identify.

Again, help with the constrained model would be greatly appreciated.

Brant Inman


From pinard at iro.umontreal.ca  Tue Jan  9 04:11:03 2007
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Mon, 8 Jan 2007 22:11:03 -0500
Subject: [R] A question about R environment
In-Reply-To: <dc87d92f2693.45a288d8@usc.edu>
References: <dc87d92f2693.45a288d8@usc.edu>
Message-ID: <20070109031103.GA6104@phenix.progiciels-bpi.ca>

[Tong Wang]

>I created environment "mytoolbox" by : mytoolbox <- 
>new.env(parent=baseenv()).  Is there anyway I put it in the search 
>path?  In a project, I often write some small functions, and load them 
>into my workspace directly, so when I list the objects with ls(), it 
>looks pretty messy.  So I am wondering if it is possible to creat an 
>environment, and put these tools into this environment.  For example, 
>I have functions fun1(), fun2() ... and creat an environment mytoolbox 
>which contains all these functions.  And it should be somewhere in the 
>search path:  ".GlobalEnv" "mytoolbox" "package:methods".

Here is a trick, shown as a fairly simplified copy of my ~/.Rprofile.  
It allows for a few simple functions always available, yet without 
having to create a package, and leaving ls() and any later .RData file 
unencumbered.

The idea is to use local() to prevent any unwanted clutter to leak out 
(my real ~/.Rprofile holds more than shown below and use temporary 
variables), to initialise a list meant to hold a bunch of functions or 
other R things, and to save that list on the search path.

This example also demonstrate a few useful functions for when I read the 
R mailing list.  I often need to transfer part of emails containing
code excerpts within the window where R executes, while removing 
quotation marks, white lines and other noise.  I merely highlight-select 
part of the message with the mouse, and then, within R, do things like:

   xs()   source the highlighted region
   xd()   read in a data.frame
   xm()   read in a matrix
   xe()   evaluate and print an expression
   xv()   read a list of values as a vector

The list above in decreasing order of usefulness (for me).  Except for 
xs(), which has no automatic printout, you may either let the others 
print what they got, or assign their value to some variable.  Arguments 
are also possible, for example like this:

   xd(T)  read in a data.frame when the first line holds column names



if (interactive()) {
    local({

        fp.etc <- list()

        fp.etc$xsel.vector <- function (...) {
            connexion <- textConnection(xselection())
            on.exit(close(connexion))
            scan(connexion, ...)
        }
        fp.etc$xsel.dataframe <- function (...) {
            connexion <- textConnection(xselection())
            on.exit(close(connexion))
            read.table(connexion, ...)
        }
        fp.etc$xsel.matrix <- function (...) {
            connexion <- textConnection(xselection())
            on.exit(close(connexion))
            data.matrix(read.table(connexion, ...))
        }
        fp.etc$xsel.eval <- function (...) {
            connexion <- textConnection(xselection())
            on.exit(close(connexion))
            eval(parse(connexion, ...))
        }
        fp.etc$xsel.source <- function (...) {
            connexion <- textConnection(xselection())
            on.exit(close(connexion))
            source(connexion, ...)
        }

        fp.etc$xselection <- function ()
        {
            lignes <- suppressWarnings(readLines('clipboard'))
            lignes <- lignes[lignes != '']
            stopifnot(length(lignes) != 0)
            marge <- substr(lignes, 1, 1)
            while (all(marge %in% c('>', '+', ':', '|'))
                  || all(marge == ' ')) {
                lignes <- substring(lignes, 2)
                marge <- substr(lignes, 1, 1)
            }
            lignes
        }

        fp.etc$xv <- fp.etc$xsel.vector
        fp.etc$xd <- fp.etc$xsel.dataframe
        fp.etc$xm <- fp.etc$xsel.matrix
        fp.etc$xe <- fp.etc$xsel.eval
        fp.etc$xs <- fp.etc$xsel.source

        attach(fp.etc, warn=FALSE)

    })
}

# vim: ft=r


-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca


From phgrosjean at sciviews.org  Tue Jan  9 07:09:03 2007
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Tue, 09 Jan 2007 07:09:03 +0100
Subject: [R] A question about R environment
In-Reply-To: <20070109031103.GA6104@phenix.progiciels-bpi.ca>
References: <dc87d92f2693.45a288d8@usc.edu>
	<20070109031103.GA6104@phenix.progiciels-bpi.ca>
Message-ID: <45A3317F.50506@sciviews.org>

Please, don't reinvent the wheel: putting functions in a dedicated 
environment is one of the things done by R packages (together with a 
good documentation of the function, and making them easily installable 
on any R implementation). So, this is probably the time for you to read 
the "Writing R extensions" manual, and to start implementing your own R 
package!
Best,

Philippe Grosjean

Fran?ois Pinard wrote:
> [Tong Wang]
> 
>> I created environment "mytoolbox" by : mytoolbox <- 
>> new.env(parent=baseenv()).  Is there anyway I put it in the search 
>> path?  In a project, I often write some small functions, and load them 
>> into my workspace directly, so when I list the objects with ls(), it 
>> looks pretty messy.  So I am wondering if it is possible to creat an 
>> environment, and put these tools into this environment.  For example, 
>> I have functions fun1(), fun2() ... and creat an environment mytoolbox 
>> which contains all these functions.  And it should be somewhere in the 
>> search path:  ".GlobalEnv" "mytoolbox" "package:methods".
> 
> Here is a trick, shown as a fairly simplified copy of my ~/.Rprofile.  
> It allows for a few simple functions always available, yet without 
> having to create a package, and leaving ls() and any later .RData file 
> unencumbered.
> 
> The idea is to use local() to prevent any unwanted clutter to leak out 
> (my real ~/.Rprofile holds more than shown below and use temporary 
> variables), to initialise a list meant to hold a bunch of functions or 
> other R things, and to save that list on the search path.
> 
> This example also demonstrate a few useful functions for when I read the 
> R mailing list.  I often need to transfer part of emails containing
> code excerpts within the window where R executes, while removing 
> quotation marks, white lines and other noise.  I merely highlight-select 
> part of the message with the mouse, and then, within R, do things like:
> 
>    xs()   source the highlighted region
>    xd()   read in a data.frame
>    xm()   read in a matrix
>    xe()   evaluate and print an expression
>    xv()   read a list of values as a vector
> 
> The list above in decreasing order of usefulness (for me).  Except for 
> xs(), which has no automatic printout, you may either let the others 
> print what they got, or assign their value to some variable.  Arguments 
> are also possible, for example like this:
> 
>    xd(T)  read in a data.frame when the first line holds column names
> 
> 
> 
> if (interactive()) {
>     local({
> 
>         fp.etc <- list()
> 
>         fp.etc$xsel.vector <- function (...) {
>             connexion <- textConnection(xselection())
>             on.exit(close(connexion))
>             scan(connexion, ...)
>         }
>         fp.etc$xsel.dataframe <- function (...) {
>             connexion <- textConnection(xselection())
>             on.exit(close(connexion))
>             read.table(connexion, ...)
>         }
>         fp.etc$xsel.matrix <- function (...) {
>             connexion <- textConnection(xselection())
>             on.exit(close(connexion))
>             data.matrix(read.table(connexion, ...))
>         }
>         fp.etc$xsel.eval <- function (...) {
>             connexion <- textConnection(xselection())
>             on.exit(close(connexion))
>             eval(parse(connexion, ...))
>         }
>         fp.etc$xsel.source <- function (...) {
>             connexion <- textConnection(xselection())
>             on.exit(close(connexion))
>             source(connexion, ...)
>         }
> 
>         fp.etc$xselection <- function ()
>         {
>             lignes <- suppressWarnings(readLines('clipboard'))
>             lignes <- lignes[lignes != '']
>             stopifnot(length(lignes) != 0)
>             marge <- substr(lignes, 1, 1)
>             while (all(marge %in% c('>', '+', ':', '|'))
>                   || all(marge == ' ')) {
>                 lignes <- substring(lignes, 2)
>                 marge <- substr(lignes, 1, 1)
>             }
>             lignes
>         }
> 
>         fp.etc$xv <- fp.etc$xsel.vector
>         fp.etc$xd <- fp.etc$xsel.dataframe
>         fp.etc$xm <- fp.etc$xsel.matrix
>         fp.etc$xe <- fp.etc$xsel.eval
>         fp.etc$xs <- fp.etc$xsel.source
> 
>         attach(fp.etc, warn=FALSE)
> 
>     })
> }
> 
> # vim: ft=r
> 
>


From hb at stat.berkeley.edu  Tue Jan  9 07:40:36 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Tue, 9 Jan 2007 17:40:36 +1100
Subject: [R] A question about R environment
In-Reply-To: <971536df0701081824p7f7ab7f5xf4b4c0f1e4ef6856@mail.gmail.com>
References: <dc87d92f2693.45a288d8@usc.edu>
	<971536df0701081824p7f7ab7f5xf4b4c0f1e4ef6856@mail.gmail.com>
Message-ID: <59d7961d0701082240v284824bfj2e884c065df2c09c@mail.gmail.com>

sourceTo() in R.utils will allow you to source() a file into an environment.

/Henrik

On 1/9/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Try this:
>
> > e <- new.env()
> > e$f <- function(x)x
> > attach(e)
> > search()
>  [1] ".GlobalEnv"        "e"                 "package:stats"
>  [4] "package:graphics"  "package:grDevices" "package:utils"
>  [7] "package:datasets"  "package:methods"   "Autoloads"
> [10] "package:base"
> > f
> function(x)x
>
> On 1/8/07, Tong Wang <wangtong at usc.edu> wrote:
> > Hi  all,
> >
> > I created environment  "mytoolbox" by :   mytoolbox <- new.env(parent=baseenv())
> > Is there anyway I put it in the search path ?
> >
> > If you need some background :
> >  In a project, I often write some small functions,  and load them into my workspace directly,   so when I list the objects
> >  with ls(), it looks pretty messy.  So I am wondering if it is possible to creat an environment,  and put these tools into
> >  this environment.  For example,  I have functions    fun1(), fun2() ......    and creat an environment  mytoolbox  which
> >  contains all these functions.  And it should be somewhere in the search path:   ".GlobalEnv"        "mytoolbox"                        "package:methods"  ........
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From georg.hoermann at gmx.de  Tue Jan  9 08:05:32 2007
From: georg.hoermann at gmx.de (Georg Hoermann)
Date: Tue, 09 Jan 2007 08:05:32 +0100
Subject: [R] Simple spectral analysis
In-Reply-To: <45A2D5D6.1020002@biostat.ku.dk>
References: <45A26D72.1040404@gmx.de> <enufb3$702$1@sea.gmane.org>
	<45A2D5D6.1020002@biostat.ku.dk>
Message-ID: <45A33EBC.3050503@gmx.de>

Peter Dalgaard wrote:
> Earl F. Glynn wrote:
>>   
> The defaults for detrending and tapering could be involved. Putting, 
> e.g., detrend=F gives me a spectrum with substantially higher 
> low-frequency components.
> 
> But what was the problem in the first place?
> 
understanding how this things work in R compared to other packages 8-)

Thanks a lot for the help. I will post the script when its ready
(an introduction for our biology students to time series, just 8 hours)

Georg



-- 
Georg Hoermann, Dep. of Hydrology, Ecology, Kiel University, Germany
+49/431/23761412, mo: +49/171/4995884, icq:348340729, skype: ghoermann


From abas at agro.duth.gr  Tue Jan  9 08:17:50 2007
From: abas at agro.duth.gr (Zaphiris Abas)
Date: Tue, 9 Jan 2007 09:17:50 +0200
Subject: [R] no linear model with many objects
Message-ID: <003401c733be$499a4bc0$59ce5cc1@ZAPHIRIS>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070109/5a4745e3/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Jan  9 08:32:17 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Jan 2007 07:32:17 +0000 (GMT)
Subject: [R] ACCESS/Office : connecting
In-Reply-To: <377449.24974.qm@web56602.mail.re3.yahoo.com>
References: <377449.24974.qm@web56602.mail.re3.yahoo.com>
Message-ID: <Pine.LNX.4.64.0701090731040.24092@gannet.stats.ox.ac.uk>

It is easy with package RODBC.

Connecting to databases is discussed in the 'R Data Import/Export' manual.

On Mon, 8 Jan 2007, Milton Cezar Ribeiro wrote:

>  How can I connect to a ACCESS (.mdb) file? In fact, I would like to 
> connect to a blank file, write a data.frame as table and after that 
> INSERT records using some "insert" command.
>
>  Kind regards,
>
>  Miltinho
>  Brazil

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From petr.pikal at precheza.cz  Tue Jan  9 08:46:49 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 09 Jan 2007 08:46:49 +0100
Subject: [R] Simple spectral analysis
In-Reply-To: <45A26D72.1040404@gmx.de>
Message-ID: <45A35679.18218.4AA31D@localhost>

Hi

without beeing specific in spectrum analysis you will get frequencies 
and spectral densities fro spectrum()

>From help page

An object of class "spec", which is a list containing at least the 
following components: 

freq vector of frequencies at which the spectral density is 
estimated. (Possibly approximate Fourier frequencies.) The units are 
the reciprocal of cycles per unit time (and not per observation 
spacing): see Details below. 
spec Vector (for univariate series) or matrix (for multivariate 
series) of estimates of the spectral density at frequencies 
corresponding to freq. 

<snip>

This is the important part:

**The result is returned invisibly if plot is true.**

So if you call

spectrum(data) you will get plot but in case

sp <- spectrum(data)

you will get also object sp which has above mentioned components. 
Actual periods are obtainable by

n/sp$freq

HTH
Petr

On 8 Jan 2007 at 17:12, Georg Hoermann wrote:

Date sent:      	Mon, 08 Jan 2007 17:12:34 +0100
From:           	Georg Hoermann <georg.hoermann at gmx.de>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Simple spectral analysis

> Hello world,
> 
> I am actually trying to transfer a lecture from Statistica to
> R and I ran into problems with spectral analysis, I think I
> just don't get it 8-(
> (The posting from "FFT, frequs, magnitudes, phases" from 2005
> did not enlighten me)
> 
> As a starter for the students I have a 10year data set of air 
> temperature with daily values  and I try to
> get a periodogram where the annual period (365 days) should be clearly
> visible (in statistica I can get the frequencies and the period). I
> tried the spectrum() and pgram() functions, but did not find a way
> through... The final aim would be to get the periodogram (and the
> residuals from the reassembled data set...)
> 
> Thanks and greetings,
> Georg
> 
> The data set:
> 
> air =
> read.csv("http://www.hydrology.uni-kiel.de/~schorsch/air_temp.csv")
> airtemp = ts(T_air, start=c(1989,1), freq = 365) plot(airtemp)
> 
> 
> -- 
> Georg Hoermann, Dep. of Hydrology, Ecology, Kiel University, Germany
> +49/431/23761412, mo: +49/171/4995884, icq:348340729, skype: ghoermann
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Tue Jan  9 08:55:58 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 09 Jan 2007 08:55:58 +0100
Subject: [R] no linear model with many objects
In-Reply-To: <003401c733be$499a4bc0$59ce5cc1@ZAPHIRIS>
Message-ID: <45A3589E.22106.5303C5@localhost>

Hi

On 9 Jan 2007 at 9:17, Zaphiris Abas wrote:

From:           	"Zaphiris Abas" <abas at agro.duth.gr>
To:             	<r-help at stat.math.ethz.ch>
Date sent:      	Tue, 9 Jan 2007 09:17:50 +0200
Subject:        	[R] no linear model with many objects

> Hi  all,
> 
> Is any way to estimate the parameters of a curve, not manualy,  from
> many subsets of my dataset

Most probably yes. But you probably meant *how*.
To continue questioning see.

> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Maybe you are looking for nlme.

HTH
Petr
Petr Pikal
petr.pikal at precheza.cz


From wangtong at usc.edu  Tue Jan  9 09:55:41 2007
From: wangtong at usc.edu (Tong Wang)
Date: Tue, 09 Jan 2007 00:55:41 -0800
Subject: [R] A question about R environment
In-Reply-To: <45A3317F.50506@sciviews.org>
References: <dc87d92f2693.45a288d8@usc.edu>
	<20070109031103.GA6104@phenix.progiciels-bpi.ca>
	<45A3317F.50506@sciviews.org>
Message-ID: <e27d9ebe7b05.45a2e80d@usc.edu>

Francois (sorry can't type the right letter) , thanks a million for the detailed response,  you have really cool functions there !

Philippe :  I will definitely follow your advice later,  but I got some time pressure from the current project, so have to go with the easy sol. now ,   thanks a lot. 

Happy new year every one!

cheers

tong

----- Original Message -----
From: Philippe Grosjean <phgrosjean at sciviews.org>
Date: Monday, January 8, 2007 10:09 pm
Subject: Re: [R] A question about R environment
To: Fran?ois Pinard <pinard at iro.umontreal.ca>, Tong Wang <wangtong at usc.edu>, R help <r-help at stat.math.ethz.ch>

> Please, don't reinvent the wheel: putting functions in a dedicated 
> environment is one of the things done by R packages (together with 
> a 
> good documentation of the function, and making them easily 
> installable 
> on any R implementation). So, this is probably the time for you to 
> read 
> the "Writing R extensions" manual, and to start implementing your 
> own R 
> package!
> Best,
> 
> Philippe Grosjean
> 
> Fran?ois Pinard wrote:
> > [Tong Wang]
> > 
> >> I created environment "mytoolbox" by : mytoolbox <- 
> >> new.env(parent=baseenv()).  Is there anyway I put it in the 
> search 
> >> path?  In a project, I often write some small functions, and 
> load them 
> >> into my workspace directly, so when I list the objects with 
> ls(), it 
> >> looks pretty messy.  So I am wondering if it is possible to 
> creat an 
> >> environment, and put these tools into this environment.  For 
> example, 
> >> I have functions fun1(), fun2() ... and creat an environment 
> mytoolbox 
> >> which contains all these functions.  And it should be somewhere 
> in the 
> >> search path:  ".GlobalEnv" "mytoolbox" "package:methods".
> > 
> > Here is a trick, shown as a fairly simplified copy of my 
> ~/.Rprofile.  
> > It allows for a few simple functions always available, yet 
> without 
> > having to create a package, and leaving ls() and any later .RData 
> file 
> > unencumbered.
> > 
> > The idea is to use local() to prevent any unwanted clutter to 
> leak out 
> > (my real ~/.Rprofile holds more than shown below and use 
> temporary 
> > variables), to initialise a list meant to hold a bunch of 
> functions or 
> > other R things, and to save that list on the search path.
> > 
> > This example also demonstrate a few useful functions for when I 
> read the 
> > R mailing list.  I often need to transfer part of emails containing
> > code excerpts within the window where R executes, while removing 
> > quotation marks, white lines and other noise.  I merely highlight-
> select 
> > part of the message with the mouse, and then, within R, do things 
> like:> 
> >    xs()   source the highlighted region
> >    xd()   read in a data.frame
> >    xm()   read in a matrix
> >    xe()   evaluate and print an expression
> >    xv()   read a list of values as a vector
> > 
> > The list above in decreasing order of usefulness (for me).  
> Except for 
> > xs(), which has no automatic printout, you may either let the 
> others 
> > print what they got, or assign their value to some variable.  
> Arguments 
> > are also possible, for example like this:
> > 
> >    xd(T)  read in a data.frame when the first line holds column 
> names> 
> > 
> > 
> > if (interactive()) {
> >     local({
> > 
> >         fp.etc <- list()
> > 
> >         fp.etc$xsel.vector <- function (...) {
> >             connexion <- textConnection(xselection())
> >             on.exit(close(connexion))
> >             scan(connexion, ...)
> >         }
> >         fp.etc$xsel.dataframe <- function (...) {
> >             connexion <- textConnection(xselection())
> >             on.exit(close(connexion))
> >             read.table(connexion, ...)
> >         }
> >         fp.etc$xsel.matrix <- function (...) {
> >             connexion <- textConnection(xselection())
> >             on.exit(close(connexion))
> >             data.matrix(read.table(connexion, ...))
> >         }
> >         fp.etc$xsel.eval <- function (...) {
> >             connexion <- textConnection(xselection())
> >             on.exit(close(connexion))
> >             eval(parse(connexion, ...))
> >         }
> >         fp.etc$xsel.source <- function (...) {
> >             connexion <- textConnection(xselection())
> >             on.exit(close(connexion))
> >             source(connexion, ...)
> >         }
> > 
> >         fp.etc$xselection <- function ()
> >         {
> >             lignes <- suppressWarnings(readLines('clipboard'))
> >             lignes <- lignes[lignes != '']
> >             stopifnot(length(lignes) != 0)
> >             marge <- substr(lignes, 1, 1)
> >             while (all(marge %in% c('>', '+', ':', '|'))
> >                   || all(marge == ' ')) {
> >                 lignes <- substring(lignes, 2)
> >                 marge <- substr(lignes, 1, 1)
> >             }
> >             lignes
> >         }
> > 
> >         fp.etc$xv <- fp.etc$xsel.vector
> >         fp.etc$xd <- fp.etc$xsel.dataframe
> >         fp.etc$xm <- fp.etc$xsel.matrix
> >         fp.etc$xe <- fp.etc$xsel.eval
> >         fp.etc$xs <- fp.etc$xsel.source
> > 
> >         attach(fp.etc, warn=FALSE)
> > 
> >     })
> > }
> > 
> > # vim: ft=r
> > 
> > 
>


From wangtong at usc.edu  Tue Jan  9 09:58:14 2007
From: wangtong at usc.edu (Tong Wang)
Date: Tue, 09 Jan 2007 00:58:14 -0800
Subject: [R] A question about R environment
In-Reply-To: <59d7961d0701082240v284824bfj2e884c065df2c09c@mail.gmail.com>
References: <dc87d92f2693.45a288d8@usc.edu>
	<971536df0701081824p7f7ab7f5xf4b4c0f1e4ef6856@mail.gmail.com>
	<59d7961d0701082240v284824bfj2e884c065df2c09c@mail.gmail.com>
Message-ID: <f6868c1546de.45a2e8a6@usc.edu>

Hi,
   Thanks a lot for your response,   but it is strange that when I do attach() , I got the follow error: 
  Error in attach(e) : attach only works for lists and data frames

Any suggestions on this?


tong

----- Original Message -----
From: Henrik Bengtsson <hb at stat.berkeley.edu>
Date: Monday, January 8, 2007 10:40 pm
Subject: Re: [R] A question about R environment
To: Gabor Grothendieck <ggrothendieck at gmail.com>
Cc: Tong Wang <wangtong at usc.edu>, R help <r-help at stat.math.ethz.ch>

> sourceTo() in R.utils will allow you to source() a file into an 
> environment.
> /Henrik
> 
> On 1/9/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Try this:
> >
> > > e <- new.env()
> > > e$f <- function(x)x
> > > attach(e)
> > > search()
> >  [1] ".GlobalEnv"        "e"                 "package:stats"
> >  [4] "package:graphics"  "package:grDevices" "package:utils"
> >  [7] "package:datasets"  "package:methods"   "Autoloads"
> > [10] "package:base"
> > > f
> > function(x)x
> >
> > On 1/8/07, Tong Wang <wangtong at usc.edu> wrote:
> > > Hi  all,
> > >
> > > I created environment  "mytoolbox" by :   mytoolbox <- 
> new.env(parent=baseenv())> > Is there anyway I put it in the search 
> path ?
> > >
> > > If you need some background :
> > >  In a project, I often write some small functions,  and load 
> them into my workspace directly,   so when I list the objects
> > >  with ls(), it looks pretty messy.  So I am wondering if it is 
> possible to creat an environment,  and put these tools into
> > >  this environment.  For example,  I have functions    fun1(), 
> fun2() ......    and creat an environment  mytoolbox  which
> > >  contains all these functions.  And it should be somewhere in 
> the search path:   ".GlobalEnv"        "mytoolbox"                  
>      "package:methods"  ........
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-
> project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html> and provide commented, minimal, self-contained, 
> reproducible code.
> >
>


From r_253309 at yahoo.com  Mon Jan  8 13:10:26 2007
From: r_253309 at yahoo.com (r400 r400)
Date: Mon, 8 Jan 2007 04:10:26 -0800 (PST)
Subject: [R] Multivariate OLS
Message-ID: <20070108121026.30545.qmail@web58304.mail.re3.yahoo.com>

Dear all R users,

Suppose I have a VECTOR of time series y[t] consists of 2000 data point. For example suppose I have data frame which has two columns. First column represents a time series of exchange rate for 2000 days. And the second column represents the price of a commodity for the same period. Now I want to fit a OLS regression like that,

y[t] = a + b*delta[y[t-1]] + c*delta[y[t-2]] + epsilon[t]

as y[t] is not a vector rather a data frame containing two columns I could not use lm() function. Can anyone give me any idea how to do that in R?

Thanks and regards,
jon

Send instant messages to your online friends http://uk.messenger.yahoo.com


From dray at biomserv.univ-lyon1.fr  Tue Jan  9 10:23:21 2007
From: dray at biomserv.univ-lyon1.fr (=?ISO-8859-1?Q?St=E9phane_Dray?=)
Date: Tue, 09 Jan 2007 10:23:21 +0100
Subject: [R] Cross-compilation of R and ld bug ?
In-Reply-To: <Pine.LNX.4.64.0701081731100.319@gannet.stats.ox.ac.uk>
References: <45A27901.2030000@biomserv.univ-lyon1.fr>
	<Pine.LNX.4.64.0701081731100.319@gannet.stats.ox.ac.uk>
Message-ID: <45A35F09.5030107@biomserv.univ-lyon1.fr>

Not sure to really understand 1) . The makefile (available at 
http://cran.r-project.org/doc/contrib/Makefile-rcb) use  wget 
--passive-ftp 
http://www.stats.ox.ac.uk/pub/Rtools/mingw-cross5.tar.bz2;  This version 
seems to be the current, I am correct ? Note that :

stephane at pcdray:~/Rdev/CrossCompileBuild$ cross-tools/bin/i586-mingw32-ld -v
GNU ld version 2.16.91 20050827
stephane at pcdray:~/Rdev/CrossCompileBuild$ cross-tools/i586-mingw32/bin/ld -v
GNU ld version 2.16.91 20050827


However, I have use R-2.4.1 and everything seems ok.

Thanks.


Prof Brian Ripley wrote:
> There are two problems here:
>
> 1) your binutils is not current.
> 2) your R is not current.
>
> Updating either will solve this, but please update both.
> Almost up-to-date cross-compilers are (as ever) available from
>
> http://www.stats.ox.ac.uk/pub/Rtools/
>
> (If you want to compile R-devel you will need to update the mingw, 
> which can be done from the i386 distribution.  I will rebuild the 
> cross-compilers in due course.)
>
> On Mon, 8 Jan 2007, St?phane Dray wrote:
>
>> Hello list,
>>
>> I would like to cross-compile R packages using R 2.4.0. I am working on
>> Linux Debian and cross-compiled (windows binaries) without problems with
>> older R version.
>> I have used the doc of Yan and Rossini in the contributed section of the
>> R documentation (same version of MinGW...).
>> When I try to cross-compile R (make R), the procedure stopped and 
>> returns :
>> ************************************
>> i586-mingw32-windres  --include-dir ../include -i dllversion.rc -o
>> dllversion.o
>> i586-mingw32-gcc  -shared -s -mwindows -o R.dll R.def console.o
>> dataentry.o dynl oad.o edit.o editor.o embeddedR.o extra.o opt.o pager.o
>> preferences.o psignal.o rhome.o rui.o run.o shext.o sys-win32.o system.o
>> e_pow.o malloc.o ../main/libmai n.a ../appl/libappl.a
>> ../nmath/libnmath.a graphapp/ga.a getline/gl.a ../extra/xd r/libxdr.a
>> ../extra/zlib/libz.a ../extra/pcre/libpcre.a ../extra/bzip2/libbz2.a
>> ../extra/intl/libintl.a ../extra/trio/libtrio.a dllversion.o -L. -lg2c
>> -lRblas - lcomctl32 -lversion
>> ../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1703):
>> undefin ed reference to `__pcre_ucp_findprop'
>> ../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1740):
>> undefin ed reference to `__pcre_ucp_findprop'
>> ../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1848):
>> undefin ed reference to `__pcre_ucp_findprop'
>> ../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x187f):
>> undefin ed reference to `__pcre_ucp_findprop'
>> ../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1966):
>> undefin ed reference to `__pcre_ucp_findprop'
>> ../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1f2a):
>> more un defined references to `__pcre_ucp_findprop' follow
>> collect2: ld returned 1 exit status
>> make[4]: *** [R.dll] Erreur 1
>> make[3]: *** [../../bin/R.dll] Erreur 2
>> make[2]: *** [rbuild] Erreur 2
>> make[1]: *** [all] Erreur 2
>> make[1]: quittant le r?pertoire ?
>> /home/stephane/Rdev/CrossCompileBuild/WinR/R-2 .4.0/src/gnuwin32 ?
>> ****************************************
>> I have read (http://www.murdoch-sutherland.com/Rtools/) that a bug
>> exists for ld version 2.16.91 20050827 and that Prof Ripley produced a
>> patched version. It seems that my problem is also related to ld (and I
>> have the same version). So I wonder if the same bug could be responsible
>> of the error in the cross-compiler.
>> Two questions:
>> - Are they other people which have the same problem when cross-compiling
>> R on Linux
>> - Is it possible that the problem is related to ld. If yes, is it
>> possible to obtain a patched version ?
>>
>> Thanks in advance.
>> Sincerely,
>>
>>
>


-- 
St?phane DRAY (dray at biomserv.univ-lyon1.fr )
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - Lyon I
43, Bd du 11 Novembre 1918, 69622 Villeurbanne Cedex, France
Tel: 33 4 72 43 27 57       Fax: 33 4 72 43 13 88
http://biomserv.univ-lyon1.fr/~dray/


From ripley at stats.ox.ac.uk  Tue Jan  9 10:28:09 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Jan 2007 09:28:09 +0000 (GMT)
Subject: [R] Cross-compilation of R and ld bug ?
In-Reply-To: <45A35F09.5030107@biomserv.univ-lyon1.fr>
References: <45A27901.2030000@biomserv.univ-lyon1.fr>
	<Pine.LNX.4.64.0701081731100.319@gannet.stats.ox.ac.uk>
	<45A35F09.5030107@biomserv.univ-lyon1.fr>
Message-ID: <Pine.LNX.4.64.0701090925210.28652@gannet.stats.ox.ac.uk>

On Tue, 9 Jan 2007, St?phane Dray wrote:

> Not sure to really understand 1) . The makefile (available at 
> http://cran.r-project.org/doc/contrib/Makefile-rcb) use  wget --passive-ftp 
> http://www.stats.ox.ac.uk/pub/Rtools/mingw-cross5.tar.bz2;  This version 
> seems to be the current, I am correct ? Note that :

Well, those tools built 2.4.0 for me, but earlier versions of 
mingw-cross5.tar.bz2 did not because bintools was too old.  So assuming 
you didn't get a cached older file, I don't understand.


> stephane at pcdray:~/Rdev/CrossCompileBuild$ cross-tools/bin/i586-mingw32-ld -v
> GNU ld version 2.16.91 20050827
> stephane at pcdray:~/Rdev/CrossCompileBuild$ cross-tools/i586-mingw32/bin/ld -v
> GNU ld version 2.16.91 20050827
>
>
> However, I have use R-2.4.1 and everything seems ok.
>
> Thanks.
>
>
> Prof Brian Ripley wrote:
>> There are two problems here:
>> 
>> 1) your binutils is not current.
>> 2) your R is not current.
>> 
>> Updating either will solve this, but please update both.
>> Almost up-to-date cross-compilers are (as ever) available from
>> 
>> http://www.stats.ox.ac.uk/pub/Rtools/
>> 
>> (If you want to compile R-devel you will need to update the mingw, which 
>> can be done from the i386 distribution.  I will rebuild the cross-compilers 
>> in due course.)
>> 
>> On Mon, 8 Jan 2007, St?phane Dray wrote:
>> 
>>> Hello list,
>>> 
>>> I would like to cross-compile R packages using R 2.4.0. I am working on
>>> Linux Debian and cross-compiled (windows binaries) without problems with
>>> older R version.
>>> I have used the doc of Yan and Rossini in the contributed section of the
>>> R documentation (same version of MinGW...).
>>> When I try to cross-compile R (make R), the procedure stopped and returns 
>>> :
>>> ************************************
>>> i586-mingw32-windres  --include-dir ../include -i dllversion.rc -o
>>> dllversion.o
>>> i586-mingw32-gcc  -shared -s -mwindows -o R.dll R.def console.o
>>> dataentry.o dynl oad.o edit.o editor.o embeddedR.o extra.o opt.o pager.o
>>> preferences.o psignal.o rhome.o rui.o run.o shext.o sys-win32.o system.o
>>> e_pow.o malloc.o ../main/libmai n.a ../appl/libappl.a
>>> ../nmath/libnmath.a graphapp/ga.a getline/gl.a ../extra/xd r/libxdr.a
>>> ../extra/zlib/libz.a ../extra/pcre/libpcre.a ../extra/bzip2/libbz2.a
>>> ../extra/intl/libintl.a ../extra/trio/libtrio.a dllversion.o -L. -lg2c
>>> -lRblas - lcomctl32 -lversion
>>> ../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1703):
>>> undefin ed reference to `__pcre_ucp_findprop'
>>> ../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1740):
>>> undefin ed reference to `__pcre_ucp_findprop'
>>> ../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1848):
>>> undefin ed reference to `__pcre_ucp_findprop'
>>> ../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x187f):
>>> undefin ed reference to `__pcre_ucp_findprop'
>>> ../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1966):
>>> undefin ed reference to `__pcre_ucp_findprop'
>>> ../extra/pcre/libpcre.a(pcre_dfa_exec.o):pcre_dfa_exec.c:(.text+0x1f2a):
>>> more un defined references to `__pcre_ucp_findprop' follow
>>> collect2: ld returned 1 exit status
>>> make[4]: *** [R.dll] Erreur 1
>>> make[3]: *** [../../bin/R.dll] Erreur 2
>>> make[2]: *** [rbuild] Erreur 2
>>> make[1]: *** [all] Erreur 2
>>> make[1]: quittant le r?pertoire ?
>>> /home/stephane/Rdev/CrossCompileBuild/WinR/R-2 .4.0/src/gnuwin32 ?
>>> ****************************************
>>> I have read (http://www.murdoch-sutherland.com/Rtools/) that a bug
>>> exists for ld version 2.16.91 20050827 and that Prof Ripley produced a
>>> patched version. It seems that my problem is also related to ld (and I
>>> have the same version). So I wonder if the same bug could be responsible
>>> of the error in the cross-compiler.
>>> Two questions:
>>> - Are they other people which have the same problem when cross-compiling
>>> R on Linux
>>> - Is it possible that the problem is related to ld. If yes, is it
>>> possible to obtain a patched version ?
>>> 
>>> Thanks in advance.
>>> Sincerely,
>>> 
>>> 
>> 
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Tue Jan  9 10:33:13 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Jan 2007 09:33:13 +0000 (GMT)
Subject: [R] A question about R environment
In-Reply-To: <f6868c1546de.45a2e8a6@usc.edu>
References: <dc87d92f2693.45a288d8@usc.edu>
	<971536df0701081824p7f7ab7f5xf4b4c0f1e4ef6856@mail.gmail.com>
	<59d7961d0701082240v284824bfj2e884c065df2c09c@mail.gmail.com>
	<f6868c1546de.45a2e8a6@usc.edu>
Message-ID: <Pine.LNX.4.64.0701090921200.28652@gannet.stats.ox.ac.uk>

On Tue, 9 Jan 2007, Tong Wang wrote:

> Hi,
>   Thanks a lot for your response,   but it is strange that when I do attach() , I got the follow error:
>  Error in attach(e) : attach only works for lists and data frames
>
> Any suggestions on this?

Update your R, as the posting guide asked you to in this circumstance.
This was new in R 2.4.0.

You don't need the baggage of the R.utils package here: the base function 
sys.source loads into an environment.  So the definitive way to do this is

env <- attach(NULL, name = "myenv")
sys.source("some file", env)

and that has worked since well before 2.0.0.

>
>
> tong
>
> ----- Original Message -----
> From: Henrik Bengtsson <hb at stat.berkeley.edu>
> Date: Monday, January 8, 2007 10:40 pm
> Subject: Re: [R] A question about R environment
> To: Gabor Grothendieck <ggrothendieck at gmail.com>
> Cc: Tong Wang <wangtong at usc.edu>, R help <r-help at stat.math.ethz.ch>
>
>> sourceTo() in R.utils will allow you to source() a file into an
>> environment.
>> /Henrik
>>
>> On 1/9/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>>> Try this:
>>>
>>>> e <- new.env()
>>>> e$f <- function(x)x
>>>> attach(e)
>>>> search()
>>>  [1] ".GlobalEnv"        "e"                 "package:stats"
>>>  [4] "package:graphics"  "package:grDevices" "package:utils"
>>>  [7] "package:datasets"  "package:methods"   "Autoloads"
>>> [10] "package:base"
>>>> f
>>> function(x)x
>>>
>>> On 1/8/07, Tong Wang <wangtong at usc.edu> wrote:
>>>> Hi  all,
>>>>
>>>> I created environment  "mytoolbox" by :   mytoolbox <-
>> new.env(parent=baseenv())> > Is there anyway I put it in the search
>> path ?
>>>>
>>>> If you need some background :
>>>>  In a project, I often write some small functions,  and load
>> them into my workspace directly,   so when I list the objects
>>>>  with ls(), it looks pretty messy.  So I am wondering if it is
>> possible to creat an environment,  and put these tools into
>>>>  this environment.  For example,  I have functions    fun1(),
>> fun2() ......    and creat an environment  mytoolbox  which
>>>>  contains all these functions.  And it should be somewhere in
>> the search path:   ".GlobalEnv"        "mytoolbox"
>>      "package:methods"  ........
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-
>> project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html> and provide commented, minimal, self-contained,
>> reproducible code.
>>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From olivier.eterradossi at ema.fr  Tue Jan  9 11:17:52 2007
From: olivier.eterradossi at ema.fr (Olivier ETERRADOSSI)
Date: Tue, 09 Jan 2007 11:17:52 +0100
Subject: [R] R scripts to plot Taylor Diagram
Message-ID: <45A36BD0.4030406@ema.fr>

Happy New Year, dear useRs...and Linda.
I have a small toy-script that plots Taylor Diagrams for vectors, it is 
not wonderful but may help...
perhaps you can change some details for your own needs.
It is far from optimization,... perhaps someone can do this and put it 
into a package ?
Hope this helps.
Regards. Olivier
> # fonction TAYLOR
> # construction d'un diagramme de Taylor
> # Taylor K.E. "Summarizing multiple aspects of model performance in a 
> single diagram"
> # J. Geophys. Res., 106, 7183-7192, 2001
>
> # version 1.0
> # progr. Olivier.Eterradossi, 12/2007
>
> Taylor<-function(ref,batch,add=F,couleur="red"){ # ref, batch : vecteurs
> x<- ref
> y<- batch
>
> grad.corr.full<-c(0,0.2,0.4,0.6,0.8,0.9,0.95,0.99,1)
> grad.corr.lines<-c(0.2,0.4,0.6,0.8,0.9)
>
> R<-cor(x,y,use="pairwise")
>
> sd.r<-sd(x)
> sd.f<-sd(y)
>
> if (add==F) {
> # pourtour du diagramme
> maxray<-1.5*max(sd.f,sd.r)
> plot(c(-maxray,maxray),c(0,maxray),type="n",asp=1,bty="n",xaxt="n",yaxt="n",xlab="",ylab="",main="Taylor 
> Diagram")
> discrete<-seq(180,0,by=-1)
> listepoints<-NULL
> for (i in discrete){
> listepoints<-cbind(listepoints,maxray*cos(i*pi/180),maxray*sin(i*pi/180))
> }
> listepoints<-matrix(listepoints,2,length(listepoints)/2)
> listepoints<-t(listepoints)
> lines(listepoints[,1],listepoints[,2])
>
> # axes x,y
> lines(c(-maxray,maxray),c(0,0))
> lines(c(0,0),c(0,maxray))
>
> # lignes radiales jusqu'? R = +/- 0.8
> for (i in grad.corr.lines){
> lines(c(0,maxray*i),c(0,maxray*sqrt(1-i^2)),lty=3)
> lines(c(0,-maxray*i),c(0,maxray*sqrt(1-i^2)),lty=3)
> }
>
> # texte radial
> for (i in grad.corr.full){
>
> text(1.05*maxray*i,1.05*maxray*sqrt(1-i^2),i,cex=0.6)
> text(-1.05*maxray*i,1.05*maxray*sqrt(1-i^2),-i,cex=0.6)
> }
>
> # sd concentriques autour de la reference
>
> seq.sd<-seq.int(0,2*maxray,by=(maxray/10))
> for (i in seq.sd){
> xcircle<-sd.r+(cos(discrete*pi/180)*i)
> ycircle<-sin(discrete*pi/180)*i
> for (j in 1:length(xcircle)){
> if 
> ((xcircle[j]^2+ycircle[j]^2)<(maxray^2)){points(xcircle[j],ycircle[j], 
> col="darkgreen",pch=".")
> if 
> (j==10){text(xcircle[j],ycircle[j],signif(i,2),cex=0.5,col="darkgreen")}}
> }
> }
>
>
> # sd concentriques autour de l'origine
>
> seq.sd<-seq.int(0,maxray,length.out=5)
> for (i in seq.sd){
> xcircle<-(cos(discrete*pi/180)*i)
> ycircle<-sin(discrete*pi/180)*i
>
> lines(xcircle,ycircle,lty=3,col="blue")
> text(min(xcircle),-0.03*maxray,signif(i,2),cex=0.5,col="blue")
> text(max(xcircle),-0.03*maxray,signif(i,2),cex=0.5,col="blue")
> }
>
> text(0,-0.08*maxray,"Standard Deviation",cex=0.7,col="blue")
> text(0,-0.12*maxray,"Centered RMS Difference",cex=0.7,col="darkgreen")
> points(sd.r,0,pch=22,bg="darkgreen",cex=1.1)
>
> text(0,1.1*maxray,"Correlation Coefficient",cex=0.7)
> }
>
>
> # placer les points
> points(sd.f*cos(acos(R)),sd.f*sin(acos(R)),pch=21,bg=couleur,cex=0.8)
> }
-- 

Olivier ETERRADOSSI
Ma?tre-Assistant
CMGD / Equipe "Propri?t?s Psycho-Sensorielles des Mat?riaux"
Ecole des Mines d'Al?s
H?lioparc, 2 av. P. Angot, F-64053 PAU CEDEX 9
tel std: +33 (0)5.59.30.54.25
nouveau tel direct: +33 (0)5.59.30.90.35 
fax: +33 (0)5.59.30.63.68
http://www.ema.fr


From j.logsdon at quantex-research.com  Tue Jan  9 11:23:09 2007
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Tue, 9 Jan 2007 10:23:09 +0000 (GMT)
Subject: [R] Random effects and level 1 censoring
Message-ID: <Pine.LNX.4.10.10701091022130.17908-100000@mercury.quantex>

I have a question about constructing the likelihood function where there
is censoring at level 1 in a two-level random effects sum.

In a conventional solution, the likelihood function is constructed using
the density for failures and the survivor function for (in this case,
right) censored results.  Within (for example) an R environment, this is
easy to do and gives the same solution as survreg even if it is a little
heavy. 

But where there is an hierarchical situation, we need to consider the
contributions at level 2.  

y_ij=X_ij.beta'+err2_i+err1_ij

If all the units at level 1 for a given level 2 are censored, then the
information we have for the level 2 is itself censored and we should
presumably use the survivor function.  Conversely if none of the units at
level 1 are censored, then the information at level 2 is complete and the
density should be used.

But what do we do if only some of the level 1 units for a given level 2
are censored?  My instinct is to weight the density and survivor functions
for that given level 2 case according to the proportion of level 1
failures.

Am I right?

For a number of reasons I don't want to code for specific distributions
and I am quite happy to use a sledge hammer to crack a walnut with
optim().:) 

Best wishes

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com


From mark_difford at yahoo.co.uk  Tue Jan  9 12:13:41 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Tue, 9 Jan 2007 11:13:41 +0000 (GMT)
Subject: [R] contingency table analysis; generalized linear model
Message-ID: <20070109111341.18024.qmail@web27308.mail.ukl.yahoo.com>

Dear List,

I would appreciate help on the following matter:

I am aware that higher dimensional contingency tables can be analysed using either log-linear models or as a poisson regression using a generalized linear model:

log-linear:
loglm(~Age+Site, data=xtabs(~Age+Site, data=SSites.Rev, drop.unused.levels=T))

GLM:
glm.table <- as.data.frame(xtabs(~Age+Site, data=SSites.Rev, drop.unused.levels=T))
glm(Freq ~ Age + Site, data=glm.table, family='poisson')

where Site is a factor and Age is cast as a factor by xtabs() and treated as such.

**Question**:
Is it acceptable to step away from contingency table analysis by recasting Age as a numerical variable, and redoing the analysis as:

glm(Freq ~ as.numeric(Age) + Site, data=glm.table, family='poisson')

My reasons for wanting to do this are to be able to include non-linear terms in the model, using say restricted or natural cubic splines.

Thank you in advance for your help.
Regards,
Mark Difford.


--------------------------------------------------------------- 
Mark Difford
Ph.D. candidate, Botany Department,
Nelson Mandela Metropolitan University,
Port Elizabeth, SA.


From r_253309 at yahoo.com  Tue Jan  9 13:02:29 2007
From: r_253309 at yahoo.com (r400 r400)
Date: Tue, 9 Jan 2007 04:02:29 -0800 (PST)
Subject: [R] Multivariate OLS
Message-ID: <20070109120229.50416.qmail@web58312.mail.re3.yahoo.com>

Dear David,
 
Thank you very much for this reply. But unfortunately it is not that I want. In my equation 'a' is a vector with length 2, 'b' and 'c' are matrix with row and columns 2. It is like Vector Autoregressive model, but there is some difference between that model and my model. Is there any suggestion?
 
Thanks and regards,

----- Original Message ----
From: David Barron <mothsailor at googlemail.com>
To: r400 r400 <r_253309 at yahoo.com>
Sent: Tuesday, January 9, 2007 3:15:27 PM
Subject: Re: [R] Multivariate OLS


I don't understand the problem; why can't you use lm with a data
frame?  Is something like this what you are after?

dat <- data.frame(y=rnorm(100),x=rnorm(100,5))
dif1 <- diff(dat[,1])
dif2 <- diff(dat[,1],lag=2)

lm(y ~ dif1[-1] + dif2, data=dat[-(1:2),])


On 08/01/07, r400 r400 <r_253309 at yahoo.com> wrote:
> Dear all R users,
>
> Suppose I have a VECTOR of time series y[t] consists of 2000 data point. For example suppose I have data frame which has two columns. First column represents a time series of exchange rate for 2000 days. And the second column represents the price of a commodity for the same period. Now I want to fit a OLS regression like that,
>
> y[t] = a + b*delta[y[t-1]] + c*delta[y[t-2]] + epsilon[t]
>
> as y[t] is not a vector rather a data frame containing two columns I could not use lm() function. Can anyone give me any idea how to do that in R?
>
> Thanks and regards,
> jon
>
> Send instant messages to your online friends http://uk.messenger.yahoo.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From bgreen at dyson.brisnet.org.au  Tue Jan  9 13:16:21 2007
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Tue, 09 Jan 2007 22:16:21 +1000
Subject: [R] logistic regression in R - changing defaults
In-Reply-To: <mailman.11.1165057203.16307.r-help@stat.math.ethz.ch>
Message-ID: <5.1.0.14.0.20061204061255.032715b8@pop3.brisnet.org.au>

Hello,

I was hoping for some advice in changing 2 defaults in a logistic regression.

1. It looks like the first category is the reference category?  In the 
following syntax 'where' has 4 levels, how can I make the reference 
category the third category?

model<- glm(cbind(sucesses, failures) ~ where + who + firstep + dxnarrow + 
age + sex + medyear, family = binomial, data=life.use)
model

2. Is it possible to round results to 4 decimal points? If so, what syntax 
is required?

Any assistance is appreciated,

Bob Green


From dimitris.rizopoulos at med.kuleuven.be  Tue Jan  9 13:35:52 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 9 Jan 2007 13:35:52 +0100
Subject: [R] logistic regression in R - changing defaults
References: <5.1.0.14.0.20061204061255.032715b8@pop3.brisnet.org.au>
Message-ID: <001c01c733ea$b2efeae0$0540210a@www.domain>

an option is to use ?relevel(), e.g.,

life.use$where. <- relevel(life.use$where, 3)
model<- glm(cbind(sucesses, failures) ~ where. + who + firstep + 
dxnarrow +
            age + sex + medyear, family = binomial, data = life.use)
print(model, digits = 1)
print(model, digits = 2)
print(model, digits = 3)
print(model, digits = 4)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Bob Green" <bgreen at dyson.brisnet.org.au>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 09, 2007 1:16 PM
Subject: [R] logistic regression in R - changing defaults


> Hello,
>
> I was hoping for some advice in changing 2 defaults in a logistic 
> regression.
>
> 1. It looks like the first category is the reference category?  In 
> the
> following syntax 'where' has 4 levels, how can I make the reference
> category the third category?
>
> model<- glm(cbind(sucesses, failures) ~ where + who + firstep + 
> dxnarrow +
> age + sex + medyear, family = binomial, data=life.use)
> model
>
> 2. Is it possible to round results to 4 decimal points? If so, what 
> syntax
> is required?
>
> Any assistance is appreciated,
>
> Bob Green
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From dusa.adrian at gmail.com  Tue Jan  9 13:38:16 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Tue, 9 Jan 2007 14:38:16 +0200
Subject: [R] a question of substitute
Message-ID: <200701091438.16341.dusa.adrian@gmail.com>


Hi all,

I want to write a wrapper for an analysis of variance and I face a curious 
problem. Here are two different wrappers:

fun.1 <- function(formula) {
    summary(aov(formula))
}

fun.2 <- function(formula) {
    oneway.test(formula)
}

values <- c(15, 8, 17, 7, 26, 12, 8, 11, 16, 9, 16,
            24, 20, 19, 9, 17, 11, 8, 15, 6, 14)
group <- rep(1:3, each=7)

# While the first wrapper works just fine:
fun.1(values ~ group)

# the second throws an error:
fun.2(values ~ group)
Error in substitute(formula)[[2]] : object is not subsettable

###

I also tried binding the two vectors in a data.frame, with no avail.
I did find a hack, creating two new vectors inside the function and creating a 
fresh formula, so I presume this has something to do with environments.

Could anybody give me a hint on this?
Thank you,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From ggrothendieck at gmail.com  Tue Jan  9 14:14:13 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 9 Jan 2007 08:14:13 -0500
Subject: [R] a question of substitute
In-Reply-To: <200701091438.16341.dusa.adrian@gmail.com>
References: <200701091438.16341.dusa.adrian@gmail.com>
Message-ID: <971536df0701090514q7d732d4by4d327da6cfdd5564@mail.gmail.com>

oneway.test is using substitute on its arguments so its literally
getting formula rather than the value of formula.  Try these:

fun.3 <- function(formula) {
	mc <- match.call()
	mc[[1]] <- as.name("oneway.test")
	eval.parent(mc)
}
fun.3(values ~ group)

fun.4 <- function(formula) {
	do.call(oneway.test, list(formula))
}
fun.4(values ~ group)

On 1/9/07, Adrian Dusa <dusa.adrian at gmail.com> wrote:
>
> Hi all,
>
> I want to write a wrapper for an analysis of variance and I face a curious
> problem. Here are two different wrappers:
>
> fun.1 <- function(formula) {
>    summary(aov(formula))
> }
>
> fun.2 <- function(formula) {
>    oneway.test(formula)
> }
>
> values <- c(15, 8, 17, 7, 26, 12, 8, 11, 16, 9, 16,
>            24, 20, 19, 9, 17, 11, 8, 15, 6, 14)
> group <- rep(1:3, each=7)
>
> # While the first wrapper works just fine:
> fun.1(values ~ group)
>
> # the second throws an error:
> fun.2(values ~ group)
> Error in substitute(formula)[[2]] : object is not subsettable
>
> ###
>
> I also tried binding the two vectors in a data.frame, with no avail.
> I did find a hack, creating two new vectors inside the function and creating a
> fresh formula, so I presume this has something to do with environments.
>
> Could anybody give me a hint on this?
> Thank you,
> Adrian
>
> --
> Adrian Dusa
> Romanian Social Data Archive
> 1, Schitu Magureanu Bd
> 050025 Bucharest sector 5
> Romania
> Tel./Fax: +40 21 3126618 \
>          +40 21 3120210 / int.101
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From zkmetty at gmail.com  Tue Jan  9 14:24:09 2007
From: zkmetty at gmail.com (Zoltan Kmetty)
Date: Tue, 9 Jan 2007 14:24:09 +0100
Subject: [R] memory problem --- use sparse matrices
In-Reply-To: <17826.35570.767005.207748@stat.math.ethz.ch>
References: <c1055ec10701061225o18e66bc9r7b315970dc7d4c36@mail.gmail.com>
	<45A0B260.7030302@statistik.uni-dortmund.de>
	<17826.35570.767005.207748@stat.math.ethz.ch>
Message-ID: <c1055ec10701090524v76cc48b6g57c830afe41271c8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070109/ea009c69/attachment.pl 

From dusa.adrian at gmail.com  Tue Jan  9 14:34:28 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Tue, 9 Jan 2007 15:34:28 +0200
Subject: [R] a question of substitute
In-Reply-To: <971536df0701090514q7d732d4by4d327da6cfdd5564@mail.gmail.com>
References: <200701091438.16341.dusa.adrian@gmail.com>
	<971536df0701090514q7d732d4by4d327da6cfdd5564@mail.gmail.com>
Message-ID: <200701091534.28801.dusa.adrian@gmail.com>

On Tuesday 09 January 2007 15:14, Gabor Grothendieck wrote:
> oneway.test is using substitute on its arguments so its literally
> getting formula rather than the value of formula.

Ah-haa... I understand now. Thanks for the tips, they both work as expected.
Best,
Adrian

> Try these: 
>
> fun.3 <- function(formula) {
> 	mc <- match.call()
> 	mc[[1]] <- as.name("oneway.test")
> 	eval.parent(mc)
> }
> fun.3(values ~ group)
>
> fun.4 <- function(formula) {
> 	do.call(oneway.test, list(formula))
> }
> fun.4(values ~ group)
>
> On 1/9/07, Adrian Dusa <dusa.adrian at gmail.com> wrote:
> > Hi all,
> >
> > I want to write a wrapper for an analysis of variance and I face a
> > curious problem. Here are two different wrappers:
> >
> > fun.1 <- function(formula) {
> >    summary(aov(formula))
> > }
> >
> > fun.2 <- function(formula) {
> >    oneway.test(formula)
> > }
> >
> > values <- c(15, 8, 17, 7, 26, 12, 8, 11, 16, 9, 16,
> >            24, 20, 19, 9, 17, 11, 8, 15, 6, 14)
> > group <- rep(1:3, each=7)
> >
> > # While the first wrapper works just fine:
> > fun.1(values ~ group)
> >
> > # the second throws an error:
> > fun.2(values ~ group)
> > Error in substitute(formula)[[2]] : object is not subsettable
> >
> > ###
> >
> > I also tried binding the two vectors in a data.frame, with no avail.
> > I did find a hack, creating two new vectors inside the function and
> > creating a fresh formula, so I presume this has something to do with
> > environments.
> >
> > Could anybody give me a hint on this?
> > Thank you,
> > Adrian
> >
> > --
> > Adrian Dusa
> > Romanian Social Data Archive
> > 1, Schitu Magureanu Bd
> > 050025 Bucharest sector 5
> > Romania
> > Tel./Fax: +40 21 3126618 \
> >          +40 21 3120210 / int.101
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.

-- 
Adrian Dusa
Arhiva Romana de Date Sociale
Bd. Schitu Magureanu nr.1
050025 Bucuresti sectorul 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From ripley at stats.ox.ac.uk  Tue Jan  9 14:41:09 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Jan 2007 13:41:09 +0000 (GMT)
Subject: [R] a question of substitute
In-Reply-To: <200701091438.16341.dusa.adrian@gmail.com>
References: <200701091438.16341.dusa.adrian@gmail.com>
Message-ID: <Pine.LNX.4.64.0701091329370.16337@gannet.stats.ox.ac.uk>

oneway.test expects a literal formula, not a variable containing a 
formula.  The help page says

  formula: a formula of the form 'lhs ~ rhs' where 'lhs' gives the
           sample values and 'rhs' the corresponding groups.

Furthermore, if you had

foo.2 <- function() oneway.test(value ~ group)

it would still not work, as

     data: an optional matrix or data frame (or similar: see
           'model.frame') containing the variables in the formula
           'formula'.  By default the variables are taken from
           'environment(formula)'.

I could show you several complicated workarounds, but why do you want to 
do this?


On Tue, 9 Jan 2007, Adrian Dusa wrote:

>
> Hi all,
>
> I want to write a wrapper for an analysis of variance and I face a curious
> problem. Here are two different wrappers:
>
> fun.1 <- function(formula) {
>    summary(aov(formula))
> }
>
> fun.2 <- function(formula) {
>    oneway.test(formula)
> }
>
> values <- c(15, 8, 17, 7, 26, 12, 8, 11, 16, 9, 16,
>            24, 20, 19, 9, 17, 11, 8, 15, 6, 14)
> group <- rep(1:3, each=7)
>
> # While the first wrapper works just fine:
> fun.1(values ~ group)
>
> # the second throws an error:
> fun.2(values ~ group)
> Error in substitute(formula)[[2]] : object is not subsettable
>
> ###
>
> I also tried binding the two vectors in a data.frame, with no avail.
> I did find a hack, creating two new vectors inside the function and creating a
> fresh formula, so I presume this has something to do with environments.
>
> Could anybody give me a hint on this?
> Thank you,
> Adrian
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From br44114 at gmail.com  Tue Jan  9 14:50:25 2007
From: br44114 at gmail.com (bogdan romocea)
Date: Tue, 9 Jan 2007 08:50:25 -0500
Subject: [R] Access, Process and Read Information from Web Sites
Message-ID: <8d5a36350701090550q13f73b06n2f1854a09ea5eb25@mail.gmail.com>

Not sure about R, but for a Perl example check
http://yosucker.sourceforge.net/ .


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tudor Bodea
> Sent: Monday, January 08, 2007 11:53 AM
> To: r-help at stat.math.ethz.ch
> Cc: Tudor Bodea
> Subject: [R] Access, Process and Read Information from Web Sites
>
> Dear R useRs,
>
> Does any of you know if it is possible to access a web site (e.g.,
> www.marriott.com), fill in the requested information (e.g.,
> city, check-in
> date, etc), and save the results (e.g., room availability and
> room rates) in
> text files through R? I started with url and url.show but it
> seems that this
> does not do what I would like to do. Any lead would be
> greatly appreciated.
> Thanks.
>
> Tudor
>
> --
> Tudor Dan Bodea
> Ph.D. Candidate
> Georgia Institute of Technology
> School of Civil and Environmental Engineering
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dusa.adrian at gmail.com  Tue Jan  9 14:54:35 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Tue, 9 Jan 2007 15:54:35 +0200
Subject: [R] a question of substitute
In-Reply-To: <Pine.LNX.4.64.0701091329370.16337@gannet.stats.ox.ac.uk>
References: <200701091438.16341.dusa.adrian@gmail.com>
	<Pine.LNX.4.64.0701091329370.16337@gannet.stats.ox.ac.uk>
Message-ID: <200701091554.35442.dusa.adrian@gmail.com>

On Tuesday 09 January 2007 15:41, Prof Brian Ripley wrote:
> oneway.test expects a literal formula, not a variable containing a
> formula.  The help page says
>
>   formula: a formula of the form 'lhs ~ rhs' where 'lhs' gives the
>            sample values and 'rhs' the corresponding groups.
>
> Furthermore, if you had
>
> foo.2 <- function() oneway.test(value ~ group)
>
> it would still not work, as
>
>      data: an optional matrix or data frame (or similar: see
>            'model.frame') containing the variables in the formula
>            'formula'.  By default the variables are taken from
>            'environment(formula)'.
>
> I could show you several complicated workarounds, but why do you want to
> do this?

Thank you for your reply. The data argument was exactly the next problem I 
faced. My workaround involves checking if(missing(data)) then uses different 
calls to oneway.test(). I am certainly interested in other solutions, this 
one is indeed limited.

I do this for the students in the anova class, checking first the homogeneity 
of variances with fligner.test(), printing the p.value and based on that 
changing the var.equal argument in the oneway.test()
It's just for convenience, but they do like having it all-in-one.

Best regards,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From milton_ruser at yahoo.com.br  Tue Jan  9 15:23:02 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Tue, 9 Jan 2007 14:23:02 +0000 (GMT)
Subject: [R] min() return factor class values
Message-ID: <576027.50641.qm@web56603.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070109/d9ceecae/attachment.pl 

From fararooei at yahoo.com  Tue Jan  9 15:27:28 2007
From: fararooei at yahoo.com (mohammad frarouei)
Date: Tue, 9 Jan 2007 06:27:28 -0800 (PST)
Subject: [R] limitation in the number of covariates in nlme
In-Reply-To: <Pine.LNX.4.64.0701081633480.27068@gannet.stats.ox.ac.uk>
Message-ID: <544363.97393.qm@web56809.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070109/441d7fb3/attachment.pl 

From fjbuch at gmail.com  Tue Jan  9 15:29:51 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Tue, 9 Jan 2007 09:29:51 -0500
Subject: [R] dimensions of a all objects
Message-ID: <bd93cdad0701090629g5b9e563ete6bb8bc905ceac36@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070109/638b4aa0/attachment.pl 

From richard.gott at dur.ac.uk  Tue Jan  9 15:40:53 2007
From: richard.gott at dur.ac.uk (R Gott)
Date: Tue, 09 Jan 2007 14:40:53 +0000
Subject: [R] RSQLite NA on input
Message-ID: <1168353653.3386.13.camel@home>

Hello

I haev some .csv data files with missing values - eg below

1,'F','C04','X100',20.93,'C','B',7,8,7.5,2421,2230,2230,2,1,85,43,85,46,48,60

If I have a missing value - so file looks like ,85,46,,48, etc
then RSQLite reads it as zero.  ,85,46,0,48, etc
I need it read it as NA.

Tried various combinations with no success, adn found nothing on help
site.  Must be somehting very simple but . . . .

Help much appreciated.

Richard.


From Thierry.ONKELINX at inbo.be  Tue Jan  9 15:47:38 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 9 Jan 2007 15:47:38 +0100
Subject: [R] dimensions of a all objects
In-Reply-To: <bd93cdad0701090629g5b9e563ete6bb8bc905ceac36@mail.gmail.com>
Message-ID: <2E9C414912813E4EB981326983E0A1040273DE69@inboexch.inbo.be>

You need something like this: 
sapply(objects() , function(x)(dim(eval(parse(text = x)))))

a <- rnorm(1)
b <- matrix(rnorm(4), ncol = 2, nrow = 2)
sapply(objects() , function(x)(dim(eval(parse(text = x)))))

$a
NULL

$b
[1] 2 2

Cheers,

Thierry

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens Farrel Buchinsky
Verzonden: dinsdag 9 januari 2007 15:30
Aan: r-help op stat.math.ethz.ch
Onderwerp: [R] dimensions of a all objects

Why will the following command not work
sapply(objects(),dim)
What does it say about the objects list? What does it say about the dim
command?

Likewise, the following also does not work
all<-ls()
for (f in all) print(dim(f))
-- 
Farrel Buchinsky

	[[alternative HTML version deleted]]

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From P.Dalgaard at biostat.ku.dk  Tue Jan  9 15:47:57 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 09 Jan 2007 15:47:57 +0100
Subject: [R] min() return factor class values
In-Reply-To: <576027.50641.qm@web56603.mail.re3.yahoo.com>
References: <576027.50641.qm@web56603.mail.re3.yahoo.com>
Message-ID: <45A3AB1D.8070505@biostat.ku.dk>

Milton Cezar Ribeiro wrote:
> Hi R-friends
>    
>   I don?t know why the "min()" function below return the min value as factor. When i force the aicc.min using a as.numeric() function, it return a factor index (1,2,..) and not min value as I want. By the way, I included a sessionInfo() at the end of this e-mail.
>   
min() is not doing anything out of the ordinary, but cbind'ing it with
the character vector sp coerces it to character and rbind'ing to a data
frame turns character vectors into factors...

The whole thing looks like it could be a straightforward application of
aggregate().
>    
>   In fact I had the same problem (values as factor) on other part of my script and I noticed that it occour when I use "cbind()". It is real?
>    
>   Any idea? 
>    
>   Kind regards,
>    
>   Miltinho
>    
>   > especies.aicc.min<-data.frame()
>   
>> for (sp in levels(especies.aicc$especie)) 
>>     
> + {
> + sele<-subset(especies.aicc,especie==sp)
> + especies.aicc.min<-rbind(especies.aicc.min,cbind(sp,aicc.min=min(sele$aicc)))
> + }
>   
>> especies.aicc.min
>>     
>                         sp         aicc.min
> 1             Attila.rufus  6.7387056413613
> 2 Automolus.leucophthalmus 125.791300522824
>   
>> class(especies.aicc.min$aicc.min)
>>     
> [1] "factor"
>   
> -----------
>   > sessionInfo()
> R version 2.4.0 (2006-10-03) 
> i386-pc-mingw32 
>   locale:
> LC_COLLATE=English_Jamaica.1252;LC_CTYPE=English_Jamaica.1252;LC_MONETARY=English_Jamaica.1252;LC_NUMERIC=C;LC_TIME=English_Jamaica.1252
>   attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"  "base"     
>   
>
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From b.rowlingson at lancaster.ac.uk  Tue Jan  9 15:53:05 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 09 Jan 2007 14:53:05 +0000
Subject: [R] dimensions of a all objects
In-Reply-To: <bd93cdad0701090629g5b9e563ete6bb8bc905ceac36@mail.gmail.com>
References: <bd93cdad0701090629g5b9e563ete6bb8bc905ceac36@mail.gmail.com>
Message-ID: <45A3AC51.7020304@lancaster.ac.uk>

Farrel Buchinsky wrote:
> Why will the following command not work
> sapply(objects(),dim)
> What does it say about the objects list? What does it say about the dim
> command?
> 
> Likewise, the following also does not work
> all<-ls()
> for (f in all) print(dim(f))

   'objects()' returns character strings - the names of objects - then 
the dim of the character strings are all NULL.

  I'll assume that's what you are getting at - you've not posted an 
example or the output you are getting or why it 'does not work'.

  Maybe you want this:
  > sapply(objects(),function(x){dim(get(x))})
  $f
  NULL

  $m
  [1] 2 5

  $x
  NULL

  $y
  [1] 5 2

  - where m and y are matrices, f is a function, x is a scalar.

Barry


From zkmetty at gmail.com  Tue Jan  9 15:57:27 2007
From: zkmetty at gmail.com (Zoltan Kmetty)
Date: Tue, 9 Jan 2007 15:57:27 +0100
Subject: [R] min() return factor class values
In-Reply-To: <576027.50641.qm@web56603.mail.re3.yahoo.com>
References: <576027.50641.qm@web56603.mail.re3.yahoo.com>
Message-ID: <c1055ec10701090657k24c7cc74kde4102da46c7d3df@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070109/588d431e/attachment.pl 

From maechler at stat.math.ethz.ch  Tue Jan  9 16:09:20 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 9 Jan 2007 16:09:20 +0100
Subject: [R] dimensions of a all objects
In-Reply-To: <45A3AC51.7020304@lancaster.ac.uk>
References: <bd93cdad0701090629g5b9e563ete6bb8bc905ceac36@mail.gmail.com>
	<45A3AC51.7020304@lancaster.ac.uk>
Message-ID: <17827.45088.826732.637098@stat.math.ethz.ch>

>>>>> "BaRow" == Barry Rowlingson <b.rowlingson at lancaster.ac.uk>
>>>>>     on Tue, 09 Jan 2007 14:53:05 +0000 writes:

    BaRow> Farrel Buchinsky wrote:
    >> Why will the following command not work
    >> sapply(objects(),dim)
    >> What does it say about the objects list? What does it say about the dim
    >> command?
    >> 
    >> Likewise, the following also does not work
    >> all<-ls()
    >> for (f in all) print(dim(f))

    BaRow> 'objects()' returns character strings - the names of objects - then 
    BaRow> the dim of the character strings are all NULL.

    BaRow> I'll assume that's what you are getting at - you've not posted an 
    BaRow> example or the output you are getting or why it 'does not work'.

    BaRow> Maybe you want this:
    >> sapply(objects(),function(x){dim(get(x))})
    BaRow> $f
    BaRow> NULL

    BaRow> $m
    BaRow> [1] 2 5

    BaRow> $x
    BaRow> NULL

    BaRow> $y
    BaRow> [1] 5 2

    BaRow> - where m and y are matrices, f is a function, x is a scalar.


Yes.
Since he's just interested in "print"ing, maybe

   ls.str()  # would be even more revealing (or maybe too
	     #   confusing for a newbie)

Martin


From walter.durka at ufz.de  Tue Jan  9 16:15:44 2007
From: walter.durka at ufz.de (Walter Durka)
Date: Tue, 09 Jan 2007 16:15:44 +0100
Subject: [R] posthoc tests with ANCOVA
Message-ID: <45A3B1A0.7070701@ufz.de>

dear all,

I want to perform a posthoc test for my ANCOVA:
a1<-aov(seeds~treatment*length)

With
summary(glht(a1, linfct = mcp(treatment = "Tukey")))
R tells me: "covariate interactions found -- please choose appropriate 
contrast"

How do I build these contrasts?

Ideally, I would like to have the posthoc test for the ANCOVA including 
a block-effect
a2<-aov(seeds~treatment*length+Error(site))

How do I make a posthoc test here?

Thanks for any comments
Walter


-- 

*****
Dr. Walter Durka
Department Bioz?noseforschung
Department of community ecology

Helmholtz-Zentrum f?r Umweltforschung GmbH - UFZ
Helmholtz Centre for Environmental Research - UFZ
Theodor-Lieser-Str. 4 / 06120 Halle / Germany

walter.durka at ufz.de / http://www.ufz.de/index.php?en=798
phone +49 345 558 5314 / Fax +49 345 558 5329

+++++++++++++++++++++++++++++++++++++++++++++++++

Das UFZ hat einen neuen Namen:
Helmholtz-Zentrum f?r Umweltforschung GmbH - UFZ

The UFZ has a new name:
Helmholtz Centre for Environmental Research - UFZ

+++++++++++++++++++++++++++++++++++++++++++++++++


From fjbuch at gmail.com  Tue Jan  9 16:16:14 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Tue, 9 Jan 2007 10:16:14 -0500
Subject: [R] dimensions of a all objects
In-Reply-To: <45A3AC51.7020304@lancaster.ac.uk>
References: <bd93cdad0701090629g5b9e563ete6bb8bc905ceac36@mail.gmail.com>
	<45A3AC51.7020304@lancaster.ac.uk>
Message-ID: <bd93cdad0701090716h64044a27hedfbe5c6dcf46316@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070109/f23d2be7/attachment.pl 

From sfalcon at fhcrc.org  Tue Jan  9 16:19:42 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 09 Jan 2007 07:19:42 -0800
Subject: [R] RSQLite NA on input
In-Reply-To: <1168353653.3386.13.camel@home> (R. Gott's message of "Tue,
	09 Jan 2007 14:40:53 +0000")
References: <1168353653.3386.13.camel@home>
Message-ID: <m27ivw5isx.fsf@fhcrc.org>

Hi Richard,

It would help if you provided a bit more on how you are going about
the import (along with your version of R and RSQLite).

R Gott <richard.gott at dur.ac.uk> writes:
> I haev some .csv data files with missing values - eg below
>
> 1,'F','C04','X100',20.93,'C','B',7,8,7.5,2421,2230,2230,2,1,85,43,85,46,48,60
>
> If I have a missing value - so file looks like ,85,46,,48, etc
> then RSQLite reads it as zero.  ,85,46,0,48, etc
> I need it read it as NA.
>
> Tried various combinations with no success, adn found nothing on help
> site.  Must be somehting very simple but . . . .
>
> Help much appreciated.

The most flexible approach would be to use read.table to read in your
csv file.  Then use dbWriteTable to put the resulting data.frame
object into the DB.

+ seth


From news at aspden.com  Tue Jan  9 16:22:19 2007
From: news at aspden.com (John Lawrence Aspden)
Date: Tue, 09 Jan 2007 15:22:19 +0000
Subject: [R] scripts with littler
References: <ent9nm$rc6$1@sea.gmane.org>
	<20070108132920.GA4640@phenix.progiciels-bpi.ca>
	<entlul$ng$1@sea.gmane.org>
	<971536df0701080703q48e04a5cv96720798b8b30c11@mail.gmail.com>
	<Pine.LNX.4.64.0701082235040.9254@gannet.stats.ox.ac.uk>
Message-ID: <eo0bvb$sr0$1@sea.gmane.org>

Guys, thanks very much for your help. Rscript looks great and I'll look
forward to it.

The /usr/bin/env thing seems to be a general difficulty with the mechanism.
One's first thought has to be to modify env to parse and then pass the
arguments in the expected way (maybe #!/usr/bin/env2?), and of course one's
second is that this must already have been done...

An S of TFW produces 'Citizens for a better env', but this isn't as hopeful
as it sounds.

I'm actually tempted to use

#!/usr/bin/env r
rm(list=ls())

as the first two lines of every script, rather than messing about trying to
pass options.

Cheers, John.


-- 
Contractor in Cambridge UK -- http://www.aspden.com


From christoph.heibl at gmx.net  Tue Jan  9 16:26:18 2007
From: christoph.heibl at gmx.net (Christoph Heibl)
Date: Tue, 9 Jan 2007 16:26:18 +0100
Subject: [R] manipulating  elements of lists
Message-ID: <13EB6AF4-795F-4A08-9B99-A4C67C961506@gmx.net>

I want to manipulate lists as described below:
Imagine these two lists:

 > list1
$WR7
[1] 1 2 3 4

$YH5YH6
[1] 3 4 5 6 7

$YH4
[1] 4 5

$UC4UC8
[1] 4 5 6 7 8 9

 > list2
           V1             	V2
1        WR7       	Averrhoa
2         ?     		Sarcotheca
3     	YH5YH6        	caesia
4        YH4      		arbuscula
5     UC4UC8          	rosea
6          ?     		acetosella

How can I exchange the names(list1) by the entries in the second  
column of list2,
if (a) length(list1) ? length(list2) and (b) the elements of both  
lists are not in the same order? Is there a easy way to do this?

Thank you!




________________________________________________________

Christoph Heibl

PhD student

'Phylogenetics and phylogeography of endemic Atacama Desert flora'

Systematic Botany
Ludwig-Maximilians-Universit?t M?nchen
Menzinger Str. 67
D-80638 M?nchen
GERMANY

phone:     +49-(0)89-17861-251
e-mail:    heibl at lmu.de


From sfalcon at fhcrc.org  Tue Jan  9 16:29:03 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 09 Jan 2007 07:29:03 -0800
Subject: [R] listing all functions in R
In-Reply-To: <enu91s$ep7$1@sea.gmane.org> (Earl F. Glynn's message of "Mon,
	8 Jan 2007 14:20:09 -0600")
References: <1168088147.3149.6.camel@dhcppc2.my.nat.localnet>
	<Pine.LNX.4.64.0701061331420.8651@gannet.stats.ox.ac.uk>
	<enu91s$ep7$1@sea.gmane.org>
Message-ID: <m23b6k5idc.fsf@fhcrc.org>

"Earl F. Glynn" <efg at stowers-institute.org> writes:
> "Prof Brian Ripley" <ripley at stats.ox.ac.uk> wrote in message 
> news:Pine.LNX.4.64.0701061331420.8651 at gannet.stats.ox.ac.uk...
>> Here is a reasonable shot:
>>
>> findfuns <- function(x) {
>>     if(require(x, character.only=TRUE)) {
>>        env <- paste("package", x, sep=":")
>>        nm <- ls(env, all=TRUE)
>>        nm[unlist(lapply(nm, function(n) exists(n, where=env,
>>                                               mode="function",
>>                                               inherits=FALSE)))]
>>     } else character(0)
>> }
>> pkgs <- dir(.Library)
>> z <-  lapply(pkgs, findfuns)
>> names(z) <- pkgs
>
> Any recommendations on how to trap problems with "require" when using 
> findfuns?  One bad package and the lapply above doesn't return
> anything.

Are you sure you need to?  I just tried your code above with:

pkgs <- c("Biobase", "GOstats", "flrblr", "bazbaz")

And while I see warning messages about the flrblr and bazbaz packages,
the function completed and I get the expected results in z.

Oh, perhaps you have some broken installs?  Broken in the sense that
you have a package installed but not its dependencies?

How about this:

safeRequire <- function(x) {
    tryCatch(require(x, character.only=TRUE),
             error=function(e) FALSE)
}

And then replace the call to require in findfuns().

+ seth


From ggrothendieck at gmail.com  Tue Jan  9 16:38:35 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 9 Jan 2007 10:38:35 -0500
Subject: [R] dimensions of a all objects
In-Reply-To: <bd93cdad0701090716h64044a27hedfbe5c6dcf46316@mail.gmail.com>
References: <bd93cdad0701090629g5b9e563ete6bb8bc905ceac36@mail.gmail.com>
	<45A3AC51.7020304@lancaster.ac.uk>
	<bd93cdad0701090716h64044a27hedfbe5c6dcf46316@mail.gmail.com>
Message-ID: <971536df0701090738p40b4068fh83b64ad3a7ae2a5c@mail.gmail.com>

See below.

On 1/9/07, Farrel Buchinsky <fjbuch at gmail.com> wrote:
> You have assumed everything as I meant it.
> I understand what is happening now. ls() simply creates a vector with
> character elements and dim() sees each element as not having dimensions. The
> critical part of what you have shown is the get(command) that turns what is
> just a string into the dataframe or vector whose name is the string. The
> other issue which you showed, and one that I have come across before is that
> sapply and tapply and lapply cannot handle a function on a function. I would
> have thought that I should get the same result from
>
> lapply(ls(),dim(get())) or something such as that.

The gsubfn package can do nearly that. Just preface the function
of interest (in this case sapply) with fn$ and then you can
write the function as a formula:

> library(gsubfn)
> fn$sapply(c("iris", "CO2"), ~ dim(get(x)), simplify = FALSE)
$iris
[1] 150   5

$CO2
[1] 84  5

>
> But instead one has to create a function command within the lapply to handle
> a dimension command upon a get command.
>
>
> On 1/9/07, Barry Rowlingson <b.rowlingson at lancaster.ac.uk> wrote:
> >
> > Farrel Buchinsky wrote:
> > > Why will the following command not work
> > > sapply(objects(),dim)
> > > What does it say about the objects list? What does it say about the dim
> > > command?
> > >
> > > Likewise, the following also does not work
> > > all<-ls()
> > > for (f in all) print(dim(f))
> >
> >   'objects()' returns character strings - the names of objects - then
> > the dim of the character strings are all NULL.
> >
> > I'll assume that's what you are getting at - you've not posted an
> > example or the output you are getting or why it 'does not work'.
> >
> > Maybe you want this:
> > > sapply(objects(),function(x){dim(get(x))})
> > $f
> > NULL
> >
> > $m
> > [1] 2 5
> >
> > $x
> > NULL
> >
> > $y
> > [1] 5 2
> >
> > - where m and y are matrices, f is a function, x is a scalar.
> >
> > Barry
> >
> >
>
>
> --
> Farrel Buchinsky
> Mobile: (412) 779-1073
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Max.Kuhn at pfizer.com  Tue Jan  9 16:39:27 2007
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Tue, 9 Jan 2007 10:39:27 -0500
Subject: [R] odfWeave and figures in MS Word Format
In-Reply-To: <45A29081.2060004@free.fr>
Message-ID: <71257D09F114DA4A8E134DEAC70F25D3072F142F@groamrexm03.amer.pfizer.com>

Laurent,

Since you question was not about odfWeave (despite the message title),
it would have been better to send this question to an OpenOffice mailing
list.

That said, I think that the issues was your workflow (odt -> html ->
doc). HTML is plain text, so when you saved the file in that format the
image files are saved separately. Links are creating in the html file to
the image files. When you converted the html to Word format, the images
are still linked (not embedded into the Word document). When you move
the word file to a different location, the links are broken and you
won't be able to see the images. Rtf worked because the images are
embedded into the rtf file. odt -> doc works just fine with images, so
that would be a better idea.

Max


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Laurent Rhelp
Sent: Monday, January 08, 2007 1:42 PM
To: R-help at stat.math.ethz.ch
Subject: [R] odfWeave and figures in MS Word Format

I answer to myself.
I understood my error : first of all, we have to save the file in the 
.rtf format !
Then, from the rtf file, we can generate the file in the .doc format.

I am sorry for my question.

Thanks

Laurent

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From milton_ruser at yahoo.com.br  Tue Jan  9 16:51:18 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Tue, 9 Jan 2007 15:51:18 +0000 (GMT)
Subject: [R] min() return factor class values
In-Reply-To: <45A3AB1D.8070505@biostat.ku.dk>
Message-ID: <20070109155118.96009.qmail@web56606.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070109/a26808b1/attachment.pl 

From P.Dalgaard at biostat.ku.dk  Tue Jan  9 17:03:02 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 09 Jan 2007 17:03:02 +0100
Subject: [R] min() return factor class values
In-Reply-To: <20070109155118.96009.qmail@web56606.mail.re3.yahoo.com>
References: <20070109155118.96009.qmail@web56606.mail.re3.yahoo.com>
Message-ID: <45A3BCB6.4010400@biostat.ku.dk>

Milton Cezar Ribeiro wrote:
> Dear Peter,
>    
>   I tryed something like 
>    
>   >head(especies.aicc)
>          especie     aicc
> 1 Attila.rufus 17.15934
> 2 Attila.rufus 11.41371
> 3 Attila.rufus 11.41371
> 4 Attila.rufus 19.55998
> 5 Attila.rufus 17.23780
> 6 Attila.rufus 19.22545
>    
>   > especies.min<-aggregate.data.frame(especies.aicc,list
>              (Especie=especies.aicc$especie),max)
>   
Make sure to aggregate only the part of your data frame that is numeric:

> x <- read.table(stdin())

0:          especie     aicc

1: 1 Attila.rufus 17.15934

2: 2 Attila.rufus 11.41371

3: 3 Attila.rufus 11.41371

4: 4 Attila.rufus 19.55998

5: 5 Attila.rufus 17.23780

6: 6 Attila.rufus 19.22545

7:

> aggregate(x[2], list(x$especie), min)

       Group.1     aicc

1 Attila.rufus 11.41371

> aggregate(x[2], list(x$especie), max)

       Group.1     aicc

1 Attila.rufus 19.55998

> aggregate(x, list(x$especie), max) # this breaks

Error in Summary.factor(..., na.rm = na.rm) :

        max not meaningful for factors

>    
>   But it works fine only for "mean" FUN and not for "min" and "max". Also also, when I use "mean" I got the following warnings:
>    
>   > especies.min<-aggregate.data.frame(especies.aicc,list 
>          (Especie=especies.aicc$especie),mean)
>
>   Warning messages:
> 1: argument is not numeric or logical: returning NA in: mean.default(X[[1]], ...) 
> 2: argument is not numeric or logical: returning NA in: mean.default(X[[2]], ...) 
>    
>   In fact I need only min() and max().
>    
>   Miltinho
>   -----------------
>
> Peter Dalgaard <P.Dalgaard at biostat.ku.dk> escreveu:
>   Milton Cezar Ribeiro wrote:
>   
>> Hi R-friends
>>
>> I don?t know why the "min()" function below return the min value as factor. When i force the aicc.min using a as.numeric() function, it return a factor index (1,2,..) and not min value as I want. By the way, I included a sessionInfo() at the end of this e-mail.
>>
>>     
> min() is not doing anything out of the ordinary, but cbind'ing it with
> the character vector sp coerces it to character and rbind'ing to a data
> frame turns character vectors into factors...
>
> The whole thing looks like it could be a straightforward application of
> aggregate().
>   
>> In fact I had the same problem (values as factor) on other part of my script and I noticed that it occour when I use "cbind()". It is real?
>>
>> Any idea? 
>>
>> Kind regards,
>>
>> Miltinho
>>
>>     
>>> especies.aicc.min<-data.frame()
>>>       
>>> for (sp in levels(especies.aicc$especie)) 
>>>
>>>       
>> + {
>> + sele<-subset(especies.aicc,especie==sp)
>> + especies.aicc.min<-rbind(especies.aicc.min,cbind(sp,aicc.min=min(sele$aicc)))
>> + }
>>
>>     
>>> especies.aicc.min
>>>
>>>       
>> sp aicc.min
>> 1 Attila.rufus 6.7387056413613
>> 2 Automolus.leucophthalmus 125.791300522824
>>
>>     
>>> class(especies.aicc.min$aicc.min)
>>>
>>>       
>> [1] "factor"
>>
>> -----------
>>     
>>> sessionInfo()
>>>       
>> R version 2.4.0 (2006-10-03) 
>> i386-pc-mingw32 
>> locale:
>> LC_COLLATE=English_Jamaica.1252;LC_CTYPE=English_Jamaica.1252;LC_MONETARY=English_Jamaica.1252;LC_NUMERIC=C;LC_TIME=English_Jamaica.1252
>> attached base packages:
>> [1] "methods" "stats" "graphics" "grDevices" "utils" "datasets" "base" 
>>
>>
>>
>>     
>
>
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From dickgiesser at gmail.com  Tue Jan  9 17:47:51 2007
From: dickgiesser at gmail.com (Benjamin Dickgiesser)
Date: Tue, 9 Jan 2007 16:47:51 +0000
Subject: [R] Logical operations or selecting data from data.frames
Message-ID: <b75d67340701090847j30da3959yc95d694abf0925dd@mail.gmail.com>

Hi all,

why doesn't something like this does not work?

speedy <-
	(sdata$VaR < sdata$DdtdAbs) && sdata$DdtdDuration >= qpois(pct,lambda) &&
	sdata$Ddtd > MinDD

or sdata$Ddtd[sdata$Ddtd > 0 && sdata$VaR < sdata$DdtdAbs]

sdata looks like this:

           dataId       date  value Ddtd VaR DdtdAbs DdtdDuration
18948  79637 2004-07-27 10085.10           NA         NA    0.00            0
18949  79638 2004-07-28 10117.10           NA         NA    0.00            0
18950  79639 2004-07-29 10129.20           NA         NA    0.00            0
18951  79640 2004-07-30 10139.70           NA         NA    0.00            0
18952  79641 2004-08-02 10179.20           NA         NA    0.00            0
18953  79642 2004-08-03 10120.20  0.579613329 336.060090   59.00            1
18954  79643 2004-08-04 10126.50           NA         NA    0.00            0
18955  79644 2004-08-05  9963.03  1.614279366 334.306978  163.47            1
18956  79645 2004-08-06  9815.33  3.072828717 386.173057  311.17            2
18957  79646 2004-08-09  9814.66  3.079445020 420.167049  311.84            3
18958  79647 2004-08-10  9944.67           NA         NA    0.00            0
18959  79648 2004-08-11  9938.32  0.063853300 328.315992    6.35            1
18960  79649 2004-08-12  9814.59  1.308037371 379.182568  130.08            2

I am trying to select rows from the data.frame which have Ddtd > x,
VaR < DdtdAbs and DdtdDuration > z.

Thank you,
Benjamin


From dickgiesser at gmail.com  Tue Jan  9 18:06:25 2007
From: dickgiesser at gmail.com (Benjamin Dickgiesser)
Date: Tue, 9 Jan 2007 17:06:25 +0000
Subject: [R] Logical operations or selecting data from data.frames
In-Reply-To: <b75d67340701090847j30da3959yc95d694abf0925dd@mail.gmail.com>
References: <b75d67340701090847j30da3959yc95d694abf0925dd@mail.gmail.com>
Message-ID: <b75d67340701090906w256f92a0g96aead43ab12edd0@mail.gmail.com>

I suppose this doesn't work for the same reason as
sdata$VaR < sdata$DdtdAbs && sdata$DdtdDuration >= 1

does only return  FALSE and not a vector of TRUE and FALSE as

sdata$VaR < sdata$DdtdAbs

would return.

Is there a ways around this?
Benjamin

On 1/9/07, Benjamin Dickgiesser <dickgiesser at gmail.com> wrote:
> Hi all,
>
> why doesn't something like this does not work?
>
> speedy <-
>         (sdata$VaR < sdata$DdtdAbs) && sdata$DdtdDuration >= qpois(pct,lambda) &&
>         sdata$Ddtd > MinDD
>
> or sdata$Ddtd[sdata$Ddtd > 0 && sdata$VaR < sdata$DdtdAbs]
>
> sdata looks like this:
>
>            dataId       date  value Ddtd VaR DdtdAbs DdtdDuration
> 18948  79637 2004-07-27 10085.10           NA         NA    0.00            0
> 18949  79638 2004-07-28 10117.10           NA         NA    0.00            0
> 18950  79639 2004-07-29 10129.20           NA         NA    0.00            0
> 18951  79640 2004-07-30 10139.70           NA         NA    0.00            0
> 18952  79641 2004-08-02 10179.20           NA         NA    0.00            0
> 18953  79642 2004-08-03 10120.20  0.579613329 336.060090   59.00            1
> 18954  79643 2004-08-04 10126.50           NA         NA    0.00            0
> 18955  79644 2004-08-05  9963.03  1.614279366 334.306978  163.47            1
> 18956  79645 2004-08-06  9815.33  3.072828717 386.173057  311.17            2
> 18957  79646 2004-08-09  9814.66  3.079445020 420.167049  311.84            3
> 18958  79647 2004-08-10  9944.67           NA         NA    0.00            0
> 18959  79648 2004-08-11  9938.32  0.063853300 328.315992    6.35            1
> 18960  79649 2004-08-12  9814.59  1.308037371 379.182568  130.08            2
>
> I am trying to select rows from the data.frame which have Ddtd > x,
> VaR < DdtdAbs and DdtdDuration > z.
>
> Thank you,
> Benjamin
>


From bcarvalh at jhsph.edu  Tue Jan  9 18:09:52 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Tue, 9 Jan 2007 12:09:52 -0500
Subject: [R] min() return factor class values
In-Reply-To: <20070109155118.96009.qmail@web56606.mail.re3.yahoo.com>
References: <20070109155118.96009.qmail@web56606.mail.re3.yahoo.com>
Message-ID: <51B1AFE5-A9B6-4EF6-9741-531D75B7D322@jhsph.edu>

Milton,

have you looked at the structure of your data.frame?

str(especies.aicc)

Are you sure especies.aicc is defined as numeric?

b

On Jan 9, 2007, at 10:51 AM, Milton Cezar Ribeiro wrote:

> Dear Peter,
>
>   I tryed something like
>
>> head(especies.aicc)
>          especie     aicc
> 1 Attila.rufus 17.15934
> 2 Attila.rufus 11.41371
> 3 Attila.rufus 11.41371
> 4 Attila.rufus 19.55998
> 5 Attila.rufus 17.23780
> 6 Attila.rufus 19.22545
>
>> especies.min<-aggregate.data.frame(especies.aicc,list
>              (Especie=especies.aicc$especie),max)
>
>   But it works fine only for "mean" FUN and not for "min" and  
> "max". Also also, when I use "mean" I got the following warnings:
>
>> especies.min<-aggregate.data.frame(especies.aicc,list
>          (Especie=especies.aicc$especie),mean)
>
>   Warning messages:
> 1: argument is not numeric or logical: returning NA in: mean.default 
> (X[[1]], ...)
> 2: argument is not numeric or logical: returning NA in: mean.default 
> (X[[2]], ...)
>
>   In fact I need only min() and max().
>
>   Miltinho
>   -----------------
>
> Peter Dalgaard <P.Dalgaard at biostat.ku.dk> escreveu:
>   Milton Cezar Ribeiro wrote:
>> Hi R-friends
>>
>> I don?t know why the "min()" function below return the min value  
>> as factor. When i force the aicc.min using a as.numeric()  
>> function, it return a factor index (1,2,..) and not min value as I  
>> want. By the way, I included a sessionInfo() at the end of this e- 
>> mail.
>>
> min() is not doing anything out of the ordinary, but cbind'ing it with
> the character vector sp coerces it to character and rbind'ing to a  
> data
> frame turns character vectors into factors...
>
> The whole thing looks like it could be a straightforward  
> application of
> aggregate().
>>
>> In fact I had the same problem (values as factor) on other part of  
>> my script and I noticed that it occour when I use "cbind()". It is  
>> real?
>>
>> Any idea?
>>
>> Kind regards,
>>
>> Miltinho
>>
>>> especies.aicc.min<-data.frame()
>>
>>> for (sp in levels(especies.aicc$especie))
>>>
>> + {
>> + sele<-subset(especies.aicc,especie==sp)
>> + especies.aicc.min<-rbind(especies.aicc.min,cbind(sp,aicc.min=min 
>> (sele$aicc)))
>> + }
>>
>>> especies.aicc.min
>>>
>> sp aicc.min
>> 1 Attila.rufus 6.7387056413613
>> 2 Automolus.leucophthalmus 125.791300522824
>>
>>> class(especies.aicc.min$aicc.min)
>>>
>> [1] "factor"
>>
>> -----------
>>> sessionInfo()
>> R version 2.4.0 (2006-10-03)
>> i386-pc-mingw32
>> locale:
>> LC_COLLATE=English_Jamaica.1252;LC_CTYPE=English_Jamaica. 
>> 1252;LC_MONETARY=English_Jamaica. 
>> 1252;LC_NUMERIC=C;LC_TIME=English_Jamaica.1252
>> attached base packages:
>> [1] "methods" "stats" "graphics" "grDevices" "utils" "datasets"  
>> "base"
>>
>>
>>
>
>
> -- 
> O__ ---- Peter Dalgaard ?ster Farimagsgade 5, Entr.B
> c/ /'_ --- Dept. of Biostatistics PO Box 2099, 1014 Cph. K
> (*) \(*) -- University of Copenhagen Denmark Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk) FAX: (+45) 35327907
>
>
>
>
>  __________________________________________________
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From efg at stowers-institute.org  Tue Jan  9 18:17:32 2007
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Tue, 9 Jan 2007 11:17:32 -0600
Subject: [R] Simple spectral analysis
References: <45A26D72.1040404@gmx.de>
	<enufb3$702$1@sea.gmane.org><45A2D5D6.1020002@biostat.ku.dk>
	<45A33EBC.3050503@gmx.de>
Message-ID: <eo0inf$pfu$1@sea.gmane.org>

"Georg Hoermann" <georg.hoermann at gmx.de> wrote in message 
news:45A33EBC.3050503 at gmx.de...
> Peter Dalgaard wrote:
>> Earl F. Glynn wrote:

> Thanks a lot for the help. I will post the script when its ready
> (an introduction for our biology students to time series, just 8 hours)

I've been working with one of our labs here to find "cyclic" genes from 
microarray data.  The supplement for the paper we published is here (I 
haven't bothered making the code general enough for a package yet):

http://research.stowers-institute.org/efg/2005/LombScargle/index.htm



Normally we only have about 20 to 50 points in our time series for each 
gene.  With missing data a problem, I used a "Lomb-Scargle" approach to find 
the periodicity.  With Fourier analysis, one must impute any missing data 
points, but with Lomb-Scargle you just process the data you have without any 
imputation.



Perhaps you or your students would be interested in the "Numerical 
Experiments" on this page 
http://research.stowers-institute.org/efg/2005/LombScargle/supplement/NumericalExperiments/index.htm



I was curious how well the Lomb-Scargle technique would work with your 
data -- see the R code below.   Normally the Lomb-Scargle periodogram shows 
a single peak when there is a single dominant frequency.  The Peak 
Significance curve for all your data is a difficult to interpret, and I'm 
not sure the statistical tests are valid (without some tweaks) for your size 
dataset.



I took a random sample of 50 of your ~3000 data points and analyzed those --  
see the second code block below.  [For 50 data points I know all the 
assumptions are "good enough" for the statistics being computed.]  The 
periodogram here shows a single peak for period 365.6 days, which has good 
statistical significance.  Other subset samples can show harmonic 
frequencies, sometimes.





# efg, 9 Jan 2007

air = read.csv("http://www.hydrology.uni-kiel.de/~schorsch/air_temp.csv")
#air <- read.csv("air_temp.csv")

TempAirC <- air$T_air
Time     <- as.Date(air$Date, "%d.%m.%Y")
N <- length(Time)

# Lomb-Scargle code
source("http://research.stowers-institute.org/efg/2005/LombScargle/R/LombScargle.R")
MAXSPD <<- 1500
unit <<- "day"
M <- N    # Usually use factor of 2 or 4, but with large N use 1 instead

# Look at test frequencies corresponding to periods of 200 days to 500 days: 
f = 1/T
TestFrequencies <- (1/500) + (1/200 - 1/500) * (1:M / M)

# Use Horne & Baliunas' estimate of independent frequencies
Nindependent <- NHorneBaliunas(length(Time))  # valid for this size?

# Fairly slow with this large dataset

ComputeAndPlotLombScargle(as.numeric(Time), TempAirC,
  TestFrequencies, Nindependent,
  "Air Temperature [C]")





# Could get good results with fewer points too, say 50 chosen at random

MAXSPD <<- 25
TempAirC <- air$T_air
Time     <- as.Date(air$Date, "%d.%m.%Y")

set.seed(19)  # For reproducible results
RandomSet <- sample(1:length(Time), 50)
TempAirC <- TempAirC[RandomSet]
Time <-Time[RandomSet]

N <- length(Time)
M <- 4 * N    # Usually use factor of 2 or 4

# Look at test frequencies corresponding to periods of 200 days to 500 days: 
f = 1/T
TestFrequencies <- (1/500) + (1/200 - 1/500) * (1:M / M)

# Use Horne & Baliunas' estimate of independent frequencies
Nindependent <- NHorneBaliunas(length(Time))

# Very fast to compute for only 50 points

ComputeAndPlotLombScargle(as.numeric(Time), TempAirC,
  TestFrequencies, Nindependent,
  "Air Temperature [C]")





efg



Earl F. Glynn

Scientific Programmer

Stowers Institute


From johan.sandblom at ki.se  Tue Jan  9 18:56:16 2007
From: johan.sandblom at ki.se (Johan Sandblom)
Date: Tue, 9 Jan 2007 18:56:16 +0100
Subject: [R] Logical operations or selecting data from data.frames
In-Reply-To: <b75d67340701090906w256f92a0g96aead43ab12edd0@mail.gmail.com>
References: <b75d67340701090847j30da3959yc95d694abf0925dd@mail.gmail.com>
	<b75d67340701090906w256f92a0g96aead43ab12edd0@mail.gmail.com>
Message-ID: <97a06f070701090956jaaedbe3h6377e770fe9d4e35@mail.gmail.com>

Is the solution this simple?

sdata$VaR < sdata$DdtdAbs & sdata$DdtdDuration >= 1

Regards, Johan

2007/1/9, Benjamin Dickgiesser <dickgiesser at gmail.com>:
> I suppose this doesn't work for the same reason as
> sdata$VaR < sdata$DdtdAbs && sdata$DdtdDuration >= 1
>
> does only return  FALSE and not a vector of TRUE and FALSE as
>
> sdata$VaR < sdata$DdtdAbs
>
> would return.
>
> Is there a ways around this?
> Benjamin
>
> On 1/9/07, Benjamin Dickgiesser <dickgiesser at gmail.com> wrote:
> > Hi all,
> >
> > why doesn't something like this does not work?
> >
> > speedy <-
> >         (sdata$VaR < sdata$DdtdAbs) && sdata$DdtdDuration >= qpois(pct,lambda) &&
> >         sdata$Ddtd > MinDD
> >
> > or sdata$Ddtd[sdata$Ddtd > 0 && sdata$VaR < sdata$DdtdAbs]
> >
> > sdata looks like this:
> >
> >            dataId       date  value Ddtd VaR DdtdAbs DdtdDuration
> > 18948  79637 2004-07-27 10085.10           NA         NA    0.00            0
> > 18949  79638 2004-07-28 10117.10           NA         NA    0.00            0
> > 18950  79639 2004-07-29 10129.20           NA         NA    0.00            0
> > 18951  79640 2004-07-30 10139.70           NA         NA    0.00            0
> > 18952  79641 2004-08-02 10179.20           NA         NA    0.00            0
> > 18953  79642 2004-08-03 10120.20  0.579613329 336.060090   59.00            1
> > 18954  79643 2004-08-04 10126.50           NA         NA    0.00            0
> > 18955  79644 2004-08-05  9963.03  1.614279366 334.306978  163.47            1
> > 18956  79645 2004-08-06  9815.33  3.072828717 386.173057  311.17            2
> > 18957  79646 2004-08-09  9814.66  3.079445020 420.167049  311.84            3
> > 18958  79647 2004-08-10  9944.67           NA         NA    0.00            0
> > 18959  79648 2004-08-11  9938.32  0.063853300 328.315992    6.35            1
> > 18960  79649 2004-08-12  9814.59  1.308037371 379.182568  130.08            2
> >
> > I am trying to select rows from the data.frame which have Ddtd > x,
> > VaR < DdtdAbs and DdtdDuration > z.
> >
> > Thank you,
> > Benjamin
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Johan Sandblom  N8, MRC, Karolinska sjh
t +46851776108  17176 Stockholm
m +46735521477  Sweden
"What is wanted is not the will to believe, but the
will to find out, which is the exact opposite"
- Bertrand Russell


From Roger.Bivand at nhh.no  Tue Jan  9 19:05:25 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 9 Jan 2007 19:05:25 +0100 (CET)
Subject: [R] Logical operations or selecting data from data.frames
In-Reply-To: <b75d67340701090906w256f92a0g96aead43ab12edd0@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0701091904060.14204-100000@reclus.nhh.no>

On Tue, 9 Jan 2007, Benjamin Dickgiesser wrote:

> I suppose this doesn't work for the same reason as
> sdata$VaR < sdata$DdtdAbs && sdata$DdtdDuration >= 1
> 
> does only return  FALSE and not a vector of TRUE and FALSE as

See ?"&":

     '&' and '&&' indicate logical AND and '|' and '||' indicate
     logical OR.  The shorter form performs elementwise comparisons in
     much the same way as arithmetic operators.  The longer form
     evaluates left to right examining only the first element of each
     vector...

a <- rep(c(1,2), 5)
b <- rep(1,10)
a > 1 && b <= 1
a > 1 & b <= 1


> 
> sdata$VaR < sdata$DdtdAbs
> 
> would return.
> 
> Is there a ways around this?
> Benjamin
> 
> On 1/9/07, Benjamin Dickgiesser <dickgiesser at gmail.com> wrote:
> > Hi all,
> >
> > why doesn't something like this does not work?
> >
> > speedy <-
> >         (sdata$VaR < sdata$DdtdAbs) && sdata$DdtdDuration >= qpois(pct,lambda) &&
> >         sdata$Ddtd > MinDD
> >
> > or sdata$Ddtd[sdata$Ddtd > 0 && sdata$VaR < sdata$DdtdAbs]
> >
> > sdata looks like this:
> >
> >            dataId       date  value Ddtd VaR DdtdAbs DdtdDuration
> > 18948  79637 2004-07-27 10085.10           NA         NA    0.00            0
> > 18949  79638 2004-07-28 10117.10           NA         NA    0.00            0
> > 18950  79639 2004-07-29 10129.20           NA         NA    0.00            0
> > 18951  79640 2004-07-30 10139.70           NA         NA    0.00            0
> > 18952  79641 2004-08-02 10179.20           NA         NA    0.00            0
> > 18953  79642 2004-08-03 10120.20  0.579613329 336.060090   59.00            1
> > 18954  79643 2004-08-04 10126.50           NA         NA    0.00            0
> > 18955  79644 2004-08-05  9963.03  1.614279366 334.306978  163.47            1
> > 18956  79645 2004-08-06  9815.33  3.072828717 386.173057  311.17            2
> > 18957  79646 2004-08-09  9814.66  3.079445020 420.167049  311.84            3
> > 18958  79647 2004-08-10  9944.67           NA         NA    0.00            0
> > 18959  79648 2004-08-11  9938.32  0.063853300 328.315992    6.35            1
> > 18960  79649 2004-08-12  9814.59  1.308037371 379.182568  130.08            2
> >
> > I am trying to select rows from the data.frame which have Ddtd > x,
> > VaR < DdtdAbs and DdtdDuration > z.
> >
> > Thank you,
> > Benjamin
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From news at aspden.com  Tue Jan  9 19:39:12 2007
From: news at aspden.com (John Lawrence Aspden)
Date: Tue, 09 Jan 2007 18:39:12 +0000
Subject: [R] scripts with littler
References: <ent9nm$rc6$1@sea.gmane.org>
	<20070108132920.GA4640@phenix.progiciels-bpi.ca>
	<entlul$ng$1@sea.gmane.org>
	<971536df0701080703q48e04a5cv96720798b8b30c11@mail.gmail.com>
	<Pine.LNX.4.64.0701082235040.9254@gannet.stats.ox.ac.uk>
	<eo0bvb$sr0$1@sea.gmane.org>
Message-ID: <eo0ngg$7p1$1@sea.gmane.org>

John Lawrence Aspden wrote:

> I'm actually tempted to use
> 
> #!/usr/bin/env r
> rm(list=ls())

Ahem, it turns out to be better to use:

#!/usr/bin/env r
rm(list=ls()[ls()!="argv"])

-- 
Contractor in Cambridge UK -- http://www.aspden.com


From dickgiesser at gmail.com  Tue Jan  9 20:00:12 2007
From: dickgiesser at gmail.com (Benjamin Dickgiesser)
Date: Tue, 9 Jan 2007 19:00:12 +0000
Subject: [R] Logical operations or selecting data from data.frames
In-Reply-To: <45A3D968.8060501@pburns.seanet.com>
References: <b75d67340701090847j30da3959yc95d694abf0925dd@mail.gmail.com>
	<b75d67340701090906w256f92a0g96aead43ab12edd0@mail.gmail.com>
	<45A3D968.8060501@pburns.seanet.com>
Message-ID: <b75d67340701091100h4a6534abm3dbc7693d644eadd@mail.gmail.com>

Thx for the help, sorry I am just used to use && from php and simply
assumed it would work identically.

On 1/9/07, Patrick Burns <pburns at pburns.seanet.com> wrote:
> S Poetry (and other documentation) will tell you
> the difference between '&&' and '&'.
>
> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Benjamin Dickgiesser wrote:
>
> >I suppose this doesn't work for the same reason as
> >sdata$VaR < sdata$DdtdAbs && sdata$DdtdDuration >= 1
> >
> >does only return  FALSE and not a vector of TRUE and FALSE as
> >
> >sdata$VaR < sdata$DdtdAbs
> >
> >would return.
> >
> >Is there a ways around this?
> >Benjamin
> >
> >On 1/9/07, Benjamin Dickgiesser <dickgiesser at gmail.com> wrote:
> >
> >
> >>Hi all,
> >>
> >>why doesn't something like this does not work?
> >>
> >>speedy <-
> >>        (sdata$VaR < sdata$DdtdAbs) && sdata$DdtdDuration >= qpois(pct,lambda) &&
> >>        sdata$Ddtd > MinDD
> >>
> >>or sdata$Ddtd[sdata$Ddtd > 0 && sdata$VaR < sdata$DdtdAbs]
> >>
> >>sdata looks like this:
> >>
> >>           dataId       date  value Ddtd VaR DdtdAbs DdtdDuration
> >>18948  79637 2004-07-27 10085.10           NA         NA    0.00            0
> >>18949  79638 2004-07-28 10117.10           NA         NA    0.00            0
> >>18950  79639 2004-07-29 10129.20           NA         NA    0.00            0
> >>18951  79640 2004-07-30 10139.70           NA         NA    0.00            0
> >>18952  79641 2004-08-02 10179.20           NA         NA    0.00            0
> >>18953  79642 2004-08-03 10120.20  0.579613329 336.060090   59.00            1
> >>18954  79643 2004-08-04 10126.50           NA         NA    0.00            0
> >>18955  79644 2004-08-05  9963.03  1.614279366 334.306978  163.47            1
> >>18956  79645 2004-08-06  9815.33  3.072828717 386.173057  311.17            2
> >>18957  79646 2004-08-09  9814.66  3.079445020 420.167049  311.84            3
> >>18958  79647 2004-08-10  9944.67           NA         NA    0.00            0
> >>18959  79648 2004-08-11  9938.32  0.063853300 328.315992    6.35            1
> >>18960  79649 2004-08-12  9814.59  1.308037371 379.182568  130.08            2
> >>
> >>I am trying to select rows from the data.frame which have Ddtd > x,
> >>VaR < DdtdAbs and DdtdDuration > z.
> >>
> >>Thank you,
> >>Benjamin
> >>
> >>
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> >
>


From laurentRhelp at free.fr  Tue Jan  9 20:18:13 2007
From: laurentRhelp at free.fr (Laurent Rhelp)
Date: Tue, 09 Jan 2007 20:18:13 +0100
Subject: [R] odfWeave and figures in MS Word Format
In-Reply-To: <71257D09F114DA4A8E134DEAC70F25D3072F142F@groamrexm03.amer.pfizer.com>
References: <71257D09F114DA4A8E134DEAC70F25D3072F142F@groamrexm03.amer.pfizer.com>
Message-ID: <45A3EA75.5070607@free.fr>

Kuhn, Max a ?crit :

>Laurent,
>
>Since you question was not about odfWeave (despite the message title),
>it would have been better to send this question to an OpenOffice mailing
>list.
>
>That said, I think that the issues was your workflow (odt -> html ->
>doc). HTML is plain text, so when you saved the file in that format the
>image files are saved separately. Links are creating in the html file to
>the image files. When you converted the html to Word format, the images
>are still linked (not embedded into the Word document). When you move
>the word file to a different location, the links are broken and you
>won't be able to see the images. Rtf worked because the images are
>embedded into the rtf file. odt -> doc works just fine with images, so
>that would be a better idea.
>
>Max
>
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Laurent Rhelp
>Sent: Monday, January 08, 2007 1:42 PM
>To: R-help at stat.math.ethz.ch
>Subject: [R] odfWeave and figures in MS Word Format
>
>I answer to myself.
>I understood my error : first of all, we have to save the file in the 
>.rtf format !
>Then, from the rtf file, we can generate the file in the .doc format.
>
>I am sorry for my question.
>
>Thanks
>
>Laurent
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>----------------------------------------------------------------------
>LEGAL NOTICE
>Unless expressly stated otherwise, this message is confidential and may be privileged.  It is intended for the addressee(s) only.  Access to this E-mail by anyone else is unauthorized.  If you are not an addressee, any disclosure or copying of the contents of this E-mail or any action taken (or not taken) in reliance on it is unauthorized and may be unlawful.  If you are not an addressee, please inform the sender immediately.
>
>
>  
>
Thank you very much for your detailed explanations.


From Graham.Williams at togaware.com  Tue Jan  9 20:51:30 2007
From: Graham.Williams at togaware.com (Graham Williams)
Date: Wed, 10 Jan 2007 06:51:30 +1100
Subject: [R] How to use Rattle for Data mining through R?
In-Reply-To: <907370.79084.qm@web34311.mail.mud.yahoo.com>
References: <907370.79084.qm@web34311.mail.mud.yahoo.com>
Message-ID: <20070109195130.GA5548@athene.togaware.com>

Received Tue 09 Jan 2007 12:36pm +1100 from Bhanu Kalyan.K:
> Dear Mr. Bengtsson,
> 
> Now i am able to work with R-Matlab interface comfortably. Thanks to you. Recently, I came to know that R can be used for data mining as well. I went through the following site for this : 
> 
> http://rattle.togaware.com/  
> 
> As they have suggested, I have also installed the two packages:
> 
> > install.packages("RGtk2")     
> >     install.packages("rattle")     
> 
> Now, can you explain how to work on this?
> Can this be used to implement the clustering algorithms? If yes, Kindly elaborate.

A patchy (but regularly updated) user guide is under development and
freely available as HTML from:

  http://datamining.togaware.com/

(follow the links provided from rattle.togaware.com)

In particular the starting point it:

  http://datamining.togaware.com/survivor/Data_Mining.html

There is a cluster tab under the Unsupervised paradigm, but not
documented yet.

I noticed some formatting problems with the HTML. Currently being fixed.

Regards,
Graham


From jeff.horner at vanderbilt.edu  Tue Jan  9 20:52:41 2007
From: jeff.horner at vanderbilt.edu (Jeffrey Horner)
Date: Tue, 09 Jan 2007 13:52:41 -0600
Subject: [R] scripts with littler
In-Reply-To: <eo0ngg$7p1$1@sea.gmane.org>
References: <ent9nm$rc6$1@sea.gmane.org>	<20070108132920.GA4640@phenix.progiciels-bpi.ca>	<entlul$ng$1@sea.gmane.org>	<971536df0701080703q48e04a5cv96720798b8b30c11@mail.gmail.com>	<Pine.LNX.4.64.0701082235040.9254@gannet.stats.ox.ac.uk>	<eo0bvb$sr0$1@sea.gmane.org>
	<eo0ngg$7p1$1@sea.gmane.org>
Message-ID: <45A3F289.40705@vanderbilt.edu>

John Lawrence Aspden wrote:
> John Lawrence Aspden wrote:
> 
>> I'm actually tempted to use
>>
>> #!/usr/bin/env r
>> rm(list=ls())
> 
> Ahem, it turns out to be better to use:
> 
> #!/usr/bin/env r
> rm(list=ls()[ls()!="argv"])
> 

Eww!! I'm not sure you want to do that. I would recommend sticking with:

#!/usr/bin/r -v

as that gives you a "truer" scripting environment. I understand that 
won't load the libraries in your home area automatically, but consider 
the way scripts in other languages are written and distributed: they 
usually load the libraries at the beginning of the script. Silently 
loading them before the script is run hides behavior from the script user.

If you have libraries installed outside of the library search path, 
consider expanding it with .libPaths() before calling library() or 
require().

Cheers,

Jeff
-- 
http://biostat.mc.vanderbilt.edu/JeffreyHorner


From fshen at ufl.edu  Tue Jan  9 22:09:02 2007
From: fshen at ufl.edu (SHEN,FENG)
Date: Tue, 9 Jan 2007 16:09:02 -0500 (EST)
Subject: [R] differential item function for item response theory
Message-ID: <1487832355.261051168376942907.JavaMail.osg@osgjas02.cns.ufl.edu>

Hi my friends,

I'm very new to R and need your help.

I used R and ltm package for item response theory (IRT) modeling.  
I also need to compute differential item function (DIF) for IRT 
models.  I searched the archive but basically found nothing.  
Could you help me find some sources about handling DIF of IRT?

Many thanks in advance!

Feng


From christos at nuverabio.com  Tue Jan  9 22:10:39 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Tue, 9 Jan 2007 16:10:39 -0500
Subject: [R] A vectorization question
Message-ID: <007401c73432$9deecab0$0e010a0a@headquarters.silicoinsights>

Hi,
 
A function calculates the absolute difference between the two largest values
of each row of a matrix, as shown in the following example code:
 
cx <- matrix(runif(15),5)
cy <- t( apply(cx, 1, order, decreasing=TRUE) )
 
cz <- rep(0, nrow(cx))
for( i in 1:nrow(cx) ) cz[i] <- abs(diff(cx[i, cy[i,1:2]]))
 
Anybody has any ideas on how the last loop can be vectorized?

Thanks. 
 
Christos Hatzis, Ph.D.
Nuvera Biosciences, Inc.
400 West Cummings Park
Suite 5350
Woburn, MA 01801
Tel: 781-938-3830
www.nuverabio.com <http://www.nuverabio.com/>


From HDoran at air.org  Tue Jan  9 22:15:36 2007
From: HDoran at air.org (Doran, Harold)
Date: Tue, 9 Jan 2007 16:15:36 -0500
Subject: [R] differential item function for item response theory
Message-ID: <2323A6D37908A847A7C32F1E3662C80E6B1076@dc1ex01.air.org>

I don't know of any functions specifically designed to handle DIF (e.g.,
mantel-hantzel). But, ltm does give you the point estimates and standard
errors so you can do a t-test between the focal and reference groups.



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of SHEN,FENG
> Sent: Tuesday, January 09, 2007 4:09 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] differential item function for item response theory
> 
> Hi my friends,
> 
> I'm very new to R and need your help.
> 
> I used R and ltm package for item response theory (IRT) modeling.  
> I also need to compute differential item function (DIF) for 
> IRT models.  I searched the archive but basically found nothing.  
> Could you help me find some sources about handling DIF of IRT?
> 
> Many thanks in advance!
> 
> Feng
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From czucz at botanika.hu  Tue Jan  9 22:30:48 2007
From: czucz at botanika.hu (=?ISO-8859-1?Q?B=E1lint_Cz=FAcz?=)
Date: Tue, 9 Jan 2007 22:30:48 +0100
Subject: [R] randomForest and missing data
In-Reply-To: <20070104211314.GA27922@cs.umn.edu>
References: <20070104211314.GA27922@cs.umn.edu>
Message-ID: <fab4bcf70701091330l74b41959hd0deb655c93fefb8@mail.gmail.com>

There is an improved version of the original random forest algorithm
available in the "party" package (you can find some additional
information on the details here:
http://www.stat.uni-muenchen.de/sfb386/papers/dsp/paper490.pdf ).

I do not know whether it yields a solution to your problem about
missing data, but maybe it's a check worth...

Best regards:

B?lint

On 1/4/07, Darin A. England <england at cs.umn.edu> wrote:
>
> Does anyone know a reason why, in principle, a call to randomForest
> cannot accept a data frame with missing predictor values? If each
> individual tree is built using CART, then it seems like this
> should be possible. (I understand that one may impute missing values
> using rfImpute or some other method, but I would like to avoid doing
> that.)
>
> If this functionality were available, then when the trees are being
> constructed and when subsequent data are put through the forest, one
> would also specify an argument for the use of surrogate rules, just
> like in rpart.
>
> I realize this question is very specific to randomForest, as opposed
> to R in general, but any comments are appreciated. I suppose I am
> looking for someone to say "It's not appropriate, and here's why
> ..." or "Good idea. Please implement and post your code."
>
> Thanks,
>
> Darin England, Senior Scientist
> Ingenix
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From marc_schwartz at comcast.net  Tue Jan  9 22:32:09 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 09 Jan 2007 15:32:09 -0600
Subject: [R] A vectorization question
In-Reply-To: <007401c73432$9deecab0$0e010a0a@headquarters.silicoinsights>
References: <007401c73432$9deecab0$0e010a0a@headquarters.silicoinsights>
Message-ID: <1168378329.8612.26.camel@localhost.localdomain>

On Tue, 2007-01-09 at 16:10 -0500, Christos Hatzis wrote:
> Hi,
>  
> A function calculates the absolute difference between the two largest values
> of each row of a matrix, as shown in the following example code:
>  
> cx <- matrix(runif(15),5)
> cy <- t( apply(cx, 1, order, decreasing=TRUE) )
>  
> cz <- rep(0, nrow(cx))
> for( i in 1:nrow(cx) ) cz[i] <- abs(diff(cx[i, cy[i,1:2]]))
>  
> Anybody has any ideas on how the last loop can be vectorized?
> 
> Thanks. 


How about this:

> mat <- matrix(sample(1:50, 12), ncol = 4)

> mat
     [,1] [,2] [,3] [,4]
[1,]   39    1   22   11
[2,]   34   28   13   48
[3,]   25   40   38    3


> apply(mat, 1, function(x) abs(diff(sort(x, decreasing = TRUE)[1:2])))
[1] 17 14  2

Or

> apply(mat, 1, function(x) diff(sort(x)[3:4]))
[1] 17 14  2

HTH,

Marc Schwartz


From christos at nuverabio.com  Tue Jan  9 22:39:47 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Tue, 9 Jan 2007 16:39:47 -0500
Subject: [R] A vectorization question
In-Reply-To: <1168378329.8612.26.camel@localhost.localdomain>
Message-ID: <007801c73436$b0119890$0e010a0a@headquarters.silicoinsights>

Thanks, Marc.
This is what I was trying to do but could not get it to work.

-Christos 

-----Original Message-----
From: Marc Schwartz [mailto:marc_schwartz at comcast.net] 
Sent: Tuesday, January 09, 2007 4:32 PM
To: christos at nuverabio.com
Cc: 'R-help'
Subject: Re: [R] A vectorization question

On Tue, 2007-01-09 at 16:10 -0500, Christos Hatzis wrote:
> Hi,
>  
> A function calculates the absolute difference between the two largest 
> values of each row of a matrix, as shown in the following example code:
>  
> cx <- matrix(runif(15),5)
> cy <- t( apply(cx, 1, order, decreasing=TRUE) )
>  
> cz <- rep(0, nrow(cx))
> for( i in 1:nrow(cx) ) cz[i] <- abs(diff(cx[i, cy[i,1:2]]))
>  
> Anybody has any ideas on how the last loop can be vectorized?
> 
> Thanks. 


How about this:

> mat <- matrix(sample(1:50, 12), ncol = 4)

> mat
     [,1] [,2] [,3] [,4]
[1,]   39    1   22   11
[2,]   34   28   13   48
[3,]   25   40   38    3


> apply(mat, 1, function(x) abs(diff(sort(x, decreasing = TRUE)[1:2])))
[1] 17 14  2

Or

> apply(mat, 1, function(x) diff(sort(x)[3:4]))
[1] 17 14  2

HTH,

Marc Schwartz


From news at aspden.com  Tue Jan  9 22:46:41 2007
From: news at aspden.com (John Lawrence Aspden)
Date: Tue, 09 Jan 2007 21:46:41 +0000
Subject: [R] scripts with littler
References: <ent9nm$rc6$1@sea.gmane.org>	<20070108132920.GA4640@phenix.progiciels-bpi.ca>	<entlul$ng$1@sea.gmane.org>	<971536df0701080703q48e04a5cv96720798b8b30c11@mail.gmail.com>	<Pine.LNX.4.64.0701082235040.9254@gannet.stats.ox.ac.uk>	<eo0bvb$sr0$1@sea.gmane.org>
	<eo0ngg$7p1$1@sea.gmane.org> <45A3F289.40705@vanderbilt.edu>
Message-ID: <eo12g2$lme$1@sea.gmane.org>

Jeffrey Horner wrote:
>> John Lawrence Aspden wrote:
>> 
>>> I'm actually tempted to use
>>>
>> #!/usr/bin/env r
>> rm(list=ls()[ls()!="argv"])
> 
> Eww!! I'm not sure you want to do that. I would recommend sticking with:
> 
> #!/usr/bin/r -v
> 
> as that gives you a "truer" scripting environment. I understand that
> won't load the libraries in your home area automatically, but consider
> the way scripts in other languages are written and distributed: they
> usually load the libraries at the beginning of the script. Silently
> loading them before the script is run hides behavior from the script user.
> 
> If you have libraries installed outside of the library search path,
> consider expanding it with .libPaths() before calling library() or
> require().
> 
> Cheers,
> 
> Jeff


Hi, thanks, it's not that it doesn't load the libraries automatically (which
I'd hate), it's that it no longer knows how to load them.

I've got a library (brainwaver), installed locally in ~/R/library, and this
information is recorded in the ~/.Renviron file.

This is because there's no debian package for it, and I don't want to mess
up the system by trying to install it manually as root (after all, it
should be fairly obvious that I don't know what I'm doing!...)

In my script I load the library, but if I call it using
#!/usr/bin/r --vanilla, this stops working.

(I can still load the system-wide libraries, it's the ones installed in my
home directory that break)

Since I can't use subroutines without using the library mechanism, and I
want to use brainwaver, and I want people to be able to use this stuff
without needing root privileges, or needing to hack hard-coded file
locations into every script, I'd prefer ~/.Renviron read.

Also, of course, using #!/usr/bin/r depends on it being installed there, and
I can't use the env mechanism and still pass it the vanilla option.

My main problem with R's/littler's default behaviour is that it introduces
lots of spurious variables pulled in from .Rdata that are different
depending where it's invoked.

I'm aware that the rm(list... is a nasty hack, but it seems like the least
bad option. Most of the other things seem to produce scripts that won't
work if you tar them up and send them to people.

Thanks for the hint about .libPaths(), but without ~/.Renviron how am I to
know which directories to add to it?

Of course I'm not saying that there might not be other subtle difficulties
with the default. But so far I prefer the default + explicitly remove all
variables to --vanilla, and I can't pass --vanilla without being sure where
R's installed anyway!

Is anyone still reading by this point?? Thanks for your perseverance if so!

Cheers, John.

-- 
Contractor in Cambridge UK -- http://www.aspden.com


From marc_schwartz at comcast.net  Tue Jan  9 23:07:22 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 09 Jan 2007 16:07:22 -0600
Subject: [R] A vectorization question
In-Reply-To: <007801c73436$b0119890$0e010a0a@headquarters.silicoinsights>
References: <007801c73436$b0119890$0e010a0a@headquarters.silicoinsights>
Message-ID: <1168380442.8612.48.camel@localhost.localdomain>

Welcome Christos.

Note that my first example can actually be simplified to:

  apply(mat, 1, function(x) -diff(sort(x, decreasing = TRUE)[1:2]))

Since we really just need to negate the difference, rather than take the
abs().

The advantage of this approach is that the two max values will always be
the first and second values, so will be independent of the length of
'x' (number of columns in the matrix).

Using the second example more generally, you would have to use something
like:

  apply(mat, 1, function(x) diff(sort(x)[-c(1:(length(x) - 2))]))

in the subsetting of the sort() results or precalcuate the indices (ie.
ncol(mat) and ncol(mat) - 1).

Might add a bit more overhead, but testing would give you more empiric
timing data. That might have to be balanced by whether the rows tend to
be random in order or closer to being sorted in increasing/decreasing
order, which would affect the sort time. Worst case scenario is
generally having to reverse the sort order. Of course, if the matrices
are "relatively" small, sorting time would likely be a non-issue.

HTH,

Marc

On Tue, 2007-01-09 at 16:39 -0500, Christos Hatzis wrote:
> Thanks, Marc.
> This is what I was trying to do but could not get it to work.
> 
> -Christos 

<snip>


From christos at nuverabio.com  Tue Jan  9 23:24:12 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Tue, 9 Jan 2007 17:24:12 -0500
Subject: [R] A vectorization question
In-Reply-To: <1168380442.8612.48.camel@localhost.localdomain>
Message-ID: <008101c7343c$e47df2d0$0e010a0a@headquarters.silicoinsights>


That's true.  Just need to negate the difference.  Actually, straight diff
can be used after reversing the vector:

apply(mat, 1, function(x) diff(sort(x, decreasing = TRUE)[2:1]))

I only have 3 columns in my matrix so sorting should not add much overhead,
but I will time both versions.

Thanks again.
-Christos 

-----Original Message-----
From: Marc Schwartz [mailto:marc_schwartz at comcast.net] 
Sent: Tuesday, January 09, 2007 5:07 PM
To: christos at nuverabio.com
Cc: 'R-help'
Subject: RE: [R] A vectorization question

Welcome Christos.

Note that my first example can actually be simplified to:

  apply(mat, 1, function(x) -diff(sort(x, decreasing = TRUE)[1:2]))

Since we really just need to negate the difference, rather than take the
abs().

The advantage of this approach is that the two max values will always be the
first and second values, so will be independent of the length of 'x' (number
of columns in the matrix).

Using the second example more generally, you would have to use something
like:

  apply(mat, 1, function(x) diff(sort(x)[-c(1:(length(x) - 2))]))

in the subsetting of the sort() results or precalcuate the indices (ie.
ncol(mat) and ncol(mat) - 1).

Might add a bit more overhead, but testing would give you more empiric
timing data. That might have to be balanced by whether the rows tend to be
random in order or closer to being sorted in increasing/decreasing order,
which would affect the sort time. Worst case scenario is generally having to
reverse the sort order. Of course, if the matrices are "relatively" small,
sorting time would likely be a non-issue.

HTH,

Marc

On Tue, 2007-01-09 at 16:39 -0500, Christos Hatzis wrote:
> Thanks, Marc.
> This is what I was trying to do but could not get it to work.
> 
> -Christos

<snip>


From efg at stowers-institute.org  Tue Jan  9 23:51:05 2007
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Tue, 9 Jan 2007 16:51:05 -0600
Subject: [R] listing all functions in R
References: <1168088147.3149.6.camel@dhcppc2.my.nat.localnet><Pine.LNX.4.64.0701061331420.8651@gannet.stats.ox.ac.uk><enu91s$ep7$1@sea.gmane.org>
	<m23b6k5idc.fsf@fhcrc.org>
Message-ID: <eo168v$2pl$1@sea.gmane.org>

"Seth Falcon" <sfalcon at fhcrc.org> wrote in message 
news:m23b6k5idc.fsf at fhcrc.org...
> Are you sure you need to?  I just tried your code above with:
>
> pkgs <- c("Biobase", "GOstats", "flrblr", "bazbaz")
>
> And while I see warning messages about the flrblr and bazbaz packages,
> the function completed and I get the expected results in z.
>
> Oh, perhaps you have some broken installs?  Broken in the sense that
> you have a package installed but not its dependencies?

I installed all known CRAN packages after installing R 2.4.1 last week on a 
PC.  Perhaps some new consistency checks checks could be be made to catch 
such dependency problems?

>
> How about this:
>
> safeRequire <- function(x) {
>    tryCatch(require(x, character.only=TRUE),
>             error=function(e) FALSE)
> }

Thanks.  That's a much better function.

But if your process a lot of packages, even with just safeRequire (or 
findfuns), the search() path grows quite long, and things break, so it's not 
really possible to get a list of all functions in R if you have all packages 
installed.

Consider:

pkgs <- dir(.Library)

length(pkgs)        #957

length( search() )  # 9

# First 100 Packages
set1 <- lapply(pkgs[1:100], safeRequire)
pkgs[which(!unlist(set1))]
#[1] "bcp"         "cairoDevice" "caMassClass"
length( search() )  # 135

safeRequire("bcp")

####################################
Loading required package: bcp
Loading required package: DNAcopy
Warning in library(pkg, character.only = TRUE, logical = TRUE, lib.loc = 
lib.loc) :
         there is no package called 'DNAcopy'
[1] FALSE
####################################


In the 2nd 100 many packages seem to be affected by the "Maximal number of 
DLLs reached..."

I didn't bother trying to process packages 201 through 957.

efg

Earl F. Glynn
Scientific Programmer
Bioinformatics
Stowers Institute for Medical Research


From mailing-lists at rhkoning.com  Wed Jan 10 08:31:54 2007
From: mailing-lists at rhkoning.com (rhk)
Date: Wed, 10 Jan 2007 08:31:54 +0100
Subject: [R] roc and lattice
Message-ID: <45A4966A.3020406@rhkoning.com>

Hello, I am afraid I do not fully understand all intricacies of 
programming in lattice plots. In the code below I try to plot an ROC 
curve, following R-news 4(1). When I condition on the variable 'group' I 
get the error message below, when I plot the curve for all data (i.e., y 
~ pred.prob), I get the plot I want. Can someone point out why 
conditioning gives that message? Thanks, Ruud

 > plot.a <- xyplot(y ~ pred.prob|group, data=x.df,
+  xlim=c(0,1),xlab="1-specificiteit",
+  ylab="sensitiviteit",
+  panel=function(x,y,subscripts,...){
+   DD <- table(-x,y)
+   sens <- cumsum(DD[,2])/sum(DD[,2])
+   mspec <- cumsum(DD[,1])/sum(DD[,1])
+   panel.xyplot(mspec,sens,type="l",...)
+   panel.abline(0,1)
+  })
 > print(plot.a)
Error in panel(x = c(0.000265710002003069, 0.000345712857778025, 
0.000265710002003069,  :
        subscript out of bounds


From deepayan.sarkar at gmail.com  Wed Jan 10 09:00:07 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 10 Jan 2007 00:00:07 -0800
Subject: [R] roc and lattice
In-Reply-To: <45A4966A.3020406@rhkoning.com>
References: <45A4966A.3020406@rhkoning.com>
Message-ID: <eb555e660701100000o41094a17t434a3c8485caa2ff@mail.gmail.com>

On 1/9/07, rhk <mailing-lists at rhkoning.com> wrote:
> Hello, I am afraid I do not fully understand all intricacies of
> programming in lattice plots. In the code below I try to plot an ROC
> curve, following R-news 4(1). When I condition on the variable 'group' I
> get the error message below, when I plot the curve for all data (i.e., y
> ~ pred.prob), I get the plot I want. Can someone point out why
> conditioning gives that message? Thanks, Ruud
>
>  > plot.a <- xyplot(y ~ pred.prob|group, data=x.df,
> +  xlim=c(0,1),xlab="1-specificiteit",
> +  ylab="sensitiviteit",
> +  panel=function(x,y,subscripts,...){
> +   DD <- table(-x,y)
> +   sens <- cumsum(DD[,2])/sum(DD[,2])
> +   mspec <- cumsum(DD[,1])/sum(DD[,1])
> +   panel.xyplot(mspec,sens,type="l",...)
> +   panel.abline(0,1)
> +  })
>  > print(plot.a)
> Error in panel(x = c(0.000265710002003069, 0.000345712857778025,
> 0.000265710002003069,  :
>         subscript out of bounds
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Since you haven't bothered to give us a reproducible example, all we
can say is that somewhere in your code, a subscript is out of bounds.
Since the only place where you use subscripts is as DD[,2] etc, I
would start by checking if DD indeed has two columns and two rows as
you seem to expect.

Deepayan


From carmei3 at web.de  Wed Jan 10 09:52:32 2007
From: carmei3 at web.de (Carmen Meier)
Date: Wed, 10 Jan 2007 09:52:32 +0100
Subject: [R] where is the NIR dataset?
Message-ID: <45A4A950.7010409@web.de>

I did just the download of the pls package, but the NIR dataset is not 
available

 require(pls)
[1] TRUE
 data(NIR)
Warning message:
data set 'NIR' not found in: data(NIR)

is there another package with the dataset for the examples?
 
With regards Carmen


From Torsten.Hothorn at rzmail.uni-erlangen.de  Wed Jan 10 10:57:27 2007
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Wed, 10 Jan 2007 10:57:27 +0100 (CET)
Subject: [R] randomForest and missing data
In-Reply-To: <fab4bcf70701091330l74b41959hd0deb655c93fefb8@mail.gmail.com>
References: <20070104211314.GA27922@cs.umn.edu>
	<fab4bcf70701091330l74b41959hd0deb655c93fefb8@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701101055440.17987@imbe153.imbe.med.uni-erlangen.de>


On Tue, 9 Jan 2007, B?lint Cz?cz wrote:

> There is an improved version of the original random forest algorithm
> available in the "party" package (you can find some additional
> information on the details here:
> http://www.stat.uni-muenchen.de/sfb386/papers/dsp/paper490.pdf ).
>
> I do not know whether it yields a solution to your problem about
> missing data, but maybe it's a check worth...
>

yes, `cforest()' is able to deal with missing values. More specifically, 
the implementation is based on conditional trees (`ctree()') which are 
able to set up surrogate splits.

Torsten

> Best regards:
>
> B?lint
>
> On 1/4/07, Darin A. England <england at cs.umn.edu> wrote:
>>
>> Does anyone know a reason why, in principle, a call to randomForest
>> cannot accept a data frame with missing predictor values? If each
>> individual tree is built using CART, then it seems like this
>> should be possible. (I understand that one may impute missing values
>> using rfImpute or some other method, but I would like to avoid doing
>> that.)
>>
>> If this functionality were available, then when the trees are being
>> constructed and when subsequent data are put through the forest, one
>> would also specify an argument for the use of surrogate rules, just
>> like in rpart.
>>
>> I realize this question is very specific to randomForest, as opposed
>> to R in general, but any comments are appreciated. I suppose I am
>> looking for someone to say "It's not appropriate, and here's why
>> ..." or "Good idea. Please implement and post your code."
>>
>> Thanks,
>>
>> Darin England, Senior Scientist
>> Ingenix
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>

From Torsten.Hothorn at rzmail.uni-erlangen.de  Wed Jan 10 11:02:21 2007
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Wed, 10 Jan 2007 11:02:21 +0100 (CET)
Subject: [R] posthoc tests with ANCOVA
In-Reply-To: <45A3B1A0.7070701@ufz.de>
References: <45A3B1A0.7070701@ufz.de>
Message-ID: <Pine.LNX.4.64.0701101057380.17987@imbe153.imbe.med.uni-erlangen.de>


On Tue, 9 Jan 2007, Walter Durka wrote:

> dear all,
>

Walter,

_please_ cc questions on contributed packages to the maintainer, since not 
everybody follows r-help closely...

> I want to perform a posthoc test for my ANCOVA:
> a1<-aov(seeds~treatment*length)
>
> With
> summary(glht(a1, linfct = mcp(treatment = "Tukey")))
> R tells me: "covariate interactions found -- please choose appropriate
> contrast"
>

one needs to specify a certain value for `length' which, I assume, is a 
numeric covariate, right? The current interface doesn't support this (this 
on my to-do-list), however, you can set up the matrix of linear functions 
by yourself (contact me privately if you have problems to do that).

> How do I build these contrasts?
>
> Ideally, I would like to have the posthoc test for the ANCOVA including
> a block-effect
> a2<-aov(seeds~treatment*length+Error(site))
>
> How do I make a posthoc test here?
>

its on the to-do-list as well :-(

Torsten

> Thanks for any comments
> Walter
>
>
> -- 
>
> *****
> Dr. Walter Durka
> Department Bioz?noseforschung
> Department of community ecology
>
> Helmholtz-Zentrum f?r Umweltforschung GmbH - UFZ
> Helmholtz Centre for Environmental Research - UFZ
> Theodor-Lieser-Str. 4 / 06120 Halle / Germany
>
> walter.durka at ufz.de / http://www.ufz.de/index.php?en=798
> phone +49 345 558 5314 / Fax +49 345 558 5329
>
> +++++++++++++++++++++++++++++++++++++++++++++++++
>
> Das UFZ hat einen neuen Namen:
> Helmholtz-Zentrum f?r Umweltforschung GmbH - UFZ
>
> The UFZ has a new name:
> Helmholtz Centre for Environmental Research - UFZ
>
> +++++++++++++++++++++++++++++++++++++++++++++++++
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>

From neo27 at rakers.de  Wed Jan 10 12:10:05 2007
From: neo27 at rakers.de (Mark Hempelmann)
Date: Wed, 10 Jan 2007 12:10:05 +0100
Subject: [R] select subsets in data frame
Message-ID: <45A4C98D.7030200@rakers.de>

Dear WizaRds!

A trivial question indeed on selecting subsets in data frames. I am 
sorry. Unfortunately, I did not find any helpful information on the 
introduction, searched the help archive and read in introductory books. 
Please help:

I want to select column "KB" which is read via read.csv2 as a data.frame 
into d. I checked that it is indeed a data.frame object and included the 
correct header information in line 1. For example purposes, look at this 
small object:
<<*>>= (4)
d <- data.frame(A=1:3, Date=c("01.01.07","02.01.07","03.01.07"),
KB=c("Eenie", "Meenie", "Miney") )

d["KB"=="Eenie",] # gives
@
output-start
[1] A    Date KB
<0 rows> (or 0-length row.names)
output-end
@
If I follow Venables/ Ripley in Modern Applied Statistics with S, it 
should look like this:

<<*>>= (5)
library(MASS)
attach(painters)
painters[Colour>=17,]
@
gives the correct subset. But
d[KB=="Eenie",] # gives

Error in `[.data.frame`(d, KB == "Eenie", ) :
         object "KB" not found

I need every KB named Eenie. What did I do wrong? The alternative I 
found seems to be quite complicated:

<<*>>= (6)
d[which( d[,"KB"]=="Eenie" ), ]
@
output-start
   A     Date    KB
1 1 01.01.07 Eenie
output-end

Thank you so much for your help.

cheers
mark


"I believe I found the missing link between animal and civilized man. 
It's us." -- Konrad Lorenz


From ccleland at optonline.net  Wed Jan 10 12:27:19 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 10 Jan 2007 06:27:19 -0500
Subject: [R] select subsets in data frame
In-Reply-To: <45A4C98D.7030200@rakers.de>
References: <45A4C98D.7030200@rakers.de>
Message-ID: <45A4CD97.6060509@optonline.net>

Mark Hempelmann wrote:
> Dear WizaRds!
> 
> A trivial question indeed on selecting subsets in data frames. I am 
> sorry. Unfortunately, I did not find any helpful information on the 
> introduction, searched the help archive and read in introductory books. 
> Please help:
> 
> I want to select column "KB" which is read via read.csv2 as a data.frame 
> into d. I checked that it is indeed a data.frame object and included the 
> correct header information in line 1. For example purposes, look at this 
> small object:
> <<*>>= (4)
> d <- data.frame(A=1:3, Date=c("01.01.07","02.01.07","03.01.07"),
> KB=c("Eenie", "Meenie", "Miney") )
> 
> d["KB"=="Eenie",] # gives
> @
> output-start
> [1] A    Date KB
> <0 rows> (or 0-length row.names)
> output-end
> @

  Try this instead:

subset(d, KB == "Eenie")

  A     Date    KB
1 1 01.01.07 Eenie

?subset

> If I follow Venables/ Ripley in Modern Applied Statistics with S, it 
> should look like this:
> 
> <<*>>= (5)
> library(MASS)
> attach(painters)
> painters[Colour>=17,]
> @
> gives the correct subset. But
> d[KB=="Eenie",] # gives
> 
> Error in `[.data.frame`(d, KB == "Eenie", ) :
>          object "KB" not found

  Works for me if I attach the data frame first:

attach(d)

d[KB == "Eenie",]

  A     Date    KB
1 1 01.01.07 Eenie

> I need every KB named Eenie. What did I do wrong? The alternative I 
> found seems to be quite complicated:
> 
> <<*>>= (6)
> d[which( d[,"KB"]=="Eenie" ), ]
> @
> output-start
>    A     Date    KB
> 1 1 01.01.07 Eenie
> output-end
> 
> Thank you so much for your help.
> 
> cheers
> mark
> 
> 
> "I believe I found the missing link between animal and civilized man. 
> It's us." -- Konrad Lorenz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From bxc at steno.dk  Wed Jan 10 12:33:43 2007
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Wed, 10 Jan 2007 12:33:43 +0100
Subject: [R] dimensions of a all objects
In-Reply-To: <bd93cdad0701090629g5b9e563ete6bb8bc905ceac36@mail.gmail.com>
Message-ID: <40D3930AC1C8EA469E39536E5BC80835039A7C57@EXDKBA021.corp.novocorp.net>

Generally it is difficult to get an overview of what's there.
But the following function I acquired from (???) ages ago does a nice
job:

lls <-
function (pos = 1, pat = "") 
{
    dimx <- function(dd) if (is.null(dim(dd))) 
        length(dd)
    else dim(dd)
    lll <- ls(pos = pos, pat = pat)
    cat(formatC("mode", 1, 15), formatC("class", 1, 18), formatC("name",

        1, max(nchar(lll)) + 1), "
size\n-------------------------------------------------------\n")
    if (length(lll) > 0) {
        for (i in 1:length(lll)) {
            cat(formatC(eval(parse(t = paste("mode(", lll[i], 
                ")"))), 1, 15), formatC(paste(eval(parse(t =
paste("class(", 
                lll[i], ")"))), collapse = " "), 1, 18), formatC(lll[i],

                1, max(nchar(lll)) + 1), " ", eval(parse(t =
paste("dimx(", 
                lll[i], ")"))), "\n")
        }
    }
}

Just say 

lls()

and you get a reasnoable listing of obejcts.

Best,
Bendix
______________________________________________

Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2-4
DK-2820 Gentofte
Denmark
+45 44 43 87 38 (direct)
+45 30 75 87 38 (mobile)
+45 44 43 73 13 (fax)
bxc at steno.dk   http://www.biostat.ku.dk/~bxc
______________________________________________


> -----Original Message-----
> From: Farrel Buchinsky [mailto:fjbuch at gmail.com] 
> Sent: Tuesday, January 09, 2007 3:30 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] dimensions of a all objects
> 
> Why will the following command not work
> sapply(objects(),dim)
> What does it say about the objects list? What does it say 
> about the dim command?
> 
> Likewise, the following also does not work
> all<-ls()
> for (f in all) print(dim(f))
> --
> Farrel Buchinsky
> 
> 	[[alternative HTML version deleted]]
> 
> 
>


From news at aspden.com  Wed Jan 10 13:12:56 2007
From: news at aspden.com (John Lawrence Aspden)
Date: Wed, 10 Jan 2007 12:12:56 +0000
Subject: [R] scripts with littler
References: <ent9nm$rc6$1@sea.gmane.org>	<20070108132920.GA4640@phenix.progiciels-bpi.ca>	<entlul$ng$1@sea.gmane.org>	<971536df0701080703q48e04a5cv96720798b8b30c11@mail.gmail.com>	<Pine.LNX.4.64.0701082235040.9254@gannet.stats.ox.ac.uk>	<eo0bvb$sr0$1@sea.gmane.org>
	<eo0ngg$7p1$1@sea.gmane.org> <45A3F289.40705@vanderbilt.edu>
	<eo12g2$lme$1@sea.gmane.org>
Message-ID: <eo2l88$v2m$1@sea.gmane.org>

John Lawrence Aspden wrote:

> I've got a library (brainwaver), installed locally in ~/R/library, and
> this information is recorded in the ~/.Renviron file.

> In my script I load the library, but if I call it using
> #!/usr/bin/r --vanilla, this stops working.

(Various private e-mails exchanged. Again, thanks Dirk!)

Just in case anyone else is trying to do this, it turns out that if you can
persuade your end users to install the library to ~/R/library, then you can
say:

#!/usr/bin/r --vanilla
library(brainwaver, lib.loc='~/R/library')

although in my case, brainwaver depends on another library, which it now
can't find, so actually I have to load them in order:

#!/usr/bin/r --vanilla

library(waveslim, lib.loc='~/R/library')
library(brainwaver, lib.loc='~/R/library')





Alternatively, 

#!/usr/bin/r --vanilla

.libPaths('~/R/library')
library(brainwaver)

works, although be careful, I've noticed that it seems to behave a bit
strangely on my debian setup.

e.g.

#!/usr/bin/r --vanilla
cat(.Library,'*****', .libPaths(),"\n")
.libPaths('~/R/library')
cat(.Library,'*****', .libPaths(),"\n")

gives output
/usr/lib/R/library
***** /usr/local/lib/R/site-library /usr/lib/R/site-library /usr/lib/R/library
/usr/lib/R/library ***** ~/R/library /usr/lib/R/library

that is, it seems to have removed /usr/local/lib/R/site-library
and /usr/lib/R/site-library as well as added ~/R/library

Cheers, John.

-- 
Contractor in Cambridge UK -- http://www.aspden.com


From s.ruegg at access.unizh.ch  Wed Jan 10 13:18:04 2007
From: s.ruegg at access.unizh.ch (Simon Ruegg)
Date: Wed, 10 Jan 2007 13:18:04 +0100
Subject: [R] problems with optim, "for"-loops and machine precision
Message-ID: <200701101218.l0ACI48g013139@idmailgate1.unizh.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070110/b6e91959/attachment.pl 

From iriskolder at yahoo.com  Wed Jan 10 13:44:16 2007
From: iriskolder at yahoo.com (Iris Kolder)
Date: Wed, 10 Jan 2007 04:44:16 -0800 (PST)
Subject: [R] Fw: Memory problem on a linux cluster using a large data set
	[Broadcast]
Message-ID: <20070110124416.9546.qmail@web51708.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070110/8a9f665d/attachment.pl 

From finbref.2006 at gmail.com  Wed Jan 10 14:15:40 2007
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Wed, 10 Jan 2007 14:15:40 +0100
Subject: [R] prime in expression in plot
Message-ID: <d0f55a670701100515w586d9884w21af12d5454e87d5@mail.gmail.com>

I want to write something like (LaTeX style)
b_{norm}=\frac{F\prime(0)}{R\prime(0)}

how do I add the "prime" (first derivative) to a R-plot? The help of
"plotmath" just talks about "partialdiff". Can you complete this
command?

text( 30,0.05,labels=expression(b[plain("norm")]==frac(F(0),R(0))) )

Thanks,
Thomas


From jholtman at gmail.com  Wed Jan 10 14:18:42 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 10 Jan 2007 08:18:42 -0500
Subject: [R] select subsets in data frame
In-Reply-To: <45A4C98D.7030200@rakers.de>
References: <45A4C98D.7030200@rakers.de>
Message-ID: <644e1f320701100518r225b6751qbd28715340201a98@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070110/4da14a1f/attachment.pl 

From finbref.2006 at gmail.com  Wed Jan 10 14:36:18 2007
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Wed, 10 Jan 2007 14:36:18 +0100
Subject: [R] prime in expression in plot
In-Reply-To: <d0f55a670701100515w586d9884w21af12d5454e87d5@mail.gmail.com>
References: <d0f55a670701100515w586d9884w21af12d5454e87d5@mail.gmail.com>
Message-ID: <d0f55a670701100536pbd6dc8ch60c4dff8c7825ce1@mail.gmail.com>

> how do I add the "prime" (first derivative) to a R-plot?

sorry for the noise, I found it myself:
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/20984.html
I use now (works fine)
plot(1,1,xlab=expression(frac("F'"(0),"R'"(0))),xaxt="n")
Thomas


From jmb at mssl.ucl.ac.uk  Wed Jan 10 14:43:20 2007
From: jmb at mssl.ucl.ac.uk (Jenny Barnes)
Date: Wed, 10 Jan 2007 13:43:20 +0000 (GMT)
Subject: [R] correlation value and map
Message-ID: <200701101343.l0ADhKmU004299@msslhb.mssl.ucl.ac.uk>

Dear R-help community,

I have 2 different arrays of precipitation data each of the same dimensions of 
[longitude, latitude, time] dim=[30,32,43], called array1 and array2. I need to 
correlate them. This is the code I used to get one overall correlation value for 
the whole of the area of interest:

> result <- cor(array1,array2,use="complete.obs")
> result

This give me a single value but I'm not convinced it is actually a correlation 
value for the total area for the total time period of 43 years....can anybody 
tell me if I am indeed wrong in my coding and/or indeed my low knowledge of the 
statistics of correlation.

Also, I wanted to produce a correlation map over the 43 years. Could you also 
advise me if this is correct, I am more confident that this is than the above 
code:

> result <- array(NA, c(30,32))
> 
> for(i in 1:30){
> for(j in 1:32){
>	array1.ts <- array1[i,j,]
>	array2.ts <- array2[i,j,]
>	result[i,j] <- cor(array1.ts,array2.ts,use= "complete.obs")
> }
> }

I appreciate your time very much. If I don't iron out this problem now the 
ground-work for my entire PhD will not be stable at all,

Many thanks for reading my problem, happy 2007 :-)

Jenny Barnes





~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jennifer Barnes
PhD student - long range drought prediction
Climate Extremes
Department of Space and Climate Physics
University College London
Holmbury St Mary, Dorking
Surrey
RH5 6NT
01483 204149
Web: http://climate.mssl.ucl.ac.uk


From edzabc at gmail.com  Wed Jan 10 14:53:16 2007
From: edzabc at gmail.com (ED)
Date: Wed, 10 Jan 2007 08:53:16 -0500
Subject: [R] Fractional brownian motion
Message-ID: <5b42b3f80701100553q5912a22cq3a94f9b93eb84501@mail.gmail.com>

Dear All;

I have used fbmSim to simulate a fbm sequence, however, when I tried to
estimate the Hurst effect, none of the nine procedures gave me an answer
close enough to the real value, which is 0.5 (n=1000). So, would you please
advice,

1. which is the best method to estimate the H among the 9 mehods, R/S,
higuchi or Whittle?

2. how to choose the levels (default=50), minnpts, cutoff values or if there
is any other issues to consider? Would you please send your code if you can
get the right estimate for the attached dataset?

3. I also simulated multiple sequences, but some of the estimated results
have H>1, how would you explain this, and how to correct it.

4. if I have a sample size of 30, what are your suggestions for estimating
the hurst effect.

I really appreciate your help, and thanks in advance.

Sincerely;

Qing

list(c(0, 0.0602883614054335, 0.0792551804556156, 0.109563140496959,
0.125726475390261, 0.101877497256596, 0.0571926667502657,
0.00262554012667387,
-0.0651825041744684, -0.0696503406796901, -0.0688337926741136,
-0.0361989921035026, -0.0496524166395568, -0.0718218507093852,
-0.109583486993751, -0.0451891749832022, -0.0260755094163026,
-0.0201228029445018, -0.08818052945358, -0.0285680272562049,
-0.032752629105123, -0.00199553309054604, 0.00508312076775159,
-0.0382257524810562, -0.0398084284989033, -0.0502088451482971,
-0.0502183047870017, -0.0174725900800023, -0.031457396692228,
-0.0497298422601627, -0.0499422794745919, -0.0361646489128442,
0.0266653302426084, -0.00219111075589404, 0.0200305010061950,
-0.054431205284715, -0.00789973517059908, -0.0085425214867059,
-0.0541809440614641, -0.110006586793787, -0.127781398474619,
-0.149159749479771, -0.0888724748560444, -0.0768016585462973,
-0.0427333724364433, -0.0562443902614682, -0.062866380084345,
-0.0883889349587378, -0.08412713827269, -0.124694662643563, -
0.0692092347299578,
-0.062046981143375, -0.0853376756560203, -0.0994827776216755,
-0.0599248702862878, -0.086369926350214, -0.0890587510305954,
-0.0599052704934926, -0.0393644348210234, -0.0364342257334110,
-0.0352152731793845, 0.00857219866032972, 0.0180050537574112,
-0.0356947245542089, -0.0615077884110117, -0.0650043906408892,
-0.0357502728853327, 0.00485292461594026, -0.0458490824792766,
-0.0735087346178546, -0.0820482814570969, -0.124917946967105,
-0.102342757999288, -0.154497291254316, -0.181381845540350, -
0.166583783090691,
-0.130709738638527, -0.124693013403996, -0.120174007595549, -
0.204900741024172,
-0.187508161990262, -0.200032164010717, -0.224311714556569, -
0.207979212824500,
-0.232450923263701, -0.198989498760939, -0.215501785889735, -
0.226880158415004,
-0.203545247216445, -0.182379290331033, -0.197038481353429, -
0.183796217531869,
-0.215299419717715, -0.226095790998872, -0.201080854888686, -
0.261516787028393,
-0.228388047078831, -0.233878235687051, -0.250375640774054, -
0.245746001611628,
-0.232938343176222, -0.204702689747725, -0.259350393091606, -
0.266266298423241,
-0.261596784773769, -0.259875864886204, -0.288719457448324, -
0.318140871065267,
-0.304593775001704, -0.263467947228369, -0.323808258544402, -
0.368149351192211,
-0.347335883864181, -0.400024671714471, -0.371544405921674, -
0.34497157633729,
-0.418657437186672, -0.446912741927435, -0.443280747761798, -
0.384739689692638,
-0.408072099289829, -0.44994167683647, -0.495273953054503, -
0.495799924479583,
-0.43697383254148, -0.440650597876129, -0.458929575025839, -
0.464585181026337,
-0.455843249117235, -0.505571057794026, -0.528316203388037, -
0.58236723395483,
-0.519865988395428, -0.555187599676786, -0.532496699903787, -
0.522718022822901,
-0.559598211031558, -0.547080566663308, -0.554634133071482, -
0.541090495738355,
-0.558915651522986, -0.541917600099341, -0.578140869146858, -
0.53827826166188,
-0.52995892714492, -0.502706986562639, -0.497002923985933, -
0.488096453915811,
-0.51306171001298, -0.536938942207764, -0.515525690037527, -
0.527192086185579,
-0.511195825348352, -0.531104487863626, -0.521038783472886, -
0.519956505596612,
-0.510535242028159, -0.531234776730198, -0.526733559471536, -
0.583555032968273,
-0.551226199377139, -0.489093347568197, -0.505836247919194, -
0.485420121220644,
-0.511258369655571, -0.505044073824655, -0.496646898991666, -
0.49767793016268,
-0.496864531091594, -0.545965735276333, -0.588151447068976, -
0.594481697088302,
-0.633356216248837, -0.649947987535778, -0.698052359479746, -
0.651147537552288,
-0.678918599568793, -0.707817506128165, -0.69962378236799, -
0.664061266573256,
-0.686809968963275, -0.70940931290113, -0.710042179498104, -
0.700294406700751,
-0.69089796594532, -0.687411073827526, -0.660740247977237, -0.61857834066703,

-0.611492191585365, -0.617870447722936, -0.590305790642949, -
0.616713901938279,
-0.619126236382548, -0.618591413756597, -0.604886044395737, -
0.598434945889939,
-0.481524771809731, -0.424388823122359, -0.490492676413456, -
0.506402791875861,
-0.480172973435785, -0.517922095978117, -0.502318428525046, -
0.496731810144271,
-0.531408048623419, -0.546479167267974, -0.583073243018783, -
0.532934946326789,
-0.551949267354811, -0.526047642710959, -0.510085874249115, -
0.516695891848832,
-0.577524134973206, -0.552023065025717, -0.486593593181791, -
0.505244962413324,
-0.50233923782961, -0.520060671266441, -0.528767509770539, -
0.540028769162771,
-0.547477720179665, -0.56531037947022, -0.50193374661204, -0.558262849369318,

-0.494995083625537, -0.497213426057201, -0.500258491337576, -
0.461559035461970,
-0.432242682833388, -0.41117570817118, -0.415407961563806, -
0.372008266805533,
-0.372088740001050, -0.391799048717280, -0.377849107962199, -
0.344184542514291,
-0.348413544886666, -0.380049118743689, -0.39517058021528, -
0.435019093917616,
-0.396825266989612, -0.406175696404068, -0.363939520468838, -
0.276849706077195,
-0.299937554612409, -0.268322605052497, -0.291469417623139, -
0.287282726264279,
-0.264242114241804, -0.276046060096993, -0.217798594356271, -
0.175986521497293,
-0.165844402989144, -0.20246139574587, -0.222148041601722, -
0.204831806500164,
-0.153457268552386, -0.176174002538195, -0.156751478372055, -
0.057878180960031,
-0.0832537960834704, -0.032455515861184, -0.03837970395084,
0.0108416606722306,
0.00435135652516059, -0.0183482961380528, 0.000731922390127772,
0.00999593602758636, 0.0181517073212863, -0.0110713504391765,
-0.00717138666398991, -0.0244655145429809, -0.0435081997698830,
-0.0723796931986123, -0.0812497739510341, -0.167671363055567,
-0.153618438366021, -0.177425107515775, -0.233451742001837, -
0.294328321350601,
-0.262070533304864, -0.304696117238305, -0.308367573404234, -
0.349836635531722,
-0.378702938622887, -0.371478566955775, -0.406474391212437, -
0.427722230759569,
-0.433991757828819, -0.450189851480229, -0.472370053196594, -
0.489099244001516,
-0.489467600993215, -0.485596891281682, -0.49779042627552, -0.46165300963434,

-0.448086803874086, -0.443670308509739, -0.45288256928109, -
0.467547617361111,
-0.411723602022512, -0.454977398955907, -0.444371010673532, -
0.422930806587484,
-0.407809257077638, -0.42665889906324, -0.439836785887343, -0.39462640277382,

-0.456506177242119, -0.417786390221254, -0.3945654618087, -0.380526113175225,

-0.435803862954649, -0.408673249901993, -0.443310483445116, -
0.435285529615121,
-0.391246171534911, -0.423719854507996, -0.408551661568188, -
0.450754029735524,
-0.472602255523911, -0.508979053023968, -0.48467115752141, -
0.483538980395181,
-0.48117952852397, -0.527682963662319, -0.530048158756582, -
0.516321493054729,
-0.505999169443683, -0.481713418746599, -0.509650229070124, -
0.497698126365811,
-0.498898861303871, -0.513440145657546, -0.506018315131867, -
0.513210881839601,
-0.517653436271622, -0.517966483189416, -0.493197050249104, -
0.457616082862051,
-0.445730060732531, -0.445076775112659, -0.418087113066591, -
0.36276831926877,
-0.375690724221468, -0.377881676382754, -0.371887783371946, -
0.425624891596683,
-0.381690010011701, -0.364221504087779, -0.382256197499271, -
0.427591005336707,
-0.400395407899293, -0.384022614564292, -0.418485018168370, -
0.439423306152362,
-0.387746741397525, -0.419803656308417, -0.446329175681457, -
0.485574391008224,
-0.494700574968507, -0.476085998563161, -0.485009731706272, -
0.486608912431453,
-0.446653718022881, -0.449368213900281, -0.448383989719459, -
0.448038045747502,
-0.446146531320324, -0.412949964719027, -0.409053112488588, -
0.381923754371735,
-0.373175213060887, -0.37981480418489, -0.418067908641623, -0.45013561487107,

-0.449711660269272, -0.439274610780474, -0.385770827914678, -
0.372747104530212,
-0.386867708368611, -0.388172052642435, -0.357844117340447, -
0.378325067585380,
-0.367948775430377, -0.356183512307615, -0.368755500006355, -
0.347651383123023,
-0.335184483613088, -0.329242820022007, -0.321073285008462, -
0.340146136324349,
-0.311831319422315, -0.320217397857061, -0.356625496546983, -
0.372687092716155,
-0.304876020028656, -0.313825909214317, -0.258608442215043, -
0.180512658085543,
-0.166410400807192, -0.210524515575469, -0.247866669513052, -
0.263465896656237,
-0.304407304525663, -0.294349034314515, -0.286981852571012, -
0.289152655728849,
-0.222120657232285, -0.201859415849224, -0.224396781073319, -
0.252214854031049,
-0.241481461911827, -0.254155560259281, -0.234920660548415, -
0.266435837149800,
-0.295460049739560, -0.301573757633749, -0.293580836871516, -
0.286929047556988,
-0.277698455775873, -0.273735769220511, -0.300415449827285, -
0.32491594248558,
-0.323226612941381, -0.324478124714544, -0.343944391275079, -
0.332753931098016,
-0.354544735052724, -0.351136814924285, -0.443972485195655, -
0.480378162322601,
-0.456466271168926, -0.490811357473404, -0.495228423726588, -
0.473875567999266,
-0.461202095397426, -0.428236514267841, -0.43316660778517, -
0.441026820386022,
-0.391501536306998, -0.383834931520124, -0.446076138382415, -
0.471862767230103,
-0.484409089668503, -0.512638847721949, -0.57615753476778, -
0.612910121929396,
-0.59409101483892, -0.576073124820514, -0.558917860644171, -0.51741302487145,

-0.444874701848801, -0.452634836362976, -0.501076461133736, -
0.506139148830469,
-0.477096807758099, -0.47903948936219, -0.474304789092253, -
0.519983954579823,
-0.536006287118978, -0.533221006407308, -0.530703365114487, -
0.489301276239221,
-0.482504240833116, -0.482672471692736, -0.496699054247686, -
0.486365502289346,
-0.478302125166766, -0.41699619892767, -0.419811322713434, -
0.461764943597718,
-0.444944325641512, -0.482067207681564, -0.453787868299434, -
0.445862832779393,
-0.500514950852453, -0.54597180181228, -0.59370469930558, -0.622642218811955,

-0.654856796909716, -0.662951487359358, -0.628950615954113, -
0.639558958447642,
-0.68318330922208, -0.696308156898031, -0.672057658680688, -
0.661777110541341,
-0.60490005242343, -0.604182417214347, -0.592238365842442, -0.56106239727129,

-0.604686121506906, -0.56831788795367, -0.575747374999608, -
0.576470254631367,
-0.599655515108991, -0.576602039304853, -0.620742702835376, -
0.623936272446067,
-0.678729593640248, -0.700089982580816, -0.724519962982749, -
0.744258196887065,
-0.77249579598234, -0.767772880386214, -0.830516027924284, -
0.780768413310955,
-0.770728227679768, -0.807497232811467, -0.879388092402374, -
0.877323508209862,
-0.90594146000351, -0.915337533037419, -0.960077494402497, -
0.996453875810982,
-1.00171935025677, -0.971945793840293, -1.02385322331957, -1.03545216489193,

-1.03042547742818, -1.05688598006666, -1.05132715324897, -1.03923643510103,
-1.04441463738159, -1.02606950821931, -1.02207418726124, -1.09384167822686,
-1.10759852188960, -1.03022988278253, -0.979906979321654, -0.98140738601237,

-0.955516498832963, -0.939980354153678, -0.975803371605102, -
0.978220217918253,
-0.91696505099489, -0.94341626578855, -0.89133658495402, -0.855700592049311,

-0.895033078919327, -0.89627932177841, -0.938753796944238, -
0.950927820337492,
-0.96380837671684, -0.974038158542758, -1.01277050185216, -1.02419954622913,

-1.08846210248524, -1.10659736422949, -1.09389877696098, -1.12153568516753,
-1.09295618980445, -1.07529181899901, -1.03767165603343, -1.02442281832222,
-1.07439802122888, -1.05377483017375, -1.0485017136285, -1.05988777343173,
-1.05902132289274, -1.00325932676755, -0.97680173673455, -0.974825258532387,

-1.01655342958140, -0.977698033964966, -0.92760046211231, -0.955327438302301,

-0.99713716544251, -0.971561439917154, -0.980895949333255, -
0.968004870171057,
-0.97614953144124, -1.03366215332186, -1.04409457003187, -1.05158861021004,
-1.09196993450594, -1.11264011244533, -1.06717708434083, -1.06480370833963,
-1.08575759875714, -1.09341227228900, -1.09177567504520, -1.06910800772099,
-1.08712926865485, -1.09686409752487, -1.09774736862532, -1.11213772549334,
-1.15403915449551, -1.22373850734154, -1.21648676898269, -1.22018337968052,
-1.24952942783237, -1.27656503710055, -1.25972114225839, -1.24703179692361,
-1.20919050916248, -1.1842207616518, -1.19287391961853, -1.19721171380420,
-1.19501363104455, -1.18939215272863, -1.14678652882893, -1.15465661360344,
-1.11813358574639, -1.10040843911149, -1.16032096536330, -1.14919807942398,
-1.19048875998298, -1.19989772675020, -1.20398620784167, -1.26586491882295,
-1.25851869647003, -1.26544165917516, -1.22693569033306, -1.25725659511552,
-1.21010427615498, -1.34232359165001, -1.32660610047776, -1.29610987865021,
-1.26013011718281, -1.21766261392912, -1.17439826445927, -1.17622836607061,
-1.19410480592852, -1.20980753722721, -1.23210362227815, -1.22356037273062,
-1.28685023169650, -1.31147384975096, -1.31325943087577, -1.32294429181585,
-1.27764956444795, -1.28185065227558, -1.30324581243296, -1.35720257804154,
-1.31485149250653, -1.31249005177055, -1.33192918666462, -1.32949449292688,
-1.34651906053735, -1.37881888127112, -1.36863148386580, -1.35635968160029,
-1.31362991720627, -1.33018045547167, -1.31016961130448, -1.28971757113766,
-1.27303302564893, -1.31791694333118, -1.30060097705289, -1.31787074800653,
-1.29868914777550, -1.30857532844143, -1.28573219864486, -1.32410102371601,
-1.33639969073421, -1.34007415835075, -1.38507735502386, -1.34107837659624,
-1.32257301638253, -1.31159626761432, -1.30609936880576, -1.29975894236571,
-1.27624841277619, -1.25405147690197, -1.22910085941265, -1.20795472697643,
-1.24853851088728, -1.27438582338932, -1.26400606495224, -1.23302999402678,
-1.23178880710196, -1.24518199542743, -1.23331612167119, -1.22903127206189,
-1.30490842217184, -1.28576361752545, -1.31074792460780, -1.30894341383401,
-1.25965302705916, -1.24232528469753, -1.19631320277742, -1.18300183600512,
-1.17795022721718, -1.17403776779038, -1.12859939113535, -1.17956095926659,
-1.14352875660976, -1.15024124231263, -1.12826553775509, -1.13025738690559,
-1.13484753813885, -1.13056767812354, -1.15714576109888, -1.14068343396429,
-1.12261477644592, -1.09392813920472, -1.13016278463302, -1.12804293615061,
-1.15706655799472, -1.19894078640923, -1.13702840262820, -1.13987336894122,
-1.08890542138072, -1.07906342544466, -1.09459214157365, -1.10581794169538,
-1.05652075609808, -1.09629145098239, -1.06052971723101, -1.03095393415127,
-1.03757475536182, -1.04433286723235, -1.04144782691406, -1.05020397226132,
-1.11335366423297, -1.13308242788617, -1.11881260434659, -1.10274785767592,
-1.10409688950570, -1.07848493782634, -1.05652152591989, -1.10303694008154,
-1.13047872199259, -1.11449882444427, -1.10209602713833, -1.08728702613152,
-1.08016151023848, -1.14207794356016, -1.14108708993475, -1.11437528006918,
-1.13074737179464, -1.10080001358879, -1.13380131056501, -1.02974340003599,
-1.03118429349646, -1.01159974250007, -1.01706186526495, -1.01352921118355,
-0.990699389037849, -1.03145411246970, -1.08279225328073, -1.08031351179812,

-1.12183163205751, -1.13694519045933, -1.10873154083012, -1.09048086125836,
-1.10496312564202, -1.12271481183688, -1.14404655640216, -1.10727842881992,
-1.09827438761092, -1.08984538074143, -1.02245675642832, -0.982400792644428,

-1.00758031375945, -0.997985602590168, -1.01785304453019, -1.01973792693547,

-1.05359645335894, -1.04117220263969, -1.00443264885732, -0.94462766276219,
-0.93052780914025, -0.901150831889696, -0.876218739372558, -
0.889781755283235,
-0.922689983868312, -0.986812563893075, -1.00577437687499, -1.05528836194302,

-1.06361757010428, -1.05509423969802, -1.07764076165386, -1.05393419503949,
-1.03141748838034, -1.07666158378067, -1.08826736188342, -1.03825851781967,
-1.05838978627658, -1.05513301624640, -1.10147358321021, -1.09601673752277,
-1.10974357792115, -1.10308596910112, -1.13992099599508, -1.15495903878899,
-1.12672669661503, -1.15364353419803, -1.14456118003669, -1.13725384673776,
-1.12789761787739, -1.08972321956154, -1.06817521360676, -1.00612585643724,
-1.01721188607368, -1.00086920668929, -1.01165701637624, -0.996640056212344,

-0.98313986002211, -0.984299091792497, -0.96583710325524, -1.00232690455114,

-0.982922825052307, -1.02459712149803, -1.07881579816258, -1.03163543147277,

-1.07729789184894, -1.01180574553278, -1.06293062885976, -1.04345761615412,
-1.04507118212109, -1.06662413792108, -1.10636689376820, -1.12222320840262,
-1.16318956376199, -1.22192850138826, -1.24348755430912, -1.25220663040648,
-1.22215708457057, -1.25236192402018, -1.22111129025221, -1.24724680332852,
-1.22611890659054, -1.23082404646155, -1.21893054743993, -1.22794164100034,
-1.24178410222557, -1.23491411880606, -1.21518417771487, -1.17966072752994,
-1.15118328785644, -1.11210377352095, -1.09023084325471, -1.11849942190869,
-1.08675131543657, -1.09597339581439, -1.10809871281075, -1.10221229540946,
-1.07320636516498, -1.12163406259843, -1.12479846098100, -1.11007095163810,
-1.18712453125076, -1.17536343414050, -1.18780217004682, -1.18605983953425,
-1.18095143598242, -1.18140299327442, -1.20254462890750, -1.18702354604552,
-1.18338989324559, -1.25961943500943, -1.26207336479591, -1.27358337177702,
-1.28871107136099, -1.23882265409051, -1.25505293124848, -1.18747335150791,
-1.22188479870869, -1.22512921075359, -1.27267669879780, -1.29897687823820,
-1.30983284713907, -1.31092491248901, -1.30162429691371, -1.33581710215692,
-1.37449534029653, -1.3624275416685, -1.38856284059204, -1.37690066885045,
-1.35812270817097, -1.26439412795243, -1.29012098316903, -1.30021268606416,
-1.29777924276391, -1.3330372548974, -1.29014528078931, -1.27029202224966,
-1.28153308247510, -1.27170307834150, -1.29079391277124, -1.27810834543040,
-1.29612763984711, -1.29674584868698, -1.26923823763037, -1.27642570501084,
-1.23329549212125, -1.24558112749617, -1.21620176421750, -1.23394969740689,
-1.21143465389589, -1.19397690475935, -1.20538829239449, -1.22385912544701,
-1.22335448659759, -1.18764026930546, -1.18460739308550, -1.18544657910073,
-1.17488653166367, -1.21217413413652, -1.24804440395677, -1.27705906222957,
-1.24670841596142, -1.22433660324650, -1.14970333463809, -1.16375279138769,
-1.21568964613097, -1.21945917869375, -1.22821658985229, -1.19934594306181,
-1.14643724079028, -1.12643944455629, -1.12233497987804, -1.07034913586819,
-1.05097392060227, -1.1717978905894, -1.16695008848828, -1.20329793233844,
-1.22449878968466, -1.23620777156718, -1.29283095807763, -1.30456841492716,
-1.29956251426570, -1.35321339995369, -1.30781507790772, -1.25820955708785,
-1.20144277085079, -1.20233755352439, -1.1957463134694, -1.24548177598815,
-1.23362870938309, -1.23751123003283, -1.16881036271405, -1.17867785852131,
-1.13299980838684, -1.13575190845236, -1.15265011862258, -1.13951844565801,
-1.20445908347423, -1.19738791438949, -1.17891493280400, -1.14633612729150,
-1.18945230462923, -1.25911000157446, -1.21092734531082, -1.19892430224433,
-1.20094139684876, -1.20934910682816, -1.19981222813604, -1.20380034427012,
-1.24216556039083, -1.27832948787905, -1.29714241228392, -1.24544589067096,
-1.22418764631655, -1.1588736470015, -1.13559976408933, -1.14546273748605,
-1.07796573614627, -1.08111480654649, -1.05358267953188, -1.08838275274950,
-1.06160134081697, -1.03782319420854, -1.01718992134545, -0.98004212412867,
-0.995178761325842, -1.03019373420257, -1.05128326974742, -1.05013618518556,

-1.09826884471200, -1.09809384521198, -1.08519850594646, -1.01064238573864,
-1.03473773757524, -1.04378027348155, -1.06594802906565, -1.05967833110358,
-1.09239793955592, -1.10372033897668, -1.11398074918965, -1.07142615682281,
-1.04147669240454, -1.02369590981874, -1.00377810316095, -0.992689744428071,

-1.0585698310738, -1.08344112379608, -1.10562560362484, -1.09246302856515,
-1.08639186584488, -1.10794139071165, -1.08098657419634, -1.03139971026396,
-1.03491525139187, -1.00675304495887, -1.0124392044228, -0.992602075541911
))


From shubhak at ambaresearch.com  Wed Jan 10 14:58:53 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Wed, 10 Jan 2007 19:28:53 +0530
Subject: [R] Column names in Zoo object
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3C9F8EA@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070110/68506176/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Jan 10 15:05:33 2007
From: ripley at stats.ox.ac.uk (Brian Ripley)
Date: Wed, 10 Jan 2007 14:05:33 +0000 (GMT)
Subject: [R] scripts with littler
In-Reply-To: <eo2l88$v2m$1@sea.gmane.org>
References: <ent9nm$rc6$1@sea.gmane.org>
	<20070108132920.GA4640@phenix.progiciels-bpi.ca>
	<entlul$ng$1@sea.gmane.org>
	<971536df0701080703q48e04a5cv96720798b8b30c11@mail.gmail.com>
	<Pine.LNX.4.64.0701082235040.9254@gannet.stats.ox.ac.uk>
	<eo0bvb$sr0$1@sea.gmane.org>
	<eo0ngg$7p1$1@sea.gmane.org> <45A3F289.40705@vanderbilt.edu>
	<eo12g2$lme$1@sea.gmane.org> <eo2l88$v2m$1@sea.gmane.org>
Message-ID: <Pine.LNX.4.64.0701101354240.12401@auk.stats>

[By now way off the subject line.  Something like 'how to set the 
libraries inside an R session'.]

On Wed, 10 Jan 2007, John Lawrence Aspden wrote:

> John Lawrence Aspden wrote:
>
>> I've got a library (brainwaver), installed locally in ~/R/library, and
>> this information is recorded in the ~/.Renviron file.
>
>> In my script I load the library, but if I call it using
>> #!/usr/bin/r --vanilla, this stops working.
>
> (Various private e-mails exchanged. Again, thanks Dirk!)
>
> Just in case anyone else is trying to do this, it turns out that if you can
> persuade your end users to install the library to ~/R/library, then you can
> say:
>
> #!/usr/bin/r --vanilla
> library(brainwaver, lib.loc='~/R/library')
>
> although in my case, brainwaver depends on another library, which it now
> can't find, so actually I have to load them in order:
>
> #!/usr/bin/r --vanilla
>
> library(waveslim, lib.loc='~/R/library')
> library(brainwaver, lib.loc='~/R/library')
>
> Alternatively,
>
> #!/usr/bin/r --vanilla
>
> .libPaths('~/R/library')
> library(brainwaver)
>
> works, although be careful, I've noticed that it seems to behave a bit
> strangely on my debian setup.

'It' (R) is behaving as you asked it to.  Most likely you intended to ask 
for

.libPaths(c("~/R/library", .libPaths()))

> e.g.
>
> #!/usr/bin/r --vanilla
> cat(.Library,'*****', .libPaths(),"\n")
> .libPaths('~/R/library')
> cat(.Library,'*****', .libPaths(),"\n")
>
> gives output
> /usr/lib/R/library
> ***** /usr/local/lib/R/site-library /usr/lib/R/site-library /usr/lib/R/library
> /usr/lib/R/library ***** ~/R/library /usr/lib/R/library
>
> that is, it seems to have removed /usr/local/lib/R/site-library
> and /usr/lib/R/site-library as well as added ~/R/library

Exactly as documented.  The argument is named 'new' and not 'add', BTW.
Please 'be careful' in what you say about the work of others.

>
> Cheers, John.
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From shubhak at ambaresearch.com  Wed Jan 10 15:10:36 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Wed, 10 Jan 2007 19:40:36 +0530
Subject: [R] Column names in Zoo object/BlpGetata problem
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3C9F8F9@BAN-MAILSRV03.Amba.com>

I think the problem is in the BlpGetData function than in the zoo
object. Because instead of zoo object, the data was put into a data
frame, but then also the problem prevails.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Shubha Vishwanath
Karanth
Sent: Wednesday, January 10, 2007 7:29 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Column names in Zoo object

Hi,

 

I am downloading Bloomberg data from R. This data will be stored in a
zoo object by default. The command is

 

dat<-blpGetData(con,c("NOK1V FH Equity","AUA AV
Equity"),"PX_OPEN",start=as.chron(as.Date("12/1/2006",
"%m/%d/%Y")),end=as.chron(as.Date("12/28/2006", "%m/%d/%Y")))

Here I am downloading the data for two tickers, ("NOK1V FH Equity" and
"AUA AV Equity") simultaneously. Then the column names of my zoo object
will be NOK1V.FH.Equity and AUA.AV.Equity respectively, which are the
ticker names itself.

 

But if I download for only one ticker by the code,

dat<-blpGetData(con,c("NOK1V FH Equity
"),"PX_OPEN",start=as.chron(as.Date("12/1/2006",
"%m/%d/%Y")),end=as.chron(as.Date("12/28/2006", "%m/%d/%Y")))

Now, the column name of my zoo object is "PX_OPEN", the field name
instead of the ticker name "NOK1V FH Equity".

 

I need my second code to work as the first one. i.e., if only one ticker
is used instead of two or more, then the column name taken by my zoo
object should be the ticker name and not the field name. Could somebody
help me on this?

 

Thanks in advance.

 

 

 

 


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Peter.Rabinovitch at alcatel-lucent.com  Wed Jan 10 15:42:00 2007
From: Peter.Rabinovitch at alcatel-lucent.com (RABINOVITCH Peter)
Date: Wed, 10 Jan 2007 08:42:00 -0600
Subject: [R] Fractional brownian motion
In-Reply-To: <5b42b3f80701100553q5912a22cq3a94f9b93eb84501@mail.gmail.com>
Message-ID: <06CEB8C71520C045B6C490901B6A2CD50584EB11@USDALSMBS01.ad3.ad.alcatel.com>

You have applied the Hurst parameter estimating function to the fBm, not
the fGn as described in the help page for Long Range Dependence
Modelling in fSeries.

"The function aggvarFit computes the Hurst exponent from the variance of
an aggregated FGN or FARIMA time series process"

Trying perFit(diff(x)) (where x is your fBm series) gives an estimate of
the Hurst Exponent as 0.53527922 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ED
Sent: Wednesday, January 10, 2007 8:53 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Fractional brownian motion

Dear All;

I have used fbmSim to simulate a fbm sequence, however, when I tried to
estimate the Hurst effect, none of the nine procedures gave me an answer
close enough to the real value, which is 0.5 (n=1000). So, would you
please advice,

1. which is the best method to estimate the H among the 9 mehods, R/S,
higuchi or Whittle?

2. how to choose the levels (default=50), minnpts, cutoff values or if
there is any other issues to consider? Would you please send your code
if you can get the right estimate for the attached dataset?

3. I also simulated multiple sequences, but some of the estimated
results have H>1, how would you explain this, and how to correct it.

4. if I have a sample size of 30, what are your suggestions for
estimating the hurst effect.

I really appreciate your help, and thanks in advance.

Sincerely;

Qing

list(c(0, 0.0602883614054335, 0.0792551804556156, 0.109563140496959,
0.125726475390261, 0.101877497256596, 0.0571926667502657,
0.00262554012667387, -0.0651825041744684, -0.0696503406796901,
-0.0688337926741136, -0.0361989921035026, -0.0496524166395568,
-0.0718218507093852, -0.109583486993751, -0.0451891749832022,
-0.0260755094163026, -0.0201228029445018, -0.08818052945358,
-0.0285680272562049, -0.032752629105123, -0.00199553309054604,
0.00508312076775159, -0.0382257524810562, -0.0398084284989033,
-0.0502088451482971, -0.0502183047870017, -0.0174725900800023,
-0.031457396692228, -0.0497298422601627, -0.0499422794745919,
-0.0361646489128442, 0.0266653302426084, -0.00219111075589404,
0.0200305010061950, -0.054431205284715, -0.00789973517059908,
-0.0085425214867059, -0.0541809440614641, -0.110006586793787,
-0.127781398474619, -0.149159749479771, -0.0888724748560444,
-0.0768016585462973, -0.0427333724364433, -0.0562443902614682,
-0.062866380084345, -0.0883889349587378, -0.08412713827269,
-0.124694662643563, - 0.0692092347299578, -0.062046981143375,
-0.0853376756560203, -0.0994827776216755, -0.0599248702862878,
-0.086369926350214, -0.0890587510305954, -0.0599052704934926,
-0.0393644348210234, -0.0364342257334110, -0.0352152731793845,
0.00857219866032972, 0.0180050537574112, -0.0356947245542089,
-0.0615077884110117, -0.0650043906408892, -0.0357502728853327,
0.00485292461594026, -0.0458490824792766, -0.0735087346178546,
-0.0820482814570969, -0.124917946967105, -0.102342757999288,
-0.154497291254316, -0.181381845540350, - 0.166583783090691,
-0.130709738638527, -0.124693013403996, -0.120174007595549, -
0.204900741024172, -0.187508161990262, -0.200032164010717,
-0.224311714556569, - 0.207979212824500, -0.232450923263701,
-0.198989498760939, -0.215501785889735, - 0.226880158415004,
-0.203545247216445, -0.182379290331033, -0.197038481353429, -
0.183796217531869, -0.215299419717715, -0.226095790998872,
-0.201080854888686, - 0.261516787028393, -0.228388047078831,
-0.233878235687051, -0.250375640774054, - 0.245746001611628,
-0.232938343176222, -0.204702689747725, -0.259350393091606, -
0.266266298423241, -0.261596784773769, -0.259875864886204,
-0.288719457448324, - 0.318140871065267, -0.304593775001704,
-0.263467947228369, -0.323808258544402, - 0.368149351192211,
-0.347335883864181, -0.400024671714471, -0.371544405921674, -
0.34497157633729, -0.418657437186672, -0.446912741927435,
-0.443280747761798, - 0.384739689692638, -0.408072099289829,
-0.44994167683647, -0.495273953054503, - 0.495799924479583,
-0.43697383254148, -0.440650597876129, -0.458929575025839, -
0.464585181026337, -0.455843249117235, -0.505571057794026,
-0.528316203388037, - 0.58236723395483, -0.519865988395428,
-0.555187599676786, -0.532496699903787, - 0.522718022822901,
-0.559598211031558, -0.547080566663308, -0.554634133071482, -
0.541090495738355, -0.558915651522986, -0.541917600099341,
-0.578140869146858, - 0.53827826166188, -0.52995892714492,
-0.502706986562639, -0.497002923985933, - 0.488096453915811,
-0.51306171001298, -0.536938942207764, -0.515525690037527, -
0.527192086185579, -0.511195825348352, -0.531104487863626,
-0.521038783472886, - 0.519956505596612, -0.510535242028159,
-0.531234776730198, -0.526733559471536, - 0.583555032968273,
-0.551226199377139, -0.489093347568197, -0.505836247919194, -
0.485420121220644, -0.511258369655571, -0.505044073824655,
-0.496646898991666, - 0.49767793016268, -0.496864531091594,
-0.545965735276333, -0.588151447068976, - 0.594481697088302,
-0.633356216248837, -0.649947987535778, -0.698052359479746, -
0.651147537552288, -0.678918599568793, -0.707817506128165,
-0.69962378236799, - 0.664061266573256, -0.686809968963275,
-0.70940931290113, -0.710042179498104, - 0.700294406700751,
-0.69089796594532, -0.687411073827526, -0.660740247977237,
-0.61857834066703,

-0.611492191585365, -0.617870447722936, -0.590305790642949, -
0.616713901938279, -0.619126236382548, -0.618591413756597,
-0.604886044395737, - 0.598434945889939, -0.481524771809731,
-0.424388823122359, -0.490492676413456, - 0.506402791875861,
-0.480172973435785, -0.517922095978117, -0.502318428525046, -
0.496731810144271, -0.531408048623419, -0.546479167267974,
-0.583073243018783, - 0.532934946326789, -0.551949267354811,
-0.526047642710959, -0.510085874249115, - 0.516695891848832,
-0.577524134973206, -0.552023065025717, -0.486593593181791, -
0.505244962413324, -0.50233923782961, -0.520060671266441,
-0.528767509770539, - 0.540028769162771, -0.547477720179665,
-0.56531037947022, -0.50193374661204, -0.558262849369318,

-0.494995083625537, -0.497213426057201, -0.500258491337576, -
0.461559035461970, -0.432242682833388, -0.41117570817118,
-0.415407961563806, - 0.372008266805533, -0.372088740001050,
-0.391799048717280, -0.377849107962199, - 0.344184542514291,
-0.348413544886666, -0.380049118743689, -0.39517058021528, -
0.435019093917616, -0.396825266989612, -0.406175696404068,
-0.363939520468838, - 0.276849706077195, -0.299937554612409,
-0.268322605052497, -0.291469417623139, - 0.287282726264279,
-0.264242114241804, -0.276046060096993, -0.217798594356271, -
0.175986521497293, -0.165844402989144, -0.20246139574587,
-0.222148041601722, - 0.204831806500164, -0.153457268552386,
-0.176174002538195, -0.156751478372055, - 0.057878180960031,
-0.0832537960834704, -0.032455515861184, -0.03837970395084,
0.0108416606722306, 0.00435135652516059, -0.0183482961380528,
0.000731922390127772, 0.00999593602758636, 0.0181517073212863,
-0.0110713504391765, -0.00717138666398991, -0.0244655145429809,
-0.0435081997698830, -0.0723796931986123, -0.0812497739510341,
-0.167671363055567, -0.153618438366021, -0.177425107515775,
-0.233451742001837, - 0.294328321350601, -0.262070533304864,
-0.304696117238305, -0.308367573404234, - 0.349836635531722,
-0.378702938622887, -0.371478566955775, -0.406474391212437, -
0.427722230759569, -0.433991757828819, -0.450189851480229,
-0.472370053196594, - 0.489099244001516, -0.489467600993215,
-0.485596891281682, -0.49779042627552, -0.46165300963434,

-0.448086803874086, -0.443670308509739, -0.45288256928109, -
0.467547617361111, -0.411723602022512, -0.454977398955907,
-0.444371010673532, - 0.422930806587484, -0.407809257077638,
-0.42665889906324, -0.439836785887343, -0.39462640277382,

-0.456506177242119, -0.417786390221254, -0.3945654618087,
-0.380526113175225,

-0.435803862954649, -0.408673249901993, -0.443310483445116, -
0.435285529615121, -0.391246171534911, -0.423719854507996,
-0.408551661568188, - 0.450754029735524, -0.472602255523911,
-0.508979053023968, -0.48467115752141, - 0.483538980395181,
-0.48117952852397, -0.527682963662319, -0.530048158756582, -
0.516321493054729, -0.505999169443683, -0.481713418746599,
-0.509650229070124, - 0.497698126365811, -0.498898861303871,
-0.513440145657546, -0.506018315131867, - 0.513210881839601,
-0.517653436271622, -0.517966483189416, -0.493197050249104, -
0.457616082862051, -0.445730060732531, -0.445076775112659,
-0.418087113066591, - 0.36276831926877, -0.375690724221468,
-0.377881676382754, -0.371887783371946, - 0.425624891596683,
-0.381690010011701, -0.364221504087779, -0.382256197499271, -
0.427591005336707, -0.400395407899293, -0.384022614564292,
-0.418485018168370, - 0.439423306152362, -0.387746741397525,
-0.419803656308417, -0.446329175681457, - 0.485574391008224,
-0.494700574968507, -0.476085998563161, -0.485009731706272, -
0.486608912431453, -0.446653718022881, -0.449368213900281,
-0.448383989719459, - 0.448038045747502, -0.446146531320324,
-0.412949964719027, -0.409053112488588, - 0.381923754371735,
-0.373175213060887, -0.37981480418489, -0.418067908641623,
-0.45013561487107,

-0.449711660269272, -0.439274610780474, -0.385770827914678, -
0.372747104530212, -0.386867708368611, -0.388172052642435,
-0.357844117340447, - 0.378325067585380, -0.367948775430377,
-0.356183512307615, -0.368755500006355, - 0.347651383123023,
-0.335184483613088, -0.329242820022007, -0.321073285008462, -
0.340146136324349, -0.311831319422315, -0.320217397857061,
-0.356625496546983, - 0.372687092716155, -0.304876020028656,
-0.313825909214317, -0.258608442215043, - 0.180512658085543,
-0.166410400807192, -0.210524515575469, -0.247866669513052, -
0.263465896656237, -0.304407304525663, -0.294349034314515,
-0.286981852571012, - 0.289152655728849, -0.222120657232285,
-0.201859415849224, -0.224396781073319, - 0.252214854031049,
-0.241481461911827, -0.254155560259281, -0.234920660548415, -
0.266435837149800, -0.295460049739560, -0.301573757633749,
-0.293580836871516, - 0.286929047556988, -0.277698455775873,
-0.273735769220511, -0.300415449827285, - 0.32491594248558,
-0.323226612941381, -0.324478124714544, -0.343944391275079, -
0.332753931098016, -0.354544735052724, -0.351136814924285,
-0.443972485195655, - 0.480378162322601, -0.456466271168926,
-0.490811357473404, -0.495228423726588, - 0.473875567999266,
-0.461202095397426, -0.428236514267841, -0.43316660778517, -
0.441026820386022, -0.391501536306998, -0.383834931520124,
-0.446076138382415, - 0.471862767230103, -0.484409089668503,
-0.512638847721949, -0.57615753476778, - 0.612910121929396,
-0.59409101483892, -0.576073124820514, -0.558917860644171,
-0.51741302487145,

-0.444874701848801, -0.452634836362976, -0.501076461133736, -
0.506139148830469, -0.477096807758099, -0.47903948936219,
-0.474304789092253, - 0.519983954579823, -0.536006287118978,
-0.533221006407308, -0.530703365114487, - 0.489301276239221,
-0.482504240833116, -0.482672471692736, -0.496699054247686, -
0.486365502289346, -0.478302125166766, -0.41699619892767,
-0.419811322713434, - 0.461764943597718, -0.444944325641512,
-0.482067207681564, -0.453787868299434, - 0.445862832779393,
-0.500514950852453, -0.54597180181228, -0.59370469930558,
-0.622642218811955,

-0.654856796909716, -0.662951487359358, -0.628950615954113, -
0.639558958447642, -0.68318330922208, -0.696308156898031,
-0.672057658680688, - 0.661777110541341, -0.60490005242343,
-0.604182417214347, -0.592238365842442, -0.56106239727129,

-0.604686121506906, -0.56831788795367, -0.575747374999608, -
0.576470254631367, -0.599655515108991, -0.576602039304853,
-0.620742702835376, - 0.623936272446067, -0.678729593640248,
-0.700089982580816, -0.724519962982749, - 0.744258196887065,
-0.77249579598234, -0.767772880386214, -0.830516027924284, -
0.780768413310955, -0.770728227679768, -0.807497232811467,
-0.879388092402374, - 0.877323508209862, -0.90594146000351,
-0.915337533037419, -0.960077494402497, - 0.996453875810982,
-1.00171935025677, -0.971945793840293, -1.02385322331957,
-1.03545216489193,

-1.03042547742818, -1.05688598006666, -1.05132715324897,
-1.03923643510103, -1.04441463738159, -1.02606950821931,
-1.02207418726124, -1.09384167822686, -1.10759852188960,
-1.03022988278253, -0.979906979321654, -0.98140738601237,

-0.955516498832963, -0.939980354153678, -0.975803371605102, -
0.978220217918253, -0.91696505099489, -0.94341626578855,
-0.89133658495402, -0.855700592049311,

-0.895033078919327, -0.89627932177841, -0.938753796944238, -
0.950927820337492, -0.96380837671684, -0.974038158542758,
-1.01277050185216, -1.02419954622913,

-1.08846210248524, -1.10659736422949, -1.09389877696098,
-1.12153568516753, -1.09295618980445, -1.07529181899901,
-1.03767165603343, -1.02442281832222, -1.07439802122888,
-1.05377483017375, -1.0485017136285, -1.05988777343173,
-1.05902132289274, -1.00325932676755, -0.97680173673455,
-0.974825258532387,

-1.01655342958140, -0.977698033964966, -0.92760046211231,
-0.955327438302301,

-0.99713716544251, -0.971561439917154, -0.980895949333255, -
0.968004870171057, -0.97614953144124, -1.03366215332186,
-1.04409457003187, -1.05158861021004, -1.09196993450594,
-1.11264011244533, -1.06717708434083, -1.06480370833963,
-1.08575759875714, -1.09341227228900, -1.09177567504520,
-1.06910800772099, -1.08712926865485, -1.09686409752487,
-1.09774736862532, -1.11213772549334, -1.15403915449551,
-1.22373850734154, -1.21648676898269, -1.22018337968052,
-1.24952942783237, -1.27656503710055, -1.25972114225839,
-1.24703179692361, -1.20919050916248, -1.1842207616518,
-1.19287391961853, -1.19721171380420, -1.19501363104455,
-1.18939215272863, -1.14678652882893, -1.15465661360344,
-1.11813358574639, -1.10040843911149, -1.16032096536330,
-1.14919807942398, -1.19048875998298, -1.19989772675020,
-1.20398620784167, -1.26586491882295, -1.25851869647003,
-1.26544165917516, -1.22693569033306, -1.25725659511552,
-1.21010427615498, -1.34232359165001, -1.32660610047776,
-1.29610987865021, -1.26013011718281, -1.21766261392912,
-1.17439826445927, -1.17622836607061, -1.19410480592852,
-1.20980753722721, -1.23210362227815, -1.22356037273062,
-1.28685023169650, -1.31147384975096, -1.31325943087577,
-1.32294429181585, -1.27764956444795, -1.28185065227558,
-1.30324581243296, -1.35720257804154, -1.31485149250653,
-1.31249005177055, -1.33192918666462, -1.32949449292688,
-1.34651906053735, -1.37881888127112, -1.36863148386580,
-1.35635968160029, -1.31362991720627, -1.33018045547167,
-1.31016961130448, -1.28971757113766, -1.27303302564893,
-1.31791694333118, -1.30060097705289, -1.31787074800653,
-1.29868914777550, -1.30857532844143, -1.28573219864486,
-1.32410102371601, -1.33639969073421, -1.34007415835075,
-1.38507735502386, -1.34107837659624, -1.32257301638253,
-1.31159626761432, -1.30609936880576, -1.29975894236571,
-1.27624841277619, -1.25405147690197, -1.22910085941265,
-1.20795472697643, -1.24853851088728, -1.27438582338932,
-1.26400606495224, -1.23302999402678, -1.23178880710196,
-1.24518199542743, -1.23331612167119, -1.22903127206189,
-1.30490842217184, -1.28576361752545, -1.31074792460780,
-1.30894341383401, -1.25965302705916, -1.24232528469753,
-1.19631320277742, -1.18300183600512, -1.17795022721718,
-1.17403776779038, -1.12859939113535, -1.17956095926659,
-1.14352875660976, -1.15024124231263, -1.12826553775509,
-1.13025738690559, -1.13484753813885, -1.13056767812354,
-1.15714576109888, -1.14068343396429, -1.12261477644592,
-1.09392813920472, -1.13016278463302, -1.12804293615061,
-1.15706655799472, -1.19894078640923, -1.13702840262820,
-1.13987336894122, -1.08890542138072, -1.07906342544466,
-1.09459214157365, -1.10581794169538, -1.05652075609808,
-1.09629145098239, -1.06052971723101, -1.03095393415127,
-1.03757475536182, -1.04433286723235, -1.04144782691406,
-1.05020397226132, -1.11335366423297, -1.13308242788617,
-1.11881260434659, -1.10274785767592, -1.10409688950570,
-1.07848493782634, -1.05652152591989, -1.10303694008154,
-1.13047872199259, -1.11449882444427, -1.10209602713833,
-1.08728702613152, -1.08016151023848, -1.14207794356016,
-1.14108708993475, -1.11437528006918, -1.13074737179464,
-1.10080001358879, -1.13380131056501, -1.02974340003599,
-1.03118429349646, -1.01159974250007, -1.01706186526495,
-1.01352921118355, -0.990699389037849, -1.03145411246970,
-1.08279225328073, -1.08031351179812,

-1.12183163205751, -1.13694519045933, -1.10873154083012,
-1.09048086125836, -1.10496312564202, -1.12271481183688,
-1.14404655640216, -1.10727842881992, -1.09827438761092,
-1.08984538074143, -1.02245675642832, -0.982400792644428,

-1.00758031375945, -0.997985602590168, -1.01785304453019,
-1.01973792693547,

-1.05359645335894, -1.04117220263969, -1.00443264885732,
-0.94462766276219, -0.93052780914025, -0.901150831889696,
-0.876218739372558, - 0.889781755283235, -0.922689983868312,
-0.986812563893075, -1.00577437687499, -1.05528836194302,

-1.06361757010428, -1.05509423969802, -1.07764076165386,
-1.05393419503949, -1.03141748838034, -1.07666158378067,
-1.08826736188342, -1.03825851781967, -1.05838978627658,
-1.05513301624640, -1.10147358321021, -1.09601673752277,
-1.10974357792115, -1.10308596910112, -1.13992099599508,
-1.15495903878899, -1.12672669661503, -1.15364353419803,
-1.14456118003669, -1.13725384673776, -1.12789761787739,
-1.08972321956154, -1.06817521360676, -1.00612585643724,
-1.01721188607368, -1.00086920668929, -1.01165701637624,
-0.996640056212344,

-0.98313986002211, -0.984299091792497, -0.96583710325524,
-1.00232690455114,

-0.982922825052307, -1.02459712149803, -1.07881579816258,
-1.03163543147277,

-1.07729789184894, -1.01180574553278, -1.06293062885976,
-1.04345761615412, -1.04507118212109, -1.06662413792108,
-1.10636689376820, -1.12222320840262, -1.16318956376199,
-1.22192850138826, -1.24348755430912, -1.25220663040648,
-1.22215708457057, -1.25236192402018, -1.22111129025221,
-1.24724680332852, -1.22611890659054, -1.23082404646155,
-1.21893054743993, -1.22794164100034, -1.24178410222557,
-1.23491411880606, -1.21518417771487, -1.17966072752994,
-1.15118328785644, -1.11210377352095, -1.09023084325471,
-1.11849942190869, -1.08675131543657, -1.09597339581439,
-1.10809871281075, -1.10221229540946, -1.07320636516498,
-1.12163406259843, -1.12479846098100, -1.11007095163810,
-1.18712453125076, -1.17536343414050, -1.18780217004682,
-1.18605983953425, -1.18095143598242, -1.18140299327442,
-1.20254462890750, -1.18702354604552, -1.18338989324559,
-1.25961943500943, -1.26207336479591, -1.27358337177702,
-1.28871107136099, -1.23882265409051, -1.25505293124848,
-1.18747335150791, -1.22188479870869, -1.22512921075359,
-1.27267669879780, -1.29897687823820, -1.30983284713907,
-1.31092491248901, -1.30162429691371, -1.33581710215692,
-1.37449534029653, -1.3624275416685, -1.38856284059204,
-1.37690066885045, -1.35812270817097, -1.26439412795243,
-1.29012098316903, -1.30021268606416, -1.29777924276391,
-1.3330372548974, -1.29014528078931, -1.27029202224966,
-1.28153308247510, -1.27170307834150, -1.29079391277124,
-1.27810834543040, -1.29612763984711, -1.29674584868698,
-1.26923823763037, -1.27642570501084, -1.23329549212125,
-1.24558112749617, -1.21620176421750, -1.23394969740689,
-1.21143465389589, -1.19397690475935, -1.20538829239449,
-1.22385912544701, -1.22335448659759, -1.18764026930546,
-1.18460739308550, -1.18544657910073, -1.17488653166367,
-1.21217413413652, -1.24804440395677, -1.27705906222957,
-1.24670841596142, -1.22433660324650, -1.14970333463809,
-1.16375279138769, -1.21568964613097, -1.21945917869375,
-1.22821658985229, -1.19934594306181, -1.14643724079028,
-1.12643944455629, -1.12233497987804, -1.07034913586819,
-1.05097392060227, -1.1717978905894, -1.16695008848828,
-1.20329793233844, -1.22449878968466, -1.23620777156718,
-1.29283095807763, -1.30456841492716, -1.29956251426570,
-1.35321339995369, -1.30781507790772, -1.25820955708785,
-1.20144277085079, -1.20233755352439, -1.1957463134694,
-1.24548177598815, -1.23362870938309, -1.23751123003283,
-1.16881036271405, -1.17867785852131, -1.13299980838684,
-1.13575190845236, -1.15265011862258, -1.13951844565801,
-1.20445908347423, -1.19738791438949, -1.17891493280400,
-1.14633612729150, -1.18945230462923, -1.25911000157446,
-1.21092734531082, -1.19892430224433, -1.20094139684876,
-1.20934910682816, -1.19981222813604, -1.20380034427012,
-1.24216556039083, -1.27832948787905, -1.29714241228392,
-1.24544589067096, -1.22418764631655, -1.1588736470015,
-1.13559976408933, -1.14546273748605, -1.07796573614627,
-1.08111480654649, -1.05358267953188, -1.08838275274950,
-1.06160134081697, -1.03782319420854, -1.01718992134545,
-0.98004212412867, -0.995178761325842, -1.03019373420257,
-1.05128326974742, -1.05013618518556,

-1.09826884471200, -1.09809384521198, -1.08519850594646,
-1.01064238573864, -1.03473773757524, -1.04378027348155,
-1.06594802906565, -1.05967833110358, -1.09239793955592,
-1.10372033897668, -1.11398074918965, -1.07142615682281,
-1.04147669240454, -1.02369590981874, -1.00377810316095,
-0.992689744428071,

-1.0585698310738, -1.08344112379608, -1.10562560362484,
-1.09246302856515, -1.08639186584488, -1.10794139071165,
-1.08098657419634, -1.03139971026396, -1.03491525139187,
-1.00675304495887, -1.0124392044228, -0.992602075541911
))

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Wed Jan 10 16:06:05 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 10 Jan 2007 15:06:05 +0000 (GMT)
Subject: [R] prime in expression in plot
In-Reply-To: <d0f55a670701100515w586d9884w21af12d5454e87d5@mail.gmail.com>
References: <d0f55a670701100515w586d9884w21af12d5454e87d5@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701101502210.29972@gannet.stats.ox.ac.uk>

Two possibilities are ' (which is the usual way to write \prime in TeX) 
and minute (which is the nearest character in the Adobe symbol font that 
plotmath uses).  TeX has its own symbol fonts, and most R devices are 
based rather on the Adobe one.

On Wed, 10 Jan 2007, Thomas Steiner wrote:

> I want to write something like (LaTeX style)
> b_{norm}=\frac{F\prime(0)}{R\prime(0)}
>
> how do I add the "prime" (first derivative) to a R-plot? The help of
> "plotmath" just talks about "partialdiff". Can you complete this
> command?
>
> text( 30,0.05,labels=expression(b[plain("norm")]==frac(F(0),R(0))) )
>
> Thanks,
> Thomas

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From hastie at stanford.edu  Wed Jan 10 16:06:55 2007
From: hastie at stanford.edu (Trevor Hastie)
Date: Wed, 10 Jan 2007 07:06:55 -0800
Subject: [R] contingency table analysis; generalized linear model
In-Reply-To: <mailman.13.1168426804.25406.r-help@stat.math.ethz.ch>
References: <mailman.13.1168426804.25406.r-help@stat.math.ethz.ch>
Message-ID: <3617B8DA-9709-40FE-808F-14543E49E465@stanford.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070110/6eff368b/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Jan 10 16:09:12 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 10 Jan 2007 15:09:12 +0000 (GMT)
Subject: [R] Fw: Memory problem on a linux cluster using a large data
 set [Broadcast]
In-Reply-To: <20070110124416.9546.qmail@web51708.mail.yahoo.com>
References: <20070110124416.9546.qmail@web51708.mail.yahoo.com>
Message-ID: <Pine.LNX.4.64.0701101506480.29972@gannet.stats.ox.ac.uk>

On Wed, 10 Jan 2007, Iris Kolder wrote:

> Hi
>
> I listened to all your advise and ran my data on a computer with a 64 
> bits procesor but i still get the same error saying "it cannot allocate 
> a vector of that size 1240 kb" . I don't want to cut my data in smaller 
> pieces because we are looking at interaction. So are there any other 
> options for me to try out or should i wait for the development of more 
> advanced computers!

Did you use a 64-bit build of R on that machine?  If the message is the 
same, I strongly suspect not.  64-bit builds are not the default on most 
OSes.

>
> Thanks,
>
> Iris
>
>
> ----- Forwarded Message ----
> From: Iris Kolder <iriskolder at yahoo.com>
> To: r-help at stat.math.ethz.ch
> Sent: Thursday, December 21, 2006 2:07:08 PM
> Subject: Re: [R] Memory problem on a linux cluster using a large data set [Broadcast]
>
>
> Thank you all for your help!
>
> So with all your suggestions we will try to run it on a computer with a 
> 64 bits proccesor. But i've been told that the new R versions all work 
> on a 32bits processor. I read in other posts that only the old R 
> versions were capable of larger data sets and were running under 64 bit 
> proccesors. I also read that they are adapting the new R version for 64 
> bits proccesors again so does anyone now if there is a version available 
> that we could use?
>
> Iris Kolder
>
> ----- Original Message ----
> From: "Liaw, Andy" <andy_liaw at merck.com>
> To: Martin Morgan <mtmorgan at fhcrc.org>; Iris Kolder <iriskolder at yahoo.com>
> Cc: r-help at stat.math.ethz.ch; N.C. Onland-moret <n.c.onland at umcutrecht.nl>
> Sent: Monday, December 18, 2006 7:48:23 PM
> Subject: RE: [R] Memory problem on a linux cluster using a large data set [Broadcast]
>
>
> In addition to my off-list reply to Iris (pointing her to an old post of
> mine that detailed the memory requirement of RF in R), she might
> consider the following:
>
> - Use larger nodesize
> - Use sampsize to control the size of bootstrap samples
>
> Both of these have the effect of reducing sizes of trees grown.  For a
> data set that large, it may not matter to grow smaller trees.
>
> Still, with data of that size, I'd say 64-bit is the better solution.
>
> Cheers,
> Andy
>
> From: Martin Morgan
>>
>> Iris --
>>
>> I hope the following helps; I think you have too much data
>> for a 32-bit machine.
>>
>> Martin
>>
>> Iris Kolder <iriskolder at yahoo.com> writes:
>>
>>> Hello,
>>>
>>> I have a large data set 320.000 rows and 1000 columns. All the data
>>> has the values 0,1,2.
>>
>> It seems like a single copy of this data set will be at least
>> a couple of gigabytes; I think you'll have access to only 4
>> GB on a 32-bit machine (see section 8 of the R Installation
>> and Administration guide), and R will probably end up, even
>> in the best of situations, making at least a couple of copies
>> of your data. Probably you'll need a 64-bit machine, or
>> figure out algorithms that work on chunks of data.
>>
>>> on a linux cluster with R version R 2.1.0.  which operates on a 32
>>
>> This is quite old, and in general it seems like R has become
>> more sensitive to big-data issues and tracking down
>> unnecessary memory copying.
>>
>>> "cannot allocate vector size 1240 kb". I've searched through
>>
>> use traceback() or options(error=recover) to figure out where
>> this is actually occurring.
>>
>>> SNP <- read.table("file.txt", header=FALSE, sep="")    #
>> read in data file
>>
>> This makes a data.frame, and data frames have several aspects
>> (e.g., automatic creation of row names on sub-setting) that
>> can be problematic in terms of memory use. Probably better to
>> use a matrix, for which:
>>
>>      'read.table' is not the right tool for reading large matrices,
>>      especially those with many columns: it is designed to read _data
>>      frames_ which may have columns of very different classes. Use
>>      'scan' instead.
>>
>> (from the help page for read.table). I'm not sure of the
>> details of the algorithms you'll invoke, but it might be a
>> false economy to try to get scan to read in 'small' versions
>> (e.g., integer, rather than
>> numeric) of the data -- the algorithms might insist on
>> numeric data, and then make a copy during coercion from your
>> small version to numeric.
>>
>>> SNP$total.NAs = rowSums(is.na(SN         # calculate the
>> number of NA per row and adds a colum with total Na's
>>
>> This adds a column to the data.frame or matrix, probably
>> causing at least one copy of the entire data. Create a
>> separate vector instead, even though this unties the
>> coordination between columns that a data frame provides.
>>
>>> SNP  = t(as.matrix(SNP))                          #
>> transpose rows and columns
>>
>> This will also probably trigger a copy;
>>
>>> snp.na<-SNP
>>
>> R might be clever enough to figure out that this simple
>> assignment does not trigger a copy. But it probably means
>> that any subsequent modification of snp.na or SNP *will*
>> trigger a copy, so avoid the assignment if possible.
>>
>>> snp.roughfix<-na.roughfix(snp.na)
>>
>>> fSNP<-factor(snp.roughfix[, 1])                # Asigns
>> factor to case control status
>>>
>>> snp.narf<- randomForest(snp.roughfix[,-1], fSNP,
>>> na.action=na.roughfix, ntree=500, mtry=10, importance=TRUE,
>>> keep.forest=FALSE, do.trace=100)
>>
>> Now you're entirely in the hands of the randomForest. If
>> memory problems occur here, perhaps you'll have gained enough
>> experience to point the package maintainer to the problem and
>> suggest a possible solution.
>>
>>> set it should be able to cope with that amount. Perhaps someone has
>>> tried this before in R or is Fortram a better choice? I added my R
>>
>> If you mean a pure Fortran solution, including coding the
>> random forest algorithm, then of course you have complete
>> control over memory management. You'd still likely be limited
>> to addressing 4 GB of memory.
>>
>>
>>> I wrote a script to remove all the rows with more than 46 missing
>>> values. This works perfect on a smaller dataset. But the problem
>>> arises when I try to run it on the larger data set I get an error
>>> "cannot allocate vector size 1240 kb". I've searched
>> through previous
>>> posts and found out that it might be because i'm running it
>> on a linux
>>> cluster with R version R 2.1.0.  which operates on a 32 bit
>> processor.
>>> But I could not find a solution for this problem. The cluster is a
>>> really fast one and should be able to cope with these large
>> amounts of
>>> data the systems configuration are Speed: 3.4 GHz, memory
>> 4GByte. Is
>>> there a way to change the settings or processor under R? I
>> want to run
>>> the function Random Forest on my large data set it should
>> be able to
>>> cope with that amount. Perhaps someone has tried this
>> before in R or
>>> is Fortram a better choice? I added my R script down below.
>>>
>>> Best regards,
>>>
>>> Iris Kolder
>>>
>>> SNP <- read.table("file.txt", header=FALSE, sep="")    #
>> read in data file
>>> SNP[SNP==9]<-NA                                   # change
>> missing values from a 9 to a NA
>>> SNP$total.NAs = rowSums(is.na(SN         # calculate the
>> number of NA per row and adds a colum with total Na's
>>> SNP = SNP[ SNP$total.NAs < 46,  ]         # create a subset
>> with no more than 5%(46) NA's
>>> SNP$total.NAs=NULL                              # remove
>> added column with sum of NA's
>>> SNP  = t(as.matrix(SNP))                          #
>> transpose rows and columns
>>> set.seed(1)
>>
>>> snp.na<-SNP
>>> snp.roughfix<-na.roughfix(snp.na)
>>
>>> fSNP<-factor(snp.roughfix[, 1])                # Asigns
>> factor to case control status
>>>
>>> snp.narf<- randomForest(snp.roughfix[,-1], fSNP,
>>> na.action=na.roughfix, ntree=500, mtry=10, importance=TRUE,
>>> keep.forest=FALSE, do.trace=100)
>>>
>>> print(snp.narf)
>>>
>>> __________________________________________________
>>>
>>>
>>>
>>>     [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Martin T. Morgan
>> Bioconductor / Computational Biology
>> http://bioconductor.org
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>
>
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments, contains
> information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station,
> New Jersey, USA 08889), and/or its affiliates (which may be known
> outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD
> and in Japan, as Banyu - direct contact information for affiliates is
> available at http://www.merck.com/contact/contacts.html) that may be
> confidential, proprietary copyrighted and/or legally privileged. It is
> intended solely for the use of the individual or entity named on this
> message. If you are not the intended recipient, and have received this
> message in error, please notify us immediately by reply e-mail and then
> delete it from your system.
>
> ------------------------------------------------------------------------------
>
>
>
> __________________________________________________
>
>
>
>
>
>
> ____________________________________________________________________________________
> Want to start your own business?
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From daltonj at ccf.org  Wed Jan 10 16:32:38 2007
From: daltonj at ccf.org (Dalton, Jarrod)
Date: Wed, 10 Jan 2007 10:32:38 -0500
Subject: [R] Implementation of Wei-Lachin Test
Message-ID: <71E07DF10E789741BAD6F8B0923D700B90971F@CCHSCLEXMB68.cc.ad.cchs.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070110/ce6f8a01/attachment.pl 

From ssj1364 at gmail.com  Wed Jan 10 16:38:14 2007
From: ssj1364 at gmail.com (sj)
Date: Wed, 10 Jan 2007 08:38:14 -0700
Subject: [R] ARIMA and xreg
Message-ID: <1c6126db0701100738o9a1b792y1ad6f4f826a6503a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070110/8f0b5273/attachment.pl 

From hellokisas at gmail.com  Wed Jan 10 16:54:44 2007
From: hellokisas at gmail.com (Feng Qiu)
Date: Wed, 10 Jan 2007 10:54:44 -0500
Subject: [R]  logistic regression packages
References: <5b42b3f80701100553q5912a22cq3a94f9b93eb84501@mail.gmail.com>
Message-ID: <002c01c734cf$a68a61f0$6400a8c0@Aglog>

Hi All:
           I'm testing a set of data classification algorithms in this paper 
(www.stat.wisc.edu/~loh/treeprogs/quest1.7/mach1317.pdf )
I couldn't find such algorithms in R packages:
           1. LOG: polytomous logistic regression (there was one in MASS 
library: multinom. But after I update MASS library, multinom was lost.)
           2. POL: POLYCLASS algorithm. There is a S-Plus package(polyclass 
library) for this algorithm, so there should be a corresponding package in 
R, but I haven't found it so far.
           Any advice is appreciated.

Best,

Feng


From DrJones at alum.MIT.edu  Wed Jan 10 06:13:10 2007
From: DrJones at alum.MIT.edu (Thomas L Jones)
Date: Wed, 10 Jan 2007 00:13:10 -0500
Subject: [R] "go" or "goto" command
Message-ID: <000301c73476$0b7f5cf0$2f01a8c0@DrJones>

Some computer languages, including C, have a "go" or "go to" command 
which can be used to shift control to a different part of a function.

Question: Does the R language have a corresponding command? (Yes, I am 
aware that it can be abused; but, in the hands of a good programmer, 
it is simple to use.)

Tom Jones


From zkmetty at gmail.com  Wed Jan 10 15:17:59 2007
From: zkmetty at gmail.com (Zoltan Kmetty)
Date: Wed, 10 Jan 2007 15:17:59 +0100
Subject: [R] correlation value and map
In-Reply-To: <200701101343.l0ADhKmU004299@msslhb.mssl.ucl.ac.uk>
References: <200701101343.l0ADhKmU004299@msslhb.mssl.ucl.ac.uk>
Message-ID: <c1055ec10701100617h7726055eo3b1a84391122dc98@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070110/e87a864f/attachment.pl 

From Tord.Snall at nvb.slu.se  Wed Jan 10 17:49:56 2007
From: Tord.Snall at nvb.slu.se (=?ISO-8859-1?Q?Tord_Sn=E4ll?=)
Date: Wed, 10 Jan 2007 17:49:56 +0100
Subject: [R] map data.frame() data after having linked them to a
 read.shape() object
Message-ID: <45A51934.2020708@nvb.slu.se>

Dear all,
I try to first link data in a data.frame() to a (polygon) read.shape() 
object and then to plot a polygon map showing the data in the 
data.frame. The first linking is called "link" in ArcView and "relate" 
in ArcMap. I use the code shown below, though without success.

Help with this would be greatly appreciated.

Thanks!

Tord

require(maptools)
# Read shape file (one row per county)
a=read.shape("myshp.shp", dbf.data=TRUE, verbose=TRUE)
str(a)
  ..- attr(*, "maxbb")= num [1:4] -100   49    0    0
  ..- attr(*, "class")= chr "ShapeList"
$ att.data:'data.frame':       490 obs. of  60 variables:
  ..$ STATE_FIPS: Factor w/ 12 levels "04","06","08",..: 11 11 11 11 4 5
5 5 5 5 ...
[snip]
  ..$ STACOUNT4 : Factor w/ 489 levels "ArizonaApache",..: 437 460 451
453 147 207 195 198 231 206 ...
  ..- attr(*, "data_types")= chr [1:60] "C" "C" "C" "C" ...
- attr(*, "class")= chr "Map"


# Read case data (one row per case)
cases = read.table("cases.txt", h=T,)
str(cases)
'data.frame':   431 obs. of  8 variables:
$ Year    : int  1950 1950 1950 1951 1956 1957 1959 1959 1959 1959 ...
$ Case    : int  3 1 2 1 1 1 2 4 1 3 ...
$ stacount: Factor w/ 106 levels "ArizonaApache",..: 1 66 76 66 26 29
15 25 30 60 ...

# table the cases data PER Year, PER County (County = "stacount")
temp = t(table(cases[,c("Year","stacount")]))
stacount = dimnames(temp)$stacount
temp = cbind(stacount, as.data.frame(temp[,1:ncol(temp)],row.names=F))
str(temp)
'data.frame':   106 obs. of  50 variables:
$ stacount: Factor w/ 106 levels "ArizonaApache",..: 1 2 3 4 5 6 7 8 9
10 ...
$ 1950    : int  1 0 0 0 0 0 0 0 0 0 ...
[snip]
$ 2005    : int  0 0 0 0 0 0 0 0 0 0 ...

# Pick out a temporary attribute data.frame
datfr = a$att.data

# Merge the temporaty data frame with tabled "cases"
for(i in 2:ncol(temp)){
     datfr = merge(datfr, temp[,c(1,i)], by.x="STACOUNT4",
by.y="stacount", all.x=T, all.y=F)
}

#Replace NAs with 0:
for(i in 61:109){
     datfr[,i] = ifelse(is.na(datfr[,i])==T,0,datfr[,i])
}

str(a$att.data)
'data.frame':   490 obs. of  60 variables:
$ NAME      : Factor w/ 416 levels "Ada","Adams",..: 120 352 265 277 33
210 122 135 372 209 ...
[snip]
$ STACOUNT4 : Factor w/ 489 levels "ArizonaApache",..: 437 460 451 453
147 207 195 198 231 206 ...
- attr(*, "data_types")= chr  "C" "C" "C" "C" ...
# Note that the above data is of "attribute type"

str(datfr)
'data.frame':   490 obs. of  109 variables:
$ STACOUNT4 : Factor w/ 489 levels "ArizonaApache",..: 1 2 3 4 5 6 7 8
9 10 ...
[snip]
$ 1951      : num  0 0 0 0 0 0 0 0 0 0 ...
[snip]
$ 2005      : num  0 0 0 0 0 0 0 0 0 0 ...
# Note that at the end of this, data type is not described - it is a 
"simple" data frame

# bind data together:
#Alternative 1:
a$att.data = cbind(a$att.data, datfr[,61:109])
# Other alternatives:
test = matrix(ncol=49)
a$att.data[,61:109] = test
a$att.data[,61:109] = datfr[,61:109]

# plot:
plot(a, auxvar=a$att.data[,61], xlim=c(-125,-99),ylim=c(28,52), xlab="",
ylab="", frame.plot=F,axes=F)
There were 50 or more warnings (use warnings() to see the first 50)
warnings()
49: "axes" is not a graphical parameter in: polygon(xy$x, xy$y, 
col,border, lty, ...)
50: "frame.plot" is not a graphical parameter in: polygon(xy$x, 
xy$y,col, border, lty, ...)

# The a$att.data type has changed to becoming a typical data.frame - 
"attr" is not mentioned:
str(a$att.data)
[snip]
$ 2003      : num  0 0 0 0 0 0 0 0 0 0 ...
$ 2004      : num  0 0 0 0 0 0 0 0 0 0 ...
$ 2005      : num  0 0 0 0 0 0 0 0 0 0 ...



-- 

Tord Sn?ll
Department of Conservation Biology
Swedish University of Agricultural Sciences (SLU)
P.O. 7002, SE-750 07 Uppsala, Sweden
Office/Mobile/Fax
+46-18-672612/+46-730-891356/+46-18-673537
E-mail: tord.snall at nvb.slu.se
www.nvb.slu.se/staff_tordsnall


From jmb at mssl.ucl.ac.uk  Wed Jan 10 17:24:34 2007
From: jmb at mssl.ucl.ac.uk (Jenny Barnes)
Date: Wed, 10 Jan 2007 16:24:34 +0000 (GMT)
Subject: [R] correlation value and map
Message-ID: <200701101624.l0AGOY6e004504@msslhb.mssl.ucl.ac.uk>

Hi Zoltan,

Right, I have 30x32=960 data points per year (It is actually the mean febuary 
precipitation total in case you were wondering) at each grid point over the 
world, so I have 960 data points each of the 43 years. Therefore can I do 
anything with a trend and residuals? I don't think I can if it's just mean feb 
precipitation, one data point per grid square per year... I apreicate your help 
though very much.....although I do still need to perform a spatial correlation 
if anyone else can help?

Many thanks,

Jenny



Hi Jenny!

So if i understand your datafile corect you have 960 case for a year. Any
you have 43 years.. Yes?

I'm not sure you should use correlation in this situation because of the
autocorrelation of the data. There are big autocorrelation on spatial data's
like what you use, and there are also a very big autocorrelation in time
series data. I think you have to decompose your time series, and you have to
cut down, the trend (and maybe some kind of sesonality), and than for the
residuals you should do a correlation. You have to filter out the
autocorrelation on the spatial data too, some way..

And because of the above problems, don't calculate correlation for the
entierly databases!

bye,
Zoltan

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jennifer Barnes
PhD student - long range drought prediction
Climate Extremes
Department of Space and Climate Physics
University College London
Holmbury St Mary, Dorking
Surrey
RH5 6NT
01483 204149
07916 139187
Web: http://climate.mssl.ucl.ac.uk


From Setzer.Woodrow at epamail.epa.gov  Wed Jan 10 17:57:25 2007
From: Setzer.Woodrow at epamail.epa.gov (Setzer.Woodrow at epamail.epa.gov)
Date: Wed, 10 Jan 2007 11:57:25 -0500
Subject: [R] problems with optim, "for"-loops and machine precision
In-Reply-To: <200701101218.l0ACI48g013139@idmailgate1.unizh.ch>
Message-ID: <OF8AA378D3.35141148-ON8525725F.00579D31-8525725F.005D232A@epamail.epa.gov>

Without more detail - a reproducible example - it is hard to give you
concrete advice.

I wonder if the functions NLL23 and NLL21 depend on numerical solutions
of a system of ODEs, since you invoke the odesolve package?  If so, try
switching to the Nelder-Mead optimizer, enforcing the parameter
constraints using transformation.  Probably you are using the finite
difference derivatives calculated internally to optim for the gradient
information used in the L-BFGS-B optimizer.  These can be unstable when
based on numerical solutions of odes, causing the optimizer to fail, or
sometimes to converge to a non-optimal point.

Some other points:
- you cannot change machine precision by changing values in .Machine.
To change the number of digits printed, use options(digits=8).
- use 'library()' instead of 'require()', unless you need to use the
return value from 'require()'

R. Woodrow Setzer, Ph. D.
National Center for Computational Toxicology
US Environmental Protection Agency
Mail Drop B205-01/US EPA/RTP, NC 27711
Ph: (919) 541-0128    Fax: (919) 541-1194


                                                                        
             Simon Ruegg                                                
             <s.ruegg at access.                                           
             unizh.ch>                                               To 
             Sent by:                 r-help at stat.math.ethz.ch          
             r-help-bounces at s                                        cc 
             tat.math.ethz.ch                                           
                                                                Subject 
                                      [R] problems with optim,          
             01/10/2007 07:18         "for"-loops and machine precision 
             AM                                                         
                                                                        
                                                                        
                                                                        
                                                                        
                                                                        




Dear R experts,



I have been encountering problems with the "optim" routine using "for"
loops. I am determining the optimal parameters of several nested models
by
minimizing the negative Log-Likelihood (NLL) of a dataset.



The aim is to find the model which best describes the data. To this end,
I
am simulating artificial data sets based on the model with the least
number
of parameters (6) and the parameters determined with the field data. For
each artificial set I estimate the parameters of the model with 6
parameters
and the next more complex model with 7 parameters (two of these
parameters
are equal in the 6-parameter model) by minimizing the corresponding NLL
with
"optim". In theory the 7-parameter model should fit the data either
equally
or better than the 6-parameter model. Therefore the difference of the
minimal NLLs should be 0 or larger.

For 500 data sets I use the following code:



require(odesolve)

res=matrix(nrow=500,ncol=18)

colnames(res)=c("a_23","beta_23","mu_23","d_23","I_23","M_23","NLL_23",


"a_21","beta_21","mu_21","e_21","d_21","I_21","M_21","NLL_21",

            "NLL23_min_21","conv23","conv21")

for (s in 1:500)

{


assign("data",read.table(paste("populations/TE23simset_",s,".txt",sep="")),e

nv=MyEnv) #reading a data set



  M23=optim(rep(0.1,6),NLL23,method="L-BFGS-B",lower=0,

      upper=c(Inf,Inf,Inf,Inf,1,1),control=c(maxit=150))

  if (M23$convergence==0)

    {

       M21=optim(rep(0.1,7),NLL21,method="L-BFGS-B",lower=0,

            upper=c(Inf,Inf,Inf,Inf,Inf,1,1),control=c(maxit=150))

    }

      res[s,1]=M23$par[1]

      res[s,2]=M23$par[2]

      res[s,3]=M23$par[3]

      res[s,4]=M23$par[4]

      res[s,5]=M23$par[5]

      res[s,6]=M23$par[6]

      res[s,7]=M23$value

      res[s,8]=M21$par[1]

      res[s,9]=M21$par[2]

      res[s,10]=M21$par[3]

      res[s,11]=M21$par[4]

      res[s,12]=M21$par[5]

      res[s,13]=M21$par[6]

      res[s,14]=M21$par[7]

      res[s,15]=M21$value

      res[s,16]=M23$value-M21$value

      res[s,17]=M23$convergence

      res[s,18]=M21$convergence

      write.table(res,"compare23_21TE061221.txt")

      rm(M23,M21)

   }

}



For some strange reason the results do not correspond to what I expect:
about 10% of the solutions have a difference of NLL smaller than 0. I
have
verified the optimisation of these results manually and found that a
minimal
NLL was ignored and a higher NLL was returned at "convergence". To check
what was happening I inserted a printing line in the NLL function to
print
all parameters and the NLL as the procedure goes on. To my surprise
"optim"
then stopped at the minimal NLL which had been ignored before. I have
then
reduced the machine precision to .Machine$double.digits=8 thinking, that
the
printing was slowing down the procedure and by reducing the machine
precision to speed up the calculations. For an individual calculation
this
solved my problem. However if I implemented the same procedure in the
loop
above, the same impossible results occurred again.



Can anyone tell me where I should be looking for the problem, or what it
is
and how I could solve it?



Thanks a lot for your help





Sincerely



Simon







********************************************************************

Simon Ruegg

Dr.med.vet.,  PhD candidate

Institute for Parasitology

Winterthurstr. 266a

8057 Zurich

Switzerland



phone: +41 44 635 85 93

fax: +41 44 635 89 07

e-mail: s.ruegg at access.unizh.ch




             [[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Wed Jan 10 18:03:22 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 10 Jan 2007 12:03:22 -0500
Subject: [R] a question of substitute
In-Reply-To: <200701091534.28801.dusa.adrian@gmail.com>
References: <200701091438.16341.dusa.adrian@gmail.com>
	<971536df0701090514q7d732d4by4d327da6cfdd5564@mail.gmail.com>
	<200701091534.28801.dusa.adrian@gmail.com>
Message-ID: <971536df0701100903o56da2a24pc7b37142ced3d2c1@mail.gmail.com>

Looks like oneway.test has been changed for R 2.5.0.

Paste the code in this file:

   https://svn.r-project.org/R/trunk/src/library/stats/R/oneway.test.R

into your session.  Then fun.2 from your post will work without
the workarounds I posted:

   fun.2(values ~ group)


On 1/9/07, Adrian Dusa <dusa.adrian at gmail.com> wrote:
> On Tuesday 09 January 2007 15:14, Gabor Grothendieck wrote:
> > oneway.test is using substitute on its arguments so its literally
> > getting formula rather than the value of formula.
>
> Ah-haa... I understand now. Thanks for the tips, they both work as expected.
> Best,
> Adrian
>
> > Try these:
> >
> > fun.3 <- function(formula) {
> >       mc <- match.call()
> >       mc[[1]] <- as.name("oneway.test")
> >       eval.parent(mc)
> > }
> > fun.3(values ~ group)
> >
> > fun.4 <- function(formula) {
> >       do.call(oneway.test, list(formula))
> > }
> > fun.4(values ~ group)
> >
> > On 1/9/07, Adrian Dusa <dusa.adrian at gmail.com> wrote:
> > > Hi all,
> > >
> > > I want to write a wrapper for an analysis of variance and I face a
> > > curious problem. Here are two different wrappers:
> > >
> > > fun.1 <- function(formula) {
> > >    summary(aov(formula))
> > > }
> > >
> > > fun.2 <- function(formula) {
> > >    oneway.test(formula)
> > > }
> > >
> > > values <- c(15, 8, 17, 7, 26, 12, 8, 11, 16, 9, 16,
> > >            24, 20, 19, 9, 17, 11, 8, 15, 6, 14)
> > > group <- rep(1:3, each=7)
> > >
> > > # While the first wrapper works just fine:
> > > fun.1(values ~ group)
> > >
> > > # the second throws an error:
> > > fun.2(values ~ group)
> > > Error in substitute(formula)[[2]] : object is not subsettable
> > >
> > > ###
> > >
> > > I also tried binding the two vectors in a data.frame, with no avail.
> > > I did find a hack, creating two new vectors inside the function and
> > > creating a fresh formula, so I presume this has something to do with
> > > environments.
> > >
> > > Could anybody give me a hint on this?
> > > Thank you,
> > > Adrian
> > >
> > > --
> > > Adrian Dusa
> > > Romanian Social Data Archive
> > > 1, Schitu Magureanu Bd
> > > 050025 Bucharest sector 5
> > > Romania
> > > Tel./Fax: +40 21 3126618 \
> > >          +40 21 3120210 / int.101
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and provide commented,
> > > minimal, self-contained, reproducible code.
>
> --
> Adrian Dusa
> Arhiva Romana de Date Sociale
> Bd. Schitu Magureanu nr.1
> 050025 Bucuresti sectorul 5
> Romania
> Tel./Fax: +40 21 3126618 \
>          +40 21 3120210 / int.101
>


From murdoch at stats.uwo.ca  Wed Jan 10 18:06:23 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 10 Jan 2007 12:06:23 -0500
Subject: [R] "go" or "goto" command
In-Reply-To: <000301c73476$0b7f5cf0$2f01a8c0@DrJones>
References: <000301c73476$0b7f5cf0$2f01a8c0@DrJones>
Message-ID: <45A51D0F.5050104@stats.uwo.ca>

On 1/10/2007 12:13 AM, Thomas L Jones wrote:
> Some computer languages, including C, have a "go" or "go to" command 
> which can be used to shift control to a different part of a function.
> 
> Question: Does the R language have a corresponding command? (Yes, I am 
> aware that it can be abused; but, in the hands of a good programmer, 
> it is simple to use.)

No, it doesn't.

Duncan Murdoch


From mothsailor at googlemail.com  Wed Jan 10 18:14:58 2007
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 10 Jan 2007 17:14:58 +0000
Subject: [R] logistic regression packages
In-Reply-To: <815b70590701100913x6c132c1eh535f14b9e64c9909@mail.gmail.com>
References: <5b42b3f80701100553q5912a22cq3a94f9b93eb84501@mail.gmail.com>
	<002c01c734cf$a68a61f0$6400a8c0@Aglog>
	<815b70590701100913x6c132c1eh535f14b9e64c9909@mail.gmail.com>
Message-ID: <815b70590701100914g609245e2l92f3f17d9427bde1@mail.gmail.com>

1. multinom is is the nnet package

2. There is a polyclass function in package polspline

On 10/01/07, Feng Qiu <hellokisas at gmail.com> wrote:
> Hi All:
>            I'm testing a set of data classification algorithms in this paper
> (www.stat.wisc.edu/~loh/treeprogs/quest1.7/mach1317.pdf )
> I couldn't find such algorithms in R packages:
>            1. LOG: polytomous logistic regression (there was one in MASS
> library: multinom. But after I update MASS library, multinom was lost.)
>            2. POL: POLYCLASS algorithm. There is a S-Plus package(polyclass
> library) for this algorithm, so there should be a corresponding package in
> R, but I haven't found it so far.
>            Any advice is appreciated.
>
> Best,
>
> Feng
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


--
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From Colleen.Ross at kp.org  Wed Jan 10 18:30:28 2007
From: Colleen.Ross at kp.org (Colleen.Ross at kp.org)
Date: Wed, 10 Jan 2007 10:30:28 -0700
Subject: [R] SAS and R code hazard ratios
Message-ID: <OF8DC40419.A8696603-ON8725725F.005F6CEE-8725725F.00602C55@KP.ORG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070110/c3012362/attachment.pl 

From dusa.adrian at gmail.com  Wed Jan 10 18:30:12 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Wed, 10 Jan 2007 19:30:12 +0200
Subject: [R] a question of substitute
In-Reply-To: <971536df0701100903o56da2a24pc7b37142ced3d2c1@mail.gmail.com>
References: <200701091438.16341.dusa.adrian@gmail.com>
	<200701091534.28801.dusa.adrian@gmail.com>
	<971536df0701100903o56da2a24pc7b37142ced3d2c1@mail.gmail.com>
Message-ID: <200701101930.12578.dusa.adrian@gmail.com>

On Wednesday 10 January 2007 19:03, Gabor Grothendieck wrote:
> Looks like oneway.test has been changed for R 2.5.0.
> Paste the code in this file:
>    https://svn.r-project.org/R/trunk/src/library/stats/R/oneway.test.R
> into your session.  Then fun.2 from your post will work without
> the workarounds I posted:
>    fun.2(values ~ group)

Brilliant :)
Super fast change, this is why I love R.
Cheers,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From news at aspden.com  Wed Jan 10 19:10:18 2007
From: news at aspden.com (John Lawrence Aspden)
Date: Wed, 10 Jan 2007 18:10:18 +0000
Subject: [R] scripts with littler
References: <ent9nm$rc6$1@sea.gmane.org>
	<20070108132920.GA4640@phenix.progiciels-bpi.ca>
	<entlul$ng$1@sea.gmane.org>
	<971536df0701080703q48e04a5cv96720798b8b30c11@mail.gmail.com>
	<Pine.LNX.4.64.0701082235040.9254@gannet.stats.ox.ac.uk>
	<eo0bvb$sr0$1@sea.gmane.org> <eo0ngg$7p1$1@sea.gmane.org>
	<45A3F289.40705@vanderbilt.edu> <eo12g2$lme$1@sea.gmane.org>
	<eo2l88$v2m$1@sea.gmane.org>
	<Pine.LNX.4.64.0701101354240.12401@auk.stats>
Message-ID: <eo3a6a$l4n$1@sea.gmane.org>

Brian Ripley wrote:

> Exactly as documented.  The argument is named 'new' and not 'add', BTW.
> Please 'be careful' in what you say about the work of others.

Agreed, no criticism intended. I really like R. Sorry.

Cheers, John.

-- 
Contractor in Cambridge UK -- http://www.aspden.com


From kati.schweitzer at ims.uni-stuttgart.de  Wed Jan 10 19:23:43 2007
From: kati.schweitzer at ims.uni-stuttgart.de (Kati Schweitzer)
Date: Wed, 10 Jan 2007 19:23:43 +0100
Subject: [R] 2 problems with latex.table (quantreg package) - reproducible
In-Reply-To: <a907aeaa0701100933y52441c75w62373a5b224d1def@mail.gmail.com>
References: <a907aeaa0701100933y52441c75w62373a5b224d1def@mail.gmail.com>
Message-ID: <a907aeaa0701101023k693c649fgcf67f63b2e5df910@mail.gmail.com>

Dear all,

When using latex.table from the quantreg package, I don't seem to be able to set
table.env=FALSE: when I don't specify caption (as I think I should, when
understanding the R help rightly(?)), I get an error message, and when I
do so, of course I get one, as well.
The funny thing is, that a table is indeed produced in the first case,
so I get a nice tabular, but as I'm using the command within a for -
loop, the loop stops due to the error and only one latex table is
produced.

Example R-Code:

library(quantreg)

v1 <- c("val1","val1","val2")
v2 <- c("val1","val2","val2")
tab <- table(v1,v2)

latex.table(tab,table.env=FALSE)
#error - german R error message (saying that caption  is missing and
has no default :-) ):
#Fehler in cat(caption, "\n", file = fi, append = TRUE) :
#       Argument "caption" fehlt (ohne Standardwert)

latex.table(tab,table.env=FALSE,caption="nothing")
#error - german R error message:
#Fehler in latex.table(tab, table.env = FALSE, caption = "nothing") :
#       you must have table.env=TRUE if caption is given


The second problem is, that - when using latex.table to produce a
tabular within a table environment - I would like to specify cgroup
with only one value - one multicolumn being a heading for both columns
in the table.
But I'm not able to produce latex-compilable code:

latex.table(tab,cgroup="v2",caption="my table")

gives me the following latex code:
\begin{table}[hptb]
\begin{center}
\begin{tabular}{|l||c|c|} \hline
\multicolumn{1}{|l||}{\bf
tab}&\multicolumn{}{c||}{}&\multicolumn{2}{c|}{\bf v2}\\ \cline{2-3}
\multicolumn{1}{|l||}{}&\multicolumn{1}{c|}{val1}&\multicolumn{1}{c|}{val2}\\
\hline
val1&1&1\\
val2&0&1\\
\hline
\end{tabular}
\vspace{3mm}
\caption{my table\label{tab}}
\end{center}
\end{table}

and within this code the problem is the second multicolumn
(&\multicolumn{}{c||}{}), as it has no number specifying how many
columns the multicolumn should cover. Latex (at least my version)
complains.
When deleting this part of the code, the table is compiled and looks
exactly how I want it to look. I'm doing this with a system call and
an shell script right now, but this seems pretty ugly to me...

When I specify 2 columns, this problem doesn't occur:
latex.table(tab,cgroup=c("blah","v2"),caption="my table")

I'm running R Version 2.3.0 (2006-04-24) on a linux machine Fedora
Core 5 (i386).

Can anyone help me find my mistakes?

Thanks a lot
... and sorry for my bad English and potential newbie mistakes!!
Kati


From Stephen.Bond at gmacrfc.com  Wed Jan 10 19:39:10 2007
From: Stephen.Bond at gmacrfc.com (Bond, Stephen)
Date: Wed, 10 Jan 2007 12:39:10 -0600
Subject: [R] using DBI
Message-ID: <18E4DA2E91FDBC4780AD37B8DE6717AA02311393@VMSPAP1102CAA.na.corp.gmacrfc.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070110/aed5e70e/attachment.pl 

From rkoenker at uiuc.edu  Wed Jan 10 19:47:26 2007
From: rkoenker at uiuc.edu (roger koenker)
Date: Wed, 10 Jan 2007 12:47:26 -0600
Subject: [R] 2 problems with latex.table (quantreg package) -
	reproducible
In-Reply-To: <a907aeaa0701101023k693c649fgcf67f63b2e5df910@mail.gmail.com>
References: <a907aeaa0701100933y52441c75w62373a5b224d1def@mail.gmail.com>
	<a907aeaa0701101023k693c649fgcf67f63b2e5df910@mail.gmail.com>
Message-ID: <66601029-1CF9-42DE-A9F1-6A3C088E1112@uiuc.edu>

The usual R-help etiquette recommends:

1.  questions about packages go to the maintainer, not to R-help.

2.  examples should be reproducible:  ie self contained.

if you look carefully at the function latex.summary.rqs  you will see
that there is a failure to pass the argument "..." on to  
latex.table.  This
_may_ be the source of your problem if in fact your v1 and v2 were
summary.rqs objects, but I doubt that they are.

You might try caption = "".  More generally there are much improved
latex tools elsewhere in R; if you aren't making tables that are  
specific
to quantreg, you might want to use them.


url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


On Jan 10, 2007, at 12:23 PM, Kati Schweitzer wrote:

> Dear all,
>
> When using latex.table from the quantreg package, I don't seem to  
> be able to set
> table.env=FALSE: when I don't specify caption (as I think I should,  
> when
> understanding the R help rightly(?)), I get an error message, and  
> when I
> do so, of course I get one, as well.
> The funny thing is, that a table is indeed produced in the first case,
> so I get a nice tabular, but as I'm using the command within a for -
> loop, the loop stops due to the error and only one latex table is
> produced.
>
> Example R-Code:
>
> library(quantreg)
>
> v1 <- c("val1","val1","val2")
> v2 <- c("val1","val2","val2")
> tab <- table(v1,v2)
>
> latex.table(tab,table.env=FALSE)
> #error - german R error message (saying that caption  is missing and
> has no default :-) ):
> #Fehler in cat(caption, "\n", file = fi, append = TRUE) :
> #       Argument "caption" fehlt (ohne Standardwert)
>
> latex.table(tab,table.env=FALSE,caption="nothing")
> #error - german R error message:
> #Fehler in latex.table(tab, table.env = FALSE, caption = "nothing") :
> #       you must have table.env=TRUE if caption is given
>
>
> The second problem is, that - when using latex.table to produce a
> tabular within a table environment - I would like to specify cgroup
> with only one value - one multicolumn being a heading for both columns
> in the table.
> But I'm not able to produce latex-compilable code:
>
> latex.table(tab,cgroup="v2",caption="my table")
>
> gives me the following latex code:
> \begin{table}[hptb]
> \begin{center}
> \begin{tabular}{|l||c|c|} \hline
> \multicolumn{1}{|l||}{\bf
> tab}&\multicolumn{}{c||}{}&\multicolumn{2}{c|}{\bf v2}\\ \cline{2-3}
> \multicolumn{1}{|l||}{}&\multicolumn{1}{c|}{val1}&\multicolumn{1} 
> {c|}{val2}\\
> \hline
> val1&1&1\\
> val2&0&1\\
> \hline
> \end{tabular}
> \vspace{3mm}
> \caption{my table\label{tab}}
> \end{center}
> \end{table}
>
> and within this code the problem is the second multicolumn
> (&\multicolumn{}{c||}{}), as it has no number specifying how many
> columns the multicolumn should cover. Latex (at least my version)
> complains.
> When deleting this part of the code, the table is compiled and looks
> exactly how I want it to look. I'm doing this with a system call and
> an shell script right now, but this seems pretty ugly to me...
>
> When I specify 2 columns, this problem doesn't occur:
> latex.table(tab,cgroup=c("blah","v2"),caption="my table")
>
> I'm running R Version 2.3.0 (2006-04-24) on a linux machine Fedora
> Core 5 (i386).
>
> Can anyone help me find my mistakes?
>
> Thanks a lot
> ... and sorry for my bad English and potential newbie mistakes!!
> Kati
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pmathews at apk.net  Wed Jan 10 19:57:20 2007
From: pmathews at apk.net (Paul Mathews)
Date: Wed, 10 Jan 2007 13:57:20 -0500
Subject: [R] Meeting announcement: An Introduction to Data Analysis Using R
Message-ID: <45A53710.3080008@apk.net>

*An Introduction to Data Analysis Using R*
by Paul Mathews
for The Quality Managers Network, Lakeland Community College, Kirtland, Ohio
26 January 2007, 7:30-9:00 AM, Room T136.

R is free software used for graphical and statistical data analysis that 
can be downloaded from the Internet. Because R code is written and 
shared by many people around the world, it has a tremendous range of 
capabilities. Consequently, most graphical and statistical methods that 
have been implemented in any commercially-available statistical software 
package have been implemented in R.

Paul Mathews will present an introduction to data analysis using R 
including:
  * How to obtain and install R and R packages.
  * How to access limited R functions from the R Commander GUI package.
  * How to enter your data into R.
  * How to graph your data.
  * How to perform basic statistical analyses.
  * How to analyze designed experiments.
  * How to save your work.
  * How to learn more about R.

-- 
Paul Mathews
Mathews Malnar and Bailey, Inc.
217 Third Street, Fairport Harbor, OH 44077
Phone: 440-350-0911
Fax: 440-350-7210
E-mail:
        Paul: pmathews at apk.net, paul at mmbstatistical.com
        Rebecca: rmalnar at apk.net, bek at mmbstatistical.com
Web: www.mmbstatistical.com


From chaogai at duineveld.demon.nl  Wed Jan 10 20:34:00 2007
From: chaogai at duineveld.demon.nl (chao gai)
Date: Wed, 10 Jan 2007 20:34:00 +0100
Subject: [R] using DBI
In-Reply-To: <18E4DA2E91FDBC4780AD37B8DE6717AA02311393@VMSPAP1102CAA.na.corp.gmacrfc.com>
References: <18E4DA2E91FDBC4780AD37B8DE6717AA02311393@VMSPAP1102CAA.na.corp.gmacrfc.com>
Message-ID: <200701102034.00354.chaogai@duineveld.demon.nl>

The way MySQL works, I use RMySQL to contact, which in turn uses DBI.
There is a library ROracle which, from the manual, works the same. Hence I 
would start by looking at Roracle for the connection and proceed from there.

Kees

On Wednesday 10 January 2007 19:39, Bond, Stephen wrote:
> can anybody provide a reference to a document on using DBI and what is
> needed to make it work?
>
> I try: drv=dbDriver("Oracle")
>
> and it complains about not finding a function.
> The DBI.pdf does not require anything else installed, but looking at the
> archive I discovered there is more stuff to be installed.
> This is on Solaris 5.10 with a full Oracle install, so all Oracle libs
> are available and sqlplus works.
> I am trying to connect remotely to another Oracle instance.
>
> Thank you very much
> Stephen
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.


From dpleydel at univ-fcomte.fr  Wed Jan 10 20:12:39 2007
From: dpleydel at univ-fcomte.fr (David)
Date: Wed, 10 Jan 2007 19:12:39 +0000
Subject: [R] Installation problem with package mixtools
Message-ID: <20070110191239.GA12945@univ-fcomte.fr>

I am trying to install mixtools on Debian Etch and get the following error

dell-2 /usr/lib # R CMD INSTALL mixtools_0.1.0.tar.gz
* Installing *source* package 'mixtools' ...
** libs
gcc -I/usr/share/R/include -I/usr/share/R/include     -fpic  -g -O2 -std=gnu99 -c new_svalues.c -o new_svalues.o
gfortran   -fpic  -g -O2 -c sphericaldepth.f -o sphericaldepth.o
make: gfortran: Command not found
make: *** [sphericaldepth.o] Error 127
ERROR: compilation failed for package 'mixtools'
** Removing '/usr/local/lib/R/site-library/mixtools'

I have upgraded my installation of libgfortran and have added /usr/lib
to PATH but the error persists.

The same file has installed perfectly on a similar machine. I am not
sure how to proceed from here.

thanks
Dave


From Roger.Bivand at nhh.no  Wed Jan 10 20:38:05 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 10 Jan 2007 20:38:05 +0100 (CET)
Subject: [R] map data.frame() data after having linked them to a
 read.shape() object
In-Reply-To: <45A51934.2020708@nvb.slu.se>
Message-ID: <Pine.LNX.4.44.0701102020130.14970-100000@reclus.nhh.no>

On Wed, 10 Jan 2007, Tord Sn?ll wrote:

> Dear all,
> I try to first link data in a data.frame() to a (polygon) read.shape() 
> object and then to plot a polygon map showing the data in the 
> data.frame. The first linking is called "link" in ArcView and "relate" 
> in ArcMap. I use the code shown below, though without success.
> 
> Help with this would be greatly appreciated.

Please consider continuing this thread on the R-sig-geo list. Searching 
the archives might also have given you some leads. For now, and apart from 
not using sp classes (see note in R News in 2005), you have 490 polygons 
in the shapefile - probably one duplicate and 489 unique entities in 
STACOUNT4, but only 106 unique stacount of 431 observations in the data 
frame. The plot method for Map objects is deprecated. The classes in the 
sp package use S4, not S3, specifically to help users avoid putting things 
inside objects that break methods.

In maptools, see ?readShapePoly, and ?spCbind to see how to read a
shapefile into an sp object (check the 490/489 issue), and how the
Polygons IDs can be used as a key with the data frame row names to make
this easier to do. Please also consider using FIPS numbers as IDs for
counties; the five digit ssccc ID is fairly standard, and avoids the
problem of repetitive county names across US states.

> 
> Thanks!
> 
> Tord
> 
> require(maptools)
> # Read shape file (one row per county)
> a=read.shape("myshp.shp", dbf.data=TRUE, verbose=TRUE)
> str(a)
>   ..- attr(*, "maxbb")= num [1:4] -100   49    0    0
>   ..- attr(*, "class")= chr "ShapeList"
> $ att.data:'data.frame':       490 obs. of  60 variables:
>   ..$ STATE_FIPS: Factor w/ 12 levels "04","06","08",..: 11 11 11 11 4 5
> 5 5 5 5 ...
> [snip]
>   ..$ STACOUNT4 : Factor w/ 489 levels "ArizonaApache",..: 437 460 451
> 453 147 207 195 198 231 206 ...
>   ..- attr(*, "data_types")= chr [1:60] "C" "C" "C" "C" ...
> - attr(*, "class")= chr "Map"
> 
> 
> # Read case data (one row per case)
> cases = read.table("cases.txt", h=T,)
> str(cases)
> 'data.frame':   431 obs. of  8 variables:
> $ Year    : int  1950 1950 1950 1951 1956 1957 1959 1959 1959 1959 ...
> $ Case    : int  3 1 2 1 1 1 2 4 1 3 ...
> $ stacount: Factor w/ 106 levels "ArizonaApache",..: 1 66 76 66 26 29
> 15 25 30 60 ...
> 
> # table the cases data PER Year, PER County (County = "stacount")
> temp = t(table(cases[,c("Year","stacount")]))
> stacount = dimnames(temp)$stacount
> temp = cbind(stacount, as.data.frame(temp[,1:ncol(temp)],row.names=F))
> str(temp)
> 'data.frame':   106 obs. of  50 variables:
> $ stacount: Factor w/ 106 levels "ArizonaApache",..: 1 2 3 4 5 6 7 8 9
> 10 ...
> $ 1950    : int  1 0 0 0 0 0 0 0 0 0 ...
> [snip]
> $ 2005    : int  0 0 0 0 0 0 0 0 0 0 ...
> 
> # Pick out a temporary attribute data.frame
> datfr = a$att.data
> 
> # Merge the temporaty data frame with tabled "cases"
> for(i in 2:ncol(temp)){
>      datfr = merge(datfr, temp[,c(1,i)], by.x="STACOUNT4",
> by.y="stacount", all.x=T, all.y=F)
> }
> 
> #Replace NAs with 0:
> for(i in 61:109){
>      datfr[,i] = ifelse(is.na(datfr[,i])==T,0,datfr[,i])
> }
> 
> str(a$att.data)
> 'data.frame':   490 obs. of  60 variables:
> $ NAME      : Factor w/ 416 levels "Ada","Adams",..: 120 352 265 277 33
> 210 122 135 372 209 ...
> [snip]
> $ STACOUNT4 : Factor w/ 489 levels "ArizonaApache",..: 437 460 451 453
> 147 207 195 198 231 206 ...
> - attr(*, "data_types")= chr  "C" "C" "C" "C" ...
> # Note that the above data is of "attribute type"
> 
> str(datfr)
> 'data.frame':   490 obs. of  109 variables:
> $ STACOUNT4 : Factor w/ 489 levels "ArizonaApache",..: 1 2 3 4 5 6 7 8
> 9 10 ...
> [snip]
> $ 1951      : num  0 0 0 0 0 0 0 0 0 0 ...
> [snip]
> $ 2005      : num  0 0 0 0 0 0 0 0 0 0 ...
> # Note that at the end of this, data type is not described - it is a 
> "simple" data frame
> 
> # bind data together:
> #Alternative 1:
> a$att.data = cbind(a$att.data, datfr[,61:109])
> # Other alternatives:
> test = matrix(ncol=49)
> a$att.data[,61:109] = test
> a$att.data[,61:109] = datfr[,61:109]
> 
> # plot:
> plot(a, auxvar=a$att.data[,61], xlim=c(-125,-99),ylim=c(28,52), xlab="",
> ylab="", frame.plot=F,axes=F)
> There were 50 or more warnings (use warnings() to see the first 50)
> warnings()
> 49: "axes" is not a graphical parameter in: polygon(xy$x, xy$y, 
> col,border, lty, ...)
> 50: "frame.plot" is not a graphical parameter in: polygon(xy$x, 
> xy$y,col, border, lty, ...)
> 
> # The a$att.data type has changed to becoming a typical data.frame - 
> "attr" is not mentioned:
> str(a$att.data)
> [snip]
> $ 2003      : num  0 0 0 0 0 0 0 0 0 0 ...
> $ 2004      : num  0 0 0 0 0 0 0 0 0 0 ...
> $ 2005      : num  0 0 0 0 0 0 0 0 0 0 ...
> 
> 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From hellokisas at gmail.com  Wed Jan 10 21:04:55 2007
From: hellokisas at gmail.com (Feng Qiu)
Date: Wed, 10 Jan 2007 15:04:55 -0500
Subject: [R] logistic regression packages
References: <5b42b3f80701100553q5912a22cq3a94f9b93eb84501@mail.gmail.com><002c01c734cf$a68a61f0$6400a8c0@Aglog><815b70590701100913x6c132c1eh535f14b9e64c9909@mail.gmail.com>
	<815b70590701100914g609245e2l92f3f17d9427bde1@mail.gmail.com>
Message-ID: <002401c734f2$9964ab70$9e703d80@Aglog>

Hi David:
             Thanks for you information. 2 further questions:
              1. I found out that multinom is not doing politomous logistic 
regression, do you know which function does this?
              2. the polyclass in polspline does "polychotomous" regression, 
while I'm looking for "polytomous" regression. Do you think these two are 
similar int erms of prediction?

Best,

Feng


----- Original Message ----- 
From: "David Barron" <mothsailor at googlemail.com>
To: "r-help" <r-help at stat.math.ethz.ch>
Sent: Wednesday, January 10, 2007 12:14 PM
Subject: Re: [R] logistic regression packages


> 1. multinom is is the nnet package
>
> 2. There is a polyclass function in package polspline
>
> On 10/01/07, Feng Qiu <hellokisas at gmail.com> wrote:
>> Hi All:
>>            I'm testing a set of data classification algorithms in this 
>> paper
>> (www.stat.wisc.edu/~loh/treeprogs/quest1.7/mach1317.pdf )
>> I couldn't find such algorithms in R packages:
>>            1. LOG: polytomous logistic regression (there was one in MASS
>> library: multinom. But after I update MASS library, multinom was lost.)
>>            2. POL: POLYCLASS algorithm. There is a S-Plus 
>> package(polyclass
>> library) for this algorithm, so there should be a corresponding package 
>> in
>> R, but I haven't found it so far.
>>            Any advice is appreciated.
>>
>> Best,
>>
>> Feng
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
> --
> =================================
> David Barron
> Said Business School
> University of Oxford
> Park End Street
> Oxford OX1 1HP
>
>
> -- 
> =================================
> David Barron
> Said Business School
> University of Oxford
> Park End Street
> Oxford OX1 1HP
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From p.dalgaard at biostat.ku.dk  Wed Jan 10 21:28:23 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 10 Jan 2007 21:28:23 +0100
Subject: [R] "go" or "goto" command
In-Reply-To: <000301c73476$0b7f5cf0$2f01a8c0@DrJones>
References: <000301c73476$0b7f5cf0$2f01a8c0@DrJones>
Message-ID: <45A54C67.6030307@biostat.ku.dk>

Thomas L Jones wrote:
> Some computer languages, including C, have a "go" or "go to" command 
> which can be used to shift control to a different part of a function.
>
> Question: Does the R language have a corresponding command? (Yes, I am 
> aware that it can be abused; but, in the hands of a good programmer, 
> it is simple to use.)
>   
Nope. Only break, next, and return.


From p.dalgaard at biostat.ku.dk  Wed Jan 10 21:43:04 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 10 Jan 2007 21:43:04 +0100
Subject: [R] SAS and R code hazard ratios
In-Reply-To: <OF8DC40419.A8696603-ON8725725F.005F6CEE-8725725F.00602C55@KP.ORG>
References: <OF8DC40419.A8696603-ON8725725F.005F6CEE-8725725F.00602C55@KP.ORG>
Message-ID: <45A54FD8.4010806@biostat.ku.dk>

Colleen.Ross at kp.org wrote:
> Greetings,
>
> I am new to R and have been comparing CPH survival analysis hazard ratios 
> between R and SAS PhReg.  The binary covariates' HRs are the same, however 
> the continuous variables, for example age, have quite different HRs 
> although in the same direction.  SAS PhReg produces HRs which are the 
> change in risk for every one increment change in the independent variable. 
>  How do I interpret the HRs produced by R?    Thanks much, C
>
>   
I'm not aware of peculiarities. You're not giving us much to go on 
though. In fact, not even the function used to fit the model with.
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   
Exactly...


From tlumley at u.washington.edu  Wed Jan 10 21:50:30 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 10 Jan 2007 12:50:30 -0800 (PST)
Subject: [R] SAS and R code hazard ratios
In-Reply-To: <OF8DC40419.A8696603-ON8725725F.005F6CEE-8725725F.00602C55@KP.ORG>
References: <OF8DC40419.A8696603-ON8725725F.005F6CEE-8725725F.00602C55@KP.ORG>
Message-ID: <Pine.LNX.4.64.0701101218110.17533@homer21.u.washington.edu>

On Wed, 10 Jan 2007, Colleen.Ross at kp.org wrote:
> I am new to R and have been comparing CPH survival analysis hazard ratios
> between R and SAS PhReg.  The binary covariates' HRs are the same, however
> the continuous variables, for example age, have quite different HRs
> although in the same direction.  SAS PhReg produces HRs which are the
> change in risk for every one increment change in the independent variable.
> How do I interpret the HRs produced by R?    Thanks much, C
>

What function did you use to fit the model in R?  If you used coxph(), in 
the survival package then you should get the same answers as SAS. If you 
used cph() in the design package then the output says what the increment 
is that correponds to the quoted hazard ratio, and the default is the 
interquartile range.

 	-thomas


From Mike.Prager at noaa.gov  Wed Jan 10 23:08:04 2007
From: Mike.Prager at noaa.gov (Michael Prager)
Date: Wed, 10 Jan 2007 17:08:04 -0500
Subject: [R] TCL/TK and R documentation?
Message-ID: <45A563C4.8080904@noaa.gov>

I am hoping something has changed since I last asked about this.

Is there any good source of documentation, including examples, of using 
tcl/tk as a gui for R applications? 


From darrenleeweber at gmail.com  Wed Jan 10 23:19:02 2007
From: darrenleeweber at gmail.com (Darren Weber)
Date: Wed, 10 Jan 2007 14:19:02 -0800
Subject: [R] axis labels at subset of tick marks
Message-ID: <d2095b8c0701101419i796bb095q90c17e94afa832f1@mail.gmail.com>

For example, this works:

x = seq(-100, 1000, 25)
y = x * x
plot(x,y, xaxt="n")
axis(1,x,FALSE,tcl=-0.3)
axis(1,x[x %% 100 ==0])

It creates two axis objects and the values of the x-axis are the
labels.  The following scenario is more difficult, because it uses
'image' to plot a grid of values:

a = matrix(rnorm(100),ncol=10)
image(1:ncol(a), 1:nrow(a), a, xaxt="n", yaxt="n")
# adding arbitrary categorical labels to y-axis
ylabels=c('A','B','C','D','E','F','G','H','I','J')
axis(2, at=1:10, labels=ylabels, las=HORIZONTAL<-1)
# adding arbitrary categorical labels to x-axis,
# for every nth category; first add tick marks,
# then the labels
axis(1, at=1:10, FALSE)
xlabels=c('A','','C','','E','','G','','I','')  # this is length=10
axis(1, at=1:10, labels=xlabels)

This works, but when using axis, the 'at=1:10' and the length(xlabels)
must be the same length.  Is it ever possible to specify axis when
these values are not the same length?  That is, the following does not
work:

x = seq(-100, 1000, 25)
y = x * x
plot(x,y, xaxt="n")
axis(1,x,labels=x[x %% 100 ==0])

Error in axis(side, at, labels, tick, line, pos, outer, font, lty, lwd,  :
        'at' and 'label' lengths differ, 45 != 12

Would it be possible to have axis automatically find the right
location within the x-axis for these labels?  Perhaps something like:

xt = (x %% 100 == 0)
xlabels = x
xlabels[!xt] = ''

This would leave the intermittent label values.  I guess a for loop is
needed to generate xt for irregular intervals in the labels (eg, c(10,
50, 75, 150, 400)).  If the axis command could do this, it would not
be necessary to call it twice to get a subset of labels for all the
tick marks.  That is, it would create tick marks for the 'at'
specification and labels at the 'labels' specification (with no
restraint on these being equal lengths).

Regards, Darren


From kbeath at efs.mq.edu.au  Wed Jan 10 23:34:10 2007
From: kbeath at efs.mq.edu.au (Ken Beath)
Date: Thu, 11 Jan 2007 09:34:10 +1100
Subject: [R] problems with optim, "for"-loops and machine precision
Message-ID: <s5a604ae.073@mail.efs.mq.edu.au>

Two possibilities for why your 7 parameter model fits worse than the 6
are that you are finding a local maximum, which might suggest using a
different parameterisation or the functions are accessing some global
data and so aren't behaving as expected. This could be why they work
properly when run on their own.

I would also check what happens if convergence fails for the 7 parameter
model, in your code this isn't handled well. Also if the constraint on
parameters of >=0 is actually >0, it may be better to work with
parameters on the log scale, avoiding the constraints.

I have found with simulations it is useful to save the fitted objects,
so they can be inspected later, or for the parameters to be extracted
after the models are fitted. This method allows restarting in case of
computer crashes.

Ken

>>> "Simon Ruegg" <s.ruegg at access.unizh.ch> 01/10/07 11:18 PM >>>
Dear R experts,

 

I have been encountering problems with the "optim" routine using "for"
loops. I am determining the optimal parameters of several nested models
by
minimizing the negative Log-Likelihood (NLL) of a dataset. 

 

The aim is to find the model which best describes the data. To this end,
I
am simulating artificial data sets based on the model with the least
number
of parameters (6) and the parameters determined with the field data. For
each artificial set I estimate the parameters of the model with 6
parameters
and the next more complex model with 7 parameters (two of these
parameters
are equal in the 6-parameter model) by minimizing the corresponding NLL
with
"optim". In theory the 7-parameter model should fit the data either
equally
or better than the 6-parameter model. Therefore the difference of the
minimal NLLs should be 0 or larger.

For 500 data sets I use the following code:

 

require(odesolve)

res=matrix(nrow=500,ncol=18)

colnames(res)=c("a_23","beta_23","mu_23","d_23","I_23","M_23","NLL_23",

           
"a_21","beta_21","mu_21","e_21","d_21","I_21","M_21","NLL_21",

            "NLL23_min_21","conv23","conv21")

for (s in 1:500)

{

 
assign("data",read.table(paste("populations/TE23simset_",s,".txt",sep="")),e
nv=MyEnv) #reading a data set

 

  M23=optim(rep(0.1,6),NLL23,method="L-BFGS-B",lower=0,

      upper=c(Inf,Inf,Inf,Inf,1,1),control=c(maxit=150))

  if (M23$convergence==0)

    {

       M21=optim(rep(0.1,7),NLL21,method="L-BFGS-B",lower=0,

            upper=c(Inf,Inf,Inf,Inf,Inf,1,1),control=c(maxit=150))

    }

      res[s,1]=M23$par[1]

      res[s,2]=M23$par[2]

      res[s,3]=M23$par[3]

      res[s,4]=M23$par[4]

      res[s,5]=M23$par[5]

      res[s,6]=M23$par[6]

      res[s,7]=M23$value

      res[s,8]=M21$par[1]

      res[s,9]=M21$par[2]

      res[s,10]=M21$par[3]

      res[s,11]=M21$par[4]

      res[s,12]=M21$par[5]

      res[s,13]=M21$par[6]

      res[s,14]=M21$par[7]

      res[s,15]=M21$value

      res[s,16]=M23$value-M21$value

      res[s,17]=M23$convergence

      res[s,18]=M21$convergence

      write.table(res,"compare23_21TE061221.txt")

      rm(M23,M21)

   }

}

 

For some strange reason the results do not correspond to what I expect:
about 10% of the solutions have a difference of NLL smaller than 0. I
have
verified the optimisation of these results manually and found that a
minimal
NLL was ignored and a higher NLL was returned at "convergence". To check
what was happening I inserted a printing line in the NLL function to
print
all parameters and the NLL as the procedure goes on. To my surprise
"optim"
then stopped at the minimal NLL which had been ignored before. I have
then
reduced the machine precision to .Machine$double.digits=8 thinking, that
the
printing was slowing down the procedure and by reducing the machine
precision to speed up the calculations. For an individual calculation
this
solved my problem. However if I implemented the same procedure in the
loop
above, the same impossible results occurred again.

 

Can anyone tell me where I should be looking for the problem, or what it
is
and how I could solve it?

 

Thanks a lot for your help

 

 

Sincerely

 

Simon

 

 

 

********************************************************************

Simon Ruegg

Dr.med.vet.,  PhD candidate

Institute for Parasitology

Winterthurstr. 266a

8057 Zurich

Switzerland

 

phone: +41 44 635 85 93

fax: +41 44 635 89 07

e-mail: s.ruegg at access.unizh.ch

 


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From p.dalgaard at biostat.ku.dk  Thu Jan 11 00:32:22 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 11 Jan 2007 00:32:22 +0100
Subject: [R] TCL/TK and R documentation?
In-Reply-To: <45A563C4.8080904@noaa.gov>
References: <45A563C4.8080904@noaa.gov>
Message-ID: <45A57786.8060208@biostat.ku.dk>

Michael Prager wrote:
> I am hoping something has changed since I last asked about this.
>
> Is there any good source of documentation, including examples, of 
> using tcl/tk as a gui for R applications?
>
Well, this probably hasn't changed, but did you see it in the first place?

http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/


From pinard at iro.umontreal.ca  Thu Jan 11 01:06:55 2007
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Wed, 10 Jan 2007 19:06:55 -0500
Subject: [R] A question about R environment
In-Reply-To: <45A3317F.50506@sciviews.org>
References: <dc87d92f2693.45a288d8@usc.edu>
	<20070109031103.GA6104@phenix.progiciels-bpi.ca>
	<45A3317F.50506@sciviews.org>
Message-ID: <20070111000655.GA20145@alcyon.progiciels-bpi.ca>

[Philippe Grosjean]

>Please, don't reinvent the wheel: putting functions in a dedicated 
>environment is one of the things done by R packages (together with a 
>good documentation of the function, and making them easily installable 
>on any R implementation).

>[...] this is probably the time for you to read the "Writing 
>R extensions" manual, and to start implementing your own R package!

Hi, Philippe, and gang!

I read this manual long ago and used it to create packages already.  You 
really got the impression I did not read it? :-)

You know, there are small wheels, and huge wheels.  I do not see why 
I would use complex devices for tiny problems, merely because those 
complex devices exist.  R packages undoubtedly have their virtues, of 
course.  But just like many statistical tests, they do not always apply.

Why go at length organising package directories populated with many 
files, resorting to initialisation scripts, using package validators,
creating documentation files and processing them, go through the cycle 
of creating a package and installing it, all that merely for a few small 
quickies that fit very well in the ubiquitous .Rprofile file?  Why worry 
about installation on any R implementation, for little things only meant 
for myself, and too simple to warrant publication anyway?

                                        Keep happy, all.

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca


From karl.sommer at dpi.vic.gov.au  Thu Jan 11 02:07:00 2007
From: karl.sommer at dpi.vic.gov.au (karl.sommer at dpi.vic.gov.au)
Date: Thu, 11 Jan 2007 12:07:00 +1100
Subject: [R] axis date format in lattice
Message-ID: <OFC20A7B7C.D44B8DAD-ONCA257260.00029392-CA257260.00062248@nre.vic.gov.au>

Hello list,

plotting the following example 1 in lattice only labels the x-axis with one
tick mark at the beginning of February.
In contrast the standard plot() function labels the x-axis with equally
spaced tick marks.

#----example 1 -------
library(lattice)
date <- as.Date(c("1/10/2006", "01/11/2006", "01/12/2006",
                  "01/01/2007", "01/02/2007"),format="%d/%m/%Y")
xx <- c(1,3,5,6,7)
xx1 <- as.data.frame(cbind(date, xx))
xyplot(xx~date, scales=list(x=list(format = "%b")))
plot(xx~date)

Interestingly the behaviour in lattice only seems to arise when there is a
change in calendar years.  This is demonstrated by the second example with
dates from within 1 calendar year only where lattice works as expected.

I was wondering if the behaviour in example 1 was due to a bug or am I
missing something and, more importantly, is there a way around it?

#---example 2 -------
date <- as.Date(c("1/08/2006", "01/09/2006", "01/10/2006",
                  "01/11/2006", "01/12/2006"),format="%d/%m/%Y")
xx <- c(1,3,5,6,7)
xx1 <- as.data.frame(cbind(date, xx))
xyplot(xx~date, scales=list(x=list(format = "%b")))
plot(xx~date)

Cheers
Karl

_________________________________
Dr Karl J Sommer,
Department of Primary Industries,
Catchment & Agriculture Services,
PO Box 905
Mildura, VIC, Australia 3502

Tel: +61 (0)3 5051 4390
Fax +61 (0)3 5051 4534

Email:     karl.sommer at dpi.vic.gov.au


From ripley at stats.ox.ac.uk  Thu Jan 11 02:08:36 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Jan 2007 01:08:36 +0000 (GMT)
Subject: [R] a question of substitute
In-Reply-To: <200701091554.35442.dusa.adrian@gmail.com>
References: <200701091438.16341.dusa.adrian@gmail.com>
	<Pine.LNX.4.64.0701091329370.16337@gannet.stats.ox.ac.uk>
	<200701091554.35442.dusa.adrian@gmail.com>
Message-ID: <Pine.LNX.4.64.0701110043020.16431@gannet.stats.ox.ac.uk>

The 'Right Thing' is for oneway.test() to allow a variable for the first 
argument, and I have altered it in R-patched and R-devel to do so. So if 
your students can make use of R-patched that would be the best solution. 
If not, perhaps you could make a copy of oneway.test from R-patched 
available to them.  Normally I would worry about namespace issues, but it 
seems unlikely they would matter here: if they did assignInNamespace is 
likely to work to insert the fix.

Grothendieck's suggestions are steps towards a morass: they may work in 
simple cases but can make more complicated ones worse (such as looking for 
'data' in the wrong place).  These model fitting functions have rather 
precise requirements for where they look for their components:

 	'data'
 	the environment of 'formula'
 	the environment of the caller

and that includes where they look for 'data'.  It is easy to use 
substitute or such to make a literal formula out of 'formula', but doing 
so changes its environment.  So one needs to either

(a) fix up an environment within which to evaluate the modified call that 
emulates the scoping rules or

(b) create a new 'data' that has references to all the variables needed, 
and just call the function with the new 'formula' and new 'data'.

At first sight model.frame() looks the way to do (b), but it is not, since 
if there are function calls in the formula (e.g. log()) the model frame 
includes the derived variables and not the original ones.  There are 
workarounds (e.g. in glmmPQL), like using all.vars, creating a formula 
from that, setting its environment to that of the original function and 
then calling model.frame.

This comes up often enough that I have contemplated adding a solution to 
(b) to the stats package.

Doing either of these right is really pretty complicated, and not 
something to dash off code in a fairly quick reply (or even to check that 
the code in glmmPQL was general enough to be applicable).

On Tue, 9 Jan 2007, Adrian Dusa wrote:

> On Tuesday 09 January 2007 15:41, Prof Brian Ripley wrote:
>> oneway.test expects a literal formula, not a variable containing a
>> formula.  The help page says
>>
>>   formula: a formula of the form 'lhs ~ rhs' where 'lhs' gives the
>>            sample values and 'rhs' the corresponding groups.
>>
>> Furthermore, if you had
>>
>> foo.2 <- function() oneway.test(value ~ group)
>>
>> it would still not work, as
>>
>>      data: an optional matrix or data frame (or similar: see
>>            'model.frame') containing the variables in the formula
>>            'formula'.  By default the variables are taken from
>>            'environment(formula)'.
>>
>> I could show you several complicated workarounds, but why do you want to
>> do this?
>
> Thank you for your reply. The data argument was exactly the next problem I
> faced. My workaround involves checking if(missing(data)) then uses different
> calls to oneway.test(). I am certainly interested in other solutions, this
> one is indeed limited.
>
> I do this for the students in the anova class, checking first the homogeneity
> of variances with fligner.test(), printing the p.value and based on that
> changing the var.equal argument in the oneway.test()
> It's just for convenience, but they do like having it all-in-one.
>
> Best regards,
> Adrian
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rmh at temple.edu  Thu Jan 11 03:09:48 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 10 Jan 2007 21:09:48 -0500 (EST)
Subject: [R] posthoc tests with ANCOVA
Message-ID: <20070110210948.BRU09834@po-d.temple.edu>

## The WoodEnergy example in package HH (available on CRAN) is similar
## to the example you asked about.  I have two factors and a
## covariate, and the interaction between the factors and covariate is
## significant.  I construct the multiple comparisons of factor Stove
## at each level of factor Wood at a specified value of the covariate.


## open a trellis.device() with history recording on.
## There are many plots and you will need to move among them.

require(HH)

source(paste(file.path(.path.package(package="HH")[1], "scripts"),
             "MMC.WoodEnergy-aov.R", sep="/"))
## This call prints an Error message that you can ignore because
## the relevant call is inside a try() expression.
anova(energy.aov.4)  ## source() needs a print() statement, demo() doesn't.


source(paste(file.path(.path.package(package="HH")[1], "scripts"),
             "MMC.WoodEnergy.R", sep="/"))
## The contrast matrix is developed beginning on line 91 of file
## MMC.WoodEnergy.R


## The MMC (mean-mean multiple comparisons) plots are described in R
## with ?MMC.  The MMC plots in R use the glht package.

## Both files are commented.  Please read the comments.

## Those source() commands in HH_1.17 will be replaced by demo()
## commands in the next version of HH.
##
## demo("MMC.WoodEnergy-aov", package="HH")
## demo("MMC.WoodEnergy", package="HH")


## glht is not currently able to work with aov objects that have the
## Error() function.  You will need to respecify your model formula
## using the terms() function.  See the maiz.aov example in the ?MMC
## help page.


From drf5n at maplepark.com  Thu Jan 11 04:18:32 2007
From: drf5n at maplepark.com (David Forrest)
Date: Wed, 10 Jan 2007 21:18:32 -0600 (CST)
Subject: [R] zero margin / marginless plots
Message-ID: <Pine.LNX.4.64.0701102047580.13593@maplepark.com>

Hi,

I'd like to produce a marginless or zero margin plot so that the pixel 
coordinates represent the mathematics.

xy<-data.frame(x=c(0,1,1,0,0),y=c(0,1,0,0,1))
png('junk.png',width=300,height=300)
par(mar=c(0,0,0,0))
plot(xy$x,xy$y,xlim=c(0,1),ylim=c(,1))
dev.off()

The resultant file has about a 10 pixel margin around these lines, and I'm 
not sure what parameter or function is controlling this offset.  Any 
hints?

Thanks for your time,
Dave
-- 
  Dr. David Forrest
  drf at vims.edu                                    (804)684-7900w
  drf5n at maplepark.com                             (804)642-0662h
                                    http://maplepark.com/~drf5n/


From marc_schwartz at comcast.net  Thu Jan 11 04:36:33 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 10 Jan 2007 21:36:33 -0600
Subject: [R] zero margin / marginless plots
In-Reply-To: <Pine.LNX.4.64.0701102047580.13593@maplepark.com>
References: <Pine.LNX.4.64.0701102047580.13593@maplepark.com>
Message-ID: <1168486593.5008.4.camel@localhost.localdomain>

On Wed, 2007-01-10 at 21:18 -0600, David Forrest wrote:
> Hi,
> 
> I'd like to produce a marginless or zero margin plot so that the pixel 
> coordinates represent the mathematics.
> 
> xy<-data.frame(x=c(0,1,1,0,0),y=c(0,1,0,0,1))
> png('junk.png',width=300,height=300)
> par(mar=c(0,0,0,0))
> plot(xy$x,xy$y,xlim=c(0,1),ylim=c(,1))
> dev.off()
> 
> The resultant file has about a 10 pixel margin around these lines, and I'm 
> not sure what parameter or function is controlling this offset.  Any 
> hints?
> 
> Thanks for your time,
> Dave

By default, the axis ranges are extended by +/- 4%.  You can change this
by using:

  plot(xy$x, xy$y, xlim = c(0, 1), ylim = c(0, 1), 
       xaxs = "i", yaxs = "i")

where 'xaxs' and 'yaxs' set the axis ranges to the actual data ranges.

See ?par for more information.

HTH,

Marc Schwartz


From Lukas.Indermaur at eawag.ch  Thu Jan 11 09:34:17 2007
From: Lukas.Indermaur at eawag.ch (Indermaur Lukas)
Date: Thu, 11 Jan 2007 09:34:17 +0100
Subject: [R] batch job GLM calculations
Message-ID: <FE8C160D1505B24497FA7C78D4DADACA0478B5@EA-MAIL.eawag.wroot.emp-eaw.ch>

Hello

I want to batch job the calculation of many GLM-models, extract some values and store them in a file. Almost everything in the script below works (read file, extract values and write them to file) except I fail in indexing the GLM with the modelstructure it should run. Running GLM's conventionally is no problem.

 

Conventionally a GLM is calculated as:

--------------------------------------

glm(ZlogHRS ~ ZRi+ZE+ZPROX_MN+ZED+ZAlwd+ZT2+ZW+ZN+Sex+y, family = gaussian, data=t.data) 

(just a note: dependent variable is ZlogHRS, while the others are indepent variables)

 

Desired way: sequentially run GLM

---------------------------------

I want R to take the model structure to take from a vector called "modelstructure" and paste it into the GLM like:

glm(modelstructure[i], family = gaussian, data=t.data).

It would considerably ease my workload if there is a solution to the indexing problem within a GLM. I appreciate any hint.

Best regards

Lukas

p.s.

my R skills are rather poor

 

----------------------------------------START R-CODE-----------------------------------

# Read file

t.url <- "C://HR_calculations/2005_2006/HR_SIZE/Kandidatenmodelle_Berechnung/inputfiles/"

t.tuti <- read.table(paste(t.url, "All_animals.txt", sep=""),header=T)

 

collect.results <- function(x) {

#resets vectors which will be filled

i <- 0

AICA <- NA;

 

#put models names hierarchically in vector

modelnames <- c("1=global", "2=biotic1", "3=biotic2", "4=abiotic") #keep track of changes in model names and number

for (i in 1:length(modelnames)) 

#model structure of the four models given for all models to run

#global

modelstructure <- c(

"ZlogHRS ~ ZRi+ZE+ZPROX_MN+ZED+ZAlwd+ZT2+ZW+ZN+Sex+y",

#biotic1

"ZlogHRS ~ ZRi",

#biotic2

"ZlogHRS ~ ZPROX_MN",

#abiotic

"ZlogHRS ~ ZE")

 

ts.model <- glm(modelstructure[i], family = gaussian, data=t.data) 

 

# Extracts some mode results

n[i] <- length(resid(ts.model))

AICA[i] <- AIC(ts.model)

}

#Writes results to data file

x = data.frame(

modelnames, n, AICA)

write.table(x, paste(t.url, file="Results.txt"), sep="\t", quote=F) 

----------------------------------------END R-CODE-----------------------------------

 

 

 

 
??? 
Lukas Indermaur, PhD student 
eawag / Swiss Federal Institute of Aquatic Science and Technology 
ECO - Department of Aquatic Ecology
?berlandstrasse 133
CH-8600 D?bendorf
Switzerland
 
Phone: +41 (0) 71 220 38 25
Fax    : +41 (0) 44 823 53 15 
Email: lukas.indermaur at eawag.ch
www.lukasindermaur.ch


From dusa.adrian at gmail.com  Thu Jan 11 09:39:33 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Thu, 11 Jan 2007 10:39:33 +0200
Subject: [R] a question of substitute
In-Reply-To: <Pine.LNX.4.64.0701110043020.16431@gannet.stats.ox.ac.uk>
References: <200701091438.16341.dusa.adrian@gmail.com>
	<200701091554.35442.dusa.adrian@gmail.com>
	<Pine.LNX.4.64.0701110043020.16431@gannet.stats.ox.ac.uk>
Message-ID: <200701111039.33967.dusa.adrian@gmail.com>

Dear Prof. Ripley,

Thank you for this extensive explanation. It looks like my first solution is 
similar to (b): creating new variables inside the wrapper (and new data if 
not missing).
This course is only introductory, with simple models, and I do point students 
to each test separately if they want more complicated things.

I'm looking forward to the release of the 2.5.0 version.
Best regards,
Adrian


On Thursday 11 January 2007 03:08, Prof Brian Ripley wrote:
> The 'Right Thing' is for oneway.test() to allow a variable for the first
> argument, and I have altered it in R-patched and R-devel to do so. So if
> your students can make use of R-patched that would be the best solution.
> If not, perhaps you could make a copy of oneway.test from R-patched
> available to them.  Normally I would worry about namespace issues, but it
> seems unlikely they would matter here: if they did assignInNamespace is
> likely to work to insert the fix.
>
> Grothendieck's suggestions are steps towards a morass: they may work in
> simple cases but can make more complicated ones worse (such as looking for
> 'data' in the wrong place).  These model fitting functions have rather
> precise requirements for where they look for their components:
>
>  	'data'
>  	the environment of 'formula'
>  	the environment of the caller
>
> and that includes where they look for 'data'.  It is easy to use
> substitute or such to make a literal formula out of 'formula', but doing
> so changes its environment.  So one needs to either
>
> (a) fix up an environment within which to evaluate the modified call that
> emulates the scoping rules or
>
> (b) create a new 'data' that has references to all the variables needed,
> and just call the function with the new 'formula' and new 'data'.
>
> At first sight model.frame() looks the way to do (b), but it is not, since
> if there are function calls in the formula (e.g. log()) the model frame
> includes the derived variables and not the original ones.  There are
> workarounds (e.g. in glmmPQL), like using all.vars, creating a formula
> from that, setting its environment to that of the original function and
> then calling model.frame.
>
> This comes up often enough that I have contemplated adding a solution to
> (b) to the stats package.
>
> Doing either of these right is really pretty complicated, and not
> something to dash off code in a fairly quick reply (or even to check that
> the code in glmmPQL was general enough to be applicable).

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From kbeath at efs.mq.edu.au  Thu Jan 11 10:06:23 2007
From: kbeath at efs.mq.edu.au (Ken Beath)
Date: Thu, 11 Jan 2007 20:06:23 +1100
Subject: [R] batch job GLM calculations
Message-ID: <s5a698d5.036@mail.efs.mq.edu.au>

as.formula(modelstructure[i]) in the glm function

>>> "Indermaur Lukas" <Lukas.Indermaur at eawag.ch> 01/11/07 7:34 PM >>>
Hello

I want to batch job the calculation of many GLM-models, extract some values and store them in a file. Almost everything in the script below works (read file, extract values and write them to file) except I fail in indexing the GLM with the modelstructure it should run. Running GLM's conventionally is no problem.

 

Conventionally a GLM is calculated as:

--------------------------------------

glm(ZlogHRS ~ ZRi+ZE+ZPROX_MN+ZED+ZAlwd+ZT2+ZW+ZN+Sex+y, family = gaussian, data=t.data) 

(just a note: dependent variable is ZlogHRS, while the others are indepent variables)

 

Desired way: sequentially run GLM

---------------------------------

I want R to take the model structure to take from a vector called "modelstructure" and paste it into the GLM like:

glm(modelstructure[i], family = gaussian, data=t.data).

It would considerably ease my workload if there is a solution to the indexing problem within a GLM. I appreciate any hint.

Best regards

Lukas

p.s.

my R skills are rather poor

 

----------------------------------------START R-CODE-----------------------------------

# Read file

t.url <- "C://HR_calculations/2005_2006/HR_SIZE/Kandidatenmodelle_Berechnung/inputfiles/"

t.tuti <- read.table(paste(t.url, "All_animals.txt", sep=""),header=T)

 

collect.results <- function(x) {

#resets vectors which will be filled

i <- 0

AICA <- NA;

 

#put models names hierarchically in vector

modelnames <- c("1=global", "2=biotic1", "3=biotic2", "4=abiotic") #keep track of changes in model names and number

for (i in 1:length(modelnames)) 

#model structure of the four models given for all models to run

#global

modelstructure <- c(

"ZlogHRS ~ ZRi+ZE+ZPROX_MN+ZED+ZAlwd+ZT2+ZW+ZN+Sex+y",

#biotic1

"ZlogHRS ~ ZRi",

#biotic2

"ZlogHRS ~ ZPROX_MN",

#abiotic

"ZlogHRS ~ ZE")

 

ts.model <- glm(modelstructure[i], family = gaussian, data=t.data) 

 

# Extracts some mode results

n[i] <- length(resid(ts.model))

AICA[i] <- AIC(ts.model)

}

#Writes results to data file

x = data.frame(

modelnames, n, AICA)

write.table(x, paste(t.url, file="Results.txt"), sep="\t", quote=F) 

----------------------------------------END R-CODE-----------------------------------

 

 

 

 
??? 
Lukas Indermaur, PhD student 
eawag / Swiss Federal Institute of Aquatic Science and Technology 
ECO - Department of Aquatic Ecology
?berlandstrasse 133
CH-8600 D?bendorf
Switzerland
 
Phone: +41 (0) 71 220 38 25
Fax    : +41 (0) 44 823 53 15 
Email: lukas.indermaur at eawag.ch
www.lukasindermaur.ch

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Thu Jan 11 10:16:51 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Jan 2007 09:16:51 +0000 (GMT)
Subject: [R] batch job GLM calculations
In-Reply-To: <FE8C160D1505B24497FA7C78D4DADACA0478B5@EA-MAIL.eawag.wroot.emp-eaw.ch>
References: <FE8C160D1505B24497FA7C78D4DADACA0478B5@EA-MAIL.eawag.wroot.emp-eaw.ch>
Message-ID: <Pine.LNX.4.64.0701110913370.15307@gannet.stats.ox.ac.uk>

>From ?glm:

  formula: a symbolic description of the model to be fit. The details of
           model specification are given below.

which could be clearer. It needs to be a object of type 'formula'.

I believe as.formula(modelstructure[i]) will do what you want (but your 
posting has come out rather awkwardly laid out in double spacing).

On Thu, 11 Jan 2007, Indermaur Lukas wrote:

> Hello
>
> I want to batch job the calculation of many GLM-models, extract some values and store them in a file. Almost everything in the script below works (read file, extract values and write them to file) except I fail in indexing the GLM with the modelstructure it should run. Running GLM's conventionally is no problem.
>
>
>
> Conventionally a GLM is calculated as:
>
> --------------------------------------
>
> glm(ZlogHRS ~ ZRi+ZE+ZPROX_MN+ZED+ZAlwd+ZT2+ZW+ZN+Sex+y, family = gaussian, data=t.data)
>
> (just a note: dependent variable is ZlogHRS, while the others are indepent variables)
>
>
>
> Desired way: sequentially run GLM
>
> ---------------------------------
>
> I want R to take the model structure to take from a vector called "modelstructure" and paste it into the GLM like:
>
> glm(modelstructure[i], family = gaussian, data=t.data).
>
> It would considerably ease my workload if there is a solution to the indexing problem within a GLM. I appreciate any hint.
>
> Best regards
>
> Lukas
>
> p.s.
>
> my R skills are rather poor
>
>
>
> ----------------------------------------START R-CODE-----------------------------------
>
> # Read file
>
> t.url <- "C://HR_calculations/2005_2006/HR_SIZE/Kandidatenmodelle_Berechnung/inputfiles/"
>
> t.tuti <- read.table(paste(t.url, "All_animals.txt", sep=""),header=T)
>
>
>
> collect.results <- function(x) {
>
> #resets vectors which will be filled
>
> i <- 0
>
> AICA <- NA;
>
>
>
> #put models names hierarchically in vector
>
> modelnames <- c("1=global", "2=biotic1", "3=biotic2", "4=abiotic") #keep track of changes in model names and number
>
> for (i in 1:length(modelnames))
>
> #model structure of the four models given for all models to run
>
> #global
>
> modelstructure <- c(
>
> "ZlogHRS ~ ZRi+ZE+ZPROX_MN+ZED+ZAlwd+ZT2+ZW+ZN+Sex+y",
>
> #biotic1
>
> "ZlogHRS ~ ZRi",
>
> #biotic2
>
> "ZlogHRS ~ ZPROX_MN",
>
> #abiotic
>
> "ZlogHRS ~ ZE")
>
>
>
> ts.model <- glm(modelstructure[i], family = gaussian, data=t.data)
>
>
>
> # Extracts some mode results
>
> n[i] <- length(resid(ts.model))
>
> AICA[i] <- AIC(ts.model)
>
> }
>
> #Writes results to data file
>
> x = data.frame(
>
> modelnames, n, AICA)
>
> write.table(x, paste(t.url, file="Results.txt"), sep="\t", quote=F)
>
> ----------------------------------------END R-CODE-----------------------------------
>
>
>
>
>
>
>
>
> ???
> Lukas Indermaur, PhD student
> eawag / Swiss Federal Institute of Aquatic Science and Technology
> ECO - Department of Aquatic Ecology
> ?berlandstrasse 133
> CH-8600 D?bendorf
> Switzerland
>
> Phone: +41 (0) 71 220 38 25
> Fax    : +41 (0) 44 823 53 15
> Email: lukas.indermaur at eawag.ch
> www.lukasindermaur.ch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From niederlein-rstat at yahoo.de  Thu Jan 11 11:37:05 2007
From: niederlein-rstat at yahoo.de (Antje)
Date: Thu, 11 Jan 2007 11:37:05 +0100
Subject: [R] Error in plot.new() : Figure margins too large
Message-ID: <45A61351.10206@yahoo.de>


Hello,

was could be the reason for such an error message???
I'd like to create a window with 10x6 barplot and save it as pdf.

I tried:

pdf("histogram.pdf",width=7, height=7)

windows(cols, rows)
par(mfcol = c(rows,cols))

sapply(mat, calcHist)

dev.off()

Within the method of sapply, I call barplot

What is wrong???

Antje


From P.Dalgaard at biostat.ku.dk  Thu Jan 11 12:03:39 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 11 Jan 2007 12:03:39 +0100
Subject: [R] Error in plot.new() : Figure margins too large
In-Reply-To: <45A61351.10206@yahoo.de>
References: <45A61351.10206@yahoo.de>
Message-ID: <45A6198B.4030701@biostat.ku.dk>

Antje wrote:
> Hello,
>
> was could be the reason for such an error message???
>   
Generically, that the (per-subplot) figure region is so small that
subtracting margins leaves nowhere to plot. Reasons include: Plotting
area too small, too many subplots, too many lines of text in margins,
too large font size in margins.

> I'd like to create a window with 10x6 barplot and save it as pdf.
>
> I tried:
>
> pdf("histogram.pdf",width=7, height=7)
>   
What did you expect the following line to do? I think it doesn't...
> windows(cols, rows)
>   


> par(mfcol = c(rows,cols))
>
> sapply(mat, calcHist)
>
> dev.off()
>
> Within the method of sapply, I call barplot
>
> What is wrong???
>
> Antje
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From niederlein-rstat at yahoo.de  Thu Jan 11 12:51:24 2007
From: niederlein-rstat at yahoo.de (Antje)
Date: Thu, 11 Jan 2007 12:51:24 +0100
Subject: [R] Error in plot.new() : Figure margins too large
In-Reply-To: <45A6198B.4030701@biostat.ku.dk>
References: <45A61351.10206@yahoo.de> <45A6198B.4030701@biostat.ku.dk>
Message-ID: <45A624BC.8050002@yahoo.de>

Hmm, but what can be the solution? Any idea? Or any documentation on 
that which I could read to find a solution by myself?

with the windows statement, I wanted to achive a format which fits for 
my plots, so that each plot will have a quadratic area.

I guess, I did not unstand that much of windows / margins and plotting 
areas. Maybe someone can help me (even with a good overview on these 
basics...)

Antje



Peter Dalgaard schrieb:
> Antje wrote:
>> Hello,
>>
>> was could be the reason for such an error message???
>>   
> Generically, that the (per-subplot) figure region is so small that
> subtracting margins leaves nowhere to plot. Reasons include: Plotting
> area too small, too many subplots, too many lines of text in margins,
> too large font size in margins.
> 
>> I'd like to create a window with 10x6 barplot and save it as pdf.
>>
>> I tried:
>>
>> pdf("histogram.pdf",width=7, height=7)
>>   
> What did you expect the following line to do? I think it doesn't...
>> windows(cols, rows)
>>   
> 
> 
>> par(mfcol = c(rows,cols))
>>
>> sapply(mat, calcHist)
>>
>> dev.off()
>>
>> Within the method of sapply, I call barplot
>>
>> What is wrong???
>>
>> Antje
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>   
> 
>


From P.Dalgaard at biostat.ku.dk  Thu Jan 11 14:31:11 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 11 Jan 2007 14:31:11 +0100
Subject: [R] Error in plot.new() : Figure margins too large
In-Reply-To: <45A624BC.8050002@yahoo.de>
References: <45A61351.10206@yahoo.de> <45A6198B.4030701@biostat.ku.dk>
	<45A624BC.8050002@yahoo.de>
Message-ID: <45A63C1F.7070703@biostat.ku.dk>

Antje wrote:
> Hmm, but what can be the solution? Any idea? Or any documentation on 
> that which I could read to find a solution by myself?
>
> with the windows statement, I wanted to achive a format which fits for 
> my plots, so that each plot will have a quadratic area.
>   
OK, so what was the pdf() for?...

Notice that this creates subplots which are one inch square.  By
default  plots are surrounded by 9.2 lines of text vertically (and 6.2
horisontally). Now the distance between lines of text depends on the
pointsize, which is 12pt by default. In subplots, I believe it is
somewhat smaller (8pt?), but 9.2 lines of text still leaves no room for
the actual plot. So, look at help(par) and help(windows) and start
fiddling with pointsize, mar, mgp, and maybe more.
> I guess, I did not unstand that much of windows / margins and plotting 
> areas. Maybe someone can help me (even with a good overview on these 
> basics...)
>
> Antje
>
>
>
> Peter Dalgaard schrieb:
>   
>> Antje wrote:
>>     
>>> Hello,
>>>
>>> was could be the reason for such an error message???
>>>   
>>>       
>> Generically, that the (per-subplot) figure region is so small that
>> subtracting margins leaves nowhere to plot. Reasons include: Plotting
>> area too small, too many subplots, too many lines of text in margins,
>> too large font size in margins.
>>
>>     
>>> I'd like to create a window with 10x6 barplot and save it as pdf.
>>>
>>> I tried:
>>>
>>> pdf("histogram.pdf",width=7, height=7)
>>>   
>>>       
>> What did you expect the following line to do? I think it doesn't...
>>     
>>> windows(cols, rows)
>>>   
>>>       
>>     
>>> par(mfcol = c(rows,cols))
>>>
>>> sapply(mat, calcHist)
>>>
>>> dev.off()
>>>
>>> Within the method of sapply, I call barplot
>>>
>>> What is wrong???
>>>
>>> Antje
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>   
>>>       
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From bebeboom06 at gmail.com  Thu Jan 11 14:47:20 2007
From: bebeboom06 at gmail.com (Marta Rufino)
Date: Thu, 11 Jan 2007 13:47:20 +0000
Subject: [R] warning in GAM
Message-ID: <45A63FE8.2040700@cripsul.ipimar.pt>

Hello,

I have a problem when doing gam (from gam library; I am using R 2.4.0, 
windows xp platform)
When doing:
example(gam)

There is this error message (which also happens when using my data)
....
Warning: a final empty element has been omitted
the part of the args list of 'list' being evaluated was:
   (coefficients = fit$beta, residuals = fit$y - fit$eta, fitted.values 
= fit$eta, effects = effects, weights = w, rank = qrank, assign = 
attr(x, "assign"), qr = qrx, smooth = fit$s, nl.df = fit$df - 1, )

Any ideias?

Thank you very much in advance,
Marta

PS: happy new year :-)

-- 
.......................................................................
Marta M. Rufino (PhD)

.....
Instituto Nacional de Investiga??o Agr?ria e das Pescas (INIAP/IPIMAR),
Centro Regional de Investiga??o Pesqueira do Sul (CRIPSul)
Avenida 5 de Outubro s/n
P-8700-305 Olh?o, Portugal
+351 289 700 541

..... 
Institut de Ci?ncies del Mar - CMIMA (CSIC)
Passeig Mar?tim de la Barceloneta, 37-49      
08003 BARCELONA - Catalunya
Spain


From rjimenez at caminos.upm.es  Thu Jan 11 11:44:45 2007
From: rjimenez at caminos.upm.es (Rafael Jimenez)
Date: Thu, 11 Jan 2007 11:44:45 +0100 (CET)
Subject: [R] Three horizontal axes OR Two axes on same side?
Message-ID: <Pine.LNX.4.58.0701111134100.25106@rjimenez.caminos.upm.es>

Dear list:

I need to reproduce a plot with three different horizontal axes.

I know how to make plot with two different horizontal axes (one
above, one below) using axis():
   axis(1, ....)
   axis(3, ....)

However, I don't know how to produce two axes on the same side of the
plot.

Any pointers or examples?

--
R. Jimenez


From P.Dalgaard at biostat.ku.dk  Thu Jan 11 15:14:30 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 11 Jan 2007 15:14:30 +0100
Subject: [R] Three horizontal axes OR Two axes on same side?
In-Reply-To: <Pine.LNX.4.58.0701111134100.25106@rjimenez.caminos.upm.es>
References: <Pine.LNX.4.58.0701111134100.25106@rjimenez.caminos.upm.es>
Message-ID: <45A64646.9090105@biostat.ku.dk>

Rafael Jimenez wrote:
> Dear list:
>
> I need to reproduce a plot with three different horizontal axes.
>
> I know how to make plot with two different horizontal axes (one
> above, one below) using axis():
>    axis(1, ....)
>    axis(3, ....)
>
> However, I don't know how to produce two axes on the same side of the
> plot.
>
> Any pointers or examples?
>   
Try this, and I think you'll see the light:

plot(0)
axis(1, line=-3)


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From a.l.w.kuijper at rug.nl  Thu Jan 11 15:16:50 2007
From: a.l.w.kuijper at rug.nl (Bram Kuijper)
Date: Thu, 11 Jan 2007 15:16:50 +0100
Subject: [R] levelplot not adjusting colors
Message-ID: <45A646D2.6020805@rug.nl>

Hi all,

I try to make a levelplot from the Trellis graphics package of count 
data given a certain x and y variable.

The problem is that I can adjust the colorkey colors, but the colors of 
the actual values in the plot will be unchanged if I change the colorkey.

e.g.
my_lvl_plot <- levelplot(my_z ~ my_x * my_y, colorkey = 		 		 
list(col=some_color_vector))

will result in a levelplot that draws new colors in the colorkey, but 
still draws the default colors in the plotting space.

anybody ideas how to change also the colors in the plotting space to the 
values depicted in the color key?

thanks,
Bram


From wl at eimb.ru  Thu Jan 11 15:18:49 2007
From: wl at eimb.ru (Vladimir Eremeev)
Date: Thu, 11 Jan 2007 14:18:49 +0000 (UTC)
Subject: [R] Three horizontal axes OR Two axes on same side?
References: <Pine.LNX.4.58.0701111134100.25106@rjimenez.caminos.upm.es>
Message-ID: <loom.20070111T151521-209@post.gmane.org>

> However, I don't know how to produce two axes on the same side of the
> plot.
> 
> Any pointers or examples?

Try manually drawing the axis using lines() or segments() or arrows() and 
text() or mtext()


From h0125130 at wu-wien.ac.at  Thu Jan 11 11:52:23 2007
From: h0125130 at wu-wien.ac.at (Ingo Feinerer)
Date: Thu, 11 Jan 2007 11:52:23 +0100
Subject: [R] [R-pkgs] tm 0.1 uploaded to CRAN
Message-ID: <45A616E7.9060404@wu-wien.ac.at>

Dear useRs,

a first version of tm has just been released on CRAN.

tm provides a sophisticated framework for text mining applications
within R.

It offers functionality for managing text documents, abstracts the
process of document manipulation and eases the usage of heterogeneous
text formats in R. An advanced metadata management is
implemented for collections of text documents to alleviate the usage
of large and with metadata enriched document sets.

With the package ships native support for handling
   *) the Reuters 21578 dataset,
   *) the Reuters Corpus Volume 1 dataset,
   *) Gmane RSS feeds,
   *) e-mails, and
   *) several classic file formats (e.g. plain text or CSV text).

tm provides easy access to preprocessing and manipulation mechanisms, like
   *) whitespace removal,
   *) stemming, or
   *) conversion between file formats (e.g., Reuters21578 to plain
   text).

Further a generic filter architecture is available in order to
   *) filter documents for certain criteria,
   *) or perform fulltext search.

The package supports the export from document collections to
term-document matrices as frequently used in the text mining
literature. This allows the straight-forward integration of existing
methods for classification, clustering, visualizations, etc.

The package is designed in a modular way to enable easy integration of
new file formats, parsers, transformations and filter operations.

Best regards,

Ingo Feinerer

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From wl at eimb.ru  Thu Jan 11 15:25:44 2007
From: wl at eimb.ru (Vladimir Eremeev)
Date: Thu, 11 Jan 2007 14:25:44 +0000 (UTC)
Subject: [R] simpler solution (untested)
References: <Pine.LNX.4.58.0701111134100.25106@rjimenez.caminos.upm.es>
Message-ID: <loom.20070111T152330-368@post.gmane.org>

?axis says that this function has the logical parameter outer "indicating 
whether the axis should be drawn in the outer plot margin, rather than the 
standard plot margin".
You could try two calls to it with different outer values.


From marcella.marinelli at uniroma1.it  Thu Jan 11 15:27:19 2007
From: marcella.marinelli at uniroma1.it (march)
Date: Thu, 11 Jan 2007 06:27:19 -0800 (PST)
Subject: [R] gbn with jumps
Message-ID: <8278025.post@talk.nabble.com>


Hi everybody
I'd like to simulate a Generalized Wiener Process with jumps. Any
suggestion?
Thanks
Marcella
-- 
View this message in context: http://www.nabble.com/gbn-with-jumps-tf2959051.html#a8278025
Sent from the R help mailing list archive at Nabble.com.


From marcella.marinelli at uniroma1.it  Thu Jan 11 15:27:38 2007
From: marcella.marinelli at uniroma1.it (march)
Date: Thu, 11 Jan 2007 06:27:38 -0800 (PST)
Subject: [R] gbm with jumps
Message-ID: <8278026.post@talk.nabble.com>


Hi everybody
I'd like to simulate a Generalized Wiener Process with jumps. Any
suggestion?
Thanks
Marcella
-- 
View this message in context: http://www.nabble.com/gbm-with-jumps-tf2959052.html#a8278026
Sent from the R help mailing list archive at Nabble.com.


From wl at eimb.ru  Thu Jan 11 15:34:27 2007
From: wl at eimb.ru (Vladimir Eremeev)
Date: Thu, 11 Jan 2007 14:34:27 +0000 (UTC)
Subject: [R] Three horizontal axes OR Two axes on same side?
References: <Pine.LNX.4.58.0701111134100.25106@rjimenez.caminos.upm.es>
	<45A64646.9090105@biostat.ku.dk>
Message-ID: <loom.20070111T153320-449@post.gmane.org>

> axis(1, line=-3)

I have just come to this solution.
However, it seems, I don't understand the meaning of the outer parameter.
What is it for?


From ripley at stats.ox.ac.uk  Thu Jan 11 16:55:13 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Jan 2007 15:55:13 +0000 (GMT)
Subject: [R] warning in GAM
In-Reply-To: <45A63FE8.2040700@cripsul.ipimar.pt>
References: <45A63FE8.2040700@cripsul.ipimar.pt>
Message-ID: <Pine.LNX.4.64.0701111553070.13776@gannet.stats.ox.ac.uk>

Please update your gam package: this was fixed in 0.98.

And the _warning_ (not 'error') message is explained in the NEWS file for 
R 2.4.0.  (It will be an error in R 2.5.0.)

On Thu, 11 Jan 2007, Marta Rufino wrote:

> Hello,
>
> I have a problem when doing gam (from gam library; I am using R 2.4.0,
> windows xp platform)
> When doing:
> example(gam)
>
> There is this error message (which also happens when using my data)
> ....
> Warning: a final empty element has been omitted
> the part of the args list of 'list' being evaluated was:
>   (coefficients = fit$beta, residuals = fit$y - fit$eta, fitted.values
> = fit$eta, effects = effects, weights = w, rank = qrank, assign =
> attr(x, "assign"), qr = qrx, smooth = fit$s, nl.df = fit$df - 1, )
>
> Any ideias?
>
> Thank you very much in advance,
> Marta
>
> PS: happy new year :-)
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From drf5n at maplepark.com  Thu Jan 11 17:06:15 2007
From: drf5n at maplepark.com (David Forrest)
Date: Thu, 11 Jan 2007 10:06:15 -0600 (CST)
Subject: [R] zero margin / marginless plots (in lattice?)
In-Reply-To: <1168486593.5008.4.camel@localhost.localdomain>
References: <Pine.LNX.4.64.0701102047580.13593@maplepark.com>
	<1168486593.5008.4.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.64.0701110948100.13593@maplepark.com>

Thanks.  The xaxs|yaxs='i' works well for the base graphics.  Is there an 
additional parameter in play for lattice graphics?  The closest I could 
gotten is the below which still leaves a bit of a margin:

xy<-data.frame(x=c(0,1,1,0,0),y=c(0,1,0,0,1))
xyplot(y~x,xy,scales=list(axs='i',draw=FALSE),type='l',xlab=NULL,ylab=NULL)

Dave

On Wed, 10 Jan 2007, Marc Schwartz wrote:

> On Wed, 2007-01-10 at 21:18 -0600, David Forrest wrote:
>> Hi,
>>
>> I'd like to produce a marginless or zero margin plot so that the pixel
>> coordinates represent the mathematics.
>>
>> xy<-data.frame(x=c(0,1,1,0,0),y=c(0,1,0,0,1))
>> png('junk.png',width=300,height=300)
>> par(mar=c(0,0,0,0))
>> plot(xy$x,xy$y,xlim=c(0,1),ylim=c(,1))
>> dev.off()
>>
>> The resultant file has about a 10 pixel margin around these lines, and I'm
>> not sure what parameter or function is controlling this offset.  Any
>> hints?
>>
>> Thanks for your time,
>> Dave
>
> By default, the axis ranges are extended by +/- 4%.  You can change this
> by using:
>
>  plot(xy$x, xy$y, xlim = c(0, 1), ylim = c(0, 1),
>       xaxs = "i", yaxs = "i")
>
> where 'xaxs' and 'yaxs' set the axis ranges to the actual data ranges.
>
> See ?par for more information.
>
> HTH,
>
> Marc Schwartz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
  Dr. David Forrest
  drf at vims.edu                                    (804)684-7900w
  drf5n at maplepark.com                             (804)642-0662h
                                    http://maplepark.com/~drf5n/


From evaiannario at libero.it  Thu Jan 11 17:07:11 2007
From: evaiannario at libero.it (evaiannario)
Date: Thu, 11 Jan 2007 17:07:11 +0100
Subject: [R] overdispersion
Message-ID: <JBPORZ$62F50EE5FE3C451ACC548BD488063F16@libero.it>

How can I eliminate the overdispersion for binary data apart the use of the quasibinomial? 
help me 
Eva Iannario



------------------------------------------------------
Passa a Infostrada. ADSL e Telefono senza limiti e senza canone Telecom
http://click.libero.it/infostrada11gen07


From francogrex at mail.com  Thu Jan 11 17:21:25 2007
From: francogrex at mail.com (francogrex)
Date: Thu, 11 Jan 2007 08:21:25 -0800 (PST)
Subject: [R] maximum likelihood, 1st and 2nd derivative
Message-ID: <8278073.post@talk.nabble.com>


Hi guys again, it seems I haven't been doing the maximum likelihood
estimation correctly. I quote below, can someone explain to me please what
does it mean that the 2nd and 3rd derivatives of the function equals zero
and how to compute that in R.

"We have our initial estimated, subjective parameters for the gamma mixture
and we have our likelihood that is the mixture of negative binomials
representing the distribution of actual observed values. We 'pool' these
distributions and determine which expression for the parameters would be
most likely to produce the sample of observed negative binomial counts
(determine the MLE). This maximisation involves a search in five-dimensional
parameter space {?: ?1,?2, ?1, ?2, P} for the vector that maximises the
likelihood as evidenced by the first and second derivatives of the function
being zero. The likelihood is L(?) = ?ij {P f (Nij; ?1, ?1, Eij) + (1-P) f
(Nij; ?2, ?2, Eij)} This involves millions of calculations. The
computational procedures required for these calculations are based on the
Newton-Raphson method. This is an old calculus-based technique that was
devised to find the roots of an equation (e.g. the values of the independent
variable (e.g. x) for which the value of the function (e.g. f(x)) equals
zero."

-- 
View this message in context: http://www.nabble.com/maximum-likelihood%2C-1st-and-2nd-derivative-tf2959077.html#a8278073
Sent from the R help mailing list archive at Nabble.com.


From hellokisas at gmail.com  Thu Jan 11 17:36:34 2007
From: hellokisas at gmail.com (Feng Qiu)
Date: Thu, 11 Jan 2007 11:36:34 -0500
Subject: [R]  How to use RBF nueral network to predict
References: <45A63FE8.2040700@cripsul.ipimar.pt>
	<Pine.LNX.4.64.0701111553070.13776@gannet.stats.ox.ac.uk>
Message-ID: <00ca01c7359e$a88baf20$6400a8c0@Aglog>

Hi all:
          I'm trying to use RBF neural network for predicting. The package 
I'm using now is "neural". The type of network that I have to use is RBF. 
But I didn't find predict function in this package. Does anyone have such an 
experience?  Any advice is appreciated!
           Thank you!

Best,

Feng


From gamoric at gmail.com  Thu Jan 11 17:51:42 2007
From: gamoric at gmail.com (Gerald Gamoric)
Date: Thu, 11 Jan 2007 10:51:42 -0600
Subject: [R] Treatment for Unequal Column Lengths?
Message-ID: <ca1195810701110851y336df23escaeab84809288566@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070111/b69e6678/attachment.pl 

From elvis at xlsolutions-corp.com  Thu Jan 11 18:50:57 2007
From: elvis at xlsolutions-corp.com (elvis at xlsolutions-corp.com)
Date: Thu, 11 Jan 2007 10:50:57 -0700
Subject: [R] February Courses: (1) Prof Frank Harrell *** Regression
	Modeling Strategies in R/Splus, (2) R/Splus Advanced Programming,
	(3) R/Splus Fundamentals and Programming Techniques
Message-ID: <20070111105057.9f08cc34deb45d78e54b3b5664e21546.392c45b1c9.wbe@email.secureserver.net>


XLSolutions Corporation (www.xlsolutions-corp.com) is proud to announce
February courses:
 
(1) Regression Modeling Strategies in R/Splus --- by Prof Frank Harrell
 
                   http://www.xlsolutions-corp.com/Rstats2.htm
                         
                       *** New York, February 22-23, 2007 ***
                       *** San Francisco, March 8-9, 2007 ***
                       
 
(2) R/Splus Advanced Programming  --- by the R Development Core Team
Guru!
 
                  http://www.xlsolutions-corp.com/Radv.htm
 
                    *** Seattle & San Francisco  in March / Dates TBD
***
 
(3) R/Splus Fundamentals and Programming Techniques  
         
                 http://www.xlsolutions-corp.com/Rfund.htm
 
                   *** Chicago / February 1-2, 2007, ***
                   *** Seattle / February 1-2, 2007, ***
                   *** San Diego / February 8-9, 2007, ***
                   *** Princeton / February 15-16, 2007, ***
                   *** Salt Lake City /  February 15-16, 2007 ***
 
Ask for group discount and reserve your seat Now - Earlybird Rates
Payment due after the class! Email Sue Turner:  sue at xlsolutions-corp.com

 
Email us for group discounts: sue at xlsolutions-corp.com
Phone:  206 686 1578
 
Visit us: www.xlsolutions-corp.com/training.htm
 
Please let us know if you and your colleagues are interested in this
class to take advantage of group discount. Register now to secure your
seat!
 
Cheers,
 
Elvis Miller, PhD
Manager Training
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com/training.htm
elvis at xlsolutions-corp.com


From tagett at ipsogen.com  Thu Jan 11 19:09:25 2007
From: tagett at ipsogen.com (Rebecca Tagett)
Date: Thu, 11 Jan 2007 19:09:25 +0100
Subject: [R] R graphics with Linux (libpng)
Message-ID: <200701111758.l0BHwXVF018367@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070111/79c31b13/attachment.pl 

From p.dalgaard at biostat.ku.dk  Thu Jan 11 19:04:26 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 11 Jan 2007 19:04:26 +0100
Subject: [R] overdispersion
In-Reply-To: <JBPORZ$62F50EE5FE3C451ACC548BD488063F16@libero.it>
References: <JBPORZ$62F50EE5FE3C451ACC548BD488063F16@libero.it>
Message-ID: <45A67C2A.6000700@biostat.ku.dk>

evaiannario wrote:
> How can I eliminate the overdispersion for binary data apart the use of the quasibinomial?
There is no such thing as overdispersion for binary data. (The variance 
of a two-point distribution is a known function of the mean.) If what 
you want to do is include random effects of some sort of grouping then 
you might look into generalized linear mixed models via lmer() from the 
lme4 package or glmmPQL from MASS.


From bcarvalh at jhsph.edu  Thu Jan 11 19:06:10 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Thu, 11 Jan 2007 13:06:10 -0500
Subject: [R] R graphics with Linux (libpng)
In-Reply-To: <200701111758.l0BHwXVF018367@hypatia.math.ethz.ch>
References: <200701111758.l0BHwXVF018367@hypatia.math.ethz.ch>
Message-ID: <ECA7E8EC-CC65-4ADE-A0D4-DB885AC8D48E@jhsph.edu>

Hi Rebecca,

png (and also jpeg, for example) require an X11 connection.

So, assuming you're working from the command line and that your X11  
server is up, you would need to do something like:

linux$ export DISPLAY=:0.0

before loading R...

and if your linux machine is remote (and you're connecting through  
SSH), you'd need

linux$ ssh -X remote.machine.somewhere

b


On Jan 11, 2007, at 1:09 PM, Rebecca Tagett wrote:

> Hello,
>
> I'm trying to adapt some R code that works on Windows so that it  
> will work
> on a Linux machine.
>
> The command :
>
> png("myFile.png", width=600, height=600)
>
> fails claiming that it is impossible to establish a connection with  
> X11.
> (Error messages are in French, so I'm not pasting them here!)
>
>
>
> I have libpng installed:
>
>> rpm -qa libpng*
>
> libpng-1.0.12-2
>
> libpng-devel-1.0.12-2
>
>
>
> So I don't understand why R thinks I'm trying to connect to X11. I  
> haven't
> been able to find many examples of R graphics code specifically for  
> Linux,
> but I have the impression that if libpng is installed, the graphics  
> commands
> are identical. The libpng manual is not useful, because it does not  
> mention
> use of libpng commands in an R environment.
>
>
>
> I'm using R 2.4.1, which I recently installed. Do I have to install  
> a more
> recent libpng ? If so, do I have to reconfigure and remake R ?
>
>
>
> I've also read that no graphics devices are available under R CMD  
> BATCH.
> Does that really mean that I can't create graphics from some R code  
> that I
> launch in noninteractive mode?
>
>
>
> Thanks in advance !
>
>
>
> Rebecca Tagett
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bcarvalh at jhsph.edu  Thu Jan 11 19:06:58 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Thu, 11 Jan 2007 13:06:58 -0500
Subject: [R] R graphics with Linux (libpng)
In-Reply-To: <200701111758.l0BHwXVF018367@hypatia.math.ethz.ch>
References: <200701111758.l0BHwXVF018367@hypatia.math.ethz.ch>
Message-ID: <404E19F8-9F52-4421-B97F-296B58D34074@jhsph.edu>

I forgot to mention that

bitmap()

will do what you want without an X11 connection.

b

On Jan 11, 2007, at 1:09 PM, Rebecca Tagett wrote:

> Hello,
>
> I'm trying to adapt some R code that works on Windows so that it  
> will work
> on a Linux machine.
>
> The command :
>
> png("myFile.png", width=600, height=600)
>
> fails claiming that it is impossible to establish a connection with  
> X11.
> (Error messages are in French, so I'm not pasting them here!)
>
>
>
> I have libpng installed:
>
>> rpm -qa libpng*
>
> libpng-1.0.12-2
>
> libpng-devel-1.0.12-2
>
>
>
> So I don't understand why R thinks I'm trying to connect to X11. I  
> haven't
> been able to find many examples of R graphics code specifically for  
> Linux,
> but I have the impression that if libpng is installed, the graphics  
> commands
> are identical. The libpng manual is not useful, because it does not  
> mention
> use of libpng commands in an R environment.
>
>
>
> I'm using R 2.4.1, which I recently installed. Do I have to install  
> a more
> recent libpng ? If so, do I have to reconfigure and remake R ?
>
>
>
> I've also read that no graphics devices are available under R CMD  
> BATCH.
> Does that really mean that I can't create graphics from some R code  
> that I
> launch in noninteractive mode?
>
>
>
> Thanks in advance !
>
>
>
> Rebecca Tagett
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Thu Jan 11 19:26:44 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Jan 2007 18:26:44 +0000 (GMT)
Subject: [R] R graphics with Linux (libpng)
In-Reply-To: <200701111758.l0BHwXVF018367@hypatia.math.ethz.ch>
References: <200701111758.l0BHwXVF018367@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0701111818300.30884@gannet.stats.ox.ac.uk>

On Thu, 11 Jan 2007, Rebecca Tagett wrote:

> Hello,
>
> I'm trying to adapt some R code that works on Windows so that it will work
> on a Linux machine.
>
> The command :
>
> png("myFile.png", width=600, height=600)
>
> fails claiming that it is impossible to establish a connection with X11.
> (Error messages are in French, so I'm not pasting them here!)
>
>
>
> I have libpng installed:
>
>> rpm -qa libpng*
>
> libpng-1.0.12-2
>
> libpng-devel-1.0.12-2
>
>
>
> So I don't understand why R thinks I'm trying to connect to X11. I haven't
> been able to find many examples of R graphics code specifically for Linux,
> but I have the impression that if libpng is installed, the graphics commands
> are identical. The libpng manual is not useful, because it does not mention
> use of libpng commands in an R environment.

?png says

      R can be compiled without support for either or both of these
      devices: this will be reported if you attempt to use them on a
      system where they are not supported.  They may not be usable
      unless the X11 display is available to the owner of the R process.

Note:

      These are based on the 'X11' device, so the additional arguments
      to that device work, but are rarely appropriate.  The colour
      handling will be that of the 'X11' device in use.

      'bitmap' provides an alternative way to generate PNG and JPEG
      plots that does not depend on accessing the X11 display but does
      depend on having GhostScript installed.  (Device 'GDD' in CRAN
      package 'GDD' is another alternative using several other
      additional pieces of software.)

so the help page seems quite specific about the connection to X11.


> I'm using R 2.4.1, which I recently installed. Do I have to install a more
> recent libpng ? If so, do I have to reconfigure and remake R ?

The libpng is very old, but even more likely is that libz is too old.
There were messages when you configured: did you read them at all?  If 
not, plese do rebuild R and read them this time.  Without the exact 
error message I cannot be sure, but I don't think this is the problem.

> I've also read that no graphics devices are available under R CMD BATCH.
> Does that really mean that I can't create graphics from some R code that I
> launch in noninteractive mode?

Your source is incorrect: please advise the author(s) to correct it.

> Thanks in advance !
>
> Rebecca Tagett
>
> 	[[alternative HTML version deleted]]

Did you also read that you are asked not to send HTML mail? (That is from 
a reliable source.)

>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Greg.Snow at intermountainmail.org  Thu Jan 11 19:50:27 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Thu, 11 Jan 2007 11:50:27 -0700
Subject: [R] Treatment for Unequal Column Lengths?
Message-ID: <07E228A5BE53C24CAD490193A7381BBB77CCF6@LP-EXCHVS07.CO.IHC.COM>

One of the ways that R (and S-plus) is different from most other stats
packages (all that I can think of) is that it forces you to think about
your data up front.  This is a good thing.  It sounds like you really
have multiple datasets in one file, it is best to read them into R as
separate datasets, not try to force them into 1 dataset (like other
packages do).  If you need to keep the multiple datasets grouped
together you can combine them together in a list.

To read one dataset out of the file you can use read.table (read.csv)
with the colClasses (set other columns to "NULL") and nrows to grab just
the columns and rows from a given dataset.  I would recommend writing a
script to read each of the separate pieces.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gerald Gamoric
> Sent: Thursday, January 11, 2007 9:52 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Treatment for Unequal Column Lengths?
> 
> Fellow R Users:
> 
> I have a .csv dataset that I have brought into R via 
> read.table (and also via read.csv). The dataset has columns 
> that are not equal in length.
> Essentially, this data file has vectors/columns in which I 
> plan to use different analyses on, hence they are unequal in 
> length. Also, the columns are either numeric or calendar 
> dates. Is there a way to prevent R from appending "NA"s to 
> the numeric columns that are not the longest? Is there a way 
> to prevent R from appending blank cells to the columns of 
> dates in the dataset that are not the longest? In other 
> words, I'd like to have R maintain each column's length.
> 
> I am aware that I can use "na.omit" before calling each 
> numeric column in my analysis in order to work with the 
> subset of that column that does not contain the "NA" values. 
> However, the na.omit command does not work when R appends 
> blank cells to my date column lengths. Is there something 
> analogous to "na.omit" that I might be able to use when I am 
> working with a column of dates to ignore the blank cells?
> 
> Further, I am curious as to whether there is an option that 
> one might use when the dataset is read in to R in order to 
> keep all the column lengths as they are. Any ideas/hints 
> would be very much appreciated.
> 
> platform i386-pc-mingw32
> 
> arch i386
> 
> os mingw32
> 
> system i386, mingw32
> 
> status
> 
> major 2
> 
> minor 3.1
> 
> year 2006
> 
> month 06
> 
> day 01
> 
> svn rev 38247
> 
> language R
> 
> Thank you,
> 
> Dave H
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tjf2n at virginia.edu  Thu Jan 11 20:53:49 2007
From: tjf2n at virginia.edu (Tim Finney)
Date: Thu, 11 Jan 2007 14:53:49 -0500
Subject: [R] Exploratory multivariate analysis of categorical data
Message-ID: <1168545229.25214.94.camel@localhost.localdomain>

This is my first post to R-help. I am doing some research into the text
of the New Testament, specifically places where textual variation occurs
across manuscripts. (See http://purl.org/tfinney/NTText/book/index.html
for details.)

New Testament textual critics call places where the text varies
"variation units," and each state of the text in a variation unit is
called a "reading." The apparatus of a critical edition can be
transformed into a data matrix by making each witness (typically a
manuscript, but might be an early version or church father) an
observation (i.e. a row) and each variation unit a variable (i.e. a
column). I encode readings, which consist of words or phrases, as
numerals in the data matrix. (There are often more than two readings in
a variation unit.) I make a dissimilarity matrix by calculating the
proportion of variation units in which each pair of witnesses disagrees.

Here is my question: Which exploratory multivariate techniques are
applicable to this kind of data matrix and this kind of dissimilarity
matrix? From reading the R docs, it seems to me that MDS (metric and
non-metric) and hierarchical clustering are appropriate, but I am not so
sure about others.

Best

Tim Finney


From fjbuch at gmail.com  Thu Jan 11 21:56:52 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Thu, 11 Jan 2007 15:56:52 -0500
Subject: [R] Matching on multiple columns
Message-ID: <bd93cdad0701111256s57a3545n9ffe0a6a84101351@mail.gmail.com>

Am I correct in believing that one cannot match on multiple columns?
One can indeed subset on multiple criteria from different variables
(or columns) but not from unique combinations thereof.
I need to exclude about 10000 rows from 108000 rows of data based on
several unique combinations of identifiers in two columns. Only
merge() seems to be able to do that. Merge would allow me to
positively select but it would not allow me to deselect (or exclude).
Look at how I got around the problem.
It is inelegant. Have a missed a more direct function?


x<-seq(from=1,to=10)
y <-rep(1:2,5)
data<-data.frame(cbind(x,y))
x<-seq(from=1,to=10)
y <-rep(1:5,2)
data1<-data.frame(cbind(x,y))
newdata<-rbind(data,data1)
newdata[10,2] <- 3
exclude<-newdata[18:20,]
#This is a simulation of real life problem
#We now have two dataframes. Newdata is the data set from an
experiment. In real #life it has an additional 9 columns of data
#exclude is a dataframe that was manually created after discovering
some quality #control issues
#Any row in newdata that matches any row in exclude must be discarded
match(newdata$x,exclude$x)# useless because it is only on one column
match(newdata$y,exclude$y)# useless because it is only on one column
newdata$x %in%  exclude$x  # useless because it is only on one column
newdata$y %in%  exclude$y # useless because it is only on one column
newdata$x %in%  exclude$x &  newdata$y %in%  exclude$y  # useless,
#eventhough it is using both columns, because it is not
#using them in a synchronous manner. Row 10 in new data should not
have been #marked "TRUE"
#It was only labeled such because the 10 in the x column is indeed in
the exclude x #column and the 3 in the y column is indeed in the
exclude y column but not #together
which(newdata$x %in%  exclude$x &  newdata$y %in%  exclude$y)#also
gets it #wrong
match(newdata,exclude)# intuitively this could have worked but alas
match can #only handle vectors and not dataframes. It cannot match on
multiple columns
#I have to stoop to the inelegant maneuver of creating a combined
variable of the #two columns, albeit only temporarily
paste(newdata$x,newdata$y,sep=":") %in% paste(exclude$x,exclude$y,sep=":")
#or one could do this
match(paste(newdata$x,newdata$y,sep=":"),paste(exclude$x,exclude$y,sep=":"))
#or
{which(paste(newdata$x,newdata$y,sep=":") %in% paste
(exclude$x,exclude$y,sep=":"))}
#Or if you want to use the result in an index term or a selection
argument in a subset command
{paste(newdata$x,newdata$y,sep=":") %in%
paste(exclude$x,exclude$y,sep=":")==FALSE}
#As in
{newdata[paste(newdata$x,newdata$y,sep=":") %in%
paste(exclude$x,exclude$y,sep=":")==FALSE,]}
#or
{subset(newdata,paste(newdata$x,newdata$y,sep=":") %in%
paste(exclude$x,exclude$y,sep=":")==FALSE)}


-- 
Farrel Buchinsky


From bolker at zoo.ufl.edu  Thu Jan 11 23:03:40 2007
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Thu, 11 Jan 2007 22:03:40 +0000 (UTC)
Subject: [R] maximum likelihood, 1st and 2nd derivative
References: <8278073.post@talk.nabble.com>
Message-ID: <loom.20070111T225832-712@post.gmane.org>

francogrex <francogrex <at> mail.com> writes:

> [SNIP]
> This maximisation involves a search in five-dimensional
> parameter space {?: ?1,?2, ?1, ?2, P} for the vector that maximises the
> likelihood as evidenced by the first and second derivatives of the function
> being zero. The likelihood is L(?) = ?ij {P f (Nij; ?1, ?1, Eij) + (1-P) f
> (Nij; ?2, ?2, Eij)} This involves millions of calculations. The
> computational procedures required for these calculations are based on the
> Newton-Raphson method. This is an old calculus-based technique that was
> devised to find the roots of an equation (e.g. the values of the independent
> variable (e.g. x) for which the value of the function (e.g. f(x)) equals
> zero."

  I'm sure someone will correct me if I'm wrong, but this seems wrong
to me.  We only want the first derivatives to be zero.  It wouldn't be
impossible for second and higher derivatives to be zero, but it would
be somewhat pathological.
  While optimization will be faster and more stable if you can compute
the first derivatives (the _gradient_) analytically (and R has the
D() and deriv() functions for doing so), R will compute derivatives
numerically by finite differences if you don't.  See ?optim.

  Blatant plug:  www.zoo.ufl.edu/bolker/emdbook/chap7A.pdf  pp. 3-4
might be helpful too.

  Ben Bolker


From cberry at tajo.ucsd.edu  Thu Jan 11 23:26:05 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Thu, 11 Jan 2007 14:26:05 -0800
Subject: [R] Matching on multiple columns
In-Reply-To: <bd93cdad0701111256s57a3545n9ffe0a6a84101351@mail.gmail.com>
References: <bd93cdad0701111256s57a3545n9ffe0a6a84101351@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701111334170.11958@tajo.ucsd.edu>

On Thu, 11 Jan 2007, Farrel Buchinsky wrote:

> Am I correct in believing that one cannot match on multiple columns?
> One can indeed subset on multiple criteria from different variables
> (or columns) but not from unique combinations thereof.
> I need to exclude about 10000 rows from 108000 rows of data based on
> several unique combinations of identifiers in two columns. Only
> merge() seems to be able to do that. Merge would allow me to
> positively select but it would not allow me to deselect (or exclude).
> Look at how I got around the problem.
> It is inelegant. Have a missed a more direct function?
>


I guess that depends on what you regard as 'more direct'.

  	newdata[ !is.element( interaction(newdata), interaction(exclude) ) , ]


>
> x<-seq(from=1,to=10)
> y <-rep(1:2,5)
> data<-data.frame(cbind(x,y))
> x<-seq(from=1,to=10)
> y <-rep(1:5,2)
> data1<-data.frame(cbind(x,y))
> newdata<-rbind(data,data1)
> newdata[10,2] <- 3
> exclude<-newdata[18:20,]
> #This is a simulation of real life problem
> #We now have two dataframes. Newdata is the data set from an
> experiment. In real #life it has an additional 9 columns of data
> #exclude is a dataframe that was manually created after discovering
> some quality #control issues
> #Any row in newdata that matches any row in exclude must be discarded
> match(newdata$x,exclude$x)# useless because it is only on one column
> match(newdata$y,exclude$y)# useless because it is only on one column
> newdata$x %in%  exclude$x  # useless because it is only on one column
> newdata$y %in%  exclude$y # useless because it is only on one column
> newdata$x %in%  exclude$x &  newdata$y %in%  exclude$y  # useless,
> #eventhough it is using both columns, because it is not
> #using them in a synchronous manner. Row 10 in new data should not
> have been #marked "TRUE"
> #It was only labeled such because the 10 in the x column is indeed in
> the exclude x #column and the 3 in the y column is indeed in the
> exclude y column but not #together
> which(newdata$x %in%  exclude$x &  newdata$y %in%  exclude$y)#also
> gets it #wrong
> match(newdata,exclude)# intuitively this could have worked but alas
> match can #only handle vectors and not dataframes. It cannot match on
> multiple columns
> #I have to stoop to the inelegant maneuver of creating a combined
> variable of the #two columns, albeit only temporarily
> paste(newdata$x,newdata$y,sep=":") %in% paste(exclude$x,exclude$y,sep=":")
> #or one could do this
> match(paste(newdata$x,newdata$y,sep=":"),paste(exclude$x,exclude$y,sep=":"))
> #or
> {which(paste(newdata$x,newdata$y,sep=":") %in% paste
> (exclude$x,exclude$y,sep=":"))}
> #Or if you want to use the result in an index term or a selection
> argument in a subset command
> {paste(newdata$x,newdata$y,sep=":") %in%
> paste(exclude$x,exclude$y,sep=":")==FALSE}
> #As in
> {newdata[paste(newdata$x,newdata$y,sep=":") %in%
> paste(exclude$x,exclude$y,sep=":")==FALSE,]}
> #or
> {subset(newdata,paste(newdata$x,newdata$y,sep=":") %in%
> paste(exclude$x,exclude$y,sep=":")==FALSE)}
>
>
> -- 
> Farrel Buchinsky
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901


From topkatz at msn.com  Fri Jan 12 00:29:16 2007
From: topkatz at msn.com (Talbot Katz)
Date: Thu, 11 Jan 2007 18:29:16 -0500
Subject: [R] rank function and NA in 2.3.1
Message-ID: <BAY132-F1295E91DF01D096F170BC8AAB10@phx.gbl>

Hi.

I am using R 2.3.1 on WIndows XP, and I am having trouble with the rank 
function in the presence of numerical NA data.  I want the NA's all to get 
the same rank, but they don't.  Here is an example from my session:


>ct_align_rets_f2$liq[6851:6859]
[1] 115396     NA 362595     NA 242986 340805     NA 692905 251533
>rankl=rank(ct_align_rets_f2$liq,na.last=FALSE,ties.method="min")
>rankl[6851:6859]
[1] 4392 2424 5535 2425 5037 5451 2426 6625 5082


What am I doing wrong?  Is there a way to check whether there's a problem 
with the data, i.e., somehow the NA's have different values?  (By the way, I 
have tried not using na.last, and also different ties.methods, but the NA 
ranks have never come out equal.)

Thanks!


--  TMK  --
212-460-5430	home
917-656-5351	cell


From fjbuch at gmail.com  Fri Jan 12 02:12:26 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Thu, 11 Jan 2007 20:12:26 -0500
Subject: [R] mean on a table
Message-ID: <bd93cdad0701111712q3f274974t6731395b56a36262@mail.gmail.com>

Please suggest areas that I should troubleshoot. This command used to
give me an answer and now it gives me an error.
> mean(no.genot,na.rm=T)
Error in tapply(x, by, sum, na.rm = TRUE) :
        arguments must have same length
I tried removing the na.rm=T)
> mean(no.genot)
Error in `names<-.default`(`*tmp*`, value = c("by.1", "by.0")) :
        attempt to set an attribute on NULL

The structure of my table is as follows.
> str(no.genot)
 table [, 1:467] 0.000 0.261 0.315 0.274 0.349 ...
 - attr(*, "dimnames")=List of 1
  ..$ : chr [1:467] "NC" "RP138" "RP139" "RP140" ...

Strangely enough
> mean(no.genot[1:467])
[1] 0.2426167

That seems to work.

-- 
Farrel Buchinsky
Mobile: (412) 779-1073


From p.dalgaard at biostat.ku.dk  Fri Jan 12 02:35:01 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 12 Jan 2007 02:35:01 +0100
Subject: [R] mean on a table
In-Reply-To: <bd93cdad0701111712q3f274974t6731395b56a36262@mail.gmail.com>
References: <bd93cdad0701111712q3f274974t6731395b56a36262@mail.gmail.com>
Message-ID: <45A6E5C5.3070509@biostat.ku.dk>

Farrel Buchinsky wrote:
> Please suggest areas that I should troubleshoot. This command used to
> give me an answer and now it gives me an error.
>   
>> mean(no.genot,na.rm=T)
>>     
> Error in tapply(x, by, sum, na.rm = TRUE) :
>         arguments must have same length
> I tried removing the na.rm=T)
>   
>> mean(no.genot)
>>     
> Error in `names<-.default`(`*tmp*`, value = c("by.1", "by.0")) :
>         attempt to set an attribute on NULL
>
> The structure of my table is as follows.
>   
>> str(no.genot)
>>     
>  table [, 1:467] 0.000 0.261 0.315 0.274 0.349 ...
>  - attr(*, "dimnames")=List of 1
>   ..$ : chr [1:467] "NC" "RP138" "RP139" "RP140" ...
>
> Strangely enough
>   
>> mean(no.genot[1:467])
>>     
> [1] 0.2426167
>
> That seems to work.
>   
Have you perhaps a stray mean() or mean.table() function lying around?

Doesn't look like it would normally happen:

 > tb <- as.table(as.double(1:10))
 > str(tb)
 table [, 1:10] 1 2 3 4 5 6 7 8 9 10
 - attr(*, "dimnames")=List of 1
  ..$ : chr [1:10] "A" "B" "C" "D" ...
 > mean(tb)
[1] 5.5


From Inman.Brant at mayo.edu  Fri Jan 12 04:33:18 2007
From: Inman.Brant at mayo.edu (Inman, Brant A.   M.D.)
Date: Thu, 11 Jan 2007 21:33:18 -0600
Subject: [R] Making TIFF images with rtiff
Message-ID: <6021CA6EF4C8374084D4F5A141F1CBBB664A32@msgebe23.mfad.mfroot.org>

Many medical journals and publishers require that images, whether
photographs or line art, be submitted as high resolution .TIFF images.
One option for R users is to produce an image in one format and to
convert it to a .TIFF file using a second software program.  My
experience has been that this option often results in images of poorer
quality, often with blurry contours, and a loss of resolution.  A second
and better option would be to make .TIFF files directly from the graphic
output of R.  

I recently noticed that there is a library called "rtiff" that may be
able to do this.  However, I have not been able to get it to work,
principally because I do not know how to install the required supporting
software, libtiff and tiffio.h, correctly on my computer. I am running R
2.4.0 on a Windows XP machine.  So far I have done the following:

1) Loaded the rtiff library

2) Downloaded and installed the TIFF library 3.8.2 (complete package and
sources) from the following website:
http://gnuwin32.sourceforge.net/packages/tiff.htm

I would like to ask the R experts for help with the following things:

1) Where do I get the tiffio.h file?

2) Where do I install or relocate the tiffio.h and TIFF library to so
that rtiff will work?

Thanks for your help.

Brant Inman
Mayo Clinic


From Eric.Kort at vai.org  Fri Jan 12 04:54:49 2007
From: Eric.Kort at vai.org (Kort, Eric)
Date: Thu, 11 Jan 2007 22:54:49 -0500
Subject: [R] Making TIFF images with rtiff
References: <6021CA6EF4C8374084D4F5A141F1CBBB664A32@msgebe23.mfad.mfroot.org>
Message-ID: <CEA39A213F7F2E44A0DED9210BCD352F01B11603@VAIEXCH04.vai.org>


From: Inman, Brant A.   M.D. [mailto:Inman.Brant at mayo.edu]
 
>Many medical journals and publishers require that images, whether
>photographs or line art, be submitted as high resolution .TIFF images.
>One option for R users is to produce an image in one format and to
>convert it to a .TIFF file using a second software program.  My
>experience has been that this option often results in images of poorer
>quality, often with blurry contours, and a loss of resolution.  A second
>and better option would be to make .TIFF files directly from the graphic
>output of R.  
>
>I recently noticed that there is a library called "rtiff" that may be
>able to do this.  However, I have not been able to get it to work,
>principally because I do not know how to install the required supporting
>software, libtiff and tiffio.h, correctly on my computer. I am running R
>2.4.0 on a Windows XP machine.  So far I have done the following:

If only.  Sadly, rtiff only reads tiffs, and writes pixmap objects as tiffs. 
It does not create a rendering device for capturing plots to a tiff file, in 
the way that jpeg() does for jpegs.

This very thought occurred to me a couple months ago, but I was skeptical
that the effort would be worth it.  I usually render to postscript and 
convert from there with high quality results...but this is a bit tedious and
time consuming.  Now that there are two of us in the universe who would like 
a tiff() rendering device for R, maybe it would be worth it.  It would be 
especially worth it if there was someone else conversant in the non-tiff 
specific aspects of the task.  If that would be the case I would be happy to 
code the TIFF I/O.  Lacking that, I will need to look at the requirements 
some more to verify whether I have the requisite neurons to carry it off.

To answer your installation question, all that is required for the windows 
binary to run is to copy the tiff related DLLs provided in the gnuwin binary 
package somewhere on your PATH (tiffio.h and other development files are 
not required unless you are going to build the package from source).

-Eric

(apologies for the legal notice that may appear here...beyond my control!)


This email message, including any attachments, is for the so...{{dropped}}


From fjbuch at gmail.com  Fri Jan 12 05:11:00 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Thu, 11 Jan 2007 23:11:00 -0500
Subject: [R] mean on a table
In-Reply-To: <45A6E5C5.3070509@biostat.ku.dk>
References: <bd93cdad0701111712q3f274974t6731395b56a36262@mail.gmail.com>
	<45A6E5C5.3070509@biostat.ku.dk>
Message-ID: <bd93cdad0701112011g64df9b1dj6f92c3e7ed47543a@mail.gmail.com>

Not that I know of.
When I get back to the office I will check it out.
I certainly do not recall having created one.
I wonder if a library I am working with maybe created a mean function.
How do I figure out if I have a loose cannon mean function on the run
in my system?

On 1/11/07, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Farrel Buchinsky wrote:
> > Please suggest areas that I should troubleshoot. This command used to
> > give me an answer and now it gives me an error.
> >
> >> mean(no.genot,na.rm=T)
> >>
> > Error in tapply(x, by, sum, na.rm = TRUE) :
> >         arguments must have same length
> > I tried removing the na.rm=T)
> >
> >> mean(no.genot)
> >>
> > Error in `names<-.default`(`*tmp*`, value = c("by.1", "by.0")) :
> >         attempt to set an attribute on NULL
> >
> > The structure of my table is as follows.
> >
> >> str(no.genot)
> >>
> >  table [, 1:467] 0.000 0.261 0.315 0.274 0.349 ...
> >  - attr(*, "dimnames")=List of 1
> >   ..$ : chr [1:467] "NC" "RP138" "RP139" "RP140" ...
> >
> > Strangely enough
> >
> >> mean(no.genot[1:467])
> >>
> > [1] 0.2426167
> >
> > That seems to work.
> >
> Have you perhaps a stray mean() or mean.table() function lying around?
>
> Doesn't look like it would normally happen:
>
>  > tb <- as.table(as.double(1:10))
>  > str(tb)
>  table [, 1:10] 1 2 3 4 5 6 7 8 9 10
>  - attr(*, "dimnames")=List of 1
>  ..$ : chr [1:10] "A" "B" "C" "D" ...
>  > mean(tb)
> [1] 5.5
>
>
>
>


-- 
Farrel Buchinsky
Mobile: (412) 779-1073


From weigand.stephen at charter.net  Fri Jan 12 05:25:04 2007
From: weigand.stephen at charter.net (Stephen D. Weigand)
Date: Thu, 11 Jan 2007 22:25:04 -0600
Subject: [R] Making TIFF images with rtiff
In-Reply-To: <6021CA6EF4C8374084D4F5A141F1CBBB664A32@msgebe23.mfad.mfroot.org>
References: <6021CA6EF4C8374084D4F5A141F1CBBB664A32@msgebe23.mfad.mfroot.org>
Message-ID: <de54ad333367fcc4da1f68742b267ee1@charter.net>

Brant,

On Jan 11, 2007, at 9:33 PM, Inman, Brant A. M.D. wrote:

> Many medical journals and publishers require that images, whether
> photographs or line art, be submitted as high resolution .TIFF images.
> One option for R users is to produce an image in one format and to
> convert it to a .TIFF file using a second software program.  My
> experience has been that this option often results in images of poorer
> quality, often with blurry contours, and a loss of resolution.  A 
> second
> and better option would be to make .TIFF files directly from the 
> graphic
> output of R.
>

[...]

Have you tried bitmap()?. You might need to install ghostscript
before it works on your system but the results are super.

I use

bitmap(blah, type = "png256", res = 1200)

to make *.png files which are small and can be inserted in to an
MS Word manuscript for review.

When a TIFF is needed by the journal,

bitmap(blah, type = "tifflzw", res = 1200)

will do the trick. (If a color plot is being created, you can use
something like type = "tiff24nc". The color files are huge but
can be zipped to something reasonable.)

Hope this helps,

Stephen
Rochester, Minnesota, USA


From amitsoni.84 at gmail.com  Fri Jan 12 05:53:22 2007
From: amitsoni.84 at gmail.com (Amit Soni)
Date: Thu, 11 Jan 2007 20:53:22 -0800
Subject: [R] Linear Optimization
Message-ID: <9fb977ed0701112053m6906b5a6g2c299f1142108a9f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070111/dcf5dfd2/attachment.pl 

From f.harrell at vanderbilt.edu  Fri Jan 12 06:02:00 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 11 Jan 2007 23:02:00 -0600
Subject: [R] Making TIFF images with rtiff
In-Reply-To: <CEA39A213F7F2E44A0DED9210BCD352F01B11603@VAIEXCH04.vai.org>
References: <6021CA6EF4C8374084D4F5A141F1CBBB664A32@msgebe23.mfad.mfroot.org>
	<CEA39A213F7F2E44A0DED9210BCD352F01B11603@VAIEXCH04.vai.org>
Message-ID: <45A71648.2030400@vanderbilt.edu>

Kort, Eric wrote:
> From: Inman, Brant A.   M.D. [mailto:Inman.Brant at mayo.edu]
>  
>> Many medical journals and publishers require that images, whether
>> photographs or line art, be submitted as high resolution .TIFF images.
>> One option for R users is to produce an image in one format and to
>> convert it to a .TIFF file using a second software program.  My
>> experience has been that this option often results in images of poorer
>> quality, often with blurry contours, and a loss of resolution.  A second
>> and better option would be to make .TIFF files directly from the graphic
>> output of R.  
>>
>> I recently noticed that there is a library called "rtiff" that may be
>> able to do this.  However, I have not been able to get it to work,
>> principally because I do not know how to install the required supporting
>> software, libtiff and tiffio.h, correctly on my computer. I am running R
>> 2.4.0 on a Windows XP machine.  So far I have done the following:
> 
> If only.  Sadly, rtiff only reads tiffs, and writes pixmap objects as tiffs. 
> It does not create a rendering device for capturing plots to a tiff file, in 
> the way that jpeg() does for jpegs.
> 
> This very thought occurred to me a couple months ago, but I was skeptical
> that the effort would be worth it.  I usually render to postscript and 
> convert from there with high quality results...but this is a bit tedious and
> time consuming.  Now that there are two of us in the universe who would like 
> a tiff() rendering device for R, maybe it would be worth it.  It would be 
> especially worth it if there was someone else conversant in the non-tiff 
> specific aspects of the task.  If that would be the case I would be happy to 
> code the TIFF I/O.  Lacking that, I will need to look at the requirements 
> some more to verify whether I have the requisite neurons to carry it off.
> 
> To answer your installation question, all that is required for the windows 
> binary to run is to copy the tiff related DLLs provided in the gnuwin binary 
> package somewhere on your PATH (tiffio.h and other development files are 
> not required unless you are going to build the package from source).
> 
> -Eric

Some journals are not really serious about the TIFF rule and will accept 
postscript or pdf anyway.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From petr.pikal at precheza.cz  Fri Jan 12 07:49:12 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 12 Jan 2007 07:49:12 +0100
Subject: [R] mean on a table
In-Reply-To: <bd93cdad0701112011g64df9b1dj6f92c3e7ed47543a@mail.gmail.com>
References: <45A6E5C5.3070509@biostat.ku.dk>
Message-ID: <45A73D78.15917.17278D@localhost>

Hi

On 11 Jan 2007 at 23:11, Farrel Buchinsky wrote:

Date sent:      	Thu, 11 Jan 2007 23:11:00 -0500
From:           	"Farrel Buchinsky" <fjbuch at gmail.com>
To:             	"Peter Dalgaard" <p.dalgaard at biostat.ku.dk>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] mean on a table

> Not that I know of.
> When I get back to the office I will check it out.
> I certainly do not recall having created one.
> I wonder if a library I am working with maybe created a mean function.
> How do I figure out if I have a loose cannon mean function on the run
> in my system?

easiest way would by to type
> mean
function (x, ...) 
UseMethod("mean")
<environment: namespace:base>

or

> mean.default
function (x, trim = 0, na.rm = FALSE, ...) 
{
    if (!is.numeric(x) && !is.complex(x) && !is.logical(x)) {
        warning("argument is not numeric or logical: returning NA")
        return(as.numeric(NA))
    }
    if (na.rm) 
        x <- x[!is.na(x)]
    trim <- trim[1]
    n <- length(x)
    if (trim > 0 && n > 0) {
        if (is.complex(x)) 
            stop("trimmed means are not defined for complex data")
        if (trim >= 0.5) 
            return(stats::median(x, na.rm = FALSE))
        lo <- floor(n * trim) + 1
        hi <- n + 1 - lo
        x <- sort.int(x, partial = unique(c(lo, hi)))[lo:hi]
        n <- hi - lo + 1
    }
    .Internal(mean(x))
}
<environment: namespace:base>

and this shall be the result.

HTH
Petr


> 
> On 1/11/07, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> > Farrel Buchinsky wrote:
> > > Please suggest areas that I should troubleshoot. This command used
> > > to give me an answer and now it gives me an error.
> > >
> > >> mean(no.genot,na.rm=T)
> > >>
> > > Error in tapply(x, by, sum, na.rm = TRUE) :
> > >         arguments must have same length
> > > I tried removing the na.rm=T)
> > >
> > >> mean(no.genot)
> > >>
> > > Error in `names<-.default`(`*tmp*`, value = c("by.1", "by.0")) :
> > >         attempt to set an attribute on NULL
> > >
> > > The structure of my table is as follows.
> > >
> > >> str(no.genot)
> > >>
> > >  table [, 1:467] 0.000 0.261 0.315 0.274 0.349 ...
> > >  - attr(*, "dimnames")=List of 1
> > >   ..$ : chr [1:467] "NC" "RP138" "RP139" "RP140" ...
> > >
> > > Strangely enough
> > >
> > >> mean(no.genot[1:467])
> > >>
> > > [1] 0.2426167
> > >
> > > That seems to work.
> > >
> > Have you perhaps a stray mean() or mean.table() function lying
> > around?
> >
> > Doesn't look like it would normally happen:
> >
> >  > tb <- as.table(as.double(1:10))
> >  > str(tb)
> >  table [, 1:10] 1 2 3 4 5 6 7 8 9 10
> >  - attr(*, "dimnames")=List of 1
> >  ..$ : chr [1:10] "A" "B" "C" "D" ...
> >  > mean(tb)
> > [1] 5.5
> >
> >
> >
> >
> 
> 
> -- 
> Farrel Buchinsky
> Mobile: (412) 779-1073
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Fri Jan 12 08:00:23 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 12 Jan 2007 08:00:23 +0100
Subject: [R] rank function and NA in 2.3.1
In-Reply-To: <BAY132-F1295E91DF01D096F170BC8AAB10@phx.gbl>
Message-ID: <45A74017.25882.2164A5@localhost>

Hi

one workaround could be to change your NA values to some number and 
do the rank with average, min or max option.

> x<-1:12
> x[c(5,10)]<-NA
> rank(x)
 [1]  1  2  3  4 11  5  6  7  8 12  9 10
> rank(x, ties="average")
 [1]  1  2  3  4 11  5  6  7  8 12  9 10
> x[which(is.na(x))]<-999
> rank(x, ties="average")
 [1]  1.0  2.0  3.0  4.0 11.5  5.0  6.0  7.0  8.0 11.5  9.0 10.0

Or you can go through rank source code and to change it so as it will 
behave as you wish.

HTH
Petr


On 11 Jan 2007 at 18:29, Talbot Katz wrote:

From:           	"Talbot Katz" <topkatz at msn.com>
To:             	r-help at stat.math.ethz.ch
Date sent:      	Thu, 11 Jan 2007 18:29:16 -0500
Subject:        	[R] rank function and NA in 2.3.1

> Hi.
> 
> I am using R 2.3.1 on WIndows XP, and I am having trouble with the
> rank function in the presence of numerical NA data.  I want the NA's
> all to get the same rank, but they don't.  Here is an example from my
> session:
> 
> 
> >ct_align_rets_f2$liq[6851:6859]
> [1] 115396     NA 362595     NA 242986 340805     NA 692905 251533
> >rankl=rank(ct_align_rets_f2$liq,na.last=FALSE,ties.method="min")
> >rankl[6851:6859] [1] 4392 2424 5535 2425 5037 5451 2426 6625 5082
> 
> 
> What am I doing wrong?  Is there a way to check whether there's a
> problem with the data, i.e., somehow the NA's have different values? 
> (By the way, I have tried not using na.last, and also different
> ties.methods, but the NA ranks have never come out equal.)
> 
> Thanks!
> 
> 
> --  TMK  --
> 212-460-5430	home
> 917-656-5351	cell
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From ripley at stats.ox.ac.uk  Fri Jan 12 09:07:39 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 12 Jan 2007 08:07:39 +0000 (GMT)
Subject: [R] Making TIFF images with rtiff
In-Reply-To: <6021CA6EF4C8374084D4F5A141F1CBBB664A32@msgebe23.mfad.mfroot.org>
References: <6021CA6EF4C8374084D4F5A141F1CBBB664A32@msgebe23.mfad.mfroot.org>
Message-ID: <Pine.LNX.4.64.0701120728040.25092@gannet.stats.ox.ac.uk>

Let us be clear that TIFF is not a format, it is a collection of formats 
and for example my camera's RAW images are TIFFs, as are Adobe's DNG 
files.  (See e.g. http://en.wikipedia.org/wiki/TIFF.) So the journals 
concerned are misusing the term.  It is very easy to convert either JPEGs 
or PNGs to a TIFF subformat with ImageMagick, PhotoShop or other tools, 
and the bitmap() driver can write several versions of TIFF by converting 
from postscript (transparently to the user).

I think the complaints in this thread are a bit rich given how much R 
aleady provides.  We thought about adding a few TIFF formats to PNG, JPEG 
and BMP but agreed it was not worth effort (especially given the vagueness 
with which 'TIFF' is used and stories about journals rejecting perfectly 
valid TIFF files). [I don't recall anyone ever writing to thank us for the 
PNG or JPEG or bitmap drivers, and lack of appreciation does play a part.]

An 8-bit TIFF device is not going to produce better images than the PNG 
one, just larger files.  (It could if you rendered HDR TIFF images, but 
the journal will not be able to cope with those.)

On Thu, 11 Jan 2007, Inman, Brant A.   M.D. wrote:

> Many medical journals and publishers require that images, whether
> photographs or line art, be submitted as high resolution .TIFF images.
> One option for R users is to produce an image in one format and to
> convert it to a .TIFF file using a second software program.  My
> experience has been that this option often results in images of poorer
> quality, often with blurry contours, and a loss of resolution.  A second
> and better option would be to make .TIFF files directly from the graphic
> output of R.

As TIFF is collection of bitmap formats, the blurriness is intrinisic to 
the format.  You are not telling us what you converted *from* (nor what 
sort of images these are nor what convertor you used): for most R 
applications PNG is the right image format to convert from and conversion 
from PNG to TIFF will be lossless.  (PS/PDF are not image formats, but 
also good starting points for conversion.)

> I recently noticed that there is a library called "rtiff" that may be
> able to do this.  However, I have not been able to get it to work,
> principally because I do not know how to install the required supporting
> software, libtiff and tiffio.h, correctly on my computer. I am running R
> 2.4.0 on a Windows XP machine.  So far I have done the following:
>
> 1) Loaded the rtiff library

Do you mean 'package'?  There is a precompiled Windows build: did you use 
that?  What it needs is libtiff3.dll, which is in the software you 
downloaded: put it in R_HOME/rtiff/libs or somewhere on your PATH.
(Sadly, the package is lacking adequate instructions.)

> 2) Downloaded and installed the TIFF library 3.8.2 (complete package and
> sources) from the following website:
> http://gnuwin32.sourceforge.net/packages/tiff.htm
>
> I would like to ask the R experts for help with the following things:
>
> 1) Where do I get the tiffio.h file?

It is part of the libtiff software you downloaded, but it says it is only 
needed to install from the sources.

> 2) Where do I install or relocate the tiffio.h and TIFF library to so
> that rtiff will work?
>
> Thanks for your help.
>
> Brant Inman
> Mayo Clinic
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jgengler at umich.edu  Fri Jan 12 00:59:03 2007
From: jgengler at umich.edu (Justin Gengler)
Date: Thu, 11 Jan 2007 18:59:03 -0500
Subject: [R] a way to control xlim in multhist?
Message-ID: <200701112359.l0BNxBgD006172@hypatia.math.ethz.ch>

Thanks in advance for any suggestions:

I am using the 'multhist' function in the 'plotrix' package to 
display histograms of some variable 'x' given some value of another 
variable 'z' -- for example, separate histograms for some variable 
according to sex (or another dichotomous) variable.  Thus I am using 
something like the following:

multhist(list(x[z==1],x[z==0]),...).

This generates histograms well enough, but because one cannot control 
the 'xlim' parameter the x-axis labels are very odd (or rather, 
strictly speaking, you CAN control it, but doing so does not change 
the x-axis value labels in the same way that it would if you were 
using the simple 'hist' command).  More specifically, the labels 
almost always seem to progress in increments of 5 or 0.5, depending 
on the scale (see, for example, this graph: 
http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=82). 
Moreover, if x is, say, a categorical variable with possible values 
of 1 through 5, the x-axis labels will remain in the base-5 form 
(e.g., 0.5, 1.5, 2.5, etc.) rather than taking on the various 
categorical values, regardless of any xlim specification.

I realize that the issue here is that multhist relies on the 
'barplot' function, so I have attempted to hack the xlim change 
there.  But no dice.

So the bottom line is: is there a way to control x-axis scale and 
labeling through xliim when using multhist?

Thanks,

Justin Gengler

Also: for a concrete demonstration of what I mean, compare the following:

#1

mh <- list(rnorm(200, mean=100, sd=10), rnorm(200, mean=100, sd=10))
multhist(mh)

#2

multhist(mh,xlim=c(0,20))


From p.dalgaard at biostat.ku.dk  Fri Jan 12 09:43:26 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 12 Jan 2007 09:43:26 +0100
Subject: [R] mean on a table
In-Reply-To: <45A73D78.15917.17278D@localhost>
References: <45A6E5C5.3070509@biostat.ku.dk> <45A73D78.15917.17278D@localhost>
Message-ID: <45A74A2E.2040401@biostat.ku.dk>

Petr Pikal wrote:
> Hi
>
> On 11 Jan 2007 at 23:11, Farrel Buchinsky wrote:
>   
>> Not that I know of.
>> When I get back to the office I will check it out.
>> I certainly do not recall having created one.
>> I wonder if a library I am working with maybe created a mean function.
>> How do I figure out if I have a loose cannon mean function on the run
>> in my system?
>>     
> easiest way would by to type
>   
>> mean
>>     
> or
>   
>> mean.default
>>     
Also, methods(mean) (a rogue mean.table wouldn't be caught by the above) 
and do a traceback() at the fault to see what is actually being called.


From antoniababe at yahoo.se  Wed Jan 10 22:07:56 2007
From: antoniababe at yahoo.se (antoniababe at yahoo.se)
Date: Wed, 10 Jan 2007 22:07:56 +0100 (CET)
Subject: [R] labels outliers in boxplot
Message-ID: <20070110210756.74442.qmail@web26008.mail.ukl.yahoo.com>

Dear R-users,

Following is part of my data, where slide has 36
levels and block 48 levels. I have done boxplot for
each slide on the same graph. There are outliers for
each slide and I tried to use indentify functtion to
identify outliers in such a way that when I click on
an outlier or point, the points will be labelled by
either their block or ID or by both but without
success. How can I make it work or are there other
ways to do it than using identify function?


Thanks in advance, 

Jenny,

dat1[1:10,]
             y Slide Block              ID Control
1   0.03147823     1     1       IgG-human       5
2  -0.23815974     1     1 LPPAANDVSVLTAAR       0
3  -0.71926359     1     1 HTKHYRVVSKPAALV       0
4  -0.14607826     1     1 FVALPAATADAYATT       0
5   0.89553073     1     1 NYPAMMAHAGDMAGY       0
6  -0.67587100     1     1 RRALRQIGVLERPVG       0
7   0.32636034     1     1 DCGTIRVGSFRGRWL       0
8  -1.44057259     1     1 MAKLSTDELLDAFKE       0
9  -0.37064338     1     1 LELSDFVKKFEETFE       0
10 -0.20387233     1     1 VSRRAKVDVLIVHTT       0


 tb_ncs<-subset(dat1,dat1$Control==1)     ### this
data contains only negative controls

       par(las=2,mar=c(10.1,4.1,4.1,2.1)) 
          
boxplot(split(tb_ncs$y,tb_ncs$Slide),col="orange",
cex=.65,
                   outline=TRUE,main="Negative control
response of each patient", cex.main=1, font.main=1,
                            col.main="blue",
names=c(1:35,"B"))
                   grid(nx=NA, ny=NULL)               
          ### grid over boxplot 
           legend("bottomright", "B = Buffer +
sec",text.col="blue")
               

            out.block<-
identify(tb_ncs$y,tb_ncs$Slide) 



	
	
		
_________________________________________________________
Flyger tiden iv?g? F?nga dagen med Yahoo! Mails inbyggda


From backer at psych.uib.no  Fri Jan 12 10:39:23 2007
From: backer at psych.uib.no (Tom Backer Johnsen)
Date: Fri, 12 Jan 2007 10:39:23 +0100
Subject: [R] Regression lines
Message-ID: <45A7574B.6050001@psych.uib.no>

My simpleminded understanding of simple regression is that when 
plotting regression lines for x on y and y on x in the same plot, the 
lines should cross each other at the respective means.  But, given the 
R function below, abline (lm(y~x)) works fine, but abline (lm(x~y)) 
does not.  Why?

function () {
attach (attitude)
x <- rating
y <- learning
detach (attitude)
plot (x, y)
abline(v=mean(x))
abline(h=mean(y))
abline (lm(y~x))
abline (lm(x~y))
}


From knoblauch at lyon.inserm.fr  Fri Jan 12 10:47:40 2007
From: knoblauch at lyon.inserm.fr (ken knoblauch)
Date: Fri, 12 Jan 2007 10:47:40 +0100
Subject: [R]  Regression lines
Message-ID: <e180aecf97cef7fc13530ac1b24a0621@lyon.inserm.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070112/daa907ab/attachment.pl 

From Roger.Bivand at nhh.no  Fri Jan 12 10:59:06 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 12 Jan 2007 10:59:06 +0100 (CET)
Subject: [R] Regression lines
In-Reply-To: <45A7574B.6050001@psych.uib.no>
Message-ID: <Pine.LNX.4.44.0701121054400.22070-100000@reclus.nhh.no>

On Fri, 12 Jan 2007, Tom Backer Johnsen wrote:

> My simpleminded understanding of simple regression is that when 
> plotting regression lines for x on y and y on x in the same plot, the 
> lines should cross each other at the respective means.  But, given the 
> R function below, abline (lm(y~x)) works fine, but abline (lm(x~y)) 
> does not.  Why?
> 
> function () {
> attach (attitude)
> x <- rating
> y <- learning
> detach (attitude)
> plot (x, y)
> abline(v=mean(x))
> abline(h=mean(y))
> abline (lm(y~x))
> abline (lm(x~y))
> }

The axes are getting reversed:

xylm <- lm(x~y)
newdata <- data.frame(y=0:80)
lines(predict(xylm, newdata), newdata$y, col="blue")

gets them back.

Roger


> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From ripley at stats.ox.ac.uk  Fri Jan 12 11:01:47 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 12 Jan 2007 10:01:47 +0000 (GMT)
Subject: [R] Regression lines
In-Reply-To: <45A7574B.6050001@psych.uib.no>
References: <45A7574B.6050001@psych.uib.no>
Message-ID: <Pine.LNX.4.64.0701120955170.4875@gannet.stats.ox.ac.uk>

On Fri, 12 Jan 2007, Tom Backer Johnsen wrote:

> My simpleminded understanding of simple regression is that when
> plotting regression lines for x on y and y on x in the same plot, the
> lines should cross each other at the respective means.  But, given the
> R function below, abline (lm(y~x)) works fine, but abline (lm(x~y))
> does not.  Why?

Where did you tell it 'x' was the abscissa and 'y' the ordinate?
(Nowhere: R is lacking a mind_read() function!)  From the help page:

   reg is a regression object with a coef method. If this returns a vector
   of   length 1 then the value is taken to be the slope of a line
   through the origin, otherwise, the first 2 values are taken to be
   the intercept and slope.

There are some changes in R-devel, but not to recognize names of 
coefficients.


> function () {
> attach (attitude)
> x <- rating
> y <- learning
> detach (attitude)
> plot (x, y)
> abline(v=mean(x))
> abline(h=mean(y))
> abline (lm(y~x))
> abline (lm(x~y))
> }
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ligges at statistik.uni-dortmund.de  Fri Jan 12 11:10:21 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 12 Jan 2007 11:10:21 +0100
Subject: [R] Regression lines
In-Reply-To: <45A7574B.6050001@psych.uib.no>
References: <45A7574B.6050001@psych.uib.no>
Message-ID: <45A75E8D.8040507@statistik.uni-dortmund.de>



Tom Backer Johnsen wrote:
> My simpleminded understanding of simple regression is that when 
> plotting regression lines for x on y and y on x in the same plot, the 
> lines should cross each other at the respective means.  But, given the 
> R function below, abline (lm(y~x)) works fine, but abline (lm(x~y)) 
> does not.  Why?


Well, abline() in fact plots a line using the estimated coefficients for 
intercept and slope and assumes you have plotted LeftHandSideOfFormula 
against RightHandSideOfFormula.
If you did vice versa, abline() does not respect your mistake.

Uwe Ligges




> function () {
> attach (attitude)
> x <- rating
> y <- learning
> detach (attitude)
> plot (x, y)
> abline(v=mean(x))
> abline(h=mean(y))
> abline (lm(y~x))
> abline (lm(x~y))
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r.hankin at noc.soton.ac.uk  Fri Jan 12 11:12:35 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 12 Jan 2007 10:12:35 +0000
Subject: [R] image() and nonsquare matrices
Message-ID: <07CDD833-BB31-495F-A147-9E94894E8306@soc.soton.ac.uk>

How do I draw non-square matrices with image() and get the axes right?


Try 1:

  a <- matrix(rnorm(100),20,5)
image(1:20,1:5,a,asp=1,xlab="label here")
# No good because the axes don't touch the image



Try 2:

image(1:20,1:5,a,asp=1,axes=F,xlab="label here")
axis(side=1,pos=0)
# No good because the x axis label is floating far from the x axis.



Try 3:
  image(1:20,1:5,a,asp=1,axes=F,xlab="",ylab="")
  axis(side=1,pos=0)
# No good because the x axis label is absent.


How to use image() with a non-square matrix and make axes and labels  
appear correctly?





--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From scruveil at genoscope.cns.fr  Fri Jan 12 11:15:03 2007
From: scruveil at genoscope.cns.fr (Stephane Cruveiller)
Date: Fri, 12 Jan 2007 11:15:03 +0100
Subject: [R] .C interface and Strings...
Message-ID: <45A75FA7.6080800@genoscope.cns.fr>

Dear R users,

I am trying to include C code into R via the .C interface.  I have read
that arguments passed to a C function have to be correctly DEreferenced.
This is something that can be easily done for numbers (integers or 
float) by adding
a * before the reference like for instance:

------------------------------------------------------------------------
 #include<R.h>


void hellofct(int *n)
    {
        int i;
        for (i=0;i<*n;i++)
            {
                Rprintf("Hello, world!\n");
            }
    }
------------------------------------------------------------------------  

However, I can not figure out how that can be achieved for strings.
My prototype function would be something like:

------------------------------------------------------------------------
 #include<R.h>


void displaystring(char *str)
    {
        Rprintf("String displayed:%s\n", ????);
    }
------------------------------------------------------------------------  

any hints?

Thx.

St?phane.

-- 
==========================================================
Stephane CRUVEILLER Ph. D.
Genoscope - Centre National de Sequencage
Atelier de Genomique Comparative
2, Rue Gaston Cremieux   CP 5706
91057 Evry Cedex - France
Phone: +33 (0)1 60 87 84 58
Fax: +33 (0)1 60 87 25 14
EMails: scruveil at genoscope.cns.fr ,scruvell at infobiogen.fr
===========================================================


From P.Dalgaard at biostat.ku.dk  Fri Jan 12 11:21:58 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 12 Jan 2007 11:21:58 +0100
Subject: [R] Regression lines
In-Reply-To: <Pine.LNX.4.64.0701120955170.4875@gannet.stats.ox.ac.uk>
References: <45A7574B.6050001@psych.uib.no>
	<Pine.LNX.4.64.0701120955170.4875@gannet.stats.ox.ac.uk>
Message-ID: <45A76146.7040307@biostat.ku.dk>

Prof Brian Ripley wrote:
>
> Where did you tell it 'x' was the abscissa and 'y' the ordinate?
> (Nowhere: R is lacking a mind_read() function!)  
Please stop complaining about missing features. Patches will be considered.

Oh, it's you, Brian. Never mind then. You'll get to it, I'm sure.

;-)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From knoblauch at lyon.inserm.fr  Fri Jan 12 11:31:48 2007
From: knoblauch at lyon.inserm.fr (ken knoblauch)
Date: Fri, 12 Jan 2007 11:31:48 +0100
Subject: [R]  Regression lines
Message-ID: <c1cf9e2f4af9264c385a81f6eecbc88c@lyon.inserm.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070112/6a79de6f/attachment.pl 

From b.rowlingson at lancaster.ac.uk  Fri Jan 12 11:52:41 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 12 Jan 2007 10:52:41 +0000
Subject: [R] image() and nonsquare matrices
In-Reply-To: <07CDD833-BB31-495F-A147-9E94894E8306@soc.soton.ac.uk>
References: <07CDD833-BB31-495F-A147-9E94894E8306@soc.soton.ac.uk>
Message-ID: <45A76879.8060509@lancaster.ac.uk>

Robin Hankin wrote:
> How do I draw non-square matrices with image() and get the axes right?

> Try 2:
> 
> image(1:20,1:5,a,asp=1,axes=F,xlab="label here")
> axis(side=1,pos=0)
> # No good because the x axis label is floating far from the x axis.

  Its only no good if your plot device isnt a similar aspect ratio to 
your plot... Resize your graphics window and your label will float around...

  Barry


From b.rowlingson at lancaster.ac.uk  Fri Jan 12 11:56:16 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 12 Jan 2007 10:56:16 +0000
Subject: [R] Regression lines
In-Reply-To: <c1cf9e2f4af9264c385a81f6eecbc88c@lyon.inserm.fr>
References: <c1cf9e2f4af9264c385a81f6eecbc88c@lyon.inserm.fr>
Message-ID: <45A76950.60604@lancaster.ac.uk>

ken knoblauch wrote:
> This should do the trick:
> 
> mind_reader <- function() {
> 	ll <- letters[round(runif(6, 1, 26))]

  I see my paraNormal distribution package hasn't found its way to CRAN yet:

http://tolstoy.newcastle.edu.au/R/help/05/04/1701.html


Barry


From backer at psych.uib.no  Fri Jan 12 12:31:36 2007
From: backer at psych.uib.no (Tom Backer Johnsen)
Date: Fri, 12 Jan 2007 12:31:36 +0100
Subject: [R] Regression lines
In-Reply-To: <45A76950.60604@lancaster.ac.uk>
References: <c1cf9e2f4af9264c385a81f6eecbc88c@lyon.inserm.fr>
	<45A76950.60604@lancaster.ac.uk>
Message-ID: <45A77198.108@psych.uib.no>

Barry Rowlingson wrote:
> ken knoblauch wrote:
>> This should do the trick:
>>
>> mind_reader <- function() {
>> 	ll <- letters[round(runif(6, 1, 26))]
> 
>   I see my paraNormal distribution package hasn't found its way to CRAN yet:
> 
> http://tolstoy.newcastle.edu.au/R/help/05/04/1701.html

LOL!  Nice!

Tom


From murdoch at stats.uwo.ca  Fri Jan 12 12:35:34 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 12 Jan 2007 06:35:34 -0500
Subject: [R] Regression lines
In-Reply-To: <45A76950.60604@lancaster.ac.uk>
References: <c1cf9e2f4af9264c385a81f6eecbc88c@lyon.inserm.fr>
	<45A76950.60604@lancaster.ac.uk>
Message-ID: <45A77286.8010300@stats.uwo.ca>

On 1/12/2007 5:56 AM, Barry Rowlingson wrote:
> ken knoblauch wrote:
>> This should do the trick:
>>
>> mind_reader <- function() {
>> 	ll <- letters[round(runif(6, 1, 26))]
> 
>   I see my paraNormal distribution package hasn't found its way to CRAN yet:
> 
> http://tolstoy.newcastle.edu.au/R/help/05/04/1701.html

It probably hasn't passed their checks.  The CRAN folks never expect 
packages to pass.

Duncan Murdoch


From sundar.dorai-raj at pdf.com  Fri Jan 12 13:32:27 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 12 Jan 2007 06:32:27 -0600
Subject: [R] .C interface and Strings...
In-Reply-To: <45A75FA7.6080800@genoscope.cns.fr>
References: <45A75FA7.6080800@genoscope.cns.fr>
Message-ID: <45A77FDB.8090406@pdf.com>



Stephane Cruveiller said the following on 1/12/2007 4:15 AM:
> Dear R users,
> 
> I am trying to include C code into R via the .C interface.  I have read
> that arguments passed to a C function have to be correctly DEreferenced.
> This is something that can be easily done for numbers (integers or 
> float) by adding
> a * before the reference like for instance:
> 
> ------------------------------------------------------------------------
> #include<R.h>
> 
> 
> void hellofct(int *n)
>    {
>        int i;
>        for (i=0;i<*n;i++)
>            {
>                Rprintf("Hello, world!\n");
>            }
>    }
> ------------------------------------------------------------------------ 
> However, I can not figure out how that can be achieved for strings.
> My prototype function would be something like:
> 
> ------------------------------------------------------------------------
> #include<R.h>
> 
> 
> void displaystring(char *str)
>    {
>        Rprintf("String displayed:%s\n", ????);
>    }
> ------------------------------------------------------------------------ 
> any hints?
> 

Yes, Section 5.2 in "Writing R Extensions".

<untested>

displaystring.c:
#include<R.h>
void displaystring(char **str, int *n)
{
   int i;
   for(i = 0; i < *n; i++)
     Rprintf("String displayed:%s\n", str[i]);
}

displaystring.dll:
R CMD SHLIB displaystring.c

displaystring.R:
dyn.load("displaystring")
displaystring <- function(x) {
   .C("displaystring", x, length(x))
   invisible()
}
displaystring(c("A", "B"))

</untested>

HTH,

--sundar


From guillet at stat.ucl.ac.be  Fri Jan 12 14:21:16 2007
From: guillet at stat.ucl.ac.be (Alain Guillet)
Date: Fri, 12 Jan 2007 14:21:16 +0100
Subject: [R] Maximum likelihood acf
Message-ID: <45A78B4C.1020702@stat.ucl.ac.be>

Hello!

I am looking for a function which computes the maximum likelihood 
estimator of the autocorrelation function for a gaussian time series. 
Does a such function already exist in R?
The estimator by default in R, acf(), uses the method of moments.

Thanks a lot,
Alain


-- 
Alain Guillet
Statistician and Computer Scientist

Institut de statistique - Universit? catholique de Louvain
Bureau d.126
Voie du Roman Pays, 20
B-1348 Louvain-la-Neuve
Belgium

tel: +32 10 47 30 50


From antoniababe at yahoo.se  Fri Jan 12 14:48:15 2007
From: antoniababe at yahoo.se (antoniababe at yahoo.se)
Date: Fri, 12 Jan 2007 14:48:15 +0100 (CET)
Subject: [R] labels outliers in boxplot
In-Reply-To: <3948d9e50701120150t29ea9f13peadada331f3e5d09@mail.gmail.com>
Message-ID: <20070112134815.467.qmail@web26006.mail.ukl.yahoo.com>

Dear talepande,

Thanks for your suggestion,  
I have already tried to use it, but the identify
function gave me only  the observation number
everytime I clicked on any point.What I want is
instead of obervation numbers it would be block and/or
slide numbers.

Any other idea how I can make it works ?

Thanks 
--- talepanda <talepanda at gmail.com> skrev:

> because given data is a part of your data, I cannot
> examine,
> however, try:
> 
> ##out.block<-identify(tb_ncs$y,tb_ncs$Slide)
> out.block<-identify(tb_ncs$Slide,tb_ncs$y)
> 
> 
> On 1/11/07, antoniababe at yahoo.se
> <antoniababe at yahoo.se> wrote:
> > Dear R-users,
> >
> > Following is part of my data, where slide has 36
> > levels and block 48 levels. I have done boxplot
> for
> > each slide on the same graph. There are outliers
> for
> > each slide and I tried to use indentify functtion
> to
> > identify outliers in such a way that when I click
> on
> > an outlier or point, the points will be labelled
> by
> > either their block or ID or by both but without
> > success. How can I make it work or are there other
> > ways to do it than using identify function?
> >
> >
> > Thanks in advance,
> >
> > 
> >
> > dat1[1:10,]
> >             y Slide Block              ID Control
> > 1   0.03147823     1     1       IgG-human       5
> > 2  -0.23815974     1     1 LPPAANDVSVLTAAR       0
> > 3  -0.71926359     1     1 HTKHYRVVSKPAALV       0
> > 4  -0.14607826     1     1 FVALPAATADAYATT       0
> > 5   0.89553073     1     1 NYPAMMAHAGDMAGY       0
> > 6  -0.67587100     1     1 RRALRQIGVLERPVG       0
> > 7   0.32636034     1     1 DCGTIRVGSFRGRWL       0
> > 8  -1.44057259     1     1 MAKLSTDELLDAFKE       0
> > 9  -0.37064338     1     1 LELSDFVKKFEETFE       0
> > 10 -0.20387233     1     1 VSRRAKVDVLIVHTT       0
> >
> >
> >  tb_ncs<-subset(dat1,dat1$Control==1)     ### this
> > data contains only negative controls
> >
> >       par(las=2,mar=c(10.1,4.1,4.1,2.1))
> >
> > boxplot(split(tb_ncs$y,tb_ncs$Slide),col="orange",
> > cex=.65,
> >                   outline=TRUE,main="Negative
> control
> > response of each patient", cex.main=1,
> font.main=1,
> >                            col.main="blue",
> > names=c(1:35,"B"))
> >                   grid(nx=NA, ny=NULL)
> >          ### grid over boxplot
> >           legend("bottomright", "B = Buffer +
> > sec",text.col="blue")
> >
> >
> >            out.block<-
> > identify(tb_ncs$y,tb_ncs$Slide)
> >
> >
> >
> >
> >
> >
> >
>
_________________________________________________________
> > Flyger tiden iv?g? F?nga dagen med Yahoo! Mails
> inbyggda
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> >
> 



	
	
		
_________________________________________________________
Flyger tiden iv?g? F?nga dagen med Yahoo! Mails inbyggda


From r.hankin at noc.soton.ac.uk  Fri Jan 12 14:22:45 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 12 Jan 2007 13:22:45 +0000
Subject: [R] [R-pkgs] Dummy's guide to S4 methods: package Brobdingnag
Message-ID: <06676A7D-69A0-4443-B078-DA15E7767331@soc.soton.ac.uk>

Hello List.

please find uploaded to CRAN a new package, Brobdingnag.

This package does two things:

(1) allows computation of very large numbers using a logarithmic  
representation.

(2) provides a "Hello, World" example of S4 methods in use: there are  
two classes of object
   (brob and glub) and one virtual class (swift).  The package  
includes a vignette that is a
    step-by-step guide to using S4 methods in the context of an R  
package.


Enjoy

Robin

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From Lukas.Indermaur at eawag.ch  Fri Jan 12 14:55:26 2007
From: Lukas.Indermaur at eawag.ch (Indermaur Lukas)
Date: Fri, 12 Jan 2007 14:55:26 +0100
Subject: [R] extract standard errors, write them to file
Message-ID: <FE8C160D1505B24497FA7C78D4DADACA0478C2@EA-MAIL.eawag.wroot.emp-eaw.ch>

Hello

I want to repeatedly extract coefficients and standard errors from a GLM and write them into a file (1row=all coefficients of model A, 2 row=all coefficients of model B, etc.). I can extract coefficients but not standard errors. furthermore I fail to write extracted values line by line into the predifined matrix G.

I appreciate any idea to solve the problem

best regards

Lukas

 

#---------start code------------------------------------------------------------------------------------------------------------------------------------

global                           <- formula(logHRS~Ri + E  + Co + LWD +Alwd +W  + T2 + A  + N + Sex + y)      #1
richness_evenness        <- formula(logHRS~Ri + E  + D1 + D2  +D3   +D4 + D5 + D6 + N + Sex + y)        #2

all_models <- c(global, richness_evenness)
for (i in 1:length(all_models)) 
{
ts.model <- glm(all_models[[i]],family=gaussian,data=t.data)
G            <- matrix(NA,length(all_models),length(all_models))
G            <- coefficients(ts.model)             #regression coefficents (betas)
}
write.table(G, paste(t.url, file="Coefficients.txt"), sep="\t", quote=F)    

#---------end code-------------------------------------------------------------------------------------------------------------------------------------
 
 
 
??? 
Lukas Indermaur, PhD student 
eawag / Swiss Federal Institute of Aquatic Science and Technology 
ECO - Department of Aquatic Ecology
?berlandstrasse 133
CH-8600 D?bendorf
Switzerland
 
Phone: +41 (0) 71 220 38 25
Fax    : +41 (0) 44 823 53 15 
Email: lukas.indermaur at eawag.ch
www.lukasindermaur.ch


From ripley at stats.ox.ac.uk  Fri Jan 12 15:04:19 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 12 Jan 2007 14:04:19 +0000 (GMT)
Subject: [R] Maximum likelihood acf
In-Reply-To: <45A78B4C.1020702@stat.ucl.ac.be>
References: <45A78B4C.1020702@stat.ucl.ac.be>
Message-ID: <Pine.LNX.4.64.0701121401170.351@gannet.stats.ox.ac.uk>

You will need to give us a reference, as the acf is not a parameter in a 
model in your description and MLEs apply to model parameters.

Just possibly ar.mle is what you are looking for, perhaps plus ARMAacf?

On Fri, 12 Jan 2007, Alain Guillet wrote:

> Hello!
>
> I am looking for a function which computes the maximum likelihood
> estimator of the autocorrelation function for a gaussian time series.
> Does a such function already exist in R?
> The estimator by default in R, acf(), uses the method of moments.
>
> Thanks a lot,
> Alain
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From sundar.dorai-raj at pdf.com  Fri Jan 12 15:04:30 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 12 Jan 2007 08:04:30 -0600
Subject: [R] extract standard errors, write them to file
In-Reply-To: <FE8C160D1505B24497FA7C78D4DADACA0478C2@EA-MAIL.eawag.wroot.emp-eaw.ch>
References: <FE8C160D1505B24497FA7C78D4DADACA0478C2@EA-MAIL.eawag.wroot.emp-eaw.ch>
Message-ID: <45A7956E.5030205@pdf.com>



Indermaur Lukas said the following on 1/12/2007 7:55 AM:
> Hello
> 
> I want to repeatedly extract coefficients and standard errors from a GLM and write them into a file (1row=all coefficients of model A, 2 row=all coefficients of model B, etc.). I can extract coefficients but not standard errors. furthermore I fail to write extracted values line by line into the predifined matrix G.
> 
> I appreciate any idea to solve the problem
> 
> best regards
> 
> Lukas
> 
>  
> 
> #---------start code------------------------------------------------------------------------------------------------------------------------------------
> 
> global                           <- formula(logHRS~Ri + E  + Co + LWD +Alwd +W  + T2 + A  + N + Sex + y)      #1
> richness_evenness        <- formula(logHRS~Ri + E  + D1 + D2  +D3   +D4 + D5 + D6 + N + Sex + y)        #2
> 
> all_models <- c(global, richness_evenness)
> for (i in 1:length(all_models)) 
> {
> ts.model <- glm(all_models[[i]],family=gaussian,data=t.data)
> G            <- matrix(NA,length(all_models),length(all_models))
> G            <- coefficients(ts.model)             #regression coefficents (betas)
> }
> write.table(G, paste(t.url, file="Coefficients.txt"), sep="\t", quote=F)    
> 
> #---------end code-------------------------------------------------------------------------------------------------------------------------------------
>  
>  
>  
> ??? 
> Lukas Indermaur, PhD student 
> eawag / Swiss Federal Institute of Aquatic Science and Technology 
> ECO - Department of Aquatic Ecology
> ?berlandstrasse 133
> CH-8600 D?bendorf
> Switzerland
>  
> Phone: +41 (0) 71 220 38 25
> Fax    : +41 (0) 44 823 53 15 
> Email: lukas.indermaur at eawag.ch
> www.lukasindermaur.ch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


Your code is barely readable because of poor spacing, but I believe you 
want:

coef(summary(ts.model))[, 1:2]

--sundar


From sarah.goslee at gmail.com  Fri Jan 12 15:06:03 2007
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 12 Jan 2007 09:06:03 -0500
Subject: [R] Regression lines
In-Reply-To: <45A76146.7040307@biostat.ku.dk>
References: <45A7574B.6050001@psych.uib.no>
	<Pine.LNX.4.64.0701120955170.4875@gannet.stats.ox.ac.uk>
	<45A76146.7040307@biostat.ku.dk>
Message-ID: <efb536d50701120606v3e003a81i427be3684e27728d@mail.gmail.com>

Fortune?

On 1/12/07, Peter Dalgaard <P.Dalgaard at biostat.ku.dk> wrote:
> Prof Brian Ripley wrote:
> >
> > Where did you tell it 'x' was the abscissa and 'y' the ordinate?
> > (Nowhere: R is lacking a mind_read() function!)
> Please stop complaining about missing features. Patches will be considered.
>
> Oh, it's you, Brian. Never mind then. You'll get to it, I'm sure.
>
> ;-)
>

-- 
Sarah Goslee
http://www.functionaldiversity.org


From fjbuch at gmail.com  Fri Jan 12 15:08:09 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Fri, 12 Jan 2007 09:08:09 -0500
Subject: [R] R editor vs. Tinn-R
Message-ID: <bd93cdad0701120608scdf1a91oe63906b69553e697@mail.gmail.com>

Have you used Tinn-R and what landmines await the inexperienced?

I could not understand why a script that used to work stopped working.
Look at these two scenarios
I opened an excel spreadsheet and copied several cells to the clipboard
Then Scenario 1
Executed from Tinn-R
> prelim<-read.delim("clipboard")
> str(prelim)
'data.frame':   0 obs. of  1 variable:
 $ prelim..read.delim.clipboard.: logi

Scenario 2
Executed from R editor
> prelim<-read.delim("clipboard")
> str(prelim)
'data.frame':   18 obs. of  13 variables:

-- 
Farrel Buchinsky
Mobile: (412) 779-1073


From s.wood at bath.ac.uk  Fri Jan 12 15:03:48 2007
From: s.wood at bath.ac.uk (Simon Wood)
Date: Fri, 12 Jan 2007 14:03:48 +0000
Subject: [R] DF for GAM function (mgcv package)
In-Reply-To: <4582DD92020000F500007D2A@cis27.hosts.jhmi.edu>
References: <4582DD92020000F500007D2A@cis27.hosts.jhmi.edu>
Message-ID: <200701121403.49047.s.wood@bath.ac.uk>

On Friday 15 December 2006 22:38, BRENDAN KLICK wrote:
> For summary(GAM) in the mgcv package smooth the degrees of freedom for
> the F value for test of smooth terms are the rank of covariance matrix
> of \hat{beta} and the residuals df.  I've noticed that in a lot of GAMs
> I've fit the rank of the covariance turns out to be 9.  In Simon Wood's
> book, the rank of covariance matrix is usually either 9 or 99 (pages
> 239-230 and 259).
>
> Can anyone comment on why so many smooth terms have a denominator
> degree of freedom involving 9.  Simon Wood writes "r is usually
> determinted numerically, while forming the pseudoinverse of the
> covariance matrix, or with reference to the effective degrees of freedom
> of the term" which doesn't really clarify the issue for me at least.

The rank used for the covariance matrix is often the number of free 
coefficients associated with the term (i.e. k-1, the maximum EDF for the term 
less the identifiability constraint). The idea is to base the test statistic 
on the parts of the model space that are not completley supressed by the 
penalization of the terms, so if penalization is not very high then this may 
mean the whole space. 9 occurs frequently because by default k=10 for a 1-D 
smooth. Where 99 occurs it's because a basis dimension (k) of 100 was being 
employed. The rank used is less than k-1 when some subspace of the model 
space has been very heavily penalized, so that it should not contribute 
anything to the test statistic. 

Finally... these tests are not great, and only  provide a rough guide to 
significance: the worst failing is the neglect of smoothing parameter 
uncertainty. See the final example in ?summary.gam to get an indication of 
how well/badly the p-values perform in practice. 

best,
Simon

>
> Thanks.
>
> Brendan Klick
> Johns Hopkins University School of Medicine.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
> Simon Wood, Mathematical Sciences, University of Bath, Bath, BA2 7AY UK
> +44 1225 386603  www.maths.bath.ac.uk/~sw283


From Inman.Brant at mayo.edu  Fri Jan 12 15:16:52 2007
From: Inman.Brant at mayo.edu (Inman, Brant A.   M.D.)
Date: Fri, 12 Jan 2007 08:16:52 -0600
Subject: [R] Making TIFF images with rtiff
References: <Pine.LNX.4.64.0701120728040.25092@gannet.stats.ox.ac.uk>
Message-ID: <6021CA6EF4C8374084D4F5A141F1CBBB664A36@msgebe23.mfad.mfroot.org>


Thanks to all the responders.  Here are some replies to the comments:


1) Concerning the term TIFF "format".
It may be that the journals are misusing the term TIFF, but it would
also appear that wikipedia is as well. The first sentence in the wiki
link sent below states:

"Tagged Image File FORMAT (abbreviated TIFF) is a file FORMAT for mainly
storing images, including photographs and line art."

Either way, the semantics of the word TIFF are probably not that
important for the current query.  If a publisher wants images in TIFF, I
would like to provide them in that format, regardless of whether or not
I deem the request proper.  After all they are the publishing experts!

2) Converting PNGs to TIFFs with Photoshop.
This is principally what I have done in the past that has given the poor
results that I have noted. I thought that it could be something that I
was specifically doing wrong so I consulted the medical imaging and
design department of my institution (Mayo Clinic) which informed me that
there is often a loss of information, some times quite large, in these
types of image format conversions. They suggested that it is best to
work with the TIFFs from the start if possible, which is why I am trying
to explore this option in R.  It is interesting that my imaging
department was able to convert the WMF format to TIFF with much better
success.  However, since Photoshop does not support WMFs, I am unable to
do this myself.  I have downloaded ImageMagick and will try that.

3)Lack of gratitude by R users.
It is interesting to note upon reviewing the R-help files that many
queries (perhaps even the majority?) do, in fact, convey gratitude.
Unfortunately, I have also noted that there are several messages from R
developers that appear to feel underappreciated. I suspect that one
reason that R is experiencing an explosion of users is precisely that
people appreciate and value the donation of free time provided by
statistical experts--such as Harrell, Weigand, Ripley and Kort in this
thread--for the development of accurate and powerful statistical
software.  Furthermore, the support provided for the software in the
form of R-help is outstanding, again something that I think is part of
the package deal that is attracting new users to R.  In other words, one
should not assume a general lack of gratitude on behalf of R-help users
but should see the growth of R as evidence that the software and its
developpers/supporters are indeed greatly valued.  I do not think that R
would be used much if people did not appreciate the nice packages,
functions and help provided.  Indeed, those of us that have access to
multiple software programs (I have access to JMP, SPSS, SPLUS and SAS)
choose R as our primary method of analysis because we feel that the
sharing environment supported by CRAN is a better way of doing
statistical computing.  Enough said.

4)Flexible journal policies.
Of 4 papers that I have submitted in the last 3 months for publication
in 3 journals (all to cancer related journals), all were subjected to
online file checkers that forced me to upload TIFF files instead of
PDFs.  Not only do they check the format, but also various other
resolution related items. In other words, I would not have even made it
past the online submission stage if I only PDFs to work with. 

5)Using the bitmap function to make TIFFs.
This sounds like a very attractive option.  I have tried this option
using the simple code below:

-------------
attach(cars)
plot(speed ~ dist)	# Simple plot to test
bitmap(file='C:\\...\\test.tif',type = "tifflzw", res = 1200)

Error in system(paste(gsexe, "-help"), intern = TRUE, invisible = TRUE)
: 
        gswin32c.exe not found

-------------

Despite what the error message suggests, I do have a functional
Ghostscript 8.54 program installed on my Windows XP machine with the
executable found in the directory: C:\Program
Files\gs\gs8.54\bin\gswin32c.  I am not sure why R is not finding the
program.  I tried making a Windows environmental variable, R_GSCMD, with
this system address but that did not have any success.  Does the
gswin32c file need to be in my R PATH?

Brant Inman
Mayo Clinic


From martin.becker at mx.uni-saarland.de  Fri Jan 12 15:40:37 2007
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Fri, 12 Jan 2007 15:40:37 +0100
Subject: [R] R editor vs. Tinn-R
In-Reply-To: <bd93cdad0701120608scdf1a91oe63906b69553e697@mail.gmail.com>
References: <bd93cdad0701120608scdf1a91oe63906b69553e697@mail.gmail.com>
Message-ID: <45A79DE5.9060803@mx.uni-saarland.de>

Farrel Buchinsky wrote:
> Have you used Tinn-R and what landmines await the inexperienced?
>
>   

Depending on which button you press in Tinn-R, the clipboard is used to 
transfer the commands to R, so sometimes you can't rely on the previous 
contents of the clipboard while using Tinn-R. If you use the 
"(source)"-versions of the buttons, the content of the clipboard should 
be preserved.

Regards,
  Martin

> I could not understand why a script that used to work stopped working.
> Look at these two scenarios
> I opened an excel spreadsheet and copied several cells to the clipboard
> Then Scenario 1
> Executed from Tinn-R
>   
>> prelim<-read.delim("clipboard")
>> str(prelim)
>>     
> 'data.frame':   0 obs. of  1 variable:
>  $ prelim..read.delim.clipboard.: logi
>
> Scenario 2
> Executed from R editor
>   
>> prelim<-read.delim("clipboard")
>> str(prelim)
>>     
> 'data.frame':   18 obs. of  13 variables:
>
>


From ggrothendieck at gmail.com  Fri Jan 12 15:49:32 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 12 Jan 2007 09:49:32 -0500
Subject: [R] Making TIFF images with rtiff
In-Reply-To: <6021CA6EF4C8374084D4F5A141F1CBBB664A36@msgebe23.mfad.mfroot.org>
References: <Pine.LNX.4.64.0701120728040.25092@gannet.stats.ox.ac.uk>
	<6021CA6EF4C8374084D4F5A141F1CBBB664A36@msgebe23.mfad.mfroot.org>
Message-ID: <971536df0701120649w542ee3fi88d9aa2a2c02fa08@mail.gmail.com>

I have had good results post processing xfig output
with fig2dev:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/60735.html

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/60762.html

On 1/12/07, Inman, Brant A.   M.D. <Inman.Brant at mayo.edu> wrote:
>
> Thanks to all the responders.  Here are some replies to the comments:
>
>
> 1) Concerning the term TIFF "format".
> It may be that the journals are misusing the term TIFF, but it would
> also appear that wikipedia is as well. The first sentence in the wiki
> link sent below states:
>
> "Tagged Image File FORMAT (abbreviated TIFF) is a file FORMAT for mainly
> storing images, including photographs and line art."
>
> Either way, the semantics of the word TIFF are probably not that
> important for the current query.  If a publisher wants images in TIFF, I
> would like to provide them in that format, regardless of whether or not
> I deem the request proper.  After all they are the publishing experts!
>
> 2) Converting PNGs to TIFFs with Photoshop.
> This is principally what I have done in the past that has given the poor
> results that I have noted. I thought that it could be something that I
> was specifically doing wrong so I consulted the medical imaging and
> design department of my institution (Mayo Clinic) which informed me that
> there is often a loss of information, some times quite large, in these
> types of image format conversions. They suggested that it is best to
> work with the TIFFs from the start if possible, which is why I am trying
> to explore this option in R.  It is interesting that my imaging
> department was able to convert the WMF format to TIFF with much better
> success.  However, since Photoshop does not support WMFs, I am unable to
> do this myself.  I have downloaded ImageMagick and will try that.
>
> 3)Lack of gratitude by R users.
> It is interesting to note upon reviewing the R-help files that many
> queries (perhaps even the majority?) do, in fact, convey gratitude.
> Unfortunately, I have also noted that there are several messages from R
> developers that appear to feel underappreciated. I suspect that one
> reason that R is experiencing an explosion of users is precisely that
> people appreciate and value the donation of free time provided by
> statistical experts--such as Harrell, Weigand, Ripley and Kort in this
> thread--for the development of accurate and powerful statistical
> software.  Furthermore, the support provided for the software in the
> form of R-help is outstanding, again something that I think is part of
> the package deal that is attracting new users to R.  In other words, one
> should not assume a general lack of gratitude on behalf of R-help users
> but should see the growth of R as evidence that the software and its
> developpers/supporters are indeed greatly valued.  I do not think that R
> would be used much if people did not appreciate the nice packages,
> functions and help provided.  Indeed, those of us that have access to
> multiple software programs (I have access to JMP, SPSS, SPLUS and SAS)
> choose R as our primary method of analysis because we feel that the
> sharing environment supported by CRAN is a better way of doing
> statistical computing.  Enough said.
>
> 4)Flexible journal policies.
> Of 4 papers that I have submitted in the last 3 months for publication
> in 3 journals (all to cancer related journals), all were subjected to
> online file checkers that forced me to upload TIFF files instead of
> PDFs.  Not only do they check the format, but also various other
> resolution related items. In other words, I would not have even made it
> past the online submission stage if I only PDFs to work with.
>
> 5)Using the bitmap function to make TIFFs.
> This sounds like a very attractive option.  I have tried this option
> using the simple code below:
>
> -------------
> attach(cars)
> plot(speed ~ dist)      # Simple plot to test
> bitmap(file='C:\\...\\test.tif',type = "tifflzw", res = 1200)
>
> Error in system(paste(gsexe, "-help"), intern = TRUE, invisible = TRUE)
> :
>        gswin32c.exe not found
>
> -------------
>
> Despite what the error message suggests, I do have a functional
> Ghostscript 8.54 program installed on my Windows XP machine with the
> executable found in the directory: C:\Program
> Files\gs\gs8.54\bin\gswin32c.  I am not sure why R is not finding the
> program.  I tried making a Windows environmental variable, R_GSCMD, with
> this system address but that did not have any success.  Does the
> gswin32c file need to be in my R PATH?
>
> Brant Inman
> Mayo Clinic
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From P.Dalgaard at biostat.ku.dk  Fri Jan 12 15:51:56 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 12 Jan 2007 15:51:56 +0100
Subject: [R] Making TIFF images with rtiff
In-Reply-To: <6021CA6EF4C8374084D4F5A141F1CBBB664A36@msgebe23.mfad.mfroot.org>
References: <Pine.LNX.4.64.0701120728040.25092@gannet.stats.ox.ac.uk>
	<6021CA6EF4C8374084D4F5A141F1CBBB664A36@msgebe23.mfad.mfroot.org>
Message-ID: <45A7A08C.5010007@biostat.ku.dk>

Inman, Brant A. M.D. wrote:
> Thanks to all the responders.  Here are some replies to the comments:
>
>
> 1) Concerning the term TIFF "format".
> It may be that the journals are misusing the term TIFF, but it would
> also appear that wikipedia is as well. The first sentence in the wiki
> link sent below states:
>
> "Tagged Image File FORMAT (abbreviated TIFF) is a file FORMAT for mainly
> storing images, including photographs and line art."
>
> Either way, the semantics of the word TIFF are probably not that
> important for the current query.  If a publisher wants images in TIFF, I
> would like to provide them in that format, regardless of whether or not
> I deem the request proper.  After all they are the publishing experts!
>
> 2) Converting PNGs to TIFFs with Photoshop.
> This is principally what I have done in the past that has given the poor
> results that I have noted. I thought that it could be something that I
> was specifically doing wrong so I consulted the medical imaging and
> design department of my institution (Mayo Clinic) which informed me that
> there is often a loss of information, some times quite large, in these
> types of image format conversions. They suggested that it is best to
> work with the TIFFs from the start if possible, which is why I am trying
> to explore this option in R.  It is interesting that my imaging
> department was able to convert the WMF format to TIFF with much better
> success.  However, since Photoshop does not support WMFs, I am unable to
> do this myself.  I have downloaded ImageMagick and will try that.
>
> 3)Lack of gratitude by R users.
> It is interesting to note upon reviewing the R-help files that many
> queries (perhaps even the majority?) do, in fact, convey gratitude.
> Unfortunately, I have also noted that there are several messages from R
> developers that appear to feel underappreciated. I suspect that one
> reason that R is experiencing an explosion of users is precisely that
> people appreciate and value the donation of free time provided by
> statistical experts--such as Harrell, Weigand, Ripley and Kort in this
> thread--for the development of accurate and powerful statistical
> software.  Furthermore, the support provided for the software in the
> form of R-help is outstanding, again something that I think is part of
> the package deal that is attracting new users to R.  In other words, one
> should not assume a general lack of gratitude on behalf of R-help users
> but should see the growth of R as evidence that the software and its
> developpers/supporters are indeed greatly valued.  I do not think that R
> would be used much if people did not appreciate the nice packages,
> functions and help provided.  Indeed, those of us that have access to
> multiple software programs (I have access to JMP, SPSS, SPLUS and SAS)
> choose R as our primary method of analysis because we feel that the
> sharing environment supported by CRAN is a better way of doing
> statistical computing.  Enough said.
>
> 4)Flexible journal policies.
> Of 4 papers that I have submitted in the last 3 months for publication
> in 3 journals (all to cancer related journals), all were subjected to
> online file checkers that forced me to upload TIFF files instead of
> PDFs.  Not only do they check the format, but also various other
> resolution related items. In other words, I would not have even made it
> past the online submission stage if I only PDFs to work with. 
>
> 5)Using the bitmap function to make TIFFs.
> This sounds like a very attractive option.  I have tried this option
> using the simple code below:
>
> -------------
> attach(cars)
> plot(speed ~ dist)	# Simple plot to test
> bitmap(file='C:\\...\\test.tif',type = "tifflzw", res = 1200)
>
> Error in system(paste(gsexe, "-help"), intern = TRUE, invisible = TRUE)
> : 
>         gswin32c.exe not found
>
> -------------
>
> Despite what the error message suggests, I do have a functional
> Ghostscript 8.54 program installed on my Windows XP machine with the
> executable found in the directory: C:\Program
> Files\gs\gs8.54\bin\gswin32c.  I am not sure why R is not finding the
> program.  I tried making a Windows environmental variable, R_GSCMD, with
> this system address but that did not have any success.  Does the
> gswin32c file need to be in my R PATH?
>
>   
It needs to be in your Path. If you open up a "DOS box" and type
gswin32c, I bet you get the same error message. You can fix this by
editing the Path (via My Computer/Properties/Advanced/Environment
variables, as you seem to know). If you use the R_GSCMD route, you may
get in trouble with the embedded space in "Program Files" ("dir/x c:"
will tell you the equivalent space-free name). Also, remember that
environment changes do not affect running programs so you may need to
exit R and restart.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From kim.mouridsen at gmail.com  Fri Jan 12 15:54:49 2007
From: kim.mouridsen at gmail.com (Kim Mouridsen)
Date: Fri, 12 Jan 2007 15:54:49 +0100
Subject: [R] Within-subject factors in lme
Message-ID: <403c92d40701120654h3828cf8dy6ac3720d8778e42@mail.gmail.com>

Dear R-users

I'm considering a repeated measures experiment where two
within-subject factors A (2 levels) and B (3 levels) have been
measured for each of 14 subjects, S. I wish to test the effect of
factor A. I know that a variance component model with random effects
S, S:A, S:B and S:A:B can be fitted using aov:

aov( y ~ A*B + Error(S/(A*B)) )

If there is no significant interaction, the test for the effect of A
is carried out in the S:A error strata.

How can a test for the effect of A be performed using lme from the nlme package?

( lme( y ~ A*B, random=~1|S/(A*B)) is apparently not correct )

Thanks in advance for your advice.
Kim.


From epistat at gmail.com  Fri Jan 12 16:09:09 2007
From: epistat at gmail.com (zhijie zhang)
Date: Fri, 12 Jan 2007 23:09:09 +0800
Subject: [R] Is it the PPS samples i needed in R?
Message-ID: <2fc17e30701120709m2699040ah6e3dc25e1dffac5f@mail.gmail.com>

Dear friends,
  I want to do a unequal probability sampling, that is, Probability
Proportionate to size, Is it right for the following programs?
Say my original dataset is:

 ID  Population
 1     100
 2     200
 3     300
 IF the population is large ,then the corresponding ID has the large
Probability to be selected.

sample(A$ID, size=2, replace = FALSE, prob = A$population)
#suppose the dataset name is A.
Is it the PPS samples  i needed ?
 Any suggestions are greatly welcome.

-- 
With Kind Regards,

oooO:::::::::
(..):::::::::
:\.(:::Oooo::
::\_)::(..)::
:::::::)./:::
::::::(_/::::
:::::::::::::
[***********************************************************************]
Zhi Jie,Zhang ,PHD
Tel:86-21-54237149   epistat at gmail.com
Dept. of Epidemiology,school of public health,Fudan University
Address:No. 138 Yi Xue Yuan Road,Shanghai,China
Postcode:200032
[***********************************************************************]
oooO:::::::::
(..):::::::::
:\.(:::Oooo::
::\_)::(..)::
:::::::)./:::
::::::(_/::::
:::::::::::::


From guillet at stat.ucl.ac.be  Fri Jan 12 16:08:50 2007
From: guillet at stat.ucl.ac.be (Alain Guillet)
Date: Fri, 12 Jan 2007 16:08:50 +0100
Subject: [R] Maximum likelihood acf
In-Reply-To: <Pine.LNX.4.64.0701121401170.351@gannet.stats.ox.ac.uk>
References: <45A78B4C.1020702@stat.ucl.ac.be>
	<Pine.LNX.4.64.0701121401170.351@gannet.stats.ox.ac.uk>
Message-ID: <45A7A482.6080203@stat.ucl.ac.be>

Prof. Brian Ripley,

You are right, my question was not clear.

In fact, I want to estimate the k first components of the acf, i.e. I 
want to estimate the k parameters (c(0),c(1),...c(k-1)), where c is the 
autocorrelation function, by a maximum likelihood estimator.

Alain



Prof Brian Ripley a ?crit :
> You will need to give us a reference, as the acf is not a parameter in 
> a model in your description and MLEs apply to model parameters.
>
> Just possibly ar.mle is what you are looking for, perhaps plus ARMAacf?
>
> On Fri, 12 Jan 2007, Alain Guillet wrote:
>
>> Hello!
>>
>> I am looking for a function which computes the maximum likelihood
>> estimator of the autocorrelation function for a gaussian time series.
>> Does a such function already exist in R?
>> The estimator by default in R, acf(), uses the method of moments.
>>
>> Thanks a lot,
>> Alain
>>
>>
>>
>

-- 
Alain Guillet
Statistician and Computer Scientist

Institut de statistique - Universit? catholique de Louvain
Bureau d.126
Voie du Roman Pays, 20
B-1348 Louvain-la-Neuve
Belgium

tel: +32 10 47 30 50


From s-walker at ti.com  Fri Jan 12 16:22:07 2007
From: s-walker at ti.com (Walker, Sam)
Date: Fri, 12 Jan 2007 09:22:07 -0600
Subject: [R] wafer map drawing
Message-ID: <76E525F192FF454A9712272E207B714B513164@dlee12.ent.ti.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070112/4cb776d1/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Jan 12 16:40:13 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 12 Jan 2007 15:40:13 +0000 (GMT)
Subject: [R] Maximum likelihood acf
In-Reply-To: <45A7A482.6080203@stat.ucl.ac.be>
References: <45A78B4C.1020702@stat.ucl.ac.be>
	<Pine.LNX.4.64.0701121401170.351@gannet.stats.ox.ac.uk>
	<45A7A482.6080203@stat.ucl.ac.be>
Message-ID: <Pine.LNX.4.64.0701121538050.7073@gannet.stats.ox.ac.uk>

On Fri, 12 Jan 2007, Alain Guillet wrote:

> Prof. Brian Ripley,
>
> You are right, my question was not clear.
>
> In fact, I want to estimate the k first components of the acf, i.e. I
> want to estimate the k parameters (c(0),c(1),...c(k-1)), where c is the
> autocorrelation function, by a maximum likelihood estimator.

And does ARMAacf applied to the result of ar.mle not do just that?
An accessible reference would help us, if not.

>
> Alain
>
>
>
> Prof Brian Ripley a ?crit :
>> You will need to give us a reference, as the acf is not a parameter in
>> a model in your description and MLEs apply to model parameters.
>>
>> Just possibly ar.mle is what you are looking for, perhaps plus ARMAacf?
>>
>> On Fri, 12 Jan 2007, Alain Guillet wrote:
>>
>>> Hello!
>>>
>>> I am looking for a function which computes the maximum likelihood
>>> estimator of the autocorrelation function for a gaussian time series.
>>> Does a such function already exist in R?
>>> The estimator by default in R, acf(), uses the method of moments.
>>>
>>> Thanks a lot,
>>> Alain
>>>
>>>
>>>
>>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From Inman.Brant at mayo.edu  Fri Jan 12 16:40:38 2007
From: Inman.Brant at mayo.edu (Inman, Brant A.   M.D.)
Date: Fri, 12 Jan 2007 09:40:38 -0600
Subject: [R] Making TIFF images with rtiff
Message-ID: <6021CA6EF4C8374084D4F5A141F1CBBB664A37@msgebe23.mfad.mfroot.org>


Thank you Peter Dalgaard.

When I open a DOS box and type gswin32c, I do indeed get an error message saying that it can't find the program.  I edited the Windows system environmental variable "Path" and the user environmental variable "PATH" (wasn't sure which to edit), to contain the follwing after a semicolon "C:\Program Files\gs\gs8.54\bin\". This effectively fixed the Dos box problem. I now get a GS prompt when I type gswin32c.

When I restart R and use the following code, I no longer get an error message.

--------
> attach(cars)
> bitmap(file='C:\\Documents and Settings\\m007704\\Desktop\\test.tif',
+ type = "tifflzw", res = 1200)
> plot(speed ~ dist)	
--------

Alas, if it was only that easy!  When I look on my desktop (to which the file address above correctly refers to), there is no image file of any sort to be found. Any ideas as to what I am doing wrong?

Brant Inman


---------------------------------------------------------------------

It needs to be in your Path. If you open up a "DOS box" and type
gswin32c, I bet you get the same error message. You can fix this by
editing the Path (via My Computer/Properties/Advanced/Environment
variables, as you seem to know). If you use the R_GSCMD route, you may
get in trouble with the embedded space in "Program Files" ("dir/x c:"
will tell you the equivalent space-free name). Also, remember that
environment changes do not affect running programs so you may need to
exit R and restart.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From tkellermann at ukaachen.de  Fri Jan 12 16:42:25 2007
From: tkellermann at ukaachen.de (Thilo Kellermann)
Date: Fri, 12 Jan 2007 16:42:25 +0100
Subject: [R] Within-subject factors in lme
In-Reply-To: <403c92d40701120654h3828cf8dy6ac3720d8778e42@mail.gmail.com>
References: <403c92d40701120654h3828cf8dy6ac3720d8778e42@mail.gmail.com>
Message-ID: <200701121642.25825.tkellermann@ukaachen.de>

Dear Kim,
as far as I understandyour problem correct the specification of the model in 
lme is:

lme( fixed=y ~ A*B, random=~1|S)

Thilo

On Friday 12 January 2007 15:54, Kim Mouridsen wrote:
> Dear R-users
>
> I'm considering a repeated measures experiment where two
> within-subject factors A (2 levels) and B (3 levels) have been
> measured for each of 14 subjects, S. I wish to test the effect of
> factor A. I know that a variance component model with random effects
> S, S:A, S:B and S:A:B can be fitted using aov:
>
> aov( y ~ A*B + Error(S/(A*B)) )
>
> If there is no significant interaction, the test for the effect of A
> is carried out in the S:A error strata.
>
> How can a test for the effect of A be performed using lme from the nlme
> package?
>
> ( lme( y ~ A*B, random=~1|S/(A*B)) is apparently not correct )
>
> Thanks in advance for your advice.
> Kim.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
________________________
Thilo Kellermann
Department of Psychiatry und Psychotherapy
RWTH Aachen University
Pauwelstr. 30
52074 Aachen
Tel.: +49 (0)241 / 8089977
Fax.: +49 (0)241 / 8082401
E-Mail: tkellermann at ukaachen.de


From P.Dalgaard at biostat.ku.dk  Fri Jan 12 16:47:40 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 12 Jan 2007 16:47:40 +0100
Subject: [R] Making TIFF images with rtiff
In-Reply-To: <6021CA6EF4C8374084D4F5A141F1CBBB664A37@msgebe23.mfad.mfroot.org>
References: <6021CA6EF4C8374084D4F5A141F1CBBB664A37@msgebe23.mfad.mfroot.org>
Message-ID: <45A7AD9C.5080804@biostat.ku.dk>

Inman, Brant A. M.D. wrote:
> Thank you Peter Dalgaard.
>
> When I open a DOS box and type gswin32c, I do indeed get an error message saying that it can't find the program.  I edited the Windows system environmental variable "Path" and the user environmental variable "PATH" (wasn't sure which to edit), to contain the follwing after a semicolon "C:\Program Files\gs\gs8.54\bin\". This effectively fixed the Dos box problem. I now get a GS prompt when I type gswin32c.
>
> When I restart R and use the following code, I no longer get an error message.
>
> --------
>   
>> attach(cars)
>> bitmap(file='C:\\Documents and Settings\\m007704\\Desktop\\test.tif',
>>     
> + type = "tifflzw", res = 1200)
>   
>> plot(speed ~ dist)	
>>     
> --------
>
> Alas, if it was only that easy!  When I look on my desktop (to which the file address above correctly refers to), there is no image file of any sort to be found. Any ideas as to what I am doing wrong?
>
>   
Hmmm.... You may be missing a final dev.off() or/and you need the
space-free version of "Documents and Settings" (dir/x c:\). Beyond that,
I'm out of ideas -- I don't use Windows all that often.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From janusz.rstuff at gmail.com  Fri Jan 12 16:52:14 2007
From: janusz.rstuff at gmail.com (Janusz Kawczak)
Date: Fri, 12 Jan 2007 10:52:14 -0500
Subject: [R] Making TIFF images with rtiff
In-Reply-To: <6021CA6EF4C8374084D4F5A141F1CBBB664A37@msgebe23.mfad.mfroot.org>
References: <6021CA6EF4C8374084D4F5A141F1CBBB664A37@msgebe23.mfad.mfroot.org>
Message-ID: <45A7AEAE.9090404@gmail.com>

How about dev.off() after your plot! It is easy as you predicted :-)
Janusz.

Inman, Brant A. M.D. wrote:

>Thank you Peter Dalgaard.
>
>When I open a DOS box and type gswin32c, I do indeed get an error message saying that it can't find the program.  I edited the Windows system environmental variable "Path" and the user environmental variable "PATH" (wasn't sure which to edit), to contain the follwing after a semicolon "C:\Program Files\gs\gs8.54\bin\". This effectively fixed the Dos box problem. I now get a GS prompt when I type gswin32c.
>
>When I restart R and use the following code, I no longer get an error message.
>
>--------
>  
>
>>attach(cars)
>>bitmap(file='C:\\Documents and Settings\\m007704\\Desktop\\test.tif',
>>    
>>
>+ type = "tifflzw", res = 1200)
>  
>
>>plot(speed ~ dist)	
>>    
>>
>--------
>
>Alas, if it was only that easy!  When I look on my desktop (to which the file address above correctly refers to), there is no image file of any sort to be found. Any ideas as to what I am doing wrong?
>
>Brant Inman
>
>
>---------------------------------------------------------------------
>
>It needs to be in your Path. If you open up a "DOS box" and type
>gswin32c, I bet you get the same error message. You can fix this by
>editing the Path (via My Computer/Properties/Advanced/Environment
>variables, as you seem to know). If you use the R_GSCMD route, you may
>get in trouble with the embedded space in "Program Files" ("dir/x c:"
>will tell you the equivalent space-free name). Also, remember that
>environment changes do not affect running programs so you may need to
>exit R and restart.
>
>  
>


From ccleland at optonline.net  Fri Jan 12 17:17:53 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 12 Jan 2007 11:17:53 -0500
Subject: [R] Making TIFF images with rtiff
In-Reply-To: <45A7AD9C.5080804@biostat.ku.dk>
References: <6021CA6EF4C8374084D4F5A141F1CBBB664A37@msgebe23.mfad.mfroot.org>
	<45A7AD9C.5080804@biostat.ku.dk>
Message-ID: <45A7B4B1.8030103@optonline.net>

Peter Dalgaard wrote:
> Inman, Brant A. M.D. wrote:
>> Thank you Peter Dalgaard.
>>
>> When I open a DOS box and type gswin32c, I do indeed get an error message saying that it can't find the program.  I edited the Windows system environmental variable "Path" and the user environmental variable "PATH" (wasn't sure which to edit), to contain the follwing after a semicolon "C:\Program Files\gs\gs8.54\bin\". This effectively fixed the Dos box problem. I now get a GS prompt when I type gswin32c.
>>
>> When I restart R and use the following code, I no longer get an error message.
>>
>> --------
>>   
>>> attach(cars)
>>> bitmap(file='C:\\Documents and Settings\\m007704\\Desktop\\test.tif',
>>>     
>> + type = "tifflzw", res = 1200)
>>   
>>> plot(speed ~ dist)	
>>>     
>> --------
>>
>> Alas, if it was only that easy!  When I look on my desktop (to which the file address above correctly refers to), there is no image file of any sort to be found. Any ideas as to what I am doing wrong?
>>
>>   
> Hmmm.... You may be missing a final dev.off() or/and you need the
> space-free version of "Documents and Settings" (dir/x c:\). Beyond that,
> I'm out of ideas -- I don't use Windows all that often.

  I'm on WinXP Pro and the following puts test.tif on the Desktop for me:

attach(cars)
bitmap(file='C:\\DOCUME~1\\CHARLE~1\\Desktop\\test.tif', type =
"tifflzw", res = 1200)
plot(speed ~ dist)
dev.off()

  It does not work with spaces in the path.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ripley at stats.ox.ac.uk  Fri Jan 12 17:28:23 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 12 Jan 2007 16:28:23 +0000 (GMT)
Subject: [R] Making TIFF images with rtiff
In-Reply-To: <6021CA6EF4C8374084D4F5A141F1CBBB664A36@msgebe23.mfad.mfroot.org>
References: <Pine.LNX.4.64.0701120728040.25092@gannet.stats.ox.ac.uk>
	<6021CA6EF4C8374084D4F5A141F1CBBB664A36@msgebe23.mfad.mfroot.org>
Message-ID: <Pine.LNX.4.64.0701121425310.27901@gannet.stats.ox.ac.uk>

On Fri, 12 Jan 2007, Inman, Brant A.   M.D. wrote:

> Thanks to all the responders.  Here are some replies to the comments:

[...]

> 3)Lack of gratitude by R users.
> It is interesting to note upon reviewing the R-help files that many
> queries (perhaps even the majority?) do, in fact, convey gratitude.
> Unfortunately, I have also noted that there are several messages from R
> developers that appear to feel underappreciated. I suspect that one
> reason that R is experiencing an explosion of users is precisely that
> people appreciate and value the donation of free time provided by
> statistical experts--such as Harrell, Weigand, Ripley and Kort in this
> thread--for the development of accurate and powerful statistical
> software.  Furthermore, the support provided for the software in the
> form of R-help is outstanding, again something that I think is part of
> the package deal that is attracting new users to R.  In other words, one
> should not assume a general lack of gratitude on behalf of R-help users
> but should see the growth of R as evidence that the software and its
> developpers/supporters are indeed greatly valued.  I do not think that R
> would be used much if people did not appreciate the nice packages,
> functions and help provided.  Indeed, those of us that have access to
> multiple software programs (I have access to JMP, SPSS, SPLUS and SAS)
> choose R as our primary method of analysis because we feel that the
> sharing environment supported by CRAN is a better way of doing
> statistical computing.  Enough said.

I see no mention by anyone of 'gratitude', and nothing relevant above to a 
thread about graphics devices and formats 'unfortunately'.

What _I_ actually said was

> [I don't recall anyone ever writing to thank us for the PNG or
> JPEG or bitmap drivers, and lack of appreciation does play a part.]

and people _do_ write to thank us for other additions.  Areas that people 
do show appreciation for do tend to get more attention, and things which 
get frequent negative comment, less.

Had several people said 'the png() device is very useful but what I really 
need is a TIFF variant', it might exist.  What happened is that people 
commented (and continued to comment) about the difficulty of use and (for 
some) poor quality of output of the png() and jpeg() devices and 
separately about the bitmap() device.  The idea of adding TIFF variants 
(which would have identical image quality) was dropped as a result.

So my comment was specific and factual.

-----------------------------------------------------

For my i686 box rebooted into Windows XP,

Sys.putenv(GS_CMD='C:/packages/gs/gs8.53/bin/gswin32c.exe')
bitmap("test.tif", type="tiff24nc")
...
dev.off()

works.  Note that gs's documentation says

   TIFF is a loose collection of formats, now largely superceded by PNG
   except in applications where backward compatibility or special
   compression is required.

(that is a collection of formats is clear from a reading of the whole 
Wikipedia entry).

I have little idea which of these formats publishers will accept.  I once 
had one who asked for 32-bit CMYK TIFF and then could not read what gs 
wrote (and had to subcontract the work to a printer who could).  It really 
is the case that 'loose collection' is true and causes problems.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From macq at llnl.gov  Fri Jan 12 17:48:12 2007
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 12 Jan 2007 08:48:12 -0800
Subject: [R] Making TIFF images with rtiff
In-Reply-To: <Pine.LNX.4.64.0701120728040.25092@gannet.stats.ox.ac.uk>
References: <6021CA6EF4C8374084D4F5A141F1CBBB664A32@msgebe23.mfad.mfroot.org>
	<Pine.LNX.4.64.0701120728040.25092@gannet.stats.ox.ac.uk>
Message-ID: <p06230901c1cd69926f29@[128.115.153.6]>

At 8:07 AM +0000 1/12/07, Prof Brian Ripley wrote:
>Let us be clear that TIFF is not a format, it is a collection of formats
>and for example my camera's RAW images are TIFFs, as are Adobe's DNG
>files.  (See e.g. http://en.wikipedia.org/wiki/TIFF.) So the journals
>concerned are misusing the term.  It is very easy to convert either JPEGs
>or PNGs to a TIFF subformat with ImageMagick, PhotoShop or other tools,
>and the bitmap() driver can write several versions of TIFF by converting
>from postscript (transparently to the user).
>
>I think the complaints in this thread are a bit rich given how much R
>aleady provides.  We thought about adding a few TIFF formats to PNG, JPEG
>and BMP but agreed it was not worth effort (especially given the vagueness
>with which 'TIFF' is used and stories about journals rejecting perfectly
>valid TIFF files). [I don't recall anyone ever writing to thank us for the
>PNG or JPEG or bitmap drivers, and lack of appreciation does play a part.]

[ remainder omitted ]

Then let me correct that omission right now. I appreciate these 
graphics drivers very much, especially the PNG driver that I now use 
routinely for placing R-generated graphics into so-called 
"presentation" software such as Microsoft PowerPoint.

I remember very well how inconvenient I used to find it to transfer 
graphics into such documents, i.e., those of both MS Word and MS 
PowerPoint. Not so any longer.

Thank you,
-Don
-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA


From kim.mouridsen at gmail.com  Fri Jan 12 18:25:28 2007
From: kim.mouridsen at gmail.com (Kim Mouridsen)
Date: Fri, 12 Jan 2007 18:25:28 +0100
Subject: [R] Within-subject factors in lme
In-Reply-To: <200701121642.25825.tkellermann@ukaachen.de>
References: <403c92d40701120654h3828cf8dy6ac3720d8778e42@mail.gmail.com>
	<200701121642.25825.tkellermann@ukaachen.de>
Message-ID: <403c92d40701120925p71b45437i806640a3837f660e@mail.gmail.com>

Dear Thilo

Thanks for your suggestion. I guess the model you are fitting here has
only a single random effect term, namely subject. If the effect of A
depends on S, one needs to include an additional random effects term
for the S:A interaction.

With lme I can get output for the effect of A which is very similar to
the aov output using

lme( y ~ A + B, random=~ 1|S/A )

but here I have cheated by not including factor B in the 'random='
terms. But the output from

anova( lme( y ~ A + B, random=~ 1|S/A ) )

is

            numDF denDF  F-value p-value
(Intercept)     1    54 388.4006  <.0001
B                2    54 154.0193  <.0001
A                1    13   4.4581  0.0547

where the last line appears equivalent to the aov output:

Error: Subject:Treatment
          Df  Sum Sq Mean Sq F value  Pr(>F)
A         1 0.66074 0.66074  4.4581 0.05467 .
Residuals 13 1.92676 0.14821

But I still need to account for the random S:B interaction.

I can see a similar issue has been discussed earlier, see eg

https://stat.ethz.ch/pipermail/r-help/2006-August/111018.html

Here, lme( y ~ A*B, random=~1|S ) was also suggested (essentially),
but this gives quite different results from aov and the lme example
above. In this particular case I get

            numDF denDF  F-value p-value
(Intercept)     1    67 388.3976  <.0001
B               2    67 104.8436  <.0001
A               1    67  10.3707   0.002

I have seen instances of something like
random=list(S=pdBlocked(list(pdIdent(~A-1)..., but I can't get this to
work (and I have no idea what this does).

Best regards,
Kim.


2007/1/12, Thilo Kellermann <tkellermann at ukaachen.de>:
> Dear Kim,
> as far as I understandyour problem correct the specification of the model in
> lme is:
>
> lme( fixed=y ~ A*B, random=~1|S)
>
> Thilo
>
> On Friday 12 January 2007 15:54, Kim Mouridsen wrote:
> > Dear R-users
> >
> > I'm considering a repeated measures experiment where two
> > within-subject factors A (2 levels) and B (3 levels) have been
> > measured for each of 14 subjects, S. I wish to test the effect of
> > factor A. I know that a variance component model with random effects
> > S, S:A, S:B and S:A:B can be fitted using aov:
> >
> > aov( y ~ A*B + Error(S/(A*B)) )
> >
> > If there is no significant interaction, the test for the effect of A
> > is carried out in the S:A error strata.
> >
> > How can a test for the effect of A be performed using lme from the nlme
> > package?
> >
> > ( lme( y ~ A*B, random=~1|S/(A*B)) is apparently not correct )
> >
> > Thanks in advance for your advice.
> > Kim.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented, minimal,
> > self-contained, reproducible code.
>
> --
> ________________________
> Thilo Kellermann
> Department of Psychiatry und Psychotherapy
> RWTH Aachen University
> Pauwelstr. 30
> 52074 Aachen
> Tel.: +49 (0)241 / 8089977
> Fax.: +49 (0)241 / 8082401
> E-Mail: tkellermann at ukaachen.de
>
>


From amnakhan493 at gmail.com  Fri Jan 12 18:37:08 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Fri, 12 Jan 2007 09:37:08 -0800
Subject: [R] Help for RFA
Message-ID: <3ffd3bb60701120937h17c0cc08qf01bbc17926932d0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070112/1ae9159f/attachment.pl 

From fjbuch at gmail.com  Fri Jan 12 18:37:49 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Fri, 12 Jan 2007 12:37:49 -0500
Subject: [R] R editor vs. Tinn-R
References: <bd93cdad0701120608scdf1a91oe63906b69553e697@mail.gmail.com>
	<45A79DE5.9060803@mx.uni-saarland.de>
Message-ID: <eo8h1f$vcl$1@sea.gmane.org>

The only button I pressed was the "send line" button in version 1.19.1.5. I 
changed my command to

prelim<-read.delim(source("clipboard"))

and still got the same problem.
I do not know how to use the source versions of the buttons. Can you please 
tell me more?

Thanks

Farrel


"Martin Becker" <martin.becker at mx.uni-saarland.de> wrote in message 
news:45A79DE5.9060803 at mx.uni-saarland.de...
> Farrel Buchinsky wrote:
>> Have you used Tinn-R and what landmines await the inexperienced?
>>
>>
>
> Depending on which button you press in Tinn-R, the clipboard is used to
> transfer the commands to R, so sometimes you can't rely on the previous
> contents of the clipboard while using Tinn-R. If you use the
> "(source)"-versions of the buttons, the content of the clipboard should
> be preserved.
>
> Regards,
>  Martin
>
>> I could not understand why a script that used to work stopped working.
>> Look at these two scenarios
>> I opened an excel spreadsheet and copied several cells to the clipboard
>> Then Scenario 1
>> Executed from Tinn-R
>>
>>> prelim<-read.delim("clipboard")
>>> str(prelim)
>>>
>> 'data.frame':   0 obs. of  1 variable:
>>  $ prelim..read.delim.clipboard.: logi
>>
>> Scenario 2
>> Executed from R editor
>>
>>> prelim<-read.delim("clipboard")
>>> str(prelim)
>>>
>> 'data.frame':   18 obs. of  13 variables:
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From drvar at mail.dhz.hr  Fri Jan 12 19:00:11 2007
From: drvar at mail.dhz.hr (Dunja Drvar)
Date: Fri, 12 Jan 2007 19:00:11 +0100 (CET)
Subject: [R] quilt.plot
Message-ID: <3338.193.198.128.94.1168624811.squirrel@mail.dhz.hr>

Dear all,
I have a problem with plotting.
I have an irregular set of data, where latitude is listed from bigger to
smaller values. So, I cannot plot with image.plot.
On the other hand, quilt.plot solves the problem, but it can't take the
definition of color. It gives the feedback:

Error in image.plot(out.p, col = tim.colors(64), ...) :
        formal argument "col" matched by multiple actual arguments

The question is: is there a way to define the color palette in quilt.plot?
Thanks in advance,
        Dunja


Weather analysis and forecasting division
Meteorological and Hydrological Service
Gric 3, Zagreb HR-10000, Croatia
Tel: +385 1 4565 783
Fax: +385 1 4565 757


From martin.becker at mx.uni-saarland.de  Fri Jan 12 19:11:42 2007
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Fri, 12 Jan 2007 19:11:42 +0100
Subject: [R] R editor vs. Tinn-R
In-Reply-To: <eo8h1f$vcl$1@sea.gmane.org>
References: <bd93cdad0701120608scdf1a91oe63906b69553e697@mail.gmail.com>
	<45A79DE5.9060803@mx.uni-saarland.de> <eo8h1f$vcl$1@sea.gmane.org>
Message-ID: <20070112191142.8qimphz7tsgkco8s@webmail.rz.uni-saarland.de>

Zitat von Farrel Buchinsky <fjbuch at gmail.com>:

> The only button I pressed was the "send line" button in version 1.19.1.5. I
> changed my command to
>
> prelim<-read.delim(source("clipboard"))
>

The problem is not using the wrong R-command, but using a (Tinn-R)  
button that leads to a replacement of the current (windows) clipboard...

> and still got the same problem.
> I do not know how to use the source versions of the buttons. Can you please
> tell me more?
>

There are three button groups on the left of the "Send line" button,  
containing two buttons each. The left button of each of these three  
button groups has the same name as the right button of each group,  
apart from having a "(source)" suffix. Now it should be obvious what  
was meant with '"(source)"-version of buttons'. There is no such  
version for the "Send line" button, so you have to select the  
corresponding line and use the "Send selection (source)" button, e.g.

> Thanks
>
> Farrel
>

Regards,
   Martin

>
> "Martin Becker" <martin.becker at mx.uni-saarland.de> wrote in message
> news:45A79DE5.9060803 at mx.uni-saarland.de...
>> Farrel Buchinsky wrote:
>>> Have you used Tinn-R and what landmines await the inexperienced?
>>>
>>>
>>
>> Depending on which button you press in Tinn-R, the clipboard is used to
>> transfer the commands to R, so sometimes you can't rely on the previous
>> contents of the clipboard while using Tinn-R. If you use the
>> "(source)"-versions of the buttons, the content of the clipboard should
>> be preserved.
>>
>> Regards,
>>  Martin
>>
>>> I could not understand why a script that used to work stopped working.
>>> Look at these two scenarios
>>> I opened an excel spreadsheet and copied several cells to the clipboard
>>> Then Scenario 1
>>> Executed from Tinn-R
>>>
>>>> prelim<-read.delim("clipboard")
>>>> str(prelim)
>>>>
>>> 'data.frame':   0 obs. of  1 variable:
>>>  $ prelim..read.delim.clipboard.: logi
>>>
>>> Scenario 2
>>> Executed from R editor
>>>
>>>> prelim<-read.delim("clipboard")
>>>> str(prelim)
>>>>
>>> 'data.frame':   18 obs. of  13 variables:
>>>
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From chris at psyctc.org  Fri Jan 12 12:08:33 2007
From: chris at psyctc.org (Chris Evans)
Date: Fri, 12 Jan 2007 11:08:33 +0000
Subject: [R] Making TIFF images with rtiff
In-Reply-To: <Pine.LNX.4.64.0701120728040.25092@gannet.stats.ox.ac.uk>
References: <6021CA6EF4C8374084D4F5A141F1CBBB664A32@msgebe23.mfad.mfroot.org>
	<Pine.LNX.4.64.0701120728040.25092@gannet.stats.ox.ac.uk>
Message-ID: <45A76C31.2090709@psyctc.org>

Prof Brian Ripley sent the following  at 12/01/2007 08:07:
> Let us be clear that TIFF is not a format, it is a collection of formats 
... snip ...

> [I don't recall anyone ever writing to thank us for the
> PNG or JPEG or bitmap drivers, and lack of appreciation does play a part.]

... rest snipped ...

I hope I'm not going to turn out to be one a deluge of people now who
hit the list with this but, I think you have a real point there.  I
sometimes wince at the tartness of some responses on this list but I owe
you and many other people a completely unredeemable debt both for R
itself including all its drivers, libraries, extraordinary portability,
etc., etc. _AND_ for the fantastic quality of the advice that is given
away daily here.

If it's any consolation, I've started offering workshops to conferences
in my own area to introduce novices away from SPSS and other packages (I
loved SAS for a long time) and onto R.  Trouble is, if I succeed,
that'll probably only increase the rate of feature requests and
questions here.

Perhaps the sheer unredeemable nature of the debt to you all is part of
what makes us quiet.

Cheers all,

Chris

P.S. as ever, really helpful information re TIFFs confirming things I
utterly failed to convince a printers and a subeditor about last year,
fortunately the editor was a friend and we ended up getting the paper
accepted with PS graphics as files!


-- 
Chris Evans <chris at psyctc.org> Skype: chris-psyctc
Professor of Psychotherapy, Nottingham University;
Consultant Psychiatrist in Psychotherapy, Notts PDD network;
Research Programmes Director, Nottinghamshire NHS Trust;
Hon. SL Institute of Psychiatry, Hon. Con., Tavistock & Portman Trust
*If I am writing from one of those roles, it will be clear. Otherwise*
*my views are my own and not representative of those institutions    *


From Inman.Brant at mayo.edu  Fri Jan 12 19:17:49 2007
From: Inman.Brant at mayo.edu (Inman, Brant A.   M.D.)
Date: Fri, 12 Jan 2007 12:17:49 -0600
Subject: [R] Making TIFF images with rtiff
Message-ID: <6021CA6EF4C8374084D4F5A141F1CBBB664A38@msgebe23.mfad.mfroot.org>


Several R Helpers pointed out that I forgot to include the dev.off()
statement in my code.  This solved my problem with one caviat: the
output file address cannot have any spaces in it (as pointed out by
Chuck Cleland).  For instance:

------------
# This file location works great
bitmap(file='C:\\temp\\test.tif',
	type = "tifflzw", res = 1200)
plot(speed ~ dist)	
dev.off()

# This file location does not work, despite being accurate
bitmap(file='C:\\Documents and Settings\\m007704\\Desktop\\test.tif',
	type = "tifflzw", res = 1200)
plot(speed ~ dist)	
dev.off() 
------------

For the benefit of those that may need to make TIFF files in the future
and don't know how to do it, I will recapitulate below the steps
required to produce a TIFF file using R on a Windows XP machine.

1) Download and install a current version of Ghostscript. 

You probably need GPL(GNU General Public License) version of
Ghostscript. For Windows, the correct file to download is called:
gs854w32-gpl.exe. To download this file, go to one of the following
websites:

http://www.cs.wisc.edu/~ghost/
http://sourceforge.net/projects/ghostscript/

2) Add Ghostscript to a Windows environmental variable.

Right click on the My Computer icon on your desktop.  Select: Properties
> Advanced > Environmental Variables.  You will see 2 boxes, one for
user variables and one for system variables.  In the user variables
section, highlight the variable called "PATH" and then click edit. Click
on the variable value box and go to the end of whatever is written there
(don't erase it).  Enter the following after the last bit of text:
";C:\Program Files\gs\gs8.54\bin\".  This is the location on you
computer where it can find the gswin32c.exe file that it needs to start
Ghostscript.

3) Use the bitmap function to produce a TIFF

Now you should be ready to make a TIFF.  The following code is a simple
example that you can use to see if everything is set up right on your
PC.

attach(cars)
bitmap(file='C:\\temp\\test.tif',
	type = "tifflzw", res = 1200)
plot(speed ~ dist)	
dev.off()

This should produce a TIFF file called 'test.tif' in the 'temp'
directory of your PC.  If you do not have a directory of this name,
substitute for one that exists (or create one).  Note that the file
argument does not seem to handle and spaces in the directory address, so
select an address without spaces in it. Note also that, as pointed out
by Ripley in this thread, there are different TIFF formats which can be
made with R.  My understanding is that the different formats have to do
with different image compression algorithms, but you can read more about
these details (and clues to why TIFF files seem to be prefered by some
publishers and imaging software makers) at:

http://en.wikipedia.org/wiki/Comparison_of_graphics_file_formats

You can also type ?bitmap to see what R can output for you and read more
about the TIFF file format at:

http://en.wikipedia.org/wiki/Tiff

I regret that I cannot comment on Unix or Mac computers as it has been
nearly 15 years since I have used these types of machines and I
therefore have no knowledge whatsoever that might be of use for users of
these systems.

Brant Inman


From drf5n at maplepark.com  Fri Jan 12 19:40:58 2007
From: drf5n at maplepark.com (David Forrest)
Date: Fri, 12 Jan 2007 12:40:58 -0600 (CST)
Subject: [R] zero margin / marginless plots (in lattice?)
In-Reply-To: <Pine.LNX.4.64.0701110948100.13593@maplepark.com>
References: <Pine.LNX.4.64.0701102047580.13593@maplepark.com>
	<1168486593.5008.4.camel@localhost.localdomain>
	<Pine.LNX.4.64.0701110948100.13593@maplepark.com>
Message-ID: <Pine.LNX.4.64.0701121237540.9417@maplepark.com>

On Thu, 11 Jan 2007, David Forrest wrote:

> Thanks.  The xaxs|yaxs='i' works well for the base graphics.  Is there an
> additional parameter in play for lattice graphics?  The closest I could
> gotten is the below which still leaves a bit of a margin:
>
> xy<-data.frame(x=c(0,1,1,0,0),y=c(0,1,0,0,1))
> xyplot(y~x,xy,scales=list(axs='i',draw=FALSE),type='l',xlab=NULL,ylab=NULL)

Hi again,

If there isn't a way to completely eliminate the margins in lattice/grid 
plots, is there a way to find the graphic device extents as measured in 
data coordinates?

Dave


>
> Dave
>
> On Wed, 10 Jan 2007, Marc Schwartz wrote:
>
>> On Wed, 2007-01-10 at 21:18 -0600, David Forrest wrote:
>>> Hi,
>>>
>>> I'd like to produce a marginless or zero margin plot so that the pixel
>>> coordinates represent the mathematics.
>>>
>>> xy<-data.frame(x=c(0,1,1,0,0),y=c(0,1,0,0,1))
>>> png('junk.png',width=300,height=300)
>>> par(mar=c(0,0,0,0))
>>> plot(xy$x,xy$y,xlim=c(0,1),ylim=c(,1))
>>> dev.off()
>>>
>>> The resultant file has about a 10 pixel margin around these lines, and I'm
>>> not sure what parameter or function is controlling this offset.  Any
>>> hints?
>>>
>>> Thanks for your time,
>>> Dave
>>
>> By default, the axis ranges are extended by +/- 4%.  You can change this
>> by using:
>>
>>  plot(xy$x, xy$y, xlim = c(0, 1), ylim = c(0, 1),
>>       xaxs = "i", yaxs = "i")
>>
>> where 'xaxs' and 'yaxs' set the axis ranges to the actual data ranges.
>>
>> See ?par for more information.
>>
>> HTH,
>>
>> Marc Schwartz
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

-- 
  Dr. David Forrest
  drf at vims.edu                                    (804)684-7900w
  drf5n at maplepark.com                             (804)642-0662h
                                    http://maplepark.com/~drf5n/


From bbands at gmail.com  Fri Jan 12 19:47:54 2007
From: bbands at gmail.com (BBands)
Date: Fri, 12 Jan 2007 10:47:54 -0800
Subject: [R] R editor vs. Tinn-R
In-Reply-To: <45A79DE5.9060803@mx.uni-saarland.de>
References: <bd93cdad0701120608scdf1a91oe63906b69553e697@mail.gmail.com>
	<45A79DE5.9060803@mx.uni-saarland.de>
Message-ID: <6e8360ad0701121047v595bf9d0uf3aaeb326f8a3d47@mail.gmail.com>

On 1/12/07, Martin Becker <martin.becker at mx.uni-saarland.de> wrote:
> Depending on which button you press in Tinn-R, the clipboard is used to
> transfer the commands to R, so sometimes you can't rely on the previous
> contents of the clipboard while using Tinn-R. If you use the
> "(source)"-versions of the buttons, the content of the clipboard should
> be preserved.

Something odd is going on. I can confirm that read.delim("clipboard")
works from R, but not from Tinn-R. What seems odd is that the contents
of the clipboard _are_ preserved after the Tinn-R send, so that a
subsequent paste or read.delim("clipboard") from R works correctly.
Perhaps Tinn-R restores the contents of the clipboard after sending to
R such that the R command runs before the restore takes place?

     jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From Thierry.ONKELINX at inbo.be  Fri Jan 12 21:06:42 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Fri, 12 Jan 2007 21:06:42 +0100
Subject: [R] R editor vs. Tinn-R
References: <bd93cdad0701120608scdf1a91oe63906b69553e697@mail.gmail.com><45A79DE5.9060803@mx.uni-saarland.de>
	<6e8360ad0701121047v595bf9d0uf3aaeb326f8a3d47@mail.gmail.com>
Message-ID: <2E9C414912813E4EB981326983E0A1042E49B7@inboexch.inbo.be>


Instead of discussing this odd behaviour of TINN-R, I would prefer a discussion on importing data through the clipboard. In my opinion it isn't a good a idea to import data with the clipboard. I know that it's a quick and dirty way to get your data fast into R. 
But I see two major drawbacks. First of all you have no chance of checking what data you imported. This is important when you need to check your results a few days (weeks, months or even years) later. A second drawback is that you won't feel the need to store your data in an orderly fashion. Which often leads to a huge pile of junk, instead of a valuable dataset...

Cheers,

Thierry

-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch namens BBands
Verzonden: vr 12-1-2007 19:47
Aan: R-Help
Onderwerp: Re: [R] R editor vs. Tinn-R
 
On 1/12/07, Martin Becker <martin.becker op mx.uni-saarland.de> wrote:
> Depending on which button you press in Tinn-R, the clipboard is used to
> transfer the commands to R, so sometimes you can't rely on the previous
> contents of the clipboard while using Tinn-R. If you use the
> "(source)"-versions of the buttons, the content of the clipboard should
> be preserved.

Something odd is going on. I can confirm that read.delim("clipboard")
works from R, but not from Tinn-R. What seems odd is that the contents
of the clipboard _are_ preserved after the Tinn-R send, so that a
subsequent paste or read.delim("clipboard") from R works correctly.
Perhaps Tinn-R restores the contents of the clipboard after sending to
R such that the R command runs before the restore takes place?

     jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From gunter.berton at gene.com  Fri Jan 12 21:41:04 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Fri, 12 Jan 2007 12:41:04 -0800
Subject: [R] R editor vs. Tinn-R
In-Reply-To: <2E9C414912813E4EB981326983E0A1042E49B7@inboexch.inbo.be>
Message-ID: <006e01c73689$fb069300$4d908980@gne.windows.gene.com>

Thierry:

Instead of discussing this odd behaviour of TINN-R, I would prefer a
discussion on importing data through the clipboard. In my opinion it isn't a
good a idea to import data with the clipboard. I know that it's a quick and
dirty way to get your data fast into R. 
But I see two major drawbacks. First of all you have no chance of checking
what data you imported. This is important when you need to check your
results a few days (weeks, months or even years) later. A second drawback is
that you won't feel the need to store your data in an orderly fashion. Which
often leads to a huge pile of junk, instead of a valuable dataset...
-------------

I do not understand this. I do this all the time, easily check the data in R
(which has all sorts of powerful capabilities to do this), and easily store
the data as part of the .Rdata file that also contains functions,
transformations, analyses, etc. that I have used on the data. I do not know
what is more orderly and useful than that! So would you care to
elaborate?....

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404


From lee.daejin at gmail.com  Fri Jan 12 19:41:24 2007
From: lee.daejin at gmail.com (Dae-Jin Lee)
Date: Fri, 12 Jan 2007 19:41:24 +0100
Subject: [R] R2WinBugs and Compare DIC versus BIC or AIC
Message-ID: <cbca975a0701121041m1d3a5365qaa9cf91288d1f5b3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070112/9f57acb3/attachment.pl 

From f.savorani at unibo.it  Fri Jan 12 18:55:16 2007
From: f.savorani at unibo.it (Francesco Savorani)
Date: Fri, 12 Jan 2007 18:55:16 +0100
Subject: [R] PCA (prcomp) details info.
Message-ID: <001b01c73672$d20c94c0$0cd3cc89@fsavor>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070112/f875b939/attachment.pl 

From drf5n at maplepark.com  Fri Jan 12 22:42:06 2007
From: drf5n at maplepark.com (David Forrest)
Date: Fri, 12 Jan 2007 15:42:06 -0600 (CST)
Subject: [R] zero margin / marginless plots (in lattice?)
In-Reply-To: <Pine.LNX.4.64.0701110948100.13593@maplepark.com>
References: <Pine.LNX.4.64.0701102047580.13593@maplepark.com>
	<1168486593.5008.4.camel@localhost.localdomain>
	<Pine.LNX.4.64.0701110948100.13593@maplepark.com>
Message-ID: <Pine.LNX.4.64.0701121507530.9417@maplepark.com>

On Thu, 11 Jan 2007, David Forrest wrote:

> Thanks.  The xaxs|yaxs='i' works well for the base graphics.  Is there an
> additional parameter in play for lattice graphics?  The closest I could
> gotten is the below which still leaves a bit of a margin:
>
> xy<-data.frame(x=c(0,1,1,0,0),y=c(0,1,0,0,1))
> xyplot(y~x,xy,scales=list(axs='i',draw=FALSE),type='l',xlab=NULL,ylab=NULL)


>From an old email of Depayan's I see these margins in lattice are are due 
to 'padding' and this lattice theme seems to eliminate them:

theme.novpadding <-
   list(layout.heights =
        list(top.padding = 0,
 	    main.key.padding = 0,
 	    key.axis.padding = 0,
 	    axis.xlab.padding = 0,
 	    xlab.key.padding = 0,
 	    key.sub.padding = 0,
 	    bottom.padding = 0),
        layout.widths =
        list(left.padding = 0,
 	    key.ylab.padding = 0,
 	    ylab.axis.padding = 0,
 	    axis.key.padding = 0,
 	    right.padding = 0))


Then, using the quakes data example from ?xyplot :

library(stats)
xyplot(lat~long,quakes,
   scales=list(axs='i',draw=FALSE),
   ,xlab=NULL,ylab=NULL,par.settings = theme.novpadding)


...Then this graphic should drop nicely into GoogleEarth with a 
boundingbox matching the data:

range(quakes$lat);range(quakes$long)
[1] -38.59 -10.72
[1] 165.67 188.13

dev.copy(png,width=400,height=400,file='quakes.png')

and post it to the web and KMZ file as at:

http://www.maplepark.com/drf5n/extras/R_xyplot_googleEarth.kmz

Thanks.

>
> Dave
>
> On Wed, 10 Jan 2007, Marc Schwartz wrote:
>
>> On Wed, 2007-01-10 at 21:18 -0600, David Forrest wrote:
>>> Hi,
>>>
>>> I'd like to produce a marginless or zero margin plot so that the pixel
>>> coordinates represent the mathematics.
>>>
>>> xy<-data.frame(x=c(0,1,1,0,0),y=c(0,1,0,0,1))
>>> png('junk.png',width=300,height=300)
>>> par(mar=c(0,0,0,0))
>>> plot(xy$x,xy$y,xlim=c(0,1),ylim=c(,1))
>>> dev.off()
>>>
>>> The resultant file has about a 10 pixel margin around these lines, and I'm
>>> not sure what parameter or function is controlling this offset.  Any
>>> hints?
>>>
>>> Thanks for your time,
>>> Dave
>>
>> By default, the axis ranges are extended by +/- 4%.  You can change this
>> by using:
>>
>>  plot(xy$x, xy$y, xlim = c(0, 1), ylim = c(0, 1),
>>       xaxs = "i", yaxs = "i")
>>
>> where 'xaxs' and 'yaxs' set the axis ranges to the actual data ranges.
>>
>> See ?par for more information.
>>
>> HTH,
>>
>> Marc Schwartz
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

-- 
  Dr. David Forrest
  drf at vims.edu                                    (804)684-7900w
  drf5n at maplepark.com                             (804)642-0662h
                                    http://maplepark.com/~drf5n/


From john.maindonald at anu.edu.au  Fri Jan 12 22:48:06 2007
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Sat, 13 Jan 2007 08:48:06 +1100
Subject: [R] overdispersion
In-Reply-To: <mailman.8.1168599603.21300.r-help@stat.math.ethz.ch>
References: <mailman.8.1168599603.21300.r-help@stat.math.ethz.ch>
Message-ID: <09CA0E49-A0A4-469F-BAF0-F80E8CD697BA@anu.edu.au>

I would say rather that for binary data (binomial data with n=1) it  
is not possible to detect overdispersion from examination of the  
Pearson chi-square or the deviance.   Overdispersion may be, and  
often is, nevertheless present.  I am arguing that overdispersion is  
properly regarded as a function of the variance-covariance structure,  
not as a function of the sample data.

The variance of a two-point distribution is a known function of the  
mean, providing that independence and identity of distribution can be  
assumed, or providing that the correlation structure is otherwise  
known and the mean is constant. That proviso is crucial!

If there is some sort of grouping, it may be appropriate to aggregate  
data over the groups, yielding data that have a binomial form with  
n>1.  Over-dispersion can now be detected from the Pearson chi-square  
or from the deviance.  Note that the quasi models assume that the  
multiplier for the binomial or other variance is constant with p;  
that may or may not be realistic.  Generalized linear mixed models  
make their own different assumptions about how the variance changes  
as a function of p; again these may or may not be realistic.

It is then the "error" structure that is crucial. To the extent that  
distracts from careful thinking about that structure, the term  
"overdispersion is unsatisfactory.

There's no obvious way that I can see to supply glm() with an  
estimate of the dispersion that has been derived independently of the  
current analysis.  Especially in the binary case, this would  
sometimes be useful.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 12 Jan 2007, at 10:00 PM, r-help-request at stat.math.ethz.ch wrote:

> From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> Date: 12 January 2007 5:04:26 AM
> To: evaiannario <evaiannario at libero.it>
> Cc: "r-help at stat.math.ethz.ch" <r-help at stat.math.ethz.ch>
> Subject: Re: [R] overdispersion
>
>
> evaiannario wrote:
>> How can I eliminate the overdispersion for binary data apart the  
>> use of the quasibinomial?
> There is no such thing as overdispersion for binary data. (The  
> variance of a two-point distribution is a known function of the  
> mean.) If what you want to do is include random effects of some  
> sort of grouping then you might look into generalized linear mixed  
> models via lmer() from the lme4 package or glmmPQL from MASS.


From lisas at salford-systems.com  Fri Jan 12 23:07:14 2007
From: lisas at salford-systems.com (Lisa Solomon)
Date: Fri, 12 Jan 2007 14:07:14 -0800
Subject: [R] Free Webinar: Vendor Neutral Intro to Data Mining for Absolute
	Beginners, January 26, 2007
Message-ID: <Q0NHQjlNRCZFT1QmIC8pMTU4ODgwNjg3@sspc-lisa>

ONLINE VENDOR NEUTRAL INTRO TO DATA MINING FOR ABSOLUTE BEGINNERS
(no charge)

A non-technical data mining introduction for absolute beginners
January 26, 2007, 10AM - 11AM PST
Future Sessions (May 23, June 14, Sept 7)
To register: contact lisas at salford-systems.com

This one-hour webinar is a perfect place to start if you are new to data mining and have little-to-no background in statistics or machine learning. 

In one hour, we will discuss:

**Data basics: what kind of data is required for data mining and predictive analytics; In what format must the data be; what steps are necessary to prepare data appropriately 

**What kinds of questions can we answer with data mining

**How data mining models work: the inputs, the outputs, and the nature of the predictive mechanism 

**Evaluation criteria: how predictive models can be assessed and their value measured 

**Specific background knowledge to prepare you to begin a data mining project.

Please do not hesitate to contact me if you have any questions or if you wish to register.

Sincerely,
Lisa Solomon
lisas at salford-systems.com


From martin.becker at mx.uni-saarland.de  Fri Jan 12 23:43:51 2007
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Fri, 12 Jan 2007 23:43:51 +0100
Subject: [R] R editor vs. Tinn-R
In-Reply-To: <6e8360ad0701121047v595bf9d0uf3aaeb326f8a3d47@mail.gmail.com>
References: <bd93cdad0701120608scdf1a91oe63906b69553e697@mail.gmail.com>
	<45A79DE5.9060803@mx.uni-saarland.de>
	<6e8360ad0701121047v595bf9d0uf3aaeb326f8a3d47@mail.gmail.com>
Message-ID: <20070112234351.pi48ftfogg4k44k0@webmail.rz.uni-saarland.de>

Zitat von BBands <bbands at gmail.com>:

> On 1/12/07, Martin Becker <martin.becker at mx.uni-saarland.de> wrote:
>> Depending on which button you press in Tinn-R, the clipboard is used to
>> transfer the commands to R, so sometimes you can't rely on the previous
>> contents of the clipboard while using Tinn-R. If you use the
>> "(source)"-versions of the buttons, the content of the clipboard should
>> be preserved.
>
> Something odd is going on. I can confirm that read.delim("clipboard")
> works from R, but not from Tinn-R. What seems odd is that the contents
> of the clipboard _are_ preserved after the Tinn-R send, so that a
> subsequent paste or read.delim("clipboard") from R works correctly.
> Perhaps Tinn-R restores the contents of the clipboard after sending to
> R such that the R command runs before the restore takes place?
>

Sorry, my mistake: Apparently only "Send file (source)" preserves the  
clipboard while passing the source-command to R.
Regards,
   Martin

>      jab
> --
> John Bollinger, CFA, CMT
> www.BollingerBands.com
>
> If you advance far enough, you arrive at the beginning.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From p.dalgaard at biostat.ku.dk  Sat Jan 13 00:27:30 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Sat, 13 Jan 2007 00:27:30 +0100
Subject: [R] overdispersion
In-Reply-To: <09CA0E49-A0A4-469F-BAF0-F80E8CD697BA@anu.edu.au>
References: <mailman.8.1168599603.21300.r-help@stat.math.ethz.ch>
	<09CA0E49-A0A4-469F-BAF0-F80E8CD697BA@anu.edu.au>
Message-ID: <45A81962.4050207@biostat.ku.dk>

John Maindonald wrote:
> I would say rather that for binary data (binomial data with n=1) it  
> is not possible to detect overdispersion from examination of the  
> Pearson chi-square or the deviance.   Overdispersion may be, and  
> often is, nevertheless present.  I am arguing that overdispersion is  
> properly regarded as a function of the variance-covariance structure,  
> not as a function of the sample data.
>
> The variance of a two-point distribution is a known function of the  
> mean, providing that independence and identity of distribution can be  
> assumed, or providing that the correlation structure is otherwise  
> known and the mean is constant. That proviso is crucial!
>   
I don't really disagree, of course. I was mainly being provocative.

However, these models play tricks on our intuition. When people speak of 
overdispersion, they usually imply just what you said: independent data 
with the correct mean, but somehow a different variance - a mathematical 
impossibility for binary data.

One particular thing to notice is that if the individual means are 
heterogeneous but sampled independently from the same underlying 
distribution; you still end up with a marginal binomial distribution. If 
they are not sampled independently, then you get departures from the 
binomial, but it may well be in the direction of underdispersion. For an 
extreme case, take a sample of 50 men and 50 women and count the number 
of people with breasts.

(If you do the same thing with a random sample of 100 _people_, you get 
the binomial distribution again. Unless you're counting the number of 
breasts...)
> If there is some sort of grouping, it may be appropriate to aggregate  
> data over the groups, yielding data that have a binomial form with  
> n>1.  Over-dispersion can now be detected from the Pearson chi-square  
> or from the deviance.  Note that the quasi models assume that the  
> multiplier for the binomial or other variance is constant with p;  
> that may or may not be realistic.  Generalized linear mixed models  
> make their own different assumptions about how the variance changes  
> as a function of p; again these may or may not be realistic.
>
> It is then the "error" structure that is crucial. To the extent that  
> distracts from careful thinking about that structure, the term  
> "overdispersion is unsatisfactory.
>
> There's no obvious way that I can see to supply glm() with an  
> estimate of the dispersion that has been derived independently of the  
> current analysis.  Especially in the binary case, this would  
> sometimes be useful.
>
> John Maindonald             email: john.maindonald at anu.edu.au
> phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
> Centre for Mathematics & Its Applications, Room 1194,
> John Dedman Mathematical Sciences Building (Building 27)
> Australian National University, Canberra ACT 0200.
>
>
> On 12 Jan 2007, at 10:00 PM, r-help-request at stat.math.ethz.ch wrote:
>
>   
>> From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
>> Date: 12 January 2007 5:04:26 AM
>> To: evaiannario <evaiannario at libero.it>
>> Cc: "r-help at stat.math.ethz.ch" <r-help at stat.math.ethz.ch>
>> Subject: Re: [R] overdispersion
>>
>>
>> evaiannario wrote:
>>     
>>> How can I eliminate the overdispersion for binary data apart the  
>>> use of the quasibinomial?
>>>       
>> There is no such thing as overdispersion for binary data. (The  
>> variance of a two-point distribution is a known function of the  
>> mean.) If what you want to do is include random effects of some  
>> sort of grouping then you might look into generalized linear mixed  
>> models via lmer() from the lme4 package or glmmPQL from MASS.
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From fjbuch at gmail.com  Sat Jan 13 00:50:14 2007
From: fjbuch at gmail.com (Farrel Buchinsky)
Date: Fri, 12 Jan 2007 18:50:14 -0500
Subject: [R] R editor vs. Tinn-R
In-Reply-To: <20070112191142.8qimphz7tsgkco8s@webmail.rz.uni-saarland.de>
References: <bd93cdad0701120608scdf1a91oe63906b69553e697@mail.gmail.com>
	<45A79DE5.9060803@mx.uni-saarland.de> <eo8h1f$vcl$1@sea.gmane.org>
	<20070112191142.8qimphz7tsgkco8s@webmail.rz.uni-saarland.de>
Message-ID: <bd93cdad0701121550s63c9e587u22b1dd4d3b4c8fcc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070112/9f626738/attachment.pl 

From jholtman at gmail.com  Sat Jan 13 02:01:56 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 12 Jan 2007 20:01:56 -0500
Subject: [R] wafer map drawing
In-Reply-To: <76E525F192FF454A9712272E207B714B513164@dlee12.ent.ti.com>
References: <76E525F192FF454A9712272E207B714B513164@dlee12.ent.ti.com>
Message-ID: <644e1f320701121701i7d8f5b8exca8a45fee5d1e316@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070112/c61e0130/attachment.pl 

From guillet at stat.ucl.ac.be  Sat Jan 13 02:34:43 2007
From: guillet at stat.ucl.ac.be (Alain Guillet)
Date: Sat, 13 Jan 2007 02:34:43 +0100
Subject: [R] Maximum likelihood acf
In-Reply-To: <Pine.LNX.4.64.0701121538050.7073@gannet.stats.ox.ac.uk>
References: <45A78B4C.1020702@stat.ucl.ac.be>
	<Pine.LNX.4.64.0701121401170.351@gannet.stats.ox.ac.uk>
	<45A7A482.6080203@stat.ucl.ac.be>
	<Pine.LNX.4.64.0701121538050.7073@gannet.stats.ox.ac.uk>
Message-ID: <45A83733.4040601@stat.ucl.ac.be>

In fact, I need it in the general case, not only for an ARMA process. 
Unfortunately, I have no reference to give so I will code it. Sorry for 
the trouble.

Alain



Prof Brian Ripley a ?crit :
> On Fri, 12 Jan 2007, Alain Guillet wrote:
>
>> Prof. Brian Ripley,
>>
>> You are right, my question was not clear.
>>
>> In fact, I want to estimate the k first components of the acf, i.e. I
>> want to estimate the k parameters (c(0),c(1),...c(k-1)), where c is the
>> autocorrelation function, by a maximum likelihood estimator.
>
> And does ARMAacf applied to the result of ar.mle not do just that?
> An accessible reference would help us, if not.
>
>>
>> Alain
>>
>>
>>
>> Prof Brian Ripley a ?crit :
>>> You will need to give us a reference, as the acf is not a parameter in
>>> a model in your description and MLEs apply to model parameters.
>>>
>>> Just possibly ar.mle is what you are looking for, perhaps plus ARMAacf?
>>>
>>> On Fri, 12 Jan 2007, Alain Guillet wrote:
>>>
>>>> Hello!
>>>>
>>>> I am looking for a function which computes the maximum likelihood
>>>> estimator of the autocorrelation function for a gaussian time series.
>>>> Does a such function already exist in R?
>>>> The estimator by default in R, acf(), uses the method of moments.
>>>>
>>>> Thanks a lot,
>>>> Alain
>>>>
>>>>
>>>>
>>>
>>
>>
>

-- 
Alain Guillet
Statistician and Computer Scientist

Institut de statistique - Universit? catholique de Louvain
Bureau d.126
Voie du Roman Pays, 20
B-1348 Louvain-la-Neuve
Belgium

tel: +32 10 47 30 50


From jim at bitwrit.com.au  Sat Jan 13 04:21:59 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sat, 13 Jan 2007 14:21:59 +1100
Subject: [R] quilt.plot
In-Reply-To: <3338.193.198.128.94.1168624811.squirrel@mail.dhz.hr>
References: <3338.193.198.128.94.1168624811.squirrel@mail.dhz.hr>
Message-ID: <45A85057.1040503@bitwrit.com.au>

Dunja Drvar wrote:
> Dear all,
> I have a problem with plotting.
> I have an irregular set of data, where latitude is listed from bigger to
> smaller values. So, I cannot plot with image.plot.
> On the other hand, quilt.plot solves the problem, but it can't take the
> definition of color. It gives the feedback:
> 
> Error in image.plot(out.p, col = tim.colors(64), ...) :
>         formal argument "col" matched by multiple actual arguments
> 
> The question is: is there a way to define the color palette in quilt.plot?

Hi Dunja,
I think the problem is due to the position of the ... argument. In 
image.plot it is first, meaning that all other arguments have to be 
named. Thus when you pass col= as part of ... in quilt.plot, it is 
placed in the initial ... in the call to image.plot, generating two col= 
arguments. I think the quickest hack is to insert an explicit col= 
argument into the code for quilt.plot and add it into the call to 
image.plot within the function as below.

function (x, y, z, nrow = 64, ncol = 64, grid = NULL, add.legend = TRUE,
    col=tim.colors(64), ...)
{
     x <- as.matrix(x)
     if (ncol(x) == 2) {
         z <- y
     }
     if (ncol(x) == 1) {
         x <- cbind(x, y)
     }
     if (ncol(x) == 3) {
         z <- x[, 3]
         x <- x[, 1:2]
     }
     out.p <-
      as.image(z, x = x, nrow = nrow, ncol = ncol, na.rm = TRUE)
     if (add.legend) {
         image.plot(out.p, col = col, ...)
     }
     else {
         image(out.p, col = col, ...)
     }
}

The modified example:

quilt.plot(ozone2$lon.lat, ozone2$y[16, ],
  add.legend = FALSE,col=rainbow(64))

works for me.

Jim


From ploua at allstate.com  Sat Jan 13 06:49:23 2007
From: ploua at allstate.com (Louisell, Paul)
Date: Fri, 12 Jan 2007 21:49:23 -0800
Subject: [R] R on UNIX Sun-Solaris 10.0 vs. S-Plus
Message-ID: <633AD4C78F3E8F489614A06990F6077B02027DD9@a0203-xpo0111-s.hodc.ad.allstate.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070112/c3e57fbc/attachment.pl 

From wangtong at usc.edu  Sat Jan 13 08:01:28 2007
From: wangtong at usc.edu (Tong Wang)
Date: Fri, 12 Jan 2007 23:01:28 -0800
Subject: [R] What command does the "cin >> " in R ?
Message-ID: <dd87ea704361.45a81348@usc.edu>

Hi all,
      Sorry about the simple question,  but I have searched the web with " prompt , input " etc.  and never got the answer .

thanks a lot

tong


From ligges at statistik.uni-dortmund.de  Sat Jan 13 12:12:14 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 13 Jan 2007 12:12:14 +0100
Subject: [R] R editor vs. Tinn-R
In-Reply-To: <bd93cdad0701121550s63c9e587u22b1dd4d3b4c8fcc@mail.gmail.com>
References: <bd93cdad0701120608scdf1a91oe63906b69553e697@mail.gmail.com>	<45A79DE5.9060803@mx.uni-saarland.de>
	<eo8h1f$vcl$1@sea.gmane.org>	<20070112191142.8qimphz7tsgkco8s@webmail.rz.uni-saarland.de>
	<bd93cdad0701121550s63c9e587u22b1dd4d3b4c8fcc@mail.gmail.com>
Message-ID: <45A8BE8E.40107@statistik.uni-dortmund.de>

Why do people post so much details about their *guesses* on R-help 
rather than looking into the sources or quickly asking the corresponding 
developers?

Anyway, before people start to mention and guess about RWinEdt:
RWinEdt uses the clipboard to transfer code from WinEdt to R, hence any 
Excel data copied to the clipboard will be lost after some code has been 
sent to R.

Uwe Ligges



Farrel Buchinsky wrote:
> I just do not understand this. You are going to be amazed by what I tell
> you.
> Firstly thank you for orienting me to the Tinn-R source buttons.
> 
> I selected the first two lines in my script that were as follows.
> 
> prelim<-read.delim("clipboard")
> str(prelim)
> 
> Then I went over to Excel and copied the cells I wanted to read
> 
> Then I went to Tinn-R and clicked on send selection (source) and this is
> what I got in the R Console
>> source(file('clipboard'))
> 'data.frame':   1 obs. of  1 variable:
>  $ prelim..read.delim.clipboard.: Factor w/ 1 level "str(prelim)": 1
> Warning message:
> incomplete final line found by readTableHeader on 'clipboard'
> It did not even do the second command and it turned my command from prelim<-
> read.delim("clipboard") to source(file('clipboard'))
> 
> I then repeated the whole exercise, using send selection button and got this
>> prelim<-read.delim("clipboard")
> Warning message:
> incomplete final line found by readTableHeader on 'clipboard'
>> str(prelim)
> 'data.frame':   1 obs. of  1 variable:
>  $ prelim..read.delim.clipboard.: Factor w/ 1 level "str(prelim)": 1
> 
> In disgust, I closed Tinn-R and opened the same script in R-editor, selected
> the first two lines and ran them and got the same junk
>> prelim<-read.delim("clipboard")
> Warning message:
> incomplete final line found by readTableHeader on 'clipboard'
>> str(prelim)
> 'data.frame':   1 obs. of  1 variable:
>  $ prelim..read.delim.clipboard.: Factor w/ 1 level "str(prelim)": 1
> 
> In desperation I resorted to typing the commands directly in the R console
> which worked as expected.
>> prelim<-read.delim("clipboard")
>> str(prelim)
> 'data.frame':   18 obs. of  13 variables:
>  $ tag       : int  742 742 749 749 747 747 745 745 727 727 ...
>  $ left      : int  1 0 1 0 1 0 1 0 1 0 ...
>  $ Day.1     : int  2 2 0 0 2 1 1 2 2 1 ...
>  $ Day.2     : int  2 2 0 2 3 1 1 2 3 2 ...
>  $ Day.3     : int  2 2 1 1 3 1 2 2 3 1 ...
>  $ Day.4     : int  2 2 1 1 3 1 1 2 3 2 ...
>  $ Day.5     : int  2 1 2 2 3 2 1 3 3 1 ...
>  $ Day.6     : int  3 2 2 2 3 2 0 3 2 2 ...
>  $ Day.7     : int  3 2 1 1 3 1 1 2 3 3 ...
>  $ Day.8     : int  3 2 2 1 3 3 3 3 3 2 ...
>  $ Day.9     : int  2 2 1 1 2 2 1 2 3 3 ...
>  $ Day.10    : int  2 2 2 2 3 3 1 2 3 2 ...
>  $ dissection: Factor w/ 7 levels "clear","little pus",..: 1 6 4 5 5 4 7 7 4
> 4 ...
> 
> What gives?
> What on earth is going on with the clipboard that only direct manipulation
> from the package works.
> It appears to me as if what is selected in the editor is being sent to the
> clipboard so that it can be put into the console and that in so doing it is
> knocking my Excel data from the number one position in the clipboard. Is
> this possible?
> 
> To check this I went back to the R-editor because I was sure that I had got
> this to work from the R-editor before.
> You cannot believe what I found
> If I ran the two lines by first selecting them and then hitting Ctrl-R I ran
> into the same problem. But if I simply left my cursor in the first line and
> hit Ctrl-R, then the first line was executed and the cursor automatically
> dropped to the next line in my script. I hit Ctrl-R again and then got the
> expected structure output. I find this amazing and difficult to believe. Any
> insights?
> 
> On 1/12/07, Martin Becker <martin.becker at mx.uni-saarland.de> wrote:
>> Zitat von Farrel Buchinsky <fjbuch at gmail.com>:
>>
>>> The only button I pressed was the "send line" button in version 1.19.1.5.
>> I
>>> changed my command to
>>>
>>> prelim<-read.delim(source("clipboard"))
>>>
>> The problem is not using the wrong R-command, but using a (Tinn-R)
>> button that leads to a replacement of the current (windows) clipboard...
>>
>>> and still got the same problem.
>>> I do not know how to use the source versions of the buttons. Can you
>> please
>>> tell me more?
>>>
>> There are three button groups on the left of the "Send line" button,
>> containing two buttons each. The left button of each of these three
>> button groups has the same name as the right button of each group,
>> apart from having a "(source)" suffix. Now it should be obvious what
>> was meant with '"(source)"-version of buttons'. There is no such
>> version for the "Send line" button, so you have to select the
>> corresponding line and use the "Send selection (source)" button, e.g.
>>
>>> Thanks
>>>
>>> Farrel
>>>
>> Regards,
>>   Martin
>>
>>> "Martin Becker" <martin.becker at mx.uni-saarland.de> wrote in message
>>> news:45A79DE5.9060803 at mx.uni-saarland.de...
>>>> Farrel Buchinsky wrote:
>>>>> Have you used Tinn-R and what landmines await the inexperienced?
>>>>>
>>>>>
>>>> Depending on which button you press in Tinn-R, the clipboard is used to
>>>> transfer the commands to R, so sometimes you can't rely on the previous
>>>> contents of the clipboard while using Tinn-R. If you use the
>>>> "(source)"-versions of the buttons, the content of the clipboard should
>>>> be preserved.
>>>>
>>>> Regards,
>>>>  Martin
>>>>
>>>>> I could not understand why a script that used to work stopped working.
>>>>> Look at these two scenarios
>>>>> I opened an excel spreadsheet and copied several cells to the
>> clipboard
>>>>> Then Scenario 1
>>>>> Executed from Tinn-R
>>>>>
>>>>>> prelim<-read.delim("clipboard")
>>>>>> str(prelim)
>>>>>>
>>>>> 'data.frame':   0 obs. of  1 variable:
>>>>>  $ prelim..read.delim.clipboard.: logi
>>>>>
>>>>> Scenario 2
>>>>> Executed from R editor
>>>>>
>>>>>> prelim<-read.delim("clipboard")
>>>>>> str(prelim)
>>>>>>
>>>>> 'data.frame':   18 obs. of  13 variables:
>>>>>
>>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>>
> 
>


From ligges at statistik.uni-dortmund.de  Sat Jan 13 12:26:52 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 13 Jan 2007 12:26:52 +0100
Subject: [R] What command does the "cin >> " in R ?
In-Reply-To: <3948d9e50701122311q618250a9g78c86cc679ba66fe@mail.gmail.com>
References: <dd87ea704361.45a81348@usc.edu>
	<3948d9e50701122311q618250a9g78c86cc679ba66fe@mail.gmail.com>
Message-ID: <45A8C1FC.80705@statistik.uni-dortmund.de>



talepanda wrote:
> try:
> 
> readLines(n=1)->str

1. Please do always use "<-" and spaces around it as in
    temp <- readLines(n=1)
rather than "->", it is *that* confusing!!!

2. It is always wise to avoid duplication of object names (and str() 
already is a function), even if it would work in this case.

Uwe Ligges


> 
> 
> On 1/13/07, Tong Wang <wangtong at usc.edu> wrote:
>> Hi all,
>>       Sorry about the simple question,  but I have searched the web with "
>> prompt , input " etc.  and never got the answer .
>>
>> thanks a lot
>>
>> tong
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Sat Jan 13 12:30:39 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 13 Jan 2007 12:30:39 +0100
Subject: [R] R on UNIX Sun-Solaris 10.0 vs. S-Plus
In-Reply-To: <633AD4C78F3E8F489614A06990F6077B02027DD9@a0203-xpo0111-s.hodc.ad.allstate.com>
References: <633AD4C78F3E8F489614A06990F6077B02027DD9@a0203-xpo0111-s.hodc.ad.allstate.com>
Message-ID: <45A8C2DF.2020300@statistik.uni-dortmund.de>

Louisell, Paul wrote:
> This is a general question to people who've installed R on a UNIX
> sparc-sun-solaris platform:
> 
> Have you had any issues related to maintaining R on this platform, e.g.,
> installations that didn't work, instances of R crashing and possibly
> requiring a new installation, etc?
> 
> I'm especially interested in anyone who has experience with both R and
> S-Plus on this OS. Is there any reason to prefer one over the other
> considering ONLY installation and updating issues? I don't have
> experience with R on UNIX, but I'm guessing that it's not much (if any)
> more difficult to maintain than S-Plus. I'd appreciate input from anyone
> who has experience.

In fact, I always found it much simpler to install R, because you can 
compile it yourself for your platform and it does not expect certain 
versions of certain libraries that might not be available on your platform.
Why not quickly try it out yourself and follow the instructions in the R 
  Installation and Administration manual?

Uwe Ligges


> Thanks,
> 
> Paul Louisell
> 650-833-6254
> ploua at allstate.com
> Research Associate (Statistician)
> Modeling & Data Analytics
> ARPC
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Sat Jan 13 12:40:11 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 13 Jan 2007 12:40:11 +0100
Subject: [R] R2WinBugs and Compare DIC versus BIC or AIC
In-Reply-To: <cbca975a0701121041m1d3a5365qaa9cf91288d1f5b3@mail.gmail.com>
References: <cbca975a0701121041m1d3a5365qaa9cf91288d1f5b3@mail.gmail.com>
Message-ID: <45A8C51B.5080403@statistik.uni-dortmund.de>



Dae-Jin Lee wrote:
> Dear All
> 
> 1)
> 
> I'm fitting spatial CAR models
> 
> using R2Winbugs and although everything seems to go reasonably well (or I
> think so)
> 
> the next message appears from WINBUGS 1.4 window:
> 
> gen.inits()
> Command #Bugs: gen.inits cannot be executed (is greyed out)

"greyed out" refers to the corresponding GUI. In this case it could mean 
that either your preparations before were not successful or you have 
already specified all initial values and no further have to be 
generated. But this is a WinBUGS question and not R related.

Perhaps you have transposed some matrix? This is always extremely 
confusing ...


> The question is if this message means that something is wrong and the
> results are consequently wrong, or Can I assume it as a simple warning
> message???...
> 
> I've tried to fit the model using just WinBugs (not calling form R) and
> gen.inits command is available and the results obtain are practically the
> same...
> 
> 2)
> 
> The other question is, once several bayesian CAR models are fitted I use DIC
> to model selection
> 
> How can I compare DIC with AIC or BIC obtain for a spatial glm? I mean for
> example:
> 
> - DIC of 300 and Pd of 32
> 
> versus
> 
> - BIC of 220 and Effective dimension (measured of trace of hat matrix) equal
> to 14
> 
> * I've read that DIC is intended as a generalisation of Akaike's Information
> Criterion (AIC), but is it possible to compare them simply looking which is
> the lowest???


Well they are intended to be compared (DIC with DIC, AIC with AIC, but 
do not mix them!) that way given you are comparing within the same class 
of models. And then, there is a bit of fortunetelling with all these 
information criterion, at least in my personal opinion (and I know that 
others disagree).

Uwe Ligges


> 
> Thanks,
> 
> Dae-Jin Lee
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mepstein at tvg-inc.com  Fri Jan 12 22:19:10 2007
From: mepstein at tvg-inc.com (Mike Epstein)
Date: Fri, 12 Jan 2007 16:19:10 -0500
Subject: [R] Aware of MaxDiff in R?
Message-ID: <65D115022DCEA6458A40A729AEF6DE5C9E3C7C@ftw-p-exch2.pdi-inc.pdii.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070112/365c00c4/attachment.pl 

From rcyranek at smail.uni-koeln.de  Fri Jan 12 23:16:16 2007
From: rcyranek at smail.uni-koeln.de (=?iso-8859-1?Q?Ren=E9_Cyranek?=)
Date: Fri, 12 Jan 2007 23:16:16 +0100
Subject: [R] Appropriate test for correlation of two discretely distributed
	variables
Message-ID: <200701122216.l0CMGMk4030367@smtp.uni-koeln.de>

Good evening!

I am looking for the right test to analyze my data. I have got more than
3,500 observations which have two dimensions: Age (in integers and ranging
from 14 to 70) and another parameter (also integers and ranging from 0 to
10).

What I want to test is, whether there is some correlation between these two
dimensions. But unfortunately I have no idea, which test to choose. Is there
a test, I can use for this?

Thanks and kind regards,
Ren?


From bbaker at TNC.ORG  Sat Jan 13 00:48:16 2007
From: bbaker at TNC.ORG (Barry Baker)
Date: Fri, 12 Jan 2007 16:48:16 -0700
Subject: [R] Magnitude of trend in time series
Message-ID: <C1CD6C50.3CCB%bbaker@tnc.org>

Hello,

I am analyzing some climate time series data using the Mann Kendall package
and was wondering if there was a way to calculate the trend using Sen's
nonparametric estimator slope in R?

Thank you in advance,
Barry
_________________________
Barry Baker, Ph.D.
Global Climate Change Initiative
The Nature Conservancy
2424 Spruce St., Suite 100
Boulder, CO 80302

Tel: (303)-541-0322
Fax: (303)-449-4328

http://nature.org/tncscience/scientists/misc/baker.html


From rhurlin at gwdg.de  Sat Jan 13 14:32:47 2007
From: rhurlin at gwdg.de (Rainer Hurling)
Date: Sat, 13 Jan 2007 14:32:47 +0100
Subject: [R] package 'rimage' install error on FreeBSD
Message-ID: <45A8DF7F.4020908@gwdg.de>

After not reaching the package maintainer (takashina at computer.org) I 
hope someone on R-help@ can help.


I tried to install rimage_0.5-7.tar.gz under FreeBSD-7.0 CURRENT as root
with the following command:

    R CMD INSTALL rimage_0.5-7.tar.gz

It stops with the following errors in the configure script:

    [...snip...]
    checking fftw.h usability... no
    checking fftw.h presence... no
    checking for fftw.h... no
    [...snip...]

On my system this header file is located under

    /usr/local/include/fftw.h

Different FreeBSD packages need different versions of fftw, so I have
three versions installed:

    fftw-2.1.5_4
    fftw3-3.1.2
    fftw3-float-3.1.2

Is it possible that this is the reason for not finding the header?

The installation works well, when giving configure-args with the paths:

R CMD INSTALL --configure-args="--with-fftw-include=/usr/local/include/ 
--with-jpeg-include=/usr/local/include/" rimage_0.5-7.tar.gz

Anyone else having this problem on FreeBSD?


Many thanks in advance,

Rainer Hurling


From dr.mike at ntlworld.com  Sat Jan 13 15:10:26 2007
From: dr.mike at ntlworld.com (mike waters)
Date: Sat, 13 Jan 2007 14:10:26 -0000
Subject: [R] Magnitude of trend in time series
In-Reply-To: <C1CD6C50.3CCB%bbaker@tnc.org>
References: <C1CD6C50.3CCB%bbaker@tnc.org>
Message-ID: <006201c7371c$969e30b0$0200a8c0@C400XP>

 Rand Wilcox has produced a set of functions for S-Plus and (IIRC) R, which
includes Theil-Sen regression.

The following url gives a pdf to the workshop that they were designed to
accompany, along with instructions on how to source and use them.

http://psychology.usc.edu/rwilcox/workshop.pdf 

The following url is to his homepage which gives the above link plus those
to the function files themselves.

http://psychology.usc.edu/faculty_homepage.php?id=43


HTH

Regards

Mike

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Barry Baker
Sent: 12 January 2007 23:48
To: r-help at stat.math.ethz.ch
Subject: [R] Magnitude of trend in time series

Hello,

I am analyzing some climate time series data using the Mann Kendall package
and was wondering if there was a way to calculate the trend using Sen's
nonparametric estimator slope in R?

Thank you in advance,
Barry
_________________________
Barry Baker, Ph.D.
Global Climate Change Initiative
The Nature Conservancy
2424 Spruce St., Suite 100
Boulder, CO 80302

Tel: (303)-541-0322
Fax: (303)-449-4328

http://nature.org/tncscience/scientists/misc/baker.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From algorithms at gmx.de  Sat Jan 13 19:20:10 2007
From: algorithms at gmx.de (algorithms at gmx.de)
Date: Sat, 13 Jan 2007 19:20:10 +0100
Subject: [R] Definition of t-value
Message-ID: <20070113182010.234950@gmx.net>

Hello,

I'd like to ask for the exact definition of the t-value, which R uses in its summaries of a linear model for judging the importance of an independent variable in explaining the dependent variable.
I searched the documentation, some groups, and the web for quite a long time, but the best I could come up with is the following from

www.answers.com/topic/value 

which reads:

Measure of the statistical significance of an independent variable b in explaining the dependent variable y. It is determined by dividing the estimated regression coefficient b by its standard error Sb. That is

t-Value = b/Sb

Thus, the t-statistic measures how many standard errors the coefficient is away from zero. Generally, any t-value greater than +2 or less than - 2 is acceptable. The higher the t-value, the greater the confidence we have in the coefficient as a predictor. Low t-values are indications of low reliability of the predictive power of that coefficient.


My problem is that I do not know how to compute the standard error Sb of some regression coefficient, when I have done nothing more than to use the lm command in this manner:

Out = lm(A~ data$B + data$C + data$D)


Does anyone know in detail, how R computes the t-value displayed in summaries?


Thank you very much,

Peter
--


From Dimitris.Rizopoulos at med.kuleuven.be  Sat Jan 13 20:14:37 2007
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitrios Rizopoulos)
Date: Sat, 13 Jan 2007 20:14:37 +0100
Subject: [R] Definition of t-value
In-Reply-To: <20070113182010.234950@gmx.net>
References: <20070113182010.234950@gmx.net>
Message-ID: <20070113201437.0085d3egorcw0wwg@webmail5.kuleuven.be>

try this

Out <-  lm(A ~ data$B + data$C + data$D)
summary(Out)

moreover, by typing 'summary.lm' in your R console you may see how the  
t-values are computed; check also ?summary.lm.

Another way, though less efficient, to obtain the standard errors is  
the following

summ.Out <- summary(Out)
X <- model.matrix(Out) # the design matrix
var.betas <- solve(crossprod(X)) * summ.Out$sigma^2
# standard errors
sqrt(diag(var.betas))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting algorithms at gmx.de:

> Hello,
>
> I'd like to ask for the exact definition of the t-value, which R   
> uses in its summaries of a linear model for judging the importance   
> of an independent variable in explaining the dependent variable.
> I searched the documentation, some groups, and the web for quite a   
> long time, but the best I could come up with is the following from
>
> www.answers.com/topic/value
>
> which reads:
>
> Measure of the statistical significance of an independent variable b  
>  in explaining the dependent variable y. It is determined by  
> dividing  the estimated regression coefficient b by its standard  
> error Sb.  That is
>
> t-Value = b/Sb
>
> Thus, the t-statistic measures how many standard errors the   
> coefficient is away from zero. Generally, any t-value greater than   
> +2 or less than - 2 is acceptable. The higher the t-value, the   
> greater the confidence we have in the coefficient as a predictor.   
> Low t-values are indications of low reliability of the predictive   
> power of that coefficient.
>
>
> My problem is that I do not know how to compute the standard error   
> Sb of some regression coefficient, when I have done nothing more   
> than to use the lm command in this manner:
>
> Out = lm(A~ data$B + data$C + data$D)
>
>
> Does anyone know in detail, how R computes the t-value displayed in   
> summaries?
>
>
> Thank you very much,
>
> Peter
> --
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From h.wickham at gmail.com  Sat Jan 13 20:59:09 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sat, 13 Jan 2007 13:59:09 -0600
Subject: [R] Magnitude of trend in time series
In-Reply-To: <C1CD6C50.3CCB%bbaker@tnc.org>
References: <C1CD6C50.3CCB%bbaker@tnc.org>
Message-ID: <f8e6ff050701131159u46d16419pb1f9e2b251a70680@mail.gmail.com>

> I am analyzing some climate time series data using the Mann Kendall package
> and was wondering if there was a way to calculate the trend using Sen's
> nonparametric estimator slope in R?

I think you can do it with the mblm package.  But I vaguely remember
that the results didn't look reasonable.  I've included some
simulation/testing code that I used below.

b1 <- 5
x <- 1:20 - mean(1:20)
y <- 0 + b1 * x

n1 <- function() y + rnorm(20, 0, 2)
ln1 <- function() y + log(rnorm(20, 0, 0.7))
ln2 <- function() y + log(rnorm(20, 0, 1.5))

fitlm <- function(f=n1) {
  m <- lm(n1() ~ x)
  ci <- t(sapply(c(0.9, 0.95, 0.99), function(x) confint(m, level=x)[2,]))
  b1 > ci[,1] & b1 < ci[,2]
}
fitts <- function(f=n1) {
  x <- x # because mblm mucks up the environments
  y2 <- n1()
  m <- mblm(y2 ~ x, repeated=FALSE)
  ci <- t(sapply(c(0.9, 0.95, 0.99), function(x) confint(m, level=x)[2,]))
  b1 > ci[,1] & b1 < ci[,2]
}

n1lm <- replicate(1000, fitlm(n1))
n1ts <- replicate(100, fitts(n1))
ln1lm <- replicate(1000, fitlm(ln1))
ln1ts <- replicate(100, fitts(ln1))
ln2lm <- replicate(1000, fitlm(ln2))
ln2ts <- replicate(100, fitts(ln2))


Hadley


From algorithms at gmx.de  Sat Jan 13 21:03:24 2007
From: algorithms at gmx.de (Peter Heinig)
Date: Sat, 13 Jan 2007 21:03:24 +0100
Subject: [R] Definition of t-value
In-Reply-To: <20070113201437.0085d3egorcw0wwg@webmail5.kuleuven.be>
References: <20070113182010.234950@gmx.net>
	<20070113201437.0085d3egorcw0wwg@webmail5.kuleuven.be>
Message-ID: <20070113200324.111090@gmx.net>

Thank you for the suggestion to use summary.lm. It does help.
Best Regards,
Peter
--


From Achim.Zeileis at wu-wien.ac.at  Sat Jan 13 21:11:37 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Sat, 13 Jan 2007 21:11:37 +0100 (CET)
Subject: [R] Definition of t-value
In-Reply-To: <20070113182010.234950@gmx.net>
Message-ID: <Pine.LNX.4.44.0701132106350.14164-100000@disco.wu-wien.ac.at>

On Sat, 13 Jan 2007 algorithms at gmx.de wrote:

> My problem is that I do not know how to compute the standard error Sb of some regression coefficient, when I have done nothing more than to use the lm command in this manner:
>
> Out = lm(A~ data$B + data$C + data$D)

better make that
  Out <- lm(A ~ B + C + D, data = mydata)
and then
  summary(Out)
computes the usual t statistics. See the source code for what is exactly
computed. Several components of the summary also have their own extractor
function, e.g.
  coef(Out)
  vcov(Out)
extract the estimated coefficients and covariance matrix respectively.
Thus, you can compute the t statistics by hand via
  coef(Out) / sqrt(diag(vcov(Out)))

hth,
Z


From jeff.hamann at forestinformatics.com  Sat Jan 13 21:16:20 2007
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Sat, 13 Jan 2007 12:16:20 -0800 (PST)
Subject: [R] comments in Sweave
Message-ID: <1746.128.193.139.129.1168719380.squirrel@www.forestinformatics.com>

Has anyone been able to include R comments in Sweave docs so that the
comments aren't deleted?


-- 
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon 97339-1421


From mcarr052 at uottawa.ca  Sat Jan 13 21:37:18 2007
From: mcarr052 at uottawa.ca (mcarr052 at uottawa.ca)
Date: Sat, 13 Jan 2007 15:37:18 -0500 (EST)
Subject: [R] graph results of logistic regression
Message-ID: <49600.74.101.183.83.1168720638.squirrel@web16.uottawa.ca>

I would like to graphically represent the results from a matched-pairs
logistic regression. I am looking to represent each variable seperately in
a graph, displaying the probability of selection for each. My study is
looking at habitat selection in turtles so I am comparing habitat
variables to turtle locations compared to paired random points
(differences in habitat).

Consulting previous questions in R help archives I found a somewhat
similar request in september of 2005.
A response suggested some representation ideas using R code in a Bulletin
of the ESA 
<http://www.esapubs.org/bulletin/backissues/086-1/bulletinjan2005.htm#et>
(with a correction included in Erratum from volume 86(2)
http://www.esapubs.org/bulletin/backissues/086-2/et_bulletin86_2print.pdf 
As explained the code only tries to ?efficiently? represent raw data in
the traditional logistic plot that most ecologist are familiar with.
However the code which is plot.logi.hist is not recognized when I tried
and I could not find it in any of the packages. I am wondering how to run
this code (ie- which package it might be in).

Any help would be greatly appreciated.


From murdoch at stats.uwo.ca  Sat Jan 13 21:59:55 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 13 Jan 2007 15:59:55 -0500
Subject: [R] comments in Sweave
In-Reply-To: <1746.128.193.139.129.1168719380.squirrel@www.forestinformatics.com>
References: <1746.128.193.139.129.1168719380.squirrel@www.forestinformatics.com>
Message-ID: <45A9484B.3080804@stats.uwo.ca>

On 1/13/2007 3:16 PM, Jeff D. Hamann wrote:
> Has anyone been able to include R comments in Sweave docs so that the
> comments aren't deleted?

This has been fixed in R-devel (to become 2.5.0 next spring).  You need 
to specify the "keep.source=TRUE" Sweave option, either in the chunk, or 
in the file as a whole.

Duncan Murdoch


From jeff.hamann at forestinformatics.com  Sat Jan 13 22:13:27 2007
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Sat, 13 Jan 2007 13:13:27 -0800 (PST)
Subject: [R] comments in Sweave
In-Reply-To: <45A9484B.3080804@stats.uwo.ca>
References: <1746.128.193.139.129.1168719380.squirrel@www.forestinformatics.com>
	<45A9484B.3080804@stats.uwo.ca>
Message-ID: <1767.128.193.139.129.1168722807.squirrel@www.forestinformatics.com>

Sa-weeet! Does next spring mean in a couple of months?

-- 
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon 97339-1421

> On 1/13/2007 3:16 PM, Jeff D. Hamann wrote:
>> Has anyone been able to include R comments in Sweave docs so that the
>> comments aren't deleted?
>
> This has been fixed in R-devel (to become 2.5.0 next spring).  You need
> to specify the "keep.source=TRUE" Sweave option, either in the chunk, or
> in the file as a whole.
>
> Duncan Murdoch
>


From murdoch at stats.uwo.ca  Sat Jan 13 23:13:42 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 13 Jan 2007 17:13:42 -0500
Subject: [R] comments in Sweave
In-Reply-To: <1767.128.193.139.129.1168722807.squirrel@www.forestinformatics.com>
References: <1746.128.193.139.129.1168719380.squirrel@www.forestinformatics.com>
	<45A9484B.3080804@stats.uwo.ca>
	<1767.128.193.139.129.1168722807.squirrel@www.forestinformatics.com>
Message-ID: <45A95996.3000809@stats.uwo.ca>

On 1/13/2007 4:13 PM, Jeff D. Hamann wrote:
> Sa-weeet! Does next spring mean in a couple of months?

Probably sometime in this coming April.

Duncan Murdoch


From wangtong at usc.edu  Sun Jan 14 04:14:02 2007
From: wangtong at usc.edu (Tong Wang)
Date: Sat, 13 Jan 2007 19:14:02 -0800
Subject: [R] Questions about paste and assign
Message-ID: <dd3aaa7632a5.45a92f7a@usc.edu>

Hi,
    I would like to assign a value to a member b  of the list  a  in position 3,   by calling:  

                                            assign( target, 2.34, 3)

My question is what the "target" should be.   I tried target <- paste("a", $, "b")   and something else, 
but haven't got the right answer yet.  

BTW, if I attached a list named  "mylist" in position 3,  to refer to this environment, besides 3, what's the 
right name ?

Thanks a lot for any help

tong


From skiadas at hanover.edu  Sun Jan 14 05:12:00 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Sat, 13 Jan 2007 23:12:00 -0500
Subject: [R] Controlling size of boxplot when it is added in a plot
Message-ID: <01493A00-1651-41BF-9868-6ED42610634F@hanover.edu>

Greetings,

	I am trying to add a boxplot to the bottom of a histogram, right  
between the histogram bars and the x axis. Here is the code I am  
using at the moment (the par line is probably not relevant for our  
discussion):

hs <- hist(x, breaks = 20, plot = F)
par(mar = c(3,3,2,1))
hist(x, breaks = 20, main = NULL, ylim = c(-2, max(hs$counts)))
boxplot(x, horizontal = T, axes = T, add = T, at = -1)

The problem is the following. As it is, the boxplot restricts itself  
to the -1 line. I would like it to occupy both the -1 and the -2  
lines ( I guess more generally I would like to control how much  
vertical space the "embedded boxplot" occupies). I tried to set the  
width parameter in the boxplot, but that seemed to have no effect at  
all.

On an OT note, I haven't seen this way of combining a histogram with  
a boxplot (perhaps I haven't looked really hard). I thought it would  
be useful for my students to see them next to each other, to develop  
a feeling for what histograms might correspond to what boxplots. Is  
there perhaps some reason why I should avoid showing those graphs to  
them like that, that I am not aware of? Or just a reason why I  
haven't seen them combined like this much?

TIA

Charilaos Skiadas
Department of Mathematics
Hanover College
P.O.Box 108
Hanover, IN 47243


From ggrothendieck at gmail.com  Sun Jan 14 05:42:04 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 13 Jan 2007 23:42:04 -0500
Subject: [R] Questions about paste and assign
In-Reply-To: <dd3aaa7632a5.45a92f7a@usc.edu>
References: <dd3aaa7632a5.45a92f7a@usc.edu>
Message-ID: <971536df0701132042q5dfdb16dmdf74042f8a1ab400@mail.gmail.com>

Try this:

> a <- list(a = 1, b = 2, c = 3)
> assign("a", replace(get("a"), 3, 2.34))
> a
$a
[1] 1

$b
[1] 2

$c
[1] 2.34

On 1/13/07, Tong Wang <wangtong at usc.edu> wrote:
> Hi,
>    I would like to assign a value to a member b  of the list  a  in position 3,   by calling:
>
>                                            assign( target, 2.34, 3)
>
> My question is what the "target" should be.   I tried target <- paste("a", $, "b")   and something else,
> but haven't got the right answer yet.
>
> BTW, if I attached a list named  "mylist" in position 3,  to refer to this environment, besides 3, what's the
> right name ?
>
> Thanks a lot for any help
>
> tong
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dkaplan at education.wisc.edu  Sun Jan 14 06:17:17 2007
From: dkaplan at education.wisc.edu (David Kaplan)
Date: Sat, 13 Jan 2007 23:17:17 -0600
Subject: [R] Newbie question about saving results
Message-ID: <45A9BCDD.2010906@education.wisc.edu>

Hi all,

When I run a procedure and the results are printed to the console, is 
there a way to just save the results?  When I save to file, it also 
saves the syntax of the procedure.  Thanks in advance,

David


-- 
=======================================================================
David Kaplan, Ph.D.
Professor
Department of Educational Psychology
University of Wisconsin - Madison
Educational Sciences, Room 1061
1025 W. Johnson Street
Madison, WI 53706

email: dkaplan at education.wisc.edu
Web:   http://www.education.wisc.edu/edpsych/facstaff/kaplan/kaplan.htm
Phone: 608-262-0836
Fax:   608-262-0843


From murdoch at stats.uwo.ca  Sun Jan 14 06:25:25 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 14 Jan 2007 00:25:25 -0500
Subject: [R] Newbie question about saving results
In-Reply-To: <45A9BCDD.2010906@education.wisc.edu>
References: <45A9BCDD.2010906@education.wisc.edu>
Message-ID: <45A9BEC5.4010408@stats.uwo.ca>

On 1/14/2007 12:17 AM, David Kaplan wrote:
> Hi all,
> 
> When I run a procedure and the results are printed to the console, is 
> there a way to just save the results?  When I save to file, it also 
> saves the syntax of the procedure.  Thanks in advance,

You can redirect results to a file, using the sink() function, e.g.

sink("results.txt")
1+1

will print the answer in the file.  Use sink() to redirect output back 
to the console.

Duncan Murdoch


From ccleland at optonline.net  Sun Jan 14 11:50:40 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Sun, 14 Jan 2007 05:50:40 -0500
Subject: [R] Controlling size of boxplot when it is added in a plot
In-Reply-To: <01493A00-1651-41BF-9868-6ED42610634F@hanover.edu>
References: <01493A00-1651-41BF-9868-6ED42610634F@hanover.edu>
Message-ID: <45AA0B00.5000202@optonline.net>

Charilaos Skiadas wrote:
> Greetings,
> 
> 	I am trying to add a boxplot to the bottom of a histogram, right  
> between the histogram bars and the x axis. Here is the code I am  
> using at the moment (the par line is probably not relevant for our  
> discussion):
> 
> hs <- hist(x, breaks = 20, plot = F)
> par(mar = c(3,3,2,1))
> hist(x, breaks = 20, main = NULL, ylim = c(-2, max(hs$counts)))
> boxplot(x, horizontal = T, axes = T, add = T, at = -1)
> 
> The problem is the following. As it is, the boxplot restricts itself  
> to the -1 line. I would like it to occupy both the -1 and the -2  
> lines ( I guess more generally I would like to control how much  
> vertical space the "embedded boxplot" occupies). I tried to set the  
> width parameter in the boxplot, but that seemed to have no effect at  
> all.

  Try setting the boxwex argument instead:

par(mfrow=c(1,2))

set.seed(54321)
x <- rnorm(100)
hs <- hist(x, breaks = 20, plot = F)
hist(x, breaks = 20, main = NULL, ylim = c(-2, max(hs$counts)))
boxplot(x, horizontal = T, axes = T, add = T, at = -1, boxwex = 1)

hist(x, breaks = 20, main = NULL, ylim = c(-2, max(hs$counts)))
boxplot(x, horizontal = T, axes = T, add = T, at = -1, boxwex = 3)

> On an OT note, I haven't seen this way of combining a histogram with  
> a boxplot (perhaps I haven't looked really hard). I thought it would  
> be useful for my students to see them next to each other, to develop  
> a feeling for what histograms might correspond to what boxplots. Is  
> there perhaps some reason why I should avoid showing those graphs to  
> them like that, that I am not aware of? Or just a reason why I  
> haven't seen them combined like this much?
> 
> TIA
> 
> Charilaos Skiadas
> Department of Mathematics
> Hanover College
> P.O.Box 108
> Hanover, IN 47243
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code. 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From dickgiesser at gmail.com  Sun Jan 14 12:20:58 2007
From: dickgiesser at gmail.com (Benjamin Dickgiesser)
Date: Sun, 14 Jan 2007 11:20:58 +0000
Subject: [R] ks.test not working?
Message-ID: <b75d67340701140320x1691c476n6f59666db73890cc@mail.gmail.com>

Hi,

I am trying the following:

library(ismev)
library(evd)

fit <- gev.fit(x,show=FALSE)
ks.test(x,pgev,fit$mle[1],fit$mle[2],fit$mle[3])


but I am getting:
Warning message:
cannot compute correct p-values with ties in: ks.test(x, pgev,
fit$mle[1], fit$mle[2], fit$mle[3])

where x is:
 [1]  239   38    1   43   22    1    5    9   15    6    1    9  156
 25    3  100    6
 [18]    5  100   10  103   25    5    1    2   11    8   68   13  154
  67   12    5   15
 [35]    5  130   47  143  176  573  592  213   54   10   17    9  198
 293   77   11   44
 [52]    6   22    2   10    8    1    2   16    4   70   12    4    7
 134   41  515    8
 [69]  200  169    2   13   49  218   48   34   74   19   44  128    6
  96  238   17  167
 [86]  308  204   41    6   32   77   14   62   10    3    6    4    2
   1    1    1    4
[103]   22   15   13   12   34   14   13    3    1    1    2    2   52
  34    6    9   31
[120]  342  348    2    7   52   39   79    5   88  238   40  294   69
 878   75    1    6
[137]    5  381   58   84  588  345  161 1293    6  403  516   16    1
1112   54  381    2
[154]  526   38   17   20   17  800  189    1   57   90   92   16   17
  31   11    4   17
[171]   12    9   10   46   14   23    1    1    1  313


Can anyone tell me why that could be?

Thank you very much,
Benjamin


From ligges at statistik.uni-dortmund.de  Sun Jan 14 12:30:42 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 14 Jan 2007 12:30:42 +0100
Subject: [R] ks.test not working?
In-Reply-To: <b75d67340701140320x1691c476n6f59666db73890cc@mail.gmail.com>
References: <b75d67340701140320x1691c476n6f59666db73890cc@mail.gmail.com>
Message-ID: <45AA1462.5030905@statistik.uni-dortmund.de>



Benjamin Dickgiesser wrote:
> Hi,
> 
> I am trying the following:
> 
> library(ismev)
> library(evd)
> 
> fit <- gev.fit(x,show=FALSE)
> ks.test(x,pgev,fit$mle[1],fit$mle[2],fit$mle[3])

1. The test *is* working. It simply warns (and does not report an error) 
that you have ties in your data and the assumption for calculating 
p-values do not hold.
2. The KS tests tends to be very conservative given you specify 
estimated parameters of the distribution.

Uwe Ligges

> 
> but I am getting:
> Warning message:
> cannot compute correct p-values with ties in: ks.test(x, pgev,
> fit$mle[1], fit$mle[2], fit$mle[3])
> 
> where x is:
>  [1]  239   38    1   43   22    1    5    9   15    6    1    9  156
>  25    3  100    6
>  [18]    5  100   10  103   25    5    1    2   11    8   68   13  154
>   67   12    5   15
>  [35]    5  130   47  143  176  573  592  213   54   10   17    9  198
>  293   77   11   44
>  [52]    6   22    2   10    8    1    2   16    4   70   12    4    7
>  134   41  515    8
>  [69]  200  169    2   13   49  218   48   34   74   19   44  128    6
>   96  238   17  167
>  [86]  308  204   41    6   32   77   14   62   10    3    6    4    2
>    1    1    1    4
> [103]   22   15   13   12   34   14   13    3    1    1    2    2   52
>   34    6    9   31
> [120]  342  348    2    7   52   39   79    5   88  238   40  294   69
>  878   75    1    6
> [137]    5  381   58   84  588  345  161 1293    6  403  516   16    1
> 1112   54  381    2
> [154]  526   38   17   20   17  800  189    1   57   90   92   16   17
>   31   11    4   17
> [171]   12    9   10   46   14   23    1    1    1  313
> 
> 
> Can anyone tell me why that could be?
> 
> Thank you very much,
> Benjamin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bhs2 at mevik.net  Sun Jan 14 12:59:20 2007
From: bhs2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Sun, 14 Jan 2007 12:59:20 +0100
Subject: [R] PCA (prcomp) details info.
In-Reply-To: <001b01c73672$d20c94c0$0cd3cc89@fsavor> (Francesco Savorani's
	message of "Fri, 12 Jan 2007 18:55:16 +0100")
References: <001b01c73672$d20c94c0$0cd3cc89@fsavor>
Message-ID: <m0sledq0o7.fsf@bar.nemo-project.org>

Francesco Savorani wrote:

> I'm handling a matrix dataset composed by a number of variables much
> higher than the objects (900 vs 100) and performing a prcomp
> (centered and scaled) PCA on it. What I get is a Loadings (rotation)
> matrix limited by my lower number of objects and thus 900x100
> instead of 900x900. If I try to manually calculate the matrix scores
> multiplying the original variables (centered and scaled) for such a
> loadings matrix I cannot obtain the same values calculated by R and
> stored on the prcomp$x matrix (100x100).

This works for me:

M <- matrix(rnorm(900*100), ncol = 900)
pca <- prcomp(M, scale = TRUE)
S <- scale(M) %*% pca$rotation
all.equal(S, pca$x) ## => TRUE

-- 
Bj?rn-Helge Mevik


From ewisdom at gmail.com  Sun Jan 14 14:37:10 2007
From: ewisdom at gmail.com (Teng Sun)
Date: Sun, 14 Jan 2007 08:37:10 -0500
Subject: [R] merging two lists but get indexes
Message-ID: <9f3ee8d70701140537n12896db6n452e20bfb3c54bb1@mail.gmail.com>

Suppose I have two columns of entries, how can I get the union of the
two columns? Please note: I input my columns through excel. These
entries have text format in excel. Also, out of curiosity, how can I
find out the data type of a data frame ?


> a <- read.csv("book1.csv")
> a
      n1     n2
1  apple   soda
2 orange  apple
3   soda  green
4    red yellow
5  white   blue
6         white

> union(a$n1,a$n2)
[1] 2 3 5 4 6 1

I want the actual names instead of the indexes.


From ccleland at optonline.net  Sun Jan 14 15:40:56 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Sun, 14 Jan 2007 09:40:56 -0500
Subject: [R] merging two lists but get indexes
In-Reply-To: <9f3ee8d70701140537n12896db6n452e20bfb3c54bb1@mail.gmail.com>
References: <9f3ee8d70701140537n12896db6n452e20bfb3c54bb1@mail.gmail.com>
Message-ID: <45AA40F8.6070802@optonline.net>

Teng Sun wrote:
> Suppose I have two columns of entries, how can I get the union of the
> two columns? Please note: I input my columns through excel. These
> entries have text format in excel. Also, out of curiosity, how can I
> find out the data type of a data frame ?

df <- data.frame(n1 = c("apple","orange","soda","red","white",""),
                 n2 = c("soda","apple","green","yellow","blue","white"),
                 x = rnorm(6))

> str(df)
'data.frame':   6 obs. of  3 variables:
 $ n1: Factor w/ 6 levels "","apple","orange",..: 2 3 5 4 6 1
 $ n2: Factor w/ 6 levels "apple","blue",..: 4 1 3 6 2 5
 $ x : num  -0.0932 -2.0714 -0.9539  0.7249 -0.7039 ...

> lapply(df, class)
$n1
[1] "factor"

$n2
[1] "factor"

$x
[1] "numeric"

>> a <- read.csv("book1.csv")
>> a
>       n1     n2
> 1  apple   soda
> 2 orange  apple
> 3   soda  green
> 4    red yellow
> 5  white   blue
> 6         white
> 
>> union(a$n1,a$n2)
> [1] 2 3 5 4 6 1
> 
> I want the actual names instead of the indexes.

  You are getting the union of factor levels rather than the union of
the strings.  Try this:

> union(as.character(df$n1), as.character(df$n2))

[1] "apple"  "orange" "soda"
[4] "red"    "white"  ""
[7] "green"  "yellow" "blue"

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ggrothendieck at gmail.com  Sun Jan 14 15:55:50 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 14 Jan 2007 09:55:50 -0500
Subject: [R] merging two lists but get indexes
In-Reply-To: <9f3ee8d70701140537n12896db6n452e20bfb3c54bb1@mail.gmail.com>
References: <9f3ee8d70701140537n12896db6n452e20bfb3c54bb1@mail.gmail.com>
Message-ID: <971536df0701140655ga648069w667fded7498cf55b@mail.gmail.com>

Try:

a <- read.csv("book1.csv", as.is = TRUE)

to read the columns in as "character" class rather than the default of
"factor" .
See ?read.csv

On 1/14/07, Teng Sun <ewisdom at gmail.com> wrote:
> Suppose I have two columns of entries, how can I get the union of the
> two columns? Please note: I input my columns through excel. These
> entries have text format in excel. Also, out of curiosity, how can I
> find out the data type of a data frame ?
>
>
> > a <- read.csv("book1.csv")
> > a
>      n1     n2
> 1  apple   soda
> 2 orange  apple
> 3   soda  green
> 4    red yellow
> 5  white   blue
> 6         white
>
> > union(a$n1,a$n2)
> [1] 2 3 5 4 6 1
>
> I want the actual names instead of the indexes.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From joris.dewolf at cropdesign.com  Sun Jan 14 19:15:43 2007
From: joris.dewolf at cropdesign.com (joris.dewolf at cropdesign.com)
Date: Sun, 14 Jan 2007 19:15:43 +0100
Subject: [R] changes in the structure of mer objects?
Message-ID: <OF3662A524.5B0E8987-ONC1257263.0062FA7F-C1257263.00645147@basf-c-s.be>


Dear all,

I try to run the example of lmer and get the following error message.

> library(lme4)
> example(lmer)
lmer> (fm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy))
[[1]]
Error in get(x, envir, mode, inherits) : variable "as.dpoMatrix" was not
found

This error message is similar to what I get with other models. It looks
like the mer class has a slightly different structure. Anybody an idea how
to solve this?


I am using R 2.4.1 under linux and the latest releases of lme4 and Matrix

lme4_0.9975-10
Matrix_0.9975-8

> version
               _
platform       x86_64-unknown-linux-gnu
arch           x86_64
os             linux-gnu
system         x86_64, linux-gnu
status
major          2
minor          4.1
year           2006
month          12
day            18
svn rev        40228
language       R
version.string R version 2.4.1 (2006-12-18)

Thanks

Joris De Wolf
Phone: +32 9 2429155, E-Mail: joris.dewolf at cropdesign.com
Postal Address: CropDesign N.V. Technologiepark 3, 9052 Gent Belgium


From jenny197806 at yahoo.se  Sun Jan 14 21:58:04 2007
From: jenny197806 at yahoo.se (Jenny persson)
Date: Sun, 14 Jan 2007 21:58:04 +0100 (CET)
Subject: [R] extracting data into different  subsets
Message-ID: <20070114205804.58927.qmail@web28009.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: inte tillg?nglig
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070114/756160fa/attachment.pl 

From jholtman at gmail.com  Sun Jan 14 22:32:07 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 14 Jan 2007 16:32:07 -0500
Subject: [R] extracting data into different subsets
In-Reply-To: <20070114205804.58927.qmail@web28009.mail.ukl.yahoo.com>
References: <20070114205804.58927.qmail@web28009.mail.ukl.yahoo.com>
Message-ID: <644e1f320701141332r7addadb0lba6779aff64f2a92@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070114/1b8ee738/attachment.pl 

From skiadas at hanover.edu  Sun Jan 14 22:33:54 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Sun, 14 Jan 2007 16:33:54 -0500
Subject: [R] Controlling size of boxplot when it is added in a plot
In-Reply-To: <03CB0263-07D5-478C-8AB0-0B7BCD9EF6FE@hanover.edu>
References: <01493A00-1651-41BF-9868-6ED42610634F@hanover.edu>
	<45AA0B00.5000202@optonline.net>
	<03CB0263-07D5-478C-8AB0-0B7BCD9EF6FE@hanover.edu>
Message-ID: <2A213893-4F68-4E52-B701-4427FB8469DC@hanover.edu>

Sorry, meant to send this to the list.

On Jan 14, 2007, at 4:28 PM, Charilaos Skiadas wrote:

> On Jan 14, 2007, at 5:50 AM, Chuck Cleland wrote:
>
>>   Try setting the boxwex argument instead:
>
> Thanks Chuck, that does indeed seem to work pretty well. I'm not  
> quite sure what the best way to determine an appropriate size for  
> the boxplot would be, but the following kind of works, at least for  
> the cases I tried. Though I'm not entirely happy with it. And I'm  
> sure I've made a bunch of errors along the way, that someone more  
> experienced in R could spot easily. Feel free to criticize the  
> code. the boxwex default I guess is probably terribly named. One  
> over it is supposed to be the size of the boxplot over the size of  
> the histogram.
>
> force.odd <- function(x) {
> 	x + 1 - x %% 2;
> }
> boxhist <- function(x, boxwex = 8, ...) {
> 	hs <- hist(x, breaks = 20, plot = F)
> 	space <- force.odd(max(floor(hs$counts / boxwex), 1))
> 	plot(hs, main = NULL, ylim = c(-space, max(hs$counts)), ...)
> 	boxplot(x, horizontal = T, axes = T, add = T, at = -space/2,  
> boxwex = space)
> }
> x <- rweibull(300,1,1); boxhist(x)
>
> Haris

Haris


From adrian at maths.uwa.edu.au  Mon Jan 15 07:05:48 2007
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Mon, 15 Jan 2007 14:05:48 +0800
Subject: [R] [R-pkgs] spatstat 1.11-0
Message-ID: <17835.6588.479880.317926@maths.uwa.edu.au>


	spatstat 1.11-0

Version 1.11-0 of package 'spatstat' is now available.

Spatstat is a package for the analysis of spatial data, 
mostly point pattern data. See <www.spatstat.org>

Important changes in version 1.11-0:

	New format for ppm objects (objects in old format are still handled).
	More stringent checking of function arguments.
	Improved handling of pixellation effects.
	Extensions to variance-covariance calculations for fitted models.

Adrian Baddeley and Rolf Turner

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From Philippe.Grosjean at umh.ac.be  Mon Jan 15 08:45:00 2007
From: Philippe.Grosjean at umh.ac.be (Philippe Grosjean)
Date: Mon, 15 Jan 2007 08:45:00 +0100
Subject: [R] How to detect if R is running on Mac OS X?
Message-ID: <45AB30FC.7060501@umh.ac.be>

This question is probably trivial, but I don't find the answer. I have 
code that is different for Windows, Unix/Linux and Mac OSX. The man page 
of .Platform tells that .Platform$OS.type is the right way to test for 
it... but it also tels that it returns either "windows" or "unix". Is 
Mac OS X reported as "unix"? If yes, how do I make the difference?
Thanks,

Philippe Grosjean


From javalkon at hytti.uku.fi  Mon Jan 15 09:20:19 2007
From: javalkon at hytti.uku.fi (Jarimatti Valkonen)
Date: Mon, 15 Jan 2007 10:20:19 +0200
Subject: [R] How to detect if R is running on Mac OS X?
In-Reply-To: <45AB30FC.7060501@umh.ac.be>
References: <45AB30FC.7060501@umh.ac.be>
Message-ID: <45AB3943.6090405@hytti.uku.fi>

Philippe Grosjean wrote:
> This question is probably trivial, but I don't find the answer. I have 
> code that is different for Windows, Unix/Linux and Mac OSX. The man page 
> of .Platform tells that .Platform$OS.type is the right way to test for 
> it... but it also tels that it returns either "windows" or "unix". Is 
> Mac OS X reported as "unix"? If yes, how do I make the difference?

At least the R GUI reports .Platform$OS.type as "unix" on Mac OS X. 
Sys.info() gives some additional information:

 > Sys.info()["sysname"]
  sysname
"Darwin"

Note that not all Darwins are necessarily Mac OS X. The R GUI has 
.Platform$GUI = "AQUA", which I believe is unique to Mac platform. Then 
again, I have no idea what R installed from sources, MacPorts or Fink 
reports. The GUI is probably different.

Also note that the Mac OS 10.4 has BSD inside by default, so you may get 
away with the same code for Unix/Linux and Mac.


HTH,
Jarimatti Valkonen


From f_bresson at yahoo.fr  Mon Jan 15 10:21:40 2007
From: f_bresson at yahoo.fr (Florent Bresson)
Date: Mon, 15 Jan 2007 09:21:40 +0000 (GMT)
Subject: [R] Kernel density output
Message-ID: <20070115092140.89487.qmail@web26813.mail.ukl.yahoo.com>

Hi, 

I'm using the density() command for a given vector x and I would like to know how to get the estimated value of the density for each element of the vector x instead of values corresponding to points from a grid.

Thanks

Florent Bresson


From martin.becker at mx.uni-saarland.de  Mon Jan 15 10:36:16 2007
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Mon, 15 Jan 2007 10:36:16 +0100
Subject: [R] Kernel density output
In-Reply-To: <20070115092140.89487.qmail@web26813.mail.ukl.yahoo.com>
References: <20070115092140.89487.qmail@web26813.mail.ukl.yahoo.com>
Message-ID: <45AB4B10.5050404@mx.uni-saarland.de>

Florent Bresson schrieb:
> Hi, 
>
> I'm using the density() command for a given vector x and I would like to know how to get the estimated value of the density for each element of the vector x instead of values corresponding to points from a grid.
>
>   

Maybe not the best/most efficient way to do this, but

   splinefun(density(x))(x)

may work for you.
> Thanks
>
> Florent Bresson
>
>   
Regards,

  Martin
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Mon Jan 15 10:39:39 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Jan 2007 09:39:39 +0000 (GMT)
Subject: [R] How to detect if R is running on Mac OS X?
In-Reply-To: <45AB30FC.7060501@umh.ac.be>
References: <45AB30FC.7060501@umh.ac.be>
Message-ID: <Pine.LNX.4.64.0701150928060.8289@gannet.stats.ox.ac.uk>

I think there are two different questions here:

1) Is R running under darwin, the underlying OS of MacOS X?  You can test 
that by Sys.info or R.version$platform.  It seems unlikely that you would 
need this.   We do need to distinguish darwin at C level, as it is a very 
unusual 'unix', a much-modified version of FreeBSD.  One thing that
does show up at R level is the use of DYLD_LIBRARY_PATH rather than 
LD_LIBRARY_PATH.

2) Is R runnng as part of the R.app GUI?  For that, test .Platform$GUI (it 
is I believe "AQUA" iff R.app is in use).


On Mon, 15 Jan 2007, Philippe Grosjean wrote:

> This question is probably trivial, but I don't find the answer. I have
> code that is different for Windows, Unix/Linux and Mac OSX. The man page
> of .Platform tells that .Platform$OS.type is the right way to test for
> it... but it also tels that it returns either "windows" or "unix". Is
> Mac OS X reported as "unix"? If yes, how do I make the difference?
> Thanks,
>
> Philippe Grosjean

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Mon Jan 15 10:41:15 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Jan 2007 09:41:15 +0000 (GMT)
Subject: [R] Kernel density output
In-Reply-To: <20070115092140.89487.qmail@web26813.mail.ukl.yahoo.com>
References: <20070115092140.89487.qmail@web26813.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.64.0701150940350.8289@gannet.stats.ox.ac.uk>

You can use approx() to interpolate from the grid.

On Mon, 15 Jan 2007, Florent Bresson wrote:

> I'm using the density() command for a given vector x and I would like to 
> know how to get the estimated value of the density for each element of 
> the vector x instead of values corresponding to points from a grid.
>
> Thanks
>
> Florent Bresson

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From TobiasBr at Taquanta.com  Mon Jan 15 11:05:15 2007
From: TobiasBr at Taquanta.com (Brandt, T. (Tobias))
Date: Mon, 15 Jan 2007 12:05:15 +0200
Subject: [R] Problem with pdf, png,
	jpeg devices and files named CON on Window	s
Message-ID: <A77412E534FCD248A93A81F37CC75B7A0723293F@waxbill.africa.nedcor.net>

Hi
 
I cannot seem to create any files that have the name "CON" before the file
extension, i.e. all of the following fail:
 
> pdf("CON.pdf")
Error in pdf("CON.pdf") : unable to start device pdf
> jpeg('CON.jpeg')
Error in jpeg("CON.jpeg") : unable to start device devWindows
> png('CON.png')
Error in png("CON.png") : unable to start device devWindows
> sink('CON.txt')
Error in file(file, ifelse(append, "a", "w")) : 
        unable to open connection
> 

Any filename other than this works fine.
 
> png('ICON.png')
> dev.off()
null device 
          1 
> png('CON1.png')
> dev.off()
null device 
          1 
> sink('log.txt')
> sink()
> 

Remembering that in the old DOS days, commands like 'copy CON myfile.bat'
were common, I suspected that this has something to do with CON being a
reserved word so I tried to create the files manually in Windows explorer.
Creating files named 'CON.png', 'CON.pdf', 'CON.txt' all work fine. However
I did notice that Windows won't let me create a file named just "CON".
However since the devices that I'm trying to create have extensions, I don't
understand why they fail.  
 
Who can help?
 
Thanks
Tobias
 
 
> version
               _                         
platform       i386-pc-mingw32           
arch           i386                      
os             mingw32                   
system         i386, mingw32             
status                                   
major          2                         
minor          3.1                       
year           2006                      
month          06                        
day            01                        
svn rev        38247                     
language       R                         
version.string Version 2.3.1 (2006-06-01)
> 

> Sys.info()
                      sysname                       release
version                      nodename 
                    "Windows"                      "NT 5.1" "(build 2600)
Service Pack 2"            
                      machine                         login
user 
                        "x86"                                        
> 

********************
Nedbank Limited Reg No 1951/000009/06. The following link displays the names of the Nedbank Board of Directors and Company Secretary. [ http://www.nedbank.co.za/terms/DirectorsNedbank.htm ]
This email is confidential and is intended for the addressee only. The following link will take you to Nedbank's legal notice. [ http://www.nedbank.co.za/terms/EmailDisclaimer.htm ]


From wl at eimb.ru  Mon Jan 15 11:15:53 2007
From: wl at eimb.ru (Vladimir Eremeev)
Date: Mon, 15 Jan 2007 10:15:53 +0000 (UTC)
Subject: [R]
	=?utf-8?q?Problem_with_pdf=2C_png=2C_jpeg_devices_and_files_n?=
	=?utf-8?q?amed_CON_on_Window=09s?=
References: <A77412E534FCD248A93A81F37CC75B7A0723293F@waxbill.africa.nedcor.net>
Message-ID: <loom.20070115T111421-601@post.gmane.org>

Looks like it's a windows' problem.
You also cannot create a file with the name "con" and any extension from the 
command line.
Try 
  echo something > con.txt


From hstevens at muohio.edu  Mon Jan 15 11:34:48 2007
From: hstevens at muohio.edu (MHH Stevens)
Date: Mon, 15 Jan 2007 05:34:48 -0500
Subject: [R] Conflict in .Rprofile documentation FAQ vs. Help?
Message-ID: <1B51D8F5-D2B0-4669-B623-61898974BE6E@muohio.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070115/8f0f683f/attachment.pl 

From sekhon at berkeley.edu  Mon Jan 15 12:42:06 2007
From: sekhon at berkeley.edu (Jasjeet Singh Sekhon)
Date: Mon, 15 Jan 2007 03:42:06 -0800
Subject: [R] ks.test not working?
In-Reply-To: <b75d67340701140320x1691c476n6f59666db73890cc@mail.gmail.com>
References: <b75d67340701140320x1691c476n6f59666db73890cc@mail.gmail.com>
Message-ID: <17835.26766.864949.771771@lapo.berkeley.edu>


> cannot compute correct p-values with ties in: ks.test(x, pgev,
> fit$mle[1], fit$mle[2], fit$mle[3])

You may want to use the ks.boot function in the Matching package which
implements a bootstrap ks-test which provides consistent pvalues
(achieved significance levels) when there are ties.

Cheers,
Jas.

=======================================
Jasjeet S. Sekhon                     
                                      
Associate Professor             
Survey Research Center          
UC Berkeley                     

http://sekhon.berkeley.edu/
V: 510-642-9974  F: 617-507-5524
=======================================

Benjamin Dickgiesser writes:
 > Hi,
 > 
 > I am trying the following:
 > 
 > library(ismev)
 > library(evd)
 > 
 > fit <- gev.fit(x,show=FALSE)
 > ks.test(x,pgev,fit$mle[1],fit$mle[2],fit$mle[3])
 > 
 > 
 > but I am getting:
 > Warning message:
 > cannot compute correct p-values with ties in: ks.test(x, pgev,
 > fit$mle[1], fit$mle[2], fit$mle[3])
 > 
 > where x is:
 >  [1]  239   38    1   43   22    1    5    9   15    6    1    9  156
 >  25    3  100    6
 >  [18]    5  100   10  103   25    5    1    2   11    8   68   13  154
 >   67   12    5   15
 >  [35]    5  130   47  143  176  573  592  213   54   10   17    9  198
 >  293   77   11   44
 >  [52]    6   22    2   10    8    1    2   16    4   70   12    4    7
 >  134   41  515    8
 >  [69]  200  169    2   13   49  218   48   34   74   19   44  128    6
 >   96  238   17  167
 >  [86]  308  204   41    6   32   77   14   62   10    3    6    4    2
 >    1    1    1    4
 > [103]   22   15   13   12   34   14   13    3    1    1    2    2   52
 >   34    6    9   31
 > [120]  342  348    2    7   52   39   79    5   88  238   40  294   69
 >  878   75    1    6
 > [137]    5  381   58   84  588  345  161 1293    6  403  516   16    1
 > 1112   54  381    2
 > [154]  526   38   17   20   17  800  189    1   57   90   92   16   17
 >   31   11    4   17
 > [171]   12    9   10   46   14   23    1    1    1  313
 > 
 > 
 > Can anyone tell me why that could be?
 > 
 > Thank you very much,
 > Benjamin
 > 
 >


From afer at broadpark.no  Mon Jan 15 13:17:53 2007
From: afer at broadpark.no (Andrea Ferrandi)
Date: Mon, 15 Jan 2007 13:17:53 +0100
Subject: [R] Matrex R adapter released
Message-ID: <3a1afc9a4d95.45ab7f01@broadpark.no>

Matrex (http://matrex.sourceforge.net/) is a spreadsheet equivalent
application that calculates formulas with matrices parameters, not
cells.
Matrex has charts and presentations (spreadsheets) and is stable and
multithreaded.

The 12th of january we released MatrexR, that allows to have
Matrex as front end of R for matrices calculations.

You can download both Matrex and MatrexR from
https://sourceforge.net/projects/matrex/.


From milton_ruser at yahoo.com.br  Mon Jan 15 13:26:16 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Mon, 15 Jan 2007 12:26:16 +0000 (GMT)
Subject: [R] batch job GLM calculations
In-Reply-To: <FE8C160D1505B24497FA7C78D4DADACA0478B5@EA-MAIL.eawag.wroot.emp-eaw.ch>
Message-ID: <20070115122616.28863.qmail@web56605.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070115/bd3e18e3/attachment.pl 

From maechler at stat.math.ethz.ch  Mon Jan 15 14:00:12 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 15 Jan 2007 14:00:12 +0100
Subject: [R] changes in the structure of mer objects?
In-Reply-To: <OF3662A524.5B0E8987-ONC1257263.0062FA7F-C1257263.00645147@basf-c-s.be>
References: <OF3662A524.5B0E8987-ONC1257263.0062FA7F-C1257263.00645147@basf-c-s.be>
Message-ID: <17835.31452.843541.793475@stat.math.ethz.ch>

Hi Joris,

I suspect you somehow load an older version lme4 or Matrix than
you think you are loading.
Or then you have an lmer() function or a class definition 
{from a saved workspace ????}
in your work space.

example(lmer) *must* run correctly for the 'lme4' package to get
onto CRAN at all,
hence it must be unique to your setup.

Maybe as a first step,
run R as "R --vanilla" ?

Regards,
Martin Maechler, ETH Zurich

>>>>> "joris" == joris dewolf <joris.dewolf at cropdesign.com>
>>>>>     on Sun, 14 Jan 2007 19:15:43 +0100 writes:

    joris> Dear all,

    joris> I try to run the example of lmer and get the following error message.

    >> library(lme4)
    >> example(lmer)
    lmer> (fm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy))
    joris> [[1]]
    joris> Error in get(x, envir, mode, inherits) : variable "as.dpoMatrix" was not
    joris> found

    joris> This error message is similar to what I get with
    joris> other models. It looks like the mer class has a
    joris> slightly different structure. Anybody an idea how to
    joris> solve this?


    joris> I am using R 2.4.1 under linux and the latest releases of lme4 and Matrix

    joris> lme4_0.9975-10
    joris> Matrix_0.9975-8

    >> version
    joris> _
    joris> platform       x86_64-unknown-linux-gnu
    joris> arch           x86_64
    joris> os             linux-gnu
    joris> system         x86_64, linux-gnu
    joris> status
    joris> major          2
    joris> minor          4.1
    joris> year           2006
    joris> month          12
    joris> day            18
    joris> svn rev        40228
    joris> language       R
    joris> version.string R version 2.4.1 (2006-12-18)


From maechler at stat.math.ethz.ch  Mon Jan 15 14:08:51 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 15 Jan 2007 14:08:51 +0100
Subject: [R] Controlling size of boxplot when it is added in a plot
In-Reply-To: <01493A00-1651-41BF-9868-6ED42610634F@hanover.edu>
References: <01493A00-1651-41BF-9868-6ED42610634F@hanover.edu>
Message-ID: <17835.31971.513030.709083@stat.math.ethz.ch>

>>>>> "Charilaos" == Charilaos Skiadas <skiadas at hanover.edu>
>>>>>     on Sat, 13 Jan 2007 23:12:00 -0500 writes:

    Charilaos> I am trying to add a boxplot to the bottom of a
    Charilaos> histogram, right between the histogram bars and
    Charilaos> the x axis. Here is the code I am using at the
    Charilaos> moment (the par line is probably not relevant for
    Charilaos> our discussion):

    Charilaos> hs <- hist(x, breaks = 20, plot = F)
    Charilaos> par(mar = c(3,3,2,1))
    Charilaos> hist(x, breaks = 20, main = NULL, ylim = c(-2, max(hs$counts)))
    Charilaos> boxplot(x, horizontal = T, axes = T, add = T, at = -1)

    Charilaos> The problem is the following. 
    Charilaos> ........................

    Charilaos> On an OT note, I haven't seen this way of
    Charilaos> combining a histogram with a boxplot (perhaps I
    Charilaos> haven't looked really hard). I thought it would
    Charilaos> be useful for my students to see them next to
    Charilaos> each other, to develop a feeling for what
    Charilaos> histograms might correspond to what boxplots. Is
    Charilaos> there perhaps some reason why I should avoid
    Charilaos> showing those graphs to them like that, that I am
    Charilaos> not aware of? Or just a reason why I haven't seen
    Charilaos> them combined like this much?

The 'sfsmisc' package (on CRAN) has been containing a function 
called  hist.bxp()   which does these plots,
for a very long time --- actually for almost a longer time than R
exists. It was part of our internal collection of S-plus
"goodies" since 1995.

Martin Maechler, ETH Zurich


From Thierry.ONKELINX at inbo.be  Mon Jan 15 14:12:22 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 15 Jan 2007 14:12:22 +0100
Subject: [R] graph results of logistic regression
In-Reply-To: <49600.74.101.183.83.1168720638.squirrel@web16.uottawa.ca>
Message-ID: <2E9C414912813E4EB981326983E0A10402772FA9@inboexch.inbo.be>

The code of the function plot.logi.hist is defined in appendix A of the
article you referred to.

Cheers,

Thierry
------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens mcarr052 op uottawa.ca
Verzonden: zaterdag 13 januari 2007 21:37
Aan: r-help op stat.math.ethz.ch
Onderwerp: [R] graph results of logistic regression

I would like to graphically represent the results from a matched-pairs
logistic regression. I am looking to represent each variable seperately
in
a graph, displaying the probability of selection for each. My study is
looking at habitat selection in turtles so I am comparing habitat
variables to turtle locations compared to paired random points
(differences in habitat).

Consulting previous questions in R help archives I found a somewhat
similar request in september of 2005.
A response suggested some representation ideas using R code in a
Bulletin
of the ESA 
<http://www.esapubs.org/bulletin/backissues/086-1/bulletinjan2005.htm#et
>
(with a correction included in Erratum from volume 86(2)
http://www.esapubs.org/bulletin/backissues/086-2/et_bulletin86_2print.pd
f 
As explained the code only tries to "efficiently" represent raw data in
the traditional logistic plot that most ecologist are familiar with.
However the code which is plot.logi.hist is not recognized when I tried
and I could not find it in any of the packages. I am wondering how to
run
this code (ie- which package it might be in).

Any help would be greatly appreciated.

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From joris.dewolf at cropdesign.com  Mon Jan 15 14:37:24 2007
From: joris.dewolf at cropdesign.com (joris.dewolf at cropdesign.com)
Date: Mon, 15 Jan 2007 14:37:24 +0100
Subject: [R] changes in the structure of mer objects?
In-Reply-To: <17835.31452.843541.793475@stat.math.ethz.ch>
Message-ID: <OF84A72616.35F7E684-ONC1257264.0049F6B0-C1257264.004AD56C@basf-c-s.be>

Thanks Martin,

R --vanilla did the trick.
By the way, is there a way to check which version of a lme4 or Matrix I am
using in a certain instance of R?

We are running multiple versions of R on the same server until we are sure
all our operational code is behaving well under a new version of R or of a
package.
Now I start doubting if we ever have been testing what we were intendng to
test...

Joris


r-help-bounces at stat.math.ethz.ch wrote on 15/01/2007 14:00:12:

> Hi Joris,
>
> I suspect you somehow load an older version lme4 or Matrix than
> you think you are loading.
> Or then you have an lmer() function or a class definition
> {from a saved workspace ????}
> in your work space.
>
> example(lmer) *must* run correctly for the 'lme4' package to get
> onto CRAN at all,
> hence it must be unique to your setup.
>
> Maybe as a first step,
> run R as "R --vanilla" ?
>
> Regards,
> Martin Maechler, ETH Zurich
>
> >>>>> "joris" == joris dewolf <joris.dewolf at cropdesign.com>
> >>>>>     on Sun, 14 Jan 2007 19:15:43 +0100 writes:
>
>     joris> Dear all,
>
>     joris> I try to run the example of lmer and get the following
> error message.
>
>     >> library(lme4)
>     >> example(lmer)
>     lmer> (fm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy))
>     joris> [[1]]
>     joris> Error in get(x, envir, mode, inherits) : variable "as.
> dpoMatrix" was not
>     joris> found
>
>     joris> This error message is similar to what I get with
>     joris> other models. It looks like the mer class has a
>     joris> slightly different structure. Anybody an idea how to
>     joris> solve this?
>
>
>     joris> I am using R 2.4.1 under linux and the latest releases of
> lme4 and Matrix
>
>     joris> lme4_0.9975-10
>     joris> Matrix_0.9975-8
>
>     >> version
>     joris> _
>     joris> platform       x86_64-unknown-linux-gnu
>     joris> arch           x86_64
>     joris> os             linux-gnu
>     joris> system         x86_64, linux-gnu
>     joris> status
>     joris> major          2
>     joris> minor          4.1
>     joris> year           2006
>     joris> month          12
>     joris> day            18
>     joris> svn rev        40228
>     joris> language       R
>     joris> version.string R version 2.4.1 (2006-12-18)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From huber at ebi.ac.uk  Mon Jan 15 14:42:26 2007
From: huber at ebi.ac.uk (Wolfgang Huber)
Date: Mon, 15 Jan 2007 13:42:26 +0000
Subject: [R] Advanced course R programming and Bioconductor in Cambridge UK
	30.3.+1.4.
In-Reply-To: <098B617579F1AC4696C59B41A8A01AAA0104093F@hhlmail.hinxton.wellcome.ac.uk>
References: <098B617579F1AC4696C59B41A8A01AAA01040892@hhlmail.hinxton.wellcome.ac.uk>
	<45AB5DFD.90303@ebi.ac.uk>
	<098B617579F1AC4696C59B41A8A01AAA0104093F@hhlmail.hinxton.wellcome.ac.uk>
Message-ID: <45AB84C2.1040108@ebi.ac.uk>

Dear R users & developers,

Seth Falcon and Martin Morgan are teaching

Advanced R Programming and Bioconductor 30 March - 1 April 2007 at the
Wellcome Trust Genome Campus, Hinxton, Cambridge, UK.

This two-day course focuses on programming skills required to develop
software for statistical analysis of high-throughput genomic data using
R and Bioconductor.  Lectures and practical sessions introduce the
diversity of packages already available for analysis of data, and
present the tools and techniques participants need for effectively
implementing their own analyses.

Closing date for applications: 7th February 2007

Further information and details of how to apply can be found on our
website: www.wellcome.ac.uk/advancedcourses and
http://www.wellcome.ac.uk/doc_WTX035299.html



Best wishes
------------------------------------------------------------------
Wolfgang Huber  EBI/EMBL  Cambridge UK  http://www.ebi.ac.uk/huber


From dickgiesser at gmail.com  Mon Jan 15 14:42:34 2007
From: dickgiesser at gmail.com (Benjamin Dickgiesser)
Date: Mon, 15 Jan 2007 13:42:34 +0000
Subject: [R] How to format R code in LaTex documents
Message-ID: <b75d67340701150542m43aa4d2rb0af6fd6b022cbf0@mail.gmail.com>

Hi,

I am planning on putting some R script in an appendix of a LaTex
document. Can anyone recommend me a way of how to format it? Is there
a way to keep all line breaks without having to insert \\ in every
single line?

Thank you!
Benjamin


From f.harrell at vanderbilt.edu  Mon Jan 15 15:12:33 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 15 Jan 2007 08:12:33 -0600
Subject: [R] How to format R code in LaTex documents
In-Reply-To: <b75d67340701150542m43aa4d2rb0af6fd6b022cbf0@mail.gmail.com>
References: <b75d67340701150542m43aa4d2rb0af6fd6b022cbf0@mail.gmail.com>
Message-ID: <45AB8BD1.5000508@vanderbilt.edu>

Benjamin Dickgiesser wrote:
> Hi,
> 
> I am planning on putting some R script in an appendix of a LaTex
> document. Can anyone recommend me a way of how to format it? Is there
> a way to keep all line breaks without having to insert \\ in every
> single line?
> 
> Thank you!
> Benjamin

Here's one way and I would appreciate anyone's improvements.  I've also 
included solutions from two others.  Please let me know what you decide 
to use.  -Frank

\usepackage{listings,relsize}
\lstloadlanguages{R}
\lstset{language=R,basicstyle=\smaller[2],commentstyle=\rmfamily\smaller,
  showstringspaces=false,%
  xleftmargin=4ex,literate={<-}{{$\leftarrow$}}1 {~}{{$\sim$}}1}
\lstset{escapeinside={(*}{*)}}   % for (*\ref{ }*) inside lstlistings (S 
code)

. . .
\begin{lstlisting}
. . . S code . . .
\end{lstlisting}

The following code was provided by Vincent Goulet:


listings is a great package to highlight R keywords and comments and --- 
that
was my main use of the package --- index those keywords. I found that I had
to slightly redefine the list of keywords included in listings. I still did
not take the time to submit a patch to the author, though...

In any case, here's what I use, if it can be of any help to anyone:

\lstloadlanguages{R}
\lstdefinelanguage{Renhanced}[]{R}{%
   morekeywords={acf,ar,arima,arima.sim,colMeans,colSums,is.na,is.null,%
     mapply,ms,na.rm,nlmin,replicate,row.names,rowMeans,rowSums,seasonal,%
     sys.time,system.time,ts.plot,which.max,which.min},
   deletekeywords={c},
   alsoletter={.\%},%
   alsoother={:_\$}}
\lstset{language=Renhanced,extendedchars=true,
   basicstyle=\small\ttfamily,
   commentstyle=\textsl,
   keywordstyle=\mdseries,
   showstringspaces=false,
   index=[1][keywords],
   indexstyle=\indexfonction}

with

   \newcommand{\indexfonction}[1]{\index{#1@\texttt{#1}}}

-- Vincent Goulet, Associate Professor ?cole d'actuariat Universit? 
Laval, Qu?bec Vincent.Goulet at act.ulaval.ca http://vgoulet.act.ulaval.ca

Anupam Tyagi provided the following:

\documentclass{report}
\usepackage{listings}
\begin{document}

Somethings .....

\lstset{% general command to set parameter(s)
basicstyle=\small, % print whole in small
stringstyle=\ttfamily, % typewriter type for strings
numbers=left, % numbers on the left
numberstyle=\tiny, % Tiny numbers
stepnumber=2, % number every second line of code
numbersep=5pt, % 5pt seperation between numbering and code listing
language=R }

\lstinputlisting{text1.R}

\end{document}


From ripley at stats.ox.ac.uk  Mon Jan 15 15:13:28 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Jan 2007 14:13:28 +0000 (GMT)
Subject: [R] Problem with pdf, png,
 jpeg devices and files named CON on Window s
In-Reply-To: <A77412E534FCD248A93A81F37CC75B7A0723293F@waxbill.africa.nedcor.net>
References: <A77412E534FCD248A93A81F37CC75B7A0723293F@waxbill.africa.nedcor.net>
Message-ID: <Pine.LNX.4.64.0701151408320.23439@gannet.stats.ox.ac.uk>

This is a restriction of the ISO C interface to the Windows API.  E.g. 
'check' says

     ## Furthermore, Uwe Ligges says that Windows still does not allow
     ## the following DOS device names (by themselves or with possible
     ## extensions):
     ##
     ## Name    Function
     ## ----    --------
     ## CON     Keyboard and display
     ## PRN     System list device, usually a parallel port
     ## AUX     Auxiliary device, usually a serial port
     ## CLOCK$  System real-time clock
     ## NUL     Bit-bucket device
     ## COM1    First serial communications port
     ## COM2    Second serial communications port
     ## COM3    Third serial communications port
     ## COM4    Fourth serial communications port
     ## LPT1    First parallel printer port
     ## LPT2    Second parallel printer port
     ## LPT3    Third parallel printer port

Windows Explorer seems to have a different set of rules, disallowing a lot 
of valid names (e.g. .Rprofile) but allowing some invalid ones.

On Mon, 15 Jan 2007, Brandt, T. (Tobias) wrote:

> Hi
>
> I cannot seem to create any files that have the name "CON" before the file
> extension, i.e. all of the following fail:
>
>> pdf("CON.pdf")
> Error in pdf("CON.pdf") : unable to start device pdf
>> jpeg('CON.jpeg')
> Error in jpeg("CON.jpeg") : unable to start device devWindows
>> png('CON.png')
> Error in png("CON.png") : unable to start device devWindows
>> sink('CON.txt')
> Error in file(file, ifelse(append, "a", "w")) :
>        unable to open connection
>>
>
> Any filename other than this works fine.
>
>> png('ICON.png')
>> dev.off()
> null device
>          1
>> png('CON1.png')
>> dev.off()
> null device
>          1
>> sink('log.txt')
>> sink()
>>
>
> Remembering that in the old DOS days, commands like 'copy CON myfile.bat'
> were common, I suspected that this has something to do with CON being a
> reserved word so I tried to create the files manually in Windows explorer.
> Creating files named 'CON.png', 'CON.pdf', 'CON.txt' all work fine. However
> I did notice that Windows won't let me create a file named just "CON".
> However since the devices that I'm trying to create have extensions, I don't
> understand why they fail.
>
> Who can help?
>
> Thanks
> Tobias
>
>
>> version
>               _
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          3.1
> year           2006
> month          06
> day            01
> svn rev        38247
> language       R
> version.string Version 2.3.1 (2006-06-01)
>>
>
>> Sys.info()
>                      sysname                       release
> version                      nodename
>                    "Windows"                      "NT 5.1" "(build 2600)
> Service Pack 2"
>                      machine                         login
> user
>                        "x86"
>>
>
> ********************
> Nedbank Limited Reg No 1951/000009/06. The following link displays the names of the Nedbank Board of Directors and Company Secretary. [ http://www.nedbank.co.za/terms/DirectorsNedbank.htm ]
> This email is confidential and is intended for the addressee only. The following link will take you to Nedbank's legal notice. [ http://www.nedbank.co.za/terms/EmailDisclaimer.htm ]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From maechler at stat.math.ethz.ch  Mon Jan 15 15:28:54 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 15 Jan 2007 15:28:54 +0100
Subject: [R] changes in the structure of mer objects?
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E6B11A1@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E6B11A1@dc1ex01.air.org>
Message-ID: <17835.36774.225183.918378@stat.math.ethz.ch>

>>>>> "HaroldD" == Doran, Harold <HDoran at air.org>
>>>>>     on Mon, 15 Jan 2007 09:18:12 -0500 writes:

    HaroldD> help(package='lme4') will tell you 

yes, or with more input but less output

   packageDescription("lme4")$Version

Further note

	sessionInfo()

which gives you all versions of all attached packages and more
and which is really *the* thing to be used in such
problem-report e-mails.

Martin

    >> -----Original Message-----
    >> From: r-help-bounces at stat.math.ethz.ch 
    >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
    >> joris.dewolf at cropdesign.com
    >> Sent: Monday, January 15, 2007 8:37 AM
    >> To: Martin Maechler
    >> Cc: Bates at wisc.edu; r-help at stat.math.ethz.ch; 
    >> r-help-bounces at stat.math.ethz.ch
    >> Subject: Re: [R] changes in the structure of mer objects?
    >> 
    >> Thanks Martin,
    >> 
    >> R --vanilla did the trick.
    >> By the way, is there a way to check which version of a lme4 
    >> or Matrix I am using in a certain instance of R?
    >> 
    >> We are running multiple versions of R on the same server 
    >> until we are sure all our operational code is behaving well 
    >> under a new version of R or of a package.
    >> Now I start doubting if we ever have been testing what we 
    >> were intendng to test...
    >> 
    >> Joris
    >> 
    >> 
    >> r-help-bounces at stat.math.ethz.ch wrote on 15/01/2007 14:00:12:
    >> 
    >> > Hi Joris,
    >> >
    >> > I suspect you somehow load an older version lme4 or Matrix than you 
    >> > think you are loading.
    >> > Or then you have an lmer() function or a class definition {from a 
    >> > saved workspace ????} in your work space.
    >> >
    >> > example(lmer) *must* run correctly for the 'lme4' package 
    >> to get onto 
    >> > CRAN at all, hence it must be unique to your setup.
    >> >
    >> > Maybe as a first step,
    >> > run R as "R --vanilla" ?
    >> >
    >> > Regards,
    >> > Martin Maechler, ETH Zurich
    >> >
    >> > >>>>> "joris" == joris dewolf <joris.dewolf at cropdesign.com>
    >> > >>>>>     on Sun, 14 Jan 2007 19:15:43 +0100 writes:
    >> >
    >> >     joris> Dear all,
    >> >
    >> >     joris> I try to run the example of lmer and get the following 
    >> > error message.
    >> >
    >> >     >> library(lme4)
    >> >     >> example(lmer)
    >> >     lmer> (fm1 <- lmer(Reaction ~ Days + (Days | Subject), 
    >> sleepstudy))
    >> >     joris> [[1]]
    >> >     joris> Error in get(x, envir, mode, inherits) : variable "as.
    >> > dpoMatrix" was not
    >> >     joris> found
    >> >
    >> >     joris> This error message is similar to what I get with
    >> >     joris> other models. It looks like the mer class has a
    >> >     joris> slightly different structure. Anybody an idea how to
    >> >     joris> solve this?
    >> >
    >> >
    >> >     joris> I am using R 2.4.1 under linux and the latest releases of
    >> > lme4 and Matrix
    >> >
    >> >     joris> lme4_0.9975-10
    >> >     joris> Matrix_0.9975-8
    >> >
    >> >     >> version
    >> >     joris> _
    >> >     joris> platform       x86_64-unknown-linux-gnu
    >> >     joris> arch           x86_64
    >> >     joris> os             linux-gnu
    >> >     joris> system         x86_64, linux-gnu
    >> >     joris> status
    >> >     joris> major          2
    >> >     joris> minor          4.1
    >> >     joris> year           2006
    >> >     joris> month          12
    >> >     joris> day            18
    >> >     joris> svn rev        40228
    >> >     joris> language       R
    >> >     joris> version.string R version 2.4.1 (2006-12-18)
    >> >
    >> > ______________________________________________
    >> > R-help at stat.math.ethz.ch mailing list
    >> > https://stat.ethz.ch/mailman/listinfo/r-help
    >> > PLEASE do read the posting guide
    >> http://www.R-project.org/posting-guide.html
    >> > and provide commented, minimal, self-contained, reproducible code.
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide 
    >> http://www.R-project.org/posting-guide.html
    >> and provide commented, minimal, self-contained, reproducible code.
    >>


From justin_bem at yahoo.fr  Mon Jan 15 15:30:45 2007
From: justin_bem at yahoo.fr (justin bem)
Date: Mon, 15 Jan 2007 14:30:45 +0000 (GMT)
Subject: [R] Re :  How to format R code in LaTex documents
Message-ID: <617217.33535.qm@web23005.mail.ird.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070115/9c5d3906/attachment.pl 

From skiadas at hanover.edu  Mon Jan 15 15:31:48 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Mon, 15 Jan 2007 09:31:48 -0500
Subject: [R] How to format R code in LaTex documents
In-Reply-To: <b75d67340701150542m43aa4d2rb0af6fd6b022cbf0@mail.gmail.com>
References: <b75d67340701150542m43aa4d2rb0af6fd6b022cbf0@mail.gmail.com>
Message-ID: <79B4601D-0771-45DF-8739-84B6B95910EB@hanover.edu>


On Jan 15, 2007, at 8:42 AM, Benjamin Dickgiesser wrote:

> Hi,
>
> I am planning on putting some R script in an appendix of a LaTex
> document. Can anyone recommend me a way of how to format it? Is there
> a way to keep all line breaks without having to insert \\ in every
> single line?

I think the LaTeX environments "lstlisting" and/or "verbatim" might  
do what you want.

> Thank you!
> Benjamin

Haris


From chrysopa at gmail.com  Mon Jan 15 16:58:35 2007
From: chrysopa at gmail.com (Ronaldo Reis Junior)
Date: Mon, 15 Jan 2007 13:58:35 -0200
Subject: [R] add points to a 1-weibull graph
Message-ID: <200701151358.36208.chrysopa@gmail.com>

Hi,

I made a survival analysis using survreg function. Its OK. But, I need to show 
in the graph 1-weibull curves.

Ploting weibull curve a add the estimated points following these steps:

fit <- survfit(Surv(Time,Censor)~Group)

points(fit[1])
points(fit[2])
points(fit[3])

I have 3 groups.

But I dont know how to plot the correct points on a inverse weilbull graph. I 
try to use points(1-fit[1]) but dont work. I try others way to make this, but 
all dont make sense.

Anybody can help me?

Thanks
Ronaldo 
--
> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/Depto. Biologia Geral/Lab. Ecologia Evolutiva
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8190 | chrysopa em gmail.com
| ICQ#: 5692561 | LinuxUser#: 205366


From dae-jin.lee at uc3m.es  Mon Jan 15 17:05:25 2007
From: dae-jin.lee at uc3m.es (Dae-Jin Lee)
Date: Mon, 15 Jan 2007 17:05:25 +0100
Subject: [R] R2WinBugs and Compare DIC versus BIC or AIC
In-Reply-To: <45A8C51B.5080403@statistik.uni-dortmund.de>
References: <cbca975a0701121041m1d3a5365qaa9cf91288d1f5b3@mail.gmail.com>
	<45A8C51B.5080403@statistik.uni-dortmund.de>
Message-ID: <cbca975a0701150805s55b1a852u93509679b1139077@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070115/37811c13/attachment.pl 

From ulrich.keller at emacs.lu  Mon Jan 15 17:14:20 2007
From: ulrich.keller at emacs.lu (Ulrich Keller)
Date: Mon, 15 Jan 2007 17:14:20 +0100
Subject: [R] options("digits") and print.default()
Message-ID: <fc.0087cdad000a4cad0087cdad000a4cad.a4cd5@emacs.lu>

Hello everyone,

I use latex() (Hmisc) for report generation and thus have been affected by
the problem with rounding decimals described, for example, in this post:

http://thread.gmane.org/gmane.comp.lang.r.general/73287/focus=73287

In short, numbers often are printed with 15 or so decimals even if there
far less significant digits. The problem has been confirmed by Frank
Harrell and Thomas Dupont according to this post:

http://thread.gmane.org/gmane.comp.lang.r.general/73172/focus=73186

But it has still not been fixed. Rather than changing all my reports I
decided I'd look at format.df() in Hmisc myself and try to fix the problem
there. I found that the problem is not in Hmisc at all, but in R itself:

> print(1.001)
[1] 1.001
> options(digits=16) #format.df does this
> print(1.001)
[1] 1.001000000000000
> print(round(1.001, 3)) #rounding does not help
[1] 1.001000000000000

This does not seem to be the behaviour described in the documentation,
which says:

"The same number of decimal places is used throughout a vector,[sic!] This
means that digits specifies the minimum number of significant digits to be
used, and that at least one entry will be encoded with that minimum
number. However, if all the encoded elements then have trailing zeroes,
the number of decimal places is reduced until at least one element has a
non-zero final digit."

If print.default() exhibited the behaviour desribed in the docs,
format.df() and thus latex() would work as advertised, I think. I would
have written a bug report instead of posting here, but the fact (?) that
Frank and Thomas have confirmed the bug seems to indicate that the problem
does indeed lie with Hmisc. Am I misunderstanding something here?

I use R version 2.4.1 Patched (2007-01-13 r40470) on Windows.


Uli


From ligges at statistik.uni-dortmund.de  Mon Jan 15 17:18:26 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 15 Jan 2007 17:18:26 +0100
Subject: [R] R2WinBugs and Compare DIC versus BIC or AIC
In-Reply-To: <cbca975a0701150805s55b1a852u93509679b1139077@mail.gmail.com>
References: <cbca975a0701121041m1d3a5365qaa9cf91288d1f5b3@mail.gmail.com>	
	<45A8C51B.5080403@statistik.uni-dortmund.de>
	<cbca975a0701150805s55b1a852u93509679b1139077@mail.gmail.com>
Message-ID: <45ABA952.9090206@statistik.uni-dortmund.de>



Dae-Jin Lee wrote:
> Thanks for your answer !
> 
> But If I have understood correctly, the procedure is correct and this only
> means that I have already generated the initial values
> 
>  From R. I define a function to generate the initial values as:
> 
> inits. <- function(){list(sdesp=runif(1),sdhet=runif(1),beta0=runif(1),
> beta1=runif(1), b=rnorm(n), phi=rnorm(n))}
> 
> 
> then It is not necessary to generate initial values, that why 
> gen.inits() is
> greyed out
> , is it ok?


Yes, alright.

Uwe


> Here is what Winbugs show
> 
> ==============================================================
> 
> display(log)
> check(D:/model.txt)
> model is syntactically correct
> data(D://data.txt)
> data loaded
> compile(3)
> model compiled
> inits(1,D:/inits1.txt)
> chain initialized but other chain(s) contain uninitialized variables
> inits(2,D:/inits2.txt)
> chain initialized but other chain(s) contain uninitialized variables
> inits(3,D:/inits3.txt)
> model is initialized
> gen.inits()
> command #Bugs:gen.inits cannot be executed (is greyed out)
> thin.updater(1)
> update(5000)
> 
> ==============================================================
> 
> Thank again in advance
> 
> Dae-Jin
> 
> 2007/1/13, Uwe Ligges <ligges at statistik.uni-dortmund.de >:
>>
>>
>>
>> Dae-Jin Lee wrote:
>> > Dear All
>> >
>> > 1)
>> >
>> > I'm fitting spatial CAR models
>> >
>> > using R2Winbugs and although everything seems to go reasonably well (or
>> I
>> > think so)
>> >
>> > the next message appears from WINBUGS 1.4 window:
>> >
>> > gen.inits()
>> > Command #Bugs: gen.inits cannot be executed (is greyed out)
>>
>> "greyed out" refers to the corresponding GUI. In this case it could mean
>> that either your preparations before were not successful or you have
>> already specified all initial values and no further have to be
>> generated. But this is a WinBUGS question and not R related.
>>
>> Perhaps you have transposed some matrix? This is always extremely
>> confusing ...
>>
>>
>> > The question is if this message means that something is wrong and the
>> > results are consequently wrong, or Can I assume it as a simple warning
>> > message???...
>> >
>> > I've tried to fit the model using just WinBugs (not calling form R) and
>> > gen.inits command is available and the results obtain are practically
>> the
>> > same...
>> >
>> > 2)
>> >
>> > The other question is, once several bayesian CAR models are fitted I 
>> use
>> DIC
>> > to model selection
>> >
>> > How can I compare DIC with AIC or BIC obtain for a spatial glm? I mean
>> for
>> > example:
>> >
>> > - DIC of 300 and Pd of 32
>> >
>> > versus
>> >
>> > - BIC of 220 and Effective dimension (measured of trace of hat matrix)
>> equal
>> > to 14
>> >
>> > * I've read that DIC is intended as a generalisation of Akaike's
>> Information
>> > Criterion (AIC), but is it possible to compare them simply looking 
>> which
>> is
>> > the lowest???
>>
>>
>> Well they are intended to be compared (DIC with DIC, AIC with AIC, but
>> do not mix them!) that way given you are comparing within the same class
>> of models. And then, there is a bit of fortunetelling with all these
>> information criterion, at least in my personal opinion (and I know that
>> others disagree).
>>
>> Uwe Ligges
>>
>>
>> >
>> > Thanks,
>> >
>> > Dae-Jin Lee
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>


From michael.greeff at env.ethz.ch  Mon Jan 15 17:44:44 2007
From: michael.greeff at env.ethz.ch (Michael Greeff)
Date: Mon, 15 Jan 2007 17:44:44 +0100
Subject: [R] model simplification in repeated anova
Message-ID: <F0EFFFD8-EFE9-4D82-A3C9-B5144AEEF4FB@env.ethz.ch>

Hi, I've done a repeated ANOVA looking as follows:

model<-aov(y~a*b*c+Error(d))

Now I want to get rid of some or all of the interactions. But neither  
AIC nor anova work. What is the appropriate function to test my model?


From s-walker at ti.com  Mon Jan 15 18:10:52 2007
From: s-walker at ti.com (Walker, Sam)
Date: Mon, 15 Jan 2007 11:10:52 -0600
Subject: [R] wafer map drawing
In-Reply-To: <644e1f320701121701i7d8f5b8exca8a45fee5d1e316@mail.gmail.com>
Message-ID: <76E525F192FF454A9712272E207B714B513923@dlee12.ent.ti.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070115/84e3db76/attachment.pl 

From rmh at temple.edu  Mon Jan 15 18:25:05 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Mon, 15 Jan 2007 12:25:05 -0500 (EST)
Subject: [R] model simplification in repeated anova
Message-ID: <20070115122505.BSC70805@po-d.temple.edu>

summary(npk.aovE)
summary(np.k.aovE)
## The block stratum has the same total in both summaries.
## In this example, the two suppressed interactions are last and
## both are not significant.

## In order to use anova(model.1, model.2), it is necessary to
## rewrite the SAME models without the Error function.
## Use terms( , keep.order=TRUE) and the dummy variables that were
## generated in the multi-stratum model.

npk.proj <- proj(npk.aovE)
npk.x <- npk.proj$block[,"N:P:K"]

npk.aov <- aov(terms(yield ~  npk.x + block + N+P+K+N:P+N:K+P:K, keep.order=TRUE), npk)

np.k.aov <- aov(terms(yield ~  block + N+P+K+N:P, keep.order=TRUE), npk)

summary(npk.aov)
summary(np.k.aov)

anova(npk.aov, np.k.aov)


From HDoran at air.org  Mon Jan 15 15:18:12 2007
From: HDoran at air.org (Doran, Harold)
Date: Mon, 15 Jan 2007 09:18:12 -0500
Subject: [R] changes in the structure of mer objects?
Message-ID: <2323A6D37908A847A7C32F1E3662C80E6B11A1@dc1ex01.air.org>

help(package='lme4') will tell you 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> joris.dewolf at cropdesign.com
> Sent: Monday, January 15, 2007 8:37 AM
> To: Martin Maechler
> Cc: Bates at wisc.edu; r-help at stat.math.ethz.ch; 
> r-help-bounces at stat.math.ethz.ch
> Subject: Re: [R] changes in the structure of mer objects?
> 
> Thanks Martin,
> 
> R --vanilla did the trick.
> By the way, is there a way to check which version of a lme4 
> or Matrix I am using in a certain instance of R?
> 
> We are running multiple versions of R on the same server 
> until we are sure all our operational code is behaving well 
> under a new version of R or of a package.
> Now I start doubting if we ever have been testing what we 
> were intendng to test...
> 
> Joris
> 
> 
> r-help-bounces at stat.math.ethz.ch wrote on 15/01/2007 14:00:12:
> 
> > Hi Joris,
> >
> > I suspect you somehow load an older version lme4 or Matrix than you 
> > think you are loading.
> > Or then you have an lmer() function or a class definition {from a 
> > saved workspace ????} in your work space.
> >
> > example(lmer) *must* run correctly for the 'lme4' package 
> to get onto 
> > CRAN at all, hence it must be unique to your setup.
> >
> > Maybe as a first step,
> > run R as "R --vanilla" ?
> >
> > Regards,
> > Martin Maechler, ETH Zurich
> >
> > >>>>> "joris" == joris dewolf <joris.dewolf at cropdesign.com>
> > >>>>>     on Sun, 14 Jan 2007 19:15:43 +0100 writes:
> >
> >     joris> Dear all,
> >
> >     joris> I try to run the example of lmer and get the following 
> > error message.
> >
> >     >> library(lme4)
> >     >> example(lmer)
> >     lmer> (fm1 <- lmer(Reaction ~ Days + (Days | Subject), 
> sleepstudy))
> >     joris> [[1]]
> >     joris> Error in get(x, envir, mode, inherits) : variable "as.
> > dpoMatrix" was not
> >     joris> found
> >
> >     joris> This error message is similar to what I get with
> >     joris> other models. It looks like the mer class has a
> >     joris> slightly different structure. Anybody an idea how to
> >     joris> solve this?
> >
> >
> >     joris> I am using R 2.4.1 under linux and the latest releases of
> > lme4 and Matrix
> >
> >     joris> lme4_0.9975-10
> >     joris> Matrix_0.9975-8
> >
> >     >> version
> >     joris> _
> >     joris> platform       x86_64-unknown-linux-gnu
> >     joris> arch           x86_64
> >     joris> os             linux-gnu
> >     joris> system         x86_64, linux-gnu
> >     joris> status
> >     joris> major          2
> >     joris> minor          4.1
> >     joris> year           2006
> >     joris> month          12
> >     joris> day            18
> >     joris> svn rev        40228
> >     joris> language       R
> >     joris> version.string R version 2.4.1 (2006-12-18)
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Mon Jan 15 18:35:21 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Jan 2007 17:35:21 +0000 (GMT)
Subject: [R] options("digits") and print.default()
In-Reply-To: <fc.0087cdad000a4cad0087cdad000a4cad.a4cd5@emacs.lu>
References: <fc.0087cdad000a4cad0087cdad000a4cad.a4cd5@emacs.lu>
Message-ID: <Pine.LNX.4.64.0701151716060.5973@gannet.stats.ox.ac.uk>

On Mon, 15 Jan 2007, Ulrich Keller wrote:

> Hello everyone,
>
> I use latex() (Hmisc) for report generation and thus have been affected by
> the problem with rounding decimals described, for example, in this post:
>
> http://thread.gmane.org/gmane.comp.lang.r.general/73287/focus=73287
>
> In short, numbers often are printed with 15 or so decimals even if there
> far less significant digits. The problem has been confirmed by Frank
> Harrell and Thomas Dupont according to this post:
>
> http://thread.gmane.org/gmane.comp.lang.r.general/73172/focus=73186
>
> But it has still not been fixed. Rather than changing all my reports I
> decided I'd look at format.df() in Hmisc myself and try to fix the problem
> there. I found that the problem is not in Hmisc at all, but in R itself:
>
>> print(1.001)
> [1] 1.001
>> options(digits=16) #format.df does this
>> print(1.001)
> [1] 1.001000000000000
>> print(round(1.001, 3)) #rounding does not help
> [1] 1.001000000000000
>
> This does not seem to be the behaviour described in the documentation,
> which says:
>
> "The same number of decimal places is used throughout a vector,[sic!] This
> means that digits specifies the minimum number of significant digits to be
> used, and that at least one entry will be encoded with that minimum
> number. However, if all the encoded elements then have trailing zeroes,
> the number of decimal places is reduced until at least one element has a
> non-zero final digit."
>
> If print.default() exhibited the behaviour desribed in the docs,
> format.df() and thus latex() would work as advertised, I think. I would
> have written a bug report instead of posting here, but the fact (?) that
> Frank and Thomas have confirmed the bug seems to indicate that the problem
> does indeed lie with Hmisc. Am I misunderstanding something here?
>
> I use R version 2.4.1 Patched (2007-01-13 r40470) on Windows.

The bug is that Hmisc uses more digits than the representation provides:

> .Machine$double.eps
[1] 2.22044604925031e-16

If it used 15 digits (as R does for as.character) all would be well. 
Since R has to do the calculations you quote in binary arithmetic, they 
are also subject to representation error and there is no way they can be 
done to 16 significant figures.  (I've traced the example, and that is 
what happens.)  See ?as.character for more details.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rmh at temple.edu  Mon Jan 15 18:39:09 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Mon, 15 Jan 2007 12:39:09 -0500 (EST)
Subject: [R] model simplification in repeated anova
Message-ID: <20070115123909.BSC73337@po-d.temple.edu>

## My previous response included only half the file.

## This is based on ?aov

## From Venables and Ripley (2002) p.165.
data(npk, package="MASS")

## as a test, not particularly sensible statistically
npk.aovE <- aov(yield ~  N*P*K + Error(block), npk)

np.k.aovE <- aov(yield ~  N*P+K + Error(block), npk)

anova(np.k.aovE, npk.aovE)  ## doesn't work, as query noted

## Compare these two
summary(npk.aovE)
summary(np.k.aovE)
## The block stratum has the same total in both summaries.
## In this example, the two suppressed interactions are last and
## both are not significant.

## In order to use anova(model.1, model.2), it is necessary to
## rewrite the SAME models without the Error function.
## Use terms( , keep.order=TRUE) and the dummy variables that were
## generated in the multi-stratum model.

npk.proj <- proj(npk.aovE)
npk.x <- npk.proj$block[,"N:P:K"]

npk.aov <- aov(terms(yield ~  npk.x + block + N+P+K+N:P+N:K+P:K, keep.order=TRUE), npk)

np.k.aov <- aov(terms(yield ~  block + N+P+K+N:P, keep.order=TRUE), npk)

summary(npk.aov)
summary(np.k.aov)

anova(npk.aov, np.k.aov)


From ripley at stats.ox.ac.uk  Mon Jan 15 18:49:29 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Jan 2007 17:49:29 +0000 (GMT)
Subject: [R] Conflict in .Rprofile documentation FAQ vs. Help?
In-Reply-To: <1B51D8F5-D2B0-4669-B623-61898974BE6E@muohio.edu>
References: <1B51D8F5-D2B0-4669-B623-61898974BE6E@muohio.edu>
Message-ID: <Pine.LNX.4.64.0701151743450.12018@gannet.stats.ox.ac.uk>

On Mon, 15 Jan 2007, MHH Stevens wrote:

> Hi folks,
> I note that in the general FAQ's we have
> 7.25 Why did my .Rprofile stop working when I updated R?
>
> Did you read the NEWS file? For functions that are not in the base
> package you need to specify the correct package namespace, since the
> code will be run before the packages are loaded. E.g.,
>
>      ps.options(horizontal = FALSE)
>      help.start()
>
> needs to be
>
>      grDevices::ps.options(horizontal = FALSE)
>      utils::help.start()
>
> yet in the Help file under start up, we have

I guess you mean Startup.Rd in some unstated version of R like 2.4.1?
That needed updating.

> # Example of .Rprofile
> options(width=65, digits=5)
> options(show.signif.stars=FALSE)
> ps.options(horizontal=FALSE)
> ....
>
> Is this a conflict?

No one actually said it was a *working* example (and it is enclosed in 
\dontrun{}), which is what the FAQ entry is about.  A better version would 
be

setHook(packageEvent("grDevices", "onLoad"),
         function(...) grDevices::ps.options(horizontal=FALSE))


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From phgrosjean at sciviews.org  Mon Jan 15 18:56:00 2007
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 15 Jan 2007 18:56:00 +0100
Subject: [R] listing all functions in R
In-Reply-To: <1168100161.3149.49.camel@dhcppc2.my.nat.localnet>
References: <1168088147.3149.6.camel@dhcppc2.my.nat.localnet>	<Pine.LNX.4.64.0701061331420.8651@gannet.stats.ox.ac.uk>	<1168093536.3149.25.camel@dhcppc2.my.nat.localnet>	<459FC386.6030004@stats.uwo.ca>	<971536df0701060758y4b23e8dal881807031a8d1b9b@mail.gmail.com>
	<1168100161.3149.49.camel@dhcppc2.my.nat.localnet>
Message-ID: <45ABC030.4040601@sciviews.org>

Hello,

First, regarding GeSHi syntax highlighting for R, I have done one for
the R Wiki (plus the R function that generates the list of keywords
automatically). I will attach it to a second email send privately to
you, since the mailing list do not accept attachments.

For the problem of keeping web pages up-to-date with R code, I am also
considering this problem with the R Wiki. Although I still do not have a
completely working solution, the approach is similar to Sweave. I have a
function which extracts the R code from wiki pages (that is, it
'Stangles' the wiki page, in the Sweave terminology). I can get, thus,
the R code from all wiki pages in turn, test them and write a report
with a couple of R code lines. Here is are my functions:

getWikiRcode <- function(wikipage, url = "http://wiki.r-project.org/rwiki",
	strip.empty.lines = TRUE, strip.output = FALSE) {
	# Read the raw wiki page
	Url <- paste(url, "/doku.php?id=", wikipage, "&do=export_raw", sep ="")
	Raw <- readLines(Url)
	
	# Get only <code r> .... </code> chunks from this page
	Codestart <- grep("^\\s*<code", Raw)
	Codeend <- grep("^\\s*</code>", Raw)
	# A little bit of checking first
	if (length(Codestart) != length(Codeend) || any(Codeend <= Codestart))
		stop("Malformed wiki page (wrong <code>... </code> sections)")	
	# Get only r code sections (those starting with <code r> from the list
	Rstart <- grep("^\\s*<code r>", Raw)
	if (length(Rstart) == 0) return(character(0))	# no R code in this page
	isRsection <- Codestart %in% Rstart
	Rend <- Codeend[isRsection]
	
	# Construct the list of text lines related to r code
	R <- data.frame(Start = Rstart, End = Rend)
	Seq <- function(x) seq(from = x[1], to = x[2])
	Rrows <- c(apply(R, 1, Seq), recursive = TRUE)
	Rcode <- Raw[Rrows]
	
	# Eliminate <code r> and </code> tags
	Rcode <- gsub("^\\s*</?code( r)?>.*$", "", Rcode)
	
	# Eliminate prompt from R code '> ', or '+ ' at the begining of a line
	Rcode <- sub("^[>+] ", "", Rcode)
	
	# Possibly eliminate empty lines
	if (strip.empty.lines) Rcode <- Rcode[Rcode != ""]
	
	# Possibly eliminate output (lines starting with '#!')
	if (strip.output) {
		Routput <- grep("^\\#\\!", Rcode)
		if (length(Routput) > 0) Rcode <- Rcode[-Routput]
	}
	
	# Return the R code
	return(Rcode)
}

rcode <- getWikiRcode("tips:data-frames:merge")
rcode

sourceWikiRcode <- function(wikipage, echo = TRUE, url =
"http://wiki.r-project.org/rwiki",
	strip.empty.lines = TRUE, strip.output = FALSE, ...) {
	# Call getWikiRcode() to extract r code from wiki pages
	Rcode <- getWikiRcode(wikipage = wikipage, url = url,
		strip.empty.lines = strip.empty.lines, strip.output = strip.output)
	if (length(Rcode) == 0) {
		warning("No r code in this page!")
	} else {
		Con <- textConnection(Rcode)
		source(Con, echo = echo, ...)
		close(Con)
	}
}

sourceWikiRcode("tips:data-frames:merge")
# Here, the last part of this page is not directly executable (data1 is
not defined)
# but the rest is fine!

This is suboptimal, and I am considering rewriting it in PHP to return R
code only from the wiki server.

Best,

Philippe Grosjean

Gavin Simpson wrote:
> On Sat, 2007-01-06 at 10:58 -0500, Gabor Grothendieck wrote:
>> The arguments to the functions can differ too even if they
>> exist on multiple platforms.   system() on Windows has the
>> input= argument but not on UNIX.
> 
> That's a good point Gabor, and one I hadn't considered as yet. As I'm
> only just setting out on the road to providing R help resources for the
> wider world (rather than the limited environs of the courses I have
> run), I tend to not have thought about these things much - though I
> guess I have a few gotchas waiting to bite me in the ass before too
> long.
> 
> I am just starting to think about the best way to organise the snippets
> of code to allow me to keep them up-to-date with current R and changes
> in package code that the snippets use. Dropping the code verbatim into
> PHP scripts isn't a good idea. At the moment I intend to store all
> snippets in individual *.R files and read them into to variables within
> the PHP scripts, from where they will be highlighted and formatted for
> display.
> 
> It would be reasonably easy to write an R script to source all *.R files
> in a directory to look for errors and problems. And having them all as
> separate files means I can still use Emacs/ESS to prepare, format, and
> run the code through R, which is my preferred environment.
> 
> All the best,
> 
> G
> 
>> On 1/6/07, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
>>> On 1/6/2007 9:25 AM, Gavin Simpson wrote:
>>>> On Sat, 2007-01-06 at 13:48 +0000, Prof Brian Ripley wrote:
>>>>> Could you tell us what you mean by
>>>> Thank you for your reply, Prof. Ripley.
>>>>
>>>>> - 'function'  (if() and + are functions in R, so do you want those?)
>>>> I was thinking about functions that are used like this: foo()
>>>> So I don't need things like "names<-". I don't need functions like +. -,
>>>> $, as I can highlight the separately if desired, though I'm not doing
>>>> this at the moment.
>>>>
>>>> Functions like for() while(), if() function() are handled separately.
>>>>
>>>>> - 'a base R installation'?   What is 'base R' (standard + recommended
>>>>> packages?)  And on what platform: the list is platform-specific?
>>>> Yes, I mean standard + recommended packages. As for platform, most of my
>>>> intended audience will be MS Windows users, though I am using Linux
>>>> (Fedora) to generate this list (i.e. my R installation is on Linux).
>>> Be careful:  the installed list of functions differs slightly from
>>> platform to platform.  For example, on Windows there's a function
>>> choose.dir in the utils package, but I don't think this exists on Unix.
>>>
>>> The list also varies from version to version, so if you could manage to
>>> run some code in the user's installed R to generate the list on the fly,
>>> you'd get the most accurate list.
>>>
>>> Duncan Murdoch
>>>
>>>>> Here is a reasonable shot:
>>>>>
>>>>> findfuns <- function(x) {
>>>>>      if(require(x, character.only=TRUE)) {
>>>>>         env <- paste("package", x, sep=":")
>>>>>         nm <- ls(env, all=TRUE)
>>>>>         nm[unlist(lapply(nm, function(n) exists(n, where=env,
>>>>>                                                mode="function",
>>>>>                                                inherits=FALSE)))]
>>>>>      } else character(0)
>>>>> }
>>>>> pkgs <- dir(.Library)
>>>>> z <-  lapply(pkgs, findfuns)
>>>>> names(z) <- pkgs
>>>> Excellent, that works just fine for me. I can edit out certain packages
>>>> that I don't expect to use, before formatting as desired. I can also use
>>>> this function on a library of packages that I use regularly and will be
>>>> using in the web pages.
>>>>
>>>>> I don't understand your desired format, but
>>>>>
>>>>> write(sQuote(sort(unique(unlist(z)))), "")
>>>> I wanted a single string "...", with entries enclosed in "''" and
>>>> separated by "," (this is to go in a PHP array). I can generate such a
>>>> string from your z, above, as follows:
>>>>
>>>> paste(sQuote(sort(unique(unlist(z)), decreasing = TRUE)),
>>>>       collapse = ", ")
>>>>
>>>>> gives a single-column quoted list.  It does include internal functions,
>>>>> operators, S3 methods ... so you probably want to edit it.
>>>> Once again, thank you.
>>>>
>>>> All the best
>>>>
>>>> Gav
>>>>
>>>>> On Sat, 6 Jan 2007, Gavin Simpson wrote:
>>>>>
>>>>>> Dear List,
>>>>>>
>>>>>> I'm building an R syntax highlighting file for GeSHi [*] for a website I
>>>>>> am currently putting together. The syntax file needs a list of keywords
>>>>>> to highlight. How can I generate a list of all the functions in a base R
>>>>>> installation?
>>>>>>
>>>>>> Ideally the list would be formatted like this:
>>>>>>
>>>>>> "'fun1', 'fun2', 'fun3'"
>>>>>>
>>>>>> when printed to the screen so I can copy and paste it into the syntax
>>>>>> file.
>>>>>>
>>>>>> I'm sure this has been asked before, but I stupidly didn't save that
>>>>>> email and I couldn't come up with a suitable query parameter for
>>>>>> Jonathan Baron's search site to return results before timing out.
>>>>>>
>>>>>> Thanks in advance,
>>>>>>
>>>>>> Gav
>>>>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From harris41 at msu.edu  Mon Jan 15 19:15:09 2007
From: harris41 at msu.edu (Scott Harrison)
Date: Mon, 15 Jan 2007 13:15:09 -0500
Subject: [R] =?utf-8?q?feedback_on_=22writing_r_extensions=22?=
Message-ID: <E1H6WMQ-0005qg-Ql@sys29.mail.msu.edu>

Hi, 


To whom should I send feedback/edits concerning the
"Writing R Extensions" web page? 


http://cran.r-project.org/doc/manuals/R-exts.html 


Scott


From pinard at iro.umontreal.ca  Tue Jan 16 00:05:19 2007
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Mon, 15 Jan 2007 18:05:19 -0500
Subject: [R] Conflict in .Rprofile documentation FAQ vs. Help?
In-Reply-To: <Pine.LNX.4.64.0701151743450.12018@gannet.stats.ox.ac.uk>
References: <1B51D8F5-D2B0-4669-B623-61898974BE6E@muohio.edu>
	<Pine.LNX.4.64.0701151743450.12018@gannet.stats.ox.ac.uk>
Message-ID: <20070115230519.GA5829@alcyon.progiciels-bpi.ca>

[Brian Ripley]

>No one actually said it was a *working* example [...]

Do you mean that, whenever we see something presented as an example 
within or around the R system, we should not take it as dependable 
unless it is explicitly said to be working?

> (and it is enclosed in \dontrun{})

Within the R online help system, many examples are marked so they are
not run.  I naively thought they were not run for friendly reasons, like 
for example, not inordinately impacting the user's environment.  Should 
I read you as saying that those examples are not to be believed?

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca


From pj.bell at yahoo.co.uk  Tue Jan 16 00:01:12 2007
From: pj.bell at yahoo.co.uk (Piet Bell)
Date: Mon, 15 Jan 2007 23:01:12 +0000 (GMT)
Subject: [R] spatial correlaton in lme and gls
Message-ID: <20070115230112.92267.qmail@web27615.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070115/88e41cc4/attachment.pl 

From tcalkins at gmail.com  Tue Jan 16 00:29:35 2007
From: tcalkins at gmail.com (Tim Calkins)
Date: Tue, 16 Jan 2007 10:29:35 +1100
Subject: [R] How to format R code in LaTex documents
In-Reply-To: <b75d67340701150542m43aa4d2rb0af6fd6b022cbf0@mail.gmail.com>
References: <b75d67340701150542m43aa4d2rb0af6fd6b022cbf0@mail.gmail.com>
Message-ID: <ca5a2f2e0701151529x785bb667v612864dc63526dc3@mail.gmail.com>

Consider using the fancyvrb package if you need additional customization.

On 1/16/07, Benjamin Dickgiesser <dickgiesser at gmail.com> wrote:
> Hi,
>
> I am planning on putting some R script in an appendix of a LaTex
> document. Can anyone recommend me a way of how to format it? Is there
> a way to keep all line breaks without having to insert \\ in every
> single line?
>
> Thank you!
> Benjamin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Tim Calkins
0406 753 997


From lloydl at cybermesa.com  Tue Jan 16 00:56:41 2007
From: lloydl at cybermesa.com (Lloyd Lubet)
Date: Mon, 15 Jan 2007 16:56:41 -0700
Subject: [R] How can I generate line segments between two sets of points on
	the same graph?
Message-ID: <000601c73900$d2fa22e0$ed2b1341@your4dacd0ea75>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070115/0724ad64/attachment.pl 

From lloydl at cybermesa.com  Tue Jan 16 01:09:17 2007
From: lloydl at cybermesa.com (Lloyd Lubet)
Date: Mon, 15 Jan 2007 17:09:17 -0700
Subject: [R] How do I generate line segments between two sets of points on
	the same graph
Message-ID: <000601c73902$9426d840$ed2b1341@your4dacd0ea75>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070115/4d4a2700/attachment.pl 

From ssj1364 at gmail.com  Tue Jan 16 02:16:06 2007
From: ssj1364 at gmail.com (sj)
Date: Mon, 15 Jan 2007 18:16:06 -0700
Subject: [R] ARIMA xreg and factors
Message-ID: <1c6126db0701151716j26486d39q9886bb7d36269c9c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070115/fd0051a9/attachment.pl 

From ggrothendieck at gmail.com  Tue Jan 16 03:43:04 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 15 Jan 2007 21:43:04 -0500
Subject: [R] ARIMA xreg and factors
In-Reply-To: <1c6126db0701151716j26486d39q9886bb7d36269c9c@mail.gmail.com>
References: <1c6126db0701151716j26486d39q9886bb7d36269c9c@mail.gmail.com>
Message-ID: <971536df0701151843h1b95285anc1b051dfac8b830d@mail.gmail.com>

Try this:


set.seed(1)
sim.ar <- arima.sim(list(ar = c(0.4, 0.4)), n = 1000)
z<- gl(5, 1, 1000)
zm <- model.matrix(~z)[,-1]
arima(sim.ar, order=c(2,0,0), xreg = zm)



On 1/15/07, sj <ssj1364 at gmail.com> wrote:
> I am using arima to develop a time series regression model, I am using arima
> b/c I have autocorrelated errors. Several of my independent variables are
> categorical and I have coded them as factors .  When I run ARIMA  I  don't
> get any warning or error message, but I do not seem to get estimates for all
> the levels of the factor. Can/how does ARIMA handle factors in xreg?
>
>
> here is some example code:
>
> sim.ar <- arima.sim(list(ar = c(0.4, 0.4)), n = 1000)
> z<- factor(rep((1:5),200))
>
> arima(sim.ar,order=c(2,0,0),xreg=z)
>
>
> I  only get a single estiamte for xreg. Am I thinking about this wrong (I
> expected 4).
>
> thank you,
>
>
> Spencer
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From johannh at gmail.com  Tue Jan 16 05:04:32 2007
From: johannh at gmail.com (Johann Hibschman)
Date: Mon, 15 Jan 2007 23:04:32 -0500
Subject: [R] nonlinear regression: nls, gnls, gnm, other?
Message-ID: <4b82d65b0701152004tcec41dcnc2b0f29bf2e286c8@mail.gmail.com>

Hi all,

I'm trying to fit a nonlinear (logistic-like) regression, and I'd like
to get some recommendations for which package to use.

The expression I want to fit is something like:

y ~ A * exp(X * Beta1) / (1 + exp(-(x + X * Beta2 - xmid)/scal))

Basically, it's a logistic function, but I want to be able to modify
the saturation amplitude by a few parameters (Beta1) and shift the
inflection point around with a few other parameters (Beta2).  I have a
ton of data, but I often have trouble getting the routine to fit.
(I've been using nlin in SAS, which seems sloppier in terms of
accepted convergence.)

Now, from what I can tell, I can use nls, gnls, or gnm to fit
something like this, but I can't tell which would be better, or if
there's something else I should be trying.  To do this right, though,
I have to do a lot more reading, but I'd like to know where to start.

(I have more of a physics/computer background, so I immediately jump
to thinking of regression as minimizing some cost function across a
multidimensional space and then start mumbling about simulated
annealing or some such, but this isn't helping me much in interpreting
the available literature.)

So, does anyone have any suggestions?  I imagine I'm going to have to
pick up a book, but should it be Pinheiro & Bates on nlme, Bates &
Watts, the pdf manual to gnm, or what?

Thanks for any suggestions,

Johann


From neil.a.mcleod at gmail.com  Tue Jan 16 06:49:53 2007
From: neil.a.mcleod at gmail.com (Neil McLeod)
Date: Tue, 16 Jan 2007 00:49:53 -0500
Subject: [R] How to format R code in LaTex documents
In-Reply-To: <ca5a2f2e0701151529x785bb667v612864dc63526dc3@mail.gmail.com>
References: <b75d67340701150542m43aa4d2rb0af6fd6b022cbf0@mail.gmail.com>
	<ca5a2f2e0701151529x785bb667v612864dc63526dc3@mail.gmail.com>
Message-ID: <2c4b23d60701152149r664d0e72j8ae547c07e156482@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070116/fcf7915b/attachment.pl 

From lorenz.gygax at art.admin.ch  Tue Jan 16 07:31:40 2007
From: lorenz.gygax at art.admin.ch (lorenz.gygax at art.admin.ch)
Date: Tue, 16 Jan 2007 07:31:40 +0100
Subject: [R] Within-subject factors in lme
In-Reply-To: <403c92d40701120925p71b45437i806640a3837f660e@mail.gmail.com>
Message-ID: <145C63777EF3ED41A5A99035845F7DD9D3119C@EVD-C8001.bk.evdad.admin.ch>

> Thanks for your suggestion. I guess the model you are fitting here
> has only a single random effect term, namely subject. If the effect
> of A depends on S, one needs to include an additional random effects
> term for the S:A interaction.

It is not quite clear what you are after, but the 'if' sounds as you would like to include interactions between your fixed and random effects? This is done in the following way:

lme (y ~ A + B, random= ~ A | S )
or
lme (y ~ A + B, random= ~ B | S )
or
lme (y ~ A + B, random= ~ A + B | S )

with redard to discussions on degrees of freedom (and thus testing in mixed-effects models) you may also want to refer to:

I hope this helps.
Regards, Lorenz Gygax
- 
Swiss Federal Veterinary Office
Centre for proper housing of ruminants and pigs
Agroscope Reckenholz-T?nikon Research Station ART


From lorenz.gygax at art.admin.ch  Tue Jan 16 07:32:49 2007
From: lorenz.gygax at art.admin.ch (lorenz.gygax at art.admin.ch)
Date: Tue, 16 Jan 2007 07:32:49 +0100
Subject: [R] Within-subject factors in lme: complemented
In-Reply-To: <145C63777EF3ED41A5A99035845F7DD9D3119C@EVD-C8001.bk.evdad.admin.ch>
Message-ID: <145C63777EF3ED41A5A99035845F7DD9D3119D@EVD-C8001.bk.evdad.admin.ch>

The internet address to complement this is:
http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests

sorry for the double posting!

> -----Original Message-----
> From: Gygax Lorenz ART 
> Sent: Tuesday, January 16, 2007 7:32 AM
> To: r-help at stat.math.ethz.ch
> Subject: RE: [R] Within-subject factors in lme
> 
> 
> > Thanks for your suggestion. I guess the model you are fitting here
> > has only a single random effect term, namely subject. If the effect
> > of A depends on S, one needs to include an additional random effects
> > term for the S:A interaction.
> 
> It is not quite clear what you are after, but the 'if' sounds 
> as you would like to include interactions between your fixed 
> and random effects? This is done in the following way:
> 
> lme (y ~ A + B, random= ~ A | S )
> or
> lme (y ~ A + B, random= ~ B | S )
> or
> lme (y ~ A + B, random= ~ A + B | S )
> 
> with redard to discussions on degrees of freedom (and thus 
> testing in mixed-effects models) you may also want to refer to:
> 
> I hope this helps.
> Regards, Lorenz Gygax
> - 
> Swiss Federal Veterinary Office
> Centre for proper housing of ruminants and pigs
> Agroscope Reckenholz-T?nikon Research Station ART
>


From petr.pikal at precheza.cz  Tue Jan 16 08:38:09 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 16 Jan 2007 08:38:09 +0100
Subject: [R] R editor vs. Tinn-R
In-Reply-To: <20070112191142.8qimphz7tsgkco8s@webmail.rz.uni-saarland.de>
References: <eo8h1f$vcl$1@sea.gmane.org>
Message-ID: <45AC8EF1.9809.881799@localhost>

Hi

On 12 Jan 2007 at 19:11, Martin Becker wrote:

Date sent:      	Fri, 12 Jan 2007 19:11:42 +0100
From:           	Martin Becker <martin.becker at mx.uni-saarland.de>
To:             	Farrel Buchinsky <fjbuch at gmail.com>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] R editor vs. Tinn-R

> Zitat von Farrel Buchinsky <fjbuch at gmail.com>:
> 
> > The only button I pressed was the "send line" button in version
> > 1.19.1.5. I changed my command to
> >
> > prelim<-read.delim(source("clipboard"))
> >
> 
> The problem is not using the wrong R-command, but using a (Tinn-R) 
> button that leads to a replacement of the current (windows)
> clipboard...
> 
> > and still got the same problem.
> > I do not know how to use the source versions of the buttons. Can you
> > please tell me more?


I also do not know about "source" buttons. I have only send buttons. 
Maybe different Tinn-R version.

I usually issue such command from Tinn-R and then by up arrow I 
execute it again with correct content of clipboard e.g. copy from 
Excel like program.

HTH
Petr


> >
> 
> There are three button groups on the left of the "Send line" button, 
> containing two buttons each. The left button of each of these three 
> button groups has the same name as the right button of each group, 
> apart from having a "(source)" suffix. Now it should be obvious what 
> was meant with '"(source)"-version of buttons'. There is no such 
> version for the "Send line" button, so you have to select the 
> corresponding line and use the "Send selection (source)" button, e.g.
> 
> > Thanks
> >
> > Farrel
> >
> 
> Regards,
>    Martin
> 
> >
> > "Martin Becker" <martin.becker at mx.uni-saarland.de> wrote in message
> > news:45A79DE5.9060803 at mx.uni-saarland.de...
> >> Farrel Buchinsky wrote:
> >>> Have you used Tinn-R and what landmines await the inexperienced?
> >>>
> >>>
> >>
> >> Depending on which button you press in Tinn-R, the clipboard is
> >> used to transfer the commands to R, so sometimes you can't rely on
> >> the previous contents of the clipboard while using Tinn-R. If you
> >> use the "(source)"-versions of the buttons, the content of the
> >> clipboard should be preserved.
> >>
> >> Regards,
> >>  Martin
> >>
> >>> I could not understand why a script that used to work stopped
> >>> working. Look at these two scenarios I opened an excel spreadsheet
> >>> and copied several cells to the clipboard Then Scenario 1 Executed
> >>> from Tinn-R
> >>>
> >>>> prelim<-read.delim("clipboard")
> >>>> str(prelim)
> >>>>
> >>> 'data.frame':   0 obs. of  1 variable:
> >>>  $ prelim..read.delim.clipboard.: logi
> >>>
> >>> Scenario 2
> >>> Executed from R editor
> >>>
> >>>> prelim<-read.delim("clipboard")
> >>>> str(prelim)
> >>>>
> >>> 'data.frame':   18 obs. of  13 variables:
> >>>
> >>>
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From ripley at stats.ox.ac.uk  Tue Jan 16 08:50:56 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 16 Jan 2007 07:50:56 +0000 (GMT)
Subject: [R] ARIMA xreg and factors
In-Reply-To: <1c6126db0701151716j26486d39q9886bb7d36269c9c@mail.gmail.com>
References: <1c6126db0701151716j26486d39q9886bb7d36269c9c@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701160746460.25112@gannet.stats.ox.ac.uk>

>From the help page

     xreg: Optionally, a vector or matrix of external regressors, which
           must have the same number of rows as 'x'.

Note, not a factor.  (It handles your factor like the integer vector it is 
internally.) If you want a design matrix you need to make one, something 
like

include.mean = FALSE, xreg = model.matrix(~z)


On Mon, 15 Jan 2007, sj wrote:

> I am using arima to develop a time series regression model, I am using arima
> b/c I have autocorrelated errors. Several of my independent variables are
> categorical and I have coded them as factors .  When I run ARIMA  I  don't
> get any warning or error message, but I do not seem to get estimates for all
> the levels of the factor. Can/how does ARIMA handle factors in xreg?
>
>
> here is some example code:
>
> sim.ar <- arima.sim(list(ar = c(0.4, 0.4)), n = 1000)
> z<- factor(rep((1:5),200))
>
> arima(sim.ar,order=c(2,0,0),xreg=z)
>
>
> I  only get a single estiamte for xreg. Am I thinking about this wrong (I
> expected 4).
>
> thank you,
>
>
> Spencer
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From maechler at stat.math.ethz.ch  Tue Jan 16 09:46:25 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 16 Jan 2007 09:46:25 +0100
Subject: [R] Mailing list attachments
In-Reply-To: <45ABC030.4040601@sciviews.org>
References: <1168088147.3149.6.camel@dhcppc2.my.nat.localnet>
	<Pine.LNX.4.64.0701061331420.8651@gannet.stats.ox.ac.uk>
	<1168093536.3149.25.camel@dhcppc2.my.nat.localnet>
	<459FC386.6030004@stats.uwo.ca>
	<971536df0701060758y4b23e8dal881807031a8d1b9b@mail.gmail.com>
	<1168100161.3149.49.camel@dhcppc2.my.nat.localnet>
	<45ABC030.4040601@sciviews.org>
Message-ID: <17836.37089.460632.958693@stat.math.ethz.ch>

>>>>> "PhGr" == Philippe Grosjean <phgrosjean at sciviews.org>
>>>>>     on Mon, 15 Jan 2007 18:56:00 +0100 writes:

    PhGr> ............
    PhGr> I will attach it to a second email send privately to
    PhGr> you, since the mailing list do not accept attachments.

That's not correct.
http://www.r-project.org/mail.html  (search for "attachment")
explains that most ** binary ** attachments are not accepted,
lists the exceptions and further says that  
text/plain is well accepted
(BTW,  text/html is accepted too, but translated to text/plain
 by the filters)

One problem is that many e-mail client programs do not seem to
let you attach "text/plain" easily.

Martin


From tagett at ipsogen.com  Fri Jan 12 14:07:30 2007
From: tagett at ipsogen.com (Rebecca Tagett)
Date: Fri, 12 Jan 2007 13:07:30 +0000 (UTC)
Subject: [R] R graphics with Linux (libpng)
References: <200701111758.l0BHwXVF018367@hypatia.math.ethz.ch>
	<Pine.LNX.4.64.0701111818300.30884@gannet.stats.ox.ac.uk>
Message-ID: <loom.20070112T122249-448@post.gmane.org>

Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:

> 
> On Thu, 11 Jan 2007, Rebecca Tagett wrote:
> 
> > Hello,
> >
> > I'm trying to adapt some R code that works on Windows so that it will work
> > on a Linux machine.
> >
> > The command :
> >
> > png("myFile.png", width=600, height=600)
> >
> > fails claiming that it is impossible to establish a connection with X11.
> > (Error messages are in French, so I'm not pasting them here!)
> >
> >
> >
> > I have libpng installed:
> >
> >> rpm -qa libpng*
> >
> > libpng-1.0.12-2
> >
> > libpng-devel-1.0.12-2
> >
> >
> >
> > So I don't understand why R thinks I'm trying to connect to X11. I haven't
> > been able to find many examples of R graphics code specifically for Linux,
> > but I have the impression that if libpng is installed, the graphics 
commands
> > are identical. The libpng manual is not useful, because it does not mention
> > use of libpng commands in an R environment.
> 
> ?png says
> 
>       R can be compiled without support for either or both of these
>       devices: this will be reported if you attempt to use them on a
>       system where they are not supported.  They may not be usable
>       unless the X11 display is available to the owner of the R process.
> 
> Note:
> 
>       These are based on the 'X11' device, so the additional arguments
>       to that device work, but are rarely appropriate.  The colour
>       handling will be that of the 'X11' device in use.
> 
>       'bitmap' provides an alternative way to generate PNG and JPEG
>       plots that does not depend on accessing the X11 display but does
>       depend on having GhostScript installed.  (Device 'GDD' in CRAN
>       package 'GDD' is another alternative using several other
>       additional pieces of software.)
> 
> so the help page seems quite specific about the connection to X11.


I was referring to the libpng manual (see above), and not the R help page.

The 'Devices' help page is the one that threw me off course. It states that
X11 is the graphics driver for the X11 Window system, which I took to mean "MS 
Windows" as opposed to "X window", thus I was surprised that Linux should go 
looking for such a thing. I'm now wading through "R installation and 
administration", which is very useful. Our systems manager is away, so your 
patience is appreciated.

> 
> > I'm using R 2.4.1, which I recently installed. Do I have to install a more
> > recent libpng ? If so, do I have to reconfigure and remake R ?
> 
> The libpng is very old, but even more likely is that libz is too old.
> There were messages when you configured: did you read them at all?  If 
> not, plese do rebuild R and read them this time.  Without the exact 
> error message I cannot be sure, but I don't think this is the problem.

There were no errors on installation. However, I see that libz came with the R 
package (it is in .../R-2.4.1/src/extra/zlib/libz.a), so it would have been 
correctly installed along with the rest. Evidently libpng does not come with 
the package. I'll download the recent version, install it, rebuild R, and try 
to find out why my Linux doesn't recognize X11. Or perhaps I'll use bitmap() 
with ghostscript, as Benilton Carvalho suggested.

> 
> > I've also read that no graphics devices are available under R CMD BATCH.
> > Does that really mean that I can't create graphics from some R code that I
> > launch in noninteractive mode?
> 
> Your source is incorrect: please advise the author(s) to correct it.

The source is the 'Devices' help page :
QUOTE 
     The following devices will be available if R was compiled to use
     them and started with the appropriate '--gui' argument:

        *  'X11' The graphics driver for the X11 Window system

        *  'png' PNG bitmap device

        *  'jpeg' JPEG bitmap device

     None of these are available under 'R CMD BATCH'.
UNQUOTE
(no authors listed)


> 
> Did you also read that you are asked not to send HTML mail? (That is from 
> a reliable source.)

I knew that. I didn't do it on purpose... so very sorry. 

Thanks.


From maechler at stat.math.ethz.ch  Tue Jan 16 10:11:22 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 16 Jan 2007 10:11:22 +0100
Subject: [R] =?utf-8?q?feedback_on_=22writing_r_extensions=22?=
In-Reply-To: <E1H6WMQ-0005qg-Ql@sys29.mail.msu.edu>
References: <E1H6WMQ-0005qg-Ql@sys29.mail.msu.edu>
Message-ID: <17836.38586.556842.839027@stat.math.ethz.ch>

>>>>> "Scott" == Scott Harrison <harris41 at msu.edu>
>>>>>     on Mon, 15 Jan 2007 13:15:09 -0500 writes:

    Scott> Hi, To whom should I send feedback/edits concerning
    Scott> the "Writing R Extensions" web page?

    Scott> http://cran.r-project.org/doc/manuals/R-exts.html

To R-devel (the mailing list), usually;
If it's just simple typos, maybe just to R-core directly.

For "edits", we'd mostly like to receive feedback on the
*source* of the above automatically produced document
(which is also the source of the PDF version, same URL as above,
 just replace ".html" by ".pdf")

The development source is always available at
  https://svn.r-project.org/R/trunk/doc/manual/R-exts.texi

Thanking you for offering feedback,
Martin Maechler, ETH Zurich


From Heather.Turner at warwick.ac.uk  Tue Jan 16 10:29:00 2007
From: Heather.Turner at warwick.ac.uk (Turner, Heather)
Date: Tue, 16 Jan 2007 09:29:00 -0000
Subject: [R] nonlinear regression: nls, gnls, gnm, other?
In-Reply-To: <4b82d65b0701152004tcec41dcnc2b0f29bf2e286c8@mail.gmail.com>
Message-ID: <1072002710EB6047A212400EEB65F0060B0C2C@ELDER.ads.warwick.ac.uk>

Hi Johann,

The current version of gnm is unable to fit this type of model, though a
new version with more flexibility is soon to be released.

In any case, you probably want to use nls or gnls, depending on the
assumptions that can be made about the model errors. For nls it is usual
to assume that the errors are normally distributed with mean zero and
constant variance, though the normal assumption is not strictly
necessary. If you have reason to think the errors are correlated and/or
have unequal variances, then gnls would be appropriate.

The examples on ?nls may be enough to get you started,

Heather

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Johann Hibschman
Sent: 16 January 2007 04:05
To: Turner, Heather; r-help
Subject: [R] nonlinear regression: nls, gnls, gnm, other?

Hi all,

I'm trying to fit a nonlinear (logistic-like) regression, and I'd like
to get some recommendations for which package to use.

The expression I want to fit is something like:

y ~ A * exp(X * Beta1) / (1 + exp(-(x + X * Beta2 - xmid)/scal))

Basically, it's a logistic function, but I want to be able to modify
the saturation amplitude by a few parameters (Beta1) and shift the
inflection point around with a few other parameters (Beta2).  I have a
ton of data, but I often have trouble getting the routine to fit.
(I've been using nlin in SAS, which seems sloppier in terms of
accepted convergence.)

Now, from what I can tell, I can use nls, gnls, or gnm to fit
something like this, but I can't tell which would be better, or if
there's something else I should be trying.  To do this right, though,
I have to do a lot more reading, but I'd like to know where to start.

(I have more of a physics/computer background, so I immediately jump
to thinking of regression as minimizing some cost function across a
multidimensional space and then start mumbling about simulated
annealing or some such, but this isn't helping me much in interpreting
the available literature.)

So, does anyone have any suggestions?  I imagine I'm going to have to
pick up a book, but should it be Pinheiro & Bates on nlme, Bates &
Watts, the pdf manual to gnm, or what?

Thanks for any suggestions,

Johann

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dawn.ashcourt at gmail.com  Tue Jan 16 10:30:22 2007
From: dawn.ashcourt at gmail.com (Dawn Ashcourt)
Date: Tue, 16 Jan 2007 03:30:22 -0600
Subject: [R] Gaussian glm for grouped data with unequal variances
Message-ID: <f4feb8a00701160130m539db439webabf33503d7a1cd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070116/c89c7a9c/attachment.pl 

From sleevecity at robkeller.us  Tue Jan 16 10:10:56 2007
From: sleevecity at robkeller.us (Rob Keller)
Date: Tue, 16 Jan 2007 04:10:56 -0500
Subject: [R] Help with labeling a page with multiple graphs.
Message-ID: <45AC96A0.50701@robkeller.us>

Hi,

I am trying to output pdf files with 8 graphs per page. My current study 
has 8 evaluation criteria, and 38 study conditions. This would result in 
38 pages of 8 graphs. I have figured out how to get the graphs looking 
like I would like them, and I am using the layout() function to put them 
on the page, as that allows me the control I would like over the 
appearance of the individual plots. What I can't figure out is how to 
put a page title on each page. Something simple like, "Condition X" at 
the top of each page. A second line of title would also be nice. Any 
help is truely appreciated.

rob


From ripley at stats.ox.ac.uk  Tue Jan 16 11:02:10 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 16 Jan 2007 10:02:10 +0000 (GMT)
Subject: [R] nonlinear regression: nls, gnls, gnm, other?
In-Reply-To: <1072002710EB6047A212400EEB65F0060B0C2C@ELDER.ads.warwick.ac.uk>
References: <1072002710EB6047A212400EEB65F0060B0C2C@ELDER.ads.warwick.ac.uk>
Message-ID: <Pine.LNX.4.64.0701160959060.27321@gannet.stats.ox.ac.uk>

On Tue, 16 Jan 2007, Turner, Heather wrote:

> Hi Johann,
>
> The current version of gnm is unable to fit this type of model, though a
> new version with more flexibility is soon to be released.
>
> In any case, you probably want to use nls or gnls, depending on the
> assumptions that can be made about the model errors. For nls it is usual
> to assume that the errors are normally distributed with mean zero and
> constant variance, though the normal assumption is not strictly
> necessary. If you have reason to think the errors are correlated and/or
> have unequal variances, then gnls would be appropriate.

nls is able to handle unequal variances since 2.3.0: from the help

  weights: an optional numeric vector of (fixed) weights.  When present,
           the objective function is weighted least squares.


>
> The examples on ?nls may be enough to get you started,
>
> Heather
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Johann Hibschman
> Sent: 16 January 2007 04:05
> To: Turner, Heather; r-help
> Subject: [R] nonlinear regression: nls, gnls, gnm, other?
>
> Hi all,
>
> I'm trying to fit a nonlinear (logistic-like) regression, and I'd like
> to get some recommendations for which package to use.
>
> The expression I want to fit is something like:
>
> y ~ A * exp(X * Beta1) / (1 + exp(-(x + X * Beta2 - xmid)/scal))
>
> Basically, it's a logistic function, but I want to be able to modify
> the saturation amplitude by a few parameters (Beta1) and shift the
> inflection point around with a few other parameters (Beta2).  I have a
> ton of data, but I often have trouble getting the routine to fit.
> (I've been using nlin in SAS, which seems sloppier in terms of
> accepted convergence.)
>
> Now, from what I can tell, I can use nls, gnls, or gnm to fit
> something like this, but I can't tell which would be better, or if
> there's something else I should be trying.  To do this right, though,
> I have to do a lot more reading, but I'd like to know where to start.
>
> (I have more of a physics/computer background, so I immediately jump
> to thinking of regression as minimizing some cost function across a
> multidimensional space and then start mumbling about simulated
> annealing or some such, but this isn't helping me much in interpreting
> the available literature.)
>
> So, does anyone have any suggestions?  I imagine I'm going to have to
> pick up a book, but should it be Pinheiro & Bates on nlme, Bates &
> Watts, the pdf manual to gnm, or what?
>
> Thanks for any suggestions,
>
> Johann
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ccleland at optonline.net  Tue Jan 16 11:04:43 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 16 Jan 2007 05:04:43 -0500
Subject: [R] Help with labeling a page with multiple graphs.
In-Reply-To: <45AC96A0.50701@robkeller.us>
References: <45AC96A0.50701@robkeller.us>
Message-ID: <45ACA33B.5020803@optonline.net>

Rob Keller wrote:
> Hi,
> 
> I am trying to output pdf files with 8 graphs per page. My current study 
> has 8 evaluation criteria, and 38 study conditions. This would result in 
> 38 pages of 8 graphs. I have figured out how to get the graphs looking 
> like I would like them, and I am using the layout() function to put them 
> on the page, as that allows me the control I would like over the 
> appearance of the individual plots. What I can't figure out is how to 
> put a page title on each page. Something simple like, "Condition X" at 
> the top of each page. A second line of title would also be nice. Any 
> help is truely appreciated.

  RSiteSearch("title multiple figure") shows the following possibilities:

http://finzi.psych.upenn.edu/R/library/sfsmisc/html/mult.fig.html

http://finzi.psych.upenn.edu/R/library/Hmisc/html/mtitle.html

> rob
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From frainj at gmail.com  Tue Jan 16 11:08:34 2007
From: frainj at gmail.com (John C Frain)
Date: Tue, 16 Jan 2007 10:08:34 +0000
Subject: [R] How to format R code in LaTex documents
In-Reply-To: <b75d67340701150542m43aa4d2rb0af6fd6b022cbf0@mail.gmail.com>
References: <b75d67340701150542m43aa4d2rb0af6fd6b022cbf0@mail.gmail.com>
Message-ID: <fad888a10701160208g42a2864bu3e6f9d4690a97b9f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070116/512752d8/attachment.pl 

From kalyansikha at yahoo.com  Tue Jan 16 11:09:05 2007
From: kalyansikha at yahoo.com (Bhanu Kalyan.K)
Date: Tue, 16 Jan 2007 02:09:05 -0800 (PST)
Subject: [R] Request regarding cluster package
Message-ID: <20070116100906.18951.qmail@web34307.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070116/c6c6326c/attachment.pl 

From r.hankin at noc.soton.ac.uk  Tue Jan 16 11:10:42 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 16 Jan 2007 10:10:42 +0000
Subject: [R] "[[" gotcha
Message-ID: <62C917FB-A9C2-4FDB-9267-BA104D91DD74@soc.soton.ac.uk>

The following gotcha caught me off-guard just now.

I have two matrices, a and b:


a <- matrix(1,3,3)
b <- matrix(1,1,1)

(note that both "a" and "b" are matrices).

I want them in a list:

 > B <- NULL
 > B[[1]] <- a
 > B[[2]] <- b
 > B
[[1]]
      [,1] [,2] [,3]
[1,]    1    1    1
[2,]    1    1    1
[3,]    1    1    1

[[2]]
      [,1]
[1,]    1

 >

This is fine.

But swapping "a" and "b" over does not behave as desired:


 > B <- NULL
 > B[[1]] <- b
 > B[[2]] <- a
Error in B[[2]] <- a : more elements supplied than there are to replace
 >



The error is given because after B[[1]] <- a,   the variable B is  
just a scalar and
not a matrix (why is this?)

What's the bulletproof method for assigning matrices to a list (whose  
length is
not known at runtime)?








--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From b.rowlingson at lancaster.ac.uk  Tue Jan 16 11:44:26 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 16 Jan 2007 10:44:26 +0000
Subject: [R] "[[" gotcha
In-Reply-To: <62C917FB-A9C2-4FDB-9267-BA104D91DD74@soc.soton.ac.uk>
References: <62C917FB-A9C2-4FDB-9267-BA104D91DD74@soc.soton.ac.uk>
Message-ID: <45ACAC8A.8010004@lancaster.ac.uk>

Robin Hankin wrote:

> The error is given because after B[[1]] <- a,   the variable B is  
> just a scalar and
> not a matrix (why is this?)
> 

  Because [[i]] indexes more general vectors, and if you do B[[1]] when 
B is NULL, R doesnt know if you want B to be a list or a simple vector.

  If you initialise B as an empty list then R knows:

  > B=list()
  > B
  list()
  > B[[1]]=b
  > B
  [[1]]
       [,1]
  [1,]    1

Barry


From petr.pikal at precheza.cz  Tue Jan 16 11:43:03 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 16 Jan 2007 11:43:03 +0100
Subject: [R] "[[" gotcha
In-Reply-To: <62C917FB-A9C2-4FDB-9267-BA104D91DD74@soc.soton.ac.uk>
Message-ID: <45ACBA47.15470.1316843@localhost>

Hi

try different way of list definition, see below.

On 16 Jan 2007 at 10:10, Robin Hankin wrote:

From:           	Robin Hankin <r.hankin at noc.soton.ac.uk>
Date sent:      	Tue, 16 Jan 2007 10:10:42 +0000
To:             	RHelp help <r-help at stat.math.ethz.ch>
Subject:        	[R] "[[" gotcha

> The following gotcha caught me off-guard just now.
> 
> I have two matrices, a and b:
> 
> 
> a <- matrix(1,3,3)
> b <- matrix(1,1,1)
> 
> (note that both "a" and "b" are matrices).
> 
> I want them in a list:
> 

B<-vector("list", 2)
B[[1]]<-a
B[[1]]<-b

here is no complain.
HTH
Petr

>  > B <- NULL
>  > B[[1]] <- a
>  > B[[2]] <- b
>  > B
> [[1]]
>       [,1] [,2] [,3]
> [1,]    1    1    1
> [2,]    1    1    1
> [3,]    1    1    1
> 
> [[2]]
>       [,1]
> [1,]    1
> 
>  >
> 
> This is fine.
> 
> But swapping "a" and "b" over does not behave as desired:
> 
> 
>  > B <- NULL
>  > B[[1]] <- b
>  > B[[2]] <- a
> Error in B[[2]] <- a : more elements supplied than there are to
> replace
>  >
> 
> 
> 
> The error is given because after B[[1]] <- a,   the variable B is 
> just a scalar and not a matrix (why is this?)
> 
> What's the bulletproof method for assigning matrices to a list (whose 
> length is not known at runtime)?
> 
> 
> 
> 
> 
> 
> 
> 
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Tue Jan 16 11:47:07 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 16 Jan 2007 11:47:07 +0100
Subject: [R] Fast Removing Duplicates from Every Column
In-Reply-To: <20070116093641.2FFA8419D@europa.telenet-ops.be>
References: AAAAAOXvfE/tUWVMujI1J0cWactkaSYA
Message-ID: <45ACBB3B.21758.1352163@localhost>

Hi

I have no idea how Test data look like. However help pages of 
functions

data.frame()
as.data.frame()
str()

and maybe few others can help you find how to change objects to data 
frames.

HTH
Petr


On 16 Jan 2007 at 10:36, Bert Jacobs wrote:

From:           	"Bert Jacobs" <b.jacobs at pandora.be>
To:             	"'Petr Pikal'" <petr.pikal at precheza.cz>
Subject:        	RE: [R] Fast Removing Duplicates from Every Column
Date sent:      	Tue, 16 Jan 2007 10:36:42 +0100

> Hi Petr,
> 
> Thx for answeringen me on the question below.
> Actually I could use this line of code to get my problem solved.
> 
> Test = apply(X=my_data, MARGIN=2, FUN=unique)
> 
> Now I was wondering how to transform 'Test' into a dataframe, while
> there are different rows implied.
> 
> Thx,
> Bert
> 
> _____________________________
> 
> Bert Jacobs
> Marketing Intelligence Engineer
> Plasveldlaan 5
> 9400 Ninove
> Tel: 0477/68.74.07
> Fax: 054/25.00.35
> E-mail: b.jacobs at pandora.be
> 
> -----Original Message-----
> From: Petr Pikal [mailto:petr.pikal at precheza.cz] 
> Sent: 05 January 2007 11:51
> To: Bert Jacobs; 'R help list'
> Subject: Re: [R] Fast Removing Duplicates from Every Column
> 
> Hi
> 
> I am not sure if I understand how do you want to select unique items.
> 
> with
>  sapply(DF, function(x) !duplicated(x))
> you can get data frame with TRUE when an item in particular column is
> unique and FALSE in opposite. However then you need to choose which
> rows to keep or discard
> 
> e.g.
> 
> DF[rowSums(sapply(comp, function(x) !duplicated(x)))>1,]
> 
> selects all rows in which are 2 or more unique values.
> 
> HTH
> Petr
> 
> 
> On 5 Jan 2007 at 9:54, Bert Jacobs wrote:
> 
> From:           	"Bert Jacobs" <b.jacobs at pandora.be>
> To:             	"'R help list'" <r-help at stat.math.ethz.ch>
> Date sent:      	Fri, 5 Jan 2007 09:54:17 +0100
> Subject:        	Re: [R] Fast Removing Duplicates from Every Column
> 
> > Hi,
> > 
> > I'm looking for some lines of code that does the following:
> > I have a dataframe with 160 Columns and a number of rows (max 30):
> > 
> >   Col1 Col2 Col3 ... Col 159 Col 160 
> > Row 1 	0 	0 	LD ... 0	   VD 
> > Row 2 	HD 	0 	0 	 0 	   MD 
> > Row 3 	0 	HD 	HD 	 0       LD 
> > Row 4 	LD 	HD 	HD 	 0 	   LD 
> > ...		...
> > LastRow	HD    HD    LD     0       MD
> > 
> > 
> > Now I want a dataframe that looks like this. As you see all
> > duplicates are removed. Can this dataframe be constructed in a fast
> > way?
> > 
> >   Col1 Col2 Col3 ... Col 159 Col 160 
> > Row 1       0    0    LD       0	    VD
> > Row 2     	HD   HD   0        0        MD
> > Row 3     	LD   0    HD       0        LD
> > 
> > Thx for helping me out.
> > Bert
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> 

Petr Pikal
petr.pikal at precheza.cz


From hb at stat.berkeley.edu  Tue Jan 16 11:48:05 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Tue, 16 Jan 2007 21:48:05 +1100
Subject: [R] "[[" gotcha
In-Reply-To: <62C917FB-A9C2-4FDB-9267-BA104D91DD74@soc.soton.ac.uk>
References: <62C917FB-A9C2-4FDB-9267-BA104D91DD74@soc.soton.ac.uk>
Message-ID: <59d7961d0701160248i55af575bsa82571e9bcbaedfe@mail.gmail.com>

To create a empty list do:

   B <- list()

/H


On 1/16/07, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
> The following gotcha caught me off-guard just now.
>
> I have two matrices, a and b:
>
>
> a <- matrix(1,3,3)
> b <- matrix(1,1,1)
>
> (note that both "a" and "b" are matrices).
>
> I want them in a list:
>
>  > B <- NULL
>  > B[[1]] <- a
>  > B[[2]] <- b
>  > B
> [[1]]
>       [,1] [,2] [,3]
> [1,]    1    1    1
> [2,]    1    1    1
> [3,]    1    1    1
>
> [[2]]
>       [,1]
> [1,]    1
>
>  >
>
> This is fine.
>
> But swapping "a" and "b" over does not behave as desired:
>
>
>  > B <- NULL
>  > B[[1]] <- b
>  > B[[2]] <- a
> Error in B[[2]] <- a : more elements supplied than there are to replace
>  >
>
>
>
> The error is given because after B[[1]] <- a,   the variable B is
> just a scalar and
> not a matrix (why is this?)
>
> What's the bulletproof method for assigning matrices to a list (whose
> length is
> not known at runtime)?
>
>
>
>
>
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From j.van_den_hoff at fzd.de  Tue Jan 16 12:02:24 2007
From: j.van_den_hoff at fzd.de (Joerg van den Hoff)
Date: Tue, 16 Jan 2007 12:02:24 +0100
Subject: [R] "[[" gotcha
In-Reply-To: <62C917FB-A9C2-4FDB-9267-BA104D91DD74@soc.soton.ac.uk>
References: <62C917FB-A9C2-4FDB-9267-BA104D91DD74@soc.soton.ac.uk>
Message-ID: <45ACB0C0.9010000@fz-rossendorf.de>

Robin Hankin wrote:
> The following gotcha caught me off-guard just now.
> 
> I have two matrices, a and b:
> 
> 
> a <- matrix(1,3,3)
> b <- matrix(1,1,1)
> 
> (note that both "a" and "b" are matrices).
> 
> I want them in a list:
> 
>  > B <- NULL
>  > B[[1]] <- a
>  > B[[2]] <- b
>  > B
> [[1]]
>       [,1] [,2] [,3]
> [1,]    1    1    1
> [2,]    1    1    1
> [3,]    1    1    1
> 
> [[2]]
>       [,1]
> [1,]    1
> 
>  >
> 
> This is fine.
> 
> But swapping "a" and "b" over does not behave as desired:
> 
> 
>  > B <- NULL
>  > B[[1]] <- b
>  > B[[2]] <- a
> Error in B[[2]] <- a : more elements supplied than there are to replace
>  >
> 
> 
> 
> The error is given because after B[[1]] <- a,   the variable B is  
> just a scalar and
> not a matrix (why is this?)
> 
> What's the bulletproof method for assigning matrices to a list (whose  
> length is
> not known at runtime)?
> 
> 
not sure about "bulletproof", but:

you should tell R that B is really intended to be a list in the first place:

B <- list()

the rest then works as you intended. whether the 'simplification' of your 1x1 matrix to
a scalar in your example is canonical (and desirable) behaviour seems a question for some 
of the experts (it's a bit reminiscent of the `drop = TRUE' vs. `drop = FALSE' problem)

joerg
> 
> 
> 
> 
> 
> 
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Tue Jan 16 12:11:36 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 16 Jan 2007 11:11:36 +0000 (GMT)
Subject: [R] "[[" gotcha
In-Reply-To: <62C917FB-A9C2-4FDB-9267-BA104D91DD74@soc.soton.ac.uk>
References: <62C917FB-A9C2-4FDB-9267-BA104D91DD74@soc.soton.ac.uk>
Message-ID: <Pine.LNX.4.64.0701161057010.17169@gannet.stats.ox.ac.uk>

On Tue, 16 Jan 2007, Robin Hankin wrote:

> The following gotcha caught me off-guard just now.

Checking the relevant help page is always a good idea (and that is why the 
posting guide asks that you do so before posting).

> I have two matrices, a and b:
>
>
> a <- matrix(1,3,3)
> b <- matrix(1,1,1)
>
> (note that both "a" and "b" are matrices).
>
> I want them in a list:
>
> > B <- NULL

So why did you not create a list, e.g. by list() or vector("list", 2)?

> > B[[1]] <- a
> > B[[2]] <- b
> > B
> [[1]]
>      [,1] [,2] [,3]
> [1,]    1    1    1
> [2,]    1    1    1
> [3,]    1    1    1
>
> [[2]]
>      [,1]
> [1,]    1
>
> >
>
> This is fine.
>
> But swapping "a" and "b" over does not behave as desired:
>
>
> > B <- NULL
> > B[[1]] <- b
> > B[[2]] <- a
> Error in B[[2]] <- a : more elements supplied than there are to replace
> >
>
>
>
> The error is given because after B[[1]] <- a,   the variable B is
> just a scalar and
> not a matrix (why is this?)

You said wanted a list, and got a numeric vector (R has no scalars).
In your first example you got a length-one list, not a matrix.

The type-promotion rules for [<- and [[<- are complex, and you should not 
rely on knowing what they currently are (they do change from time to 
time).  But this one is right there on the help page (?"[["):

      When '$<-' is applied to a 'NULL' 'x', it first coerces 'x' to
      'list()'.  This is what also happens with '[[<-' if the
      replacement value 'value' is of length greater than one: if
      'value' has length 1 or 0, 'x' is first coerced to a zero-length
      vector of the type of 'value'.

> What's the bulletproof method for assigning matrices to a list (whose 
> length is not known at runtime)?

Start with a list.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ligges at statistik.uni-dortmund.de  Tue Jan 16 12:16:54 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 16 Jan 2007 12:16:54 +0100
Subject: [R] Request regarding cluster package
In-Reply-To: <20070116100906.18951.qmail@web34307.mail.mud.yahoo.com>
References: <20070116100906.18951.qmail@web34307.mail.mud.yahoo.com>
Message-ID: <45ACB426.9070607@statistik.uni-dortmund.de>



Bhanu Kalyan.K wrote:
> Dear Mr. Bengtsson,
> 
> I see that there is a package exclusively for clustering data, named as "Cluster Package" in R library. It has some clustering algorithms implemented. 
> Can you tell me how to implement the CLARA and PAM functions from that package for my data?


What about typing

library("cluster")
?pam
?clara

Uwe Ligges


> 
> Bhanu Kalyan K
> B.Tech Final Year, CSE
> 
> Tel: +91-9885238228
> 
> Alternate E-Mail: 
> reach4kalyan at gmail.com
> 
>  
> ---------------------------------
> 8:00? 8:25? 8:40?  Find a flick in no time
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ccleland at optonline.net  Tue Jan 16 12:51:25 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 16 Jan 2007 06:51:25 -0500
Subject: [R] Gaussian glm for grouped data with unequal variances
In-Reply-To: <f4feb8a00701160130m539db439webabf33503d7a1cd@mail.gmail.com>
References: <f4feb8a00701160130m539db439webabf33503d7a1cd@mail.gmail.com>
Message-ID: <45ACBC3D.9060806@optonline.net>

Dawn Ashcourt wrote:
> Hello - I am fairly new to R, (i.e., ability to create functions/write
> programs insignificant) and was wondering if there might be a convenient way
> to model the following: I want to fit a gaussian glm to grouped data, while
> allowing for unequal variances in each of the groups.
> More specifically, my data set looks something like this:
> ----------------
>    data group
> 1    76     1
> 2    82     1
> 3    83     1
> 4    54     1
> 5    35     1
> 6    46     1
> 7    87     1
> 8    68     1
> 9    87     2
> 10   95     2
> 11   98     2
> 12  100     2
> 13  109     2
> 14  109     2
> 15  100     2
> 16   81     2
> 17   75     2
> 18   68     2
> 19   67     2
> 20  105     3
> .... et cetera.
> ---------------
> There are seven groups in all, each with a different number of observations.
> The idea is to compare a model in which all the data points can be modeled
> with a single mean (i.e., if all the group means are equal), or if the  data
> suggests that each of the groups has a different mean. In other words, I
> want to do a Likelihood ratio test on whether or not the group means are
> significantly different from each other: the full model would be glm(data ~
> as.factor(group)-1, family = gaussian), to be compared against a restricted
> model that only includes an intercept. However, I also need to allow for the
> fact that each group has a different variance. And this I have no idea how
> to do. I would really appreciate some help in this matter.

  Have you considered oneway.test()?  For example:

## Not assuming equal variances
oneway.test(breaks ~ tension, data = warpbreaks)

        One-way analysis of means (not assuming equal variances)

data:  breaks and tension
F = 5.8018, num df = 2.00, denom df = 32.32, p-value = 0.007032

> Thank you in advance,
> Dawn.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From tkellermann at ukaachen.de  Tue Jan 16 13:16:33 2007
From: tkellermann at ukaachen.de (Thilo Kellermann)
Date: Tue, 16 Jan 2007 13:16:33 +0100
Subject: [R] Gaussian glm for grouped data with unequal variances
In-Reply-To: <f4feb8a00701160130m539db439webabf33503d7a1cd@mail.gmail.com>
References: <f4feb8a00701160130m539db439webabf33503d7a1cd@mail.gmail.com>
Message-ID: <200701161316.34230.tkellermann@ukaachen.de>

Hi,
I would suggest to use lme for doing that:
library(nlme)
vf1Ident <- varIdent( c(m = 0.5), form = ~ 1 | group )
fm.lme <- lme(data ~ group, weights = vf1Ident. data = DATA)
anova(fm.lme)

Hope this helps,
Thilo


On Tuesday 16 January 2007 10:30, Dawn Ashcourt wrote:
> Hello - I am fairly new to R, (i.e., ability to create functions/write
> programs insignificant) and was wondering if there might be a convenient
> way to model the following: I want to fit a gaussian glm to grouped data,
> while allowing for unequal variances in each of the groups.
> More specifically, my data set looks something like this:
> ----------------
>    data group
> 1    76     1
> 2    82     1
> 3    83     1
> 4    54     1
> 5    35     1
> 6    46     1
> 7    87     1
> 8    68     1
> 9    87     2
> 10   95     2
> 11   98     2
> 12  100     2
> 13  109     2
> 14  109     2
> 15  100     2
> 16   81     2
> 17   75     2
> 18   68     2
> 19   67     2
> 20  105     3
> .... et cetera.
> ---------------
> There are seven groups in all, each with a different number of
> observations. The idea is to compare a model in which all the data points
> can be modeled with a single mean (i.e., if all the group means are equal),
> or if the  data suggests that each of the groups has a different mean. In
> other words, I want to do a Likelihood ratio test on whether or not the
> group means are significantly different from each other: the full model
> would be glm(data ~ as.factor(group)-1, family = gaussian), to be compared
> against a restricted model that only includes an intercept. However, I also
> need to allow for the fact that each group has a different variance. And
> this I have no idea how to do. I would really appreciate some help in this
> matter.
> Thank you in advance,
> Dawn.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
________________________
Thilo Kellermann
Department of Psychiatry und Psychotherapy
RWTH Aachen University
Pauwelstr. 30
52074 Aachen
Tel.: +49 (0)241 / 8089977
Fax.: +49 (0)241 / 8082401
E-Mail: tkellermann at ukaachen.de


From yanniggoude at yahoo.fr  Tue Jan 16 13:52:24 2007
From: yanniggoude at yahoo.fr (yannig goude)
Date: Tue, 16 Jan 2007 13:52:24 +0100 (CET)
Subject: [R] SARIMA problem
Message-ID: <20070116125224.60842.qmail@web26708.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070116/92d39d61/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Jan 16 14:19:44 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 16 Jan 2007 13:19:44 +0000 (GMT)
Subject: [R] SARIMA problem
In-Reply-To: <20070116125224.60842.qmail@web26708.mail.ukl.yahoo.com>
References: <20070116125224.60842.qmail@web26708.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.64.0701161312430.32058@gannet.stats.ox.ac.uk>

On Tue, 16 Jan 2007, yannig goude wrote:

> Hi,

>  I have a problem with the ARIMA function, occuring when I set the 
> parameter per (the period of SARIMA model) to a high value (see the 
> exemple bellow). It seems that when per is high it takes a too large 
> amount of memory to calculate the model and I have a memory storage 
> error. But I don't really understand why it takes more memory when per 
> is high, as there is the same number of parameter to estimate.

Try arima0.

>  Does anyone know what to do?
>
>  exemple:
>  x = arima.sim(list(order=c(1,0,0), ar=.9), n=1000)
> per<-200
> arima(x, order = c(1, 0, 0),seasonal = list(order =c(1, 0, 0), period =per),
         method='CSS')
>  fails whereas for per=175 it works well.
>
>
>
>
>
>
>
> ---------------------------------
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Thomas.Brunel at ifremer.fr  Tue Jan 16 14:48:19 2007
From: Thomas.Brunel at ifremer.fr (Thomas BRUNEL)
Date: Tue, 16 Jan 2007 14:48:19 +0100
Subject: [R] nlme : convergence problem and other errors
Message-ID: <45ACD7A3.8090401@ifremer.fr>

Dear R-user,

I am trying to use the R "nlme" function to fit a non linear mixed 
effects model. The model I wand to fit is an individual somatic growth 
model with 4 parameters. For all parameters both fixed and random 
effects have to be estimated, as well as their covariance matrix (see 
the formula bellow).
The data are simulated with the same growth model as in the nlme, with 
know parameters, and covariance matrix.

I tried to fit the model with several simulated data sets, but most of 
the time, R returns an error message.
there are three main types of errors :
- there is a singular matrix
- a factor is reduced bellow the PNLS level.
- max number of iteration is reached but there is no convergence.


Do you know how to resolve these problems. Is there a way to modify the 
parameters of the maximization algorithm to avoid these error messages?

Furthermore, do you know if it is possible to fix the values of the 
fixed effects so that the model only has to estimate the random effects?

Thank you for your help and answers.

Regards,

Thomas Brunel



pds.fit<-nlme(pds~(exp (lna)/(exp (lnb) + 1/(1 + exp(1000 * (exp 
(lntmat) - t + 0.5))) * exp (lnc)))^4 * (1 - (1 - (  0.051 * (1 - 1/(1 + 
exp(1000 * (exp (lntmat) - t + 0.5)))) + (exp (lna)/exp (lnb))^4 * (1 - 
(1 - (0.051)^0.25 * (exp (lnb)/exp (lna))) * exp( - ((exp (lnb) * exp 
(lntmat))/4)))^4 * 1/(1 + exp(1000 * (exp (lntmat) - t + 0.5))))^0.25 * 
((exp (lnb) + 1/(1 + exp(1000 * (exp (lntmat) - t + 0.5))) * exp 
(lnc))/exp (lna))) * exp( - (((exp (lnb) + 1/(1 + exp(1000 * (exp 
(lntmat) - t + 0.5))) * exp (lnc)) *(t - (1/(1 + exp(1000 * (exp 
(lntmat) - t + 0.5))) * exp 
(lntmat))))/4)))^4,data=pdsdata,fixed=lna+lnb+lnc+lntmat~1,random= 
lna+lnb+lnc+lntmat~1|indi ,start=c(lna=log(a2),lnb=log(b2) ,lnc = 
log(c2), lntmat=log(1200)))




-- 


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Laboratoire Halieutique de Port-en-Bessin


avenue du G?n?ral de Gaulle
14520 Port-en-Bessin
t?l : 02 31 51 56 00 (standard)
fax : 02 31 51 56 01


page personnelle (CV online):
http://www.ifremer.fr/drvrhbr/personnel/brunel/index.htm

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


From jlampur at eagrof.UdL.es  Tue Jan 16 13:10:10 2007
From: jlampur at eagrof.UdL.es (Jorge Lampurlanes Castel)
Date: Tue, 16 Jan 2007 13:10:10 +0100 (CET)
Subject: [R]  problems with for loop
Message-ID: <1347.10.100.13.47.1168949410.squirrel@correu.udl.es>

Hello,

With this program I try to repeat analysis for different years. The
results of the analysis are not printed when in the loop, except for the
year sequence. What is wrong?

Thanks a lot.

for (i in 92:99){
	cat("\n",
	"=============================================================\n",
	"YEAR =",i,"\n",
	"=============================================================\n"
	)
	canos1 <- subset (canos, canos$YEAR==i)
	LinearModel.1 <- lm(MM_P  ~ BLOC  + TIL, data=canos1)
	summary(LinearModel.1)
	Anova(LinearModel.1, type="II")
	Anova(LinearModel.1, type="III")

}


-- 
**************************************************
Jorge Lampurlan?s Castel
Departament d'Enginyeria Agroforestal
Escola T?cnica Superior d'Enginyeria Agr?ria
Universitat de Lleida
Avinguda Rovira Roure, 191
25198-LLEIDA
SPAIN

Tl.: +34 973 70 25 37
Fax.:+34 073 70 26 73
e-mail: jlampur at eagrof.udl.es


From goncalo.carrera at dq.fct.unl.pt  Tue Jan 16 14:20:17 2007
From: goncalo.carrera at dq.fct.unl.pt (=?iso-8859-1?b?R29u52Fsbw==?= Carrera)
Date: Tue, 16 Jan 2007 13:20:17 +0000
Subject: [R] number of Nodes in Random Forest
Message-ID: <1168953616.45acd11102c26@webmail.dq.fct.unl.pt>

I'm calculating nodes using Random Forest in R but i only get nodes for a
fraction of the compounds i want to calculate, the rest is ommited and is not
printed in the output file, (i'm working with 3012 compounds). What can i do to
have nodes printed for all the compounds. Thanks

Gon?alo


From m.mader at gsf.de  Tue Jan 16 15:17:05 2007
From: m.mader at gsf.de (Michael T. Mader)
Date: Tue, 16 Jan 2007 15:17:05 +0100
Subject: [R] problems with for loop
In-Reply-To: <1347.10.100.13.47.1168949410.squirrel@correu.udl.es>
References: <1347.10.100.13.47.1168949410.squirrel@correu.udl.es>
Message-ID: <45ACDE61.1080007@gsf.de>

If you really want the summary() etc to print to STDOUT use cat() or 
print(). However other ways to post-process results may be preferrable, 
I think about Sweave, xtable, etc.

Regards

Michael

Jorge Lampurlanes Castel wrote:
> Hello,
> 
> With this program I try to repeat analysis for different years. The
> results of the analysis are not printed when in the loop, except for the
> year sequence. What is wrong?
> 
> Thanks a lot.
> 
> for (i in 92:99){
> 	cat("\n",
> 	"=============================================================\n",
> 	"YEAR =",i,"\n",
> 	"=============================================================\n"
> 	)
> 	canos1 <- subset (canos, canos$YEAR==i)
> 	LinearModel.1 <- lm(MM_P  ~ BLOC  + TIL, data=canos1)
> 	summary(LinearModel.1)
> 	Anova(LinearModel.1, type="II")
> 	Anova(LinearModel.1, type="III")
> 
> }
> 
> 

-- 
Michael T. Mader
Institute of Stem Cell Research
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
D-85764 Neuherberg
0049-89-3187-3683

The limits of my language are the limits of my world.
	Ludwig Wittgenstein Tractatus Logico-Philosophicus 5.6, 1918


From Sicotte.Hugues at mayo.edu  Tue Jan 16 15:35:40 2007
From: Sicotte.Hugues at mayo.edu (Sicotte, Hugues   Ph.D.)
Date: Tue, 16 Jan 2007 08:35:40 -0600
Subject: [R]  R on Windows Vista
Message-ID: <2E17292A64E6ED418A60BE89326B1AAB3224FD@msgebe11.mfad.mfroot.org>

Did anyone try to run R under Window Vista, especially Windows Vista
64bit?

Thanks.


From justin_bem at yahoo.fr  Tue Jan 16 15:42:46 2007
From: justin_bem at yahoo.fr (justin bem)
Date: Tue, 16 Jan 2007 14:42:46 +0000 (GMT)
Subject: [R] Re : problems with for loop
Message-ID: <236599.37004.qm@web23005.mail.ird.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070116/581c91c2/attachment.pl 

From murdoch at stats.uwo.ca  Tue Jan 16 15:45:57 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 16 Jan 2007 09:45:57 -0500
Subject: [R] R on Windows Vista
In-Reply-To: <2E17292A64E6ED418A60BE89326B1AAB3224FD@msgebe11.mfad.mfroot.org>
References: <2E17292A64E6ED418A60BE89326B1AAB3224FD@msgebe11.mfad.mfroot.org>
Message-ID: <45ACE525.4050901@stats.uwo.ca>

On 1/16/2007 9:35 AM, Sicotte, Hugues Ph.D. wrote:
> Did anyone try to run R under Window Vista, especially Windows Vista
> 64bit?

I know someone who has (but I don't know if it was 32 bit or 64 bit). 
He had a little trouble installing, because Vista tightens security 
slightly:  you need to run the installer with higher than default 
permissions.

Whichever version of Vista you run, R will be 32 bit for some time to 
come:  we don't have a 64 bit toolchain available yet, or a volunteer to 
do the builds.  (I'm unlikely to be upgrading to 64 bit Windows in the 
foreseeable future.)

Duncan Murdoch


From ccleland at optonline.net  Tue Jan 16 16:25:44 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 16 Jan 2007 10:25:44 -0500
Subject: [R] problems with for loop
In-Reply-To: <1347.10.100.13.47.1168949410.squirrel@correu.udl.es>
References: <1347.10.100.13.47.1168949410.squirrel@correu.udl.es>
Message-ID: <45ACEE78.4040703@optonline.net>

Jorge Lampurlanes Castel wrote:
> Hello,
> 
> With this program I try to repeat analysis for different years. The
> results of the analysis are not printed when in the loop, except for the
> year sequence. What is wrong?
> 
> Thanks a lot.
> 
> for (i in 92:99){
> 	cat("\n",
> 	"=============================================================\n",
> 	"YEAR =",i,"\n",
> 	"=============================================================\n"
> 	)
> 	canos1 <- subset (canos, canos$YEAR==i)
> 	LinearModel.1 <- lm(MM_P  ~ BLOC  + TIL, data=canos1)
> 	summary(LinearModel.1)
> 	Anova(LinearModel.1, type="II")
> 	Anova(LinearModel.1, type="III")
> 
> }

  You might consider using by() rather than a loop as follows:

library(car)

by(canos, canos$YEAR,
   function(x){fm <- lm(MM_P ~ BLOC + TIL, data=x)
   ans <- list(summary(fm), Anova(fm, type="II"), Anova(fm, type="III"))
   }
)

  The result can be conveniently stored as a list of lists.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From drf5n at maplepark.com  Tue Jan 16 16:49:13 2007
From: drf5n at maplepark.com (David Forrest)
Date: Tue, 16 Jan 2007 09:49:13 -0600 (CST)
Subject: [R] =?utf-8?q?feedback_on_=22writing_r_extensions=22?=
In-Reply-To: <17836.38586.556842.839027@stat.math.ethz.ch>
References: <E1H6WMQ-0005qg-Ql@sys29.mail.msu.edu>
	<17836.38586.556842.839027@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0701160943270.19124@maplepark.com>

On Tue, 16 Jan 2007, Martin Maechler wrote:

>>>>>> "Scott" == Scott Harrison <harris41 at msu.edu>
>>>>>>     on Mon, 15 Jan 2007 13:15:09 -0500 writes:
...
> For "edits", we'd mostly like to receive feedback on the
> *source* of the above automatically produced document
> (which is also the source of the PDF version, same URL as above,
> just replace ".html" by ".pdf")
>
> The development source is always available at
>  https://svn.r-project.org/R/trunk/doc/manual/R-exts.texi

Would it be horrid to include the reference to the sources in the derived 
documents?  It might help with self-documenting the process and help 
demonstrate the benefits of R's open source philosophy.

> Thanking you for offering feedback,

Thank you.

Dave
-- 
  Dr. David Forrest
  drf at vims.edu                                    (804)684-7900w
  drf5n at maplepark.com                             (804)642-0662h
                                    http://maplepark.com/~drf5n/


From ggrothendieck at gmail.com  Tue Jan 16 17:06:55 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 16 Jan 2007 11:06:55 -0500
Subject: [R] feedback on "writing r extensions"
In-Reply-To: <Pine.LNX.4.64.0701160943270.19124@maplepark.com>
References: <E1H6WMQ-0005qg-Ql@sys29.mail.msu.edu>
	<17836.38586.556842.839027@stat.math.ethz.ch>
	<Pine.LNX.4.64.0701160943270.19124@maplepark.com>
Message-ID: <971536df0701160806k52615512u74b0e2f5ad44f821@mail.gmail.com>

It would also be nice if there were links in the .Rd files back
to the source of the commands, not just the source of the
documentation.   That would facilitate
and encourage readers to paruse the source to clarify
the meaning of the help files.

On 1/16/07, David Forrest <drf5n at maplepark.com> wrote:
> On Tue, 16 Jan 2007, Martin Maechler wrote:
>
> >>>>>> "Scott" == Scott Harrison <harris41 at msu.edu>
> >>>>>>     on Mon, 15 Jan 2007 13:15:09 -0500 writes:
> ...
> > For "edits", we'd mostly like to receive feedback on the
> > *source* of the above automatically produced document
> > (which is also the source of the PDF version, same URL as above,
> > just replace ".html" by ".pdf")
> >
> > The development source is always available at
> >  https://svn.r-project.org/R/trunk/doc/manual/R-exts.texi
>
> Would it be horrid to include the reference to the sources in the derived
> documents?  It might help with self-documenting the process and help
> demonstrate the benefits of R's open source philosophy.
>
> > Thanking you for offering feedback,
>
> Thank you.
>
> Dave
> --
>  Dr. David Forrest
>  drf at vims.edu                                    (804)684-7900w
>  drf5n at maplepark.com                             (804)642-0662h
>                                    http://maplepark.com/~drf5n/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From a9947883 at unet.univie.ac.at  Tue Jan 16 17:15:59 2007
From: a9947883 at unet.univie.ac.at (Marco Helbich)
Date: Tue, 16 Jan 2007 17:15:59 +0100 (CET)
Subject: [R] parallel coordinates plot
Message-ID: <2636.193.170.84.103.1168964159.squirrel@webmail.univie.ac.at>

Dear List,

I want to make a parallel coordinates plot with the specific variables on
the abscissa and the cases on the ordinate should be dyed dependent on
another nominal variable from the data frame. I use the parcoord function.

Thanks for your help!

Best regards,
Marco


From bbands at gmail.com  Tue Jan 16 17:16:20 2007
From: bbands at gmail.com (BBands)
Date: Tue, 16 Jan 2007 08:16:20 -0800
Subject: [R] plot portion of a line
Message-ID: <6e8360ad0701160816k3abc8b08jfea1fe83a8f2f2c1@mail.gmail.com>

Dear HelpeRs,

Given:
x <- rnorm(50)
y <- rnorm(50)
plot(x,y)
abline(lm(x ~ y))

Is there a way to plot just a portion of the line? Say for values of x
> 2.0 or x > -2.0 and x < 4.0. (Still fitting all the points.)

Thank you,

    jab
-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.


From dimitris.rizopoulos at med.kuleuven.be  Tue Jan 16 17:32:06 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 16 Jan 2007 17:32:06 +0100
Subject: [R] plot portion of a line
References: <6e8360ad0701160816k3abc8b08jfea1fe83a8f2f2c1@mail.gmail.com>
Message-ID: <00a701c7398b$dcb18f90$0540210a@www.domain>

you could try something like the following:

x <- rnorm(50)
y <- rnorm(50)
obj <- lm(y ~ x)

par(mfrow = c(2, 2))
plot(x, y, main = "x < -1")
x. <- c(min(x), -1)
y. <- predict(obj, data.frame(x = x.))
lines(x., y.)

plot(x, y, main = "x > 1")
x. <- c(1, max(x))
y. <- predict(obj, data.frame(x = x.))
lines(x., y.)

plot(x, y, main = "x > -1 & x < 1")
x. <- c(-1, 1)
y. <- predict(obj, data.frame(x = x.))
lines(x., y.)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "BBands" <bbands at gmail.com>
To: "R-Help" <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 16, 2007 5:16 PM
Subject: [R] plot portion of a line


> Dear HelpeRs,
>
> Given:
> x <- rnorm(50)
> y <- rnorm(50)
> plot(x,y)
> abline(lm(x ~ y))
>
> Is there a way to plot just a portion of the line? Say for values of 
> x
>> 2.0 or x > -2.0 and x < 4.0. (Still fitting all the points.)
>
> Thank you,
>
>    jab
> -- 
> John Bollinger, CFA, CMT
> www.BollingerBands.com
>
> If you advance far enough, you arrive at the beginning.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ggrothendieck at gmail.com  Tue Jan 16 17:36:06 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 16 Jan 2007 11:36:06 -0500
Subject: [R] plot portion of a line
In-Reply-To: <6e8360ad0701160816k3abc8b08jfea1fe83a8f2f2c1@mail.gmail.com>
References: <6e8360ad0701160816k3abc8b08jfea1fe83a8f2f2c1@mail.gmail.com>
Message-ID: <971536df0701160836n202ed5efpa0b0289956c74371@mail.gmail.com>

Use segments. In the following we overlay the black abline with
a wider red segments line segment:


set.seed(1)
x <- rnorm(50)
y <- rnorm(50)

plot(y ~ x)

y.lm <- lm(y ~ x)

abline(y.lm) # omit this line if black abline not wanted

x0 <- c(-2, 4)
y0 <- predict(y.lm, list(x = x0))
segments(x0[1], y0[1], x0[2], y0[2], col = "red", lwd = 2)



On 1/16/07, BBands <bbands at gmail.com> wrote:
> Dear HelpeRs,
>
> Given:
> x <- rnorm(50)
> y <- rnorm(50)
> plot(x,y)
> abline(lm(x ~ y))
>
> Is there a way to plot just a portion of the line? Say for values of x
> > 2.0 or x > -2.0 and x < 4.0. (Still fitting all the points.)
>
> Thank you,
>
>    jab
> --
> John Bollinger, CFA, CMT
> www.BollingerBands.com
>
> If you advance far enough, you arrive at the beginning.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Pierre.Lapointe at nbf.ca  Tue Jan 16 17:37:07 2007
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Tue, 16 Jan 2007 11:37:07 -0500
Subject: [R] system(mysql... Does not recognize < as passing an attribute
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0DF364D8@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070116/1120a8f3/attachment.pl 

From Pierre.Lapointe at nbf.ca  Tue Jan 16 17:42:58 2007
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Tue, 16 Jan 2007 11:42:58 -0500
Subject: [R] system(mysql... Does not recognize < as passing an attribute
	(No HTML)
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0DF364D9@lbmsg002.fbn-nbf.local>

Hi, 

This is my command line request: mysql -u root -ppassword -D quant
<c:/cline.txt
This line works from the command line in windows.

In R, when I try to use the system function, it does not work,

> system(paste('mysql -u root -ppassword -D quant
<c:/cline.txt'),show.output.on.console = TRUE) 
ERROR 1102 (42000): Incorrect database name '<c:/cline.txt'

It seems that the "<" character is not recognized as an attribute.

Thanks,

Pierre

> version
               _                           
platform       i386-pc-mingw32             
arch           i386                        
os             mingw32                     
system         i386, mingw32               
status                                     
major          2                           
minor          4.1                         
year           2006                        
month          12                          
day            18                          
svn rev        40228                       
language       R                           
version.string R version 2.4.1 (2006-12-18)

**************************************************
AVIS DE NON-RESPONSABILITE: Ce document transmis par courrie...{{dropped}}


From Charles.Annis at StatisticalEngineering.com  Tue Jan 16 17:47:11 2007
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Tue, 16 Jan 2007 11:47:11 -0500
Subject: [R] R on Windows Vista
In-Reply-To: <2E17292A64E6ED418A60BE89326B1AAB3224FD@msgebe11.mfad.mfroot.org>
References: <2E17292A64E6ED418A60BE89326B1AAB3224FD@msgebe11.mfad.mfroot.org>
Message-ID: <024901c7398d$f84fcda0$6400a8c0@DD4XFW31>

I've run R on the 32 bit version of Vista.  Because of Vista's extra
security controls you can't install a library easily, however, even though I
had not trouble installing R.  You must first set the security level to
allow it (I can't recall the details but it's not to difficult to figure
out), then install the libraries, then re-set the security levels.  

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sicotte, Hugues Ph.D.
Sent: Tuesday, January 16, 2007 9:36 AM
To: r-help at stat.math.ethz.ch
Subject: [R] R on Windows Vista

Did anyone try to run R under Window Vista, especially Windows Vista
64bit?

Thanks.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From JelmerAnne.Elzinga at unil.ch  Tue Jan 16 18:00:41 2007
From: JelmerAnne.Elzinga at unil.ch (Jelmer Elzinga)
Date: Tue, 16 Jan 2007 18:00:41 +0100
Subject: [R] nested hierarchical design
Message-ID: <45AD04B9.4090506@unil.ch>

Dear R-Helpers,

I would like to know what syntax I need to use to do a nested anova for 
1. a continuous variable and 2. count data (x out of y)
1. The first I used to do in SPSS and I would like to be able to do it 
in R as well.
This is the hierarchical model I would like to use: a continuous 
variable explained by factor A(fixed) + factor B(random) nested in A + 
factor C (random) nested in factor B (which is nested in A).
The data is unbalanced at all levels, meaning different nrs of B per A, 
different nrs of C for B, different nrs of values per C
In SPSS this seems to be quite simple with a small writing in the syntax 
(see below). But how do I get the same results in R with lme?

SPSS syntax:
UNIANOVA
  var  BY A B C
  /RANDOM = B C
  /METHOD = SSTYPE(3)
  /INTERCEPT = INCLUDE
  /CRITERIA = ALPHA(.05)
  /DESIGN = A B(A) C(B(A))).

2. the same model but than for count data (like 15 out of 30, 23 out of 
60) instead of the continous variable(I know the basics of glm in R)

Thanks a lot for your help
Jelmer


From ripley at stats.ox.ac.uk  Tue Jan 16 18:05:13 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 16 Jan 2007 17:05:13 +0000 (GMT)
Subject: [R] system(mysql... Does not recognize < as passing an attribute
In-Reply-To: <834204C0D7C6D611A3BB000255FC6E9D0DF364D8@lbmsg002.fbn-nbf.local>
References: <834204C0D7C6D611A3BB000255FC6E9D0DF364D8@lbmsg002.fbn-nbf.local>
Message-ID: <Pine.LNX.4.64.0701161658480.3640@gannet.stats.ox.ac.uk>

You are on Windows, and that is how system() works on Windows (not 
just in R, but in C, Perl ...).  The help says

command: the system command to be invoked, as a string.

A command line is not a 'system command', and the < is interpreted by the 
shell you are running as your 'command line in WIndows'.

Try shell() instead.

On Tue, 16 Jan 2007, Lapointe, Pierre wrote:

> Hi,
>
> This is my command line request: mysql -u root -ppassword -D quant
> <c:/cline.txt
> This line works from the command line in windows.
>
> In R, when I try to use the system function, it does not work,
>
>> system(paste('mysql -u root -ppassword -D quant
> <c:/cline.txt'),show.output.on.console = TRUE)
> ERROR 1102 (42000): Incorrect database name '<c:/cline.txt'
>
> It seems that the "<" caracter is not recognized as an attribute.
>
> Thanks,
>
> Pierre
>
>> version
>               _
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          4.1
> year           2006
> month          12
> day            18
> svn rev        40228
> language       R
> version.string R version 2.4.1 (2006-12-18)
>
>
> **************************************************
> AVIS DE NON-RESPONSABILITE: Ce document transmis par courrie...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From wwwhsd at gmail.com  Tue Jan 16 18:10:24 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Tue, 16 Jan 2007 15:10:24 -0200
Subject: [R] system(mysql... Does not recognize < as passing an
	attribute (No HTML)
In-Reply-To: <834204C0D7C6D611A3BB000255FC6E9D0DF364D9@lbmsg002.fbn-nbf.local>
References: <834204C0D7C6D611A3BB000255FC6E9D0DF364D9@lbmsg002.fbn-nbf.local>
Message-ID: <da79af330701160910j7f48877exf2b266416321b1b5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070116/cc2ae4bc/attachment.pl 

From info at aghmed.fsnet.co.uk  Tue Jan 16 18:13:24 2007
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Tue, 16 Jan 2007 17:13:24 +0000
Subject: [R] How to format R code in LaTex documents
In-Reply-To: <45AB8BD1.5000508@vanderbilt.edu>
References: <b75d67340701150542m43aa4d2rb0af6fd6b022cbf0@mail.gmail.com>
	<45AB8BD1.5000508@vanderbilt.edu>
Message-ID: <7.0.0.16.0.20070116171044.01963858@aghmed.fsnet.co.uk>

At 14:12 15/01/2007, Frank E Harrell Jr wrote:
>Benjamin Dickgiesser wrote:
>>Hi,
>>I am planning on putting some R script in an appendix of a LaTex
>>document. Can anyone recommend me a way of how to format it? Is there
>>a way to keep all line breaks without having to insert \\ in every
>>single line?
>>Thank you!
>>Benjamin
>
>Here's one way and I would appreciate anyone's 
>improvements.  I've also included solutions from 
>two others.  Please let me know what you decide to use.  -Frank

Some interesting ideas below so just to add that 
I find I often have to use the linewidth 
parameter to avoid lines wrapping. This is 
valuable if you are also going to include bits of 
R output as well (you can control the width of 
your own code fragments of course).


>\usepackage{listings,relsize}
>\lstloadlanguages{R}
>\lstset{language=R,basicstyle=\smaller[2],commentstyle=\rmfamily\smaller,
>  showstringspaces=false,%
>  xleftmargin=4ex,literate={<-}{{$\leftarrow$}}1 {~}{{$\sim$}}1}
>\lstset{escapeinside={(*}{*)}}   % for (*\ref{ }*) inside lstlistings (S code)
>
>. . .
>\begin{lstlisting}
>. . . S code . . .
>\end{lstlisting}
>
>The following code was provided by Vincent Goulet:
>
>
>listings is a great package to highlight R keywords and comments and --- that
>was my main use of the package --- index those keywords. I found that I had
>to slightly redefine the list of keywords included in listings. I still did
>not take the time to submit a patch to the author, though...
>
>In any case, here's what I use, if it can be of any help to anyone:
>
>\lstloadlanguages{R}
>\lstdefinelanguage{Renhanced}[]{R}{%
>   morekeywords={acf,ar,arima,arima.sim,colMeans,colSums,is.na,is.null,%
>     mapply,ms,na.rm,nlmin,replicate,row.names,rowMeans,rowSums,seasonal,%
>     sys.time,system.time,ts.plot,which.max,which.min},
>   deletekeywords={c},
>   alsoletter={.\%},%
>   alsoother={:_\$}}
>\lstset{language=Renhanced,extendedchars=true,
>   basicstyle=\small\ttfamily,
>   commentstyle=\textsl,
>   keywordstyle=\mdseries,
>   showstringspaces=false,
>   index=[1][keywords],
>   indexstyle=\indexfonction}
>
>with
>
>   \newcommand{\indexfonction}[1]{\index{#1@\texttt{#1}}}
>
>-- Vincent Goulet, Associate Professor ?cole 
>d'actuariat Universit? Laval, Qu?bec 
>Vincent.Goulet at act.ulaval.ca http://vgoulet.act.ulaval.ca
>
>Anupam Tyagi provided the following:
>
>\documentclass{report}
>\usepackage{listings}
>\begin{document}
>
>Somethings .....
>
>\lstset{% general command to set parameter(s)
>basicstyle=\small, % print whole in small
>stringstyle=\ttfamily, % typewriter type for strings
>numbers=left, % numbers on the left
>numberstyle=\tiny, % Tiny numbers
>stepnumber=2, % number every second line of code
>numbersep=5pt, % 5pt seperation between numbering and code listing
>language=R }
>
>\lstinputlisting{text1.R}
>
>\end{document}
>
>

Michael Dewey
http://www.aghmed.fsnet.co.uk


From goncalo.carrera at dq.fct.unl.pt  Tue Jan 16 12:41:03 2007
From: goncalo.carrera at dq.fct.unl.pt (=?iso-8859-1?b?R29u52Fsbw==?= Carrera)
Date: Tue, 16 Jan 2007 11:41:03 +0000
Subject: [R] (no subject)
Message-ID: <1168947663.45acb9cf183ef@webmail.dq.fct.unl.pt>

I'm calculating nodes using Random Forest in R but i only get nodes for a
fraction of the compounds i want to calculate, the rest is ommited and is not
printed in the output file, (i'm working with 3012 compounds). What can i do to
have nodes printed for all the compounds. Thanks

Gon?alo


From BEN at SSANET.COM  Tue Jan 16 18:23:44 2007
From: BEN at SSANET.COM (Ben Fairbank)
Date: Tue, 16 Jan 2007 11:23:44 -0600
Subject: [R] RODBC:  sqlQuery is successful,
	but a similar sqlFetch returns error
Message-ID: <CA612484A337C6479EA341DF9EEE14AC05CC3B9D@hercules.ssainfo>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070116/971b6b3f/attachment.pl 

From ligges at statistik.uni-dortmund.de  Tue Jan 16 18:38:29 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 16 Jan 2007 18:38:29 +0100
Subject: [R] R on Windows Vista
In-Reply-To: <024901c7398d$f84fcda0$6400a8c0@DD4XFW31>
References: <2E17292A64E6ED418A60BE89326B1AAB3224FD@msgebe11.mfad.mfroot.org>
	<024901c7398d$f84fcda0$6400a8c0@DD4XFW31>
Message-ID: <45AD0D95.9030305@statistik.uni-dortmund.de>



Charles Annis, P.E. wrote:
> I've run R on the 32 bit version of Vista.  Because of Vista's extra
> security controls you can't install a library 

You mean you can't install a *package* into a library.
Are you sure that you cannot install into some other library that is in 
some directory where you have got write access to?

Uwe Ligges




easily, however, even though I
> had not trouble installing R.  You must first set the security level to
> allow it (I can't recall the details but it's not to difficult to figure
> out), then install the libraries, then re-set the security levels.  
> 
> Charles Annis, P.E.
> 
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>  
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sicotte, Hugues Ph.D.
> Sent: Tuesday, January 16, 2007 9:36 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] R on Windows Vista
> 
> Did anyone try to run R under Window Vista, especially Windows Vista
> 64bit?
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Tue Jan 16 18:39:27 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 16 Jan 2007 12:39:27 -0500
Subject: [R] plot portion of a line
In-Reply-To: <971536df0701160836n202ed5efpa0b0289956c74371@mail.gmail.com>
References: <6e8360ad0701160816k3abc8b08jfea1fe83a8f2f2c1@mail.gmail.com>
	<971536df0701160836n202ed5efpa0b0289956c74371@mail.gmail.com>
Message-ID: <971536df0701160939u7253e8e2k2bc770e3b4c7abf1@mail.gmail.com>

As in Dmitris' post lines is a somewhat more succint so here it is
again replacing segments with lines:


set.seed(1)

x <- rnorm(50)
y <- rnorm(50)


plot(y ~ x)

y.lm <- lm(y ~ x)

abline(y.lm) # omit this line if black abline not wanted

x0 <- c(-2, 4)
y0 <- predict(y.lm, list(x = x0))
lines(y0 ~ x0, col = "red", lwd = 2)


On 1/16/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Use segments. In the following we overlay the black abline with
> a wider red segments line segment:
>
>
> set.seed(1)
> x <- rnorm(50)
> y <- rnorm(50)
>
> plot(y ~ x)
>
> y.lm <- lm(y ~ x)
>
> abline(y.lm) # omit this line if black abline not wanted
>
> x0 <- c(-2, 4)
> y0 <- predict(y.lm, list(x = x0))
> segments(x0[1], y0[1], x0[2], y0[2], col = "red", lwd = 2)
>
>
>
> On 1/16/07, BBands <bbands at gmail.com> wrote:
> > Dear HelpeRs,
> >
> > Given:
> > x <- rnorm(50)
> > y <- rnorm(50)
> > plot(x,y)
> > abline(lm(x ~ y))
> >
> > Is there a way to plot just a portion of the line? Say for values of x
> > > 2.0 or x > -2.0 and x < 4.0. (Still fitting all the points.)
> >
> > Thank you,
> >
> >    jab
> > --
> > John Bollinger, CFA, CMT
> > www.BollingerBands.com
> >
> > If you advance far enough, you arrive at the beginning.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From ripley at stats.ox.ac.uk  Tue Jan 16 18:58:05 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 16 Jan 2007 17:58:05 +0000 (GMT)
Subject: [R] RODBC:  sqlQuery is successful,
 but a similar sqlFetch returns error
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05CC3B9D@hercules.ssainfo>
References: <CA612484A337C6479EA341DF9EEE14AC05CC3B9D@hercules.ssainfo>
Message-ID: <Pine.LNX.4.64.0701161754330.27616@gannet.stats.ox.ac.uk>

'.' is invalid in an SQL table name.  I don't know what driver you are 
using (you seem very reluctant to tell us anything useful about your 
DBMS), but dbo.vwC1198_2006_RawData_With_CMPL_EXCL probably means table 
vwC1198_2006_RawData_With_CMPL_EXCL in database dbo, and that is valid as 
part of SQL, but not in your usage in sqlFetch.

On Tue, 16 Jan 2007, Ben Fairbank wrote:

> Greetings guRus --
>
>
>
> I have successfully queried a large (24,445 rows by 281 cols.) in-house
> database using the following RODBC query (without the line breaks)
>
>
>
> testout <- sqlQuery(channel, "select idSchedule,EXCL_Total from
> dbo.vwC1198_2006_RawData_With_CMPL_EXCL")
>
>
>
> This returns a dataframe of 24445 rows and two columns (as intended),
> but the following command
>
>
>
> testout <-
> sqlFetch(channel,"dbo.vwC1198_2006_RawData_With_CMPL_EXCL",colnames =
> TRUE,rownames = "idSchedule")
>
>
>
> returns the error message
>
>
>
> in odbcTableExists(channel, sqtable) :
>
>        'dbo.vwC1198_2006_RawData_With_CMPL_EXCL': table not found on
> channel
>
>
>
> The value of channel did not change between the two commands.
>
>
>
> Based on the help files for sqlFetch and sqlQuery, it is not clear to me
> why one command would find the table and the other would not.  I am
> running R 2.4.1 on a Windows XP pro machine with 2 gig of memory.
>
>
>
> Thanks for any suggestions or hints,
>
>
>
> Ben Fairbank
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From helprhelp at gmail.com  Tue Jan 16 19:00:44 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Tue, 16 Jan 2007 13:00:44 -0500
Subject: [R] number of Nodes in Random Forest
In-Reply-To: <1168953616.45acd11102c26@webmail.dq.fct.unl.pt>
References: <1168953616.45acd11102c26@webmail.dq.fct.unl.pt>
Message-ID: <cdf817830701161000g5e1a2a24y3286f634baf02a4b@mail.gmail.com>

Hi, Goncalo:

did u use compounds as features when building rf? If so, some
compounds are NOT randomly selected in splitting nodes, which might
explain what you observed. But you need to provide more info for us to
help you, like those parameters you used to build rf.

You can use varImpPlot to see what exactly compounds were used by
setting n.var (see ?varImpPlot) or you can use
nrow(YourTrainingX$importance) to get that number too.

HTH,

w.

On 1/16/07, Gon?alo Carrera <goncalo.carrera at dq.fct.unl.pt> wrote:
> I'm calculating nodes using Random Forest in R but i only get nodes for a
> fraction of the compounds i want to calculate, the rest is ommited and is not
> printed in the output file, (i'm working with 3012 compounds). What can i do to
> have nodes printed for all the compounds. Thanks
>
> Gon?alo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From jlampur at eagrof.UdL.es  Tue Jan 16 19:15:55 2007
From: jlampur at eagrof.UdL.es (Jorge Lampurlanes Castel)
Date: Tue, 16 Jan 2007 19:15:55 +0100 (CET)
Subject: [R]  LSD multiple comparison test
Message-ID: <3322.213.0.196.176.1168971355.squirrel@correu.udl.es>

Hello,

There is any way of performing a LSD (Least Significant Diference)
multiple comparison test to separate the levels of the significant factors
of a model? I am migrating from SAS.

Thanks a lot.

-- 
**************************************************
Jorge Lampurlan?s Castel
Departament d'Enginyeria Agroforestal
Escola T?cnica Superior d'Enginyeria Agr?ria
Universitat de Lleida
Avinguda Rovira Roure, 191
25198-LLEIDA
SPAIN

Tl.: +34 973 70 25 37
Fax.:+34 073 70 26 73
e-mail: jlampur at eagrof.udl.es


From rmh at temple.edu  Tue Jan 16 19:50:04 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Tue, 16 Jan 2007 13:50:04 -0500 (EST)
Subject: [R] LSD multiple comparison test
Message-ID: <20070116135004.BSE90167@po-d.temple.edu>

Look at the glht function in the multcomp package and the
MMC functions in the HH package.

Rich


From tlumley at u.washington.edu  Tue Jan 16 19:57:13 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 16 Jan 2007 10:57:13 -0800 (PST)
Subject: [R] Is it the PPS samples i needed in R?
In-Reply-To: <2fc17e30701120709m2699040ah6e3dc25e1dffac5f@mail.gmail.com>
References: <2fc17e30701120709m2699040ah6e3dc25e1dffac5f@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701161056380.31153@homer21.u.washington.edu>

On Fri, 12 Jan 2007, zhijie zhang wrote:

> Dear friends,
>  I want to do a unequal probability sampling, that is, Probability
> Proportionate to size, Is it right for the following programs?
> Say my original dataset is:
>
> ID  Population
> 1     100
> 2     200
> 3     300
> IF the population is large ,then the corresponding ID has the large
> Probability to be selected.
>
> sample(A$ID, size=2, replace = FALSE, prob = A$population)
> #suppose the dataset name is A.
> Is it the PPS samples  i needed ?

No, this does not give PPS samples for size>1.  The "pps" and "sampling" 
packages have code for PPS samples.

 	-thomas


From drf5n at maplepark.com  Tue Jan 16 20:31:14 2007
From: drf5n at maplepark.com (David Forrest)
Date: Tue, 16 Jan 2007 13:31:14 -0600 (CST)
Subject: [R] feedback on "writing r extensions"
In-Reply-To: <971536df0701160806k52615512u74b0e2f5ad44f821@mail.gmail.com>
References: <E1H6WMQ-0005qg-Ql@sys29.mail.msu.edu>
	<17836.38586.556842.839027@stat.math.ethz.ch>
	<Pine.LNX.4.64.0701160943270.19124@maplepark.com>
	<971536df0701160806k52615512u74b0e2f5ad44f821@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701161322060.20503@maplepark.com>

On Tue, 16 Jan 2007, Gabor Grothendieck wrote:

> It would also be nice if there were links in the .Rd files back
> to the source of the commands, not just the source of the
> documentation.   That would facilitate
> and encourage readers to paruse the source to clarify
> the meaning of the help files.

Perhaps it would be feasible to make an RSourceSearch() function (like 
RSiteSearch()) that could recognize things like documentation, functions, 
and internals and direct users to the source code.

Dave

> On 1/16/07, David Forrest <drf5n at maplepark.com> wrote:
>> On Tue, 16 Jan 2007, Martin Maechler wrote:
>>
>>>>>>>> "Scott" == Scott Harrison <harris41 at msu.edu>
>>>>>>>>     on Mon, 15 Jan 2007 13:15:09 -0500 writes:
>> ...
>>> For "edits", we'd mostly like to receive feedback on the
>>> *source* of the above automatically produced document
>>> (which is also the source of the PDF version, same URL as above,
>>> just replace ".html" by ".pdf")
>>>
>>> The development source is always available at
>>>  https://svn.r-project.org/R/trunk/doc/manual/R-exts.texi
>>
>> Would it be horrid to include the reference to the sources in the derived
>> documents?  It might help with self-documenting the process and help
>> demonstrate the benefits of R's open source philosophy.
>>
>>> Thanking you for offering feedback,
>>
>> Thank you.
>>
>> Dave
>> --
>>  Dr. David Forrest
>>  drf at vims.edu                                    (804)684-7900w
>>  drf5n at maplepark.com                             (804)642-0662h
>>                                    http://maplepark.com/~drf5n/
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
  Dr. David Forrest
  drf at vims.edu                                    (804)684-7900w
  drf5n at maplepark.com                             (804)642-0662h
                                    http://maplepark.com/~drf5n/


From robert at sanctumfi.com  Tue Jan 16 20:44:23 2007
From: robert at sanctumfi.com (Robert Sams)
Date: Tue, 16 Jan 2007 19:44:23 -0000
Subject: [R] Rgui crashes when calling odbcClose()
Message-ID: <3ABD97F82224D64A81DB6860B40E506C13A9A1@blommbergmds>

Hi,

I'm running Rgui v2.4.0, RODBC v1.1-8, and psqlODBC v8.2.0002. Whenever
I do the following

chan <- odbcConnect("mydatabase", uid="admin") # connection to a
postgresql db on a linux box
tmp <- sqlQuery(chan, "select * from sometable;") # this is successfull
odbcClose(chan)  # this crashes Rgui and raises an error window
containing the following MS error signature:

	AppName: rgui.exe AppVer 2.4.39566.0 	ModName: psqlodbc35w.dll
	ModVer: 8.2.0.2    Offset: 00003248

I didn't find anything relevant in the R Windows FAQ or a few queries of
the help archives; appologies if this has been addressed before.

Robert


From Greg.Snow at intermountainmail.org  Tue Jan 16 21:19:01 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 16 Jan 2007 13:19:01 -0700
Subject: [R] image() and nonsquare matrices
Message-ID: <07E228A5BE53C24CAD490193A7381BBB7B29C1@LP-EXCHVS07.CO.IHC.COM>

Look at the "squishplot" function in the TeachingDemos package.  Does
this set of commands work for you?

> library(TeachingDemos)
> squishplot(c(0.5,20.5),c(0.5,5.5),1)
> image(1:20,1:5,a,asp=1,xlab="label here")
> box()

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robin Hankin
> Sent: Friday, January 12, 2007 3:13 AM
> To: RHelp help
> Subject: [R] image() and nonsquare matrices
> 
> How do I draw non-square matrices with image() and get the axes right?
> 
> 
> Try 1:
> 
>   a <- matrix(rnorm(100),20,5)
> image(1:20,1:5,a,asp=1,xlab="label here") # No good because 
> the axes don't touch the image
> 
> 
> 
> Try 2:
> 
> image(1:20,1:5,a,asp=1,axes=F,xlab="label here")
> axis(side=1,pos=0)
> # No good because the x axis label is floating far from the x axis.
> 
> 
> 
> Try 3:
>   image(1:20,1:5,a,asp=1,axes=F,xlab="",ylab="")
>   axis(side=1,pos=0)
> # No good because the x axis label is absent.
> 
> 
> How to use image() with a non-square matrix and make axes and 
> labels appear correctly?
> 
> 
> 
> 
> 
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton European Way, 
> Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Greg.Snow at intermountainmail.org  Tue Jan 16 21:35:43 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 16 Jan 2007 13:35:43 -0700
Subject: [R] labels outliers in boxplot
Message-ID: <07E228A5BE53C24CAD490193A7381BBB7B29D7@LP-EXCHVS07.CO.IHC.COM>

Some information on the outliers is returned from the boxplot function.  Try something like:

set.seed(123)
tmp <- data.frame( group=gl(3,10), 
  y=rcauchy(30), sex=gl(2,5,30,c('M','F')) )

tmp2 <- boxplot( split(tmp$y,tmp$group) )
identify( tmp2$group, tmp2$out, tmp2$group )

Or if your grouping variable works out to the same values as the x axis (used integers) and you want to specify a 3rd variable to be the labels you can do:

Identify( tmp$group, tmp$y, tmp$sex )

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> antoniababe at yahoo.se
> Sent: Wednesday, January 10, 2007 2:08 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] labels outliers in boxplot
> 
> Dear R-users,
> 
> Following is part of my data, where slide has 36 levels and 
> block 48 levels. I have done boxplot for each slide on the 
> same graph. There are outliers for each slide and I tried to 
> use indentify functtion to identify outliers in such a way 
> that when I click on an outlier or point, the points will be 
> labelled by either their block or ID or by both but without 
> success. How can I make it work or are there other ways to do 
> it than using identify function?
> 
> 
> Thanks in advance, 
> 
> Jenny,
> 
> dat1[1:10,]
>              y Slide Block              ID Control
> 1   0.03147823     1     1       IgG-human       5
> 2  -0.23815974     1     1 LPPAANDVSVLTAAR       0
> 3  -0.71926359     1     1 HTKHYRVVSKPAALV       0
> 4  -0.14607826     1     1 FVALPAATADAYATT       0
> 5   0.89553073     1     1 NYPAMMAHAGDMAGY       0
> 6  -0.67587100     1     1 RRALRQIGVLERPVG       0
> 7   0.32636034     1     1 DCGTIRVGSFRGRWL       0
> 8  -1.44057259     1     1 MAKLSTDELLDAFKE       0
> 9  -0.37064338     1     1 LELSDFVKKFEETFE       0
> 10 -0.20387233     1     1 VSRRAKVDVLIVHTT       0
> 
> 
>  tb_ncs<-subset(dat1,dat1$Control==1)     ### this
> data contains only negative controls
> 
>        par(las=2,mar=c(10.1,4.1,4.1,2.1)) 
>           
> boxplot(split(tb_ncs$y,tb_ncs$Slide),col="orange",
> cex=.65,
>                    outline=TRUE,main="Negative control 
> response of each patient", cex.main=1, font.main=1,
>                             col.main="blue",
> names=c(1:35,"B"))
>                    grid(nx=NA, ny=NULL)               
>           ### grid over boxplot 
>            legend("bottomright", "B = Buffer +
> sec",text.col="blue")
>                
> 
>             out.block<-
> identify(tb_ncs$y,tb_ncs$Slide) 
> 
> 
> 
> 	
> 	
> 		
> _________________________________________________________
> Flyger tiden iv?g? F?nga dagen med Yahoo! Mails inbyggda
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Thomas.Petzoldt at TU-Dresden.de  Tue Jan 16 22:24:05 2007
From: Thomas.Petzoldt at TU-Dresden.de (Thomas Petzoldt)
Date: Tue, 16 Jan 2007 22:24:05 +0100
Subject: [R] Reminder:  JSS Special volume R in ecology
Message-ID: <45AD4275.3050804@TU-Dresden.de>

Dear UseRs,

one of the outcomes of the useR! 2006 conference held in Vienna was that 
JSS, the Journal of Statistical Software, is planning to publish a 
series of special volumes. They will be guest edited and each volume 
will have 5-10 issues (articles) of the usual JSS format.

One of the volumes is related to the use of the R software in ecology.

The submission deadline will end in one month on 2007-02-15. Please 
consult the call for papers in the R Wiki for details:

http://wiki.r-project.org/rwiki/doku.php?id=misc:r_in_ecology_and_ecological_modelling


Thomas Kneib & Thomas Petzoldt, guest editors


The original CFP was posted on Fri, 6 Oct 2006:
   https://listserv.umd.edu/cgi-bin/wa?A2=ind0610a&L=ecolog-l&P=11582


-- 
Thomas Petzoldt
Technische Universitaet Dresden
Institut fuer Hydrobiologie        thomas.petzoldt at tu-dresden.de
01062 Dresden                      http://tu-dresden.de/hydrobiologie/
GERMANY


From Greg.Snow at intermountainmail.org  Tue Jan 16 22:25:04 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 16 Jan 2007 14:25:04 -0700
Subject: [R] Controlling size of boxplot when it is added in a plot
Message-ID: <07E228A5BE53C24CAD490193A7381BBB7B29EC@LP-EXCHVS07.CO.IHC.COM>

One approach is to use the subplot function from the TeachingDemos
package.  I could only get everything to line up properly if I specified
the xlim argument to hist directly and the same value as the ylim
argument to boxplot.

Try this:

> x <- rnorm(100)
> library(TeachingDemos)
> hist(x, breaks = 20, main = NULL, ylim = c(-2,
max(hs$counts)),xlim=c(-3,3))
> subplot( boxplot(x, horizontal=T, ylim=c(-3,3),axes=F ),
par('usr')[1:2], c(par('usr')[3],0))
> box()

I have seen things like this in teaching situations, but not much in the
non-academic world.

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Charilaos Skiadas
> Sent: Saturday, January 13, 2007 9:12 PM
> To: R-Mailingliste
> Subject: [R] Controlling size of boxplot when it is added in a plot
> 
> Greetings,
> 
> 	I am trying to add a boxplot to the bottom of a 
> histogram, right between the histogram bars and the x axis. 
> Here is the code I am using at the moment (the par line is 
> probably not relevant for our
> discussion):
> 
> hs <- hist(x, breaks = 20, plot = F)
> par(mar = c(3,3,2,1))
> hist(x, breaks = 20, main = NULL, ylim = c(-2, 
> max(hs$counts))) boxplot(x, horizontal = T, axes = T, add = 
> T, at = -1)
> 
> The problem is the following. As it is, the boxplot restricts 
> itself to the -1 line. I would like it to occupy both the -1 
> and the -2 lines ( I guess more generally I would like to 
> control how much vertical space the "embedded boxplot" 
> occupies). I tried to set the width parameter in the boxplot, 
> but that seemed to have no effect at all.
> 
> On an OT note, I haven't seen this way of combining a 
> histogram with a boxplot (perhaps I haven't looked really 
> hard). I thought it would be useful for my students to see 
> them next to each other, to develop a feeling for what 
> histograms might correspond to what boxplots. Is there 
> perhaps some reason why I should avoid showing those graphs 
> to them like that, that I am not aware of? Or just a reason 
> why I haven't seen them combined like this much?
> 
> TIA
> 
> Charilaos Skiadas
> Department of Mathematics
> Hanover College
> P.O.Box 108
> Hanover, IN 47243
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From btyner at stat.purdue.edu  Tue Jan 16 22:30:37 2007
From: btyner at stat.purdue.edu (Benjamin Tyner)
Date: Tue, 16 Jan 2007 16:30:37 -0500
Subject: [R] curious about dimension of 'apply' output when MARGIN=1
Message-ID: <45AD43FD.8050100@stat.purdue.edu>

Reading the documentation for 'apply', I understand the following is 
working exactly as documented:

 > M<-matrix(1:6,ncol=2)
 > M
     [,1] [,2]
[1,]    1    4
[2,]    2    5
[3,]    3    6
 > apply(M,2,function(column) column+c(1,2,3))
     [,1] [,2]
[1,]    2    5
[2,]    4    7
[3,]    6    9
 > apply(M,1,function(row) row+c(1,2))
     [,1] [,2] [,3]
[1,]    2    3    4
[2,]    6    7    8

I'm not proposing any changes or extra arguments to 'apply'. Rather, I'm 
wondering what is the benefit for (or rationale behind) this somewhat 
unintuitive behavior in the case that MARGIN=1.

Thanks,
Ben


From Greg.Snow at intermountainmail.org  Tue Jan 16 23:20:52 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 16 Jan 2007 15:20:52 -0700
Subject: [R] plot portion of a line
Message-ID: <07E228A5BE53C24CAD490193A7381BBB7B2A05@LP-EXCHVS07.CO.IHC.COM>

Try the clipplot function from the TeachingDemos package:

> x <- rnorm(50)
> y <- rnorm(50)
> plot(x,y)
> clipplot( abline(lm(y~x), col='red'), xlim=c(1,3))
> clipplot( abline(lm(y~x), col='blue'), xlim=c(-2,1))


Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of BBands
> Sent: Tuesday, January 16, 2007 9:16 AM
> To: R-Help
> Subject: [R] plot portion of a line
> 
> Dear HelpeRs,
> 
> Given:
> x <- rnorm(50)
> y <- rnorm(50)
> plot(x,y)
> abline(lm(x ~ y))
> 
> Is there a way to plot just a portion of the line? Say for values of x
> > 2.0 or x > -2.0 and x < 4.0. (Still fitting all the points.)
> 
> Thank you,
> 
>     jab
> --
> John Bollinger, CFA, CMT
> www.BollingerBands.com
> 
> If you advance far enough, you arrive at the beginning.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Wed Jan 17 01:16:56 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 16 Jan 2007 19:16:56 -0500
Subject: [R] curious about dimension of 'apply' output when MARGIN=1
In-Reply-To: <45AD43FD.8050100@stat.purdue.edu>
References: <45AD43FD.8050100@stat.purdue.edu>
Message-ID: <971536df0701161616n5d586fedhdbbdd29e3131f8dd@mail.gmail.com>

The reshape package has an idempotent apply, iapply:

> library(reshape)
> iapply(M,1,function(row) row+c(1,2))
     [,1] [,2]
[1,]    2    6
[2,]    3    7
[3,]    4    8

On 1/16/07, Benjamin Tyner <btyner at stat.purdue.edu> wrote:
> Reading the documentation for 'apply', I understand the following is
> working exactly as documented:
>
>  > M<-matrix(1:6,ncol=2)
>  > M
>     [,1] [,2]
> [1,]    1    4
> [2,]    2    5
> [3,]    3    6
>  > apply(M,2,function(column) column+c(1,2,3))
>     [,1] [,2]
> [1,]    2    5
> [2,]    4    7
> [3,]    6    9
>  > apply(M,1,function(row) row+c(1,2))
>     [,1] [,2] [,3]
> [1,]    2    3    4
> [2,]    6    7    8
>
> I'm not proposing any changes or extra arguments to 'apply'. Rather, I'm
> wondering what is the benefit for (or rationale behind) this somewhat
> unintuitive behavior in the case that MARGIN=1.
>
> Thanks,
> Ben
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rchulyadyo at gmail.com  Wed Jan 17 06:22:18 2007
From: rchulyadyo at gmail.com (Rupendra Chulyadyo)
Date: Wed, 17 Jan 2007 11:07:18 +0545
Subject: [R] Help on variable ranking
Message-ID: <b5a9b2f50701162122k1b8dce9cyc8c4f1af40f8d063@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070117/ec3ecec8/attachment.pl 

From A.Robinson at ms.unimelb.edu.au  Wed Jan 17 06:45:43 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 17 Jan 2007 16:45:43 +1100
Subject: [R] Help on variable ranking
In-Reply-To: <b5a9b2f50701162122k1b8dce9cyc8c4f1af40f8d063@mail.gmail.com>
References: <b5a9b2f50701162122k1b8dce9cyc8c4f1af40f8d063@mail.gmail.com>
Message-ID: <20070117054543.GV39898@ms.unimelb.edu.au>

Rupendra,

depending on the nature of your data (which you haven't mentioned),
you might try hierarchical partitioning, as found in the hier.part
package on CRAN.

Cheers

Andrew

On Wed, Jan 17, 2007 at 11:07:18AM +0545, Rupendra Chulyadyo wrote:
> Hello all,
> 
> I want to assign relative score to the predictor variables on the basis of
> its influence on the dependent variable. But I could not find any standard
> statistical approach appropriate for this purpose.
> Please suggest the possible approaches.
> 
> Thanks in advance,
> 
> Rupendra Chulyadyo
> Institute of Engineering,
> Tribhuvan University,
> Nepal
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From blomsp at ozemail.com.au  Wed Jan 17 07:04:24 2007
From: blomsp at ozemail.com.au (Simon Blomberg)
Date: Wed, 17 Jan 2007 17:04:24 +1100
Subject: [R] Help on variable ranking
In-Reply-To: <20070117054543.GV39898@ms.unimelb.edu.au>
References: <b5a9b2f50701162122k1b8dce9cyc8c4f1af40f8d063@mail.gmail.com>
	<20070117054543.GV39898@ms.unimelb.edu.au>
Message-ID: <45ADBC68.3090102@ozemail.com.au>

Before you do that, you might try reading this paper:

Bring, J. 1995. Variable importance by partitioning R^2. Quality and 
Quantity 29:173-189.

Cheers,

Simon.

Andrew Robinson wrote:
> Rupendra,
> 
> depending on the nature of your data (which you haven't mentioned),
> you might try hierarchical partitioning, as found in the hier.part
> package on CRAN.
> 
> Cheers
> 
> Andrew
> 
> On Wed, Jan 17, 2007 at 11:07:18AM +0545, Rupendra Chulyadyo wrote:
>> Hello all,
>>
>> I want to assign relative score to the predictor variables on the basis of
>> its influence on the dependent variable. But I could not find any standard
>> statistical approach appropriate for this purpose.
>> Please suggest the possible approaches.
>>
>> Thanks in advance,
>>
>> Rupendra Chulyadyo
>> Institute of Engineering,
>> Tribhuvan University,
>> Nepal
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C

The combination of some data and an aching desire for
an answer does not ensure that a reasonable answer
can be extracted from a given body of data.
- John Tukey.


From justin_bem at yahoo.fr  Wed Jan 17 08:13:02 2007
From: justin_bem at yahoo.fr (justin bem)
Date: Wed, 17 Jan 2007 07:13:02 +0000 (GMT)
Subject: [R] Re :  labels outliers in boxplot
Message-ID: <316479.97446.qm@web23004.mail.ird.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070117/4fb60711/attachment.pl 

From mkeller at fam.tuwien.ac.at  Wed Jan 17 10:57:49 2007
From: mkeller at fam.tuwien.ac.at (Martin Keller-Ressel)
Date: Wed, 17 Jan 2007 09:57:49 -0000
Subject: [R] percent sign in plot annotation
Message-ID: <op.tmalynct0efqu7@neyman.fam.tuwien.ac.at>

Hello,

I would like to annotate a graph with the expression 'alpha = 5%' (the  
alpha should be displayed as the greek letter).
I tried

> text(1,1,expression(alpha == 5%))

which gives a syntax error.
escaping the percent sign (\%) or doubling (%%) does not help.
What do I do?

Thanks,

Martin Keller-Ressel



-- 
Martin Keller-Ressel
Research Unit of Financial and Actuarial Mathematics
TU Vienna


From jim at bitwrit.com.au  Wed Jan 17 10:11:33 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Wed, 17 Jan 2007 20:11:33 +1100
Subject: [R] parallel coordinates plot
In-Reply-To: <2636.193.170.84.103.1168964159.squirrel@webmail.univie.ac.at>
References: <2636.193.170.84.103.1168964159.squirrel@webmail.univie.ac.at>
Message-ID: <45ADE845.3070406@bitwrit.com.au>

Marco Helbich wrote:
> Dear List,
> 
> I want to make a parallel coordinates plot with the specific variables on
> the abscissa and the cases on the ordinate should be dyed dependent on
> another nominal variable from the data frame. I use the parcoord function.
> 
Hi Marco,
If I understand your question, you want to color the points displayed 
for each variable dependent upon some nominal value (probably a factor).
Say you have a data frame like this, you could plot:

my.df<-data.frame(var.names=paste("V",1:10,sep=""),
  var.values=rnorm(10)+5,var.colors=sample(2:4,10,TRUE))
plot(my.df$var.values,col=my.df$var.colors,axes=FALSE)
box()
axis(1,at=1:10,labels=as.character(my.df$var.names))
axis(2)

Jim


From gruen at ci.tuwien.ac.at  Wed Jan 17 10:08:31 2007
From: gruen at ci.tuwien.ac.at (Bettina Gruen)
Date: Wed, 17 Jan 2007 10:08:31 +0100
Subject: [R] percent sign in plot annotation
In-Reply-To: <op.tmalynct0efqu7@neyman.fam.tuwien.ac.at>
References: <op.tmalynct0efqu7@neyman.fam.tuwien.ac.at>
Message-ID: <45ADE78F.6060302@ci.tuwien.ac.at>

Martin Keller-Ressel wrote:
> Hello,
> 
> I would like to annotate a graph with the expression 'alpha = 5%' (the  
> alpha should be displayed as the greek letter).
> I tried
> 
>> text(1,1,expression(alpha == 5%))

Try
text(1,1,expression(alpha == 5*"%"))

Best,
Bettina


From dimitris.rizopoulos at med.kuleuven.be  Wed Jan 17 10:12:37 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 17 Jan 2007 10:12:37 +0100
Subject: [R] percent sign in plot annotation
References: <op.tmalynct0efqu7@neyman.fam.tuwien.ac.at>
Message-ID: <008f01c73a17$a18ed620$0540210a@www.domain>

you can try

text(1, 1, expression(paste(alpha, " = 5%")))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Martin Keller-Ressel" <mkeller at fam.tuwien.ac.at>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, January 17, 2007 10:57 AM
Subject: [R] percent sign in plot annotation


> Hello,
>
> I would like to annotate a graph with the expression 'alpha = 5%' 
> (the
> alpha should be displayed as the greek letter).
> I tried
>
>> text(1,1,expression(alpha == 5%))
>
> which gives a syntax error.
> escaping the percent sign (\%) or doubling (%%) does not help.
> What do I do?
>
> Thanks,
>
> Martin Keller-Ressel
>
>
>
> -- 
> Martin Keller-Ressel
> Research Unit of Financial and Actuarial Mathematics
> TU Vienna
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From philipp.pagel.lists at t-online.de  Wed Jan 17 10:23:34 2007
From: philipp.pagel.lists at t-online.de (Philipp Pagel)
Date: Wed, 17 Jan 2007 10:23:34 +0100
Subject: [R] percent sign in plot annotation
In-Reply-To: <op.tmalynct0efqu7@neyman.fam.tuwien.ac.at>
References: <op.tmalynct0efqu7@neyman.fam.tuwien.ac.at>
Message-ID: <20070117092334.GA6919@gsf.de>

On Wed, Jan 17, 2007 at 09:57:49AM -0000, Martin Keller-Ressel wrote:
> I would like to annotate a graph with the expression 'alpha = 5%' (the  
> alpha should be displayed as the greek letter).
> I tried
> 
> > text(1,1,expression(alpha == 5%))

text(1,1, expression(paste(alpha == 5, '%')) )

cu
	Philipp

-- 
Dr. Philipp Pagel                            Tel.  +49-8161-71 2131
Dept. of Genome Oriented Bioinformatics      Fax.  +49-8161-71 2186
Technical University of Munich
Science Center Weihenstephan
85350 Freising, Germany
http://mips.gsf.de/staff/pagel


From gavin.simpson at ucl.ac.uk  Wed Jan 17 10:28:13 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 17 Jan 2007 09:28:13 +0000
Subject: [R] percent sign in plot annotation
In-Reply-To: <op.tmalynct0efqu7@neyman.fam.tuwien.ac.at>
References: <op.tmalynct0efqu7@neyman.fam.tuwien.ac.at>
Message-ID: <1169026093.2964.15.camel@dhcppc2.my.nat.localnet>

On Wed, 2007-01-17 at 09:57 +0000, Martin Keller-Ressel wrote:
> Hello,
> 
> I would like to annotate a graph with the expression 'alpha = 5%' (the  
> alpha should be displayed as the greek letter).
> I tried
> 
> > text(1,1,expression(alpha == 5%))
> 
> which gives a syntax error.
> escaping the percent sign (\%) or doubling (%%) does not help.
> What do I do?
> 
> Thanks,
> 
> Martin Keller-Ressel

Escaping a % with \ and then escaping the \ is not valid syntactically.

This works, but there may be better ways to do this:

plot(0:10, 0:10, type = "n")
text(5,5,expression(paste(alpha == 5, "%", sep = "")))

HTH

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT                          [w] http://www.freshwaters.org.uk/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From mkeller at fam.tuwien.ac.at  Wed Jan 17 11:37:00 2007
From: mkeller at fam.tuwien.ac.at (Martin Keller-Ressel)
Date: Wed, 17 Jan 2007 10:37:00 -0000
Subject: [R] percent sign in plot annotation
In-Reply-To: <1169026093.2964.15.camel@dhcppc2.my.nat.localnet>
References: <op.tmalynct0efqu7@neyman.fam.tuwien.ac.at>
	<1169026093.2964.15.camel@dhcppc2.my.nat.localnet>
Message-ID: <op.tmanryvg0efqu7@neyman.fam.tuwien.ac.at>

Thanks to Bettina, Dimitris and Gavin for their help.
All their solutions work nicely.
For future reference, here are three ways to draw a percent sign in R  
plots:

plot(0:10, 0:10, type = "n")
text(5,7,expression(paste(alpha == 5, "%", sep = "")))
text(5, 5, expression(paste(alpha, " = 5%")))
text(5,3,expression(alpha == 5*"%"))

best regards,
Martin Keller-Ressel



-- 
Martin Keller-Ressel
Research Unit of Financial and Actuarial Mathematics
TU Vienna


From backer at psych.uib.no  Wed Jan 17 10:37:10 2007
From: backer at psych.uib.no (Tom Backer Johnsen)
Date: Wed, 17 Jan 2007 10:37:10 +0100
Subject: [R] sleep data
Message-ID: <45ADEE46.6000803@psych.uib.no>

When reading the documentation for the "sleep" data set in R, the 
impression is clear, this is an "independent groups" kind of design 
(two groups of 10 subjects each).  However, when browsing the original 
article (referred to in the help file), my impression is quite clear, 
this is really a "repeated measures" kind of data (one group of 10 
subjects, two observations).  What is correct?

Tom


From lorenz.gygax at art.admin.ch  Wed Jan 17 10:41:20 2007
From: lorenz.gygax at art.admin.ch (lorenz.gygax at art.admin.ch)
Date: Wed, 17 Jan 2007 10:41:20 +0100
Subject: [R] nested hierarchical design
In-Reply-To: <45AD04B9.4090506@unil.ch>
Message-ID: <145C63777EF3ED41A5A99035845F7DD9D311A6@EVD-C8001.bk.evdad.admin.ch>

> 1. The first I used to do in SPSS and I would like to be able 
> to do it in R as well.
> This is the hierarchical model I would like to use: a continuous 
> variable explained by factor A(fixed) + factor B(random) 
> nested in A + factor C (random) nested in factor B (which is nested
> in A).

You would need the following, I guess:

lme (var ~ A, data= NameOfDataFrame, random= ~ 1 | B/C)

This means that C is nested in B. If all C within B have the same levels of A, the model automatically treats B as nested in A(but you can check that based on the degrees of freedom in the output).

> 2. the same model but than for count data (like 15 out of 30, 
> 23 out of 60) instead of the continous variable(I know the basics
> of glm in R)

I guess, this can be viewed as a data set with binary outcomes and thus modeled by a generalised mixed-effects model with a binomial distribution using glmmPQL (in library MASS) or lmer (in library lme4). Please refer to:

@Book{Pin:00a,
  author = {Pinheiro, Jose C and Bates, Douglas M},
  title = {Mixed-Effects Models in {S} and {S}-{P}{L}{U}{S}},
  publisher = {Springer},
  year = {2000},
  address = {New York}
}

@BOOK {Ven:02,
  AUTHOR = {Venables, W N and Ripley, B D},
  TITLE = {Modern Applied Statistics with {S}},
  PUBLISHER = {Springer},
  YEAR = {2002},
  ADDRESS = {New York},
  EDITION = {fourth}
}

@Article{Rnews:Bates:2005,
  author       = {Douglas Bates},
  title	       = {Fitting Linear Mixed Models in {R}},
  journal      = {R News},
  year	       = 2005,
  volume       = 5,
  number       = 1,
  pages	       = {27--30},
  month	       = {May},
  url	       = {http://CRAN.R-project.org/doc/Rnews/},
}

and: http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests

Do not be surprised if the results are not identical to those in SPSS ...

Regards, Lorenz Gygax
- 
Swiss Federal Veterinary Office
Centre for proper housing of ruminants and pigs
Agroscope Reckenholz-T?nikon Research Station ART


From H.Schuurmans at geo.uu.nl  Wed Jan 17 10:42:32 2007
From: H.Schuurmans at geo.uu.nl (Hanneke Schuurmans)
Date: Wed, 17 Jan 2007 10:42:32 +0100
Subject: [R] problem with unlist POSIX date at midnight
Message-ID: <45ADEF88.40106@geo.uu.nl>

Dear R-users,

I use unlist of POSIX dates to extract the year, hour etc. With that I 
can search for files in my database which are in the form 
'yyyymmddhh_synops.txt'
However, I get stucked during midnight where unlist just gives NA's.
The script is given below, the problem accurs at acc.period[16] 
(midnight). However when I write out the character, unlist works well. 
But as.character(acc.period[16]) is not the solution....

begin=paste(dates[i]-1,"09:00:00")
end=paste(dates[i],"08:00:00")
acc.period=seq(as.POSIXct(begin),as.POSIXct(end),"hour")

unlist(strptime(acc.period[16],format="%Y-%m-%d %H:%M:%S"))
# sec   min  hour  mday   mon  year  wday  yday isdst
#  NA    NA    NA    NA    NA    NA    NA    NA    -1
unlist(strptime(acc.period[17],format="%Y-%m-%d %H:%M:%S"))
#  sec   min  hour  mday   mon  year  wday  yday isdst
 #   0     0     1     1     2   106     3    59     0
 
 a="2006-03-01 00:00:00"
 unlist(strptime(a,format="%Y-%m-%d %H:%M:%S"))
#  sec   min  hour  mday   mon  year  wday  yday isdst
#   0     0     0     1     2   106     3    59     0

Could someone help?

Thanks in advance!

Hanneke


-- 
---------------------------------------------
ir J.M. (Hanneke) Schuurmans
PhD student Hydrology
Department of Physical Geography
Faculty of Geosciences - Universiteit Utrecht
P.O. Box 80115  3508 TC  Utrecht
T +31 (0)30 2532988  F +31 (0)30 2531145
W www.geo.uu.nl/staff/schuurmans


From backer at psych.uib.no  Wed Jan 17 10:53:48 2007
From: backer at psych.uib.no (Tom Backer Johnsen)
Date: Wed, 17 Jan 2007 10:53:48 +0100
Subject: [R] Repeated measures
Message-ID: <45ADF22C.8040406@psych.uib.no>

I am having a hard time understanding how to perform a "repeated 
measures" type of ANOVA with R.  When reading the document found here:

http://cran.r-project.org/doc/contrib/Lemon-kickstart/kr_repms.html

I find that there is a reference to a function make.rm () that is 
supposed to rearrange a "one row per person" type of frame to a "one 
row per observation" type of frame.  But that function does not seem 
to be there.  Nor does the help.search suggest anything.  Is that 
function buried in some package?

Is there  some simple documentation that might be useful somewhere? 
Starting with a really simple problem (one group, two observations)?

Tom


From cagnacci at cealp.it  Wed Jan 17 10:58:28 2007
From: cagnacci at cealp.it (Francesca)
Date: Wed, 17 Jan 2007 10:58:28 +0100
Subject: [R] lmer or glm with family=binomial :   probability variable
Message-ID: <001301c73a1e$0af0c690$6000a8c0@CEALP.IT>

Dear all,
We are dealing with a variable (BA) which indicates the overlap between
small mammal home ranges. It varies between 0 and 1 and it can be
interpreted as "the probability of two home ranges to overlap",
therefore we would have modelled it with the binomial family, also
supported by  the distribution of the variable itself.  However, lmer or
glm require the data to be presented as successes vs failures. In our
case, this is not possible as BA is calculated by GIS on raster maps; in
other words, BA expressess p (probability of success), but it is not
possible to know from how many cases/attempts p came from. 
Therefore, what we get from the analysis is:

       IDAN_IDAN     SESSO   SESSIONE         BA            
1   1D00AD9_1D1421F   F_F        1 	5.909904e-06                
2   1D00AD9_602F513   M_F        1 	5.640469e-03                
3   1D00AD9_602FEAB   M_F        1 	3.715911e-13                
4   1D00AD9_603086B   F_F        1 	2.350365e-17                
5   1D00AD9_60778A4   M_F        1 	1.589195e-08                
6   1D00AD9_60779D7   F_F        1 	7.343189e-22                
7   1D00AD9_6723D30   M_F        1 	8.725496e-01                
8   1D1421F_602F513   M_F        1 	6.757339e-02                
9   1D1421F_602FEAB   M_F        1 	7.612337e-01                
10  1D1421F_603086B   F_F        1 	4.623883e-06                
11  1D1421F_60778A4   M_F        1 	2.856006e-01                
12  1D1421F_60779D7   F_F        1 	9.752100e-11                
13  1D1421F_6723D30   M_F        1 	8.921498e-08                
14  602F513_602FEAB   M_M        1 	2.127866e-02                
15  602F513_603086B   M_F        1 	6.695516e-05                
16  1D00AD9_671ED61   M_F        2 	3.873126e-01                
17  1D00AD9_6723D30   M_F        2 	2.080799e-01                
18  1D00AD9_672594F   M_F        2 	3.983634e-15                
19  1D1421F_602FEAB   M_F        2 	2.956002e-01                
20  1D1421F_603086B   F_F        2 	2.150006e-06                
21  1D1421F_60314C4   F_F        2 	1.947681e-21                
22  1D1421F_6033E53   M_F        2 	1.855792e-01                
23  1D1421F_60655F4   F_F        2 	1.242808e-02                
24  1D1421F_60778A4   M_F        2 	1.398984e-02                

> SESSIONE1<-factor(SESSIONE)
> model<-lmer(BA~ SESSO + (1|SESSIONE1:IDAN_IDAN) + (1|SESSIONE1),
data=foglio1, family=binomial)

Warning messages:
1: #non integer successes in glm binomial model! in: eval(expr, envir,
enclos) 
2: nlminb returned message singular convergence (7) 
 in: LMEopt(x = mer, value = cv) 
3: nlminb returned message false convergence (8) 
 in: LMEopt(x = mer, value = cv) 
4: nlminb returned message singular convergence (7) 
 in: LMEopt(x = mer, value = cv) 
5: nlminb returned message false convergence (8) 
 in: LMEopt(x = mer, value = cv) 
6: nlminb returned message false convergence (8) 
 in: LMEopt(x = mer, value = cv) 
7: IRLS iterations for PQL did not converge 

Is there any possibility to model p vs q=1-p without passing by
successes vs failures frequencies?

Thank you very much for helping!!!

Best regards

Francesca Cagnacci


Francesca Cagnacci, PhD
****************************************
Centro di Ecologia Alpina
Viote del Monte Bondone
38040 Trento
Tel. +393388668767 or +393397481073
Email cagnacci at cealp.it or frcagnac at tin.it


From ripley at stats.ox.ac.uk  Wed Jan 17 11:12:58 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Jan 2007 10:12:58 +0000 (GMT)
Subject: [R] problem with unlist POSIX date at midnight
In-Reply-To: <45ADEF88.40106@geo.uu.nl>
References: <45ADEF88.40106@geo.uu.nl>
Message-ID: <Pine.LNX.4.64.0701171006220.824@gannet.stats.ox.ac.uk>

On Wed, 17 Jan 2007, Hanneke Schuurmans wrote:

> Dear R-users,
>
> I use unlist of POSIX dates to extract the year, hour etc. With that I
> can search for files in my database which are in the form
> 'yyyymmddhh_synops.txt'
> However, I get stucked during midnight where unlist just gives NA's.
> The script is given below, the problem accurs at acc.period[16]
> (midnight). However when I write out the character, unlist works well.
> But as.character(acc.period[16]) is not the solution....
>
> begin=paste(dates[i]-1,"09:00:00")
> end=paste(dates[i],"08:00:00")
> acc.period=seq(as.POSIXct(begin),as.POSIXct(end),"hour")
>
> unlist(strptime(acc.period[16],format="%Y-%m-%d %H:%M:%S"))
> # sec   min  hour  mday   mon  year  wday  yday isdst
> #  NA    NA    NA    NA    NA    NA    NA    NA    -1
> unlist(strptime(acc.period[17],format="%Y-%m-%d %H:%M:%S"))
> #  sec   min  hour  mday   mon  year  wday  yday isdst
> #   0     0     1     1     2   106     3    59     0
>
> a="2006-03-01 00:00:00"
> unlist(strptime(a,format="%Y-%m-%d %H:%M:%S"))
> #  sec   min  hour  mday   mon  year  wday  yday isdst
> #   0     0     0     1     2   106     3    59     0
>
> Could someone help?

Please see the footer comment

> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

We do need that to help you.  Using unclass() rather than unlist() will 
help us see what strptime actually returned (I suspect the NAs are from 
there as your (unshown) input was invalid, e.g. had hour=24).

We also need the other information the posting guide asks for, such as the 
output of sessionInfo().

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From lorenz.gygax at art.admin.ch  Wed Jan 17 11:19:23 2007
From: lorenz.gygax at art.admin.ch (lorenz.gygax at art.admin.ch)
Date: Wed, 17 Jan 2007 11:19:23 +0100
Subject: [R] lmer or glm with family=binomial :   probability variable
In-Reply-To: <001301c73a1e$0af0c690$6000a8c0@CEALP.IT>
Message-ID: <145C63777EF3ED41A5A99035845F7DD9D311A7@EVD-C8001.bk.evdad.admin.ch>

> We are dealing with a variable (BA) which indicates the overlap
> between small mammal home ranges. It varies between 0 and 1 and it
> can be interpreted as "the probability of two home ranges to
> overlap", therefore we would have modelled it with the binomial
> family, also supported by  the distribution of the variable itself.  
> However, lmer or glm require the data to be presented as successes
> vs failures. In our case, this is not possible as BA is calculated
> by GIS on raster maps; in other words, BA expressess p (probability
> of success), but it is not possible to know from how many
> cases/attempts p came from. 

These models internally use the logit link. You could potentially transform your response variable accordingly and fit a model based on the assumption of normally distributed variables (of course, you have to check whether the corresponding assumptions were met e.g. by conducting an analysis of the residuals).

the logit link is: log (P/(1-P))

Regards, Lorenz Gygax
- 
Swiss Federal Veterinary Office
Centre for proper housing of ruminants and pigs
Agroscope Reckenholz-T?nikon Research Station ART


From kalyansikha at yahoo.com  Wed Jan 17 11:36:56 2007
From: kalyansikha at yahoo.com (Bhanu Kalyan.K)
Date: Wed, 17 Jan 2007 02:36:56 -0800 (PST)
Subject: [R] Does R implement DBSCAN , ROCK, BIRCH?
Message-ID: <20070117103656.84595.qmail@web34314.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070117/94269d78/attachment.pl 

From kalyansikha at yahoo.com  Wed Jan 17 11:36:56 2007
From: kalyansikha at yahoo.com (Bhanu Kalyan.K)
Date: Wed, 17 Jan 2007 02:36:56 -0800 (PST)
Subject: [R] Does R implement DBSCAN , ROCK, BIRCH?
Message-ID: <20070117103656.84595.qmail@web34314.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070117/94269d78/attachment-0001.pl 

From pburns at pburns.seanet.com  Wed Jan 17 11:45:14 2007
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Wed, 17 Jan 2007 10:45:14 +0000
Subject: [R] curious about dimension of 'apply' output when MARGIN=1
In-Reply-To: <971536df0701161616n5d586fedhdbbdd29e3131f8dd@mail.gmail.com>
References: <45AD43FD.8050100@stat.purdue.edu>
	<971536df0701161616n5d586fedhdbbdd29e3131f8dd@mail.gmail.com>
Message-ID: <45ADFE3A.7050409@pburns.seanet.com>

A logical reason for the phenomenon is that
matrices are stored down their columns. For
example:

 > matrix(1:15,5)
     [,1] [,2] [,3]
[1,]    1    6   11
[2,]    2    7   12
[3,]    3    8   13
[4,]    4    9   14
[5,]    5   10   15

When an 'apply' across rows is done, it will be
the values corresponding to each of the rows that
are together.

For matrices, merely transposing the result fixes
the "problem", but it is considerably more complex
in higher dimensional arrays.

There could be a spectrum of opinion from:

the original programmer was lazy and didn't adequately
serve users

to:

the simpler the program the fewer bugs there will be.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")


Gabor Grothendieck wrote:

>The reshape package has an idempotent apply, iapply:
>
>  
>
>>library(reshape)
>>iapply(M,1,function(row) row+c(1,2))
>>    
>>
>     [,1] [,2]
>[1,]    2    6
>[2,]    3    7
>[3,]    4    8
>
>On 1/16/07, Benjamin Tyner <btyner at stat.purdue.edu> wrote:
>  
>
>>Reading the documentation for 'apply', I understand the following is
>>working exactly as documented:
>>
>> > M<-matrix(1:6,ncol=2)
>> > M
>>    [,1] [,2]
>>[1,]    1    4
>>[2,]    2    5
>>[3,]    3    6
>> > apply(M,2,function(column) column+c(1,2,3))
>>    [,1] [,2]
>>[1,]    2    5
>>[2,]    4    7
>>[3,]    6    9
>> > apply(M,1,function(row) row+c(1,2))
>>    [,1] [,2] [,3]
>>[1,]    2    3    4
>>[2,]    6    7    8
>>
>>I'm not proposing any changes or extra arguments to 'apply'. Rather, I'm
>>wondering what is the benefit for (or rationale behind) this somewhat
>>unintuitive behavior in the case that MARGIN=1.
>>
>>Thanks,
>>Ben
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From ccleland at optonline.net  Wed Jan 17 11:45:01 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 17 Jan 2007 05:45:01 -0500
Subject: [R] Repeated measures
In-Reply-To: <45ADF22C.8040406@psych.uib.no>
References: <45ADF22C.8040406@psych.uib.no>
Message-ID: <45ADFE2D.2060208@optonline.net>

Tom Backer Johnsen wrote:
> I am having a hard time understanding how to perform a "repeated 
> measures" type of ANOVA with R.  When reading the document found here:
> 
> http://cran.r-project.org/doc/contrib/Lemon-kickstart/kr_repms.html
> 
> I find that there is a reference to a function make.rm () that is 
> supposed to rearrange a "one row per person" type of frame to a "one 
> row per observation" type of frame.  But that function does not seem 
> to be there.  Nor does the help.search suggest anything.  Is that 
> function buried in some package?

  I'm not able to find that function.  Perhaps that document is out of date.

> Is there  some simple documentation that might be useful somewhere? 
> Starting with a really simple problem (one group, two observations)?

  Here is an example showing the use of reshape() and analysis via aov()
and lme() in the nlme package.

tolerance <-
read.table("http://www.ats.ucla.edu/stat/Splus/examples/alda/tolerance1.txt",
            sep=",", header=TRUE)

tolerance.long <- reshape(tolerance,
                          varying = list(c("tol11","tol12","tol13",
                                           "tol14", "tol15")),
                          v.names = c("tol"), timevar = "time",
                          times = 11:15, direction = "long")

tolerance.aov <- aov(tol ~ as.factor(time) * male + Error(id),
                     data = tolerance.long)

summary(tolerance.aov)

Error: id
     Df   Sum Sq  Mean Sq
male  1 0.085168 0.085168

Error: Within
                     Df  Sum Sq Mean Sq F value  Pr(>F)
as.factor(time)       4  2.8326  0.7081  3.0538 0.02236 *
male                  1  0.3024  0.3024  1.3039 0.25745
as.factor(time):male  4  0.1869  0.0467  0.2015 0.93670
Residuals            69 16.0002  0.2319
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

library(nlme)

tolerance.lme <- lme(tol ~ as.factor(time) * male, random = ~ 1 | id,
                     data = tolerance.long)

anova(tolerance.lme)
                     numDF denDF  F-value p-value
(Intercept)              1    56 353.9049  <.0001
as.factor(time)          4    56   5.1309  0.0014
male                     1    14   0.6071  0.4488
as.factor(time):male     4    56   0.3386  0.8508

  RSiteSearch("repeated measures") points to other examples, functions,
and documentation.

> Tom
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From P.Dalgaard at biostat.ku.dk  Wed Jan 17 12:00:40 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 17 Jan 2007 12:00:40 +0100
Subject: [R] percent sign in plot annotation
In-Reply-To: <op.tmanryvg0efqu7@neyman.fam.tuwien.ac.at>
References: <op.tmalynct0efqu7@neyman.fam.tuwien.ac.at>	<1169026093.2964.15.camel@dhcppc2.my.nat.localnet>
	<op.tmanryvg0efqu7@neyman.fam.tuwien.ac.at>
Message-ID: <45AE01D8.20808@biostat.ku.dk>

Martin Keller-Ressel wrote:
> Thanks to Bettina, Dimitris and Gavin for their help.
> All their solutions work nicely.
> For future reference, here are three ways to draw a percent sign in R  
> plots:
>
> plot(0:10, 0:10, type = "n")
> text(5,7,expression(paste(alpha == 5, "%", sep = "")))
> text(5, 5, expression(paste(alpha, " = 5%")))
> text(5,3,expression(alpha == 5*"%"))
>
>   
A bit surprising that nobody mentioned

expression(alpha == "5%")

    -p



-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From antonio.fabio at gmail.com  Wed Jan 17 12:00:48 2007
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Wed, 17 Jan 2007 12:00:48 +0100
Subject: [R] curious about dimension of 'apply' output when MARGIN=1
In-Reply-To: <45ADFE3A.7050409@pburns.seanet.com>
References: <45AD43FD.8050100@stat.purdue.edu>
	<971536df0701161616n5d586fedhdbbdd29e3131f8dd@mail.gmail.com>
	<45ADFE3A.7050409@pburns.seanet.com>
Message-ID: <b0808fdc0701170300q6e0f157av35d143e1683ea5ae@mail.gmail.com>

2007/1/17, Patrick Burns <pburns a pburns.seanet.com>:
> A logical reason for the phenomenon is that
> matrices are stored down their columns. For
> example:
>
>  > matrix(1:15,5)
>      [,1] [,2] [,3]
> [1,]    1    6   11
> [2,]    2    7   12
> [3,]    3    8   13
> [4,]    4    9   14
> [5,]    5   10   15
>
> When an 'apply' across rows is done, it will be
> the values corresponding to each of the rows that
> are together.
>
> For matrices, merely transposing the result fixes
> the "problem", but it is considerably more complex
> in higher dimensional arrays.

I routinely use 'aperm'

>
> There could be a spectrum of opinion from:
>
> the original programmer was lazy and didn't adequately
> serve users
>
> to:
>
> the simpler the program the fewer bugs there will be.
>
> Patrick Burns
> patrick a burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
>
> Gabor Grothendieck wrote:
>
> >The reshape package has an idempotent apply, iapply:
> >
> >
> >
> >>library(reshape)
> >>iapply(M,1,function(row) row+c(1,2))
> >>
> >>
> >     [,1] [,2]
> >[1,]    2    6
> >[2,]    3    7
> >[3,]    4    8
> >
> >On 1/16/07, Benjamin Tyner <btyner a stat.purdue.edu> wrote:
> >
> >
> >>Reading the documentation for 'apply', I understand the following is
> >>working exactly as documented:
> >>
> >> > M<-matrix(1:6,ncol=2)
> >> > M
> >>    [,1] [,2]
> >>[1,]    1    4
> >>[2,]    2    5
> >>[3,]    3    6
> >> > apply(M,2,function(column) column+c(1,2,3))
> >>    [,1] [,2]
> >>[1,]    2    5
> >>[2,]    4    7
> >>[3,]    6    9
> >> > apply(M,1,function(row) row+c(1,2))
> >>    [,1] [,2] [,3]
> >>[1,]    2    3    4
> >>[2,]    6    7    8
> >>
> >>I'm not proposing any changes or extra arguments to 'apply'. Rather, I'm
> >>wondering what is the benefit for (or rationale behind) this somewhat
> >>unintuitive behavior in the case that MARGIN=1.
> >>
> >>Thanks,
> >>Ben
> >>
> >>______________________________________________
> >>R-help a stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >>
> >
> >______________________________________________
> >R-help a stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> >
>
> ______________________________________________
> R-help a stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Antonio, Fabio Di Narzo
Ph.D. student at
Department of Statistical Sciences
University of Bologna, Italy


From Jerzy.Behnke at nottingham.ac.uk  Wed Jan 17 13:19:00 2007
From: Jerzy.Behnke at nottingham.ac.uk (Behnke Jerzy)
Date: Wed, 17 Jan 2007 12:19:00 -0000
Subject: [R] Effect size in GLIM models
Message-ID: <BAA2EE790D828F4E8B262F17B3B0A7A3028E6E93@VUIEXCH1.ad.nottingham.ac.uk>

Dear All,
I wonder if anyone can advise me as to whether there is a consensus as
to how the effect size should be calculated from GLIM models in R for
any specified significant main effect or interaction.

In investigating the causes of variation in infection in wild animals,
we have fitted 4-way GLIM models in R with negative binomial errors.
These are then simplified using the STEP procedure, and finally each of
the remaining terms is deleted in turn, and the model without that term
compared to a model with that term to estimate probability

An ANOVA of each model gives the deviance explained by each interaction
and main effect, and the percentage deviance attributable to each factor
can be calculated from NULL deviance.

However, we estimate probabilities by subsequent deletion of terms, and
this gives the LR statistic. Expressing the value of the LR statistic as
a percentage of 2xlog-like in a model without any factors, gives lower
values than the former procedure.

Are either of these appropriate? If so which is best, or alternatively
how can % deviance be calculated. We require % deviance explained by
each factor or interaction,  because we need to compare individual
factors (say host age) across a range of infections.

Any advice will be most gratefully appreciated. I can send you a worked
example if you require more information.

Jerzy. M. Behnke,
The School of Biology,
The University of Nottingham,
University Park,
NOTTINGHAM, NG7 2RD

Tel: +0044 (0) 115 951 3208
Fax: +0044 (0) 115 951 3251

http://www.nottingham.ac.uk/biology/contact/academics/behnke/overview.ph
tml?P=1&R=1&S=&ID=11&from=iai&m1=&m2=
Useful links to field stations:
http://www.quintastudies.info/index.htm



This message has been checked for viruses but the contents of an attachment
may still contain software viruses, which could damage your computer system:
you are advised to perform your own checks. Email communications with the
University of Nottingham may be monitored as permitted by UK legislation.


From rchulyadyo at gmail.com  Wed Jan 17 13:37:09 2007
From: rchulyadyo at gmail.com (Rupendra Chulyadyo)
Date: Wed, 17 Jan 2007 18:22:09 +0545
Subject: [R] Help on variable ranking
In-Reply-To: <45ADBC68.3090102@ozemail.com.au>
References: <b5a9b2f50701162122k1b8dce9cyc8c4f1af40f8d063@mail.gmail.com>
	<20070117054543.GV39898@ms.unimelb.edu.au>
	<45ADBC68.3090102@ozemail.com.au>
Message-ID: <b5a9b2f50701170437t7bd8a6c8p29dcdb8c96442733@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070117/a51e3d9e/attachment.pl 

From f.harrell at vanderbilt.edu  Wed Jan 17 13:50:27 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 17 Jan 2007 06:50:27 -0600
Subject: [R] Help on variable ranking
In-Reply-To: <b5a9b2f50701162122k1b8dce9cyc8c4f1af40f8d063@mail.gmail.com>
References: <b5a9b2f50701162122k1b8dce9cyc8c4f1af40f8d063@mail.gmail.com>
Message-ID: <45AE1B93.5000806@vanderbilt.edu>

Rupendra Chulyadyo wrote:
> Hello all,
> 
> I want to assign relative score to the predictor variables on the basis of
> its influence on the dependent variable. But I could not find any standard
> statistical approach appropriate for this purpose.
> Please suggest the possible approaches.
> 
> Thanks in advance,
> 
> Rupendra Chulyadyo
> Institute of Engineering,
> Tribhuvan University,
> Nepal

You might consider using the bootstrap to get confidence intervals of 
the rank of each predictor's partial chi-square or partial R-square. 
The following takes into account all terms that might be associated with 
a variable (nonlinear and interaction terms, dummy variables).  The code 
is taken from an example in the anova.Design help file in the Design 
package.  Unless the dataset is huge and there is little collinearity, 
you will be surprised how difficult it is to pick winners and losers 
from the predictors [try ranking gene expressions from gene microarray 
data for even more of a shock].  Note that Bayesian ranking procedures 
are more accurate, but this quick and dirty approach isn't bad.

mydata <- data.frame(x1=runif(200), x2=runif(200),
                      sex=factor(sample(c('female','male'),200,TRUE)))
set.seed(9)  # so can reproduce example
mydata$y <- ifelse(runif(200)<=plogis(mydata$x1-.5 + .5*(mydata$x2-.5) +
                    .5*(mydata$sex=='male')),1,0)

library(Design)
library(boot)
b <- boot(mydata, function(data, i, ...) rank(-plot(anova(
                 lrm(y ~ rcs(x1,4)+pol(x2,2)+sex,data,subset=i)),
                 sort='none', pl=FALSE)),
                 R=25)  # should really do R=500 but will take a while
Rank <- b$t0
lim <- t(apply(b$t, 2, quantile, probs=c(.025,.975)))
# Use the Hmisc Dotplot function to display ranks and their confidence
# intervals.  Sort the categories by descending adj. chi-square, for ranks
original.chisq <- plot(anova(lrm(y ~ rcs(x1,4)+pol(x2,2)+sex,data=mydata)),
                        sort='none', pl=FALSE)
predictor <- as.factor(names(original.chisq))
predictor <- reorder.factor(predictor, -original.chisq)

Dotplot(predictor ~ Cbind(Rank, lim), pch=3, xlab='Rank',
                 main=expression(paste(
'Ranks and 0.95 Confidence Limits for ',chi^2,' - d.f.')))

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From H.Schuurmans at geo.uu.nl  Wed Jan 17 13:50:35 2007
From: H.Schuurmans at geo.uu.nl (Hanneke Schuurmans)
Date: Wed, 17 Jan 2007 13:50:35 +0100
Subject: [R] problem with unlist POSIX date at midnight
Message-ID: <45AE1B9B.5030205@geo.uu.nl>

Dear R-users,

I use unlist of POSIX dates to extract the year, hour etc. With that I 
can search for files in my database which are in the form 
'yyyymmddhh_synops.txt'
However, I get stucked during midnight where unlist just gives NA's.
The script is given below; the problem accurs at acc.period[16] 
(midnight). However when I write out the character, unlist works well. 
But as.character(acc.period[16]) is not the solution....

 > dates=seq(as.Date("2006-03-01"), as.Date("2006-11-01"), "days")
 > i=1
 > begin=paste(dates[i]-1,"09:00:00")
 > end=paste(dates[i],"08:00:00")
 > acc.period=seq(as.POSIXct(begin),as.POSIXct(end),"hour")
 >
 > unlist(strptime(acc.period[15],format="%Y-%m-%d %H:%M:%S"))
  sec   min  hour  mday   mon  year  wday  yday isdst
    0     0    23    28     1   106     2    58     0
 > unlist(strptime(acc.period[16],format="%Y-%m-%d %H:%M:%S"))
  sec   min  hour  mday   mon  year  wday  yday isdst
   NA    NA    NA    NA    NA    NA    NA    NA    -1
 > unclass(strptime(acc.period[16],format="%Y-%m-%d %H:%M:%S"))
$sec
[1] NA

$min
[1] NA

$hour
[1] NA

$mday
[1] NA

$mon
[1] NA

$year
[1] NA

$wday
[1] NA

$yday
[1] NA

$isdst
[1] -1

 >
 > a="2006-03-01 00:00:00"
 > unlist(strptime(a,format="%Y-%m-%d %H:%M:%S"))
  sec   min  hour  mday   mon  year  wday  yday isdst
    0     0     0     1     2   106     3    59     0
 >
 > sessionInfo()
R version 2.4.0 (2006-10-03)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
States.1252;LC_MONETARY=English_United 
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"    

Thanks in advance!

Hanneke

-- 
---------------------------------------------
ir J.M. (Hanneke) Schuurmans
PhD student Hydrology
Department of Physical Geography
Faculty of Geosciences - Universiteit Utrecht
P.O. Box 80115  3508 TC  Utrecht
T +31 (0)30 2532988  F +31 (0)30 2531145
W www.geo.uu.nl/staff/schuurmans


From ripley at stats.ox.ac.uk  Wed Jan 17 14:17:49 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Jan 2007 13:17:49 +0000 (GMT)
Subject: [R] problem with unlist POSIX date at midnight
In-Reply-To: <45AE1B9B.5030205@geo.uu.nl>
References: <45AE1B9B.5030205@geo.uu.nl>
Message-ID: <Pine.LNX.4.64.0701171310540.5482@gannet.stats.ox.ac.uk>

[Unstated, but a near repeat of a posting earlier today with the same 
subject, in response to unacknowledged help.]

The problem is not with unlist (as in your subject line) but your input to
strptime.

strptime() is intended to convert character strings, and

> as.character(acc.period[16])
[1] "2006-03-01"

is not in the format you specified.  Why are you using strptime() to 
convert objects of class POSIXct to class POSIXlt and not as.POSIXlt()?


On Wed, 17 Jan 2007, Hanneke Schuurmans wrote:

> Dear R-users,
>
> I use unlist of POSIX dates to extract the year, hour etc. With that I
> can search for files in my database which are in the form
> 'yyyymmddhh_synops.txt'
> However, I get stucked during midnight where unlist just gives NA's.
> The script is given below; the problem accurs at acc.period[16]
> (midnight). However when I write out the character, unlist works well.
> But as.character(acc.period[16]) is not the solution....
>
> > dates=seq(as.Date("2006-03-01"), as.Date("2006-11-01"), "days")
> > i=1
> > begin=paste(dates[i]-1,"09:00:00")
> > end=paste(dates[i],"08:00:00")
> > acc.period=seq(as.POSIXct(begin),as.POSIXct(end),"hour")
> >
> > unlist(strptime(acc.period[15],format="%Y-%m-%d %H:%M:%S"))
>  sec   min  hour  mday   mon  year  wday  yday isdst
>    0     0    23    28     1   106     2    58     0
> > unlist(strptime(acc.period[16],format="%Y-%m-%d %H:%M:%S"))
>  sec   min  hour  mday   mon  year  wday  yday isdst
>   NA    NA    NA    NA    NA    NA    NA    NA    -1
> > unclass(strptime(acc.period[16],format="%Y-%m-%d %H:%M:%S"))
> $sec
> [1] NA
>
> $min
> [1] NA
>
> $hour
> [1] NA
>
> $mday
> [1] NA
>
> $mon
> [1] NA
>
> $year
> [1] NA
>
> $wday
> [1] NA
>
> $yday
> [1] NA
>
> $isdst
> [1] -1
>
> >
> > a="2006-03-01 00:00:00"
> > unlist(strptime(a,format="%Y-%m-%d %H:%M:%S"))
>  sec   min  hour  mday   mon  year  wday  yday isdst
>    0     0     0     1     2   106     3    59     0
> >
> > sessionInfo()
> R version 2.4.0 (2006-10-03)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
>
> Thanks in advance!
>
> Hanneke
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Wed Jan 17 15:01:43 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Jan 2007 14:01:43 +0000 (GMT)
Subject: [R] Effect size in GLIM models
In-Reply-To: <BAA2EE790D828F4E8B262F17B3B0A7A3028E6E93@VUIEXCH1.ad.nottingham.ac.uk>
References: <BAA2EE790D828F4E8B262F17B3B0A7A3028E6E93@VUIEXCH1.ad.nottingham.ac.uk>
Message-ID: <Pine.LNX.4.64.0701171300010.5482@gannet.stats.ox.ac.uk>

On Wed, 17 Jan 2007, Behnke Jerzy wrote:

> Dear All,
> I wonder if anyone can advise me as to whether there is a consensus as
> to how the effect size should be calculated from GLIM models in R for
> any specified significant main effect or interaction.

I think there is consensus that effect sizes are not measured by 
significance tests.  If you have a log link (you did not say), the model 
coefficients have a direct interpretation via multiplicative increases in 
rates.

> In investigating the causes of variation in infection in wild animals,
> we have fitted 4-way GLIM models in R with negative binomial errors.

What exactly do you mean by 'GLIM models in R with negative binomial 
errors'?  Negative binomial regression is within the GLM framework only 
for fixed shape theta. Package MASS has glm.nb() which extends the 
framework and you may be using without telling us.  (AFAIK GLIM is a 
software package, not a class of models.)

I suspect you are using the code from MASS without reference to the book
it supports, which has a worked example of model selection.

> These are then simplified using the STEP procedure, and finally each of
> the remaining terms is deleted in turn, and the model without that term
> compared to a model with that term to estimate probability

'probability' of what?

> An ANOVA of each model gives the deviance explained by each interaction
> and main effect, and the percentage deviance attributable to each factor
> can be calculated from NULL deviance.

If theta is not held fixed, anova() is probably not appropriate: see the 
help for anova.negbin.

> However, we estimate probabilities by subsequent deletion of terms, and
> this gives the LR statistic. Expressing the value of the LR statistic as
> a percentage of 2xlog-like in a model without any factors, gives lower
> values than the former procedure.

I don't know anything to suggest percentages of LR statistics are 
reasonable summary measures.  There are extensions of R^2 to these models, 
but AFAIK they share the well-attested drawbacks of R^2.

> Are either of these appropriate? If so which is best, or alternatively
> how can % deviance be calculated. We require % deviance explained by
> each factor or interaction,  because we need to compare individual
> factors (say host age) across a range of infections.
>
> Any advice will be most gratefully appreciated. I can send you a worked
> example if you require more information.

We do ask for more information in the posting guide and the footer of 
every message.  I have had to guess uncomfortably much in formulating my 
answers.

> Jerzy. M. Behnke,
> The School of Biology,
> The University of Nottingham,
> University Park,
> NOTTINGHAM, NG7 2RD
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From michael.greeff at env.ethz.ch  Wed Jan 17 15:51:29 2007
From: michael.greeff at env.ethz.ch (Michael Greeff)
Date: Wed, 17 Jan 2007 15:51:29 +0100
Subject: [R] Fwd:  model simplification in repeated anova
References: <20070115123909.BSC73337@po-d.temple.edu>
Message-ID: <7B9F4BBE-FF82-4C70-A470-0B177EF49D89@env.ethz.ch>

Hi,

I tried to do a model simplification in repeated anovas. obviously  
it's not possible to use the function "anova" straight away. I got  
the suggestion to do it with proj (see below). In my data, however, I  
have in "block" not the third order interaction (n:p:k), but to  
factors and their interaction (population, site, population:site).

I somehow didn't manage to get the right aov after the projection.


> From: "Richard M. Heiberger" <rmh at temple.edu>
>
>
> npk.aovE <- aov(yield ~  N*P*K + Error(block), npk)
>
> np.k.aovE <- aov(yield ~  N*P+K + Error(block), npk)
>
>
> npk.proj <- proj(npk.aovE)
> npk.x <- npk.proj$block[,"N:P:K"]
>
> npk.aov <- aov(terms(yield ~  npk.x + block + N+P+K+N:P+N:K+P:K,  
> keep.order=TRUE), npk)
>
> np.k.aov <- aov(terms(yield ~  block + N+P+K+N:P, keep.order=TRUE),  
> npk)
>
> summary(npk.aov)
> summary(np.k.aov)
>
> anova(npk.aov, np.k.aov)


From Tom.Reader at nottingham.ac.uk  Wed Jan 17 15:48:29 2007
From: Tom.Reader at nottingham.ac.uk (Reader Tom)
Date: Wed, 17 Jan 2007 14:48:29 -0000
Subject: [R] Effect size in GLIM models
Message-ID: <BAA2EE790D828F4E8B262F17B3B0A7A3028E7027@VUIEXCH1.ad.nottingham.ac.uk>

Dear All

Thanks very much for the rapid reply Prof Ripley. I had been looking at
this anaysis for my colleague (Prof Behnke) and suggested that he
contact the R mailing list because I couldn't answer his question. I
think some of the detail got lost in translation (he grew up with the
GLIM package!). So here are some more details:

We are indeed following your guidelines in the MASS book, and using
glm.nb to analyse some data on the abundance of several parasite species
in mice. We proceeded with model selection as suggested, and we are
reasonably happy that we end up with decent models for our several
parasite species. 

The question that Prof Behnke asked is: if we fit similar models
(initial full models have the same factors and covariates) with
different response variables (abundances of different species of
parasite), is there a way of comparing the relative effect sizes of the
key explanatory variables across different models? For example, if we
find that the best models for two different species include the term
"sex", is there a way of determining if sex explains more of the
variance in parasite abundance in species A than in species B? 

In a simple ANOVA with Guassian errors, we might compare the percentage
variance explained. We could also look at the overall r^2 for the models
and determine how well (relatively) our different models perform. We
might end up concluding that for species A, we have found the most
important biolgoical factors explaining parasite abundance, but that for
species B we have yet to explain a large proportion of the variance.

Is there something similar we can do with our glm.nb models? Clearly the
coefficients will tell us about relative effect sizes WITHIN a given
model, but what can we do when comparing completely different response
variables?!

Regards

Tom Reader

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: 17 January 2007 14:02
To: Behnke Jerzy
Cc: r-help at stat.math.ethz.ch; Reader Tom
Subject: Re: [R] Effect size in GLIM models

On Wed, 17 Jan 2007, Behnke Jerzy wrote:

> Dear All,
> I wonder if anyone can advise me as to whether there is a consensus as

> to how the effect size should be calculated from GLIM models in R for 
> any specified significant main effect or interaction.

I think there is consensus that effect sizes are not measured by
significance tests.  If you have a log link (you did not say), the model
coefficients have a direct interpretation via multiplicative increases
in rates.

> In investigating the causes of variation in infection in wild animals,

> we have fitted 4-way GLIM models in R with negative binomial errors.

What exactly do you mean by 'GLIM models in R with negative binomial
errors'?  Negative binomial regression is within the GLM framework only
for fixed shape theta. Package MASS has glm.nb() which extends the
framework and you may be using without telling us.  (AFAIK GLIM is a
software package, not a class of models.)

I suspect you are using the code from MASS without reference to the book
it supports, which has a worked example of model selection.

> These are then simplified using the STEP procedure, and finally each 
> of the remaining terms is deleted in turn, and the model without that 
> term compared to a model with that term to estimate probability

'probability' of what?

> An ANOVA of each model gives the deviance explained by each 
> interaction and main effect, and the percentage deviance attributable 
> to each factor can be calculated from NULL deviance.

If theta is not held fixed, anova() is probably not appropriate: see the
help for anova.negbin.

> However, we estimate probabilities by subsequent deletion of terms, 
> and this gives the LR statistic. Expressing the value of the LR 
> statistic as a percentage of 2xlog-like in a model without any 
> factors, gives lower values than the former procedure.

I don't know anything to suggest percentages of LR statistics are
reasonable summary measures.  There are extensions of R^2 to these
models, but AFAIK they share the well-attested drawbacks of R^2.

> Are either of these appropriate? If so which is best, or alternatively

> how can % deviance be calculated. We require % deviance explained by 
> each factor or interaction,  because we need to compare individual 
> factors (say host age) across a range of infections.
>
> Any advice will be most gratefully appreciated. I can send you a 
> worked example if you require more information.

We do ask for more information in the posting guide and the footer of
every message.  I have had to guess uncomfortably much in formulating my
answers.

> Jerzy. M. Behnke,
> The School of Biology,
> The University of Nottingham,
> University Park,
> NOTTINGHAM, NG7 2RD
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

This message has been checked for viruses but the contents of an attachment
may still contain software viruses, which could damage your computer system:
you are advised to perform your own checks. Email communications with the
University of Nottingham may be monitored as permitted by UK legislation.


From fmccown at cs.odu.edu  Wed Jan 17 16:39:56 2007
From: fmccown at cs.odu.edu (Frank McCown)
Date: Wed, 17 Jan 2007 10:39:56 -0500
Subject: [R] Row limit for read.table
Message-ID: <45AE434C.3080102@cs.odu.edu>

I have been trying to read in a large data set using read.table, but 
I've only been able to grab the first 50,871 rows of the total 122,269 rows.

 > f <- 
read.table("http://www.cs.odu.edu/~fmccown/R/Tchange_rates_crawled.dat", 
header=TRUE, nrows=123000, comment.char="", sep="\t")
 > length(f$change_rate)
[1] 50871

 From searching the email archives, I believe this is due to size limits 
of a data frame.  So...

1) Why doesn't read.table give a proper warning when it doesn't place 
every read item into a data frame?

2) Why isn't there a parameter to read.table that allows the user to 
specify which columns s/he is interested in?  This functionality would 
allow extraneous columns to be ignored which would improve memory usage.

I've already made a work-around by loading the table into mysql and 
doing a select on the 2 columns I need.  I just wonder why the above 2 
points aren't implemented.  Maybe they are and I'm totally missing it.

Thanks,
Frank


-- 
Frank McCown
Old Dominion University
http://www.cs.odu.edu/~fmccown/


From msakals at interchange.ubc.ca  Wed Jan 17 16:42:45 2007
From: msakals at interchange.ubc.ca (Matt Sakals)
Date: Wed, 17 Jan 2007 07:42:45 -0800
Subject: [R] add non-linear line
Message-ID: <5953EFB2-8FC9-4B79-AA51-96A302EE8246@interchange.ubc.ca>

I am trying to plot a non-linear trend line along with my data. I am  
sure the solution is simple; any help greatly appreciated.

Recent and historic attempts:
fit = nls((sd~a+b*exp(-c/time)+d*geology), start=list(a=1, b=1, c=10,  
d=-1), model=TRUE)
plot(time, sd, col=3, main = "Regression", sub=formula(fit))
lines(time, fitted.values(fit), lwd=2)

#tt = seq(from=0, to=200, by=10)
#lines(time, predict(fit, time=tt))
#lines(time, predict(fit))


From luis at lsu.edu  Wed Jan 17 17:08:24 2007
From: luis at lsu.edu (Luis A Escobar)
Date: Wed, 17 Jan 2007 10:08:24 -0600
Subject: [R] How to annotate a graph with non-transparent math labels?
Message-ID: <AB87A247CEA3C645AF4B46E04F8EE0BC13338F@email002.lsu.edu>

I do not know the precise language to describe the situation. But here
it is what I want to do.

I need to annotate a graph that contains two or more curves with labels
that contain math symbols. The label must go on top of the curve. 

The problem is that when I annotate the plot, I can see the curve
behind the label. Here it is an example using a simple straight line.

x<-c(0,1)
plot(x,x,type='l')
text(0.5,0.5,expression(theta),cex=3)

The line is visible inside the theta symbol. I would like to avoid that.

Following the solution to problem 3.2 in the ISwR book, for a related
problem,  I used the symbols package to write a rectangle (it can a
circle, etc)
with white bf and white fg and then write the label on top of that
rectangle.

Here is it is the code

x<-c(0,1)
plot(x,x,type='l')
text(0.5,0.5,expression(theta),cex=3)
xmax<-max(abs(x))
dimensions<-matrix(c(xmax/24,xmax/24),nrow=1)
symbols(0.5,0.5,rectangle=dimensions,bg='white',fg='white',add=TRUE,inch
es=FALSE)
text(0.5,0.5,expression(theta),cex=3)

It seems to work, but I am not happy with this solution. The symbols
statement above has fixed dimensions and does not work well with
multiple labels of different sizes. With multiple curves the symbols
statement might peel off other curves, axis, etc. Also if one changes
the size of the label (using cex), the size of the "rectangle" might
be too big or too small and it has to be adjusted.

What I would like to have is a simple way to write the label over the
line such
that the label only wipes out what is needed to avoid seeing the curve
behind. It is like I need a rectangles with rubbery dimensions to
contain just the label I am writing.

I searched the FAQ questions for related problems but I did not find
something related. 

My R skills are very rudimentary and I would not surprised the
solution is trivial.

Thank you for any help.

Luis Escobar


From dcn208 at nyu.edu  Wed Jan 17 17:12:57 2007
From: dcn208 at nyu.edu (Damion Colin Nero)
Date: Wed, 17 Jan 2007 11:12:57 -0500
Subject: [R] Correlation to a Single Gene
Message-ID: <d564dff3c100.45ae04b9@mail.nyu.edu>

I am trying to find a way to perform pairwise correlations against one
gene in a matrix rather than computing every pairwise correlation.  I am
interested in how 1 transcription factor correlates to every gene in a
matrix of 55 experiments (columns) by 23,000 genes (rows), performing
the correlation by rows.  Trying to perform every pairwise correlation
in this fashion is too memory intensive for any computer I am currently
using so I am wondering if anyone had a method for doing pairwise
correlations to a single gene or if there is a preexisting package in R
that might address this.

Damion Nero
Plant Molecular Biology Lab
Department of Biology 
New York University


From milton_ruser at yahoo.com.br  Wed Jan 17 17:21:59 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Wed, 17 Jan 2007 16:21:59 +0000 (GMT)
Subject: [R] sort dataframe by field
Message-ID: <20070117162159.29824.qmail@web56605.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070117/1e217398/attachment.pl 

From michael.greeff at env.ethz.ch  Wed Jan 17 17:24:28 2007
From: michael.greeff at env.ethz.ch (Michael Greeff)
Date: Wed, 17 Jan 2007 17:24:28 +0100
Subject: [R] Fwd:  model simplification in repeated anova
Message-ID: <727EAE38-ADCA-46A0-B138-74FA553BC1E3@env.ethz.ch>

Hi,

I tried to do a model simplification in a repeated anova with 3  
factors (population,treatment,site). I tried to get rid of some  
interactions. Obviously it's not possible to use the function "anova"  
straight away to compare the different models. I got the suggestion  
to use the function proj (see below). In my data, however, I have in  
"block" not the third order interaction (n:p:k), but two factors and  
their interaction (population, site, population:site).

I somehow didn't manage to get the right aov after the projection.



> From: "Richard M. Heiberger" <rmh at temple.edu>
>
>
> npk.aovE <- aov(yield ~  N*P*K + Error(block), npk)
>
> np.k.aovE <- aov(yield ~  N*P+K + Error(block), npk)
>
>
> npk.proj <- proj(npk.aovE)
> npk.x <- npk.proj$block[,"N:P:K"]
>
> npk.aov <- aov(terms(yield ~  npk.x + block + N+P+K+N:P+N:K+P:K,  
> keep.order=TRUE), npk)
>
> np.k.aov <- aov(terms(yield ~  block + N+P+K+N:P, keep.order=TRUE),  
> npk)
>
> summary(npk.aov)
> summary(np.k.aov)
>
> anova(npk.aov, np.k.aov)
>


From bcarvalh at jhsph.edu  Wed Jan 17 17:26:30 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Wed, 17 Jan 2007 11:26:30 -0500
Subject: [R] sort dataframe by field
In-Reply-To: <20070117162159.29824.qmail@web56605.mail.re3.yahoo.com>
References: <20070117162159.29824.qmail@web56605.mail.re3.yahoo.com>
Message-ID: <7B85113E-23BF-43A2-9B7B-8AAEDC65464B@jhsph.edu>

say your data.frame is called "df"

df[order(df$evidence),]

or

df[order(df$evidence, decreasing=T),] # if you want the other way  
around.

b

On Jan 17, 2007, at 11:21 AM, Milton Cezar Ribeiro wrote:

> Hi there,
>
>   How can I sort (order?) a data.frame using a dataframe field  
> (evidence) as classifyer? My data.frame looks like:
>
>   record           model   evidence
> 1             areatotha 6638.32581
> 2       areatotha_ca000 8111.01860
> 3  areatotha_ca000_Pais 1721.41828
> 4       areatotha_ca020  827.33097
> 5  areatotha_ca020_Pais 2212.40899
> 6       areatotha_ca040 3569.17169
> 7  areatotha_ca040_Pais 2940.01636
> 8       areatotha_ca060  992.62852
> 9  areatotha_ca060_Pais 4237.95709
> 10      areatotha_ca080   62.74915
> 11 areatotha_ca080_Pais 1726.55082
> 12       areatotha_Pais   52.02524
> 13      areatotha_ca000 3391.92930
> 14 areatotha_ca000_Pais   39.52170
> 15      areatotha_ca020  268.55875
> 16 areatotha_ca020_Pais   20.43317
> 17      areatotha_ca040 1698.75892
> 18 areatotha_ca040_Pais   43.90613
> 19      areatotha_ca060  350.79857
> 20 areatotha_ca060_Pais   51.04471
>
>   Cheers,
>
>   Miltinho, Brazil
>
>  __________________________________________________
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From alkauffm at rz.uni-potsdam.de  Wed Jan 17 17:34:07 2007
From: alkauffm at rz.uni-potsdam.de (Albrecht Kauffmann)
Date: Wed, 17 Jan 2007 17:34:07 +0100 (CET)
Subject: [R] sp: proj4string has no impact
Message-ID: <Pine.GSO.4.58.0701171716550.8323@persius.rz.uni-potsdam.de>

Hi all,

I'm faced with a problem applying the sp package: The projection argument in

readShapePoly(Shapefile,proj4string="CRS class argument")

e.g.: CRS("+proj=aea +lat_1=46 +lat_2=73 +lat_0=60 +lon_0=84 +x_0=0
+y_0=0 +ellps=clrk66 +units=m  +no_defs")

doesn't have any impact on the plotted object. I also tested the simple
example:

xy = cbind(x = 2 * runif(100) - 1, y = 2 * runif(100) - 1)
plot(SpatialPoints(xy, proj4string =
CRS("+proj=longlat")),xlim=c(-1,1),ylim=c(-1,1))

looks exactly like

plot(SpatialPoints(xy, proj4string =CRS("+proj=stere +lon_0=98  +over"))

or

plot(SpatialPoints(xy))

without any projection.

What I'm doing wrong?

I use the latest versions of sp and rgdal.

With many thanks for any hint,

Albrecht Kauffmann
Potsdam University


From gunter.berton at gene.com  Wed Jan 17 17:36:14 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Wed, 17 Jan 2007 08:36:14 -0800
Subject: [R] Effect size in GLIM models
In-Reply-To: <Pine.LNX.4.64.0701171300010.5482@gannet.stats.ox.ac.uk>
Message-ID: <001801c73a55$9b99fd60$4d908980@gne.windows.gene.com>

Folks:

I think this and several other recent posts on ranking predictors are nice
illustrations of a fundamental conundrum: Empirical models are fit as good
*predictors*; "meaningful" interpretation of separate parameters/components
of the predictors may well be difficult or impossible, especially in complex
models.  All that the fitting process guarantees if it works well is a good
overall predictor to data sampled from the same process. Unfortunately,
most/much of the time, those who apply the procedures are interested in
interpretation, not prediction.

Addendum: Interpretation is helped by well-designed studies and experiments,
hindered by data mining of observational data.

I don't think any of this is profound, just sometimes forgotten; however, I
would welcome public or private reaction to this comment, and especially
refinement/corrections.

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof Brian Ripley
Sent: Wednesday, January 17, 2007 6:02 AM
To: Behnke Jerzy
Cc: Reader Tom; r-help at stat.math.ethz.ch
Subject: Re: [R] Effect size in GLIM models

On Wed, 17 Jan 2007, Behnke Jerzy wrote:

> Dear All,
> I wonder if anyone can advise me as to whether there is a consensus as
> to how the effect size should be calculated from GLIM models in R for
> any specified significant main effect or interaction.

I think there is consensus that effect sizes are not measured by 
significance tests.  If you have a log link (you did not say), the model 
coefficients have a direct interpretation via multiplicative increases in 
rates.

> In investigating the causes of variation in infection in wild animals,
> we have fitted 4-way GLIM models in R with negative binomial errors.

What exactly do you mean by 'GLIM models in R with negative binomial 
errors'?  Negative binomial regression is within the GLM framework only 
for fixed shape theta. Package MASS has glm.nb() which extends the 
framework and you may be using without telling us.  (AFAIK GLIM is a 
software package, not a class of models.)

I suspect you are using the code from MASS without reference to the book
it supports, which has a worked example of model selection.

> These are then simplified using the STEP procedure, and finally each of
> the remaining terms is deleted in turn, and the model without that term
> compared to a model with that term to estimate probability

'probability' of what?

> An ANOVA of each model gives the deviance explained by each interaction
> and main effect, and the percentage deviance attributable to each factor
> can be calculated from NULL deviance.

If theta is not held fixed, anova() is probably not appropriate: see the 
help for anova.negbin.

> However, we estimate probabilities by subsequent deletion of terms, and
> this gives the LR statistic. Expressing the value of the LR statistic as
> a percentage of 2xlog-like in a model without any factors, gives lower
> values than the former procedure.

I don't know anything to suggest percentages of LR statistics are 
reasonable summary measures.  There are extensions of R^2 to these models, 
but AFAIK they share the well-attested drawbacks of R^2.

> Are either of these appropriate? If so which is best, or alternatively
> how can % deviance be calculated. We require % deviance explained by
> each factor or interaction,  because we need to compare individual
> factors (say host age) across a range of infections.
>
> Any advice will be most gratefully appreciated. I can send you a worked
> example if you require more information.

We do ask for more information in the posting guide and the footer of 
every message.  I have had to guess uncomfortably much in formulating my 
answers.

> Jerzy. M. Behnke,
> The School of Biology,
> The University of Nottingham,
> University Park,
> NOTTINGHAM, NG7 2RD
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From P.Dalgaard at biostat.ku.dk  Wed Jan 17 17:39:48 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 17 Jan 2007 17:39:48 +0100
Subject: [R] Row limit for read.table
In-Reply-To: <45AE434C.3080102@cs.odu.edu>
References: <45AE434C.3080102@cs.odu.edu>
Message-ID: <45AE5154.7090600@biostat.ku.dk>

Frank McCown wrote:
> I have been trying to read in a large data set using read.table, but 
> I've only been able to grab the first 50,871 rows of the total 122,269 rows.
>
>  > f <- 
> read.table("http://www.cs.odu.edu/~fmccown/R/Tchange_rates_crawled.dat", 
> header=TRUE, nrows=123000, comment.char="", sep="\t")
>  > length(f$change_rate)
> [1] 50871
>
>  From searching the email archives, I believe this is due to size limits 
> of a data frame.  So...
>   
I think you believe wrongly...
> 1) Why doesn't read.table give a proper warning when it doesn't place 
> every read item into a data frame?
>   
That isn't the problem, it is a somewhat obscure interaction between
quote= and sep= that is doing you in. Remove the sep="\t" and/or add
quote="" and your life should be easier.
> 2) Why isn't there a parameter to read.table that allows the user to 
> specify which columns s/he is interested in?  This functionality would 
> allow extraneous columns to be ignored which would improve memory usage.
>
>   
There is!  check out colClasses

> cc <- rep("NULL",5)
> cc[4:5] <- NA
> f <-
read.table("http://www.cs.odu.edu/~fmccown/R/Tchange_rates_crawled.dat",
header=TRUE, sep="\t", quote="", colClasses=cc)
> str(f)
'data.frame':   122271 obs. of  2 variables:
 $ recovered  : Factor w/ 5 levels "changed","identical",..: 5 3 3 3 2 2
2 2 1 2 ...
 $ change_rate: num  1 0 0 1 0 0 0 0 0 0 ...

> I've already made a work-around by loading the table into mysql and 
> doing a select on the 2 columns I need.  I just wonder why the above 2 
> points aren't implemented.  Maybe they are and I'm totally missing it.
>
> Thanks,
> Frank
>
>
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From martin.becker at mx.uni-saarland.de  Wed Jan 17 17:40:10 2007
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Wed, 17 Jan 2007 17:40:10 +0100
Subject: [R] Row limit for read.table
In-Reply-To: <45AE434C.3080102@cs.odu.edu>
References: <45AE434C.3080102@cs.odu.edu>
Message-ID: <45AE516A.2010903@mx.uni-saarland.de>

Frank McCown schrieb:
> I have been trying to read in a large data set using read.table, but 
> I've only been able to grab the first 50,871 rows of the total 122,269 rows.
>
>  > f <- 
> read.table("http://www.cs.odu.edu/~fmccown/R/Tchange_rates_crawled.dat", 
> header=TRUE, nrows=123000, comment.char="", sep="\t")
>  > length(f$change_rate)
> [1] 50871
>
>  From searching the email archives, I believe this is due to size limits 
> of a data frame.  So...
>
>   
It is not due to size limits, see below.
> 1) Why doesn't read.table give a proper warning when it doesn't place 
> every read item into a data frame?
>
>   
In your case, read.table behaves as documented.
The ' - character is one of the standard quoting characters. Some (but 
very few) of the entrys contain single ' chars, so sometimes more than 
ten thousand lines are just treated as a single entry. Try using 
quote="" to disable quoting, as documented on the help page:

f<-read.table("http://www.cs.odu.edu/~fmccown/R/Tchange_rates_crawled.dat",
header=TRUE, nrows=123000, comment.char="", sep="\t",quote="")

length(f$change_rate)
[1] 122271

> 2) Why isn't there a parameter to read.table that allows the user to 
> specify which columns s/he is interested in?  This functionality would 
> allow extraneous columns to be ignored which would improve memory usage.
>
>   
There is (colClasses, works as documented). Try

 f<-read.table("http://www.cs.odu.edu/~fmccown/R/Tchange_rates_crawled.dat", 

+ header=TRUE, nrows=123000, comment.char="", 
sep="\t",quote="",colClasses=c("character","NULL","NULL","NULL","NULL"))
 > dim(f)
[1] 122271      1

> I've already made a work-around by loading the table into mysql and 
> doing a select on the 2 columns I need.  I just wonder why the above 2 
> points aren't implemented.  Maybe they are and I'm totally missing it.
>
>   
Did you read the help page?
> Thanks,
> Frank
>
>
>   
Regards,
   Martin


From wl at eimb.ru  Wed Jan 17 17:57:40 2007
From: wl at eimb.ru (Vladimir Eremeev)
Date: Wed, 17 Jan 2007 16:57:40 +0000 (UTC)
Subject: [R] Row limit for read.table
References: <45AE434C.3080102@cs.odu.edu>
Message-ID: <loom.20070117T174658-798@post.gmane.org>

The problem is somewhere in the file, probably with tab characters, as removing 
sep="" from your call does the job.

> dfr<-read.table("Tchange_rates_crawled.dat",header=TRUE)
> str(dfr)
'data.frame':   122271 obs. of  5 variables:
[skipped]
> dfr<-
read.table("Tchange_rates_crawled.dat",header=TRUE,stringsAsFactors=FALSE)
> str(dfr)
'data.frame':   122271 obs. of  5 variables:
[skipped]

R has no limitations you're talking about. A couple hours ago my R has 
successfully read in 460000 rows of data from a text file in a data frame using 
read.table. 
I have also processed even larger data sets (more than 500000 rows).


From fmccown at cs.odu.edu  Wed Jan 17 18:22:40 2007
From: fmccown at cs.odu.edu (Frank McCown)
Date: Wed, 17 Jan 2007 12:22:40 -0500
Subject: [R] Row limit for read.table
In-Reply-To: <45AE516A.2010903@mx.uni-saarland.de>
References: <45AE434C.3080102@cs.odu.edu> <45AE516A.2010903@mx.uni-saarland.de>
Message-ID: <45AE5B60.5040308@cs.odu.edu>

> In your case, read.table behaves as documented.
> The ' - character is one of the standard quoting characters. Some (but 
> very few) of the entrys contain single ' chars, so sometimes more than 
> ten thousand lines are just treated as a single entry. Try using 
> quote="" to disable quoting, as documented on the help page:
> 
> f<-read.table("http://www.cs.odu.edu/~fmccown/R/Tchange_rates_crawled.dat",
> header=TRUE, nrows=123000, comment.char="", sep="\t",quote="")
> 
> length(f$change_rate)
> [1] 122271


So either adding quote="" works or removing sep="\t" (and not using 
quote) works.  It seems an odd side-effect that specifying the separator 
changes the default behavior of quoting (because of the ' character).  I 
don't see that association made in the help file.


> There is (colClasses, works as documented). Try
> 
> f<-read.table("http://www.cs.odu.edu/~fmccown/R/Tchange_rates_crawled.dat",
> + header=TRUE, nrows=123000, comment.char="", 
> sep="\t",quote="",colClasses=c("character","NULL","NULL","NULL","NULL"))
>  > dim(f)
> [1] 122271      1

> Did you read the help page?

Of course I did.  For me the definition of colClasses wasn't clear... 
"A vector of classes to be assumed for the columns" didn't seem to be 
the same thing as "the columns you would like to be read."  I may have 
made the association if the help page had contained a simple example of 
using colClasses.

Thanks for the help,
Frank


-- 
Frank McCown
Old Dominion University
http://www.cs.odu.edu/~fmccown/


From murdoch at stats.uwo.ca  Wed Jan 17 18:24:05 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 17 Jan 2007 12:24:05 -0500
Subject: [R] add non-linear line
In-Reply-To: <5953EFB2-8FC9-4B79-AA51-96A302EE8246@interchange.ubc.ca>
References: <5953EFB2-8FC9-4B79-AA51-96A302EE8246@interchange.ubc.ca>
Message-ID: <45AE5BB5.10909@stats.uwo.ca>

On 1/17/2007 10:42 AM, Matt Sakals wrote:
> I am trying to plot a non-linear trend line along with my data. I am  
> sure the solution is simple; any help greatly appreciated.
> 
> Recent and historic attempts:
> fit = nls((sd~a+b*exp(-c/time)+d*geology), start=list(a=1, b=1, c=10,  
> d=-1), model=TRUE)
> plot(time, sd, col=3, main = "Regression", sub=formula(fit))
> lines(time, fitted.values(fit), lwd=2)
> 
> #tt = seq(from=0, to=200, by=10)
> #lines(time, predict(fit, time=tt))
> #lines(time, predict(fit))

Your model has two independent variables, time and geology.  You won't 
get a simple trend line unless the value of geology is kept fixed.  You 
could do that with something like this code:

lines(time, predict(fit, newdata=data.frame(time=time, geology=0)))

Duncan Murdoch


From murdoch at stats.uwo.ca  Wed Jan 17 18:31:57 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 17 Jan 2007 12:31:57 -0500
Subject: [R] add non-linear line
In-Reply-To: <45AE5BB5.10909@stats.uwo.ca>
References: <5953EFB2-8FC9-4B79-AA51-96A302EE8246@interchange.ubc.ca>
	<45AE5BB5.10909@stats.uwo.ca>
Message-ID: <45AE5D8D.2080209@stats.uwo.ca>

On 1/17/2007 12:24 PM, Duncan Murdoch wrote:
> On 1/17/2007 10:42 AM, Matt Sakals wrote:
>> I am trying to plot a non-linear trend line along with my data. I am  
>> sure the solution is simple; any help greatly appreciated.
>> 
>> Recent and historic attempts:
>> fit = nls((sd~a+b*exp(-c/time)+d*geology), start=list(a=1, b=1, c=10,  
>> d=-1), model=TRUE)
>> plot(time, sd, col=3, main = "Regression", sub=formula(fit))
>> lines(time, fitted.values(fit), lwd=2)
>> 
>> #tt = seq(from=0, to=200, by=10)
>> #lines(time, predict(fit, time=tt))
>> #lines(time, predict(fit))
> 
> Your model has two independent variables, time and geology.  You won't 
> get a simple trend line unless the value of geology is kept fixed.  You 
> could do that with something like this code:
> 
> lines(time, predict(fit, newdata=data.frame(time=time, geology=0)))

One other suggestion:  in the new data, make sure your "time" variable 
is sorted into increasing order, or the lines will be a mess.

Duncan Murdoch


From helprhelp at gmail.com  Wed Jan 17 19:10:06 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 17 Jan 2007 13:10:06 -0500
Subject: [R] Help on variable ranking
In-Reply-To: <b5a9b2f50701162122k1b8dce9cyc8c4f1af40f8d063@mail.gmail.com>
References: <b5a9b2f50701162122k1b8dce9cyc8c4f1af40f8d063@mail.gmail.com>
Message-ID: <cdf817830701171010hdf4ad85j61d8dc0736cbe999@mail.gmail.com>

I suggest using permutation on each predictor and see how much the
accuracy drops, no matter what modeling approach you used.


HTH,

weiwei

On 1/17/07, Rupendra Chulyadyo <rchulyadyo at gmail.com> wrote:
> Hello all,
>
> I want to assign relative score to the predictor variables on the basis of
> its influence on the dependent variable. But I could not find any standard
> statistical approach appropriate for this purpose.
> Please suggest the possible approaches.
>
> Thanks in advance,
>
> Rupendra Chulyadyo
> Institute of Engineering,
> Tribhuvan University,
> Nepal
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From Tom.Reader at nottingham.ac.uk  Wed Jan 17 19:35:16 2007
From: Tom.Reader at nottingham.ac.uk (Reader Tom)
Date: Wed, 17 Jan 2007 18:35:16 -0000
Subject: [R] Effect size in GLIM models
References: <BAA2EE790D828F4E8B262F17B3B0A7A3028E6E93@VUIEXCH1.ad.nottingham.ac.uk>
	<Pine.LNX.4.64.0701171300010.5482@gannet.stats.ox.ac.uk>
Message-ID: <BAA2EE790D828F4E8B262F17B3B0A7A374EED5@VUIEXCH1.ad.nottingham.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070117/a2005801/attachment.pl 

From cberry at tajo.ucsd.edu  Wed Jan 17 19:48:13 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Wed, 17 Jan 2007 10:48:13 -0800
Subject: [R] sleep data
In-Reply-To: <45ADEE46.6000803@psych.uib.no>
References: <45ADEE46.6000803@psych.uib.no>
Message-ID: <Pine.LNX.4.64.0701171043310.27414@tajo.ucsd.edu>


Yes, you refer to

 	Cushny, A. R. and Peebles, A. R. The action of optical isomers: II
 	hyoscines. The Journal of Physiology, 1905, 32: 501.510.

which was used by 'Student' to illustrate the paired t-test.

This is indeed a crossover design.

On Wed, 17 Jan 2007, Tom Backer Johnsen wrote:

> When reading the documentation for the "sleep" data set in R, the
> impression is clear, this is an "independent groups" kind of design
> (two groups of 10 subjects each).  However, when browsing the original
> article (referred to in the help file), my impression is quite clear,
> this is really a "repeated measures" kind of data (one group of 10
> subjects, two observations).  What is correct?
>
> Tom
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901


From Roger.Bivand at nhh.no  Wed Jan 17 19:56:00 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 17 Jan 2007 19:56:00 +0100 (CET)
Subject: [R] sp: proj4string has no impact
In-Reply-To: <Pine.GSO.4.58.0701171716550.8323@persius.rz.uni-potsdam.de>
Message-ID: <Pine.LNX.4.44.0701171946010.31005-100000@reclus.nhh.no>

On Wed, 17 Jan 2007, Albrecht Kauffmann wrote:

> Hi all,
> 
> I'm faced with a problem applying the sp package: The projection argument in
> 
> readShapePoly(Shapefile,proj4string="CRS class argument")
> 
> e.g.: CRS("+proj=aea +lat_1=46 +lat_2=73 +lat_0=60 +lon_0=84 +x_0=0
> +y_0=0 +ellps=clrk66 +units=m  +no_defs")
> 
> doesn't have any impact on the plotted object. I also tested the simple
> example:
> 
> xy = cbind(x = 2 * runif(100) - 1, y = 2 * runif(100) - 1)
> plot(SpatialPoints(xy, proj4string =
> CRS("+proj=longlat")),xlim=c(-1,1),ylim=c(-1,1))
> 
> looks exactly like
> 
> plot(SpatialPoints(xy, proj4string =CRS("+proj=stere +lon_0=98  +over"))
> 
> or
> 
> plot(SpatialPoints(xy))
> 
> without any projection.
> 
> What I'm doing wrong?

What impact would you like? The vertical scaling adjustment for 
geographical coordinates ("+proj=longlat") is proportional to the distance 
of the midpoint of ylim from the Equator, so in your case no adjustment 
would be expected:

library(sp)
set.seed(1)
x <- runif(20, -10, 10)
y <- runif(20, -10, 10)
par(mfrow=c(3,2))
plot(SpatialPoints(cbind(x, y)), axes=TRUE)
plot(SpatialPoints(cbind(x, y), proj4string=CRS("+proj=longlat")), 
axes=TRUE)
plot(SpatialPoints(cbind(x, y+50)), axes=TRUE)
plot(SpatialPoints(cbind(x, y+50), proj4string=CRS("+proj=longlat")), 
  axes=TRUE)
plot(SpatialPoints(cbind(x, y+70)), axes=TRUE)
plot(SpatialPoints(cbind(x, y+70), proj4string=CRS("+proj=longlat")),
  axes=TRUE)
par(mfrow=c(1,1))



> 
> I use the latest versions of sp and rgdal.
> 
> With many thanks for any hint,
> 
> Albrecht Kauffmann
> Potsdam University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From cberry at tajo.ucsd.edu  Wed Jan 17 19:59:39 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Wed, 17 Jan 2007 10:59:39 -0800
Subject: [R] Correlation to a Single Gene
In-Reply-To: <d564dff3c100.45ae04b9@mail.nyu.edu>
References: <d564dff3c100.45ae04b9@mail.nyu.edu>
Message-ID: <Pine.LNX.4.64.0701171057000.27414@tajo.ucsd.edu>

On Wed, 17 Jan 2007, Damion Colin Nero wrote:

> I am trying to find a way to perform pairwise correlations against one
> gene in a matrix rather than computing every pairwise correlation.  I am
> interested in how 1 transcription factor correlates to every gene in a
> matrix of 55 experiments (columns) by 23,000 genes (rows), performing
> the correlation by rows.  Trying to perform every pairwise correlation
> in this fashion is too memory intensive for any computer I am currently
> using so I am wondering if anyone had a method for doing pairwise
> correlations to a single gene or if there is a preexisting package in R
> that might address this.


You measure the transcription factor once in each of 55 experiments and 
you measure gene *expression* (or some other quantity) on each of 23000 
genes?

 	cor.vec <- cor (transfac, t( gene.mat ) )

will do.

Questions like this might best be posted to the bioconductor mail list.


>
> Damion Nero
> Plant Molecular Biology Lab
> Department of Biology
> New York University
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901


From rgentlem at fhcrc.org  Wed Jan 17 20:17:45 2007
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Wed, 17 Jan 2007 11:17:45 -0800
Subject: [R] Correlation to a Single Gene
In-Reply-To: <Pine.LNX.4.64.0701171057000.27414@tajo.ucsd.edu>
References: <d564dff3c100.45ae04b9@mail.nyu.edu>
	<Pine.LNX.4.64.0701171057000.27414@tajo.ucsd.edu>
Message-ID: <45AE7659.4030303@fhcrc.org>

In the package genefilter, from www.bioconductor.org there is a function 
to do this (genefinder, if I recall correctly)
best wishes
  Robert


Charles C. Berry wrote:
> On Wed, 17 Jan 2007, Damion Colin Nero wrote:
> 
>> I am trying to find a way to perform pairwise correlations against one
>> gene in a matrix rather than computing every pairwise correlation.  I am
>> interested in how 1 transcription factor correlates to every gene in a
>> matrix of 55 experiments (columns) by 23,000 genes (rows), performing
>> the correlation by rows.  Trying to perform every pairwise correlation
>> in this fashion is too memory intensive for any computer I am currently
>> using so I am wondering if anyone had a method for doing pairwise
>> correlations to a single gene or if there is a preexisting package in R
>> that might address this.
> 
> 
> You measure the transcription factor once in each of 55 experiments and 
> you measure gene *expression* (or some other quantity) on each of 23000 
> genes?
> 
>  	cor.vec <- cor (transfac, t( gene.mat ) )
> 
> will do.
> 
> Questions like this might best be posted to the bioconductor mail list.
> 
> 
>> Damion Nero
>> Plant Molecular Biology Lab
>> Department of Biology
>> New York University
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> Charles C. Berry                        (858) 534-2098
>                                           Dept of Family/Preventive Medicine
> E mailto:cberry at tajo.ucsd.edu	         UC San Diego
> http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Robert Gentleman, PhD
Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M2-B876
PO Box 19024
Seattle, Washington 98109-1024
206-667-7700
rgentlem at fhcrc.org


From lauri.nikkinen at iki.fi  Wed Jan 17 20:43:56 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Wed, 17 Jan 2007 21:43:56 +0200
Subject: [R] tapply, data.frame problem
Message-ID: <ba8c09910701171143x3e68c8dckdbeba9339f603579@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070117/d4118745/attachment.pl 

From ccleland at optonline.net  Wed Jan 17 20:54:42 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 17 Jan 2007 14:54:42 -0500
Subject: [R] tapply, data.frame problem
In-Reply-To: <ba8c09910701171143x3e68c8dckdbeba9339f603579@mail.gmail.com>
References: <ba8c09910701171143x3e68c8dckdbeba9339f603579@mail.gmail.com>
Message-ID: <45AE7F02.1030802@optonline.net>

Lauri Nikkinen wrote:
> Hi R-users,
> 
> I'm quite new to R and trying to learn the basics. I have a following
> problem concerning the convertion of array object into data frame. I have
> made following data sets
> 
> tmp1 <- rnorm(100)
> tmp2 <- gl(10,2,length=100)
> tmp3 <- as.data.frame(cbind(tmp1,tmp2))
> tmp3.sum <- tapply(tmp3$tmp1,tmp3$tmp2,sum)
> tmp3.sum <- as.data.frame(tapply(tmp1,tmp2,sum))
> and I want the levels from tmp2 be shown as a column in the data.frame, not
> as row name as it now does. To put it in another way, as a result, I want a
> data frame with two columns: levels and the sums of those levels. Row names
> can be, for example, numbers from 1 to 10.

aggregate(tmp3[1], tmp3[2], sum)
   tmp2        tmp1
1     1  8.41550650
2     2  3.65831086
3     3 -0.26296334
4     4  3.45368671
5     5 -4.64383794
6     6  0.25640949
7     7  0.02832348
8     8 -0.03811150
9     9  1.41724121
10   10 -1.06780900

?aggregate

> -Lauri Nikkinen
> Lahti, Finland
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From rdiaz02 at gmail.com  Wed Jan 17 22:01:56 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Wed, 17 Jan 2007 22:01:56 +0100
Subject: [R] eval(parse(text vs. get when accessing a function
In-Reply-To: <971536df0701071540r4d75cb8cm64d9002573442cda@mail.gmail.com>
References: <07E228A5BE53C24CAD490193A7381BBB77C654@LP-EXCHVS07.CO.IHC.COM>
	<624934630701060616i2718ab71xb981fbca39c0c910@mail.gmail.com>
	<6phps9sm73o.fsf@gopher4.fhcrc.org>
	<624934630701061123j1f779e5bma21728f8cbf4bf09@mail.gmail.com>
	<971536df0701071540r4d75cb8cm64d9002573442cda@mail.gmail.com>
Message-ID: <624934630701171301x4b871fa7te9e306604b543840@mail.gmail.com>

(I overlooked the reply).

Thanks, Gabor. That is neat and easy! (and I should have been able to
see it on my own :-(

Best,

R.

On 1/8/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> The S4 is not essential.  You could do it in S3 too:
>
> > f.a <- function(x) x+1
> > f.b <- function(x) x+2
> > f <- function(x) UseMethod("f")
> >
> > f(structure(10, class = "a"))
> [1] 11
> attr(,"class")
> [1] "a"
>
> On 1/6/07, Ramon Diaz-Uriarte <rdiaz02 at gmail.com> wrote:
> > Hi Martin,
> >
> >
> >
> > On 1/6/07, Martin Morgan <mtmorgan at fhcrc.org> wrote:
> > > Hi Ramon,
> > >
> > > It seems like a naming convention (f.xxx) and eval(parse(...)) are
> > > standing in for objects (of class 'GeneSelector', say, representing a
> > > function with a particular form and doing a particular operation) and
> > > dispatch (a function 'geneConverter' might handle a converter of class
> > > 'GeneSelector' one way, user supplied ad-hoc functions more carefully;
> > > inside geneConverter the only real concern is that the converter
> > > argument is in fact a callable function).
> > >
> > > eval(parse(...)) brings scoping rules to the fore as an explicit
> > > programming concern; here scope is implicit, but that's probably better
> > > -- R will get its own rules right.
> > >
> > > Martin
> > >
> > > Here's an S4 sketch:
> > >
> > > setClass("GeneSelector",
> > >          contains="function",
> > >          representation=representation(description="character"),
> > >          validity=function(object) {
> > >              msg <- NULL
> > >              argNames <- names(formals(object))
> > >              if (argNames[1]!="x")
> > >                msg <- c(msg, "\n  GeneSelector requires a first argument named 'x'")
> > >              if (!"..." %in% argNames)
> > >                msg <- c(msg, "\n  GeneSelector requires '...' in its signature")
> > >              if (0==length(object at description))
> > >                msg <- c(msg, "\n  Please describe your GeneSelector")
> > >              if (is.null(msg)) TRUE else msg
> > >          })
> > >
> > > setGeneric("geneConverter",
> > >            function(converter, x, ...) standardGeneric("geneConverter"),
> > >            signature=c("converter"))
> > >
> > > setMethod("geneConverter",
> > >           signature(converter="GeneSelector"),
> > >           function(converter, x, ...) {
> > >               ## important stuff here
> > >               converter(x, ...)
> > >           })
> > >
> > > setMethod("geneConverter",
> > >           signature(converter="function"),
> > >           function(converter, x, ...) {
> > >               message("ad-hoc converter; hope it works!")
> > >               converter(x, ...)
> > >           })
> > >
> > > and then...
> > >
> > > > c1 <- new("GeneSelector",
> > > +           function(x, ...) prod(x, ...),
> > > +           description="Product of x")
> > > >
> > > > c2 <- new("GeneSelector",
> > > +           function(x, ...) sum(x, ...),
> > > +           description="Sum of x")
> > > >
> > > > geneConverter(c1, 1:4)
> > > [1] 24
> > > > geneConverter(c2, 1:4)
> > > [1] 10
> > > > geneConverter(mean, 1:4)
> > > ad-hoc converter; hope it works!
> > > [1] 2.5
> > > >
> > > > cvterr <- new("GeneSelector", function(y) {})
> > > Error in validObject(.Object) : invalid class "GeneSelector" object: 1:
> > >   GeneSelector requires a first argument named 'x'
> > > invalid class "GeneSelector" object: 2:
> > >   GeneSelector requires '...' in its signature
> > > invalid class "GeneSelector" object: 3:
> > >   Please describe your GeneSelector
> > > > xxx <- 10
> > > > geneConverter(xxx, 1:4)
> > > Error in function (classes, fdef, mtable)  :
> > >         unable to find an inherited method for function "geneConverter", for signature "numeric"
> > >
> >
> >
> >
> > Thanks!! That is actually a rather interesting alternative approach
> > and I can see it also adds a lot of structure to the problem. I have
> > to confess, though, that I am not a fan of OOP (nor of S4 classes); in
> > this case, in particular, it seems there is a lot of scaffolding in
> > the code above (the counterpoint to the structure?) and, regarding
> > scoping rules, I prefer to think about them explicitly (I find it much
> > simpler than inheritance).
> >
> > Best,
> >
> > R.
> >
> >
> > >
> > > "Ramon Diaz-Uriarte" <rdiaz02 at gmail.com> writes:
> > >
> > > > Dear Greg,
> > > >
> > > >
> > > > On 1/5/07, Greg Snow <Greg.Snow at intermountainmail.org> wrote:
> > > >> Ramon,
> > > >>
> > > >> I prefer to use the list method for this type of thing, here are a couple of reasons why (maybe you are more organized than me and would never do some of the stupid things that I have, so these don't apply to you, but you can see that the general suggestion applys to some of the rest of us).
> > > >>
> > > >
> > > >
> > > > Those suggestions do apply to me of course (no claim to being
> > > > organized nor beyond idiocy here). And actually the suggestions on
> > > > this thread are being very useful. I think, though, that I was not
> > > > very clear on the context and my examples were too dumbed down. So
> > > > I'll try to give more detail (nothing here is secret, I am just trying
> > > > not to bore people).
> > > >
> > > > The code is part of a web-based application, so there is no
> > > > interactive user. The R code is passed the arguments (and optional
> > > > user functions) from the CGI.
> > > >
> > > > There is one "core" function (call it cvFunct) that, among other
> > > > things, does cross-validation. So this is one way to do things:
> > > >
> > > > cvFunct <- function(whatever, genefiltertype, whateverelse) {
> > > >       internalGeneSelect <- eval(parse(text = paste("geneSelect",
> > > >                                              genefiltertype, sep = ".")))
> > > >
> > > >       ## do things calling internalGeneSelect,
> > > > }
> > > >
> > > > and now define all possible functions as
> > > >
> > > > geneSelect.Fratio <- function(x, y, z) {##something}
> > > > geneSelect.Wilcoxon <- function(x, y, z) {## something else}
> > > >
> > > > If I want more geneSelect functions, adding them is simple. And I can
> > > > even allow the user to pass her/his own functions, with the only
> > > > restriction that it takes three args, x, y, z, and that the function
> > > > is to be called: "geneSelect." and a user choosen string. (Yes, I need
> > > > to make sure no calls to "system", etc, are in the user code, etc,
> > > > etc, but that is another issue).
> > > >
> > > > The general idea is not new of course. For instance, in package
> > > > "e1071", a somewhat similar thing is done in function "tune", and
> > > > David Meyer there uses "do.call". However, tune is a lot more general
> > > > than what I had in mind. For instance, "tune" deals with arbitrary
> > > > functions, with arbitrary numbers and names of parameters, whereas my
> > > > functions above all take only three arguments (x: a matrix, y: a
> > > > vector; z: an integer), so the neat functionality provided by
> > > > "do.call", and passing the args as a list is not really needed.
> > > >
> > > > So, given that my situation is so structured, and I do not need
> > > > "do.call", I think the approach via eval(parse(paste makes my life
> > > > simple:
> > > >
> > > > a) the central function (cvFunct) uses something I can easily
> > > > recognize: "internalGeneSelect"
> > > >
> > > > b) after the initial eval(parse(text I do not need to worry anymore
> > > > about what the "true" gene selection function is called
> > > >
> > > > c) adding new functions and calling them is simple: function naming
> > > > follows a simple pattern ("geneSelect." + postfix) and calling the
> > > > user function only requires passing the postfix to cvFunct.
> > > >
> > > > d) notice also that, at least the functs. I define, will of course not
> > > > be named "f.1", etc, but rather things like "geneSelect.Fratio" or
> > > > "geneSelect.namesThatStartWithCuteLetters";
> > > >
> > > > I hope this makes things more clear. I did not include this detail
> > > > because this is probably boring (I guess most of you have stopped
> > > > reading by now :-).
> > > >
> > > >
> > > >> Using the list forces you to think about what functions may be called and thinking about things before doing them is usually a good idea.  Personally I don't trust the user of my functions (usually my future self who has forgotten something that seemed obvious at the time) to not do something stupid with them.
> > > >>
> > > >> With list elements you can have names for the functions and access them either by the name or by a number, I find that a lot easier when I go back to edit/update than to remember which function f.1 or f.2 did what.
> > > >>
> > > >
> > > > But I don't see how having your functions as list elements is easier
> > > > (specially if the function is longer than 2 to 3 lines) than having
> > > > all functions systematically named things such as:
> > > >
> > > > geneSelect.Fratio
> > > > geneSelect.Random
> > > > geneSelect.LetterA
> > > > etc
> > > >
> > > > Of course, I could have a list with the components named "Fratio"
> > > > "Random", "LetterA". But I fail to see what it adds. And it forces me
> > > > to build the list, and probably rebuild it whe (or not build it until)
> > > > the user enters her/his own selection function. But the later I do not
> > > > need to do with the scheme above.
> > > >
> > > >
> > > >> With your function, what if the user runs:
> > > >>
> > > >> > g(5,3)
> > > >>
> > > >> What should it do?  (you have only shown definitions for f.1 and f.2).  With my luck I would accidentily type that and just happen to have a f.3 function sitting around from a previous project that does something that I really don't want it to do now.  If I use the list approach then I will get a subscript out of bounds error rather than running something unintended.
> > > >>
> > > >>
> > > >
> > > > I see the general concern, but not how it applies here. If I pass
> > > > argument "Fratio" then either I use geneSelect.Fratio or I get an
> > > > error if "geneSelect.Fratio" does not exist. Similar to what would
> > > > happen if I do
> > > >
> > > > g1(2, 8)
> > > >
> > > > when f.8 is not defined:
> > > >
> > > > Error in eval(expr, envir, enclos) : object "f.8" not found
> > > > So even in more general cases, except for function redefinitions, etc,
> > > > you are not able to call non-existent stuff.
> > > >
> > > >> 2nd, If I used the eval-parse approach then I would probably at some point redefine f.1 or f.2 to the output of a regression analysis or something, then go back and run the g function at a later time and wonder why I am getting an error, then once I have finally figured it out, now I need to remember what f.1 did and rewrite it again.  I am much less likely to accidentally replace an element of a list, and if the list is well named I am unlikely to replace the whole list by accident.
> > > >>
> > > >>
> > > >
> > > > Yes, that is true. Again, it does not apply to the actual case I have
> > > > in mind, but of course, without the detailed info on context I just
> > > > gave, you could not know that.
> > > >
> > > >
> > > >> 3rd, If I ever want to use this code somewhere else (new version of R, on the laptop, give to coworker, ...), it is a lot easier to save and load a single list than to try to think of all the functions that need to be saved.
> > > >>
> > > >
> > > > Oh, sure. But all the functions above live in a single file (actually,
> > > > a minipackage) except for the optional use function (which is read
> > > > from a file).
> > > >
> > > >
> > > >>
> > > >> Personally I have never regretted trying not to underestimate my own future stupidity.
> > > >>
> > > >
> > > > Neither do I. And actually, that is why I asked: if Thomas Lumley
> > > > said, in the fortune, that I better rethink about it, then I should
> > > > try rethinking about it. But I asked because I failed to see what the
> > > > problem is.
> > > >
> > > >
> > > >> Hope this helps,
> > > >>
> > > >
> > > > It certainly does.
> > > >
> > > >
> > > > Best,
> > > >
> > > > R.
> > > >
> > > >
> > > >> --
> > > >> Gregory (Greg) L. Snow Ph.D.
> > > >> Statistical Data Center
> > > >> Intermountain Healthcare
> > > >> greg.snow at intermountainmail.org
> > > >> (801) 408-8111
> > > >>
> > > >>
> > > >>
> > > >> > -----Original Message-----
> > > >> > From: r-help-bounces at stat.math.ethz.ch
> > > >> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ramon
> > > >> > Diaz-Uriarte
> > > >> > Sent: Friday, January 05, 2007 11:41 AM
> > > >> > To: Peter Dalgaard
> > > >> > Cc: r-help; rdiaz02 at gmail.com
> > > >> > Subject: Re: [R] eval(parse(text vs. get when accessing a function
> > > >> >
> > > >> > On Friday 05 January 2007 19:21, Peter Dalgaard wrote:
> > > >> > > Ramon Diaz-Uriarte wrote:
> > > >> > > > Dear All,
> > > >> > > >
> > > >> > > > I've read Thomas Lumley's fortune "If the answer is parse() you
> > > >> > > > should usually rethink the question.". But I am not sure it that
> > > >> > > > also applies (and why) to other situations (Lumley's comment
> > > >> > > > http://tolstoy.newcastle.edu.au/R/help/05/02/12204.html
> > > >> > > > was in reply to accessing a list).
> > > >> > > >
> > > >> > > > Suppose I have similarly called functions, except for a
> > > >> > postfix. E.g.
> > > >> > > >
> > > >> > > > f.1 <- function(x) {x + 1}
> > > >> > > > f.2 <- function(x) {x + 2}
> > > >> > > >
> > > >> > > > And sometimes I want to call f.1 and some other times f.2 inside
> > > >> > > > another function. I can either do:
> > > >> > > >
> > > >> > > > g <- function(x, fpost) {
> > > >> > > >     calledf <- eval(parse(text = paste("f.", fpost, sep = "")))
> > > >> > > >     calledf(x)
> > > >> > > >     ## do more stuff
> > > >> > > > }
> > > >> > > >
> > > >> > > >
> > > >> > > > Or:
> > > >> > > >
> > > >> > > > h <- function(x, fpost) {
> > > >> > > >     calledf <- get(paste("f.", fpost, sep = ""))
> > > >> > > >     calledf(x)
> > > >> > > >     ## do more stuff
> > > >> > > > }
> > > >> > > >
> > > >> > > >
> > > >> > > > Two questions:
> > > >> > > > 1) Why is the second better?
> > > >> > > >
> > > >> > > > 2) By changing g or h I could use "do.call" instead; why
> > > >> > would that
> > > >> > > > be better? Because I can handle differences in argument lists?
> > > >> >
> > > >> > Dear Peter,
> > > >> >
> > > >> > Thanks for your answer.
> > > >> >
> > > >> > >
> > > >> > > Who says that they are better?  If the question is how to call a
> > > >> > > function specified by half of its name, the answer could well be to
> > > >> > > use parse(), the point is that you should rethink whether that was
> > > >> > > really the right question.
> > > >> > >
> > > >> > > Why not instead, e.g.
> > > >> > >
> > > >> > > f <- list("1"=function(x) {x + 1} , "2"=function(x) {x + 2}) h <-
> > > >> > > function(x, fpost) f[[fpost]](x)
> > > >> > >
> > > >> > > > h(2,"2")
> > > >> > >
> > > >> > > [1] 4
> > > >> > >
> > > >> > > > h(2,"1")
> > > >> > >
> > > >> > > [1] 3
> > > >> > >
> > > >> >
> > > >> > I see, this is direct way of dealing with the problem.
> > > >> > However, you first need to build the f list, and you might
> > > >> > not know about that ahead of time. For instance, if I build a
> > > >> > function so that the only thing that you need to do to use my
> > > >> > function g is to call your function "f.something", and then
> > > >> > pass the "something".
> > > >> >
> > > >> > I am still under the impression that, given your answer,
> > > >> > using "eval(parse(text" is not your preferred way.  What are
> > > >> > the possible problems (if there are any, that is). I guess I
> > > >> > am puzzled by "rethink whether that was really the right question".
> > > >> >
> > > >> >
> > > >> > Thanks,
> > > >> >
> > > >> > R.
> > > >> >
> > > >> >
> > > >> >
> > > >> >
> > > >> >
> > > >> >
> > > >> >
> > > >> > > > Thanks,
> > > >> > > >
> > > >> > > >
> > > >> > > > R.
> > > >> >
> > > >> > --
> > > >> > Ram?n D?az-Uriarte
> > > >> > Centro Nacional de Investigaciones Oncol?gicas (CNIO)
> > > >> > (Spanish National Cancer Center) Melchor Fern?ndez Almagro, 3
> > > >> > 28029 Madrid (Spain)
> > > >> > Fax: +-34-91-224-6972
> > > >> > Phone: +-34-91-224-6900
> > > >> >
> > > >> > http://ligarto.org/rdiaz
> > > >> > PGP KeyID: 0xE89B3462
> > > >> > (http://ligarto.org/rdiaz/0xE89B3462.asc)
> > > >> >
> > > >> >
> > > >> >
> > > >> > **NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en
> > > >> > s...{{dropped}}
> > > >> >
> > > >> > ______________________________________________
> > > >> > R-help at stat.math.ethz.ch mailing list
> > > >> > https://stat.ethz.ch/mailman/listinfo/r-help
> > > >> > PLEASE do read the posting guide
> > > >> > http://www.R-project.org/posting-guide.html
> > > >> > and provide commented, minimal, self-contained, reproducible code.
> > > >> >
> > > >>
> > > >>
> > > >
> > > >
> > > > --
> > > > Ramon Diaz-Uriarte
> > > > Statistical Computing Team
> > > > Structural Biology and Biocomputing Programme
> > > > Spanish National Cancer Centre (CNIO)
> > > > http://ligarto.org/rdiaz
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > > --
> > > Martin T. Morgan
> > > Bioconductor / Computational Biology
> > > http://bioconductor.org
> > >
> >
> >
> > --
> > Ramon Diaz-Uriarte
> > Statistical Computing Team
> > Structural Biology and Biocomputing Programme
> > Spanish National Cancer Centre (CNIO)
> > http://ligarto.org/rdiaz
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From waltman at cs.nyu.edu  Wed Jan 17 22:54:31 2007
From: waltman at cs.nyu.edu (Peter Waltman)
Date: Wed, 17 Jan 2007 16:54:31 -0500
Subject: [R] Memory leak with character arrays?
Message-ID: <45AE9B17.1080205@cs.nyu.edu>

Hi -

When I'm trying to read in a text file into a labeled character array, 
the memory stamp/footprint of R will exceed 4 gigs or more.  I've seen 
this behavior on Mac OS X, Linux for AMD_64 and X86_64., and the R 
versions are 2.4, 2.4 and 2.2, respectively.  So, it would seem that 
this is platform and R version independant.

The file that I'm reading contains the upstream regions of the yeast 
genome, with each upstream region labeled using a FASTA header, i.e.:

    FASTA header for gene 1
    upstream region.....
    .....
    ....
    FASTA header for gene 2
    upstream....
    ....

The script I use - code below - opens the file, parses for a FASTA 
header, and then parses the header for the gene name.  Once this is 
done, it reads the following lines which contain the upstream region, 
and then adds it as an item to the character array, using the gene name 
as the name of the item it adds.  And then continues on to the following 
genes.

Each upstream region (the text to be added) is 550 bases (characters) 
long.  With ~6000 genes in the file I'm reading it, this would be 550 * 
6000 * 8 (if we're using ascii chars) ~= 25 Megs (if we're using ascii 
chars).

I realize that the character arrays/vectors will have a higher memory 
stamp b/c they are a named array and most likely aren't storing the text 
as ascii, but 4 gigs and up seems a bit excessive.  Or is it?

For an example, this is the output of top, at the point which R has 
processed around 5000 genes:

      PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND 
     4969 waltman   18   0 *6746m 3.4g*  920 D  2.7 88.2  19:09.19 R    

Is this expected behavior?  Can anyone recommend a less memory intensive 
way to store this data?  The relevant code that reads in the file follows:

     ....code....
         lines <- readLines( gzfile( seqs.fname ) )
         
          n.seqs <- 0
         
          upstream <- gene.names <- character()
          syn <- character( 0 )
          gene.start <- gene.end <- integer()
          gene <- seq <- ""


          for ( i in 1:length( lines ) ) {
            line <- lines[ i ]
            if ( line == "" ) next
            if ( substr( line, 1, 1 ) == ">" ) {

              if ( seq != "" && gene != "" ) upstream[ gene ] <-
    toupper( seq )
              splitted <- strsplit( line, "\t" )[[ 1 ]]
              splitted <- strsplit( splitted[ 1 ], ";\\ " )[[ 1 ]]
              gene <- toupper( substr( splitted[ 1 ], 2, nchar(
    splitted[ 1 ] ) ) )
              syn <- splitted[ 2 ]
              if ( ! is.null( syn ) &&
                  length( grep( valid.gene.regexp, gene, perl=T ) ) == 0 &&
                  length( grep( valid.gene.regexp, syn, perl=T ) ) == 1
    ) gene <- syn
              else if ( length( grep( valid.gene.regexp, gene, perl=T,
    ignore.case=T ) ) == 0 &&
                       length( grep( valid.gene.regexp, syn, perl=T,
    ignore.case=T ) ) == 0 ) next
              gene.start[ gene ] <- as.integer( splitted[ 9 ] )
              gene.end[ gene ] <- as.integer( splitted[ 10 ] )
              if ( n.seqs %% 100 == 0 ) cat.new( n.seqs, gene, "|", syn,
    "| length=", nchar( seq ),
                               gene.end[gene]-gene.start[gene]+1,"\n" )
              if ( ! is.na( syn ) && syn != "" ) gene.names[ gene ] <- syn
              else gene.names[ gene ] <- toupper( gene )
              n.seqs <- n.seqs + 1
              seq <- ""
            } else {
              seq <- paste( seq, line, sep="" )
            }
          }
          if ( seq != "" && gene != "" ) upstream[ gene ] <- toupper( seq )

     ....code....

Thanks,

Peter Waltman


From endeitz at yahoo.com  Wed Jan 17 23:18:53 2007
From: endeitz at yahoo.com (endeitz)
Date: Wed, 17 Jan 2007 14:18:53 -0800 (PST)
Subject: [R] Coefficient of determination when intercept is zero
Message-ID: <8420811.post@talk.nabble.com>


I am curious as to the "lm" calculation of R2 (multiple coefficient of
determination, I assume) when intercept is zero.  I have 18 data points, two
independent variables:

First, a model with an intercept:

> mod0=lm(Div~Rain+Evap,data=test)
> summary(mod0)$r.squared
[1] 0.6257541
> cor(predict(mod0),test$Div)^2
[1] 0.6257541

The $r.squared and the result from "cor" are the same, as I would expect.

Now we try a model with zero intercept:

> mod1=lm(Div~0+Rain+Evap,data=test)
> summary(mod1)$r.squared
[1] 0.9099358
> cor(predict(mod1),test$Div)^2
[1] 0.5813659

Why has the $r.squared value increased to 0.9?  And now the result from
"cor" is not the same?  Is there a special way to calculate the coefficient
of determination when the intercept is zero?

Cheers,

Ed.

-- 
View this message in context: http://www.nabble.com/Coefficient-of-determination-when-intercept-is-zero-tf3030776.html#a8420811
Sent from the R help mailing list archive at Nabble.com.


From jholtman at gmail.com  Wed Jan 17 23:30:37 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 17 Jan 2007 17:30:37 -0500
Subject: [R] How to annotate a graph with non-transparent math labels?
In-Reply-To: <AB87A247CEA3C645AF4B46E04F8EE0BC13338F@email002.lsu.edu>
References: <AB87A247CEA3C645AF4B46E04F8EE0BC13338F@email002.lsu.edu>
Message-ID: <644e1f320701171430w6b668206y38774e114c4f1b2f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070117/3282973a/attachment.pl 

From b.jacobs at pandora.be  Wed Jan 17 23:37:38 2007
From: b.jacobs at pandora.be (Bert Jacobs)
Date: Wed, 17 Jan 2007 23:37:38 +0100
Subject: [R] Fast Removing Duplicates from Every Column
In-Reply-To: AAAAAOXvfE/tUWVMujI1J0cWactkaSYA
Message-ID: <20070117223737.929B52200AD@assei1bl6.telenet-ops.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070117/40bcf8b3/attachment.pl 

From ken.feng at citigroup.com  Thu Jan 18 00:51:36 2007
From: ken.feng at citigroup.com (Feng, Ken [CIB-EQTY])
Date: Thu, 18 Jan 2007 08:51:36 +0900
Subject: [R] R.oo Destructors
Message-ID: <0143A263BEF94644AC0D4027373EECD30543096A@exyhmb08.jpn.nsroot.net>


Has anyone figured out how to create a destructor in R.oo?

How I'd like to use it:  I have an object which opens a connection thru RODBC
(held as a private member) It would be nice if the connection closes automatically
(inside the destructor) when an object gets gc()'ed.

Thanks in advance.

Regards,
Ken
BTW, a >BIG< thanks to Henrik Bengtsson for creating the R.oo package!
Lucky for me as I am not smart enough to use S4...


From mkimpel at iupui.edu  Thu Jan 18 01:26:32 2007
From: mkimpel at iupui.edu (Kimpel, Mark William)
Date: Wed, 17 Jan 2007 19:26:32 -0500
Subject: [R] help with regexpr in gsub
In-Reply-To: <0143A263BEF94644AC0D4027373EECD30543096A@exyhmb08.jpn.nsroot.net>
References: <0143A263BEF94644AC0D4027373EECD30543096A@exyhmb08.jpn.nsroot.net>
Message-ID: <836F00680EECD340A96AD34ECFF3B534B4A65D@iu-mssg-mbx106.ads.iu.edu>

I have a very long vector of character strings of the format
"GO:0008104.ISS" and need to strip off the dot and anything that follows
it. There are always 10 characters before the dot. The actual characters
and the number of them after the dot is variable.

So, I would like to return in the format "GO:0008104" . I could do this
with substr and loop over the entire vector, but I thought there might
be a more elegant (and faster) way to do this.

I have tried gsub using regular expressions without success. The code 

gsub(pattern= "\.*?" , replacement="", x=character.vector)

correctly locates the positions in the vector that contain the dot, but
replaces all of the strings with "". Obviously not what I want. Is there
a regular expression for replacement that would accomplish what I want?

Or, does R have a better way to do this?

Thanks,

Mark

Mark W. Kimpel MD 

 

(317) 490-5129 Work, & Mobile

 

(317) 663-0513 Home (no voice mail please)

1-(317)-536-2730 FAX


From ggrothendieck at gmail.com  Thu Jan 18 01:33:54 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 17 Jan 2007 19:33:54 -0500
Subject: [R] help with regexpr in gsub
In-Reply-To: <836F00680EECD340A96AD34ECFF3B534B4A65D@iu-mssg-mbx106.ads.iu.edu>
References: <0143A263BEF94644AC0D4027373EECD30543096A@exyhmb08.jpn.nsroot.net>
	<836F00680EECD340A96AD34ECFF3B534B4A65D@iu-mssg-mbx106.ads.iu.edu>
Message-ID: <971536df0701171633t54777ca5k6c2db028a791d02f@mail.gmail.com>

Try this:

> gsub("[.].*", "", "GO:0008104.ISS")
[1] "GO:0008104"

On 1/17/07, Kimpel, Mark William <mkimpel at iupui.edu> wrote:
> I have a very long vector of character strings of the format
> "GO:0008104.ISS" and need to strip off the dot and anything that follows
> it. There are always 10 characters before the dot. The actual characters
> and the number of them after the dot is variable.
>
> So, I would like to return in the format "GO:0008104" . I could do this
> with substr and loop over the entire vector, but I thought there might
> be a more elegant (and faster) way to do this.
>
> I have tried gsub using regular expressions without success. The code
>
> gsub(pattern= "\.*?" , replacement="", x=character.vector)
>
> correctly locates the positions in the vector that contain the dot, but
> replaces all of the strings with "". Obviously not what I want. Is there
> a regular expression for replacement that would accomplish what I want?
>
> Or, does R have a better way to do this?
>
> Thanks,
>
> Mark
>
> Mark W. Kimpel MD
>
>
>
> (317) 490-5129 Work, & Mobile
>
>
>
> (317) 663-0513 Home (no voice mail please)
>
> 1-(317)-536-2730 FAX
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Hong.Ooi at iag.com.au  Thu Jan 18 01:44:33 2007
From: Hong.Ooi at iag.com.au (Hong Ooi)
Date: Thu, 18 Jan 2007 11:44:33 +1100
Subject: [R] help with regexpr in gsub
Message-ID: <200701180042.l0I0g5QS009550@hypatia.math.ethz.ch>


_______________________________________________________________________________________


substr is vectorised, so it should work fine without needing an explicit
loop.

-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
+61 (2) 9292 1566
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kimpel, Mark
William
Sent: Thursday, 18 January 2007 11:27 AM
To: r-help at stat.math.ethz.ch
Subject: [R] help with regexpr in gsub

I have a very long vector of character strings of the format
"GO:0008104.ISS" and need to strip off the dot and anything that follows
it. There are always 10 characters before the dot. The actual characters
and the number of them after the dot is variable.

So, I would like to return in the format "GO:0008104" . I could do this
with substr and loop over the entire vector, but I thought there might
be a more elegant (and faster) way to do this.

I have tried gsub using regular expressions without success. The code 

gsub(pattern= "\.*?" , replacement="", x=character.vector)

correctly locates the positions in the vector that contain the dot, but
replaces all of the strings with "". Obviously not what I want. Is there
a regular expression for replacement that would accomplish what I want?

Or, does R have a better way to do this?

Thanks,

Mark

Mark W. Kimpel MD 

 

(317) 490-5129 Work, & Mobile

 

(317) 663-0513 Home (no voice mail please)

1-(317)-536-2730 FAX

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

_______________________________________________________________________________________

The information transmitted in this message and its attachme...{{dropped}}


From sfalcon at fhcrc.org  Thu Jan 18 01:46:48 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 17 Jan 2007 16:46:48 -0800
Subject: [R] help with regexpr in gsub
In-Reply-To: <836F00680EECD340A96AD34ECFF3B534B4A65D@iu-mssg-mbx106.ads.iu.edu>
	(Mark William Kimpel's message of "Wed,
	17 Jan 2007 19:26:32 -0500")
References: <0143A263BEF94644AC0D4027373EECD30543096A@exyhmb08.jpn.nsroot.net>
	<836F00680EECD340A96AD34ECFF3B534B4A65D@iu-mssg-mbx106.ads.iu.edu>
Message-ID: <m23b695fgn.fsf@ziti.local>

"Kimpel, Mark William" <mkimpel at iupui.edu> writes:

> I have a very long vector of character strings of the format
> "GO:0008104.ISS" and need to strip off the dot and anything that follows
> it. There are always 10 characters before the dot. The actual characters
> and the number of them after the dot is variable.
>
> So, I would like to return in the format "GO:0008104" . I could do this
> with substr and loop over the entire vector, but I thought there might
> be a more elegant (and faster) way to do this.
>
> I have tried gsub using regular expressions without success. The code 
>
> gsub(pattern= "\.*?" , replacement="", x=character.vector)

I guess you want:

    sub("([GO:0-9]+)\\..*$", "\\1", goids)

[You don't need gsub here]

But I don't understand why you wouldn't want to use substr.  At least
for me substr looks to be about 20x faster than sub for this
problem...


  > library(GO)
  > goids = ls(GOTERM)
  > gids = paste(goids, "ISS", sep=".")
  > gids[1:10]
   [1] "GO:0000001.ISS" "GO:0000002.ISS" "GO:0000003.ISS" "GO:0000004.ISS"
   [5] "GO:0000006.ISS" "GO:0000007.ISS" "GO:0000009.ISS" "GO:0000010.ISS"
   [9] "GO:0000011.ISS" "GO:0000012.ISS"
  
  > system.time(z <- substr(gids, 0, 10))
     user  system elapsed 
    0.008   0.000   0.007 
  > system.time(z2 <- sub("([GO:0-9]+)\\..*$", "\\1", gids))
     user  system elapsed 
    0.136   0.000   0.134 

+ seth


From marc_schwartz at comcast.net  Thu Jan 18 02:10:30 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 17 Jan 2007 19:10:30 -0600
Subject: [R] help with regexpr in gsub
In-Reply-To: <m23b695fgn.fsf@ziti.local>
References: <0143A263BEF94644AC0D4027373EECD30543096A@exyhmb08.jpn.nsroot.net>
	<836F00680EECD340A96AD34ECFF3B534B4A65D@iu-mssg-mbx106.ads.iu.edu>
	<m23b695fgn.fsf@ziti.local>
Message-ID: <1169082630.4301.18.camel@localhost.localdomain>

On Wed, 2007-01-17 at 16:46 -0800, Seth Falcon wrote:
> "Kimpel, Mark William" <mkimpel at iupui.edu> writes:
> 
> > I have a very long vector of character strings of the format
> > "GO:0008104.ISS" and need to strip off the dot and anything that follows
> > it. There are always 10 characters before the dot. The actual characters
> > and the number of them after the dot is variable.
> >
> > So, I would like to return in the format "GO:0008104" . I could do this
> > with substr and loop over the entire vector, but I thought there might
> > be a more elegant (and faster) way to do this.
> >
> > I have tried gsub using regular expressions without success. The code 
> >
> > gsub(pattern= "\.*?" , replacement="", x=character.vector)
> 
> I guess you want:
> 
>     sub("([GO:0-9]+)\\..*$", "\\1", goids)
> 
> [You don't need gsub here]
> 
> But I don't understand why you wouldn't want to use substr.  At least
> for me substr looks to be about 20x faster than sub for this
> problem...
> 
> 
>   > library(GO)
>   > goids = ls(GOTERM)
>   > gids = paste(goids, "ISS", sep=".")
>   > gids[1:10]
>    [1] "GO:0000001.ISS" "GO:0000002.ISS" "GO:0000003.ISS" "GO:0000004.ISS"
>    [5] "GO:0000006.ISS" "GO:0000007.ISS" "GO:0000009.ISS" "GO:0000010.ISS"
>    [9] "GO:0000011.ISS" "GO:0000012.ISS"
>   
>   > system.time(z <- substr(gids, 0, 10))
>      user  system elapsed 
>     0.008   0.000   0.007 
>   > system.time(z2 <- sub("([GO:0-9]+)\\..*$", "\\1", gids))
>      user  system elapsed 
>     0.136   0.000   0.134 

I think that some of the overhead here in using sub() is due to the
effective partitioning of the source vector, a more complex regex and
then just returning the first element.

This can be shortened to:

# Note that I have 12 elements here
> gids
 [1] "GO:0000001.ISS" "GO:0000002.ISS" "GO:0000003.ISS" "GO:0000004.ISS"
 [5] "GO:0000005.ISS" "GO:0000006.ISS" "GO:0000007.ISS" "GO:0000008.ISS"
 [9] "GO:0000009.ISS" "GO:0000010.ISS" "GO:0000011.ISS" "GO:0000012.ISS"

> system.time(z2 <- sub("\\..+", "", gids))
[1] 0 0 0 0 0

> z2
 [1] "GO:0000001" "GO:0000002" "GO:0000003" "GO:0000004" "GO:0000005"
 [6] "GO:0000006" "GO:0000007" "GO:0000008" "GO:0000009" "GO:0000010"
[11] "GO:0000011" "GO:0000012"


Which would appear to be quicker than using substr().

HTH,

Marc Schwartz


From jholtman at gmail.com  Thu Jan 18 02:27:41 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 17 Jan 2007 20:27:41 -0500
Subject: [R] Fast Removing Duplicates from Every Column
In-Reply-To: <20070117223737.929B52200AD@assei1bl6.telenet-ops.be>
References: <20070117223737.929B52200AD@assei1bl6.telenet-ops.be>
Message-ID: <644e1f320701171727k65e1c5c5te651679f7989523f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070117/6237028b/attachment.pl 

From hb at stat.berkeley.edu  Thu Jan 18 02:43:24 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Thu, 18 Jan 2007 12:43:24 +1100
Subject: [R] R.oo Destructors
In-Reply-To: <0143A263BEF94644AC0D4027373EECD30543096A@exyhmb08.jpn.nsroot.net>
References: <0143A263BEF94644AC0D4027373EECD30543096A@exyhmb08.jpn.nsroot.net>
Message-ID: <59d7961d0701171743v4b408f5n84d5e588885c3acf@mail.gmail.com>

Hi,

I'm about the head out of the office next 48 hours, but the short
answer is to override the finalize() method in your subclass of
Object.  This will be called when R garbage collects the object.  From
?finalize.Object:

 setConstructorS3("MyClass", function() {
    extend(Object(), "MyClass")
  })

  setMethodS3("finalize", "MyClass", function(this) {
    cat(as.character(this), "is about to be removed from the memory!\n")
  })

  o <- MyClass()
  o <- MyClass()
  o <- MyClass()
  o <- MyClass()
  gc()

  ## Not run:
MyClass: 0x31543776 is about to be removed from the memory!
MyClass: 0x31772572 is about to be removed from the memory!
MyClass: 0x32553244 is about to be removed from the memory!
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 246049  6.6     407500 10.9   350000  9.4
Vcells 132538  1.1     786432  6.0   404358  3.1

rm(o)
gc()

MyClass: 0x31424196 is about to be removed from the memory!
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 246782  6.6     467875 12.5   354145  9.5
Vcells 110362  0.9     786432  6.0   404358  3.1

Hope this helps

Henrik

On 1/18/07, Feng, Ken [CIB-EQTY] <ken.feng at citigroup.com> wrote:
>
> Has anyone figured out how to create a destructor in R.oo?
>
> How I'd like to use it:  I have an object which opens a connection thru RODBC
> (held as a private member) It would be nice if the connection closes automatically
> (inside the destructor) when an object gets gc()'ed.
>
> Thanks in advance.
>
> Regards,
> Ken
> BTW, a >BIG< thanks to Henrik Bengtsson for creating the R.oo package!
> Lucky for me as I am not smart enough to use S4...
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mkimpel at iupui.edu  Thu Jan 18 02:44:10 2007
From: mkimpel at iupui.edu (Kimpel, Mark William)
Date: Wed, 17 Jan 2007 20:44:10 -0500
Subject: [R] help with regexpr in gsub
In-Reply-To: <1169082630.4301.18.camel@localhost.localdomain>
References: <0143A263BEF94644AC0D4027373EECD30543096A@exyhmb08.jpn.nsroot.net>
	<836F00680EECD340A96AD34ECFF3B534B4A65D@iu-mssg-mbx106.ads.iu.edu>
	<m23b695fgn.fsf@ziti.local>
	<1169082630.4301.18.camel@localhost.localdomain>
Message-ID: <836F00680EECD340A96AD34ECFF3B534B4A661@iu-mssg-mbx106.ads.iu.edu>

Thanks for 6 ways to skin this cat! I am just beginning to learn about
the power of regular expressions and appreciate the many examples of how
they can be used in this context. This knowledge will come in handy the
next time the number of characters is variable both before and after the
dot. On my machine and for my particular example, however, Seth is
correct in that substr is by far the fastest. I had forgotten that
substr is vectorized.

Below is the output of my speed trials and sessionInfo in case anyone is
curious. I artificially made the go.id vector 10X its normal length to
magnify differences. I did also check to verify that each solution
worked as predicted, which they all did.

Thanks again for your generous help, Mark

length(go.ids)
[1] 79750
>     go.ids[1:5]
[1] "GO:0006091.NA"  "GO:0008104.ISS" "GO:0008104.ISS" "GO:0006091.NA"
"GO:0006091.NAS"
>     system.time(z <- gsub("[.].*", "", go.ids))
[1] 0.47 0.00 0.47   NA   NA
>     system.time(z <- gsub('\\..+$','', go.ids))
[1] 0.56 0.00 0.56   NA   NA
>     system.time(z <- gsub('([^.]+)\\..*','\\1',go.ids))
[1] 1.08 0.00 1.09   NA   NA
>     system.time(z <- sub("([GO:0-9]+)\\..*$", "\\1", go.ids))
[1] 1.03 0.00 1.03   NA   NA
>     system.time(z <- sub("\\..+", "", go.ids))
[1] 0.49 0.00 0.48   NA   NA
>     system.time(z <- substr(go.ids, 0, 10))
[1] 0.02 0.00 0.01   NA   NA
> sessionInfo()
R version 2.4.1 (2006-12-18) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] "splines"   "stats"     "graphics"  "grDevices" "datasets"  "utils"
"tools"     "methods"   "base"     

other attached packages:
        rat2302 xlsReadWritePro          qvalue   affycoretools
biomaRt           RCurl             XML         GOstats        Category 
       "1.14.0"         "1.0.6"         "1.8.0"         "1.6.0"
"1.8.1"         "0.8-0"         "1.2-0"         "2.0.4"         "2.0.3" 
     genefilter        survival            KEGG            RBGL
annotate              GO           graph         RWinEdt           limma

       "1.12.0"          "2.30"        "1.14.1"        "1.10.0"
"1.12.1"        "1.14.1"        "1.12.0"         "1.7-5"         "2.9.1"

           affy          affyio         Biobase 
       "1.12.2"         "1.2.0"        "1.12.2"

Mark W. Kimpel MD 

 

(317) 490-5129 Work, & Mobile

 

(317) 663-0513 Home (no voice mail please)

1-(317)-536-2730 FAX


-----Original Message-----
From: Marc Schwartz [mailto:marc_schwartz at comcast.net] 
Sent: Wednesday, January 17, 2007 8:11 PM
To: Seth Falcon
Cc: Kimpel, Mark William; r-help at stat.math.ethz.ch
Subject: Re: [R] help with regexpr in gsub

On Wed, 2007-01-17 at 16:46 -0800, Seth Falcon wrote:
> "Kimpel, Mark William" <mkimpel at iupui.edu> writes:
> 
> > I have a very long vector of character strings of the format
> > "GO:0008104.ISS" and need to strip off the dot and anything that
follows
> > it. There are always 10 characters before the dot. The actual
characters
> > and the number of them after the dot is variable.
> >
> > So, I would like to return in the format "GO:0008104" . I could do
this
> > with substr and loop over the entire vector, but I thought there
might
> > be a more elegant (and faster) way to do this.
> >
> > I have tried gsub using regular expressions without success. The
code 
> >
> > gsub(pattern= "\.*?" , replacement="", x=character.vector)
> 
> I guess you want:
> 
>     sub("([GO:0-9]+)\\..*$", "\\1", goids)
> 
> [You don't need gsub here]
> 
> But I don't understand why you wouldn't want to use substr.  At least
> for me substr looks to be about 20x faster than sub for this
> problem...
> 
> 
>   > library(GO)
>   > goids = ls(GOTERM)
>   > gids = paste(goids, "ISS", sep=".")
>   > gids[1:10]
>    [1] "GO:0000001.ISS" "GO:0000002.ISS" "GO:0000003.ISS"
"GO:0000004.ISS"
>    [5] "GO:0000006.ISS" "GO:0000007.ISS" "GO:0000009.ISS"
"GO:0000010.ISS"
>    [9] "GO:0000011.ISS" "GO:0000012.ISS"
>   
>   > system.time(z <- substr(gids, 0, 10))
>      user  system elapsed 
>     0.008   0.000   0.007 
>   > system.time(z2 <- sub("([GO:0-9]+)\\..*$", "\\1", gids))
>      user  system elapsed 
>     0.136   0.000   0.134 

I think that some of the overhead here in using sub() is due to the
effective partitioning of the source vector, a more complex regex and
then just returning the first element.

This can be shortened to:

# Note that I have 12 elements here
> gids
 [1] "GO:0000001.ISS" "GO:0000002.ISS" "GO:0000003.ISS" "GO:0000004.ISS"
 [5] "GO:0000005.ISS" "GO:0000006.ISS" "GO:0000007.ISS" "GO:0000008.ISS"
 [9] "GO:0000009.ISS" "GO:0000010.ISS" "GO:0000011.ISS" "GO:0000012.ISS"

> system.time(z2 <- sub("\\..+", "", gids))
[1] 0 0 0 0 0

> z2
 [1] "GO:0000001" "GO:0000002" "GO:0000003" "GO:0000004" "GO:0000005"
 [6] "GO:0000006" "GO:0000007" "GO:0000008" "GO:0000009" "GO:0000010"
[11] "GO:0000011" "GO:0000012"


Which would appear to be quicker than using substr().

HTH,

Marc Schwartz


From jholtman at gmail.com  Thu Jan 18 02:52:51 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 17 Jan 2007 20:52:51 -0500
Subject: [R] Memory leak with character arrays?
In-Reply-To: <45AE9B17.1080205@cs.nyu.edu>
References: <45AE9B17.1080205@cs.nyu.edu>
Message-ID: <644e1f320701171752oe367aai4ca6300e30d3e8a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070117/3a3d955c/attachment.pl 

From waltman at cs.nyu.edu  Thu Jan 18 04:23:38 2007
From: waltman at cs.nyu.edu (Peter Waltman)
Date: Wed, 17 Jan 2007 22:23:38 -0500
Subject: [R] Memory leak with character arrays?
In-Reply-To: <644e1f320701171752oe367aai4ca6300e30d3e8a@mail.gmail.com>
References: <45AE9B17.1080205@cs.nyu.edu>
	<644e1f320701171752oe367aai4ca6300e30d3e8a@mail.gmail.com>
Message-ID: <45AEE83A.9030806@cs.nyu.edu>


From ripley at stats.ox.ac.uk  Thu Jan 18 05:18:10 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Jan 2007 04:18:10 +0000 (GMT)
Subject: [R] Coefficient of determination when intercept is zero
In-Reply-To: <8420811.post@talk.nabble.com>
References: <8420811.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.64.0701180415001.8716@gannet.stats.ox.ac.uk>

This is documented on ?summary.lm.

It is not that 'intercept is zero' or 'zero intercept', it is that there 
is no intercept term in the model.

On Wed, 17 Jan 2007, endeitz wrote:

>
> I am curious as to the "lm" calculation of R2 (multiple coefficient of
> determination, I assume) when intercept is zero.  I have 18 data points, two
> independent variables:
>
> First, a model with an intercept:
>
>> mod0=lm(Div~Rain+Evap,data=test)
>> summary(mod0)$r.squared
> [1] 0.6257541
>> cor(predict(mod0),test$Div)^2
> [1] 0.6257541
>
> The $r.squared and the result from "cor" are the same, as I would expect.
>
> Now we try a model with zero intercept:
>
>> mod1=lm(Div~0+Rain+Evap,data=test)
>> summary(mod1)$r.squared
> [1] 0.9099358
>> cor(predict(mod1),test$Div)^2
> [1] 0.5813659
>
> Why has the $r.squared value increased to 0.9?  And now the result from
> "cor" is not the same?  Is there a special way to calculate the coefficient
> of determination when the intercept is zero?
>
> Cheers,
>
> Ed.
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From hellokisas at gmail.com  Thu Jan 18 05:20:38 2007
From: hellokisas at gmail.com (Feng Qiu)
Date: Wed, 17 Jan 2007 23:20:38 -0500
Subject: [R]  how to get the index of entry with max value in an array?
Message-ID: <00ba01c73ab8$02919fc0$6400a8c0@Aglog>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070117/87fcd9f5/attachment.pl 

From ripley at stats.ox.ac.uk  Thu Jan 18 05:49:07 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Jan 2007 04:49:07 +0000 (GMT)
Subject: [R] help with regexpr in gsub
In-Reply-To: <836F00680EECD340A96AD34ECFF3B534B4A661@iu-mssg-mbx106.ads.iu.edu>
References: <0143A263BEF94644AC0D4027373EECD30543096A@exyhmb08.jpn.nsroot.net>
	<836F00680EECD340A96AD34ECFF3B534B4A65D@iu-mssg-mbx106.ads.iu.edu>
	<m23b695fgn.fsf@ziti.local>
	<1169082630.4301.18.camel@localhost.localdomain>
	<836F00680EECD340A96AD34ECFF3B534B4A661@iu-mssg-mbx106.ads.iu.edu>
Message-ID: <Pine.LNX.4.64.0701180425440.8820@gannet.stats.ox.ac.uk>

One thing to watch with experiments like this is that the locale will 
matter.  Character operations will be faster in a single-byte locale (as 
used here) than in a variable-byte locale (and I suspect Seth and Marc 
used UTF-8), and the relative speeds may alter.  Also, the PCRE regexps 
are often much faster, and 'useBytes' can be much faster with ASCII data 
in UTF-8.

For example:

# R-devel, x86_64 Linux
library(GO)
goids <- ls(GOTERM)
gids <- paste(goids, "ISS", sep=".")
go.ids <- rep(gids, 10)
> length(go.ids)
[1] 205950

# In en_GB (single byte)

> system.time(z <- gsub("[.].*", "", go.ids))
    user  system elapsed
   1.709   0.004   1.716
> system.time(z <- gsub("[.].*", "", go.ids, perl=TRUE))
    user  system elapsed
   0.241   0.004   0.246

> system.time(z <- gsub('\\..+$','', go.ids))
    user  system elapsed
   2.254   0.018   2.286
> system.time(z <- gsub('([^.]+)\\..*','\\1',go.ids))
    user  system elapsed
   2.890   0.002   2.895
> system.time(z <- sub("([GO:0-9]+)\\..*$", "\\1", go.ids))
    user  system elapsed
   2.716   0.002   2.721
> system.time(z <- sub("\\..+", "", go.ids))
    user  system elapsed
   1.724   0.001   1.725
> system.time(z <- substr(go.ids, 0, 10))
    user  system elapsed
   0.084   0.000   0.084

# in en_GB.utf8

> system.time(z <- gsub("[.].*", "", go.ids))
    user  system elapsed
   1.689   0.020   1.712
> system.time(z <- gsub("[.].*", "", go.ids, perl=TRUE))
    user  system elapsed
   0.718   0.017   0.736
> system.time(z <- gsub("[.].*", "", go.ids, perl=TRUE, useByte=TRUE))
    user  system elapsed
   0.243   0.001   0.244

> system.time(z <- gsub('\\..+$','', go.ids))
    user  system elapsed
   2.509   0.024   2.537
> system.time(z <- gsub('([^.]+)\\..*','\\1',go.ids))
    user  system elapsed
   3.772   0.004   3.779
> system.time(z <- sub("([GO:0-9]+)\\..*$", "\\1", go.ids))
    user  system elapsed
   4.088   0.007   4.099
> system.time(z <- sub("\\..+", "", go.ids))
    user  system elapsed
   1.920   0.004   1.927
> system.time(z <- substr(go.ids, 0, 10))
    user  system elapsed
   0.096   0.002   0.098

substr still wins, but by a much smaller margin.


On Wed, 17 Jan 2007, Kimpel, Mark William wrote:

> Thanks for 6 ways to skin this cat! I am just beginning to learn about
> the power of regular expressions and appreciate the many examples of how
> they can be used in this context. This knowledge will come in handy the
> next time the number of characters is variable both before and after the
> dot. On my machine and for my particular example, however, Seth is
> correct in that substr is by far the fastest. I had forgotten that
> substr is vectorized.
>
> Below is the output of my speed trials and sessionInfo in case anyone is
> curious. I artificially made the go.id vector 10X its normal length to
> magnify differences. I did also check to verify that each solution
> worked as predicted, which they all did.
>
> Thanks again for your generous help, Mark
>
> length(go.ids)
> [1] 79750
>>     go.ids[1:5]
> [1] "GO:0006091.NA"  "GO:0008104.ISS" "GO:0008104.ISS" "GO:0006091.NA"
> "GO:0006091.NAS"
>>     system.time(z <- gsub("[.].*", "", go.ids))
> [1] 0.47 0.00 0.47   NA   NA
>>     system.time(z <- gsub('\\..+$','', go.ids))
> [1] 0.56 0.00 0.56   NA   NA
>>     system.time(z <- gsub('([^.]+)\\..*','\\1',go.ids))
> [1] 1.08 0.00 1.09   NA   NA
>>     system.time(z <- sub("([GO:0-9]+)\\..*$", "\\1", go.ids))
> [1] 1.03 0.00 1.03   NA   NA
>>     system.time(z <- sub("\\..+", "", go.ids))
> [1] 0.49 0.00 0.48   NA   NA
>>     system.time(z <- substr(go.ids, 0, 10))
> [1] 0.02 0.00 0.01   NA   NA
>> sessionInfo()
> R version 2.4.1 (2006-12-18)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>
> attached base packages:
> [1] "splines"   "stats"     "graphics"  "grDevices" "datasets"  "utils"
> "tools"     "methods"   "base"
>
> other attached packages:
>        rat2302 xlsReadWritePro          qvalue   affycoretools
> biomaRt           RCurl             XML         GOstats        Category
>       "1.14.0"         "1.0.6"         "1.8.0"         "1.6.0"
> "1.8.1"         "0.8-0"         "1.2-0"         "2.0.4"         "2.0.3"
>     genefilter        survival            KEGG            RBGL
> annotate              GO           graph         RWinEdt           limma
>
>       "1.12.0"          "2.30"        "1.14.1"        "1.10.0"
> "1.12.1"        "1.14.1"        "1.12.0"         "1.7-5"         "2.9.1"
>
>           affy          affyio         Biobase
>       "1.12.2"         "1.2.0"        "1.12.2"
>
> Mark W. Kimpel MD
>
>
>
> (317) 490-5129 Work, & Mobile
>
>
>
> (317) 663-0513 Home (no voice mail please)
>
> 1-(317)-536-2730 FAX
>
>
> -----Original Message-----
> From: Marc Schwartz [mailto:marc_schwartz at comcast.net]
> Sent: Wednesday, January 17, 2007 8:11 PM
> To: Seth Falcon
> Cc: Kimpel, Mark William; r-help at stat.math.ethz.ch
> Subject: Re: [R] help with regexpr in gsub
>
> On Wed, 2007-01-17 at 16:46 -0800, Seth Falcon wrote:
>> "Kimpel, Mark William" <mkimpel at iupui.edu> writes:
>>
>>> I have a very long vector of character strings of the format
>>> "GO:0008104.ISS" and need to strip off the dot and anything that
> follows
>>> it. There are always 10 characters before the dot. The actual
> characters
>>> and the number of them after the dot is variable.
>>>
>>> So, I would like to return in the format "GO:0008104" . I could do
> this
>>> with substr and loop over the entire vector, but I thought there
> might
>>> be a more elegant (and faster) way to do this.
>>>
>>> I have tried gsub using regular expressions without success. The
> code
>>>
>>> gsub(pattern= "\.*?" , replacement="", x=character.vector)
>>
>> I guess you want:
>>
>>     sub("([GO:0-9]+)\\..*$", "\\1", goids)
>>
>> [You don't need gsub here]
>>
>> But I don't understand why you wouldn't want to use substr.  At least
>> for me substr looks to be about 20x faster than sub for this
>> problem...
>>
>>
>>  > library(GO)
>>  > goids = ls(GOTERM)
>>  > gids = paste(goids, "ISS", sep=".")
>>  > gids[1:10]
>>    [1] "GO:0000001.ISS" "GO:0000002.ISS" "GO:0000003.ISS"
> "GO:0000004.ISS"
>>    [5] "GO:0000006.ISS" "GO:0000007.ISS" "GO:0000009.ISS"
> "GO:0000010.ISS"
>>    [9] "GO:0000011.ISS" "GO:0000012.ISS"
>>
>>  > system.time(z <- substr(gids, 0, 10))
>>      user  system elapsed
>>     0.008   0.000   0.007
>>  > system.time(z2 <- sub("([GO:0-9]+)\\..*$", "\\1", gids))
>>      user  system elapsed
>>     0.136   0.000   0.134
>
> I think that some of the overhead here in using sub() is due to the
> effective partitioning of the source vector, a more complex regex and
> then just returning the first element.
>
> This can be shortened to:
>
> # Note that I have 12 elements here
>> gids
> [1] "GO:0000001.ISS" "GO:0000002.ISS" "GO:0000003.ISS" "GO:0000004.ISS"
> [5] "GO:0000005.ISS" "GO:0000006.ISS" "GO:0000007.ISS" "GO:0000008.ISS"
> [9] "GO:0000009.ISS" "GO:0000010.ISS" "GO:0000011.ISS" "GO:0000012.ISS"
>
>> system.time(z2 <- sub("\\..+", "", gids))
> [1] 0 0 0 0 0
>
>> z2
> [1] "GO:0000001" "GO:0000002" "GO:0000003" "GO:0000004" "GO:0000005"
> [6] "GO:0000006" "GO:0000007" "GO:0000008" "GO:0000009" "GO:0000010"
> [11] "GO:0000011" "GO:0000012"
>
>
> Which would appear to be quicker than using substr().
>
> HTH,
>
> Marc Schwartz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Thu Jan 18 05:50:24 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Jan 2007 04:50:24 +0000 (GMT)
Subject: [R] how to get the index of entry with max value in an array?
In-Reply-To: <00ba01c73ab8$02919fc0$6400a8c0@Aglog>
References: <00ba01c73ab8$02919fc0$6400a8c0@Aglog>
Message-ID: <Pine.LNX.4.64.0701180450040.8820@gannet.stats.ox.ac.uk>

?which.max
?which.is.min

On Wed, 17 Jan 2007, Feng Qiu wrote:

> Hi all:
>         A short question:
>         For example,  a=[3,4,6,2,3], obviously the 3rd entry of the array has the maxium value, what I want is index of the maxium value: 3. is there a neat expression to get this index?
>
> Thank you!
>
> Best,
>
> Feng
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From christos at nuverabio.com  Thu Jan 18 05:52:01 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Wed, 17 Jan 2007 23:52:01 -0500
Subject: [R] how to get the index of entry with max value in an array?
In-Reply-To: <3948d9e50701172045l17015007sf47ed0dcd20de08@mail.gmail.com>
References: <00ba01c73ab8$02919fc0$6400a8c0@Aglog>
	<3948d9e50701172045l17015007sf47ed0dcd20de08@mail.gmail.com>
Message-ID: <001301c73abc$64bdee20$0202a8c0@headquarters.silicoinsights>

Or
which.max(a)

-Christos 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of talepanda
Sent: Wednesday, January 17, 2007 11:45 PM
To: Feng Qiu
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] how to get the index of entry with max value in an array?

In R language, one solution is:

a<-c(3,4,6,2,3)
which(a==max(a))


On 1/18/07, Feng Qiu <hellokisas at gmail.com> wrote:
> Hi all:
>          A short question:
>          For example,  a=[3,4,6,2,3], obviously the 3rd entry of the 
> array has the maxium value, what I want is index of the maxium value: 
> 3. is there a neat expression to get this index?
>
> Thank you!
>
> Best,
>
> Feng
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jholtman at gmail.com  Thu Jan 18 05:51:08 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 17 Jan 2007 23:51:08 -0500
Subject: [R] how to get the index of entry with max value in an array?
In-Reply-To: <00ba01c73ab8$02919fc0$6400a8c0@Aglog>
References: <00ba01c73ab8$02919fc0$6400a8c0@Aglog>
Message-ID: <644e1f320701172051o7215cba0raa8a6c402b750ce7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070117/e3cb825a/attachment.pl 

From mkimpel at iupui.edu  Thu Jan 18 06:10:05 2007
From: mkimpel at iupui.edu (Kimpel, Mark William)
Date: Thu, 18 Jan 2007 00:10:05 -0500
Subject: [R] help with regexpr in gsub
In-Reply-To: <Pine.LNX.4.64.0701180425440.8820@gannet.stats.ox.ac.uk>
References: <0143A263BEF94644AC0D4027373EECD30543096A@exyhmb08.jpn.nsroot.net>
	<836F00680EECD340A96AD34ECFF3B534B4A65D@iu-mssg-mbx106.ads.iu.edu>
	<m23b695fgn.fsf@ziti.local>
	<1169082630.4301.18.camel@localhost.localdomain>
	<836F00680EECD340A96AD34ECFF3B534B4A661@iu-mssg-mbx106.ads.iu.edu>
	<Pine.LNX.4.64.0701180425440.8820@gannet.stats.ox.ac.uk>
Message-ID: <836F00680EECD340A96AD34ECFF3B534B4A663@iu-mssg-mbx106.ads.iu.edu>

Thanks Brian, that advice may help speed up my regexp operations in the
future. The computer science advice offered by those of you who are more
expert is appreciated by we biologists who are primarily working more at
the level of bioinformatics. Mark

Mark W. Kimpel MD 

 

(317) 490-5129 Work, & Mobile

 

(317) 663-0513 Home (no voice mail please)

1-(317)-536-2730 FAX


-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Wednesday, January 17, 2007 11:49 PM
To: Kimpel, Mark William
Cc: marc_schwartz at comcast.net; Seth Falcon; r-help at stat.math.ethz.ch
Subject: Re: [R] help with regexpr in gsub

One thing to watch with experiments like this is that the locale will 
matter.  Character operations will be faster in a single-byte locale (as

used here) than in a variable-byte locale (and I suspect Seth and Marc 
used UTF-8), and the relative speeds may alter.  Also, the PCRE regexps 
are often much faster, and 'useBytes' can be much faster with ASCII data

in UTF-8.

For example:

# R-devel, x86_64 Linux
library(GO)
goids <- ls(GOTERM)
gids <- paste(goids, "ISS", sep=".")
go.ids <- rep(gids, 10)
> length(go.ids)
[1] 205950

# In en_GB (single byte)

> system.time(z <- gsub("[.].*", "", go.ids))
    user  system elapsed
   1.709   0.004   1.716
> system.time(z <- gsub("[.].*", "", go.ids, perl=TRUE))
    user  system elapsed
   0.241   0.004   0.246

> system.time(z <- gsub('\\..+$','', go.ids))
    user  system elapsed
   2.254   0.018   2.286
> system.time(z <- gsub('([^.]+)\\..*','\\1',go.ids))
    user  system elapsed
   2.890   0.002   2.895
> system.time(z <- sub("([GO:0-9]+)\\..*$", "\\1", go.ids))
    user  system elapsed
   2.716   0.002   2.721
> system.time(z <- sub("\\..+", "", go.ids))
    user  system elapsed
   1.724   0.001   1.725
> system.time(z <- substr(go.ids, 0, 10))
    user  system elapsed
   0.084   0.000   0.084

# in en_GB.utf8

> system.time(z <- gsub("[.].*", "", go.ids))
    user  system elapsed
   1.689   0.020   1.712
> system.time(z <- gsub("[.].*", "", go.ids, perl=TRUE))
    user  system elapsed
   0.718   0.017   0.736
> system.time(z <- gsub("[.].*", "", go.ids, perl=TRUE, useByte=TRUE))
    user  system elapsed
   0.243   0.001   0.244

> system.time(z <- gsub('\\..+$','', go.ids))
    user  system elapsed
   2.509   0.024   2.537
> system.time(z <- gsub('([^.]+)\\..*','\\1',go.ids))
    user  system elapsed
   3.772   0.004   3.779
> system.time(z <- sub("([GO:0-9]+)\\..*$", "\\1", go.ids))
    user  system elapsed
   4.088   0.007   4.099
> system.time(z <- sub("\\..+", "", go.ids))
    user  system elapsed
   1.920   0.004   1.927
> system.time(z <- substr(go.ids, 0, 10))
    user  system elapsed
   0.096   0.002   0.098

substr still wins, but by a much smaller margin.


On Wed, 17 Jan 2007, Kimpel, Mark William wrote:

> Thanks for 6 ways to skin this cat! I am just beginning to learn about
> the power of regular expressions and appreciate the many examples of
how
> they can be used in this context. This knowledge will come in handy
the
> next time the number of characters is variable both before and after
the
> dot. On my machine and for my particular example, however, Seth is
> correct in that substr is by far the fastest. I had forgotten that
> substr is vectorized.
>
> Below is the output of my speed trials and sessionInfo in case anyone
is
> curious. I artificially made the go.id vector 10X its normal length to
> magnify differences. I did also check to verify that each solution
> worked as predicted, which they all did.
>
> Thanks again for your generous help, Mark
>
> length(go.ids)
> [1] 79750
>>     go.ids[1:5]
> [1] "GO:0006091.NA"  "GO:0008104.ISS" "GO:0008104.ISS" "GO:0006091.NA"
> "GO:0006091.NAS"
>>     system.time(z <- gsub("[.].*", "", go.ids))
> [1] 0.47 0.00 0.47   NA   NA
>>     system.time(z <- gsub('\\..+$','', go.ids))
> [1] 0.56 0.00 0.56   NA   NA
>>     system.time(z <- gsub('([^.]+)\\..*','\\1',go.ids))
> [1] 1.08 0.00 1.09   NA   NA
>>     system.time(z <- sub("([GO:0-9]+)\\..*$", "\\1", go.ids))
> [1] 1.03 0.00 1.03   NA   NA
>>     system.time(z <- sub("\\..+", "", go.ids))
> [1] 0.49 0.00 0.48   NA   NA
>>     system.time(z <- substr(go.ids, 0, 10))
> [1] 0.02 0.00 0.01   NA   NA
>> sessionInfo()
> R version 2.4.1 (2006-12-18)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>
> attached base packages:
> [1] "splines"   "stats"     "graphics"  "grDevices" "datasets"
"utils"
> "tools"     "methods"   "base"
>
> other attached packages:
>        rat2302 xlsReadWritePro          qvalue   affycoretools
> biomaRt           RCurl             XML         GOstats
Category
>       "1.14.0"         "1.0.6"         "1.8.0"         "1.6.0"
> "1.8.1"         "0.8-0"         "1.2-0"         "2.0.4"
"2.0.3"
>     genefilter        survival            KEGG            RBGL
> annotate              GO           graph         RWinEdt
limma
>
>       "1.12.0"          "2.30"        "1.14.1"        "1.10.0"
> "1.12.1"        "1.14.1"        "1.12.0"         "1.7-5"
"2.9.1"
>
>           affy          affyio         Biobase
>       "1.12.2"         "1.2.0"        "1.12.2"
>
> Mark W. Kimpel MD
>
>
>
> (317) 490-5129 Work, & Mobile
>
>
>
> (317) 663-0513 Home (no voice mail please)
>
> 1-(317)-536-2730 FAX
>
>
> -----Original Message-----
> From: Marc Schwartz [mailto:marc_schwartz at comcast.net]
> Sent: Wednesday, January 17, 2007 8:11 PM
> To: Seth Falcon
> Cc: Kimpel, Mark William; r-help at stat.math.ethz.ch
> Subject: Re: [R] help with regexpr in gsub
>
> On Wed, 2007-01-17 at 16:46 -0800, Seth Falcon wrote:
>> "Kimpel, Mark William" <mkimpel at iupui.edu> writes:
>>
>>> I have a very long vector of character strings of the format
>>> "GO:0008104.ISS" and need to strip off the dot and anything that
> follows
>>> it. There are always 10 characters before the dot. The actual
> characters
>>> and the number of them after the dot is variable.
>>>
>>> So, I would like to return in the format "GO:0008104" . I could do
> this
>>> with substr and loop over the entire vector, but I thought there
> might
>>> be a more elegant (and faster) way to do this.
>>>
>>> I have tried gsub using regular expressions without success. The
> code
>>>
>>> gsub(pattern= "\.*?" , replacement="", x=character.vector)
>>
>> I guess you want:
>>
>>     sub("([GO:0-9]+)\\..*$", "\\1", goids)
>>
>> [You don't need gsub here]
>>
>> But I don't understand why you wouldn't want to use substr.  At least
>> for me substr looks to be about 20x faster than sub for this
>> problem...
>>
>>
>>  > library(GO)
>>  > goids = ls(GOTERM)
>>  > gids = paste(goids, "ISS", sep=".")
>>  > gids[1:10]
>>    [1] "GO:0000001.ISS" "GO:0000002.ISS" "GO:0000003.ISS"
> "GO:0000004.ISS"
>>    [5] "GO:0000006.ISS" "GO:0000007.ISS" "GO:0000009.ISS"
> "GO:0000010.ISS"
>>    [9] "GO:0000011.ISS" "GO:0000012.ISS"
>>
>>  > system.time(z <- substr(gids, 0, 10))
>>      user  system elapsed
>>     0.008   0.000   0.007
>>  > system.time(z2 <- sub("([GO:0-9]+)\\..*$", "\\1", gids))
>>      user  system elapsed
>>     0.136   0.000   0.134
>
> I think that some of the overhead here in using sub() is due to the
> effective partitioning of the source vector, a more complex regex and
> then just returning the first element.
>
> This can be shortened to:
>
> # Note that I have 12 elements here
>> gids
> [1] "GO:0000001.ISS" "GO:0000002.ISS" "GO:0000003.ISS"
"GO:0000004.ISS"
> [5] "GO:0000005.ISS" "GO:0000006.ISS" "GO:0000007.ISS"
"GO:0000008.ISS"
> [9] "GO:0000009.ISS" "GO:0000010.ISS" "GO:0000011.ISS"
"GO:0000012.ISS"
>
>> system.time(z2 <- sub("\\..+", "", gids))
> [1] 0 0 0 0 0
>
>> z2
> [1] "GO:0000001" "GO:0000002" "GO:0000003" "GO:0000004" "GO:0000005"
> [6] "GO:0000006" "GO:0000007" "GO:0000008" "GO:0000009" "GO:0000010"
> [11] "GO:0000011" "GO:0000012"
>
>
> Which would appear to be quicker than using substr().
>
> HTH,
>
> Marc Schwartz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bcarvalh at jhsph.edu  Thu Jan 18 06:20:25 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Thu, 18 Jan 2007 00:20:25 -0500
Subject: [R] how to get the index of entry with max value in an array?
In-Reply-To: <00ba01c73ab8$02919fc0$6400a8c0@Aglog>
References: <00ba01c73ab8$02919fc0$6400a8c0@Aglog>
Message-ID: <003825C7-D4F7-4E23-82C3-6635FDC0161B@jhsph.edu>

which.max()
b

On Jan 17, 2007, at 11:20 PM, Feng Qiu wrote:

> Hi all:
>          A short question:
>          For example,  a=[3,4,6,2,3], obviously the 3rd entry of  
> the array has the maxium value, what I want is index of the maxium  
> value: 3. is there a neat expression to get this index?
>
> Thank you!
>
> Best,
>
> Feng


From endeitz at yahoo.com  Thu Jan 18 06:53:12 2007
From: endeitz at yahoo.com (endeitz)
Date: Wed, 17 Jan 2007 21:53:12 -0800 (PST)
Subject: [R] Coefficient of determination when intercept is zero
In-Reply-To: <Pine.LNX.4.64.0701180415001.8716@gannet.stats.ox.ac.uk>
References: <8420811.post@talk.nabble.com>
	<Pine.LNX.4.64.0701180415001.8716@gannet.stats.ox.ac.uk>
Message-ID: <8425379.post@talk.nabble.com>


Thanks for the guidance.  With the help of your explanation, I was able to
find this reference that provides a good explanation of why the definition
may be different in the "no intercept" or "regression through origin" case. 
For future readers of this list, the reference is:

Joseph G. Eisenhauer (2003)
Regression through the Origin
Teaching Statistics 25 (3), 76?80.
doi:10.1111/1467-9639.00136

Cheers,

Ed.



Prof Brian Ripley wrote:
> 
> This is documented on ?summary.lm.
> 
> It is not that 'intercept is zero' or 'zero intercept', it is that there 
> is no intercept term in the model.
> 
> On Wed, 17 Jan 2007, endeitz wrote:
> 
>>
>> I am curious as to the "lm" calculation of R2 (multiple coefficient of
>> determination, I assume) when intercept is zero.  I have 18 data points,
>> two
>> independent variables:
>>
>> First, a model with an intercept:
>>
>>> mod0=lm(Div~Rain+Evap,data=test)
>>> summary(mod0)$r.squared
>> [1] 0.6257541
>>> cor(predict(mod0),test$Div)^2
>> [1] 0.6257541
>>
>> The $r.squared and the result from "cor" are the same, as I would expect.
>>
>> Now we try a model with zero intercept:
>>
>>> mod1=lm(Div~0+Rain+Evap,data=test)
>>> summary(mod1)$r.squared
>> [1] 0.9099358
>>> cor(predict(mod1),test$Div)^2
>> [1] 0.5813659
>>
>> Why has the $r.squared value increased to 0.9?  And now the result from
>> "cor" is not the same?  Is there a special way to calculate the
>> coefficient
>> of determination when the intercept is zero?
>>
>> Cheers,
>>
>> Ed.
>>
>>
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Coefficient-of-determination-when-intercept-is-zero-tf3030776.html#a8425379
Sent from the R help mailing list archive at Nabble.com.


From arun.kumar.saha at gmail.com  Thu Jan 18 08:15:43 2007
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Thu, 18 Jan 2007 12:45:43 +0530
Subject: [R] 3D Plot
Message-ID: <d4c57560701172315t4c509c7cue0c41d450d2cf7f5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070118/bb9f5da4/attachment.pl 

From jsorkin at grecc.umaryland.edu  Thu Jan 18 08:32:59 2007
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Thu, 18 Jan 2007 02:32:59 -0500
Subject: [R] selecting rows for inclusion in lm
Message-ID: <45AEDC2E.A712.00CB.0@grecc.umaryland.edu>

I am having trouble selecting rows of a dataframe that will be included
in a regression. I am trying to select those rows for which the variable
Meno equals PRE. I have used the code below:

difffitPre<-lm(data[,"diff"]~data[,"Age"]+data[,"Race"],data=data[data[,"Meno"]=="PRE",])
summary(difffitPre)

The output from the summary indicates that more than 76 rows are
included in the regression:

Residual standard error: 2.828 on 76 degrees of freedom

where in fact only 22 rows should be included as can be seen from the
following:

print(data[length(data[,"Meno"]=="PRE","Meno"]))
[1] 22

I would appreciate any help in modifying the data= parameter of the lm
so that I include only those subjects for which Meno=PRE.

R 2.3.1
Windows XP

Thanks,
John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC,
University of Maryland School of Medicine Claude D. Pepper OAIC,
University of Maryland Clinical Nutrition Research Unit, and
Baltimore VA Center Stroke of Excellence

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)
jsorkin at grecc.umaryland.edu

Confidentiality Statement:
This email message, including any attachments, is for the so...{{dropped}}


From waltman at cs.nyu.edu  Thu Jan 18 08:36:44 2007
From: waltman at cs.nyu.edu (Peter Waltman)
Date: Thu, 18 Jan 2007 02:36:44 -0500
Subject: [R] Memory leak with character arrays?
In-Reply-To: <644e1f320701171752oe367aai4ca6300e30d3e8a@mail.gmail.com>
References: <45AE9B17.1080205@cs.nyu.edu>
	<644e1f320701171752oe367aai4ca6300e30d3e8a@mail.gmail.com>
Message-ID: <45AF238C.9010108@cs.nyu.edu>


From mothsailor at googlemail.com  Thu Jan 18 09:19:58 2007
From: mothsailor at googlemail.com (David Barron)
Date: Thu, 18 Jan 2007 08:19:58 +0000
Subject: [R] selecting rows for inclusion in lm
In-Reply-To: <45AEDC2E.A712.00CB.0@grecc.umaryland.edu>
References: <45AEDC2E.A712.00CB.0@grecc.umaryland.edu>
Message-ID: <815b70590701180019n72a2e6e5ka646ea613d4f8692@mail.gmail.com>

Why not use the subset option?  Something like:

lm(diff ~ Age + Race, data=data, subset=data$Meno=="PRE")

should do the trick, and be much easier to read!

On 18/01/07, John Sorkin <jsorkin at grecc.umaryland.edu> wrote:
> I am having trouble selecting rows of a dataframe that will be included
> in a regression. I am trying to select those rows for which the variable
> Meno equals PRE. I have used the code below:
>
> difffitPre<-lm(data[,"diff"]~data[,"Age"]+data[,"Race"],data=data[data[,"Meno"]=="PRE",])
> summary(difffitPre)
>
> The output from the summary indicates that more than 76 rows are
> included in the regression:
>
> Residual standard error: 2.828 on 76 degrees of freedom
>
> where in fact only 22 rows should be included as can be seen from the
> following:
>
> print(data[length(data[,"Meno"]=="PRE","Meno"]))
> [1] 22
>
> I would appreciate any help in modifying the data= parameter of the lm
> so that I include only those subjects for which Meno=PRE.
>
> R 2.3.1
> Windows XP
>
> Thanks,
> John
>
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC,
> University of Maryland School of Medicine Claude D. Pepper OAIC,
> University of Maryland Clinical Nutrition Research Unit, and
> Baltimore VA Center Stroke of Excellence
>
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> jsorkin at grecc.umaryland.edu
>
> Confidentiality Statement:
> This email message, including any attachments, is for the so...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From wl at eimb.ru  Thu Jan 18 09:24:09 2007
From: wl at eimb.ru (Vladimir Eremeev)
Date: Thu, 18 Jan 2007 08:24:09 +0000 (UTC)
Subject: [R] 3D Plot
References: <d4c57560701172315t4c509c7cue0c41d450d2cf7f5@mail.gmail.com>
Message-ID: <loom.20070118T091858-207@post.gmane.org>

>

About 3D plots: 
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/6439.html

Many other plot examples:
http://addictedtor.free.fr/graphiques/allgraph.php

I used rgl, it can produce interactive plots, which can be rotated, increased 
and decreased with the mouse.
 
> Hi all R users,
> 
> I want to draw a 3D plot, where the Z-axis will represent the normal
> densities each with zero mean but different volatilities, Y-axis will
> represent the SD [volatilities), and X-axis will represent time at which
> these SD are calculated.
> 
> Can anyone give me any clue? Your help will be highly appreciated.
> 
> Thanks and regards,
> Arun
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help <at> stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From wl at eimb.ru  Thu Jan 18 09:28:16 2007
From: wl at eimb.ru (Vladimir Eremeev)
Date: Thu, 18 Jan 2007 08:28:16 +0000 (UTC)
Subject: [R] about rgl package.
References: <d4c57560701172315t4c509c7cue0c41d450d2cf7f5@mail.gmail.com>
Message-ID: <loom.20070118T092550-925@post.gmane.org>

The statement that it is for windows only in the first link in my previous post 
seems to me obsolete now.
Unfortunately, I don't remember exactly, probably, I have used it in Linux.


From wl at eimb.ru  Thu Jan 18 09:51:54 2007
From: wl at eimb.ru (Vladimir Eremeev)
Date: Thu, 18 Jan 2007 08:51:54 +0000 (UTC)
Subject: [R] selecting rows for inclusion in lm
References: <45AEDC2E.A712.00CB.0@grecc.umaryland.edu>
	<815b70590701180019n72a2e6e5ka646ea613d4f8692@mail.gmail.com>
Message-ID: <loom.20070118T095052-963@post.gmane.org>

> Why not use the subset option?  Something like:
> 
> lm(diff ~ Age + Race, data=data, subset=data$Meno=="PRE")

> should do the trick, and be much easier to read!

"data$" could be omitted, simply 
lm(diff ~ Age + Race, data=data, subset=Meno=="PRE")


From ripley at stats.ox.ac.uk  Thu Jan 18 09:42:23 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Jan 2007 08:42:23 +0000 (GMT)
Subject: [R] curious about dimension of 'apply' output when MARGIN=1
In-Reply-To: <45ADFE3A.7050409@pburns.seanet.com>
References: <45AD43FD.8050100@stat.purdue.edu>
	<971536df0701161616n5d586fedhdbbdd29e3131f8dd@mail.gmail.com>
	<45ADFE3A.7050409@pburns.seanet.com>
Message-ID: <Pine.LNX.4.64.0701171537220.7223@gannet.stats.ox.ac.uk>

On Wed, 17 Jan 2007, Patrick Burns wrote:

> A logical reason for the phenomenon is that
> matrices are stored down their columns. For
> example:
>
> > matrix(1:15,5)
>     [,1] [,2] [,3]
> [1,]    1    6   11
> [2,]    2    7   12
> [3,]    3    8   13
> [4,]    4    9   14
> [5,]    5   10   15
>
> When an 'apply' across rows is done, it will be
> the values corresponding to each of the rows that
> are together.
>
> For matrices, merely transposing the result fixes
> the "problem", but it is considerably more complex
> in higher dimensional arrays.

[I don't think so, using aperm.]

> There could be a spectrum of opinion from:
>
> the original programmer was lazy and didn't adequately
> serve users
>
> to:
>
> the simpler the program the fewer bugs there will be.

Or that the vision of the original designer was not limited to matrices. 
It just so happens that in this example the replacement is a single 
dimension the same size as the single margin used.  That's atypical, and 
normally the result dimension has no connection to the margin.  The design 
is to put the result dimension first, and the first item in the 'seealso' 
list is aperm().

To my mind the only general solutions are to put the result dimension 
first or last.  I would have used last, but using first is slightly more 
efficient for the reason Pat gives.

apply() is for arrays, operating over one or more margins with a function 
returning a 'scalar', vector or array result.  Perhaps any 'lazy'-ness / 
limit of vision is not in handling array results as well as might be 
possible.

> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jgengler at umich.edu  Thu Jan 18 00:13:07 2007
From: jgengler at umich.edu (Justin Gengler)
Date: Wed, 17 Jan 2007 18:13:07 -0500
Subject: [R] label option in 'barplot'?
Message-ID: <200701172313.l0HNDC99015059@hypatia.math.ethz.ch>

Hello all,

When using the 'barplot' function, how would one go about creating 
labels on the top of the bars corresponding to the actual frequency 
values (i.e., the height of the bar).  For histograms, one can use 
the 'LABEL=T' parameter to this effect, but it is not available for 
barplot.  Must one manually create such labels for a barplot (perhaps 
using mtext)?

Thanks.

Justin Gengler


From hellokisas at gmail.com  Thu Jan 18 06:49:17 2007
From: hellokisas at gmail.com (Feng Qiu)
Date: Thu, 18 Jan 2007 00:49:17 -0500
Subject: [R] how to get the index of entry with max value in an array?
References: <00ba01c73ab8$02919fc0$6400a8c0@Aglog>
	<003825C7-D4F7-4E23-82C3-6635FDC0161B@jhsph.edu>
Message-ID: <017a01c73ac4$65657f20$6400a8c0@Aglog>

Thank you guys! I got it.

Best,

Feng

----- Original Message ----- 
From: "Benilton Carvalho" <bcarvalh at jhsph.edu>
To: "Feng Qiu" <hellokisas at gmail.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Thursday, January 18, 2007 12:20 AM
Subject: Re: [R] how to get the index of entry with max value in an array?


> which.max()
> b
> 
> On Jan 17, 2007, at 11:20 PM, Feng Qiu wrote:
> 
>> Hi all:
>>          A short question:
>>          For example,  a=[3,4,6,2,3], obviously the 3rd entry of  
>> the array has the maxium value, what I want is index of the maxium  
>> value: 3. is there a neat expression to get this index?
>>
>> Thank you!
>>
>> Best,
>>
>> Feng


From r.hankin at noc.soton.ac.uk  Thu Jan 18 10:54:04 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 18 Jan 2007 09:54:04 +0000
Subject: [R] curious about dimension of 'apply' output when MARGIN=1
In-Reply-To: <Pine.LNX.4.64.0701171537220.7223@gannet.stats.ox.ac.uk>
References: <45AD43FD.8050100@stat.purdue.edu>
	<971536df0701161616n5d586fedhdbbdd29e3131f8dd@mail.gmail.com>
	<45ADFE3A.7050409@pburns.seanet.com>
	<Pine.LNX.4.64.0701171537220.7223@gannet.stats.ox.ac.uk>
Message-ID: <3C03F1C5-C5DB-4EE5-8AE5-8A441C06773A@soc.soton.ac.uk>


On 18 Jan 2007, at 08:42, Prof Brian Ripley wrote:

> On Wed, 17 Jan 2007, Patrick Burns wrote:
>
>> A logical reason for the phenomenon is that
>> matrices are stored down their columns. For
>> example:
>
[snip]

> Or that the vision of the original designer was not limited to  
> matrices.
> It just so happens that in this example the replacement is a single
> dimension the same size as the single margin used.  That's  
> atypical, and
> normally the result dimension has no connection to the margin.  The  
> design
> is to put the result dimension first, and the first item in the  
> 'seealso'
> list is aperm().
>
> To my mind the only general solutions are to put the result dimension
> first or last.  I would have used last, but using first is slightly  
> more
> efficient for the reason Pat gives.
>


The case Brian Ripley mentions above is indeed atypical,
but it's encountered reasonably often, at least in the world of
high-dimensional magic hypercubes.

The most prominent examples would be sort()  and the identity function.

Although the emphasis is different,
matlab's  "sort" function when called with  two
arguments,  as in "sort(a,i)", sorts along  dimension "i",
leaving the dimensions of the returned array unchanged.

R-and-octave.txt includes the following:



a <- array(1:24,2:4)

....


%Sort is a bit of a problem, due to the behaviour of apply():

sort(a,1)           aperm(apply(a,c(2,3),sort),c(1,2,3))
sort(a,2)           aperm(apply(a,c(1,3),sort),c(2,1,3))
sort(a,3)           aperm(apply(a,c(1,2),sort),c(2,3,1))


It's possible to get round this by defining a little function:
asort <- function(a,i){
   j <- 1:length(dim(a))
   aperm(apply(a,j[-i],sort),append(j[-1],1,i-1))
}

Then R's asort(a,1) will return the same as Octave's sort(a,1).

Function asort() could easily be adapted to take a function name as a  
third argument.




> apply() is for arrays, operating over one or more margins with a  
> function
> returning a 'scalar', vector or array result.  Perhaps any 'lazy'- 
> ness /
> limit of vision is not in handling array results as well as might be
> possible.
>
>> Patrick Burns
>> patrick at burns-stat.com
>> +44 (0)20 8525 0696
>> http://www.burns-stat.com
>> (home of S Poetry and "A Guide for the Unwilling S User")
>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From ggrothendieck at gmail.com  Thu Jan 18 10:56:40 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 18 Jan 2007 04:56:40 -0500
Subject: [R] label option in 'barplot'?
In-Reply-To: <200701172313.l0HNDC99015059@hypatia.math.ethz.ch>
References: <200701172313.l0HNDC99015059@hypatia.math.ethz.ch>
Message-ID: <971536df0701180156l6536b30fr12f5657a8f587bb2@mail.gmail.com>

Try this where you probably only want one of the last two lines:


# population of 5 US states
pop <- state.x77[1:5,1]
bp <- barplot(pop, ylim = range(pop) * c(0, 1.1))
text(bp, pop, pop, adj = c(0.5, -0.5))  # place num above bar
mtext(1, at = bp, text = pop, line = 3)  # place num below label


On 1/17/07, Justin Gengler <jgengler at umich.edu> wrote:
> Hello all,
>
> When using the 'barplot' function, how would one go about creating
> labels on the top of the bars corresponding to the actual frequency
> values (i.e., the height of the bar).  For histograms, one can use
> the 'LABEL=T' parameter to this effect, but it is not available for
> barplot.  Must one manually create such labels for a barplot (perhaps
> using mtext)?
>
> Thanks.
>
> Justin Gengler
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ccleland at optonline.net  Thu Jan 18 11:06:05 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 18 Jan 2007 05:06:05 -0500
Subject: [R] label option in 'barplot'?
In-Reply-To: <200701172313.l0HNDC99015059@hypatia.math.ethz.ch>
References: <200701172313.l0HNDC99015059@hypatia.math.ethz.ch>
Message-ID: <45AF468D.9070905@optonline.net>

Justin Gengler wrote:
> Hello all,
> 
> When using the 'barplot' function, how would one go about creating 
> labels on the top of the bars corresponding to the actual frequency 
> values (i.e., the height of the bar).  For histograms, one can use 
> the 'LABEL=T' parameter to this effect, but it is not available for 
> barplot.  Must one manually create such labels for a barplot (perhaps 
> using mtext)?

  Yes, I believe you would need to add those labels yourself.  Use
text() rather than mtext() to get something similar to what hist() does
when labels=TRUE.  Here is how you could put the labels either just
below or just above the top of the bar:

# Below
X <- barplot(VADeaths, beside=TRUE, ylim=c(0,80))
text(X, VADeaths, labels=VADeaths, pos=1, offset=.5, col="red")

# Above
X <- barplot(VADeaths, beside=TRUE, ylim=c(0,80))
text(X, VADeaths, labels=VADeaths, pos=3, offset=.5)

> Thanks.
> 
> Justin Gengler
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From francogrex at mail.com  Thu Jan 18 11:19:39 2007
From: francogrex at mail.com (francogrex)
Date: Thu, 18 Jan 2007 02:19:39 -0800 (PST)
Subject: [R] How to formulate an analytical gradient?
Message-ID: <8428063.post@talk.nabble.com>


How to formulate an analytical gradient?

Suppose I have the following function/expression:

fr<-function(x){
x1=x[1]
x2=x[2]
x3=x[3]
x4=x[4]
x5=x[5]
z<-((gamma(x1+n)))/((gamma(x1)*factorial(n))*((1+(e/x2))^x1)*((1+(x2/e))^n))
v<-((gamma(x3+n)))/((gamma(x3)*factorial(n))*((1+(e/x4))^x3)*((1+(x4/e))^n))

sum(log( (x5*z)+ ((1-x5)*v) ))
}

These are a mix of two negative binomial distributions, where n and e are
know vectors, and I would like to calculate the maxiumum likelihood
estimates of the parameters x1,x2,x3,x4 and X5
I am relying on numerical gradients but I think if I use an analytical one
it will be more accurate especially when number of parameters is more than
4.

Thanks.
-- 
View this message in context: http://www.nabble.com/How-to-formulate-an-analytical-gradient--tf3033293.html#a8428063
Sent from the R help mailing list archive at Nabble.com.


From info at aghmed.fsnet.co.uk  Thu Jan 18 12:20:00 2007
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Thu, 18 Jan 2007 11:20:00 +0000
Subject: [R] selecting rows for inclusion in lm
In-Reply-To: <815b70590701180019n72a2e6e5ka646ea613d4f8692@mail.gmail.co
 m>
References: <45AEDC2E.A712.00CB.0@grecc.umaryland.edu>
	<815b70590701180019n72a2e6e5ka646ea613d4f8692@mail.gmail.com>
Message-ID: <7.0.0.16.0.20070118111836.019a6d88@aghmed.fsnet.co.uk>

At 08:19 18/01/2007, David Barron wrote:
>Why not use the subset option?  Something like:
>
>lm(diff ~ Age + Race, data=data, subset=data$Meno=="PRE")
>
>should do the trick, and be much easier to read!

And indeed the advice in
 > library(fortunes)
 > fortune("dog")

Firstly, don't call your matrix 'matrix'. Would you call your dog 'dog'?
Anyway, it might clash with the function 'matrix'.
    -- Barry Rowlingson
       R-help (October 2004)

 >
also helps to make life clearer I find


>On 18/01/07, John Sorkin <jsorkin at grecc.umaryland.edu> wrote:
>>I am having trouble selecting rows of a dataframe that will be included
>>in a regression. I am trying to select those rows for which the variable
>>Meno equals PRE. I have used the code below:
>>
>>difffitPre<-lm(data[,"diff"]~data[,"Age"]+data[,"Race"],data=data[data[,"Meno"]=="PRE",])
>>summary(difffitPre)
>>
>>The output from the summary indicates that more than 76 rows are
>>included in the regression:
>>
>>Residual standard error: 2.828 on 76 degrees of freedom
>>
>>where in fact only 22 rows should be included as can be seen from the
>>following:
>>
>>print(data[length(data[,"Meno"]=="PRE","Meno"]))
>>[1] 22
>>
>>I would appreciate any help in modifying the data= parameter of the lm
>>so that I include only those subjects for which Meno=PRE.
>>
>>R 2.3.1
>>Windows XP
>>
>>Thanks,
>>John
>>
>>John Sorkin M.D., Ph.D.
>>Chief, Biostatistics and Informatics
>>Baltimore VA Medical Center GRECC,
>>University of Maryland School of Medicine Claude D. Pepper OAIC,
>>University of Maryland Clinical Nutrition Research Unit, and
>>Baltimore VA Center Stroke of Excellence
>>
>>University of Maryland School of Medicine
>>Division of Gerontology
>>Baltimore VA Medical Center
>>10 North Greene Street
>>GRECC (BT/18/GR)
>>Baltimore, MD 21201-1524
>>
>>(Phone) 410-605-7119
>>(Fax) 410-605-7913 (Please call phone number above prior to faxing)
>>jsorkin at grecc.umaryland.edu
>>
>>Confidentiality Statement:
>>This email message, including any attachments, is for the so...{{dropped}}
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>
>--
>=================================
>David Barron
>Said Business School
>University of Oxford
>Park End Street
>Oxford OX1 1HP
>
>

Michael Dewey
http://www.aghmed.fsnet.co.uk


From b.jacobs at pandora.be  Thu Jan 18 13:21:59 2007
From: b.jacobs at pandora.be (Bert Jacobs)
Date: Thu, 18 Jan 2007 13:21:59 +0100
Subject: [R] Fast Removing Duplicates from Every Column
In-Reply-To: AAAAAOXvfE/tUWVMujI1J0cWacuk3SYA
Message-ID: <20070118122159.4E40B2300B0@adicia.telenet-ops.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070118/016a5a6a/attachment.pl 

From yanniggoude at yahoo.fr  Thu Jan 18 13:56:44 2007
From: yanniggoude at yahoo.fr (yannig goude)
Date: Thu, 18 Jan 2007 13:56:44 +0100 (CET)
Subject: [R] arima function
Message-ID: <96839.43735.qm@web26714.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070118/edcc958f/attachment.pl 

From lobry at biomserv.univ-lyon1.fr  Thu Jan 18 14:00:17 2007
From: lobry at biomserv.univ-lyon1.fr (Jean lobry)
Date: Thu, 18 Jan 2007 14:00:17 +0100
Subject: [R] Memory leak with character arrays?
In-Reply-To: <mailman.7.1169118004.8962.r-help@stat.math.ethz.ch>
References: <mailman.7.1169118004.8962.r-help@stat.math.ethz.ch>
Message-ID: <p0600202bc1d51bc89f73@[192.168.1.11]>

Dear Peter,

>The file that I'm reading contains the upstream regions of the yeast
>genome, with each upstream region labeled using a FASTA header, i.e.:
>
>     FASTA header for gene 1
>     upstream region.....
>     .....
>     ....
>     FASTA header for gene 2
>     upstream....
>     ....

you may want to have a look at the read.fasta() function in the
seqinr package. There is an example page 16 of this document:
http://pbil.univ-lyon1.fr/software/SeqinR/seqinr_1_0-6.pdf
about importing the content of a fasta file with 21,161 sequences
from Arabidopsis thaliana into an object which is about 15 Mb in RAM.

HTH,

-- 
Jean R. Lobry            (lobry at biomserv.univ-lyon1.fr)
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - LYON I,
43 Bd 11/11/1918, F-69622 VILLEURBANNE CEDEX, FRANCE
allo  : +33 472 43 27 56     fax    : +33 472 43 13 88
http://pbil.univ-lyon1.fr/members/lobry/


From marc_schwartz at comcast.net  Thu Jan 18 14:04:15 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 18 Jan 2007 07:04:15 -0600
Subject: [R] help with regexpr in gsub
In-Reply-To: <Pine.LNX.4.64.0701180425440.8820@gannet.stats.ox.ac.uk>
References: <0143A263BEF94644AC0D4027373EECD30543096A@exyhmb08.jpn.nsroot.net>
	<836F00680EECD340A96AD34ECFF3B534B4A65D@iu-mssg-mbx106.ads.iu.edu>
	<m23b695fgn.fsf@ziti.local>
	<1169082630.4301.18.camel@localhost.localdomain>
	<836F00680EECD340A96AD34ECFF3B534B4A661@iu-mssg-mbx106.ads.iu.edu>
	<Pine.LNX.4.64.0701180425440.8820@gannet.stats.ox.ac.uk>
Message-ID: <1169125455.4301.31.camel@localhost.localdomain>

On Thu, 2007-01-18 at 04:49 +0000, Prof Brian Ripley wrote:
> One thing to watch with experiments like this is that the locale will 
> matter.  Character operations will be faster in a single-byte locale (as 
> used here) than in a variable-byte locale (and I suspect Seth and Marc 
> used UTF-8), and the relative speeds may alter.  Also, the PCRE regexps 
> are often much faster, and 'useBytes' can be much faster with ASCII data 
> in UTF-8.
> 
> For example:
> 
> # R-devel, x86_64 Linux
> library(GO)
> goids <- ls(GOTERM)
> gids <- paste(goids, "ISS", sep=".")
> go.ids <- rep(gids, 10)
> > length(go.ids)
> [1] 205950
> 
> # In en_GB (single byte)
> 
> > system.time(z <- gsub("[.].*", "", go.ids))
>     user  system elapsed
>    1.709   0.004   1.716
> > system.time(z <- gsub("[.].*", "", go.ids, perl=TRUE))
>     user  system elapsed
>    0.241   0.004   0.246
> 
> > system.time(z <- gsub('\\..+$','', go.ids))
>     user  system elapsed
>    2.254   0.018   2.286
> > system.time(z <- gsub('([^.]+)\\..*','\\1',go.ids))
>     user  system elapsed
>    2.890   0.002   2.895
> > system.time(z <- sub("([GO:0-9]+)\\..*$", "\\1", go.ids))
>     user  system elapsed
>    2.716   0.002   2.721
> > system.time(z <- sub("\\..+", "", go.ids))
>     user  system elapsed
>    1.724   0.001   1.725
> > system.time(z <- substr(go.ids, 0, 10))
>     user  system elapsed
>    0.084   0.000   0.084
> 
> # in en_GB.utf8
> 
> > system.time(z <- gsub("[.].*", "", go.ids))
>     user  system elapsed
>    1.689   0.020   1.712
> > system.time(z <- gsub("[.].*", "", go.ids, perl=TRUE))
>     user  system elapsed
>    0.718   0.017   0.736
> > system.time(z <- gsub("[.].*", "", go.ids, perl=TRUE, useByte=TRUE))
>     user  system elapsed
>    0.243   0.001   0.244
> 
> > system.time(z <- gsub('\\..+$','', go.ids))
>     user  system elapsed
>    2.509   0.024   2.537
> > system.time(z <- gsub('([^.]+)\\..*','\\1',go.ids))
>     user  system elapsed
>    3.772   0.004   3.779
> > system.time(z <- sub("([GO:0-9]+)\\..*$", "\\1", go.ids))
>     user  system elapsed
>    4.088   0.007   4.099
> > system.time(z <- sub("\\..+", "", go.ids))
>     user  system elapsed
>    1.920   0.004   1.927
> > system.time(z <- substr(go.ids, 0, 10))
>     user  system elapsed
>    0.096   0.002   0.098
> 
> substr still wins, but by a much smaller margin.

<snip>

Just to confirm Prof. Ripley's suspicion, that I am indeed running in
en_US.UTF-8.

Thanks for taking the time to point this out.

Best regards,

Marc


From otoomet at ut.ee  Thu Jan 18 14:07:03 2007
From: otoomet at ut.ee (Ott Toomet)
Date: Thu, 18 Jan 2007 15:07:03 +0200
Subject: [R] eval while keeping NA-s
Message-ID: <200701181307.l0ID73Os003296@hugo.obs.ee>

Dear R-people,

I would like to construct a model frame while keeping eventual NA-s in
it.  The code looks like in lm():

   m <- match(c("formula", "data", "subset", "weights", "na.action",
                "offset"), names(mf), 0)
   mfO <- mf[c(1, m)]
   mfO$drop.unused.levels <- TRUE
   mfO[[1]] <- as.name("model.frame")
   names(mfO)[2] <- "formula"
   mfO <- eval(mfO, parent.frame())

The problem is that eval() removes all the observation which include
NA-s.  

Are there ways to get the frames and keep NA-s?  I see, I can play
around with the "na.action" attribute of the resulting frame, but how
can I set the na.action?

Thanks in advance,
Ott


From np at alambic.org  Thu Jan 18 14:11:11 2007
From: np at alambic.org (Nicolas Prune)
Date: Thu, 18 Jan 2007 14:11:11 +0100
Subject: [R] How to optimize this loop ?
Message-ID: <1169125871.45af71ef3aa7f@imp2.online.net>

Dear R Users,

I request your help to optimize a loop.

Given a series of observations, I want to know how many consecutive past
observations are below the last one.

e.g :
my_series <- c(3, 4, 10,14,8,3,4,6,9)

As the last number (9)  is higher than the four preceding numbers (6, 4, 3, 8),
this function should return 4.

my_series <- c(3, 4, 10,14,8,3,4,11,9)
Here, it should return 0, as 9 is immediately preceeded by a higher number.

So far, I do this awful loop :

result <- 0
for (i in 1:length(my_series-1))
{
 if (my_series[length(my_series)-i]>end(my_series)[1])
{ result <- i-1 ; break }
}

I thing there's a better way...

my_series > my_series[end][1] returns :
TRUE TRUE FALSE FALSE TRUE TRUE TRUE TRUE FALSE
, which seems more appealing (once the last "FALSE" is removed), but now, how to
know the size of the last consecutive series of "TRUE" ?

Can you see a better way ?

Thanks.


From ripley at stats.ox.ac.uk  Thu Jan 18 14:28:13 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Jan 2007 13:28:13 +0000 (GMT)
Subject: [R] eval while keeping NA-s
In-Reply-To: <200701181307.l0ID73Os003296@hugo.obs.ee>
References: <200701181307.l0ID73Os003296@hugo.obs.ee>
Message-ID: <Pine.LNX.4.64.0701181325470.32333@gannet.stats.ox.ac.uk>

On Thu, 18 Jan 2007, Ott Toomet wrote:

> Dear R-people,
>
> I would like to construct a model frame while keeping eventual NA-s in
> it.  The code looks like in lm():
>
>   m <- match(c("formula", "data", "subset", "weights", "na.action",
>                "offset"), names(mf), 0)
>   mfO <- mf[c(1, m)]
>   mfO$drop.unused.levels <- TRUE
>   mfO[[1]] <- as.name("model.frame")
>   names(mfO)[2] <- "formula"
>   mfO <- eval(mfO, parent.frame())
>
> The problem is that eval() removes all the observation which include
> NA-s.
>
> Are there ways to get the frames and keep NA-s?  I see, I can play
> around with the "na.action" attribute of the resulting frame, but how
> can I set the na.action?

Set na.action=na.pass on the call, or reset in your code (as that resets 
drop.unused.levels).


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From petr.pikal at precheza.cz  Thu Jan 18 14:51:00 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 18 Jan 2007 14:51:00 +0100
Subject: [R] How to optimize this loop ?
In-Reply-To: <1169125871.45af71ef3aa7f@imp2.online.net>
Message-ID: <45AF8954.18733.1CBE556@localhost>

Hi

discard your loop do not optimise it.
rle is your friend

> my_fun<-function(x) {
+ 
+ len<-length(x)
+ x1<-rle(x[len]>x[1:len-1])
+ last<-length(x1$values)
+ ifelse(x1$values[last],x1$lengths[last],0)
+ }
> my_fun(my_series)
[1] 0
> my_series <- c(3, 4, 10,14,8,3,4,6,9)
> my_fun(my_series)
[1] 4
>

and vectorise, vectorise, vectorise.

HTH
Petr




On 18 Jan 2007 at 14:11, Nicolas Prune wrote:

Date sent:      	Thu, 18 Jan 2007 14:11:11 +0100
From:           	Nicolas Prune <np at alambic.org>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] How to optimize this loop ?

> Dear R Users,
> 
> I request your help to optimize a loop.
> 
> Given a series of observations, I want to know how many consecutive
> past observations are below the last one.
> 
> e.g :
> my_series <- c(3, 4, 10,14,8,3,4,6,9)
> 
> As the last number (9)  is higher than the four preceding numbers (6,
> 4, 3, 8), this function should return 4.
> 
> my_series <- c(3, 4, 10,14,8,3,4,11,9)
> Here, it should return 0, as 9 is immediately preceeded by a higher
> number.
> 
> So far, I do this awful loop :
> 
> result <- 0
> for (i in 1:length(my_series-1))
> {
>  if (my_series[length(my_series)-i]>end(my_series)[1])
> { result <- i-1 ; break }
> }
> 
> I thing there's a better way...
> 
> my_series > my_series[end][1] returns :
> TRUE TRUE FALSE FALSE TRUE TRUE TRUE TRUE FALSE
> , which seems more appealing (once the last "FALSE" is removed), but
> now, how to know the size of the last consecutive series of "TRUE" ?
> 
> Can you see a better way ?
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From hkawakat at gmail.com  Thu Jan 18 14:54:18 2007
From: hkawakat at gmail.com (Hiroyuki Kawakatsu)
Date: Thu, 18 Jan 2007 13:54:18 +0000
Subject: [R] building R on freebsd 6.2 (amd64)
Message-ID: <307b90470701180554x14290e41qbb78c0eefd5cc8f0@mail.gmail.com>

Hi,

I updated my os to freebsd 6.2 and built R-patched with no changes in
configure. I pass make check and copy the R executable to
/usr/local/bin/R. Then when I start R, I get

/libexec/ld-elf.so.1: Shared object "libRblas.so" not found, required by "R"

and have to copy/link the two shared object files (libRblas.so and
libRlapack.so) to somewhere "visible" like /usr/local/lib/ as well. I
didn't have to do this last step in freebsd 6.1. Is there some
configure setting I can use to avoid having to move the two shared
object files?

h.
-- 
----------------------------------
Hiroyuki Kawakatsu
Business School
Dublin City University
Dublin 9, Ireland
Tel +353 (0)1 700 7496


From ccleland at optonline.net  Thu Jan 18 14:59:38 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 18 Jan 2007 08:59:38 -0500
Subject: [R] How to optimize this loop ?
In-Reply-To: <1169125871.45af71ef3aa7f@imp2.online.net>
References: <1169125871.45af71ef3aa7f@imp2.online.net>
Message-ID: <45AF7D4A.6060105@optonline.net>

Nicolas Prune wrote:
> Dear R Users,
> 
> I request your help to optimize a loop.
> 
> Given a series of observations, I want to know how many consecutive past
> observations are below the last one.
> 
> e.g :
> my_series <- c(3, 4, 10,14,8,3,4,6,9)
> 
> As the last number (9)  is higher than the four preceding numbers (6, 4, 3, 8),
> this function should return 4.
> 
> my_series <- c(3, 4, 10,14,8,3,4,11,9)
> Here, it should return 0, as 9 is immediately preceeded by a higher number.
> 
> So far, I do this awful loop :
> 
> result <- 0
> for (i in 1:length(my_series-1))
> {
>  if (my_series[length(my_series)-i]>end(my_series)[1])
> { result <- i-1 ; break }
> }
> 
> I thing there's a better way...

  Just in terms of finding out which values are bigger than the
preceding value and avoiding the explicit loop, you could do something
like this:

seriesA <- c(3,4,10,14,8,3,4,6,9)
seriesB <- c(3,4,10,14,8,3,4,11,9)

sign(diff(seriesA)) == 1
[1]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE

sign(diff(seriesB)) == 1
[1]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE

> my_series > my_series[end][1] returns :
> TRUE TRUE FALSE FALSE TRUE TRUE TRUE TRUE FALSE
> , which seems more appealing (once the last "FALSE" is removed), but now, how to
> know the size of the last consecutive series of "TRUE" ?
> 
> Can you see a better way ?
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From btyner at stat.purdue.edu  Thu Jan 18 15:16:27 2007
From: btyner at stat.purdue.edu (Benjamin Tyner)
Date: Thu, 18 Jan 2007 09:16:27 -0500
Subject: [R] curious about dimension of 'apply' output when MARGIN=1
In-Reply-To: <Pine.LNX.4.64.0701171537220.7223@gannet.stats.ox.ac.uk>
References: <45AD43FD.8050100@stat.purdue.edu>
	<971536df0701161616n5d586fedhdbbdd29e3131f8dd@mail.gmail.com>
	<45ADFE3A.7050409@pburns.seanet.com>
	<Pine.LNX.4.64.0701171537220.7223@gannet.stats.ox.ac.uk>
Message-ID: <45AF813B.3060905@stat.purdue.edu>

Thanks to all for your insightful comments. I must admit I was unaware 
of the application to arrays.

Ben

Prof Brian Ripley wrote:
> Or that the vision of the original designer was not limited to 
> matrices. It just so happens that in this example the replacement is a 
> single dimension the same size as the single margin used.  That's 
> atypical, and normally the result dimension has no connection to the 
> margin.  The design is to put the result dimension first, and the 
> first item in the 'seealso' list is aperm().
>
> To my mind the only general solutions are to put the result dimension 
> first or last.  I would have used last, but using first is slightly 
> more efficient for the reason Pat gives.
>
> apply() is for arrays, operating over one or more margins with a 
> function returning a 'scalar', vector or array result.  Perhaps any 
> 'lazy'-ness / limit of vision is not in handling array results as well 
> as might be possible.


From martin.becker at mx.uni-saarland.de  Thu Jan 18 15:16:21 2007
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Thu, 18 Jan 2007 15:16:21 +0100
Subject: [R] How to optimize this loop ?
In-Reply-To: <45AF8954.18733.1CBE556@localhost>
References: <45AF8954.18733.1CBE556@localhost>
Message-ID: <45AF8135.4070500@mx.uni-saarland.de>

Petr Pikal schrieb:
> Hi
>
> discard your loop do not optimise it.
> rle is your friend
>   
I do not agree. For some purposes, an efficient loop is faster. IMHO 
this is one of these purposes.
I propose the following modification of the loop (to increase speed):

myfun1 <- function(series=c(3, 4, 10,14,8,3,4,6,9)) {
  len<-length(series)
  for (i in (len-1):1)
  {
    if (series[i]>series[len])
    { result <- i-1 ; break }
  }
  return(result)
}

The speed measurement, in comparison with the rle approach:

 > system.time(for (i in 1:10000) erg<-my_fun(c(3, 4, 10,14,8,3,4,6,9)))
[1] 4.48 0.00 4.48   NA   NA

 > system.time(for (i in 1:10000) erg<-myfun1(c(3, 4, 10,14,8,3,4,6,9)))
[1] 0.33 0.00 0.33   NA   NA

Regards, Martin
>   
>> my_fun<-function(x) {
>>     
> + 
> + len<-length(x)
> + x1<-rle(x[len]>x[1:len-1])
> + last<-length(x1$values)
> + ifelse(x1$values[last],x1$lengths[last],0)
> + }
>   
>> my_fun(my_series)
>>     
> [1] 0
>   
>> my_series <- c(3, 4, 10,14,8,3,4,6,9)
>> my_fun(my_series)
>>     
> [1] 4
>   
>
> and vectorise, vectorise, vectorise.
>
> HTH
> Petr
>
>
>
>
> On 18 Jan 2007 at 14:11, Nicolas Prune wrote:
>
> Date sent:      	Thu, 18 Jan 2007 14:11:11 +0100
> From:           	Nicolas Prune <np at alambic.org>
> To:             	r-help at stat.math.ethz.ch
> Subject:        	[R] How to optimize this loop ?
>
>   
>> Dear R Users,
>>
>> I request your help to optimize a loop.
>>
>> Given a series of observations, I want to know how many consecutive
>> past observations are below the last one.
>>
>> e.g :
>> my_series <- c(3, 4, 10,14,8,3,4,6,9)
>>
>> As the last number (9)  is higher than the four preceding numbers (6,
>> 4, 3, 8), this function should return 4.
>>
>> my_series <- c(3, 4, 10,14,8,3,4,11,9)
>> Here, it should return 0, as 9 is immediately preceeded by a higher
>> number.
>>
>> So far, I do this awful loop :
>>
>> result <- 0
>> for (i in 1:length(my_series-1))
>> {
>>  if (my_series[length(my_series)-i]>end(my_series)[1])
>> { result <- i-1 ; break }
>> }
>>
>> I thing there's a better way...
>>
>> my_series > my_series[end][1] returns :
>> TRUE TRUE FALSE FALSE TRUE TRUE TRUE TRUE FALSE
>> , which seems more appealing (once the last "FALSE" is removed), but
>> now, how to know the size of the last consecutive series of "TRUE" ?
>>
>> Can you see a better way ?
>>
>> Thanks.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html and provide commented,
>> minimal, self-contained, reproducible code.
>>     
>
> Petr Pikal
> petr.pikal at precheza.cz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From martin.becker at mx.uni-saarland.de  Thu Jan 18 15:22:07 2007
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Thu, 18 Jan 2007 15:22:07 +0100
Subject: [R] How to optimize this loop ? (correction)
In-Reply-To: <45AF8135.4070500@mx.uni-saarland.de>
References: <45AF8954.18733.1CBE556@localhost>
	<45AF8135.4070500@mx.uni-saarland.de>
Message-ID: <45AF828F.3020500@mx.uni-saarland.de>

Sorry, I accidentaly lost one line of the function code (result <-0), 
see below...
Regards, Martin

Martin Becker schrieb:
>
> myfun1 <- function(series=c(3, 4, 10,14,8,3,4,6,9)) {
     result <- 0                        # NEW
>  len<-length(series)
>  for (i in (len-1):1)
>  {
>    if (series[i]>series[len])
>    { result <- i-1 ; break }
>  }
>  return(result)
> }
>
...


From gbergling at 0xfce3.net  Thu Jan 18 15:22:44 2007
From: gbergling at 0xfce3.net (Gordon Bergling)
Date: Thu, 18 Jan 2007 15:22:44 +0100
Subject: [R] building R on freebsd 6.2 (amd64)
In-Reply-To: <307b90470701180554x14290e41qbb78c0eefd5cc8f0@mail.gmail.com>
References: <307b90470701180554x14290e41qbb78c0eefd5cc8f0@mail.gmail.com>
Message-ID: <45AF82B4.7080504@0xfce3.net>

Hi,

Hiroyuki Kawakatsu wrote:
> Hi,
>
> I updated my os to freebsd 6.2 and built R-patched with no changes in
> configure. I pass make check and copy the R executable to
> /usr/local/bin/R. Then when I start R, I get
>
> /libexec/ld-elf.so.1: Shared object "libRblas.so" not found, required by "R"
>
> and have to copy/link the two shared object files (libRblas.so and
> libRlapack.so) to somewhere "visible" like /usr/local/lib/ as well. I
> didn't have to do this last step in freebsd 6.1. Is there some
> configure setting I can use to avoid having to move the two shared
> object files?
>   
have you tried to build R over the ports system?

regards,

    Gordon


From dimaiana at gmail.com  Thu Jan 18 15:34:23 2007
From: dimaiana at gmail.com (leah martell)
Date: Thu, 18 Jan 2007 09:34:23 -0500
Subject: [R] help with niave bayes
Message-ID: <6bb7be150701180634g72c2a41m371ac07e14eee625@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070118/cdca7f57/attachment.pl 

From ripley at stats.ox.ac.uk  Thu Jan 18 15:34:59 2007
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Thu, 18 Jan 2007 14:34:59 +0000 (GMT Standard Time)
Subject: [R] building R on freebsd 6.2 (amd64)
In-Reply-To: <307b90470701180554x14290e41qbb78c0eefd5cc8f0@mail.gmail.com>
References: <307b90470701180554x14290e41qbb78c0eefd5cc8f0@mail.gmail.com>
Message-ID: <Pine.WNT.4.64.0701181428290.2216@Petrel>

On Thu, 18 Jan 2007, Hiroyuki Kawakatsu wrote:

> Hi,
>
> I updated my os to freebsd 6.2 and built R-patched with no changes in
> configure. I pass make check and copy the R executable to
> /usr/local/bin/R. Then when I start R, I get
>
> /libexec/ld-elf.so.1: Shared object "libRblas.so" not found, required by "R"
>
> and have to copy/link the two shared object files (libRblas.so and
> libRlapack.so) to somewhere "visible" like /usr/local/lib/ as well. I
> didn't have to do this last step in freebsd 6.1. Is there some
> configure setting I can use to avoid having to move the two shared
> object files?

The R front end sets LD_LIBRARY_PATH to include the place it puts libR.so. 
It looks like that is not effective on your machine, but you will have to 
investigate for us why.  I am pretty sure other FreeBSD users (6.2 and 7) 
on AMD64 reported success on R 2.4.x.

How does FreeBSD 6.2 handle 64-bit systems?  Most OSes would use 
/usr/local/lib64, and there is a configure setting LIBnn to cover that 
issue.

Another solution for ELF systems is to include R_HOME/lib in the list of 
directories known to ldconfig.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From biostatistica at gmail.com  Thu Jan 18 15:45:53 2007
From: biostatistica at gmail.com (=?ISO-8859-1?Q?Niccol=F2_Bassani?=)
Date: Thu, 18 Jan 2007 15:45:53 +0100
Subject: [R] Problems replicating rows and associated increasing index
Message-ID: <7d82a6ca0701180645k5db80c0ds6a5f776ab50fd3b7@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070118/911a5d2a/attachment.pl 

From David.Crabb.1 at city.ac.uk  Thu Jan 18 15:15:45 2007
From: David.Crabb.1 at city.ac.uk (Crabb, David)
Date: Thu, 18 Jan 2007 14:15:45 -0000
Subject: [R]  Help with problem - multilevel model?
Message-ID: <B275C8556CC10248B260951E0DB0719B0338D8C3@nsq038ex.enterprise.internal.city.ac.uk>

I have what is probably a very simple problem but I would be very
grateful to the list for any help or pointers to implement a solution in
R.

We have two types of measurements on the eye that we have collected in
62 patients at 5 fixed time points during a clinical visit over an
office day. We want to establish if there is an association between the
measurements. Obviously it would be wrong to generate a simple
scatterplot of all 62*5 observations and consider something like a
simple correlation coefficient. What I really need is to set up a
2-level model with measurements nested within patients and use, perhaps,
one variable as a response and the other as a predictor. I would also
like to add in time of day (probably as a categorical) to check this
isn't affecting things. I haven't used multi-level modelling in R and
wonder how I can simply go about fitting this model and looking at the
significance of the estimates in R. I also have a plot in my mind - 62
little regression lines (n=5) and think that this might be
informative.....

Any help or pointers will be gratefully received - Many Thanks.

Dr. David Crabb
Department of Optometry and Visual Science,
City University, Northampton Square, London EC1V OHB
Tel: 44 207 040 0191   d.crabb at city.ac.uk
http://www.city.ac.uk/optometry/about/staff/crabb.html


From np at alambic.org  Thu Jan 18 16:08:45 2007
From: np at alambic.org (Nicolas Prune)
Date: Thu, 18 Jan 2007 16:08:45 +0100
Subject: [R] How to optimize this loop ? (correction)
In-Reply-To: <45AF828F.3020500@mx.uni-saarland.de>
References: <45AF8954.18733.1CBE556@localhost>
	<45AF8135.4070500@mx.uni-saarland.de>
	<45AF828F.3020500@mx.uni-saarland.de>
Message-ID: <1169132925.45af8d7d62a00@imp2.online.net>

Thanks to everyone who answered, in public or in private.

I successfully rewrote my function with rle.

For my sample, the total computation times (I call this function around 1000
times) are roughly the same. However, I thank you for letting me know rle, and
S Poetry.

Regards,
Nicolas


From rmh at temple.edu  Thu Jan 18 16:22:27 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu, 18 Jan 2007 10:22:27 -0500 (EST)
Subject: [R] Help with problem - multilevel model?
Message-ID: <20070118102227.BSI94998@po-d.temple.edu>

This should get you started


tmp <- data.frame(y1=rnorm(62*5),
                  y2=rnorm(62*5),
                  time=rep(1:5, 62),
                  id=factor(rep(1:62, each=5))
                  )

xyplot(y1 ~ y2 | id, data=tmp, group=time, pch=as.character(1:5))

tmp.aov <- aov(y1 ~ y2*factor(time) + Error(id), data=tmp)
summary(tmp.aov)


From jiajiehere at hotmail.com  Thu Jan 18 16:25:59 2007
From: jiajiehere at hotmail.com (w jj)
Date: Thu, 18 Jan 2007 15:25:59 +0000
Subject: [R] How to specify arguments in lme() ?
Message-ID: <BAY114-F2359B1924B19B8D314444BD1AA0@phx.gbl>

Hi,

I have a question about the function lme() in R.

I have a 2*2*3 layout with some missing data (labelled as *).  These 3 
factors are labelled as A,B,C, the response is Score. The layout is as 
follows:-

A              B                 C                    Score
1               1                  1                         5
1               1                  2                         *
1               1                  3                         1
1               2                  1                         4
1               2                  2                         4
1               2                  3                         *
2               1                  1                         3
2               1                  2                         *
2               1                  3                         4
2               2                  1                         2
2               2                  2                         *
2               2                  3                         5

Suppose these data are stored in a data frame called "test".

If all these 3 factors are fixed, then I can fit a model without the 3-way 
interaction as:-
fit1<-lm(Score~A*B+A*C+B*C,data=test)        

If one of these factors, say A, is a random effect variable, then I need to 
fit a mixed effect model using lme(). I have read the R documention on 
lme(), but I am still not clear how to specify the random argument. I tried 
to do:-

fit2<-lme(Score~A*B+A*C+B*C,data=test,random=~A, na.action=na.pass) 

but the system give a message as follows:-
Error in getGroups.data.frame(dataMix, groups) : 
        Invalid formula for groups

So how should I specify the arguments? 

Thank you very much for your help!

Jiajie


From HDoran at air.org  Thu Jan 18 16:27:29 2007
From: HDoran at air.org (Doran, Harold)
Date: Thu, 18 Jan 2007 10:27:29 -0500
Subject: [R] Help with problem - multilevel model?
Message-ID: <2323A6D37908A847A7C32F1E3662C80E8D43C3@dc1ex01.air.org>

Aov is not the right function for this problem. Lmer is designed for
multilevel modeling. There are a lot of resources, but start with the
following vignette

Library(lme4)
vignette("MlmRevSoft")

And then turn to Pinhiero and Bates

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Richard M. Heiberger
> Sent: Thursday, January 18, 2007 10:22 AM
> To: Crabb, David; r-help at stat.math.ethz.ch
> Subject: Re: [R] Help with problem - multilevel model?
> 
> This should get you started
> 
> 
> tmp <- data.frame(y1=rnorm(62*5),
>                   y2=rnorm(62*5),
>                   time=rep(1:5, 62),
>                   id=factor(rep(1:62, each=5))
>                   )
> 
> xyplot(y1 ~ y2 | id, data=tmp, group=time, pch=as.character(1:5))
> 
> tmp.aov <- aov(y1 ~ y2*factor(time) + Error(id), data=tmp)
> summary(tmp.aov)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Thu Jan 18 16:32:45 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 18 Jan 2007 10:32:45 -0500
Subject: [R] Problems replicating rows and associated increasing index
In-Reply-To: <7d82a6ca0701180645k5db80c0ds6a5f776ab50fd3b7@mail.gmail.com>
References: <7d82a6ca0701180645k5db80c0ds6a5f776ab50fd3b7@mail.gmail.com>
Message-ID: <644e1f320701180732u154e1e8cy5b5aaf5919c79ec3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070118/1d2de172/attachment.pl 

From ajayshah at mayin.org  Thu Jan 18 15:30:55 2007
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Thu, 18 Jan 2007 20:00:55 +0530
Subject: [R] The math underlying the `betareg' package?
Message-ID: <20070118143055.GN3663@lubyanka.local>

Folks,

The betareg package appears to be polished and works well. But I would
like to look at the exact formulas for the underlying model being
estimated, the likelihood function, etc. E.g. if one has to compute
\frac{\partial E(y)}{\partial x_i}, this requires careful calculations
through these formulas. I read "Regression analysis of variates
observed on (0,1): percentages, proportions and fractions", by
Kieschnick & MucCullogh, `Statistical Modelling" 2003, 3:193-213. They
say that the beta regression that they show is a proposal of theirs -
is this the same as what betareg does, or is this the Standard
Formulation?

What else should I be reading about beta regressions? :-)

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From peter.rossi at chicagogsb.edu  Thu Jan 18 16:42:21 2007
From: peter.rossi at chicagogsb.edu (Rossi, Peter E.)
Date: Thu, 18 Jan 2007 09:42:21 -0600
Subject: [R] [R-pkgs] Version 2.0-9 of bayesm
Message-ID: <1E7B167439290641966EB161D433079801BE1A8D@GSBEX.gsb.uchicago.edu>

Version 2.0-9 of bayesm is now available on CRAN

changes include-

1. addition of rhierLinearMixture -- linear hierarchical models with a
mixture of normals prior
2. mixDenBi is now fully vectorized and run more than 10 times faster
3. minor documentation corrections have been made
4. rnmixGibbs allows the user to specify only one component
 
peter rossi

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From atreibel3 at mail.gatech.edu  Thu Jan 18 17:15:19 2007
From: atreibel3 at mail.gatech.edu (atreibel3 at mail.gatech.edu)
Date: Thu, 18 Jan 2007 11:15:19 -0500
Subject: [R] fitCopula method in R
Message-ID: <1169136919.45af9d175d40b@webmail.mail.gatech.edu>



--
Hello,

I am attempting to fit monthly stock returns to possible copula functions using
the copula package in R.  Below is my code (mat2 is a 2x119 matrix of the two
stock returns):

my.cop <- normalCopula(param=.3, dim = 2, dispstr = "un")
myfit <- fitCopula(mat2,my.cop, start=.65, optim.control= list(NULL), method =
"BFGS")
myfit

Unfortunately, I continue to receive this error:

Error in optim(start, loglikCopula, method = method, copula = copula,  :
        initial value in 'vmmin' is not finite
In addition: Warning message:
NaNs produced in: qnorm(p, mean, sd, lower.tail, log.p)

I don't know why my start value is wrong or how to choose a correct one.  Any
help is greatly appreicated.  Thanks.

Adam


From bates at stat.wisc.edu  Thu Jan 18 17:20:44 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 18 Jan 2007 17:20:44 +0100
Subject: [R] How to specify arguments in lme() ?
In-Reply-To: <BAY114-F2359B1924B19B8D314444BD1AA0@phx.gbl>
References: <BAY114-F2359B1924B19B8D314444BD1AA0@phx.gbl>
Message-ID: <40e66e0b0701180820k528d1bd6lc964fa2b1d102a8d@mail.gmail.com>

On 1/18/07, w jj <jiajiehere at hotmail.com> wrote:

> I have a question about the function lme() in R.
>
> I have a 2*2*3 layout with some missing data (labelled as *).  These 3
> factors are labelled as A,B,C, the response is Score. The layout is as
> follows:-
>
> A              B                 C                    Score
> 1               1                  1                         5
> 1               1                  2                         *
> 1               1                  3                         1
> 1               2                  1                         4
> 1               2                  2                         4
> 1               2                  3                         *
> 2               1                  1                         3
> 2               1                  2                         *
> 2               1                  3                         4
> 2               2                  1                         2
> 2               2                  2                         *
> 2               2                  3                         5
>
> Suppose these data are stored in a data frame called "test".
>
> If all these 3 factors are fixed, then I can fit a model without the 3-way
> interaction as:-
> fit1<-lm(Score~A*B+A*C+B*C,data=test)
>
> If one of these factors, say A, is a random effect variable, then I need to
> fit a mixed effect model using lme(). I have read the R documention on
> lme(), but I am still not clear how to specify the random argument. I tried
> to do:-

You could do it but you don't really want to try to fit a model with
several random effects generated by a factor with only two levels.
Estimating variances, which is what is done for a random effect, is
more difficult than estimating means or other linear combinations of
the responses, which is what fixed effects parameters end up being
expressed as.  Trying to estimate a variance when observing a factor
at only two levels is overly optimistic.

Just for the record, the call to lmer in the lme4 package would be

fit2 <- lmer(Score ~ B*C+(1|A/B)+(1|C:A), data = test)

> fit2<-lme(Score~A*B+A*C+B*C,data=test,random=~A, na.action=na.pass)

I don't think you want to use na.pass here.  The underlying C code for
fitting lme or lmer models doesn't take kindly to finding NA's in the
data.

>
> but the system give a message as follows:-
> Error in getGroups.data.frame(dataMix, groups) :
>         Invalid formula for groups
>
> So how should I specify the arguments?
>
> Thank you very much for your help!
>
> Jiajie
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From gavin.simpson at ucl.ac.uk  Thu Jan 18 17:34:04 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 18 Jan 2007 16:34:04 +0000
Subject: [R] The math underlying the `betareg' package?
In-Reply-To: <20070118143055.GN3663@lubyanka.local>
References: <20070118143055.GN3663@lubyanka.local>
Message-ID: <1169138044.26268.54.camel@gsimpson.geog.ucl.ac.uk>

On Thu, 2007-01-18 at 20:00 +0530, Ajay Narottam Shah wrote:
> Folks,
> 
> The betareg package appears to be polished and works well. But I would
> like to look at the exact formulas for the underlying model being
> estimated, the likelihood function, etc. E.g. if one has to compute
> \frac{\partial E(y)}{\partial x_i}, this requires careful calculations
> through these formulas. I read "Regression analysis of variates
> observed on (0,1): percentages, proportions and fractions", by
> Kieschnick & MucCullogh, `Statistical Modelling" 2003, 3:193-213. They
> say that the beta regression that they show is a proposal of theirs -
> is this the same as what betareg does, or is this the Standard
> Formulation?

If you want to know, the best place to look is the source code for the
package, available as a tar.gz file from all good CRAN Mirrors.

I suggest this as the Windows binary might not contain the original
source (i.e unprocessed with comments etc) - I forget now exactly how
the binaries on that platform differ.

> 
> What else should I be reading about beta regressions? :-)

The reference cited in the References section of ?betareg would also be
a good start, esp to understand what the betareg package is doing and
how it compares to the other ref you cite.

HTH

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From solo_qau at yahoo.com  Thu Jan 18 17:38:10 2007
From: solo_qau at yahoo.com (zahid khan)
Date: Thu, 18 Jan 2007 08:38:10 -0800 (PST)
Subject: [R] problem in adf command
Message-ID: <300094.85637.qm@web34607.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070118/452b2d22/attachment.pl 

From hkawakat at gmail.com  Thu Jan 18 18:02:40 2007
From: hkawakat at gmail.com (Hiroyuki Kawakatsu)
Date: Thu, 18 Jan 2007 17:02:40 +0000
Subject: [R] building R on freebsd 6.2 (amd64)
In-Reply-To: <Pine.WNT.4.64.0701181428290.2216@Petrel>
References: <307b90470701180554x14290e41qbb78c0eefd5cc8f0@mail.gmail.com>
	<Pine.WNT.4.64.0701181428290.2216@Petrel>
Message-ID: <307b90470701180902w39f38dbajf78d888b3efc5d65@mail.gmail.com>

On 1/18/07, Prof Brian D Ripley <ripley at stats.ox.ac.uk> wrote:
>
> The R front end sets LD_LIBRARY_PATH to include the place it puts libR.so.
> It looks like that is not effective on your machine, but you will have to
> investigate for us why.  I am pretty sure other FreeBSD users (6.2 and 7)
> on AMD64 reported success on R 2.4.x.

echo returns
LD_LIBRARY_PATH: Undefined variable.

> How does FreeBSD 6.2 handle 64-bit systems?  Most OSes would use
> /usr/local/lib64, and there is a configure setting LIBnn to cover that
> issue.

I have /usr/lib/, /usr/lib32/, /usr/local/lib/, but no /usr/local/lib64/

> Another solution for ELF systems is to include R_HOME/lib in the list of
> directories known to ldconfig.

Thanks for that tip. I suppose if I symlink to R_HOME/lib/ I won't
have to redo them every time I rebuild R-patched. Just as note to
myself, the following appears to work

ln -s /opt/acml3.6.0/gnu64/lib/libacml.so /usr/local/lib/libRblas.so
ln -s /opt/acml3.6.0/gnu64/lib/libacml_mv.so /usr/local/lib/libacml_mv.so
ln -s R_HOME/lib/libRlapack.so /usr/local/lib/libRlapack.so

h.
-- 
----------------------------------
Hiroyuki Kawakatsu
Business School
Dublin City University
Dublin 9, Ireland
Tel +353 (0)1 700 7496


From rdporto1 at terra.com.br  Thu Jan 18 18:04:04 2007
From: rdporto1 at terra.com.br (rdporto1)
Date: Thu, 18 Jan 2007 14:04:04 -0300
Subject: [R] The math underlying the `betareg' package?
Message-ID: <JC2Q2S$FC55D3BCFAA8E084EDFE3C2447EAF029@terra.com.br>

>What else should I be reading about beta regressions? :-)

Ajay,

you could begin by reading the paper cited in betareg package:

FERRARI, S.L.P., CRIBARI-NETO, F. (2004).
Beta regression for modeling rates and proportions.
Journal of Applied Statistics, v. 31, n. 7, p. 799-815

I'd bet that there are answers to some of your questions
there.

HTH,

Rogerio


From charles.dupont at vanderbilt.edu  Thu Jan 18 18:06:29 2007
From: charles.dupont at vanderbilt.edu (Charles Dupont)
Date: Thu, 18 Jan 2007 11:06:29 -0600
Subject: [R] options("digits") and print.default()
In-Reply-To: <Pine.LNX.4.64.0701151716060.5973@gannet.stats.ox.ac.uk>
References: <fc.0087cdad000a4cad0087cdad000a4cad.a4cd5@emacs.lu>
	<Pine.LNX.4.64.0701151716060.5973@gannet.stats.ox.ac.uk>
Message-ID: <45AFA915.5010805@vanderbilt.edu>

Prof Brian Ripley wrote:
> On Mon, 15 Jan 2007, Ulrich Keller wrote:
> 
>> Hello everyone,
>>
>> I use latex() (Hmisc) for report generation and thus have been affected by
>> the problem with rounding decimals described, for example, in this post:
>>
>> http://thread.gmane.org/gmane.comp.lang.r.general/73287/focus=73287
>>
>> In short, numbers often are printed with 15 or so decimals even if there
>> far less significant digits. The problem has been confirmed by Frank
>> Harrell and Thomas Dupont according to this post:
>>
>> http://thread.gmane.org/gmane.comp.lang.r.general/73172/focus=73186
>>
>> But it has still not been fixed. Rather than changing all my reports I
>> decided I'd look at format.df() in Hmisc myself and try to fix the problem
>> there. I found that the problem is not in Hmisc at all, but in R itself:
>>
>>> print(1.001)
>> [1] 1.001
>>> options(digits=16) #format.df does this
>>> print(1.001)
>> [1] 1.001000000000000
>>> print(round(1.001, 3)) #rounding does not help
>> [1] 1.001000000000000
>>
>> This does not seem to be the behaviour described in the documentation,
>> which says:
>>
>> "The same number of decimal places is used throughout a vector,[sic!] This
>> means that digits specifies the minimum number of significant digits to be
>> used, and that at least one entry will be encoded with that minimum
>> number. However, if all the encoded elements then have trailing zeroes,
>> the number of decimal places is reduced until at least one element has a
>> non-zero final digit."
>>
>> If print.default() exhibited the behaviour desribed in the docs,
>> format.df() and thus latex() would work as advertised, I think. I would
>> have written a bug report instead of posting here, but the fact (?) that
>> Frank and Thomas have confirmed the bug seems to indicate that the problem
>> does indeed lie with Hmisc. Am I misunderstanding something here?
>>
>> I use R version 2.4.1 Patched (2007-01-13 r40470) on Windows.
> 
> The bug is that Hmisc uses more digits than the representation provides:
> 
>> .Machine$double.eps
> [1] 2.22044604925031e-16
> 
> If it used 15 digits (as R does for as.character) all would be well. 
> Since R has to do the calculations you quote in binary arithmetic, they 
> are also subject to representation error and there is no way they can be 
> done to 16 significant figures.  (I've traced the example, and that is 
> what happens.)  See ?as.character for more details.
> 

Is there an R object that contains the value of DBL_DIG for the current 
machine?  I have checked '.Machine' and '.Platform' and neither have the 
value of DBL_DIG in them.

Thank you

Charles Dupont

-- 
Charles Dupont	Computer System Analyst		School of Medicine
		Department of Biostatistics	Vanderbilt University


From maechler at stat.math.ethz.ch  Thu Jan 18 18:13:45 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 18 Jan 2007 18:13:45 +0100
Subject: [R] sleep data
In-Reply-To: <Pine.LNX.4.64.0701171043310.27414@tajo.ucsd.edu>
References: <45ADEE46.6000803@psych.uib.no>
	<Pine.LNX.4.64.0701171043310.27414@tajo.ucsd.edu>
Message-ID: <17839.43721.21978.452777@stat.math.ethz.ch>

Thank you, Chuck and Tom.

I'd gladly improve the help page,

particularly if you can provide a patch against

https://svn.R-project.org/R/trunk/src/library/datasets/man/sleep.Rd

where I've already added the Cushny and Peebles reference,
thanks to Chuck.

Regards,
Martin Maechler, ETH Zurich


>>>>> "ChuckB" == Charles C Berry <cberry at tajo.ucsd.edu>
>>>>>     on Wed, 17 Jan 2007 10:48:13 -0800 writes:

    ChuckB> Yes, you refer to

    ChuckB>  	Cushny, A. R. and Peebles, A. R. The action of
    ChuckB> optical isomers: II hyoscines. The Journal of
    ChuckB> Physiology, 1905, 32: 501.510.

    ChuckB> which was used by 'Student' to illustrate the paired
    ChuckB> t-test.

    ChuckB> This is indeed a crossover design.

    ChuckB> On Wed, 17 Jan 2007, Tom Backer Johnsen wrote:

    >> When reading the documentation for the "sleep" data set
    >> in R, the impression is clear, this is an "independent
    >> groups" kind of design (two groups of 10 subjects each).
    >> However, when browsing the original article (referred to
    >> in the help file), my impression is quite clear, this is
    >> really a "repeated measures" kind of data (one group of
    >> 10 subjects, two observations).  What is correct?
    >> 
    >> Tom
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
    >> read the posting guide
    >> http://www.R-project.org/posting-guide.html and provide
    >> commented, minimal, self-contained, reproducible code.
    >> 

    ChuckB> Charles C. Berry (858) 534-2098 Dept of
    ChuckB> Family/Preventive Medicine E
    ChuckB> mailto:cberry at tajo.ucsd.edu UC San Diego
    ChuckB> http://biostat.ucsd.edu/~cberry/ La Jolla, San Diego
    ChuckB> 92093-0901

    ChuckB> ______________________________________________
    ChuckB> R-help at stat.math.ethz.ch mailing list
    ChuckB> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE
    ChuckB> do read the posting guide
    ChuckB> http://www.R-project.org/posting-guide.html and
    ChuckB> provide commented, minimal, self-contained,
    ChuckB> reproducible code.


From ripley at stats.ox.ac.uk  Thu Jan 18 18:21:29 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Jan 2007 17:21:29 +0000 (GMT)
Subject: [R] building R on freebsd 6.2 (amd64)
In-Reply-To: <307b90470701180902w39f38dbajf78d888b3efc5d65@mail.gmail.com>
References: <307b90470701180554x14290e41qbb78c0eefd5cc8f0@mail.gmail.com> 
	<Pine.WNT.4.64.0701181428290.2216@Petrel>
	<307b90470701180902w39f38dbajf78d888b3efc5d65@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701181719360.15275@gannet.stats.ox.ac.uk>

On Thu, 18 Jan 2007, Hiroyuki Kawakatsu wrote:

> On 1/18/07, Prof Brian D Ripley <ripley at stats.ox.ac.uk> wrote:
>> 
>> The R front end sets LD_LIBRARY_PATH to include the place it puts libR.so.
>> It looks like that is not effective on your machine, but you will have to
>> investigate for us why.  I am pretty sure other FreeBSD users (6.2 and 7)
>> on AMD64 reported success on R 2.4.x.
>
> echo returns
> LD_LIBRARY_PATH: Undefined variable.

But I said 'The R front end sets LD_LIBRARY_PATH', so you need to check 
this from inside R, e.g. by Sys.getenv().

>> How does FreeBSD 6.2 handle 64-bit systems?  Most OSes would use
>> /usr/local/lib64, and there is a configure setting LIBnn to cover that
>> issue.
>
> I have /usr/lib/, /usr/lib32/, /usr/local/lib/, but no /usr/local/lib64/
>
>> Another solution for ELF systems is to include R_HOME/lib in the list of
>> directories known to ldconfig.
>
> Thanks for that tip. I suppose if I symlink to R_HOME/lib/ I won't
> have to redo them every time I rebuild R-patched. Just as note to
> myself, the following appears to work
>
> ln -s /opt/acml3.6.0/gnu64/lib/libacml.so /usr/local/lib/libRblas.so
> ln -s /opt/acml3.6.0/gnu64/lib/libacml_mv.so /usr/local/lib/libacml_mv.so
> ln -s R_HOME/lib/libRlapack.so /usr/local/lib/libRlapack.so
>
> h.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Thu Jan 18 18:23:27 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Jan 2007 17:23:27 +0000 (GMT)
Subject: [R] options("digits") and print.default()
In-Reply-To: <45AFA915.5010805@vanderbilt.edu>
References: <fc.0087cdad000a4cad0087cdad000a4cad.a4cd5@emacs.lu>
	<Pine.LNX.4.64.0701151716060.5973@gannet.stats.ox.ac.uk>
	<45AFA915.5010805@vanderbilt.edu>
Message-ID: <Pine.LNX.4.64.0701181721420.15275@gannet.stats.ox.ac.uk>

On Thu, 18 Jan 2007, Charles Dupont wrote:

> Prof Brian Ripley wrote:
>> On Mon, 15 Jan 2007, Ulrich Keller wrote:
>> 
>>> Hello everyone,
>>> 
>>> I use latex() (Hmisc) for report generation and thus have been affected by
>>> the problem with rounding decimals described, for example, in this post:
>>> 
>>> http://thread.gmane.org/gmane.comp.lang.r.general/73287/focus=73287
>>> 
>>> In short, numbers often are printed with 15 or so decimals even if there
>>> far less significant digits. The problem has been confirmed by Frank
>>> Harrell and Thomas Dupont according to this post:
>>> 
>>> http://thread.gmane.org/gmane.comp.lang.r.general/73172/focus=73186
>>> 
>>> But it has still not been fixed. Rather than changing all my reports I
>>> decided I'd look at format.df() in Hmisc myself and try to fix the problem
>>> there. I found that the problem is not in Hmisc at all, but in R itself:
>>> 
>>>> print(1.001)
>>> [1] 1.001
>>>> options(digits=16) #format.df does this
>>>> print(1.001)
>>> [1] 1.001000000000000
>>>> print(round(1.001, 3)) #rounding does not help
>>> [1] 1.001000000000000
>>> 
>>> This does not seem to be the behaviour described in the documentation,
>>> which says:
>>> 
>>> "The same number of decimal places is used throughout a vector,[sic!] This
>>> means that digits specifies the minimum number of significant digits to be
>>> used, and that at least one entry will be encoded with that minimum
>>> number. However, if all the encoded elements then have trailing zeroes,
>>> the number of decimal places is reduced until at least one element has a
>>> non-zero final digit."
>>> 
>>> If print.default() exhibited the behaviour desribed in the docs,
>>> format.df() and thus latex() would work as advertised, I think. I would
>>> have written a bug report instead of posting here, but the fact (?) that
>>> Frank and Thomas have confirmed the bug seems to indicate that the problem
>>> does indeed lie with Hmisc. Am I misunderstanding something here?
>>> 
>>> I use R version 2.4.1 Patched (2007-01-13 r40470) on Windows.
>> 
>> The bug is that Hmisc uses more digits than the representation provides:
>> 
>>> .Machine$double.eps
>> [1] 2.22044604925031e-16
>> 
>> If it used 15 digits (as R does for as.character) all would be well. Since 
>> R has to do the calculations you quote in binary arithmetic, they are also 
>> subject to representation error and there is no way they can be done to 16 
>> significant figures.  (I've traced the example, and that is what happens.) 
>> See ?as.character for more details.
>> 
>
> Is there an R object that contains the value of DBL_DIG for the current 
> machine?  I have checked '.Machine' and '.Platform' and neither have the 
> value of DBL_DIG in them.

No, but please do as I suggested and read ?as.character: it is 15 on all 
current platforms.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From topkatz at msn.com  Thu Jan 18 20:43:30 2007
From: topkatz at msn.com (Talbot Katz)
Date: Thu, 18 Jan 2007 14:43:30 -0500
Subject: [R] Robust PCA?
Message-ID: <BAY132-F13CB1B12FF66301BA771D1AAAA0@phx.gbl>

Hi.

I'm checking into robust methods for principal components analysis.  There 
seem to be several floating around.  I'm currently focusing my attention on 
a method of Hubert, Rousseeuw, and Vanden Branden 
(http://wis.kuleuven.be/stat/Papers/robpca.pdf) mainly because I'm familiar 
with other work by Rousseeuw and Hubert in robust methodologies.  Of course, 
I'd like to obtain code for this method, or another good robust PCA method, 
if there's one out there.  I haven't noticed the existence on CRAN of a 
package for robust PCA (the authors of the ROBPCA method do provide MATLAB 
code).

--  TMK  --
212-460-5430	home
917-656-5351	cell


From tramni at abv.bg  Thu Jan 18 20:46:51 2007
From: tramni at abv.bg (Martin Ivanov)
Date: Thu, 18 Jan 2007 21:46:51 +0200 (GMT+02:00)
Subject: [R] weighted MDS, alscal
Message-ID: <77792419.295761169149611517.JavaMail.nobody@mail08.abv.bg>

Hello!
I need to perform weighted multidimensional scaling analysis(WMDS). I did rsitesearch, googled, but I could find no info on how to perform WMDS using R. In several places they say it is possible with the ALSCAL algorithm, but I could not find the relevant function to carry it out.


-----------------------------------------------------------------
???????? ?? ??????? ?? ??? ???????????!


From ssj1364 at gmail.com  Thu Jan 18 21:20:33 2007
From: ssj1364 at gmail.com (sj)
Date: Thu, 18 Jan 2007 13:20:33 -0700
Subject: [R] multiple seasonal time series models
Message-ID: <1c6126db0701181220i12ec5ad6oa07f9a1226f8bb55@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070118/65fbc1a1/attachment.pl 

From hi_ono2001 at ybb.ne.jp  Thu Jan 18 21:44:56 2007
From: hi_ono2001 at ybb.ne.jp (Hisaji ONO)
Date: Fri, 19 Jan 2007 05:44:56 +0900 (JST)
Subject: [R] weighted MDS, alscal
In-Reply-To: <77792419.295761169149611517.JavaMail.nobody@mail08.abv.bg>
Message-ID: <20070118204456.26214.qmail@web10908.mail.bbt.yahoo.co.jp>

Hello.


--- Martin Ivanov <tramni at abv.bg> wrote:

> I need to perform weighted multidimensional scaling
> analysis(WMDS). I did rsitesearch, googled, but I
> could find no info on how to perform WMDS using R.
> In several places they say it is possible with the
> ALSCAL algorithm, but I could not find the relevant
> function to carry it out.
>

 I dont't know whether ALSCAL is available for R.

 However there's ALSCAL fortran code available in 
http://forrest.psych.unc.edu/research/alscal.html.


 Regards.


From roberto.perdisci at gmail.com  Thu Jan 18 22:39:44 2007
From: roberto.perdisci at gmail.com (Roberto Perdisci)
Date: Thu, 18 Jan 2007 16:39:44 -0500
Subject: [R] Laplace correction for rpart class probability estimate
Message-ID: <cf94d0090701181339s43ef7202rd608a219e1a1a3f5@mail.gmail.com>

Hello everybody,
  I'm using rpart to fit a classification tree. I'm interested in the
way rpart estimates the class membership probabilities. Does it
implement the Laplace correction rule? Is there any parameter I can
use to ask rpart to do that?
I was not able to find this option in the manual or on the internet.

thank you,
regards,
Roberto


From jsorkin at grecc.umaryland.edu  Thu Jan 18 23:23:21 2007
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Thu, 18 Jan 2007 17:23:21 -0500
Subject: [R] selecting rows for inclusion in lm
Message-ID: <45AFAD09.A712.00CB.0@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070118/1d8d2b96/attachment.pl 

From GPetris at uark.edu  Fri Jan 19 00:00:36 2007
From: GPetris at uark.edu (Giovanni Petris)
Date: Thu, 18 Jan 2007 17:00:36 -0600 (CST)
Subject: [R] Reading contingency tables
Message-ID: <200701182300.l0IN0axv007115@definetti.ddns.uark.edu>


I am trying to read an ftable using read.ftable, but I get the
following error message:

> jobSatTable <- read.ftable("http://definetti.uark.edu/~gpetris/stat5333/jobSatisfaction.dat",skip=2)
Error in seek(file, where = 0) : no applicable method for "seek"
In addition: Warning messages:
1: no non-missing arguments to max; returning -Inf 
2: no non-missing arguments to max; returning -Inf 


I also tried to play with the argument col.vars, to remove blank
lines in the file, to remove the blank in "Not
satisfied". Unfortunately nothing worked. 

Can anybody give me a suggestion on how to read that table?

Thanks in advance,
Giovanni

ps:

> version
               _                           
platform       sparc-sun-solaris2.8        
arch           sparc                       
os             solaris2.8                  
system         sparc, solaris2.8           
status                                     
major          2                           
minor          4.1                         
year           2006                        
month          12                          
day            18                          
svn rev        40228                       
language       R                           
version.string R version 2.4.1 (2006-12-18)

-- 

Giovanni Petris  <GPetris at uark.edu>
Associate Professor
Department of Mathematical Sciences
University of Arkansas - Fayetteville, AR 72701
Ph: (479) 575-6324, 575-8630 (fax)
http://definetti.uark.edu/~gpetris/


From cberry at tajo.ucsd.edu  Fri Jan 19 00:23:43 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Thu, 18 Jan 2007 15:23:43 -0800
Subject: [R] sleep data
In-Reply-To: <Pine.LNX.4.64.0701171043310.27414@tajo.ucsd.edu>
References: <45ADEE46.6000803@psych.uib.no>
	<Pine.LNX.4.64.0701171043310.27414@tajo.ucsd.edu>
Message-ID: <Pine.LNX.4.64.0701181431380.10704@tajo.ucsd.edu>


A further note. The Cushny & Peebles article can be viewed here:

 	http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1465734

and the page with the data is here:

 	http://www.pubmedcentral.nih.gov/pagerender.fcgi?artid=1465734&pageindex=9

A machine readable version of the data is at:

 	https://biostat.ucsd.edu/~cberry/t-test/sleep.dat

The version published in Student's Biometrika article has a typographical 
error, but it is evident that Student's computations were unaffected.

Chuck Berry

On Wed, 17 Jan 2007, Charles C. Berry wrote:

>
> Yes, you refer to
>
> 	 Cushny, A. R. and Peebles, A. R. The action of optical isomers: II
> 	 hyoscines. The Journal of Physiology, 1905, 32: 501.510.
>
> which was used by 'Student' to illustrate the paired t-test.
>
> This is indeed a crossover design.
>
> On Wed, 17 Jan 2007, Tom Backer Johnsen wrote:
>
>>  When reading the documentation for the "sleep" data set in R, the
>>  impression is clear, this is an "independent groups" kind of design
>>  (two groups of 10 subjects each).  However, when browsing the original
>>  article (referred to in the help file), my impression is quite clear,
>>  this is really a "repeated measures" kind of data (one group of 10
>>  subjects, two observations).  What is correct?
>>
>>  Tom
>>
>>  ______________________________________________
>>  R-help at stat.math.ethz.ch mailing list
>>  https://stat.ethz.ch/mailman/listinfo/r-help
>>  PLEASE do read the posting guide
>>  http://www.R-project.org/posting-guide.html
>>  and provide commented, minimal, self-contained, reproducible code.
>> 
>
> Charles C. Berry                        (858) 534-2098
>                                         Dept of Family/Preventive Medicine
> E mailto:cberry at tajo.ucsd.edu	         UC San Diego
> http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901
>
>
>
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901


From gunter.berton at gene.com  Fri Jan 19 00:28:47 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Thu, 18 Jan 2007 15:28:47 -0800
Subject: [R] Robust PCA?
In-Reply-To: <BAY132-F13CB1B12FF66301BA771D1AAAA0@phx.gbl>
Message-ID: <00c001c73b58$67a33ae0$4d908980@gne.windows.gene.com>

You seem not to have received a reply.  

You can use cov.rob in MASS or cov.Mcd in robustbase or undoubtedly others
to obtain a robust covariance matrix and then use that for PCA. 

-- Bert


Bert Gunter
Nonclinical Statistics
7-7374

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Talbot Katz
Sent: Thursday, January 18, 2007 11:44 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Robust PCA?

Hi.

I'm checking into robust methods for principal components analysis.  There 
seem to be several floating around.  I'm currently focusing my attention on 
a method of Hubert, Rousseeuw, and Vanden Branden 
(http://wis.kuleuven.be/stat/Papers/robpca.pdf) mainly because I'm familiar 
with other work by Rousseeuw and Hubert in robust methodologies.  Of course,

I'd like to obtain code for this method, or another good robust PCA method, 
if there's one out there.  I haven't noticed the existence on CRAN of a 
package for robust PCA (the authors of the ROBPCA method do provide MATLAB 
code).

--  TMK  --
212-460-5430	home
917-656-5351	cell

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From topkatz at msn.com  Fri Jan 19 00:57:54 2007
From: topkatz at msn.com (Talbot Katz)
Date: Thu, 18 Jan 2007 18:57:54 -0500
Subject: [R] Robust PCA?
In-Reply-To: <00c001c73b58$67a33ae0$4d908980@gne.windows.gene.com>
Message-ID: <BAY132-F204778CA5E9D749C2464ACAAAA0@phx.gbl>

Hi Bert.

Thank you, that sounds like an excellent idea.  After my initial post, I 
also found an implementation of ROBPCA in S-PLUS at 
(http://wis.kuleuven.be/stat/robust/programs.html).

--  TMK  --
212-460-5430	home
917-656-5351	cell


>From: Bert Gunter <gunter.berton at gene.com>
>To: "'Talbot Katz'" <topkatz at msn.com>, <r-help at stat.math.ethz.ch>
>Subject: RE: [R] Robust PCA?
>Date: Thu, 18 Jan 2007 15:28:47 -0800
>
>You seem not to have received a reply.
>
>You can use cov.rob in MASS or cov.Mcd in robustbase or undoubtedly others
>to obtain a robust covariance matrix and then use that for PCA.
>
>-- Bert
>
>
>Bert Gunter
>Nonclinical Statistics
>7-7374
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Talbot Katz
>Sent: Thursday, January 18, 2007 11:44 AM
>To: r-help at stat.math.ethz.ch
>Subject: [R] Robust PCA?
>
>Hi.
>
>I'm checking into robust methods for principal components analysis.  There
>seem to be several floating around.  I'm currently focusing my attention on
>a method of Hubert, Rousseeuw, and Vanden Branden
>(http://wis.kuleuven.be/stat/Papers/robpca.pdf) mainly because I'm familiar
>with other work by Rousseeuw and Hubert in robust methodologies.  Of 
>course,
>
>I'd like to obtain code for this method, or another good robust PCA method,
>if there's one out there.  I haven't noticed the existence on CRAN of a
>package for robust PCA (the authors of the ROBPCA method do provide MATLAB
>code).
>
>--  TMK  --
>212-460-5430	home
>917-656-5351	cell
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>


From rdporto1 at terra.com.br  Fri Jan 19 01:17:00 2007
From: rdporto1 at terra.com.br (rdporto1)
Date: Thu, 18 Jan 2007 21:17:00 -0300
Subject: [R] integrate and quadratic forms
Message-ID: <JC3A4C$7A93B236CCC7C0B6F4DF8CBF1A16BE89@terra.com.br>

Hi all.

I'm trying to numerically invert the characteristic function
of a quadratic form following Imhof's (1961, Biometrika 48)
procedure.

The parameters are:

lambda=c(.6,.3,.1)
h=c(2,2,2)
sigma=c(0,0,0)
q=3

I've implemented Imhof's procedure two ways that, for me,
should give the same answer:

#more legible
integral1 = function(u) {
  o=(1/2)*sum(h*atan(lambda*u)+sigma^2*lambda*u/(1+lambda^2*u^2)) - q*u/2
  rho=prod((1+lambda^2*u^2)^(h/4))*exp( (1/2)*sum((sigma*lambda*u)^2/(1+lambda^2*u^2)) )
  integrand = sin(o)/(u*rho) 
}

#same as above
integral2= function(u) {
((1/2)*sum(h*atan(lambda*u)+sigma^2*lambda*u/(1+lambda^2*u^2)) - q*u/2)/
(u*(prod((1+lambda^2*u^2)^(h/4))*
exp( (1/2)*sum((sigma*lambda*u)^2/(1+lambda^2*u^2)) )))
}

The following should be near 0.18. However, nor the answers are near this
value neither they agree each other!

> 1/2+(1/pi)*integrate(integral1,0,Inf)$value
[1] 1.022537
> 1/2+(1/pi)*integrate(integral2,0,Inf)$value
[1] 1.442720

What's happening? Is this a bug or OS specific? Shouldn't they give the 
same answer? Why do I get results so different from 0.18? In time:
the procedure works fine for q=.2.

I'm running R 2.4.1 in a PC with Windows XP 32bits. Other ways (in R) to
find the distribution of general quadratic forms are welcome.

Thanks in advance.

Rogerio.


From jholtman at gmail.com  Fri Jan 19 01:42:38 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 18 Jan 2007 19:42:38 -0500
Subject: [R] integrate and quadratic forms
In-Reply-To: <JC3A4C$7A93B236CCC7C0B6F4DF8CBF1A16BE89@terra.com.br>
References: <JC3A4C$7A93B236CCC7C0B6F4DF8CBF1A16BE89@terra.com.br>
Message-ID: <644e1f320701181642q7388c96fpc669d7c1ca2d994d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070118/e91f9645/attachment.pl 

From jholtman at gmail.com  Fri Jan 19 02:57:41 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 18 Jan 2007 20:57:41 -0500
Subject: [R] Problem with loading tkrplot
Message-ID: <644e1f320701181757k13a6df5byeb3f183d1c67f4db@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070118/ff79e55e/attachment.pl 

From murdoch at stats.uwo.ca  Fri Jan 19 03:09:20 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 18 Jan 2007 21:09:20 -0500
Subject: [R] Problem with loading tkrplot
In-Reply-To: <644e1f320701181757k13a6df5byeb3f183d1c67f4db@mail.gmail.com>
References: <644e1f320701181757k13a6df5byeb3f183d1c67f4db@mail.gmail.com>
Message-ID: <45B02850.1020803@stats.uwo.ca>

On 1/18/2007 8:57 PM, jim holtman wrote:
> I asked the maintainer and he indicated that it might be a dll problem.  Has
> anyone see this problem?  I had installed the latest copy of tkrplot.  I was
> trying to use teachingDemos when I first got the error.

I just installed tkrplot in R 2.4.1 and it worked fine.  I've never seen 
the error you're reporting.  However, I see what looks like a typo that 
might affect some systems:

In the tkrplot .First.lib function, I see

>         if (is.character(.Platform$r_arch) && .Platform$r_arch != 
>             "") 
>             path <- file.path("libs", .Platform$r_arc, dlname)
>         else path <- file.path("libs", dlname)

On the 3rd line, I think .Platform$r_arc should be .Platform$r_arch.  On 
my system I never execute that line, but if .Platform$r_arch is set on 
your system, it would likely cause problems.

Duncan Murdoch

> 
> On 1/18/07, Luke Tierney <luke at stat.uiowa.edu> wrote:
> 
>> Looks like tcl can't find the dll or something along those lines. As
>> this is windows you'll have to ask someone who uses that platform
>> to help figure out why.
>>
>> luke
>>
>> On Thu, 18 Jan 2007, jim holtman wrote:
>>
>>> I was trying to use the teachingDemos package and was getting an error
>>> trying to load tkrplot.
>>>
>>> I downloaded the latest copy and here is what I am getting on the
>> console.
>>> You were mentioned as the maintainer on the manual page.  Do you know
>> why
>>> this error might be occuring?
>>>
>>>
>>>
>>>> utils:::menuInstallPkgs()
>>> --- Please select a CRAN mirror for use in this session ---
>>> trying URL '
>>> http://www.stathy.com/cran/bin/windows/contrib/2.4/tkrplot_0.0-16.zip'
>>> Content type 'application/zip' length 24119 bytes
>>> opened URL
>>> downloaded 23Kb
>>>
>>> package 'tkrplot' successfully unpacked and MD5 sums checked
>>>
>>> The downloaded packages are in
>>>       C:\Documents and Settings\Compaq_Administrator\Local
>>> Settings\Temp\RtmpqEShrb\downloaded_packages
>>> updating HTML package descriptions
>>>> library(tkrplot)
>>> Loading required package: tcltk
>>> Loading Tcl/Tk interface ... done
>>> Error in structure(.External("dotTcl", ..., PACKAGE = "tcltk"), class =
>>> "tclObj") :
>>>       [tcl] could not find interpreter "Rplot".
>>> Error in library(tkrplot) : .First.lib failed for 'tkrplot'
>>>> sessionInfo()
>>> R version 2.4.1 (2006-12-18)
>>> i386-pc-mingw32
>>>
>>> locale:
>>> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
>>> States.1252;LC_MONETARY=English_United
>>> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>>>
>>> attached base packages:
>>> [1] "tcltk"     "stats"     "utils"
>> "datasets"  "graphics"  "grDevices"
>>> "methods"   "base"
>>>
>>>
> 
> 
>


From f.harrell at vanderbilt.edu  Fri Jan 19 03:30:30 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 18 Jan 2007 20:30:30 -0600
Subject: [R] kate editor for R
Message-ID: <45B02D46.1090709@vanderbilt.edu>

Like kile for LaTeX, Linux/KDE's kate editor is an excellent editor for 
R, with easy code submission to a running R process.  Syntax 
highlighting is good.  I have  not been able to figure out two things:

- how to automatically reformat a line or region of text using good 
indentation rules (Emacs/ESS make this so easy by just hitting Tab while 
the cursor is in a line, or highlighting a region and hitting Esq q)

- how to cause auto-indenting as you type braces.  For me, kate puts a { 
in column one

Thanks for any pointers.

Frank
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From hpbenton at scripps.edu  Fri Jan 19 04:02:49 2007
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Thu, 18 Jan 2007 19:02:49 -0800
Subject: [R] if else statement
Message-ID: <45B034D9.6010702@scripps.edu>

Hello,

    I'm doing some scripting and I've noticed that R doesn't seem to
have an
if (cond){
    do
}ifelse (cond) {
    do
} else {
    do
}

type block.

Is this correct or am I missing something.

THX

Paul

-- 
Research Technician
Mass Spectrometry
   o The
  /
o Scripps
  \
   o Research
  /
o Institute


From statba at nus.edu.sg  Fri Jan 19 04:13:29 2007
From: statba at nus.edu.sg (Berwin A Turlach)
Date: Fri, 19 Jan 2007 11:13:29 +0800
Subject: [R] How to optimize this loop ? (correction)
In-Reply-To: <45AF828F.3020500@mx.uni-saarland.de>
References: <45AF8954.18733.1CBE556@localhost>
	<45AF8135.4070500@mx.uni-saarland.de>
	<45AF828F.3020500@mx.uni-saarland.de>
Message-ID: <20070119111329.322a7000@berwin5>

G'day Martin and others,

On Thu, 18 Jan 2007 15:22:07 +0100
Martin Becker <martin.becker at mx.uni-saarland.de> wrote:

> Sorry, I accidentaly lost one line of the function code (result <-0), 
> see below...

But even with that line, the code doesn't work properly. :)
Since you changed the order of the for loop (from "i in 1:(len-1)" to
"i in (len-1):1") you broke the logic of the code and the result
returned is wrong (should be something like "result <- len-i" instead
of "result <- i-1").

I agree that your function is faster than the one based on rle(),
presumably because rle() does just too many additional computations
that are not necessary to solve the problem at hand.  But I still don't
like to see for() loops in R code. :)  I would suggest the following
solution (which is only marginally slower than yours on these example
inputs):

myfun2 <- function(x){
  len <- length(x)
  which.max(c(x[len] <= rev(x[-len]), TRUE) ) - 1
}

The concatenation of TRUE at the end of the logical vector is necessary
for the case that all elements before the last are smaller than the
last one.  It also has the side benefit that myfun(2) will return 0
instead of numeric(0) (or something similar).  On my machine:

> system.time(for (i in 1:10000) erg<-my_fun(c(3, 4, 10,14,8,3,4,6,9)))
[1] 2.860 0.016 2.950 0.000 0.000
> system.time(for (i in 1:10000) erg<-myfun1(c(3, 4, 10,14,8,3,4,6,9)))
[1] 0.256 0.000 0.256 0.000 0.000
> system.time(for (i in 1:10000) erg<-myfun2(c(3, 4, 10,14,8,3,4,6,9)))
[1] 0.280 0.000 0.283 0.000 0.000

Cheers,

	Berwin

=========================== Full address =============================
Berwin A Turlach                            Tel.: +65 6515 4416 (secr)        
Dept of Statistics and Applied Probability        +65 6515 6650 (self)        
Faculty of Science                          FAX : +65 6872 3919               
National University of Singapore     
6 Science Drive 2, Blk S16, Level 7          e-mail: statba at nus.edu.sg
Singapore 117546                    http://www.stat.nus.edu.sg/~statba


From spluque at gmail.com  Fri Jan 19 04:21:14 2007
From: spluque at gmail.com (Sebastian P. Luque)
Date: Thu, 18 Jan 2007 21:21:14 -0600
Subject: [R] if else statement
References: <45B034D9.6010702@scripps.edu>
Message-ID: <87fya7vh05.fsf@patagonia.sebmags.homelinux.org>

On Thu, 18 Jan 2007 19:02:49 -0800,
"H. Paul Benton" <hpbenton at scripps.edu> wrote:

> Hello,

>     I'm doing some scripting and I've noticed that R doesn't seem to
> have an
> if (cond){
>     do
> }ifelse (cond) {
>     do
> } else {
>     do
> }

> type block.


It's legal to join if else statements together:


if (cond) {
    do X
} else if (cond) {
    do Y
} else if (cond) {
    do Z
} else do W


-- 
Seb


From jholtman at gmail.com  Fri Jan 19 04:32:00 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 18 Jan 2007 22:32:00 -0500
Subject: [R] Problem with loading tkrplot
In-Reply-To: <45B02850.1020803@stats.uwo.ca>
References: <644e1f320701181757k13a6df5byeb3f183d1c67f4db@mail.gmail.com>
	<45B02850.1020803@stats.uwo.ca>
Message-ID: <644e1f320701181932n4b8b7a7s16904ace069a1882@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070118/9a4148ca/attachment.pl 

From marc_schwartz at comcast.net  Fri Jan 19 04:59:07 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 18 Jan 2007 21:59:07 -0600
Subject: [R] kate editor for R
In-Reply-To: <45B02D46.1090709@vanderbilt.edu>
References: <45B02D46.1090709@vanderbilt.edu>
Message-ID: <1169179147.5017.15.camel@localhost.localdomain>

On Thu, 2007-01-18 at 20:30 -0600, Frank E Harrell Jr wrote:
> Like kile for LaTeX, Linux/KDE's kate editor is an excellent editor for 
> R, with easy code submission to a running R process.  Syntax 
> highlighting is good.  I have  not been able to figure out two things:
> 
> - how to automatically reformat a line or region of text using good 
> indentation rules (Emacs/ESS make this so easy by just hitting Tab while 
> the cursor is in a line, or highlighting a region and hitting Esq q)
> 
> - how to cause auto-indenting as you type braces.  For me, kate puts a { 
> in column one
> 
> Thanks for any pointers.

Frank,

I'm not sure if what I found provides an optimistic outlook, however,
you might find the following helpful:

See the questions:

  Can I use different indenters for different files?

  How can I get KatePart to autoindent XXX code?

at http://kate-editor.org/faq.  KatePart is the text editing component
in Kate.

In the two answers, there are links to articles that provide additional
insight into implementing indentation functionality in Kate.

HTH,

Marc


From fshen at ufl.edu  Fri Jan 19 05:30:06 2007
From: fshen at ufl.edu (SHEN,FENG)
Date: Thu, 18 Jan 2007 23:30:06 -0500 (EST)
Subject: [R] ability estimate with GRM of IRT
Message-ID: <506954077.187451169181006194.JavaMail.osg@osgjas01.cns.ufl.edu>

Hi my friends,

I have an issue with ability estimates when running GRM of IRT.  I 
have responses from 242 subjects but got 183 ability estimates.  
Below is what I did to get the estimates.

1) I have a csv file "P1.csv" and I imported it into R and loaded 
the "ltm" package by doing:
p1<-read.table("P1.csv",header=TRUE,sep=",")
library(ltm)

2) I created a subset that included columns 2 to 9 for the 
analysis by doing:
s1<-p1[,2:9]

3) I converted the subset into data.frame format by doing:
s1df=data.frame(s1)

4) I checked the descriptive stats for the s1df by doing:
ds1df=descript(s1df)
ds1df

And it was confirmed that 242 subjects' responses were imported.

5)I ran GRM on the s1df dataset by doing:
grm<-grm(s1df, Hessian=T)
grm

6)Finally, I ran the ability estimated by doing:
aes1=factor.scores(grm)
aes1

And I got 183 factor-scores for observed response patterns.  
Besides, the 183 estimates are not for the first 183 subjects of 
the 242 because the response patterns of No 1 on the factor-scores 
list do not match those of No 1 in the s1 dataset and the same 
thing holds true for the rest of the response patterns.

Could you help me find out what the problem is?

Many thanks in advance!

Feng


From ripley at stats.ox.ac.uk  Fri Jan 19 07:05:22 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Jan 2007 06:05:22 +0000 (GMT)
Subject: [R] Problem with loading tkrplot
In-Reply-To: <644e1f320701181757k13a6df5byeb3f183d1c67f4db@mail.gmail.com>
References: <644e1f320701181757k13a6df5byeb3f183d1c67f4db@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701190600240.28506@gannet.stats.ox.ac.uk>

As it happens I used TeachingDemos/tkrplot on Windows (R 2.4.1) with a 
class example yesterday.

It looks to me as if the issue is in the R installation. Remember that Tcl 
support is optional when R is installed, and can be influenced by 
environment variables.  So my first check would be that the tcltk demos 
all work.

On Thu, 18 Jan 2007, jim holtman wrote:

> I asked the maintainer and he indicated that it might be a dll problem.  Has
> anyone see this problem?  I had installed the latest copy of tkrplot.  I was
> trying to use teachingDemos when I first got the error.
>
> On 1/18/07, Luke Tierney <luke at stat.uiowa.edu> wrote:
>
>> Looks like tcl can't find the dll or something along those lines. As
>> this is windows you'll have to ask someone who uses that platform
>> to help figure out why.
>>
>> luke
>>
>> On Thu, 18 Jan 2007, jim holtman wrote:
>>
>>> I was trying to use the teachingDemos package and was getting an error
>>> trying to load tkrplot.
>>>
>>> I downloaded the latest copy and here is what I am getting on the
>> console.
>>> You were mentioned as the maintainer on the manual page.  Do you know
>> why
>>> this error might be occuring?
>>>
>>>
>>>
>>>> utils:::menuInstallPkgs()
>>> --- Please select a CRAN mirror for use in this session ---
>>> trying URL '
>>> http://www.stathy.com/cran/bin/windows/contrib/2.4/tkrplot_0.0-16.zip'
>>> Content type 'application/zip' length 24119 bytes
>>> opened URL
>>> downloaded 23Kb
>>>
>>> package 'tkrplot' successfully unpacked and MD5 sums checked
>>>
>>> The downloaded packages are in
>>>       C:\Documents and Settings\Compaq_Administrator\Local
>>> Settings\Temp\RtmpqEShrb\downloaded_packages
>>> updating HTML package descriptions
>>>> library(tkrplot)
>>> Loading required package: tcltk
>>> Loading Tcl/Tk interface ... done
>>> Error in structure(.External("dotTcl", ..., PACKAGE = "tcltk"), class =
>>> "tclObj") :
>>>       [tcl] could not find interpreter "Rplot".
>>> Error in library(tkrplot) : .First.lib failed for 'tkrplot'
>>>> sessionInfo()
>>> R version 2.4.1 (2006-12-18)
>>> i386-pc-mingw32
>>>
>>> locale:
>>> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
>>> States.1252;LC_MONETARY=English_United
>>> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>>>
>>> attached base packages:
>>> [1] "tcltk"     "stats"     "utils"
>> "datasets"  "graphics"  "grDevices"
>>> "methods"   "base"
>>>>
>>>
>>>
>>>
>
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From roboso at gmail.com  Fri Jan 19 07:14:31 2007
From: roboso at gmail.com (Osorio Roberto)
Date: Fri, 19 Jan 2007 01:14:31 -0500
Subject: [R] Vectorize rearrangement within each column
Message-ID: <967DF70E-C70A-46B9-B438-AEA46D277FB6@gmail.com>

Consider a matrix like

 > ma = matrix(10:15, nr = 3)
 > ma
      [,1] [,2]
[1,]   10   13
[2,]   11   14
[3,]   12   15

I want to rearrange each column according to row indexes (1 to 3)  
given in another matrix, as in

 > idx = matrix(c(1,3,2, 2,3,1), nr = 3)
 > idx
      [,1] [,2]
[1,]    1    2
[2,]    3    3
[3,]    2    1

The new matrix mb will have for each column the corresponding column  
of ma indexed by the corresponding column of idx, as in

 > mb = ma
 > for (j in 1:2) mb[,j] = ma[idx[,j], j]	
 > mb
      [,1] [,2]
[1,]   10   14
[2,]   12   15
[3,]   11   13

Can I avoid the for() loop? I'm specially interested to find out if a  
fast implementation using lapply() would be feasible for large input  
matrices (analogues of ma and idx) transformed into data frames.

Roberto Osorio


From ripley at stats.ox.ac.uk  Fri Jan 19 07:15:50 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Jan 2007 06:15:50 +0000 (GMT)
Subject: [R] Reading contingency tables
In-Reply-To: <200701182300.l0IN0axv007115@definetti.ddns.uark.edu>
References: <200701182300.l0IN0axv007115@definetti.ddns.uark.edu>
Message-ID: <Pine.LNX.4.64.0701190611440.28506@gannet.stats.ox.ac.uk>

You cannot seek on a url() connection.  Try downloading the file and 
reading it locally.

There's a bug in the function which forgets to open its temporary file: I 
will attend to that.

On Thu, 18 Jan 2007, Giovanni Petris wrote:

>
> I am trying to read an ftable using read.ftable, but I get the
> following error message:
>
>> jobSatTable <- read.ftable("http://definetti.uark.edu/~gpetris/stat5333/jobSatisfaction.dat",skip=2)
> Error in seek(file, where = 0) : no applicable method for "seek"
> In addition: Warning messages:
> 1: no non-missing arguments to max; returning -Inf
> 2: no non-missing arguments to max; returning -Inf
>
>
> I also tried to play with the argument col.vars, to remove blank
> lines in the file, to remove the blank in "Not
> satisfied". Unfortunately nothing worked.
>
> Can anybody give me a suggestion on how to read that table?
>
> Thanks in advance,
> Giovanni
>
> ps:
>
>> version
>               _
> platform       sparc-sun-solaris2.8
> arch           sparc
> os             solaris2.8
> system         sparc, solaris2.8
> status
> major          2
> minor          4.1
> year           2006
> month          12
> day            18
> svn rev        40228
> language       R
> version.string R version 2.4.1 (2006-12-18)
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Jan 19 08:08:21 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Jan 2007 07:08:21 +0000 (GMT)
Subject: [R] Reading contingency tables
In-Reply-To: <Pine.LNX.4.64.0701190611440.28506@gannet.stats.ox.ac.uk>
References: <200701182300.l0IN0axv007115@definetti.ddns.uark.edu>
	<Pine.LNX.4.64.0701190611440.28506@gannet.stats.ox.ac.uk>
Message-ID: <Pine.LNX.4.64.0701190707250.17748@gannet.stats.ox.ac.uk>

On Fri, 19 Jan 2007, Prof Brian Ripley wrote:

> You cannot seek on a url() connection.  Try downloading the file and reading 
> it locally.

You will need to edit it to fit the ftable format as well.

> There's a bug in the function which forgets to open its temporary file: I 
> will attend to that.

It was a bit worse than just that: fixed in 2.4.1 patched.

>
> On Thu, 18 Jan 2007, Giovanni Petris wrote:
>
>> 
>> I am trying to read an ftable using read.ftable, but I get the
>> following error message:
>> 
>>> jobSatTable <- 
>>> read.ftable("http://definetti.uark.edu/~gpetris/stat5333/jobSatisfaction.dat",skip=2)
>> Error in seek(file, where = 0) : no applicable method for "seek"
>> In addition: Warning messages:
>> 1: no non-missing arguments to max; returning -Inf
>> 2: no non-missing arguments to max; returning -Inf
>> 
>> 
>> I also tried to play with the argument col.vars, to remove blank
>> lines in the file, to remove the blank in "Not
>> satisfied". Unfortunately nothing worked.
>> 
>> Can anybody give me a suggestion on how to read that table?
>> 
>> Thanks in advance,
>> Giovanni
>> 
>> ps:
>> 
>>> version
>>               _
>> platform       sparc-sun-solaris2.8
>> arch           sparc
>> os             solaris2.8
>> system         sparc, solaris2.8
>> status
>> major          2
>> minor          4.1
>> year           2006
>> month          12
>> day            18
>> svn rev        40228
>> language       R
>> version.string R version 2.4.1 (2006-12-18)
>> 
>> 
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Bill.Venables at csiro.au  Fri Jan 19 09:00:05 2007
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Fri, 19 Jan 2007 18:00:05 +1000
Subject: [R] Vectorize rearrangement within each column
References: <967DF70E-C70A-46B9-B438-AEA46D277FB6@gmail.com>
Message-ID: <B998A44C8986644EA8029CFE6396A924840BCB@exqld2-bne.qld.csiro.au>

As with most things like this, you can trade memory for speed.  Here is
an obfuscated solution that appears to eschew loops entirely.

> ma <- matrix(10:15, nr = 3)
> idx <- matrix(c(1,3,2, 2,3,1), nr = 3)
> mb <- ma
> mb[] <- as.vector(ma)[as.vector(idx + 
	outer(rep(nrow(ma), nrow(ma)), 1:ncol(ma)-1, '*'))]
> mb
     [,1] [,2]
[1,]   10   14
[2,]   12   15
[3,]   11   13

Ordinarily, though, my preferred solution would be the for() loop.

Bill Venables
CMIS, CSIRO Laboratories,
PO Box 120, Cleveland, Qld. 4163
AUSTRALIA
Office Phone (email preferred): +61 7 3826 7251
Fax (if absolutely necessary):    +61 7 3826 7304
Mobile (rarely used):                +61 4 1963 4642
Home Phone:                          +61 7 3286 7700
mailto:Bill.Venables at csiro.au
http://www.cmis.csiro.au/bill.venables/ 
 
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Osorio Roberto
Sent: Friday, 19 January 2007 4:15 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Vectorize rearrangement within each column

Consider a matrix like

 > ma = matrix(10:15, nr = 3)
 > ma
      [,1] [,2]
[1,]   10   13
[2,]   11   14
[3,]   12   15

I want to rearrange each column according to row indexes (1 to 3)  
given in another matrix, as in

 > idx = matrix(c(1,3,2, 2,3,1), nr = 3)
 > idx
      [,1] [,2]
[1,]    1    2
[2,]    3    3
[3,]    2    1

The new matrix mb will have for each column the corresponding column  
of ma indexed by the corresponding column of idx, as in

 > mb = ma
 > for (j in 1:2) mb[,j] = ma[idx[,j], j]	
 > mb
      [,1] [,2]
[1,]   10   14
[2,]   12   15
[3,]   11   13

Can I avoid the for() loop? I'm specially interested to find out if a  
fast implementation using lapply() would be feasible for large input  
matrices (analogues of ma and idx) transformed into data frames.

Roberto Osorio

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dimitris.rizopoulos at med.kuleuven.be  Fri Jan 19 09:20:34 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 19 Jan 2007 09:20:34 +0100
Subject: [R] ability estimate with GRM of IRT
References: <506954077.187451169181006194.JavaMail.osg@osgjas01.cns.ufl.edu>
Message-ID: <009101c73ba2$b0db47d0$0540210a@www.domain>

you need to use the 'resp.patterns' argument of factor.scores(); look 
at ?factor.scores for more info, e.g.,

library(ltm)

fit <- grm(Environment)
factor.scores(fit)
factor.scores(fit, resp.patterns = Environment)


I hope it helps.

Best,
Dimitris

ps, it'd be better not to call your fitted GRM model 'grm'; check

library(fortunes)
fortune("dog")

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "SHEN,FENG" <fshen at ufl.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, January 19, 2007 5:30 AM
Subject: [R] ability estimate with GRM of IRT


> Hi my friends,
>
> I have an issue with ability estimates when running GRM of IRT.  I
> have responses from 242 subjects but got 183 ability estimates.
> Below is what I did to get the estimates.
>
> 1) I have a csv file "P1.csv" and I imported it into R and loaded
> the "ltm" package by doing:
> p1<-read.table("P1.csv",header=TRUE,sep=",")
> library(ltm)
>
> 2) I created a subset that included columns 2 to 9 for the
> analysis by doing:
> s1<-p1[,2:9]
>
> 3) I converted the subset into data.frame format by doing:
> s1df=data.frame(s1)
>
> 4) I checked the descriptive stats for the s1df by doing:
> ds1df=descript(s1df)
> ds1df
>
> And it was confirmed that 242 subjects' responses were imported.
>
> 5)I ran GRM on the s1df dataset by doing:
> grm<-grm(s1df, Hessian=T)
> grm
>
> 6)Finally, I ran the ability estimated by doing:
> aes1=factor.scores(grm)
> aes1
>
> And I got 183 factor-scores for observed response patterns.
> Besides, the 183 estimates are not for the first 183 subjects of
> the 242 because the response patterns of No 1 on the factor-scores
> list do not match those of No 1 in the s1 dataset and the same
> thing holds true for the rest of the response patterns.
>
> Could you help me find out what the problem is?
>
> Many thanks in advance!
>
> Feng
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From Bernhard_Pfaff at fra.invesco.com  Fri Jan 19 10:44:20 2007
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Fri, 19 Jan 2007 09:44:20 -0000
Subject: [R] problem in adf command
In-Reply-To: <300094.85637.qm@web34607.mail.mud.yahoo.com>
References: <300094.85637.qm@web34607.mail.mud.yahoo.com>
Message-ID: <E4A9111DA23BA048B9A46686BF727CF461BEE5@DEFRAXMB01.corp.amvescap.net>

>that is, equation with constant and trend is used.if i did not include 
>
>constant or trend in the equation and run the
>
>command then how i can run this command in tseries.
>
> 
Dear Zahid,

you can employ ur.df() in package 'urca' or the wrapped functions from
'urca' contained in package 'fSeries', see ?ur.df and ?UnitrootTests for
more information, respectively.

Best,
Bernhard
*****************************************************************
Confidentiality Note: The information contained in this mess...{{dropped}}


From rdiaz at cnio.es  Fri Jan 19 11:18:26 2007
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Fri, 19 Jan 2007 11:18:26 +0100
Subject: [R] kate editor for R
In-Reply-To: <45B02D46.1090709@vanderbilt.edu>
References: <45B02D46.1090709@vanderbilt.edu>
Message-ID: <200701191118.26761.rdiaz@cnio.es>

On Friday 19 January 2007 03:30, Frank E Harrell Jr wrote:
> Like kile for LaTeX, Linux/KDE's kate editor is an excellent editor for
> R, with easy code submission to a running R process.  Syntax
> highlighting is good.  I have  not been able to figure out two things:
>
> - how to automatically reformat a line or region of text using good
> indentation rules (Emacs/ESS make this so easy by just hitting Tab while
> the cursor is in a line, or highlighting a region and hitting Esq q)
>
> - how to cause auto-indenting as you type braces.  For me, kate puts a {
> in column one
>
> Thanks for any pointers.


Dear Frank,

May I ask why you are moving to Kate from Emacs? I tried Kate with R (and 
Python and LaTeX) and I really liked the folding (which seems a lot better 
than all the not-really-functional hacks for getting folding with R and 
Python code) and some of the function/class browsers.

However, I specially missed:

a) the possibility of opening as many R processes as I want, and placing that 
buffer in wherever place and with whichever size I want.

b) most of the rest of emacs, actually (hey, where did my shells go? and my 
org-mode buffer? and my ...; not to talk about the keybindings).

If you feel like it, I'd like to hear about your impressions.

R.

>
> Frank

-- 
Ram?n D?az-Uriarte
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}


From dimaiana at gmail.com  Fri Jan 19 11:49:03 2007
From: dimaiana at gmail.com (leah martell)
Date: Fri, 19 Jan 2007 05:49:03 -0500
Subject: [R] naive bayes help
Message-ID: <6bb7be150701190249x7dedbe4cx9225267863ed1cc1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070119/ca58c1b3/attachment.pl 

From qjing at sibs.ac.cn  Fri Jan 19 10:02:19 2007
From: qjing at sibs.ac.cn (qing Jing)
Date: Fri, 19 Jan 2007 09:02:19 +0000 (UTC)
Subject: [R] RMySQL connection
Message-ID: <loom.20070119T092542-850@post.gmane.org>














Dear All,

What's wrong?
> library(DBI);
> library(RMySQL);
> mgr<-dbDriver("MySQL");
> con<-dbConnect(mgr,dbname="test");
Error in mysqlNewConnection(drv, ...) : RS-DBI driver: (could not connect 
(null)@(null) on dbname "test")

I also tried another example

> library(RMySQL);
> mycon<-dbConnect(MySQL
(),user='cws',dbname="cws",host="pi",password='delores');
Error in mysqlNewConnection(drv, ...) : RS-DBI driver: (could not connect 
cws at pi on dbname "cws")

I am running    R 2.1.1     DBI 0.1-9   RMySQL  0.5-6     Windows  XP

Thank you much for your help.

Qing Jing   PhD  MD


From pburns at pburns.seanet.com  Fri Jan 19 11:53:04 2007
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 19 Jan 2007 10:53:04 +0000
Subject: [R] Vectorize rearrangement within each column
In-Reply-To: <967DF70E-C70A-46B9-B438-AEA46D277FB6@gmail.com>
References: <967DF70E-C70A-46B9-B438-AEA46D277FB6@gmail.com>
Message-ID: <45B0A310.4050004@pburns.seanet.com>

Matrix subscripting can be used for this:

 > mb <- ma[cbind(as.vector(idx), as.vector(col(idx)))]
 > dim(mb) <- dim(ma)
 > mb
     [,1] [,2]
[1,]   10   14
[2,]   12   15
[3,]   11   13

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Osorio Roberto wrote:

>Consider a matrix like
>
> > ma = matrix(10:15, nr = 3)
> > ma
>      [,1] [,2]
>[1,]   10   13
>[2,]   11   14
>[3,]   12   15
>
>I want to rearrange each column according to row indexes (1 to 3)  
>given in another matrix, as in
>
> > idx = matrix(c(1,3,2, 2,3,1), nr = 3)
> > idx
>      [,1] [,2]
>[1,]    1    2
>[2,]    3    3
>[3,]    2    1
>
>The new matrix mb will have for each column the corresponding column  
>of ma indexed by the corresponding column of idx, as in
>
> > mb = ma
> > for (j in 1:2) mb[,j] = ma[idx[,j], j]	
> > mb
>      [,1] [,2]
>[1,]   10   14
>[2,]   12   15
>[3,]   11   13
>
>Can I avoid the for() loop? I'm specially interested to find out if a  
>fast implementation using lapply() would be feasible for large input  
>matrices (analogues of ma and idx) transformed into data frames.
>
>Roberto Osorio
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From maechler at stat.math.ethz.ch  Fri Jan 19 12:19:40 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 19 Jan 2007 12:19:40 +0100
Subject: [R] Robust PCA?
In-Reply-To: <00c001c73b58$67a33ae0$4d908980@gne.windows.gene.com>
References: <BAY132-F13CB1B12FF66301BA771D1AAAA0@phx.gbl>
	<00c001c73b58$67a33ae0$4d908980@gne.windows.gene.com>
Message-ID: <17840.43340.204659.173336@stat.math.ethz.ch>

>>>>> "BertG" == Bert Gunter <gunter.berton at gene.com>
>>>>>     on Thu, 18 Jan 2007 15:28:47 -0800 writes:

    BertG> You seem not to have received a reply.  You can use
    BertG> cov.rob in MASS or cov.Mcd in robustbase or
    BertG> undoubtedly others to obtain a robust covariance
    BertG> matrix and then use that for PCA.

    BertG> Bert Gunter Nonclinical Statistics 

Indeed. Thank you Bert.

BTW, (for the archives) do note that their is a
"R special interest group" (=: R-SIG) on robust statistics,
and mailing list "R-SIG-robust"
(-> https://stat.ethz.ch/mailman/listinfo/r-sig-robust, also for
 archives) with precisely the goal to foster coordinated
programming and porting of robust statistics functionality in R.

Expect to see more on this topic there, within the next few
days.

Martin Maechler, ETH Zurich


    >> -----Original Message----- From:
    >> r-help-bounces at stat.math.ethz.ch
    >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf
    >> Of Talbot Katz Sent: Thursday, January 18, 2007 11:44
    >> AM To: r-help at stat.math.ethz.ch Subject: [R] Robust
    >> PCA?

    >> Hi.

    >> I'm checking into robust methods for principal
    >> components analysis.  There seem to be several
    >> floating around.  I'm currently focusing my attention
    >> on a method of Hubert, Rousseeuw, and Vanden Branden
    >> (http://wis.kuleuven.be/stat/Papers/robpca.pdf)
    >> mainly because I'm familiar with other work by
    >> Rousseeuw and Hubert in robust methodologies.  Of
    >> course,

    >> I'd like to obtain code for this method, or another
    >> good robust PCA method, if there's one out there.  I
    >> haven't noticed the existence on CRAN of a package
    >> for robust PCA (the authors of the ROBPCA method do
    >> provide MATLAB code).

    >> -- TMK -- 212-460-5430 home 917-656-5351 cell


From ggrothendieck at gmail.com  Fri Jan 19 13:41:35 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 19 Jan 2007 07:41:35 -0500
Subject: [R] Vectorize rearrangement within each column
In-Reply-To: <967DF70E-C70A-46B9-B438-AEA46D277FB6@gmail.com>
References: <967DF70E-C70A-46B9-B438-AEA46D277FB6@gmail.com>
Message-ID: <971536df0701190441p7501fb4fjf21777b280d84704@mail.gmail.com>

Turn each matrix into a data.frame and then use mapply with the "[" function,
converting back to matrix when done:

as.matrix(mapply("[", as.data.frame(ma), as.data.frame(idx)))
     V1 V2
[1,] 10 14
[2,] 12 15
[3,] 11 13


On 1/19/07, Osorio Roberto <roboso at gmail.com> wrote:
> Consider a matrix like
>
>  > ma = matrix(10:15, nr = 3)
>  > ma
>      [,1] [,2]
> [1,]   10   13
> [2,]   11   14
> [3,]   12   15
>
> I want to rearrange each column according to row indexes (1 to 3)
> given in another matrix, as in
>
>  > idx = matrix(c(1,3,2, 2,3,1), nr = 3)
>  > idx
>      [,1] [,2]
> [1,]    1    2
> [2,]    3    3
> [3,]    2    1
>
> The new matrix mb will have for each column the corresponding column
> of ma indexed by the corresponding column of idx, as in
>
>  > mb = ma
>  > for (j in 1:2) mb[,j] = ma[idx[,j], j]
>  > mb
>      [,1] [,2]
> [1,]   10   14
> [2,]   12   15
> [3,]   11   13
>
> Can I avoid the for() loop? I'm specially interested to find out if a
> fast implementation using lapply() would be feasible for large input
> matrices (analogues of ma and idx) transformed into data frames.
>
> Roberto Osorio
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gavin.simpson at ucl.ac.uk  Fri Jan 19 13:51:12 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 19 Jan 2007 12:51:12 +0000
Subject: [R] RMySQL connection
In-Reply-To: <loom.20070119T092542-850@post.gmane.org>
References: <loom.20070119T092542-850@post.gmane.org>
Message-ID: <1169211072.4401.17.camel@gsimpson.geog.ucl.ac.uk>

On Fri, 2007-01-19 at 09:02 +0000, qing Jing wrote:
<snip />
> Dear All,
> 
> What's wrong?
<snip />
> I am running    R 2.1.1     DBI 0.1-9   RMySQL  0.5-6     Windows  XP
                  ^^^^^^^^
I'd start with updating your R installation to something less archaic -
yours is at least 18 months out of date. Latest version is R 2.4.1.

> 
> Thank you much for your help.
> 
> Qing Jing   PhD  MD

If you still have problems, first read the posting guide and then email
the list, following the instructions in the guide.

HTH

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From jhallman at frb.gov  Fri Jan 19 13:58:59 2007
From: jhallman at frb.gov (Jeffrey J. Hallman)
Date: 19 Jan 2007 07:58:59 -0500
Subject: [R] if else statement
References: <45B034D9.6010702@scripps.edu>
Message-ID: <xmrtzynnpf0.fsf@mralx1.rsma.frb.gov>

?switch

"H. Paul Benton" <hpbenton at scripps.edu> writes:
>     I'm doing some scripting and I've noticed that R doesn't seem to
> have an
> if (cond){
>     do
> }ifelse (cond) {
>     do
> } else {
>     do
> }
> 
> type block.
> 
> Is this correct or am I missing something.

-- 
Jeff


From f.harrell at vanderbilt.edu  Fri Jan 19 14:12:54 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 19 Jan 2007 07:12:54 -0600
Subject: [R] kate editor for R
In-Reply-To: <200701191118.26761.rdiaz@cnio.es>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
Message-ID: <45B0C3D6.3010907@vanderbilt.edu>

Ramon Diaz-Uriarte wrote:
> On Friday 19 January 2007 03:30, Frank E Harrell Jr wrote:
>> Like kile for LaTeX, Linux/KDE's kate editor is an excellent editor for
>> R, with easy code submission to a running R process.  Syntax
>> highlighting is good.  I have  not been able to figure out two things:
>>
>> - how to automatically reformat a line or region of text using good
>> indentation rules (Emacs/ESS make this so easy by just hitting Tab while
>> the cursor is in a line, or highlighting a region and hitting Esq q)
>>
>> - how to cause auto-indenting as you type braces.  For me, kate puts a {
>> in column one
>>
>> Thanks for any pointers.
> 
> 
> Dear Frank,
> 
> May I ask why you are moving to Kate from Emacs? I tried Kate with R (and 
> Python and LaTeX) and I really liked the folding (which seems a lot better 
> than all the not-really-functional hacks for getting folding with R and 
> Python code) and some of the function/class browsers.
> 
> However, I specially missed:
> 
> a) the possibility of opening as many R processes as I want, and placing that 
> buffer in wherever place and with whichever size I want.
> 
> b) most of the rest of emacs, actually (hey, where did my shells go? and my 
> org-mode buffer? and my ...; not to talk about the keybindings).
> 
> If you feel like it, I'd like to hear about your impressions.
> 
> R.

Good question Ramon.  We have dozens of R users in our department and 
many of them were not brought up on Emacs and find it hard to learn.  We 
are looking for an alternative to recommend for them.  I love Emacs 
myself and find that it is the fastest editor by a significant margin, 
and I am used to its keybindings.  But I prefer kate for printing and 
for managing multiple files in a project.  kate has a nice sidebar for 
navigating the files, and indicates which files have been changed since 
they were saved.  kate also schematically depicts nested code with side 
symbols connected by vertical lines for {}.  Scrolling of the R output 
window is a little more logical in kate than in ESS.  I find myself 
having to type Esc-shift-> often in ESS/Emacs to get to the bottom of 
the R output but kate puts the cursor at the bottom.  Also I get a 
little frustrated with package management in Xemacs (I know however that 
it's nice to be able to load thousands of packages) related to file 
permissions, ftp commands, anonymous logins, etc.  And from a purely 
"looks" standpoint kate is superior.

I tried jedit for a bit.  jedit has a lot of nice features but also has 
problems with indenting in R.

Frank

> 
>> Frank
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From rdiaz at cnio.es  Fri Jan 19 14:25:10 2007
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Fri, 19 Jan 2007 14:25:10 +0100
Subject: [R] kate editor for R
In-Reply-To: <45B0C3D6.3010907@vanderbilt.edu>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
	<45B0C3D6.3010907@vanderbilt.edu>
Message-ID: <200701191425.10119.rdiaz@cnio.es>

On Friday 19 January 2007 14:12, Frank E Harrell Jr wrote:
> Ramon Diaz-Uriarte wrote:
> > On Friday 19 January 2007 03:30, Frank E Harrell Jr wrote:
> >> Like kile for LaTeX, Linux/KDE's kate editor is an excellent editor for
> >> R, with easy code submission to a running R process.  Syntax
> >> highlighting is good.  I have  not been able to figure out two things:
> >>
> >> - how to automatically reformat a line or region of text using good
> >> indentation rules (Emacs/ESS make this so easy by just hitting Tab while
> >> the cursor is in a line, or highlighting a region and hitting Esq q)
> >>
> >> - how to cause auto-indenting as you type braces.  For me, kate puts a {
> >> in column one
> >>
> >> Thanks for any pointers.
> >
> > Dear Frank,
> >
> > May I ask why you are moving to Kate from Emacs? I tried Kate with R (and
> > Python and LaTeX) and I really liked the folding (which seems a lot
> > better than all the not-really-functional hacks for getting folding with
> > R and Python code) and some of the function/class browsers.
> >
> > However, I specially missed:
> >
> > a) the possibility of opening as many R processes as I want, and placing
> > that buffer in wherever place and with whichever size I want.
> >
> > b) most of the rest of emacs, actually (hey, where did my shells go? and
> > my org-mode buffer? and my ...; not to talk about the keybindings).
> >
> > If you feel like it, I'd like to hear about your impressions.
> >
> > R.


Thanks for your reply, Frank.

>
> Good question Ramon.  We have dozens of R users in our department and
> many of them were not brought up on Emacs and find it hard to learn.  We
> are looking for an alternative to recommend for them.  I love Emacs
> myself and find that it is the fastest editor by a significant margin,
> and I am used to its keybindings.  But I prefer kate for printing and
> for managing multiple files in a project.  kate has a nice sidebar for
> navigating the files, and indicates which files have been changed since
> they were saved.  

Ouch, I had missed that.


> kate also schematically depicts nested code with side 
> symbols connected by vertical lines for {}.  

Yes, this feature I _really_ like. Nothing like it that I know of for emacs (I 
use fold-dwim, but I find it clunky).


> Scrolling of the R output 
> window is a little more logical in kate than in ESS.  I find myself
> having to type Esc-shift-> often in ESS/Emacs to get to the bottom of
> the R output but kate puts the cursor at the bottom.  Also I get a
> little frustrated with package management in Xemacs (I know however that
> it's nice to be able to load thousands of packages) related to file
> permissions, ftp commands, anonymous logins, etc.  And from a purely
> "looks" standpoint kate is superior.
>
> I tried jedit for a bit.  jedit has a lot of nice features but also has
> problems with indenting in R.
>

Thanks for your feedback. I think I'll play again with kate this weekend.

Best,

R.


> Frank
>
> >> Frank

-- 
Ram?n D?az-Uriarte
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}


From David.Mas at u-paris2.fr  Fri Jan 19 14:33:46 2007
From: David.Mas at u-paris2.fr (David Mas)
Date: Fri, 19 Jan 2007 14:33:46 +0100
Subject: [R] Error in basehaz function ?
Message-ID: <20070119143346.6c795f0f@localhost>

Hello R-users.

I believe that the way basehaz (in the survival package) compute  the
baseline hazard function is false.

I come to question this function when it gives me hazard probabilities
greater than 1.

Looking at the code I think I've localised the error :

hazard probability is computed as :

H <- -log(surv)

but it seems to me that hazard probabilities is rather an instantaneous
survival rate that could be computed this way :

H[i] <- 1 - surv[i] / surv[i-1]

Using this rule I achieve satisfiable results with the two following
functions :

surv2haz <- function(surv) {

  haz <- surv

  haz[1] <- 1 - surv[1]
  
  for(i in c(2:length(surv)))
    {
      haz[i] <- 1 - surv[i] / surv[i - 1]
    }

  return(haz)
}

haz2surv <- function(haz) {

  surv <- haz

  surv[1] <- 1 - haz[1]
  
  for(i in c(2:length(haz)))
    {
      surv[i] <- (1 - haz[i]) * surv[i-1]
    }

  return(surv)
}

If I'm right, wouldn't it be a good idea to change the basehaz
function, to avoid misleading the overconfident user (as I happen to be)
?

I hope this will help contributing to a wonderful tool that speed up my
understanding of statistical analysis and my research.

David

-- 
David Mas
ERMES-FRE 2887-CNRS
Universit? Pantheon-Assas Paris II
12, place du Pantheon
F-75230 Paris Cedex 05
Tel: +33 (0)1 44 41 89 91
Mob: +33 (0)6 84 15 77 67 
Fax: +33 (0)1 40 51 81 30
http://www.u-paris2.fr/ermes/


From marc_schwartz at comcast.net  Fri Jan 19 14:35:45 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 19 Jan 2007 07:35:45 -0600
Subject: [R] kate editor for R
In-Reply-To: <45B0C3D6.3010907@vanderbilt.edu>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>  <45B0C3D6.3010907@vanderbilt.edu>
Message-ID: <1169213745.5017.24.camel@localhost.localdomain>

On Fri, 2007-01-19 at 07:12 -0600, Frank E Harrell Jr wrote:

<snip>

>  Scrolling of the R output 
> window is a little more logical in kate than in ESS.  I find myself 
> having to type Esc-shift-> often in ESS/Emacs to get to the bottom of 
> the R output but kate puts the cursor at the bottom.  

<snip>

Frank,

On the last point about getting the R output to scroll to the bottom,
put the following in your ~/.emacs file in the "custom-set-variables"
section:

...

 '(comint-move-point-for-output (quote others))

...


That will cause the R buffer to scroll to the bottom of the output,
while leaving other buffers unaltered.

HTH,

Marc


From petr.pikal at precheza.cz  Fri Jan 19 14:39:53 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 19 Jan 2007 14:39:53 +0100
Subject: [R] if else statement
In-Reply-To: <45B034D9.6010702@scripps.edu>
Message-ID: <45B0D839.4927.17B0BE3@localhost>

Hi

also note the difference between if and ifelse. The former is similar 
to other languages

if (true or false codition) {do this} else {do that}

but

ifelse(logical vector, vector for true elements, vector for false 
elements)

see corespondent help pages.

HTH
Petr


On 18 Jan 2007 at 19:02, H. Paul Benton wrote:

Date sent:      	Thu, 18 Jan 2007 19:02:49 -0800
From:           	"H. Paul Benton" <hpbenton at scripps.edu>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] if else statement

> Hello,
> 
>     I'm doing some scripting and I've noticed that R doesn't seem to
> have an if (cond){
>     do
> }ifelse (cond) {
>     do
> } else {
>     do
> }
> 
> type block.
> 
> Is this correct or am I missing something.
> 
> THX
> 
> Paul
> 
> -- 
> Research Technician
> Mass Spectrometry
>    o The
>   /
> o Scripps
>   \
>    o Research
>   /
> o Institute
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From schmidb at ibe.med.uni-muenchen.de  Fri Jan 19 15:02:31 2007
From: schmidb at ibe.med.uni-muenchen.de (Markus Schmidberger)
Date: Fri, 19 Jan 2007 15:02:31 +0100
Subject: [R] Problem with C extension
Message-ID: <45B0CF77.9020403@ibe.med.uni-muenchen.de>

Hello,

I try to write an extension in C, to get a faster functions.
Therefore I have to add an element (vector) to a vector. The command in 
R is very simple: x = c(x,a)
But in C I have the problem to reallocate my vector for getting more 
space. Everything I tried, I get a "Segmentation fault".

So, how can I combine two vectors in C and give the result back to R 
(return(x))?

Thanks
Markus Schmidberger

-- 
Dipl.-Tech. Math. Markus Schmidberger

Ludwig-Maximilians-Universit?t M?nchen
IBE - Institut f?r medizinische Informationsverarbeitung,
Biometrie und Epidemiologie
Marchioninistr. 15, D-81377 Muenchen
URL: http://ibe.web.med.uni-muenchen.de 
Mail: Markus.Schmidberger [at] ibe.med.uni-muenchen.de
Tel: +49 (089) 7095 - 4599


From edd at debian.org  Fri Jan 19 15:39:04 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 19 Jan 2007 08:39:04 -0600
Subject: [R] kate editor for R
In-Reply-To: <200701191118.26761.rdiaz@cnio.es>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
Message-ID: <17840.55304.803214.860567@basebud.nulle.part>


Ramon, Frank,

Great discussion. Nothing like an editor feud over morning coffee. Just kidding.

On 19 January 2007 at 11:18, Ramon Diaz-Uriarte wrote:
| However, I specially missed:
| 
| a) the possibility of opening as many R processes as I want, and placing that 
| buffer in wherever place and with whichever size I want.
| 
| b) most of the rest of emacs, actually (hey, where did my shells go? and my 
| org-mode buffer? and my ...; not to talk about the keybindings).

[ Thanks for the org-mode suggestion. That looks very useful. How do I get it
to sync to my Palm, though? ;-) ]

On 19 January 2007 at 07:12, Frank E Harrell Jr wrote:
[...]
| and I am used to its keybindings.  But I prefer kate for printing and 
| for managing multiple files in a project.  kate has a nice sidebar for 
| navigating the files, and indicates which files have been changed since 

As I am doing more C++ work, I glanced at oo-browser, sidebar, ecb (all in
Debian/Ubuntu).  Would a real Emacs hacker be able to these to R code too?

| they were saved.  kate also schematically depicts nested code with side 
| symbols connected by vertical lines for {}.  Scrolling of the R output 
| window is a little more logical in kate than in ESS.  I find myself 
| having to type Esc-shift-> often in ESS/Emacs to get to the bottom of 
| the R output but kate puts the cursor at the bottom.  Also I get a 
| little frustrated with package management in Xemacs (I know however that
| it's nice to be able to load thousands of packages) related to file 
| permissions, ftp commands, anonymous logins, etc.  And from a purely 
| "looks" standpoint kate is superior.

I switched back to GNU Emacs, using the emacs-snapshot-gtk package in Debian
and Ubuntu. Prettier, and still emacs :)   I get by without locally install
elisp code in /usr/local -- everything I needed was apt-get'able.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From helprhelp at gmail.com  Fri Jan 19 16:09:34 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Fri, 19 Jan 2007 10:09:34 -0500
Subject: [R] naive bayes help
In-Reply-To: <6bb7be150701190249x7dedbe4cx9225267863ed1cc1@mail.gmail.com>
References: <6bb7be150701190249x7dedbe4cx9225267863ed1cc1@mail.gmail.com>
Message-ID: <cdf817830701190709r2f024520o927b34a8e5abb7b4@mail.gmail.com>

change the last line
m.pr<-predict(mydt.nb, mydata[,-1], type="class")

into
m.pr<-predict(mydt.nb, mydata[,-1], type="raw")

see
?naiveBayes
and
?predict.naiveBayes

Your "y" is a continous dependent variable.

HTH,


weiwei


On 1/19/07, leah martell <dimaiana at gmail.com> wrote:
>  Hello
>
> I have a rather simple code and for some reason it produces an error
> message.  If someone can tell me why and how to fix it, I would be very
> greatful. Thank you in advance.
>
>
> ##### create data
> set.seed(10)
> n <- 200                     # number of training points
> n.test <- 200               # number of test points
> p<-2                            # dimension of input space
> z <- matrix(rnorm((n+n.test)*p),ncol=p)
> x <- matrix(0,nrow=n+n.test,ncol=p)
> for (i in 1:p)
>   x[,i] <- z%*%rnorm(p)
> truecoef <- c(1,2)
> prob1 <- exp(x%*%truecoef)/(1+exp(x%*%truecoef))
> # prob is the true probability of class 1
> y <- rbinom(n+n.test,1,prob1)
> # separate the data into train and test sets
> mydata <- data.frame(y=y[1:n],x=x[1:n,])
> mydata.test <- data.frame(y=y[n+(1:n.test)],x=x[n+(1: n.test),])
> ##########################
> library(e1071)
> mydt.nb<-naiveBayes(y~ ., data=mydata)
> m.pr<-predict(mydt.nb, mydata[,-1], type="class")
>
>
> regards,
> Leah
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From rdiaz at cnio.es  Fri Jan 19 16:09:40 2007
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Fri, 19 Jan 2007 16:09:40 +0100
Subject: [R] kate editor for R
In-Reply-To: <17840.55304.803214.860567@basebud.nulle.part>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
	<17840.55304.803214.860567@basebud.nulle.part>
Message-ID: <200701191609.40533.rdiaz@cnio.es>

Hi Dirk,


On Friday 19 January 2007 15:39, Dirk Eddelbuettel wrote:
> Ramon, Frank,
>
> Great discussion. Nothing like an editor feud over morning coffee. Just
> kidding.
>

Not at the "editor flame war" stage yet (nobody mentioned vim :-).


> On 19 January 2007 at 11:18, Ramon Diaz-Uriarte wrote:
> | However, I specially missed:
> |
> | a) the possibility of opening as many R processes as I want, and placing
> | that buffer in wherever place and with whichever size I want.
> |
> | b) most of the rest of emacs, actually (hey, where did my shells go? and
> | my org-mode buffer? and my ...; not to talk about the keybindings).
>
> [ Thanks for the org-mode suggestion. That looks very useful. How do I get
> it to sync to my Palm, though? ;-) ]
>

I asked the same at the org-mode list some time back and there was a short 
thread  
(http://lists.gnu.org/archive/html/emacs-orgmode/2006-11/msg00003.html). The 
bottom line is this:
a) for the general org files, you send them to the palm as text, and you edit 
them there with a suitable editor (e.g., PalmED). If org-mode files are kept 
under version control, life becomes easier.
b) dealing with calendar is a more serious problem.
c) there seems to be some (not a lot of) interest in these issues, but things 
are not smooth yet.

(I am using my Palm a lot less now, so I am no longer even doing a) 
regularly).

> On 19 January 2007 at 07:12, Frank E Harrell Jr wrote:
> [...]
>
> | and I am used to its keybindings.  But I prefer kate for printing and
> | for managing multiple files in a project.  kate has a nice sidebar for
> | navigating the files, and indicates which files have been changed since
>
> As I am doing more C++ work, I glanced at oo-browser, sidebar, ecb (all in
> Debian/Ubuntu).  Would a real Emacs hacker be able to these to R code too?
>

I use ecb with R directly out of the ecb box. No problem. 

> | they were saved.  kate also schematically depicts nested code with side
> | symbols connected by vertical lines for {}.  Scrolling of the R output
> | window is a little more logical in kate than in ESS.  I find myself
> | having to type Esc-shift-> often in ESS/Emacs to get to the bottom of
> | the R output but kate puts the cursor at the bottom.  Also I get a
> | little frustrated with package management in Xemacs (I know however that
> | it's nice to be able to load thousands of packages) related to file
> | permissions, ftp commands, anonymous logins, etc.  And from a purely
> | "looks" standpoint kate is superior.
>
> I switched back to GNU Emacs, using the emacs-snapshot-gtk package in
> Debian and Ubuntu. Prettier, and still emacs :)   I get by without locally
> install elisp code in /usr/local -- everything I needed was apt-get'able.
>

I had problems with one of the packages ecb depends upon (semantic ?), and 
emacs-snapshot. IIRC it was a documented problem related to a bug in semantic 
(?); maybe it's been fixed now. But what does emacs-snapshot-gtk provide you 
now (besides the pretinness) that you'd miss with 21-4? 

R.


> Dirk

-- 
Ram?n D?az-Uriarte
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



**NOTA DE CONFIDENCIALIDAD** Este correo electr?nico, y en s...{{dropped}}


From roland.rproject at gmail.com  Fri Jan 19 16:46:59 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Fri, 19 Jan 2007 10:46:59 -0500
Subject: [R] Error in basehaz function ?
In-Reply-To: <20070119143346.6c795f0f@localhost>
References: <20070119143346.6c795f0f@localhost>
Message-ID: <47c7c59e0701190746n1decf5f4nde89758769b4e20@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070119/b10fcfdf/attachment.pl 

From GPetris at uark.edu  Fri Jan 19 16:52:57 2007
From: GPetris at uark.edu (Giovanni Petris)
Date: Fri, 19 Jan 2007 09:52:57 -0600 (CST)
Subject: [R] Reading contingency tables
In-Reply-To: <Pine.LNX.4.64.0701190707250.17748@gannet.stats.ox.ac.uk>
	(message from Prof Brian Ripley on Fri, 19 Jan 2007 07:08:21 +0000
	(GMT))
References: <200701182300.l0IN0axv007115@definetti.ddns.uark.edu>
	<Pine.LNX.4.64.0701190611440.28506@gannet.stats.ox.ac.uk>
	<Pine.LNX.4.64.0701190707250.17748@gannet.stats.ox.ac.uk>
Message-ID: <200701191552.l0JFqvsW007670@definetti.ddns.uark.edu>


Thank you, Prof Ripley. 

However, there is still something unclear to me about read.ftable.  I
noticed the following behavior, which I found strange. I expected
read.ftable to be an "inverse" of write.ftable, so to speak. Is there
something I am missing, or is this a problem in read.ftable?

Thanks,
Giovanni

> file <- tempfile()
> cat("             Intercourse\n",
+     "Race  Gender     Yes  No\n",
+     "White Male        43 134\n",
+     "      Female      26 149\n",
+     "Black Male        29  23\n",
+     "      Female      22  36\n",
+     file = file)
> file.show(file)
             Intercourse
 Race  Gender     Yes  No
 White Male        43 134
       Female      26 149
 Black Male        29  23
       Female      22  36

> ft <- read.ftable(file)
> ft ## OK so far
             Intercourse Yes  No
Race  Gender                    
White Male                43 134
      Female              26 149
Black Male                29  23
      Female              22  36
> x <- as.table(ft)
> write.ftable(ftable(x, col.vars=2:3), file)
> file.show(file)
        "Gender"      "Male"      "Female"     
        "Intercourse"  "Yes" "No"    "Yes" "No"
"Race"                                         
"White"                   43  134       26  149
"Black"                   29   23       22   36

> ft2 <- read.ftable(file)
> ft2
      Gender      Male             Female            
      Intercourse  Yes  No Yes  No    Yes  No Yes  No
Race                                                 
White               43 134  26 149     29  23  22  36
Black               43 134  26 149     29  23  22  36
> version
               _                           
platform       sparc-sun-solaris2.8        
arch           sparc                       
os             solaris2.8                  
system         sparc, solaris2.8           
status                                     
major          2                           
minor          4.1                         
year           2006                        
month          12                          
day            18                          
svn rev        40228                       
language       R                           
version.string R version 2.4.1 (2006-12-18)

-- 

Giovanni Petris  <GPetris at uark.edu>
Associate Professor
Department of Mathematical Sciences
University of Arkansas - Fayetteville, AR 72701
Ph: (479) 575-6324, 575-8630 (fax)
http://definetti.uark.edu/~gpetris/


From finbref.2006 at gmail.com  Fri Jan 19 16:55:51 2007
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Fri, 19 Jan 2007 16:55:51 +0100
Subject: [R] x-axis in filled.contour
Message-ID: <d0f55a670701190755w1858b23x5c152dcbbddb65af@mail.gmail.com>

The filled.contour function gives me some strange output. What did I do wrong?

x=seq(0,1,length=10)
y=seq(0,1,length=10)
z=array(rnorm(100),dim=c(10,10))
filled.contour(x,y,z)
lines(0.4,0.8,type="p")
abline(v=0.4,lty="dashed")

the x-cooridnate of the line and the point is 0.4, but it's slightly
above. This problem just appears with "filled.contour", so I guess
there is a problem with the key on the right.
I use 2.4.0 under unbuntu
Thomas


From ggrothendieck at gmail.com  Fri Jan 19 17:08:22 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 19 Jan 2007 11:08:22 -0500
Subject: [R] Reading contingency tables
In-Reply-To: <200701191552.l0JFqvsW007670@definetti.ddns.uark.edu>
References: <200701182300.l0IN0axv007115@definetti.ddns.uark.edu>
	<Pine.LNX.4.64.0701190611440.28506@gannet.stats.ox.ac.uk>
	<Pine.LNX.4.64.0701190707250.17748@gannet.stats.ox.ac.uk>
	<200701191552.l0JFqvsW007670@definetti.ddns.uark.edu>
Message-ID: <971536df0701190808q4bf5354dl7c0086456afaa56d@mail.gmail.com>

You are writing out x rather than ft. If you do:

write.ftable(ft, file = stdout())

it looks the same as the input.


On 1/19/07, Giovanni Petris <GPetris at uark.edu> wrote:
>
> Thank you, Prof Ripley.
>
> However, there is still something unclear to me about read.ftable.  I
> noticed the following behavior, which I found strange. I expected
> read.ftable to be an "inverse" of write.ftable, so to speak. Is there
> something I am missing, or is this a problem in read.ftable?
>
> Thanks,
> Giovanni
>
> > file <- tempfile()
> > cat("             Intercourse\n",
> +     "Race  Gender     Yes  No\n",
> +     "White Male        43 134\n",
> +     "      Female      26 149\n",
> +     "Black Male        29  23\n",
> +     "      Female      22  36\n",
> +     file = file)
> > file.show(file)
>             Intercourse
>  Race  Gender     Yes  No
>  White Male        43 134
>       Female      26 149
>  Black Male        29  23
>       Female      22  36
>
> > ft <- read.ftable(file)
> > ft ## OK so far
>             Intercourse Yes  No
> Race  Gender
> White Male                43 134
>      Female              26 149
> Black Male                29  23
>      Female              22  36
> > x <- as.table(ft)
> > write.ftable(ftable(x, col.vars=2:3), file)
> > file.show(file)
>        "Gender"      "Male"      "Female"
>        "Intercourse"  "Yes" "No"    "Yes" "No"
> "Race"
> "White"                   43  134       26  149
> "Black"                   29   23       22   36
>
> > ft2 <- read.ftable(file)
> > ft2
>      Gender      Male             Female
>      Intercourse  Yes  No Yes  No    Yes  No Yes  No
> Race
> White               43 134  26 149     29  23  22  36
> Black               43 134  26 149     29  23  22  36
> > version
>               _
> platform       sparc-sun-solaris2.8
> arch           sparc
> os             solaris2.8
> system         sparc, solaris2.8
> status
> major          2
> minor          4.1
> year           2006
> month          12
> day            18
> svn rev        40228
> language       R
> version.string R version 2.4.1 (2006-12-18)
>
> --
>
> Giovanni Petris  <GPetris at uark.edu>
> Associate Professor
> Department of Mathematical Sciences
> University of Arkansas - Fayetteville, AR 72701
> Ph: (479) 575-6324, 575-8630 (fax)
> http://definetti.uark.edu/~gpetris/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Bernhard_Pfaff at fra.invesco.com  Fri Jan 19 17:20:26 2007
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Fri, 19 Jan 2007 16:20:26 -0000
Subject: [R] problem in adf command
In-Reply-To: <942832.98341.qm@web34611.mail.mud.yahoo.com>
References: <E4A9111DA23BA048B9A46686BF727CF461BEE5@DEFRAXMB01.corp.amvescap.net>
	<942832.98341.qm@web34611.mail.mud.yahoo.com>
Message-ID: <E4A9111DA23BA048B9A46686BF727CF461BEE9@DEFRAXMB01.corp.amvescap.net>

ur.df(y, type = c("none", "drift", "trend"), lags = 1) 

	 in urca.
	this gives me all out put .but i need only p.value fromm the
output.
	
	when i run the following command 
	ur.df(y, type = c("none", "drift", "trend"), lags =
	 1)$p.value 
	this in response null.kindly help me in this regard. thanks
	With  Best Regards


Hello Zahid,
	 
	you do not need to send your message three times to the list. As
the package's documentation outlines: 'urca' utilises formal classes
(i.e. S4). Hence, to obtain slots, you should use '@' and not '$'.
Anyway, who is telling you, that p.value is a slot of a returned ur.df
object? At least not ?ur.df. Now, if you take a look at ?UnitrootTests
in package 'fSeries' you will read that these test implementation do
contain p.values, hence:
	 
	library(urca)
	library(fSeries)
	## to generate some data use the example
	example(UnitrootTests)
	test.adf <- adfTest(y, type = 'c')
	slotNames(test.adf)
	names(test.adf at test)
	test.adf at test$p.value
	
	 
Best,
Bernhard
	 
*****************************************************************
Confidentiality Note: The information contained in this mess...{{dropped}}


From GPetris at uark.edu  Fri Jan 19 17:20:34 2007
From: GPetris at uark.edu (Giovanni Petris)
Date: Fri, 19 Jan 2007 10:20:34 -0600 (CST)
Subject: [R] Reading contingency tables
In-Reply-To: <971536df0701190808q4bf5354dl7c0086456afaa56d@mail.gmail.com>
	(message from Gabor Grothendieck on Fri, 19 Jan 2007 11:08:22 -0500)
References: <200701182300.l0IN0axv007115@definetti.ddns.uark.edu>
	<Pine.LNX.4.64.0701190611440.28506@gannet.stats.ox.ac.uk>
	<Pine.LNX.4.64.0701190707250.17748@gannet.stats.ox.ac.uk>
	<200701191552.l0JFqvsW007670@definetti.ddns.uark.edu>
	<971536df0701190808q4bf5354dl7c0086456afaa56d@mail.gmail.com>
Message-ID: <200701191620.l0JGKY8q007703@definetti.ddns.uark.edu>


> Date: Fri, 19 Jan 2007 11:08:22 -0500
> From: Gabor Grothendieck <ggrothendieck at gmail.com>
> Cc: ripley at stats.ox.ac.uk, r-help at stat.math.ethz.ch
> DomainKey-Signature: a=rsa-sha1; c=nofws;        d=gmail.com; s=beta;
> 
> You are writing out x rather than ft. If you do:
> 
> write.ftable(ft, file = stdout())
> 
> it looks the same as the input.
> 

You are wright. The point I was trying to make is that read.ftable
does not work as (I) expected for an ftable with two column
variables. 

Giovanni

> 
> On 1/19/07, Giovanni Petris <GPetris at uark.edu> wrote:
> >
> > Thank you, Prof Ripley.
> >
> > However, there is still something unclear to me about read.ftable.  I
> > noticed the following behavior, which I found strange. I expected
> > read.ftable to be an "inverse" of write.ftable, so to speak. Is there
> > something I am missing, or is this a problem in read.ftable?
> >
> > Thanks,
> > Giovanni
> >
> > > file <- tempfile()
> > > cat("             Intercourse\n",
> > +     "Race  Gender     Yes  No\n",
> > +     "White Male        43 134\n",
> > +     "      Female      26 149\n",
> > +     "Black Male        29  23\n",
> > +     "      Female      22  36\n",
> > +     file = file)
> > > file.show(file)
> >             Intercourse
> >  Race  Gender     Yes  No
> >  White Male        43 134
> >       Female      26 149
> >  Black Male        29  23
> >       Female      22  36
> >
> > > ft <- read.ftable(file)
> > > ft ## OK so far
> >             Intercourse Yes  No
> > Race  Gender
> > White Male                43 134
> >      Female              26 149
> > Black Male                29  23
> >      Female              22  36
> > > x <- as.table(ft)
> > > write.ftable(ftable(x, col.vars=2:3), file)
> > > file.show(file)
> >        "Gender"      "Male"      "Female"
> >        "Intercourse"  "Yes" "No"    "Yes" "No"
> > "Race"
> > "White"                   43  134       26  149
> > "Black"                   29   23       22   36
> >
> > > ft2 <- read.ftable(file)
> > > ft2
> >      Gender      Male             Female
> >      Intercourse  Yes  No Yes  No    Yes  No Yes  No
> > Race
> > White               43 134  26 149     29  23  22  36
> > Black               43 134  26 149     29  23  22  36
> > > version
> >               _
> > platform       sparc-sun-solaris2.8
> > arch           sparc
> > os             solaris2.8
> > system         sparc, solaris2.8
> > status
> > major          2
> > minor          4.1
> > year           2006
> > month          12
> > day            18
> > svn rev        40228
> > language       R
> > version.string R version 2.4.1 (2006-12-18)
> >
> > --
> >
> > Giovanni Petris  <GPetris at uark.edu>
> > Associate Professor
> > Department of Mathematical Sciences
> > University of Arkansas - Fayetteville, AR 72701
> > Ph: (479) 575-6324, 575-8630 (fax)
> > http://definetti.uark.edu/~gpetris/
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From ggrothendieck at gmail.com  Fri Jan 19 17:24:09 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 19 Jan 2007 11:24:09 -0500
Subject: [R] Reading contingency tables
In-Reply-To: <200701191620.l0JGKY8q007703@definetti.ddns.uark.edu>
References: <200701182300.l0IN0axv007115@definetti.ddns.uark.edu>
	<Pine.LNX.4.64.0701190611440.28506@gannet.stats.ox.ac.uk>
	<Pine.LNX.4.64.0701190707250.17748@gannet.stats.ox.ac.uk>
	<200701191552.l0JFqvsW007670@definetti.ddns.uark.edu>
	<971536df0701190808q4bf5354dl7c0086456afaa56d@mail.gmail.com>
	<200701191620.l0JGKY8q007703@definetti.ddns.uark.edu>
Message-ID: <971536df0701190824x72c650ccsfac18edbc785fba7@mail.gmail.com>

On 1/19/07, Giovanni Petris <GPetris at uark.edu> wrote:
>
> > Date: Fri, 19 Jan 2007 11:08:22 -0500
> > From: Gabor Grothendieck <ggrothendieck at gmail.com>
> > Cc: ripley at stats.ox.ac.uk, r-help at stat.math.ethz.ch
> > DomainKey-Signature: a=rsa-sha1; c=nofws;        d=gmail.com; s=beta;
> >
> > You are writing out x rather than ft. If you do:
> >
> > write.ftable(ft, file = stdout())
> >
> > it looks the same as the input.
> >
>
> You are wright. The point I was trying to make is that read.ftable
> does not work as (I) expected for an ftable with two column
> variables.
>

x is not of class "ftable".  It is of class "table".  It may print with
two column variables but it does not intrinsically have two column
variables.


From ronggui.huang at gmail.com  Fri Jan 19 17:26:23 2007
From: ronggui.huang at gmail.com (ronggui)
Date: Sat, 20 Jan 2007 00:26:23 +0800
Subject: [R] Where can I get the latex format manual?
Message-ID: <38b9f0350701190826j66de0b89k21013388027f02f9@mail.gmail.com>

In www.r-project.org  I can find html and pdf format, and the
R-x.x-x/doc/manual has texinfo format only.

I can not find latex format manual. Am I miss something? Thanks for your hints.

-- 
Ronggui Huang
Department of Sociology
Fudan University, Shanghai, China
??????
????????????????


From rdporto1 at terra.com.br  Fri Jan 19 17:30:47 2007
From: rdporto1 at terra.com.br (rdporto1)
Date: Fri, 19 Jan 2007 13:30:47 -0300
Subject: [R] integrate and quadratic forms
Message-ID: <JC4J7B$CB1756C27BEE4E2A553A8DF468087035@terra.com.br>

Thanks Jim. But a narrower problem still remains.
Please, look at this new code:

lambda=c(.6,.3)
integral = function(u) {
  theta = (atan(.6*u) + atan(.3*u))/2 - .1*u/2
  rho = (1+.6^2*u^2)^(1/4) * (1+.3^2*u^2)^(1/4)
  integrand = sin(theta)/(u*rho)
}

> integrate(integral,0,Inf)$value
[1] 1.222688

If I replace
  theta = sum(atan(lambda*u))/2 - .1*u/2
I get
> integrate(integral,0,Inf)$value
[1] 1.517134

It seems there is a problem between the sum() and integrate()
functions. 

Is there a way to solve this problem?

Thanks in advance.

Rogerio.

---------- Part of the Original Message -----------

> They do give the same answer, unfortunately the examples you sent  were not
> the same.  Integral2 was missing a 'sin'.  So I would assume there might be
> something else wrong with your functions.  You might want to try breaking it
> down into smaller steps so you can see what you are doing.  It definitely is
> hard to read in both cases.
>
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
> 
> What is the problem you are trying to solve?


From br44114 at gmail.com  Fri Jan 19 17:36:56 2007
From: br44114 at gmail.com (bogdan romocea)
Date: Fri, 19 Jan 2007 11:36:56 -0500
Subject: [R] hiccup in apply?
Message-ID: <8d5a36350701190836r5157945ao85e25faf1b1e694@mail.gmail.com>

Hello, I don't understand the behavior of apply() on the data frame below.

test <-
structure(list(Date = structure(c(13361, 13361, 13361, 13361,
13361, 13361, 13361, 13361, 13362, 13362, 13362, 13362, 13362,
13362, 13362, 13362, 13363, 13363, 13363, 13363, 13363, 13363,
13363, 13363, 13364, 13364, 13364, 13364, 13364, 13364, 13364,
13364, 13365, 13365, 13365, 13365, 13365, 13365, 13365, 13365,
13366, 13366, 13366, 13366, 13366, 13366, 13366, 13366, 13367,
13367), class = "Date"), RANK = as.integer(c(19, 7, 5, 4, 6,
3, 3, 4, 18, 7, 6, 4, 6, 3, 3, 4, 19, 7, 6, 4, 6, 3, 3, 4, 18,
6, 7, 4, 6, 3, 3, 4, 18, 6, 7, 4, 6, 3, 3, 4, 18, 6, 7, 4, 6,
3, 3, 4, 18, 6))), .Names = c("Date", "RANK"), row.names = c("1",
"2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13",
"14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
"25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35",
"36", "37", "38", "39", "40", "41", "42", "43", "44", "45", "46",
"47", "48", "49", "50"), class = "data.frame")

#---fine
> summary(test)
      Date                 RANK
 Min.   :2006-08-01   Min.   : 3.00
 1st Qu.:2006-08-02   1st Qu.: 4.00
 Median :2006-08-04   Median : 5.50
 Mean   :2006-08-03   Mean   : 6.62
 3rd Qu.:2006-08-05   3rd Qu.: 6.75
 Max.   :2006-08-07   Max.   :19.00

#---isn't this supposed to work?
> apply(test,2,mean)
Date RANK
  NA   NA
Warning messages:
1: argument is not numeric or logical: returning NA in:
mean.default(newX[, i], ...)
2: argument is not numeric or logical: returning NA in:
mean.default(newX[, i], ...)

Thank you,
b.

platform       i386-pc-mingw32
arch           i386
os             mingw32
system         i386, mingw32
status
major          2
minor          4.0
year           2006
month          10
day            03
svn rev        39566
language       R
version.string R version 2.4.0 (2006-10-03)


From Greg.Snow at intermountainmail.org  Fri Jan 19 17:42:10 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 19 Jan 2007 09:42:10 -0700
Subject: [R] Problem with loading tkrplot
Message-ID: <07E228A5BE53C24CAD490193A7381BBB7B2D64@LP-EXCHVS07.CO.IHC.COM>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof 
> Brian Ripley
> Sent: Thursday, January 18, 2007 11:05 PM
> To: jim holtman
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] Problem with loading tkrplot
> 
> As it happens I used TeachingDemos/tkrplot on Windows (R 
> 2.4.1) with a class example yesterday.

Wow, first I make it into the fortunes package.  
Now I find out that Prof. Ripley is using my package in the classroom.
I am starting to feel like a contributing member of society.

As a side note, I just realized that this spring marks 20 years since I
first used a port of the S language, I'm starting to feel old.


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111


From Greg.Snow at intermountainmail.org  Fri Jan 19 17:51:30 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 19 Jan 2007 09:51:30 -0700
Subject: [R] x-axis in filled.contour
Message-ID: <07E228A5BE53C24CAD490193A7381BBB7B2D67@LP-EXCHVS07.CO.IHC.COM>

The filled.contour function first plots the contour plot, then the
legend to the side.  The plotting parameters that are in effect after
the call are based on the legend, not the original plot, so the
coordinate system apears to be messed up.  One way around this is to use
the plot.axis argument to do your annotation, look at the example in
?filled.contour that is labelled  # Annotating a filled contour plot

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas Steiner
> Sent: Friday, January 19, 2007 8:56 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] x-axis in filled.contour
> 
> The filled.contour function gives me some strange output. 
> What did I do wrong?
> 
> x=seq(0,1,length=10)
> y=seq(0,1,length=10)
> z=array(rnorm(100),dim=c(10,10))
> filled.contour(x,y,z)
> lines(0.4,0.8,type="p")
> abline(v=0.4,lty="dashed")
> 
> the x-cooridnate of the line and the point is 0.4, but it's 
> slightly above. This problem just appears with 
> "filled.contour", so I guess there is a problem with the key 
> on the right.
> I use 2.4.0 under unbuntu
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From maechler at stat.math.ethz.ch  Fri Jan 19 17:53:07 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 19 Jan 2007 17:53:07 +0100
Subject: [R] Where can I get the latex format manual?
In-Reply-To: <38b9f0350701190826j66de0b89k21013388027f02f9@mail.gmail.com>
References: <38b9f0350701190826j66de0b89k21013388027f02f9@mail.gmail.com>
Message-ID: <17840.63347.80288.991050@stat.math.ethz.ch>

>>>>> "ronggui" == ronggui  <ronggui.huang at gmail.com>
>>>>>     on Sat, 20 Jan 2007 00:26:23 +0800 writes:

    ronggui> In www.r-project.org  I can find html and pdf format, and the
    ronggui> R-x.x-x/doc/manual has texinfo format only.

    ronggui> I can not find latex format manual. Am I miss something? Thanks for your hints.

Hint: Apart from the "Reference" (all the help pages), there is
      none, so it's understandable you can't find it.. 
Texinfo is a (very simple)  "dialect of"  TeX,
LaTeX is a different (much more sophisticated) "dialect".

BTW, I still read (and search!) these manuals in the *info*
     format via Emacs [C-h i ..] which (for me) is faster than
     anything else.. 

Martin Maechler, ETH Zurich


From maechler at stat.math.ethz.ch  Fri Jan 19 18:15:42 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 19 Jan 2007 18:15:42 +0100
Subject: [R] Problem with C extension
In-Reply-To: <45B0CF77.9020403@ibe.med.uni-muenchen.de>
References: <45B0CF77.9020403@ibe.med.uni-muenchen.de>
Message-ID: <17840.64702.85914.894624@stat.math.ethz.ch>

>>>>> "Markus" == Markus Schmidberger <schmidb at ibe.med.uni-muenchen.de>
>>>>>     on Fri, 19 Jan 2007 15:02:31 +0100 writes:

    Markus> Hello,
    Markus> I try to write an extension in C, to get a faster functions.
    Markus> Therefore I have to add an element (vector) to a vector. The command in 
    Markus> R is very simple: x = c(x,a)
aka
	x <- c(x,a)

see that's why you'd probably rather stick with R a bit longer,
and profile [-> help(Rprof)] your code and try to speedup quite
a bit before thinking about using C ...

    Markus> But in C I have the problem to reallocate my vector for getting more 
    Markus> space. Everything I tried, I get a "Segmentation fault".

    Markus> So, how can I combine two vectors in C and give the result back to R 
    Markus> (return(x))?

and you have a copy of "Writing R Extensions" right in front of
you, electronically at least, I mean?

To return the new vector via C's return() you'd definitely need
to work with .Call() {which is a good thing but really not for
C-beginners}, and that needs a bit time of reading the above manual "from
cover to cover". Note that you probably should start with (5.8) on
.Call.

I'm also tending to recommend even starting to peek into R's own
C source, notably the header files (src/include/...), and maybe
src/main/* in order to see how R objects ("SEXP"s) are handled,
how you add names(), dimnames() etc internally.. 

Hoping that helps,
Martin Maechler, ETH Zurich

    Markus> Thanks
    Markus> Markus Schmidberger

    Markus> -- 
    Markus> Dipl.-Tech. Math. Markus Schmidberger


From gavin.simpson at ucl.ac.uk  Fri Jan 19 18:22:44 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 19 Jan 2007 17:22:44 +0000
Subject: [R] hiccup in apply?
In-Reply-To: <8d5a36350701190836r5157945ao85e25faf1b1e694@mail.gmail.com>
References: <8d5a36350701190836r5157945ao85e25faf1b1e694@mail.gmail.com>
Message-ID: <1169227364.4401.52.camel@gsimpson.geog.ucl.ac.uk>

On Fri, 2007-01-19 at 11:36 -0500, bogdan romocea wrote:
> Hello, I don't understand the behavior of apply() on the data frame below.
> 
> test <-
> structure(list(Date = structure(c(13361, 13361, 13361, 13361,
> 13361, 13361, 13361, 13361, 13362, 13362, 13362, 13362, 13362,
> 13362, 13362, 13362, 13363, 13363, 13363, 13363, 13363, 13363,
> 13363, 13363, 13364, 13364, 13364, 13364, 13364, 13364, 13364,
> 13364, 13365, 13365, 13365, 13365, 13365, 13365, 13365, 13365,
> 13366, 13366, 13366, 13366, 13366, 13366, 13366, 13366, 13367,
> 13367), class = "Date"), RANK = as.integer(c(19, 7, 5, 4, 6,
> 3, 3, 4, 18, 7, 6, 4, 6, 3, 3, 4, 19, 7, 6, 4, 6, 3, 3, 4, 18,
> 6, 7, 4, 6, 3, 3, 4, 18, 6, 7, 4, 6, 3, 3, 4, 18, 6, 7, 4, 6,
> 3, 3, 4, 18, 6))), .Names = c("Date", "RANK"), row.names = c("1",
> "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13",
> "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
> "25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35",
> "36", "37", "38", "39", "40", "41", "42", "43", "44", "45", "46",
> "47", "48", "49", "50"), class = "data.frame")
> 
> #---fine
> > summary(test)
>       Date                 RANK
>  Min.   :2006-08-01   Min.   : 3.00
>  1st Qu.:2006-08-02   1st Qu.: 4.00
>  Median :2006-08-04   Median : 5.50
>  Mean   :2006-08-03   Mean   : 6.62
>  3rd Qu.:2006-08-05   3rd Qu.: 6.75
>  Max.   :2006-08-07   Max.   :19.00
> 
> #---isn't this supposed to work?
> > apply(test,2,mean)
> Date RANK
>   NA   NA
> Warning messages:
> 1: argument is not numeric or logical: returning NA in:
> mean.default(newX[, i], ...)
> 2: argument is not numeric or logical: returning NA in:
> mean.default(newX[, i], ...)

Look at ?apply and details. 

Argument X of apply is supposed to be an array. Details says:

     If 'X' is not an array but has a dimension attribute, 'apply'
     attempts to coerce it to an array via 'as.matrix' if it is
     two-dimensional (e.g., data frames) or via 'as.array'.

So you should look at what is happening with as.matrix():

str(as.matrix(test))
 chr [1:50, 1:2] "2006-08-01" "2006-08-01" "2006-08-01" ...
 - attr(*, "dimnames")=List of 2
  ..$ : chr [1:50] "1" "2" "3" "4" ...
  ..$ : chr [1:2] "Date" "RANK"

Notice this is now a character matrix and not what you thought it was.
So look at ?as.matrix and we see:

     'as.matrix' is a generic function. The method for data frames will
     convert any non-numeric/complex column into a character vector
     using 'format' and so return a character matrix, except that
     all-logical data frames will be coerced to a logical matrix.  When
     coercing a vector, it produces a one-column matrix, and promotes
     the names (if any) of the vector to the rownames of the matrix.

Which explains what is happening.

Workaround:

lapply(test, mean)
sapply(test, mean)

Both work

HTH,

G

> Thank you,
> b.
> 
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          4.0
> year           2006
> month          10
> day            03
> svn rev        39566
> language       R
> version.string R version 2.4.0 (2006-10-03)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From tlumley at u.washington.edu  Fri Jan 19 18:48:33 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 19 Jan 2007 09:48:33 -0800 (PST)
Subject: [R] integrate and quadratic forms
In-Reply-To: <JC3A4C$7A93B236CCC7C0B6F4DF8CBF1A16BE89@terra.com.br>
References: <JC3A4C$7A93B236CCC7C0B6F4DF8CBF1A16BE89@terra.com.br>
Message-ID: <Pine.LNX.4.64.0701190944370.27236@homer23.u.washington.edu>


As the documentation for integrate() says, the function must be vectorized

   f: an R function taking  a numeric first argument and returning a
           numeric vector of the same length.

so you can't use sum(). You need matrix operations or an explicit loop to 
add up the terms.


 	-thomas




On Thu, 18 Jan 2007, rdporto1 wrote:

> Hi all.
>
> I'm trying to numerically invert the characteristic function
> of a quadratic form following Imhof's (1961, Biometrika 48)
> procedure.
>
> The parameters are:
>
> lambda=c(.6,.3,.1)
> h=c(2,2,2)
> sigma=c(0,0,0)
> q=3
>
> I've implemented Imhof's procedure two ways that, for me,
> should give the same answer:
>
> #more legible
> integral1 = function(u) {
>  o=(1/2)*sum(h*atan(lambda*u)+sigma^2*lambda*u/(1+lambda^2*u^2)) - q*u/2
>  rho=prod((1+lambda^2*u^2)^(h/4))*exp( (1/2)*sum((sigma*lambda*u)^2/(1+lambda^2*u^2)) )
>  integrand = sin(o)/(u*rho)
> }
>
> #same as above
> integral2= function(u) {
> ((1/2)*sum(h*atan(lambda*u)+sigma^2*lambda*u/(1+lambda^2*u^2)) - q*u/2)/
> (u*(prod((1+lambda^2*u^2)^(h/4))*
> exp( (1/2)*sum((sigma*lambda*u)^2/(1+lambda^2*u^2)) )))
> }
>
> The following should be near 0.18. However, nor the answers are near this
> value neither they agree each other!
>
>> 1/2+(1/pi)*integrate(integral1,0,Inf)$value
> [1] 1.022537
>> 1/2+(1/pi)*integrate(integral2,0,Inf)$value
> [1] 1.442720
>
> What's happening? Is this a bug or OS specific? Shouldn't they give the
> same answer? Why do I get results so different from 0.18? In time:
> the procedure works fine for q=.2.
>
> I'm running R 2.4.1 in a PC with Windows XP 32bits. Other ways (in R) to
> find the distribution of general quadratic forms are welcome.
>
> Thanks in advance.
>
> Rogerio.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From BEN at SSANET.COM  Fri Jan 19 18:54:09 2007
From: BEN at SSANET.COM (Ben Fairbank)
Date: Fri, 19 Jan 2007 11:54:09 -0600
Subject: [R] Newbie question:  Statistical functions (e.g., mean,
	sd) in a "transform" statement?
Message-ID: <CA612484A337C6479EA341DF9EEE14AC05CF48F6@hercules.ssainfo>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070119/747f19a3/attachment.pl 

From ssj1364 at gmail.com  Fri Jan 19 18:57:24 2007
From: ssj1364 at gmail.com (sj)
Date: Fri, 19 Jan 2007 10:57:24 -0700
Subject: [R] help with ets function in forecast package
Message-ID: <1c6126db0701190957q2ec8099ch69db69da2a6eafbd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070119/5099a74c/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Jan 19 18:50:53 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Jan 2007 17:50:53 +0000 (GMT)
Subject: [R] Problem with C extension
In-Reply-To: <45B0CF77.9020403@ibe.med.uni-muenchen.de>
References: <45B0CF77.9020403@ibe.med.uni-muenchen.de>
Message-ID: <Pine.LNX.4.64.0701191750160.31069@gannet.stats.ox.ac.uk>

This is a programming question, and I am sending a solution to R-devel
(please see the posting guide).

On Fri, 19 Jan 2007, Markus Schmidberger wrote:

> Hello,
>
> I try to write an extension in C, to get a faster functions.
> Therefore I have to add an element (vector) to a vector. The command in
> R is very simple: x = c(x,a)
> But in C I have the problem to reallocate my vector for getting more
> space. Everything I tried, I get a "Segmentation fault".
>
> So, how can I combine two vectors in C and give the result back to R
> (return(x))?
>
> Thanks
> Markus Schmidberger
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Jan 19 18:54:49 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Jan 2007 17:54:49 +0000 (GMT)
Subject: [R] Problem with C extension
In-Reply-To: <45B0CF77.9020403@ibe.med.uni-muenchen.de>
References: <45B0CF77.9020403@ibe.med.uni-muenchen.de>
Message-ID: <Pine.LNX.4.64.0701191558340.29596@gannet.stats.ox.ac.uk>

[A programming question moved from R-help]

On Fri, 19 Jan 2007, Markus Schmidberger wrote:

> Hello,
>
> I try to write an extension in C, to get a faster functions.
> Therefore I have to add an element (vector) to a vector. The command in
> R is very simple: x = c(x,a)

I don't see how you are going to code this in C appreciably faster than 
the R developers already have.

> But in C I have the problem to reallocate my vector for getting more
> space. Everything I tried, I get a "Segmentation fault".

We have no idea how you are trying to do this, or even which interface 
(.C, .Call, .External) you are trying to use.

> So, how can I combine two vectors in C and give the result back to R
> (return(x))?

The code below is just a beginning (it does no coercion, there are missing 
cases and it can be improved by caching e.g. REAL(a)), but can be used by

> dyn.load("my_c.so")
> my_c <- function(a, b) .Call("my_c", a, b)

% cat my_c.c

#include <R.h>
#include <Rinternals.h>

SEXP my_c(SEXP a, SEXP b)
{
     SEXP ans;

     int i, na, nb;
     if(TYPEOF(a) != TYPEOF(b)) error("type mismatch");
     switch(TYPEOF(a)) {
     case LGLSXP:
     case INTSXP:
     case REALSXP:
     case STRSXP:
 	break;
     default:
 	error("unimplemented type");
     }
     na = LENGTH(a); nb = LENGTH(b);
     PROTECT(ans = allocVector(TYPEOF(a), na+nb));
     switch(TYPEOF(a)) {
     case LGLSXP:
     case INTSXP:
 	for(i = 0; i < na; i++) INTEGER(ans)[i] = INTEGER(a)[i];
 	for(i = 0; i < nb; i++) INTEGER(ans)[na+i] = INTEGER(a)[i];
 	break;
     case REALSXP:
 	for(i = 0; i < na; i++) REAL(ans)[i] = REAL(a)[i];
 	for(i = 0; i < nb; i++) REAL(ans)[na+i] = REAL(a)[i];
 	break;
     case STRSXP:
 	for(i = 0; i < na; i++) SET_STRING_ELT(ans, i, STRING_ELT(a, i));
 	for(i = 0; i < nb; i++) SET_STRING_ELT(ans, na+i, STRING_ELT(a, i));
 	break;
     }
     UNPROTECT(1);
     return ans;
}


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From YNing at celgene.com  Fri Jan 19 19:03:40 2007
From: YNing at celgene.com (Yuhong Ning)
Date: Fri, 19 Jan 2007 10:03:40 -0800
Subject: [R] Error in heatmap()
Message-ID: <6DB832784E7C7543B22376A1C62E8CDD01175006@MAIL02.sdcelgene.celgene.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070119/6712209a/attachment.pl 

From f.harrell at vanderbilt.edu  Fri Jan 19 19:06:59 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 19 Jan 2007 12:06:59 -0600
Subject: [R] kate editor for R
In-Reply-To: <17840.55304.803214.860567@basebud.nulle.part>
References: <45B02D46.1090709@vanderbilt.edu>	<200701191118.26761.rdiaz@cnio.es>
	<17840.55304.803214.860567@basebud.nulle.part>
Message-ID: <45B108C3.40209@vanderbilt.edu>

Dirk Eddelbuettel wrote:
> Ramon, Frank,
> 
> Great discussion. Nothing like an editor feud over morning coffee. Just kidding.
> 
> On 19 January 2007 at 11:18, Ramon Diaz-Uriarte wrote:
> | However, I specially missed:
> | 
> | a) the possibility of opening as many R processes as I want, and placing that 
> | buffer in wherever place and with whichever size I want.
> | 
> | b) most of the rest of emacs, actually (hey, where did my shells go? and my 
> | org-mode buffer? and my ...; not to talk about the keybindings).
> 
> [ Thanks for the org-mode suggestion. That looks very useful. How do I get it
> to sync to my Palm, though? ;-) ]
> 
> On 19 January 2007 at 07:12, Frank E Harrell Jr wrote:
> [...]
> | and I am used to its keybindings.  But I prefer kate for printing and 
> | for managing multiple files in a project.  kate has a nice sidebar for 
> | navigating the files, and indicates which files have been changed since 
> 
> As I am doing more C++ work, I glanced at oo-browser, sidebar, ecb (all in
> Debian/Ubuntu).  Would a real Emacs hacker be able to these to R code too?
> 
> | they were saved.  kate also schematically depicts nested code with side 
> | symbols connected by vertical lines for {}.  Scrolling of the R output 
> | window is a little more logical in kate than in ESS.  I find myself 
> | having to type Esc-shift-> often in ESS/Emacs to get to the bottom of 
> | the R output but kate puts the cursor at the bottom.  Also I get a 
> | little frustrated with package management in Xemacs (I know however that
> | it's nice to be able to load thousands of packages) related to file 
> | permissions, ftp commands, anonymous logins, etc.  And from a purely 
> | "looks" standpoint kate is superior.
> 
> I switched back to GNU Emacs, using the emacs-snapshot-gtk package in Debian
> and Ubuntu. Prettier, and still emacs :)   I get by without locally install
> elisp code in /usr/local -- everything I needed was apt-get'able.
> 
> Dirk
> 

Thanks for the pointer to emacs-snapshot-gtk Dirk.  I installed it in 
debian but it looked virtually identical to the emacs21 I've been using.

Frank


From ggrothendieck at gmail.com  Fri Jan 19 19:20:07 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 19 Jan 2007 13:20:07 -0500
Subject: [R] help with ets function in forecast package
In-Reply-To: <1c6126db0701190957q2ec8099ch69db69da2a6eafbd@mail.gmail.com>
References: <1c6126db0701190957q2ec8099ch69db69da2a6eafbd@mail.gmail.com>
Message-ID: <971536df0701191020s3c3ce587n1f08ffdc2a1deb60@mail.gmail.com>

Maybe you are using an old version?  This is what I get:


> packageDescription("forecast")$Version
[1] "1.03"
> R.version.string # XP
[1] "R version 2.4.1 Patched (2006-12-30 r40331)"
> ts2 <- ts(c(67,55,55,59,65,74,80,56,40,59,54,96,67,66,80,58,61,53,
+ 76,74,67,47,63,57,54,67,85,81,70,68,53,60,72,70,60,66,
+ 73,65,67,81,94,59,71,67,76,61,79,78,82,85,60,69,51,68,
+ 88,70,70,53,63,70,77,79,63,74,72,85,69,85,79,75,71,61,
+ 68,68,80,72,70,82,80,53,71,88,76,71,57,65,68,67,73,95,
+ 94,68,64,67,65,85,92,73,67,57,58,69,82,82,76,67,77,74),frequency=7)
>
> ets(ts2)
ETS(A,N,A)

Call:
 ets(y = ts2)

  Smoothing parameters:
           alpha = 0.0646
           gamma = 0.01

         Initial states:
           l = 64.1174
         s = 2.0613 10.2135 8.756 -7.7316 -5.1641 -6.6847
                          -1.4503



On 1/19/07, sj ssj1364 at gmail.com> wrote:
>  I have been trying to use the ets function in the forecast package on a
> daily time series (ts2 is a ts object with frequency =7). However when I run
> the following code I get an error related to etsmodel. I have looked at ets
> and I can see that there is a call to the function etsmodel, but I cant seem
> to find info on the ets function anywhere. Does anyone know anything about
> the etsmodel function?
>
>
> this is a sample data set with sample code
>
> library(forecast)
>
> ts2 <- ts(c(67,55,55,59,65,74,80,56,40,59,54,96,67,66,80,58,61,53,
> 76,74,67,47,63,57,54,67,85,81,70,68,53,60,72,70,60,66,
> 73,65,67,81,94,59,71,67,76,61,79,78,82,85,60,69,51,68,
> 88,70,70,53,63,70,77,79,63,74,72,85,69,85,79,75,71,61,
> 68,68,80,72,70,82,80,53,71,88,76,71,57,65,68,67,73,95,
> 94,68,64,67,65,85,92,73,67,57,58,69,82,82,76,67,77,74),frequency=7)
>
> ets(ts2)
>
> I get the following error:
>
> [1] "Model: ETS(A,N,A)"
> Error in etsmodel(y, errortype[i], trendtype[j], seasontype[k], damped[l],
> :
>        Parameters out of range
>
> Any help would be appreciated.
>
> thanks,
>
>
> Spencer
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From edd at debian.org  Fri Jan 19 19:24:41 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 19 Jan 2007 12:24:41 -0600
Subject: [R] kate editor for R
In-Reply-To: <45B108C3.40209@vanderbilt.edu>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
	<17840.55304.803214.860567@basebud.nulle.part>
	<45B108C3.40209@vanderbilt.edu>
Message-ID: <20070119182441.GA31823@eddelbuettel.com>

On Fri, Jan 19, 2007 at 12:06:59PM -0600, Frank E Harrell Jr wrote:
> Thanks for the pointer to emacs-snapshot-gtk Dirk.  I installed it in 
> debian but it looked virtually identical to the emacs21 I've been using.

Note that it does not replace the emacs21 binary, so you need to start it
as 'emacs-snapshot-gtk'.  

Likeweise you need to tell R/ESS to use emacslient.emacs-snapshot
instead of emacsclient as the value of optios("editor")

Below is what one of Edge machines as work has:
   
~:> ls -l /usr/bin/emacs*
lrwxrwxrwx 1 root root      23 2006-04-05 12:26 /usr/bin/emacs ->
/etc/alternatives/emacs
lrwxrwxrwx 1 root root       9 2006-12-05 11:27 /usr/bin/emacs21 ->
emacs21-x
-rwxr-xr-x 1 root root 4412588 2006-09-15 12:35 /usr/bin/emacs21-x
lrwxrwxrwx 1 root root      29 2006-04-05 12:26 /usr/bin/emacsclient
-> /etc/alternatives/emacsclient
-rwxr-xr-x 1 root root    7080 2006-09-15 12:35
/usr/bin/emacsclient.emacs21
-rwxr-xr-x 1 root root    9248 2006-09-19 16:28
/usr/bin/emacsclient.emacs-snapshot
lrwxrwxrwx 1 root root      32 2006-10-19 18:55
/usr/bin/emacs-snapshot -> /etc/alternatives/emacs-snapshot
-rwxr-xr-x 1 root root 5674120 2006-09-19 16:28
/usr/bin/emacs-snapshot-gtk


Hth, Dirk


-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From cberry at tajo.ucsd.edu  Fri Jan 19 19:36:33 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Fri, 19 Jan 2007 10:36:33 -0800
Subject: [R] Newbie question:  Statistical functions (e.g., mean,
 sd) in a "transform" statement?
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05CF48F6@hercules.ssainfo>
References: <CA612484A337C6479EA341DF9EEE14AC05CF48F6@hercules.ssainfo>
Message-ID: <Pine.LNX.4.64.0701191028220.24908@tajo.ucsd.edu>


Ben,

transform() is probably the wrong tool if what you want is to

 	'apply a function'

to the corresponding elements of time1, time2, ... , and return a vector 
of results.

If this is what you are after, the 'apply' family of functions is what you 
want.

See

 	?apply

and

 	?mapply

and the 'See Also's on each page.

Chuck Berry

On Fri, 19 Jan 2007, Ben Fairbank wrote:

> Greetings listeRs -
>
>
>
> Given a data frame such as
>
>
>
> times
>
>       time1    time2     time3    time4
>
> 1  70.408543 48.92378  7.399605 95.93050
>
> 2  17.231940 27.48530 82.962916 10.20619
>
> 3  20.279220 10.33575 66.209290 30.71846
>
> 4         NA 53.31993 12.398237 35.65782
>
> 5   9.295965       NA 48.929201       NA
>
> 6  63.966518 42.16304  1.777342       NA
>
>
>
> one can use "transform" to total all or some columns, thus,
>
>
>
> times2 <- transform(times,totaltime=time1+time2+time3+time4)
>
>
>
>> times2
>
>       time1    time2     time3    time4 totaltime
>
> 1  70.408543 48.92378  7.399605 95.93050  222.6624
>
> 2  17.231940 27.48530 82.962916 10.20619  137.8863
>
> 3  20.279220 10.33575 66.209290 30.71846  127.5427
>
> 4         NA 53.31993 12.398237 35.65782        NA
>
> 5   9.295965       NA 48.929201       NA        NA
>
> 6  63.966518 42.16304  1.777342       NA        NA
>
>
>
> I cannot, however, find a way, other than "for" looping,
>
> to use statistical functions, such as mean or sd, to
>
> compute the new column.  For example,
>
>
>
>>
> times2<-transform(times,meantime=(mean(c(time1,time2,time3,time4),na.rm=
> TRUE)))
>
>
>
>> times2
>
>
>
> time1    time2     time3    time4 meantime
>
> 1  70.408543 48.92378  7.399605 95.93050 45.54178
>
> 2  17.231940 27.48530 82.962916 10.20619 45.54178
>
> 3  20.279220 10.33575 66.209290 30.71846 45.54178
>
> 4         NA 53.31993 12.398237 35.65782 45.54178
>
> 5   9.295965       NA 48.929201       NA 45.54178
>
> 6  63.966518 42.16304  1.777342       NA 45.54178
>
>
>
> How can this be done?  And, generally, what is the recommended method
>
> for creating computed new columns in data frames when "for" loops take
>
> too long?
>
>
>
> With thanks for any suggestions,
>
>
>
> Ben Fairbank
>
>
>
> Using version 2.4.1 on a Windows XP professional operating system.
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901


From aiminy at iastate.edu  Fri Jan 19 19:43:49 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Fri, 19 Jan 2007 12:43:49 -0600
Subject: [R] split data set
Message-ID: <6.1.2.0.2.20070119123956.01c8a9b0@aiminy.mail.iastate.edu>

I have a data(ABC) like this:

      x   y
A   3   4
A   1   3
B   2   6
B   4   8
C   5   4
C   6   7

I want to split this data into

A:
   x   y
A   3   4
A   1   3

B
B   2   6
B   4   8

C
C   5   4
C   6   7

anyone knows how to do that?

thanks,

Aimin Yan


From kubovy at virginia.edu  Fri Jan 19 19:46:01 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Fri, 19 Jan 2007 13:46:01 -0500
Subject: [R] Newbie question:  Statistical functions (e.g., mean,
	sd) in a "transform" statement?
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05CF48F6@hercules.ssainfo>
References: <CA612484A337C6479EA341DF9EEE14AC05CF48F6@hercules.ssainfo>
Message-ID: <72BB4EF8-D3E2-4827-8AB1-F9F6395691F0@virginia.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070119/890b6d3f/attachment.pl 

From ggrothendieck at gmail.com  Fri Jan 19 19:51:26 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 19 Jan 2007 13:51:26 -0500
Subject: [R] Newbie question: Statistical functions (e.g., mean,
	sd) in a "transform" statement?
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05CF48F6@hercules.ssainfo>
References: <CA612484A337C6479EA341DF9EEE14AC05CF48F6@hercules.ssainfo>
Message-ID: <971536df0701191051w4b023b77j2c0d3536c09dea98@mail.gmail.com>

Try this using the builtin data set anscombe:

transform(anscombe, rowMeans = rowMeans(anscombe))

On 1/19/07, Ben Fairbank <BEN at ssanet.com> wrote:
> Greetings listeRs -
>
>
>
> Given a data frame such as
>
>
>
> times
>
>       time1    time2     time3    time4
>
> 1  70.408543 48.92378  7.399605 95.93050
>
> 2  17.231940 27.48530 82.962916 10.20619
>
> 3  20.279220 10.33575 66.209290 30.71846
>
> 4         NA 53.31993 12.398237 35.65782
>
> 5   9.295965       NA 48.929201       NA
>
> 6  63.966518 42.16304  1.777342       NA
>
>
>
> one can use "transform" to total all or some columns, thus,
>
>
>
> times2 <- transform(times,totaltime=time1+time2+time3+time4)
>
>
>
> > times2
>
>       time1    time2     time3    time4 totaltime
>
> 1  70.408543 48.92378  7.399605 95.93050  222.6624
>
> 2  17.231940 27.48530 82.962916 10.20619  137.8863
>
> 3  20.279220 10.33575 66.209290 30.71846  127.5427
>
> 4         NA 53.31993 12.398237 35.65782        NA
>
> 5   9.295965       NA 48.929201       NA        NA
>
> 6  63.966518 42.16304  1.777342       NA        NA
>
>
>
> I cannot, however, find a way, other than "for" looping,
>
> to use statistical functions, such as mean or sd, to
>
> compute the new column.  For example,
>
>
>
> >
> times2<-transform(times,meantime=(mean(c(time1,time2,time3,time4),na.rm=
> TRUE)))
>
>
>
> > times2
>
>
>
>  time1    time2     time3    time4 meantime
>
> 1  70.408543 48.92378  7.399605 95.93050 45.54178
>
> 2  17.231940 27.48530 82.962916 10.20619 45.54178
>
> 3  20.279220 10.33575 66.209290 30.71846 45.54178
>
> 4         NA 53.31993 12.398237 35.65782 45.54178
>
> 5   9.295965       NA 48.929201       NA 45.54178
>
> 6  63.966518 42.16304  1.777342       NA 45.54178
>
>
>
> How can this be done?  And, generally, what is the recommended method
>
> for creating computed new columns in data frames when "for" loops take
>
> too long?
>
>
>
> With thanks for any suggestions,
>
>
>
> Ben Fairbank
>
>
>
> Using version 2.4.1 on a Windows XP professional operating system.
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gavin.simpson at ucl.ac.uk  Fri Jan 19 19:53:31 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 19 Jan 2007 18:53:31 +0000
Subject: [R] Newbie question:  Statistical functions (e.g., mean,
	sd)	in a "transform" statement?
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05CF48F6@hercules.ssainfo>
References: <CA612484A337C6479EA341DF9EEE14AC05CF48F6@hercules.ssainfo>
Message-ID: <1169232811.4401.56.camel@gsimpson.geog.ucl.ac.uk>

On Fri, 2007-01-19 at 11:54 -0600, Ben Fairbank wrote:
> Greetings listeRs - 

Here are two solutions, depending on whether you wanted the NA's or not,
and I assume you wanted the row means:

> times3 <- transform(times, meantime = rowMeans(times))
> times3
      time1    time2     time3    time4 meantime
1 70.408543 48.92378  7.399605 95.93050 55.66561
2 17.231940 27.48530 82.962916 10.20619 34.47159
3 20.279220 10.33575 66.209290 30.71846 31.88568
4        NA 53.31993 12.398237 35.65782       NA
5  9.295965       NA 48.929201       NA       NA
6 63.966518 42.16304  1.777342       NA       NA
> times4 <- transform(times, meantime = rowMeans(times, na.rm = TRUE))
> times4
      time1    time2     time3    time4 meantime
1 70.408543 48.92378  7.399605 95.93050 55.66561
2 17.231940 27.48530 82.962916 10.20619 34.47159
3 20.279220 10.33575 66.209290 30.71846 31.88568
4        NA 53.31993 12.398237 35.65782 33.79200
5  9.295965       NA 48.929201       NA 29.11258
6 63.966518 42.16304  1.777342       NA 35.96897

HTH

G

> 
> Given a data frame such as 
> 
>  
> 
> times
> 
>        time1    time2     time3    time4
> 
> 1  70.408543 48.92378  7.399605 95.93050
> 
> 2  17.231940 27.48530 82.962916 10.20619
> 
> 3  20.279220 10.33575 66.209290 30.71846
> 
> 4         NA 53.31993 12.398237 35.65782
> 
> 5   9.295965       NA 48.929201       NA
> 
> 6  63.966518 42.16304  1.777342       NA
> 
>  
> 
> one can use "transform" to total all or some columns, thus,
> 
>  
> 
> times2 <- transform(times,totaltime=time1+time2+time3+time4)
> 
>  
> 
> > times2
> 
>        time1    time2     time3    time4 totaltime
> 
> 1  70.408543 48.92378  7.399605 95.93050  222.6624
> 
> 2  17.231940 27.48530 82.962916 10.20619  137.8863
> 
> 3  20.279220 10.33575 66.209290 30.71846  127.5427
> 
> 4         NA 53.31993 12.398237 35.65782        NA
> 
> 5   9.295965       NA 48.929201       NA        NA
> 
> 6  63.966518 42.16304  1.777342       NA        NA
> 
>  
> 
> I cannot, however, find a way, other than "for" looping,
> 
> to use statistical functions, such as mean or sd, to 
> 
> compute the new column.  For example,
> 
>  
> 
> >
> times2<-transform(times,meantime=(mean(c(time1,time2,time3,time4),na.rm=
> TRUE)))
> 
>  
> 
> > times2
> 
>  
> 
>  time1    time2     time3    time4 meantime
> 
> 1  70.408543 48.92378  7.399605 95.93050 45.54178
> 
> 2  17.231940 27.48530 82.962916 10.20619 45.54178
> 
> 3  20.279220 10.33575 66.209290 30.71846 45.54178
> 
> 4         NA 53.31993 12.398237 35.65782 45.54178
> 
> 5   9.295965       NA 48.929201       NA 45.54178
> 
> 6  63.966518 42.16304  1.777342       NA 45.54178
> 
>  
> 
> How can this be done?  And, generally, what is the recommended method 
> 
> for creating computed new columns in data frames when "for" loops take 
> 
> too long?
> 
>  
> 
> With thanks for any suggestions,
> 
>  
> 
> Ben Fairbank
> 
>  
> 
> Using version 2.4.1 on a Windows XP professional operating system.
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From roboso at gmail.com  Fri Jan 19 20:17:32 2007
From: roboso at gmail.com (Roberto Osorio)
Date: Fri, 19 Jan 2007 14:17:32 -0500
Subject: [R] Vectorize rearrangement within each column
In-Reply-To: <7fc59970701190950w4e92437dqd3ae94621245d569@mail.gmail.com>
References: <967DF70E-C70A-46B9-B438-AEA46D277FB6@gmail.com>
	<971536df0701190441p7501fb4fjf21777b280d84704@mail.gmail.com>
	<7fc59970701190950w4e92437dqd3ae94621245d569@mail.gmail.com>
Message-ID: <7fc59970701191117s7bf52f30i45a3cc20e5a8ac2b@mail.gmail.com>

Thanks for the solutions. Here are some time tests for ma and idx
being 100 X 100,000. The machine is a 2.16 GHz Intel MacBook Pro with
2 GB memory.

ma <- matrix(rnorm(1e7), nr = 100)      # 100 X 100,000
idx <- matrix(round( runif(1e7, 1, 100) ), nr = 100)

# Original:

system.time( {
    mb <- ma;
    for (j in 1:1e5) mb[,j] <- ma[idx[j],j]
} )
[1] 1.354 0.087 1.435 0.000 0.000

# Prof. Venables' version:

system.time( mb[] <- as.vector(ma)[as.vector(idx +
       outer(rep(nrow(ma), nrow(ma)), 1:ncol(ma)-1, '*'))] )
[1] 0.885 0.857 2.262 0.000 0.000

# Patrick Burns' version:

system.time( {
    mb <- ma[cbind(as.vector(idx), as.vector(col(idx)))];
    dim(mb) <- dim(ma)
} )
[1] 1.672 0.615 2.277 0.000 0.000

# Gabor Grothendieck's version led to some memory handling issue. I
stepped one order of magnitude down in the number of columns but it's
still very slow.

> ma <- matrix(rnorm(1e6), nr = 100)           # 100 X 10,000
> idx = matrix(round( runif(1e6, 1, 100) ), nr = 100)
> system.time( as.matrix(mapply("[", as.data.frame(ma), as.data.frame(idx))) )
[1] 2.060 0.133 2.768 0.000 0.000

So, Prof. Venables' solution is the fastest. In view of only moderate
time savings, I will take his advice and keep the original loop for
code clarity.

Roberto Osorio
------


From ccleland at optonline.net  Fri Jan 19 20:20:38 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 19 Jan 2007 14:20:38 -0500
Subject: [R] split data set
In-Reply-To: <6.1.2.0.2.20070119123956.01c8a9b0@aiminy.mail.iastate.edu>
References: <6.1.2.0.2.20070119123956.01c8a9b0@aiminy.mail.iastate.edu>
Message-ID: <45B11A06.2020900@optonline.net>

Aimin Yan wrote:
> I have a data(ABC) like this:
> 
>       x   y
> A   3   4
> A   1   3
> B   2   6
> B   4   8
> C   5   4
> C   6   7
> 
> I want to split this data into
> 
> A:
>    x   y
> A   3   4
> A   1   3
> 
> B
> B   2   6
> B   4   8
> 
> C
> C   5   4
> C   6   7
> 
> anyone knows how to do that?

?split

> df <- data.frame(L = rep(c("A","B","C"), each =2), x = runif(6),
y=runif(6))

> df
  L          x          y
1 A 0.57411628 0.01975706
2 A 0.21079282 0.62153084
3 B 0.46798592 0.97374423
4 B 0.08731642 0.33845989
5 C 0.08801709 0.93843814
6 C 0.54576658 0.26798882

> split(df, df$L)
$A
  L         x          y
1 A 0.5741163 0.01975706
2 A 0.2107928 0.62153084

$B
  L          x         y
3 B 0.46798592 0.9737442
4 B 0.08731642 0.3384599

$C
  L          x         y
5 C 0.08801709 0.9384381
6 C 0.54576658 0.2679888

> thanks,
> 
> Aimin Yan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From tomas.goicoa at unavarra.es  Fri Jan 19 20:32:30 2007
From: tomas.goicoa at unavarra.es (Tomas Goicoa)
Date: Fri, 19 Jan 2007 20:32:30 +0100
Subject: [R] (no subject)
Message-ID: <6.0.3.0.0.20070119200329.01aad6d0@pop.unavarra.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070119/94e97925/attachment.pl 

From marc_schwartz at comcast.net  Fri Jan 19 21:56:01 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 19 Jan 2007 14:56:01 -0600
Subject: [R] kate editor for R
In-Reply-To: <200701191609.40533.rdiaz@cnio.es>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
	<17840.55304.803214.860567@basebud.nulle.part>
	<200701191609.40533.rdiaz@cnio.es>
Message-ID: <1169240161.4838.27.camel@localhost.localdomain>

On Fri, 2007-01-19 at 16:09 +0100, Ramon Diaz-Uriarte wrote:

<snip>

> I had problems with one of the packages ecb depends upon (semantic ?), and 
> emacs-snapshot. IIRC it was a documented problem related to a bug in semantic 
> (?); maybe it's been fixed now. But what does emacs-snapshot-gtk provide you 
> now (besides the pretinness) that you'd miss with 21-4? 

<snip>

Ramon,

Just a quick heads up on the ECB issue.

I am using Emacs 23 from CVS and had to update ECB and the associated
packages to use this version of Emacs. I have emacs 23 installed and run
from a separate download folder, so that I do not overwrite the
installed stable version.

I use the CEDET cedet-1.0pre3.tar.gz aggregate package from
http://cedet.sourceforge.net/ as well as the ECB cvs snap shot package
ecb.tar.gz from http://ecb.sourceforge.net/downloads.html.

The CEDET package includes cogre, ede, eieio, semantic and speedbar.

Extract these two files and then modify ~/.emacs with the following:

;; Load ECB
(setq semantic-load-turn-everything-on t)
(load-file "/PATH/TO/CEDET/cedet-1.0pre3/common/cedet.el")

(add-to-list 'load-path "/PATH/TO/ECB/ecb-snap")
(require 'ecb)


And all seems well.

HTH,

Marc Schwartz


From rreiss at exponent.com  Fri Jan 19 23:33:16 2007
From: rreiss at exponent.com (Richard Reiss)
Date: Fri, 19 Jan 2007 14:33:16 -0800
Subject: [R] GOF for continuous data
Message-ID: <CF13ED94B9C31B469D7F9249FD936798018B15BC@EXCHANGE0.exponent.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070119/cd8d7a37/attachment.pl 

From mmourroux at mvaconsultancy.com  Fri Jan 19 15:04:17 2007
From: mmourroux at mvaconsultancy.com (Matthieu Mourroux)
Date: Fri, 19 Jan 2007 14:04:17 +0000
Subject: [R] Bartlett test
Message-ID: <s5b0cfec.029@wokingmail.wokingprime.com>

Bonjour,
Je voudrais tester l'homosc?dasdicit? entre des groupes. J'aurais alors aim? savoir quelle ?tait l'hypoth?se nulle du test de bartlett.
Merci pour votre aide.
Matthieu.


________________________________________________________________________
This e-mail has been scanned for all viruses by Star. The\ s...{{dropped}}


From bcarvalh at jhsph.edu  Sat Jan 20 00:17:26 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Fri, 19 Jan 2007 18:17:26 -0500
Subject: [R] Suggestion on how to improve efficiency when using
	MASS:::hubers on high-dimensional arrays
Message-ID: <D26799F8-AC4A-43F6-A083-DD620191D36B@jhsph.edu>

Hi Everyone,

Given the scenario I have, I was wondering if anyone would be able to  
give me a hind on how to get the results from hubers() in a more  
efficient way.

I have an outcome on an array [N x S x D].

I also have a factor (levels 1,2,3) stored on a matrix N x S.

My objective is to get "mu" and "sigma" for each of the N rows  
(outcome) stratified by the factor (levels 1, 2 and 3) for each of  
the D "levels", but using MASS:hubers().

Ideally the final result would be an array [N x D x 3 x 2].

The following toy example demonstrates what I want to do, and I'd  
like to improve the performance when working on my case, where S=400  
and N > 200000

Thank you very much for any suggestion.

benilton

## begin toy example
set.seed(1)
N <- 100
S <- 5
D <- 2

outcome <- array(rnorm(N*S*D), dim=c(N, S, D))
classes <- matrix(sample(c(1:3, NA), N*S, rep=T), ncol=S)

results <- array(NA, dim=c(N, D, 3, 2))

library(MASS)
myHubers <- function(x)
   if (length(x)>1) as.numeric(hubers(x))  else c(NA, NA)

for (n in 1:N)
   for (d in 1:D){
     tmp <- outcome[n,,d]
     grp <- classes[n,]
     results[n, d,,] <- t(sapply(split(tmp, factor(grp, levels=1:3)),  
myHubers))
   }
## end

--
Benilton Carvalho
PhD Candidate
Department of Biostatistics
Johns Hopkins University


From bcarvalh at jhsph.edu  Sat Jan 20 00:22:55 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Fri, 19 Jan 2007 18:22:55 -0500
Subject: [R] Bartlett test
In-Reply-To: <s5b0cfec.029@wokingmail.wokingprime.com>
References: <s5b0cfec.029@wokingmail.wokingprime.com>
Message-ID: <353B08AE-DB6A-4DF3-A365-1C425455E052@jhsph.edu>

The null hypothesis is that the variances do not differ across  
groups. The Bartlett test is sensitive to non-normality and that  
might lead you to consider something more robust (eg, Levene's test).

b

On Jan 19, 2007, at 9:04 AM, Matthieu Mourroux wrote:

> Bonjour,
> Je voudrais tester l'homosc?dasdicit? entre des groupes. J'aurais  
> alors aim? savoir quelle ?tait l'hypoth?se nulle du test de bartlett.
> Merci pour votre aide.
> Matthieu.


From jholtman at gmail.com  Sat Jan 20 00:48:44 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 19 Jan 2007 18:48:44 -0500
Subject: [R] split data set
In-Reply-To: <6.1.2.0.2.20070119123956.01c8a9b0@aiminy.mail.iastate.edu>
References: <6.1.2.0.2.20070119123956.01c8a9b0@aiminy.mail.iastate.edu>
Message-ID: <644e1f320701191548n445491a5n9a026f7867a202f2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070119/cf787e72/attachment.pl 

From rdiaz02 at gmail.com  Sat Jan 20 03:59:13 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Sat, 20 Jan 2007 03:59:13 +0100
Subject: [R] kate editor for R
In-Reply-To: <1169240161.4838.27.camel@localhost.localdomain>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
	<17840.55304.803214.860567@basebud.nulle.part>
	<200701191609.40533.rdiaz@cnio.es>
	<1169240161.4838.27.camel@localhost.localdomain>
Message-ID: <624934630701191859sfdbea6ey19b0a2a1929a04e9@mail.gmail.com>

Hi Marc,


Thanks a lot for the detailed explanation! I'll give it a try. (But
still, why emacs23? what is missing in v. 21 that you get in 23?).

Best,

R.

On 1/19/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> On Fri, 2007-01-19 at 16:09 +0100, Ramon Diaz-Uriarte wrote:
>
> <snip>
>
> > I had problems with one of the packages ecb depends upon (semantic ?), and
> > emacs-snapshot. IIRC it was a documented problem related to a bug in semantic
> > (?); maybe it's been fixed now. But what does emacs-snapshot-gtk provide you
> > now (besides the pretinness) that you'd miss with 21-4?
>
> <snip>
>
> Ramon,
>
> Just a quick heads up on the ECB issue.
>
> I am using Emacs 23 from CVS and had to update ECB and the associated
> packages to use this version of Emacs. I have emacs 23 installed and run
> from a separate download folder, so that I do not overwrite the
> installed stable version.
>
> I use the CEDET cedet-1.0pre3.tar.gz aggregate package from
> http://cedet.sourceforge.net/ as well as the ECB cvs snap shot package
> ecb.tar.gz from http://ecb.sourceforge.net/downloads.html.
>
> The CEDET package includes cogre, ede, eieio, semantic and speedbar.
>
> Extract these two files and then modify ~/.emacs with the following:
>
> ;; Load ECB
> (setq semantic-load-turn-everything-on t)
> (load-file "/PATH/TO/CEDET/cedet-1.0pre3/common/cedet.el")
>
> (add-to-list 'load-path "/PATH/TO/ECB/ecb-snap")
> (require 'ecb)
>
>
> And all seems well.
>
> HTH,
>
> Marc Schwartz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From marc_schwartz at comcast.net  Sat Jan 20 04:12:18 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 19 Jan 2007 21:12:18 -0600
Subject: [R] kate editor for R
In-Reply-To: <624934630701191859sfdbea6ey19b0a2a1929a04e9@mail.gmail.com>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
	<17840.55304.803214.860567@basebud.nulle.part>
	<200701191609.40533.rdiaz@cnio.es>
	<1169240161.4838.27.camel@localhost.localdomain>
	<624934630701191859sfdbea6ey19b0a2a1929a04e9@mail.gmail.com>
Message-ID: <1169262738.4853.7.camel@localhost.localdomain>

xft anti-aliasing is incorporated into the version 23 unicode trunk.

So it looks great on a hi-res LCD panel. Without xft, even using
Bitstream fonts, it was still pretty rough on the eyes.

It also fully supports GTK widgets, which is great if you are using
GNOME, which I do.

xft was added as a patch to version 22, but it was not very stable.

Note that version 23 is in alpha status, so use at your own risk if you
decide to pursue this. 21 is still the current stable release version,
but 23 has been rock solid for me.

I can provide you with a shell script to build it. Let me know.

Best regards,

Marc

On Sat, 2007-01-20 at 03:59 +0100, Ramon Diaz-Uriarte wrote:
> Hi Marc,
> 
> 
> Thanks a lot for the detailed explanation! I'll give it a try. (But
> still, why emacs23? what is missing in v. 21 that you get in 23?).
> 
> Best,
> 
> R.
> 
> On 1/19/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> > On Fri, 2007-01-19 at 16:09 +0100, Ramon Diaz-Uriarte wrote:
> >
> > <snip>
> >
> > > I had problems with one of the packages ecb depends upon (semantic ?), and
> > > emacs-snapshot. IIRC it was a documented problem related to a bug in semantic
> > > (?); maybe it's been fixed now. But what does emacs-snapshot-gtk provide you
> > > now (besides the pretinness) that you'd miss with 21-4?
> >
> > <snip>
> >
> > Ramon,
> >
> > Just a quick heads up on the ECB issue.
> >
> > I am using Emacs 23 from CVS and had to update ECB and the associated
> > packages to use this version of Emacs. I have emacs 23 installed and run
> > from a separate download folder, so that I do not overwrite the
> > installed stable version.
> >
> > I use the CEDET cedet-1.0pre3.tar.gz aggregate package from
> > http://cedet.sourceforge.net/ as well as the ECB cvs snap shot package
> > ecb.tar.gz from http://ecb.sourceforge.net/downloads.html.
> >
> > The CEDET package includes cogre, ede, eieio, semantic and speedbar.
> >
> > Extract these two files and then modify ~/.emacs with the following:
> >
> > ;; Load ECB
> > (setq semantic-load-turn-everything-on t)
> > (load-file "/PATH/TO/CEDET/cedet-1.0pre3/common/cedet.el")
> >
> > (add-to-list 'load-path "/PATH/TO/ECB/ecb-snap")
> > (require 'ecb)
> >
> >
> > And all seems well.
> >
> > HTH,
> >
> > Marc Schwartz
> >


From ripley at stats.ox.ac.uk  Sat Jan 20 08:08:49 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 20 Jan 2007 07:08:49 +0000 (GMT)
Subject: [R] Suggestion on how to improve efficiency when using
 MASS:::hubers on high-dimensional arrays
In-Reply-To: <D26799F8-AC4A-43F6-A083-DD620191D36B@jhsph.edu>
References: <D26799F8-AC4A-43F6-A083-DD620191D36B@jhsph.edu>
Message-ID: <Pine.LNX.4.64.0701200656300.12488@gannet.stats.ox.ac.uk>

The usual advice would seem to apply here:

- profile (the real application, not a toy example) to see where the
   bottlenecks are.
- rewrite those in C.

It is quite possible that hubers would benefit in your problem by being 
rewritten in C.  However, there is a reason why it was not.  It is a 
univariate location estimator, and using multiple univariate location 
estimators is not a robust multivariate location estimator.  So in so far 
as I understand your problem sketch, you would be better off with a 
multivariate estimator for your N outcomes.

There may be problems which need the application of very large numbers of 
univariate robust estimators, but they are not commonplace.

On Fri, 19 Jan 2007, Benilton Carvalho wrote:

> Hi Everyone,
>
> Given the scenario I have, I was wondering if anyone would be able to
> give me a hind on how to get the results from hubers() in a more
> efficient way.
>
> I have an outcome on an array [N x S x D].
>
> I also have a factor (levels 1,2,3) stored on a matrix N x S.
>
> My objective is to get "mu" and "sigma" for each of the N rows
> (outcome) stratified by the factor (levels 1, 2 and 3) for each of
> the D "levels", but using MASS:hubers().
>
> Ideally the final result would be an array [N x D x 3 x 2].
>
> The following toy example demonstrates what I want to do, and I'd
> like to improve the performance when working on my case, where S=400
> and N > 200000
>
> Thank you very much for any suggestion.
>
> benilton
>
> ## begin toy example
> set.seed(1)
> N <- 100
> S <- 5
> D <- 2
>
> outcome <- array(rnorm(N*S*D), dim=c(N, S, D))
> classes <- matrix(sample(c(1:3, NA), N*S, rep=T), ncol=S)
>
> results <- array(NA, dim=c(N, D, 3, 2))
>
> library(MASS)
> myHubers <- function(x)
>   if (length(x)>1) as.numeric(hubers(x))  else c(NA, NA)
>
> for (n in 1:N)
>   for (d in 1:D){
>     tmp <- outcome[n,,d]
>     grp <- classes[n,]
>     results[n, d,,] <- t(sapply(split(tmp, factor(grp, levels=1:3)),
> myHubers))
>   }
> ## end
>
> --
> Benilton Carvalho
> PhD Candidate
> Department of Biostatistics
> Johns Hopkins University
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From magepost at googlemail.com  Sat Jan 20 11:09:00 2007
From: magepost at googlemail.com (Oleg Sklyar)
Date: Sat, 20 Jan 2007 10:09:00 +0000
Subject: [R] Making TIFF images with rtiff
In-Reply-To: <6021CA6EF4C8374084D4F5A141F1CBBB664A32@msgebe23.mfad.mfroot.org>
References: <6021CA6EF4C8374084D4F5A141F1CBBB664A32@msgebe23.mfad.mfroot.org>
Message-ID: <399fc6f10701200209i1e84b0b7ma329618b364e723c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070120/54fb46cd/attachment.pl 

From rdiaz02 at gmail.com  Sat Jan 20 11:20:01 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Sat, 20 Jan 2007 11:20:01 +0100
Subject: [R] kate editor for R
In-Reply-To: <1169262738.4853.7.camel@localhost.localdomain>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
	<17840.55304.803214.860567@basebud.nulle.part>
	<200701191609.40533.rdiaz@cnio.es>
	<1169240161.4838.27.camel@localhost.localdomain>
	<624934630701191859sfdbea6ey19b0a2a1929a04e9@mail.gmail.com>
	<1169262738.4853.7.camel@localhost.localdomain>
Message-ID: <624934630701200220p445a64e6m87601f864db9a38f@mail.gmail.com>

On 1/20/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> xft anti-aliasing is incorporated into the version 23 unicode trunk.
>
> So it looks great on a hi-res LCD panel. Without xft, even using
> Bitstream fonts, it was still pretty rough on the eyes.
>


Humm, call me silly, but most of the time I do not like anti-aliased
fonts: I tend to agree with
http://modeemi.cs.tut.fi/~tuomov/ion/faq/entries/Blurred_fonts.html,
where he says "characters look like having been dragged through mud"
:-).

> It also fully supports GTK widgets, which is great if you are using
> GNOME, which I do.
>

But my .emacs gets rid of the toolbar and scroll bars on start-up (I
find toolbars confusing things that take up precious screen space),
and often work without the menubar (when I am doing familiar work). I
use ion3 (http://modeemi.cs.tut.fi/~tuomov/ion/), which, together with
wmii (and followed, at some distance, by fmwv), I find the most usable
window managers, and thus the look of widgets is not that relevant to
me.

So, for most practical purposes (except for resizing with the mouse) I
use emacs as if started with the -w command. (I know, I know, this
looks like going backwards ... must be a mid-life involution crisis
:-).


> xft was added as a patch to version 22, but it was not very stable.
>
> Note that version 23 is in alpha status, so use at your own risk if you
> decide to pursue this. 21 is still the current stable release version,
> but 23 has been rock solid for me.
>
> I can provide you with a shell script to build it. Let me know.
>


Let me try with the debian packages, and if I have problems, I'll
definitely start bugging you. Thanks a lot for your help!

Best,

R.

> Best regards,
>
> Marc
>
> On Sat, 2007-01-20 at 03:59 +0100, Ramon Diaz-Uriarte wrote:
> > Hi Marc,
> >
> >
> > Thanks a lot for the detailed explanation! I'll give it a try. (But
> > still, why emacs23? what is missing in v. 21 that you get in 23?).
> >
> > Best,
> >
> > R.
> >
> > On 1/19/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> > > On Fri, 2007-01-19 at 16:09 +0100, Ramon Diaz-Uriarte wrote:
> > >
> > > <snip>
> > >
> > > > I had problems with one of the packages ecb depends upon (semantic ?), and
> > > > emacs-snapshot. IIRC it was a documented problem related to a bug in semantic
> > > > (?); maybe it's been fixed now. But what does emacs-snapshot-gtk provide you
> > > > now (besides the pretinness) that you'd miss with 21-4?
> > >
> > > <snip>
> > >
> > > Ramon,
> > >
> > > Just a quick heads up on the ECB issue.
> > >
> > > I am using Emacs 23 from CVS and had to update ECB and the associated
> > > packages to use this version of Emacs. I have emacs 23 installed and run
> > > from a separate download folder, so that I do not overwrite the
> > > installed stable version.
> > >
> > > I use the CEDET cedet-1.0pre3.tar.gz aggregate package from
> > > http://cedet.sourceforge.net/ as well as the ECB cvs snap shot package
> > > ecb.tar.gz from http://ecb.sourceforge.net/downloads.html.
> > >
> > > The CEDET package includes cogre, ede, eieio, semantic and speedbar.
> > >
> > > Extract these two files and then modify ~/.emacs with the following:
> > >
> > > ;; Load ECB
> > > (setq semantic-load-turn-everything-on t)
> > > (load-file "/PATH/TO/CEDET/cedet-1.0pre3/common/cedet.el")
> > >
> > > (add-to-list 'load-path "/PATH/TO/ECB/ecb-snap")
> > > (require 'ecb)
> > >
> > >
> > > And all seems well.
> > >
> > > HTH,
> > >
> > > Marc Schwartz
> > >
>
>


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From arun.kumar.saha at gmail.com  Sat Jan 20 12:34:08 2007
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Sat, 20 Jan 2007 17:04:08 +0530
Subject: [R] Insert R logo
Message-ID: <d4c57560701200334g38390239p78b45391919f972c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070120/44ca147c/attachment.pl 

From tomas.goicoa at unavarra.es  Sat Jan 20 12:42:39 2007
From: tomas.goicoa at unavarra.es (Tomas Goicoa)
Date: Sat, 20 Jan 2007 12:42:39 +0100
Subject: [R] aov y lme
Message-ID: <6.0.3.0.0.20070120124213.01a57570@pop.unavarra.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070120/55aa7d07/attachment.pl 

From mothsailor at googlemail.com  Sat Jan 20 13:31:00 2007
From: mothsailor at googlemail.com (David Barron)
Date: Sat, 20 Jan 2007 12:31:00 +0000
Subject: [R] Insert R logo
In-Reply-To: <d4c57560701200334g38390239p78b45391919f972c@mail.gmail.com>
References: <d4c57560701200334g38390239p78b45391919f972c@mail.gmail.com>
Message-ID: <815b70590701200431g5e9745a7ra24b8c1ec9cc97dd@mail.gmail.com>

I expect you could do something with the addlogo function in the pixmap package.

On 20/01/07, Arun Kumar Saha <arun.kumar.saha at gmail.com> wrote:
> Dear all R users,
>
> I want to insert the R logo in every graphic that I made in my Statistical
> analysis using R. Can anyone tell me whether is it possible or not and if
> possible how to do this? your help will be highly appreciated.
>
> Thanks and Regards,
> Arun
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From mothsailor at googlemail.com  Sat Jan 20 13:52:30 2007
From: mothsailor at googlemail.com (David Barron)
Date: Sat, 20 Jan 2007 12:52:30 +0000
Subject: [R] Insert R logo
In-Reply-To: <d4c57560701200334g38390239p78b45391919f972c@mail.gmail.com>
References: <d4c57560701200334g38390239p78b45391919f972c@mail.gmail.com>
Message-ID: <815b70590701200452g42e169d1o5badc69b28ea21d6@mail.gmail.com>

Something like this:

library(pixmap)
# From the addlogo example
x <- read.pnm(system.file("pictures/logo.ppm", package="pixmap")[1])
fg <- matrix(c(0,1,0,1,0,.05,0,.05), ncol=4, byrow=TRUE)
split.screen(fg)
screen(1)
plot(rnorm(100))
screen(2)
addlogo(x,c(0,1),c(0,1))


On 20/01/07, Arun Kumar Saha <arun.kumar.saha at gmail.com> wrote:
> Dear all R users,
>
> I want to insert the R logo in every graphic that I made in my Statistical
> analysis using R. Can anyone tell me whether is it possible or not and if
> possible how to do this? your help will be highly appreciated.
>
> Thanks and Regards,
> Arun
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From blindglobe at gmail.com  Sat Jan 20 15:20:37 2007
From: blindglobe at gmail.com (AJ Rossini)
Date: Sat, 20 Jan 2007 15:20:37 +0100
Subject: [R] ECB/Sidebar/R  (Emacs)  was: Re:  kate editor for R
In-Reply-To: <17840.55304.803214.860567@basebud.nulle.part>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
	<17840.55304.803214.860567@basebud.nulle.part>
Message-ID: <200701201520.42653.blindglobe@gmail.com>

On Friday 19 January 2007 15:39, Dirk wrote:


> As I am doing more C++ work, I glanced at oo-browser, sidebar, ecb (all in
> Debian/Ubuntu).  Would a real Emacs hacker be able to these to R code too?

Dirk -

That functionality (though relatively minimal, i.e. ECB/sidebar support 
through imenu) should have existed for 2+ years now, at least it does for me.

best,
-tony

blindglobe at gmail.com
Muttenz, Switzerland.
"Commit early,commit often, and commit in a repository from which we can 
easily
roll-back your mistakes" (AJR, 4Jan05).
e at gmail.com
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070120/0f20e780/attachment.bin 

From edd at debian.org  Sat Jan 20 16:20:41 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 20 Jan 2007 09:20:41 -0600
Subject: [R] ECB/Sidebar/R  (Emacs)  was: Re:  kate editor for R
In-Reply-To: <200701201520.42653.blindglobe@gmail.com>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
	<17840.55304.803214.860567@basebud.nulle.part>
	<200701201520.42653.blindglobe@gmail.com>
Message-ID: <17842.13129.56162.112711@basebud.nulle.part>


Hi Tony,

On 20 January 2007 at 15:20, AJ Rossini wrote:
| On Friday 19 January 2007 15:39, Dirk wrote:
| > As I am doing more C++ work, I glanced at oo-browser, sidebar, ecb (all in
| > Debian/Ubuntu).  Would a real Emacs hacker be able to these to R code too?
| That functionality (though relatively minimal, i.e. ECB/sidebar support 
| through imenu) should have existed for 2+ years now, at least it does for me.

Just confirms my suspicion that even after all these years, I barely
scratched the surface of ess.  That '2+ years' old feature wouldn't happen to
be documented somewhere, would it?

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From marc_schwartz at comcast.net  Sat Jan 20 17:07:45 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 20 Jan 2007 10:07:45 -0600
Subject: [R] kate editor for R
In-Reply-To: <624934630701200220p445a64e6m87601f864db9a38f@mail.gmail.com>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
	<17840.55304.803214.860567@basebud.nulle.part>
	<200701191609.40533.rdiaz@cnio.es>
	<1169240161.4838.27.camel@localhost.localdomain>
	<624934630701191859sfdbea6ey19b0a2a1929a04e9@mail.gmail.com>
	<1169262738.4853.7.camel@localhost.localdomain>
	<624934630701200220p445a64e6m87601f864db9a38f@mail.gmail.com>
Message-ID: <1169309265.4853.28.camel@localhost.localdomain>

On Sat, 2007-01-20 at 11:20 +0100, Ramon Diaz-Uriarte wrote:
> On 1/20/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> > xft anti-aliasing is incorporated into the version 23 unicode trunk.
> >
> > So it looks great on a hi-res LCD panel. Without xft, even using
> > Bitstream fonts, it was still pretty rough on the eyes.
> >
> 
> 
> Humm, call me silly, but most of the time I do not like anti-aliased
> fonts: I tend to agree with
> http://modeemi.cs.tut.fi/~tuomov/ion/faq/entries/Blurred_fonts.html,
> where he says "characters look like having been dragged through mud"
> :-).
> 
> > It also fully supports GTK widgets, which is great if you are using
> > GNOME, which I do.
> >
> 
> But my .emacs gets rid of the toolbar and scroll bars on start-up (I
> find toolbars confusing things that take up precious screen space),
> and often work without the menubar (when I am doing familiar work). I
> use ion3 (http://modeemi.cs.tut.fi/~tuomov/ion/), which, together with
> wmii (and followed, at some distance, by fmwv), I find the most usable
> window managers, and thus the look of widgets is not that relevant to
> me.
> 
> So, for most practical purposes (except for resizing with the mouse) I
> use emacs as if started with the -w command. (I know, I know, this
> looks like going backwards ... must be a mid-life involution crisis
> :-).

We'll drag you kicking and screaming into the 21st century...

;-)

> > xft was added as a patch to version 22, but it was not very stable.
> >
> > Note that version 23 is in alpha status, so use at your own risk if you
> > decide to pursue this. 21 is still the current stable release version,
> > but 23 has been rock solid for me.
> >
> > I can provide you with a shell script to build it. Let me know.
> >
> 
> 
> Let me try with the debian packages, and if I have problems, I'll
> definitely start bugging you. Thanks a lot for your help!
> 
> Best,
> 
> R.

FWIW, here are some screen shots so that you can get a feel for what it
looks like. This is using two 1600x1200 lcd panels.

1. Basic view of main window, showing ECB and ESS:

  http://home.comcast.net/~marc_schwartz/emacs23.png

2. Full screen (3200x1600 using nVidia TwinView) capture to show GTK
file selection widget:

  http://home.comcast.net/~marc_schwartz/emacs23-2.png

3. View of main window to show the integration of SVN version control,
which I use my all of my R code:

  http://home.comcast.net/~marc_schwartz/emacs23-3.png


No doubt that the use of xft is a personal choice, and some folks do not
like it, perhaps notably on CRTs.  As I have gotten older and need
bi-focals for computer work and reading, I find the use of xft much
easier and I am less prone to eye strain, given how many hours I
typically spend working each day.

HTH,

Marc


From marc_schwartz at comcast.net  Sat Jan 20 18:14:26 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 20 Jan 2007 11:14:26 -0600
Subject: [R] Offtopic: emacs 23, was  kate editor for R
In-Reply-To: <1169312707.4853.51.camel@localhost.localdomain>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
	<17840.55304.803214.860567@basebud.nulle.part>
	<200701191609.40533.rdiaz@cnio.es>
	<1169240161.4838.27.camel@localhost.localdomain>
	<624934630701191859sfdbea6ey19b0a2a1929a04e9@mail.gmail.com>
	<45B24558.5070202@biostat.ku.dk>
	<1169312707.4853.51.camel@localhost.localdomain>
Message-ID: <1169313266.4853.54.camel@localhost.localdomain>

Re-sending to the list as I just got nailed by the "too many recipients"
issue...

On Sat, 2007-01-20 at 11:05 -0600, Marc Schwartz wrote:
> On Sat, 2007-01-20 at 17:37 +0100, Peter Dalgaard wrote:
> > Ramon Diaz-Uriarte wrote:
> > > Hi Marc,
> > >
> > >
> > > Thanks a lot for the detailed explanation! I'll give it a try. (But
> > > still, why emacs23? what is missing in v. 21 that you get in 23?).
> > >
> > > Best,
> > >
> > > R.
> > >   
> > Ability to load files with UTF-8 characters in the name? (This is pretty 
> > maddening if you find yourself with such a beast.)
> > 
> > BTW, any inkling when/whether this is heading for Fedora N?

Peter,

21.4 is what is presently in the FC7 development trunk (aka rawhide), so
I would not expect to see it as a mainstream offering for some time.

Needless to say, 23 is still alpha, so the FC timeline is likely more
dependent on the upstream timeline to bring 23 to a stable release.

Bill Nottingham at RH recently posted this summary of planned key
updates to FC7:

http://www.redhat.com/archives/fedora-devel-list/2007-January/msg00091.html

and based upon subsequent communications, there has been at least one
addition, which is the previously discussed replacement of teTeX with
TeXLive:

  http://fedoraproject.org/wiki/Releases/FeatureTexLive

BTW, for the Ubuntu users in the audience, I happened to come across
these sites:

  http://peadrop.com/blog/2007/01/06/pretty-emacs/

  http://debs.peadrop.com/dists/edgy/backports/

HTH,

Marc


From lindeman at bard.edu  Sat Jan 20 19:30:33 2007
From: lindeman at bard.edu (lindeman at bard.edu)
Date: Sat, 20 Jan 2007 13:30:33 -0500
Subject: [R] simple q: returning a logical vector of substring matches
Message-ID: <20070120133033.7ih7ktc2kgk4k0so@webmail.bard.edu>

I'm a relative R novice, and sometimes the simple things trip me up.

Suppose I have

a <- c("apple", "pear")

and I want a logical vector of whether each of these strings contains  
"ear" (in this case, F T). What is the idiom?

Quizzically,
Mark Lindeman


From christos at nuverabio.com  Sat Jan 20 19:58:51 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Sat, 20 Jan 2007 13:58:51 -0500
Subject: [R] simple q: returning a logical vector of substring matches
In-Reply-To: <20070120133033.7ih7ktc2kgk4k0so@webmail.bard.edu>
References: <20070120133033.7ih7ktc2kgk4k0so@webmail.bard.edu>
Message-ID: <001401c73cc5$0684f870$0202a8c0@headquarters.silicoinsights>

You can try the following:

 a == grep("ear", a, value=T)

-Christos

Christos Hatzis, Ph.D.
Nuvera Biosciences, Inc.
400 West Cummings Park
Suite 5350
Woburn, MA 01801
Tel: 781-938-3830
www.nuverabio.com
 
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of lindeman at bard.edu
Sent: Saturday, January 20, 2007 1:31 PM
To: r-help at stat.math.ethz.ch
Subject: [R] simple q: returning a logical vector of substring matches

I'm a relative R novice, and sometimes the simple things trip me up.

Suppose I have

a <- c("apple", "pear")

and I want a logical vector of whether each of these strings contains "ear"
(in this case, F T). What is the idiom?

Quizzically,
Mark Lindeman

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at comcast.net  Sat Jan 20 19:58:04 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 20 Jan 2007 12:58:04 -0600
Subject: [R] simple q: returning a logical vector of substring matches
In-Reply-To: <20070120133033.7ih7ktc2kgk4k0so@webmail.bard.edu>
References: <20070120133033.7ih7ktc2kgk4k0so@webmail.bard.edu>
Message-ID: <1169319484.4853.76.camel@localhost.localdomain>

On Sat, 2007-01-20 at 13:30 -0500, lindeman at bard.edu wrote:
> I'm a relative R novice, and sometimes the simple things trip me up.
> 
> Suppose I have
> 
> a <- c("apple", "pear")
> 
> and I want a logical vector of whether each of these strings contains  
> "ear" (in this case, F T). What is the idiom?
> 
> Quizzically,
> Mark Lindeman

See ?grep and ?regexp

a <- c("apple", "pear")

> grep("ear", a)
[1] 2

> grep("ear", a, value = TRUE)
[1] "pear"

If you actually want the answer to be FALSE TRUE, then:

> a %in% grep("ear", a, value = TRUE)
[1] FALSE  TRUE

In that case see ?"%in%"

HTH,

Marc Schwartz


From jholtman at gmail.com  Sat Jan 20 19:59:24 2007
From: jholtman at gmail.com (jim holtman)
Date: Sat, 20 Jan 2007 13:59:24 -0500
Subject: [R] simple q: returning a logical vector of substring matches
In-Reply-To: <20070120133033.7ih7ktc2kgk4k0so@webmail.bard.edu>
References: <20070120133033.7ih7ktc2kgk4k0so@webmail.bard.edu>
Message-ID: <644e1f320701201059p381bd1b3y8f2f6e874ac25770@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070120/9394709e/attachment.pl 

From robert.barber at comcast.net  Sat Jan 20 20:00:18 2007
From: robert.barber at comcast.net (Robert Barber)
Date: Sat, 20 Jan 2007 14:00:18 -0500
Subject: [R] Question about converting from square roots to decimals and back
Message-ID: <1169319618.3982.21.camel@localhost.localdomain>

Hi,

I apologize if there is a simple answer to this question that I've
missed.  I did search the mailing list but I might not have used the
right keywords. Why does sum(A3^2) give the result of 1, but
sum(A3^2)==1 give the result of FALSE? 

> A3<-matrix(nrow=3,c(1/(2^.5),1/(2^.5),0))
> A3
          [,1]
[1,] 0.7071068
[2,] 0.7071068
[3,] 0.0000000
> sum(A3^2)
[1] 1
> sum(A3^2)^.5
[1] 1
> sum(A3^2)==1		# here's the part I don't understand
[1] FALSE
> sum(A3^2)^.5==1	# here's the part I don't understand
[1] FALSE

I realize that it has something to do with the conversion of the square
roots into decimals.  But shouldn't it then give me some number other
than 1 as the result for sum(A3^2)?  Are there other ways to do this
than what I've tried?  I'm trying to confirm that A3 is a unit vector.

Thank you for your help.

Bob B.


From p.dalgaard at biostat.ku.dk  Sat Jan 20 17:37:44 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Sat, 20 Jan 2007 17:37:44 +0100
Subject: [R] Offtopic: emacs 23, was  kate editor for R
In-Reply-To: <624934630701191859sfdbea6ey19b0a2a1929a04e9@mail.gmail.com>
References: <45B02D46.1090709@vanderbilt.edu>	<200701191118.26761.rdiaz@cnio.es>	<17840.55304.803214.860567@basebud.nulle.part>	<200701191609.40533.rdiaz@cnio.es>	<1169240161.4838.27.camel@localhost.localdomain>
	<624934630701191859sfdbea6ey19b0a2a1929a04e9@mail.gmail.com>
Message-ID: <45B24558.5070202@biostat.ku.dk>

Ramon Diaz-Uriarte wrote:
> Hi Marc,
>
>
> Thanks a lot for the detailed explanation! I'll give it a try. (But
> still, why emacs23? what is missing in v. 21 that you get in 23?).
>
> Best,
>
> R.
>   
Ability to load files with UTF-8 characters in the name? (This is pretty 
maddening if you find yourself with such a beast.)

BTW, any inkling when/whether this is heading for Fedora N?


From marc_schwartz at comcast.net  Sat Jan 20 20:09:19 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 20 Jan 2007 13:09:19 -0600
Subject: [R] Question about converting from square roots to decimals	and
	back
In-Reply-To: <1169319618.3982.21.camel@localhost.localdomain>
References: <1169319618.3982.21.camel@localhost.localdomain>
Message-ID: <1169320159.4853.87.camel@localhost.localdomain>

On Sat, 2007-01-20 at 14:00 -0500, Robert Barber wrote:
> Hi,
> 
> I apologize if there is a simple answer to this question that I've
> missed.  I did search the mailing list but I might not have used the
> right keywords. Why does sum(A3^2) give the result of 1, but
> sum(A3^2)==1 give the result of FALSE? 
> 
> > A3<-matrix(nrow=3,c(1/(2^.5),1/(2^.5),0))
> > A3
>           [,1]
> [1,] 0.7071068
> [2,] 0.7071068
> [3,] 0.0000000
> > sum(A3^2)
> [1] 1
> > sum(A3^2)^.5
> [1] 1
> > sum(A3^2)==1		# here's the part I don't understand
> [1] FALSE
> > sum(A3^2)^.5==1	# here's the part I don't understand
> [1] FALSE
> 
> I realize that it has something to do with the conversion of the square
> roots into decimals.  But shouldn't it then give me some number other
> than 1 as the result for sum(A3^2)?  Are there other ways to do this
> than what I've tried?  I'm trying to confirm that A3 is a unit vector.
> 
> Thank you for your help.
> 
> Bob B.

See R FAQ:

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f

> print(sum(A3^2)^.5, 20)
[1] 0.99999999999999988898

HTH,

Marc Schwartz


From robert.barber at comcast.net  Sat Jan 20 20:15:48 2007
From: robert.barber at comcast.net (Robert Barber)
Date: Sat, 20 Jan 2007 14:15:48 -0500
Subject: [R] Question about converting from square roots to decimals	and	back
In-Reply-To: 1169319618.3982.21.camel@localhost.localdomain
Message-ID: <1169320548.3982.23.camel@localhost.localdomain>

Thank you very much.  That was what I needed to know.  

Bob B.


From A.Robinson at ms.unimelb.edu.au  Sat Jan 20 20:15:21 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Sun, 21 Jan 2007 06:15:21 +1100
Subject: [R] Question about converting from square roots to decimals and
	back
In-Reply-To: <1169319618.3982.21.camel@localhost.localdomain>
References: <1169319618.3982.21.camel@localhost.localdomain>
Message-ID: <20070120191521.GG39898@ms.unimelb.edu.au>

Hi Robert,

does this help?

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f

A3<-matrix(nrow=3,c(1/(2^.5),1/(2^.5),0))
sum(A3^2)==1
all.equal(sum(A3^2), 1)


Cheers,

Andrew


On Sat, Jan 20, 2007 at 02:00:18PM -0500, Robert Barber wrote:
> Hi,
> 
> I apologize if there is a simple answer to this question that I've
> missed.  I did search the mailing list but I might not have used the
> right keywords. Why does sum(A3^2) give the result of 1, but
> sum(A3^2)==1 give the result of FALSE? 
> 
> > A3<-matrix(nrow=3,c(1/(2^.5),1/(2^.5),0))
> > A3
>           [,1]
> [1,] 0.7071068
> [2,] 0.7071068
> [3,] 0.0000000
> > sum(A3^2)
> [1] 1
> > sum(A3^2)^.5
> [1] 1
> > sum(A3^2)==1		# here's the part I don't understand
> [1] FALSE
> > sum(A3^2)^.5==1	# here's the part I don't understand
> [1] FALSE
> 
> I realize that it has something to do with the conversion of the square
> roots into decimals.  But shouldn't it then give me some number other
> than 1 as the result for sum(A3^2)?  Are there other ways to do this
> than what I've tried?  I'm trying to confirm that A3 is a unit vector.
> 
> Thank you for your help.
> 
> Bob B.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From ggrothendieck at gmail.com  Sat Jan 20 20:32:18 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 20 Jan 2007 14:32:18 -0500
Subject: [R] simple q: returning a logical vector of substring matches
In-Reply-To: <20070120133033.7ih7ktc2kgk4k0so@webmail.bard.edu>
References: <20070120133033.7ih7ktc2kgk4k0so@webmail.bard.edu>
Message-ID: <971536df0701201132i7a855991jb1975ef503b3524e@mail.gmail.com>

Using the builtin month.abb try this:

  regexpr("ov", month.abb) > 0

Although not needed here, if "ov" were a character string that could have
special characters such as . and * that have special meaning in a regular
expression then do this to prevent such interpretation:

  regexpr("ov", month.abb, fixed = TRUE) > 0

See ?regexpr


On 1/20/07, lindeman at bard.edu <lindeman at bard.edu> wrote:
> I'm a relative R novice, and sometimes the simple things trip me up.
>
> Suppose I have
>
> a <- c("apple", "pear")
>
> and I want a logical vector of whether each of these strings contains
> "ear" (in this case, F T). What is the idiom?
>
> Quizzically,
> Mark Lindeman
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Ted.Harding at manchester.ac.uk  Sat Jan 20 20:53:07 2007
From: Ted.Harding at manchester.ac.uk ( (Ted Harding))
Date: Sat, 20 Jan 2007 19:53:07 -0000 (GMT)
Subject: [R] Question about converting from square roots to decimals
In-Reply-To: <1169319618.3982.21.camel@localhost.localdomain>
Message-ID: <XFMail.070120195307.Ted.Harding@manchester.ac.uk>

On 20-Jan-07 Robert Barber wrote:
> Hi,
> 
> I apologize if there is a simple answer to this question that I've
> missed.  I did search the mailing list but I might not have used the
> right keywords. Why does sum(A3^2) give the result of 1, but
> sum(A3^2)==1 give the result of FALSE? 
> 
>> A3<-matrix(nrow=3,c(1/(2^.5),1/(2^.5),0))
>> A3
>           [,1]
> [1,] 0.7071068
> [2,] 0.7071068
> [3,] 0.0000000
>> sum(A3^2)
> [1] 1
>> sum(A3^2)^.5
> [1] 1
>> sum(A3^2)==1         # here's the part I don't understand
> [1] FALSE
>> sum(A3^2)^.5==1      # here's the part I don't understand
> [1] FALSE
> 
> I realize that it has something to do with the conversion of the square
> roots into decimals.  But shouldn't it then give me some number other
> than 1 as the result for sum(A3^2)?  Are there other ways to do this
> than what I've tried?  I'm trying to confirm that A3 is a unit vector.

This is an instance of what must be a candidate for the MFAQAT
(most frequently asked qustion of all time).

The nub of the matter can be found in FAQ 7.31:

  http://www.r-project.org/  -->  FAQs

where, at 7.31, it says:

  7.31 Why doesn't R think these numbers are equal?

  The only numbers that can be represented exactly in R's
  numeric type are integers and fractions whose denominator
  is a power of 2. Other numbers have to be rounded to
  (typically) 53 binary digits accuracy. As a result, two
  floating point numbers will not reliably be equal unless
  they have been computed by the same algorithm, and not
  always even then. For example

     R> a <- sqrt(2)
     R> a * a == 2
     [1] FALSE
     R> a * a - 2
     [1] 4.440892e-16

  The function all.equal() compares two objects using a
  numeric tolerance of .Machine$double.eps ^ 0.5. If you
  want much greater accuracy than this you will need to
  consider error propagation carefully.

  For more information, see e.g. David Goldberg (1991),
  "What Every Computer Scientist Should Know About Floating-Point
  Arithmetic", ACM Computing Surveys, 23/1, 5-48, also
  available via
  http://docs.sun.com/source/806-3568/ncg_goldberg.html.

However, what this FAQ does not point out is that "==" tests
for exact equality, and even the 'help' page

  ?"=="

is not as explicit about this:

  For numerical values, remember '==' and '!=' do not allow
  for the finite representation of fractions, nor for
  rounding error. Using 'all.equal' with 'identical' is
  almost always preferable.  See the examples.

Nor does the help

  ?all.equal

give the explanation:

  'all.equal(x,y)' is a utility to compare R objects 'x'
  and 'y' testing "near equality". ...

and a user who is not aware of the imprecision problems inherent
in computer arithmetic may well not see the point of testing for
"near equality" when the user expects exact equality mathematically.

Statistically speaking, the frequency of occurrence of variants
of this question is a phenomenon!

I hypothesise that it arises from a combination of two main
circumstances:

a) Many users who are unfamiliar with the technical details of
   finite-length binary arithmetic will not be expecting that
   there could be a problem of this kind in the first place.
   So, when it occurs, they will simply be puzzled.

b) It's actually quite difficult to find your way to the above
   explanation in the FAQs. First, you need to anticipate that
   this is the sort of thing that will be a FAQ. If you're subject
   to (a) above, you may not be thinking on these lines.

   Secondly, even if you get as far as looking at the FAQs at
   the above URL, you have a lot of scrolling down to do before
   you find the question

     7.31 Why doesn't R think these numbers are equal?

   and even then, if you blink you'll miss it: trying it just
   now, I in fact didn't spot it in the list of questions
   immediately below the header

     7 R Miscellanea

   even though I already knew it was there somewhere and was
   actively looking for it. It was only when I landed on the
   full FAQ itself that I recognised it.

   It would have been quicker (I just tried that too) to start
   at the top of the FAQs page, and use the browser's Search
   tool to search for == : the sixth occurrence of "==" was it!

   But, even then, you still need to be thinking that the answer
   is to be found in connection with "==". If you're subject to
   (a), you'll be wondering instead why the answer was wrong.

This is such a frequently occurring issue that I feel there is
a case for a prominently displayed link, maybe in FAQs but maybe
better at the top level of r-project.org, to a brief but adequate
discourse with a title like

  Arithmetic [im]precision and apparent errors in R

What do others think?

Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 20-Jan-07                                       Time: 19:53:03
------------------------------ XFMail ------------------------------


From huber at ebi.ac.uk  Sat Jan 20 21:10:01 2007
From: huber at ebi.ac.uk (Wolfgang Huber)
Date: Sat, 20 Jan 2007 20:10:01 +0000
Subject: [R] Error in heatmap()
In-Reply-To: <6DB832784E7C7543B22376A1C62E8CDD01175006@MAIL02.sdcelgene.celgene.com>
References: <6DB832784E7C7543B22376A1C62E8CDD01175006@MAIL02.sdcelgene.celgene.com>
Message-ID: <45B27719.8070308@ebi.ac.uk>

Dear Yuhong,

heatmap deals gracefully with sparse occurences of NA in the matrix, but 
will fail if whole rows or columns are NA. Try preprocessing your xx as 
follows:

xx = xx[rowSums(!is.na(x))!=0, colSums(!is.na(x))!=0]

  Best wishes
   Wolfgang

------------------------------------------------------------------
Wolfgang Huber  EBI/EMBL  Cambridge UK  http://www.ebi.ac.uk/huber

> Hi, 
> 
>  
> 
> I run into following error when using heatmap() for data matrix "xx".
> Any help is appreciated? "xx" contains many "NA"s.
> 
>  
> 
>> hv <- heatmap(data.matrix(xx))
> 
> Error in hclustfun(distfun(if (symm) x else t(x))) : 
> 
>         NA/NaN/Inf in foreign function call (arg 11)
> 
>  
> 
> Thanks a lot.
> 
>  
> 
> Yuhong
>


From dusa.adrian at gmail.com  Sat Jan 20 23:14:45 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Sun, 21 Jan 2007 00:14:45 +0200
Subject: [R] comparing two matrices
Message-ID: <200701210014.45644.dusa.adrian@gmail.com>


Dear helpeRs,

I have two matrices:
mat1 <- expand.grid(0:2, 0:2, 0:2)
mat2 <- aa[c(19, 16, 13, 24, 8), ]

where mat2 is always a subset of mat1

I need to find the corersponding row numbers in mat1 for each row in mat2.
For this I have the following code:

apply(mat2, 1, function(x) {
    which(apply(mat1, 1, function(y) {
        sum(x == y)
        }) == ncol(mat1))
    })

The code is vectorized, but I wonder if there is a simpler (hence faster) 
matrix computation that I miss.

Thank you,
Adrian


-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From christos at nuverabio.com  Sat Jan 20 23:56:09 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Sat, 20 Jan 2007 17:56:09 -0500
Subject: [R] comparing two matrices
In-Reply-To: <200701210014.45644.dusa.adrian@gmail.com>
References: <200701210014.45644.dusa.adrian@gmail.com>
Message-ID: <002201c73ce6$2d9a73b0$0202a8c0@headquarters.silicoinsights>

Here is a slightly more compact version of your function which might run
faster (I did not test timings) since it does not use the sum:

apply(mat2, 1, function(x) which(apply(mat1, 1, function(y) all(x == y)) ==
TRUE))

-Christos
 
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Adrian Dusa
Sent: Saturday, January 20, 2007 5:15 PM
To: r-help at stat.math.ethz.ch
Subject: [R] comparing two matrices


Dear helpeRs,

I have two matrices:
mat1 <- expand.grid(0:2, 0:2, 0:2)
mat2 <- aa[c(19, 16, 13, 24, 8), ]

where mat2 is always a subset of mat1

I need to find the corersponding row numbers in mat1 for each row in mat2.
For this I have the following code:

apply(mat2, 1, function(x) {
    which(apply(mat1, 1, function(y) {
        sum(x == y)
        }) == ncol(mat1))
    })

The code is vectorized, but I wonder if there is a simpler (hence faster)
matrix computation that I miss.

Thank you,
Adrian


--
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Greg.Snow at intermountainmail.org  Sun Jan 21 00:00:45 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Sat, 20 Jan 2007 16:00:45 -0700
Subject: [R] Insert R logo
Message-ID: <0bc001c73ce6$cfd562f3$2e80320a@CO.IHC.COM>

Look at the last example in the help for the subplot function in the TeachingDemos package.

-----Original Message-----
From: "Arun Kumar Saha" <arun.kumar.saha at gmail.com>
To: "r-help at stat.math.ethz.ch" <R-help at stat.math.ethz.ch>
Sent: 1/20/07 4:35 AM
Subject: [R] Insert R logo

Dear all R users,

I want to insert the R logo in every graphic that I made in my Statistical
analysis using R. Can anyone tell me whether is it possible or not and if
possible how to do this? your help will be highly appreciated.

Thanks and Regards,
Arun

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at comcast.net  Sun Jan 21 00:06:30 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 20 Jan 2007 17:06:30 -0600
Subject: [R] comparing two matrices
In-Reply-To: <200701210014.45644.dusa.adrian@gmail.com>
References: <200701210014.45644.dusa.adrian@gmail.com>
Message-ID: <1169334390.4853.97.camel@localhost.localdomain>

On Sun, 2007-01-21 at 00:14 +0200, Adrian Dusa wrote:
> Dear helpeRs,
> 
> I have two matrices:
> mat1 <- expand.grid(0:2, 0:2, 0:2)
> mat2 <- aa[c(19, 16, 13, 24, 8), ]
> 
> where mat2 is always a subset of mat1
> 
> I need to find the corersponding row numbers in mat1 for each row in mat2.
> For this I have the following code:
> 
> apply(mat2, 1, function(x) {
>     which(apply(mat1, 1, function(y) {
>         sum(x == y)
>         }) == ncol(mat1))
>     })
> 
> The code is vectorized, but I wonder if there is a simpler (hence faster) 
> matrix computation that I miss.
> 
> Thank you,
> Adrian


I have not fully tested this, but how about:

mat1 <- matrix(1:20, ncol = 4, byrow = TRUE)
mat2 <- matrix(1:60, ncol = 4, byrow = TRUE)
mat2 <- mat2[sample(15), ]

> mat1
     [,1] [,2] [,3] [,4]
[1,]    1    2    3    4
[2,]    5    6    7    8
[3,]    9   10   11   12
[4,]   13   14   15   16
[5,]   17   18   19   20

> mat2
      [,1] [,2] [,3] [,4]
 [1,]   13   14   15   16
 [2,]    5    6    7    8
 [3,]   41   42   43   44
 [4,]   17   18   19   20
 [5,]   21   22   23   24
 [6,]   25   26   27   28
 [7,]   53   54   55   56
 [8,]    9   10   11   12
 [9,]   57   58   59   60
[10,]   33   34   35   36
[11,]   49   50   51   52
[12,]   45   46   47   48
[13,]    1    2    3    4
[14,]   29   30   31   32
[15,]   37   38   39   40

> which(apply(matrix(mat2 %in% mat1, dim(mat2)), 1, all))
[1]  1  2  4  8 13


HTH,

Marc Schwartz


From marc_schwartz at comcast.net  Sun Jan 21 02:25:37 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 20 Jan 2007 19:25:37 -0600
Subject: [R] Question about converting from square roots to decimals
In-Reply-To: <XFMail.070120195307.Ted.Harding@manchester.ac.uk>
References: <XFMail.070120195307.Ted.Harding@manchester.ac.uk>
Message-ID: <1169342737.4853.151.camel@localhost.localdomain>

On Sat, 2007-01-20 at 19:53 +0000, Ted.Harding at manchester.ac.uk wrote:
> On 20-Jan-07 Robert Barber wrote:
> > Hi,
> > 
> > I apologize if there is a simple answer to this question that I've
> > missed.  I did search the mailing list but I might not have used the
> > right keywords. Why does sum(A3^2) give the result of 1, but
> > sum(A3^2)==1 give the result of FALSE? 
> > 
> >> A3<-matrix(nrow=3,c(1/(2^.5),1/(2^.5),0))
> >> A3
> >           [,1]
> > [1,] 0.7071068
> > [2,] 0.7071068
> > [3,] 0.0000000
> >> sum(A3^2)
> > [1] 1
> >> sum(A3^2)^.5
> > [1] 1
> >> sum(A3^2)==1         # here's the part I don't understand
> > [1] FALSE
> >> sum(A3^2)^.5==1      # here's the part I don't understand
> > [1] FALSE
> > 
> > I realize that it has something to do with the conversion of the square
> > roots into decimals.  But shouldn't it then give me some number other
> > than 1 as the result for sum(A3^2)?  Are there other ways to do this
> > than what I've tried?  I'm trying to confirm that A3 is a unit vector.
> 
> This is an instance of what must be a candidate for the MFAQAT
> (most frequently asked qustion of all time).
> 
> The nub of the matter can be found in FAQ 7.31:
> 
>   http://www.r-project.org/  -->  FAQs
> 
> where, at 7.31, it says:
> 
>   7.31 Why doesn't R think these numbers are equal?
> 
>   The only numbers that can be represented exactly in R's
>   numeric type are integers and fractions whose denominator
>   is a power of 2. Other numbers have to be rounded to
>   (typically) 53 binary digits accuracy. As a result, two
>   floating point numbers will not reliably be equal unless
>   they have been computed by the same algorithm, and not
>   always even then. For example
> 
>      R> a <- sqrt(2)
>      R> a * a == 2
>      [1] FALSE
>      R> a * a - 2
>      [1] 4.440892e-16
> 
>   The function all.equal() compares two objects using a
>   numeric tolerance of .Machine$double.eps ^ 0.5. If you
>   want much greater accuracy than this you will need to
>   consider error propagation carefully.
> 
>   For more information, see e.g. David Goldberg (1991),
>   "What Every Computer Scientist Should Know About Floating-Point
>   Arithmetic", ACM Computing Surveys, 23/1, 5-48, also
>   available via
>   http://docs.sun.com/source/806-3568/ncg_goldberg.html.
> 
> However, what this FAQ does not point out is that "==" tests
> for exact equality, and even the 'help' page
> 
>   ?"=="
> 
> is not as explicit about this:
> 
>   For numerical values, remember '==' and '!=' do not allow
>   for the finite representation of fractions, nor for
>   rounding error. Using 'all.equal' with 'identical' is
>   almost always preferable.  See the examples.
> 
> Nor does the help
> 
>   ?all.equal
> 
> give the explanation:
> 
>   'all.equal(x,y)' is a utility to compare R objects 'x'
>   and 'y' testing "near equality". ...
> 
> and a user who is not aware of the imprecision problems inherent
> in computer arithmetic may well not see the point of testing for
> "near equality" when the user expects exact equality mathematically.
> 
> Statistically speaking, the frequency of occurrence of variants
> of this question is a phenomenon!
> 
> I hypothesise that it arises from a combination of two main
> circumstances:
> 
> a) Many users who are unfamiliar with the technical details of
>    finite-length binary arithmetic will not be expecting that
>    there could be a problem of this kind in the first place.
>    So, when it occurs, they will simply be puzzled.
> 
> b) It's actually quite difficult to find your way to the above
>    explanation in the FAQs. First, you need to anticipate that
>    this is the sort of thing that will be a FAQ. If you're subject
>    to (a) above, you may not be thinking on these lines.
> 
>    Secondly, even if you get as far as looking at the FAQs at
>    the above URL, you have a lot of scrolling down to do before
>    you find the question
> 
>      7.31 Why doesn't R think these numbers are equal?
> 
>    and even then, if you blink you'll miss it: trying it just
>    now, I in fact didn't spot it in the list of questions
>    immediately below the header
> 
>      7 R Miscellanea
> 
>    even though I already knew it was there somewhere and was
>    actively looking for it. It was only when I landed on the
>    full FAQ itself that I recognised it.
> 
>    It would have been quicker (I just tried that too) to start
>    at the top of the FAQs page, and use the browser's Search
>    tool to search for == : the sixth occurrence of "==" was it!
> 
>    But, even then, you still need to be thinking that the answer
>    is to be found in connection with "==". If you're subject to
>    (a), you'll be wondering instead why the answer was wrong.
> 
> This is such a frequently occurring issue that I feel there is
> a case for a prominently displayed link, maybe in FAQs but maybe
> better at the top level of r-project.org, to a brief but adequate
> discourse with a title like
> 
>   Arithmetic [im]precision and apparent errors in R
> 
> What do others think?
> 
> Ted.

Ted,

I think in Robert's case, and perhaps in others as well, there are two
confounding issues.

The first is the issue that we all have pointed to, which is the
representation of floating point numbers on binary computers.

The second, which tends to come up relatively frequently as well, is
understanding the difference between the precision of what is displayed
in R versus the precision of numeric objects as they are actually stored
internally.

For example, if Robert had seen:

options(digits = 20)
A3 <- matrix(nrow=3,c(1/(2^.5),1/(2^.5),0))

> sum(A3^2)
[1] 0.99999999999999977796

> sum(A3^2)^.5
[1] 0.99999999999999988898

I suspect that his questions might have been different.

I am not advocating changing the default for 'digits', just pointing out
that the two issues are somewhat inter-related in being a source of
confusion for useRs.

In addition, I would put some level of blame on the exposure to the use
of spreadsheets, which is a whole different matter and has come up on
the list frequently as well. 

Principally, if Excel and now unfortunately, OO.org's Calc, did not
include the modification of the IEEE754 standard, whereby numbers "close
to zero" are rounded to 0, users might have a differing appreciation for
how numbers are represented and displayed on binary computers. For the
sake of completeness, Gnumeric does not engage in such deviant behavior.

I won't belabor the point, but in Calc, the cell formula:

  =0.5 - 0.4 - 0.1 

displays (manually increasing to 20 places):

  0.00000000000000000000

Whereas in R:

> 0.5 - 0.4 - 0.1 
[1] -2.775558e-17

Thanks to Excel, users are insulated from such details, whereas in R,
users are expected to be more savvy in such things and to understand
what they are doing in more detail.

I think that arguments could be made as to the relative frequency of
such queries and whether certain questions should be given more or less
prominence. Two thoughts however to address your points:

1. Perhaps the Miscellanea section of the FAQ is better labelled as
"Unexpected Results in R and Miscellanea".

2. Since we tend to refer folks to the R Posting Guide and there is of
course a link to it at the bottom of all posts, perhaps some brief
comments on both floating point representations and precision issues
(display versus storage) could be added to a new section there.

That's my US\$ $\displaystyle e^{i\pi} + \sum_{n=1}^\infty \frac{1}{2^n} + 2(10^{-2})

:-)

Regards,

Marc


From JILWIL at SAFECO.com  Sun Jan 21 02:26:50 2007
From: JILWIL at SAFECO.com (WILLIE, JILL)
Date: Sat, 20 Jan 2007 17:26:50 -0800
Subject: [R] Can we do GLM on 2GB data set with R?
Message-ID: <916551423F01504BA339BF69CBA3BE72016E79E6@psmrdcex18.psm.pin.safeco.com>

We are wanting to use R instead of/in addition to our existing stats
package because of it's huge assortment of stat functions.  But, we
routinely need to fit GLM models to files that are approximately 2-4GB
(as SQL tables, un-indexed, w/tinyint-sized fields except for the
response & weight variables).  Is this feasible, does anybody know,
given sufficient hardware, using R?  It appears to use a great deal of
memory on the small files I've tested.

I've read the data import, memory.limit, memory.size & general
documentation but can't seem to find a way to tell what the boundaries
are & roughly gauge the needed memory...other than trial & error.  I've
started by testing the data.frame & run out of memory on my PC.  I'm new
to R so please be forgiving if this is a poorly-worded question.

Jill Willie 
Open Seas
Safeco Insurance
jilwil at safeco.com 
206-545-5673


From jcroot at gmail.com  Sun Jan 21 03:53:50 2007
From: jcroot at gmail.com (James Root)
Date: Sat, 20 Jan 2007 21:53:50 -0500
Subject: [R] importing selected rows and columns from text
Message-ID: <acb1f1cc0701201853jcd8d247rcf3d46bad7cf7b68@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070120/d4137423/attachment.pl 

From adam_6242 at yahoo.com  Sun Jan 21 05:21:36 2007
From: adam_6242 at yahoo.com (aat)
Date: Sat, 20 Jan 2007 20:21:36 -0800 (PST)
Subject: [R] for loop problem
Message-ID: <8472217.post@talk.nabble.com>


Hello R users,

A beginners question which I could not find the answer to in earler posts.

My thought process:
Here "z" is a 119 x 15 data matrix
Step 1: start at column one, bind every column with column 1
Step2: use the new matrix, "test", in the fitCopula package
Step3: store each result in myfit, bind each result to "answer"
Step4: return "answer"


copula_est <- function(z)
{
	for(i in 1:length(z[1,]))
	{
		my.cop <- normalCopula(param = 0.5, dim = 2)
		test <- cbind(z[,1],z[,i])
		myfit[i] <- fitCopula(test,my.cop, start=0.3)
	}
	answer <- cbind(myfit[i])
	return(answer)
}

Errors received:
Error: object "test" not found

Could my syntax be incorrect, or is it  a deeper faulty logic error.
Thank you for your help, it is much appreciated.

aat 

-- 
View this message in context: http://www.nabble.com/for-loop-problem-tf3047849.html#a8472217
Sent from the R help mailing list archive at Nabble.com.


From jonesme11 at yahoo.com  Sun Jan 21 07:08:41 2007
From: jonesme11 at yahoo.com (M Jones)
Date: Sat, 20 Jan 2007 22:08:41 -0800 (PST)
Subject: [R] Working with a set of matrices
Message-ID: <495914.3001.qm@web55204.mail.re4.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070120/7d35f450/attachment.pl 

From aiminy at iastate.edu  Sun Jan 21 09:33:19 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Sun, 21 Jan 2007 02:33:19 -0600
Subject: [R] naiveBayes question
Message-ID: <6.1.2.0.2.20070121023108.01c33e08@aiminy.mail.iastate.edu>

I try to use naiveBayes

 > p.nb.90<-naiveBayes(y~aa_three+bas+bcu+aa_ss,data=training)
 > pr.nb.90<-table(predict(p.nb.90,training[,-13],type="class"),training[,13])

bur I get this error
Error in object$tables[[v]] : subscript out of bounds
 >
head is data set
 > head(training)
     pr aa_three aa_one aa_ss aa_pos    aas bas   ams bms        acu 
bcu     omega       y index
1 1acx      ALA      A     C      1 127.71   0 69.99   0 
-0.2498560   0  79.91470 outward  TRUE
2 1acx      PRO      P     C      2  68.55   0 55.44   0 
-0.0949008   0  76.60380 outward  TRUE
3 1acx      ALA      A     E      3  52.72   0 47.82   0 
-0.0396550   0  52.19970 outward  TRUE
4 1acx      PHE      F     E      4  22.62   0 31.21   0  0.1270330   0 
169.52500  inward  TRUE
5 1acx      SER      S     E      5  71.32   0 52.84   0 
-0.1312380   0   7.47528 outward  TRUE
6 1acx      VAL      V     E      6  12.92   0 22.40   0  0.1728390   0 
149.09400  inward  TRUE

anyone know why?

Aimin


From Dimitris.Rizopoulos at med.kuleuven.be  Sun Jan 21 09:56:54 2007
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Sun, 21 Jan 2007 09:56:54 +0100
Subject: [R] comparing two matrices
In-Reply-To: <1169334390.4853.97.camel@localhost.localdomain>
References: <200701210014.45644.dusa.adrian@gmail.com>
	<1169334390.4853.97.camel@localhost.localdomain>
Message-ID: <20070121095654.mkwyn6na164cgs84@webmail5.kuleuven.be>

based on Marc's approach, I think you can even use

which((mat2 %in% mat1)[1:nrow(mat2)])

instead of

which(apply(matrix(mat2 %in% mat1, dim(mat2)), 1, all))


I hope it helps.

Best,
Dimitris


Quoting Marc Schwartz <marc_schwartz at comcast.net>:

> On Sun, 2007-01-21 at 00:14 +0200, Adrian Dusa wrote:
>> Dear helpeRs,
>>
>> I have two matrices:
>> mat1 <- expand.grid(0:2, 0:2, 0:2)
>> mat2 <- aa[c(19, 16, 13, 24, 8), ]
>>
>> where mat2 is always a subset of mat1
>>
>> I need to find the corersponding row numbers in mat1 for each row in mat2.
>> For this I have the following code:
>>
>> apply(mat2, 1, function(x) {
>>     which(apply(mat1, 1, function(y) {
>>         sum(x == y)
>>         }) == ncol(mat1))
>>     })
>>
>> The code is vectorized, but I wonder if there is a simpler (hence faster)
>> matrix computation that I miss.
>>
>> Thank you,
>> Adrian
>
>
> I have not fully tested this, but how about:
>
> mat1 <- matrix(1:20, ncol = 4, byrow = TRUE)
> mat2 <- matrix(1:60, ncol = 4, byrow = TRUE)
> mat2 <- mat2[sample(15), ]
>
>> mat1
>      [,1] [,2] [,3] [,4]
> [1,]    1    2    3    4
> [2,]    5    6    7    8
> [3,]    9   10   11   12
> [4,]   13   14   15   16
> [5,]   17   18   19   20
>
>> mat2
>       [,1] [,2] [,3] [,4]
>  [1,]   13   14   15   16
>  [2,]    5    6    7    8
>  [3,]   41   42   43   44
>  [4,]   17   18   19   20
>  [5,]   21   22   23   24
>  [6,]   25   26   27   28
>  [7,]   53   54   55   56
>  [8,]    9   10   11   12
>  [9,]   57   58   59   60
> [10,]   33   34   35   36
> [11,]   49   50   51   52
> [12,]   45   46   47   48
> [13,]    1    2    3    4
> [14,]   29   30   31   32
> [15,]   37   38   39   40
>
>> which(apply(matrix(mat2 %in% mat1, dim(mat2)), 1, all))
> [1]  1  2  4  8 13
>
>
> HTH,
>
> Marc Schwartz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ripley at stats.ox.ac.uk  Sun Jan 21 10:13:57 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 21 Jan 2007 09:13:57 +0000 (GMT)
Subject: [R] Can we do GLM on 2GB data set with R?
In-Reply-To: <916551423F01504BA339BF69CBA3BE72016E79E6@psmrdcex18.psm.pin.safeco.com>
References: <916551423F01504BA339BF69CBA3BE72016E79E6@psmrdcex18.psm.pin.safeco.com>
Message-ID: <Pine.LNX.4.64.0701210821010.10650@gannet.stats.ox.ac.uk>

'given sufficient hardware' and a suitable OS, 'yes'.

You will see quoted on this list from time to time:

> library(fortunes)
> fortune("Yoda")

Evelyn Hall: I would like to know how (if) I can extract some of the
information from the summary of my nlme.
Simon Blomberg: This is R. There is no if. Only how.
    -- Evelyn Hall and Simon `Yoda' Blomberg
       R-help (April 2005)

You then mention 'my PC'.  If your 'PC' is running Windows, the answer is 
'with some work', since we don't have a version of R for Win64 and Win32 
is limited to 3GB user address space.

To be more precise, we would have to know more about your GLM (and even if 
you mean GLM in the commonly accepted sense or the SASism with a redundant 
G), including what the variables are (I guess categorical stored as small 
integers?)

My DPhil student Fei Chen looked at ways of applying R to large GLMs with 
data stored in a MySQL database.  His tests were about 4 years ago on 
32-bit Linux, and he was able to run about 1 million cases on 30 
categorical (mainly binary) variables with (I think) up to 5-way 
interactions.  That is a very large GLM problem, and it is unusual for
it to be worth fitting a (mainly linear) model with over 10,000 cases.
(Also, there are normally problems with the homogeneity of very large 
datasets that taint the independence assumptions made by GLMs.)

My guess is that you have been considering the function glm().  There is 
function bigglm() in package biglm (by Thomas Lumley).  I don't think you 
would be able even to load your data into 32-bit R, but it would be 
possible to use the ideas behind bigglm (which was one of the approaches 
Fei assessed) and perhaps even bigglm itself with one of the DBMS 
interfaces to R to retrieve data in chunks.  (bigglm uses chunks of rows, 
but chunks of columns may be more efficient.)

Another possibility is that you want to fit a log-linear model to purely 
categorical data, and could make use of loglin().  That will be more 
efficient if the contingency table is densely populated.

My experience suggests that the important issues here are likely to be 
statistical rather than computational, and this is more a topic for a 
consultant than volunteer help on a discussion list.


On Sat, 20 Jan 2007, WILLIE, JILL wrote:

> We are wanting to use R instead of/in addition to our existing stats
> package because of it's huge assortment of stat functions.  But, we
> routinely need to fit GLM models to files that are approximately 2-4GB
> (as SQL tables, un-indexed, w/tinyint-sized fields except for the
> response & weight variables).  Is this feasible, does anybody know,
> given sufficient hardware, using R?  It appears to use a great deal of
> memory on the small files I've tested.
>
> I've read the data import, memory.limit, memory.size & general
> documentation but can't seem to find a way to tell what the boundaries
> are & roughly gauge the needed memory...other than trial & error.  I've
> started by testing the data.frame & run out of memory on my PC.  I'm new
> to R so please be forgiving if this is a poorly-worded question.
>
> Jill Willie
> Open Seas
> Safeco Insurance
> jilwil at safeco.com
> 206-545-5673


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From dusa.adrian at gmail.com  Sun Jan 21 10:52:16 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Sun, 21 Jan 2007 11:52:16 +0200
Subject: [R] comparing two matrices
In-Reply-To: <002201c73ce6$2d9a73b0$0202a8c0@headquarters.silicoinsights>
References: <200701210014.45644.dusa.adrian@gmail.com>
	<002201c73ce6$2d9a73b0$0202a8c0@headquarters.silicoinsights>
Message-ID: <200701211152.16922.dusa.adrian@gmail.com>

Hi Christos,

It's... more or less the same thing. I was looking for a 
matrix.to.matrix.run.me() function :)

Cheers,
Adrian

On Sunday 21 January 2007 00:56, Christos Hatzis wrote:
> Here is a slightly more compact version of your function which might run
> faster (I did not test timings) since it does not use the sum:
>
> apply(mat2, 1, function(x) which(apply(mat1, 1, function(y) all(x == y)) ==
> TRUE))
>
> -Christos

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From dusa.adrian at gmail.com  Sun Jan 21 10:53:04 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Sun, 21 Jan 2007 11:53:04 +0200
Subject: [R] comparing two matrices
In-Reply-To: <1169334390.4853.97.camel@localhost.localdomain>
References: <200701210014.45644.dusa.adrian@gmail.com>
	<1169334390.4853.97.camel@localhost.localdomain>
Message-ID: <200701211153.04414.dusa.adrian@gmail.com>

Hello Marc and Dimitris,

There was an error in my first example (therefore not reproducible), so
mat1 <- expand.grid(0:2, 0:2, 0:2)
mat2 <- mat1[c(19, 16, 13, 24, 8), ]

Your solution works if and only if the elements in both matrices are unique. 
Unfortunately, it does not apply for my matrices where elements do repeat 
(only the rows are unique).

> which(apply(matrix(mat1 %in% mat2, dim(mat1)), 1, all))
integer(0)

> which((mat1 %in% mat2)[1:nrow(mat1)])
integer(0)


Another solution would be using base 3 operations:
mat1 <- expand.grid(0:2, 0:2, 0:2)[, 3:1]
mat2 <- mat1[c(19, 16, 13, 24, 8), ]

mylines <- mat2[, 1]
for (i in 2:ncol(mat2)) {mylines <- 3*mylines + mat2[, i]}
mylines + 1
[1] 19 16 13 24  8


I was still hoping for a direct matrix function to avoid the for() loop.
Thanks,
Adrian


On Sunday 21 January 2007 01:06, Marc Schwartz wrote:
> On Sun, 2007-01-21 at 00:14 +0200, Adrian Dusa wrote:
> > Dear helpeRs,
> >
> > I have two matrices:
> > mat1 <- expand.grid(0:2, 0:2, 0:2)
> > mat2 <- aa[c(19, 16, 13, 24, 8), ]
> >
> > where mat2 is always a subset of mat1
> >
> > I need to find the corersponding row numbers in mat1 for each row in
> > mat2. For this I have the following code:
> >
> > apply(mat2, 1, function(x) {
> >     which(apply(mat1, 1, function(y) {
> >         sum(x == y)
> >         }) == ncol(mat1))
> >     })
> >
> > The code is vectorized, but I wonder if there is a simpler (hence faster)
> > matrix computation that I miss.
> >
> > Thank you,
> > Adrian
>
> I have not fully tested this, but how about:
>
> mat1 <- matrix(1:20, ncol = 4, byrow = TRUE)
> mat2 <- matrix(1:60, ncol = 4, byrow = TRUE)
> mat2 <- mat2[sample(15), ]
>
> > mat1
>
>      [,1] [,2] [,3] [,4]
> [1,]    1    2    3    4
> [2,]    5    6    7    8
> [3,]    9   10   11   12
> [4,]   13   14   15   16
> [5,]   17   18   19   20
>
> > mat2
>
>       [,1] [,2] [,3] [,4]
>  [1,]   13   14   15   16
>  [2,]    5    6    7    8
>  [3,]   41   42   43   44
>  [4,]   17   18   19   20
>  [5,]   21   22   23   24
>  [6,]   25   26   27   28
>  [7,]   53   54   55   56
>  [8,]    9   10   11   12
>  [9,]   57   58   59   60
> [10,]   33   34   35   36
> [11,]   49   50   51   52
> [12,]   45   46   47   48
> [13,]    1    2    3    4
> [14,]   29   30   31   32
> [15,]   37   38   39   40
>
> > which(apply(matrix(mat2 %in% mat1, dim(mat2)), 1, all))
>
> [1]  1  2  4  8 13
>
>
> HTH,
>
> Marc Schwartz

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From huber at ebi.ac.uk  Sun Jan 21 10:57:16 2007
From: huber at ebi.ac.uk (Wolfgang Huber)
Date: Sun, 21 Jan 2007 09:57:16 +0000
Subject: [R] Working with a set of matrices
In-Reply-To: <495914.3001.qm@web55204.mail.re4.yahoo.com>
References: <495914.3001.qm@web55204.mail.re4.yahoo.com>
Message-ID: <45B338FC.7000504@ebi.ac.uk>


Dear M(?),

you could pack your matrices into a list or an environment and then
iterate through that, either using "for" and subset operator "[[" or
using the lapply / eapply functions.

And instead of looping it is sometimes more elegant to put what you want 
to do with each of your matrices into a function and then repeatedly 
call the function.

Also have a look at "get" for accessing objects e.g. in the global
workspace by their name.

  Best wishes
    Wolfgang

------------------------------------------------------------------
Wolfgang Huber  EBI/EMBL  Cambridge UK  http://www.ebi.ac.uk/huber



M Jones wrote:
> Hi,
> 
> I have a set of matrices (MAT.1, MAT.2, ...) and I'd like to perform the same operation on each of them (for simplicity, say . I'm writing a function for this so that it can be repeated for different sets with different numbers of matrices. The matrices have the same number of columns, but do not have the same number of rows. 
> 
> My thought is to loop thru the set. but I'm not sure how to set the loop up so that the matrix number changes for each cycle. I have tried paste, but that leaves quotation marks around the matrix name (e.g., "MAT.1", and I need just MAT.1), or at least the way I'm using paste..
> 
> Any thoughts?  If there is a better way than the loop, then I'm of course open to that.
> 
> Thanks in advance.
>


From Dimitris.Rizopoulos at med.kuleuven.be  Sun Jan 21 11:04:48 2007
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Sun, 21 Jan 2007 11:04:48 +0100
Subject: [R] comparing two matrices
In-Reply-To: <200701211153.04414.dusa.adrian@gmail.com>
References: <200701210014.45644.dusa.adrian@gmail.com>
	<1169334390.4853.97.camel@localhost.localdomain>
	<200701211153.04414.dusa.adrian@gmail.com>
Message-ID: <20070121110448.wt5sd8edd2p4oc88@webmail5.kuleuven.be>

I think the following should work in your case:

mat1 <- data.matrix(expand.grid(0:2, 0:2, 0:2))
mat2 <- mat1[c(19, 16, 13, 24, 8), ]
############
ind1 <- apply(mat1, 1, paste, collapse = "/")
ind2 <- apply(mat2, 1, paste, collapse = "/")
match(ind2, ind1)


I hope it helps.

Best,
Dimitris


Quoting Adrian Dusa <dusa.adrian at gmail.com>:

> Hello Marc and Dimitris,
>
> There was an error in my first example (therefore not reproducible), so
> mat1 <- expand.grid(0:2, 0:2, 0:2)
> mat2 <- mat1[c(19, 16, 13, 24, 8), ]
>
> Your solution works if and only if the elements in both matrices are unique.
> Unfortunately, it does not apply for my matrices where elements do repeat
> (only the rows are unique).
>
>> which(apply(matrix(mat1 %in% mat2, dim(mat1)), 1, all))
> integer(0)
>
>> which((mat1 %in% mat2)[1:nrow(mat1)])
> integer(0)
>
>
> Another solution would be using base 3 operations:
> mat1 <- expand.grid(0:2, 0:2, 0:2)[, 3:1]
> mat2 <- mat1[c(19, 16, 13, 24, 8), ]
>
> mylines <- mat2[, 1]
> for (i in 2:ncol(mat2)) {mylines <- 3*mylines + mat2[, i]}
> mylines + 1
> [1] 19 16 13 24  8
>
>
> I was still hoping for a direct matrix function to avoid the for() loop.
> Thanks,
> Adrian
>
>
> On Sunday 21 January 2007 01:06, Marc Schwartz wrote:
>> On Sun, 2007-01-21 at 00:14 +0200, Adrian Dusa wrote:
>> > Dear helpeRs,
>> >
>> > I have two matrices:
>> > mat1 <- expand.grid(0:2, 0:2, 0:2)
>> > mat2 <- aa[c(19, 16, 13, 24, 8), ]
>> >
>> > where mat2 is always a subset of mat1
>> >
>> > I need to find the corersponding row numbers in mat1 for each row in
>> > mat2. For this I have the following code:
>> >
>> > apply(mat2, 1, function(x) {
>> >     which(apply(mat1, 1, function(y) {
>> >         sum(x == y)
>> >         }) == ncol(mat1))
>> >     })
>> >
>> > The code is vectorized, but I wonder if there is a simpler (hence faster)
>> > matrix computation that I miss.
>> >
>> > Thank you,
>> > Adrian
>>
>> I have not fully tested this, but how about:
>>
>> mat1 <- matrix(1:20, ncol = 4, byrow = TRUE)
>> mat2 <- matrix(1:60, ncol = 4, byrow = TRUE)
>> mat2 <- mat2[sample(15), ]
>>
>> > mat1
>>
>>      [,1] [,2] [,3] [,4]
>> [1,]    1    2    3    4
>> [2,]    5    6    7    8
>> [3,]    9   10   11   12
>> [4,]   13   14   15   16
>> [5,]   17   18   19   20
>>
>> > mat2
>>
>>       [,1] [,2] [,3] [,4]
>>  [1,]   13   14   15   16
>>  [2,]    5    6    7    8
>>  [3,]   41   42   43   44
>>  [4,]   17   18   19   20
>>  [5,]   21   22   23   24
>>  [6,]   25   26   27   28
>>  [7,]   53   54   55   56
>>  [8,]    9   10   11   12
>>  [9,]   57   58   59   60
>> [10,]   33   34   35   36
>> [11,]   49   50   51   52
>> [12,]   45   46   47   48
>> [13,]    1    2    3    4
>> [14,]   29   30   31   32
>> [15,]   37   38   39   40
>>
>> > which(apply(matrix(mat2 %in% mat1, dim(mat2)), 1, all))
>>
>> [1]  1  2  4  8 13
>>
>>
>> HTH,
>>
>> Marc Schwartz
>
> --
> Adrian Dusa
> Romanian Social Data Archive
> 1, Schitu Magureanu Bd
> 050025 Bucharest sector 5
> Romania
> Tel./Fax: +40 21 3126618 \
>           +40 21 3120210 / int.101
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From dusa.adrian at gmail.com  Sun Jan 21 11:17:13 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Sun, 21 Jan 2007 12:17:13 +0200
Subject: [R] comparing two matrices
In-Reply-To: <20070121110448.wt5sd8edd2p4oc88@webmail5.kuleuven.be>
References: <200701210014.45644.dusa.adrian@gmail.com>
	<200701211153.04414.dusa.adrian@gmail.com>
	<20070121110448.wt5sd8edd2p4oc88@webmail5.kuleuven.be>
Message-ID: <200701211217.13723.dusa.adrian@gmail.com>

On Sunday 21 January 2007 12:04, Dimitris Rizopoulos wrote:
> I think the following should work in your case:
>
> mat1 <- data.matrix(expand.grid(0:2, 0:2, 0:2))
> mat2 <- mat1[c(19, 16, 13, 24, 8), ]
> ############
> ind1 <- apply(mat1, 1, paste, collapse = "/")
> ind2 <- apply(mat2, 1, paste, collapse = "/")
> match(ind2, ind1)


Oh yes, I thought about that too.
It works fast enough for small matrices, but I deal with very large ones. 
Using paste() on such matrices decreases the speed dramatically.

Thanks again,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From nitin.jindal at gmail.com  Sun Jan 21 11:51:41 2007
From: nitin.jindal at gmail.com (nitin jindal)
Date: Sun, 21 Jan 2007 04:51:41 -0600
Subject: [R] logistic regression model + Cross-Validation
Message-ID: <2b808b010701210251v2c7c5d02s8b3eeb6425c649f0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070121/bd2940c5/attachment.pl 

From ccleland at optonline.net  Sun Jan 21 12:30:14 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Sun, 21 Jan 2007 06:30:14 -0500
Subject: [R] importing selected rows and columns from text
In-Reply-To: <acb1f1cc0701201853jcd8d247rcf3d46bad7cf7b68@mail.gmail.com>
References: <acb1f1cc0701201853jcd8d247rcf3d46bad7cf7b68@mail.gmail.com>
Message-ID: <45B34EC6.8080106@optonline.net>

James Root wrote:
> I read through the import/export manual a few times and did not see this
> mentioned and checked the archives as well.  I am dealing with data sheets
> generated by Eprime (a software package for generating experimental
> psychology paradigms) that output subject responses in a proprietary Edat
> file format.  All individual subject response spread sheets can be merged to
> form one long file of all subjects.  As an example, if an experiment has 240
> responses per subject (120 for each condition, say), each subject will
> generate a spreadsheet with 240 rows.  After merging 10 subjects then, a
> merged spreadsheet will have 2400 rows, with each subject's responses
> starting every 240 rows.  Formally being an SPSS user, I would do a datalist
> command that would pick out the columns and rows of interest, and using a
> rows command, indicate how many rows were equal to one subject.

  You might just read the entire file and then use subset() to select
the rows and columns you need.  If you are reading the data with
read.table(), then the colClasses argument can be used to skip one or
more columns.

> Is there an equivalent way to do this directly from a text file to import
> into R?  That is, give R one long spread sheet composed of all subjects'
> responses, tell R how many rows make up a subject, which rows make up a
> discrete condition, and import and flag only the data that I need, and then
> organize these as discrete variables so that each subject will have 240
> columns of responses and only one row (instead of 240 rows and one column as
> in the original text file).  Sorry for the long-winded email, and thanks in
> advance for any help on this.

  Have a look at reshape() in package stats and cast() and melt() in
package reshape by Hadley Wickham for ways to reorganize the data.

> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From dusa.adrian at gmail.com  Sun Jan 21 13:39:33 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Sun, 21 Jan 2007 14:39:33 +0200
Subject: [R] multiple bases to decimal (was: comparing two matrices)
Message-ID: <200701211439.33575.dusa.adrian@gmail.com>


Hi again,

I was contemplating the solution using base 3:
set.seed(3)
mat2 <- matrix(sample(0:2, 15, replace=T), 5, 3)

Extracting the line numbers is simple:
bases <- c(3, 3, 3)^(2:0)       # or just 3^(2:0)
colSums(apply(mat2, 1, function(x) x*bases)) + 1
[1]  7 23 25  8  1

The problem is sometimes the columns have different number of levels, as in:
mat1 <- expand.grid(0:2, 0:2, 0:1)[,3:1]

Is there any chance to combine different bases in order to obtain the 
corresponding line numbers?
I thought of something like:
bases <- c(3, 3, 2)^(2:0)

but it doesn't work (sigh).

Thanks for any hint,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From ligges at statistik.uni-dortmund.de  Sun Jan 21 15:10:22 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 21 Jan 2007 15:10:22 +0100
Subject: [R] for loop problem
In-Reply-To: <8472217.post@talk.nabble.com>
References: <8472217.post@talk.nabble.com>
Message-ID: <45B3744E.1070909@statistik.uni-dortmund.de>



aat wrote:
> Hello R users,
> 
> A beginners question which I could not find the answer to in earler posts.
> 
> My thought process:
> Here "z" is a 119 x 15 data matrix
> Step 1: start at column one, bind every column with column 1
> Step2: use the new matrix, "test", in the fitCopula package
> Step3: store each result in myfit, bind each result to "answer"
> Step4: return "answer"
> 
> 
> copula_est <- function(z)
> {
> 	for(i in 1:length(z[1,]))
> 	{
> 		my.cop <- normalCopula(param = 0.5, dim = 2)
> 		test <- cbind(z[,1],z[,i])
> 		myfit[i] <- fitCopula(test,my.cop, start=0.3)
> 	}
> 	answer <- cbind(myfit[i])
> 	return(answer)
> }


The example is not reproducible for us, since we do not have z.

I'd try to rewrite it as follows, without having tried anything:

my.cop <- normalCopula(param = 0.5, dim = 2)
answer <- apply(z[,-1], 2,
    function(x) fitCopula(cbind(z[,1], x), my.cop, start=0.3),
    my.cop = my.cop)


Uwe Ligges



> Errors received:
> Error: object "test" not found
> 
> Could my syntax be incorrect, or is it  a deeper faulty logic error.
> Thank you for your help, it is much appreciated.
> 
> aat 
>


From jholtman at gmail.com  Sun Jan 21 15:30:23 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 21 Jan 2007 09:30:23 -0500
Subject: [R] multiple bases to decimal (was: comparing two matrices)
In-Reply-To: <200701211439.33575.dusa.adrian@gmail.com>
References: <200701211439.33575.dusa.adrian@gmail.com>
Message-ID: <644e1f320701210630j49bc514av3ff1e7b3225a04ef@mail.gmail.com>

I think you are computing your bases in the wrong way.  If the data
represents 3 columns with base 3,3,2, then the multiplier has to be
c(6,2,1) not c(9,3,1).  I think this should compute it correctly:

# create a matrix of all combination of bases 3,3,2
mat1 <- expand.grid(0:1, 0:2, 0:2)[,3:1]
base <- c(3,3,2)  # define the bases
# now create the multiplier
mbase <-  c(rev(cumprod(rev(base))),1)[-1]
# show the data
mat1
base
mbase
# combine with original
cbind(mat1, conv=colSums(apply(mat1, 1, function(x) x*mbase)))


On 1/21/07, Adrian Dusa <dusa.adrian at gmail.com> wrote:
>
> Hi again,
>
> I was contemplating the solution using base 3:
> set.seed(3)
> mat2 <- matrix(sample(0:2, 15, replace=T), 5, 3)
>
> Extracting the line numbers is simple:
> bases <- c(3, 3, 3)^(2:0)       # or just 3^(2:0)
> colSums(apply(mat2, 1, function(x) x*bases)) + 1
> [1]  7 23 25  8  1
>
> The problem is sometimes the columns have different number of levels, as in:
> mat1 <- expand.grid(0:2, 0:2, 0:1)[,3:1]
>
> Is there any chance to combine different bases in order to obtain the
> corresponding line numbers?
> I thought of something like:
> bases <- c(3, 3, 2)^(2:0)
>
> but it doesn't work (sigh).
>
> Thanks for any hint,
> Adrian
>
> --
> Adrian Dusa
> Romanian Social Data Archive
> 1, Schitu Magureanu Bd
> 050025 Bucharest sector 5
> Romania
> Tel./Fax: +40 21 3126618 \
>          +40 21 3120210 / int.101
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From dusa.adrian at gmail.com  Sun Jan 21 15:38:41 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Sun, 21 Jan 2007 16:38:41 +0200
Subject: [R] multiple bases to decimal (was: comparing two matrices)
In-Reply-To: <644e1f320701210630j49bc514av3ff1e7b3225a04ef@mail.gmail.com>
References: <200701211439.33575.dusa.adrian@gmail.com>
	<644e1f320701210630j49bc514av3ff1e7b3225a04ef@mail.gmail.com>
Message-ID: <200701211638.42054.dusa.adrian@gmail.com>

On Sunday 21 January 2007 16:30, jim holtman wrote:
> I think you are computing your bases in the wrong way.  If the data
> represents 3 columns with base 3,3,2, then the multiplier has to be
> c(6,2,1) not c(9,3,1).  I think this should compute it correctly:
>
> # create a matrix of all combination of bases 3,3,2
> mat1 <- expand.grid(0:1, 0:2, 0:2)[,3:1]
> base <- c(3,3,2)  # define the bases
> # now create the multiplier
> mbase <-  c(rev(cumprod(rev(base))),1)[-1]
> # show the data
> mat1
> base
> mbase
> # combine with original
> cbind(mat1, conv=colSums(apply(mat1, 1, function(x) x*mbase)))

YES!
Thank you so much Jim, this made my day :))
Adrian


-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From ripley at stats.ox.ac.uk  Sun Jan 21 15:49:03 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 21 Jan 2007 14:49:03 +0000 (GMT)
Subject: [R] for loop problem
In-Reply-To: <45B3744E.1070909@statistik.uni-dortmund.de>
References: <8472217.post@talk.nabble.com>
	<45B3744E.1070909@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.64.0701211438350.27315@gannet.stats.ox.ac.uk>

On Sun, 21 Jan 2007, Uwe Ligges wrote:

> aat wrote:
>> Hello R users,
>>
>> A beginners question which I could not find the answer to in earler posts.
>>
>> My thought process:
>> Here "z" is a 119 x 15 data matrix
>> Step 1: start at column one, bind every column with column 1
>> Step2: use the new matrix, "test", in the fitCopula package
>> Step3: store each result in myfit, bind each result to "answer"
>> Step4: return "answer"
>>
>>
>> copula_est <- function(z)
>> {
>> 	for(i in 1:length(z[1,]))
>> 	{
>> 		my.cop <- normalCopula(param = 0.5, dim = 2)
>> 		test <- cbind(z[,1],z[,i])
>> 		myfit[i] <- fitCopula(test,my.cop, start=0.3)
>> 	}
>> 	answer <- cbind(myfit[i])
>> 	return(answer)
>> }
>
>
> The example is not reproducible for us, since we do not have z.

Nor is there a package 'fitCopula' available to us.

> I'd try to rewrite it as follows, without having tried anything:
>
> my.cop <- normalCopula(param = 0.5, dim = 2)
> answer <- apply(z[,-1], 2,
>    function(x) fitCopula(cbind(z[,1], x), my.cop, start=0.3),
>    my.cop = my.cop)

That is not quite the same thing, as he included i=1 in the loop.

Assuming this is package 'copula' the result is an S4 classed object 
(although that is far from clear on the help page).  apply() is not said 
to work with such functions (and I have little idea what as.vector will 
do, most likely fail), so I think I would use lapply().  Something like

answer <- lapply(seq_len(ncol(z)),
    function(i) fitCopula(cbind(z[,1], z[,i]), my.cop, start=0.3),
    my.cop = my.cop)

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From f.harrell at vanderbilt.edu  Sun Jan 21 15:54:00 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 21 Jan 2007 08:54:00 -0600
Subject: [R] logistic regression model + Cross-Validation
In-Reply-To: <2b808b010701210251v2c7c5d02s8b3eeb6425c649f0@mail.gmail.com>
References: <2b808b010701210251v2c7c5d02s8b3eeb6425c649f0@mail.gmail.com>
Message-ID: <45B37E88.4050500@vanderbilt.edu>

nitin jindal wrote:
> Hi,
> 
> I am trying to cross-validate a logistic regression model.
> I am using logistic regression model (lrm) of package Design.
> 
> f <- lrm( cy ~ x1 + x2, x=TRUE, y=TRUE)
> val <- validate.lrm(f, method="cross", B=5)

val <- validate(f, ...)    # .lrm not needed

> 
> My class cy has values 0 and 1.
> 
> "val" variable will give me indicators like slope and AUC. But, I also need
> the vector of predicted values of class variable "cy" for each record while
> cross-validation, so that I can manually look at the results. So, is there
> any way to get those probabilities assigned to each class.
> 
> regards,
> Nitin

No, validate.lrm does not have that option.  Manually looking at the 
results will not be easy when you do enough cross-validations.  A single 
5-fold cross-validation does not provide accurate estimates.  Either use 
the bootstrap or repeat k-fold cross-validation between 20 and 50 times. 
  k is often 10 but the optimum value may not be 10.  Code for averaging 
repeated cross-validations is in 
http://biostat.mc.vanderbilt.edu/twiki/pub/Main/RmS/logistic.val.pdf
along with simulations of bootstrap vs. a few cross-validation methods 
for binary logistic models.

Frank
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From miraceti at gmail.com  Sun Jan 21 18:37:49 2007
From: miraceti at gmail.com (miraceti)
Date: Sun, 21 Jan 2007 12:37:49 -0500
Subject: [R] efficient code. how to reduce running time?
Message-ID: <dadd130701210937q59937737l6c2361b8127f5fdc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070121/4691fcdf/attachment.pl 

From rdiaz02 at gmail.com  Sun Jan 21 19:42:05 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Sun, 21 Jan 2007 19:42:05 +0100
Subject: [R] kate editor for R
In-Reply-To: <1169309265.4853.28.camel@localhost.localdomain>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
	<17840.55304.803214.860567@basebud.nulle.part>
	<200701191609.40533.rdiaz@cnio.es>
	<1169240161.4838.27.camel@localhost.localdomain>
	<624934630701191859sfdbea6ey19b0a2a1929a04e9@mail.gmail.com>
	<1169262738.4853.7.camel@localhost.localdomain>
	<624934630701200220p445a64e6m87601f864db9a38f@mail.gmail.com>
	<1169309265.4853.28.camel@localhost.localdomain>
Message-ID: <624934630701211042o39e52ea0s4474532586c5c53a@mail.gmail.com>

On 1/20/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> On Sat, 2007-01-20 at 11:20 +0100, Ramon Diaz-Uriarte wrote:
> > On 1/20/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> > > xft anti-aliasing is incorporated into the version 23 unicode trunk.
> > >
> > > So it looks great on a hi-res LCD panel. Without xft, even using
> > > Bitstream fonts, it was still pretty rough on the eyes.
> > >
> >
> >
> > Humm, call me silly, but most of the time I do not like anti-aliased
> > fonts: I tend to agree with
> > http://modeemi.cs.tut.fi/~tuomov/ion/faq/entries/Blurred_fonts.html,
> > where he says "characters look like having been dragged through mud"
> > :-).
> >
> > > It also fully supports GTK widgets, which is great if you are using
> > > GNOME, which I do.
> > >
> >
> > But my .emacs gets rid of the toolbar and scroll bars on start-up (I
> > find toolbars confusing things that take up precious screen space),
> > and often work without the menubar (when I am doing familiar work). I
> > use ion3 (http://modeemi.cs.tut.fi/~tuomov/ion/), which, together with
> > wmii (and followed, at some distance, by fmwv), I find the most usable
> > window managers, and thus the look of widgets is not that relevant to
> > me.
> >
> > So, for most practical purposes (except for resizing with the mouse) I
> > use emacs as if started with the -w command. (I know, I know, this
> > looks like going backwards ... must be a mid-life involution crisis
> > :-).
>
> We'll drag you kicking and screaming into the 21st century...
>
> ;-)
>


I was afraid someone would suggest that sooner or later :-).

> > > xft was added as a patch to version 22, but it was not very stable.
> > >
> > > Note that version 23 is in alpha status, so use at your own risk if you
> > > decide to pursue this. 21 is still the current stable release version,
> > > but 23 has been rock solid for me.
> > >
> > > I can provide you with a shell script to build it. Let me know.
> > >
> >
> >
> > Let me try with the debian packages, and if I have problems, I'll
> > definitely start bugging you. Thanks a lot for your help!
> >
> > Best,
> >
> > R.
>
> FWIW, here are some screen shots so that you can get a feel for what it
> looks like. This is using two 1600x1200 lcd panels.
>
> 1. Basic view of main window, showing ECB and ESS:
>
>   http://home.comcast.net/~marc_schwartz/emacs23.png
>
> 2. Full screen (3200x1600 using nVidia TwinView) capture to show GTK
> file selection widget:
>
>   http://home.comcast.net/~marc_schwartz/emacs23-2.png
>
> 3. View of main window to show the integration of SVN version control,
> which I use my all of my R code:
>
>   http://home.comcast.net/~marc_schwartz/emacs23-3.png
>


Hey, those look very neat! (I'd get rid of all those toolbars :-). But
very neat. Time to try it. (But no way I am giving up ion3).


>
> No doubt that the use of xft is a personal choice, and some folks do not
> like it, perhaps notably on CRTs.  As I have gotten older and need
> bi-focals for computer work and reading, I find the use of xft much
> easier and I am less prone to eye strain, given how many hours I
> typically spend working each day.
>

Marc, but the solution for that problem are not xft fonts. The
solution is ... working less hours. (I'll blackmail my boss: if you
force me to work more hours, I'll use xft fonts. I bet it'll be a
great strategy).


Best

> HTH,
>
> Marc
>
>
>


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From g_smits at verizon.net  Sun Jan 21 20:22:07 2007
From: g_smits at verizon.net (Gerard Smits)
Date: Sun, 21 Jan 2007 11:22:07 -0800
Subject: [R] sequential processing
Message-ID: <7.0.1.0.2.20070121111345.034095f8@verizon.net>

Like many others, I am new to R but old to SAS.

Am I correct in understanding that R processes a data frame in a 
sequential ly?  This would imply that large input files could be 
read, without the need to load the entire file into memory.
Related to the manner of reading a frame, I have been looking for the 
equivalent of SAS _n_ (I realize that I can use a variant of which to 
identify an index value) as well as  useful SAS features such as 
first., last., retain, etc.  Any help with this conversion appreciated.

Thanks,

Gerard Smits


From mail at nhoeller.de  Sun Jan 21 20:27:35 2007
From: mail at nhoeller.de (Nils Hoeller)
Date: Sun, 21 Jan 2007 20:27:35 +0100
Subject: [R] Integration + Normal Distribution + Directory Browsing
 Processing Questions
Message-ID: <45B3BEA7.2010001@nhoeller.de>

Hi everyone,

I am new to R, but it's really great and helped me a lot!

But now I have 2 questions. It would be great, if someone can help me:

1. I want to integrate a normal distribution, given a median and sd.
The integrate function works great BUT the first argument has to be a 
function

so I do integrate(dnorm,0,1) and it works with standard m. and sd.

But I have the m and sd given.

So for fixed m and sd I work around with a new function mynorm

mynorm <- function(n) {
    ret <- dnorm(n,0.6,0.15)
    ret
}

for example.

BUT what can I do for dynamic m and sd?
I want something like integrate(dnorm(,0.6,0.15),0,1), with the first 
dnorm parameter open for the
integration but fixed m and sd.

I hope you can help me.

2. I am working with textfiles with rows of measure data.
read.table("file") works fine.

Now I want R to read.table all files within a given directory and 
process them one by the other.

for(all files in dir xy) {
x <- read.table(nextfile)
process x
}

Is that possible with R? I hope so. Can anyone give me a link to examples.

Thanks for your help

Nils


From miraceti at gmail.com  Sun Jan 21 20:40:15 2007
From: miraceti at gmail.com (miraceti)
Date: Sun, 21 Jan 2007 14:40:15 -0500
Subject: [R] efficient code. how to reduce running time?
In-Reply-To: <dadd130701210937q59937737l6c2361b8127f5fdc@mail.gmail.com>
References: <dadd130701210937q59937737l6c2361b8127f5fdc@mail.gmail.com>
Message-ID: <dadd130701211140g2d96a1a4n545f9a51f52460ff@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070121/4f61820a/attachment.pl 

From jenny197806 at yahoo.se  Sun Jan 21 21:14:11 2007
From: jenny197806 at yahoo.se (Jenny persson)
Date: Sun, 21 Jan 2007 21:14:11 +0100 (CET)
Subject: [R] identify selected substances across individuals
Message-ID: <20070121201411.92802.qmail@web28005.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: inte tillg?nglig
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070121/436ed377/attachment.pl 

From Dimitris.Rizopoulos at med.kuleuven.be  Sun Jan 21 21:37:51 2007
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Sun, 21 Jan 2007 21:37:51 +0100
Subject: [R] Integration + Normal Distribution + Directory
	Browsing	Processing Questions
In-Reply-To: <45B3BEA7.2010001@nhoeller.de>
References: <45B3BEA7.2010001@nhoeller.de>
Message-ID: <20070121213751.6zi4i99mp28k4kwc@webmail4.kuleuven.be>

you can use the `...' argument of integrate, e.g.,

integrate(dnorm, 0, 1)
integrate(dnorm, 0, 1, mean = 0.1)
integrate(dnorm, 0, 1, mean = 0.1, sd = 1.2)

look at ?integrate for more info.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting Nils Hoeller <mail at nhoeller.de>:

> Hi everyone,
>
> I am new to R, but it's really great and helped me a lot!
>
> But now I have 2 questions. It would be great, if someone can help me:
>
> 1. I want to integrate a normal distribution, given a median and sd.
> The integrate function works great BUT the first argument has to be a
> function
>
> so I do integrate(dnorm,0,1) and it works with standard m. and sd.
>
> But I have the m and sd given.
>
> So for fixed m and sd I work around with a new function mynorm
>
> mynorm <- function(n) {
>     ret <- dnorm(n,0.6,0.15)
>     ret
> }
>
> for example.
>
> BUT what can I do for dynamic m and sd?
> I want something like integrate(dnorm(,0.6,0.15),0,1), with the first
> dnorm parameter open for the
> integration but fixed m and sd.
>
> I hope you can help me.
>
> 2. I am working with textfiles with rows of measure data.
> read.table("file") works fine.
>
> Now I want R to read.table all files within a given directory and
> process them one by the other.
>
> for(all files in dir xy) {
> x <- read.table(nextfile)
> process x
> }
>
> Is that possible with R? I hope so. Can anyone give me a link to examples.
>
> Thanks for your help
>
> Nils
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From nitin.jindal at gmail.com  Sun Jan 21 21:51:37 2007
From: nitin.jindal at gmail.com (nitin jindal)
Date: Sun, 21 Jan 2007 14:51:37 -0600
Subject: [R] logistic regression model + Cross-Validation
In-Reply-To: <45B37E88.4050500@vanderbilt.edu>
References: <2b808b010701210251v2c7c5d02s8b3eeb6425c649f0@mail.gmail.com>
	<45B37E88.4050500@vanderbilt.edu>
Message-ID: <2b808b010701211251k13e263dap4941b2458e94db9c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070121/f82c185c/attachment.pl 

From PAlspach at hortresearch.co.nz  Sun Jan 21 21:58:27 2007
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Mon, 22 Jan 2007 09:58:27 +1300
Subject: [R] identify selected substances across individuals
Message-ID: <EC0F8FF776F3F74E9C63CE16641C962801C34399@AKLEXB02.hort.net.nz>


Jenny

>   Below is an example data, which contains three id-numbers.
> For each id there are three substances in each of the three
> blocks. Some substances are repeated twice.The subsatances
> are the same for all ids. The id number 3 is actually a
> control so all responses (y) that are equal or greater than 4
> are supposed to be removed from this id number. This I can do
> easily in R but what I need help with is I want to have those
> substances that are removed from id number 3 also removed
> from other ids as well. I could do an algorithm like : for id
> in 1:2, if substance = c("abc","dgf") then delete but if the
> substances to be removed have long strings and are more than
> 2 (for example 20 substances) then it would take long time to
> list the substances manually. Can you guys please show me a
> clever way to do what I described above ?

Use subsetting to identify that substances, and then !(...%in%...) to
remove records with these substances:

yourData[!(yourData$substance %in% yourData[yourData$id==3 &
yourData$y>=4, 'substance']),]

The above is untested and will need modification is yourData$id or
yourData$y contains missing values.
   
Peter Alspach

______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}


From jfox at mcmaster.ca  Sun Jan 21 23:00:44 2007
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 21 Jan 2007 17:00:44 -0500
Subject: [R] efficient code. how to reduce running time?
In-Reply-To: <dadd130701210937q59937737l6c2361b8127f5fdc@mail.gmail.com>
Message-ID: <20070121220044.NUOL5067.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Mira,

I didn't work through your code in detail, but I did notice that you're
doing something that's very inefficient in R -- building up objects
element-by-element, e.g., by indexing beyond their current length. Instead,
you can preallocate the objects and simply replace elements. For example,
create the vector F in your perm.F() as F <- numeric(nperms) rather than as
an empty vector. (BTW, I'd probably not name a variable "F" since this is
usually a synonym for the logical value FALSE.) There's a similar problem in
your handling of maxF, which you build up column-by-column via cbind().
(BTW, is maxF really a matrix?) You also needlessly recompute max(F), never
appear to use MSSid, and end lines with unnecessary semicolons.

I hope that this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of miraceti
> Sent: Sunday, January 21, 2007 12:38 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] efficient code. how to reduce running time?
> 
> Hi,
> I am new to R.
> and even though I've made my code to run and do what it needs to .
> It is taking forever and I can't use it like this.
> I was wondering if you could help me find ways to fix the 
> code to run faster.
> Here are my codes..
> the data set is a bunch of 0s and 1s in a data.frame.
> What I am doing is this.
> I pick a column and make up a new column Y with values 
> associated with that column I picked.
> then I remove the column.
> and see if any other columns in the data have a significant 
> association with the Y column I've generated.
> If you see anything obvious that takes a long time, any 
> suggestions would be appreciated.
> thanks a lot.
> 
> Mira
> 
> --------------------------------------------------------------
> --------------------------------
> #sub function for finding index
> rfind <- function(x)seq(along=x)[x*(1-x)>MAF*(1-MAF)]
> 
> #sub function for permutation test
> perm.F = function(y,x,nperms,sites)
> {
>   maxF = c();
>   for (i in 1:nperms)
>   {
>     F=numeric(S)   #create an empty vector to store the F-values
>     newY=sample(y,length(y))     #permute the cancer types
>     newX = cbind(x, newY);
>     # anova for all sites
>     for ( i in sites )
>     {
>       a <- anova(lm(newY~factor(newX[,i])));
>       F[i] <- a$`F value`[1];
>     }
>     MSSid <- which (F == max(F)); # index of MSS (Most 
> Significant Site)
>     maxF = cbind(maxF,max(F));
>   }
>   maxF;
> }
> 
> 
> # set the output file
> sink("/tmp/R.out.3932.100")
> # load the dataset
> snp = read.table(file("/tmp/msoutput.3932.100"))
> #print (snp);
> 
> # pi: desired proportion of variation due to QTN pi = 0.05; 
> print (paste("pi:", pi)); MAF = 0.05; print (paste("MAF:", 
> MAF)); # S: number of segregating sites S = length(snp[1,]); 
> # N: number of samples N = length(snp[,1]); Dips = 
> sample(1:N,N) DipA = Dips[1:50] DipB = Dips[51:100] disnp = 
> snp[DipA,]+snp[DipB,] snp = as.data.frame(disnp, 
> row.names=NULL); N = length(snp[,1]);
> 
> # get allele freq for all SNPs
> allele_f <- mean(snp[,1:S])/2;
> print (allele_f);
> sites = rfind(allele_f);
> print(sites);
> 
> # collapse sites that have perfect correlation newsites <- 
> sites; for (i in 1:(length(sites)-1)) {
>   for (j in (i+1):length(sites))
>   {
>     test = (snp[sites[i]] == snp[sites[j]])
>     if ( all(test) || all(!test) )
>     {
>       print (paste("perfect correlation with", sites[i]));
>       print (paste("removing alleles", sites[j]));
>       newsites <- newsites[newsites!=sites[j]];
>     }
>   }
> }
> sites <- newsites;
> print(sites);
> 
> # QTN: the site nearest right to S/4
> sitesid = floor(length(sites)/4);
> QTNid = sites[sitesid];
> QTN = snp[,QTNid];
> 
> print (paste("QTN:", names(snp[QTNid]))); print (QTN);
> 
> # remove QTN from sites
> sites <- sites [ sites != QTNid ];
> print(sites);
> print (paste("Number of usable SNPs:", length(sites)));
> 
> # p: allele frequency of QTN
> p0 = allele_f[QTNid];
> p = min(p0, 1-p0);
> print (paste("QTN_freq:", p));
> 
> # z: random normal deviate
> z = rnorm(N, mean = 0, sd = 1);
> # foreach sample give quantitative phenotype # each row is a 
> sample # phenotype value depends on QTN genotype, pi, p, and 
> z Y <- sqrt(10-(10*pi))*z + QTN*sqrt((10*pi)/(2*p*(1-p))); 
> snp = data.frame(cbind(snp, Y)); # anova for QTN 
> df=data.frame(Y=Y, QTN=factor(QTN)); QTN_a <- anova(lm(Y~QTN, 
> data=df)); print (QTN_a); SSB <- QTN_a$`Sum Sq`[1]; SSW <- 
> QTN_a$`Sum Sq`[2]; QTN_PRE <- SSB / (SSB + SSW); print 
> (paste("var_QTN/var_tot:", QTN_PRE));
> 
> # anova for all sites
> F=numeric(S)   #create an empty vector to store the F-values
> Pval=rep(1,S)   #create an empty vector to store the Pval
> PRE=numeric(S)   #create an empty vector to store the PRE
> 
> for ( i in sites )
> {
>   a <- anova(lm(Y~factor(snp[,i])));
>   print (a);
>   F[i] <- a$`F value`[1];
>   Pval[i] <- a$`Pr`[1];
>   SSB <- a$`Sum Sq`[1];
>   SSW <- a$`Sum Sq`[2];
>   PRE[i] <- SSB / (SSB + SSW);
> 
> }
> print (paste("Max F:", max(F)));
> MSSid <- which (F == max(F)); # index of MSS (Most 
> Significant Site) MSS = snp[,MSSid]; print (paste("MSS(Most 
> Significant Site):", MSSid)); p0 = length(MSS[MSS==0])/N; p = 
> min(p0, 1-p0); print (paste("assoc_freq:", p)); print 
> (paste("assoc_var:", PRE[MSSid])); #lets do a permutation 
> test Fdist <- perm.F(Y, snp[,1:S], 1000, sites); print 
> ("permutation test maxF dist"); print (Fdist); pvalue <- 
> mean(Fdist>F[MSSid]); print (paste("assoc_prob:", pvalue));
> 
> # close the output file
> sink()
> --------------------------------------------------------------
> ------------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Bill.Venables at csiro.au  Sun Jan 21 23:44:06 2007
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Mon, 22 Jan 2007 08:44:06 +1000
Subject: [R] comparing two matrices
References: <200701210014.45644.dusa.adrian@gmail.com><200701211153.04414.dusa.adrian@gmail.com><20070121110448.wt5sd8edd2p4oc88@webmail5.kuleuven.be>
	<200701211217.13723.dusa.adrian@gmail.com>
Message-ID: <B998A44C8986644EA8029CFE6396A924840BEA@exqld2-bne.qld.csiro.au>

But this is using paste() the wrong way round. A better way would be

> join <- function(x) do.call("paste", c(as.data.frame(x), sep = "\r"))
> which(join(mat1) %in% join(mat2))
[1]  8 13 16 19 24

This is essentially the technique used by duplicated.data.frame

Bill Venables 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Adrian Dusa
Sent: Sunday, 21 January 2007 8:17 PM
To: Dimitris Rizopoulos
Cc: marc_schwartz at comcast.net; r-help at stat.math.ethz.ch
Subject: Re: [R] comparing two matrices

On Sunday 21 January 2007 12:04, Dimitris Rizopoulos wrote:
> I think the following should work in your case:
>
> mat1 <- data.matrix(expand.grid(0:2, 0:2, 0:2))
> mat2 <- mat1[c(19, 16, 13, 24, 8), ]
> ############
> ind1 <- apply(mat1, 1, paste, collapse = "/")
> ind2 <- apply(mat2, 1, paste, collapse = "/")
> match(ind2, ind1)


Oh yes, I thought about that too.
It works fast enough for small matrices, but I deal with very large
ones. 
Using paste() on such matrices decreases the speed dramatically.

Thanks again,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From miraceti at gmail.com  Sun Jan 21 23:55:49 2007
From: miraceti at gmail.com (miraceti)
Date: Sun, 21 Jan 2007 17:55:49 -0500
Subject: [R] efficient code. how to reduce running time?
In-Reply-To: <20070121220044.NUOL5067.tomts5-srv.bellnexxia.net@JohnDesktop8300>
References: <dadd130701210937q59937737l6c2361b8127f5fdc@mail.gmail.com>
	<20070121220044.NUOL5067.tomts5-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <dadd130701211455k3fee4e86t9d4608c5a316a347@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070121/e368a614/attachment.pl 

From rdiaz02 at gmail.com  Mon Jan 22 00:05:14 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Mon, 22 Jan 2007 00:05:14 +0100
Subject: [R] ECB/Sidebar/R (Emacs) was: Re: kate editor for R
In-Reply-To: <17842.13129.56162.112711@basebud.nulle.part>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
	<17840.55304.803214.860567@basebud.nulle.part>
	<200701201520.42653.blindglobe@gmail.com>
	<17842.13129.56162.112711@basebud.nulle.part>
Message-ID: <624934630701211505k6dd5e53fl8bf134bd08fce01c@mail.gmail.com>

On 1/20/07, Dirk Eddelbuettel <edd at debian.org> wrote:
>
> Hi Tony,
>
> On 20 January 2007 at 15:20, AJ Rossini wrote:
> | On Friday 19 January 2007 15:39, Dirk wrote:
> | > As I am doing more C++ work, I glanced at oo-browser, sidebar, ecb (all in
> | > Debian/Ubuntu).  Would a real Emacs hacker be able to these to R code too?
> | That functionality (though relatively minimal, i.e. ECB/sidebar support
> | through imenu) should have existed for 2+ years now, at least it does for me.
>
> Just confirms my suspicion that even after all these years, I barely
> scratched the surface of ess.  That '2+ years' old feature wouldn't happen to
> be documented somewhere, would it?


Dirk, I must be missing something. All I do is: M-x ecb-activate
Everything works. I do nothing special with ess. For that matter, I do
nothing special when editing LaTeX or Python, and ecb (et al) do work
as intended.

Best,

R.

>
> Dirk
>
> --
> Hell, there are no rules here - we're trying to accomplish something.
>                                                   -- Thomas A. Edison
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From f.harrell at vanderbilt.edu  Mon Jan 22 00:17:32 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 21 Jan 2007 17:17:32 -0600
Subject: [R] logistic regression model + Cross-Validation
In-Reply-To: <2b808b010701211251k13e263dap4941b2458e94db9c@mail.gmail.com>
References: <2b808b010701210251v2c7c5d02s8b3eeb6425c649f0@mail.gmail.com>	<45B37E88.4050500@vanderbilt.edu>
	<2b808b010701211251k13e263dap4941b2458e94db9c@mail.gmail.com>
Message-ID: <45B3F48C.4050905@vanderbilt.edu>

nitin jindal wrote:
> If validate.lrm does not has this option, do any other function has it.
> I will certainly look into your advice on cross validation. Thnx.
> 
> nitin

Not that I know of, but easy to program.
Frank

> 
> On 1/21/07, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
>> nitin jindal wrote:
>>> Hi,
>>>
>>> I am trying to cross-validate a logistic regression model.
>>> I am using logistic regression model (lrm) of package Design.
>>>
>>> f <- lrm( cy ~ x1 + x2, x=TRUE, y=TRUE)
>>> val <- validate.lrm(f, method="cross", B=5)
>> val <- validate(f, ...)    # .lrm not needed
>>
>>> My class cy has values 0 and 1.
>>>
>>> "val" variable will give me indicators like slope and AUC. But, I also
>> need
>>> the vector of predicted values of class variable "cy" for each record
>> while
>>> cross-validation, so that I can manually look at the results. So, is
>> there
>>> any way to get those probabilities assigned to each class.
>>>
>>> regards,
>>> Nitin
>> No, validate.lrm does not have that option.  Manually looking at the
>> results will not be easy when you do enough cross-validations.  A single
>> 5-fold cross-validation does not provide accurate estimates.  Either use
>> the bootstrap or repeat k-fold cross-validation between 20 and 50 times.
>>   k is often 10 but the optimum value may not be 10.  Code for averaging
>> repeated cross-validations is in
>> http://biostat.mc.vanderbilt.edu/twiki/pub/Main/RmS/logistic.val.pdf
>> along with simulations of bootstrap vs. a few cross-validation methods
>> for binary logistic models.
>>
>> Frank
>> --
>> Frank E Harrell Jr   Professor and Chair           School of Medicine
>>                       Department of Biostatistics   Vanderbilt University


From fuyaonv at hotmail.com  Mon Jan 22 00:24:13 2007
From: fuyaonv at hotmail.com (Lynette)
Date: Sun, 21 Jan 2007 18:24:13 -0500
Subject: [R] How to get correct integration in C for step function?
References: <45B02D46.1090709@vanderbilt.edu><200701191118.26761.rdiaz@cnio.es><17840.55304.803214.860567@basebud.nulle.part><200701201520.42653.blindglobe@gmail.com><17842.13129.56162.112711@basebud.nulle.part>
	<624934630701211505k6dd5e53fl8bf134bd08fce01c@mail.gmail.com>
Message-ID: <BAY131-DAV35D1B17CAFB03DDD716ACB3AF0@phx.gbl>

Dear all,

I am using Rdqags in C to realize the integration. It seems for the 
continous C function I can get correct results. However, for step functions, 
the results are not correct. For example, the following one, when integrated 
from 0 to 1 gives 1 instead of the correct 1.5

void func( double *x, int n, void *ex )
{
 int i;

 for(i=0;i<n;i++) { x[i]=( ((x>=1/4)&&(x<=3/4)) ? 2:1 ) ; }
        return;
 }

while the following one when integrated from 0 to 1 gives the correct 
0.7853983

void func( double *x, int n, void *ex )
{
 int i;

 for(i=0;i<n;i++) { x[i]= pow(1-x[i]*x[i],.5); }
        return;
 }

Please advise the problems. Thanks a lot.

Best,
Lynette


From skiadas at hanover.edu  Mon Jan 22 01:00:34 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Sun, 21 Jan 2007 19:00:34 -0500
Subject: [R] efficient code. how to reduce running time?
In-Reply-To: <dadd130701211455k3fee4e86t9d4608c5a316a347@mail.gmail.com>
References: <dadd130701210937q59937737l6c2361b8127f5fdc@mail.gmail.com>
	<20070121220044.NUOL5067.tomts5-srv.bellnexxia.net@JohnDesktop8300>
	<dadd130701211455k3fee4e86t9d4608c5a316a347@mail.gmail.com>
Message-ID: <E4BCC286-7D20-4634-878E-A0FAE3520AF4@hanover.edu>

On Jan 21, 2007, at 5:55 PM, miraceti wrote:

> Thank you all for lookin at it.
> I'll fix the code to preallocate the objects.
> and I wonder if there is a way to call anova on all the columns at  
> the same
> time..
> Right now I am calling (Y~V1, data) from V1 to V50 thru a loop.
> I tried (Y~., data) but it gave me different values from the  
> results I get
> when I call them separately,
> So I can't help but call them 25,000 times...

Have you looked at lapply, sapply, apply and friends?

Haris


From joe.crombie at affa.gov.au  Mon Jan 22 01:03:40 2007
From: joe.crombie at affa.gov.au (joe.crombie at affa.gov.au)
Date: Mon, 22 Jan 2007 11:03:40 +1100
Subject: [R] [UNCLASSIFIED] predict.survreg() with frailty term and newdata
Message-ID: <61C2DEA055980B418D063F8646FCAEFC020B5838@ACT001CL03EX03.agdaff.gov.au>

Dear All,
 
I am attempting to make predictions based on a survreg() model with some censoring and a frailty term, as below:  predict works fine on the original data, but not if I specify newdata.
 
# a model with groups as fixed effect
model1 <- survreg(Surv(y,cens)~ x1 + x2 + groups,
                 dist = "gaussian")

# and with groups as a random effect
fr <- frailty(groups, dist = "gaussian", sparse = F)
model2 <- survreg(Surv(y,cens)~ x1 + x2 + fr,
                  dist = "gaussian")

# predict() works fine for groups as fixed effect
predict(model1, newdata = list(x1 = 1, x2 = 1, groups = groups[1]))
#       [,1]
# 1 10.51833  # no problem!

# but not so well with frailty(groups)
predict(model2, newdata = list(x1 = 1, x2 = 1, fr = factor(1, levels = 1:10)))
#> Error in x %*% coef : non-conformable arguments

 
I have found references on the R and S-Plus lists which suggest that others have had this problem (in both packages), but I can't find any solution or further explanation.  Can anyone suggest a way of getting the predictions I'm after?  Any help is greatly appreciated.
 
Kind regards  Joe
 
Joe Crombie
 
Information and Risk Sciences
Bureau of Rural Science
Canberra  Australia
 
p: +61 2 6272 5906
e: joe.crombie at brs.gov.au <blocked::mailto:joe.crombie at brs.gov.au> 

---------------------------------------------------------------------- 
IMPORTANT - This message has been issued by The Department of Agriculture, Fisheries and Forestry (DAFF).  The information transmitted is for the use of the intended recipient only and may contain confidential and/or legally privileged material.  It is your responsibility to check any attachments for viruses and defects before opening or sending them on.  
Any reproduction, publication, communication, re-transmission, disclosure, dissemination or other use of the information contained in this e-mail by persons or entities other than the intended recipient is prohibited.  The taking of any action in reliance upon this information by persons or entities other than the intended recipient is prohibited.  If you have received this e-mail in error please notify the sender and delete all copies of this transmission together with any attachments.  If you have received this e-mail as part of a valid mailing list and no longer want to receive a message such as this one advise the sender by return e-mail accordingly.  Only e-mail correspondence which includes this footer, has been authorised by DAFF


From jfox at mcmaster.ca  Mon Jan 22 02:11:19 2007
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 21 Jan 2007 20:11:19 -0500
Subject: [R] efficient code. how to reduce running time?
In-Reply-To: <E4BCC286-7D20-4634-878E-A0FAE3520AF4@hanover.edu>
Message-ID: <20070122011119.KOPX11361.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear Haris,

Using lapply() et al. may produce cleaner code, but it won't necessarily
speed up a computation. For example:

> X <- data.frame(matrix(rnorm(1000*1000), 1000, 1000))
> y <- rnorm(1000)
> 
> mods <- as.list(1:1000)
> system.time(for (i in 1:1000) mods[[i]] <- lm(y ~ X[,i]))
[1] 40.53  0.05 40.61    NA    NA
> 
> system.time(mods <- lapply(as.list(X), function(x) lm(y ~ x)))
[1] 53.29  0.37 53.94    NA    NA

In cases such as this, I don't even find the code using *apply() easier to
read.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Charilaos Skiadas
> Sent: Sunday, January 21, 2007 7:01 PM
> To: miraceti
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] efficient code. how to reduce running time?
> 
> On Jan 21, 2007, at 5:55 PM, miraceti wrote:
> 
> > Thank you all for lookin at it.
> > I'll fix the code to preallocate the objects.
> > and I wonder if there is a way to call anova on all the 
> columns at the 
> > same time..
> > Right now I am calling (Y~V1, data) from V1 to V50 thru a loop.
> > I tried (Y~., data) but it gave me different values from 
> the results I 
> > get when I call them separately, So I can't help but call 
> them 25,000 
> > times...
> 
> Have you looked at lapply, sapply, apply and friends?
> 
> Haris
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sundar.dorai-raj at pdf.com  Mon Jan 22 02:26:22 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Sun, 21 Jan 2007 19:26:22 -0600
Subject: [R] How to get correct integration in C for step function?
In-Reply-To: <BAY131-DAV35D1B17CAFB03DDD716ACB3AF0@phx.gbl>
References: <45B02D46.1090709@vanderbilt.edu><200701191118.26761.rdiaz@cnio.es><17840.55304.803214.860567@basebud.nulle.part><200701201520.42653.blindglobe@gmail.com><17842.13129.56162.112711@basebud.nulle.part>	<624934630701211505k6dd5e53fl8bf134bd08fce01c@mail.gmail.com>
	<BAY131-DAV35D1B17CAFB03DDD716ACB3AF0@phx.gbl>
Message-ID: <45B412BE.9080402@pdf.com>

Hi, Lynette,

A few pointers:

1. Not an R question.
2. Not an ESS question.
3. No reproducible example.
4. (x >= 1/4) is comparing a pointer. Surely, this is not what you 
intended to do. Plus, if you're using void* then this is not even a C 
function called by R. And I'm not familiar with Rdqags.
5. Please use the posting guide in the future.

http://www.R-project.org/posting-guide.html

HTH,

--sundar


Lynette said the following on 1/21/2007 5:24 PM:
> Dear all,
> 
> I am using Rdqags in C to realize the integration. It seems for the 
> continous C function I can get correct results. However, for step functions, 
> the results are not correct. For example, the following one, when integrated 
> from 0 to 1 gives 1 instead of the correct 1.5
> 
> void func( double *x, int n, void *ex )
> {
>  int i;
> 
>  for(i=0;i<n;i++) { x[i]=( ((x>=1/4)&&(x<=3/4)) ? 2:1 ) ; }
>         return;
>  }
> 
> while the following one when integrated from 0 to 1 gives the correct 
> 0.7853983
> 
> void func( double *x, int n, void *ex )
> {
>  int i;
> 
>  for(i=0;i<n;i++) { x[i]= pow(1-x[i]*x[i],.5); }
>         return;
>  }
> 
> Please advise the problems. Thanks a lot.
> 
> Best,
> Lynette
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From qjing at sibs.ac.cn  Mon Jan 22 02:30:12 2007
From: qjing at sibs.ac.cn (qing)
Date: Mon, 22 Jan 2007 01:30:12 +0000 (UTC)
Subject: [R] the value of Delta
Message-ID: <loom.20070119T092542-850@post.gmane.org>







Dear all,

I am running R 2.4.1.

> library(siggenes);
> library(multtest);
> cl<-rep(c(0,1),c(3,3));
> sub<-exprs(AffyExpData[,c(1:3,7:9)]);
> gn<-geneNames(AffyRAwData);
> sam.out<-sam(sub,cl,rand=123,gene.names=gn);

We're doing 20 complete permutations

> sam.out
SAM Analysis for the Two-Class Unpaired Case Assuming Unequal Variances 
 
   Delta    p0  False Called   FDR
1    0.1 0.929 292.25    293 0.927
2    0.4 0.929  43.60     56 0.724
3    0.7 0.929  12.25     20 0.569
4    1.0 0.929   7.25     14 0.481
5    1.3 0.929   2.60      7 0.345
6    1.7 0.929   1.30      5 0.242
7    2.0 0.929   1.30      5 0.242
8    2.3 0.929   0.45      2 0.209
9    2.6 0.929   0.45      2 0.209
10   2.9 0.929   0.45      2 0.209

> sum.sam.out<-summary(sam.out,1,ll=FALSE);
> sum.sam.out$row.sig.genes;
NULL
> sum.sam.out<-summary(sam.out,0.1,ll=FALSE);
> sum.sam.out$row.sig.genes;
NULL
> sum.sam.out<-summary(sam.out,0.00000000001,ll=FALSE);
> sum.sam.out$row.sig.genes;
NULL
> sum.sam.out<-summary(sam.out,1000,ll=FALSE);
> sum.sam.out$row.sig.genes;
NULL
   

Any value of Delta I chosen: 1000,1,0,1 0.00000000001, the outcome is NULL.

Any good suggestions?

Thank you very much to your great help.

Qing Phd


From rmh at temple.edu  Mon Jan 22 03:52:30 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Sun, 21 Jan 2007 21:52:30 -0500 (EST)
Subject: [R] inconsistent behavior of mean, median, var
Message-ID: <20070121215230.BSP07204@po-d.temple.edu>

> mean(c("-1","0", "1"))
[1] NA
Warning message:
argument is not numeric or logical: returning NA in: mean.default(c("-1", "0", "1")) 
> median(c("-1","0", "1"))
Error in median.default(c("-1", "0", "1")) : 
        need numeric data
> var(c("-1","0", "1"))
[1] 1
> 


Is the above intentional behavior?  mean and median of character data
give a warning and error.  var gives the result of coercing the argument
to numeric.


From mardones.p at gmail.com  Mon Jan 22 05:05:25 2007
From: mardones.p at gmail.com (Pedro Mardones)
Date: Sun, 21 Jan 2007 23:05:25 -0500
Subject: [R] "kennard-stone" selection algorithm
Message-ID: <83dca7860701212005hb6204dbr9921736310345a7f@mail.gmail.com>

Dear R users;
I've been looking for an R implementation of the cadex and/or duplex
algorithm [Kennard-Stone (1969), Snee (1977)] for selecting samples
for calibration models with no success so far. Does anyone know if
these algorithms have been implemented in R, maybe with different
name(s)?
Thanks for any idea
PM


From mrennie at utm.utoronto.ca  Mon Jan 22 06:05:18 2007
From: mrennie at utm.utoronto.ca (Mike Rennie)
Date: Mon, 22 Jan 2007 00:05:18 -0500
Subject: [R] Advice on an appropriate condition for warning in a function
Message-ID: <1169442318.45b4460e99d07@webmail.utm.utoronto.ca>



Hi, there

I have written a function that will allow me to calculate the variance 
components for a model II (random effects) single factor ANOVA (perhaps this is 
me re-inventing the wheel, but I handn't yet come across anything in R that 
does this for me, but I'm sure people will let me know from this posting if 
there are). The function takes as it's argument an object which holds the 
results of an lm call. e.g.,

object1<_lm(y~x)

The function is as follows (some comments included):

comp.var.estimates<-function(object.lm)
		{
		anovmod<-anova(object.lm) #get the anova table for the lm
		MStreat<-anovmod[1,3]; MSErr<-anovmod[2,3] #extract Mean Squares
		dataframe<- as.data.frame(object.lm[13]) 		
		ni <- tapply(dataframe[,1], dataframe[,2], length) 		
		nisq<-ni^2
		no<-(1/(length(ni)-1))*(sum(ni)-(sum(nisq)/sum(ni)))
		s2a<-((MSTreat-MSErr)/no)
		stot<-s2a + MSErr
		treatvar<-s2a/stot*100 
		errorvar<-MSErr/stot*100
		list(treat.var.comp=s2a, err.var.comp=MSErr, 			
		p.var.treat=treatvar, p.var.err=errorvar)
		}
comp.var.estimates(object1)

I'd like to include a "warning" statement in the function that 
returns 'function requires arguments that are objects of the form obj<-lm
(y~x)', but I need a case to evaluate the object against in order to throw the 
warning.

Any advice? 

I feel like my best opportunity is after the first line into the function, 
where I ask for the ANOVA table. Since this is a 2 X 5 table, presumably I 
should be able to evaluate it against the size of that table? Any thoughts on 
how to check that? I welcome any suggestions.

Cheers,

Mike 

-- 
Michael Rennie
Ph.D. Candidate, University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga, ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792
www.utm.utoronto.ca/~w3rennie


From Matthias.Kohl at stamats.de  Mon Jan 22 07:02:03 2007
From: Matthias.Kohl at stamats.de (Matthias Kohl)
Date: Mon, 22 Jan 2007 07:02:03 +0100 (MET)
Subject: [R] Integration + Normal Distribution + Directory Browsing
 Processing Questions
Message-ID: <200701220602.l0M623Un019556@post.webmailer.de>

Hi,

why don't you use pnorm?
E.g.,

pnorm(1, mean = 0.1, sd = 1.2) - pnorm(0, mean = 0.1, sd = 1.2)

Matthias

----- original message --------

Subject: Re: [R] Integration + Normal Distribution + Directory Browsing Processing Questions
Sent: Sun, 21 Jan 2007
From: Dimitris Rizopoulos<Dimitris.Rizopoulos at med.kuleuven.be>

> you can use the `...' argument of integrate, e.g.,
> 
> integrate(dnorm, 0, 1)
> integrate(dnorm, 0, 1, mean = 0.1)
> integrate(dnorm, 0, 1, mean = 0.1, sd = 1.2)
> 
> look at ?integrate for more info.
> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>       http://www.student.kuleuven.be/~m0390867/dimitris.htm
> 
> 
> Quoting Nils Hoeller <mail at nhoeller.de>:
> 
> > Hi everyone,
> >
> > I am new to R, but it's really great and helped me a lot!
> >
> > But now I have 2 questions. It would be great, if someone can help me:
> >
> > 1. I want to integrate a normal distribution, given a median and sd.
> > The integrate function works great BUT the first argument has to be a
> > function
> >
> > so I do integrate(dnorm,0,1) and it works with standard m. and sd.
> >
> > But I have the m and sd given.
> >
> > So for fixed m and sd I work around with a new function mynorm
> >
> > mynorm <- function(n) {
> >     ret <- dnorm(n,0.6,0.15)
> >     ret
> > }
> >
> > for example.
> >
> > BUT what can I do for dynamic m and sd?
> > I want something like integrate(dnorm(,0.6,0.15),0,1), with the first
> > dnorm parameter open for the
> > integration but fixed m and sd.
> >
> > I hope you can help me.
> >
> > 2. I am working with textfiles with rows of measure data.
> > read.table("file") works fine.
> >
> > Now I want R to read.table all files within a given directory and
> > process them one by the other.
> >
> > for(all files in dir xy) {
> > x <- read.table(nextfile)
> > process x
> > }
> >
> > Is that possible with R? I hope so. Can anyone give me a link to
> examples.
> >
> > Thanks for your help
> >
> > Nils
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> 
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

--- original message end ----


--
Dr. rer. nat. Matthias Kohl
Matthias.Kohl at stamats.de
www.stamats.de


From skiadas at hanover.edu  Mon Jan 22 07:27:06 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Mon, 22 Jan 2007 01:27:06 -0500
Subject: [R] Integration + Normal Distribution + Directory Browsing
	Processing Questions
In-Reply-To: <45B3BEA7.2010001@nhoeller.de>
References: <45B3BEA7.2010001@nhoeller.de>
Message-ID: <FF7A96A5-3836-4229-9BB0-C8E6003BDE27@hanover.edu>

On Jan 21, 2007, at 2:27 PM, Nils Hoeller wrote:

> Now I want R to read.table all files within a given directory and
> process them one by the other.

?list.files
?"for"

Haris


From qjing at sibs.ac.cn  Mon Jan 22 07:46:43 2007
From: qjing at sibs.ac.cn (qing)
Date: Mon, 22 Jan 2007 06:46:43 +0000 (UTC)
Subject: [R] RMySQL connection
Message-ID: <loom.20070119T092542-850@post.gmane.org>






Dear all,
 
I am running R 2.4.1    RMySQL_0.5-7  DBI_0.1-10  Windows XP

> library(DBI);
> library(RMySQL);
> con<-dbConnect(dbDriver("MySQL"),dbname="test");
Error in function (classes, fdef, mtable)  : 
        unable to find an inherited method for function "dbConnect", for 
signature "MySQLDriver"

What can I do?

Thank you very much for your help.

Qing phd


From arun.kumar.saha at gmail.com  Mon Jan 22 08:25:17 2007
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Mon, 22 Jan 2007 12:55:17 +0530
Subject: [R] Finding the effect of Box-Cox transformation using "vis.boxcoxu"
Message-ID: <d4c57560701212325v7765b1b4g259618a7eed7fabe@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070122/2d6d9f2f/attachment.pl 

From rdiaz02 at gmail.com  Mon Jan 22 00:02:33 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Mon, 22 Jan 2007 00:02:33 +0100
Subject: [R] Offtopic: emacs 23, was  kate editor for R
In-Reply-To: <45B24558.5070202@biostat.ku.dk>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
	<17840.55304.803214.860567@basebud.nulle.part>
	<200701191609.40533.rdiaz@cnio.es>
	<1169240161.4838.27.camel@localhost.localdomain>
	<624934630701191859sfdbea6ey19b0a2a1929a04e9@mail.gmail.com>
	<45B24558.5070202@biostat.ku.dk>
Message-ID: <624934630701211502j7ec2e6f7l4aef4e155890b6@mail.gmail.com>

On 1/20/07, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Ramon Diaz-Uriarte wrote:
> > Hi Marc,
> >
> >
> > Thanks a lot for the detailed explanation! I'll give it a try. (But
> > still, why emacs23? what is missing in v. 21 that you get in 23?).
> >
> > Best,
> >
> > R.
> >
> Ability to load files with UTF-8 characters in the name? (This is pretty
> maddening if you find yourself with such a beast.)
>

Aha, thanks. I try to stay away from those creatures. I guess I'll be
able to start adding a nice, good looking spanish "?" to file names
:-).

R.

> BTW, any inkling when/whether this is heading for Fedora N?
>


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From edd at debian.org  Mon Jan 22 01:12:18 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 21 Jan 2007 18:12:18 -0600
Subject: [R] ECB/Sidebar/R (Emacs) was: Re: kate editor for R
In-Reply-To: <624934630701211505k6dd5e53fl8bf134bd08fce01c@mail.gmail.com>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
	<17840.55304.803214.860567@basebud.nulle.part>
	<200701201520.42653.blindglobe@gmail.com>
	<17842.13129.56162.112711@basebud.nulle.part>
	<624934630701211505k6dd5e53fl8bf134bd08fce01c@mail.gmail.com>
Message-ID: <17844.354.297037.512928@basebud.nulle.part>


On 22 January 2007 at 00:05, Ramon Diaz-Uriarte wrote:
| On 1/20/07, Dirk Eddelbuettel <edd at debian.org> wrote:
| > Just confirms my suspicion that even after all these years, I barely
| > scratched the surface of ess.  That '2+ years' old feature wouldn't happen to
| > be documented somewhere, would it?
| 
| Dirk, I must be missing something. All I do is: M-x ecb-activate
| Everything works. I do nothing special with ess. For that matter, I do
| nothing special when editing LaTeX or Python, and ecb (et al) do work
| as intended.

I had looked at ECB for C++ programming. It simply hadn't occurred to me that
it would plug into ESS. Score another one for Emacs as an operating system.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From dchandra at ics.uci.edu  Mon Jan 22 04:46:36 2007
From: dchandra at ics.uci.edu (Deepak Chandra)
Date: Sun, 21 Jan 2007 19:46:36 -0800
Subject: [R] command-line arguments in R
Message-ID: <7387cdce0701211946s4144a5eanc9e3a4673b71b1ce@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070121/b16e35e1/attachment.pl 

From gnvshqp at gmail.com  Sun Jan 21 18:43:35 2007
From: gnvshqp at gmail.com (gnv shqp)
Date: Sun, 21 Jan 2007 12:43:35 -0500
Subject: [R] Scatterplot help
In-Reply-To: <6ab3ba540701210916r403ef8bbp460f016e58637aa5@mail.gmail.com>
References: <6ab3ba540701210916r403ef8bbp460f016e58637aa5@mail.gmail.com>
Message-ID: <6ab3ba540701210943kab25a27l5ae5727ade4f1fae@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070121/187f291d/attachment.pl 

From mail at nhoeller.de  Mon Jan 22 08:49:59 2007
From: mail at nhoeller.de (Nils Hoeller)
Date: Mon, 22 Jan 2007 08:49:59 +0100
Subject: [R] Integration + Normal Distribution + Directory Browsing
 Processing Questions
In-Reply-To: <200701220602.l0M623Un019556@post.webmailer.de>
References: <200701220602.l0M623Un019556@post.webmailer.de>
Message-ID: <45B46CA7.4070001@nhoeller.de>

Thank you,

both work fine. Why is pnorm to prefer?

Nils


Matthias Kohl schrieb:
> Hi,
>
> why don't you use pnorm?
> E.g.,
>
> pnorm(1, mean = 0.1, sd = 1.2) - pnorm(0, mean = 0.1, sd = 1.2)
>
> Matthias
>
> ----- original message --------
>
> Subject: Re: [R] Integration + Normal Distribution + Directory Browsing Processing Questions
> Sent: Sun, 21 Jan 2007
> From: Dimitris Rizopoulos<Dimitris.Rizopoulos at med.kuleuven.be>
>
>   
>> you can use the `...' argument of integrate, e.g.,
>>
>> integrate(dnorm, 0, 1)
>> integrate(dnorm, 0, 1, mean = 0.1)
>> integrate(dnorm, 0, 1, mean = 0.1, sd = 1.2)
>>
>> look at ?integrate for more info.
>>
>> I hope it helps.
>>
>> Best,
>> Dimitris
>>
>> ----
>> Dimitris Rizopoulos
>> Ph.D. Student
>> Biostatistical Centre
>> School of Public Health
>> Catholic University of Leuven
>>
>> Address: Kapucijnenvoer 35, Leuven, Belgium
>> Tel: +32/(0)16/336899
>> Fax: +32/(0)16/337015
>> Web: http://med.kuleuven.be/biostat/
>>       http://www.student.kuleuven.be/~m0390867/dimitris.htm
>>
>>
>> Quoting Nils Hoeller <mail at nhoeller.de>:
>>
>>     
>>> Hi everyone,
>>>
>>> I am new to R, but it's really great and helped me a lot!
>>>
>>> But now I have 2 questions. It would be great, if someone can help me:
>>>
>>> 1. I want to integrate a normal distribution, given a median and sd.
>>> The integrate function works great BUT the first argument has to be a
>>> function
>>>
>>> so I do integrate(dnorm,0,1) and it works with standard m. and sd.
>>>
>>> But I have the m and sd given.
>>>
>>> So for fixed m and sd I work around with a new function mynorm
>>>
>>> mynorm <- function(n) {
>>>     ret <- dnorm(n,0.6,0.15)
>>>     ret
>>> }
>>>
>>> for example.
>>>
>>> BUT what can I do for dynamic m and sd?
>>> I want something like integrate(dnorm(,0.6,0.15),0,1), with the first
>>> dnorm parameter open for the
>>> integration but fixed m and sd.
>>>
>>> I hope you can help me.
>>>
>>> 2. I am working with textfiles with rows of measure data.
>>> read.table("file") works fine.
>>>
>>> Now I want R to read.table all files within a given directory and
>>> process them one by the other.
>>>
>>> for(all files in dir xy) {
>>> x <- read.table(nextfile)
>>> process x
>>> }
>>>
>>> Is that possible with R? I hope so. Can anyone give me a link to
>>>       
>> examples.
>>     
>>> Thanks for your help
>>>
>>> Nils
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>>       
>> http://www.R-project.org/posting-guide.html
>>     
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>       
>>
>> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>     
>
> --- original message end ----
>
>
> --
> Dr. rer. nat. Matthias Kohl
> Matthias.Kohl at stamats.de
> www.stamats.de 
>
>


From Matthias.Kohl at stamats.de  Mon Jan 22 09:08:18 2007
From: Matthias.Kohl at stamats.de (Matthias Kohl)
Date: Mon, 22 Jan 2007 09:08:18 +0100 (MET)
Subject: [R] Integration + Normal Distribution + Directory Browsing
 Processing Questions
Message-ID: <200701220808.l0M88Ia6015520@post.webmailer.de>

Hi Nils,

I would say, pnorm is faster and has a higher precision.

Best,
Matthias

----- original message --------

Subject: Re: [R] Integration + Normal Distribution + Directory Browsing Processing Questions
Sent: Mon, 22 Jan 2007
From: Nils Hoeller<mail at nhoeller.de>

> Thank you,
> 
> both work fine. Why is pnorm to prefer?
> 
> Nils
> 
> 
> Matthias Kohl schrieb:
> > Hi,
> >
> > why don't you use pnorm?
> > E.g.,
> >
> > pnorm(1, mean = 0.1, sd = 1.2) - pnorm(0, mean = 0.1, sd = 1.2)
> >
> > Matthias
> >
> > ----- original message --------
> >
> > Subject: Re: [R] Integration + Normal Distribution + Directory Browsing
> Processing Questions
> > Sent: Sun, 21 Jan 2007
> > From: Dimitris Rizopoulos<Dimitris.Rizopoulos at med.kuleuven.be>
> >
> >   
> >> you can use the `...' argument of integrate, e.g.,
> >>
> >> integrate(dnorm, 0, 1)
> >> integrate(dnorm, 0, 1, mean = 0.1)
> >> integrate(dnorm, 0, 1, mean = 0.1, sd = 1.2)
> >>
> >> look at ?integrate for more info.
> >>
> >> I hope it helps.
> >>
> >> Best,
> >> Dimitris
> >>
> >> ----
> >> Dimitris Rizopoulos
> >> Ph.D. Student
> >> Biostatistical Centre
> >> School of Public Health
> >> Catholic University of Leuven
> >>
> >> Address: Kapucijnenvoer 35, Leuven, Belgium
> >> Tel: +32/(0)16/336899
> >> Fax: +32/(0)16/337015
> >> Web: http://med.kuleuven.be/biostat/
> >>       http://www.student.kuleuven.be/~m0390867/dimitris.htm
> >>
> >>
> >> Quoting Nils Hoeller <mail at nhoeller.de>:
> >>
> >>     
> >>> Hi everyone,
> >>>
> >>> I am new to R, but it's really great and helped me a lot!
> >>>
> >>> But now I have 2 questions. It would be great, if someone can help me:
> >>>
> >>> 1. I want to integrate a normal distribution, given a median and sd.
> >>> The integrate function works great BUT the first argument has to be a
> >>> function
> >>>
> >>> so I do integrate(dnorm,0,1) and it works with standard m. and sd.
> >>>
> >>> But I have the m and sd given.
> >>>
> >>> So for fixed m and sd I work around with a new function mynorm
> >>>
> >>> mynorm <- function(n) {
> >>>     ret <- dnorm(n,0.6,0.15)
> >>>     ret
> >>> }
> >>>
> >>> for example.
> >>>
> >>> BUT what can I do for dynamic m and sd?
> >>> I want something like integrate(dnorm(,0.6,0.15),0,1), with the first
> >>> dnorm parameter open for the
> >>> integration but fixed m and sd.
> >>>
> >>> I hope you can help me.
> >>>
> >>> 2. I am working with textfiles with rows of measure data.
> >>> read.table("file") works fine.
> >>>
> >>> Now I want R to read.table all files within a given directory and
> >>> process them one by the other.
> >>>
> >>> for(all files in dir xy) {
> >>> x <- read.table(nextfile)
> >>> process x
> >>> }
> >>>
> >>> Is that possible with R? I hope so. Can anyone give me a link to
> >>>       
> >> examples.
> >>     
> >>> Thanks for your help
> >>>
> >>> Nils
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>>       
> >> http://www.R-project.org/posting-guide.html
> >>     
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>>
> >>>       
> >>
> >> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >>     
> >
> > --- original message end ----
> >
> >
> > --
> > Dr. rer. nat. Matthias Kohl
> > Matthias.Kohl at stamats.de
> > www.stamats.de 
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

--- original message end ----


--
Dr. rer. nat. Matthias Kohl
Matthias.Kohl at stamats.de
www.stamats.de


From dimitris.rizopoulos at med.kuleuven.be  Mon Jan 22 09:16:55 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 22 Jan 2007 09:16:55 +0100
Subject: [R] Integration + Normal Distribution + Directory Browsing
	Processing Questions
References: <200701220602.l0M623Un019556@post.webmailer.de>
	<45B46CA7.4070001@nhoeller.de>
Message-ID: <00e801c73dfd$ae06f420$0540210a@www.domain>

the reason is that is more natural to use pnorm(), which also should a 
more efficient approximation of the Normal integral than intgrate(), 
you may even use

diff(pnorm(0:1, mean = 0.5, sd = 1.2))

however, the point I meant to make was to use the '...' argument that 
can found in many R functions.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Nils Hoeller" <mail at nhoeller.de>
To: "Matthias Kohl" <Matthias.Kohl at stamats.de>
Cc: "Dimitris Rizopoulos" <Dimitris.Rizopoulos at med.kuleuven.be>; 
<r-help at stat.math.ethz.ch>
Sent: Monday, January 22, 2007 8:49 AM
Subject: Re: [R] Integration + Normal Distribution + Directory 
Browsing Processing Questions


> Thank you,
>
> both work fine. Why is pnorm to prefer?
>
> Nils
>
>
> Matthias Kohl schrieb:
>> Hi,
>>
>> why don't you use pnorm?
>> E.g.,
>>
>> pnorm(1, mean = 0.1, sd = 1.2) - pnorm(0, mean = 0.1, sd = 1.2)
>>
>> Matthias
>>
>> ----- original message --------
>>
>> Subject: Re: [R] Integration + Normal Distribution + Directory 
>> Browsing Processing Questions
>> Sent: Sun, 21 Jan 2007
>> From: Dimitris Rizopoulos<Dimitris.Rizopoulos at med.kuleuven.be>
>>
>>
>>> you can use the `...' argument of integrate, e.g.,
>>>
>>> integrate(dnorm, 0, 1)
>>> integrate(dnorm, 0, 1, mean = 0.1)
>>> integrate(dnorm, 0, 1, mean = 0.1, sd = 1.2)
>>>
>>> look at ?integrate for more info.
>>>
>>> I hope it helps.
>>>
>>> Best,
>>> Dimitris
>>>
>>> ----
>>> Dimitris Rizopoulos
>>> Ph.D. Student
>>> Biostatistical Centre
>>> School of Public Health
>>> Catholic University of Leuven
>>>
>>> Address: Kapucijnenvoer 35, Leuven, Belgium
>>> Tel: +32/(0)16/336899
>>> Fax: +32/(0)16/337015
>>> Web: http://med.kuleuven.be/biostat/
>>>       http://www.student.kuleuven.be/~m0390867/dimitris.htm
>>>
>>>
>>> Quoting Nils Hoeller <mail at nhoeller.de>:
>>>
>>>
>>>> Hi everyone,
>>>>
>>>> I am new to R, but it's really great and helped me a lot!
>>>>
>>>> But now I have 2 questions. It would be great, if someone can 
>>>> help me:
>>>>
>>>> 1. I want to integrate a normal distribution, given a median and 
>>>> sd.
>>>> The integrate function works great BUT the first argument has to 
>>>> be a
>>>> function
>>>>
>>>> so I do integrate(dnorm,0,1) and it works with standard m. and 
>>>> sd.
>>>>
>>>> But I have the m and sd given.
>>>>
>>>> So for fixed m and sd I work around with a new function mynorm
>>>>
>>>> mynorm <- function(n) {
>>>>     ret <- dnorm(n,0.6,0.15)
>>>>     ret
>>>> }
>>>>
>>>> for example.
>>>>
>>>> BUT what can I do for dynamic m and sd?
>>>> I want something like integrate(dnorm(,0.6,0.15),0,1), with the 
>>>> first
>>>> dnorm parameter open for the
>>>> integration but fixed m and sd.
>>>>
>>>> I hope you can help me.
>>>>
>>>> 2. I am working with textfiles with rows of measure data.
>>>> read.table("file") works fine.
>>>>
>>>> Now I want R to read.table all files within a given directory and
>>>> process them one by the other.
>>>>
>>>> for(all files in dir xy) {
>>>> x <- read.table(nextfile)
>>>> process x
>>>> }
>>>>
>>>> Is that possible with R? I hope so. Can anyone give me a link to
>>>>
>>> examples.
>>>
>>>> Thanks for your help
>>>>
>>>> Nils
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>>
>>> http://www.R-project.org/posting-guide.html
>>>
>>>> and provide commented, minimal, self-contained, reproducible 
>>>> code.
>>>>
>>>>
>>>>
>>>
>>> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>
>> --- original message end ----
>>
>>
>> --
>> Dr. rer. nat. Matthias Kohl
>> Matthias.Kohl at stamats.de
>> www.stamats.de
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From anil_rohilla at rediffmail.com  Mon Jan 22 09:53:27 2007
From: anil_rohilla at rediffmail.com (anil kumar rohilla)
Date: 22 Jan 2007 08:53:27 -0000
Subject: [R] Combination diffrent variables ,how to calculates
Message-ID: <20070122085327.6238.qmail@webmail66.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070122/85c1cfd4/attachment.pl 

From jtuimala at csc.fi  Mon Jan 22 09:57:59 2007
From: jtuimala at csc.fi (Jarno Tuimala)
Date: Mon, 22 Jan 2007 10:57:59 +0200 (EET)
Subject: [R] How to disable existing menus in tcltk?
Message-ID: <Pine.LNX.4.62.0701221029310.23753@sampo3.csc.fi>

Hi!

I've constructed a small menu-driven interface to a couple of R functions 
using the possibilities offered by the tcltk package. When user runs some 
specific analyses, I would then like to disable some of the menus (or menu 
choises) that are not applicable after the performed analysis. I tried to 
modify the state of an existing menu, but it seems that neither 
tkconfigure nor tkentryconfigure contains the state as one of its options.

Here's a snip of the code. How could I disable, for example, the Edit 
data menu choise after already creating the menu (I want it to be active 
initially)?

gui<-tktoplevel()
topMenu<-tkmenu(gui)
tkconfigure(gui,menu=topMenu)
editMenu<-tkmenu(topMenu, tearoff=FALSE)
tkadd(editMenu, "command", label="Edit data", command=function() editData())
tkadd(editMenu, "command", label="Preferences", command=function() editPref())
tkadd(topMenu, "cascade", label="Edit", menu=editMenu)

Thanks,
Jarno Tuimala


From buser at stat.math.ethz.ch  Mon Jan 22 10:08:54 2007
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 22 Jan 2007 10:08:54 +0100
Subject: [R] aov y lme
In-Reply-To: <6.0.3.0.0.20070120124213.01a57570@pop.unavarra.es>
References: <6.0.3.0.0.20070120124213.01a57570@pop.unavarra.es>
Message-ID: <17844.32550.213213.405226@stat.math.ethz.ch>

Dear Tomas

You can produce the results in Montgomery Montgomery with
lme. Please remark that you should indicate the nesting with the
levels in your nested factor. Therefore I recreated your data,
but used 1,...,12 for the levels of batch instead of 1,...,4.

purity<-c(1,-2,-2,1,-1,-3,0,4, 0,-4, 1,0, 1,0,-1,0,-2,4,0,3,
          -3,2,-2,2,2,-2,1,3,4,0,-1,2,0,2,2,1)
suppli<-factor(c(rep(1,12),rep(2,12),rep(3,12)))
batch<-factor(c(rep(1:4,3), rep(5:8,3), rep(9:12,3)))
material<-data.frame(purity,suppli,batch)

As you remarked you can use aov

summary(material.aov<-aov(purity~suppli+suppli:batch,data=material))
             Df Sum Sq Mean Sq F value  Pr(>F)  
suppli        2 15.056   7.528  2.8526 0.07736 .
suppli:batch  9 69.917   7.769  2.9439 0.01667 *
Residuals    24 63.333   2.639                  
---
Signif. codes:  0 $,1rx(B***$,1ry(B 0.001 $,1rx(B**$,1ry(B 0.01 $,1rx(B*$,1ry(B 0.05 $,1rx(B.$,1ry(B 0.1 $,1rx(B $,1ry(B 1 

Remark that the test of "suppli" is not the same as in
Montgomery. Here it is wrongly tested against the Residuals and
you should perform the calculate the test with: 

(Fsuppi <- summary(material.aov)[[1]][1,"Mean Sq"]/
  summary(material.aov)[[1]][2,"Mean Sq"])
pf(Fsuppi, df1 = 2, df2 = 9)

To use lme the correct level numbering is now important to
indicate the nesting. You should specify your random component
as 

random=~1|batch

If you use "random=~1|suppli/batch" instead, random components
for batch AND suppli are included in the model and you specify a 
model that incorporates suppli as random and fixed. Therefore
the correct syntax is

library(nlme)
material.lme<-lme(purity~suppli,random=~1|batch,data=material)
## You obtain the F-test for suppli using "anova"
anova(material.lme)
summary(material.lme)
## Remark that in the summary output, the random effects are
## standard deviations and not variance components and you 
## should square them to compare them with Montgomery
## 1.307622^2 = 1.71             1.624466^2 = 2.64

## Or you can use 
VarCorr(material.lme)

I hope this helps you.

Best regards,

Christoph Buser

--------------------------------------------------------------

Credit and Surety PML study: visit our web page www.cs-pml.org

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH Zurich	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------

Tomas Goicoa writes:
 > 
 > 
 > 
 > Dear R user,
 > 
 > I am trying to reproduce the results in Montgomery D.C (2001, chap 13, 
 > example 13-1).
 > 
 > Briefly, there are three suppliers, four batches nested within suppliers 
 > and three determinations of purity (response variable) on each batch. It is 
 > a two stage nested design, where suppliers are fixed and batches are random.
 > 
 > y_ijk=mu+tau_i+beta_j(nested in tau_i)+epsilon_ijk
 > 
 > Here are the data,
 > 
 > purity<-c(1,-2,-2,1,
 >           -1,-3, 0,4,
 >            0,-4, 1, 0,
 >            1,0,-1,0,
 >            -2,4,0,3,
 >            -3,2,-2,2,
 >            2,-2,1,3,
 >            4,0,-1,2,
 >            0,2,2,1)
 > 
 > suppli<-factor(c(rep(1,12),rep(2,12),rep(3,12)))
 > batch<-factor(rep(c(1,2,3,4),9))
 > 
 > material<-data.frame(purity,suppli,batch)
 > 
 > If I use the function aov, I get
 > 
 > material.aov<-aov(purity~suppli+suppli:batch,data=material)
 > summary(material.aov)
 >               Df Sum Sq Mean Sq F value  Pr(>F)
 > suppli        2 15.056   7.528  2.8526 0.07736 .
 > suppli:batch  9 69.917   7.769  2.9439 0.01667 *
 > Residuals    24 63.333   2.639
 > ---
 > Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
 > 
 > and I can estimate the variance component for the batches as
 > 
 > (7.769- 2.639)/3=1.71
 > 
 > which is the way it is done in Montgomery, D.
 > 
 > I want to use the function lme because I would like to make a diagnosis of 
 > the model, and I think it is more appropriate.
 > 
 > Looking at Pinheiro and Bates, I have tried the following,
 > 
 > library(nlme)
 > material.lme<-lme(purity~suppli,random=~1|suppli/batch,data=material)
 > VarCorr(material.lme)
 > 
 >              Variance     StdDev
 > suppli =    pdLogChol(1)
 > (Intercept) 1.563785     1.250514
 > batch =     pdLogChol(1)
 > (Intercept) 1.709877     1.307622
 > Residual    2.638889     1.624466
 > 
 > material.lme
 > 
 > Linear mixed-effects model fit by REML
 >    Data: material
 >    Log-restricted-likelihood: -71.42198
 >    Fixed: purity ~ suppli
 > (Intercept)     suppli2     suppli3
 >   -0.4166667   0.7500000   1.5833333
 > 
 > Random effects:
 >   Formula: ~1 | suppli
 >          (Intercept)
 > StdDev:    1.250514
 > 
 >   Formula: ~1 | batch %in% suppli
 >          (Intercept) Residual
 > StdDev:    1.307622 1.624466
 > 
 > Number of Observations: 36
 > Number of Groups:
 >             suppli batch %in% suppli
 >                  3                12
 > 
 >  From VarCorr I obtain the variance component 1.71, but I am not sure if 
 > this is the way to fit the model for the nested design. Here, I also have a 
 > variance component for suppli and this is a fixed factor. Can anyone give 
 > me a clue?  
 > 	[[alternative HTML version deleted]]
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
 > and provide commented, minimal, self-contained, reproducible code.


From mothsailor at googlemail.com  Mon Jan 22 10:23:01 2007
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 22 Jan 2007 09:23:01 +0000
Subject: [R] Scatterplot help
In-Reply-To: <6ab3ba540701210943kab25a27l5ae5727ade4f1fae@mail.gmail.com>
References: <6ab3ba540701210916r403ef8bbp460f016e58637aa5@mail.gmail.com>
	<6ab3ba540701210943kab25a27l5ae5727ade4f1fae@mail.gmail.com>
Message-ID: <815b70590701220123u6b2f97o1b74dd67500df892@mail.gmail.com>

The following three lines will do what you want.  You will probably
want to change some of the default behaviour; just look at the
relevant help pages.

plot(x,y)
text(x,y,ID)
grid(2)

On 21/01/07, gnv shqp <gnvshqp at gmail.com> wrote:
> Hi my friends,
>
> I'm trying to make a scatterplot like this.
>
> 1) I have a 3-variable dataset.  They are ID, x, and y.
>
> 2) "x" is for the X-axis, "y" for the Y-axis, and "ID" is used to label all
> the cases in the scatterplot.
>
> 3) After creating the scatterplot, I need to add both a X-axis  reference
> line and a Y-axis reference line.  The X-axis reference line is a vertical
> line starting from the center of the X-axis.  The Y-axis reference line is a
> horizontal line starting from the center of the Y-axis.  In other words, by
> creating the two reference lines, the scatterplot is divided into 4
> quadrants.
>
> Please help me figure out how to do that in R.
>
> Many thanks in advance!
>
> Feng
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From jpressnell at westnet.com.au  Mon Jan 22 11:10:50 2007
From: jpressnell at westnet.com.au (Jerry Pressnell)
Date: Mon, 22 Jan 2007 21:10:50 +1100
Subject: [R] Unusual behaviour get.hist.quote
Message-ID: <000801c73e0d$9830cb20$6401a8c0@DESKTOP>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070122/9cf678f6/attachment.pl 

From rdiaz02 at gmail.com  Mon Jan 22 11:58:41 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Mon, 22 Jan 2007 11:58:41 +0100
Subject: [R] ECB/Sidebar/R (Emacs) was: Re: kate editor for R
In-Reply-To: <17844.354.297037.512928@basebud.nulle.part>
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
	<17840.55304.803214.860567@basebud.nulle.part>
	<200701201520.42653.blindglobe@gmail.com>
	<17842.13129.56162.112711@basebud.nulle.part>
	<624934630701211505k6dd5e53fl8bf134bd08fce01c@mail.gmail.com>
	<17844.354.297037.512928@basebud.nulle.part>
Message-ID: <624934630701220258t460ddde0w371d49c7b23f82a7@mail.gmail.com>

On 1/22/07, Dirk Eddelbuettel <edd at debian.org> wrote:
>
> On 22 January 2007 at 00:05, Ramon Diaz-Uriarte wrote:
> | On 1/20/07, Dirk Eddelbuettel <edd at debian.org> wrote:
> | > Just confirms my suspicion that even after all these years, I barely
> | > scratched the surface of ess.  That '2+ years' old feature wouldn't happen to
> | > be documented somewhere, would it?
> |
> | Dirk, I must be missing something. All I do is: M-x ecb-activate
> | Everything works. I do nothing special with ess. For that matter, I do
> | nothing special when editing LaTeX or Python, and ecb (et al) do work
> | as intended.
>
> I had looked at ECB for C++ programming. It simply hadn't occurred to me that
> it would plug into ESS.

I wasn't aware of it either until I attended Tony Rossini's tutorial
at useR! 2006.


> Score another one for Emacs as an operating system.

Oh yes, and almost coffee maker and pizza deliverer :-).


R.

>
> Dirk
>
> --
> Hell, there are no rules here - we're trying to accomplish something.
>                                                   -- Thomas A. Edison
>


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From b.rowlingson at lancaster.ac.uk  Mon Jan 22 12:11:25 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 22 Jan 2007 11:11:25 +0000
Subject: [R] Unusual behaviour get.hist.quote
In-Reply-To: <000801c73e0d$9830cb20$6401a8c0@DESKTOP>
References: <000801c73e0d$9830cb20$6401a8c0@DESKTOP>
Message-ID: <45B49BDD.1030306@lancaster.ac.uk>

Jerry Pressnell wrote:
> I have been running this script regularly for some time. This morning the
> following error message appeared.

> EWL<-get.hist.quote("EWL",start=(today <- Sys.Date())-350,quote="Cl")

> Error in if (dat[n] != start) cat(format(dat[n], "time series starts
> %Y-%m-%d\n")) : 
> 
> missing value where TRUE/FALSE needed
> 

Looks like this has happened before and spontaneously fixed itself then:

http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg77866.html

http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg77869.html

  "Now working, must have been a Yahoo! issue."

Barry


From anil_rohilla at rediffmail.com  Mon Jan 22 09:51:22 2007
From: anil_rohilla at rediffmail.com (anil kumar rohilla)
Date: 22 Jan 2007 08:51:22 -0000
Subject: [R] Combination of variables
Message-ID: <20070122085122.15995.qmail@webmail101.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070122/a7b2ad2c/attachment.pl 

From mmourroux at mvaconsultancy.com  Mon Jan 22 12:33:43 2007
From: mmourroux at mvaconsultancy.com (Matthieu Mourroux)
Date: Mon, 22 Jan 2007 11:33:43 +0000
Subject: [R] D'Agostino test
Message-ID: <s5b4a137.063@wokingmail.wokingprime.com>

Hello,

I'd like to know if the D'Agostino test of normality is reliable,
because somme of our results are not really coherent.
This test seems to be very sensitive. Even compared to a normal
distribution generated by R, the results are not very clear.

thanks for any help
Matthieu.

________________________________________________________________________
This e-mail has been scanned for all viruses by Star. The\ s...{{dropped}}


From tomas.goicoa at unavarra.es  Mon Jan 22 12:44:15 2007
From: tomas.goicoa at unavarra.es (Tomas Goicoa)
Date: Mon, 22 Jan 2007 12:44:15 +0100
Subject: [R] Fwd: Re:  aov y lme
Message-ID: <6.0.3.0.0.20070122124247.01adbee0@pop.unavarra.es>


Dear Prof. Ripley and Christoph,

thank you very much for your comments. You have helped me a lot.

Thanks,

Tomas Goicoa










>Dear Prof. Ripley
>
>Thank you for your email. Yes, this is of course the correct
>syntax to save us the extra calculation. And I forgot the
>"lower.tail = FALSE" for pf() in my example to obtain the
>p-value.
>
>Thank you for the corrections and
>
>Best regards,
>
>Christoph Buser
>
>
>Prof Brian Ripley writes:
>  > On Mon, 22 Jan 2007, Christoph Buser wrote:
>  >
>  > > Dear Tomas
>  > >
>  > > You can produce the results in Montgomery Montgomery with
>  > > lme. Please remark that you should indicate the nesting with the
>  > > levels in your nested factor. Therefore I recreated your data,
>  > > but used 1,...,12 for the levels of batch instead of 1,...,4.
>  > >
>  > > purity<-c(1,-2,-2,1,-1,-3,0,4, 0,-4, 1,0, 1,0,-1,0,-2,4,0,3,
>  > >          -3,2,-2,2,2,-2,1,3,4,0,-1,2,0,2,2,1)
>  > > suppli<-factor(c(rep(1,12),rep(2,12),rep(3,12)))
>  > > batch<-factor(c(rep(1:4,3), rep(5:8,3), rep(9:12,3)))
>  > > material<-data.frame(purity,suppli,batch)
>  > >
>  > > As you remarked you can use aov
>  > >
>  > > summary(material.aov<-aov(purity~suppli+suppli:batch,data=material))
>  > >             Df Sum Sq Mean Sq F value  Pr(>F)
>  > > suppli        2 15.056   7.528  2.8526 0.07736 .
>  > > suppli:batch  9 69.917   7.769  2.9439 0.01667 *
>  > > Residuals    24 63.333   2.639
>  > > ---
>  > > Signif. codes:  0 $,1rx(B***$,1ry(B 0.001 $,1rx(B**$,1ry(B 0.01 
> $,1rx(B*$,1ry(B 0.05 $,1rx(B.$,1ry(B 0.1 $,1rx(B $,1ry(B 1
>  > >
>  > > Remark that the test of "suppli" is not the same as in
>  > > Montgomery. Here it is wrongly tested against the Residuals and
>  >
>  > I don't think so: aov() is doing the correct thing for the model
>  > specified. The aov() model you want is probably
>  >
>  > aov(purity ~ suppli + Error(suppli:batch), data=material)
>  >
>  > and this gives
>  >
>  > > summary(.Last.value)
>  >
>  > Error: suppli:batch
>  >            Df Sum Sq Mean Sq F value Pr(>F)
>  > suppli     2 15.056   7.528   0.969 0.4158
>  > Residuals  9 69.917   7.769
>  >
>  > Error: Within
>  >            Df Sum Sq Mean Sq F value Pr(>F)
>  > Residuals 24 63.333   2.639
>  >
>  >
>  > > you should perform the calculate the test with:
>  >
>  > > (Fsuppi <- summary(material.aov)[[1]][1,"Mean Sq"]/
>  > >  summary(material.aov)[[1]][2,"Mean Sq"])
>  > > pf(Fsuppi, df1 = 2, df2 = 9)
>  >
>  > You want the other tail.
>  >
>  > --
>  > Brian D. Ripley,                  ripley at stats.ox.ac.uk
>  > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>  > University of Oxford,             Tel:  +44 1865 272861 (self)
>  > 1 South Parks Road,                     +44 1865 272866 (PA)
>  > Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From qjing at sibs.ac.cn  Mon Jan 22 13:01:42 2007
From: qjing at sibs.ac.cn (qing)
Date: Mon, 22 Jan 2007 12:01:42 +0000 (UTC)
Subject: [R] value of Delta
Message-ID: <loom.20070119T092542-850@post.gmane.org>








Dear all,

I am running R 2.4.1.

> library(siggenes);
> library(multtest);
> cl<-rep(c(0,1),c(3,3));
> sub<-exprs(AffyExpData[,c(1:3,7:9)]);
> gn<-geneNames(AffyRAwData);
> sam.out<-sam(sub,cl,rand=123,gene.names=gn);

We're doing 20 complete permutations

> sam.out
SAM Analysis for the Two-Class Unpaired Case Assuming Unequal Variances 
 
   Delta    p0  False Called   FDR
1    0.1 0.929 292.25    293 0.927
2    0.4 0.929  43.60     56 0.724
3    0.7 0.929  12.25     20 0.569
4    1.0 0.929   7.25     14 0.481
5    1.3 0.929   2.60      7 0.345
6    1.7 0.929   1.30      5 0.242
7    2.0 0.929   1.30      5 0.242
8    2.3 0.929   0.45      2 0.209
9    2.6 0.929   0.45      2 0.209
10   2.9 0.929   0.45      2 0.209

> sum.sam.out<-summary(sam.out,1,ll=FALSE);
> sum.sam.out$row.sig.genes;
NULL
> sum.sam.out<-summary(sam.out,0.1,ll=FALSE);
> sum.sam.out$row.sig.genes;
NULL
> sum.sam.out<-summary(sam.out,0.00000000001,ll=FALSE);
> sum.sam.out$row.sig.genes;
NULL
> sum.sam.out<-summary(sam.out,1000,ll=FALSE);
> sum.sam.out$row.sig.genes;
NULL
   

Any value of Delta I chosen: 1000,1,0,1 0.00000000001, the outcome is NULL.


Any help or suggestions that you can provide will be greatly appreciated.


Qing Phd


From qjing at sibs.ac.cn  Mon Jan 22 13:04:40 2007
From: qjing at sibs.ac.cn (qing)
Date: Mon, 22 Jan 2007 12:04:40 +0000 (UTC)
Subject: [R] RMySQL connection
Message-ID: <loom.20070119T092542-850@post.gmane.org>








Dear all,
 
I am running R 2.4.1    RMySQL_0.5-7  DBI_0.1-12  Windows XP


> library(RMySQL);
> con<-dbConnect(dbDriver("MySQL"),dbname="test");
Error in function (classes, fdef, mtable)  : 
        unable to find an inherited method for function "dbConnect", for 
signature "MySQLDriver"

Any help or suggestions that you can provide will be greatly appreciated.

Qing phd


From mothsailor at googlemail.com  Mon Jan 22 14:22:57 2007
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 22 Jan 2007 13:22:57 +0000
Subject: [R] Combination of variables
In-Reply-To: <20070122085122.15995.qmail@webmail101.rediffmail.com>
References: <20070122085122.15995.qmail@webmail101.rediffmail.com>
Message-ID: <815b70590701220522g4fe6140icec1d1f9317c5e40@mail.gmail.com>

You don't say what model you want to do.  It isn't necessary to store
each combination of predictors in a separate matrix unless you really
need to do this for some other reason, in which case I imagine you
could adopt this idea. I dare say there are better ways, but this
should work (assuming you want a linear model):

x<-matrix(runif(30*6),ncol=6)
y <- rnorm(30)
cmbs <- list()
for (i in 1:6) cmbs <- c(cmbs,combn(6,i,simplify=FALSE))
for (i in 1:length(cmbs))  print(summary(lm(y ~ x[,cmbs[[i]]])))


On 22 Jan 2007 08:51:22 -0000, anil kumar rohilla
<anil_rohilla at rediffmail.com> wrote:
>
> Hi,
>  List , i have 6 predictor variables and i want to make possible combinations of these 6 predictors ,all the data is in matrix form ,
> if i am having 6 predictors than possible combination of sets are 64 2 power 6, or 63 ,whatever it may be i want to store the result in another variable to each combination and that i want to put in some model ,
>
> i want to put every combination in some model ,please help me how to find suitable combinations of these variables ........
>
> i was not able to do this .
> Any package is there in R which can help me in this regards
> any help .
> thanks in Advance
>
>
> ANIL KUMAR( METEOROLOGIST)
> LRF SECTION
> NATIONAL CLIMATE CENTER
> ADGM(RESEARCH)
> INDIA METEOROLOGICAL DEPARTMENT
> SHIVIJI NAGAR
> PUNE-411005 INDIA
> MOBILE +919422023277
> anilkumar at imdpune.gov.in
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From h.wickham at gmail.com  Mon Jan 22 15:05:12 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 22 Jan 2007 08:05:12 -0600
Subject: [R] Combination of variables
In-Reply-To: <20070122085122.15995.qmail@webmail101.rediffmail.com>
References: <20070122085122.15995.qmail@webmail101.rediffmail.com>
Message-ID: <f8e6ff050701220605s48e62d2dhb65428d11d291c88@mail.gmail.com>

You might find the following code useful.  It's part of a package I'm
developing for interactive model exploration.

Hadley

# Generate all models
# Fit all combinations of x variables ($2^p$)
#
# This technique generalises \code{\link{fitbest}}.  While it is much
# slower it will work for any type of model.
#
# @arguments vector y values
# @arguments matrix of x values
# @arguments method used to fit the model, eg
\code{\link{lm}},\code{\link[MASS]{rlm}}
# @keyword regression
fitall <- function(y, x, method=lm, ...) {
	data <- cbind(y=y, x)

	combs <- do.call(expand.grid, rep(list(c(FALSE, TRUE)), ncol(x)))[-1, ]

	vars <- apply(combs, 1, function(i) names(x)[i])
	form <- paste("y ~ ", lapply(vars, paste, collapse=" + "), sep = "")
	form <- lapply(form, as.formula)

	models <- lapply(form, function(f) eval(substitute(method(f,
data=data, ...), list(f=f, data=data, method=method))))
	names(models) <- 1:length(models)
	class(models) <- c("ensemble", class(models))
	models
}

# Generate best linear models
# Use the leaps package to generate the best subsets.
#
# @arguments model formula
# @arguments data frame
# @arguments number of subsets of each size to record
# @arguments other arguments passed to \code{\link[leaps]{regsubsets}}
# @keyword regression
fitbest <- function(formula, data, nbest=10, ...) {
	b <- regsubsets(formula, data=data, nbest=nbest, ...)
	mat <- summary(b, matrix.logical = TRUE)$which

	intercept <- c("", "-1")[as.numeric(mat[,1])]
	vars <- apply(mat[,-1], 1, function(x) colnames(mat[, -1])[x])
	form <- paste(formula[[2]], " ~ ", lapply(vars, paste, collapse=" +
"), sep = "")
	form <- lapply(form, as.formula)

	models <- lapply(form, function(f) eval(substitute(lm(f, data=data),
list(f=f, data=data))))
	names(models) <- 1:length(models)
	class(models) <- c("ensemble", class(models))
	models
}

On 22 Jan 2007 08:51:22 -0000, anil kumar rohilla
<anil_rohilla at rediffmail.com> wrote:
>
> Hi,
>  List , i have 6 predictor variables and i want to make possible combinations of these 6 predictors ,all the data is in matrix form ,
> if i am having 6 predictors than possible combination of sets are 64 2 power 6, or 63 ,whatever it may be i want to store the result in another variable to each combination and that i want to put in some model ,
>
> i want to put every combination in some model ,please help me how to find suitable combinations of these variables ........
>
> i was not able to do this .
> Any package is there in R which can help me in this regards
> any help .
> thanks in Advance
>
>
> ANIL KUMAR( METEOROLOGIST)
> LRF SECTION
> NATIONAL CLIMATE CENTER
> ADGM(RESEARCH)
> INDIA METEOROLOGICAL DEPARTMENT
> SHIVIJI NAGAR
> PUNE-411005 INDIA
> MOBILE +919422023277
> anilkumar at imdpune.gov.in
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From amsa36060 at yahoo.com  Mon Jan 22 15:16:36 2007
From: amsa36060 at yahoo.com (Amir Safari)
Date: Mon, 22 Jan 2007 06:16:36 -0800 (PST)
Subject: [R] Time-varying correlation calculation
Message-ID: <507797.34402.qm@web60418.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070122/4b50515c/attachment.pl 

From skiadas at hanover.edu  Mon Jan 22 15:59:33 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Mon, 22 Jan 2007 09:59:33 -0500
Subject: [R] efficient code. how to reduce running time?
In-Reply-To: <20070122011119.KOPX11361.tomts43-srv.bellnexxia.net@JohnDesktop8300>
References: <20070122011119.KOPX11361.tomts43-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <2DF68A04-1F26-4BA8-80B8-9DCD57167F6F@hanover.edu>

On Jan 21, 2007, at 8:11 PM, John Fox wrote:

> Dear Haris,
>
> Using lapply() et al. may produce cleaner code, but it won't  
> necessarily
> speed up a computation. For example:
>
>> X <- data.frame(matrix(rnorm(1000*1000), 1000, 1000))
>> y <- rnorm(1000)
>>
>> mods <- as.list(1:1000)
>> system.time(for (i in 1:1000) mods[[i]] <- lm(y ~ X[,i]))
> [1] 40.53  0.05 40.61    NA    NA
>>
>> system.time(mods <- lapply(as.list(X), function(x) lm(y ~ x)))
> [1] 53.29  0.37 53.94    NA    NA
>
Interesting, in my system the results are quite different:

 > system.time(for (i in 1:1000) mods[[i]] <- lm(y ~ X[,i]))
[1] 192.035  12.601 797.094   0.000   0.000
 > system.time(mods <- lapply(as.list(X), function(x) lm(y ~ x)))
[1]  59.913   9.918 289.030   0.000   0.000

Regular MacOSX install with ~760MB memory.

> In cases such as this, I don't even find the code using *apply()  
> easier to
> read.
>
> Regards,
>  John

Haris


From r.nieuwenhuis at student.ru.nl  Mon Jan 22 16:16:05 2007
From: r.nieuwenhuis at student.ru.nl (Rense Nieuwenhuis)
Date: Mon, 22 Jan 2007 16:16:05 +0100
Subject: [R] Compare effects between lm-models
Message-ID: <9D56C418-B952-4F3B-9FF0-E09E13E05E9E@student.ru.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070122/5b44d094/attachment.pl 

From david.meyer at wu-wien.ac.at  Mon Jan 22 16:26:09 2007
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Mon, 22 Jan 2007 16:26:09 +0100
Subject: [R]  naiveBayes question
Message-ID: <45B4D791.7080309@wu-wien.ac.at>

Aimin:

The problem is that the columns you choose for training (only 4 
variables) do not match the ones used for prediction (all except y).

David

----

I try to use naiveBayes

  > p.nb.90<-naiveBayes(y~aa_three+bas+bcu+aa_ss,data=training)
  > 
pr.nb.90<-table(predict(p.nb.90,training[,-13],type="class"),training[,13])

bur I get this error
Error in object$tables[[v]] : subscript out of bounds
  >
head is data set
  > head(training)
      pr aa_three aa_one aa_ss aa_pos    aas bas   ams bms        acu
bcu     omega       y index
1 1acx      ALA      A     C      1 127.71   0 69.99   0
-0.2498560   0  79.91470 outward  TRUE
2 1acx      PRO      P     C      2  68.55   0 55.44   0
-0.0949008   0  76.60380 outward  TRUE
3 1acx      ALA      A     E      3  52.72   0 47.82   0
-0.0396550   0  52.19970 outward  TRUE
4 1acx      PHE      F     E      4  22.62   0 31.21   0  0.1270330   0
169.52500  inward  TRUE
5 1acx      SER      S     E      5  71.32   0 52.84   0
-0.1312380   0   7.47528 outward  TRUE
6 1acx      VAL      V     E      6  12.92   0 22.40   0  0.1728390   0
149.09400  inward  TRUE

anyone know why?

Aimin


From Max.Kuhn at pfizer.com  Mon Jan 22 16:26:54 2007
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Mon, 22 Jan 2007 10:26:54 -0500
Subject: [R] Compare effects between lm-models
In-Reply-To: <9D56C418-B952-4F3B-9FF0-E09E13E05E9E@student.ru.nl>
Message-ID: <71257D09F114DA4A8E134DEAC70F25D30746CC42@groamrexm03.amer.pfizer.com>

You can use the anova function a la:

   > anova(model1, model2)
   Analysis of Variance Table
   
   Model 1: y ~ x
   Model 2: y ~ x + z
     Res.Df    RSS Df Sum of Sq      F Pr(>F)
   1     13 4.4947                           
   2     12 4.4228  1    0.0720 0.1952 0.6665

I would suggest getting a copy of MASS and/or reading 

   http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf


Max

   

----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From therneau at mayo.edu  Mon Jan 22 16:26:47 2007
From: therneau at mayo.edu (Terry Therneau)
Date: Mon, 22 Jan 2007 09:26:47 -0600 (CST)
Subject: [R] predict.survreg() with frailty term and newdata
Message-ID: <200701221526.l0MFQmi07338@hsrnfs-101.mayo.edu>

  It can't be done with the current code.
  
  In a nutshell, you are trying to use a feature that I never got around to
coding.  It's been on my "to do" list, but may never make it to the top.

	Terry


From jfox at mcmaster.ca  Mon Jan 22 16:39:09 2007
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 22 Jan 2007 10:39:09 -0500
Subject: [R] efficient code. how to reduce running time?
In-Reply-To: <2DF68A04-1F26-4BA8-80B8-9DCD57167F6F@hanover.edu>
Message-ID: <20070122153908.HVMB8030.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Haris,

My timings were on a 3 GHz Pentium 4 system with 1 GB of memory running Win
XP SP2 and R 2.4.1.

I'm no expert on these matters, and I wouldn't have been surprised by
qualitatively different results on different systems, but this difference is
larger than I would have expected. One thing that seems particularly
striking in your results is the large difference between elapsed time and
user CPU time, making me wonder what else was going on when you ran these
examples.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Charilaos Skiadas [mailto:skiadas at hanover.edu] 
> Sent: Monday, January 22, 2007 10:00 AM
> To: John Fox
> Cc: r-help at stat.math.ethz.ch; 'miraceti'
> Subject: Re: [R] efficient code. how to reduce running time?
> 
> On Jan 21, 2007, at 8:11 PM, John Fox wrote:
> 
> > Dear Haris,
> >
> > Using lapply() et al. may produce cleaner code, but it won't 
> > necessarily speed up a computation. For example:
> >
> >> X <- data.frame(matrix(rnorm(1000*1000), 1000, 1000)) y <- 
> >> rnorm(1000)
> >>
> >> mods <- as.list(1:1000)
> >> system.time(for (i in 1:1000) mods[[i]] <- lm(y ~ X[,i]))
> > [1] 40.53  0.05 40.61    NA    NA
> >>
> >> system.time(mods <- lapply(as.list(X), function(x) lm(y ~ x)))
> > [1] 53.29  0.37 53.94    NA    NA
> >
> Interesting, in my system the results are quite different:
> 
>  > system.time(for (i in 1:1000) mods[[i]] <- lm(y ~ X[,i]))
> [1] 192.035  12.601 797.094   0.000   0.000
>  > system.time(mods <- lapply(as.list(X), function(x) lm(y ~ x)))
> [1]  59.913   9.918 289.030   0.000   0.000
> 
> Regular MacOSX install with ~760MB memory.
> 
> > In cases such as this, I don't even find the code using *apply() 
> > easier to read.
> >
> > Regards,
> >  John
> 
> Haris
> 
>


From tlumley at u.washington.edu  Mon Jan 22 16:48:14 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 22 Jan 2007 07:48:14 -0800 (PST)
Subject: [R] How to get correct integration in C for step function?
In-Reply-To: <BAY131-DAV35D1B17CAFB03DDD716ACB3AF0@phx.gbl>
References: <45B02D46.1090709@vanderbilt.edu><200701191118.26761.rdiaz@cnio.es><17840.55304.803214.860567@basebud.nulle.part><200701201520.42653.blindglobe@gmail.com><17842.13129.56162.112711@basebud.nulle.part>
	<624934630701211505k6dd5e53fl8bf134bd08fce01c@mail.gmail.com>
	<BAY131-DAV35D1B17CAFB03DDD716ACB3AF0@phx.gbl>
Message-ID: <Pine.LNX.4.64.0701220745120.17344@homer22.u.washington.edu>

On Sun, 21 Jan 2007, Lynette wrote:

> Dear all,
>
> I am using Rdqags in C to realize the integration. It seems for the
> continous C function I can get correct results. However, for step functions,
> the results are not correct. For example, the following one, when integrated
> from 0 to 1 gives 1 instead of the correct 1.5
>

Using integrate() in R for an R-defined step function gives the right 
answer (eg on the example in ?ecdf).

This suggests a problem in your C code, since integrate() just calls 
dqags.

 	-thomas


From prasannaprakash at gmail.com  Mon Jan 22 16:59:58 2007
From: prasannaprakash at gmail.com (Prasanna)
Date: Mon, 22 Jan 2007 16:59:58 +0100
Subject: [R] Latin hyper cube sampling from expand.grid()
Message-ID: <fc5b8ae70701220759o55f2a2few5850b9b66a99a922@mail.gmail.com>

Dear R experts

I am looking for a package which gives me latin hyper cube samples
from the grid of values produced from the command "expand.grid". Any
pointers to this issue might be very useful. Basically, I am doing the
following:

> a<-(1:10)
> b<-(20:30)
> dataGrid<-expand.grid(a,b)

Now, is there a way to use this "dataGrid" in the package "lhs" to get
latin hyper cube samples.

Thanking you
Prasanna


From kubovy at virginia.edu  Mon Jan 22 17:01:11 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Mon, 22 Jan 2007 11:01:11 -0500
Subject: [R] Finding the effect of Box-Cox transformation using
	"vis.boxcoxu"
In-Reply-To: <d4c57560701212325v7765b1b4g259618a7eed7fabe@mail.gmail.com>
References: <d4c57560701212325v7765b1b4g259618a7eed7fabe@mail.gmail.com>
Message-ID: <687BE236-216C-469C-8BE2-AB1E01828C75@virginia.edu>

?box.cox

?boxcox

On Jan 22, 2007, at 2:25 AM, Arun Kumar Saha wrote:

> I have a dataset 'data' and I want to see the effect of Box-Cox
> transformation on it Interactively for different lambda values. I  
> already
> got a look on function "vis.boxcoxu" in package "TeachingDemos". But I
> didn't find any option to put user's own dataset. Can anyone tell  
> me how to
> put my own dataset here i.e. "data"?

_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From ripley at stats.ox.ac.uk  Mon Jan 22 17:06:12 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 22 Jan 2007 16:06:12 +0000 (GMT)
Subject: [R] efficient code. how to reduce running time?
In-Reply-To: <2DF68A04-1F26-4BA8-80B8-9DCD57167F6F@hanover.edu>
References: <20070122011119.KOPX11361.tomts43-srv.bellnexxia.net@JohnDesktop8300>
	<2DF68A04-1F26-4BA8-80B8-9DCD57167F6F@hanover.edu>
Message-ID: <Pine.LNX.4.64.0701221559130.26502@gannet.stats.ox.ac.uk>

On Mon, 22 Jan 2007, Charilaos Skiadas wrote:

> On Jan 21, 2007, at 8:11 PM, John Fox wrote:
>
>> Dear Haris,
>>
>> Using lapply() et al. may produce cleaner code, but it won't
>> necessarily
>> speed up a computation. For example:
>>
>>> X <- data.frame(matrix(rnorm(1000*1000), 1000, 1000))
>>> y <- rnorm(1000)
>>>
>>> mods <- as.list(1:1000)
>>> system.time(for (i in 1:1000) mods[[i]] <- lm(y ~ X[,i]))
>> [1] 40.53  0.05 40.61    NA    NA
>>>
>>> system.time(mods <- lapply(as.list(X), function(x) lm(y ~ x)))
>> [1] 53.29  0.37 53.94    NA    NA
>>
> Interesting, in my system the results are quite different:
>
> > system.time(for (i in 1:1000) mods[[i]] <- lm(y ~ X[,i]))
> [1] 192.035  12.601 797.094   0.000   0.000
> > system.time(mods <- lapply(as.list(X), function(x) lm(y ~ x)))
> [1]  59.913   9.918 289.030   0.000   0.000
>
> Regular MacOSX install with ~760MB memory.

But MacOS X is infamous for having rather specific speed problems with its 
malloc, and so gives different timing results from all other platforms.
We are promised a solution in MacOS 10.5.

Both of your machines seem very slow compared to mine:

> system.time(for (i in 1:1000) mods[[i]] <- lm(y ~ X[,i]))
    user  system elapsed
  11.011   0.250  11.311
> system.time(mods <- lapply(as.list(X), function(x) lm(y ~ x)))
    user  system elapsed
  13.463   0.260  13.812

and that on a 64-bit platform (AMD64 Linux FC5).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From r.nieuwenhuis at student.ru.nl  Mon Jan 22 17:36:02 2007
From: r.nieuwenhuis at student.ru.nl (Rense Nieuwenhuis)
Date: Mon, 22 Jan 2007 17:36:02 +0100
Subject: [R] Compare effects between lm-models
In-Reply-To: <71257D09F114DA4A8E134DEAC70F25D30746CC42@groamrexm03.amer.pfizer.com>
References: <71257D09F114DA4A8E134DEAC70F25D30746CC42@groamrexm03.amer.pfizer.com>
Message-ID: <16DEFFDF-7845-4024-82B6-BC98C28F7FD5@student.ru.nl>

Thank you Alain and Max for your swift responses.

  It might be that I'm misunderstanding your responses, but aren't  
you testing if there is a difference between the two full models?
What I want to know, os whether the effect of a specific predictor  
(x) differs between model1 and model2. I'm not interested (presently)  
if the fit of model 2 is better than that of model 1 (for instance).

thanks again,

Rense




On Jan 22, 2007, at 16:26 , Kuhn, Max wrote:

> You can use the anova function a la:
>
>> anova(model1, model2)
>    Analysis of Variance Table
>
>    Model 1: y ~ x
>    Model 2: y ~ x + z
>      Res.Df    RSS Df Sum of Sq      F Pr(>F)
>    1     13 4.4947
>    2     12 4.4228  1    0.0720 0.1952 0.6665
>
> I would suggest getting a copy of MASS and/or reading
>
>    http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf
>
>
> Max
>
>
>
> ----------------------------------------------------------------------
> LEGAL NOTICE
> Unless expressly stated otherwise, this message is confidential and  
> may be privileged.  It is intended for the addressee(s) only.   
> Access to this E-mail by anyone else is unauthorized.  If you are  
> not an addressee, any disclosure or copying of the contents of this  
> E-mail or any action taken (or not taken) in reliance on it is  
> unauthorized and may be unlawful.  If you are not an addressee,  
> please inform the sender immediately.
>


From jfox at mcmaster.ca  Mon Jan 22 17:36:00 2007
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 22 Jan 2007 11:36:00 -0500
Subject: [R] efficient code. how to reduce running time?
In-Reply-To: <Pine.LNX.4.64.0701221559130.26502@gannet.stats.ox.ac.uk>
Message-ID: <20070122163559.HWRQ24907.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Brian,


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof 
> Brian Ripley
> Sent: Monday, January 22, 2007 11:06 AM
> To: Charilaos Skiadas
> Cc: John Fox; r-help at stat.math.ethz.ch
> Subject: Re: [R] efficient code. how to reduce running time?
> 
> On Mon, 22 Jan 2007, Charilaos Skiadas wrote:
> 
> > On Jan 21, 2007, at 8:11 PM, John Fox wrote:
> >
> >> Dear Haris,
> >>
> >> Using lapply() et al. may produce cleaner code, but it won't 
> >> necessarily speed up a computation. For example:
> >>
> >>> X <- data.frame(matrix(rnorm(1000*1000), 1000, 1000)) y <- 
> >>> rnorm(1000)
> >>>
> >>> mods <- as.list(1:1000)
> >>> system.time(for (i in 1:1000) mods[[i]] <- lm(y ~ X[,i]))
> >> [1] 40.53  0.05 40.61    NA    NA
> >>>
> >>> system.time(mods <- lapply(as.list(X), function(x) lm(y ~ x)))
> >> [1] 53.29  0.37 53.94    NA    NA
> >>
> > Interesting, in my system the results are quite different:
> >
> > > system.time(for (i in 1:1000) mods[[i]] <- lm(y ~ X[,i]))
> > [1] 192.035  12.601 797.094   0.000   0.000
> > > system.time(mods <- lapply(as.list(X), function(x) lm(y ~ x)))
> > [1]  59.913   9.918 289.030   0.000   0.000
> >
> > Regular MacOSX install with ~760MB memory.
> 
> But MacOS X is infamous for having rather specific speed 
> problems with its malloc, and so gives different timing 
> results from all other platforms.
> We are promised a solution in MacOS 10.5.
> 

Thanks for the clarification.

> Both of your machines seem very slow compared to mine:
> 
> > system.time(for (i in 1:1000) mods[[i]] <- lm(y ~ X[,i]))
>     user  system elapsed
>   11.011   0.250  11.311
> > system.time(mods <- lapply(as.list(X), function(x) lm(y ~ x)))
>     user  system elapsed
>   13.463   0.260  13.812
> 
> and that on a 64-bit platform (AMD64 Linux FC5).
> 

As you can see from the specs (in a previous message), my system is quite
old, which probably accounts for at least part of the difference. The ratios
of the user times for my and your system aren't too different though:

> 53.29/40.53  # mine
[1] 1.314829

> 13.463/11.011  # yours
[1] 1.222686

Regards,
 John

> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From fuyaonv at hotmail.com  Mon Jan 22 17:53:26 2007
From: fuyaonv at hotmail.com (Lynette)
Date: Mon, 22 Jan 2007 11:53:26 -0500
Subject: [R] How to get correct integration in C for step function?
References: <45B02D46.1090709@vanderbilt.edu><200701191118.26761.rdiaz@cnio.es><17840.55304.803214.860567@basebud.nulle.part><200701201520.42653.blindglobe@gmail.com><17842.13129.56162.112711@basebud.nulle.part>
	<624934630701211505k6dd5e53fl8bf134bd08fce01c@mail.gmail.com>
	<BAY131-DAV35D1B17CAFB03DDD716ACB3AF0@phx.gbl>
	<Pine.LNX.4.64.0701220745120.17344@homer22.u.washington.edu>
Message-ID: <BAY131-DAV14C6ED392D2EF0D11D50B2B3AE0@phx.gbl>

Well,

I have no idea either. I can get correct answers for continous functions but 
incorrect for step functions.

Sign, I have been trying to realize the integration in C for long time.

Thank you for your answering.

Best,
Lynette

----- Original Message ----- 
From: "Thomas Lumley" <tlumley at u.washington.edu>
To: "Lynette" <fuyaonv at hotmail.com>
Cc: <r-help at stat.math.ethz.ch>; "AJ Rossini" <blindglobe at gmail.com>; 
<ess-help at stat.math.ethz.ch>
Sent: Monday, January 22, 2007 10:48 AM
Subject: Re: [R] How to get correct integration in C for step function?


> On Sun, 21 Jan 2007, Lynette wrote:
>
>> Dear all,
>>
>> I am using Rdqags in C to realize the integration. It seems for the
>> continous C function I can get correct results. However, for step 
>> functions,
>> the results are not correct. For example, the following one, when 
>> integrated
>> from 0 to 1 gives 1 instead of the correct 1.5
>>
>
> Using integrate() in R for an R-defined step function gives the right 
> answer (eg on the example in ?ecdf).
>
> This suggests a problem in your C code, since integrate() just calls 
> dqags.
>
>  -thomas
>


From schmidb at ibe.med.uni-muenchen.de  Mon Jan 22 18:00:29 2007
From: schmidb at ibe.med.uni-muenchen.de (Markus Schmidberger)
Date: Mon, 22 Jan 2007 18:00:29 +0100
Subject: [R] Problem with C extension
Message-ID: <45B4EDAD.8020400@ibe.med.uni-muenchen.de>

Hello,
thanks for help and code.
We did a lot of work to speedup our function in R. We have a nested 
loop, vectorizing is the fastest way. But there we got a very big matrix 
and problems with memory. So we want to stay by loops an speedup with C.

My code is similar to this. (my_c is code from Brian D. Ripley)

SEXP test(SEXP a, SEXP b, SEXP in)
{
    SEXP ans, new;
    int n=INTEGER(in)[0],i,j;
    PROTECT(ans = allocVector(REALSXP, 1));
    REAL(ans)[0]=REAL(a)[0];
/*    for(j = 0; i < m; j++)*/
    for(i = 0; i < n; i++)
    {
       /* b= ... ^i ....*j*/
        PROTECT(new = allocVector(REALSXP, i+2));
        new = my_c(ans,b);
        PROTECT(ans = allocVector(REALSXP, i+2));
        ans = new;
        UNPROTECT(2);
    }
    UNPROTECT(1);
    return ans;
}

We get an error by in=1300

 > .Call("test",1,3,as.integer(1300));
Fehler: type mismatch
 > .Call("test",1,3,as.integer(1300));
Speicherzugriffsfehler

Is there a possibility to free allocated memory? free(...) does not work.
Is there a possibility to reallocate a vector?

Thanks a lot
Markus


From skiadas at hanover.edu  Mon Jan 22 18:08:03 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Mon, 22 Jan 2007 12:08:03 -0500
Subject: [R] efficient code. how to reduce running time?
In-Reply-To: <20070122153908.HVMB8030.tomts22-srv.bellnexxia.net@JohnDesktop8300>
References: <20070122153908.HVMB8030.tomts22-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <1C1C0FAE-EEB9-44C5-AB26-D0C142F038F4@hanover.edu>

On Jan 22, 2007, at 10:39 AM, John Fox wrote:

>  One thing that seems particularly
> striking in your results is the large difference between elapsed  
> time and
> user CPU time, making me wonder what else was going on when you ran  
> these
> examples.

Yes, indeed there were a lot of other things going on, this is the  
only machine I have and I use it continuously. I'll try to run  
another test tonight when the machine is not in use.
It did seem a very striking difference though.

But am I wrong in thinking that these measurements should be  
independent of what other applications are running at the same time,  
and should measure exactly the time in terms of CPU cycles needed to  
finish this task, regardless of how often the process got to use the  
CPU? I guess I was working under that assumption, which indeed makes  
the above comparison a very unfair one, because there was a lot more  
going on during the first system.time call.

Still, the difference is quite large, which of course could simply  
have to do with the internals of the two commands, coupled with Prof.  
Ripley's comments about malloc in Mac OS X.

> Regards,
>  John

Haris


From Thomas.Brunel at ifremer.fr  Mon Jan 22 18:08:17 2007
From: Thomas.Brunel at ifremer.fr (Thomas BRUNEL)
Date: Mon, 22 Jan 2007 18:08:17 +0100
Subject: [R] lnme convergence
Message-ID: <45B4EF81.5050103@ifremer.fr>

Dear R-user,

I am trying to use the R "nlme" function to fit a non linear mixed
effects model. The method has some problem to reach the convergence.

I am trying to understand causes of the problem by following step by 
step evolution of the iterative algorithm (verbose=TRUE command).

However, I don't understand the meaning of some information given while 
the algorithm is running, especialy the last part referring to convergence :

Convergence:
      fixed   reStruct
0.01168598 0.82634050

Futhermore, is the tolerance criteria (nlmeControl) related to any of 
the these two values?

Thank you for any help

Thomas


From ripley at stats.ox.ac.uk  Mon Jan 22 18:06:12 2007
From: ripley at stats.ox.ac.uk (Brian Ripley)
Date: Mon, 22 Jan 2007 17:06:12 +0000 (GMT)
Subject: [R] Problem with C extension
In-Reply-To: <45B4EDAD.8020400@ibe.med.uni-muenchen.de>
References: <45B4EDAD.8020400@ibe.med.uni-muenchen.de>
Message-ID: <Pine.LNX.4.64.0701221704220.2776@auk.stats>

On Mon, 22 Jan 2007, Markus Schmidberger wrote:

> Hello,
> thanks for help and code.
> We did a lot of work to speedup our function in R. We have a nested loop, 
> vectorizing is the fastest way. But there we got a very big matrix and 
> problems with memory. So we want to stay by loops an speedup with C.
>
> My code is similar to this. (my_c is code from Brian D. Ripley)

And not in many ways similar to mine, hence your error message.
You really do have to handle all the types, as I did most.

>
> SEXP test(SEXP a, SEXP b, SEXP in)
> {
>   SEXP ans, new;
>   int n=INTEGER(in)[0],i,j;
>   PROTECT(ans = allocVector(REALSXP, 1));
>   REAL(ans)[0]=REAL(a)[0];
> /*    for(j = 0; i < m; j++)*/
>   for(i = 0; i < n; i++)
>   {
>      /* b= ... ^i ....*j*/
>       PROTECT(new = allocVector(REALSXP, i+2));
>       new = my_c(ans,b);
>       PROTECT(ans = allocVector(REALSXP, i+2));
>       ans = new;
>       UNPROTECT(2);
>   }
>   UNPROTECT(1);
>   return ans;
> }
>
> We get an error by in=1300
>
>> .Call("test",1,3,as.integer(1300));
> Fehler: type mismatch
>> .Call("test",1,3,as.integer(1300));
> Speicherzugriffsfehler
>
> Is there a possibility to free allocated memory? free(...) does not work.

No, but garbage collection does.

> Is there a possibility to reallocate a vector?

Yes, sort of.  See lengthgets().

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From tlumley at u.washington.edu  Mon Jan 22 18:42:01 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 22 Jan 2007 09:42:01 -0800 (PST)
Subject: [R] [ESS] How to get correct integration in C for step function?
In-Reply-To: <BAY131-DAV14C6ED392D2EF0D11D50B2B3AE0@phx.gbl>
References: <45B02D46.1090709@vanderbilt.edu><200701191118.26761.rdiaz@cnio.es><17840.55304.803214.860567@basebud.nulle.part><200701201520.42653.blindglobe@gmail.com><17842.13129.56162.112711@basebud.nulle.part>
	<624934630701211505k6dd5e53fl8bf134bd08fce01c@mail.gmail.com>
	<BAY131-DAV35D1B17CAFB03DDD716ACB3AF0@phx.gbl>
	<Pine.LNX.4.64.0701220745120.17344@homer22.u.washington.edu>
	<BAY131-DAV14C6ED392D2EF0D11D50B2B3AE0@phx.gbl>
Message-ID: <Pine.LNX.4.64.0701220938570.13495@homer22.u.washington.edu>

On Mon, 22 Jan 2007, Lynette wrote:

> Well,
>
> I have no idea either. I can get correct answers for continous functions but
> incorrect for step functions.
>

I have just tried using Rdqags from C for the function x>0 and it worked 
fine (once I had declared all the arguments correctly). The code is below.

 	-thomas





#include "Rinternals.h"
#include "R_ext/Applic.h"

double stepfn(double x){
    return (x>0);
}


SEXP call_stepfn(SEXP x){
SEXP answer;
int i,n;

n=length(x);
PROTECT(answer=allocVector(REALSXP,n));
for(i=0;i<n;i++){
    REAL(answer)[i]=stepfn(REAL(x)[i]);
}
UNPROTECT(1);
return answer;
}


void vec_stepfn(double *x, int n, void *ex){
int i;
for (i=0;i<n;i++) x[i]=stepfn(x[i]);
return;
}

void Cvec_stepfn(double *x, double *n){

  vec_stepfn(x, *n, (void *) NULL);
  return;
}

SEXP int_stepfn(SEXP lower, SEXP upper){

SEXP answer; 
double result, abserr;
int last, neval, ier;
int lenw;
int *iwork;
double *work;
int limit=100;
double reltol=0.00001;
double abstol=0.00001;

          lenw = 4 * limit;
          iwork =   (int *) R_alloc(limit, sizeof(int));
          work = (double *) R_alloc(lenw,  sizeof(double));

          Rdqags(vec_stepfn, (void *) NULL, REAL(lower), REAL(upper),
                        &abstol,  &reltol,
                       &result,  &abserr,  &neval,  &ier,
                        &limit,  &lenw, &last,
                        iwork, work);

printf("%f %f %d\n", result,abserr, ier);

    PROTECT(answer=allocVector(REALSXP,1));
    REAL(answer)[0]=result;
    UNPROTECT(1);

    return answer;
}


From richard.c.yeh at bankofamerica.com  Mon Jan 22 20:01:53 2007
From: richard.c.yeh at bankofamerica.com (Yeh, Richard C)
Date: Mon, 22 Jan 2007 14:01:53 -0500
Subject: [R] Example function for bigglm (biglm) data input from file
Message-ID: <31AFBE76660ED2459C6F75E6EDFD203C068CBE05@ex2k.bankofamerica.com>

This is to submit a commented example function for use in the data
argument to the bigglm(biglm) function, when you want to read the data
from a file (instead of a URL), or rescale or modify the data before
fitting the model.  In the hope that this may be of help to someone out
there.


make.data <- function (filename, chunksize, ...) {
  conn<-NULL;
  function (reset=FALSE) { 
    if (reset) {
      if (!is.null(conn)) {
        close(conn);
      };
      # This is for a file.
      # For other methods, see: help("connections")
      # and replace the following definition of conn
      # (and possibly the read.table call).
      conn <<- file (description=filename, open="r");
    } else {
      # It's best that the file you use has no header 
      # line, because when you use the connection to 
      # read each excerpt, any header won't get re-read.
      # If you choose to skip the first line, then the 
      # first line of each excerpt will be skipped.
      rval <- read.table (conn, nrows=chunksize, 
        skip=0, header=FALSE,...);
      if (nrow(rval)==0) {
        # Then we have reached the end of the input.
        # Clean up:
        close(conn);
        conn<<-NULL;
        rval<-NULL;
      } else {
        # We did not reach the end of the input,
        # so this function will return data.
        # Here, you can define any derived fields
        # or put instructions to rescale input data
        # that you want done after the data are read
        # but before they are used for fitting.
        # For example:
        rval$rescaled_column <- rval$original_column / 1000000.0;
        # If you don't want to do anything like this,
        # then delete this "else" clause, and make
        # the end of the function resemble the URL 
        # example in bigglm.
      };
    return(rval);
    }
  }
};

a <- make.data ( filename = "myfile", chunksize = 1000000, 
  # In our definition of make.data, any remaining 
  # arguments get passed to the read.table function by 
  # the ... argument.
  # Define column types:
  colClasses = list ("character", "character", 
    "integer", "numeric", "numeric"),
  # Define the column names in the call:
  # (recall that we cannot rely on the file header)
  col.names = c("fromState", "toState",
    "first", "original_column", "second")
);

library(biglm);

bigglm (formula = toState ~ 1 + first + rescaled_column,
  data = a, family = binomial(link='logit'), 
  weights = ~second);

summary(.Last.value)



NOTICE TO RECIPIENTS: Any information contained in or attach...{{dropped}}


From fuyaonv at hotmail.com  Mon Jan 22 20:03:20 2007
From: fuyaonv at hotmail.com (Lynette)
Date: Mon, 22 Jan 2007 14:03:20 -0500
Subject: [R] How to get correct integration in C for step function?
References: <45B02D46.1090709@vanderbilt.edu><200701191118.26761.rdiaz@cnio.es><17840.55304.803214.860567@basebud.nulle.part><200701201520.42653.blindglobe@gmail.com><17842.13129.56162.112711@basebud.nulle.part><624934630701211505k6dd5e53fl8bf134bd08fce01c@mail.gmail.com>
	<BAY131-DAV35D1B17CAFB03DDD716ACB3AF0@phx.gbl>
Message-ID: <BAY131-DAV18E0D72D4F938B82B16F36B3AE0@phx.gbl>

Dear all, especially to Thomas,

I have figured out the problem. For the step function, something wrong with 
my C codes. I should use the expression ((x>=0.25)&&(x<=0.75)) ? 2:1 instead 
of ((x>=1/4)&&(x<=3/4)) ? 2:1 ). Have no idea why 0.25 makes difference from 
1/4 in C. But now I can go ahead with the correct integration in C. Thank 
you all. And hope this helps to others.

Best wishes,
Lynette

----- Original Message ----- 
From: "Lynette" <fuyaonv at hotmail.com>
To: <r-help at stat.math.ethz.ch>
Cc: "AJ Rossini" <blindglobe at gmail.com>; <ess-help at stat.math.ethz.ch>
Sent: Sunday, January 21, 2007 6:24 PM
Subject: [R] How to get correct integration in C for step function?


> Dear all,
>
> I am using Rdqags in C to realize the integration. It seems for the
> continous C function I can get correct results. However, for step 
> functions,
> the results are not correct. For example, the following one, when 
> integrated
> from 0 to 1 gives 1 instead of the correct 1.5
>
> void func( double *x, int n, void *ex )
> {
> int i;
>
> for(i=0;i<n;i++) { x[i]=( ((x>=1/4)&&(x<=3/4)) ? 2:1 ) ; }
>        return;
> }
>
> while the following one when integrated from 0 to 1 gives the correct
> 0.7853983
>
> void func( double *x, int n, void *ex )
> {
> int i;
>
> for(i=0;i<n;i++) { x[i]= pow(1-x[i]*x[i],.5); }
>        return;
> }
>
> Please advise the problems. Thanks a lot.
>
> Best,
> Lynette
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tlumley at u.washington.edu  Mon Jan 22 20:37:01 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 22 Jan 2007 11:37:01 -0800 (PST)
Subject: [R] How to get correct integration in C for step function?
In-Reply-To: <BAY131-DAV18E0D72D4F938B82B16F36B3AE0@phx.gbl>
References: <45B02D46.1090709@vanderbilt.edu><200701191118.26761.rdiaz@cnio.es><17840.55304.803214.860567@basebud.nulle.part><200701201520.42653.blindglobe@gmail.com><17842.13129.56162.112711@basebud.nulle.part><624934630701211505k6dd5e53fl8bf134bd08fce01c@mail.gmail.com>
	<BAY131-DAV35D1B17CAFB03DDD716ACB3AF0@phx.gbl>
	<BAY131-DAV18E0D72D4F938B82B16F36B3AE0@phx.gbl>
Message-ID: <Pine.LNX.4.64.0701221136120.13495@homer22.u.washington.edu>

On Mon, 22 Jan 2007, Lynette wrote:

> Dear all, especially to Thomas,
>
> I have figured out the problem. For the step function, something wrong with 
> my C codes. I should use the expression ((x>=0.25)&&(x<=0.75)) ? 2:1 instead 
> of ((x>=1/4)&&(x<=3/4)) ? 2:1 ). Have no idea why 0.25 makes difference from 
> 1/4 in C. But now I can go ahead with the correct integration in C. Thank you 
> all. And hope this helps to others.
>

1 and 4 are ints in C, so 1/4 is 0 (integer division).

 	-thomas


From erich.neuwirth at univie.ac.at  Mon Jan 22 20:38:07 2007
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Mon, 22 Jan 2007 20:38:07 +0100
Subject: [R] Integration + Normal Distribution + Directory Browsing
 Processing Questions
In-Reply-To: <45B3BEA7.2010001@nhoeller.de>
References: <45B3BEA7.2010001@nhoeller.de>
Message-ID: <45B5129F.9060406@univie.ac.at>

Nils Hoeller wrote:
> 
> for example.
> 
> BUT what can I do for dynamic m and sd?
> I want something like integrate(dnorm(,0.6,0.15),0,1), with the first 
> dnorm parameter open for the
> integration but fixed m and sd.

integrate(function(x)dnorm(x,0.1,1.2), 0, 1)
is a way of "fixing" additional parameters.



-- 
Erich Neuwirth, University of Vienna
Faculty of Computer Science
Computer Supported Didactics Working Group
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-39464 Fax: +43-1-4277-39459


From daniel.kumpik at physiol.ox.ac.uk  Mon Jan 22 21:10:24 2007
From: daniel.kumpik at physiol.ox.ac.uk (dan kumpik)
Date: Mon, 22 Jan 2007 20:10:24 +0000
Subject: [R] R interpretation
Message-ID: <45B51A30.40200@physiol.ox.ac.uk>

Hi,

I am new to R (and not really a stats expert) and am having trouble 
interpreting its output. I am running a human learning experiment, with 
6 scores per subject in both the pretest and the posttest. I believe I 
have fitted the correct model for my data- a mixed-effects design, with 
subject as a random factor and session (pre vs post) nested within group 
(trained vs control).

I am confused about the output. The summary command gives me this table:


  > D.lme<- lme(score~GROUP/session, random=~1|subject, data=ILD4L )
  > summary(D.lme)


Linear mixed-effects model fit by REML
   Data: ILD4L
    Subset: EXP == "F"
          AIC       BIC   logLik
    -63.69801 -45.09881 37.84900

Random effects:
   Formula: ~1 | subject
          (Intercept)  Residual
StdDev:   0.1032511 0.1727145

Fixed effects: score ~ GROUP/session
                           Value  Std.Error  DF   t-value p-value
(Intercept)         0.10252778 0.05104328 152  2.008644  0.0463
GROUPT              0.09545347 0.06752391  12  1.413625  0.1829
GROUPC:sessionpost -0.00441389 0.04070919 152 -0.108425  0.9138
GROUPT:sessionpost -0.23586042 0.03525520 152 -6.690090  0.0000
   Correlation:
                     (Intr) GROUPT GROUPC
GROUPT             -0.756
GROUPC:sessionpost -0.399  0.301
GROUPT:sessionpost  0.000 -0.261  0.000

Standardized Within-Group Residuals:
          Min          Q1         Med          Q3         Max
-2.66977386 -0.52935645 -0.08616759  0.57215015  3.26532101

Number of Observations: 168
Number of Groups: 14


I believe the fixed-effects section of this output to be telling me that
my model intercept (which I assume to be the control group pretest?) is
significantly different from 0, and that GROUPT (i.e. the trained group)
does not differ significantly from the intercept- therefore no pretest
difference between groups?
	The next line is, I believe showing that the GROUPC x sessionpost
interaction (i.e., control posttest scores?) is not significantly
different from the intercept (i.e. control pretest scores). Finally, I
am interpreting the final line as indicating that the GROUPT x
sessionpost interaction (ie, trained posttest scores?) is significantly
different from the trained pretest scores (GROUPT). A treatment contrast 
that I would like to apply would be for Control-post vs Trained-post, to 
see if the groups differ after training, but I'm not sure how to do 
this- and I feel I am probably overcomplicating the matter.

also,
I am confused about how to report this output in my publication. For 
instance, what should I be reporting for df? Those found on the output 
of the anova table?

Would it be possible to look through this for me and indicate how to
interpret the R output, and also how I should be reporting this? 
Apologies for asking such basic questions, but I would like to start 
using R more regularly and to make sure I am doing so correctly.

Many thanks,

Dan


From replant at ucdavis.edu  Mon Jan 22 21:17:38 2007
From: replant at ucdavis.edu (Richard Plant)
Date: Mon, 22 Jan 2007 12:17:38 -0800
Subject: [R] Repeated measures
In-Reply-To: <mailman.7.1169031603.9339.r-help@stat.math.ethz.ch>
Message-ID: <70B0CAB623BFDA448F85DA2AFEE8BD42014BDDB6@EXCHANGE.plantsciences.ucdavis.edu>

In the two solutions for the repeated measures problem given in the
original reply below, the F and p values given by aov() with the error
strata defined by Error() are different from those given by lme().
However, when one does the problem "by hand" using the standard split
plot model, the results agree with those of nlme(). The difference
between the two aov() solutions is in the partitioning of sums of
squares. Is there a ready explanation for this discrepancy?

Thanks,
Richard Plant

> tolerance <- tolerance <-
+
read.table("http://www.ats.ucla.edu/stat/Splus/examples/alda/tolerance1.
txt",
+             sep=",", header=TRUE)
> tolerance.long <- reshape(tolerance,
+                           varying = list(c("tol11","tol12","tol13",
+                                            "tol14", "tol15")),
+                           v.names = c("tol"), timevar = "time",
+                           times = 11:15, direction = "long")
> tolerance.aov2 <- aov(tol ~ factor(male) + factor(male):factor(id) +
factor(time) + factor(time):male, data = tolerance.long)
> tolerance.sum <- summary(tolerance.aov2)
> tolerance.sum
                        Df Sum Sq Mean Sq F value    Pr(>F)    
factor(male)             1 0.3599  0.3599  2.6077  0.111967    
factor(time)             4 2.8326  0.7081  5.1309  0.001358 ** 
factor(male):factor(id) 14 8.2990  0.5928  4.2951 4.295e-05 ***
factor(time):male        4 0.1869  0.0467  0.3386  0.850786    
Residuals               56 7.7289  0.1380                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> tolerance.list <- tolerance.sum[[1]]
> tolerance.mat <- as.matrix(tolerance.list[3])
> tolerance.F.male <- tolerance.mat[1,1]/tolerance.mat[3,1]
> tolerance.F.male
[1] 0.607137
> tolerance.df <- as.matrix(tolerance.list[1])
> tolerance.p.male <- 1 -
pf(tolerance.F.male,tolerance.df[1,1],tolerance.df[3,1])
> tolerance.p.male
[1] 0.4488394
> 
> Message: 68
> Date: Wed, 17 Jan 2007 05:45:01 -0500
> From: Chuck Cleland <ccleland at optonline.net>
> Subject: Re: [R] Repeated measures
> To: Tom Backer Johnsen <backer at psych.uib.no>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <45ADFE2D.2060208 at optonline.net>
> Content-Type: text/plain; charset=ISO-8859-1
> 
> Tom Backer Johnsen wrote:
> > I am having a hard time understanding how to perform a "repeated
> > measures" type of ANOVA with R.  When reading the document found
here:
> >
> > http://cran.r-project.org/doc/contrib/Lemon-kickstart/kr_repms.html
> >
> > I find that there is a reference to a function make.rm () that is
> > supposed to rearrange a "one row per person" type of frame to a "one
> > row per observation" type of frame.  But that function does not seem
> > to be there.  Nor does the help.search suggest anything.  Is that
> > function buried in some package?
> 
>   I'm not able to find that function.  Perhaps that document is out of
> date.
> 
> > Is there  some simple documentation that might be useful somewhere?
> > Starting with a really simple problem (one group, two observations)?
> 
>   Here is an example showing the use of reshape() and analysis via
aov()
> and lme() in the nlme package.
> 
> tolerance <-
>
read.table("http://www.ats.ucla.edu/stat/Splus/examples/alda/tolerance1.
tx
> t",
>             sep=",", header=TRUE)
> 
> tolerance.long <- reshape(tolerance,
>                           varying = list(c("tol11","tol12","tol13",
>                                            "tol14", "tol15")),
>                           v.names = c("tol"), timevar = "time",
>                           times = 11:15, direction = "long")
> 
> tolerance.aov <- aov(tol ~ as.factor(time) * male + Error(id),
>                      data = tolerance.long)
> 
> summary(tolerance.aov)
> 
> Error: id
>      Df   Sum Sq  Mean Sq
> male  1 0.085168 0.085168
> 
> Error: Within
>                      Df  Sum Sq Mean Sq F value  Pr(>F)
> as.factor(time)       4  2.8326  0.7081  3.0538 0.02236 *
> male                  1  0.3024  0.3024  1.3039 0.25745
> as.factor(time):male  4  0.1869  0.0467  0.2015 0.93670
> Residuals            69 16.0002  0.2319
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> library(nlme)
> 
> tolerance.lme <- lme(tol ~ as.factor(time) * male, random = ~ 1 | id,
>                      data = tolerance.long)
> 
> anova(tolerance.lme)
>                      numDF denDF  F-value p-value
> (Intercept)              1    56 353.9049  <.0001
> as.factor(time)          4    56   5.1309  0.0014
> male                     1    14   0.6071  0.4488
> as.factor(time):male     4    56   0.3386  0.8508
> 
>   RSiteSearch("repeated measures") points to other examples,
functions,
> and documentation.
> 
> > Tom
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 512-0171 (M, W, F)
> fax: (917) 438-0894
> 
> 
> 
> ------------------------------
> 
> _______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From lalithaviswanath at yahoo.com  Mon Jan 22 21:43:28 2007
From: lalithaviswanath at yahoo.com (lalitha viswanath)
Date: Mon, 22 Jan 2007 12:43:28 -0800 (PST)
Subject: [R] Query about using optimizers in R without causing program to
	crash
Message-ID: <473950.42315.qm@web34103.mail.mud.yahoo.com>

Hi
I am a newbie to R and am using  the lm function to
fit my data.
This optimization is to be performed for around 45000
files not all of which lend themselves to
optimization. Some of these will and do crash.
 
However, How do I ensure that the program simply goes
to the next file in line without exiting the code with
the error
"Error in lm.fit(x, y, offset = offset, singular.ok =
singular.ok, ...) : 
        NA/NaN/Inf in foreign function call (arg 4)"
everytime it encounters troublesome data?

I would greatly appreciate your input as it would
avoid me having to manually type
for fileId in (c(4351:46000)) { ... }
for fileId in (c(5761:46000)) { ... }, etc...

Thanks
Lalitha


 
____________________________________________________________________________________
Now that's room service!  Choose from over 150,000 hotels


From mnair at iusb.edu  Mon Jan 22 21:50:20 2007
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Mon, 22 Jan 2007 15:50:20 -0500
Subject: [R] Recursive-SVM (R-SVM)
Message-ID: <A32055BDEA88C34BB3DBBCD22938077805103D@iu-mssg-mbx109.ads.iu.edu>

I am trying to implement a simple r-svm example using the iris data (only two of the classes are taken and data is within the code). I am running into some errors. I am not an expert on svm's. If any one has used it, I would appreciate their help. I am appending the code below. 
Thanks../Murli
 
#######################################################
### R-code for R-SVM

### use leave-one-out / Nfold or bootstrape to permute data for external CV

### build SVM model and use mean-balanced weight to sort genes on training set

### and recursive elimination of least important genes

### author: Dr. Xin Lu, Research Scientist

### Biostatistics Department, Harvard School of Public Health

library(e1071)

## read in SVM formated data in filename

## the format following the defination of SVMTorch

## the first line contains 2 integer: nSample nFeature+1 

## followed by a matrix, each row for one sample, with the last column being +/1 1 for class label

ReadSVMdata <- function(filename)

{

dd <- read.table( filename, header=F, skip=1)

x <- as.matrix( dd[, 1:(ncol(dd)-1)] ) 

y <- factor( dd[, ncol(dd)] )

ret <- list(x=x, y=y)

}

## create a decreasing ladder for recursive feature elimination

CreatLadder <- function( Ntotal, pRatio=0.75, Nmin=5 )

{

x <- vector()

x[1] <- Ntotal

for( i in 1:100 )

{

pp <- round(x[i] * pRatio)

if( pp == x[i] )

{

pp <- pp-1

} 

if( pp >= Nmin )

{

x[i+1] <- pp

} else

{

break

}

}

x

}

## R-SVM core code

## input:

## x: row matrix of data

## y: class label: 1 / -1 for 2 classes

## CVtype: 

## integer: N fold CV

## "LOO": leave-one-out CV

## "bootstrape": bootstrape CV

## CVnum: number of CVs

## LOO: defined as sample size

## Nfold and bootstrape: user defined, default as sample size

## output: a named list

## Error: a vector of CV error on each level

## SelFreq: a matrix for the frequency of each gene being selected in each level

## with each column corresponds to a level of selection

## and each row for a gene

## The top important gene in each level are those high-freqent ones

RSVM <- function(x, y, ladder, CVtype, CVnum=0 )

{

## check if y is binary response

Ytype <- names(table(y))

if( length(Ytype) != 2) 

{

print("ERROR!! RSVM can only deal with 2-class problem")

return(0)

}

## class mean

m1 <- apply(x[ which(y==Ytype[1]), ], 2, mean)

m2 <- apply(x[ which(y==Ytype[2]), ], 2, mean)

md <- m1-m2

yy <- vector( length=length(y))

yy[which(y==Ytype[1])] <- 1

yy[which(y==Ytype[2])] <- -1 

y <- yy

## check ladder

if( min(diff(ladder)) >= 0 )

{

print("ERROR!! ladder must be monotonously decreasing")

return(0);

}

if( ladder[1] != ncol(x) )

{

ladder <- c(ncol(x), ladder)

}

nSample <- nrow(x)

nGene <- ncol(x)

SampInd <- seq(1, nSample)

if( CVtype == "LOO" )

{

CVnum <- nSample

} else

{

if( CVnum == 0 )

{

CVnum <- nSample

}

}

## vector for test error and number of tests

ErrVec <- vector( length=length(ladder))

names(ErrVec) <- paste("Lev_", ladder, sep="")

nTests <- 0

SelFreq <- matrix( 0, nrow=nGene, ncol=length(ladder))

colnames(SelFreq) <- paste("Lev_", ladder, sep="")

## for each CV 

for( i in 1:CVnum )

{

## split data

if( CVtype == "LOO" )

{

TestInd <- i

TrainInd <- SampInd[ -TestInd]

} else

{

if( CVtype == "bootstrape" )

{

TrainInd <- sample(SampInd, nSample, replace=T )

TestInd <- SampInd[ which(!(SampInd %in% TrainInd ))]

} else

{

## Nfold

TrainInd <- sample(SampInd, nSample*(CVtype-1)/CVtype )

TestInd <- SampInd[ which(!(SampInd %in% TrainInd ))]

}

}

nTests <- nTests + length(TestInd)

## in each level, train a SVM model and record test error

xTrain <- x[TrainInd, ]

yTrain <- y[TrainInd]

xTest <- x[TestInd,]

yTest <- y[TestInd]

## index of the genes used in the 

SelInd <- seq(1, nGene)

for( gLevel in 1:length(ladder) )

{

## record the genes selected in this ladder

SelFreq[SelInd, gLevel] <- SelFreq[SelInd, gLevel] +1

## train SVM model and test error

svmres <- svm(xTrain[, SelInd], yTrain, scale=F, type="C-classification", kernel="linear" )

if( CVtype == "LOO" )

{

svmpred <- predict(svmres, matrix(xTest[SelInd], nrow=1) )

} else

{

svmpred <- predict(svmres, xTest[, SelInd] )

}

ErrVec[gLevel] <- ErrVec[gLevel] + sum(svmpred != yTest )

## weight vector

W <- t(svmres$coefs*yTrain[svmres$index]) %*% svmres$SV * md[SelInd]

rkW <- rank(W)

if( gLevel < length(ladder) )

{

SelInd <- SelInd[which(rkW > (ladder[gLevel] - ladder[gLevel+1]))]

}

}

}

ret <- list(ladder=ladder, Error=ErrVec/nTests, SelFreq=SelFreq)

}

SummaryRSVM <- function( RSVMres )

{

ERInd <- max( which(RSVMres$Error == min(RSVMres$Error)) )

MinLevel <- RSVMres$ladder[ERInd]

FreqVec <- RSVMres$SelFreq[, ERInd]

SelInd <- which( rank(FreqVec) >= (ladder[1]-MinLevel) )

# print("MinCV error of", min(RSVMres$Error), "at", MinLevel, "genes" )

ret <- list( MinER=min(RSVMres$Error), MinLevel=MinLevel, SelInd=SelInd)

}

###########################################

#my code starts below

#data<-ReadSVMdata("iris_r-svm.txt")

#The data read from the file is given below. 

data<-structure(list(x = structure(c(5.1, 4.9, 4.7, 4.6, 5, 5.4, 4.6, 

5, 4.4, 4.9, 5.4, 4.8, 4.8, 4.3, 5.8, 5.7, 5.4, 5.1, 5.7, 5.1, 

5.4, 5.1, 4.6, 5.1, 4.8, 5, 5, 5.2, 5.2, 4.7, 4.8, 5.4, 5.2, 

5.5, 4.9, 5, 5.5, 4.9, 4.4, 5.1, 5, 4.5, 4.4, 5, 5.1, 4.8, 5.1, 

4.6, 5.3, 5, 7, 6.4, 6.9, 5.5, 6.5, 5.7, 6.3, 4.9, 6.6, 5.2, 

5, 5.9, 6, 6.1, 5.6, 6.7, 5.6, 5.8, 6.2, 5.6, 5.9, 6.1, 6.3, 

6.1, 6.4, 6.6, 6.8, 6.7, 6, 5.7, 5.5, 5.5, 5.8, 6, 5.4, 6, 6.7, 

6.3, 5.6, 5.5, 5.5, 6.1, 5.8, 5, 5.6, 5.7, 5.7, 6.2, 5.1, 5.7, 

3.5, 3, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.4, 3, 

3, 4, 4.4, 3.9, 3.5, 3.8, 3.8, 3.4, 3.7, 3.6, 3.3, 3.4, 3, 3.4, 

3.5, 3.4, 3.2, 3.1, 3.4, 4.1, 4.2, 3.1, 3.2, 3.5, 3.6, 3, 3.4, 

3.5, 2.3, 3.2, 3.5, 3.8, 3, 3.8, 3.2, 3.7, 3.3, 3.2, 3.2, 3.1, 

2.3, 2.8, 2.8, 3.3, 2.4, 2.9, 2.7, 2, 3, 2.2, 2.9, 2.9, 3.1, 

3, 2.7, 2.2, 2.5, 3.2, 2.8, 2.5, 2.8, 2.9, 3, 2.8, 3, 2.9, 2.6, 

2.4, 2.4, 2.7, 2.7, 3, 3.4, 3.1, 2.3, 3, 2.5, 2.6, 3, 2.6, 2.3, 

2.7, 3, 2.9, 2.9, 2.5, 2.8, 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 

1.5, 1.4, 1.5, 1.5, 1.6, 1.4, 1.1, 1.2, 1.5, 1.3, 1.4, 1.7, 1.5, 

1.7, 1.5, 1, 1.7, 1.9, 1.6, 1.6, 1.5, 1.4, 1.6, 1.6, 1.5, 1.5, 

1.4, 1.5, 1.2, 1.3, 1.4, 1.3, 1.5, 1.3, 1.3, 1.3, 1.6, 1.9, 1.4, 

1.6, 1.4, 1.5, 1.4, 4.7, 4.5, 4.9, 4, 4.6, 4.5, 4.7, 3.3, 4.6, 

3.9, 3.5, 4.2, 4, 4.7, 3.6, 4.4, 4.5, 4.1, 4.5, 3.9, 4.8, 4, 

4.9, 4.7, 4.3, 4.4, 4.8, 5, 4.5, 3.5, 3.8, 3.7, 3.9, 5.1, 4.5, 

4.5, 4.7, 4.4, 4.1, 4, 4.4, 4.6, 4, 3.3, 4.2, 4.2, 4.2, 4.3, 

3, 4.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 

0.2, 0.1, 0.1, 0.2, 0.4, 0.4, 0.3, 0.3, 0.3, 0.2, 0.4, 0.2, 0.5, 

0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.4, 0.1, 0.2, 0.2, 0.2, 0.2, 

0.1, 0.2, 0.2, 0.3, 0.3, 0.2, 0.6, 0.4, 0.3, 0.2, 0.2, 0.2, 0.2, 

1.4, 1.5, 1.5, 1.3, 1.5, 1.3, 1.6, 1, 1.3, 1.4, 1, 1.5, 1, 1.4, 

1.3, 1.4, 1.5, 1, 1.5, 1.1, 1.8, 1.3, 1.5, 1.2, 1.3, 1.4, 1.4, 

1.7, 1.5, 1, 1.1, 1, 1.2, 1.6, 1.5, 1.6, 1.5, 1.3, 1.3, 1.3, 

1.2, 1.4, 1.2, 1, 1.3, 1.2, 1.3, 1.3, 1.1, 1.3), .Dim = c(100, 

4), .Dimnames = list(c("1", "2", "3", "4", "5", "6", "7", "8", 

"9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", 

"20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30", 

"31", "32", "33", "34", "35", "36", "37", "38", "39", "40", "41", 

"42", "43", "44", "45", "46", "47", "48", "49", "50", "51", "52", 

"53", "54", "55", "56", "57", "58", "59", "60", "61", "62", "63", 

"64", "65", "66", "67", "68", "69", "70", "71", "72", "73", "74", 

"75", "76", "77", "78", "79", "80", "81", "82", "83", "84", "85", 

"86", "87", "88", "89", "90", "91", "92", "93", "94", "95", "96", 

"97", "98", "99", "100"), c("V1", "V2", "V3", "V4"))), y = structure(c(2, 

2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 

2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 

2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 

1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 

1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), .Label = c("-1", 

"1"), class = "factor")), .Names = c("x", "y"))

len<-length(data$y)

x<-data$x

y<-data$y

ladder<-CreatLadder(len)

RSVM(x,y,ladder,"LOO")


From tlumley at u.washington.edu  Mon Jan 22 22:06:23 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 22 Jan 2007 13:06:23 -0800 (PST)
Subject: [R] Query about using optimizers in R without causing program
 to crash
In-Reply-To: <473950.42315.qm@web34103.mail.mud.yahoo.com>
References: <473950.42315.qm@web34103.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0701221306100.13495@homer22.u.washington.edu>


This is answered in the FAQ list.

 	-thomas

On Mon, 22 Jan 2007, lalitha viswanath wrote:

> Hi
> I am a newbie to R and am using  the lm function to
> fit my data.
> This optimization is to be performed for around 45000
> files not all of which lend themselves to
> optimization. Some of these will and do crash.
>
> However, How do I ensure that the program simply goes
> to the next file in line without exiting the code with
> the error
> "Error in lm.fit(x, y, offset = offset, singular.ok =
> singular.ok, ...) :
>        NA/NaN/Inf in foreign function call (arg 4)"
> everytime it encounters troublesome data?
>
> I would greatly appreciate your input as it would
> avoid me having to manually type
> for fileId in (c(4351:46000)) { ... }
> for fileId in (c(5761:46000)) { ... }, etc...
>
> Thanks
> Lalitha
>
>
>
> ____________________________________________________________________________________
> Now that's room service!  Choose from over 150,000 hotels
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From lalithaviswanath at yahoo.com  Mon Jan 22 22:11:38 2007
From: lalithaviswanath at yahoo.com (lalitha viswanath)
Date: Mon, 22 Jan 2007 13:11:38 -0800 (PST)
Subject: [R] Query about using try block
In-Reply-To: <45B52391.3050306@gmail.com>
Message-ID: <20070122211138.38467.qmail@web34111.mail.mud.yahoo.com>

Hi
Thanks for your response.
However I seem to be doing something wrong regarding
the try block resulting in yet another error described
below.

I have a function that takes in a file name and
does the fit for the data in that file.
Hence based on your input, I tried

try ( (fit = lm(y~x, data = data_fitting)), silent =
T);


I left the subsequent lines of my code unchanged.
coeffs = as.list(coef(fit);
lambda = exp(coeffs$x)....

After the change using try, when I tried to resume
processing under R as follows
source("fitting.R")
for filename in list { process(filename); }
It says "Cannot find object "fit" "...(in the line
trying to get the coefficients...)

Am I closing the try block in the wrong place?
This function does some post processing on the
coefficients returned by coef(fit), puts them in a
list and sends it to another function.
(i.e. around 6 lines of code after the call to fit).
Thanks
Lalitha
--- Andreas Hary <andreashary at googlemail.com> wrote:

> Look at ?try
> 
> Your code will probably need to be something like
> the following:
> 
> fit <- list()
> for(fileId in 1:n){
>    try(fit[i] <- lm(formula,data=???,...), silent=F)
>    #or silent=T if you would like to be made aware
> of problems
> }
> 
> Best wishes,
> 
> Andreas
> 
> 
> 
> 
> lalitha viswanath wrote:
> > Hi
> > I am a newbie to R and am using  the lm function
> to
> > fit my data.
> > This optimization is to be performed for around
> 45000
> > files not all of which lend themselves to
> > optimization. Some of these will and do crash.
> >  
> > However, How do I ensure that the program simply
> goes
> > to the next file in line without exiting the code
> with
> > the error
> > "Error in lm.fit(x, y, offset = offset,
> singular.ok =
> > singular.ok, ...) : 
> >         NA/NaN/Inf in foreign function call (arg
> 4)"
> > everytime it encounters troublesome data?
> > 
> > I would greatly appreciate your input as it would
> > avoid me having to manually type
> > for fileId in (c(4351:46000)) { ... }
> > for fileId in (c(5761:46000)) { ... }, etc...
> > 
> > Thanks
> > Lalitha
> > 
> > 
> >  
> >
>
____________________________________________________________________________________
> > Now that's room service!  Choose from over 150,000
> hotels
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> > 
> 
> -- 
> =============================
> Andreas Hary
> Flat 5, 70 Finsbury Park Road
> Lond, N4 2JX, UK
> 
> Email:	andreashary at gmail.com
> Mobile: 07906 860 987
> 



 
____________________________________________________________________________________
Want to start your own business?


From br44114 at gmail.com  Mon Jan 22 22:16:28 2007
From: br44114 at gmail.com (bogdan romocea)
Date: Mon, 22 Jan 2007 16:16:28 -0500
Subject: [R] sequential processing
Message-ID: <8d5a36350701221316v3b34f8f6j47f9b0323c0106cc@mail.gmail.com>

One option for processing very large files with R is split:
  ## split a large file into pieces
  #--parameters: the folder, file and number of parts
  FLD=/home/user/data
  F=very_large_file.dat
  parts=50
  #---split
  cd $FLD
  fn=`echo $F | awk -F\. '{print $1}'`  #file name without extension
  ln=`wc -l $F | awk '{print $1}'`  #number of lines in the file
  forsplit=`expr $ln / $parts + 1`  #number of lines in each part
  echo "====== $F will be split in $parts parts of $forsplit lines each."
  split -l $forsplit $F $fn
You could also load the entire file into a DBMS then pull parts of it
into R, or read specific lines through a pipe e.g.
readLines(pipe("sed, grep, python... command")).

Don't try to replicate the SAS processing into R. The exact
translations of the SAS DATA STEP usage of _N_, first., last., retain
etc into R would be: inefficient, ugly, retrogressive, wrong, rigid,
complicated, silly and so on. For a start, read up on indexing - this
seemingly simple and innocuous R feature is in fact far more powerful
than the entire DATA STEP with its whole bag of tricks. Then search
the list for similar questions, for example
http://thread.gmane.org/gmane.comp.lang.r.general/44332/focus=44343


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gerard Smits
> Sent: Sunday, January 21, 2007 2:22 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] sequential processing
>
> Like many others, I am new to R but old to SAS.
>
> Am I correct in understanding that R processes a data frame in a
> sequential ly?  This would imply that large input files could be
> read, without the need to load the entire file into memory.
> Related to the manner of reading a frame, I have been looking for the
> equivalent of SAS _n_ (I realize that I can use a variant of which to
> identify an index value) as well as  useful SAS features such as
> first., last., retain, etc.  Any help with this conversion
> appreciated.
>
> Thanks,
>
> Gerard Smits
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From A.Robinson at ms.unimelb.edu.au  Mon Jan 22 21:53:00 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Tue, 23 Jan 2007 07:53:00 +1100
Subject: [R] Query about using optimizers in R without causing program
	to	crash
In-Reply-To: <473950.42315.qm@web34103.mail.mud.yahoo.com>
References: <473950.42315.qm@web34103.mail.mud.yahoo.com>
Message-ID: <20070122205300.GQ13602@ms.unimelb.edu.au>

Hi Lalitha,

Use 

try()

or 

tryCatch()

Cheers

Andrew

On Mon, Jan 22, 2007 at 12:43:28PM -0800, lalitha viswanath wrote:
> Hi
> I am a newbie to R and am using  the lm function to
> fit my data.
> This optimization is to be performed for around 45000
> files not all of which lend themselves to
> optimization. Some of these will and do crash.
>  
> However, How do I ensure that the program simply goes
> to the next file in line without exiting the code with
> the error
> "Error in lm.fit(x, y, offset = offset, singular.ok =
> singular.ok, ...) : 
>         NA/NaN/Inf in foreign function call (arg 4)"
> everytime it encounters troublesome data?
> 
> I would greatly appreciate your input as it would
> avoid me having to manually type
> for fileId in (c(4351:46000)) { ... }
> for fileId in (c(5761:46000)) { ... }, etc...
> 
> Thanks
> Lalitha
> 
> 
>  
> ____________________________________________________________________________________
> Now that's room service!  Choose from over 150,000 hotels
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From btyner at stat.purdue.edu  Mon Jan 22 22:18:28 2007
From: btyner at stat.purdue.edu (Benjamin Tyner)
Date: Mon, 22 Jan 2007 16:18:28 -0500
Subject: [R] lattice: put key where unused panel would have been
Message-ID: <45B52A24.30803@stat.purdue.edu>

Hi,

Say I have

z<-data.frame(y=runif(190),
                       x=runif(190),
                       f=gl(5,38),
                       g=gl(19,10))

plot<-xyplot(y~x|g,
                     data=z,
                     layout=c(5,4),
                     groups=f,
                     auto.key=TRUE)

How might one place the key in the empty space where the twentieth panel 
would have been?

Thanks,
Ben


From phhs80 at gmail.com  Mon Jan 22 23:26:00 2007
From: phhs80 at gmail.com (Paul Smith)
Date: Mon, 22 Jan 2007 22:26:00 +0000
Subject: [R] Question about rpart and regression trees
Message-ID: <6ade6f6c0701221426n6abf43acx31caa82edd339ee9@mail.gmail.com>

Dear All

I would like to use rpart to obtain a regression tree for a dataset
like the following:

Y	X1	X2	X3	X4
5.500033	B	A	3	2
0.35625148	D	B	6	5
0.8062546	E	C	4	3
5.100014	C	A	3	2
5.7000422	A	A	3	2
0.76875436	C	A	6	5
1.0312537	D	A	4	1

Y is the objective variable. X1, X2, X3 and X4 can take, respectively,
the following values:

X1: A,B,C,D,E
X2: A,B,C,D,E
X3: 3,4,5,6
X4. 1,2,3,4,5

Should I convert X3 and X4 to factor before running rpart?

Thanks in advance,

Paul


From g_smits at verizon.net  Tue Jan 23 00:12:18 2007
From: g_smits at verizon.net (Gerard Smits)
Date: Mon, 22 Jan 2007 15:12:18 -0800
Subject: [R] sequential processing
In-Reply-To: <8d5a36350701221316v3b34f8f6j47f9b0323c0106cc@mail.gmail.co m>
References: <8d5a36350701221316v3b34f8f6j47f9b0323c0106cc@mail.gmail.com>
Message-ID: <7.0.1.0.2.20070122150253.03429410@verizon.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070122/f8f5a6a1/attachment.pl 

From sundar.dorai-raj at pdf.com  Tue Jan 23 00:50:35 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 22 Jan 2007 17:50:35 -0600
Subject: [R] lattice: put key where unused panel would have been
In-Reply-To: <45B52A24.30803@stat.purdue.edu>
References: <45B52A24.30803@stat.purdue.edu>
Message-ID: <45B54DCB.7000101@pdf.com>



Benjamin Tyner said the following on 1/22/2007 3:18 PM:
> Hi,
> 
> Say I have
> 
> z<-data.frame(y=runif(190),
>                        x=runif(190),
>                        f=gl(5,38),
>                        g=gl(19,10))
> 
> plot<-xyplot(y~x|g,
>                      data=z,
>                      layout=c(5,4),
>                      groups=f,
>                      auto.key=TRUE)
> 
> How might one place the key in the empty space where the twentieth panel 
> would have been?
> 
> Thanks,
> Ben
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

You can try supplying x and y coordinates to auto.key

xyplot(y ~ x | g,
        data = z,
        layout = c(5, 4),
        groups = f,
        auto.key = list(x = .85, y = .9))

HTH,

--sundar


From helprhelp at gmail.com  Tue Jan 23 02:30:31 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Mon, 22 Jan 2007 20:30:31 -0500
Subject: [R] logistic regression model + Cross-Validation
In-Reply-To: <2b808b010701211251k13e263dap4941b2458e94db9c@mail.gmail.com>
References: <2b808b010701210251v2c7c5d02s8b3eeb6425c649f0@mail.gmail.com>
	<45B37E88.4050500@vanderbilt.edu>
	<2b808b010701211251k13e263dap4941b2458e94db9c@mail.gmail.com>
Message-ID: <cdf817830701221730x40b442fcj39c9d546d6d99815@mail.gmail.com>

why not use lda{MASS} and it has cv=T option; it does "loo", though.
Or use randomForest.

if you have to use lrm, then the following code might help:

n.fold <- 5 # 5-fold cv
n.sample <- 50 # assumed 50 samples
s <- sample(1:n.fold, size=n.sample, replace=T)
for (i in 1:n.fold){
  # create your training data and validation data for each fold
  trn <- YOURWHOLEDATAFRAME[s!=i,]
  val <- YOURWHOLEDATAFRAME[s==i,]
  # now do your own modeling using lrm
  # todo
}

HTH,

weiwei

On 1/21/07, nitin jindal <nitin.jindal at gmail.com> wrote:
> If validate.lrm does not has this option, do any other function has it.
> I will certainly look into your advice on cross validation. Thnx.
>
> nitin
>
> On 1/21/07, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
> >
> > nitin jindal wrote:
> > > Hi,
> > >
> > > I am trying to cross-validate a logistic regression model.
> > > I am using logistic regression model (lrm) of package Design.
> > >
> > > f <- lrm( cy ~ x1 + x2, x=TRUE, y=TRUE)
> > > val <- validate.lrm(f, method="cross", B=5)
> >
> > val <- validate(f, ...)    # .lrm not needed
> >
> > >
> > > My class cy has values 0 and 1.
> > >
> > > "val" variable will give me indicators like slope and AUC. But, I also
> > need
> > > the vector of predicted values of class variable "cy" for each record
> > while
> > > cross-validation, so that I can manually look at the results. So, is
> > there
> > > any way to get those probabilities assigned to each class.
> > >
> > > regards,
> > > Nitin
> >
> > No, validate.lrm does not have that option.  Manually looking at the
> > results will not be easy when you do enough cross-validations.  A single
> > 5-fold cross-validation does not provide accurate estimates.  Either use
> > the bootstrap or repeat k-fold cross-validation between 20 and 50 times.
> >   k is often 10 but the optimum value may not be 10.  Code for averaging
> > repeated cross-validations is in
> > http://biostat.mc.vanderbilt.edu/twiki/pub/Main/RmS/logistic.val.pdf
> > along with simulations of bootstrap vs. a few cross-validation methods
> > for binary logistic models.
> >
> > Frank
> > --
> > Frank E Harrell Jr   Professor and Chair           School of Medicine
> >                       Department of Biostatistics   Vanderbilt University
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From ploua at allstate.com  Tue Jan 23 03:43:42 2007
From: ploua at allstate.com (Louisell, Paul)
Date: Mon, 22 Jan 2007 18:43:42 -0800
Subject: [R] Loess with more than 4 predictors / offsets
Message-ID: <633AD4C78F3E8F489614A06990F6077B02027E0C@a0203-xpo0111-s.hodc.ad.allstate.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070122/816ed89d/attachment.pl 

From skiadas at hanover.edu  Tue Jan 23 03:50:42 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Mon, 22 Jan 2007 21:50:42 -0500
Subject: [R] Questions about xtable and print.xtable
Message-ID: <6219E50D-5F85-41B9-80B4-CAF4B6796AD4@hanover.edu>

I have been using the wonderful xtable package lately, in combination  
with Sweave, and I have a couple of general questions along with a  
more particular one.

I'll start with the particular question. I basically have a 1x3 array  
with column names but no row names. I want to create a latex table  
with column setting set to "|rrr|". I want the column names to  
appear, but the row names not to appear. The code I am trying is this:

library(xtable)
x <- matrix(c(1:3), c(1,3), dimnames=list(NULL,c(1:3)))
tab <- xtable(x, align="|rrrr|")
print.xtable(tab, include.rownames=FALSE)
print.xtable(tab)

The problem here is that the xtable call requires an align value that  
has one extra row setting, I suppose to account for a possible row  
name. However, the first print.xtable call seems to ignore the align  
argument set in the xtable call, when include.rownames is included.  
Any workarounds will be most welcomed.

More generally, I have the following questions:
1) Why are the include.rownames and include.colnames parameters not  
appearing in the xtable call, but only in the print.xtable call  
instead? Why do I need to specify n+1 arguments for things like align  
and digits, when I don't want the row names to be printed? In  
general, why are the align and digits calls not setable in  
print.xtable, but only in xtable?
2) I like to enclose my tabular environments in a center environment,  
instead of a table environment. Unless I've missed it, I don't see  
how I can do that from within the xtable package. Is this really not  
possible, and if so why not? The latex.environments setting seems to  
only be allowed when floating=TRUE, which is exactly what I want to  
avoid. Any particular reason it is not allowed when floating=FALSE as  
well?

That's it really, thanks in advance for any responses.

Haris


From qjing at sibs.ac.cn  Tue Jan 23 07:23:19 2007
From: qjing at sibs.ac.cn (qing)
Date: Tue, 23 Jan 2007 06:23:19 +0000 (UTC)
Subject: [R] factor()
Message-ID: <loom.20070119T092542-850@post.gmane.org>















Dear All,

I am running Windows XP, R 2.4.1, and doing an example about factor().

> read.csv("c:\\TeamInfo.csv")->TeamInfo;
> TeamInfo;
     TEAM    NAME LEVEL WORKTIME BONUS
1   batch   sunan     B      135 9,818
2   batch  Chenqi     E      121 6,050
3   batch jiangxu     F       97 4,189
4  online  zhouxi     F       63 2,720
5  online  chenhe     H       36 1,064
6  online                     NA      
7  online                     NA      
8  online                     NA      
9  online                     NA      
10 client                     NA      
11 client                     NA      
12 client                     NA      
13 client                     NA      
14 client                     NA      
> factor(TEAM)->Teamactor;
Error in typeof(x) : object "TEAM" not found

Any help or suggestions that you can provide will be greatly appreciated.

Qing


From ripley at stats.ox.ac.uk  Tue Jan 23 07:38:11 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Jan 2007 06:38:11 +0000 (GMT)
Subject: [R] Question about rpart and regression trees
In-Reply-To: <6ade6f6c0701221426n6abf43acx31caa82edd339ee9@mail.gmail.com>
References: <6ade6f6c0701221426n6abf43acx31caa82edd339ee9@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701230637030.29155@gannet.stats.ox.ac.uk>

On Mon, 22 Jan 2007, Paul Smith wrote:

> Dear All
>
> I would like to use rpart to obtain a regression tree for a dataset
> like the following:
>
> Y	X1	X2	X3	X4
> 5.500033	B	A	3	2
> 0.35625148	D	B	6	5
> 0.8062546	E	C	4	3
> 5.100014	C	A	3	2
> 5.7000422	A	A	3	2
> 0.76875436	C	A	6	5
> 1.0312537	D	A	4	1
>
> Y is the objective variable. X1, X2, X3 and X4 can take, respectively,
> the following values:
>
> X1: A,B,C,D,E
> X2: A,B,C,D,E
> X3: 3,4,5,6
> X4. 1,2,3,4,5
>
> Should I convert X3 and X4 to factor before running rpart?

If they really are factors, yes.
If they are ordered factors, no.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Tue Jan 23 08:00:39 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Jan 2007 07:00:39 +0000 (GMT)
Subject: [R] Loess with more than 4 predictors / offsets
In-Reply-To: <633AD4C78F3E8F489614A06990F6077B02027E0C@a0203-xpo0111-s.hodc.ad.allstate.com>
References: <633AD4C78F3E8F489614A06990F6077B02027E0C@a0203-xpo0111-s.hodc.ad.allstate.com>
Message-ID: <Pine.LNX.4.64.0701230639150.29155@gannet.stats.ox.ac.uk>

On Mon, 22 Jan 2007, Louisell, Paul wrote:

> Hello,
>
> Does anyone know of an R version of loess that allows more than 4
> predictors and/or allows the specification of offsets? For that matter,
> does anyone know of _any_ version of loess that does either of the
> things I mention?

Why would you want offsets in a regression?: just subtract them from the 
lhs.  (R's lm has gained offsets by analogy with glm, but the S original 
did not have them).  If you would be more comfortable working with them, 
it would be very easy to create a modified version that supports them.

Also, have you heard of the 'curse of dimensionality'?  Localization even 
to 4 dimensions is no longer really an appropriate term, and Euclidean 
distance will be the main determinant of 'local' and is quite arbitrary.


> Thanks,
>
> Paul Louisell
> 650-833-6254
> ploua at allstate.com
> Research Associate (Statistician)
> Modeling & Data Analytics
> ARPC
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From lbaring at stochastik.uni-hannover.de  Tue Jan 23 08:08:06 2007
From: lbaring at stochastik.uni-hannover.de (Ludwig Baringhaus)
Date: Tue, 23 Jan 2007 08:08:06 +0100
Subject: [R] D'Agostino test
In-Reply-To: <s5b4a137.063@wokingmail.wokingprime.com>
References: <s5b4a137.063@wokingmail.wokingprime.com>
Message-ID: <200701230808.06719.lbaring@stochastik.uni-hannover.de>

Am Montag, 22. Januar 2007 12:33 schrieb Matthieu Mourroux:
> Hello,
>
> I'd like to know if the D'Agostino test of normality is reliable,
The test is not consistent. The test statistic   
can be used for testing the hypothesis of uniformity.

See the paper  

Baringhaus, L.; Henze, N.
A test for uniformity with unknown limits based on D'Agostino's $D$.
Statist. Probab. Lett. 9 (1990), no. 4, 299--304.

for details.

L. Baringhaus
Leibniz Universitaet Hannover
Institut fuer Mathematische Stochastik


From frederic.chiroleu at cirad.fr  Tue Jan 23 08:05:56 2007
From: frederic.chiroleu at cirad.fr (=?ISO-8859-1?Q?Fr=E9d=E9ric_Chiroleu?=)
Date: Tue, 23 Jan 2007 11:05:56 +0400
Subject: [R] error in arules package
Message-ID: <45B5B3D4.70403@cirad.fr>

Hi,

we noticed there was a error in the "arules" package.

After reading the source code, we saw that the Dice similarity index was 
"miscalculated" in "dissimilarity" function : an copy-paste from Jaccard 
Index was not corrected (2* a_b_c, ie 2*(a+b+c) in the code instead of 
2*a +b + c !!!).

After our mail to R-help (21/11/2006), we thought the authors could do 
something but I just try the function and the error is still there.

I hope the authors will read my mail.

Sincerely yours,

Fred.

-- 
Dr. Fr?d?ric Chiroleu
Biom?tricien
UMR 53 PVBMT (Peuplements V?g?taux et Bio-agresseurs en Milieu Tropical)
CIRAD-AMIS
P?le de Protection des Plantes (3P)
Laboratoire d'?cologie Terrestre et de Lutte Int?gr?e (LETLI)
7, chemin de l'IRAT
Ligne Paradis
97410 Saint-Pierre
?le de la R?union - France
T?l. : 02 62 49 92 30
Standard : 02 62 49 92 00
Fax  : 02 62 49 92 93
courriel : frederic.chiroleu at cirad.fr


From gregor.gorjanc at bfro.uni-lj.si  Tue Jan 23 09:02:02 2007
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Tue, 23 Jan 2007 08:02:02 +0000 (UTC)
Subject: [R] ECB/Sidebar/R (Emacs) was: Re: kate editor for R
References: <45B02D46.1090709@vanderbilt.edu>
	<200701191118.26761.rdiaz@cnio.es>
	<17840.55304.803214.860567@basebud.nulle.part>
	<200701201520.42653.blindglobe@gmail.com>
	<17842.13129.56162.112711@basebud.nulle.part>
	<624934630701211505k6dd5e53fl8bf134bd08fce01c@mail.gmail.com>
	<17844.354.297037.512928@basebud.nulle.part>
	<624934630701220258t460ddde0w371d49c7b23f82a7@mail.gmail.com>
Message-ID: <loom.20070123T090118-949@post.gmane.org>

Ramon Diaz-Uriarte <rdiaz02 <at> gmail.com> writes:
> > I had looked at ECB for C++ programming. It simply hadn't occurred to me that
> > it would plug into ESS.
> 
> I wasn't aware of it either until I attended Tony Rossini's tutorial
> at useR! 2006.
> 

Are there any materials for those who missed the course?

Gregor


From ripley at stats.ox.ac.uk  Tue Jan 23 09:12:12 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Jan 2007 08:12:12 +0000 (GMT)
Subject: [R] sequential processing
In-Reply-To: <7.0.1.0.2.20070122150253.03429410@verizon.net>
References: <8d5a36350701221316v3b34f8f6j47f9b0323c0106cc@mail.gmail.com>
	<7.0.1.0.2.20070122150253.03429410@verizon.net>
Message-ID: <Pine.LNX.4.64.0701230808520.2163@gannet.stats.ox.ac.uk>

On Mon, 22 Jan 2007, Gerard Smits wrote:

> So, I take it, given that the use of a pipe is suggested for
> sequential reading, that the standard approach to processing a data
> frame is to load the entire file?  Please correct if wrong.

Yes, because most data frames are tiny compared to current RAM sizes.
But the R has connections and lots of means to read from them indicates 
that other approaches are also supported.  Large datasets are often
kept in DBMSs, and data transferred to R as required.

There is an 'R Data Import/Export' manual, and this would have illuminated 
the subject for you.


> BTW, I am not interested in finding direct translations of SAS data
> step statements to R, but instead in finding an approach by which I
> can address the type of problems I consistent have to deal with
> (grouped processing with retention of baseline records, etc.).  I'll
> read more on the indexing as a means of dealing with relative position issues
>
> Thanks,
>
> Gerard
>
>
>
>> You could also load the entire file into a DBMS then pull parts of it
>> into R, or read specific lines through a pipe e.g.
>> readLines(pipe("sed, grep, python... command")).
>>
>> Don't try to replicate the SAS processing into R. The exact
>> translations of the SAS DATA STEP usage of _N_, first., last., retain
>> etc into R would be: inefficient, ugly, retrogressive, wrong, rigid,
>> complicated, silly and so on. For a start, read up on indexing - this
>> seemingly simple and innocuous R feature is in fact far more powerful
>> than the entire DATA STEP with its whole bag of tricks. Then search
>> the list for similar questions, for example
>> http://thread.gmane.org/gmane.comp.lang.r.general/44332/focus=44343
>>
>>
>>> -----Original Message-----
>>> From: r-help-bounces at stat.math.ethz.ch
>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gerard Smits
>>> Sent: Sunday, January 21, 2007 2:22 PM
>>> To: r-help at stat.math.ethz.ch
>>> Subject: [R] sequential processing
>>>
>>> Like many others, I am new to R but old to SAS.
>>>
>>> Am I correct in understanding that R processes a data frame in a
>>> sequential ly?  This would imply that large input files could be
>>> read, without the need to load the entire file into memory.
>>> Related to the manner of reading a frame, I have been looking for the
>>> equivalent of SAS _n_ (I realize that I can use a variant of which to
>>> identify an index value) as well as  useful SAS features such as
>>> first., last., retain, etc.  Any help with this conversion
>>> appreciated.
>>>
>>> Thanks,
>>>
>>> Gerard Smits
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From qjing at sibs.ac.cn  Tue Jan 23 09:24:04 2007
From: qjing at sibs.ac.cn (jing)
Date: Tue, 23 Jan 2007 08:24:04 +0000 (UTC)
Subject: [R] the value of Delta
Message-ID: <loom.20070119T092542-850@post.gmane.org>





Dear all,

I am running R 2.4.1.

> library(siggenes);
> library(multtest);
> cl<-rep(c(0,1),c(3,3));
> sub<-exprs(AffyExpData[,c(1:3,7:9)]);
> gn<-geneNames(AffyRAwData);
> sam.out<-sam(sub,cl,rand=123,gene.names=gn);

We're doing 20 complete permutations

> sam.out
SAM Analysis for the Two-Class Unpaired Case Assuming Unequal Variances 
 
   Delta    p0  False Called   FDR
1    0.1 0.929 292.25    293 0.927
2    0.4 0.929  43.60     56 0.724
3    0.7 0.929  12.25     20 0.569
4    1.0 0.929   7.25     14 0.481
5    1.3 0.929   2.60      7 0.345
6    1.7 0.929   1.30      5 0.242
7    2.0 0.929   1.30      5 0.242
8    2.3 0.929   0.45      2 0.209
9    2.6 0.929   0.45      2 0.209
10   2.9 0.929   0.45      2 0.209

> sum.sam.out<-summary(sam.out,1,ll=FALSE);
> sum.sam.out$row.sig.genes;
NULL
> sum.sam.out<-summary(sam.out,0.1,ll=FALSE);
> sum.sam.out$row.sig.genes;
NULL
> sum.sam.out<-summary(sam.out,0.00000000001,ll=FALSE);
> sum.sam.out$row.sig.genes;
NULL
> sum.sam.out<-summary(sam.out,1000,ll=FALSE);
> sum.sam.out$row.sig.genes;
NULL
   

Any value of Delta I chosen: 1000,1,0,1 0.00000000001, the outcome is NULL.

Any help or suggestions that you can provide will be greatly appreciated.

Qing


From wl2776 at gmail.com  Tue Jan 23 10:04:31 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Tue, 23 Jan 2007 01:04:31 -0800 (PST)
Subject: [R] why not to enclose in 'try' everything?
In-Reply-To: <3948d9e50701222111y1d0673dcsa44ea6e012d8d3e4@mail.gmail.com>
References: <20070122211138.38467.qmail@web34111.mail.mud.yahoo.com>
	<3948d9e50701222111y1d0673dcsa44ea6e012d8d3e4@mail.gmail.com>
Message-ID: <8517796.post@talk.nabble.com>


fit<-NULL
try (
       {
          fit <- lm(y~x, data = data_fitting)
          coeffs <- as.list(coef(fit))
          ## other subsequent processes
       }, 
       silent =TRUE)

-- 
View this message in context: http://www.nabble.com/Re%3A--R--Query-about-using-try-block-tf3060494.html#a8517796
Sent from the R help mailing list archive at Nabble.com.


From buser at stat.math.ethz.ch  Tue Jan 23 10:17:05 2007
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Tue, 23 Jan 2007 10:17:05 +0100
Subject: [R] the value of Delta
In-Reply-To: <loom.20070119T092542-850@post.gmane.org>
References: <loom.20070119T092542-850@post.gmane.org>
Message-ID: <17845.53905.98212.139705@stat.math.ethz.ch>

I have not installed the package "siggenes", but from the manual
on" 

http://www.ugrad.stat.ubc.ca/R/library/siggenes/

I assume it is programmed as an S4 class since they use 

sum.sam.out at row.sig.genes

instead of "$"

Hope this helps

Christoph

--------------------------------------------------------------

Credit and Surety PML study: visit our web page www.cs-pml.org

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH Zurich	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


jing writes:
 > 
 > 
 > 
 > 
 > Dear all,
 > 
 > I am running R 2.4.1.
 > 
 > > library(siggenes);
 > > library(multtest);
 > > cl<-rep(c(0,1),c(3,3));
 > > sub<-exprs(AffyExpData[,c(1:3,7:9)]);
 > > gn<-geneNames(AffyRAwData);
 > > sam.out<-sam(sub,cl,rand=123,gene.names=gn);
 > 
 > We're doing 20 complete permutations
 > 
 > > sam.out
 > SAM Analysis for the Two-Class Unpaired Case Assuming Unequal Variances 
 >  
 >    Delta    p0  False Called   FDR
 > 1    0.1 0.929 292.25    293 0.927
 > 2    0.4 0.929  43.60     56 0.724
 > 3    0.7 0.929  12.25     20 0.569
 > 4    1.0 0.929   7.25     14 0.481
 > 5    1.3 0.929   2.60      7 0.345
 > 6    1.7 0.929   1.30      5 0.242
 > 7    2.0 0.929   1.30      5 0.242
 > 8    2.3 0.929   0.45      2 0.209
 > 9    2.6 0.929   0.45      2 0.209
 > 10   2.9 0.929   0.45      2 0.209
 > 
 > > sum.sam.out<-summary(sam.out,1,ll=FALSE);
 > > sum.sam.out$row.sig.genes;
 > NULL
 > > sum.sam.out<-summary(sam.out,0.1,ll=FALSE);
 > > sum.sam.out$row.sig.genes;
 > NULL
 > > sum.sam.out<-summary(sam.out,0.00000000001,ll=FALSE);
 > > sum.sam.out$row.sig.genes;
 > NULL
 > > sum.sam.out<-summary(sam.out,1000,ll=FALSE);
 > > sum.sam.out$row.sig.genes;
 > NULL
 >    
 > 
 > Any value of Delta I chosen: 1000,1,0,1 0.00000000001, the outcome is NULL.
 > 
 > Any help or suggestions that you can provide will be greatly appreciated.
 > 
 > Qing
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
 > and provide commented, minimal, self-contained, reproducible code.


From buser at stat.math.ethz.ch  Tue Jan 23 10:20:28 2007
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Tue, 23 Jan 2007 10:20:28 +0100
Subject: [R] the value of Delta
In-Reply-To: <loom.20070119T092542-850@post.gmane.org>
References: <loom.20070119T092542-850@post.gmane.org>
Message-ID: <17845.54108.796354.911850@stat.math.ethz.ch>


Or better

slot(sum.sam.out, "row.sig.genes")



jing writes:
 > 
 > 
 > 
 > 
 > Dear all,
 > 
 > I am running R 2.4.1.
 > 
 > > library(siggenes);
 > > library(multtest);
 > > cl<-rep(c(0,1),c(3,3));
 > > sub<-exprs(AffyExpData[,c(1:3,7:9)]);
 > > gn<-geneNames(AffyRAwData);
 > > sam.out<-sam(sub,cl,rand=123,gene.names=gn);
 > 
 > We're doing 20 complete permutations
 > 
 > > sam.out
 > SAM Analysis for the Two-Class Unpaired Case Assuming Unequal Variances 
 >  
 >    Delta    p0  False Called   FDR
 > 1    0.1 0.929 292.25    293 0.927
 > 2    0.4 0.929  43.60     56 0.724
 > 3    0.7 0.929  12.25     20 0.569
 > 4    1.0 0.929   7.25     14 0.481
 > 5    1.3 0.929   2.60      7 0.345
 > 6    1.7 0.929   1.30      5 0.242
 > 7    2.0 0.929   1.30      5 0.242
 > 8    2.3 0.929   0.45      2 0.209
 > 9    2.6 0.929   0.45      2 0.209
 > 10   2.9 0.929   0.45      2 0.209
 > 
 > > sum.sam.out<-summary(sam.out,1,ll=FALSE);
 > > sum.sam.out$row.sig.genes;
 > NULL
 > > sum.sam.out<-summary(sam.out,0.1,ll=FALSE);
 > > sum.sam.out$row.sig.genes;
 > NULL
 > > sum.sam.out<-summary(sam.out,0.00000000001,ll=FALSE);
 > > sum.sam.out$row.sig.genes;
 > NULL
 > > sum.sam.out<-summary(sam.out,1000,ll=FALSE);
 > > sum.sam.out$row.sig.genes;
 > NULL
 >    
 > 
 > Any value of Delta I chosen: 1000,1,0,1 0.00000000001, the outcome is NULL.
 > 
 > Any help or suggestions that you can provide will be greatly appreciated.
 > 
 > Qing
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
 > and provide commented, minimal, self-contained, reproducible code.


From yanniggoude at yahoo.fr  Tue Jan 23 10:36:00 2007
From: yanniggoude at yahoo.fr (yannig goude)
Date: Tue, 23 Jan 2007 10:36:00 +0100 (CET)
Subject: [R] SARIMA with dynlm
Message-ID: <630702.22097.qm@web26714.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070123/d67f0b75/attachment.pl 

From Achim.Zeileis at wu-wien.ac.at  Tue Jan 23 11:29:55 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 23 Jan 2007 11:29:55 +0100 (CET)
Subject: [R] SARIMA with dynlm
In-Reply-To: <630702.22097.qm@web26714.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.44.0701231125270.10308-100000@disco.wu-wien.ac.at>

Yannig:

> Does anyone have an exemple of how to fit a SARIMA model , with a MA
> part, with the package dynlm?

This is not yet possible. Currently, dynlm() just offers the functionality
of lm() for time series data, i.e., OLS for auto-regressive models. I hope
to interface arima() one day as well, but I haven't started working on
this. So currently, just use arima() directly.

hth,
Z


From RAJDL at fnplzen.cz  Tue Jan 23 11:52:43 2007
From: RAJDL at fnplzen.cz (Rajdl Daniel)
Date: Tue, 23 Jan 2007 11:52:43 +0100
Subject: [R] Error in odfWeave
Message-ID: <DB879673B6E2E84C8517AB9A25C3706F0516711D@erika.fnplzen.cz>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070123/719d194a/attachment.pl 

From justin_bem at yahoo.fr  Tue Jan 23 12:03:15 2007
From: justin_bem at yahoo.fr (justin bem)
Date: Tue, 23 Jan 2007 11:03:15 +0000 (GMT)
Subject: [R] Problem with ordered probit model in MASS
Message-ID: <262863.91311.qm@web23014.mail.ird.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070123/4ac1c973/attachment.pl 

From mucha at nihon.no  Tue Jan 23 11:12:21 2007
From: mucha at nihon.no (Mucha)
Date: Tue, 23 Jan 2007 11:12:21 +0100
Subject: [R] Calling R function from C++ when C++ was not invoced by R,
	or	Calling R from PL/SQL
Message-ID: <WorldClient-F200701231112.AA12210146@nihon.no>

I have what many might consider a strange problem. I have about 250 lines
of R code that I need to use in my system. (Need because I do not
understand it enough to translate it to anything else). 

The final purpose is to use it from a PL/SQL function in Oracle 10g. The
previous version of this system called it directly from the DB
(PostgreSQL) in form of PL/R, I haven't found Oracle to have this
capability. So I'm thinking that the solution might be to make a short C++
wrapper for the R function. 

Looking at the documentation I was only able to run R functions in C++ if
the C++ function was first invoked by R, because I needed the environment
and had trouble with casting the result to something comprehend able by C++.

So my two questions is:
1) Is it possible to call a R function from PL/SQL, and how?
2) Is it possible to call a R function from C++ without the C++ function
being invoked by R, and how? 

I hope some one is able to point me in the right directions. 

yours sincerely

Marius


From phhs80 at gmail.com  Tue Jan 23 12:13:28 2007
From: phhs80 at gmail.com (Paul Smith)
Date: Tue, 23 Jan 2007 11:13:28 +0000
Subject: [R] Question about rpart and regression trees
In-Reply-To: <Pine.LNX.4.64.0701230637030.29155@gannet.stats.ox.ac.uk>
References: <6ade6f6c0701221426n6abf43acx31caa82edd339ee9@mail.gmail.com>
	<Pine.LNX.4.64.0701230637030.29155@gannet.stats.ox.ac.uk>
Message-ID: <6ade6f6c0701230313ued97c26sab32219432b0dc1c@mail.gmail.com>

On 1/23/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> > I would like to use rpart to obtain a regression tree for a dataset
> > like the following:
> >
> > Y     X1      X2      X3      X4
> > 5.500033      B       A       3       2
> > 0.35625148    D       B       6       5
> > 0.8062546     E       C       4       3
> > 5.100014      C       A       3       2
> > 5.7000422     A       A       3       2
> > 0.76875436    C       A       6       5
> > 1.0312537     D       A       4       1
> >
> > Y is the objective variable. X1, X2, X3 and X4 can take, respectively,
> > the following values:
> >
> > X1: A,B,C,D,E
> > X2: A,B,C,D,E
> > X3: 3,4,5,6
> > X4. 1,2,3,4,5
> >
> > Should I convert X3 and X4 to factor before running rpart?
>
> If they really are factors, yes.
> If they are ordered factors, no.

Thanks, Prof. Ripley. Is it correct to adopt the same procedure in
case of classification trees, i.e., in case the objective variable (Y)
is categorical and X1, X2, X3 and X4 are as above?

Paul


From enzo83 at wp.pl  Mon Jan 22 23:16:18 2007
From: enzo83 at wp.pl (Jakub Jurdziak)
Date: Mon, 22 Jan 2007 23:16:18 +0100
Subject: [R] [R-pkgs] eval() parse() and problem with square brackets
Message-ID: <45B537B2.70807@wp.pl>

Hello,

i have problem with the following code (I'm using sqlQuery function from 
RODBC package):
eval(parse(text="g_1 <- sqlQuery(cnn_1, \"select aa from 
bb.[cc\\dd].ee\")")).

I get the error message:
"[RODBC] ERROR: Could not SQLExecDirect" 

"S0002 208 [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid object 
name 'bb.cc\dd.ee'."

It seems that R is replacing square brackets that are needed for 
database to execute query.

How can I force R to change its behavior and leave square brackets 
unchanged?

Any ideas appreciated

Kuba

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From attenka at utu.fi  Tue Jan 23 12:38:30 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Tue, 23 Jan 2007 13:38:30 +0200
Subject: [R] Scatterplot3d-proposal
Message-ID: <f739d89b169b1.45b60fd6@utu.fi>

Hi,

Does somebody know if there is some trick to betoken the starting points ("on the floor") of the vertical lines in scatterplot3d-plots.
I'd prefer something like that one in the example here

http://users.utu.fi/attenka/proposal3d.jpg

If this is not possible, could this option be added? Sometimes it is not easy to distinguish objects behind the others and for instance in this example picture the case nr. 21 is important.

Atte Tenkanen
University of Turku, Finland


From ccleland at optonline.net  Tue Jan 23 12:52:45 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 23 Jan 2007 06:52:45 -0500
Subject: [R] Repeated measures
In-Reply-To: <70B0CAB623BFDA448F85DA2AFEE8BD42014BDDB6@EXCHANGE.plantsciences.ucdavis.edu>
References: <70B0CAB623BFDA448F85DA2AFEE8BD42014BDDB6@EXCHANGE.plantsciences.ucdavis.edu>
Message-ID: <45B5F70D.8000705@optonline.net>

Richard Plant wrote:
> In the two solutions for the repeated measures problem given in the
> original reply below, the F and p values given by aov() with the error
> strata defined by Error() are different from those given by lme().
> However, when one does the problem "by hand" using the standard split
> plot model, the results agree with those of nlme(). The difference
> between the two aov() solutions is in the partitioning of sums of
> squares. Is there a ready explanation for this discrepancy?

  The discrepancy in this case is due to a mistake on my part.  The id
variable should be a factor.

tolerance <-
read.table("http://www.ats.ucla.edu/stat/Splus/examples/alda/tolerance1.txt",
            sep=",", header=TRUE)

tolerance.long <- reshape(tolerance,
                          varying = list(c("tol11","tol12","tol13",
                                           "tol14", "tol15")),
                          v.names = c("tol"), timevar = "time",
                          times = 11:15, direction = "long")

tolerance.aov <- aov(tol ~ factor(time) * male + Error(factor(id)),
                     data = tolerance.long)

summary(tolerance.aov)

Error: factor(id)
          Df Sum Sq Mean Sq F value Pr(>F)
male       1 0.3599  0.3599  0.6071 0.4488
Residuals 14 8.2990  0.5928

Error: Within
                  Df Sum Sq Mean Sq F value   Pr(>F)
factor(time)       4 2.8326  0.7081  5.1309 0.001358 **
factor(time):male  4 0.1869  0.0467  0.3386 0.850786
Residuals         56 7.7289  0.1380
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

library(nlme)

tolerance.lme <- lme(tol ~ as.factor(time) * male, random = ~ 1 | id,
                     data = tolerance.long)

anova(tolerance.lme)
                     numDF denDF  F-value p-value
(Intercept)              1    56 353.9049  <.0001
as.factor(time)          4    56   5.1309  0.0014
male                     1    14   0.6071  0.4488
as.factor(time):male     4    56   0.3386  0.8508

  Could anyone point me to an example where the variable specified
inside of Error() is not a factor?

> Thanks,
> Richard Plant
> 
>> tolerance <- tolerance <-
> +
> read.table("http://www.ats.ucla.edu/stat/Splus/examples/alda/tolerance1.
> txt",
> +             sep=",", header=TRUE)
>> tolerance.long <- reshape(tolerance,
> +                           varying = list(c("tol11","tol12","tol13",
> +                                            "tol14", "tol15")),
> +                           v.names = c("tol"), timevar = "time",
> +                           times = 11:15, direction = "long")
>> tolerance.aov2 <- aov(tol ~ factor(male) + factor(male):factor(id) +
> factor(time) + factor(time):male, data = tolerance.long)
>> tolerance.sum <- summary(tolerance.aov2)
>> tolerance.sum
>                         Df Sum Sq Mean Sq F value    Pr(>F)    
> factor(male)             1 0.3599  0.3599  2.6077  0.111967    
> factor(time)             4 2.8326  0.7081  5.1309  0.001358 ** 
> factor(male):factor(id) 14 8.2990  0.5928  4.2951 4.295e-05 ***
> factor(time):male        4 0.1869  0.0467  0.3386  0.850786    
> Residuals               56 7.7289  0.1380                      
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>> tolerance.list <- tolerance.sum[[1]]
>> tolerance.mat <- as.matrix(tolerance.list[3])
>> tolerance.F.male <- tolerance.mat[1,1]/tolerance.mat[3,1]
>> tolerance.F.male
> [1] 0.607137
>> tolerance.df <- as.matrix(tolerance.list[1])
>> tolerance.p.male <- 1 -
> pf(tolerance.F.male,tolerance.df[1,1],tolerance.df[3,1])
>> tolerance.p.male
> [1] 0.4488394
>> Message: 68
>> Date: Wed, 17 Jan 2007 05:45:01 -0500
>> From: Chuck Cleland <ccleland at optonline.net>
>> Subject: Re: [R] Repeated measures
>> To: Tom Backer Johnsen <backer at psych.uib.no>
>> Cc: r-help at stat.math.ethz.ch
>> Message-ID: <45ADFE2D.2060208 at optonline.net>
>> Content-Type: text/plain; charset=ISO-8859-1
>>
>> Tom Backer Johnsen wrote:
>>> I am having a hard time understanding how to perform a "repeated
>>> measures" type of ANOVA with R.  When reading the document found
> here:
>>> http://cran.r-project.org/doc/contrib/Lemon-kickstart/kr_repms.html
>>>
>>> I find that there is a reference to a function make.rm () that is
>>> supposed to rearrange a "one row per person" type of frame to a "one
>>> row per observation" type of frame.  But that function does not seem
>>> to be there.  Nor does the help.search suggest anything.  Is that
>>> function buried in some package?
>>   I'm not able to find that function.  Perhaps that document is out of
>> date.
>>
>>> Is there  some simple documentation that might be useful somewhere?
>>> Starting with a really simple problem (one group, two observations)?
>>   Here is an example showing the use of reshape() and analysis via
> aov()
>> and lme() in the nlme package.
>>
>> tolerance <-
>>
> read.table("http://www.ats.ucla.edu/stat/Splus/examples/alda/tolerance1.
> tx
>> t",
>>             sep=",", header=TRUE)
>>
>> tolerance.long <- reshape(tolerance,
>>                           varying = list(c("tol11","tol12","tol13",
>>                                            "tol14", "tol15")),
>>                           v.names = c("tol"), timevar = "time",
>>                           times = 11:15, direction = "long")
>>
>> tolerance.aov <- aov(tol ~ as.factor(time) * male + Error(id),
>>                      data = tolerance.long)
>>
>> summary(tolerance.aov)
>>
>> Error: id
>>      Df   Sum Sq  Mean Sq
>> male  1 0.085168 0.085168
>>
>> Error: Within
>>                      Df  Sum Sq Mean Sq F value  Pr(>F)
>> as.factor(time)       4  2.8326  0.7081  3.0538 0.02236 *
>> male                  1  0.3024  0.3024  1.3039 0.25745
>> as.factor(time):male  4  0.1869  0.0467  0.2015 0.93670
>> Residuals            69 16.0002  0.2319
>> ---
>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>
>> library(nlme)
>>
>> tolerance.lme <- lme(tol ~ as.factor(time) * male, random = ~ 1 | id,
>>                      data = tolerance.long)
>>
>> anova(tolerance.lme)
>>                      numDF denDF  F-value p-value
>> (Intercept)              1    56 353.9049  <.0001
>> as.factor(time)          4    56   5.1309  0.0014
>> male                     1    14   0.6071  0.4488
>> as.factor(time):male     4    56   0.3386  0.8508
>>
>>   RSiteSearch("repeated measures") points to other examples,
> functions,
>> and documentation.
>>
>>> Tom
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> --
>> Chuck Cleland, Ph.D.
>> NDRI, Inc.
>> 71 West 23rd Street, 8th floor
>> New York, NY 10010
>> tel: (212) 845-4495 (Tu, Th)
>> tel: (732) 512-0171 (M, W, F)
>> fax: (917) 438-0894
>>
>>
>>
>> ------------------------------
>>
>> _______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From murdoch at stats.uwo.ca  Tue Jan 23 13:01:40 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 23 Jan 2007 07:01:40 -0500
Subject: [R] Scatterplot3d-proposal
In-Reply-To: <f739d89b169b1.45b60fd6@utu.fi>
References: <f739d89b169b1.45b60fd6@utu.fi>
Message-ID: <45B5F924.4090800@stats.uwo.ca>

On 1/23/2007 6:38 AM, Atte Tenkanen wrote:
> Hi,
> 
> Does somebody know if there is some trick to betoken the starting points ("on the floor") of the vertical lines in scatterplot3d-plots.
> I'd prefer something like that one in the example here
> 
> http://users.utu.fi/attenka/proposal3d.jpg
> 
> If this is not possible, could this option be added? Sometimes it is not easy to distinguish objects behind the others and for instance in this example picture the case nr. 21 is important.

scatterplot3d() returns a list of functions which could help with this, 
in particular the points3d function.

Duncan Murdoch


From ligges at statistik.uni-dortmund.de  Tue Jan 23 13:40:48 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 23 Jan 2007 13:40:48 +0100
Subject: [R] Scatterplot3d-proposal
In-Reply-To: <45B5F924.4090800@stats.uwo.ca>
References: <f739d89b169b1.45b60fd6@utu.fi> <45B5F924.4090800@stats.uwo.ca>
Message-ID: <45B60250.1000507@statistik.uni-dortmund.de>



Duncan Murdoch wrote:
> On 1/23/2007 6:38 AM, Atte Tenkanen wrote:
>> Hi,
>>
>> Does somebody know if there is some trick to betoken the starting points ("on the floor") of the vertical lines in scatterplot3d-plots.
>> I'd prefer something like that one in the example here
>>
>> http://users.utu.fi/attenka/proposal3d.jpg
>>
>> If this is not possible, could this option be added? Sometimes it is not easy to distinguish objects behind the others and for instance in this example picture the case nr. 21 is important.
> 
> scatterplot3d() returns a list of functions which could help with this, 
> in particular the points3d function.

Right, or rather use the returned function xyz.convert, as in:

s3d <- scatterplot3d(1:10, 1:10, 1:10, type="h")
s3d$xyz.convert(5, 5, 0)

Now you got coordinates for the point on the plane with z=0 projected 
from (5,5,5). It should be easy now to add circles, ellipses or whatever 
you like.

Uwe Ligges


> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From powlekwuem at savagecanada.com  Tue Jan 23 14:53:31 2007
From: powlekwuem at savagecanada.com (Mahomet Hollowell)
Date: Tue, 23 Jan 2007 10:53:31 -0300
Subject: [R] RXvisua
Message-ID: <01c73eed$7c38d8a0$8800a8c0@maq3>

Good day,

CIA_ALIS $3, 75
VAL_LIUM $1, 30
VIA_AGRA $3, 35
AMB_BIEN $2, 90
SO_MMA   $1, 15

http://www.22rx*com ( Important! Replace "*" with "." )

--
and Hermione sniping at each other over their homework in the common
room that he took Siriuss food up to the Owlery that evening on his own.
Pigwidgeon was much too small to carry an entire ham up to the mountain


From francogrex at mail.com  Tue Jan 23 13:57:05 2007
From: francogrex at mail.com (francogrex)
Date: Tue, 23 Jan 2007 04:57:05 -0800 (PST)
Subject: [R] automated integration?
Message-ID: <8521241.post@talk.nabble.com>


When I run the script below it works well it ouputs the result that I need,
in this case were n=45 and e=5.02689 the result is: 6.669578
-----------------------------------------------
dens<-function(x){
n=45
e=5.02689
a1=0.0987 
b1=0.04261
a2=1.043
b2=1.222
p=0.121
if (n>200)  c((n=n/2),(e=e/2))
if (e>30)  c((n=n/2),(e=e/2))
lpp<-((lgamma(a1+n)))-(lgamma(a1)+lfactorial(n)+log((1+(e/b1))^a1)+log((1+(b1/e))^n))
lzz<-((lgamma(a2+n)))-(lgamma(a2)+lfactorial(n)+log((1+(e/b2))^a2)+log((1+(b2/e))^n))
pp<-exp(lpp)
zz<-exp(lzz)
qq= (p*pp)/((p*pp)+((1-p)*zz))
lgam<-(log((b1+e)^(a1+n)) + log(x^((a1+n)-1)))   - (lgamma((a1+n))   +
((b1+e)*x))
lgom<-(log((b2+e)^(a2+n)) + log(x^((a2+n)-1)))   - (lgamma((a2+n))   +
((b2+e)*x))
gam<-exp(lgam)
gom<-exp(lgom)
(qq*gam)  + ((1-qq)*gom)
}
integ<-function(x){
n=45
e=5.02689
if (x>(n/e)) return (x=Inf) 
if (x<0) return (x=0) 
u<-integrate(dens,lower=0, upper=x)$value
u-0.05
}
uni<-uniroot(integ,c(0,100), tol=1e-10)
uni$root
-----------------------------------------------------------------

But what if n and e are vectors (like: n=c(31,22,47,38) and
e=c(5.2,2.8,3.4,2.1)? can I have a string of results all at once instead of
entering the values of n and e one by one it takes such a long time
especially if the n and e contain a couple of 100 values!

Any suggestions are appreciated. Thanks
-- 
View this message in context: http://www.nabble.com/automated-integration--tf3063979.html#a8521241
Sent from the R help mailing list archive at Nabble.com.


From jzhang1982 at gmail.com  Tue Jan 23 14:20:53 2007
From: jzhang1982 at gmail.com (Zhang Jian)
Date: Tue, 23 Jan 2007 21:20:53 +0800
Subject: [R] "tapply" and "data.frame"?
Message-ID: <3f2938d50701230520l28291a82v8343fb9f06dbda2d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070123/b3ed7698/attachment.pl 

From attenka at utu.fi  Tue Jan 23 14:29:02 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Tue, 23 Jan 2007 15:29:02 +0200
Subject: [R] Scatterplot3d-proposal
In-Reply-To: <45B60250.1000507@statistik.uni-dortmund.de>
References: <f739d89b169b1.45b60fd6@utu.fi> <45B5F924.4090800@stats.uwo.ca>
	<45B60250.1000507@statistik.uni-dortmund.de>
Message-ID: <fd6fa2e414963.45b629be@utu.fi>


> 
> 
> Duncan Murdoch wrote:
> > On 1/23/2007 6:38 AM, Atte Tenkanen wrote:
> >> Hi,
> >>
> >> Does somebody know if there is some trick to betoken the 
> starting points ("on the floor") of the vertical lines in 
> scatterplot3d-plots.
> >> I'd prefer something like that one in the example here
> >>
> >> http://users.utu.fi/attenka/proposal3d.jpg
> >>
> >> If this is not possible, could this option be added? Sometimes 
> it is not easy to distinguish objects behind the others and for 
> instance in this example picture the case nr. 21 is important.
> > 
> > scatterplot3d() returns a list of functions which could help with 
> this, 
> > in particular the points3d function.
> 
> Right, or rather use the returned function xyz.convert, as in:
> 
> s3d <- scatterplot3d(1:10, 1:10, 1:10, type="h")
> s3d$xyz.convert(5, 5, 0)
> 
> Now you got coordinates for the point on the plane with z=0 
> projected 
> from (5,5,5). It should be easy now to add circles, ellipses or 
> whatever 
> you like.
> 
> Uwe Ligges
> 

Thanks! This seems to work. 

http://users.utu.fi/attenka/ellipses_in3Db.png

I only had to change that (5,5,0) to (5,5,0.4). I'm not sure about the reason, is it because of the minimum of x- or y-coordinate in my plot or what?

Atte


From murdoch at stats.uwo.ca  Tue Jan 23 14:32:03 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 23 Jan 2007 08:32:03 -0500
Subject: [R] Scatterplot3d-proposal
In-Reply-To: <fd6fa2e414963.45b629be@utu.fi>
References: <f739d89b169b1.45b60fd6@utu.fi> <45B5F924.4090800@stats.uwo.ca>
	<45B60250.1000507@statistik.uni-dortmund.de>
	<fd6fa2e414963.45b629be@utu.fi>
Message-ID: <45B60E53.40003@stats.uwo.ca>

On 1/23/2007 8:29 AM, Atte Tenkanen wrote:
>> 
>> 
>> Duncan Murdoch wrote:
>> > On 1/23/2007 6:38 AM, Atte Tenkanen wrote:
>> >> Hi,
>> >>
>> >> Does somebody know if there is some trick to betoken the 
>> starting points ("on the floor") of the vertical lines in 
>> scatterplot3d-plots.
>> >> I'd prefer something like that one in the example here
>> >>
>> >> http://users.utu.fi/attenka/proposal3d.jpg
>> >>
>> >> If this is not possible, could this option be added? Sometimes 
>> it is not easy to distinguish objects behind the others and for 
>> instance in this example picture the case nr. 21 is important.
>> > 
>> > scatterplot3d() returns a list of functions which could help with 
>> this, 
>> > in particular the points3d function.
>> 
>> Right, or rather use the returned function xyz.convert, as in:
>> 
>> s3d <- scatterplot3d(1:10, 1:10, 1:10, type="h")
>> s3d$xyz.convert(5, 5, 0)
>> 
>> Now you got coordinates for the point on the plane with z=0 
>> projected 
>> from (5,5,5). It should be easy now to add circles, ellipses or 
>> whatever 
>> you like.
>> 
>> Uwe Ligges
>> 
> 
> Thanks! This seems to work. 
> 
> http://users.utu.fi/attenka/ellipses_in3Db.png
> 
> I only had to change that (5,5,0) to (5,5,0.4). I'm not sure about the reason, is it because of the minimum of x- or y-coordinate in my plot or what?

It is a point in user coordinates, and the bottom of your box has z=0.4.

Duncan Murdoch


From jholtman at gmail.com  Tue Jan 23 15:05:38 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 23 Jan 2007 09:05:38 -0500
Subject: [R] "tapply" and "data.frame"?
In-Reply-To: <3f2938d50701230520l28291a82v8343fb9f06dbda2d@mail.gmail.com>
References: <3f2938d50701230520l28291a82v8343fb9f06dbda2d@mail.gmail.com>
Message-ID: <644e1f320701230605j62aa2bb7n10e15be3b6dd3041@mail.gmail.com>

Is this what you want:

> tst
   p1   p10  p100 p1000 p1001 p1002 p1003 p1004 p1005 p1006
    1     5     1     8     6     5     8     7     4     4
> data.frame(point=names(tst), ind=tst)
      point ind
p1       p1   1
p10     p10   5
p100   p100   1
p1000 p1000   8
p1001 p1001   6
p1002 p1002   5
p1003 p1003   8
p1004 p1004   7
p1005 p1005   4
p1006 p1006   4
>


On 1/23/07, Zhang Jian <jzhang1982 at gmail.com> wrote:
> I want to transform the data by "tapply" to one dataframe. But I can not get
> it.
> For example:
> > tst=tapply(point,pp,length)
> > tst[1:10]
>  p1   p10 p100 p1000 p1001 p1002 p1003 p1004 p1005 p1006
>  1   5   1   8   6   5   8   7   4   4
> > res=as.data.frame(tst)  # I try to transform it
> > res[1:10,]
>  p1   p10 p100 p1000 p1001 p1002 p1003 p1004 p1005 p1006
>  1   5   1   8   6   5   8   7   4   4
> How to transfrom it like the following:
> >res
> point ind
> 1   p1   1
> 2   p10   5
> 3 p100   1
> 4 p1000   8
> 5 p1001   6
>
> Thanks!
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From shubhak at ambaresearch.com  Tue Jan 23 15:10:56 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Tue, 23 Jan 2007 19:40:56 +0530
Subject: [R] Vector to Matrix transformation
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3DDB2DE@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070123/ad407adf/attachment.pl 

From P.Dalgaard at biostat.ku.dk  Tue Jan 23 15:28:03 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 23 Jan 2007 15:28:03 +0100
Subject: [R] Vector to Matrix transformation
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3DDB2DE@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC3DDB2DE@BAN-MAILSRV03.Amba.com>
Message-ID: <45B61B73.9010505@biostat.ku.dk>

Shubha Vishwanath Karanth wrote:
> Hi R,
>
>  
>
> I have a vector V1 of unknown length, say n. I need to convert this into
> a matrix C of row size=5, and accordingly the column should be updated.
> I tried with:
>
>  
>
> C=as.matrix(V1,5,n/5)
>
>  
>
> But it is not working...Could somebody help me on this?
matrix(), not as.matrix()

The help page for as.matrix displays quite clearly that as.matrix takes
only a single argument x.

You can also set the dimensions using dim(V1) <- c(5, n/5)

>  
>
> Thanks in advance...
>
>  
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ccleland at optonline.net  Tue Jan 23 15:29:35 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 23 Jan 2007 09:29:35 -0500
Subject: [R] Vector to Matrix transformation
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3DDB2DE@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC3DDB2DE@BAN-MAILSRV03.Amba.com>
Message-ID: <45B61BCF.2070607@optonline.net>

Shubha Vishwanath Karanth wrote:
> Hi R,
> 
> I have a vector V1 of unknown length, say n. I need to convert this into
> a matrix C of row size=5, and accordingly the column should be updated.
> I tried with:
> 
> C=as.matrix(V1,5,n/5)
> 
> But it is not working...Could somebody help me on this?

  You could try the following:

matrix(V1, nrow=5)

  but note what happens when the length of V1 is not a multiple of 5.

> Thanks in advance...
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From dimitris.rizopoulos at med.kuleuven.be  Tue Jan 23 15:34:41 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 23 Jan 2007 15:34:41 +0100
Subject: [R] Vector to Matrix transformation
References: <A36876D3F8A5734FA84A4338135E7CC3DDB2DE@BAN-MAILSRV03.Amba.com>
Message-ID: <00f401c73efb$9e6b6420$0540210a@www.domain>

check the help page for ?matrix(); you probably want either

matrix(V1, nrow = 5)

or

matrix(V1, nrow = 5, byrow = TRUE)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Shubha Vishwanath Karanth" <shubhak at ambaresearch.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 23, 2007 3:10 PM
Subject: [R] Vector to Matrix transformation


> Hi R,
>
>
>
> I have a vector V1 of unknown length, say n. I need to convert this 
> into
> a matrix C of row size=5, and accordingly the column should be 
> updated.
> I tried with:
>
>
>
> C=as.matrix(V1,5,n/5)
>
>
>
> But it is not working...Could somebody help me on this?
>
>
>
> Thanks in advance...
>
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From HDoran at air.org  Tue Jan 23 16:21:52 2007
From: HDoran at air.org (Doran, Harold)
Date: Tue, 23 Jan 2007 10:21:52 -0500
Subject: [R] Matrix operations in a list
Message-ID: <2323A6D37908A847A7C32F1E3662C80E8D44DD@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070123/9b6dc8f0/attachment.pl 

From helmut.schuetz at bebac.at  Tue Jan 23 16:36:04 2007
From: helmut.schuetz at bebac.at (=?ISO-8859-1?Q?Helmut_Sch=FCtz?=)
Date: Tue, 23 Jan 2007 16:36:04 +0100
Subject: [R] How to generate 'minor' ticks in lattice (qqmath)
Message-ID: <45B62B64.9060700@bebac.at>

Dear group,

I tried to generate labels for every second tick in lattice (qqmath).
Version: 0.14-16
Date: 2006/12/01

R version 2.4.1 (2006-12-18)

An example:

library(lattice)
numy=100
y=runif(numy,min=0,max=1)
sig=0.05
numsig=length(which(y<sig))

tck.no=11 # number of ticks
tcks=1:tck.no
labl=as.character(0.1*tcks-0.1) # label for all ticks
labl[seq(2,tck.no-1,2)]="" # delete every second label
nums=seq(0,(tck.no-1)/10,(tck.no-1)/100)

qqmath(y,distribution=qunif,
  prepanel=NULL,
  panel=function(x)
    {
    panel.abline(c(0,1),lty=2)
    
panel.polygon(c(0,0,numsig/numy,numsig/numy,0),c(0,sig,sig,0,0),lwd=0.75)
    panel.qqmath(x,distribution=qunif,col=1)
    },
    scales=list(
      tck=c(1,0),
      x=list(at=nums,labels=labl),
      cex=0.8),
    xlab=paste("uniform [0,1] quantiles"),
    ylab="runif [0,1]",
    min=0,max=1)

Two questions:
1. How do I define the same labels for the second (left) axis (I suspect 
a second dimension in scales(list(x=list...) would do the job, but I 
haven't been able finding out, how...
2. Is there any alternative to obtain minor ticks in lattice like 
"minor.ticks" in Hmisc?

Best regards,
Helmut

-- 
Helmut Sch?tz
BEBAC
Consultancy Services for Bioequivalence and Bioavailability Studies
Neubaugasse 36/11
1070 Vienna/Austria
tel/fax +43 1 2311746
Web http://BEBAC.at
BE/BA Forum http://forum.bebac.at
http://www.goldmark.org/netrants/no-word/attach.html


From nitin.jindal at gmail.com  Tue Jan 23 16:39:26 2007
From: nitin.jindal at gmail.com (nitin jindal)
Date: Tue, 23 Jan 2007 09:39:26 -0600
Subject: [R] "Divergence or Singularity"
Message-ID: <2b808b010701230739s3596cda5mc52b82636b09ba8b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070123/9634e310/attachment.pl 

From marc_schwartz at comcast.net  Tue Jan 23 16:43:53 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 23 Jan 2007 09:43:53 -0600
Subject: [R] Matrix operations in a list
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E8D44DD@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E8D44DD@dc1ex01.air.org>
Message-ID: <1169567033.4816.13.camel@localhost.localdomain>

On Tue, 2007-01-23 at 10:21 -0500, Doran, Harold wrote:
> I have matrices stored within a list like something as follows:
> 
> a <- list(matrix(rnorm(50), ncol=5), matrix(rnorm(50), ncol=5))
> b <- list(matrix(rnorm(50), nrow=5), matrix(rnorm(50), nrow=5))
> 
> I don't recall how to perform matrix multiplication on each list element
> such that the result is a new list 
> 
> result <- list(a[[1]]%*%b[[1]], a[[2]]%*%b[[2]])
> 
> I think I'm close with mapply(), but I'm doing something silly
> 
> mapply('%*%', a,b)
> 
> Thanks.
> Harold

Harold,

That should basically be working.  Just note that by default, each
resultant matrix is put into a column (vector) format, rather than as a
matrix.

 Res <- mapply("%*%", a, b)
 Res1 <- a[[1]] %*% b[[1]]
 Res2 <- a[[2]] %*% b[[2]]


> str(Res)
 num [1:100, 1:2]  0.1713  0.8290 -0.0864  3.5420 -1.4638 ...
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : NULL

> all.equal(Res[, 1], as.vector(Res1))
[1] TRUE

> all.equal(Res[, 2], as.vector(Res2))
[1] TRUE

If you want the results to be a list of two matrices, you would do
something like:

Res <- mapply("%*%", a, b, SIMPLIFY = FALSE)

> str(Res)
List of 2
 $ : num [1:10, 1:10]  0.1713  0.8290 -0.0864  3.5420 -1.4638 ...
 $ : num [1:10, 1:10]  0.220 -2.048 -0.135 -2.121 -0.399 ...

> all.equal(Res1, Res[[1]])
[1] TRUE

> all.equal(Res2, Res[[2]])
[1] TRUE


HTH,

Marc Schwartz


From ggrothendieck at gmail.com  Tue Jan 23 16:44:19 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 23 Jan 2007 10:44:19 -0500
Subject: [R] How to generate 'minor' ticks in lattice (qqmath)
In-Reply-To: <45B62B64.9060700@bebac.at>
References: <45B62B64.9060700@bebac.at>
Message-ID: <971536df0701230744i19479d4cq213b85b229f63b57@mail.gmail.com>

Another approach is shown in the last example at the bottom of:

library(zoo)
?xyplot.zoo

On 1/23/07, Helmut Sch?tz <helmut.schuetz at bebac.at> wrote:
> Dear group,
>
> I tried to generate labels for every second tick in lattice (qqmath).
> Version: 0.14-16
> Date: 2006/12/01
>
> R version 2.4.1 (2006-12-18)
>
> An example:
>
> library(lattice)
> numy=100
> y=runif(numy,min=0,max=1)
> sig=0.05
> numsig=length(which(y<sig))
>
> tck.no=11 # number of ticks
> tcks=1:tck.no
> labl=as.character(0.1*tcks-0.1) # label for all ticks
> labl[seq(2,tck.no-1,2)]="" # delete every second label
> nums=seq(0,(tck.no-1)/10,(tck.no-1)/100)
>
> qqmath(y,distribution=qunif,
>  prepanel=NULL,
>  panel=function(x)
>    {
>    panel.abline(c(0,1),lty=2)
>
> panel.polygon(c(0,0,numsig/numy,numsig/numy,0),c(0,sig,sig,0,0),lwd=0.75)
>    panel.qqmath(x,distribution=qunif,col=1)
>    },
>    scales=list(
>      tck=c(1,0),
>      x=list(at=nums,labels=labl),
>      cex=0.8),
>    xlab=paste("uniform [0,1] quantiles"),
>    ylab="runif [0,1]",
>    min=0,max=1)
>
> Two questions:
> 1. How do I define the same labels for the second (left) axis (I suspect
> a second dimension in scales(list(x=list...) would do the job, but I
> haven't been able finding out, how...
> 2. Is there any alternative to obtain minor ticks in lattice like
> "minor.ticks" in Hmisc?
>
> Best regards,
> Helmut
>
> --
> Helmut Sch?tz
> BEBAC
> Consultancy Services for Bioequivalence and Bioavailability Studies
> Neubaugasse 36/11
> 1070 Vienna/Austria
> tel/fax +43 1 2311746
> Web http://BEBAC.at
> BE/BA Forum http://forum.bebac.at
> http://www.goldmark.org/netrants/no-word/attach.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From christos at nuverabio.com  Tue Jan 23 16:46:35 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Tue, 23 Jan 2007 10:46:35 -0500
Subject: [R] Matrix operations in a list
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E8D44DD@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E8D44DD@dc1ex01.air.org>
Message-ID: <002c01c73f05$aa3a6ad0$0e010a0a@headquarters.silicoinsights>

Try,

mapply('%*%', a, b, SIMPLIFY=FALSE)

-Christos 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Doran, Harold
Sent: Tuesday, January 23, 2007 10:22 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Matrix operations in a list

I have matrices stored within a list like something as follows:

a <- list(matrix(rnorm(50), ncol=5), matrix(rnorm(50), ncol=5)) b <-
list(matrix(rnorm(50), nrow=5), matrix(rnorm(50), nrow=5))

I don't recall how to perform matrix multiplication on each list element
such that the result is a new list 

result <- list(a[[1]]%*%b[[1]], a[[2]]%*%b[[2]])

I think I'm close with mapply(), but I'm doing something silly

mapply('%*%', a,b)

Thanks.
Harold


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From wmyonwesit at gmail.com  Tue Jan 23 16:46:51 2007
From: wmyonwesit at gmail.com (myat wai)
Date: Tue, 23 Jan 2007 15:46:51 +0000
Subject: [R] how to write randomforest in r
Message-ID: <6c1e1b7f0701230746h7c829dd4s82223486574ec496@mail.gmail.com>

Dear Sir,
I want to know how to do for getting the results.
1. data set is not in r.
how to use my data set in r.
2. using randomForest function to build tree with my data set
how to write for it
3. using this random forest how to predict the new data
Please reply me.I want to bulid random forest in r and predict the new
data. My data set is in the attachment file.Like my attachment file,I
want to get the results in R  as the output.
Please help me.
Your Sincerely,
Myat

From dimitris.rizopoulos at med.kuleuven.be  Tue Jan 23 16:47:49 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 23 Jan 2007 16:47:49 +0100
Subject: [R] Matrix operations in a list
References: <2323A6D37908A847A7C32F1E3662C80E8D44DD@dc1ex01.air.org>
Message-ID: <011b01c73f05$d56f0e40$0540210a@www.domain>

you need the 'SIMPLIFY' argument of mapply(), i.e.,

mapply("%*%", a, b, SIMPLIFY = FALSE)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm

----- Original Message ----- 
From: "Doran, Harold" <HDoran at air.org>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 23, 2007 4:21 PM
Subject: [R] Matrix operations in a list


>I have matrices stored within a list like something as follows:
>
> a <- list(matrix(rnorm(50), ncol=5), matrix(rnorm(50), ncol=5))
> b <- list(matrix(rnorm(50), nrow=5), matrix(rnorm(50), nrow=5))
>
> I don't recall how to perform matrix multiplication on each list 
> element
> such that the result is a new list
>
> result <- list(a[[1]]%*%b[[1]], a[[2]]%*%b[[2]])
>
> I think I'm close with mapply(), but I'm doing something silly
>
> mapply('%*%', a,b)
>
> Thanks.
> Harold
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From enzo83 at wp.pl  Tue Jan 23 17:29:32 2007
From: enzo83 at wp.pl (Jakub Jurdziak)
Date: Tue, 23 Jan 2007 17:29:32 +0100
Subject: [R] eval() parse() and problem with square brackets
Message-ID: <45B637EC.4010307@wp.pl>

Hello,

i have problem with the following code (I'm using sqlQuery function from 
RODBC package):
eval(parse(text="g_1 <- sqlQuery(cnn_1, \"select aa from 
bb.[cc\\dd].ee\")")).

I get the error message:
"[RODBC] ERROR: Could not SQLExecDirect"
"S0002 208 [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid object 
name 'bb.cc\dd.ee'."

It seems that R is replacing square brackets that are needed for 
database to execute query.

How can I force R to change its behavior and leave square brackets 
unchanged?

Any ideas appreciated.

Kuba


From helprhelp at gmail.com  Tue Jan 23 17:54:50 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Tue, 23 Jan 2007 11:54:50 -0500
Subject: [R] which classification package in R can assign sample weight?
Message-ID: <cdf817830701230854p13b570a5r982b17e5150f70eb@mail.gmail.com>

Hi,

I am looking for some function implemented in R for classification,
which has an option to allow me to assign sample weights in learning
process? Implementation of a wrapper function is possible but I am
curious if it already exists somewhere.

Thanks,

-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From chaogai at duineveld.demon.nl  Tue Jan 23 17:59:20 2007
From: chaogai at duineveld.demon.nl (chao gai)
Date: Tue, 23 Jan 2007 17:59:20 +0100
Subject: [R] eval() parse() and problem with square brackets
In-Reply-To: <45B637EC.4010307@wp.pl>
References: <45B637EC.4010307@wp.pl>
Message-ID: <200701231759.21023.chaogai@duineveld.demon.nl>

I am just confused by the eval parse \" and such.
what happens if simplify to :
g_1 <- sqlQuery(cnn_1, "select aa from bb.[cc\\dd].ee")
how would the query look in a native sql program?

Cheers,
Kees

On Tuesday 23 January 2007 17:29, Jakub Jurdziak wrote:
> Hello,
>
> i have problem with the following code (I'm using sqlQuery function from
> RODBC package):
> eval(parse(text="g_1 <- sqlQuery(cnn_1, \"select aa from
> bb.[cc\\dd].ee\")")).
>
> I get the error message:
> "[RODBC] ERROR: Could not SQLExecDirect"
> "S0002 208 [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid object
> name 'bb.cc\dd.ee'."
>
> It seems that R is replacing square brackets that are needed for
> database to execute query.
>
> How can I force R to change its behavior and leave square brackets
> unchanged?
>
> Any ideas appreciated.
>
> Kuba
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.


From ggrothendieck at gmail.com  Tue Jan 23 18:06:33 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 23 Jan 2007 12:06:33 -0500
Subject: [R] eval() parse() and problem with square brackets
In-Reply-To: <200701231759.21023.chaogai@duineveld.demon.nl>
References: <45B637EC.4010307@wp.pl>
	<200701231759.21023.chaogai@duineveld.demon.nl>
Message-ID: <971536df0701230906r33a4d3fs786908c252bd547d@mail.gmail.com>

If the question is how to represent a character string containing double
quotes without using \" then just use single quotes for the entire string
instead:

'He said "dog" and she said "cat".'

On 1/23/07, chao gai <chaogai at duineveld.demon.nl> wrote:
> I am just confused by the eval parse \" and such.
> what happens if simplify to :
> g_1 <- sqlQuery(cnn_1, "select aa from bb.[cc\\dd].ee")
> how would the query look in a native sql program?
>
> Cheers,
> Kees
>
> On Tuesday 23 January 2007 17:29, Jakub Jurdziak wrote:
> > Hello,
> >
> > i have problem with the following code (I'm using sqlQuery function from
> > RODBC package):
> > eval(parse(text="g_1 <- sqlQuery(cnn_1, \"select aa from
> > bb.[cc\\dd].ee\")")).
> >
> > I get the error message:
> > "[RODBC] ERROR: Could not SQLExecDirect"
> > "S0002 208 [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid object
> > name 'bb.cc\dd.ee'."
> >
> > It seems that R is replacing square brackets that are needed for
> > database to execute query.
> >
> > How can I force R to change its behavior and leave square brackets
> > unchanged?
> >
> > Any ideas appreciated.
> >
> > Kuba
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented, minimal,
> > self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From lalithaviswanath at yahoo.com  Tue Jan 23 18:28:12 2007
From: lalithaviswanath at yahoo.com (lalitha viswanath)
Date: Tue, 23 Jan 2007 09:28:12 -0800 (PST)
Subject: [R] Query about extracting subsets from a table
Message-ID: <474740.32935.qm@web43127.mail.sp1.yahoo.com>

Hi
I am trying to process tabular data as follows:

Data in the input file is of the form

genome1 genome2 tree-dist log10escore

Genome1 and genome2 are alphabetic.
Tree-dist and log10escore are numeric.

I wish to extract only those  rows from this table
where the log10escore is less than -3.


data <-read.table(filename);
data$log10escore = data$log10escore[ data$log10escore
< -3];

I would like to use this pruned list of escores to get
the corresponding genomenames and treedist.

I did not find anything useful in the FAQs and Notes
on R for this part of the data extraction.

As I am just beginning programming in R, I would
appreciate your input about this.

Thanks
L


 
____________________________________________________________________________________
Food fight? Enjoy some healthy debate


From ccleland at optonline.net  Tue Jan 23 18:37:15 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 23 Jan 2007 12:37:15 -0500
Subject: [R] Query about extracting subsets from a table
In-Reply-To: <474740.32935.qm@web43127.mail.sp1.yahoo.com>
References: <474740.32935.qm@web43127.mail.sp1.yahoo.com>
Message-ID: <45B647CB.4000500@optonline.net>

lalitha viswanath wrote:
> Hi
> I am trying to process tabular data as follows:
> 
> Data in the input file is of the form
> 
> genome1 genome2 tree-dist log10escore
> 
> Genome1 and genome2 are alphabetic.
> Tree-dist and log10escore are numeric.
> 
> I wish to extract only those  rows from this table
> where the log10escore is less than -3.
> 
> 
> data <-read.table(filename);
> data$log10escore = data$log10escore[ data$log10escore
> < -3];

> library(fortunes)
> fortune("dog")

Firstly, don't call your matrix 'matrix'. Would you call your dog 'dog'?
Anyway, it might clash with the function 'matrix'.
   -- Barry Rowlingson
      R-help (October 2004)

> I would like to use this pruned list of escores to get
> the corresponding genomenames and treedist.

?subset

df.sub <- subset(df, log10escore < -3)

summary(df.sub)

> I did not find anything useful in the FAQs and Notes
> on R for this part of the data extraction.
> 
> As I am just beginning programming in R, I would
> appreciate your input about this.
> 
> Thanks
> L
> 
> 
>  
> ____________________________________________________________________________________
> Food fight? Enjoy some healthy debate
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From marc_schwartz at comcast.net  Tue Jan 23 18:48:33 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 23 Jan 2007 11:48:33 -0600
Subject: [R] Query about extracting subsets from a table
In-Reply-To: <474740.32935.qm@web43127.mail.sp1.yahoo.com>
References: <474740.32935.qm@web43127.mail.sp1.yahoo.com>
Message-ID: <1169574513.4816.68.camel@localhost.localdomain>

On Tue, 2007-01-23 at 09:28 -0800, lalitha viswanath wrote:
> Hi
> I am trying to process tabular data as follows:
> 
> Data in the input file is of the form
> 
> genome1 genome2 tree-dist log10escore
> 
> Genome1 and genome2 are alphabetic.
> Tree-dist and log10escore are numeric.
> 
> I wish to extract only those  rows from this table
> where the log10escore is less than -3.
> 
> 
> data <-read.table(filename);
> data$log10escore = data$log10escore[ data$log10escore
> < -3];
> 
> I would like to use this pruned list of escores to get
> the corresponding genomenames and treedist.
> 
> I did not find anything useful in the FAQs and Notes
> on R for this part of the data extraction.
> 
> As I am just beginning programming in R, I would
> appreciate your input about this.
> 
> Thanks
> L

help.search("subset") would lead you to ?subset, where you could do
something like:

DF <- subset(YourData, log10escore < -3)

If you just wanted the values of the two other columns, you could also
use:

DF <- subset(YourData, log10escore < -3, 
             select = c(genomenames, treedist))


One additional alternative is to use which(). This will return the
_indices_ of the values that match the criteria.  For example:

  Ind <- which(YourData$log10escore < -3)

In that case, you could then use:

  YourData$genomename[Ind]

and 
 
  YourData$treedist[Ind]

These would return vectors of the two columns meeting the criteria. 

Which approach you take depends upon what else you may want to do with
the data.

See ?which for more information.

HTH,

Marc Schwartz


From jholtman at gmail.com  Tue Jan 23 18:50:04 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 23 Jan 2007 12:50:04 -0500
Subject: [R] Query about extracting subsets from a table
In-Reply-To: <474740.32935.qm@web43127.mail.sp1.yahoo.com>
References: <474740.32935.qm@web43127.mail.sp1.yahoo.com>
Message-ID: <644e1f320701230950q2769b7k12e268188e1dc980@mail.gmail.com>

Here is how you can do the extraction back to the original input:

data <- data[ data$log10escore < -3, ]


On 1/23/07, lalitha viswanath <lalithaviswanath at yahoo.com> wrote:
> Hi
> I am trying to process tabular data as follows:
>
> Data in the input file is of the form
>
> genome1 genome2 tree-dist log10escore
>
> Genome1 and genome2 are alphabetic.
> Tree-dist and log10escore are numeric.
>
> I wish to extract only those  rows from this table
> where the log10escore is less than -3.
>
>
> data <-read.table(filename);
> data$log10escore = data$log10escore[ data$log10escore
> < -3];
>
> I would like to use this pruned list of escores to get
> the corresponding genomenames and treedist.
>
> I did not find anything useful in the FAQs and Notes
> on R for this part of the data extraction.
>
> As I am just beginning programming in R, I would
> appreciate your input about this.
>
> Thanks
> L
>
>
>
> ____________________________________________________________________________________
> Food fight? Enjoy some healthy debate
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ripley at stats.ox.ac.uk  Tue Jan 23 19:06:54 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Jan 2007 18:06:54 +0000 (GMT)
Subject: [R] which classification package in R can assign sample weight?
In-Reply-To: <cdf817830701230854p13b570a5r982b17e5150f70eb@mail.gmail.com>
References: <cdf817830701230854p13b570a5r982b17e5150f70eb@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701231804260.3226@gannet.stats.ox.ac.uk>

Almost all methods I know of do: logistic regression, neural nets, 
classification trees, PPR ....

Have you looked at the help pages for any of these?

On Tue, 23 Jan 2007, Weiwei Shi wrote:

> Hi,
>
> I am looking for some function implemented in R for classification,
> which has an option to allow me to assign sample weights in learning
> process? Implementation of a wrapper function is possible but I am
> curious if it already exists somewhere.
>
> Thanks,
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From kashei at sip-oy.com  Tue Jan 23 19:18:36 2007
From: kashei at sip-oy.com (Kaskelma, Heikki)
Date: Tue, 23 Jan 2007 20:18:36 +0200
Subject: [R] How to optimize this loop ?
Message-ID: <58FDC30CAA92594996B65C93926950AF1E0208@epont3.mas-oy.com>

>Given a series of observations, I want to know how many consecutive
past
>observations are below the last one.

prune=function(m)
{ mr=rev(m)
  ms=cumsum(mr < mr[1])
  sum(seq_along(ms) - ms == 1) - 1
}

prune(c(3, 4, 10, 14, 8, 3, 4, 8, 9))  # 4
prune(c(3, 4, 10, 14, 8, 3, 4, 11, 9)) # 0


Heikki Kaskelma


From topkatz at msn.com  Tue Jan 23 19:18:49 2007
From: topkatz at msn.com (Talbot Katz)
Date: Tue, 23 Jan 2007 13:18:49 -0500
Subject: [R] cov.nnve error message
Message-ID: <BAY132-F55EE12B8F7F17B53EC998AAAD0@phx.gbl>

Hi.

I have a data matrix of size 3072x1910.  I can compute a standard 1910x1910 
covariance matrix for this, and it comes out positive definite.  I wanted to 
compute a robust covariance matrix with cov.nnve (in the covRobust library). 
  It failed with the following error message:

Error in while (abs(loglik.new - loglik.old)/(1 + abs(loglik.new)) > 
convergence) { :
        missing value where TRUE/FALSE needed
In addition: Warning message:
value out of range in 'gammafn'

So I'm seeking advice / assistance.  I have also tried covRob (in the robust 
library), which has some different methods for robust covariance estimation, 
but it runs out of memory (I usually have about 1.5Gb available).

--  TMK  --
212-460-5430	home
917-656-5351	cell


From enzo83 at wp.pl  Tue Jan 23 19:43:39 2007
From: enzo83 at wp.pl (Jakub Jurdziak)
Date: Tue, 23 Jan 2007 19:43:39 +0100
Subject: [R] eval() parse() and problem with square brackets
In-Reply-To: <200701231759.21023.chaogai@duineveld.demon.nl>
References: <45B637EC.4010307@wp.pl>
	<200701231759.21023.chaogai@duineveld.demon.nl>
Message-ID: <45B6575B.9040603@wp.pl>

Hi,

unfortunately I'm not able to check how it behaves at the moment.

Even if it works ok, it is impossible to drop eval and parse in my 
situation.

I?m using RExcel and RInterface.RRun to execute code from the string: 
"g_1 <- sqlQuery(cnn_1, "select aa from bb.[cc\\dd].ee")". RExcel adds 
eval and parse and finally R gets the transformed code: 
eval(parse(text="g_1 <- sqlQuery(cnn_1, \"select aa from 
bb.[cc\\dd].ee\")")).

Regards


From helprhelp at gmail.com  Tue Jan 23 19:44:19 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Tue, 23 Jan 2007 13:44:19 -0500
Subject: [R] which classification package in R can assign sample weight?
In-Reply-To: <Pine.LNX.4.64.0701231804260.3226@gannet.stats.ox.ac.uk>
References: <cdf817830701230854p13b570a5r982b17e5150f70eb@mail.gmail.com>
	<Pine.LNX.4.64.0701231804260.3226@gannet.stats.ox.ac.uk>
Message-ID: <cdf817830701231044q138bbc1qb0a479b5583e52f6@mail.gmail.com>

Just checked
?lrm
?nnet
?rpart
?ppr

thanks.

but wondering if the last one can do classification?


On 1/23/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> Almost all methods I know of do: logistic regression, neural nets,
> classification trees, PPR ....
>
> Have you looked at the help pages for any of these?
>
> On Tue, 23 Jan 2007, Weiwei Shi wrote:
>
> > Hi,
> >
> > I am looking for some function implemented in R for classification,
> > which has an option to allow me to assign sample weights in learning
> > process? Implementation of a wrapper function is possible but I am
> > curious if it already exists somewhere.
> >
> > Thanks,
> >
> >
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From jerome.lemaitre.1 at ulaval.ca  Tue Jan 23 19:48:16 2007
From: jerome.lemaitre.1 at ulaval.ca (jerome lemaitre)
Date: Tue, 23 Jan 2007 13:48:16 -0500
Subject: [R] New lmer: How to recode random effect ?
Message-ID: <000001c73f1f$0bf23ff0$27cacb84@lemaitrej>

Dear all,

I ran the following model without any problem previously to the update of
lme4:

fm2<-lmer(data=NGud,family=poisson,
	seed~hab*seedtray
	+(1|site)+(1|site:hab))

 
I have 25 sites, 2 habitats ("hab") per site, 8 seedtrays per habitat (4 as
control, and 4 as treatment), and I'm interested in comparing the number of
seed in seedtrays as a function of the treatment and its interaction with
habitat.

My problem is that I have just updated lme4 with the version posted in the
last days. Running the same model implies that R crashes rather than
printing an error message. I read the post of Douglas Bates in the "what
news" section but I don't know what is the new appropriate formulation for
this model. 

Any help would be very appreciated.

Many thanks by advance.

PS: I tested 

fm2<-lmer(data=NGud,family=poisson,
	seed~hab*seedtray
	+(1|site)+(1|site/hab))

which also made R crashing.

And,

fm2<-lmer(data=NGud,family=poisson,
	seed~hab*seedtray
	+(1|site/hab))

and for this, I obtained :

CHOLMOD warning: matrix not positive definite
Error in devLaplace(PQLpars) : Cholmod error `matrix not positive definite'
at file:../Supernodal/t_cholmod_super_numeric.c, line 614




J?r?me Lema?tre


?tudiant au doctorat
Chaire de recherche industrielle CRSNG - Universit? Laval en sylviculture et
faune
& D?partement de biologie,
Facult? des sciences et de g?nie
Pavillon Alexandre-Vachon
Universit? Laval
Qu?bec, QC  G1K 7P4
t?l : (418) 656-2131 poste 2917
Local : VCH-2044
Courriel: jerome.lemaitre.1 at ulaval.ca


From chaogai at duineveld.demon.nl  Tue Jan 23 20:06:07 2007
From: chaogai at duineveld.demon.nl (chao gai)
Date: Tue, 23 Jan 2007 20:06:07 +0100
Subject: [R] eval() parse() and problem with square brackets
In-Reply-To: <45B6575B.9040603@wp.pl>
References: <45B637EC.4010307@wp.pl>
	<200701231759.21023.chaogai@duineveld.demon.nl>
	<45B6575B.9040603@wp.pl>
Message-ID: <200701232006.08373.chaogai@duineveld.demon.nl>

Jakub,

I was trying to simplify the situation in order to know where the fault 
happens.
Appearently, from Excel, you go to R, which does some query in an MS 
application ? The square brackets now make me think that is Excel again, but 
I do not know.
Had I known that I would not have touched your question with a pole, I never 
used that stuff. 

Kees


On Tuesday 23 January 2007 19:43, Jakub Jurdziak wrote:
> Hi,
>
> unfortunately I'm not able to check how it behaves at the moment.
>
> Even if it works ok, it is impossible to drop eval and parse in my
> situation.
>
> I?m using RExcel and RInterface.RRun to execute code from the string:
> "g_1 <- sqlQuery(cnn_1, "select aa from bb.[cc\\dd].ee")". RExcel adds
> eval and parse and finally R gets the transformed code:
> eval(parse(text="g_1 <- sqlQuery(cnn_1, \"select aa from
> bb.[cc\\dd].ee\")")).
>
> Regards


From p.dalgaard at biostat.ku.dk  Tue Jan 23 20:08:54 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 23 Jan 2007 20:08:54 +0100
Subject: [R] eval() parse() and problem with square brackets
In-Reply-To: <45B6575B.9040603@wp.pl>
References: <45B637EC.4010307@wp.pl>	<200701231759.21023.chaogai@duineveld.demon.nl>
	<45B6575B.9040603@wp.pl>
Message-ID: <45B65D46.9090704@biostat.ku.dk>

Jakub Jurdziak wrote:
> Hi,
>
> unfortunately I'm not able to check how it behaves at the moment.
>
> Even if it works ok, it is impossible to drop eval and parse in my 
> situation.
>
> I?m using RExcel and RInterface.RRun to execute code from the string: 
> "g_1 <- sqlQuery(cnn_1, "select aa from bb.[cc\\dd].ee")". RExcel adds 
> eval and parse and finally R gets the transformed code: 
> eval(parse(text="g_1 <- sqlQuery(cnn_1, \"select aa from 
> bb.[cc\\dd].ee\")")).
>
>   
Hmm, I can see how that might lose you the backslash, but not the 
brackets...


From vsbuffalo at ucdavis.edu  Tue Jan 23 20:12:52 2007
From: vsbuffalo at ucdavis.edu (Vince S. Buffalo)
Date: Tue, 23 Jan 2007 11:12:52 -0800
Subject: [R] Question about functionality of sum()
Message-ID: <c5851c050701231112n5b265af9kb6bb50ff912b073c@mail.gmail.com>

R community,

My question is a bit complex to explain (and consequently difficult to
search for in the archives). I will illustrate it with a simplified
example.

If I have an array of sample sizes for various factor levels, of all
different sizes (due to missing data), and another array of those same
factor levels' means, and I would like to sum the squares and weight
them with their corresponding sample size as such:

ss <- sum( n*(yi.bar - y..bar)^2)

[note that y..bar is not an array]

Will R parse the array n at the same rate as it is yi.bar (an equally
long array of the means of those n samples)? Note that I am more sure
of using something like:
x <- c(data...)
ss <- sum(length(na.exclude(x))*(mean(na.exclude(x)) - y..bar)^2)

But alas, there are cases when only the means and the sample sizes are
supplied, but not the actual data. I would also like to stray away
from using built-in functions (weighted.mean perhaps?) do gain an
understanding of how this would be done.

Thank you!
Vince

-- 
"The greatest challenge to any thinker is stating the problem in a way
that will allow a solution."
-Bertrand Russell


From timh at insightful.com  Tue Jan 23 20:26:35 2007
From: timh at insightful.com (Tim Hesterberg)
Date: 23 Jan 2007 11:26:35 -0800
Subject: [R] Time-varying correlation calculation
In-Reply-To: <507797.34402.qm@web60418.mail.yahoo.com> (message from Amir
	Safari on Mon, 22 Jan 2007 06:16:36 -0800 (PST))
References: <507797.34402.qm@web60418.mail.yahoo.com>
Message-ID: <SEWINEXCH00oG1VRIjn0000009b@sewinexch00.insightful.com>

You can do this using an interaction of bs(time) and x, e.g.:

# Time-varying coefficient
x <- rnorm(100)
time <- ppoints(100)
y <- sin(time) + cos(time)*x + rnorm(100)/10

library("splines")
fit <- lm(y ~ bs(time, df=5) * x)
fit
# Plot estimated intercept vs time
plot(time, coef(fit)[1] + bs(time, df=5) %*% coef(fit)[2:6])
lines(time, sin(time))
# Plot estimated slope vs time
plot(time, coef(fit)[7] + bs(time, df=5) %*% coef(fit)[8:12])
lines(time, cos(time))

Thanks to Trevor Hastie for suggesting this approach, when
I made a similar query on S-news years ago.

Tim Hesterberg

>  I'm interested in getting a series of time-varying correlation, simply between two random variables.
>   
>  Could you please introduce a package to do this task?
>   
>  Thank you so much for any help.
>  Amir 

========================================================
| Tim Hesterberg       Senior Research Scientist       |
| timh at insightful.com  Insightful Corp.                |
| (206)802-2319        1700 Westlake Ave. N, Suite 500 |
| (206)283-8691 (fax)  Seattle, WA 98109-3044, U.S.A.  |
|                      www.insightful.com/Hesterberg   |
========================================================
Download S+Resample from www.insightful.com/downloads/libraries


From enzo83 at wp.pl  Tue Jan 23 20:39:47 2007
From: enzo83 at wp.pl (Jakub Jurdziak)
Date: Tue, 23 Jan 2007 20:39:47 +0100
Subject: [R] eval() parse() and problem with square brackets
In-Reply-To: <200701232006.08373.chaogai@duineveld.demon.nl>
References: <45B637EC.4010307@wp.pl>
	<200701231759.21023.chaogai@duineveld.demon.nl>
	<45B6575B.9040603@wp.pl>
	<200701232006.08373.chaogai@duineveld.demon.nl>
Message-ID: <45B66483.9030900@wp.pl>

RExcel and R (D)COM Server together allow you to write R commands in 
Excel, execute them in hidden instance of R and get results back to 
Excel. For more details - see http://sunsite.univie.ac.at/rcom/.

The problem I have arises because R commands written in Excel are 
transformed before execution in R (among other thing eval and parse are 
added). I can get the result of the transformation and try to execute it 
in normal instance of R (not hidden one started from Excel) - this is 
how I've found where the error comes from.

In my first post I have simplified situation and haven't mention about 
using Excel at all.

Jakub


From dalfes at itu.edu.tr  Tue Jan 23 21:09:17 2007
From: dalfes at itu.edu.tr (=?UTF-8?B?TsO8emhldCBEYWxmZXM=?=)
Date: Tue, 23 Jan 2007 22:09:17 +0200
Subject: [R] Installing packages...
Message-ID: <45B66B6D.6060707@itu.edu.tr>

Hi,

I am a total newbie to R. I am using R (2.4.1) on Mac OS X 10.4.8 and
trying to install some packages using GUI Packages & Data/Package Installer
interface...

Every time I get:

trying URL
'http://umfragen.sowi.uni-mainz.de/CRAN/bin/macosx/universal/contrib/2.4/neural_1.4.1.tgz'
Content type 'application/x-tar' length 18920 bytes
opened URL
==================================================
downloaded 18Kb

Error in gzfile(file, "r") : unable to open connection
In addition: Warning message:
cannot open compressed file 'neural/DESCRIPTION'
>

What am I doing wrong?

Any help will be much appreciated.

N?zhet Dalfes

Istanbul Tech. Univ.

From bcarvalh at jhsph.edu  Tue Jan 23 21:25:42 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Tue, 23 Jan 2007 15:25:42 -0500
Subject: [R] Installing packages...
In-Reply-To: <45B66B6D.6060707@itu.edu.tr>
References: <45B66B6D.6060707@itu.edu.tr>
Message-ID: <25B15F62-1951-4557-9715-321F649CAE2A@jhsph.edu>

well, i don't use RGui, but trying:

install.packages("neural", type="source") ## from the R command line

i could not reproduce the error (which may be a problem with the copy  
on the mirror you're using)...
b

On Jan 23, 2007, at 3:09 PM, N?zhet Dalfes wrote:

> Hi,
>
> I am a total newbie to R. I am using R (2.4.1) on Mac OS X 10.4.8 and
> trying to install some packages using GUI Packages & Data/Package  
> Installer
> interface...
>
> Every time I get:
>
> trying URL
> 'http://umfragen.sowi.uni-mainz.de/CRAN/bin/macosx/universal/ 
> contrib/2.4/neural_1.4.1.tgz'
> Content type 'application/x-tar' length 18920 bytes
> opened URL
> ==================================================
> downloaded 18Kb
>
> Error in gzfile(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open compressed file 'neural/DESCRIPTION'
>>
>
> What am I doing wrong?
>
> Any help will be much appreciated.
>
> N?zhet Dalfes
>
> Istanbul Tech. Univ.
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From vsbuffalo at ucdavis.edu  Tue Jan 23 21:28:20 2007
From: vsbuffalo at ucdavis.edu (Vince S. Buffalo)
Date: Tue, 23 Jan 2007 12:28:20 -0800
Subject: [R] Installing packages...
In-Reply-To: <45B66B6D.6060707@itu.edu.tr>
References: <45B66B6D.6060707@itu.edu.tr>
Message-ID: <c5851c050701231228u3e9f1101rd3b25b29eae3608d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070123/0c32f10f/attachment.pl 

From ibrahimmutlay at gmail.com  Tue Jan 23 21:37:20 2007
From: ibrahimmutlay at gmail.com (=?ISO-8859-9?Q?=DDbrahim_Mutlay?=)
Date: Tue, 23 Jan 2007 15:37:20 -0500
Subject: [R] Installing packages...
In-Reply-To: <45B66B6D.6060707@itu.edu.tr>
References: <45B66B6D.6060707@itu.edu.tr>
Message-ID: <eb21cbcd0701231237s5030fd8cgcea208ba04d37ee@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070123/cde6cf20/attachment.pl 

From p.murrell at auckland.ac.nz  Tue Jan 23 21:28:32 2007
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 24 Jan 2007 09:28:32 +1300
Subject: [R] DSC 2007 Additional Funding
Message-ID: <45B66FF0.7090408@stat.auckland.ac.nz>

Hi

DSC 2007, a conference on systems and environments for statistical
computing, will take place in Auckland, New Zealand on February 15 & 16,
2007.

	       *** Travel and Accommodation Support ***

Additional NSF funding to provide partial support for travel and
accommodation for graduate students and junior faculty at
U.S. post-secondary institutions to attend DSC 2007 may be available.
If you wish to apply for this support, please send an application to
dsc2007 at stat.auckland.ac.nz.  Your application should include

    a brief CV

    a statement that demonstrates your eligibility, your need for
    support, and an amount of support requested

    students should include a brief letter of support from their
    supervisor

    faculty or post-doc applicants should include a brief statement
    about other funding sources

The allocation will be based on merit and need; women and minority
candidates are encouraged to apply.

Applications must be made by February 2 (2007-02-02) and successful
applicants will be notified by email soon thereafter.

Please visit the conference web page at
http://www.stat.auckland.ac.nz/dsc-2007/

Paul
(on behalf of the Organising Committee)
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From ploua at allstate.com  Tue Jan 23 21:39:58 2007
From: ploua at allstate.com (Louisell, Paul)
Date: Tue, 23 Jan 2007 12:39:58 -0800
Subject: [R] Loess with more than 4 predictors / offsets
Message-ID: <633AD4C78F3E8F489614A06990F6077B02027E0E@a0203-xpo0111-s.hodc.ad.allstate.com>

In response to your questions:

I asked about including the offset for convenience; I currently put the
offset in by subtracting it from the response, just as you suggest. The
reason for including them is that I'm doing something slightly unusual
with loess:

I'm fitting loess to log((response+1)/offset) because the response is
actually a vector of counts. This is intended to give a rough
approximation to a Poisson regression; the reason for using loess is
that the mean response should be approximated by a Poisson process with
4 predictor variables which can be divided into 2 pairs, each pair of
which are geographic location coordinates. The two location pairs are
expected to exhibit strong interaction; hence, the reason for fitting
loess to all 4 predictors.

I'm aware of the curse of dimensionality, but I have a very large
dataset--over 600,000 observations. Since each pair of predictors
represents a point on a grid, I think Euclidean distance is probably a
good choice. And this brings me to the motivation for wanting to fit
with 5 predictors:

The offset is not _really_ an offset; it's just an approximation to what
the real offset should be. Hence, I'd rather include it as a predictor
than artificially force it to be included linearly with a coefficient of
1. I'm less concerned with linearity than I am with forcing the
coefficient. In fact, I'd like to specify that it be unconditionally
linear, but with an estimated coefficient. 

Thanks,


Paul Louisell
650-833-6254
ploua at allstate.com
Research Associate (Statistician)
Modeling & Data Analytics
ARPC

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Monday, January 22, 2007 11:01 PM
To: Louisell, Paul
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Loess with more than 4 predictors / offsets

On Mon, 22 Jan 2007, Louisell, Paul wrote:

> Hello,
>
> Does anyone know of an R version of loess that allows more than 4
> predictors and/or allows the specification of offsets? For that
matter,
> does anyone know of _any_ version of loess that does either of the
> things I mention?

Why would you want offsets in a regression?: just subtract them from the

lhs.  (R's lm has gained offsets by analogy with glm, but the S original

did not have them).  If you would be more comfortable working with them,

it would be very easy to create a modified version that supports them.

Also, have you heard of the 'curse of dimensionality'?  Localization
even 
to 4 dimensions is no longer really an appropriate term, and Euclidean 
distance will be the main determinant of 'local' and is quite arbitrary.


> Thanks,
>
> Paul Louisell
> 650-833-6254
> ploua at allstate.com
> Research Associate (Statistician)
> Modeling & Data Analytics
> ARPC
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From quesada at gmail.com  Tue Jan 23 21:40:50 2007
From: quesada at gmail.com (Jose Quesada)
Date: Tue, 23 Jan 2007 21:40:50 +0100
Subject: [R] vectorized nested loop: apply a function that takes two rows
Message-ID: <op.tmmjqcgii4ukn7@delllap.ugr.es>

Dear R users,

I want to apply a function that takes two vectors as input to all pairs
(combinations (nrow(X), 2))of matrix rows in a matrix.
I know that ideally, one should avoid loops in R, but after reading the docs for
do.call, apply, etc, I still don't know how to write the nested loop in a
vectorized way.

Example data:
x 		= matrix(rnorm(1000), 100, 100)
zeros	= runif(90)
x[]
# this is actually a very large sparse matrix, but it doesn't matter for the
# example
library(Matrix)
x = as(dat,"CsparseMatrix")

# cosine function
cosine = function (x, y){
	if (is.vector(x) && is.vector(y)) {
		return(crossprod(x, y)/sqrt(crossprod(x) * crossprod(y)))
	} else {stop("cosine: argument mismatch. Two vectors needed as input.")}
}

# The loop-based solution I have is:
		if (is(x, "Matrix") ) {
			cos 	= array(NA, c(ncol(x), ncol(x))) # preallocate memory
			for (i in 2:ncol(x)) {
				for (j in 1:(i - 1)) {
					cos[i, j] = cosine(x[, i], x[, j])
				}
			}
		}

This solution seems inneficient. Is there an easy way of achieving this with a
clever do.call + apply combination?
-- 
Thanks in advance,
-Jose

--
Jose Quesada, PhD
Research fellow, Psychology Dept.
Sussex University, Brighton, UK
http://www.andrew.cmu.edu/~jquesada


From quesada at gmail.com  Tue Jan 23 21:46:27 2007
From: quesada at gmail.com (Jose Quesada)
Date: Tue, 23 Jan 2007 21:46:27 +0100
Subject: [R] [fixed] vectorized nested loop: apply a function that takes two
	rows
Message-ID: <op.tmmjzpf6i4ukn7@delllap.ugr.es>

(Extremely sorry, disregard previous email as I hit send before pasting the latest version of the example; this one is smaller too)
Dear R users,

I want to apply a function that takes two vectors as input to all pairs
(combinations (nrow(X), 2))of matrix rows in a matrix.
I know that ideally, one should avoid loops in R, but after reading the docs for
do.call, apply, etc, I still don't know how to write the nested loop in a
vectorized way.

Example data:
x 		= matrix(rnorm(100), 10, 10)
# this is actually a very large sparse matrix, but it doesn't matter for the
# example
library(Matrix)
x = as(x,"CsparseMatrix")

# cosine function
cosine = function (x, y){
	if (is.vector(x) && is.vector(y)) {
		return(crossprod(x, y)/sqrt(crossprod(x) * crossprod(y)))
	} else {stop("cosine: argument mismatch. Two vectors needed as input.")}
}

# The loop-based solution I have is:
		if (is(x, "Matrix") ) {
			cos 	= array(NA, c(ncol(x), ncol(x)))
			for (i in 2:ncol(x)) {
				for (j in 1:(i - 1)) {
					cos[i, j] = cosine(x[, i], x[, j])
				}
			}
		}

This solution seems inneficient. Is there an easy way of achieving this with a
clever do.call + apply combination?

Also, I have noticed that getting a row from a Matrix object produces a normal
array (i.e., it does not inherit Matrix class). However, selecting >1 rows, does
produce a same-class matrix. If I convert with as() the output of selecting one
row, am I losing performance? Is there any way to make the resulting vector be a
1-D Matrix object?
This solution seems inneficient. Is there an easy way of achieving this with a
clever do.call + apply combination?
-- 
Thanks in advance,
-Jose

--
Jose Quesada, PhD
Research fellow, Psychology Dept.
Sussex University, Brighton, UK
http://www.andrew.cmu.edu/~jquesada


From helprhelp at gmail.com  Tue Jan 23 22:19:41 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Tue, 23 Jan 2007 16:19:41 -0500
Subject: [R] Installing packages...
In-Reply-To: <45B66B6D.6060707@itu.edu.tr>
References: <45B66B6D.6060707@itu.edu.tr>
Message-ID: <cdf817830701231319q6bead11cv36e5c389d50a44df@mail.gmail.com>

why not just type in console:
install.packages("neural")

and see what happens.



On 1/23/07, N?zhet Dalfes <dalfes at itu.edu.tr> wrote:
> Hi,
>
> I am a total newbie to R. I am using R (2.4.1) on Mac OS X 10.4.8 and
> trying to install some packages using GUI Packages & Data/Package Installer
> interface...
>
> Every time I get:
>
> trying URL
> 'http://umfragen.sowi.uni-mainz.de/CRAN/bin/macosx/universal/contrib/2.4/neural_1.4.1.tgz'
> Content type 'application/x-tar' length 18920 bytes
> opened URL
> ==================================================
> downloaded 18Kb
>
> Error in gzfile(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open compressed file 'neural/DESCRIPTION'
> >
>
> What am I doing wrong?
>
> Any help will be much appreciated.
>
> N?zhet Dalfes
>
> Istanbul Tech. Univ.
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From ploua at allstate.com  Tue Jan 23 22:51:41 2007
From: ploua at allstate.com (Louisell, Paul)
Date: Tue, 23 Jan 2007 13:51:41 -0800
Subject: [R] Loess with more than 4 predictors / offsets
Message-ID: <633AD4C78F3E8F489614A06990F6077B02027E11@a0203-xpo0111-s.hodc.ad.allstate.com>

I apologize for clogging up inboxes, but I realized I needed to amend
what I said in my last comment below:

In fact, I'd like to specify that it be unconditionally linear, but with
estimated coefficients, _both an intercept and a slope_.

If the "offset" were only multiplied by a nonzero constant c, this would
have the effect of moving the whole response surface -log(c) units
parallel to the response axis in the scenario I outline below. This
would effectively give me the same thing I already have.


Paul Louisell
650-833-6254
ploua at allstate.com
Research Associate (Statistician)
Modeling & Data Analytics
ARPC


-----Original Message-----
From: Louisell, Paul 
Sent: Tuesday, January 23, 2007 12:40 PM
To: 'Prof Brian Ripley'
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] Loess with more than 4 predictors / offsets

In response to your questions:

I asked about including the offset for convenience; I currently put the
offset in by subtracting it from the response, just as you suggest. The
reason for including them is that I'm doing something slightly unusual
with loess:

I'm fitting loess to log((response+1)/offset) because the response is
actually a vector of counts. This is intended to give a rough
approximation to a Poisson regression; the reason for using loess is
that the mean response should be approximated by a Poisson process with
4 predictor variables which can be divided into 2 pairs, each pair of
which are geographic location coordinates. The two location pairs are
expected to exhibit strong interaction; hence, the reason for fitting
loess to all 4 predictors.

I'm aware of the curse of dimensionality, but I have a very large
dataset--over 600,000 observations. Since each pair of predictors
represents a point on a grid, I think Euclidean distance is probably a
good choice. And this brings me to the motivation for wanting to fit
with 5 predictors:

The offset is not _really_ an offset; it's just an approximation to what
the real offset should be. Hence, I'd rather include it as a predictor
than artificially force it to be included linearly with a coefficient of
1. I'm less concerned with linearity than I am with forcing the
coefficient. In fact, I'd like to specify that it be unconditionally
linear, but with an estimated coefficient. 

Thanks,


Paul Louisell
650-833-6254
ploua at allstate.com
Research Associate (Statistician)
Modeling & Data Analytics
ARPC

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Monday, January 22, 2007 11:01 PM
To: Louisell, Paul
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Loess with more than 4 predictors / offsets

On Mon, 22 Jan 2007, Louisell, Paul wrote:

> Hello,
>
> Does anyone know of an R version of loess that allows more than 4
> predictors and/or allows the specification of offsets? For that
matter,
> does anyone know of _any_ version of loess that does either of the
> things I mention?

Why would you want offsets in a regression?: just subtract them from the

lhs.  (R's lm has gained offsets by analogy with glm, but the S original

did not have them).  If you would be more comfortable working with them,

it would be very easy to create a modified version that supports them.

Also, have you heard of the 'curse of dimensionality'?  Localization
even 
to 4 dimensions is no longer really an appropriate term, and Euclidean 
distance will be the main determinant of 'local' and is quite arbitrary.


> Thanks,
>
> Paul Louisell
> 650-833-6254
> ploua at allstate.com
> Research Associate (Statistician)
> Modeling & Data Analytics
> ARPC
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From kjbeath at kjbeath.com.au  Tue Jan 23 23:09:04 2007
From: kjbeath at kjbeath.com.au (Ken Beath)
Date: Wed, 24 Jan 2007 09:09:04 +1100
Subject: [R] Installing packages...
In-Reply-To: <45B66B6D.6060707@itu.edu.tr>
References: <45B66B6D.6060707@itu.edu.tr>
Message-ID: <EB68C961-2513-42E3-ACDC-770B2F6D5055@kjbeath.com.au>

On 24/01/2007, at 7:09 AM, N?zhet Dalfes wrote:

> Hi,
>
> I am a total newbie to R. I am using R (2.4.1) on Mac OS X 10.4.8 and
> trying to install some packages using GUI Packages & Data/Package  
> Installer
> interface...
>
> Every time I get:
>
> trying URL
> 'http://umfragen.sowi.uni-mainz.de/CRAN/bin/macosx/universal/ 
> contrib/2.4/neural_1.4.1.tgz'
> Content type 'application/x-tar' length 18920 bytes
> opened URL
> ==================================================
> downloaded 18Kb
>
> Error in gzfile(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open compressed file 'neural/DESCRIPTION'
>>
>
> What am I doing wrong?
>
> Any help will be much appreciated.
>
> N?zhet Dalfes
>
> Istanbul Tech. Univ.

This looks like a  permissions problem, possibly your account doesn't  
have Administrator privileges. Try selecting "At User level" for the  
Install Location in the install packages dialog.

There is a special mailling list for mac problems, at https:// 
stat.ethz.ch/mailman/listinfo/r-sig-mac which may provide a better  
answer.

Ken


From helprhelp at gmail.com  Tue Jan 23 23:23:15 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Tue, 23 Jan 2007 17:23:15 -0500
Subject: [R] total correlation
Message-ID: <cdf817830701231423k82743c7na60122d0cc7828f3@mail.gmail.com>

Hi, there:

I have an idea and it needs to calculate "total correlation" for a set
of variables, because I need to evalate how cohesive or related a
group of variables are (If someone knows other ways to do that, please
be advised! thanks!). After searching the archives and googling, I
just two related functions entropy and entropy.joint while the latter
only deals with two variables.

So, I am wondering if I miss something here during my search (btw, I
am not very good at searching so sometimes I posted some questions
which are simple by searching).

BTW, if there are functions not only dealing with discrete but also
with continuous ones, it will be great !

thanks,

-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From ripley at stats.ox.ac.uk  Wed Jan 24 00:02:21 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Jan 2007 23:02:21 +0000 (GMT)
Subject: [R] which classification package in R can assign sample weight?
In-Reply-To: <cdf817830701231044q138bbc1qb0a479b5583e52f6@mail.gmail.com>
References: <cdf817830701230854p13b570a5r982b17e5150f70eb@mail.gmail.com> 
	<Pine.LNX.4.64.0701231804260.3226@gannet.stats.ox.ac.uk>
	<cdf817830701231044q138bbc1qb0a479b5583e52f6@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701232300400.6124@gannet.stats.ox.ac.uk>

On Tue, 23 Jan 2007, Weiwei Shi wrote:

> Just checked
> ?lrm

(I would use glm or multinom.)

> ?nnet
> ?rpart
> ?ppr
>
> thanks.
>
> but wondering if the last one can do classification?

Yes, it has been so used since it can do multivariate outcomes.  See 
Chapter 4 of my PRNN book, for example.


> On 1/23/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>> Almost all methods I know of do: logistic regression, neural nets,
>> classification trees, PPR ....
>> 
>> Have you looked at the help pages for any of these?
>> 
>> On Tue, 23 Jan 2007, Weiwei Shi wrote:
>> 
>> > Hi,
>> >
>> > I am looking for some function implemented in R for classification,
>> > which has an option to allow me to assign sample weights in learning
>> > process? Implementation of a wrapper function is possible but I am
>> > curious if it already exists somewhere.
>> >
>> > Thanks,
>> >
>> >
>> 
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>> 
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From cberry at tajo.ucsd.edu  Wed Jan 24 00:20:53 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Tue, 23 Jan 2007 15:20:53 -0800
Subject: [R] [fixed] vectorized nested loop: apply a function that takes
 two rows
In-Reply-To: <op.tmmjzpf6i4ukn7@delllap.ugr.es>
References: <op.tmmjzpf6i4ukn7@delllap.ugr.es>
Message-ID: <Pine.LNX.4.64.0701231507190.18732@tajo.ucsd.edu>



I am rusty on 'Matrix', but I see there are crossprod methods for those 
classes.

 	res <- crossprod( x , x )

gives your result up to scale factors of sqrt(res[i,i]*res[j,j]), so 
something like

 	diagnl <- Diagonal( ncol(x), sqrt( diag( res ) )

 	final.res <- diagnl %*% res %*% diagnl

should do it.

On Tue, 23 Jan 2007, Jose Quesada wrote:

> (Extremely sorry, disregard previous email as I hit send before pasting the latest version of the example; this one is smaller too)
> Dear R users,
>
> I want to apply a function that takes two vectors as input to all pairs
> (combinations (nrow(X), 2))of matrix rows in a matrix.
> I know that ideally, one should avoid loops in R, but after reading the docs for
> do.call, apply, etc, I still don't know how to write the nested loop in a
> vectorized way.
>
> Example data:
> x 		= matrix(rnorm(100), 10, 10)
> # this is actually a very large sparse matrix, but it doesn't matter for the
> # example
> library(Matrix)
> x = as(x,"CsparseMatrix")
>
> # cosine function
> cosine = function (x, y){
> 	if (is.vector(x) && is.vector(y)) {
> 		return(crossprod(x, y)/sqrt(crossprod(x) * crossprod(y)))
> 	} else {stop("cosine: argument mismatch. Two vectors needed as input.")}
> }
>
> # The loop-based solution I have is:
> 		if (is(x, "Matrix") ) {
> 			cos 	= array(NA, c(ncol(x), ncol(x)))
> 			for (i in 2:ncol(x)) {
> 				for (j in 1:(i - 1)) {
> 					cos[i, j] = cosine(x[, i], x[, j])
> 				}
> 			}
> 		}
>
> This solution seems inneficient. Is there an easy way of achieving this with a
> clever do.call + apply combination?
>
> Also, I have noticed that getting a row from a Matrix object produces a normal
> array (i.e., it does not inherit Matrix class). However, selecting >1 rows, does
> produce a same-class matrix. If I convert with as() the output of selecting one
> row, am I losing performance? Is there any way to make the resulting vector be a
> 1-D Matrix object?
> This solution seems inneficient. Is there an easy way of achieving this with a
> clever do.call + apply combination?
> -- 
> Thanks in advance,
> -Jose
>
> --
> Jose Quesada, PhD
> Research fellow, Psychology Dept.
> Sussex University, Brighton, UK
> http://www.andrew.cmu.edu/~jquesada
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901


From deming.mi at vanderbilt.edu  Wed Jan 24 00:34:07 2007
From: deming.mi at vanderbilt.edu (Deming Mi)
Date: Tue, 23 Jan 2007 17:34:07 -0600
Subject: [R] Estimate and plot hazard function using "muhaz" package
References: <mailman.13.1169550004.5576.r-help@stat.math.ethz.ch>
Message-ID: <001101c73f46$fca4d530$730bc80a@msrc.mc.vanderbilt.edu>

Dear R users,
I am trying to use "muhaz" and "plot.muhaz" functions in "muhaz" package to 
estimate and plot hazard funciton.  However function "muhaz" always gives 
error message "Error in Surv(times, delta) : object "times" not found".  I 
could not even run their sample codes in the user's manual as follows:

data(ovarian)
attach(ovarian)
fit1 <- muhaz(futime, fustat)

it gave the same error message.  By the way, the "survival" package is 
installed and functions well on my computer.

Does anyone have the same problem?  Thanks!

Deming


From kjbeath at kjbeath.com.au  Wed Jan 24 00:44:21 2007
From: kjbeath at kjbeath.com.au (Ken Beath)
Date: Wed, 24 Jan 2007 10:44:21 +1100
Subject: [R] Estimate and plot hazard function using "muhaz" package
In-Reply-To: <001101c73f46$fca4d530$730bc80a@msrc.mc.vanderbilt.edu>
References: <mailman.13.1169550004.5576.r-help@stat.math.ethz.ch>
	<001101c73f46$fca4d530$730bc80a@msrc.mc.vanderbilt.edu>
Message-ID: <DA74A4C6-6398-42AD-914B-7BA125CD8650@kjbeath.com.au>

On 24/01/2007, at 10:34 AM, Deming Mi wrote:

> Dear R users,
> I am trying to use "muhaz" and "plot.muhaz" functions in "muhaz"  
> package to
> estimate and plot hazard funciton.  However function "muhaz" always  
> gives
> error message "Error in Surv(times, delta) : object "times" not  
> found".  I
> could not even run their sample codes in the user's manual as follows:
>
> data(ovarian)
> attach(ovarian)
> fit1 <- muhaz(futime, fustat)
>
> it gave the same error message.  By the way, the "survival" package is
> installed and functions well on my computer.
>
> Does anyone have the same problem?  Thanks!
>
> Deming
>

Works perfectly

 > data(ovarian)
 > attach(ovarian)
 > fit1 <- muhaz(futime, fustat)
 >
 > fit1
$pin
$pin$times
[1]   59  115  156  268  329  353  365  377  421  431  448  464  475   
477  563  638  744  769  770  803  855 1040 1106 1129 1206 1227

$pin$delta
[1] 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0

$pin$nobs
[1] 26

etc

Ken


From hotza at walkerwarehouse.com  Wed Jan 24 01:18:12 2007
From: hotza at walkerwarehouse.com (Ilari Bausch)
Date: Wed, 24 Jan 2007 01:18:12 +0100
Subject: [R] RXdiscountenanc
Message-ID: <01c73f4d$227b0df0$0c00a8c0@packard>

Good day,

CIA_ALIS $3, 75
VAL_LIUM $1, 30
VIA_AGRA $3, 35
AMB_BIEN $2, 90
SO_MMA   $1, 15

http://www.22rx*com ( Important! Replace "*" with "." )

--
disturbed the water. Then, at long last, he heard a snatch of haunting
mersong.
An hour long youll have to look,


From paul at paultopia.org  Wed Jan 24 02:03:32 2007
From: paul at paultopia.org (Paul Gowder)
Date: Tue, 23 Jan 2007 17:03:32 -0800
Subject: [R] package servers
In-Reply-To: <mailman.13.1169550004.5576.r-help@stat.math.ethz.ch>
References: <mailman.13.1169550004.5576.r-help@stat.math.ethz.ch>
Message-ID: <p062309d4c1dc6068c427@[128.12.133.128]>

Hi everyone...

Here's a good one.  I'm using the R installation for mac osx.  An 
overly helpful friend tried to download a couple of packages for me 
(via the menu bar), and in the process, set my default package 
server.  To a server that is not responding.  I can't seem to get R 
to give me the option to set a new server.  Does anyone know how to 
tell R to stop looking for the default server?

thanks,

	-Paul
-- 
----####----
Paul Gowder <paul at paultopia.org>
"If I sleep, who'll give me the moon?" -Camus, speaking through Caligula.

PGP public key (use only when necessary): http://www.paultopia.org/pgp.html


From kjbeath at kjbeath.com.au  Wed Jan 24 02:08:13 2007
From: kjbeath at kjbeath.com.au (Ken Beath)
Date: Wed, 24 Jan 2007 12:08:13 +1100
Subject: [R] package servers
In-Reply-To: <p062309d4c1dc6068c427@[128.12.133.128]>
References: <mailman.13.1169550004.5576.r-help@stat.math.ethz.ch>
	<p062309d4c1dc6068c427@[128.12.133.128]>
Message-ID: <06A05C0D-A9E2-4590-99FC-88D19A38DDC8@kjbeath.com.au>

On 24/01/2007, at 12:03 PM, Paul Gowder wrote:

> Hi everyone...
>
> Here's a good one.  I'm using the R installation for mac osx.  An
> overly helpful friend tried to download a couple of packages for me
> (via the menu bar), and in the process, set my default package
> server.  To a server that is not responding.  I can't seem to get R
> to give me the option to set a new server.  Does anyone know how to
> tell R to stop looking for the default server?
>

In Preferences, Startup, Default CRAN mirror

Ken


From bcarvalh at jhsph.edu  Wed Jan 24 02:12:33 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Tue, 23 Jan 2007 20:12:33 -0500
Subject: [R] package servers
In-Reply-To: <p062309d4c1dc6068c427@[128.12.133.128]>
References: <mailman.13.1169550004.5576.r-help@stat.math.ethz.ch>
	<p062309d4c1dc6068c427@[128.12.133.128]>
Message-ID: <632DE31F-532B-40DD-8573-19AEE5EDDACE@jhsph.edu>

go to R / Preferences / Startup

than you'll know what to do when you see: "Default CRAN mirror".

b

On Jan 23, 2007, at 8:03 PM, Paul Gowder wrote:

> Hi everyone...
>
> Here's a good one.  I'm using the R installation for mac osx.  An
> overly helpful friend tried to download a couple of packages for me
> (via the menu bar), and in the process, set my default package
> server.  To a server that is not responding.  I can't seem to get R
> to give me the option to set a new server.  Does anyone know how to
> tell R to stop looking for the default server?
>
> thanks,
>
> 	-Paul


From cberry at tajo.ucsd.edu  Wed Jan 24 02:39:00 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Tue, 23 Jan 2007 17:39:00 -0800
Subject: [R] [fixed] vectorized nested loop: apply a function that takes
 two rows
In-Reply-To: <Pine.LNX.4.64.0701231507190.18732@tajo.ucsd.edu>
References: <op.tmmjzpf6i4ukn7@delllap.ugr.es>
	<Pine.LNX.4.64.0701231507190.18732@tajo.ucsd.edu>
Message-ID: <Pine.LNX.4.64.0701231738010.22577@tajo.ucsd.edu>

On Tue, 23 Jan 2007, Charles C. Berry wrote:

>
>
> I am rusty on 'Matrix', but I see there are crossprod methods for those
> classes.
>
> 	res <- crossprod( x , x )
>
> gives your result up to scale factors of sqrt(res[i,i]*res[j,j]), so
> something like
>
> 	diagnl <- Diagonal( ncol(x), sqrt( diag( res ) )
>

OOPS! Better make that

  	diagnl <- Diagonal( ncol(x), 1 / sqrt( diag( res ) )


>
> 	final.res <- diagnl %*% res %*% diagnl
>
> should do it.
>
> On Tue, 23 Jan 2007, Jose Quesada wrote:
>
>> (Extremely sorry, disregard previous email as I hit send before pasting the latest version of the example; this one is smaller too)
>> Dear R users,
>>
>> I want to apply a function that takes two vectors as input to all pairs
>> (combinations (nrow(X), 2))of matrix rows in a matrix.
>> I know that ideally, one should avoid loops in R, but after reading the docs for
>> do.call, apply, etc, I still don't know how to write the nested loop in a
>> vectorized way.
>>
>> Example data:
>> x 		= matrix(rnorm(100), 10, 10)
>> # this is actually a very large sparse matrix, but it doesn't matter for the
>> # example
>> library(Matrix)
>> x = as(x,"CsparseMatrix")
>>
>> # cosine function
>> cosine = function (x, y){
>> 	if (is.vector(x) && is.vector(y)) {
>> 		return(crossprod(x, y)/sqrt(crossprod(x) * crossprod(y)))
>> 	} else {stop("cosine: argument mismatch. Two vectors needed as input.")}
>> }
>>
>> # The loop-based solution I have is:
>> 		if (is(x, "Matrix") ) {
>> 			cos 	= array(NA, c(ncol(x), ncol(x)))
>> 			for (i in 2:ncol(x)) {
>> 				for (j in 1:(i - 1)) {
>> 					cos[i, j] = cosine(x[, i], x[, j])
>> 				}
>> 			}
>> 		}
>>
>> This solution seems inneficient. Is there an easy way of achieving this with a
>> clever do.call + apply combination?
>>
>> Also, I have noticed that getting a row from a Matrix object produces a normal
>> array (i.e., it does not inherit Matrix class). However, selecting >1 rows, does
>> produce a same-class matrix. If I convert with as() the output of selecting one
>> row, am I losing performance? Is there any way to make the resulting vector be a
>> 1-D Matrix object?
>> This solution seems inneficient. Is there an easy way of achieving this with a
>> clever do.call + apply combination?
>> --
>> Thanks in advance,
>> -Jose
>>
>> --
>> Jose Quesada, PhD
>> Research fellow, Psychology Dept.
>> Sussex University, Brighton, UK
>> http://www.andrew.cmu.edu/~jquesada
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> Charles C. Berry                        (858) 534-2098
>                                          Dept of Family/Preventive Medicine
> E mailto:cberry at tajo.ucsd.edu	         UC San Diego
> http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901


From dchandra at ics.uci.edu  Wed Jan 24 04:31:27 2007
From: dchandra at ics.uci.edu (Deepak Chandra)
Date: Tue, 23 Jan 2007 19:31:27 -0800
Subject: [R] writing R shell scripts?
Message-ID: <7387cdce0701231931j799a3756h91ed80699bd42929@mail.gmail.com>

Hi All,

Another newbie question. I want to write an R script that takes
argument from command line and runs and produces output to stdin.

For example if there is file foo.R with following in it.

args = commandArgs()
print(args)

then, when I run it like
 $ R foo.R hello
it should print 'hello' on the terminal.

I searched the mainling list and found a very old post that said said
that this feature will be implemented soon. I am expecting this has
already  been implemented by now.

Thanks,
Deepak


From carnellr at battelle.org  Wed Jan 24 04:49:06 2007
From: carnellr at battelle.org (Rob Carnell)
Date: Wed, 24 Jan 2007 03:49:06 +0000 (UTC)
Subject: [R] Latin hyper cube sampling from expand.grid()
References: <fc5b8ae70701220759o55f2a2few5850b9b66a99a922@mail.gmail.com>
Message-ID: <loom.20070124T044428-788@post.gmane.org>

Prasanna <prasannaprakash <at> gmail.com> writes:

> 
> Dear R experts
> 
> I am looking for a package which gives me latin hyper cube samples
> from the grid of values produced from the command "expand.grid". Any
> pointers to this issue might be very useful. Basically, I am doing the
> following:
> 
> > a<-(1:10)
> > b<-(20:30)
> > dataGrid<-expand.grid(a,b)
> 
> Now, is there a way to use this "dataGrid" in the package "lhs" to get
> latin hyper cube samples.
> 
> Thanking you
> Prasanna
> 
> ______________________________________________
> R-help <at> stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

Prasanna,

I think I understand your question, please let me know if this explanation is 
not what you need.  Since, lhs is a contributed package, you could contact me 
directly first.

a <- 1:10
b <- 20:30
dataGrid <- expand.grid(a, b)

I believe that you want a Latin hypercube sample from the integers 1-10 in the 
first variable and 20-30 in the second.  I will offer a way to do something 
similar with the lhs package, but then also offer alternatives way which may 
meet your needs better.

The lhs package returns a uniformly distributed stratified sample from the 
unit hypercube.  The marginal distributions can then be transformed to your 
distribution of choice.  If you wanted a uniform Latin hypercube on [1,10] and 
[20,30] with 22 samples, you could do:

require(lhs)
X <- randomLHS(22, 2)
X[,1] <- 1+9*X[,1]
X[,2] <- 20+10*X[,2]
X

OR

X <- randomLHS(22, 2)
X[,1] <- qunif(X[,1], 1, 9)
X[,2] <- qunif(X[,2], 20, 30)
X


Since I think you want integers (which I haven't thought about before now), 
then I think we must be careful about what we mean by a Latin hypercube 
sample.  If you wanted exactly 3 points, then you could divide up the range 
[1,10] into three almost equal parts and sample from 1:3, 4:6, and 7:10.  The 
problem is that it wouldn't be uniform sample across the range. (7 would be 
sampled less often than 2 for example)

I think that to do a Latin hypercube sample on the intgers, you should have a 
number of integers on the margins which have the number of points sampled as a 
common factor.  For example if you sample 3 points from 1:9, and 21:32 then 
you could sample as follows:

a <- c(sample(1:3,1), sample(4:6, 1), sample(7:9, 1))
b <- c(sample(21:24,1), sample(25:28, 1), sample(29:32,1))

and then randomly permute the entries of a and b.

Or more generally, take n samples from the list of integer groups:

integerLHS <- function(n, intGroups)
{
  stopifnot(all(lapply(intGroups, function(X) length(X)%%n)==0))
  stopifnot(require(lhs))
  stopifnot(is.list(intGroups))
  ranges <- lapply(intGroups, function(X) max(X)-min(X))
  A <- matrix(nrow=n, ncol=length(intGroups))
  for(j in 1:length(ranges))
  {
    sequ <- order(runif(n))
    if(length(intGroups[[1]]) > 1)
    {
      spacing <- intGroups[[j]][2]-intGroups[[j]][1]
    } else stop("must have more than 1 intGroup")
    for(k in 1:n)
    {
      i <- sequ[k]
      a <- min(intGroups[[j]])+(i-1)*(ranges[[j]]+spacing)/n
      b <- min(intGroups[[j]])+i*(ranges[[j]]+spacing)/n-1
      if(a < b)
      {
        A[k,j] <- sample(seq(a,b,spacing), 1)
      } else if(a==b)
      {
        A[k,j] <- a
      } else stop("error")
    }
  }
  return(A)
}

integerLHS(10, list(1:10, 31:40))
integerLHS(5, list(1:10, 31:40))
integerLHS(2, list(1:10, 31:40))
integerLHS(5, list(1:20, 31:60, 101:115))
integerLHS(5, list(seq(2,20,2), 31:60, 101:115))

The function above is neither efficient nor tested, but it is a place for you 
to start.

Rob


From ripley at stats.ox.ac.uk  Wed Jan 24 06:45:13 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Jan 2007 05:45:13 +0000 (GMT)
Subject: [R] writing R shell scripts?
In-Reply-To: <7387cdce0701231931j799a3756h91ed80699bd42929@mail.gmail.com>
References: <7387cdce0701231931j799a3756h91ed80699bd42929@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701240516300.10866@gannet.stats.ox.ac.uk>

On Tue, 23 Jan 2007, Deepak Chandra wrote:

> Hi All,
>
> Another newbie question. I want to write an R script that takes
> argument from command line and runs and produces output to stdin.

You will find that difficult: did you mean stdout?

> For example if there is file foo.R with following in it.
>
> args = commandArgs()
> print(args)
>
> then, when I run it like
> $ R foo.R hello
> it should print 'hello' on the terminal.
>
> I searched the mainling list and found a very old post that said said

Please give an exact reference so we know what you are referring to.
This is not what is usually meant by 'R shell scripts' (it is an R script, 
not a shell script).

> that this feature will be implemented soon. I am expecting this has
> already  been implemented by now.

That is not what commandArgs() is documented to do so you will need to do 
something slightly different.  But in the development version of R you can 
come very close:

gannet% cat foo.R
args <- commandArgs(TRUE)
print(args)
gannet% ~/R/R-devel/bin/Rscript foo.R hello
[1] "hello"
gannet%

and in any recent version of R

gannet% cat foo2.R
args <- commandArgs()
m <- match("--args", args)
print(args[-(1:m)])
gannet% R --slave --args hello < foo2.R
[1] "hello"

If you have a platform and build of R for which it works, there is also 
littler (http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/LittleR), 
but beware that are restrictions not stated on that page (such as the need 
for R built as a shared library, which is not the default).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From res90sx5 at verizon.net  Wed Jan 24 07:49:08 2007
From: res90sx5 at verizon.net (Daniel Nordlund)
Date: Tue, 23 Jan 2007 22:49:08 -0800
Subject: [R] solving a structural equation model using sem or other package
Message-ID: <01cd01c73f83$bf66f5d0$6501a8c0@Aragorn>

I am trying to work my way through the book "Singer, JD and Willett, JB, Applied Longitudinal Data Analysis. Oxford University Press, 2003"  using R.  I have the SAS code and S-Plus code from the UCLA site (doesn't include chapter 8 or later problems).  In chapter 8, there is a structural equation/path model which can be specified for the sem package as follows 

S <- cov(al2) #al2 contains the variables alc1, alc2, alc3, and cons
N <- 1122

modelA.ram <- specify.model()
  f1    -> alc1,  NA,  1
  f1    -> alc2,  NA,  1
  f1    -> alc3,  NA,  1
  f2    -> alc1,  NA,  0
  f2    -> alc2,  NA,  .75
  f2    -> alc3,  NA,  1.75
  cons  -> f1,    p0,  1
  cons  -> f2,    p1,  1
  alc1 <-> alc1,  u1,  1
  alc2 <-> alc2,  u2,  1
  alc3 <-> alc3,  u3,  1
  cons <-> cons,  u4,  1
  f1   <-> f1,    s1,  1
  f2   <-> f2,    s2,  1
  f1   <-> f2,    s3,  1 

modelA <- sem(modelA.ram, S, N, analytic.gradient=FALSE)

An equivalent specification in SAS produces the solution presented in the book.  The variable cons is a constant vector of 1's.  The problem with the sem package is that the covariance matrix which includes the variable cons is singular and sem says so and will not continue.  Is there an alternative way to specify this problem for sem to obtain a solution?  If not, is there another package that would produce a solution?

Thanks,

Dan Nordlund
Bothell, WA


From gallon.li at gmail.com  Wed Jan 24 08:04:15 2007
From: gallon.li at gmail.com (gallon li)
Date: Wed, 24 Jan 2007 15:04:15 +0800
Subject: [R] Matrix question: obtaining the square root of a positive
	definite matrix?
Message-ID: <54f7e7c30701232304h3f600a5di72e1b5f36d1d8581@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070124/5656ec5f/attachment.ksh 

From ripley at stats.ox.ac.uk  Wed Jan 24 09:02:19 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Jan 2007 08:02:19 +0000 (GMT)
Subject: [R] Matrix question: obtaining the square root of a positive
 definite matrix?
In-Reply-To: <54f7e7c30701232304h3f600a5di72e1b5f36d1d8581@mail.gmail.com>
References: <54f7e7c30701232304h3f600a5di72e1b5f36d1d8581@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701240743400.15298@gannet.stats.ox.ac.uk>

On Wed, 24 Jan 2007, gallon li wrote:

> I want to compute B=A^{1/2} such that B*B=A.

According to your subject line A is positive definite and hence 
symmetric?  The usual definition of a matrix square root involves a 
transpose, e.g. B'B = A.  There are many square roots: were you looking 
for a symmetric one?

For such an A,

> e <- eigen(A)
> V <- e$vectors
> V %*% diag(e$values) %*% t(V)

recovers A (up to rounding errors), and

> B <- V %*% diag(sqrt(e$values)) %*% t(V)

is such that B %*% B = A.  Even that is not unique, e.g. -B is an equally 
good answer.


> For example

(with A = b and B = a, it seems)

> a=matrix(c(1,.2,.2,.2,1,.2,.2,.2,1),ncol=3)
>
> so
>> a
>     [,1] [,2] [,3]
> [1,]  1.0  0.2  0.2
> [2,]  0.2  1.0  0.2
> [3,]  0.2  0.2  1.0
>> a%*%a
>     [,1] [,2] [,3]
> [1,] 1.08 0.44 0.44
> [2,] 0.44 1.08 0.44
> [3,] 0.44 0.44 1.08
>> b=a%*%a
>
> i have tried to use singular value decomposion
>
>> c=svd(b)
>
>> c$u%*%diag(sqrt(c$d))
>           [,1]          [,2]       [,3]
> [1,] -0.8082904  2.043868e-18  0.6531973
> [2,] -0.8082904 -5.656854e-01 -0.3265986
> [3,] -0.8082904  5.656854e-01 -0.3265986
>
> this does not come close to the original a. Can anybody on this forum
> enlight me on how to get a which is the square root of b?
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From shubhak at ambaresearch.com  Wed Jan 24 09:07:10 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Wed, 24 Jan 2007 13:37:10 +0530
Subject: [R] Vector to Matrix transformation
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3DDB549@BAN-MAILSRV03.Amba.com>

How to suppress the recycling of items in a matrix..instead NA can be
filled.
-----Original Message-----
From: Chuck Cleland [mailto:ccleland at optonline.net] 
Sent: Tuesday, January 23, 2007 8:00 PM
To: Shubha Vishwanath Karanth
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Vector to Matrix transformation

Shubha Vishwanath Karanth wrote:
> Hi R,
> 
> I have a vector V1 of unknown length, say n. I need to convert this
into
> a matrix C of row size=5, and accordingly the column should be
updated.
> I tried with:
> 
> C=as.matrix(V1,5,n/5)
> 
> But it is not working...Could somebody help me on this?

  You could try the following:

matrix(V1, nrow=5)

  but note what happens when the length of V1 is not a multiple of 5.

> Thanks in advance...
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From shubhak at ambaresearch.com  Wed Jan 24 09:11:50 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Wed, 24 Jan 2007 13:41:50 +0530
Subject: [R] Checking for the existence of an R object
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3DDB54B@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070124/8630b8ee/attachment.ksh 

From shubhak at ambaresearch.com  Wed Jan 24 09:41:31 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Wed, 24 Jan 2007 14:11:31 +0530
Subject: [R] Checking for the existence of an R object
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3DDB56E@BAN-MAILSRV03.Amba.com>

Thanks all of you...

-----Original Message-----
From: talepanda [mailto:talepanda at gmail.com] 
Sent: Wednesday, January 24, 2007 2:10 PM
To: Shubha Vishwanath Karanth
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Checking for the existence of an R object

see:

?exists

HTH.

On 1/24/07, Shubha Vishwanath Karanth <shubhak at ambaresearch.com> wrote:
> Hi,
>
>
>
> Is there any way to check whether an R object exists or not? Say
> example: a data frame.
>
>
>
> Thanks,
>
> Shubha
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From P.Dalgaard at biostat.ku.dk  Wed Jan 24 09:50:46 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 24 Jan 2007 09:50:46 +0100
Subject: [R] Matrix question: obtaining the square root of a positive
 definite matrix?
In-Reply-To: <Pine.LNX.4.64.0701240743400.15298@gannet.stats.ox.ac.uk>
References: <54f7e7c30701232304h3f600a5di72e1b5f36d1d8581@mail.gmail.com>
	<Pine.LNX.4.64.0701240743400.15298@gannet.stats.ox.ac.uk>
Message-ID: <45B71DE6.3070102@biostat.ku.dk>

Prof Brian Ripley wrote:
> On Wed, 24 Jan 2007, gallon li wrote:
>
>   
>> I want to compute B=A^{1/2} such that B*B=A.
>>     
>
> According to your subject line A is positive definite and hence 
> symmetric?  The usual definition of a matrix square root involves a 
> transpose, e.g. B'B = A.  There are many square roots: were you looking 
> for a symmetric one?
>
>   
If not, Choleski decomposition by chol() is often the expedient way.

> For such an A,
>
>   
>> e <- eigen(A)
>> V <- e$vectors
>> V %*% diag(e$values) %*% t(V)
>>     
>
> recovers A (up to rounding errors), and
>
>   
>> B <- V %*% diag(sqrt(e$values)) %*% t(V)
>>     
>
> is such that B %*% B = A.  Even that is not unique, e.g. -B is an equally 
> good answer.
>
>
>   
and you can flip the sign of the individual root eigenvalues too, and if
the eigenvalues are not unique, you can rotate the eigenspace coordinate
systems at will and then flip signs.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From stat700004 at yahoo.co.in  Wed Jan 24 10:46:45 2007
From: stat700004 at yahoo.co.in (stat stat)
Date: Wed, 24 Jan 2007 09:46:45 +0000 (GMT)
Subject: [R] Date variable
Message-ID: <20070124094645.54531.qmail@web7609.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070124/22a78e12/attachment.pl 

From maechler at stat.math.ethz.ch  Wed Jan 24 11:57:48 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 24 Jan 2007 11:57:48 +0100
Subject: [R] Matrix subsetting {was "... vectorized nested loop..."}
In-Reply-To: <op.tmmjzpf6i4ukn7@delllap.ugr.es>
References: <op.tmmjzpf6i4ukn7@delllap.ugr.es>
Message-ID: <17847.15276.988909.832523@stat.math.ethz.ch>

Hi Jose,
I'm answering your second batch of questions, since
Chuck Berry has already well done so with the first one

>>>>> "Jose" == Jose Quesada <quesada at gmail.com>
>>>>>     on Tue, 23 Jan 2007 21:46:27 +0100 writes:

[........]

    Jose> # example
    Jose> library(Matrix)
    Jose> x = as(x,"CsparseMatrix")

[..........]

    Jose> Also, I have noticed that getting a row from a Matrix
    Jose> object produces a normal array (i.e., it does not
    Jose> inherit Matrix class). 

This is very much on purpose, following the principle of "least
surprise" so I'm surprised you're suprised.. :

The 'Matrix' behavior has been modelled to follow the more than
20 years old 'matrix' behavior :

 > matrix(1:9, 3) [,2]
 [1] 4 5 6
 > matrix(1:9, 3) [,2 , drop=FALSE]
      [,1]
 [1,]    4
 [2,]    5
 [3,]    6
 > library(Matrix)
 Loading required package: lattice
 > Matrix(1:9, 3) [,2]
 [1] 4 5 6
 > Matrix(1:9, 3) [,2, drop = FALSE]
 3 x 1 Matrix of class "dgeMatrix"
      [,1]
 [1,]    4
 [2,]    5
 [3,]    6
 > 

But then I should not be surprised, because
there has been the R FAQ

>> 7.5 Why do my matrices lose dimensions?

for quite a while.

*And* I think that there is only one "thing" in the S language
about which every "knowledgable one" agrees that it's a "design
bug", and that's the fact that 'drop = TRUE' is the default, and
not 'drop = FALSE' {but it's not possible to change now, please
don't start that discussion!}

    
Given what I say above, I wonder if our ("new-style") 'Matrix'
objects should not behave differently than ("old-style") 'matrix' and
indeed do use a default 'drop = FALSE'.
This might break some Matrix-based code though, but then
'Matrix' is young enough, and working Matrix indexing is much
younger,  and there are only about 4 CRAN/Bioconductor
packages depending on 'Matrix'.
--> This discussion (about changing this behavior in the
"Matrix" package) should definitely be lead on the R-devel
mailing list --> CC'ing to R-devel
{hence one (but please *only* one !) cross-post}

    Jose> However, selecting >1 rows,
    Jose> does produce a same-class matrix. If I convert with
    Jose> as() the output of selecting one row, am I losing
    Jose> performance? Is there any way to make the resulting
    Jose> vector be a 1-D Matrix object?

yes, ", drop = FALSE", see above

Martin


From statadat at gmail.com  Wed Jan 24 12:23:56 2007
From: statadat at gmail.com (domenico pestalozzi)
Date: Wed, 24 Jan 2007 12:23:56 +0100
Subject: [R] how to change the dataframe labels' ?
Message-ID: <e591a95b0701240323v7602de7fy57f83bdaf8bc279c@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070124/36f4b88b/attachment.pl 

From mothsailor at googlemail.com  Wed Jan 24 12:32:26 2007
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 24 Jan 2007 11:32:26 +0000
Subject: [R] solving a structural equation model using sem or other
	package
In-Reply-To: <01cd01c73f83$bf66f5d0$6501a8c0@Aragorn>
References: <01cd01c73f83$bf66f5d0$6501a8c0@Aragorn>
Message-ID: <815b70590701240332v7bb4c445n86f338751e8088a6@mail.gmail.com>

This is an extract from the sem help page, which deals with your situation:

S covariance matrix among observed variables; may be input as a
symmetric matrix, or as a lower- or upper-triangular matrix. S may
also be a raw (i.e., ``uncorrected'') moment matrix ? that is, a
sum-of-squares-and-products matrix divided by N. This form of input is
useful for fitting models with intercepts, in which case the moment
matrix should include the mean square and cross-products for a unit
variable all of whose entries are 1; of course, the raw mean square
for the unit variable is 1. Raw-moment matrices may be computed by
raw.moments.

On 24/01/07, Daniel Nordlund <res90sx5 at verizon.net> wrote:
> I am trying to work my way through the book "Singer, JD and Willett, JB, Applied Longitudinal Data Analysis. Oxford University Press, 2003"  using R.  I have the SAS code and S-Plus code from the UCLA site (doesn't include chapter 8 or later problems).  In chapter 8, there is a structural equation/path model which can be specified for the sem package as follows
>
> S <- cov(al2) #al2 contains the variables alc1, alc2, alc3, and cons
> N <- 1122
>
> modelA.ram <- specify.model()
>   f1    -> alc1,  NA,  1
>   f1    -> alc2,  NA,  1
>   f1    -> alc3,  NA,  1
>   f2    -> alc1,  NA,  0
>   f2    -> alc2,  NA,  .75
>   f2    -> alc3,  NA,  1.75
>   cons  -> f1,    p0,  1
>   cons  -> f2,    p1,  1
>   alc1 <-> alc1,  u1,  1
>   alc2 <-> alc2,  u2,  1
>   alc3 <-> alc3,  u3,  1
>   cons <-> cons,  u4,  1
>   f1   <-> f1,    s1,  1
>   f2   <-> f2,    s2,  1
>   f1   <-> f2,    s3,  1
>
> modelA <- sem(modelA.ram, S, N, analytic.gradient=FALSE)
>
> An equivalent specification in SAS produces the solution presented in the book.  The variable cons is a constant vector of 1's.  The problem with the sem package is that the covariance matrix which includes the variable cons is singular and sem says so and will not continue.  Is there an alternative way to specify this problem for sem to obtain a solution?  If not, is there another package that would produce a solution?
>
> Thanks,
>
> Dan Nordlund
> Bothell, WA
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From helmut.schuetz at bebac.at  Wed Jan 24 13:41:29 2007
From: helmut.schuetz at bebac.at (=?ISO-8859-1?Q?Helmut_Sch=FCtz?=)
Date: Wed, 24 Jan 2007 13:41:29 +0100
Subject: [R] How to generate 'minor' ticks in lattice (qqmath)
In-Reply-To: <971536df0701230744i19479d4cq213b85b229f63b57@mail.gmail.com>
References: <45B62B64.9060700@bebac.at>
	<971536df0701230744i19479d4cq213b85b229f63b57@mail.gmail.com>
Message-ID: <45B753F9.5070708@bebac.at>

Dear Gabor!

thanks for your hints; as a side-effect I learned a lot about lattice.

The working example is:

library("lattice")
library("grid")
numy <- 100
y <- runif(numy,min=0,max=1)
sig <- 0.05
numsig <- length(which(y<sig))

Lower <- 0
Upper <- 1
MajorInterval <- 5   # interval for major ticks
MinorInterval <- 4   # interval within major
Major <- seq( Lower,Upper,(Upper-Lower)/MajorInterval )
Minor <- seq( Lower,Upper,(Upper-Lower)/(MajorInterval*MinorInterval) )
labl <- as.character(Major)

trellis.focus("panel", 1, 1, clip.off = TRUE)
qqmath(y, distribution = qunif,
  prepanel = NULL,
  panel = function(x)
    {
    panel.abline(c(0,1), lty = 2)
    panel.polygon(c(0,0,numsig/numy,numsig/numy,0), c(0,sig,sig,0,0), 
lwd = 0.75)
    panel.qqmath(x, distribution = qunif, col = 1)
    },
    scales=list(x = list(at = Major), y = list(at = Major), tck=c(1,0), 
labels=labl, cex=0.9),
    xlab = "uniform [0,1] quantiles",
    ylab = "runif [0,1]",
    min = 0, max = 1)
trellis.focus("panel", 1, 1, clip.off = TRUE)
panel.axis("bottom", check.overlap = TRUE, outside = TRUE, labels = 
FALSE, tck = .5, at = Minor)
panel.axis("left", check.overlap = TRUE, outside = TRUE, labels = FALSE, 
tck = .5, at = Minor)
trellis.unfocus()

Best regards,
Helmut

-- 
Helmut Sch?tz
BEBAC
Consultancy Services for Bioequivalence and Bioavailability Studies
Neubaugasse 36/11
1070 Vienna/Austria
tel/fax +43 1 2311746
Web http://BEBAC.at
BE/BA Forum http://forum.bebac.at
http://www.goldmark.org/netrants/no-word/attach.html


From ripley at stats.ox.ac.uk  Wed Jan 24 14:11:39 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Jan 2007 13:11:39 +0000 (GMT)
Subject: [R] how to change the dataframe labels' ?
In-Reply-To: <3948d9e50701240354w22038951o3a688adc5155a25f@mail.gmail.com>
References: <e591a95b0701240323v7602de7fy57f83bdaf8bc279c@mail.gmail.com>
	<3948d9e50701240354w22038951o3a688adc5155a25f@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701241305540.21410@gannet.stats.ox.ac.uk>

Sorry, no: row.names<-() is what you want.

rownames for matrices (and arrays)
row.names for data frames.

Using them the other way round usually works but can be very inefficient.
>From R-devel (where the worst inefficiencies are circumvented)

      The extractor functions try to do something sensible for any
      matrix-like object 'x'.  If the object has 'dimnames' the first
      component is used as the row names, and the second component (if
      any) is used for the column names.  For a data frame, 'rownames'
      and 'colnames' are calls to 'row.names' and 'names' respectively,
      but the latter are preferred.


On Wed, 24 Jan 2007, talepanda wrote:

> rownames()<-  is what you want.
>
>> dat<-data.frame(V1=sample(10),V2=sample(10))
>> dat
>   V1 V2
> 1   2  5
> 2   3  8
> 3   8  4
> 4   9  6
> 5   6  2
> 6   5  7
> 7  10  3
> 8   4  9
> 9   1 10
> 10  7  1
>> dat<-dat[order(dat$V2),]
>> dat
>   V1 V2
> 10  7  1
> 5   6  2
> 7  10  3
> 3   8  4
> 1   2  5
> 4   9  6
> 6   5  7
> 2   3  8
> 8   4  9
> 9   1 10
>> rownames(dat)<-1:dim(dat)[1]
> ## or rownames(dat)<-dat$V2
>> dat
>   V1 V2
> 1   7  1
> 2   6  2
> 3  10  3
> 4   8  4
> 5   2  5
> 6   9  6
> 7   5  7
> 8   3  8
> 9   4  9
> 10  1 10
>
> HTH
>
> On 1/24/07, domenico pestalozzi <statadat at gmail.com> wrote:
>> I import a dataframe composed by 2 variables and 14000 cases. Now I need the
>> labels of the cases sorted by the second variable V2, but if I sort the
>> dataframe according to the second variable:
>>
>> mydataframe<- mydataframe[order(mydataframe$V2),]
>>
>> I notice that the labels are always the same (that is, not ordered by V2).
>>
>> How to change them?
>> I tried :
>>
>> labels(mydataframe)<-1:14000
>>
>> but it doesn't work.
>>
>> Thanks
>>
>> domenico
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From shubhak at ambaresearch.com  Wed Jan 24 14:12:46 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Wed, 24 Jan 2007 18:42:46 +0530
Subject: [R] Conversion of column matrix into a vector without duplicates
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3E4B0DD@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070124/0ca8c624/attachment.pl 

From darren.obbard at ed.ac.uk  Wed Jan 24 14:22:10 2007
From: darren.obbard at ed.ac.uk (Darren Obbard)
Date: Wed, 24 Jan 2007 13:22:10 +0000
Subject: [R] Capturing output from external executables, in windows
Message-ID: <45B75D82.6070808@ed.ac.uk>

Hi,

Any help on the following would be much appreciated

I wish to capture the output (currently going to console) from an 
external executable.

The executable is successfully run using

     system("program -switch ")

and the output printed to the DOS console.

How do I capture this output? I have tried redirecting the output to a 
text file, and then reading this in
    
     system("program -switch > textfile.txt")
     data<-scan("textfile.txt")

But this does not seem to work (the textfile.txt is not written). It 
does however work if I invoke the console to be permanent 

     system("cmd /K program -switch > textfile.txt")
     data<-scan("textfile.txt")

Unfortunately, this leaves me with an open console window I have to 
close manually.

Is there a way of doing this (under windows) using system( ) or some 
other command? It appears that pipe( ) may do it, but I cannot 
understand the documentation.

An example of the appropriate syntax would be an enormous help.

Thanks in advance,

Darren

Darren.Obbard at ed.ac.uk

-- 

Darren Obbard
Institute of Evolutionary Biology
Ashworth Labs
Kings Buildings
University of Edinburgh, UK


From Thierry.ONKELINX at inbo.be  Wed Jan 24 14:24:51 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 24 Jan 2007 14:24:51 +0100
Subject: [R] Conversion of column matrix into a vector without duplicates
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3E4B0DD@BAN-MAILSRV03.Amba.com>
Message-ID: <2E9C414912813E4EB981326983E0A104027FF6B9@inboexch.inbo.be>

?unique

unique(A[, 2])

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch
[mailto:r-help-bounces op stat.math.ethz.ch] Namens Shubha Vishwanath
Karanth
Verzonden: woensdag 24 januari 2007 14:13
Aan: r-help op stat.math.ethz.ch
Onderwerp: [R] Conversion of column matrix into a vector without
duplicates

Hi R,

 

I have a matrix A,

 

A=

      [,1] [,2]

[1,]  a     u

[2,]  b     v

[3,]  c     x

[4,]  d     x

[5,]  e     x

 

I want to put the 2nd column of this matrix in a vector without
duplicates. i.e., my vector v should be (u, v, x), whose length is 3.

 

Can anybody help me on this?

 

Thanks in advance

Shubha.


	[[alternative HTML version deleted]]

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dimitris.rizopoulos at med.kuleuven.be  Wed Jan 24 14:26:29 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 24 Jan 2007 14:26:29 +0100
Subject: [R] Conversion of column matrix into a vector without duplicates
References: <A36876D3F8A5734FA84A4338135E7CC3E4B0DD@BAN-MAILSRV03.Amba.com>
Message-ID: <003301c73fbb$41ba2160$0540210a@www.domain>

you need: unique(A[, 2])


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Shubha Vishwanath Karanth" <shubhak at ambaresearch.com>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, January 24, 2007 2:12 PM
Subject: [R] Conversion of column matrix into a vector without 
duplicates


> Hi R,
>
>
>
> I have a matrix A,
>
>
>
> A=
>
>      [,1] [,2]
>
> [1,]  a     u
>
> [2,]  b     v
>
> [3,]  c     x
>
> [4,]  d     x
>
> [5,]  e     x
>
>
>
> I want to put the 2nd column of this matrix in a vector without
> duplicates. i.e., my vector v should be (u, v, x), whose length is 
> 3.
>
>
>
> Can anybody help me on this?
>
>
>
> Thanks in advance
>
> Shubha.
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ccleland at optonline.net  Wed Jan 24 14:27:16 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 24 Jan 2007 08:27:16 -0500
Subject: [R] Conversion of column matrix into a vector without duplicates
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3E4B0DD@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC3E4B0DD@BAN-MAILSRV03.Amba.com>
Message-ID: <45B75EB4.9050909@optonline.net>

Shubha Vishwanath Karanth wrote:
> Hi R,
>  
> I have a matrix A,
> 
> A=
> 
>       [,1] [,2]
> 
> [1,]  a     u
> [2,]  b     v
> [3,]  c     x
> [4,]  d     x
> [5,]  e     x
> 
> I want to put the 2nd column of this matrix in a vector without
> duplicates. i.e., my vector v should be (u, v, x), whose length is 3.
> 
> Can anybody help me on this?

> A <- matrix(c("a","b","c","d","e","u","v","x","x","x"), ncol=2)

> A[,2]
[1] "u" "v" "x" "x" "x"

> unique(A[,2])
[1] "u" "v" "x"

> is.vector(unique(A[,2]))
[1] TRUE

  You probably could have helped yourself by checking the results of
RSiteSearch("duplicate") .

> Thanks in advance
> 
> Shubha.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From nitin.jindal at gmail.com  Wed Jan 24 14:36:04 2007
From: nitin.jindal at gmail.com (nitin jindal)
Date: Wed, 24 Jan 2007 07:36:04 -0600
Subject: [R] Logistic regression model + precision/recall
Message-ID: <2b808b010701240536i428562e0h653b26bdd5fcbcaf@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070124/eef78458/attachment.pl 

From ggrothendieck at gmail.com  Wed Jan 24 14:36:45 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 24 Jan 2007 08:36:45 -0500
Subject: [R] Date variable
In-Reply-To: <20070124094645.54531.qmail@web7609.mail.in.yahoo.com>
References: <20070124094645.54531.qmail@web7609.mail.in.yahoo.com>
Message-ID: <971536df0701240536m6baf9a1byf8004c4f934e1d27@mail.gmail.com>

You don't appear to be using any functionality from the date package
but are using the "Date" class which is built into the base of R.
Assuming:

d <- as.Date("03/11/05", "%m/%d/%y")

# and try one of these:

d3 <- structure(rep(NA, 3), class = "Date")
d3[1] <- d
d3

# or

d3 <- rep(d, 3) + NA
d3[1] <- d
d3

# or

d3 <- rep(NA, 3)
class(d3) <- "Date"
d3[1] <- d
d3

# or

d3 <- vector(length = 3, mode = "numeric") + NA
class(d3) <- "Date"
d3[1] <- d
d3




On 1/24/07, stat stat <stat700004 at yahoo.co.in> wrote:
> Dear R users,
>
>  I did following with a date variable
>
>  library(date)
>  date = "03/11/05"
> date = as.Date(date, format="%m/%d/%y")
> date
> [1] "2005-03-11"
> s = vector(length=3)
> s[1] = date
> s[1]
> [1] 12853
>
>  But here I got s[1] as 12853. But this is not that I want. I need s[1] as original date.
>
>  Can anyone tell me where is the mistake?
>
>  Thanks and regards,
>
>
>
> ---------------------------------
>  Here's a new way to find what you're looking for - Yahoo! Answers
>        [[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From ggrothendieck at gmail.com  Wed Jan 24 14:42:30 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 24 Jan 2007 08:42:30 -0500
Subject: [R] How to generate 'minor' ticks in lattice (qqmath)
In-Reply-To: <45B753F9.5070708@bebac.at>
References: <45B62B64.9060700@bebac.at>
	<971536df0701230744i19479d4cq213b85b229f63b57@mail.gmail.com>
	<45B753F9.5070708@bebac.at>
Message-ID: <971536df0701240542r7b5895bbp911d61c1fa14be24@mail.gmail.com>

I think you have a stray trellis.focus statement (just before
the qqmath statement).  Probably a cut and paste problem.
Regards.

On 1/24/07, Helmut Sch?tz <helmut.schuetz at bebac.at> wrote:
> Dear Gabor!
>
> thanks for your hints; as a side-effect I learned a lot about lattice.
>
> The working example is:
>
> library("lattice")
> library("grid")
> numy <- 100
> y <- runif(numy,min=0,max=1)
> sig <- 0.05
> numsig <- length(which(y<sig))
>
> Lower <- 0
> Upper <- 1
> MajorInterval <- 5   # interval for major ticks
> MinorInterval <- 4   # interval within major
> Major <- seq( Lower,Upper,(Upper-Lower)/MajorInterval )
> Minor <- seq( Lower,Upper,(Upper-Lower)/(MajorInterval*MinorInterval) )
> labl <- as.character(Major)
>
> trellis.focus("panel", 1, 1, clip.off = TRUE)
> qqmath(y, distribution = qunif,
>  prepanel = NULL,
>  panel = function(x)
>    {
>    panel.abline(c(0,1), lty = 2)
>    panel.polygon(c(0,0,numsig/numy,numsig/numy,0), c(0,sig,sig,0,0),
> lwd = 0.75)
>    panel.qqmath(x, distribution = qunif, col = 1)
>    },
>    scales=list(x = list(at = Major), y = list(at = Major), tck=c(1,0),
> labels=labl, cex=0.9),
>    xlab = "uniform [0,1] quantiles",
>    ylab = "runif [0,1]",
>    min = 0, max = 1)
> trellis.focus("panel", 1, 1, clip.off = TRUE)
> panel.axis("bottom", check.overlap = TRUE, outside = TRUE, labels =
> FALSE, tck = .5, at = Minor)
> panel.axis("left", check.overlap = TRUE, outside = TRUE, labels = FALSE,
> tck = .5, at = Minor)
> trellis.unfocus()
>
> Best regards,
> Helmut
>
> --
> Helmut Sch?tz
> BEBAC
> Consultancy Services for Bioequivalence and Bioavailability Studies
> Neubaugasse 36/11
> 1070 Vienna/Austria
> tel/fax +43 1 2311746
> Web http://BEBAC.at
> BE/BA Forum http://forum.bebac.at
> http://www.goldmark.org/netrants/no-word/attach.html
>
>
>


From wwwhsd at gmail.com  Wed Jan 24 14:45:17 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Wed, 24 Jan 2007 11:45:17 -0200
Subject: [R] Capturing output from external executables, in windows
In-Reply-To: <45B75D82.6070808@ed.ac.uk>
References: <45B75D82.6070808@ed.ac.uk>
Message-ID: <da79af330701240545y393a788cqe299b9cbdca8976d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070124/29c85ae0/attachment.pl 

From justin_bem at yahoo.fr  Wed Jan 24 14:47:12 2007
From: justin_bem at yahoo.fr (justin bem)
Date: Wed, 24 Jan 2007 13:47:12 +0000 (GMT)
Subject: [R]  Problem with ordered probit model in MASS
Message-ID: <461145.77448.qm@web23010.mail.ird.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070124/461e0f2f/attachment.pl 

From quesada at gmail.com  Wed Jan 24 14:53:00 2007
From: quesada at gmail.com (Jose Quesada)
Date: Wed, 24 Jan 2007 14:53:00 +0100
Subject: [R] [fixed] vectorized nested loop: apply a function that takes
	two rows
Message-ID: <op.tmnvimd0i4ukn7@delllap.ugr.es>

Thanks Charles, Martin,

Substantial improvement with the vectorized solution. Here is a quick benchmark:

# The loop-based solution:
nestedCos = function (x) {
	if (is(x, "Matrix") ) {
		cos 	= array(NA, c(ncol(x), ncol(x)))
		for (i in 2:ncol(x)) {
			for (j in 1:(i - 1)) {
				cos[i, j] = cosine(x[, i], x[, j])
			}
		}
	}
	return(cos)
}
# Charles C. Berry's vectorized approach
flatCos = function (x) {
	res    		= crossprod( x , x )
	diagnl 		= Diagonal( ncol(x), 1 / sqrt( diag( res )))
	cos 		 	= diagnl %*% res %*% diagnl
	return(cos)
}

Benchmarking:

> system.time(for(i in 1:10)nestedCos(x))
(I stopped because it was taking too long)
Timing stopped at: 139.37 3.82 188.76 NA NA
> system.time(for(i in 1:10)flatCos(x))
[1] 0.43 0.00 0.48   NA   NA

#------------------------------------------------------
As much as I like to have faster code, I'm still wondering WHY flatCos gets the same results; i.e., why multiplying the inverse sqrt root of the diagonal of x BY x, then BY the diagonal again produces the expected result. I checked the wikipedia page for crossprod and other sources, but it still eludes me. I can see that scaling by the sqrt of the diagonal once makes sense with 'res <- crossprod( x , x ) gives your result up to scale factors of sqrt(res[i,i]*res[j,j])', but I still don't see why you need to postmultiply by the diagonal again.

Maybe trying to attack a simpler problem might help my understanding: e.g., calculating the cos of a column to all other colums of x (that is, the inner part of the nested loop). How would that work in a vectorized way? I'm trying to get some general technique that I can reuse later from this excellent answer.

Thanks,
-Jose

>
>
> I am rusty on 'Matrix', but I see there are crossprod methods for those
> classes.
>
> 	res <- crossprod( x , x )
>
> gives your result up to scale factors of sqrt(res[i,i]*res[j,j]), so
> something like
>
> 	diagnl <- Diagonal( ncol(x), sqrt( diag( res ) )
>

OOPS! Better make that

   	diagnl <- Diagonal( ncol(x), 1 / sqrt( diag( res ) )

>
> 	final.res <- diagnl %*% res %*% diagnl
>
> should do it.
>

-- 
Cheers,
-Jose

--
Jose Quesada, PhD
Research fellow, Psychology Dept.
Sussex University, Brighton, UK
http://www.andrew.cmu.edu/~jquesada


From jenny197806 at yahoo.se  Wed Jan 24 15:04:57 2007
From: jenny197806 at yahoo.se (Jenny persson)
Date: Wed, 24 Jan 2007 15:04:57 +0100 (CET)
Subject: [R] keep track of selected observations over time
Message-ID: <20070124140457.59268.qmail@web28007.mail.ukl.yahoo.com>

Dear all, 
   
  Attached is a description of my data, graph and the problem which I need help with. Hope you have time to open the file and help me out.
   
  Many thanks, 
  Jenny

 		
---------------------------------


From helmut.schuetz at bebac.at  Wed Jan 24 15:10:48 2007
From: helmut.schuetz at bebac.at (=?ISO-8859-1?Q?Helmut_Sch=FCtz?=)
Date: Wed, 24 Jan 2007 15:10:48 +0100
Subject: [R] How to generate 'minor' ticks in lattice (qqmath)
In-Reply-To: <971536df0701240542r7b5895bbp911d61c1fa14be24@mail.gmail.com>
References: <45B62B64.9060700@bebac.at>	
	<971536df0701230744i19479d4cq213b85b229f63b57@mail.gmail.com>	
	<45B753F9.5070708@bebac.at>
	<971536df0701240542r7b5895bbp911d61c1fa14be24@mail.gmail.com>
Message-ID: <45B768E8.1080401@bebac.at>

Yes thanks, just a kind of 'left-over' ;-)

Regards,
Helmut

Gabor Grothendieck wrote:

> I think you have a stray trellis.focus statement (just before
> the qqmath statement).  Probably a cut and paste problem.
> Regards.
>
>>
>> trellis.focus("panel", 1, 1, clip.off = TRUE)
>

-- 
Helmut Sch?tz
BEBAC
Consultancy Services for Bioequivalence and Bioavailability Studies
Neubaugasse 36/11
1070 Vienna/Austria
tel/fax +43 1 2311746
Web http://BEBAC.at
BE/BA Forum http://forum.bebac.at
http://www.goldmark.org/netrants/no-word/attach.html


From f.harrell at vanderbilt.edu  Wed Jan 24 15:13:34 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 24 Jan 2007 08:13:34 -0600
Subject: [R] Logistic regression model + precision/recall
In-Reply-To: <2b808b010701240536i428562e0h653b26bdd5fcbcaf@mail.gmail.com>
References: <2b808b010701240536i428562e0h653b26bdd5fcbcaf@mail.gmail.com>
Message-ID: <45B7698E.3070800@vanderbilt.edu>

nitin jindal wrote:
> Hi,
> 
> I am using logistic regression model named lrm(Design)
> 
> Rite now I was using Area Under Curve (AUC) for testing my model. But, now I
> have to calculate precision/recall of the model on test cases.
> For lrm, precision and recal would be simply defined with the help of 2
> terms below:
> True Positive (TP) - Number of test cases where class 1 is given probability
>> = 0.5.
> False Negative (FP) - Number of test cases where class 0 is given
> probability >= 0.5.

Why 0.5?

> 
> Precision = TP / (TP + FP)
> Recall = TP / ( Number of Positive Samples in test data)

Those are improper scoring rules that can be tricked.  If the outcome is 
rare (say 0.02 incidence) you could just predict that no one will have 
the outcome and be correct 0.98 of the time.  I suggest validating the 
model for discrimination (e.g., AUC) and calibration.

Frank

> 
> Any help is appreciated.
> 
> I an write a long code with for loops and all, but is there any inbuild
> function or just few commands that would do the task.
> 
> regards,
> Nitin
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From murdoch at stats.uwo.ca  Wed Jan 24 15:13:51 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 24 Jan 2007 09:13:51 -0500
Subject: [R] Capturing output from external executables, in windows
In-Reply-To: <45B75D82.6070808@ed.ac.uk>
References: <45B75D82.6070808@ed.ac.uk>
Message-ID: <45B7699F.5080801@stats.uwo.ca>

On 1/24/2007 8:22 AM, Darren Obbard wrote:
> Hi,
> 
> Any help on the following would be much appreciated
> 
> I wish to capture the output (currently going to console) from an 
> external executable.
> 
> The executable is successfully run using
> 
>      system("program -switch ")
> 
> and the output printed to the DOS console.
> 
> How do I capture this output? I have tried redirecting the output to a 
> text file, and then reading this in
>     
>      system("program -switch > textfile.txt")
>      data<-scan("textfile.txt")
> 
> But this does not seem to work (the textfile.txt is not written). 

The redirection character ">" is normally handled by the shell.  The 
system() function is low level, so it will pass it directly to your program.
 >
It
> does however work if I invoke the console to be permanent 
> 
>      system("cmd /K program -switch > textfile.txt")
>      data<-scan("textfile.txt")
> 
> Unfortunately, this leaves me with an open console window I have to 
> close manually.

Use the /C option instead of /K; it executes and then terminates.  Or 
use the "intern" or "show.output.on.console" arg to system().

Duncan Murdoch

> 
> Is there a way of doing this (under windows) using system( ) or some 
> other command? It appears that pipe( ) may do it, but I cannot 
> understand the documentation.
> 
> An example of the appropriate syntax would be an enormous help.
> 
> Thanks in advance,
> 
> Darren
> 
> Darren.Obbard at ed.ac.uk
>


From P.Dalgaard at biostat.ku.dk  Wed Jan 24 15:35:15 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 24 Jan 2007 15:35:15 +0100
Subject: [R] [fixed] vectorized nested loop: apply a function that takes
 two rows
In-Reply-To: <op.tmnvimd0i4ukn7@delllap.ugr.es>
References: <op.tmnvimd0i4ukn7@delllap.ugr.es>
Message-ID: <45B76EA3.2090908@biostat.ku.dk>

Jose Quesada wrote:
> Thanks Charles, Martin,
>
> Substantial improvement with the vectorized solution. Here is a quick benchmark:
>
> # The loop-based solution:
> nestedCos = function (x) {
> 	if (is(x, "Matrix") ) {
> 		cos 	= array(NA, c(ncol(x), ncol(x)))
> 		for (i in 2:ncol(x)) {
> 			for (j in 1:(i - 1)) {
> 				cos[i, j] = cosine(x[, i], x[, j])
> 			}
> 		}
> 	}
> 	return(cos)
> }
> # Charles C. Berry's vectorized approach
> flatCos = function (x) {
> 	res    		= crossprod( x , x )
> 	diagnl 		= Diagonal( ncol(x), 1 / sqrt( diag( res )))
> 	cos 		 	= diagnl %*% res %*% diagnl
> 	return(cos)
> }
>
> Benchmarking:
>
>   
>> system.time(for(i in 1:10)nestedCos(x))
>>     
> (I stopped because it was taking too long)
> Timing stopped at: 139.37 3.82 188.76 NA NA
>   
>> system.time(for(i in 1:10)flatCos(x))
>>     
> [1] 0.43 0.00 0.48   NA   NA
>
> #------------------------------------------------------
> As much as I like to have faster code, I'm still wondering WHY flatCos gets the same results; i.e., why multiplying the inverse sqrt root of the diagonal of x BY x, then BY the diagonal again produces the expected result. I checked the wikipedia page for crossprod and other sources, but it still eludes me. I can see that scaling by the sqrt of the diagonal once makes sense with 'res <- crossprod( x , x ) gives your result up to scale factors of sqrt(res[i,i]*res[j,j])', but I still don't see why you need to postmultiply by the diagonal again.
>
>   
Didn't follow this thread too closely, but the point would seem to be
that the inner product of two normalized vectors is the cosine of the
angle. So basically, you want

crossprod(X%*%diagnl, X%*%diagnl) == t(diagnl) %*% t(X) %*% X %*% diagnl

I think, BTW, that another version not requiring Matrix is

Cr <- crossprod(X)
D <- sqrt(diag(C))
Cr/outer(D, D)

> Maybe trying to attack a simpler problem might help my understanding: e.g., calculating the cos of a column to all other colums of x (that is, the inner part of the nested loop). How would that work in a vectorized way? I'm trying to get some general technique that I can reuse later from this excellent answer.
>
> Thanks,
> -Jose
>
>   
>> I am rusty on 'Matrix', but I see there are crossprod methods for those
>> classes.
>>
>> 	res <- crossprod( x , x )
>>
>> gives your result up to scale factors of sqrt(res[i,i]*res[j,j]), so
>> something like
>>
>> 	diagnl <- Diagonal( ncol(x), sqrt( diag( res ) )
>>
>>     
>
> OOPS! Better make that
>
>    	diagnl <- Diagonal( ncol(x), 1 / sqrt( diag( res ) )
>
>   
>> 	final.res <- diagnl %*% res %*% diagnl
>>
>> should do it.
>>
>>     
>
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From matz at davide.it  Wed Jan 24 15:37:59 2007
From: matz at davide.it (Stefano)
Date: Wed, 24 Jan 2007 15:37:59 +0100
Subject: [R] double integral with C
Message-ID: <45B76F47.5000905@davide.it>

Hi all,
This is more a C querie rather than a R one:
I'm writing a C code passing a function F to adapt fortran subroutine. I
need to integrate over two variables of F, call them x1 and x2. Then I
call the C code in R to optimize the integrated F function.
for example F could be defined as
---------------------------------------
static double marg_like(const double *param,
            double x1,
            double x2){...........}
---------------------------------------
Then I integrate over x1 and x2 with
-------------------------------------------------
F77_CALL(adapt)(2,(-5,-5),(5,5),100,1700,F,0.01,10000)
-------------------------------------------------

So here my question: I should I define x1 and x2? For the time being I
defined them as static variables, i.e.
static double x1;
static double x2;

but I'm pretty sure this is wrong

Any hint?
thanks in advance
Stefano


From nitin.jindal at gmail.com  Wed Jan 24 15:43:10 2007
From: nitin.jindal at gmail.com (nitin jindal)
Date: Wed, 24 Jan 2007 08:43:10 -0600
Subject: [R] Logistic regression model + precision/recall
In-Reply-To: <45B7698E.3070800@vanderbilt.edu>
References: <2b808b010701240536i428562e0h653b26bdd5fcbcaf@mail.gmail.com>
	<45B7698E.3070800@vanderbilt.edu>
Message-ID: <2b808b010701240643w585bd8dxc1e54858db161217@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070124/dc19124d/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Jan 24 15:44:28 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Jan 2007 14:44:28 +0000 (GMT)
Subject: [R] Capturing output from external executables, in windows
In-Reply-To: <45B75D82.6070808@ed.ac.uk>
References: <45B75D82.6070808@ed.ac.uk>
Message-ID: <Pine.LNX.4.64.0701241437310.21992@gannet.stats.ox.ac.uk>

Is this for RGui under Windows?  I will assume so (but 'in windows' is not 
unambiguous, and there are alternative front-ends like Rterm).

Have you consulted the help page for system()?: this is precisely what 
'show.output.on.console' is for.

You cannot redirect in system (it does say so on the current help page): 
you need to use shell().

On Wed, 24 Jan 2007, Darren Obbard wrote:

> Hi,
>
> Any help on the following would be much appreciated
>
> I wish to capture the output (currently going to console) from an
> external executable.
>
> The executable is successfully run using
>
>     system("program -switch ")
>
> and the output printed to the DOS console.
>
> How do I capture this output? I have tried redirecting the output to a
> text file, and then reading this in
>
>     system("program -switch > textfile.txt")
>     data<-scan("textfile.txt")
>
> But this does not seem to work (the textfile.txt is not written). It
> does however work if I invoke the console to be permanent
>
>     system("cmd /K program -switch > textfile.txt")
>     data<-scan("textfile.txt")
>
> Unfortunately, this leaves me with an open console window I have to
> close manually.
>
> Is there a way of doing this (under windows) using system( ) or some
> other command? It appears that pipe( ) may do it, but I cannot
> understand the documentation.
>
> An example of the appropriate syntax would be an enormous help.

system("program -switch ", show.output.on.console=TRUE)

could it be any easier?  (Well, in the next version of R that will be 
default, so a little.)

> Thanks in advance,
>
> Darren
>
> Darren.Obbard at ed.ac.uk
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Wed Jan 24 15:55:01 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Jan 2007 14:55:01 +0000 (GMT)
Subject: [R] Problem with ordered probit model in MASS
In-Reply-To: <461145.77448.qm@web23010.mail.ird.yahoo.com>
References: <461145.77448.qm@web23010.mail.ird.yahoo.com>
Message-ID: <Pine.LNX.4.64.0701241448410.21992@gannet.stats.ox.ac.uk>

On Wed, 24 Jan 2007, justin bem wrote:

(Again: this is a duplicate post.)

> Dear all,
>
> I got this message, while using the polr function in MASS
>
>> EQ<-as.formula("dep~fpta+tcdv+cdta+cmta+prcd+patc+lactifs+excta")
>> Estim<-polr(EQ,don, subset=(cote!=0),method="probit",na.action=na.omit)
>
> Error in polr(EQ, don, subset = (cote != 0), method = "probit", na.action = na.omit) :
>        attempt for find suitable starting values failed
> In addition: Warning messages:
> 1: algorithm did not converge in: glm.fit(X, y1, wt, family = binomial("probit"), offset = offset)
> 2: fitted probabilities numerically 0 or 1 occurred in: glm.fit(X, y1, wt, family = binomial("probit"), offset = offset)
>
> how can I initialise starting values ?

What do you think the 'start' argument to polr() is for?

If you are asking how you find suitable values, I cannot help you as I 
know nothing about your problem, and failing to find starting values 
usually means that the model is very far from appropriate.

> Justin BEM
> El?ve Ing?nieur Statisticien Economiste
> BP 294 Yaound?.
> T?l (00237)9597295.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From f.harrell at vanderbilt.edu  Wed Jan 24 15:59:44 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 24 Jan 2007 08:59:44 -0600
Subject: [R] Logistic regression model + precision/recall
In-Reply-To: <2b808b010701240643w585bd8dxc1e54858db161217@mail.gmail.com>
References: <2b808b010701240536i428562e0h653b26bdd5fcbcaf@mail.gmail.com>	<45B7698E.3070800@vanderbilt.edu>
	<2b808b010701240643w585bd8dxc1e54858db161217@mail.gmail.com>
Message-ID: <45B77460.4030109@vanderbilt.edu>

nitin jindal wrote:
> On 1/24/07, Frank E Harrell Jr <f.harrell at vanderbilt.edu> wrote:
> 
>> Why 0.5?
> 
> 
> The probability has to adjusted based on some hit and trials. I just
> mentioned it as an example

Using a cutoff is not a good idea unless the utility (loss) function is 
discontinuous and is the same for every subject (in the medical field 
utilities are almost never constant).  And if you are using the data to 
find the cutoff, this will require bootstrapping to penalize for the 
cutoff not being pre-specified.

> 
>> Those are improper scoring rules that can be tricked.  If the outcome is
>> rare (say 0.02 incidence) you could just predict that no one will have
>> the outcome and be correct 0.98 of the time.  I suggest validating the
>> model for discrimination (e.g., AUC) and calibration.
> 
> 
> I just have to calculate precision/recall for rare outcome. If the positive
> outcome is rare ( say 0.02 incidence) and I predict it to be negative all
> the time, my recall would be 0, which is bad. So, precision and recall can
> take care of skewed data.

No, that is not clear.  The overall classification error would only be 
0.02 in that case.  It is true though that one of the two conditional 
probabilities would not be good.

> 
> Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From nitin.jindal at gmail.com  Wed Jan 24 16:23:12 2007
From: nitin.jindal at gmail.com (nitin jindal)
Date: Wed, 24 Jan 2007 09:23:12 -0600
Subject: [R] Logistic regression model + precision/recall
In-Reply-To: <45B77460.4030109@vanderbilt.edu>
References: <2b808b010701240536i428562e0h653b26bdd5fcbcaf@mail.gmail.com>
	<45B7698E.3070800@vanderbilt.edu>
	<2b808b010701240643w585bd8dxc1e54858db161217@mail.gmail.com>
	<45B77460.4030109@vanderbilt.edu>
Message-ID: <2b808b010701240723t2b0de5aaq31a05ee3b80b380a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070124/fdd89d2f/attachment.pl 

From nitin.jindal at gmail.com  Wed Jan 24 16:24:29 2007
From: nitin.jindal at gmail.com (nitin jindal)
Date: Wed, 24 Jan 2007 09:24:29 -0600
Subject: [R] Logistic regression model + precision/recall
In-Reply-To: <c3ca233a0701240712y4dc76459q7f830ebe5369b89e@mail.gmail.com>
References: <2b808b010701240536i428562e0h653b26bdd5fcbcaf@mail.gmail.com>
	<c3ca233a0701240712y4dc76459q7f830ebe5369b89e@mail.gmail.com>
Message-ID: <2b808b010701240724gf1dccc6k75385316a32c32bf@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070124/ec3ceec1/attachment.pl 

From daniel.kumpik at physiol.ox.ac.uk  Wed Jan 24 16:25:39 2007
From: daniel.kumpik at physiol.ox.ac.uk (dan kumpik)
Date: Wed, 24 Jan 2007 15:25:39 +0000
Subject: [R] mixed effects or fixed effects?
Message-ID: <45B77A73.4020808@physiol.ox.ac.uk>

Hi,

I am running a learning experiment in which both training subjects and 
controls complete a pretest and posttest. All analyses are being 
conducted in R. We are looking to compare two training methodologies, 
and so have run this experiment twice, once with each methodology. 
Methodology is a between-subjects factor. Trying to run this analysis 
with every factor included (ie, subject as a random factor, session 
nested within group nested within experiment) seems to me (after having 
tried) to be clumsy and probably uninterpretable.
	My favoured model for the analysis is a linear mixed-effects model, and 
to combine the data meaningfully, I have collated all the pretest data 
for controls and trained subjects from each experiment, and assumed this 
data to represent a population sample for naive subjects for each 
experiment. I have also ditched the posttest data for the controls, and 
assumed the posttest training data to represent a population sample for 
trained subjects for each experiment. I have confirmed the validity of 
these assumptions by ascertaining that a) controls and trained listeners 
did not differ significantly at pretest for either experiment; and b) 
control listeners did not learn significantly between pretest and 
posttest (and therefore their posttest data are not relevant). This was 
done using a linear mixed-effects model for each experiment, with 
subject as a random factor and session (pretest vs posttest) nested 
within Group (trained vs control).
	Therefore, the model I want to use to analyse the data would ideally be 
a linear mixed-effects model, with subject as a random factor, and 
session (pre vs post) nested within experiment. Note that my removal of 
the Group (Trained vs Control) factor simplifies the model somewhat, and 
makes it more interpretable in terms of evaluating the relative effects 
of each experiment.
	What I would like to know is- a) would people agree that this is a 
meaningful way to combine my data? I believe the logic is sound, but am 
slightly concerned that I am ignoring a whole block of posttest data for 
the controls (even though this does not account for a significant amount 
of the variance); and b) given that each of my trained subjects appear 
twice- one in the pretest and once in the posttest, and the controls 
only appear once- in the pretest sample, is there any problem with 
making subject a random factor? Conceptually, I see no problem with 
this, but I would like to be sure before I finish writing up.

Many thanks for your time

Dan


From HDoran at air.org  Wed Jan 24 16:40:11 2007
From: HDoran at air.org (Doran, Harold)
Date: Wed, 24 Jan 2007 10:40:11 -0500
Subject: [R] Replace missing values in lapply
Message-ID: <2323A6D37908A847A7C32F1E3662C80E8D4535@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070124/b60a032a/attachment.pl 

From dimitris.rizopoulos at med.kuleuven.be  Wed Jan 24 16:48:37 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 24 Jan 2007 16:48:37 +0100
Subject: [R] Replace missing values in lapply
References: <2323A6D37908A847A7C32F1E3662C80E8D4535@dc1ex01.air.org>
Message-ID: <014901c73fcf$1ce2bc30$0540210a@www.domain>

you need to return x in the function within lapply(), e.g., something 
like

lapply(TP, function(x) { x[is.na(x)] <- 0; x })


I hope it works.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Doran, Harold" <HDoran at air.org>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, January 24, 2007 4:40 PM
Subject: [R] Replace missing values in lapply


>I have some matrices stored as elements in a list that I am working
> with. On example is provided below as TP[[18]]
>
>> TP[[18]]
>      level2
> level1  1  2  3  4
>     1 79  0  0  0
>     2  0  0  0  0
>     3  0  0  0  0
>     4  0  0  0  0
>
> Now, using prop.table on this gives
>
>> prop.table(TP[[18]],1)
>      level2
> level1   1   2   3   4
>     1   1   0   0   0
>     2
>     3
>     4
>
> It is important for the zero's to retain their position as this 
> matrix
> will subsequently be used in some matrix multiplication and hence, 
> must
> be of dimension 4 by 4 so that is it conformable for multiplcation 
> with
> another matrix.
>
> In looking at the structure of the object resulting from prop.table 
> I
> see NaNs, and so I can do this
>
>> rr <- TP[[18]]
>> rr[is.na(rr)] <- 0
>> rr
>      level2
> level1  1  2  3  4
>     1 79  0  0  0
>     2  0  0  0  0
>     3  0  0  0  0
>     4  0  0  0  0
>
> This is exactly what I want for each matrix. But, I have multiple
> matrices stored within the list that need to be changed and so I am
> trying to resolve this via lapply, but something is awry (namely the
> user), but I could use a little help.
>
> I was thinking the following function should work, but it doesn't. 
> It
> reduces each matrix within the list to a 0.
>
> PP <- lapply(TP, function(x) x[is.na(x)] <- 0)
>
> Am I missing something obvious?
>
> Harold
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From HDoran at air.org  Wed Jan 24 16:50:13 2007
From: HDoran at air.org (Doran, Harold)
Date: Wed, 24 Jan 2007 10:50:13 -0500
Subject: [R] Replace missing values in lapply
Message-ID: <2323A6D37908A847A7C32F1E3662C80E8D4539@dc1ex01.air.org>

Perfect, thxs 

> -----Original Message-----
> From: Dimitris Rizopoulos 
> [mailto:dimitris.rizopoulos at med.kuleuven.be] 
> Sent: Wednesday, January 24, 2007 10:49 AM
> To: Doran, Harold
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Replace missing values in lapply
> 
> you need to return x in the function within lapply(), e.g., 
> something like
> 
> lapply(TP, function(x) { x[is.na(x)] <- 0; x })
> 
> 
> I hope it works.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
> 
> 
> 
> ----- Original Message -----
> From: "Doran, Harold" <HDoran at air.org>
> To: <r-help at stat.math.ethz.ch>
> Sent: Wednesday, January 24, 2007 4:40 PM
> Subject: [R] Replace missing values in lapply
> 
> 
> >I have some matrices stored as elements in a list that I am working
> > with. On example is provided below as TP[[18]]
> >
> >> TP[[18]]
> >      level2
> > level1  1  2  3  4
> >     1 79  0  0  0
> >     2  0  0  0  0
> >     3  0  0  0  0
> >     4  0  0  0  0
> >
> > Now, using prop.table on this gives
> >
> >> prop.table(TP[[18]],1)
> >      level2
> > level1   1   2   3   4
> >     1   1   0   0   0
> >     2
> >     3
> >     4
> >
> > It is important for the zero's to retain their position as this 
> > matrix
> > will subsequently be used in some matrix multiplication and hence, 
> > must
> > be of dimension 4 by 4 so that is it conformable for multiplcation 
> > with
> > another matrix.
> >
> > In looking at the structure of the object resulting from prop.table 
> > I
> > see NaNs, and so I can do this
> >
> >> rr <- TP[[18]]
> >> rr[is.na(rr)] <- 0
> >> rr
> >      level2
> > level1  1  2  3  4
> >     1 79  0  0  0
> >     2  0  0  0  0
> >     3  0  0  0  0
> >     4  0  0  0  0
> >
> > This is exactly what I want for each matrix. But, I have multiple
> > matrices stored within the list that need to be changed and so I am
> > trying to resolve this via lapply, but something is awry (namely the
> > user), but I could use a little help.
> >
> > I was thinking the following function should work, but it doesn't. 
> > It
> > reduces each matrix within the list to a 0.
> >
> > PP <- lapply(TP, function(x) x[is.na(x)] <- 0)
> >
> > Am I missing something obvious?
> >
> > Harold
> >
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
>


From res90sx5 at verizon.net  Wed Jan 24 16:47:59 2007
From: res90sx5 at verizon.net (Daniel Nordlund)
Date: Wed, 24 Jan 2007 07:47:59 -0800
Subject: [R] solving a structural equation model using sem or other
	package
In-Reply-To: <815b70590701240332v7bb4c445n86f338751e8088a6@mail.gmail.com>
Message-ID: <001f01c73fcf$069e0d30$6501a8c0@Aragorn>

David,

Thanks for the help.  I missed the significance of the section you quoted below from the help.  That does indeed solve the problem.

Dan

Dan Nordlund
Bothell, WA  USA

> -----Original Message-----
> From: David Barron [mailto:mothsailor at googlemail.com]
> Sent: Wednesday, January 24, 2007 3:32 AM
> To: Daniel Nordlund; r-help
> Subject: Re: [R] solving a structural equation model using sem or other package
> 
> This is an extract from the sem help page, which deals with your situation:
> 
> S covariance matrix among observed variables; may be input as a
> symmetric matrix, or as a lower- or upper-triangular matrix. S may
> also be a raw (i.e., ``uncorrected'') moment matrix ? that is, a
> sum-of-squares-and-products matrix divided by N. This form of input is
> useful for fitting models with intercepts, in which case the moment
> matrix should include the mean square and cross-products for a unit
> variable all of whose entries are 1; of course, the raw mean square
> for the unit variable is 1. Raw-moment matrices may be computed by
> raw.moments.
> 
> On 24/01/07, Daniel Nordlund <res90sx5 at verizon.net> wrote:
> > I am trying to work my way through the book "Singer, JD and Willett, JB, Applied
> Longitudinal Data Analysis. Oxford University Press, 2003"  using R.  I have the SAS
> code and S-Plus code from the UCLA site (doesn't include chapter 8 or later
> problems).  In chapter 8, there is a structural equation/path model which can be
> specified for the sem package as follows
> >
<<<snip>>>
> > An equivalent specification in SAS produces the solution presented in the book.  The
> variable cons is a constant vector of 1's.  The problem with the sem package is that the
> covariance matrix which includes the variable cons is singular and sem says so and
> will not continue.  Is there an alternative way to specify this problem for sem to obtain
> a solution?  If not, is there another package that would produce a solution?
> >
> > Thanks,
> >
> > Dan Nordlund
> > Bothell, WA
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 
> --
> =================================
> David Barron
> Said Business School
> University of Oxford
> Park End Street
> Oxford OX1 1HP


From ggrothendieck at gmail.com  Wed Jan 24 17:06:16 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 24 Jan 2007 11:06:16 -0500
Subject: [R] Replace missing values in lapply
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E8D4535@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E8D4535@dc1ex01.air.org>
Message-ID: <971536df0701240806wa6a8271t861502dcdf39f46a@mail.gmail.com>

I wonder if a list of matrices is the best representation?
Do your matrices all have the same dimension as in:

TP <- list(matrix(c(1:3, NA), 2), matrix(c(NA, 1:3), 2))

# Then you could consider representing them as an array:

TPa <- array(unlist(TP), c(2,2,2))

# in which case its just

TPa[is.na(TPa)] <- 0
TPa


On 1/24/07, Doran, Harold <HDoran at air.org> wrote:
> I have some matrices stored as elements in a list that I am working
> with. On example is provided below as TP[[18]]
>
> > TP[[18]]
>      level2
> level1  1  2  3  4
>     1 79  0  0  0
>     2  0  0  0  0
>     3  0  0  0  0
>     4  0  0  0  0
>
> Now, using prop.table on this gives
>
> > prop.table(TP[[18]],1)
>      level2
> level1   1   2   3   4
>     1   1   0   0   0
>     2
>     3
>     4
>
> It is important for the zero's to retain their position as this matrix
> will subsequently be used in some matrix multiplication and hence, must
> be of dimension 4 by 4 so that is it conformable for multiplcation with
> another matrix.
>
> In looking at the structure of the object resulting from prop.table I
> see NaNs, and so I can do this
>
> > rr <- TP[[18]]
> > rr[is.na(rr)] <- 0
> > rr
>      level2
> level1  1  2  3  4
>     1 79  0  0  0
>     2  0  0  0  0
>     3  0  0  0  0
>     4  0  0  0  0
>
> This is exactly what I want for each matrix. But, I have multiple
> matrices stored within the list that need to be changed and so I am
> trying to resolve this via lapply, but something is awry (namely the
> user), but I could use a little help.
>
> I was thinking the following function should work, but it doesn't. It
> reduces each matrix within the list to a 0.
>
> PP <- lapply(TP, function(x) x[is.na(x)] <- 0)
>
> Am I missing something obvious?
>
> Harold
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From HDoran at air.org  Wed Jan 24 17:15:50 2007
From: HDoran at air.org (Doran, Harold)
Date: Wed, 24 Jan 2007 11:15:50 -0500
Subject: [R] Replace missing values in lapply
Message-ID: <2323A6D37908A847A7C32F1E3662C80E8D453E@dc1ex01.air.org>

I hadn't thought of that. I use the following at one point in my program

tmp <- with(data, tapply(variable, index, table)) 

Which returns a list. So, I just went with it for the rest of my
program. I'm changing code now to arrays, I think you're right and this
may be a better representation. I need to walk through this and see what
turns up.

Thanks for the recommendation.

> -----Original Message-----
> From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
> Sent: Wednesday, January 24, 2007 11:06 AM
> To: Doran, Harold
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Replace missing values in lapply
> 
> I wonder if a list of matrices is the best representation?
> Do your matrices all have the same dimension as in:
> 
> TP <- list(matrix(c(1:3, NA), 2), matrix(c(NA, 1:3), 2))
> 
> # Then you could consider representing them as an array:
> 
> TPa <- array(unlist(TP), c(2,2,2))
> 
> # in which case its just
> 
> TPa[is.na(TPa)] <- 0
> TPa
> 
> 
> On 1/24/07, Doran, Harold <HDoran at air.org> wrote:
> > I have some matrices stored as elements in a list that I am working 
> > with. On example is provided below as TP[[18]]
> >
> > > TP[[18]]
> >      level2
> > level1  1  2  3  4
> >     1 79  0  0  0
> >     2  0  0  0  0
> >     3  0  0  0  0
> >     4  0  0  0  0
> >
> > Now, using prop.table on this gives
> >
> > > prop.table(TP[[18]],1)
> >      level2
> > level1   1   2   3   4
> >     1   1   0   0   0
> >     2
> >     3
> >     4
> >
> > It is important for the zero's to retain their position as 
> this matrix 
> > will subsequently be used in some matrix multiplication and hence, 
> > must be of dimension 4 by 4 so that is it conformable for 
> > multiplcation with another matrix.
> >
> > In looking at the structure of the object resulting from 
> prop.table I 
> > see NaNs, and so I can do this
> >
> > > rr <- TP[[18]]
> > > rr[is.na(rr)] <- 0
> > > rr
> >      level2
> > level1  1  2  3  4
> >     1 79  0  0  0
> >     2  0  0  0  0
> >     3  0  0  0  0
> >     4  0  0  0  0
> >
> > This is exactly what I want for each matrix. But, I have multiple 
> > matrices stored within the list that need to be changed and so I am 
> > trying to resolve this via lapply, but something is awry 
> (namely the 
> > user), but I could use a little help.
> >
> > I was thinking the following function should work, but it 
> doesn't. It 
> > reduces each matrix within the list to a 0.
> >
> > PP <- lapply(TP, function(x) x[is.na(x)] <- 0)
> >
> > Am I missing something obvious?
> >
> > Harold
> >
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From yvonnick.noel at uhb.fr  Wed Jan 24 17:29:14 2007
From: yvonnick.noel at uhb.fr (NOEL Yvonnick)
Date: Wed, 24 Jan 2007 17:29:14 +0100
Subject: [R] as.numeric(".1")
Message-ID: <45B7895A.6090309@uhb.fr>

Hello,

I noticed the following strange behavior under R-2.4.0 (Linux Mandriva 
2007) :

 > options("OutDec")
$OutDec
[1] "."

 > as.numeric(".1")
[1] NA
Warning message:
NAs introduits lors de la conversion automatique

 > as.numeric(",1")
[1] 0,1

So I need to use the comma as the decimal separator, at least as input. 
Moreover, the last output also use a comma, though the OutDec option was 
set to ".". Basic arithmetic ops on the command line work OK with 
decimal dots.

I am pretty sure as.numeric(".1") used to work under older versions of 
R.  Could it be a localization problem ?

I would like to use the dot as the decimal separator both for input and 
output. Any suggestion ?

Thank you very much in advance,

Yvonnick Noel
Dprt of Psychology
U. of Rennes
France


platform       i686-pc-linux-gnu
arch           i686
os             linux-gnu
system         i686, linux-gnu
status
major          2
minor          4.0
year           2006
month          10
day            03
svn rev        39566
language       R


From b.otto at uke.uni-hamburg.de  Wed Jan 24 17:34:45 2007
From: b.otto at uke.uni-hamburg.de (Benjamin Otto)
Date: Wed, 24 Jan 2007 17:34:45 +0100
Subject: [R] Fit model to data and use model for data generation
Message-ID: <001501c73fd5$8fb9f060$336f12ac@matrix.com>

Hi,

Suppose I have a set of values x and I want to calculate the distribution of
the data. Ususally I would use the "density" command. Now, can I use the
resulting "density-object" model to generate a number of new values which
have the same distribution? Or do I have to use some different function?

Regards,

Benjamin

-- 
Benjamin Otto
Universitaetsklinikum Eppendorf Hamburg
Institut fuer Klinische Chemie
Martinistrasse 52
20246 Hamburg


From maechler at stat.math.ethz.ch  Wed Jan 24 17:57:01 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 24 Jan 2007 17:57:01 +0100
Subject: [R] as.numeric(".1")
In-Reply-To: <45B7895A.6090309@uhb.fr>
References: <45B7895A.6090309@uhb.fr>
Message-ID: <17847.36829.31217.7004@stat.math.ethz.ch>

>>>>> "NOEL" == NOEL Yvonnick <yvonnick.noel at uhb.fr>
>>>>>     on Wed, 24 Jan 2007 17:29:14 +0100 writes:

    NOEL> Hello,
    NOEL> I noticed the following strange behavior under R-2.4.0 (Linux Mandriva 
    NOEL> 2007) :

    >> options("OutDec")
    NOEL> $OutDec
    NOEL> [1] "."

    >> as.numeric(".1")
    NOEL> [1] NA
    NOEL> Warning message:
    NOEL> NAs introduits lors de la conversion automatique

    >> as.numeric(",1")
    NOEL> [1] 0,1

Oops !  Should not happen, given your  getOption("OutDec") 

    NOEL> So I need to use the comma as the decimal separator,
    NOEL> at least as input.  Moreover, the last output also use
    NOEL> a comma, though the OutDec option was set to
    NOEL> ".". Basic arithmetic ops on the command line work OK
    NOEL> with decimal dots.

    NOEL> I am pretty sure as.numeric(".1") used to work under older versions of 
    NOEL> R.  Could it be a localization problem ?

maybe / probably.  We cannot easily reproduce.
Instead of the output below, can you please give the full

     sessionInfo()

output?


    NOEL> I would like to use the dot as the decimal separator
    NOEL> both for input and output.

Well definitely!  (Using "," is crazyness in my eyes!!)

    NOEL> Any suggestion ?

    NOEL> Thank you very much in advance,

    NOEL> Yvonnick Noel
    NOEL> Dprt of Psychology
    NOEL> U. of Rennes
    NOEL> France


    NOEL> platform       i686-pc-linux-gnu
    NOEL> arch           i686
    NOEL> os             linux-gnu
    NOEL> system         i686, linux-gnu
    NOEL> status
    NOEL> major          2
    NOEL> minor          4.0
    NOEL> year           2006
    NOEL> month          10
    NOEL> day            03
    NOEL> svn rev        39566
    NOEL> language       R


From vincent.goulet at act.ulaval.ca  Wed Jan 24 18:03:23 2007
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Wed, 24 Jan 2007 12:03:23 -0500
Subject: [R] Easy to install GNU Emacs for Windows
Message-ID: <200701241203.23752.vincent.goulet@act.ulaval.ca>

[Sorry for cross-posting in an attempt to reach as many interested parties as 
possible.]

Users (present or future) of GNU Emacs and ESS on Windows might be interested 
in my distribution of an easy to install (read: with an installation wizard) 
version of GNU Emacs 21.3 with the following additions: 

* ESS 5.3.3 configured to work with the latest stable release of R;
* AUCTeX 11.84;
* Aspell 0.50.3;
* English and French dictionaries for Aspell;
* w32-winprint.el, to ease printing under Windows;
* htmlize.el, to print in color with w32-winprint.el;
* site-start.el, a site wide configuration file to make everything work.

For details:

	http://vgoulet.act.ulaval.ca/en/emacs/

The plan is to keep my distribution current with R releases.

The installation wizard is not all that smart for now, but I'll try to improve 
it in the future. For example, it creates a HOME environment variable (if it 
doesn't already exist), but doesn't ask for a value. %USERPROFILE% is used as 
default value.

Comments, criticisms and translation of messages appreciated!

[Disclaimer: I am not a WIndows user myself. I created this distribution to 
ease adoption of Emacs by my students.]

-- 
  Vincent Goulet, Associate Professor
  ?cole d'actuariat
  Universit? Laval, Qu?bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca


From yvonnick.noel at uhb.fr  Wed Jan 24 18:14:30 2007
From: yvonnick.noel at uhb.fr (NOEL Yvonnick)
Date: Wed, 24 Jan 2007 18:14:30 +0100
Subject: [R] as.numeric(".1") + SessionInfo
In-Reply-To: <17847.36829.31217.7004@stat.math.ethz.ch>
References: <45B7895A.6090309@uhb.fr>
	<17847.36829.31217.7004@stat.math.ethz.ch>
Message-ID: <45B793F6.9060507@uhb.fr>

>     >> as.numeric(",1")
>     NOEL> [1] 0,1
> Instead of the output below, can you please give the full
> 
>      sessionInfo()
> 
> output?


Here it is:

R version 2.4.0 (2006-10-03)
i686-pc-linux-gnu

locale:
fr_FR.UTF-8

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
       lattice   cairoDevice gWidgetsRGtk2         RGtk2      gWidgets
     "0.14-16"         "1.2"       "0.0-9"       "2.8.6"      "0.0-11"

Yvonnick Noel, PhD.
Dpt of Psychology
U. of Rennes
France


From bolker at zoo.ufl.edu  Wed Jan 24 18:21:55 2007
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Wed, 24 Jan 2007 17:21:55 +0000 (UTC)
Subject: [R] how to write randomforest in r
References: <6c1e1b7f0701230746h7c829dd4s82223486574ec496@mail.gmail.com>
Message-ID: <loom.20070124T181303-244@post.gmane.org>

myat wai <wmyonwesit <at> gmail.com> writes:

> 
> Dear Sir,
> I want to know how to do for getting the results.
> 1. data set is not in r.
> how to use my data set in r.
> 2. using randomForest function to build tree with my data set
> how to write for it
> 3. using this random forest how to predict the new data
> Please reply me.I want to bulid random forest in r and predict the new
> data. My data set is in the attachment file.Like my attachment file,I
> want to get the results in R  as the output.
> Please help me.
> Your Sincerely,
> Myat
> 

  I'm afraid your question is far too vague.
At the very least you need to (1) indicate what format your data
are in (there are very many "non-R" data formats!) and (2) indicate
that you have actually read the documentation for the randomForest function
(and the predict.randomForest function, which would probably help
you predict new data!), as well as the Introduction to R.  Then
you can tell us where you got stuck, which will both make it easier
for us to help you and perhaps help the authors improve the
documentation.
  (Your attachment appears to have gotten lost somewhere along
the way.)
  If this is too much for you, you will need to find someone
(preferably someone at your own institution) who can help you
get started with R basics.
  Reading the posting guide wouldn't hurt either.

  good luck,
    Ben Bolker


From ripley at stats.ox.ac.uk  Wed Jan 24 18:56:19 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Jan 2007 17:56:19 +0000 (GMT)
Subject: [R] as.numeric(".1") + SessionInfo
In-Reply-To: <45B793F6.9060507@uhb.fr>
References: <45B7895A.6090309@uhb.fr>
	<17847.36829.31217.7004@stat.math.ethz.ch>
	<45B793F6.9060507@uhb.fr>
Message-ID: <Pine.LNX.4.64.0701241735170.28321@gannet.stats.ox.ac.uk>

I can reproduce this via

> Sys.setlocale("LC_NUMERIC", "fr_FR")
[1] "fr_FR"
Warning message:
setting 'LC_NUMERIC' may cause R to function strangely in: 
setlocale(category, locale)
> as.numeric(",1")
[1] 0,1
> as.numeric(".1")
[1] NA
Warning message:
NAs introduced by coercion

Assuming you have not done that anywhere, it should not happen. 
If you have, you were warned.  (Have you tried starting R with --vanilla 
to be sure?)

as.numeric() is using strtod which should only be affected by the locale 
category LC_NUMERIC, and R itself does not set LC_NUMERIC.  So either you 
or some rogue OS function must have, unless there is a pretty major bug in 
the OS.  (Just using a UTF-8 fr_FR locale does not do it on either of the 
Linux variants I tried.)

On Wed, 24 Jan 2007, NOEL Yvonnick wrote:

>>    >> as.numeric(",1")
>>     NOEL> [1] 0,1
>> Instead of the output below, can you please give the full
>>
>>      sessionInfo()
>>
>> output?
>
>
> Here it is:
>
> R version 2.4.0 (2006-10-03)
> i686-pc-linux-gnu
>
> locale:
> fr_FR.UTF-8
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
>
> other attached packages:
>       lattice   cairoDevice gWidgetsRGtk2         RGtk2      gWidgets
>     "0.14-16"         "1.2"       "0.0-9"       "2.8.6"      "0.0-11"
>
> Yvonnick Noel, PhD.
> Dpt of Psychology
> U. of Rennes
> France
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From helprhelp at gmail.com  Wed Jan 24 19:17:10 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 24 Jan 2007 13:17:10 -0500
Subject: [R] Cronbach's alpha
Message-ID: <cdf817830701241017i4157c5d6q8dbfccf2c104c7f6@mail.gmail.com>

Dear Listers:

I used cronbach{psy} to evaluate the internal consistency and some set
of variables gave me alpha=-1.1003, while other, alpha=-0.2;
alpha=0.89; and so on. I am interested in knowing how to interpret
1. negative value
2. negative value less than -1.

I also want to re-mention my previous question about how to evaluate
the consistency of a set of variables and about the total correlation
(my 2 cent to answer the question). Is there any function in R to do
that?

Thank you very much!



-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From HDoran at air.org  Wed Jan 24 19:33:30 2007
From: HDoran at air.org (Doran, Harold)
Date: Wed, 24 Jan 2007 13:33:30 -0500
Subject: [R] Cronbach's alpha
Message-ID: <2323A6D37908A847A7C32F1E3662C80E8D4557@dc1ex01.air.org>

Weiwei

Something is wrong. Coefficient alpha is bounded between 0 and 1, so
negative values are outside the parameter space for a reliability
statistic. Recall that reliability is the ratio of "true score" variance
to "total score variance". That is

var(t)/ var(t) + var(e)

If all variance is true score variance, then var(e)=0 and the
reliability is var(t)/var(t)=1. On the other hand, if all variance is
measurement error, then var(t) = 0 and reliability is 0.

Here is a function I wrote to compute alpha along with an example. Maybe
try recomputing your statistic using this function and see if you get
the same result.

alpha <- function(columns){
	k <- ncol(columns)
	colVars <- apply(columns, 2, var)
	total   <- var(apply(columns, 1, sum))
	a <- (total - sum(colVars)) / total * (k/(k-1))
	a
 	}

data(LSAT, package='ltm')
> alpha(LSAT)
[1] 0.2949972 


Harold

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Weiwei Shi
> Sent: Wednesday, January 24, 2007 1:17 PM
> To: R R
> Subject: [R] Cronbach's alpha
> 
> Dear Listers:
> 
> I used cronbach{psy} to evaluate the internal consistency and 
> some set of variables gave me alpha=-1.1003, while other, 
> alpha=-0.2; alpha=0.89; and so on. I am interested in knowing 
> how to interpret 1. negative value 2. negative value less than -1.
> 
> I also want to re-mention my previous question about how to 
> evaluate the consistency of a set of variables and about the 
> total correlation (my 2 cent to answer the question). Is 
> there any function in R to do that?
> 
> Thank you very much!
> 
> 
> 
> --
> Weiwei Shi, Ph.D
> Research Scientist
> GeneGO, Inc.
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ssj1364 at gmail.com  Wed Jan 24 19:48:59 2007
From: ssj1364 at gmail.com (sj)
Date: Wed, 24 Jan 2007 11:48:59 -0700
Subject: [R] n step ahead forecasts
Message-ID: <1c6126db0701241048v7b7a6fa7g70d47e587484a55c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070124/e0b48621/attachment.pl 

From Lukas.Indermaur at eawag.ch  Wed Jan 24 20:27:04 2007
From: Lukas.Indermaur at eawag.ch (Indermaur Lukas)
Date: Wed, 24 Jan 2007 20:27:04 +0100
Subject: [R] dataframe operation
Message-ID: <FE8C160D1505B24497FA7C78D4DADACA0478CE@EA-MAIL.eawag.wroot.emp-eaw.ch>

hi
i have a dataframe "a" which looks like:
 
column1, column2, column3
10,12, 0
NA, 0,1
12,NA,50
 
i want to replace all values in column1 to column3 which do not contain "NA" with values of vector "b" (100,200,300).
 
any idea i can do it?
 
i appreciate any hint
regards
lukas
 
 
 
??? 
Lukas Indermaur, PhD student 
eawag / Swiss Federal Institute of Aquatic Science and Technology 
ECO - Department of Aquatic Ecology
?berlandstrasse 133
CH-8600 D?bendorf
Switzerland
 
Phone: +41 (0) 71 220 38 25
Fax    : +41 (0) 44 823 53 15 
Email: lukas.indermaur at eawag.ch
www.lukasindermaur.ch


From marc_schwartz at comcast.net  Wed Jan 24 21:10:54 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 24 Jan 2007 14:10:54 -0600
Subject: [R] dataframe operation
In-Reply-To: <FE8C160D1505B24497FA7C78D4DADACA0478CE@EA-MAIL.eawag.wroot.emp-eaw.ch>
References: <FE8C160D1505B24497FA7C78D4DADACA0478CE@EA-MAIL.eawag.wroot.emp-eaw.ch>
Message-ID: <1169669454.4873.18.camel@localhost.localdomain>

On Wed, 2007-01-24 at 20:27 +0100, Indermaur Lukas wrote:
> hi
> i have a dataframe "a" which looks like:
>  
> column1, column2, column3
> 10,12, 0
> NA, 0,1
> 12,NA,50
>  
> i want to replace all values in column1 to column3 which do not contain "NA" with values of vector "b" (100,200,300).
>  
> any idea i can do it?
>  
> i appreciate any hint
> regards
> lukas
>  

Here is one possibility:

> sapply(seq(along = colnames(DF)), 
         function(x) ifelse(is.na(DF[[x]]), 100 * x, DF[[x]]))
     [,1] [,2] [,3]
[1,]   10   12    0
[2,]  100    0    1
[3,]   12  200   50


Note that the returned object will be a matrix, so if you need a data
frame, just coerce the result with as.data.frame().

HTH,

Marc Schwartz


From mike.prager at noaa.gov  Wed Jan 24 21:11:17 2007
From: mike.prager at noaa.gov (Mike Prager)
Date: Wed, 24 Jan 2007 15:11:17 -0500
Subject: [R] dataframe operation
References: <FE8C160D1505B24497FA7C78D4DADACA0478CE@EA-MAIL.eawag.wroot.emp-eaw.ch>
Message-ID: <5affr2tflof5cf1af0di2n8p7hte5cusr1@4ax.com>

Hint:  Try 

?subset

at the R prompt



"Indermaur Lukas" <Lukas.Indermaur at eawag.ch> wrote:

> hi
> i have a dataframe "a" which looks like:
>  
> column1, column2, column3
> 10,12, 0
> NA, 0,1
> 12,NA,50
>  
> i want to replace all values in column1 to column3 which do not contain "NA" with values of vector "b" (100,200,300).
>  
> any idea i can do it?
>  
> i appreciate any hint

-- 
Mike Prager, NOAA, Beaufort, NC
* Opinions expressed are personal and not represented otherwise.
* Any use of tradenames does not constitute a NOAA endorsement.


From marc_schwartz at comcast.net  Wed Jan 24 21:16:06 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 24 Jan 2007 14:16:06 -0600
Subject: [R] dataframe operation
In-Reply-To: <1169669454.4873.18.camel@localhost.localdomain>
References: <FE8C160D1505B24497FA7C78D4DADACA0478CE@EA-MAIL.eawag.wroot.emp-eaw.ch>
	<1169669454.4873.18.camel@localhost.localdomain>
Message-ID: <1169669766.4873.21.camel@localhost.localdomain>

On Wed, 2007-01-24 at 14:10 -0600, Marc Schwartz wrote:
> On Wed, 2007-01-24 at 20:27 +0100, Indermaur Lukas wrote:
> > hi
> > i have a dataframe "a" which looks like:
> >  
> > column1, column2, column3
> > 10,12, 0
> > NA, 0,1
> > 12,NA,50
> >  
> > i want to replace all values in column1 to column3 which do not contain "NA" with values of vector "b" (100,200,300).
> >  
> > any idea i can do it?
> >  
> > i appreciate any hint
> > regards
> > lukas
> >  
> 
> Here is one possibility:
> 
> > sapply(seq(along = colnames(DF)), 
>          function(x) ifelse(is.na(DF[[x]]), 100 * x, DF[[x]]))
>      [,1] [,2] [,3]
> [1,]   10   12    0
> [2,]  100    0    1
> [3,]   12  200   50
> 
> 
> Note that the returned object will be a matrix, so if you need a data
> frame, just coerce the result with as.data.frame().

OK....that's what I get for pulling the trigger too fast.

Just reverse the logic in the function:

> sapply(seq(along = colnames(DF)), 
         function(x) ifelse(!is.na(DF[[x]]), 100 * x, DF[[x]]))
     [,1] [,2] [,3]
[1,]  100  200  300
[2,]   NA  200  300
[3,]  100   NA  300


I misread the query initially.

HTH,

Marc


From jgaseff at gmail.com  Wed Jan 24 21:25:31 2007
From: jgaseff at gmail.com (jgaseff)
Date: Wed, 24 Jan 2007 12:25:31 -0800 (PST)
Subject: [R] RODBC
Message-ID: <8571820.post@talk.nabble.com>


Hello, 

I am fairly new to R and its connectivity to MS-Access. I just installed
RODBC and it seems to be working well except when I use the date to
condition the query. For example the query below 

sqlQuery(channel, "select date from tblUScpi where (date > d2) order by
date")

returns the following error

[1] "[RODBC] ERROR: Could not SQLExecDirect"                                               
[2] "07001 -3010 [Microsoft][ODBC Microsoft Access Driver] Too few
parameters. Expected 1."

I checked that d2 and the elements in date belong to the same class
("POSIXt" "POSIXct"). Can anybody help me?
-- 
View this message in context: http://www.nabble.com/RODBC-tf3084357.html#a8571820
Sent from the R help mailing list archive at Nabble.com.


From lalithaviswanath at yahoo.com  Wed Jan 24 21:31:56 2007
From: lalithaviswanath at yahoo.com (lalitha viswanath)
Date: Wed, 24 Jan 2007 12:31:56 -0800 (PST)
Subject: [R] Query about extracting subset of datafram
Message-ID: <685610.79362.qm@web43118.mail.sp1.yahoo.com>

Hi
I have a table read from a mysql database which is of
the  kind

clusterid clockrate

I obtained this table in R as
clockrates_table <-sqlQuery(channel,"select....");
I have a function within which I wish to extract the
clusterid for a given cluster.
Although I know that there is just one row per
clusterid in the data frame, I am using subset to
extract the clockrate.

clockrate = subset(clockrates_table, clusterid==15,
select=c(clockrate));

Is there any way of extracting the clockrate without
using subset.

In the help section for subset, it mentioned to "see
also: [,..."
However I could find no mention for this entry when I
searched as "?[", etc.

The R manuals also, despite discussing complex
libraries, techniques etc, dont always seem to provide
such handy hints/tips and tricks for manipulating
data, which is a first stumbling block for newbies
like me.
I would greatly appreciate if you could point me to
such resources as well, for future reference.

Thanks
Lalitha 



 
____________________________________________________________________________________
8:00? 8:25? 8:40? Find a flick in no time


From marc_schwartz at comcast.net  Wed Jan 24 21:56:48 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 24 Jan 2007 14:56:48 -0600
Subject: [R] dataframe operation
In-Reply-To: <1169669766.4873.21.camel@localhost.localdomain>
References: <FE8C160D1505B24497FA7C78D4DADACA0478CE@EA-MAIL.eawag.wroot.emp-eaw.ch>
	<1169669454.4873.18.camel@localhost.localdomain>
	<1169669766.4873.21.camel@localhost.localdomain>
Message-ID: <1169672208.4873.27.camel@localhost.localdomain>

On Wed, 2007-01-24 at 14:16 -0600, Marc Schwartz wrote:
> On Wed, 2007-01-24 at 14:10 -0600, Marc Schwartz wrote:
> > On Wed, 2007-01-24 at 20:27 +0100, Indermaur Lukas wrote:
> > > hi
> > > i have a dataframe "a" which looks like:
> > >  
> > > column1, column2, column3
> > > 10,12, 0
> > > NA, 0,1
> > > 12,NA,50
> > >  
> > > i want to replace all values in column1 to column3 which do not contain "NA" with values of vector "b" (100,200,300).
> > >  
> > > any idea i can do it?
> > >  
> > > i appreciate any hint
> > > regards
> > > lukas
> > >  
> > 
> > Here is one possibility:
> > 
> > > sapply(seq(along = colnames(DF)), 
> >          function(x) ifelse(is.na(DF[[x]]), 100 * x, DF[[x]]))
> >      [,1] [,2] [,3]
> > [1,]   10   12    0
> > [2,]  100    0    1
> > [3,]   12  200   50
> > 
> > 
> > Note that the returned object will be a matrix, so if you need a data
> > frame, just coerce the result with as.data.frame().
> 
> OK....that's what I get for pulling the trigger too fast.
> 
> Just reverse the logic in the function:
> 
> > sapply(seq(along = colnames(DF)), 
>          function(x) ifelse(!is.na(DF[[x]]), 100 * x, DF[[x]]))
>      [,1] [,2] [,3]
> [1,]  100  200  300
> [2,]   NA  200  300
> [3,]  100   NA  300
> 
> 
> I misread the query initially.

Here is another possibility, which may be faster depending upon the
actual size and dims of your initial data frame.

Preallocate a matrix of replacement values:

Mat <- matrix(rep(seq(along = colnames(DF)) * 100, each = nrow(DF)),
              ncol = ncol(DF))

> Mat
     [,1] [,2] [,3]
[1,]  100  200  300
[2,]  100  200  300
[3,]  100  200  300


Now do the replacement:

> ifelse(!is.na(DF), Mat, NA)
  column1 column2 column3
1     100     200     300
2      NA     200     300
3     100      NA     300


In doing some testing, the above may be about 10 times faster than using
sapply() in my first solution, again depending upon the structure of
your DF.

HTH,

Marc


From mfgreene79 at yahoo.com  Wed Jan 24 22:00:52 2007
From: mfgreene79 at yahoo.com (Michael Greene)
Date: Wed, 24 Jan 2007 13:00:52 -0800 (PST)
Subject: [R] Importing XPORT datasets into R
Message-ID: <967233.26894.qm@web50314.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070124/f7dee711/attachment.pl 

From lalithaviswanath at yahoo.com  Wed Jan 24 22:03:44 2007
From: lalithaviswanath at yahoo.com (lalitha viswanath)
Date: Wed, 24 Jan 2007 13:03:44 -0800 (PST)
Subject: [R] User defined function calls
Message-ID: <222362.15802.qm@web43111.mail.sp1.yahoo.com>

Hi
I have a script processfiles.R that contains, amongst
other functions
1) a database access function called get_clockrates
which retreives from a database, a table containing
columns (clusterid, clockrate) and 45000 rows(one for
each clusterid).
Clusterid is an integer and clockrate is a float.

2) process_clusterid which takes clusterid as an
argument and after doing some data processing,
retrieves the clockrate corresponding to the
clusterid.

I wish to call get_clockrates only once and keep the
dataframe returned by it as a GLOBAL which the
function process_clusterid can use for each clusterid
that it processes.

To ensure that clockrates is global, I retreive it as
clockrate <<- sqlQuery......
Trust that this is correct.

Without the inclusion of get_clockrates function, I
have run this script under R as follows
> source("process_files.R");
> for (index in c(1:45000)) { try(process_file,
silent=TRUE); }

How do I get this code to execute get_clockrates only
once and subsequently call process_file for each of
the 45000 files in turn.

I would greatly appreciate your input regarding my
query.

Thanks
Lalitha


 
____________________________________________________________________________________
Finding fabulous fares is fun.


From datkins at fuller.edu  Wed Jan 24 22:08:22 2007
From: datkins at fuller.edu (Dave Atkins)
Date: Wed, 24 Jan 2007 13:08:22 -0800
Subject: [R] Cronbach's alpha
Message-ID: <45B7CAC6.3040605@fuller.edu>


Harold & Weiwei--

Actually, alpha *can* go negative, which means that items are reliably different 
as opposed to reliably similar.  This happens when the sum of the covariances 
among items is negative.  See the ATS site below for a more thorough explanation:

http://www.ats.ucla.edu/STAT/SPSS/library/negalpha.htm

Hope that helps.

cheers, Dave
-- 
Dave Atkins, PhD
Assistant Professor in Clinical Psychology
Fuller Graduate School of Psychology
Email: datkins at fuller.edu
Phone: 626.584.5554


Weiwei

Something is wrong. Coefficient alpha is bounded between 0 and 1, so
negative values are outside the parameter space for a reliability
statistic. Recall that reliability is the ratio of "true score" variance
to "total score variance". That is

var(t)/ var(t) + var(e)

If all variance is true score variance, then var(e)=0 and the
reliability is var(t)/var(t)=1. On the other hand, if all variance is
measurement error, then var(t) = 0 and reliability is 0.

Here is a function I wrote to compute alpha along with an example. Maybe
try recomputing your statistic using this function and see if you get
the same result.

alpha <- function(columns){
	k <- ncol(columns)
	colVars <- apply(columns, 2, var)
	total   <- var(apply(columns, 1, sum))
	a <- (total - sum(colVars)) / total * (k/(k-1))
	a
  	}

data(LSAT, package='ltm')
 > alpha(LSAT)
[1] 0.2949972


Harold

 > -----Original Message-----
 > From: r-help-bounces at stat.math.ethz.ch
 > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Weiwei Shi
 > Sent: Wednesday, January 24, 2007 1:17 PM
 > To: R R
 > Subject: [R] Cronbach's alpha
 >
 > Dear Listers:
 >
 > I used cronbach{psy} to evaluate the internal consistency and
 > some set of variables gave me alpha=-1.1003, while other,
 > alpha=-0.2; alpha=0.89; and so on. I am interested in knowing
 > how to interpret 1. negative value 2. negative value less than -1.
 >
 > I also want to re-mention my previous question about how to
 > evaluate the consistency of a set of variables and about the
 > total correlation (my 2 cent to answer the question). Is
 > there any function in R to do that?
 >
 > Thank you very much!
 >
 >
 >
 > --
 > Weiwei Shi, Ph.D
 > Research Scientist
 > GeneGO, Inc.
 >
 > "Did you always know?"
 > "No, I did not. But I believed..."
 > ---Matrix III
 >
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide
 > http://www.R-project.org/posting-guide.html
 > and provide commented, minimal, self-contained, reproducible code.
 >
-- 
Dave Atkins, PhD
Assistant Professor in Clinical Psychology
Fuller Graduate School of Psychology
Email: datkins at fuller.edu
Phone: 626.584.5554


From marc_schwartz at comcast.net  Wed Jan 24 22:10:51 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 24 Jan 2007 15:10:51 -0600
Subject: [R] Query about extracting subset of datafram
In-Reply-To: <685610.79362.qm@web43118.mail.sp1.yahoo.com>
References: <685610.79362.qm@web43118.mail.sp1.yahoo.com>
Message-ID: <1169673051.4873.44.camel@localhost.localdomain>

On Wed, 2007-01-24 at 12:31 -0800, lalitha viswanath wrote:
> Hi
> I have a table read from a mysql database which is of
> the  kind
> 
> clusterid clockrate
> 
> I obtained this table in R as
> clockrates_table <-sqlQuery(channel,"select....");
> I have a function within which I wish to extract the
> clusterid for a given cluster.
> Although I know that there is just one row per
> clusterid in the data frame, I am using subset to
> extract the clockrate.
> 
> clockrate = subset(clockrates_table, clusterid==15,
> select=c(clockrate));

You don't need the ';', though some will argue that it is a personal
preference.  Also, the c(...) around 'clockrate' is not needed when only
one column is being selected.

> Is there any way of extracting the clockrate without
> using subset.

You could use:

  clockrates_table[clockrates_table$clusterid == 15, 
                   clockrates_table$clockrate]

or perhaps:

  with(clockrates_table, clockrates_table[clusterid == 15, clockrate])

See ?with

If you did not need the conditional, you could of course use:

  clockrates_table$clockrate

or:

  clockrates_table[["clockrate"]]

or:

  clockrates_table[, "clockrate"]

or:

  with(clockrates_table, clockrate)


My personal preference is to use subset(), as for me, it makes the code
easier to read.

> In the help section for subset, it mentioned to "see
> also: [,..."
> However I could find no mention for this entry when I
> searched as "?[", etc.

Try:  ?"[" or ?Extract

Note the placement of the quotes in the first case.

> The R manuals also, despite discussing complex
> libraries, techniques etc, dont always seem to provide
> such handy hints/tips and tricks for manipulating
> data, which is a first stumbling block for newbies
> like me.
> I would greatly appreciate if you could point me to
> such resources as well, for future reference.

If you have not yet, reading the Posting Guide, for which there is a
link at the bottom of each e-mail is a good place to start.

Also, see ?RSiteSearch for a function which will enable you to search
the e-mail list archives.

HTH,

Marc Schwartz


From roberto.perdisci at gmail.com  Wed Jan 24 22:13:13 2007
From: roberto.perdisci at gmail.com (Roberto Perdisci)
Date: Wed, 24 Jan 2007 16:13:13 -0500
Subject: [R] Probabilities calibration error & ROCR
Message-ID: <cf94d0090701241313j15d462f1g1eb3e4f64007445d@mail.gmail.com>

Hello,
  I'd need to compute the calibration error of posterior class
probabilities p(y|x) estimated by using rpart as classification tree.
Namely, I train rpart on a dataset D and then use predict(...
type="prob") to estimate p(y|x).

  I've found the possibility to do that in the ROCR package, but I
cannot find a link to a paper/book which explains the details of the
implemented algorithm. Do you know of any reference where I can find
the details of the algorithm that computes the calibration error
implemented in ROCR (apart from ROCR's source code)?  Is there any
other function/package I can use to compute the calibration error?

thank you,
regards,
Roberto


From HDoran at air.org  Wed Jan 24 22:18:07 2007
From: HDoran at air.org (Doran, Harold)
Date: Wed, 24 Jan 2007 16:18:07 -0500
Subject: [R] Cronbach's alpha
Message-ID: <2323A6D37908A847A7C32F1E3662C80E8D4573@dc1ex01.air.org>

Hi Dave

We had a bit of an off list discussion on this. You're correct, it can
be negative IF the covariance among individual items is negative AND if
that covariance term is larger than the sum of the individual item
variances. Both of these conditions would be needed to make alpha go
negative.

Psychometrically speaking, this introduces some question as to whether
the items are measuring the same latent trait. That is, if there is a
negative covariance among items, but those items are thought to measure
a common trait, then (I'm scratching my head) I think we have a
dimensionality issue.



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dave Atkins
> Sent: Wednesday, January 24, 2007 4:08 PM
> To: R-help at stat.math.ethz.ch
> Subject: Re: [R] Cronbach's alpha
> 
> 
> Harold & Weiwei--
> 
> Actually, alpha *can* go negative, which means that items are 
> reliably different as opposed to reliably similar.  This 
> happens when the sum of the covariances among items is 
> negative.  See the ATS site below for a more thorough explanation:
> 
> http://www.ats.ucla.edu/STAT/SPSS/library/negalpha.htm
> 
> Hope that helps.
> 
> cheers, Dave
> --
> Dave Atkins, PhD
> Assistant Professor in Clinical Psychology Fuller Graduate 
> School of Psychology
> Email: datkins at fuller.edu
> Phone: 626.584.5554
> 
> 
> Weiwei
> 
> Something is wrong. Coefficient alpha is bounded between 0 and 1, so
> negative values are outside the parameter space for a reliability
> statistic. Recall that reliability is the ratio of "true 
> score" variance
> to "total score variance". That is
> 
> var(t)/ var(t) + var(e)
> 
> If all variance is true score variance, then var(e)=0 and the
> reliability is var(t)/var(t)=1. On the other hand, if all variance is
> measurement error, then var(t) = 0 and reliability is 0.
> 
> Here is a function I wrote to compute alpha along with an 
> example. Maybe
> try recomputing your statistic using this function and see if you get
> the same result.
> 
> alpha <- function(columns){
> 	k <- ncol(columns)
> 	colVars <- apply(columns, 2, var)
> 	total   <- var(apply(columns, 1, sum))
> 	a <- (total - sum(colVars)) / total * (k/(k-1))
> 	a
>   	}
> 
> data(LSAT, package='ltm')
>  > alpha(LSAT)
> [1] 0.2949972
> 
> 
> Harold
> 
>  > -----Original Message-----
>  > From: r-help-bounces at stat.math.ethz.ch
>  > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Weiwei Shi
>  > Sent: Wednesday, January 24, 2007 1:17 PM
>  > To: R R
>  > Subject: [R] Cronbach's alpha
>  >
>  > Dear Listers:
>  >
>  > I used cronbach{psy} to evaluate the internal consistency and
>  > some set of variables gave me alpha=-1.1003, while other,
>  > alpha=-0.2; alpha=0.89; and so on. I am interested in knowing
>  > how to interpret 1. negative value 2. negative value less than -1.
>  >
>  > I also want to re-mention my previous question about how to
>  > evaluate the consistency of a set of variables and about the
>  > total correlation (my 2 cent to answer the question). Is
>  > there any function in R to do that?
>  >
>  > Thank you very much!
>  >
>  >
>  >
>  > --
>  > Weiwei Shi, Ph.D
>  > Research Scientist
>  > GeneGO, Inc.
>  >
>  > "Did you always know?"
>  > "No, I did not. But I believed..."
>  > ---Matrix III
>  >
>  > ______________________________________________
>  > R-help at stat.math.ethz.ch mailing list
>  > https://stat.ethz.ch/mailman/listinfo/r-help
>  > PLEASE do read the posting guide
>  > http://www.R-project.org/posting-guide.html
>  > and provide commented, minimal, self-contained, reproducible code.
>  >
> -- 
> Dave Atkins, PhD
> Assistant Professor in Clinical Psychology
> Fuller Graduate School of Psychology
> Email: datkins at fuller.edu
> Phone: 626.584.5554
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Wed Jan 24 22:21:09 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 24 Jan 2007 16:21:09 -0500
Subject: [R] dataframe operation
In-Reply-To: <1169672208.4873.27.camel@localhost.localdomain>
References: <FE8C160D1505B24497FA7C78D4DADACA0478CE@EA-MAIL.eawag.wroot.emp-eaw.ch>
	<1169669454.4873.18.camel@localhost.localdomain>
	<1169669766.4873.21.camel@localhost.localdomain>
	<1169672208.4873.27.camel@localhost.localdomain>
Message-ID: <971536df0701241321t1596f556g1178ba9838a190c3@mail.gmail.com>

Here is a slight variation on Marc's idea:

isna <- is.na(DF)
DF[] <- replace(100 * col(isna), isna, NA)

On 1/24/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> On Wed, 2007-01-24 at 14:16 -0600, Marc Schwartz wrote:
> > On Wed, 2007-01-24 at 14:10 -0600, Marc Schwartz wrote:
> > > On Wed, 2007-01-24 at 20:27 +0100, Indermaur Lukas wrote:
> > > > hi
> > > > i have a dataframe "a" which looks like:
> > > >
> > > > column1, column2, column3
> > > > 10,12, 0
> > > > NA, 0,1
> > > > 12,NA,50
> > > >
> > > > i want to replace all values in column1 to column3 which do not contain "NA" with values of vector "b" (100,200,300).
> > > >
> > > > any idea i can do it?
> > > >
> > > > i appreciate any hint
> > > > regards
> > > > lukas
> > > >
> > >
> > > Here is one possibility:
> > >
> > > > sapply(seq(along = colnames(DF)),
> > >          function(x) ifelse(is.na(DF[[x]]), 100 * x, DF[[x]]))
> > >      [,1] [,2] [,3]
> > > [1,]   10   12    0
> > > [2,]  100    0    1
> > > [3,]   12  200   50
> > >
> > >
> > > Note that the returned object will be a matrix, so if you need a data
> > > frame, just coerce the result with as.data.frame().
> >
> > OK....that's what I get for pulling the trigger too fast.
> >
> > Just reverse the logic in the function:
> >
> > > sapply(seq(along = colnames(DF)),
> >          function(x) ifelse(!is.na(DF[[x]]), 100 * x, DF[[x]]))
> >      [,1] [,2] [,3]
> > [1,]  100  200  300
> > [2,]   NA  200  300
> > [3,]  100   NA  300
> >
> >
> > I misread the query initially.
>
> Here is another possibility, which may be faster depending upon the
> actual size and dims of your initial data frame.
>
> Preallocate a matrix of replacement values:
>
> Mat <- matrix(rep(seq(along = colnames(DF)) * 100, each = nrow(DF)),
>              ncol = ncol(DF))
>
> > Mat
>     [,1] [,2] [,3]
> [1,]  100  200  300
> [2,]  100  200  300
> [3,]  100  200  300
>
>
> Now do the replacement:
>
> > ifelse(!is.na(DF), Mat, NA)
>  column1 column2 column3
> 1     100     200     300
> 2      NA     200     300
> 3     100      NA     300
>
>
> In doing some testing, the above may be about 10 times faster than using
> sapply() in my first solution, again depending upon the structure of
> your DF.
>
> HTH,
>
> Marc
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mike.prager at noaa.gov  Wed Jan 24 22:30:23 2007
From: mike.prager at noaa.gov (Mike Prager)
Date: Wed, 24 Jan 2007 16:30:23 -0500
Subject: [R] Text position in Traditional Graphics
Message-ID: <sajfr25pdoddjr0q1g88ji8rcuudmmpiip@4ax.com>

R 2.4.1 on Windows XP.

Question:  In traditional graphics, is it possible to find out
the height of a line of text in units that can be used in
arithmetic and then in calls to text()? 

Context:  I have written a function that draws a plot and then,
depending on whether some arguments are TRUE or FALSE, draws
various lines of text in the plot. The text lines may be turned
on or off individually by the user. The function uses plot() and
several calls to text().

However, I have not found a good way to adjust the Y coordinate
of the text for lines after the first. I would like this to work
when the graphics device (windows) is opened at (or resized to)
a wide range of sizes.  The issue is that a line of text takes
up a smaller fraction of the total Y span of the plotting region
as the window gets larger.

It seems this can be done with grid graphics, but although I
plan to learn grid, I am hoping that for now, I can do this work
with traditional graphics.

Thanks!

-- 
Mike Prager, NOAA, Beaufort, NC
* Opinions expressed are personal and not represented otherwise.
* Any use of tradenames does not constitute a NOAA endorsement.


From jrgold at insightful.com  Wed Jan 24 22:38:44 2007
From: jrgold at insightful.com (Jill Goldschneider)
Date: Wed, 24 Jan 2007 13:38:44 -0800
Subject: [R] JOB: LARS Package Developer
Message-ID: <ECBC2BF14033C0458FDA3C695E14F5F607B00B@sewinexch00.insightful.com>


Insightful is seeking a Research Scientist with a strong background in
statistical methodology, algorithms development, data analysis, and software
development.  The primary responsibilities are to develop software for
high-dimensional regression and machine learning applications using least
angle regression (LARS).

The official position is listed at:
	http://www.insightful.com/company/jobdescription.asp?JobID=118

More information about the project is at:
	http://www.insightful.com/Hesterberg/glars

For technical questions contact Tim Hesterberg timh at insightful.com.

To apply, please contact Jill Goldschneider jrgold at insightful.com or Human
Resources HR at insightful.com.

Thank you,
Jill

----

Jill R. Goldschneider, Ph.D.
Director of Research
Insightful Corporation
1700 Westlake Ave. N. Suite 500
Seattle WA 98109
(206) 802-2327 (office)
(206) 953-9355 (mobile)
(206) 802-2500 (fax)


From jrgold at insightful.com  Wed Jan 24 22:39:31 2007
From: jrgold at insightful.com (Jill Goldschneider)
Date: Wed, 24 Jan 2007 13:39:31 -0800
Subject: [R] JOB: LARS internships
Message-ID: <ECBC2BF14033C0458FDA3C695E14F5F607B00C@sewinexch00.insightful.com>

Insightful is seeking a pre-doctoral student and an undergraduate student for
two internship positions.  The primary responsibilities are to assist in the
development of software for high-dimensional regression and machine learning
applications using least angle regression (LARS).

The pre-doctoral candidate should have a background and interest in
statistical methodology, algorithms, data analysis, simulation studies and
software development and documentation.  The candidate should be currently
pursuing a Ph.D. degree.  

The undergraduate intern position requires a person with at least three years
of undergraduate training and a solid background in mathematics and interest
in statistical methodology, algorithms, data analysis, simulation studies and
software development and documentation.  The candidate should be currently
pursuing a bachelor's degree. 

The official positions are listed at:
	http://www.insightful.com/company/jobdescription.asp?JobID=116
	http://www.insightful.com/company/jobdescription.asp?JobID=117

More information about the project can be found at:
	http://www.insightful.com/Hesterberg/glars

For technical questions contact Tim Hesterberg timh at insightful.com.

To apply, please contact Jill Goldschneider jrgold at insightful.com or Human
Resources HR at insightful.com.

Thank you,
Jill

----

Jill R. Goldschneider, Ph.D.
Director of Research
Insightful Corporation
1700 Westlake Ave. N. Suite 500
Seattle WA 98109
(206) 802-2327 (office)
(206) 953-9355 (mobile)
(206) 802-2500 (fax)


From saurav at sas.upenn.edu  Wed Jan 24 22:37:20 2007
From: saurav at sas.upenn.edu (Saurav Pathak)
Date: Wed, 24 Jan 2007 16:37:20 -0500
Subject: [R] modify rectangle color from image
Message-ID: <20070124213720.GA2015@mail1.sas.upenn.edu>

Hi,

I need some suggestion on how I could modify the color on some
rectangle that I have created using "image".

In other words, I have a 5x5 matrix, say, m.  

        m <- matrix(rnorm(25), nrow=5)

I create a grid of rectangles by:

        image(m)

Now I want to change the color of rectangle (3,3) to blue. 

I don't know how this could be done, and searching the web has given
me no hint.  

Thanks for your help.

-- 
saurav


From helprhelp at gmail.com  Wed Jan 24 22:44:37 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 24 Jan 2007 16:44:37 -0500
Subject: [R] Cronbach's alpha
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E8D4573@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80E8D4573@dc1ex01.air.org>
Message-ID: <cdf817830701241344h381f4a80w9be5f07e1332d8d1@mail.gmail.com>

Hi, there:

I read that article (thanks Chucks, etc to point that out). Now I
understand how those negatives are generated since my research subject
"should" have negative convariance but they "are" measuring the same
thing. So, I am confused about this "same" thing and about if it is
proper to go ahead to use this measurement.

To clear my point , I describe my idea here a little bit. My idea is
to look for a way to assign a "statistic" or measurement to a set of
variables to see if they "act" cohesively or coherently for an event.
Instead of using simple correlation, which describes var/var
correlation; I wanted to get a "total correlation" so that I can
compare between setS of variables. Initially I "made" that word but
google helps me find that statistic exists! So I read into it and post
my original post on "total correlation". (Ben, you can find total
correlation from wiki).

I was suggested to use this alpha since it measures a "one latent
construct", in which matches my idea about one event. I have a feeling
it is like factor analysis; however, the grouping of variables has
been fixed by domain knowledge.

Sorry if it is off-list topic but I feel it is very interesting to go ahead.

Thanks,

Weiwei



On 1/24/07, Doran, Harold <HDoran at air.org> wrote:
> Hi Dave
>
> We had a bit of an off list discussion on this. You're correct, it can
> be negative IF the covariance among individual items is negative AND if
> that covariance term is larger than the sum of the individual item
> variances. Both of these conditions would be needed to make alpha go
> negative.
>
> Psychometrically speaking, this introduces some question as to whether
> the items are measuring the same latent trait. That is, if there is a
> negative covariance among items, but those items are thought to measure
> a common trait, then (I'm scratching my head) I think we have a
> dimensionality issue.
>
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dave Atkins
> > Sent: Wednesday, January 24, 2007 4:08 PM
> > To: R-help at stat.math.ethz.ch
> > Subject: Re: [R] Cronbach's alpha
> >
> >
> > Harold & Weiwei--
> >
> > Actually, alpha *can* go negative, which means that items are
> > reliably different as opposed to reliably similar.  This
> > happens when the sum of the covariances among items is
> > negative.  See the ATS site below for a more thorough explanation:
> >
> > http://www.ats.ucla.edu/STAT/SPSS/library/negalpha.htm
> >
> > Hope that helps.
> >
> > cheers, Dave
> > --
> > Dave Atkins, PhD
> > Assistant Professor in Clinical Psychology Fuller Graduate
> > School of Psychology
> > Email: datkins at fuller.edu
> > Phone: 626.584.5554
> >
> >
> > Weiwei
> >
> > Something is wrong. Coefficient alpha is bounded between 0 and 1, so
> > negative values are outside the parameter space for a reliability
> > statistic. Recall that reliability is the ratio of "true
> > score" variance
> > to "total score variance". That is
> >
> > var(t)/ var(t) + var(e)
> >
> > If all variance is true score variance, then var(e)=0 and the
> > reliability is var(t)/var(t)=1. On the other hand, if all variance is
> > measurement error, then var(t) = 0 and reliability is 0.
> >
> > Here is a function I wrote to compute alpha along with an
> > example. Maybe
> > try recomputing your statistic using this function and see if you get
> > the same result.
> >
> > alpha <- function(columns){
> >       k <- ncol(columns)
> >       colVars <- apply(columns, 2, var)
> >       total   <- var(apply(columns, 1, sum))
> >       a <- (total - sum(colVars)) / total * (k/(k-1))
> >       a
> >       }
> >
> > data(LSAT, package='ltm')
> >  > alpha(LSAT)
> > [1] 0.2949972
> >
> >
> > Harold
> >
> >  > -----Original Message-----
> >  > From: r-help-bounces at stat.math.ethz.ch
> >  > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > Weiwei Shi
> >  > Sent: Wednesday, January 24, 2007 1:17 PM
> >  > To: R R
> >  > Subject: [R] Cronbach's alpha
> >  >
> >  > Dear Listers:
> >  >
> >  > I used cronbach{psy} to evaluate the internal consistency and
> >  > some set of variables gave me alpha=-1.1003, while other,
> >  > alpha=-0.2; alpha=0.89; and so on. I am interested in knowing
> >  > how to interpret 1. negative value 2. negative value less than -1.
> >  >
> >  > I also want to re-mention my previous question about how to
> >  > evaluate the consistency of a set of variables and about the
> >  > total correlation (my 2 cent to answer the question). Is
> >  > there any function in R to do that?
> >  >
> >  > Thank you very much!
> >  >
> >  >
> >  >
> >  > --
> >  > Weiwei Shi, Ph.D
> >  > Research Scientist
> >  > GeneGO, Inc.
> >  >
> >  > "Did you always know?"
> >  > "No, I did not. But I believed..."
> >  > ---Matrix III
> >  >
> >  > ______________________________________________
> >  > R-help at stat.math.ethz.ch mailing list
> >  > https://stat.ethz.ch/mailman/listinfo/r-help
> >  > PLEASE do read the posting guide
> >  > http://www.R-project.org/posting-guide.html
> >  > and provide commented, minimal, self-contained, reproducible code.
> >  >
> > --
> > Dave Atkins, PhD
> > Assistant Professor in Clinical Psychology
> > Fuller Graduate School of Psychology
> > Email: datkins at fuller.edu
> > Phone: 626.584.5554
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From Charles.Annis at StatisticalEngineering.com  Wed Jan 24 22:53:49 2007
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Wed, 24 Jan 2007 16:53:49 -0500
Subject: [R] Text position in Traditional Graphics
In-Reply-To: <sajfr25pdoddjr0q1g88ji8rcuudmmpiip@4ax.com>
References: <sajfr25pdoddjr0q1g88ji8rcuudmmpiip@4ax.com>
Message-ID: <002a01c74002$230ff8b0$6400a8c0@DD4XFW31>

A few days a go Jim Holman [jholtman at gmail.com] suggested this
(Re: [R] How to annotate a graph with non-transparent math labels?)
for a similar circumstance.  Perhaps it will for in your case.

~~~~~~~~~~~~~~~~
try using strwidth & strheight

x<-c(0,1)
plot(x,x,type='l')
dimensions<-matrix(c(strwidth(expression(theta),cex=5),strheight(expression(
theta),
cex=5)),nrow=1)
symbols(0.5,0.5
,rectangle=dimensions,bg='white',fg='white',add=TRUE,inches=FALSE)
text(0.5,0.5,expression(theta),cex=5)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mike Prager
Sent: Wednesday, January 24, 2007 4:30 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Text position in Traditional Graphics

R 2.4.1 on Windows XP.

Question:  In traditional graphics, is it possible to find out
the height of a line of text in units that can be used in
arithmetic and then in calls to text()? 

Context:  I have written a function that draws a plot and then,
depending on whether some arguments are TRUE or FALSE, draws
various lines of text in the plot. The text lines may be turned
on or off individually by the user. The function uses plot() and
several calls to text().

However, I have not found a good way to adjust the Y coordinate
of the text for lines after the first. I would like this to work
when the graphics device (windows) is opened at (or resized to)
a wide range of sizes.  The issue is that a line of text takes
up a smaller fraction of the total Y span of the plotting region
as the window gets larger.

It seems this can be done with grid graphics, but although I
plan to learn grid, I am hoping that for now, I can do this work
with traditional graphics.

Thanks!

-- 
Mike Prager, NOAA, Beaufort, NC
* Opinions expressed are personal and not represented otherwise.
* Any use of tradenames does not constitute a NOAA endorsement.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From helprhelp at gmail.com  Wed Jan 24 22:59:12 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 24 Jan 2007 16:59:12 -0500
Subject: [R] n step ahead forecasts
In-Reply-To: <1c6126db0701241048v7b7a6fa7g70d47e587484a55c@mail.gmail.com>
References: <1c6126db0701241048v7b7a6fa7g70d47e587484a55c@mail.gmail.com>
Message-ID: <cdf817830701241359l7cd9d0e9ya5b077f627d51e60@mail.gmail.com>

Dear Spencer:

just my2cent:
could you change the step from 1 to m, like 5 if you have a very large
validation set. I have a feeling that it won't change too much about
the result but I am not sure what your endpoint is.

weiwei

On 1/24/07, sj <ssj1364 at gmail.com> wrote:
> hello,
>
> I have a question about making n step ahead forecasts in cases where test
> and validation sets are availiable. For instance, I would like to make one
> step ahead forecasts  on the  WWWusage data so I hold out the last 10
> observations as the validation set and fit an ARIMA model on the first 90
> observations. I then use a for loop to sequentially add 9 of the holdout
> observations to make 1 step ahead forecasts for the last 10 periods (see
> example code). In cases where there are relatively few periods I want to
> forecast for this seems to work fine, however I am working with a rather
> large validation set and I need to make n step ahead forecasts for many
> periods and it takes a very long time. Is there a more efficient way to do
> this?
>
>
>
> vset <- WWWusage[91:100]
>
> pred <-c()
> for (i in 0:9)
>     { fit <-arima(WWWusage[1:(90+i)],c(3,1,0))
>       p<- predict(fit,se.fit=F)
>       pred <- c(pred, p)
>     }
> plot(pred,type="o",col=2)
> lines(vset,type="o",col=1)
>
>
> thanks,
>
> Spencer
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From mike.prager at noaa.gov  Wed Jan 24 23:01:04 2007
From: mike.prager at noaa.gov (Mike Prager)
Date: Wed, 24 Jan 2007 17:01:04 -0500
Subject: [R] Text position in Traditional Graphics
References: <sajfr25pdoddjr0q1g88ji8rcuudmmpiip@4ax.com>
Message-ID: <3llfr2l97urk43nrqbcclindkau7t58aoe@4ax.com>

Mike Prager <mike.prager at noaa.gov> wrote:

> R 2.4.1 on Windows XP.
> 
> Question:  In traditional graphics, is it possible to find out
> the height of a line of text in units that can be used in
> arithmetic and then in calls to text()? 
[...]

I seem to have solved my own question by setting the user scale
to the size of the window in inches, converting the point size
into inches, and going from there.  This works well for all
sizes of windows. It doesn't change the spacing when windows are
resized, but I can live with that.

There is nothing like posting to R-help to stimulate one's own
thoughts.

-- 
Mike Prager, NOAA, Beaufort, NC
* Opinions expressed are personal and not represented otherwise.
* Any use of tradenames does not constitute a NOAA endorsement.


From saurav at sas.upenn.edu  Wed Jan 24 23:06:43 2007
From: saurav at sas.upenn.edu (Saurav Pathak)
Date: Wed, 24 Jan 2007 17:06:43 -0500
Subject: [R] modify rectangle color from image
In-Reply-To: <20070124213720.GA2015@mail1.sas.upenn.edu>
References: <20070124213720.GA2015@mail1.sas.upenn.edu>
Message-ID: <20070124220643.GA7107@mail1.sas.upenn.edu>


Thanks,
Saurav


Saurav Pathak [Wed, Jan 24, 2007 at 04:37:20PM -0500]:

+  Hi,
+  
+  I need some suggestion on how I could modify the color on some
+  rectangle that I have created using "image".
+  
+  In other words, I have a 5x5 matrix, say, m.  
+  
+          m <- matrix(rnorm(25), nrow=5)
+  
+  I create a grid of rectangles by:
+  
+          image(m)
+  
+  Now I want to change the color of rectangle (3,3) to blue. 

using rect for this.

DUH.

-- 
saurav


From marc_schwartz at comcast.net  Wed Jan 24 23:10:23 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 24 Jan 2007 16:10:23 -0600
Subject: [R] Text position in Traditional Graphics
In-Reply-To: <sajfr25pdoddjr0q1g88ji8rcuudmmpiip@4ax.com>
References: <sajfr25pdoddjr0q1g88ji8rcuudmmpiip@4ax.com>
Message-ID: <1169676623.4873.54.camel@localhost.localdomain>

On Wed, 2007-01-24 at 16:30 -0500, Mike Prager wrote:
> R 2.4.1 on Windows XP.
> 
> Question:  In traditional graphics, is it possible to find out
> the height of a line of text in units that can be used in
> arithmetic and then in calls to text()? 
> 
> Context:  I have written a function that draws a plot and then,
> depending on whether some arguments are TRUE or FALSE, draws
> various lines of text in the plot. The text lines may be turned
> on or off individually by the user. The function uses plot() and
> several calls to text().
> 
> However, I have not found a good way to adjust the Y coordinate
> of the text for lines after the first. I would like this to work
> when the graphics device (windows) is opened at (or resized to)
> a wide range of sizes.  The issue is that a line of text takes
> up a smaller fraction of the total Y span of the plotting region
> as the window gets larger.
> 
> It seems this can be done with grid graphics, but although I
> plan to learn grid, I am hoping that for now, I can do this work
> with traditional graphics.
> 
> Thanks!

Mike, you might want to take a look at:

  ?strheight

The one thing to be potentially aware of, is if the plot window is
resized, some aspects of drawing text can be subject to alteration. It
may take some trial and error to determine how the method you wish to
use may be prone to such problems.

For example:

 plot(1, type = "n")
 text(1, 1, "This is a test")
 text(1, 1 + strheight("T"), "This is a test")
 text(1, 1 + strheight("T") * 2, "This is a test")
 text(1, 1 + strheight("T") * 3, "This is a test")

Now, drag and resize the plot window here

Then run:

  text(1, 1 + strheight("T") * 4, "This is a test")

This may behave differently on Windows, but on Linux, when I resize the
X window, the last line of text (* 4) is placed between (* 2) and (* 3).

HTH,

Marc Schwartz


From A.Robinson at ms.unimelb.edu.au  Wed Jan 24 23:14:38 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Thu, 25 Jan 2007 09:14:38 +1100
Subject: [R] mixed effects or fixed effects?
In-Reply-To: <45B77A73.4020808@physiol.ox.ac.uk>
References: <45B77A73.4020808@physiol.ox.ac.uk>
Message-ID: <20070124221438.GL13602@ms.unimelb.edu.au>

Hi Dan,

this is an interesting and intricate question, but only marginally
related to the subject line.

On Wed, Jan 24, 2007 at 03:25:39PM +0000, dan kumpik wrote:
> Hi,
> 
> I am running a learning experiment in which both training subjects and 
> controls complete a pretest and posttest. All analyses are being 
> conducted in R. We are looking to compare two training methodologies, 
> and so have run this experiment twice, once with each methodology. 
> Methodology is a between-subjects factor. Trying to run this analysis 
> with every factor included (ie, subject as a random factor, session 
> nested within group nested within experiment) seems to me (after having 
> tried) to be clumsy and probably uninterpretable.
> 	My favoured model for the analysis is a linear mixed-effects model, and 
> to combine the data meaningfully, I have collated all the pretest data 
> for controls and trained subjects from each experiment, and assumed this 
> data to represent a population sample for naive subjects for each 
> experiment. I have also ditched the posttest data for the controls, and 
> assumed the posttest training data to represent a population sample for 
> trained subjects for each experiment. I have confirmed the validity of 
> these assumptions by ascertaining that a) controls and trained listeners 
> did not differ significantly at pretest for either experiment; and b) 
> control listeners did not learn significantly between pretest and 
> posttest (and therefore their posttest data are not relevant). This was 
> done using a linear mixed-effects model for each experiment, with 
> subject as a random factor and session (pretest vs posttest) nested 
> within Group (trained vs control).

I don't agree with ditching the posttest data for the controls.
Although you may have failed to detect a lack of statistically
significant learning, that doesn't mean that there isn't enough
learning to imperil your inference.  

Also, under an appropriate model, posttest control data could
contribute to estimating the variance components, so by discarding
data you risk losing power. And by your description of the strategy,
you lose balance, but this is not such a problem as far as I am
aware. 

> 	Therefore, the model I want to use to analyse the data would ideally be 
> a linear mixed-effects model, with subject as a random factor, and 
> session (pre vs post) nested within experiment. Note that my removal of 
> the Group (Trained vs Control) factor simplifies the model somewhat, and 
> makes it more interpretable in terms of evaluating the relative effects 
> of each experiment.

I see that it simplifies the interpretation but not necessarily in a
constructive way!  

> 	What I would like to know is- a) would people agree that this is a 
> meaningful way to combine my data? I believe the logic is sound, but am 
> slightly concerned that I am ignoring a whole block of posttest data for 
> the controls (even though this does not account for a significant amount 
> of the variance); and b) given that each of my trained subjects appear 
> twice- one in the pretest and once in the posttest, and the controls 
> only appear once- in the pretest sample, is there any problem with 
> making subject a random factor? Conceptually, I see no problem with 
> this, but I would like to be sure before I finish writing up.
> 
> Many thanks for your time

I think that you need to make the model structure match the
experiment.  I hope that this is useful for you!

Andrew
-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From Greg.Snow at intermountainmail.org  Wed Jan 24 23:22:24 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Wed, 24 Jan 2007 15:22:24 -0700
Subject: [R] Text position in Traditional Graphics
Message-ID: <07E228A5BE53C24CAD490193A7381BBB7B3327@LP-EXCHVS07.CO.IHC.COM>

If you are going to take this approach, you may want to look at the
cnvrt.coords function in the TeachingDemos package.  That may save you a
few calculations.

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mike Prager
> Sent: Wednesday, January 24, 2007 3:01 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Text position in Traditional Graphics
> 
> Mike Prager <mike.prager at noaa.gov> wrote:
> 
> > R 2.4.1 on Windows XP.
> > 
> > Question:  In traditional graphics, is it possible to find out the 
> > height of a line of text in units that can be used in 
> arithmetic and 
> > then in calls to text()?
> [...]
> 
> I seem to have solved my own question by setting the user 
> scale to the size of the window in inches, converting the 
> point size into inches, and going from there.  This works 
> well for all sizes of windows. It doesn't change the spacing 
> when windows are resized, but I can live with that.
> 
> There is nothing like posting to R-help to stimulate one's 
> own thoughts.
> 
> --
> Mike Prager, NOAA, Beaufort, NC
> * Opinions expressed are personal and not represented otherwise.
> * Any use of tradenames does not constitute a NOAA endorsement.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Joseph.F.Lucke at uth.tmc.edu  Wed Jan 24 23:25:50 2007
From: Joseph.F.Lucke at uth.tmc.edu (Lucke, Joseph F)
Date: Wed, 24 Jan 2007 16:25:50 -0600
Subject: [R] Cronbach's alpha
References: <2323A6D37908A847A7C32F1E3662C80E8D4573@dc1ex01.air.org>
	<cdf817830701241344h381f4a80w9be5f07e1332d8d1@mail.gmail.com>
Message-ID: <4677FCB5A35A0441A0E0C99D56B23D910777FD84@UTHEVS2.mail.uthouston.edu>

Continuing off topic:

1. The range of alpha -infinity < alpha < 1.
2. Alpha is NOT reliability
3. There are trivial examples of alpha < 1 with reliability approaching
1.
4. There are trivial examples of alpha = 0 with reliability approaching
1.
5. Alpha cannot assess dimensionality.

Lucke, Joseph F. The $\alpha$ and the $\omega$ of congeneric test
theory: An extension of reliability and internal consistency to
heterogeneous tests. Applied Psychological Measurement, 2005}
29(1),65--81}.


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Weiwei Shi
Sent: Wednesday, January 24, 2007 3:45 PM
To: Doran, Harold
Cc: R-help at stat.math.ethz.ch; Dave Atkins
Subject: Re: [R] Cronbach's alpha

Hi, there:

I read that article (thanks Chucks, etc to point that out). Now I
understand how those negatives are generated since my research subject
"should" have negative convariance but they "are" measuring the same
thing. So, I am confused about this "same" thing and about if it is
proper to go ahead to use this measurement.

To clear my point , I describe my idea here a little bit. My idea is to
look for a way to assign a "statistic" or measurement to a set of
variables to see if they "act" cohesively or coherently for an event.
Instead of using simple correlation, which describes var/var
correlation; I wanted to get a "total correlation" so that I can compare
between setS of variables. Initially I "made" that word but google helps
me find that statistic exists! So I read into it and post my original
post on "total correlation". (Ben, you can find total correlation from
wiki).

I was suggested to use this alpha since it measures a "one latent
construct", in which matches my idea about one event. I have a feeling
it is like factor analysis; however, the grouping of variables has been
fixed by domain knowledge.

Sorry if it is off-list topic but I feel it is very interesting to go
ahead.

Thanks,

Weiwei



On 1/24/07, Doran, Harold <HDoran at air.org> wrote:
> Hi Dave
>
> We had a bit of an off list discussion on this. You're correct, it can

> be negative IF the covariance among individual items is negative AND 
> if that covariance term is larger than the sum of the individual item 
> variances. Both of these conditions would be needed to make alpha go 
> negative.
>
> Psychometrically speaking, this introduces some question as to whether

> the items are measuring the same latent trait. That is, if there is a 
> negative covariance among items, but those items are thought to 
> measure a common trait, then (I'm scratching my head) I think we have 
> a dimensionality issue.
>
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dave Atkins
> > Sent: Wednesday, January 24, 2007 4:08 PM
> > To: R-help at stat.math.ethz.ch
> > Subject: Re: [R] Cronbach's alpha
> >
> >
> > Harold & Weiwei--
> >
> > Actually, alpha *can* go negative, which means that items are 
> > reliably different as opposed to reliably similar.  This happens 
> > when the sum of the covariances among items is negative.  See the 
> > ATS site below for a more thorough explanation:
> >
> > http://www.ats.ucla.edu/STAT/SPSS/library/negalpha.htm
> >
> > Hope that helps.
> >
> > cheers, Dave
> > --
> > Dave Atkins, PhD
> > Assistant Professor in Clinical Psychology Fuller Graduate School of

> > Psychology
> > Email: datkins at fuller.edu
> > Phone: 626.584.5554
> >
> >
> > Weiwei
> >
> > Something is wrong. Coefficient alpha is bounded between 0 and 1, so

> > negative values are outside the parameter space for a reliability 
> > statistic. Recall that reliability is the ratio of "true score" 
> > variance to "total score variance". That is
> >
> > var(t)/ var(t) + var(e)
> >
> > If all variance is true score variance, then var(e)=0 and the 
> > reliability is var(t)/var(t)=1. On the other hand, if all variance 
> > is measurement error, then var(t) = 0 and reliability is 0.
> >
> > Here is a function I wrote to compute alpha along with an example. 
> > Maybe try recomputing your statistic using this function and see if 
> > you get the same result.
> >
> > alpha <- function(columns){
> >       k <- ncol(columns)
> >       colVars <- apply(columns, 2, var)
> >       total   <- var(apply(columns, 1, sum))
> >       a <- (total - sum(colVars)) / total * (k/(k-1))
> >       a
> >       }
> >
> > data(LSAT, package='ltm')
> >  > alpha(LSAT)
> > [1] 0.2949972
> >
> >
> > Harold
> >
> >  > -----Original Message-----
> >  > From: r-help-bounces at stat.math.ethz.ch  > 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Weiwei Shi

> > > Sent: Wednesday, January 24, 2007 1:17 PM  > To: R R  > Subject: 
> > [R] Cronbach's alpha  >  > Dear Listers:
> >  >
> >  > I used cronbach{psy} to evaluate the internal consistency and  > 
> > some set of variables gave me alpha=-1.1003, while other,  > 
> > alpha=-0.2; alpha=0.89; and so on. I am interested in knowing  > how

> > to interpret 1. negative value 2. negative value less than -1.
> >  >
> >  > I also want to re-mention my previous question about how to  > 
> > evaluate the consistency of a set of variables and about the  > 
> > total correlation (my 2 cent to answer the question). Is  > there 
> > any function in R to do that?
> >  >
> >  > Thank you very much!
> >  >
> >  >
> >  >
> >  > --
> >  > Weiwei Shi, Ph.D
> >  > Research Scientist
> >  > GeneGO, Inc.
> >  >
> >  > "Did you always know?"
> >  > "No, I did not. But I believed..."
> >  > ---Matrix III
> >  >
> >  > ______________________________________________
> >  > R-help at stat.math.ethz.ch mailing list  > 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> >  > PLEASE do read the posting guide
> >  > http://www.R-project.org/posting-guide.html
> >  > and provide commented, minimal, self-contained, reproducible
code.
> >  >
> > --
> > Dave Atkins, PhD
> > Assistant Professor in Clinical Psychology Fuller Graduate School of

> > Psychology
> > Email: datkins at fuller.edu
> > Phone: 626.584.5554
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


--
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at comcast.net  Wed Jan 24 23:31:53 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 24 Jan 2007 16:31:53 -0600
Subject: [R] modify rectangle color from image
In-Reply-To: <20070124213720.GA2015@mail1.sas.upenn.edu>
References: <20070124213720.GA2015@mail1.sas.upenn.edu>
Message-ID: <1169677913.4873.58.camel@localhost.localdomain>

On Wed, 2007-01-24 at 16:37 -0500, Saurav Pathak wrote:
> Hi,
> 
> I need some suggestion on how I could modify the color on some
> rectangle that I have created using "image".
> 
> In other words, I have a 5x5 matrix, say, m.  
> 
>         m <- matrix(rnorm(25), nrow=5)
> 
> I create a grid of rectangles by:
> 
>         image(m)
> 
> Now I want to change the color of rectangle (3,3) to blue. 
> 
> I don't know how this could be done, and searching the web has given
> me no hint.  
> 
> Thanks for your help.

Try this:

m <- matrix(rnorm(25), nrow = 5)
image(m)

# Get the plot region coords
USR <- par("usr")

# Calc the length of a side of a square
SIDE <- abs(USR[1] - USR[2]) / 5

# Draw the rect using the appropriate offsets
rect(USR[1] + (SIDE * 2), USR[3] + (SIDE * 2), 
     USR[1] + (SIDE * 3), USR[3] + (SIDE * 3), col = "blue")


See ?par and review "usr", then see ?rect

par("usr") gives you the coordinates of the plot region. Then just do
the math to calculate the coordinates of each rectangle.

HTH,

Marc Schwartz


From abeaujean at gmail.com  Wed Jan 24 23:41:29 2007
From: abeaujean at gmail.com (A. Beaujean)
Date: Wed, 24 Jan 2007 16:41:29 -0600
Subject: [R] Cronbach's alpha
In-Reply-To: <cdf817830701241344h381f4a80w9be5f07e1332d8d1@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80E8D4573@dc1ex01.air.org>
	<cdf817830701241344h381f4a80w9be5f07e1332d8d1@mail.gmail.com>
Message-ID: <c74b04310701241441w39bbf17g2b1f50b90291dcd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070124/56d389e2/attachment.pl 

From murdoch at stats.uwo.ca  Thu Jan 25 00:08:16 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 24 Jan 2007 18:08:16 -0500
Subject: [R] RODBC
In-Reply-To: <8571820.post@talk.nabble.com>
References: <8571820.post@talk.nabble.com>
Message-ID: <45B7E6E0.7090604@stats.uwo.ca>

On 1/24/2007 3:25 PM, jgaseff wrote:
> Hello, 
> 
> I am fairly new to R and its connectivity to MS-Access. I just installed
> RODBC and it seems to be working well except when I use the date to
> condition the query. For example the query below 
> 
> sqlQuery(channel, "select date from tblUScpi where (date > d2) order by
> date")
> 
> returns the following error
> 
> [1] "[RODBC] ERROR: Could not SQLExecDirect"                                               
> [2] "07001 -3010 [Microsoft][ODBC Microsoft Access Driver] Too few
> parameters. Expected 1."
> 
> I checked that d2 and the elements in date belong to the same class
> ("POSIXt" "POSIXct"). Can anybody help me?

If d2 is an R variable, you'll need to convert it to SQL syntax for 
dates (whatever that is, format() can probably do it).  R just passes 
the query string to the database, it doesn't do any variable 
substitutions for you.

Duncan Murdoch


From tobias.sing at mpi-sb.mpg.de  Thu Jan 25 00:28:05 2007
From: tobias.sing at mpi-sb.mpg.de (Tobias Sing)
Date: Thu, 25 Jan 2007 00:28:05 +0100
Subject: [R] Probabilities calibration error & ROCR
In-Reply-To: <cf94d0090701241313j15d462f1g1eb3e4f64007445d@mail.gmail.com>
References: <cf94d0090701241313j15d462f1g1eb3e4f64007445d@mail.gmail.com>
Message-ID: <c3ca233a0701241528n41431ce2j29f7aa96bd18fcce@mail.gmail.com>

Roberto,

On 1/24/07, Roberto Perdisci <roberto.perdisci at gmail.com> wrote:
> [...]. Do you know of any reference where I can find
> the details of the algorithm that computes the calibration error
> implemented in ROCR (apart from ROCR's source code)?

we use it as defined in Caruana & Niculescu-Mizil: "Data mining in
metric space: An empirical evaluation of supervised learning
performance criteria". Knowledge Discovery and Data Mining (KDD) 2006.
http://www.cs.cornell.edu/~caruana/perfs.rocai04.revised.rev1.ps

Also, have a look at ?performance.

HTH,
  Tobias


From M.J.Bojanowski at fss.uu.nl  Thu Jan 25 02:02:23 2007
From: M.J.Bojanowski at fss.uu.nl (Bojanowski, M.J.  (Michal))
Date: Thu, 25 Jan 2007 02:02:23 +0100
Subject: [R] using non-ASCII strings in R packages
Message-ID: <94E133D09AA24D43BF6341B675C01A330753C5@uu01msg-exb01.soliscom.uu.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070125/c7271c42/attachment.pl 

From jawegelin at ucdavis.edu  Thu Jan 25 02:39:39 2007
From: jawegelin at ucdavis.edu (Jacob Wegelin)
Date: Wed, 24 Jan 2007 17:39:39 -0800 (PST)
Subject: [R] poly(x) workaround when x has missing values
Message-ID: <Pine.OSX.4.58.0612251301200.11199@biostat5.ucdavis.edu>


Often in practical situations a predictor has missing values, so that poly
crashes. For instance:

> x<-1:10
> y<- x -  3 * x^2 + rnorm(10)/3
> x[3]<-NA
> lm( y ~ poly(x,2) )
Error in poly(x, 2) : missing values are not allowed in 'poly'
>
> lm( y ~ poly(x,2) , subset=!is.na(x)) # This does not help?!?
Error in poly(x, 2) : missing values are not allowed in 'poly'

The following function seems to be an okay workaround.

Poly<- function(x, degree = 1, coefs = NULL, raw = FALSE, ...) {
        notNA<-!is.na(x)
        answer<-poly(x[notNA], degree=degree, coefs=coefs, raw=raw, ...)
        THEMATRIX<-matrix(NA, nrow=length(x), ncol=degree)
        THEMATRIX[notNA,]<-answer
        attributes(THEMATRIX)[c('degree', 'coefs', 'class')]<- attributes(answer)[c('degree', 'coefs', 'class')]
        THEMATRIX
}


>  lm( y ~ Poly(x,2) )

Call:
lm(formula = y ~ Poly(x, 2))

Coefficients:
(Intercept)  Poly(x, 2)1  Poly(x, 2)2
      209.1        475.0        114.0

and it works when x and y are in a dataframe too:

> DAT<-data.frame(x=x, y=y)
> lm(y~Poly(x,2), data=DAT)

Call:
lm(formula = y ~ Poly(x, 2), data = DAT)

Coefficients:
(Intercept)  Poly(x, 2)1  Poly(x, 2)2
    -119.54      -276.11       -68.24

Is there a better way to do this? My workaround seems a bit awkward.
Whoever wrote "poly" must have had a good reason for not making it deal
with missing values?

Thanks for any thoughts

Jacob Wegelin


From john_d_mchenry at yahoo.com  Thu Jan 25 02:39:10 2007
From: john_d_mchenry at yahoo.com (John McHenry)
Date: Wed, 24 Jan 2007 17:39:10 -0800 (PST)
Subject: [R] Days of the week?
Message-ID: <32207.74873.qm@web35407.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070124/2035cec4/attachment.pl 

From JILWIL at SAFECO.com  Thu Jan 25 03:04:56 2007
From: JILWIL at SAFECO.com (WILLIE, JILL)
Date: Wed, 24 Jan 2007 18:04:56 -0800
Subject: [R] Size of data vs. needed memory...rule of thumb?
Message-ID: <916551423F01504BA339BF69CBA3BE72016E7A35@psmrdcex18.psm.pin.safeco.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070124/6084b33d/attachment.pl 

From edd at debian.org  Thu Jan 25 03:07:18 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 24 Jan 2007 20:07:18 -0600
Subject: [R] Days of the week?
In-Reply-To: <32207.74873.qm@web35407.mail.mud.yahoo.com>
References: <32207.74873.qm@web35407.mail.mud.yahoo.com>
Message-ID: <17848.4310.221874.553145@basebud.nulle.part>


On 24 January 2007 at 17:39, John McHenry wrote:
| What is the "standard" way to get the day of the week from a date such 
| as as.Date("2006-12-01")? It looks like fCalendar has some functions
| but this requires a change in the R locale to GMT. Is there another way?

Yes, go to POSIXlt and extract the wday field (see ?POSIXlt for more):


> as.POSIXlt(as.Date("2006-12-01"))$wday
[1] 5
> as.POSIXlt(as.Date("2006-12-01")+0:6)$wday
[1] 5 6 0 1 2 3 4
>                          

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From ggrothendieck at gmail.com  Thu Jan 25 03:12:24 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 24 Jan 2007 21:12:24 -0500
Subject: [R] Days of the week?
In-Reply-To: <32207.74873.qm@web35407.mail.mud.yahoo.com>
References: <32207.74873.qm@web35407.mail.mud.yahoo.com>
Message-ID: <971536df0701241812q6cf14ac7k643f67115c35ecbd@mail.gmail.com>

You can use as.numeric(format(d, "%w")) .  See ?strptime and also
the help desk article in R News 4/1.

On 1/24/07, John McHenry <john_d_mchenry at yahoo.com> wrote:
> Hi WizaRds,
>
> What is the "standard" way to get the day of the week from a date such
> as as.Date("2006-12-01")? It looks like fCalendar has some functions
> but this requires a change in the R locale to GMT. Is there another way?
>
> Thanks!
>
> Jack.
>
>
> ---------------------------------
> Be a PS3 game guru.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From weigand.stephen at charter.net  Thu Jan 25 06:03:53 2007
From: weigand.stephen at charter.net (Stephen D. Weigand)
Date: Wed, 24 Jan 2007 23:03:53 -0600
Subject: [R] Fit model to data and use model for data generation
In-Reply-To: <001501c73fd5$8fb9f060$336f12ac@matrix.com>
References: <001501c73fd5$8fb9f060$336f12ac@matrix.com>
Message-ID: <67996c0b2c522fe664820cc9829af2b8@charter.net>


On Jan 24, 2007, at 10:34 AM, Benjamin Otto wrote:

> Hi,
>
> Suppose I have a set of values x and I want to calculate the 
> distribution of
> the data. Ususally I would use the "density" command. Now, can I use 
> the
> resulting "density-object" model to generate a number of new values 
> which
> have the same distribution? Or do I have to use some different 
> function?
>
> Regards,
>
> Benjamin
>
> -- 
> Benjamin Otto
> Universitaetsklinikum Eppendorf Hamburg
> Institut fuer Klinische Chemie
> Martinistrasse 52
> 20246 Hamburg
>

You could sample from the x's in the density object with probability
given by the y's:

### Create a bimodal distribution
x <- c(rnorm(25, -2, 1), rnorm(50, 3, 2))
d <- density(x, n = 1000)
plot(d)

### Sample from the distribution and show the two
### distributions are the same
x.new <- sample(d$x, size = 100000, # large n for proof of concept
                 replace = TRUE, prob = d$y/sum(d$y))
dx.new <- density(x.new)
lines(dx.new$x, dx.new$y, col = "blue")

Hope this helps,

Stephen
Rochester, Minnesota, USA


From yvonnick.noel at uhb.fr  Wed Jan 24 20:08:58 2007
From: yvonnick.noel at uhb.fr (NOEL Yvonnick)
Date: Wed, 24 Jan 2007 20:08:58 +0100
Subject: [R] as.numeric(".1") under RGtk2
In-Reply-To: <Pine.LNX.4.64.0701241735170.28321@gannet.stats.ox.ac.uk>
References: <45B7895A.6090309@uhb.fr>
	<17847.36829.31217.7004@stat.math.ethz.ch>
	<45B793F6.9060507@uhb.fr>
	<Pine.LNX.4.64.0701241735170.28321@gannet.stats.ox.ac.uk>
Message-ID: <45B7AECA.7070108@uhb.fr>

Prof Brian Ripley a ?crit :
> I can reproduce this via
> 
>> Sys.setlocale("LC_NUMERIC", "fr_FR")
> [1] "fr_FR"
> Warning message:
> setting 'LC_NUMERIC' may cause R to function strangely in: 
> setlocale(category, locale)
>> as.numeric(",1")
> [1] 0,1
>> as.numeric(".1")
> [1] NA
> Warning message:
> NAs introduced by coercion
> 
> Assuming you have not done that anywhere, it should not happen. If you 
> have, you were warned.  (Have you tried starting R with --vanilla to be 
> sure?)
> 
> as.numeric() is using strtod which should only be affected by the locale 
> category LC_NUMERIC, and R itself does not set LC_NUMERIC.  So either 
> you or some rogue OS function must have, unless there is a pretty major 
> bug in the OS.  (Just using a UTF-8 fr_FR locale does not do it on 
> either of the Linux variants I tried.)

Thanks for these helpful indications. This seems to be related to the 
RGtk2 package :

# Before loading RGtk2
 > as.numeric(".1")
[1] 0.1
 > as.numeric(",1")
[1] NA
Warning message:
NAs introduits lors de la conversion automatique
# After library loading
 > library("RGtk2")
 > as.numeric(".1")
[1] NA
Warning message:
NAs introduits lors de la conversion automatique
 > as.numeric(",1")
[1] 0,1

I send a copy of this post to the RGtk2 package maintainers.

Thanks for your help,

Yvonnick Noel, PhD.
Dpt of Psychology
U. of Rennes
France


From jenny197806 at yahoo.se  Wed Jan 24 18:26:17 2007
From: jenny197806 at yahoo.se (Jenny persson)
Date: Wed, 24 Jan 2007 18:26:17 +0100 (CET)
Subject: [R] keep track of selected observations over time
In-Reply-To: <b3161b5f0701240629h6fb447a2rb0a2b25e80a79b36@mail.gmail.com>
Message-ID: <20070124172618.52471.qmail@web28005.mail.ukl.yahoo.com>


Thanks Peter, I forgot that the mailinglist only accept the pdf and ps. file.
   
  Here is my problem again:
   
  Attached is a graph  of four boxplots from one patient?s data at four time points, i.e. each boxplot presents the data at each time point. At day 0 there are 5 extreme values from five peptide sequences (please see the below data). Since the response is changing over time, some of these five extreme values at day 0 may be lower or higher at day 56, 112 and 252. How can I trace the location of each peptide sequence that has extreme value at day 0 on the box plots at day 56, 112 and 252 by color or number coding. For example, the most five responding peptides can be ranked from 5 (highest value) to 1 (lowest), so if I do the graph again I would see the five extreme values at day 0 as numbers 5-1 and each of these numbers can be any where on the box plot at day 56, 112 and 252 or instead of the rank numbers using the peptide sequence that corresponds to each value. Alternatively, the locations of each peptide sequence at the four time points could be linked by a line.
  I would like to repeat this procedure for time point 56, 112 and 252 as well. That is, at day 56, I want trace where on the box plot at day 0, 112 and 252, each of the four peptide sequences that have highest responses is. Again, these four values/peptides can be presented by different colors, numbers or their peptide sequences that distinguish them from the other most responding peptides at day 0, 112 and 252. Can I do the four procedures at the same time, I mean, if at each time point I want to keep track of where the most peptide responses from this time point are, then the total number of peptides at the four time points could be 20. That is for each box plot, there will be 20 id numbers corresponding to each peptide at respectively time point.  The graph can be kind of messy.
   
   I have a simple solution of how to see the most responding peptides changing over time, by plotting each peptide?s responses at the four time points. But I haven?t managed the procedure above. If you have any suggestion how I can do this in R, I would be very thankful.
   
  Many thanks 
  Jenny
   
   
   
  Part of the data:
   
  > pat1[1:20,]
            peptides       P1_D0      P1_D56      P1_D112     P1_D252
  1    AAAKKGSEQTLKS -0.06181601 -0.12610877 -0.057898384 -0.02126862
  2    AAAAPASEDEDDE -0.10972387 -0.17174722 -0.136468783 -0.16262501
  3    AAAAVSSAKRSLR  0.64156129  1.02630879  0.079891841  0.29757984
  4    AAAKKGSEQESVK -0.54943062 -0.34311337 -0.338910367 -0.14526498
  5    AAANLTKIVAYLG -1.72207627 -1.63326368 -0.459839317 -0.63302448
  6    AACGRISYNDMFE  0.52513671  0.65123495  1.151866644  1.49481479
  7    AAEAEKAASESLR -0.69366543 -0.47038765 -0.144156174 -0.16042556
  8    AAEHAQSCRSSAA -0.13373130 -0.09229543 -0.102485597 -0.09782440
  9  AAERHARLNDSYRLQ -0.19316423 -0.33164239 -0.033764989 -0.11734969
  10   AAETISAARALPS -0.49632307 -0.53666696 -0.263024663 -0.18231712
  11 AAEVQRFNRDVDETI -0.80014439 -0.91002202 -0.257201702 -0.12391146
  12   AAEWTANVTAEFK -0.41544438 -0.10980658 -0.288133150 -0.32022460
  13   AAGIQWSEEETED -0.04015673  0.08529726  0.002471231  0.07599156
  14   AAGPALSPVPPVV -0.26795462 -0.36739148 -0.512049278 -0.25449224
  15   AAGPPPSEGEEST -1.59272674 -1.69729759 -0.843351943 -0.49271773
  16   AAKIASRQPDSHI -0.40722382 -0.27236225 -0.224539441 -0.32998813
  17   AAKIQASFRGHMA  2.41234976  2.84435484  0.160852331  0.80197802
  18   AALDLGGSSDPYV -1.21202038 -1.25109705 -0.259515922 -0.24351352
  19   AALEPGPSESLTA -2.00256570 -1.57566020 -0.390584034 -0.23682626
  20 AALLELWELRRQQYE  1.42797600  1.33539104  1.486154861  1.67471189
   
   
  par(las=1)   # all axis labels horizontal
        boxplot(data.frame(pat1[,c(2:5)]), pars = list(boxwex = 0.4,                                staplewex = 0.8, outwex = .5), 
                   boxfill="lightblue",border=c(3:6), names=c("Day 0", "56 days","112 days", "252 days"),
                                    col.main="blue",  
                   main ="Averaged peptide response at 4 different time points for patient 200001",cex.main=0.9, font.main=0.9)
               
   
   
   
  
Peter Konings <peter.l.e.konings at gmail.com> skrev:
  Dear Jenny,

Your post did not have an attachment. The mailing list software strips most attachments away: see the 'technical details of posting' section of the posting guide at:
http://www.r-project.org/posting-guide.html.

HTH 
Peter.

  On 1/24/07, Jenny persson < jenny197806 at yahoo.se > wrote:   Dear all,

  Attached is a description of my data, graph and the problem which I need help with. Hope you have time to open the file and help me out.

  Many thanks,
  Jenny


--------------------------------- 



______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.






 		
---------------------------------

-------------- next part --------------
En bilaga som inte var text, skiljdes ut...
Namn       : boxplot.pdf
Typ       : application/pdf
Storlek: 138648 bytes
Beskrivning: 3741701167-boxplot.pdf
URL        : https://stat.ethz.ch/pipermail/r-help/attachments/20070124/ec536b54/attachment.pdf 

From lawremi at iastate.edu  Thu Jan 25 03:38:55 2007
From: lawremi at iastate.edu (Michael Lawrence)
Date: Wed, 24 Jan 2007 20:38:55 -0600
Subject: [R] as.numeric(".1") under RGtk2
In-Reply-To: <45B7AECA.7070108@uhb.fr>
References: <45B7895A.6090309@uhb.fr>
	<17847.36829.31217.7004@stat.math.ethz.ch> <45B793F6.9060507@uhb.fr>
	<Pine.LNX.4.64.0701241735170.28321@gannet.stats.ox.ac.uk>
	<45B7AECA.7070108@uhb.fr>
Message-ID: <509e0620701241838u39d4fe5bt20b0015b7dce21f5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070124/c0b767c1/attachment.pl 

From phwang2000 at ucla.edu  Thu Jan 25 07:28:45 2007
From: phwang2000 at ucla.edu (phwang2000 at ucla.edu)
Date: Wed, 24 Jan 2007 22:28:45 -0800
Subject: [R] R programming question, one dimensional optimization
Message-ID: <20070124222845.6bo56d6kn4g40sw0@mail.ucla.edu>

Hi,

I?have?an?optimization?for?x'Ax/x'Bx,?x?is?a?vector,?A/B?are?matrix,

I?wrote?a?small?program?which?can?take?in?2?matrices?and?a?vector?and?a?
variable,?this?
program?combine?the?variable?and?the?vector?and?generate?a?new?vector,?then?
test?the?
x'Ax/x'Bx.

However?I?dodnot?know?if?there?is?a?way?that?can?calculate?the?x?
automatically?instead?of?
I?typing?different?values?to?get?the?result?and?compare.

===================================================
getMultiVal3?=?function?(a,?b,?cc,?x)?{
+?????i-1;?j=1;?sum=0;?sum2=0
+?????n=nrow(a);?v=?c(x,?cc)
+?????for?(i?in?1:n)?{
+???????for?(j?in?1:n)?{
+??????????sum=sum+a[i,j]*v[i]*v[j];?sum2=sum2+b[i,j]*v[i]*v[j]
+???????}
+?????}
+????return(sum/sum2)
+???}


From dusa.adrian at gmail.com  Tue Jan 23 12:28:08 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Tue, 23 Jan 2007 13:28:08 +0200
Subject: [R] [R-pkgs] version 0.3 of QCA
Message-ID: <200701231328.08930.dusa.adrian@gmail.com>


Dear list members,

A new version of the QCA package is now on CRAN.
The QCA package implements the Quine-McCluskey algorithm for boolean 
minimizations, according to the Qualitative Comparative Analysis.

Along with the additional improvements in version 0.3-1 (soon to be released 
on CRAN), this code is about 100 times faster than the previous "major" 
release (0.2-6). It can now reasonably work with 11 binary variables, finding 
a complete (and exact) solution in less than 2 minutes.

This dramatic increase in speed is due to using a mathematical reduction 
instead of an algorithmic one. This approach openes the way for _exact_ 
multi-value minimizations, and an even better (and faster) approach is 
searched for the future versions.

Best,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From ripley at stats.ox.ac.uk  Thu Jan 25 08:50:08 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Jan 2007 07:50:08 +0000 (GMT)
Subject: [R] poly(x) workaround when x has missing values
In-Reply-To: <Pine.OSX.4.58.0612251301200.11199@biostat5.ucdavis.edu>
References: <Pine.OSX.4.58.0612251301200.11199@biostat5.ucdavis.edu>
Message-ID: <Pine.LNX.4.64.0701250727350.5353@gannet.stats.ox.ac.uk>

Orthpgpnality of polynomials is not defined if they contain missing 
values, which seems a good enough reason to me.

Put it another way, in your solution whether the columns are orthogonal 
depends on the unknown values of the NAs, and it looks like is only true 
if the unknown values are all zero.

On Wed, 24 Jan 2007, Jacob Wegelin wrote:

>
> Often in practical situations a predictor has missing values, so that poly
> crashes. For instance:
>
>> x<-1:10
>> y<- x -  3 * x^2 + rnorm(10)/3
>> x[3]<-NA
>> lm( y ~ poly(x,2) )
> Error in poly(x, 2) : missing values are not allowed in 'poly'
>>
>> lm( y ~ poly(x,2) , subset=!is.na(x)) # This does not help?!?
> Error in poly(x, 2) : missing values are not allowed in 'poly'
>
> The following function seems to be an okay workaround.
>
> Poly<- function(x, degree = 1, coefs = NULL, raw = FALSE, ...) {
>        notNA<-!is.na(x)
>        answer<-poly(x[notNA], degree=degree, coefs=coefs, raw=raw, ...)
>        THEMATRIX<-matrix(NA, nrow=length(x), ncol=degree)
>        THEMATRIX[notNA,]<-answer
>        attributes(THEMATRIX)[c('degree', 'coefs', 'class')]<- attributes(answer)[c('degree', 'coefs', 'class')]
>        THEMATRIX
> }
>
>
>>  lm( y ~ Poly(x,2) )
>
> Call:
> lm(formula = y ~ Poly(x, 2))
>
> Coefficients:
> (Intercept)  Poly(x, 2)1  Poly(x, 2)2
>      209.1        475.0        114.0
>
> and it works when x and y are in a dataframe too:
>
>> DAT<-data.frame(x=x, y=y)
>> lm(y~Poly(x,2), data=DAT)
>
> Call:
> lm(formula = y ~ Poly(x, 2), data = DAT)
>
> Coefficients:
> (Intercept)  Poly(x, 2)1  Poly(x, 2)2
>    -119.54      -276.11       -68.24
>
> Is there a better way to do this? My workaround seems a bit awkward.
> Whoever wrote "poly" must have had a good reason for not making it deal
> with missing values?
>
> Thanks for any thoughts
>
> Jacob Wegelin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From aiminy at iastate.edu  Thu Jan 25 08:54:05 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Thu, 25 Jan 2007 01:54:05 -0600
Subject: [R] rpart question
Message-ID: <6.1.2.0.2.20070125014723.01c72f30@aiminy.mail.iastate.edu>

I make classification tree like this code
p.t2.90 <- rpart(y~aa_three+bas+bcu+aa_ss, 
data=training,method="class",control=rpart.control(cp=0.0001))

Here I want to set weight for 4 predictors(aa_three,bas,bcu,aa_ss).

I know that there is a weight set-up in rpart.
Can this set-up satisfy my need?

If so, could someone give me an example?

Thanks,

Aimin Yan


From ripley at stats.ox.ac.uk  Thu Jan 25 09:04:58 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Jan 2007 08:04:58 +0000 (GMT)
Subject: [R] Size of data vs. needed memory...rule of thumb?
In-Reply-To: <916551423F01504BA339BF69CBA3BE72016E7A35@psmrdcex18.psm.pin.safeco.com>
References: <916551423F01504BA339BF69CBA3BE72016E7A35@psmrdcex18.psm.pin.safeco.com>
Message-ID: <Pine.LNX.4.64.0701250751490.5353@gannet.stats.ox.ac.uk>

On Wed, 24 Jan 2007, WILLIE, JILL wrote:

> I have been searching all day & most of last night, but can't find any
> benchmarking or recommendations regarding R system requirements for very
> large (2-5GB) data sets to help guide our hardware configuration.  If
> anybody has experience with this they're willing to share or could
> anybody point me in a direction that might be productive to research, it
> would be much appreciated.  Specifically:  will R simply use as much
> memory as the OS makes available to it, unlimited?

Under most OSes.  Because Windows has no means to limit the amount made 
available, R under Windows does have it own limiting mechanism (which you 
hit in the examples below).  R under Linux will allow you to run a 4GB 
process on a machine with 2GB RAM, but you are likely to get around 0.7% 
usage.  (One of my colleagues did that on a server earlier this week, 
hence the very specific answer.)

> Is there a multi-threading version R, packages?

Not to run computations in R.  Some parts of R (e.g. GUIs) and some 
libraries (e.g. some BLAS) are multithreaded.  There are multiprocess
packages, e.g. Rmpi, rpvm, snow.

> Does the core R package support 64-bit

Yes, and has for many years.

> & should I expect to see any difference in how memory's handled under 
> that version?

yes, because the address space will not get seriously fragmented.  See the 
appropriate section in R-admin.html (referenced from INSTALL).

> Is 3 GB of memory to 1GB of data a reasonable ballpark?

I'd say it was a bit low, but it really depends on the analysis you are 
doing, how 1GB of data is made up (many rows?, many cols?, etc) and so on.
Had you asked me to suggest a ratio I would have said 5.

> Our testing thus far has been on a windows 32-bit box w/1GB of RAM & 1
> CPU; it appears to indicate something like 3GB of RAM for every 1GB of
> sql table (ex-indexes, byte-sized factors).  At this point, we're
> planning on setting up a dual core 64-bit Linux box w/16GB of RAM for
> starters, since we have summed-down sql tables of approx 2-5GB
> generally.
>
> Here's details, just for context, or in case I'm misinterpreting the
> results, or in case there's some more memory-efficient way to get data
> in R's binary format than going w/the data.frame.

Well, sqlQuery returns a data frame directly.  I think you need to tell 
RODBC to translate your 'byte-sized factors' to numeric, as it will be 
going through character if these are a type it does not know about.

> R session:
> 		> library(RODBC)
> 		> channel<-odbcConnect("psmrd")
> 		> FivePer <-data.frame(sqlQuery(channel, "select * from
> AUTCombinedWA_BILossCost_5per"))
>
> 		Error: cannot allocate vector of size 2000 Kb
> 		In addition: Warning messages:
> 		1: Reached total allocation of 1023Mb: see
> help(memory.size)
> 		2: Reached total allocation of 1023Mb: see
> help(memory.size)
>
>
> ODBC connection:
> 		Microsoft SQL Server ODBC Driver Version 03.86.1830
>
> 		Data Source Name: psmrd
> 		Data Source Description:
> 		Server: psmrdcdw01\modeling
> 		Database: OpenSeas_Work1
> 		Language: (Default)
> 		Translate Character Data: Yes
> 		Log Long Running Queries: No
> 		Log Driver Statistics: No
> 		Use Integrated Security: Yes
> 		Use Regional Settings: No
> 		Prepared Statements Option: Drop temporary procedures on
> disconnect
> 		Use Failover Server: No
> 		Use ANSI Quoted Identifiers: Yes
> 		Use ANSI Null, Paddings and Warnings: Yes
> 		Data Encryption: No
>
> Please be patient, I'm a new R user (or at least I'm trying to be...at
> this point I'm mostly a new R-help-reader); I'd appreciated being
> pointed in the right direction if this isn't the right help list to send
> this question to...or if this question is poorly worded (I did read the
> posting guide).
>
> Jill Willie
> Open Seas
> Safeco Insurance
> jilwil at safeco.com
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Thu Jan 25 09:10:44 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Jan 2007 08:10:44 +0000 (GMT)
Subject: [R] rpart question
In-Reply-To: <6.1.2.0.2.20070125014723.01c72f30@aiminy.mail.iastate.edu>
References: <6.1.2.0.2.20070125014723.01c72f30@aiminy.mail.iastate.edu>
Message-ID: <Pine.LNX.4.64.0701250805450.5353@gannet.stats.ox.ac.uk>

On Thu, 25 Jan 2007, Aimin Yan wrote:

> I make classification tree like this code
> p.t2.90 <- rpart(y~aa_three+bas+bcu+aa_ss,
> data=training,method="class",control=rpart.control(cp=0.0001))
>
> Here I want to set weight for 4 predictors(aa_three,bas,bcu,aa_ss).
>
> I know that there is a weight set-up in rpart.
> Can this set-up satisfy my need?

It depends on what _you_ mean by 'set weight'.  You will need to tell us 
in detail what exactly you want the weights to do.

Using the 'weights' argument is specifying case weights (as the help 
says).  There are also 'cost' and 'parms' for other aspects of weighting.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Thu Jan 25 09:25:11 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Jan 2007 08:25:11 +0000 (GMT)
Subject: [R] Fit model to data and use model for data generation
In-Reply-To: <67996c0b2c522fe664820cc9829af2b8@charter.net>
References: <001501c73fd5$8fb9f060$336f12ac@matrix.com>
	<67996c0b2c522fe664820cc9829af2b8@charter.net>
Message-ID: <Pine.LNX.4.64.0701250816310.5353@gannet.stats.ox.ac.uk>

On Wed, 24 Jan 2007, Stephen D. Weigand wrote:

>
> On Jan 24, 2007, at 10:34 AM, Benjamin Otto wrote:
>
>> Hi,
>>
>> Suppose I have a set of values x and I want to calculate the
>> distribution of
>> the data. Ususally I would use the "density" command. Now, can I use
>> the
>> resulting "density-object" model to generate a number of new values
>> which
>> have the same distribution? Or do I have to use some different
>> function?
>>
>> Regards,
>>
>> Benjamin
>>
>> --
>> Benjamin Otto
>> Universitaetsklinikum Eppendorf Hamburg
>> Institut fuer Klinische Chemie
>> Martinistrasse 52
>> 20246 Hamburg
>>
>
> You could sample from the x's in the density object with probability
> given by the y's:

That gives a discrete distribution, which may well matter for small 
samples.

Since density() is returning an equal-weighted mixture of (by default)
normal distributions, all you need to do is

x.new <- rnorm(n, sample(x, size = n, replace=TRUE), bw)

where bw is the bandwidth used by density (d$bw in this example).
(This is known as a 'smoothed bootstrap' in some circles.)


> ### Create a bimodal distribution
> x <- c(rnorm(25, -2, 1), rnorm(50, 3, 2))
> d <- density(x, n = 1000)
> plot(d)
>
> ### Sample from the distribution and show the two
> ### distributions are the same
> x.new <- sample(d$x, size = 100000, # large n for proof of concept
>                 replace = TRUE, prob = d$y/sum(d$y))
> dx.new <- density(x.new)
> lines(dx.new$x, dx.new$y, col = "blue")

BTW, lines(density(x.news), col = "blue") works here, and you do need to 
remember that a kde is biased.  But my solution matches better than yours.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Thu Jan 25 10:17:48 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Jan 2007 09:17:48 +0000 (GMT)
Subject: [R] using non-ASCII strings in R packages
In-Reply-To: <94E133D09AA24D43BF6341B675C01A330753C5@uu01msg-exb01.soliscom.uu.nl>
References: <94E133D09AA24D43BF6341B675C01A330753C5@uu01msg-exb01.soliscom.uu.nl>
Message-ID: <Pine.LNX.4.64.0701250842080.6600@gannet.stats.ox.ac.uk>

On Thu, 25 Jan 2007, Bojanowski, M.J.  (Michal) wrote:

> Hello dear useRs and wizaRds,
>
> I am currently developing a package that will enable to use 
> administrative map of Poland in R plots. Among other things I wanted to 
> include region names in proper Polish language so that they can be used 
> in creating graphics etc. I am working on Windows and when I build the 
> package it is complaining about non-ASCII characters R code files.
>
> I was wondering what would be the best way to properly implement them in 
> a platform-independent way so that they can be used in computations as 
> well as in producing PS, PDF and other graphic output. Unfortunately I 
> have a limited knowledge of encoding schemes etc. Is it OK to include 
> them in Windows-1250 encoding (default for Polish locale, as far as I 
> know)? I believe this problem is frequently confronted for other 
> "non-latin1" languages.

Well, infrequently, and it has been answered a few times before (including 
in my talk at UseR 2006, 
http://www.r-project.org/useR-2006/Slides/Ripley.pdf).

> If it is not the way to go, I would be very grateful for suggestions.

Since a Japanese-language Windows machine cannot reproduce Polish 
non-ASCII characters, the portability you seek is not possible for reasons 
outside R.  And many other systems cannot plot in both Polish and their 
native language, or at least not in the same font.

ISOLatin2 is the standard 8-bit encoding for Polish: Windows CP1250 is a 
superset, AFAIR.  If all your users are using an 8-bit Polish locale, 
ISOLatin2 would be safe, but not otherwise.  Even then, there is no 
guarantee that the Polish characters would be in the fonts used in 
PostScript and PDF: some fonts only cover ISOLatin1.

There is one thing you can do to make this a little more portable (and 
avoid the warnings).  If you store the strings concerned in a text file in 
ISOLatin2, and read them into R at run time (e.g. when your package is 
loaded), you can make use of file(encoding=) or iconv() to convert them to 
the current encoding.  That will succeed in ISOLatin2 or CP1250 or UTF-8 
locales and fail otherwise.

Unfortunately that is not the end of the story for users of UTF-8 locales. 
as postscript() and pdf() do not support UTF-8 (as the graphics languages 
do not) and need to be told to use encoding="ISOLatin2.enc", and the X11 
system has a mind of its own and may not show non-ASCII characters in some 
fonts (or worse, render them incorrectly).

The use of Unicode was supposed to reduce the impact of Babel.  But
implementation split into two camps (Windows with UCS-2 and Unix-alikes 
with UTF-8) and some important players (e.g. Adobe) have ignored it, so it 
has only been a very partial solution.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From shubhak at ambaresearch.com  Thu Jan 25 10:41:13 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Thu, 25 Jan 2007 15:11:13 +0530
Subject: [R] Substring error
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3E4B48D@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070125/e91d666d/attachment.pl 

From hwborchers at googlemail.com  Thu Jan 25 10:55:55 2007
From: hwborchers at googlemail.com (Hans Werner Borchers)
Date: Thu, 25 Jan 2007 10:55:55 +0100
Subject: [R] Subgroup discovery in R
Message-ID: <ab3458980701250155m35dfb72dx1dfc0f66e4001cc7@mail.gmail.com>

I would very much like to apply "subgroup discovery" techniques to some
of the data I am analyzing at this moment.

Subgroup discovery is an interesting approach and is quite well known in
the Data Mining community, though in essence it is a purely statistical
approach.

To read an introductory article see "Subgroup discovery and visualization
methods" <soleunet.ijs.si/website/other/final_report/html/WP5-s14.html>.
To my knowledge it was originally developed in the 1990ies by W. Kl?sgen
and Stefan Wrobel.

I can only recommend this technique as I have had some/many successes and
surprising insights into data when in the past I had access to commercial
versions of it (MIDOS in the KEPLER tool).

Please don't mix it up with "subgroup analysis" which is often mentioned
in clinical studies, but is not meant as a discovery technique.

My R site searches have not uncovered any hits, also I am following the
R-help list for some time now and cannot remember seeing any hints to it.

If there is no program in R or piece of code that could be reused, then
I would start writing my own simple version, though it's not quite trivial
to implement and may be slow on large data sets.

Many thanks, Hans Werner Borchers


From preuth at slf.ch  Thu Jan 25 11:17:51 2007
From: preuth at slf.ch (Thomas Preuth)
Date: Thu, 25 Jan 2007 11:17:51 +0100
Subject: [R] change the attribute of a column
Message-ID: <45B883CF.2060804@slf.ch>

Hello,

I have a column which is filled by numbers, but is attributed as list.
How can I change the attributes into numeric to use statistic operations?

Greetings, Thomas

-- 
Thomas Preuth
Swiss federal institutes for forest, snow and landscape research WSL/SLF
Fluuelastrasse 11
CH- 7260 Davos
Switzerland

0041-81-4170-359
preuth at slf.ch
www.wsl.ch
www.slf.ch


From H.E.Clough at liverpool.ac.uk  Thu Jan 25 11:29:20 2007
From: H.E.Clough at liverpool.ac.uk (Clough, Helen)
Date: Thu, 25 Jan 2007 10:29:20 -0000
Subject: [R] Request for help re. opening workspace
Message-ID: <F9BF1D738D98AD4C83486F0D550DC637CF2358@EVSSTAFF1.livad.liv.ac.uk>

Dear all,
 
I am wondering if someone can help me out of a pickle - forgive me if
there is an obvious solution to the following - I am not very proficient
in the more technical aspects of R. 
 
I am working with a large data set in R, which for reasons of
housekeeping I normally remove upon finishing a session and read in
again at the next start-up, seeking to avoid problems with huge
workspaces. However, rather stupidly last time I forgot to do this, and
closed the session, which saved without complaint. However, now when I
try to start up, I get the following message:
 
Error: cannot allocate vector of size 2023kb
In addition: Warning messages:
1: Reached total allocation of 1536Mb: see help(memory.size)
2: Reached total allocation of 1536Mb: see help(memory.size)
 
followed by a window with a button which says
 
Fatal error: unable to restore saved data in .RData
 
and when I click "OK" obviously everything shuts down.
 
Is there anything I can do to resolve this?? The workspace itself
appears to be just over 100MB. I do have two backups, but I suspect that
the most recent one may be subject to the same problem, and the other is
rather older. I do have my routines etc. saved outside R but would
rather avoid a tedious process of re-entering code if I can.
 
Many thanks - all suggestions gratefully received,
 
Helen Clough
Lecturer in Risk Analysis,
National Centre for Zoonosis Research,
University of Liverpool


From ripley at stats.ox.ac.uk  Thu Jan 25 11:29:05 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Jan 2007 10:29:05 +0000 (GMT)
Subject: [R] Substring error
In-Reply-To: <3948d9e50701250209i9f5f37cn9803978d2fb1b3c6@mail.gmail.com>
References: <A36876D3F8A5734FA84A4338135E7CC3E4B48D@BAN-MAILSRV03.Amba.com>
	<3948d9e50701250209i9f5f37cn9803978d2fb1b3c6@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701251020590.21212@auk.stats>

On Thu, 25 Jan 2007, talepanda wrote:

> It occurs why start or stop could not be converted into integer of
> length > 0 by using as.interger().
>
> More presicely,
> if( !isInteger(sa) || !isInteger(so) || k == 0 || l == 0 ) # c code
> where sa is start, so is stop, k is length(start), l is length(stop)
>
> For example:
>
> substr("orz",character(0),0)
> substr("orz",numeric(0),0)
> substr("orz",logical(0),0)
> substr("orz",integer(0),0)
> substr("orz",NULL,0)
>
> produce the error because of k==0
>
>
> But there seems to be another case which I don't know.

That's essentially it.  The first two conditions are merely protection 
against someone calling the .Internal directly.

In particular, there is no documentation as to what happens if start or 
stop is NA, a case I don't think is handled correctly.

>
>
> On 1/25/07, Shubha Vishwanath Karanth <shubhak at ambaresearch.com> wrote:
>> Hi,
>>
>>
>>
>> Do anybody know when and why the below error we get?
>>
>>
>>
>> Error in substr(x, as.integer(start), as.integer(stop)) :
>>         invalid substring argument(s) in substr()
>>
>>
>>
>> Thanks in advance,
>>
>> Shubha
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mcardeal at ufba.br  Thu Jan 25 14:18:43 2007
From: mcardeal at ufba.br (Mauricio Cardeal)
Date: Thu, 25 Jan 2007 10:18:43 -0300
Subject: [R] filling the area
Message-ID: <45B8AE33.8060005@ufba.br>

Please, how to fill the area under the curve?

x <- c(1:10)
y <- c(rnorm(10))
plot(x,y)
lines(x,y)

Thanks,
Mauricio Cardeal


From bcarvalh at jhsph.edu  Thu Jan 25 14:25:39 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Thu, 25 Jan 2007 08:25:39 -0500
Subject: [R] filling the area
In-Reply-To: <45B8AE33.8060005@ufba.br>
References: <45B8AE33.8060005@ufba.br>
Message-ID: <C56E8331-E773-4A75-9B63-049BC284295A@jhsph.edu>

take a look at:

?polygon

b

On Jan 25, 2007, at 8:18 AM, Mauricio Cardeal wrote:

> Please, how to fill the area under the curve?
>
> x <- c(1:10)
> y <- c(rnorm(10))
> plot(x,y)
> lines(x,y)
>
> Thanks,
> Mauricio Cardeal


From mcardeal at ufba.br  Thu Jan 25 14:34:23 2007
From: mcardeal at ufba.br (Mauricio Cardeal)
Date: Thu, 25 Jan 2007 10:34:23 -0300
Subject: [R] filling the area
In-Reply-To: <C56E8331-E773-4A75-9B63-049BC284295A@jhsph.edu>
References: <45B8AE33.8060005@ufba.br>
	<C56E8331-E773-4A75-9B63-049BC284295A@jhsph.edu>
Message-ID: <45B8B1DF.6010208@ufba.br>

Ok. I?ve tried before and all I got was a polygon indeed with the 
extremes points connected. Please where is the error ?

polygon(x,y,col="gray", border = "red")

Benilton Carvalho escreveu:
> take a look at:
>
> ?polygon
>
> b
>
> On Jan 25, 2007, at 8:18 AM, Mauricio Cardeal wrote:
>
>> Please, how to fill the area under the curve?
>>
>> x <- c(1:10)
>> y <- c(rnorm(10))
>> plot(x,y)
>> lines(x,y)
>>
>> Thanks,
>> Mauricio Cardeal
>
> __________ NOD32 1.1752 (20060912) Information __________
>
> This message was checked by NOD32 antivirus system.
> http://www.nod32.com
>
>
>


From marc_schwartz at comcast.net  Thu Jan 25 14:35:58 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 25 Jan 2007 07:35:58 -0600
Subject: [R] filling the area
In-Reply-To: <45B8AE33.8060005@ufba.br>
References: <45B8AE33.8060005@ufba.br>
Message-ID: <1169732158.4882.8.camel@localhost.localdomain>

On Thu, 2007-01-25 at 10:18 -0300, Mauricio Cardeal wrote:
> Please, how to fill the area under the curve?
> 
> x <- c(1:10)
> y <- c(rnorm(10))
> plot(x,y)
> lines(x,y)
> 
> Thanks,
> Mauricio Cardeal

See ?polygon

x <- 1:10
y <- rnorm(10)

plot(x,y, type = "o")

polygon(c(min(x), x, max(x)), c(min(y), y, min(y)),  col = "blue")


HTH,

Marc Schwartz


From M.Cortina at ich.ucl.ac.uk  Thu Jan 25 14:43:22 2007
From: M.Cortina at ich.ucl.ac.uk (Mario Cortina Borja)
Date: Thu, 25 Jan 2007 13:43:22 -0000
Subject: [R] filling the area
In-Reply-To: <45B8AE33.8060005@ufba.br>
References: <45B8AE33.8060005@ufba.br>
Message-ID: <004b01c74086$c7baec60$cbf652c2@Dell>

Hi Mauricio,

Try:

plot(x,y); lines(x,y)
polygon( c(min(x), x, max(x)), c( min(y), y, min(y)), density=100 )

Best wishes,

Mario

Dr Mario Cortina Borja
Senior Lecturer in Statistics
Centre for Paediatric Epidemiology and Biostatistics
Institute of Child Health, University College London
M.Cortina at ich.ucl.ac.uk
Tel - 020 7905 2113
FAX - 020 7905 2381


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Mauricio Cardeal
> Sent: 25 January 2007 13:19
> To: r-help at stat.math.ethz.ch
> Subject: [R] filling the area
> 
> Please, how to fill the area under the curve?
> 
> x <- c(1:10)
> y <- c(rnorm(10))
> plot(x,y)
> lines(x,y)
> 
> Thanks,
> Mauricio Cardeal
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Thu Jan 25 14:57:19 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Jan 2007 13:57:19 +0000 (GMT)
Subject: [R] Request for help re. opening workspace
In-Reply-To: <F9BF1D738D98AD4C83486F0D550DC637CF2358@EVSSTAFF1.livad.liv.ac.uk>
References: <F9BF1D738D98AD4C83486F0D550DC637CF2358@EVSSTAFF1.livad.liv.ac.uk>
Message-ID: <Pine.LNX.4.64.0701251341240.19502@gannet.stats.ox.ac.uk>

This is I assume Windows?  If your version of Windows has the /3GB switch 
set, try starting with

Rgui --max-mem-size=2.5G

(if not, that will probably not work) and see if the workspace will load 
(slowly).

Saved workspaces are compressed, often highly compressed.

I have to say I am pretty unsure about this, as you appear to have saved a 
2023kb vector and how you manage to create that is unclear: perhaps you 
used memory.limit?


On Thu, 25 Jan 2007, Clough, Helen wrote:

> Dear all,
>
> I am wondering if someone can help me out of a pickle - forgive me if
> there is an obvious solution to the following - I am not very proficient
> in the more technical aspects of R.
>
> I am working with a large data set in R, which for reasons of
> housekeeping I normally remove upon finishing a session and read in
> again at the next start-up, seeking to avoid problems with huge
> workspaces. However, rather stupidly last time I forgot to do this, and
> closed the session, which saved without complaint. However, now when I
> try to start up, I get the following message:
>
> Error: cannot allocate vector of size 2023kb
> In addition: Warning messages:
> 1: Reached total allocation of 1536Mb: see help(memory.size)
> 2: Reached total allocation of 1536Mb: see help(memory.size)
>
> followed by a window with a button which says
>
> Fatal error: unable to restore saved data in .RData
>
> and when I click "OK" obviously everything shuts down.
>
> Is there anything I can do to resolve this?? The workspace itself
> appears to be just over 100MB. I do have two backups, but I suspect that
> the most recent one may be subject to the same problem, and the other is
> rather older. I do have my routines etc. saved outside R but would
> rather avoid a tedious process of re-entering code if I can.
>
> Many thanks - all suggestions gratefully received,
>
> Helen Clough
> Lecturer in Risk Analysis,
> National Centre for Zoonosis Research,
> University of Liverpool
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Thu Jan 25 15:14:30 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Jan 2007 14:14:30 +0000 (GMT)
Subject: [R] Request for help re. opening workspace
In-Reply-To: <Pine.LNX.4.64.0701251341240.19502@gannet.stats.ox.ac.uk>
References: <F9BF1D738D98AD4C83486F0D550DC637CF2358@EVSSTAFF1.livad.liv.ac.uk>
	<Pine.LNX.4.64.0701251341240.19502@gannet.stats.ox.ac.uk>
Message-ID: <Pine.LNX.4.64.0701251412340.24303@gannet.stats.ox.ac.uk>

On Thu, 25 Jan 2007, Prof Brian Ripley wrote:

> This is I assume Windows?  If your version of Windows has the /3GB switch 
> set, try starting with
>
> Rgui --max-mem-size=2.5G
>
> (if not, that will probably not work) and see if the workspace will load 
> (slowly).
>
> Saved workspaces are compressed, often highly compressed.
>
> I have to say I am pretty unsure about this, as you appear to have saved a 
> 2023kb vector and how you manage to create that is unclear: perhaps you used 
> memory.limit?

(I misread that: so if you do not have the /3GB switch starting with 
--max-mem-size=2000M may help.)

>
>
> On Thu, 25 Jan 2007, Clough, Helen wrote:
>
>> Dear all,
>> 
>> I am wondering if someone can help me out of a pickle - forgive me if
>> there is an obvious solution to the following - I am not very proficient
>> in the more technical aspects of R.
>> 
>> I am working with a large data set in R, which for reasons of
>> housekeeping I normally remove upon finishing a session and read in
>> again at the next start-up, seeking to avoid problems with huge
>> workspaces. However, rather stupidly last time I forgot to do this, and
>> closed the session, which saved without complaint. However, now when I
>> try to start up, I get the following message:
>> 
>> Error: cannot allocate vector of size 2023kb
>> In addition: Warning messages:
>> 1: Reached total allocation of 1536Mb: see help(memory.size)
>> 2: Reached total allocation of 1536Mb: see help(memory.size)
>> 
>> followed by a window with a button which says
>> 
>> Fatal error: unable to restore saved data in .RData
>> 
>> and when I click "OK" obviously everything shuts down.
>> 
>> Is there anything I can do to resolve this?? The workspace itself
>> appears to be just over 100MB. I do have two backups, but I suspect that
>> the most recent one may be subject to the same problem, and the other is
>> rather older. I do have my routines etc. saved outside R but would
>> rather avoid a tedious process of re-entering code if I can.
>> 
>> Many thanks - all suggestions gratefully received,
>> 
>> Helen Clough
>> Lecturer in Risk Analysis,
>> National Centre for Zoonosis Research,
>> University of Liverpool
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mcardeal at ufba.br  Thu Jan 25 15:14:00 2007
From: mcardeal at ufba.br (Mauricio Cardeal)
Date: Thu, 25 Jan 2007 11:14:00 -0300
Subject: [R] filling the area
In-Reply-To: <1169732158.4882.8.camel@localhost.localdomain>
References: <45B8AE33.8060005@ufba.br>
	<1169732158.4882.8.camel@localhost.localdomain>
Message-ID: <45B8BB28.4000208@ufba.br>

Thank you all. One more question about the syntax below: min(y) twice, is it?

polygon(c(min(x), x, max(x)), c(min(y), y, min(y)),  col = "blue")

polygon( c(min(x), x, max(x)), c( min(y), y, min(y)), density=100 )

Best whises,
Mauricio



Marc Schwartz escreveu:
> On Thu, 2007-01-25 at 10:18 -0300, Mauricio Cardeal wrote:
>   
>> Please, how to fill the area under the curve?
>>
>> x <- c(1:10)
>> y <- c(rnorm(10))
>> plot(x,y)
>> lines(x,y)
>>
>> Thanks,
>> Mauricio Cardeal
>>     
>
> See ?polygon
>
> x <- 1:10
> y <- rnorm(10)
>
> plot(x,y, type = "o")
>
> polygon(c(min(x), x, max(x)), c(min(y), y, min(y)),  col = "blue")
>
>
> HTH,
>
> Marc Schwartz
>
>
>
> __________ NOD32 1.1752 (20060912) Information __________
>
> This message was checked by NOD32 antivirus system.
> http://www.nod32.com
>
>
>
>


From pmilin at gmail.com  Thu Jan 25 15:15:21 2007
From: pmilin at gmail.com (Petar Milin)
Date: Thu, 25 Jan 2007 15:15:21 +0100
Subject: [R] Unable to install gtkDevice ...
Message-ID: <1169734521.21293.7.camel@localhost>

Hello!
I am trying to install gtkDevice package, but without success. It is
complaining about:
* Installing *source* package 'gtkDevice' ...
checking for gtk-config... no
checking for gtk12-config... no
ERROR: Cannot find gtk-config.

I am using Ubuntu 6.06, and I cannot find that package in the
repositories. Is there a remedy of any kind? What can I do?

Best,
Petar M.


From michael.conklin at markettools.com  Thu Jan 25 15:20:53 2007
From: michael.conklin at markettools.com (Michael Conklin)
Date: Thu, 25 Jan 2007 08:20:53 -0600
Subject: [R]  Creating dendrograms from a table
Message-ID: <8EA061E48306894180DB020B0C6907A16663B7@MNMAIL02.markettools.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070125/2912c7b5/attachment.pl 

From csardi at rmki.kfki.hu  Thu Jan 25 15:22:29 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Thu, 25 Jan 2007 15:22:29 +0100
Subject: [R] Unable to install gtkDevice ...
In-Reply-To: <1169734521.21293.7.camel@localhost>
References: <1169734521.21293.7.camel@localhost>
Message-ID: <20070125142229.GB6937@localdomain>

Petar,

gtk-config is likely to be in the libgtk1.2-dev Ubuntu package.

Gabor

On Thu, Jan 25, 2007 at 03:15:21PM +0100, Petar Milin wrote:
> Hello!
> I am trying to install gtkDevice package, but without success. It is
> complaining about:
> * Installing *source* package 'gtkDevice' ...
> checking for gtk-config... no
> checking for gtk12-config... no
> ERROR: Cannot find gtk-config.
> 
> I am using Ubuntu 6.06, and I cannot find that package in the
> repositories. Is there a remedy of any kind? What can I do?
> 
> Best,
> Petar M.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From petr.pikal at precheza.cz  Thu Jan 25 15:24:27 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 25 Jan 2007 15:24:27 +0100
Subject: [R] keep track of selected observations over time
In-Reply-To: <20070124172618.52471.qmail@web28005.mail.ukl.yahoo.com>
References: <b3161b5f0701240629h6fb447a2rb0a2b25e80a79b36@mail.gmail.com>
Message-ID: <45B8CBAB.32126.14CAD29@localhost>

Hi


On 24 Jan 2007 at 18:26, Jenny persson wrote:

Date sent:      	Wed, 24 Jan 2007 18:26:17 +0100 (CET)
From:           	Jenny persson <jenny197806 at yahoo.se>
To:             	r-help at stat.math.ethz.ch
Subject:        	Re: [R] keep track of selected observations over time

> 
> Thanks Peter, I forgot that the mailinglist only accept the pdf and
> ps. file.
> 

Well, not sure if I understand what you want (your example is not 
reproducible), but a boxplot beside of making actual plot also 
returns invisibly a structure for plotting, actually a list. So you 
can call

b.str<- boxplot(.....)

and if you go through b.str you can find

$out and $group values, which indicate outliers. Then you can do e.g.

sel <- your.data[,2] %in% b.str$out[ddd$group == 1]

to get logical vector which points are marked as outliers in your 
first column.Then

your.data[sel,]

gives you appropriate rows which you can use e.g. for adding lines

lines(1:4, your.data[sel,][1,-1])

HTH
Petr

>   Here is my problem again:
> 
>   Attached is a graph  of four boxplots from one patient s data at
>   four time points, i.e. each boxplot presents the data at each time
>   point. At day 0 there are 5 extreme values from five peptide
>   sequences (please see the below data). Since the response is
>   changing over time, some of these five extreme values at day 0 may
>   be lower or higher at day 56, 112 and 252. How can I trace the
>   location of each peptide sequence that has extreme value at day 0 on
>   the box plots at day 56, 112 and 252 by color or number coding. For
>   example, the most five responding peptides can be ranked from 5
>   (highest value) to 1 (lowest), so if I do the graph again I would
>   see the five extreme values at day 0 as numbers 5-1 and each of
>   these numbers can be any where on the box plot at day 56, 112 and
>   252 or instead of the rank numbers using the peptide sequence that
>   corresponds to each value. Alternatively, the locations of each
>   peptide sequence at the four time points could be linked by a line.
>   I would like to repeat this procedure for time point 56, 112 and 252
>   as well. That is, at day 56, I want trace where on the box plot at
>   day 0, 112 and 252, each of the four peptide sequences that have
>   highest responses is. Again, these four values/peptides can be
>   presented by different colors, numbers or their peptide sequences
>   that distinguish them from the other most responding peptides at day
>   0, 112 and 252. Can I do the four procedures at the same time, I
>   mean, if at each time point I want to keep track of where the most
>   peptide responses from this time point are, then the total number of
>   peptides at the four time points could be 20. That is for each box
>   plot, there will be 20 id numbers corresponding to each peptide at
>   respectively time point.  The graph can be kind of messy.
> 
>    I have a simple solution of how to see the most responding peptides
>    changing over time, by plotting each peptide s responses at the
>    four time points. But I haven t managed the procedure above. If you
>    have any suggestion how I can do this in R, I would be very
>    thankful.
> 
>   Many thanks 
>   Jenny
> 
> 
> 
>   Part of the data:
> 
>   > pat1[1:20,]
>             peptides       P1_D0      P1_D56      P1_D112     P1_D252
>   1    AAAKKGSEQTLKS -0.06181601 -0.12610877 -0.057898384 -0.02126862
>   2    AAAAPASEDEDDE -0.10972387 -0.17174722 -0.136468783 -0.16262501
>   3    AAAAVSSAKRSLR  0.64156129  1.02630879  0.079891841  0.29757984
>   4    AAAKKGSEQESVK -0.54943062 -0.34311337 -0.338910367 -0.14526498
>   5    AAANLTKIVAYLG -1.72207627 -1.63326368 -0.459839317 -0.63302448
>   6    AACGRISYNDMFE  0.52513671  0.65123495  1.151866644  1.49481479
>   7    AAEAEKAASESLR -0.69366543 -0.47038765 -0.144156174 -0.16042556
>   8    AAEHAQSCRSSAA -0.13373130 -0.09229543 -0.102485597 -0.09782440
>   9  AAERHARLNDSYRLQ -0.19316423 -0.33164239 -0.033764989 -0.11734969
>   10   AAETISAARALPS -0.49632307 -0.53666696 -0.263024663 -0.18231712
>   11 AAEVQRFNRDVDETI -0.80014439 -0.91002202 -0.257201702 -0.12391146
>   12   AAEWTANVTAEFK -0.41544438 -0.10980658 -0.288133150 -0.32022460
>   13   AAGIQWSEEETED -0.04015673  0.08529726  0.002471231  0.07599156
>   14   AAGPALSPVPPVV -0.26795462 -0.36739148 -0.512049278 -0.25449224
>   15   AAGPPPSEGEEST -1.59272674 -1.69729759 -0.843351943 -0.49271773
>   16   AAKIASRQPDSHI -0.40722382 -0.27236225 -0.224539441 -0.32998813
>   17   AAKIQASFRGHMA  2.41234976  2.84435484  0.160852331  0.80197802
>   18   AALDLGGSSDPYV -1.21202038 -1.25109705 -0.259515922 -0.24351352
>   19   AALEPGPSESLTA -2.00256570 -1.57566020 -0.390584034 -0.23682626
>   20 AALLELWELRRQQYE  1.42797600  1.33539104  1.486154861  1.67471189
> 
> 
>   par(las=1)   # all axis labels horizontal
>         boxplot(data.frame(pat1[,c(2:5)]), pars = list(boxwex = 0.4,  
>                                      staplewex = 0.8, outwex = .5), 
>                    boxfill="lightblue",border=c(3:6), names=c("Day 0",
>                    "56 days","112 days", "252 days"),
>                                    col.main="blue",  
>                    main ="Averaged peptide response at 4 different
>                    time points for patient 200001",cex.main=0.9,
>                    font.main=0.9)
> 
> 
> 
> 
> 
> Peter Konings <peter.l.e.konings at gmail.com> skrev:
>   Dear Jenny,
> 
> Your post did not have an attachment. The mailing list software strips
> most attachments away: see the 'technical details of posting' section
> of the posting guide at: http://www.r-project.org/posting-guide.html.
> 
> HTH 
> Peter.
> 
>   On 1/24/07, Jenny persson < jenny197806 at yahoo.se > wrote:   Dear
>   all,
> 
>   Attached is a description of my data, graph and the problem which I
>   need help with. Hope you have time to open the file and help me out.
> 
>   Many thanks,
>   Jenny
> 
> 
> --------------------------------- 
> 
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.
> 
> 
> 
> 
> 
> 
> 
> ---------------------------------
> 
> 

Petr Pikal
petr.pikal at precheza.cz


From ggrothendieck at gmail.com  Thu Jan 25 15:32:00 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 25 Jan 2007 09:32:00 -0500
Subject: [R] dataframe operation
In-Reply-To: <971536df0701241321t1596f556g1178ba9838a190c3@mail.gmail.com>
References: <FE8C160D1505B24497FA7C78D4DADACA0478CE@EA-MAIL.eawag.wroot.emp-eaw.ch>
	<1169669454.4873.18.camel@localhost.localdomain>
	<1169669766.4873.21.camel@localhost.localdomain>
	<1169672208.4873.27.camel@localhost.localdomain>
	<971536df0701241321t1596f556g1178ba9838a190c3@mail.gmail.com>
Message-ID: <971536df0701250632m52d543f2m24caa2abc81d1f3a@mail.gmail.com>

In conversing offline with Indermaur it seems that the elements of
b are supposed to correspond to the rows rather than columns.
In that case we can have the simpler solution:

0 * DF + b

On 1/24/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Here is a slight variation on Marc's idea:
>
> isna <- is.na(DF)
> DF[] <- replace(100 * col(isna), isna, NA)
>
> On 1/24/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> > On Wed, 2007-01-24 at 14:16 -0600, Marc Schwartz wrote:
> > > On Wed, 2007-01-24 at 14:10 -0600, Marc Schwartz wrote:
> > > > On Wed, 2007-01-24 at 20:27 +0100, Indermaur Lukas wrote:
> > > > > hi
> > > > > i have a dataframe "a" which looks like:
> > > > >
> > > > > column1, column2, column3
> > > > > 10,12, 0
> > > > > NA, 0,1
> > > > > 12,NA,50
> > > > >
> > > > > i want to replace all values in column1 to column3 which do not contain "NA" with values of vector "b" (100,200,300).
> > > > >
> > > > > any idea i can do it?
> > > > >
> > > > > i appreciate any hint
> > > > > regards
> > > > > lukas
> > > > >
> > > >
> > > > Here is one possibility:
> > > >
> > > > > sapply(seq(along = colnames(DF)),
> > > >          function(x) ifelse(is.na(DF[[x]]), 100 * x, DF[[x]]))
> > > >      [,1] [,2] [,3]
> > > > [1,]   10   12    0
> > > > [2,]  100    0    1
> > > > [3,]   12  200   50
> > > >
> > > >
> > > > Note that the returned object will be a matrix, so if you need a data
> > > > frame, just coerce the result with as.data.frame().
> > >
> > > OK....that's what I get for pulling the trigger too fast.
> > >
> > > Just reverse the logic in the function:
> > >
> > > > sapply(seq(along = colnames(DF)),
> > >          function(x) ifelse(!is.na(DF[[x]]), 100 * x, DF[[x]]))
> > >      [,1] [,2] [,3]
> > > [1,]  100  200  300
> > > [2,]   NA  200  300
> > > [3,]  100   NA  300
> > >
> > >
> > > I misread the query initially.
> >
> > Here is another possibility, which may be faster depending upon the
> > actual size and dims of your initial data frame.
> >
> > Preallocate a matrix of replacement values:
> >
> > Mat <- matrix(rep(seq(along = colnames(DF)) * 100, each = nrow(DF)),
> >              ncol = ncol(DF))
> >
> > > Mat
> >     [,1] [,2] [,3]
> > [1,]  100  200  300
> > [2,]  100  200  300
> > [3,]  100  200  300
> >
> >
> > Now do the replacement:
> >
> > > ifelse(!is.na(DF), Mat, NA)
> >  column1 column2 column3
> > 1     100     200     300
> > 2      NA     200     300
> > 3     100      NA     300
> >
> >
> > In doing some testing, the above may be about 10 times faster than using
> > sapply() in my first solution, again depending upon the structure of
> > your DF.
> >
> > HTH,
> >
> > Marc
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From P.Dalgaard at biostat.ku.dk  Thu Jan 25 15:33:38 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 25 Jan 2007 15:33:38 +0100
Subject: [R] Unable to install gtkDevice ...
In-Reply-To: <1169734521.21293.7.camel@localhost>
References: <1169734521.21293.7.camel@localhost>
Message-ID: <45B8BFC2.1000809@biostat.ku.dk>

Petar Milin wrote:
> Hello!
> I am trying to install gtkDevice package, but without success. It is
> complaining about:
> * Installing *source* package 'gtkDevice' ...
> checking for gtk-config... no
> checking for gtk12-config... no
> ERROR: Cannot find gtk-config.
>
> I am using Ubuntu 6.06, and I cannot find that package in the
> repositories. Is there a remedy of any kind? What can I do?
>   

I believe that those are not packages, rather shell-scripts in the same
manner as the tclConfig.sh which gets installed as part of the tcl-devel
package. Hence, look for packages named "gtk-devel" or so.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From csardi at rmki.kfki.hu  Thu Jan 25 15:37:09 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Thu, 25 Jan 2007 15:37:09 +0100
Subject: [R] Unable to install gtkDevice ...
In-Reply-To: <20070125142229.GB6937@localdomain>
References: <1169734521.21293.7.camel@localhost>
	<20070125142229.GB6937@localdomain>
Message-ID: <20070125143708.GC6937@localdomain>

Another possibility is to install the r-cran-gtkdevice Ubuntu package,
if 6.06 has it.

Gabor

On Thu, Jan 25, 2007 at 03:22:29PM +0100, Gabor Csardi wrote:
> Petar,
> 
> gtk-config is likely to be in the libgtk1.2-dev Ubuntu package.
> 
> Gabor
> 
> On Thu, Jan 25, 2007 at 03:15:21PM +0100, Petar Milin wrote:
> > Hello!
> > I am trying to install gtkDevice package, but without success. It is
> > complaining about:
> > * Installing *source* package 'gtkDevice' ...
> > checking for gtk-config... no
> > checking for gtk12-config... no
> > ERROR: Cannot find gtk-config.
> > 
> > I am using Ubuntu 6.06, and I cannot find that package in the
> > repositories. Is there a remedy of any kind? What can I do?
> > 
> > Best,
> > Petar M.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From d.doktor03 at imperial.ac.uk  Thu Jan 25 15:53:03 2007
From: d.doktor03 at imperial.ac.uk (Doktor, Daniel)
Date: Thu, 25 Jan 2007 14:53:03 -0000
Subject: [R] distribution overlap - how to quantify?
Message-ID: <B32CD30A6C6DB6419B2FB76DB496740AA44BCD@icex6.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070125/6d625676/attachment.pl 

From marc_schwartz at comcast.net  Thu Jan 25 15:57:24 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 25 Jan 2007 08:57:24 -0600
Subject: [R] filling the area
In-Reply-To: <45B8BB28.4000208@ufba.br>
References: <45B8AE33.8060005@ufba.br>
	<1169732158.4882.8.camel@localhost.localdomain>
	<45B8BB28.4000208@ufba.br>
Message-ID: <1169737044.4882.21.camel@localhost.localdomain>

Yep.

Keep in mind that you need to define the complete boundaries of the
region to be colored. So you need to add the coordinates of the lower
left and lower right hand points, which are [min(x), min(y)] and
[max(x), min(y)].

HTH,

Marc

On Thu, 2007-01-25 at 11:14 -0300, Mauricio Cardeal wrote:
> Thank you all. One more question about the syntax below: min(y) twice, is it?
> 
> polygon(c(min(x), x, max(x)), c(min(y), y, min(y)),  col = "blue")
> 
> polygon( c(min(x), x, max(x)), c( min(y), y, min(y)), density=100 )
> 
> Best whises,
> Mauricio
> 
> 
> 
> Marc Schwartz escreveu:
> > On Thu, 2007-01-25 at 10:18 -0300, Mauricio Cardeal wrote:
> >   
> >> Please, how to fill the area under the curve?
> >>
> >> x <- c(1:10)
> >> y <- c(rnorm(10))
> >> plot(x,y)
> >> lines(x,y)
> >>
> >> Thanks,
> >> Mauricio Cardeal
> >>     
> >
> > See ?polygon
> >
> > x <- 1:10
> > y <- rnorm(10)
> >
> > plot(x,y, type = "o")
> >
> > polygon(c(min(x), x, max(x)), c(min(y), y, min(y)),  col = "blue")
> >
> >
> > HTH,
> >
> > Marc Schwartz
> >


From pmilin at gmail.com  Thu Jan 25 16:11:21 2007
From: pmilin at gmail.com (Petar Milin)
Date: Thu, 25 Jan 2007 16:11:21 +0100
Subject: [R] Unable to install gtkDevice ...
In-Reply-To: <20070125142229.GB6937@localdomain>
References: <1169734521.21293.7.camel@localhost>
	<20070125142229.GB6937@localdomain>
Message-ID: <1169737881.18665.2.camel@localhost>


Thanks Gabor,

I managed to install libgtk1.2-dev, and then gtkDevice, flawlessly.
However, I cannot run RGtk. That should be a command, isn't it?

Best,
Petar M

On Thu, 2007-01-25 at 15:22 +0100, Gabor Csardi wrote:
> Petar,
> 
> gtk-config is likely to be in the libgtk1.2-dev Ubuntu package.
> 
> Gabor
> 
> On Thu, Jan 25, 2007 at 03:15:21PM +0100, Petar Milin wrote:
> > Hello!
> > I am trying to install gtkDevice package, but without success. It is
> > complaining about:
> > * Installing *source* package 'gtkDevice' ...
> > checking for gtk-config... no
> > checking for gtk12-config... no
> > ERROR: Cannot find gtk-config.
> > 
> > I am using Ubuntu 6.06, and I cannot find that package in the
> > repositories. Is there a remedy of any kind? What can I do?
> > 
> > Best,
> > Petar M.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>


From roberto.perdisci at gmail.com  Thu Jan 25 16:13:46 2007
From: roberto.perdisci at gmail.com (Roberto Perdisci)
Date: Thu, 25 Jan 2007 10:13:46 -0500
Subject: [R] Fit model to data and use model for data generation
In-Reply-To: <cf94d0090701250708p4c0c003fqec8be0c093937f9c@mail.gmail.com>
References: <001501c73fd5$8fb9f060$336f12ac@matrix.com>
	<67996c0b2c522fe664820cc9829af2b8@charter.net>
	<Pine.LNX.4.64.0701250816310.5353@gannet.stats.ox.ac.uk>
	<cf94d0090701250708p4c0c003fqec8be0c093937f9c@mail.gmail.com>
Message-ID: <cf94d0090701250713n7ae85b54paa649ae817654b86@mail.gmail.com>

On 1/25/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
 > That gives a discrete distribution, which may well matter for small
> samples.
>
> Since density() is returning an equal-weighted mixture of (by default)
> normal distributions, all you need to do is
>
> x.new <- rnorm(n, sample(x, size = n, replace=TRUE), bw)

Prof. Ripley,
  I didn't understand why you used
sample(x, size = n, replace=TRUE)
I though the mixture should be computed using all the points in x as
means, like in
x.new <- rnorm(n, x, bw)

Could you explain why you propose
x.new <- rnorm(n, sample(x, size = n, replace=TRUE), bw)
instead?

Could you also briefly say in what sense kde is biased?

thank you very much,
best regards,
Roberto

> where bw is the bandwidth used by density (d$bw in this example).
> (This is known as a 'smoothed bootstrap' in some circles.)
>
>
> > ### Create a bimodal distribution
> > x <- c(rnorm(25, -2, 1), rnorm(50, 3, 2))
> > d <- density(x, n = 1000)
> > plot(d)
> >
> > ### Sample from the distribution and show the two
> > ### distributions are the same
> > x.new <- sample(d$x, size = 100000, # large n for proof of concept
> >                 replace = TRUE, prob = d$y/sum(d$y))
> > dx.new <- density(x.new)
> > lines(dx.new$x, dx.new$y, col = "blue")
>
> BTW, lines(density(x.news), col = "blue") works here, and you do need to
> remember that a kde is biased.  But my solution matches better than yours.
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wwwhsd at gmail.com  Thu Jan 25 16:20:45 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Thu, 25 Jan 2007 13:20:45 -0200
Subject: [R] filling the area
In-Reply-To: <45B8AE33.8060005@ufba.br>
References: <45B8AE33.8060005@ufba.br>
Message-ID: <da79af330701250720v298d7050mf28aab3a9bec118c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070125/6f40ac76/attachment.pl 

From jon.clayden at gmail.com  Thu Jan 25 16:22:14 2007
From: jon.clayden at gmail.com (Jon Clayden)
Date: Thu, 25 Jan 2007 15:22:14 +0000
Subject: [R] rbind-ing with empty data frame produces error
Message-ID: <68e086180701250722y424b12faub6bc7aa8f7464ef4@mail.gmail.com>

Hi all,

I'm having some trouble with rbind - this may be a bug or it may be my
misunderstanding. If I do

fileName <- paste(tempdir(),"test.txt",sep="/")
file.create(fileName)
x <- read.table(fileName, col.names=c("one","two","three"))

I get a data frame with no rows, as documented. If I then try to rbind
this with another data frame with the same column names, I get an
error:

y <- data.frame(one=1,two=2,three=3)
rbind(x,y)

"Error in `*tmp*`[[jj]] : subscript out of bounds"

On the other hand, doing "rbind(as.matrix(x),as.matrix(y))" works as
expected. If this is, in fact, intended behaviour, could anyone
suggest another way of achieving what I want, please? I'm trying to
append to a data frame stored in a text file every so often, and of
course I need to create it, as above, the first time around...

My system is R-2.4.1/i686-pc-linux-gnu. Thanks in advance for your advice.

Regards,
Jon


From petr.pikal at precheza.cz  Thu Jan 25 16:24:23 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 25 Jan 2007 16:24:23 +0100
Subject: [R] change the attribute of a column
In-Reply-To: <45B883CF.2060804@slf.ch>
Message-ID: <45B8D9B7.5317.1838B6D@localhost>

Hi

did you look to ?unlist.

HTH
Petr

BTW, I would recommend to put unlist to See also section of list.



On 25 Jan 2007 at 11:17, Thomas Preuth wrote:

Date sent:      	Thu, 25 Jan 2007 11:17:51 +0100
From:           	Thomas Preuth <preuth at slf.ch>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] change the attribute of a column

> Hello,
> 
> I have a column which is filled by numbers, but is attributed as list.
> How can I change the attributes into numeric to use statistic
> operations?
> 
> Greetings, Thomas
> 
> -- 
> Thomas Preuth
> Swiss federal institutes for forest, snow and landscape research
> WSL/SLF Fluuelastrasse 11 CH- 7260 Davos Switzerland
> 
> 0041-81-4170-359
> preuth at slf.ch
> www.wsl.ch
> www.slf.ch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From george at galis.org  Thu Jan 25 16:51:01 2007
From: george at galis.org (George Georgalis)
Date: Thu, 25 Jan 2007 10:51:01 -0500
Subject: [R] r tidy
Message-ID: <20070125155101.GA10432@run.galis.org>

Is there an r-tidy program? something that works similar to perl
tidy? http://perltidy.sourceforge.net/ which takes program code
and reformats white space with standard indentations and spacing?
I did find a ruby based rtidy, but that is for html formatting.

// George


-- 
George Georgalis, systems architect, administrator <IXOYE><


From ablukacz at utm.utoronto.ca  Thu Jan 25 17:04:04 2007
From: ablukacz at utm.utoronto.ca (ablukacz)
Date: Thu, 25 Jan 2007 11:04:04 -0500 (EST)
Subject: [R]  subscripting issues
In-Reply-To: <68e086180701250722y424b12faub6bc7aa8f7464ef4@mail.gmail.com>
References: <68e086180701250722y424b12faub6bc7aa8f7464ef4@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701251050120.18308@river.utm.utoronto.ca>

Dear All,

I have a very simple problem.
I have a matrix called Predictors with headers X0...X24
>dim(Predictors)
[1] 79 25

  Predictors
           X0        X1        X2        X3        X4        X5        X6
1  13.741200 12.148036 11.909435 11.671669 11.238207 10.864697 10.566351
2  10.240200 11.883900 11.529400 11.515050 11.411640 11.105800 10.813457



I would like to take each row and all the corresponding columns with out 
the header.
I tried  Predictors[1,1:25] but that retains the headers.


Any suggestions.

Thanks,

Agnes


From wwwhsd at gmail.com  Thu Jan 25 17:15:07 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Thu, 25 Jan 2007 14:15:07 -0200
Subject: [R] r tidy
In-Reply-To: <20070125155101.GA10432@run.galis.org>
References: <20070125155101.GA10432@run.galis.org>
Message-ID: <da79af330701250815i386fa9bai36e31a60edcd790d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070125/c7421393/attachment.pl 

From marc_schwartz at comcast.net  Thu Jan 25 17:15:35 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 25 Jan 2007 10:15:35 -0600
Subject: [R] rbind-ing with empty data frame produces error
In-Reply-To: <68e086180701250722y424b12faub6bc7aa8f7464ef4@mail.gmail.com>
References: <68e086180701250722y424b12faub6bc7aa8f7464ef4@mail.gmail.com>
Message-ID: <1169741735.4826.20.camel@localhost.localdomain>

On Thu, 2007-01-25 at 15:22 +0000, Jon Clayden wrote:
> Hi all,
> 
> I'm having some trouble with rbind - this may be a bug or it may be my
> misunderstanding. If I do
> 
> fileName <- paste(tempdir(),"test.txt",sep="/")
> file.create(fileName)
> x <- read.table(fileName, col.names=c("one","two","three"))
> 
> I get a data frame with no rows, as documented. If I then try to rbind
> this with another data frame with the same column names, I get an
> error:
> 
> y <- data.frame(one=1,two=2,three=3)
> rbind(x,y)
> 
> "Error in `*tmp*`[[jj]] : subscript out of bounds"
> 
> On the other hand, doing "rbind(as.matrix(x),as.matrix(y))" works as
> expected. If this is, in fact, intended behaviour, could anyone
> suggest another way of achieving what I want, please? I'm trying to
> append to a data frame stored in a text file every so often, and of
> course I need to create it, as above, the first time around...
> 
> My system is R-2.4.1/i686-pc-linux-gnu. Thanks in advance for your advice.
> 
> Regards,
> Jon

The problem is that the classes of the columns in 'x' are:

> sapply(x, class)
   one    two  three 
"NULL" "NULL" "NULL" 


You might get more insight by reversing the two arguments:

> rbind(y, x)
Error in value[[jj]][ri] <- if (is.factor(xij)) as.vector(xij) else
xij : 
        incompatible types (from NULL to double) in subassignment type
fix


If you review the Details in ?rbind, you will note that there is a class
checking component to the rbind'ing process for data frames.  The lack
of a class type for the columns in 'x' is the source of the problem.

This is not part of rbind'ing matrices, since they can only be of a
single data type.

An alternative would be to use ifelse():

  NewDF <- ifelse(nrow(x) == 0, y, rbind(x, y))

HTH,

Marc Schwartz


From sr5 at sanger.ac.uk  Thu Jan 25 17:16:32 2007
From: sr5 at sanger.ac.uk (Shola Richards)
Date: Thu, 25 Jan 2007 16:16:32 +0000
Subject: [R] Help running RSPerl in Mod Perl
Message-ID: <45B8D7E0.3080100@sanger.ac.uk>

I am trying to get RSPerl to run within a web tool - I have managed to 
get it to work on my cgi site, however, it seems to stop everything when 
I try to run the same code in mod perl.  The cgi and mod-perl versions 
both run on the same web blades therefore utilising the same environments.
However, if I consistently refresh the page in mod-perl, the following 
error appears:   "Fatal error: R home directory is not defined" .  I 
have tried to set the R HOME environment within the perl module itself 
but this didn't fix the issue.

Does anyone have any ideas why this would occur and if there is any way 
to resolve this?

Many Thanks in advance,
Shola


From ripley at stats.ox.ac.uk  Thu Jan 25 17:29:06 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Jan 2007 16:29:06 +0000 (GMT)
Subject: [R] r tidy
In-Reply-To: <20070125155101.GA10432@run.galis.org>
References: <20070125155101.GA10432@run.galis.org>
Message-ID: <Pine.LNX.4.64.0701251627100.25714@gannet.stats.ox.ac.uk>

It is called R.

See the `Writing R Extensions' manual, section 3.1 (the obvious manual, 
surely?).


On Thu, 25 Jan 2007, George Georgalis wrote:

> Is there an r-tidy program? something that works similar to perl
> tidy? http://perltidy.sourceforge.net/ which takes program code
> and reformats white space with standard indentations and spacing?
> I did find a ruby based rtidy, but that is for html formatting.
>
> // George

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From marc_schwartz at comcast.net  Thu Jan 25 17:44:18 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 25 Jan 2007 10:44:18 -0600
Subject: [R] subscripting issues
In-Reply-To: <Pine.LNX.4.64.0701251050120.18308@river.utm.utoronto.ca>
References: <68e086180701250722y424b12faub6bc7aa8f7464ef4@mail.gmail.com>
	<Pine.LNX.4.64.0701251050120.18308@river.utm.utoronto.ca>
Message-ID: <1169743458.4826.34.camel@localhost.localdomain>

On Thu, 2007-01-25 at 11:04 -0500, ablukacz wrote:
> Dear All,
> 
> I have a very simple problem.
> I have a matrix called Predictors with headers X0...X24
> >dim(Predictors)
> [1] 79 25
> 
>   Predictors
>            X0        X1        X2        X3        X4        X5        X6
> 1  13.741200 12.148036 11.909435 11.671669 11.238207 10.864697 10.566351
> 2  10.240200 11.883900 11.529400 11.515050 11.411640 11.105800 10.813457
> 
> 
> 
> I would like to take each row and all the corresponding columns with out 
> the header.
> I tried  Predictors[1,1:25] but that retains the headers.
> 
> 
> Any suggestions.
> 
> Thanks,
> 
> Agnes

The column names of X0, etc. suggest that Predictors is a data frame
created by the use of one of the read.table() family of functions, not a
matrix. An important difference depending upon what you intend to do
with the data.

However, in both cases, you can generally manipulate the data as if the
headers were not present. In other words, most R functions will take the
numeric values and act in a predictable (sorry for the pun) fashion. 

Think of the output above as if the column names X0 through X6 were A
through G in a spreadsheet. They are simply labels, not part of the data
itself. The same goes for the row names of 1 and 2.

HTH,

Marc Schwartz


From a.evangelist at gmail.com  Thu Jan 25 16:56:01 2007
From: a.evangelist at gmail.com (andrea evangelista)
Date: Thu, 25 Jan 2007 16:56:01 +0100
Subject: [R] summary of the effects after logistic regression model
Message-ID: <880369780701250756l52e1427cofda70cc2eede442b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070125/2cd24e6f/attachment.pl 

From pbulian at cro.it  Thu Jan 25 17:21:12 2007
From: pbulian at cro.it (Pietro Bulian)
Date: Thu, 25 Jan 2007 17:21:12 +0100
Subject: [R] cox.zph vs  log-log survival plot
Message-ID: <000601c7409c$d50d8f60$2bb411ac@cro.sanita.fvg.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070125/9543b236/attachment.pl 

From edd at debian.org  Thu Jan 25 18:15:41 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 25 Jan 2007 11:15:41 -0600
Subject: [R] Unable to install gtkDevice ...
In-Reply-To: <20070125143708.GC6937@localdomain>
References: <1169734521.21293.7.camel@localhost>
	<20070125142229.GB6937@localdomain>
	<20070125143708.GC6937@localdomain>
Message-ID: <20070125171540.GA24726@eddelbuettel.com>

On Thu, Jan 25, 2007 at 03:37:09PM +0100, Gabor Csardi wrote:
> Another possibility is to install the r-cran-gtkdevice Ubuntu package,
> if 6.06 has it.

No, gtkDevice is deprecated as is the old R GUI for Gtk v1. That's why
I asked for it to be removed from Debian. 

You are much better off with the newer, better, fancier, ... RGtk2 and
its cousin cairoDevice. I maintain both as a Debian package, and
Ubuntu appears to have versions 2.8.5 + 1.0.0 (a little behind Debian)
in the universe repo too. So apt-get install away, or rebuild from
CRAN sources.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From petr.pikal at precheza.cz  Thu Jan 25 18:53:49 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 25 Jan 2007 18:53:49 +0100
Subject: [R] subscripting issues
In-Reply-To: <1169743458.4826.34.camel@localhost.localdomain>
References: <Pine.LNX.4.64.0701251050120.18308@river.utm.utoronto.ca>
Message-ID: <45B8FCBD.24951.20C5B3A@localhost>

Hi

On 25 Jan 2007 at 10:44, Marc Schwartz wrote:

From:           	Marc Schwartz <marc_schwartz at comcast.net>
To:             	ablukacz <ablukacz at utm.utoronto.ca>
Date sent:      	Thu, 25 Jan 2007 10:44:18 -0600
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] subscripting issues
Send reply to:  	marc_schwartz at comcast.net
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> On Thu, 2007-01-25 at 11:04 -0500, ablukacz wrote:
> > Dear All,
> > 
> > I have a very simple problem.
> > I have a matrix called Predictors with headers X0...X24
> > >dim(Predictors)
> > [1] 79 25
> > 
> >   Predictors
> >            X0        X1        X2        X3        X4        X5     
> >              X6
> > 1  13.741200 12.148036 11.909435 11.671669 11.238207 10.864697
> > 10.566351 2  10.240200 11.883900 11.529400 11.515050 11.411640
> > 11.105800 10.813457
> > 
> > 
> > 
> > I would like to take each row and all the corresponding columns with
> > out the header. I tried  Predictors[1,1:25] but that retains the
> > headers.
> > 
> > 
> > Any suggestions.
> > 
> > Thanks,
> > 
> > Agnes
> 
> The column names of X0, etc. suggest that Predictors is a data frame
> created by the use of one of the read.table() family of functions, not

although you are probably right, it could be matrix as well

> is.data.frame(zeta)
[1] TRUE
> is.matrix(zeta)
[1] FALSE
>
> zetam<-as.matrix(zeta)
> zetam
  tepl tio2 al2o3  iep
1   60    1   3.5 5.65
2   60    1   2.0 5.00
3   60    0   3.5 5.30
4   60    0   2.0 4.65
5   40    1   3.5 5.20
6   40    1   2.0 4.85
7   40    0   3.5 5.70
8   40    0   2.0 5.25
> is.matrix(zetam)
[1] TRUE
> is.data.frame(zetam)
[1] FALSE

Cheers
Petr

> a matrix. An important difference depending upon what you intend to do
> with the data.
> 
> However, in both cases, you can generally manipulate the data as if
> the headers were not present. In other words, most R functions will
> take the numeric values and act in a predictable (sorry for the pun)
> fashion. 
> 
> Think of the output above as if the column names X0 through X6 were A
> through G in a spreadsheet. They are simply labels, not part of the
> data itself. The same goes for the row names of 1 and 2.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From lalithaviswanath at yahoo.com  Thu Jan 25 18:53:58 2007
From: lalithaviswanath at yahoo.com (lalitha viswanath)
Date: Thu, 25 Jan 2007 09:53:58 -0800 (PST)
Subject: [R] unique/subset problem
Message-ID: <908755.72955.qm@web43127.mail.sp1.yahoo.com>

Hi
I am new to R programming and am using subset to
extract part of a data as follows

names(dataset) =
c("genome1","genome2","dist","score");
prunedrelatives <- subset(dataset, score < -5);

However when I use unique to find the number of unique
genomes now present in prunedrelatives I get results
identical to calling unique(dataset$genome1) although
subset has eliminated many genomes and records.

I would greatly appreciate your input about using
"unique" correctly  in this regard.

Thanks
Lalitha


 
____________________________________________________________________________________
TV dinner still cooling? 
Check out "Tonight's Picks" on Yahoo! TV.


From john_d_mchenry at yahoo.com  Thu Jan 25 19:02:47 2007
From: john_d_mchenry at yahoo.com (John McHenry)
Date: Thu, 25 Jan 2007 10:02:47 -0800 (PST)
Subject: [R] Days of the week?
In-Reply-To: <971536df0701241812q6cf14ac7k643f67115c35ecbd@mail.gmail.com>
Message-ID: <631588.1390.qm@web35405.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070125/869307e0/attachment.pl 

From george at galis.org  Thu Jan 25 19:20:18 2007
From: george at galis.org (George Georgalis)
Date: Thu, 25 Jan 2007 13:20:18 -0500
Subject: [R] r tidy
In-Reply-To: <da79af330701250815i386fa9bai36e31a60edcd790d@mail.gmail.com>
References: <20070125155101.GA10432@run.galis.org>
	<da79af330701250815i386fa9bai36e31a60edcd790d@mail.gmail.com>
Message-ID: <20070125182018.GA21722@run.galis.org>

On Thu, Jan 25, 2007 at 02:15:07PM -0200, Henrique Dallazuanna wrote:
>See:
>https://sourceforge.net/projects/tinn-r
>
>Perhaps help you
>

thanks. but my site is all unix.

The users use emacs+ess, I however use vim. Does anyone recommend a
vim macro to function like ess?

// George


-- 
George Georgalis, systems architect, administrator <IXOYE><


From helprhelp at gmail.com  Thu Jan 25 19:30:26 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Thu, 25 Jan 2007 13:30:26 -0500
Subject: [R] unique/subset problem
In-Reply-To: <908755.72955.qm@web43127.mail.sp1.yahoo.com>
References: <908755.72955.qm@web43127.mail.sp1.yahoo.com>
Message-ID: <cdf817830701251030y7eb7b14fqbd71d191232fee88@mail.gmail.com>

Hi,

Even you removed "many" genomes1 by setting score< -5; it is not
necessary saying you changed the uniqueness.

To check this, you can do like
p0 <- unique(dataset[dataset$score< -5, "genome1"]) # same as subset
p1 <- unique(dataset[dataset$score>= -5, "genome1"])

setdiff(p1, p0)

if the output above has NULL, then it means even though you remove
many genomes1, but it does not help changing the uniqueness.

HTH,

weiwei



On 1/25/07, lalitha viswanath <lalithaviswanath at yahoo.com> wrote:
> Hi
> I am new to R programming and am using subset to
> extract part of a data as follows
>
> names(dataset) =
> c("genome1","genome2","dist","score");
> prunedrelatives <- subset(dataset, score < -5);
>
> However when I use unique to find the number of unique
> genomes now present in prunedrelatives I get results
> identical to calling unique(dataset$genome1) although
> subset has eliminated many genomes and records.
>
> I would greatly appreciate your input about using
> "unique" correctly  in this regard.
>
> Thanks
> Lalitha
>
>
>
> ____________________________________________________________________________________
> TV dinner still cooling?
> Check out "Tonight's Picks" on Yahoo! TV.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From dchandra at ics.uci.edu  Thu Jan 25 20:22:30 2007
From: dchandra at ics.uci.edu (Deepak Chandra)
Date: Thu, 25 Jan 2007 11:22:30 -0800
Subject: [R] writing R shell scripts?
In-Reply-To: <Pine.LNX.4.64.0701240516300.10866@gannet.stats.ox.ac.uk>
References: <7387cdce0701231931j799a3756h91ed80699bd42929@mail.gmail.com>
	<Pine.LNX.4.64.0701240516300.10866@gannet.stats.ox.ac.uk>
Message-ID: <7387cdce0701251122x110874b4ldc7f84fd75c35798@mail.gmail.com>

> > Hi All,
> >
> > Another newbie question. I want to write an R script that takes
> > argument from command line and runs and produces output to stdin.
>
> You will find that difficult: did you mean stdout?

My mistake -- I meant stdout :).

>
> > For example if there is file foo.R with following in it.
> >
> > args = commandArgs()
> > print(args)
> >
> > then, when I run it like
> > $ R foo.R hello
> > it should print 'hello' on the terminal.
> >
> > I searched the mainling list and found a very old post that said said
>

> Please give an exact reference so we know what you are referring to.
> This is not what is usually meant by 'R shell scripts' (it is an R script,
> not a shell script).
>
> > that this feature will be implemented soon. I am expecting this has
> > already  been implemented by now.
>
> That is not what commandArgs() is documented to do so you will need to do
> something slightly different.  But in the development version of R you can
> come very close:
>
> gannet% cat foo.R
> args <- commandArgs(TRUE)
> print(args)
> gannet% ~/R/R-devel/bin/Rscript foo.R hello
> [1] "hello"
> gannet%
>
> and in any recent version of R
>
> gannet% cat foo2.R
> args <- commandArgs()
> m <- match("--args", args)
> print(args[-(1:m)])
> gannet% R --slave --args hello < foo2.R
> [1] "hello"
>
> If you have a platform and build of R for which it works, there is also
> littler (http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/LittleR),
> but beware that are restrictions not stated on that page (such as the need
> for R built as a shared library, which is not the default).

My apologies for being sketchy in my example: I was hoping to
elucidate my problem without too many details.
I was looking #! capabilites for running my scripts with different
arguments from my shell.
Your response was helpful, and one of the three alternatives that you
pointed to should solve my problem.

Thanks,
Deepak


From george at galis.org  Thu Jan 25 20:35:21 2007
From: george at galis.org (George Georgalis)
Date: Thu, 25 Jan 2007 14:35:21 -0500
Subject: [R] r tidy
In-Reply-To: <Pine.LNX.4.64.0701251627100.25714@gannet.stats.ox.ac.uk>
References: <20070125155101.GA10432@run.galis.org>
	<Pine.LNX.4.64.0701251627100.25714@gannet.stats.ox.ac.uk>
Message-ID: <20070125193521.GB21722@run.galis.org>

>On Thu, 25 Jan 2007, George Georgalis wrote:
>
>>Is there an r-tidy program? something that works similar to perl
>>tidy? http://perltidy.sourceforge.net/ which takes program code
>>and reformats white space with standard indentations and spacing?
>>I did find a ruby based rtidy, but that is for html formatting.

On Thu, Jan 25, 2007 at 04:29:06PM +0000, Prof Brian Ripley wrote:
>It is called R.
>
>See the `Writing R Extensions' manual, section 3.1 (the obvious manual, 
>surely?).


yes, well actually that was one of the first items I found in
my search. but on first pass I was confused, on second pass I
understood (sorta incorrectly) that it was about preserving source
when writing packages -- then I posted my question.

looking at it again I see the technique could be used to tidy r
scripts, but surly the first, the second paragraph or the rest of
the section was not written to aid programmers for script tidying?
"R treats function code loaded from packages and code entered by
users differently."...

Maybe it could go something like this "Your R scripts can be
sourced and dumped into a standard tidy format." (insert brief
explanation of keep.source, source and dump; which I think I grasp
but wouldn't try to explain)

Here's a script to tidy a file as argument $1

#!/bin/sh
R --vanilla <<EOF
     options(keep.source = FALSE)
     source("$1")
     dump(ls(all = TRUE), file = "${1}.tidy.R")
EOF


yes, works great, but as one with nearly zero r experience,
hoping to apply my other language experience to some R spaghetti,
http://cran.r-project.org/doc/manuals/R-exts.html#Tidying-R-code
did frighten me off, the first time.

// George


-- 
George Georgalis, systems architect, administrator <IXOYE><


From ripley at stats.ox.ac.uk  Thu Jan 25 20:43:58 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Jan 2007 19:43:58 +0000 (GMT)
Subject: [R] writing R shell scripts?
In-Reply-To: <7387cdce0701251122x110874b4ldc7f84fd75c35798@mail.gmail.com>
References: <7387cdce0701231931j799a3756h91ed80699bd42929@mail.gmail.com> 
	<Pine.LNX.4.64.0701240516300.10866@gannet.stats.ox.ac.uk>
	<7387cdce0701251122x110874b4ldc7f84fd75c35798@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701251943110.3033@gannet.stats.ox.ac.uk>

On Thu, 25 Jan 2007, Deepak Chandra wrote:

>> > Hi All,
>> >
>> > Another newbie question. I want to write an R script that takes
>> > argument from command line and runs and produces output to stdin.
>> 
>> You will find that difficult: did you mean stdout?
>
> My mistake -- I meant stdout :).
>
>> 
>> > For example if there is file foo.R with following in it.
>> >
>> > args = commandArgs()
>> > print(args)
>> >
>> > then, when I run it like
>> > $ R foo.R hello
>> > it should print 'hello' on the terminal.
>> >
>> > I searched the mainling list and found a very old post that said said
>> 
>
>> Please give an exact reference so we know what you are referring to.
>> This is not what is usually meant by 'R shell scripts' (it is an R script,
>> not a shell script).
>> 
>> > that this feature will be implemented soon. I am expecting this has
>> > already  been implemented by now.
>> 
>> That is not what commandArgs() is documented to do so you will need to do
>> something slightly different.  But in the development version of R you can
>> come very close:
>> 
>> gannet% cat foo.R
>> args <- commandArgs(TRUE)
>> print(args)
>> gannet% ~/R/R-devel/bin/Rscript foo.R hello
>> [1] "hello"
>> gannet%
>> 
>> and in any recent version of R
>> 
>> gannet% cat foo2.R
>> args <- commandArgs()
>> m <- match("--args", args)
>> print(args[-(1:m)])
>> gannet% R --slave --args hello < foo2.R
>> [1] "hello"
>> 
>> If you have a platform and build of R for which it works, there is also
>> littler (http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/LittleR),
>> but beware that are restrictions not stated on that page (such as the need
>> for R built as a shared library, which is not the default).
>
> My apologies for being sketchy in my example: I was hoping to
> elucidate my problem without too many details.
> I was looking #! capabilites for running my scripts with different
> arguments from my shell.

Rscript does that, and if you are lucky, littler will.

> Your response was helpful, and one of the three alternatives that you
> pointed to should solve my problem.
>
> Thanks,
> Deepak
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From george at galis.org  Thu Jan 25 20:55:05 2007
From: george at galis.org (George Georgalis)
Date: Thu, 25 Jan 2007 14:55:05 -0500
Subject: [R] r tidy
In-Reply-To: <20070125193521.GB21722@run.galis.org>
References: <20070125155101.GA10432@run.galis.org>
	<Pine.LNX.4.64.0701251627100.25714@gannet.stats.ox.ac.uk>
	<20070125193521.GB21722@run.galis.org>
Message-ID: <20070125195505.GC21722@run.galis.org>

On Thu, Jan 25, 2007 at 02:35:21PM -0500, George Georgalis wrote:
>
>Here's a script to tidy a file as argument $1
>
>#!/bin/sh
>R --vanilla <<EOF
>     options(keep.source = FALSE)
>     source("$1")
>     dump(ls(all = TRUE), file = "${1}.tidy.R")
>EOF
>
>
>yes, works great, but as one with nearly zero r experience,
>hoping to apply my other language experience to some R spaghetti,
>http://cran.r-project.org/doc/manuals/R-exts.html#Tidying-R-code
>did frighten me off, the first time.

Is there a way to tidy code that is broken? instead of a "best
effort" on broken code, I don't get a dump.

also, what is going on when I try to tidy this

     options(keep.source = FALSE)
     source("/dev/null")
     dump(ls(all = TRUE), file = "/dev/null")

with the code above? ie no output.

// George


-- 
George Georgalis, systems architect, administrator <IXOYE><


From lauri.nikkinen at iki.fi  Thu Jan 25 21:23:15 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Thu, 25 Jan 2007 22:23:15 +0200
Subject: [R] barplot x-axis problem
Message-ID: <ba8c09910701251223ofe38f08me933426c4c429789@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070125/0e171c74/attachment.pl 

From edd at debian.org  Thu Jan 25 21:35:13 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 25 Jan 2007 14:35:13 -0600
Subject: [R] writing R shell scripts?
In-Reply-To: <Pine.LNX.4.64.0701251943110.3033@gannet.stats.ox.ac.uk>
References: <7387cdce0701231931j799a3756h91ed80699bd42929@mail.gmail.com>
	<Pine.LNX.4.64.0701240516300.10866@gannet.stats.ox.ac.uk>
	<7387cdce0701251122x110874b4ldc7f84fd75c35798@mail.gmail.com>
	<Pine.LNX.4.64.0701251943110.3033@gannet.stats.ox.ac.uk>
Message-ID: <20070125203513.GA26099@eddelbuettel.com>

On Thu, Jan 25, 2007 at 07:43:58PM +0000, Prof Brian Ripley wrote:
> On Thu, 25 Jan 2007, Deepak Chandra wrote:
> > My apologies for being sketchy in my example: I was hoping to
> > elucidate my problem without too many details.
> > I was looking #! capabilites for running my scripts with different
> > arguments from my shell.
> 
> Rscript does that, and if you are lucky, littler will.

Littler comes with a few examples that show you what is possible.
What is lacking is some sort of getopt() equivalent -- you still need
evaluate yuor vector of arguments yourself.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From vinodkgul at yahoo.com  Thu Jan 25 21:42:19 2007
From: vinodkgul at yahoo.com (vinod gullu)
Date: Thu, 25 Jan 2007 12:42:19 -0800 (PST)
Subject: [R] aov and lm
In-Reply-To: <mailman.13.1169722804.11374.r-help@stat.math.ethz.ch>
Message-ID: <220138.11488.qm@web53803.mail.yahoo.com>

    I am trying to fit a model with 4 parameteres on
one response. I have 32 points and the model contains
linear, quadratic and interaction terms. Now when i
apply the Step AIC method most of my square terms drop
out (which i think is not correct if physics is
concerned) So is there any way to force square terms
whicle applying AIC. 

   Also in what way aov() differs anova(lm).

   Is there any other way to select the regressin
model terms?

Thanks in advance.
Vinod


 
____________________________________________________________________________________
Sucker-punch spam with award-winning protection.


From blakdogg at gmail.com  Thu Jan 25 22:53:37 2007
From: blakdogg at gmail.com (akintayo holder)
Date: Thu, 25 Jan 2007 16:53:37 -0500
Subject: [R] 'Fitting' a model at predefined points
Message-ID: <ada369d00701251353v4cdeb7d0t5d653ae7129e23cc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070125/d52e9061/attachment.pl 

From JILWIL at SAFECO.com  Thu Jan 25 23:27:02 2007
From: JILWIL at SAFECO.com (WILLIE, JILL)
Date: Thu, 25 Jan 2007 14:27:02 -0800
Subject: [R] reducing RODBC odbcQuery memory use?
In-Reply-To: <Pine.LNX.4.64.0701250751490.5353@gannet.stats.ox.ac.uk>
References: <916551423F01504BA339BF69CBA3BE72016E7A35@psmrdcex18.psm.pin.safeco.com>
	<Pine.LNX.4.64.0701250751490.5353@gannet.stats.ox.ac.uk>
Message-ID: <916551423F01504BA339BF69CBA3BE72016E7A43@psmrdcex18.psm.pin.safeco.com>

Basic Questions:

1.  Can I avoid having RODBC use so much memory (35 times the data size
or more) making a data.frame & then .rda file via. sqlQuery/save?  

2.  If not, is there some more appropriate way from w/in R to pull large
data sets (2-5GB) into .rda files from sql? 

3.  I get an unexpectedly high ratio of virtual memory to memory use
(10:1).  Can that be avoided?


Testing details (R transcript below): 1GB CPU, 1GB RAM windows machine.

1.  testing bigger input table (AUTCombinedWA_BILossCost_1per), size is
20MB in sql, 100000 rows, 2 numeric columns, 55 integer columns;
consumes 350000kb of memory & 800000kb of virtual memory to execute the
sqlQuery command.  Memory not released after the step finishes or upon
execution of odbcCloseAll(), or gc().

2.  tested small input table, size is 2MB in sql, 10000 rows, 2 numeric
columns, 55 integer columns; consumes 55000kb of memory & 515000kb (vm
seems oddly high to me) of virtual memory to execute the sqlQuery
command.

3.  concluded the high memory use is isolated to the odbcQuery step w/in
the sqlQuery function as opposed to sqlGetResults or ODBC itself.


Relevant R session transcript:

>library(RODBC)
>channel<-odbcConnect("psmrd") 
>df_OnePer <-data.frame(sqlQuery(channel, "select * from
AUTCombinedWA_BILossCost_1per"))
>save(df_OnePer, file = "df_OnePer.rda")


Additional testing details:
I exited R which released all memory cleanly, then started R again,
loaded the .rda saved in prior step as below.  This confirmed relatively
little of the memory is consumed going from .rda to data frame;
isolating to the RODBC step:

> load("df_OnePer.rda")
> df <- data.frame(df_OnePer)

I closed R & opened MS Access & used the same DSN "psmrd", & to import
the AUTCombinedWA_BILossCost_1per into MS Access which required about
30000kb of memory & 20000kb of virtual.

And finally, I have this excerpt from Prof Brian Ripley that seems
potentially relevant (if it's not just confusion because I called them
'byte-size' when really I should have said they're integers just having
values limited to 1-255).  In any case I'm unable to see from the RODBC
help how to specify this:  "...sqlQuery returns a data frame directly.
I think you need to tell RODBC to translate your 'byte-sized factors' to
numeric, as it will be going through character if these are a type it
does not know about."  

Read all the RODBC help, read all the data import guide & searched help
archives...can't find an answer.  Would appreciate advice, experience,
or direction.

Jill Willie 
Open Seas
Safeco Insurance
jilwil at safeco.com 


-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Thursday, January 25, 2007 12:05 AM
To: WILLIE, JILL
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Size of data vs. needed memory...rule of thumb?

On Wed, 24 Jan 2007, WILLIE, JILL wrote:

> I have been searching all day & most of last night, but can't find any
> benchmarking or recommendations regarding R system requirements for
very
> large (2-5GB) data sets to help guide our hardware configuration.  If
> anybody has experience with this they're willing to share or could
> anybody point me in a direction that might be productive to research,
it
> would be much appreciated.  Specifically:  will R simply use as much
> memory as the OS makes available to it, unlimited?

Under most OSes.  Because Windows has no means to limit the amount made 
available, R under Windows does have it own limiting mechanism (which
you 
hit in the examples below).  R under Linux will allow you to run a 4GB 
process on a machine with 2GB RAM, but you are likely to get around 0.7%

usage.  (One of my colleagues did that on a server earlier this week, 
hence the very specific answer.)

> Is there a multi-threading version R, packages?

Not to run computations in R.  Some parts of R (e.g. GUIs) and some 
libraries (e.g. some BLAS) are multithreaded.  There are multiprocess
packages, e.g. Rmpi, rpvm, snow.

> Does the core R package support 64-bit

Yes, and has for many years.

> & should I expect to see any difference in how memory's handled under 
> that version?

yes, because the address space will not get seriously fragmented.  See
the 
appropriate section in R-admin.html (referenced from INSTALL).

> Is 3 GB of memory to 1GB of data a reasonable ballpark?

I'd say it was a bit low, but it really depends on the analysis you are 
doing, how 1GB of data is made up (many rows?, many cols?, etc) and so
on.
Had you asked me to suggest a ratio I would have said 5.

> Our testing thus far has been on a windows 32-bit box w/1GB of RAM & 1
> CPU; it appears to indicate something like 3GB of RAM for every 1GB of
> sql table (ex-indexes, byte-sized factors).  At this point, we're
> planning on setting up a dual core 64-bit Linux box w/16GB of RAM for
> starters, since we have summed-down sql tables of approx 2-5GB
> generally.
>
> Here's details, just for context, or in case I'm misinterpreting the
> results, or in case there's some more memory-efficient way to get data
> in R's binary format than going w/the data.frame.

Well, sqlQuery returns a data frame directly.  I think you need to tell 
RODBC to translate your 'byte-sized factors' to numeric, as it will be 
going through character if these are a type it does not know about.

> R session:
> 		> library(RODBC)
> 		> channel<-odbcConnect("psmrd")
> 		> FivePer <-data.frame(sqlQuery(channel, "select * from
> AUTCombinedWA_BILossCost_5per"))
>
> 		Error: cannot allocate vector of size 2000 Kb
> 		In addition: Warning messages:
> 		1: Reached total allocation of 1023Mb: see
> help(memory.size)
> 		2: Reached total allocation of 1023Mb: see
> help(memory.size)
>
>
> ODBC connection:
> 		Microsoft SQL Server ODBC Driver Version 03.86.1830
>
> 		Data Source Name: psmrd
> 		Data Source Description:
> 		Server: psmrdcdw01\modeling
> 		Database: OpenSeas_Work1
> 		Language: (Default)
> 		Translate Character Data: Yes
> 		Log Long Running Queries: No
> 		Log Driver Statistics: No
> 		Use Integrated Security: Yes
> 		Use Regional Settings: No
> 		Prepared Statements Option: Drop temporary procedures on
> disconnect
> 		Use Failover Server: No
> 		Use ANSI Quoted Identifiers: Yes
> 		Use ANSI Null, Paddings and Warnings: Yes
> 		Data Encryption: No
>
> Please be patient, I'm a new R user (or at least I'm trying to be...at
> this point I'm mostly a new R-help-reader); I'd appreciated being
> pointed in the right direction if this isn't the right help list to
send
> this question to...or if this question is poorly worded (I did read
the
> posting guide).
>
> Jill Willie
> Open Seas
> Safeco Insurance
> jilwil at safeco.com
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bates at stat.wisc.edu  Fri Jan 26 00:12:00 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 25 Jan 2007 17:12:00 -0600
Subject: [R] [R-pkgs] New version of lme4 and new mailing list
	R-SIG-mixed-models
Message-ID: <40e66e0b0701251512p78612c2br31e32ea0411f301b@mail.gmail.com>

Version 0.9975-11 of the lme4 package has been uploaded to CRAN.  The
source package should be available on the mirrors in a day or two and
binary packages should follow soon after.

There are several changes in this release of the package.  The most
important is the availability of a development version of lmer called,
for the time being, lmer2.  At present lmer2 only fits linear mixed
models.  Generalized linear mixed models will be added "soon".
Furthermore there is no mcmcsamp method for a model fit by lmer2.
This deficiency will also be rectified "soon".  Once I have all the
capabilities and methods currently available for lmer also available
for the new representation I will remove the old representation and
rename lmer2 as lmer.

The current version of lmer will continue to be available throughout
the migration process.  You don't have to change anything about your
use of that function unless you want to try the new one.  It would be
a good idea, however, to save the data and the call to lmer in
addition to saving an lmer object, if you so choose, so that you can
recreate the fitted model when the development version becomes the
release version.

The package contains a vignette giving the details of the new implementation.

The reason I am releasing a development version in parallel with the
production version is because I would like feedback from useR's
regarding the development version.  In my experience, testing it
myself and with colleagues whom I visited recently, I have found that
lmer2 is faster and more reliable than the current lmer.  In
particular, on some difficult model fits I have been able to get
substantially better parameter estimates (i.e. the deviance at the
lmer2 estimates is perhaps 4 or 5 lower than that at the lmer
estimates) with lmer2 than I could with lmer.

If you have fit a linear mixed model using lmer and are willing to try
it with lmer2 I would appreciate your telling me if the parameter
estimates are comparable and which fit was faster (use system.time()
to check).  I'm primarily interested in models fit to large data sets
or "difficult" fits.

We have established a new mailing list, R-SIG-mixed-models, for
discussion of R software to fit mixed-effects models, especially lmer.
 See https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models for
information or to subscribe.

I know that I have said this before but this is the last time that I
am going to change the underlying representation.  Really - trust me -
this is the last time.  My theory of software development is expressed
in a line from an old blues song, "you just keep doing it wrong till
you do it right".  I'm convinced that this time I have it right.  That
statement sounds like "famous last words", doesn't it?  :-)

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From marc_schwartz at comcast.net  Fri Jan 26 00:36:58 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 25 Jan 2007 17:36:58 -0600
Subject: [R] barplot x-axis problem
In-Reply-To: <ba8c09910701251223ofe38f08me933426c4c429789@mail.gmail.com>
References: <ba8c09910701251223ofe38f08me933426c4c429789@mail.gmail.com>
Message-ID: <1169768218.4899.43.camel@localhost.localdomain>

On Thu, 2007-01-25 at 22:23 +0200, Lauri Nikkinen wrote:
> Hi R-users,
> 
> I'm new to R and I'm trying to make a barplot combined with two lines
> (refering to secondary y-axis). Bars should represent the number of
> transfused patients by age class and sex and lines should represent
> the amount of blood units given in age classes. I have now successfully made
> a barplot and used par(new=TRUE) to plot another empty graph at the top of
> the barplot.
> 
> #tab-table:
> #        ikar_new
> #sp       0-9 10-19 20-29 30-39 40-49 50-59 60-69 70-79 >80
> #  mies   227    93    79    92   195   451   560   577 132
> #  nainen 183    80   102   175    99   161   230   357 164
> 
> barplot(tab,
>         beside=TRUE,
>         col = c("black", "lightgrey"),
>         legend = rownames(tab),
>         ylim= c(0,800),
>         font.main = 4,
>         cex.names = 1.1,
>         main = "Transfused patients and trombocytes given by age and sex",
>         ylab="Number of transfused patients",
>         xlab="Age groups (years)")
> 
> axis(1, c(0,3.5+3*0:9), labels=FALSE, tick=TRUE)
> 
> par(new=TRUE)
> 
> #temp-table
> #  ikar_new mies nainen
> #1      0-9 2296   2224
> #2    10-19 1648   3508
> #3    20-29 2276   1464
> #4    30-39 1920   2600
> #5    40-49 3912   2020
> #6    50-59 6856   2872
> #7    60-69 8748   3592
> #8    70-79 7052   4916
> #9      >80 1436   1780
> 
> 
> plot(temp$mies, type="n", yaxt='n', xaxt='n', ann=FALSE)
> lines(temp$mies, col="blue", lwd=2)
> lines(temp$nainen, col="red", lwd=2)
> axis(4, at=NULL)
> 
> I have used lines() to draw the lines into the picture. How can I get the
> lines into the same x-axis and get the actual data points of the lines to
> be exactly in between the two barplot's bars (categories in x-axis)? Now the
> points which the lines connect are not in the middle of the groups in x-axis
> as I would want them to be. The bars in the barplot are not stacked. I'm
> sorry that I'm not able to give you the scripts to make those tables.

I suspect that this is what you might require:


# Get the maximum value for both sets of data
# divide the second set by 10 to normalize to the 
# range of the first set

Max.y <- max(tab, as.matrix(temp[, -1]) / 10)


# Now do the barplot using c(0, Max.y) for ylim
# Also save the bar midpoints in 'mp'
# See ?barplot

mp <- barplot(tab, beside=TRUE,
              col = c("black", "lightgrey"),
              legend = rownames(tab),
              ylim = c(0, Max.y),
              font.main = 4,
              cex.names = 1.1,
              main = "Transfused patients and trombocytes given by age and sex",
              ylab ="Number of transfused patients",
              xlab ="Age groups (years)")

axis(1, c(0, 3.5 + 3 * 0:9), labels = FALSE, tick = TRUE)


# Now add the lines, dividing the y values by 10
# to fit the y axis range to the first set of data
# Use colMeans(mp) for the x axis values, which will
# give the midpoints of each bar pair

lines(colMeans(mp), temp$mies / 10, col = "blue", lwd = 2)
lines(colMeans(mp), temp$nainen / 10, col = "red", lwd = 2)


# Now set the values for the right hand axis

at <- seq(0, 800, 200)


# Set the axis labels to y at * 10

axis(4, at = at, labels = at * 10)


There are multiple ways to accomplish drawing two sets of data with
differing ranges on the same plot. Typically they involve the
normalization of the data to common ranges and then adjustment of the
axis labelling accordingly.

HTH,

Marc Schwartz


From ursu at DMS.UMontreal.CA  Fri Jan 26 01:15:11 2007
From: ursu at DMS.UMontreal.CA (ursu at DMS.UMontreal.CA)
Date: Thu, 25 Jan 2007 19:15:11 -0500 (EST)
Subject: [R] r under suse
Message-ID: <14715.132.204.222.43.1169770511.squirrel@webservices.dms.umontreal.ca>

Hello,
I've been using R under win XP and I am now changing my system to opensuse 10.2
When using:
rpm -i R-base-2.4.1-2.1.i586.rpm

I find something like that

error: unpacking of archive failed on file /usr/lib/R/library/tcltk/exec/util.tc        ;45b943fe:
cpio: read failed - Bad file descriptor

Any suggestions?

Regards
Eugen


From wangtong at usc.edu  Fri Jan 26 01:19:09 2007
From: wangtong at usc.edu (Tong Wang)
Date: Thu, 25 Jan 2007 16:19:09 -0800
Subject: [R] Question about the xgobi package
Message-ID: <e0fbc1b73c94.45b8d87d@usc.edu>

Hi,
   When I tried an example of the xgobi function, I got the following error.  Could someone explain to me
what is wrong ?  Thanks a lot. 

xgobi(crabs,colors=c("SkyBlue","SlateBlue","Orange","Red")[rep(1:4,each=50)])

c:/PROGRA~1/R/R-23~1.1/library/xgobi/scripts/xgobi.bat -vtitle 'crabs' -std mmx C:/DOCUME~1/ADMINI~1/LOCALS~1/Temp/Rtmp1FVirb/xgobi-crabs2c3b6032 
The system cannot find the path specified.



tong


From marc_schwartz at comcast.net  Fri Jan 26 02:26:49 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 25 Jan 2007 19:26:49 -0600
Subject: [R] [R-pkgs] New version of lme4 and new mailing
	list	R-SIG-mixed-models
In-Reply-To: <40e66e0b0701251512p78612c2br31e32ea0411f301b@mail.gmail.com>
References: <40e66e0b0701251512p78612c2br31e32ea0411f301b@mail.gmail.com>
Message-ID: <1169774809.4899.60.camel@localhost.localdomain>

On Thu, 2007-01-25 at 17:12 -0600, Douglas Bates wrote: 
> Version 0.9975-11 of the lme4 package has been uploaded to CRAN.  The
> source package should be available on the mirrors in a day or two and
> binary packages should follow soon after.
> 
> There are several changes in this release of the package.  The most
> important is the availability of a development version of lmer called,
> for the time being, lmer2.  At present lmer2 only fits linear mixed
> models.  Generalized linear mixed models will be added "soon".
> Furthermore there is no mcmcsamp method for a model fit by lmer2.
> This deficiency will also be rectified "soon".  Once I have all the
> capabilities and methods currently available for lmer also available
> for the new representation I will remove the old representation and
> rename lmer2 as lmer.
> 
> The current version of lmer will continue to be available throughout
> the migration process.  You don't have to change anything about your
> use of that function unless you want to try the new one.  It would be
> a good idea, however, to save the data and the call to lmer in
> addition to saving an lmer object, if you so choose, so that you can
> recreate the fitted model when the development version becomes the
> release version.
> 
> The package contains a vignette giving the details of the new implementation.
> 
> The reason I am releasing a development version in parallel with the
> production version is because I would like feedback from useR's
> regarding the development version.  In my experience, testing it
> myself and with colleagues whom I visited recently, I have found that
> lmer2 is faster and more reliable than the current lmer.  In
> particular, on some difficult model fits I have been able to get
> substantially better parameter estimates (i.e. the deviance at the
> lmer2 estimates is perhaps 4 or 5 lower than that at the lmer
> estimates) with lmer2 than I could with lmer.
> 
> If you have fit a linear mixed model using lmer and are willing to try
> it with lmer2 I would appreciate your telling me if the parameter
> estimates are comparable and which fit was faster (use system.time()
> to check).  I'm primarily interested in models fit to large data sets
> or "difficult" fits.
> 
> We have established a new mailing list, R-SIG-mixed-models, for
> discussion of R software to fit mixed-effects models, especially lmer.
>  See https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models for
> information or to subscribe.
> 
> I know that I have said this before but this is the last time that I
> am going to change the underlying representation.  Really - trust me -
> this is the last time.  My theory of software development is expressed
> in a line from an old blues song, "you just keep doing it wrong till
> you do it right".  I'm convinced that this time I have it right.  That
> statement sounds like "famous last words", doesn't it?  :-)

Doug,

Your efforts are most appreciated.

To the points you raise in the last paragraph, I might note the
following quote:

"In developing a methodology, leave room for improvement. 
 It is absolutely critical not to kill a field by doing too
 good a job on the first outing." - 

          "How to be a Highly Cited Author in the Mathematical Sciences", 
           Dr. David Donoho, Stanford University, 2002
           http://www.in-cites.com/scientists/DrDavidDonoho.html

:-)

Best regards,

Marc Schwartz


From marc_schwartz at comcast.net  Fri Jan 26 02:33:53 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 25 Jan 2007 19:33:53 -0600
Subject: [R] r under suse
In-Reply-To: <14715.132.204.222.43.1169770511.squirrel@webservices.dms.umontreal.ca>
References: <14715.132.204.222.43.1169770511.squirrel@webservices.dms.umontreal.ca>
Message-ID: <1169775233.4899.65.camel@localhost.localdomain>

On Thu, 2007-01-25 at 19:15 -0500, ursu at DMS.UMontreal.CA wrote:
> Hello,
> I've been using R under win XP and I am now changing my system to opensuse 10.2
> When using:
> rpm -i R-base-2.4.1-2.1.i586.rpm
> 
> I find something like that
> 
> error: unpacking of archive failed on file /usr/lib/R/library/tcltk/exec/util.tc        ;45b943fe:
> cpio: read failed - Bad file descriptor
> 
> Any suggestions?
> 
> Regards
> Eugen

The first thing that comes to mind is the possibility that the RPM file
that you downloaded is corrupted.

I would re-download the RPM file, possibly from a different CRAN mirror,
and try it again.

HTH,

Marc Schwartz


From leog at anicca-vijja.de  Fri Jan 26 02:51:53 2007
From: leog at anicca-vijja.de (=?ISO-8859-15?Q?Leo_G=FCrtler?=)
Date: Fri, 26 Jan 2007 02:51:53 +0100
Subject: [R] CGIwithR and visible output of
	'invisible(capture.output(library(...)))'
Message-ID: <45B95EB9.4010503@anicca-vijja.de>

Dear alltogether,

I want to use CGIwithR in conjunction with R2HTML.

A small example called 'test.R':

#####

#! /usr/bin/R
invisible(capture.output(library(R2HTML)))
HTML(summary(as.numeric(scanText(formData$numbers))), file=stdout())

#####

The script gets its input via 'CGIwithR.cgi' and contains the variable
"numbers."

The 'HTML' output (-> summary() in this example) goes well, but the
loading of the R2HTML library is also printed (!) in the resulting html
page.

As I took the example from the original CGIwithR examples and I also
read the manpages of invisible() and capture.output(), I do not find a
logical reason why this still happens.

Deleting the line with "invisible ... library().." and adding
'library(R2HTML)' to .Rprofile does not solve the problem.

I use ubuntu edgy, R Version 2.3.1 (2006-06-01)

THANKS!

best wishes,

leo


PS: the output is (uncut)

<---->
MiniUnz 1.01b, demo of zLib + Unz package written by Gilles Vollant more
info at http://www.winimage.com/zLibDll/unzip.html
/usr/local/lib/R/site-library/R2HTML/output/R2HTMLstuff.zip opened
extracting: ASCIIMathML.js extracting: factor.gif extracting:
gridR2HTML.css extracting: gridR2HTML.js extracting: numeric.gif
extracting: Pastel.css extracting: R2HTML.css extracting: R2HTMLlogo.gif
creating directory: runtime/ creating directory: runtime/lib/
extracting: runtime/lib/grid.js extracting: runtime/readme.txt creating
directory: runtime/styles/ creating directory: runtime/styles/classic/
extracting: runtime/styles/classic/gecko.xml extracting:
runtime/styles/classic/grid.css extracting:
runtime/styles/classic/grid.png extracting:
runtime/styles/classic/icons.png extracting:
runtime/styles/classic/loading.gif creating directory:
runtime/styles/flat/ extracting: runtime/styles/flat/gecko.xml
extracting: runtime/styles/flat/grid.css extracting:
runtime/styles/flat/grid.png extracting: runtime/styles/flat/icons.png
extracting: runtime/styles/flat/loading.gif creating directory:
runtime/styles/xp/ extracting: runtime/styles/xp/gecko.xml extracting:
runtime/styles/xp/grid.css extracting: runtime/styles/xp/grid.png
extracting: runtime/styles/xp/icons.png extracting:
runtime/styles/xp/loading.gif extracting: SciViews.css extracting:
tablesort.htc
</---->


From quesada at gmail.com  Fri Jan 26 05:24:12 2007
From: quesada at gmail.com (Jose Quesada)
Date: Fri, 26 Jan 2007 05:24:12 +0100
Subject: [R] %*% in Matrix objects
Message-ID: <op.tmquimdji4ukn7@delllap.ugr.es>

Dear R users,

I need to normalize a bunch of row vectors. At a certain point I need to divide a matrix by a vector of norms. I find that the behavior of Matrix objects differs from normal matrix objects. Example the following code examples differ only in xnormed changing from normal to Matrix object:

	x = matrix(1:12,3,4)
	x = as(x, "CsparseMatrix")
	xnorms  = sqrt(colSums(x^2))
	(xnormed = t(x) * (1/xnorms))

This produces a "warning: coercing sparse to dense matrix for arithmetic
in: t(x) * 1/xnorms." but gets the result (a 4 x 3 matrix)

I want to stay in sparse format anyway (if it helps!) so I tried

	x = matrix(1:12,3,4)
	x = as(x, "CsparseMatrix")
	xnorms  = sqrt(colSums(x^2))
	xnorms = as(xnorms, "CsparseMatrix")
	(xnormed = t(x) * (1/xnorms))

But now, instead of a warning I get
"Error: Matrices must have same dimensions in t(x) * (1/xnorms)"

If I transpose the norms, the error dissapears, but the result is 1 x 4 (not 3 x 4 as before).

I suspect I'm facing the drop=T as before...
Also, it seems that in normal matrix objects %*% behaves the same as *, but in Matrix objects that is not the case.

What am I missing?

-- 
Thanks,
-Jose

--
Jose Quesada, PhD
Research fellow, Psychology Dept.
Sussex University, Brighton, UK
http://www.andrew.cmu.edu/~jquesada


From David.Duffy at qimr.edu.au  Fri Jan 26 07:38:56 2007
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Fri, 26 Jan 2007 16:38:56 +1000 (EST)
Subject: [R] Problem with ordered probit model in MASS
In-Reply-To: <mailman.13.1169722804.11374.r-help@stat.math.ethz.ch>
References: <mailman.13.1169722804.11374.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0701261634020.28579@orpheus.qimr.edu.au>

Justin Bem wrote:

> I got this message, while using the polr function in MASS
> 
> Error in polr(EQ, don, subset = (cote != 0), method = "probit", na.action = na.omit) :
>         attempt for find suitable starting values failed
> 
> how can I initialise starting values ?

You might try running an ordered logistic in lrm() and using those estimates
as starting values (if it runs OK).

David Duffy.


From ripley at stats.ox.ac.uk  Fri Jan 26 07:47:04 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 26 Jan 2007 06:47:04 +0000 (GMT)
Subject: [R] 'Fitting' a model at predefined points
In-Reply-To: <ada369d00701251353v4cdeb7d0t5d653ae7129e23cc@mail.gmail.com>
References: <ada369d00701251353v4cdeb7d0t5d653ae7129e23cc@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0701260645050.24309@gannet.stats.ox.ac.uk>

You called your variables V1 and V2.  You need to supply new values of V1 
and V2, not x and y.

On Thu, 25 Jan 2007, akintayo holder wrote:

> Hi,
> I have a linear model ("mod1 <- lm(V3~V1+V2) and I would like to get the
> model's prediction at values of V1 and V2 not included in the original
> sample.
>
> samp <- read.table("data.dat",nrows=100)
> attach(samp)
> out.poly <- lm(V3 ~ V1 + V2)
>
> If I try to use out.poly to predict values for the function I run into
> problems. It seems that it isn't possible to use a new data frame for the
> predict() or fitted() functions.
>
> predict(out.poly, data.frame(x=V1, y=V2)) uses the original data
> predict(out.poly, data.frame(x=V1, y=V2), newdata=gene.dat)
> - uses the original data also, complaining of the decrease in rows.
>
> If any one could point me in the correct direction, it would be appreciated.
>
> Thanks
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Jan 26 08:00:23 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 26 Jan 2007 07:00:23 +0000 (GMT)
Subject: [R] Question about the xgobi package
In-Reply-To: <e0fbc1b73c94.45b8d87d@usc.edu>
References: <e0fbc1b73c94.45b8d87d@usc.edu>
Message-ID: <Pine.LNX.4.64.0701260649400.24309@gannet.stats.ox.ac.uk>

On Thu, 25 Jan 2007, Tong Wang wrote:

> Hi,
>   When I tried an example of the xgobi function, I got the following error.  Could someone explain to me
> what is wrong ?  Thanks a lot.

1) You do not tell us your OS (presumably Windows, but what sort?)

2) There is a file called INSTALL.windows in the top-level directory of 
the installed package.  Please read it, by e.g.

> file.show(system.file("INSTALL.windows", package="xgobi"))

It says

   You will need to customize the batch files in the scripts directory
   for your installation.  The example files there work for mine.

and it seems you did not do so, and I suspect you have also not set up 
XGobi.

Using XGobi under Windows is tricky, and these days using GGobi is easier 
and stabler.

> xgobi(crabs,colors=c("SkyBlue","SlateBlue","Orange","Red")[rep(1:4,each=50)])
>
> c:/PROGRA~1/R/R-23~1.1/library/xgobi/scripts/xgobi.bat -vtitle 'crabs' -std mmx C:/DOCUME~1/ADMINI~1/LOCALS~1/Temp/Rtmp1FVirb/xgobi-crabs2c3b6032
> The system cannot find the path specified.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From detlef.steuer at hsu-hamburg.de  Fri Jan 26 08:29:50 2007
From: detlef.steuer at hsu-hamburg.de (Detlef Steuer)
Date: Fri, 26 Jan 2007 08:29:50 +0100
Subject: [R] r under suse
In-Reply-To: <1169775233.4899.65.camel@localhost.localdomain>
References: <14715.132.204.222.43.1169770511.squirrel@webservices.dms.umontreal.ca>
	<1169775233.4899.65.camel@localhost.localdomain>
Message-ID: <20070126082950.b9cc0823.detlef.steuer@hsu-hamburg.de>

On Thu, 25 Jan 2007 19:33:53 -0600
Marc Schwartz <marc_schwartz at comcast.net> wrote:

> On Thu, 2007-01-25 at 19:15 -0500, ursu at DMS.UMontreal.CA wrote:
> > Hello,
> > I've been using R under win XP and I am now changing my system to opensuse 10.2
> > When using:
> > rpm -i R-base-2.4.1-2.1.i586.rpm
> > 
> > I find something like that
> > 
> > error: unpacking of archive failed on file /usr/lib/R/library/tcltk/exec/util.tc        ;45b943fe:
> > cpio: read failed - Bad file descriptor
> > 
> > Any suggestions?
> > 
> > Regards
> > Eugen
> 
> The first thing that comes to mind is the possibility that the RPM file
> that you downloaded is corrupted.
> 
> I would re-download the RPM file, possibly from a different CRAN mirror,
> and try it again.

or try downloading from software.opensuse.org as described in the readme.

detlef

> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From lauri.nikkinen at iki.fi  Fri Jan 26 09:28:27 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Fri, 26 Jan 2007 10:28:27 +0200
Subject: [R] barplot x-axis problem
In-Reply-To: <1169768218.4899.43.camel@localhost.localdomain>
References: <ba8c09910701251223ofe38f08me933426c4c429789@mail.gmail.com>
	<1169768218.4899.43.camel@localhost.localdomain>
Message-ID: <ba8c09910701260028l219f29c7kb1f32f6689fb8c16@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070126/65168a16/attachment.pl 

From bgreen at dyson.brisnet.org.au  Fri Jan 26 10:04:27 2007
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Fri, 26 Jan 2007 19:04:27 +1000
Subject: [R] replicating the odds ratio from a published study
In-Reply-To: <mailman.17.1169636407.29747.r-help@stat.math.ethz.ch>
Message-ID: <5.1.0.14.0.20070126185720.00d09858@pop3.brisnet.org.au>

I wanted to compare odds ratio across studies and tried to replicate the 
results from a study but have not been able to determine how to do this in R.

The study reported a sample of 961 men, of whom 41 had a disorder. The 
reported raw odds ratio was 6.52 (4.70-9.00)

I did a search of the archives and came across script that looks like it 
should perform this task, however the results I obtained did not match

  > prop.test(41,961)

         1-sample proportions test with continuity correction

data:  41 out of 961, null probability 0.5
X-squared = 802.1686, df = 1, p-value < 2.2e-16
alternative hypothesis: true p is not equal to 0.5
95 percent confidence interval:
  0.03115856 0.05795744
sample estimates:
          p
0.04266389

 >
 > ci.p <- prop.test(920, 961)$conf
 > ci.odds <- ci.p/(1-ci.p)
 > ci.odds
[1] 16.25404 31.09390
attr(,"conf.level")
[1] 0.95


Any advice regarding the script I require is appreciated,

regards

bob Green


From c.beale at macaulay.ac.uk  Fri Jan 26 10:58:33 2007
From: c.beale at macaulay.ac.uk (Colin Beale)
Date: Fri, 26 Jan 2007 09:58:33 +0000
Subject: [R] Using functions within functions (environment problems)
Message-ID: <45B9D0C9.1E3C.0035.0@macaulay.ac.uk>

Hi everyone,

I've been having difficulty writing wrapper functions for some
functions where those same functions include other functions with
eval()
calls where the environment is specified. A very simple example using
function lmer from lme4:

lmerWrapper <- function(formula, data, family = gaussian, method =
c("REML", 
    "ML", "PQL", "Laplace", "AGQ"), control = list(), start = NULL, 
    subset, weights, na.action, offset, contrasts = NULL, model =
TRUE,

    ...)
{

    xNew <- runif(0,1, length(data[,1]))
    fNew <- sample(1:4, length(data[,1]), replace = T)
    data <- as.data.frame(cbind(data, xNew, fNew))
    formula <- update(formula, .~. + xNew + (1|fNew))
    out <- lmer (formula = formula, data = data, family = family,
method =
        method, weights = weights, control = control, start = start,
        subset = subset, na.action = na.action, offset = offset,
        contrasts = contrasts, model = model)
}

dat <- data.frame(Y = rnorm(100), X1 = rnorm(100), X2 = rnorm(100), 
        F1 = as.factor(sample(1:4, 400, replace = T)))
test <- lmerWrapper (Y ~ X1 + X2 + (1|F1), data = dat)


This function attempts to create two new variables, add these to the
data.frame and modify the formula for include these two new variables,
and then fit this expanded model using lmer. Clearly the example is
silly, but it illustrates the problem as it fails with the error:

Error in eval(expr, envir, enclos) : object "xNew" not found

because a function within lmer (lmerFrames) makes an eval call where
the environment is specified as the .GlobalEnv which doesn't contain
xNew - it needs to be looking in the environment from which it was
called rather than going right back to the root. In a more general
context, I might like to create a function where I don't specify a new
data.frame in the wrapper, but want the function to search the back
down
the search path for each component, finding xNew in the environment of
the wrapper function and X1 in the .GlobalEnv. Is there a general
solution to this? Or do I need to create a modified lmer function that
calls a modified lmerFrames function where I specify the environment
differently? There's quite a lot in the archives dealing with
environments and search paths, but I'm either not understanding how to
apply the wisdom therein, or not finding the specific answer to my
problem, and I should say that I have this problem with more than just
the lmer function.

Thanks for any pointers,

Colin

> sessionInfo()
R version 2.4.1 (2006-12-18) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_United Kingdom.1252;LC_CTYPE=English_United
Kingdom.1252;LC_MONETARY=English_United
Kingdom.1252;LC_NUMERIC=C;LC_TIME=English_United Kingdom.1252

attached base packages:
[1] "stats"     "graphics"  "grDevices" "datasets"  "tcltk"    
"utils"
    "methods"   "base"     

other attached packages:
      debug    mvbutils        lme4      Matrix     lattice   
svSocket
       svIO      R2HTML      svMisc       svIDE 
    "1.1.0"     "1.1.1" "0.9975-10"  "0.9975-8"   "0.14-16"    
"0.9-5"
    "0.9-5"      "1.58"     "0.9-5"     "0.9-5" 




Dr. Colin Beale
Spatial Ecologist
The Macaulay Institute
Craigiebuckler
Aberdeen
AB15 8QH
UK

Tel: 01224 498245 ext. 2427
Fax: 01224 311556
Email: c.beale at macaulay.ac.uk 



-- 
Please note that the views expressed in this e-mail are those of the
sender and do not necessarily represent the views of the Macaulay
Institute. This email and any attachments are confidential and are
intended solely for the use of the recipient(s) to whom they are
addressed. If you are not the intended recipient, you should not read,
copy, disclose or rely on any information contained in this e-mail, and
we would ask you to contact the sender immediately and delete the email
from your system. Thank you.
Macaulay Institute and Associated Companies, Macaulay Drive,
Craigiebuckler, Aberdeen, AB15 8QH.


From maechler at stat.math.ethz.ch  Fri Jan 26 11:11:00 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 26 Jan 2007 11:11:00 +0100
Subject: [R] %*% in Matrix objects
In-Reply-To: <op.tmquimdji4ukn7@delllap.ugr.es>
References: <op.tmquimdji4ukn7@delllap.ugr.es>
Message-ID: <17849.54196.280968.285617@stat.math.ethz.ch>

>>>>> "Jose" == Jose Quesada <quesada at gmail.com>
>>>>>     on Fri, 26 Jan 2007 05:24:12 +0100 writes:

    Jose> Dear R users,
    Jose> I need to normalize a bunch of row vectors. At a certain point I need to divide a matrix by a vector of norms. I find that the behavior of Matrix objects differs from normal matrix objects.

I believe you are showing evidence for that; though I know it's
still true, and will be less true for the next release of the
Matrix package.

    Jose> Example the following code examples differ only in
    Jose> xnormed changing from normal to Matrix object:

    Jose> x = matrix(1:12,3,4)
    Jose> x = as(x, "CsparseMatrix")

or directly
   
   x <- Matrix(1:12, 3,4, sparse = TRUE)

I hope that you are aware of the fact that it's not efficient at
all to store a dense matrix (it has *no* 0 entry) as a sparse one..

    Jose> xnorms  = sqrt(colSums(x^2))
    Jose> (xnormed = t(x) * (1/xnorms))

    Jose> This produces a "warning: coercing sparse to dense matrix for arithmetic
    Jose> in: t(x) * 1/xnorms." but gets the result (a 4 x 3 matrix)

    Jose> I want to stay in sparse format anyway
    Jose>  (if it helps!) 

what should it help for? Are you talking about a real
application with a proper sparse matrix as opposed to the toy
example here? In that case I agree, and as a matter of fact, the
source code of the Matrix package leading to the above warning
is the following

	      } else {
		  ## FIXME: maybe far from optimal:
		  warning("coercing sparse to dense matrix for arithmetic")
		  callGeneric(as(e1, "dgeMatrix"), e2)
	      }

and your posting is indeed an incentive for the Matrix developers
to improve that part ... ;-)

    Jose> so I tried

    Jose> x = matrix(1:12,3,4)
    Jose> x = as(x, "CsparseMatrix")
    Jose> xnorms  = sqrt(colSums(x^2))
    Jose> xnorms = as(xnorms, "CsparseMatrix")
    Jose> (xnormed = t(x) * (1/xnorms))

    Jose> But now, instead of a warning I get
    Jose> "Error: Matrices must have same dimensions in t(x) * (1/xnorms)"

yes.  And the same happens with traditional matrices -- and well so:
For arithmetic with matrices (traditional or "Matrices"),

    A o B       (o in {"+", "*", "^", ....})   
    -----

does require that matrices A and B are ``conformable'', i.e.,
have exact same dimensions.

Only when one of A or B is *not* a matrix,
then the usual S-language recycling rules are applied,
and that's what you were using in your first example 
(<Matrix> * <numeric>) above.

    Jose> If I transpose the norms, the error dissapears, but the result is 1 x 4 (not 3 x 4 as before).

That would be a bug of *not* giving an error... but I can't see
it. Can you please give an exact example -- as you well gave otherwise; and 
BTW, do you use a sensible sparse matrix such as
 (x <- Matrix(c(0,0,1,0), 3,4))


    Jose> I suspect I'm facing the drop=T as before...
why??

    Jose> Also, it seems that in normal matrix objects %*%
    Jose> behaves the same as *,

not at all!!  If that was the case, the inventors of the S
language would never have introduced '%*%' !

    Jose> but in Matrix objects that is not the case.

    Jose> What am I missing?

I guess several things, see above, notably how basic
matrix+vector - arithmetic is defined to in the S language.

Regards,
Martin Maechler, ETH Zurich


From ggrothendieck at gmail.com  Fri Jan 26 12:05:59 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 26 Jan 2007 06:05:59 -0500
Subject: [R] Using functions within functions (environment problems)
In-Reply-To: <45B9D0C9.1E3C.0035.0@macaulay.ac.uk>
References: <45B9D0C9.1E3C.0035.0@macaulay.ac.uk>
Message-ID: <971536df0701260305h7b6bd088wa87015cd2118cc6e@mail.gmail.com>

Check out this post and the entire thread to which it belongs:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/67474.html

On 1/26/07, Colin Beale <c.beale at macaulay.ac.uk> wrote:
> Hi everyone,
>
> I've been having difficulty writing wrapper functions for some
> functions where those same functions include other functions with
> eval()
> calls where the environment is specified. A very simple example using
> function lmer from lme4:
>
> lmerWrapper <- function(formula, data, family = gaussian, method =
> c("REML",
>    "ML", "PQL", "Laplace", "AGQ"), control = list(), start = NULL,
>    subset, weights, na.action, offset, contrasts = NULL, model =
> TRUE,
>
>    ...)
> {
>
>    xNew <- runif(0,1, length(data[,1]))
>    fNew <- sample(1:4, length(data[,1]), replace = T)
>    data <- as.data.frame(cbind(data, xNew, fNew))
>    formula <- update(formula, .~. + xNew + (1|fNew))
>    out <- lmer (formula = formula, data = data, family = family,
> method =
>        method, weights = weights, control = control, start = start,
>        subset = subset, na.action = na.action, offset = offset,
>        contrasts = contrasts, model = model)
> }
>
> dat <- data.frame(Y = rnorm(100), X1 = rnorm(100), X2 = rnorm(100),
>        F1 = as.factor(sample(1:4, 400, replace = T)))
> test <- lmerWrapper (Y ~ X1 + X2 + (1|F1), data = dat)
>
>
> This function attempts to create two new variables, add these to the
> data.frame and modify the formula for include these two new variables,
> and then fit this expanded model using lmer. Clearly the example is
> silly, but it illustrates the problem as it fails with the error:
>
> Error in eval(expr, envir, enclos) : object "xNew" not found
>
> because a function within lmer (lmerFrames) makes an eval call where
> the environment is specified as the .GlobalEnv which doesn't contain
> xNew - it needs to be looking in the environment from which it was
> called rather than going right back to the root. In a more general
> context, I might like to create a function where I don't specify a new
> data.frame in the wrapper, but want the function to search the back
> down
> the search path for each component, finding xNew in the environment of
> the wrapper function and X1 in the .GlobalEnv. Is there a general
> solution to this? Or do I need to create a modified lmer function that
> calls a modified lmerFrames function where I specify the environment
> differently? There's quite a lot in the archives dealing with
> environments and search paths, but I'm either not understanding how to
> apply the wisdom therein, or not finding the specific answer to my
> problem, and I should say that I have this problem with more than just
> the lmer function.
>
> Thanks for any pointers,
>
> Colin
>
> > sessionInfo()
> R version 2.4.1 (2006-12-18)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_United Kingdom.1252;LC_CTYPE=English_United
> Kingdom.1252;LC_MONETARY=English_United
> Kingdom.1252;LC_NUMERIC=C;LC_TIME=English_United Kingdom.1252
>
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "datasets"  "tcltk"
> "utils"
>    "methods"   "base"
>
> other attached packages:
>      debug    mvbutils        lme4      Matrix     lattice
> svSocket
>       svIO      R2HTML      svMisc       svIDE
>    "1.1.0"     "1.1.1" "0.9975-10"  "0.9975-8"   "0.14-16"
> "0.9-5"
>    "0.9-5"      "1.58"     "0.9-5"     "0.9-5"
>
>
>
>
> Dr. Colin Beale
> Spatial Ecologist
> The Macaulay Institute
> Craigiebuckler
> Aberdeen
> AB15 8QH
> UK
>
> Tel: 01224 498245 ext. 2427
> Fax: 01224 311556
> Email: c.beale at macaulay.ac.uk
>
>
>
> --
> Please note that the views expressed in this e-mail are those of the
> sender and do not necessarily represent the views of the Macaulay
> Institute. This email and any attachments are confidential and are
> intended solely for the use of the recipient(s) to whom they are
> addressed. If you are not the intended recipient, you should not read,
> copy, disclose or rely on any information contained in this e-mail, and
> we would ask you to contact the sender immediately and delete the email
> from your system. Thank you.
> Macaulay Institute and Associated Companies, Macaulay Drive,
> Craigiebuckler, Aberdeen, AB15 8QH.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Fabian.Mollet at wur.nl  Fri Jan 26 13:13:23 2007
From: Fabian.Mollet at wur.nl (Mollet, Fabian)
Date: Fri, 26 Jan 2007 13:13:23 +0100
Subject: [R] constrained simulated annealing
Message-ID: <2DF51316D2ACEF4891856424912C454E1F0A36@scomp0040.wurnet.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070126/723305df/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Jan 26 13:12:06 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 26 Jan 2007 12:12:06 +0000 (GMT)
Subject: [R] Using functions within functions (environment problems)
In-Reply-To: <45B9D0C9.1E3C.0035.0@macaulay.ac.uk>
References: <45B9D0C9.1E3C.0035.0@macaulay.ac.uk>
Message-ID: <Pine.LNX.4.64.0701261126240.13335@gannet.stats.ox.ac.uk>

I don't think your subject line is relevant.  You do not have 'functions 
within functions': lmerFrames is not within lmer.  (You seem to be 
confusing functions within and calls from.)

Your example does not work (did you test it?).  When the erroneous runif 
call is corrected (it gives a result of length 0), I get a different error 
about 'weights', and indeed you have not specified 'weights' nor 'subset' 
nor 'na.action' nor 'offset'.

The following does work for me:

lmerWrapper <- function(formula, data, ...)
{

     xNew <- runif(length(data[,1]))
     fNew <- sample(1:4, length(data[,1]), replace = TRUE)
     data <- as.data.frame(cbind(data, xNew, fNew))
     formula <- update(formula, .~. + xNew + (1|fNew))
     out <- lmer (formula = formula, data = data, ...)
}

dat <- data.frame(Y = rnorm(100), X1 = rnorm(100), X2 = rnorm(100),
         F1 = as.factor(sample(1:4, 400, replace = T)))
test <- lmerWrapper (Y ~ X1 + X2 + (1|F1), data = dat)

so whatever your actual problem is, it is it seems not about finding xNew.


There is one potential problem I spotted.  One of the places a standard 
model-fitting function will look for variables is in the environment of 
'formula'.  This is an argument, and update.formula changes the 
environment, so it is possible that old (rather than additional) variables 
could disappear from view.


On Fri, 26 Jan 2007, Colin Beale wrote:

> Hi everyone,
>
> I've been having difficulty writing wrapper functions for some
> functions where those same functions include other functions with
> eval()
> calls where the environment is specified. A very simple example using
> function lmer from lme4:
>
> lmerWrapper <- function(formula, data, family = gaussian, method =
> c("REML",
>    "ML", "PQL", "Laplace", "AGQ"), control = list(), start = NULL,
>    subset, weights, na.action, offset, contrasts = NULL, model =
> TRUE,
>
>    ...)
> {
>
>    xNew <- runif(0,1, length(data[,1]))
>    fNew <- sample(1:4, length(data[,1]), replace = T)
>    data <- as.data.frame(cbind(data, xNew, fNew))
>    formula <- update(formula, .~. + xNew + (1|fNew))
>    out <- lmer (formula = formula, data = data, family = family,
> method =
>        method, weights = weights, control = control, start = start,
>        subset = subset, na.action = na.action, offset = offset,
>        contrasts = contrasts, model = model)
> }
>
> dat <- data.frame(Y = rnorm(100), X1 = rnorm(100), X2 = rnorm(100),
>        F1 = as.factor(sample(1:4, 400, replace = T)))
> test <- lmerWrapper (Y ~ X1 + X2 + (1|F1), data = dat)
>
>
> This function attempts to create two new variables, add these to the
> data.frame and modify the formula for include these two new variables,
> and then fit this expanded model using lmer. Clearly the example is
> silly, but it illustrates the problem as it fails with the error:
>
> Error in eval(expr, envir, enclos) : object "xNew" not found
>
> because a function within lmer (lmerFrames) makes an eval call where
> the environment is specified as the .GlobalEnv which doesn't contain
> xNew - it needs to be looking in the environment from which it was
> called rather than going right back to the root. In a more general
> context, I might like to create a function where I don't specify a new
> data.frame in the wrapper, but want the function to search the back
> down
> the search path for each component, finding xNew in the environment of
> the wrapper function and X1 in the .GlobalEnv. Is there a general
> solution to this? Or do I need to create a modified lmer function that
> calls a modified lmerFrames function where I specify the environment
> differently? There's quite a lot in the archives dealing with
> environments and search paths, but I'm either not understanding how to
> apply the wisdom therein, or not finding the specific answer to my
> problem, and I should say that I have this problem with more than just
> the lmer function.
>
> Thanks for any pointers,
>
> Colin
>
>> sessionInfo()
> R version 2.4.1 (2006-12-18)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=English_United Kingdom.1252;LC_CTYPE=English_United
> Kingdom.1252;LC_MONETARY=English_United
> Kingdom.1252;LC_NUMERIC=C;LC_TIME=English_United Kingdom.1252
>
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "datasets"  "tcltk"
> "utils"
>    "methods"   "base"
>
> other attached packages:
>      debug    mvbutils        lme4      Matrix     lattice
> svSocket
>       svIO      R2HTML      svMisc       svIDE
>    "1.1.0"     "1.1.1" "0.9975-10"  "0.9975-8"   "0.14-16"
> "0.9-5"
>    "0.9-5"      "1.58"     "0.9-5"     "0.9-5"
>
>
>
>
> Dr. Colin Beale
> Spatial Ecologist
> The Macaulay Institute
> Craigiebuckler
> Aberdeen
> AB15 8QH
> UK
>
> Tel: 01224 498245 ext. 2427
> Fax: 01224 311556
> Email: c.beale at macaulay.ac.uk
>
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From info at aghmed.fsnet.co.uk  Fri Jan 26 14:06:58 2007
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Fri, 26 Jan 2007 13:06:58 +0000
Subject: [R] replicating the odds ratio from a published study
In-Reply-To: <5.1.0.14.0.20070126185720.00d09858@pop3.brisnet.org.au>
References: <mailman.17.1169636407.29747.r-help@stat.math.ethz.ch>
	<5.1.0.14.0.20070126185720.00d09858@pop3.brisnet.org.au>
Message-ID: <7.0.0.16.0.20070126130450.0195bf38@aghmed.fsnet.co.uk>

At 09:04 26/01/2007, Bob Green wrote:
>I wanted to compare odds ratio across studies and tried to replicate 
>the results from a study but have not been able to determine how to 
>do this in R.
>
>The study reported a sample of 961 men, of whom 41 had a disorder. 
>The reported raw odds ratio was 6.52 (4.70-9.00)

For an odds ratio you require two odds from which you form the odds ratio.
You only have one odds.
Do you have another one lying around somewhere?


>I did a search of the archives and came across script that looks 
>like it should perform this task, however the results I obtained did not match
>
>  > prop.test(41,961)
>
>         1-sample proportions test with continuity correction
>
>data:  41 out of 961, null probability 0.5
>X-squared = 802.1686, df = 1, p-value < 2.2e-16
>alternative hypothesis: true p is not equal to 0.5
>95 percent confidence interval:
>  0.03115856 0.05795744
>sample estimates:
>          p
>0.04266389
>
> >
> > ci.p <- prop.test(920, 961)$conf
> > ci.odds <- ci.p/(1-ci.p)
> > ci.odds
>[1] 16.25404 31.09390
>attr(,"conf.level")
>[1] 0.95
>
>
>Any advice regarding the script I require is appreciated,
>
>regards
>
>bob Green
>
>

Michael Dewey
http://www.aghmed.fsnet.co.uk


From statadat at gmail.com  Fri Jan 26 14:46:44 2007
From: statadat at gmail.com (domenico pestalozzi)
Date: Fri, 26 Jan 2007 14:46:44 +0100
Subject: [R] defining function into another function
Message-ID: <e591a95b0701260546p2ac64afcmbf95eb2c6b4ab52@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070126/9b872579/attachment.pl 

From carsten.steinhoff at gmx.de  Fri Jan 26 14:56:19 2007
From: carsten.steinhoff at gmx.de (Carsten Steinhoff)
Date: Fri, 26 Jan 2007 14:56:19 +0100
Subject: [R] Bayesian inference: Poisson distribution with normal (!) prior
Message-ID: <200701261357.l0QDv0OK001735@hypatia.math.ethz.ch>

Hello,

for a frequency modelling problem I want to combine expert knowledge with
incoming real-life data (which is not available up to now). The frequency
has to be modelled with a poisson distribution. The parameter lambda has to
be normal distributed (for certain reasons we did not NOT choose gamma
althoug it would make everything easier).

I've started with the subsequent two functions to obtain random numbers for
Lambda after the first observed period. My question is now, how to get the
randoms for the n following periods?

Thanks a lot for your hints! Maybe there is an easier way to do the
necessary calculations...?

Carsten

# Function 1: Posterior for the first observation
test.posterior=function(x,observation,p1,p2)
{
f1=function(x,observation,p1,p2)
dpois(observation,qnorm(pnorm(x,p1,p2),p1,p2))*dnorm(x,p1,p2)
integral=integrate(f1,0,Inf,p1=p1,p2=p2,observation=observation)$value
ausgabe=f1(x,observation,p1=p1,p2=p2)/integral
return(ausgabe)
}

# Function 2: Random numbers for Lambda in the second period
test.posterior.random=function(n,x,length,observation,p1,p2)
{
# n = random numbers to calculate
# x = maximum value for integral calculation
ret=c()
x=seq(0.001,x,length.out=length)
for (i in x)
{
ret=c(ret,integrate(test.posterior,observation=observation,p1=p1,p2=p2,lower
=1,i)$value)
}
ret=abs(ret)
pr=cbind(ret,x)
pr=which(pr[,1]==unique(pr[,1]))
k=approxfun(ret[pr],x[pr])
return(k(runif(n)))
}

# Generate 1000 random numbers
test.posterior.random(1000,.5,1000,1,.2,.05)


From P.Dalgaard at biostat.ku.dk  Fri Jan 26 14:57:27 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 26 Jan 2007 14:57:27 +0100
Subject: [R] replicating the odds ratio from a published study
In-Reply-To: <7.0.0.16.0.20070126130450.0195bf38@aghmed.fsnet.co.uk>
References: <mailman.17.1169636407.29747.r-help@stat.math.ethz.ch>	<5.1.0.14.0.20070126185720.00d09858@pop3.brisnet.org.au>
	<7.0.0.16.0.20070126130450.0195bf38@aghmed.fsnet.co.uk>
Message-ID: <45BA08C7.4060008@biostat.ku.dk>

Michael Dewey wrote:
> At 09:04 26/01/2007, Bob Green wrote:
>   
>> I wanted to compare odds ratio across studies and tried to replicate 
>> the results from a study but have not been able to determine how to 
>> do this in R.
>>
>> The study reported a sample of 961 men, of whom 41 had a disorder. 
>> The reported raw odds ratio was 6.52 (4.70-9.00)
>>     
>
> For an odds ratio you require two odds from which you form the odds ratio.
> You only have one odds.
> Do you have another one lying around somewhere?
>   
Alternatively, the odds ratio presumably compares two groups. If you
know the group sizes, the two odds ratios may be reconstructed. If I
make a wild guess that the groups are roughly equal, I might get

> M <- cbind(c(460,460),c(6,35))
> M
     [,1] [,2]
[1,]  460    6
[2,]  460   35
> fisher.test(M)

        Fisher's Exact Test for Count Data

data:  M
p-value = 7.406e-06
alternative hypothesis: true odds ratio is not equal to 1
95 percent confidence interval:
  2.393209 17.104976
sample estimates:
odds ratio
  5.824317

Judging by the c.i., the groups are probably *not* of similar size. I
suppose that the high-incidence group is a bit smaller so that the count
of advverse events is more similar. M <- cbind(c(803,117),c(21,20)) is a
bit more like it, but your (Bob's) confidence interval is narrower even
than this.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From f.harrell at vanderbilt.edu  Fri Jan 26 15:23:17 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 26 Jan 2007 08:23:17 -0600
Subject: [R] summary of the effects after logistic regression model
In-Reply-To: <880369780701250756l52e1427cofda70cc2eede442b@mail.gmail.com>
References: <880369780701250756l52e1427cofda70cc2eede442b@mail.gmail.com>
Message-ID: <45BA0ED5.7090106@vanderbilt.edu>

andrea evangelista wrote:
> Dear all, my aim is to estimate the efficacy over time  of a treatment for
> headache prevention.  Data consist of long sequences of repeated binary
> outcomes   (1 if the subject has at least 1 episode of headache  , 0
> otherwise) on subjects randomized to placebo or treatment.
> 
> I have fit a logistic regression model with Huber-White cluster sandwich
> covariance estimator.
> I have put in the model  the variables treatment (trt),sex,age and a
> restricted cubic spline of time (days)  to allow for non-linear treatment
> effects.
> 
> I use the functions lrm and robcov from R Design library:
> 
> h<-lrm(head ~ trt*rcs(days)+ age+ sex,x=T,y=T)
> h.rob<-robcov(h,id)
> 
> I want to estimate treatment effect over time, then:
> 
> k<-contrast(h.rob,list(day=1:240, trt=1),
>                           list(day=1:240, trt=0))
> 
> xYplot(Cbind(exp(Contrast), exp(Lower),exp( Upper)) ~ day, data=k)   #Plot
> of treatment effects (odds ratio).
> 
> The treatment group has a disavantage at the baseline ( for day=1 ,OR=1.16),
> however at day=210 I can see a reduction of headache risk (OR=0.58) on
> treatment group.
> 
> How can I set to 1  the OR of treatment at the baseline (day=1) with R? In
> case, is it corrent?

I don't know of an extremely simple way to do it.  But the baseline may 
be noisy and I'm not sure I would recommend doing what you want.

Frank Harrell

> 
> Best regards
> 
> Andrea Evangelista
> Italy
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From vincent.goulet at act.ulaval.ca  Fri Jan 26 16:14:14 2007
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Fri, 26 Jan 2007 10:14:14 -0500
Subject: [R] Bayesian inference: Poisson distribution with normal (!)
	prior
In-Reply-To: <200701261357.l0QDv0OK001735@hypatia.math.ethz.ch>
References: <200701261357.l0QDv0OK001735@hypatia.math.ethz.ch>
Message-ID: <200701261014.14439.vincent.goulet@act.ulaval.ca>

Le Vendredi 26 Janvier 2007 08:56, Carsten Steinhoff a ?crit?:
> Hello,
>
> for a frequency modelling problem I want to combine expert knowledge with
> incoming real-life data (which is not available up to now). The frequency
> has to be modelled with a poisson distribution. The parameter lambda has to
> be normal distributed (for certain reasons we did not NOT choose gamma
> althoug it would make everything easier).
>
> I've started with the subsequent two functions to obtain random numbers for
> Lambda after the first observed period. My question is now, how to get the
> randoms for the n following periods?
>
> Thanks a lot for your hints! Maybe there is an easier way to do the
> necessary calculations...?
>
> Carsten
>
> # Function 1: Posterior for the first observation
> test.posterior=function(x,observation,p1,p2)
> {
> f1=function(x,observation,p1,p2)
> dpois(observation,qnorm(pnorm(x,p1,p2),p1,p2))*dnorm(x,p1,p2)
> integral=integrate(f1,0,Inf,p1=p1,p2=p2,observation=observation)$value
> ausgabe=f1(x,observation,p1=p1,p2=p2)/integral
> return(ausgabe)
> }
>
> # Function 2: Random numbers for Lambda in the second period
> test.posterior.random=function(n,x,length,observation,p1,p2)
> {
> # n = random numbers to calculate
> # x = maximum value for integral calculation
> ret=c()
> x=seq(0.001,x,length.out=length)
> for (i in x)
> {
> ret=c(ret,integrate(test.posterior,observation=observation,p1=p1,p2=p2,lowe
>r =1,i)$value)
> }
> ret=abs(ret)
> pr=cbind(ret,x)
> pr=which(pr[,1]==unique(pr[,1]))
> k=approxfun(ret[pr],x[pr])
> return(k(runif(n)))
> }
>
> # Generate 1000 random numbers
> test.posterior.random(1000,.5,1000,1,.2,.05)

Ok, I'll not question the idea to use a distribution defined on real numbers 
as a prior for a strictly positive parameter... Now, if what you want are 
random numbers from the Poisson/normal mixture, this will do:

> rpois(1000, lambda = rnorm(1000, mean, sd))

The reason this works is because the r* functions are vectorized, so here 
rpois() will generate the first number with lambda equal to the first value 
returned by rnorm(), the second number with lambda equal to the second value 
returned by rnorm(), etc.

Am I missing what you want to do?

-- 
  Vincent Goulet, Associate Professor
  ?cole d'actuariat
  Universit? Laval, Qu?bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca


From lalithaviswanath at yahoo.com  Fri Jan 26 16:20:44 2007
From: lalithaviswanath at yahoo.com (lalitha viswanath)
Date: Fri, 26 Jan 2007 07:20:44 -0800 (PST)
Subject: [R] unique/subset problem
In-Reply-To: <cdf817830701251030y7eb7b14fqbd71d191232fee88@mail.gmail.com>
Message-ID: <994290.60745.qm@web43122.mail.sp1.yahoo.com>

Hi
The pruned dataset has 8 unique genomes in it while
the dataset before pruning has 65 unique genomes in
it.
However calling unique on the pruned dataset seems to
return 65 no matter what.

Any assistance in this matter would be appreciated.

Thanks
Lalitha
--- Weiwei Shi <helprhelp at gmail.com> wrote:

> Hi,
> 
> Even you removed "many" genomes1 by setting score<
> -5; it is not
> necessary saying you changed the uniqueness.
> 
> To check this, you can do like
> p0 <- unique(dataset[dataset$score< -5, "genome1"])
> # same as subset
> p1 <- unique(dataset[dataset$score>= -5, "genome1"])
> 
> setdiff(p1, p0)
> 
> if the output above has NULL, then it means even
> though you remove
> many genomes1, but it does not help changing the
> uniqueness.
> 
> HTH,
> 
> weiwei
> 
> 
> 
> On 1/25/07, lalitha viswanath
> <lalithaviswanath at yahoo.com> wrote:
> > Hi
> > I am new to R programming and am using subset to
> > extract part of a data as follows
> >
> > names(dataset) =
> > c("genome1","genome2","dist","score");
> > prunedrelatives <- subset(dataset, score < -5);
> >
> > However when I use unique to find the number of
> unique
> > genomes now present in prunedrelatives I get
> results
> > identical to calling unique(dataset$genome1)
> although
> > subset has eliminated many genomes and records.
> >
> > I would greatly appreciate your input about using
> > "unique" correctly  in this regard.
> >
> > Thanks
> > Lalitha
> >
> >
> >
> >
>
____________________________________________________________________________________
> > TV dinner still cooling?
> > Check out "Tonight's Picks" on Yahoo! TV.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> >
> 
> 
> -- 
> Weiwei Shi, Ph.D
> Research Scientist
> GeneGO, Inc.
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 



 
____________________________________________________________________________________
Bored stiff? Loosen up...


From BEN at SSANET.COM  Fri Jan 26 16:48:07 2007
From: BEN at SSANET.COM (Ben Fairbank)
Date: Fri, 26 Jan 2007 09:48:07 -0600
Subject: [R] Use a text variable's value to specify another varaible?
Message-ID: <CA612484A337C6479EA341DF9EEE14AC05D7A05E@hercules.ssainfo>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070126/76e29aa7/attachment.pl 

From liuwensui at gmail.com  Fri Jan 26 16:55:36 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Fri, 26 Jan 2007 10:55:36 -0500
Subject: [R] how to create daily / weekly ts object?
Message-ID: <1115a2b00701260755r63c4d49cuee4e704f85f7f136@mail.gmail.com>

Dear All,

Monthly and Quarterly ts obj. is easy to understand. But I couldn't
find an example in R manual how to create daily or weekly ts object.
Could you please shed some light on it?
I really appreciate it.


From marc_schwartz at comcast.net  Fri Jan 26 17:00:49 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 26 Jan 2007 10:00:49 -0600
Subject: [R] Use a text variable's value to specify another varaible?
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05D7A05E@hercules.ssainfo>
References: <CA612484A337C6479EA341DF9EEE14AC05D7A05E@hercules.ssainfo>
Message-ID: <1169827249.4899.10.camel@localhost.localdomain>

On Fri, 2007-01-26 at 09:48 -0600, Ben Fairbank wrote:
> Greetings guRus --

> If a variable, e.g., 'varname', is a character string, e.g. varname <-
> "datavector", and I want to apply a function, such as table(), to
> datavector, what syntax or method will do so using only the variable
> varname?  This seems similar to indirect addressing, but I have not seen
> a method for it in the R manuals.  Is there a general name for such
> indirect reference that one might search for?

> (This came up while writing a function that takes the value of 'varname'
> from the keyboard and then applies functions to it.)

> With thanks for any solution,

> Ben Fairbank


See ?get and ?assign


datavector <- 1:10
varname <- "datavector"

> get(varname)
 [1]  1  2  3  4  5  6  7  8  9 10

> log10(get(varname))
 [1] 0.0000000 0.3010300 0.4771213 0.6020600 0.6989700 0.7781513
 [7] 0.8450980 0.9030900 0.9542425 1.0000000


and/or

assign(varname, letters)

> datavector
 [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q"
[18] "r" "s" "t" "u" "v" "w" "x" "y" "z"

> get(varname)
 [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q"
[18] "r" "s" "t" "u" "v" "w" "x" "y" "z"


HTH,

Marc Schwartz


From martin.sikora at upf.edu  Fri Jan 26 17:05:01 2007
From: martin.sikora at upf.edu (martin sikora)
Date: Fri, 26 Jan 2007 17:05:01 +0100
Subject: [R] strange behaviour with equality after simple subtraction
Message-ID: <001801c74163$bc5c6cd0$3c2491c1@upf.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070126/5e2b8160/attachment.pl 

From zhiliang.ma at gmail.com  Fri Jan 26 17:07:52 2007
From: zhiliang.ma at gmail.com (Zhiliang Ma)
Date: Fri, 26 Jan 2007 11:07:52 -0500
Subject: [R] enumerates all possible combinations
Message-ID: <b39377d10701260807t6599a965sa622529ad218a4ac@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070126/ed9af783/attachment.pl 

From knoblauch at lyon.inserm.fr  Fri Jan 26 17:13:01 2007
From: knoblauch at lyon.inserm.fr (ken knoblauch)
Date: Fri, 26 Jan 2007 17:13:01 +0100
Subject: [R]  enumerates all possible combinations
Message-ID: <d4d1b98e61d090e2efbea4876aab08c6@lyon.inserm.fr>

does expand.grid do what you want?

expand.grid(c(0, 1), c(0, 1), c(0, 1))
   Var1 Var2 Var3
1    0    0    0
2    1    0    0
3    0    1    0
4    1    1    0
5    0    0    1
6    1    0    1
7    0    1    1
8    1    1    1


> Hi all R users,
> I want to create a matrix having n columns and 2^n rows, and all its 
> entries
> are only 0 or 1. In each row, column i is 0 means dimension i is 
> chosen, 0
> means not.  The matrix will contains all the possible combination of 
> those n
> dimensions.
>
> Here is an example, if n = 3, the matrix will look like:
> 0 0 0
> 0 0 1
> 0 1 0
> 1 0 0
> 0 1 1
> 1 0 1
> 1 1 1
>
>
> I know I can use n "for" loops to do this, but is there a better way?
> Thanks,
> -zm
>
> 	[[alternative HTML version deleted]]
>
>


From sarah.goslee at gmail.com  Fri Jan 26 17:17:27 2007
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 26 Jan 2007 11:17:27 -0500
Subject: [R] unique/subset problem
In-Reply-To: <994290.60745.qm@web43122.mail.sp1.yahoo.com>
References: <cdf817830701251030y7eb7b14fqbd71d191232fee88@mail.gmail.com>
	<994290.60745.qm@web43122.mail.sp1.yahoo.com>
Message-ID: <efb536d50701260817h6929cea5w155c3e95c7bb1088@mail.gmail.com>

Without knowing more about your data, it is hard to say for certain,
but might you be confusing unique _values_ with _factor levels_?

> mydata <- as.factor(sort(rep(1:5, 2)))
# mydata has 10 values, 5 unique values, and 5 factor levels
> mydata
 [1] 1 1 2 2 3 3 4 4 5 5
Levels: 1 2 3 4 5
> unique(mydata)
[1] 1 2 3 4 5
Levels: 1 2 3 4 5
> mydata.subset <- mydata[1:4]
# the subset now has only 2 unique values, but the output
# still lists all five factor levels
> unique(mydata.subset)
[1] 1 2
Levels: 1 2 3 4 5

# try drop=TRUE as an option to subset
> mydata.subset <- mydata[1:4, drop=TRUE]
> unique(mydata.subset)
[1] 1 2
Levels: 1 2

Alternatively, if this is the problem and you don't need those
data to be factors, you could always convert them to a more
appropriate form.

Sarah

> > On 1/25/07, lalitha viswanath
> > <lalithaviswanath at yahoo.com> wrote:
> > > Hi
> > > I am new to R programming and am using subset to
> > > extract part of a data as follows
> > >
> > > names(dataset) =
> > > c("genome1","genome2","dist","score");
> > > prunedrelatives <- subset(dataset, score < -5);
> > >
> > > However when I use unique to find the number of
> > unique
> > > genomes now present in prunedrelatives I get
> > results
> > > identical to calling unique(dataset$genome1)
> > although
> > > subset has eliminated many genomes and records.
> > >
> > > I would greatly appreciate your input about using
> > > "unique" correctly  in this regard.
> > >
> > > Thanks
> > > Lalitha
> > >

-- 
Sarah Goslee
http://www.functionaldiversity.org


From iverson at biostat.wisc.edu  Fri Jan 26 17:19:21 2007
From: iverson at biostat.wisc.edu (Erik Iverson)
Date: Fri, 26 Jan 2007 10:19:21 -0600
Subject: [R] Use a text variable's value to specify another varaible?
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05D7A05E@hercules.ssainfo>
References: <CA612484A337C6479EA341DF9EEE14AC05D7A05E@hercules.ssainfo>
Message-ID: <45BA2A09.3040605@biostat.wisc.edu>

Can I ask why you aren't just passing in the object to your function, 
but instead a text name for that object?

Ben Fairbank wrote:
> Greetings guRus --
> 
>  
> 
> If a variable, e.g., 'varname', is a character string, e.g. varname <-
> "datavector", and I want to apply a function, such as table(), to
> datavector, what syntax or method will do so using only the variable
> varname?  This seems similar to indirect addressing, but I have not seen
> a method for it in the R manuals.  Is there a general name for such
> indirect reference that one might search for?
> 
>  
> 
> (This came up while writing a function that takes the value of 'varname'
> from the keyboard and then applies functions to it.)
> 
>  
> 
> With thanks for any solution,
> 
>  
> 
> Ben Fairbank
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Fri Jan 26 17:20:46 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 26 Jan 2007 11:20:46 -0500
Subject: [R] how to create daily / weekly ts object?
In-Reply-To: <1115a2b00701260755r63c4d49cuee4e704f85f7f136@mail.gmail.com>
References: <1115a2b00701260755r63c4d49cuee4e704f85f7f136@mail.gmail.com>
Message-ID: <971536df0701260820t4600b70dv365c132d8e5863e9@mail.gmail.com>

If you want the dates to show up as such you are better off using
a zooreg object with times of class "Date" from the zoo package.
You can always use as.ts on it if you require a ts object.

library(zoo)
zd <- zooreg(1:10, start = Sys.Date())
zd
as.ts(zd)
plot(zd)

zw <- zooreg(1:10, start = Sys.Date(), delta = 7)
zw
as.ts(zw)

plot(zw, type = "p")
points(zd, pch = "+")

vignette("zoo")



On 1/26/07, Wensui Liu <liuwensui at gmail.com> wrote:
> Dear All,
>
> Monthly and Quarterly ts obj. is easy to understand. But I couldn't
> find an example in R manual how to create daily or weekly ts object.
> Could you please shed some light on it?
> I really appreciate it.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From zhiliang.ma at gmail.com  Fri Jan 26 17:22:55 2007
From: zhiliang.ma at gmail.com (Zhiliang Ma)
Date: Fri, 26 Jan 2007 11:22:55 -0500
Subject: [R] enumerates all possible combinations
In-Reply-To: <d4d1b98e61d090e2efbea4876aab08c6@lyon.inserm.fr>
References: <d4d1b98e61d090e2efbea4876aab08c6@lyon.inserm.fr>
Message-ID: <b39377d10701260822t1b029a25k81aa2929dc183cf2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070126/f2e6d32e/attachment.pl 

From helprhelp at gmail.com  Fri Jan 26 17:26:30 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Fri, 26 Jan 2007 11:26:30 -0500
Subject: [R] unique/subset problem
In-Reply-To: <994290.60745.qm@web43122.mail.sp1.yahoo.com>
References: <cdf817830701251030y7eb7b14fqbd71d191232fee88@mail.gmail.com>
	<994290.60745.qm@web43122.mail.sp1.yahoo.com>
Message-ID: <cdf817830701260826v5cc89a22i9218614f6819ee32@mail.gmail.com>

Then you need to provide more details about the calls you made and your dataset.
For example, you can tell us by
str(prunedrelatives, 1)

how did you call unique on prunedrelative and so on? I made a test
data it gave me what you wanted (omitted here).

On 1/26/07, lalitha viswanath <lalithaviswanath at yahoo.com> wrote:
> Hi
> The pruned dataset has 8 unique genomes in it while
> the dataset before pruning has 65 unique genomes in
> it.
> However calling unique on the pruned dataset seems to
> return 65 no matter what.
>
> Any assistance in this matter would be appreciated.
>
> Thanks
> Lalitha
> --- Weiwei Shi <helprhelp at gmail.com> wrote:
>
> > Hi,
> >
> > Even you removed "many" genomes1 by setting score<
> > -5; it is not
> > necessary saying you changed the uniqueness.
> >
> > To check this, you can do like
> > p0 <- unique(dataset[dataset$score< -5, "genome1"])
> > # same as subset
> > p1 <- unique(dataset[dataset$score>= -5, "genome1"])
> >
> > setdiff(p1, p0)
> >
> > if the output above has NULL, then it means even
> > though you remove
> > many genomes1, but it does not help changing the
> > uniqueness.
> >
> > HTH,
> >
> > weiwei
> >
> >
> >
> > On 1/25/07, lalitha viswanath
> > <lalithaviswanath at yahoo.com> wrote:
> > > Hi
> > > I am new to R programming and am using subset to
> > > extract part of a data as follows
> > >
> > > names(dataset) =
> > > c("genome1","genome2","dist","score");
> > > prunedrelatives <- subset(dataset, score < -5);
> > >
> > > However when I use unique to find the number of
> > unique
> > > genomes now present in prunedrelatives I get
> > results
> > > identical to calling unique(dataset$genome1)
> > although
> > > subset has eliminated many genomes and records.
> > >
> > > I would greatly appreciate your input about using
> > > "unique" correctly  in this regard.
> > >
> > > Thanks
> > > Lalitha
> > >
> > >
> > >
> > >
> >
> ____________________________________________________________________________________
> > > TV dinner still cooling?
> > > Check out "Tonight's Picks" on Yahoo! TV.
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained,
> > reproducible code.
> > >
> >
> >
> > --
> > Weiwei Shi, Ph.D
> > Research Scientist
> > GeneGO, Inc.
> >
> > "Did you always know?"
> > "No, I did not. But I believed..."
> > ---Matrix III
> >
>
>
>
>
> ____________________________________________________________________________________
> Bored stiff? Loosen up...
> Download and play hundreds of games for free on Yahoo! Games.
> http://games.yahoo.com/games/front
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From mike.prager at noaa.gov  Fri Jan 26 17:40:34 2007
From: mike.prager at noaa.gov (Mike Prager)
Date: Fri, 26 Jan 2007 11:40:34 -0500
Subject: [R] strange behaviour with equality after simple subtraction
References: <001801c74163$bc5c6cd0$3c2491c1@upf.edu>
Message-ID: <mjbkr2tqg80p01u2g7i82hiebe3u4mb5h0@4ax.com>

"martin sikora" <martin.sikora at upf.edu> wrote:

> today while trying to extract data from a list for subsequent analysis, i
> stumbled upon this funny behavior on my system:
> 
> > x<-c(0.1,0.9)
> 
> > 1-x[2]
> 
> [1] 0.1
> 
> > x[1]
> 
> [1] 0.1
> 
> > x[1]==1-x[2]
> 
> [1] FALSE
> 
> > x[1]>1-x[2]
> 
> [1] TRUE
> 

Not at all strange, an expected property of floating-point
arithmetic and one of the most frequently asked questions here.

> print(0.1, digits=17)
[1] 0.1
> print(1 - 0.9, digits=17)
[1] 0.09999999999999998
> 

A simple description of the issue is at

http://docs.python.org/tut/node16.html

In most cases, it suffices to test for approximate difference or
relative difference. The former would look like this

if (abs(x[1] - x[2]) < eps)) ...
 
with "eps" set to something you think is an insignificant
difference, say 1.0e-10.


-- 
Mike Prager, NOAA, Beaufort, NC
* Opinions expressed are personal and not represented otherwise.
* Any use of tradenames does not constitute a NOAA endorsement.


From mkimpel at iupui.edu  Fri Jan 26 17:44:32 2007
From: mkimpel at iupui.edu (Kimpel, Mark William)
Date: Fri, 26 Jan 2007 11:44:32 -0500
Subject: [R] [BioC] problem with biomaRt getHomolog function
In-Reply-To: <45BA0F11.60306@mail.nih.gov>
References: <836F00680EECD340A96AD34ECFF3B534B4AACD@iu-mssg-mbx106.ads.iu.edu>
	<45BA0F11.60306@mail.nih.gov>
Message-ID: <836F00680EECD340A96AD34ECFF3B534B4AAFA@iu-mssg-mbx106.ads.iu.edu>

Steffen,

When the new biomaRt tries to load it errors out because I do not have
RMySQL installed. There is not a Windows binary for RMySQL and it does
contain C code that I do not know how to build.

I do not use the MySQL option in biomaRt. Does RMySQL need to be a
required dependency? Below is my screen output and sessionINfo.

require(biomaRt)
Loading required package: biomaRt
Loading required package: RMySQL
Error: package 'RMySQL' could not be loaded
In addition: Warning message:
there is no package called 'RMySQL' in: library(pkg, character.only =
TRUE, logical = TRUE, lib.loc = lib.loc) 
> sessionInfo()
R version 2.5.0 Under development (unstable) (2007-01-19 r40528) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
States.1252;LC_MONETARY=English_United
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] "stats"     "graphics"  "grDevices" "datasets"  "utils"     "tools"
"methods"   "base"     

other attached packages:
      DBI     limma      affy    affyio   Biobase 
 "0.1-12"   "2.9.8" "1.13.14"   "1.3.3" "1.13.34" 
>

Mark W. Kimpel MD 

 

(317) 490-5129 Work, & Mobile

 

(317) 663-0513 Home (no voice mail please)

1-(317)-536-2730 FAX


-----Original Message-----
From: Steffen Durinck [mailto:durincks at mail.nih.gov] 
Sent: Friday, January 26, 2007 9:24 AM
To: Kimpel, Mark William
Cc: bioconductor at stat.math.ethz.ch
Subject: Re: [BioC] problem with biomaRt getHomolog function

Hi Mark,

I think the rat entrezgene id 613226 is a recently added entrezgene id 
and is not yet available in Ensembl.  Ensembl updates every two months 
and the last update of entrezgene id 613226 appears to be December 26 of

2006.  So this might be the reason.
Also I would suggest you use the developmental version of biomaRt 
(biomaRt_1.9.15) to do getHomolog queries.  A recent change in the 
BioMart suite enables the biomaRt package to retrieve both the id you 
use to query and the ids in the result of the query. 

Here's an example:

rat.entrezgene.ID=c("24842","83502","24205")
mouse.mart <- useMart("ensembl","mmusculus_gene_ensembl")
rat.mart<- useMart("ensembl", "rnorvegicus_gene_ensembl")
mouse.homolog<-getHomolog(id =rat.entrezgene.ID, from.mart = 
rat.mart,from.type = "entrezgene",to.type="entrezgene",
to.mart=mouse.mart)

 > mouse.homolog
     V1    V2
1 24842 22059
2 24205 11789
3 24205    NA
4 83502 12550

best,
Steffen

Kimpel, Mark William wrote:
> I am trying to use the getHomolog function of package "biomaRt" to map
> rat entrezgene IDs to mouse entrezgene IDs. For every ID I try, I get
> NULL as return, even when I know that a mouse mapping exists.
>
>  
>
> For example, ratID "613226" corresponds to mouse "229706" .
>
>  
>
> See my code and sessionInfo below. Anyone know what I am doing wrong?
>
>  
>
> Thanks, Mark
>
>  
>
>   
>> require(DBI)
>>     
>
> [1] TRUE
>
>   
>> require(biomaRt)
>>     
>
> [1] TRUE
>
>   
>
>   
>> mouse.mart <- useMart("ensembl","mmusculus_gene_ensembl")
>>     
>
> Checking attributes and filters ... ok
>
>   
>
>   
>> rat.mart<- useMart("ensembl", "rnorvegicus_gene_ensembl")
>>     
>
> Checking attributes and filters ... ok
>
>   
>
>   
>> rat.entrezgene.ID<-"613226"
>>     
>
>   
>>     
>>     
>
>   
>> mouse.homolog<-getHomolog(id =rat.entrezgene.ID, from.mart =
rat.mart,
>>     
> from.type = "entrezgene", 
>
> +     to.type="entrezgene", to.mart=mouse.mart)
>
> Warning message:
>
> getBM returns NULL. in: getHomolog(id = rat.entrezgene.ID, from.mart =
> rat.mart, from.type = "entrezgene",  
>
>   
>
>  
>
>   
>> sessionInfo()
>>     
>
> R version 2.4.1 (2006-12-18) 
>
> i386-pc-mingw32 
>
>  
>
> locale:
>
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United
> States.1252;LC_MONETARY=English_United
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
>
>  
>
> attached base packages:
>
> [1] "stats"     "graphics"  "grDevices" "datasets"  "utils"
"tools"
> "methods"   "base"     
>
>  
>
> other attached packages:
>
>  biomaRt    RCurl      XML      DBI  RWinEdt    limma     affy
affyio
> Biobase 
>
>  "1.8.1"  "0.8-0"  "1.2-0" "0.1-12"  "1.7-5"  "2.9.1" "1.12.2"
"1.2.0"
> "1.12.2"
>
>  
>
> Mark W. Kimpel MD 
>
>  
>
>  
>
> Official Business Address:
>
>  
>
> Department of Psychiatry
>
> Indiana University School of Medicine
>
> PR M116
>
> Institute of Psychiatric Research
>
> 791 Union Drive
>
> Indianapolis, IN 46202
>
>  
>
> Preferred Mailing Address:
>
>  
>
> 15032 Hunter Court
>
> Westfield, IN  46074
>
>  
>
> (317) 490-5129 Work, & Mobile
>
>  
>
> (317) 663-0513 Home (no voice mail please)
>
> 1-(317)-536-2730 FAX
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> Bioconductor mailing list
> Bioconductor at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/bioconductor
> Search the archives:
http://news.gmane.org/gmane.science.biology.informatics.conductor
>   


-- 
Steffen Durinck, Ph.D.

Oncogenomics Section
Pediatric Oncology Branch
National Cancer Institute, National Institutes of Health
URL: http://home.ccr.cancer.gov/oncology/oncogenomics/

Phone: 301-402-8103
Address:
Advanced Technology Center,
8717 Grovemont Circle
Gaithersburg, MD 20877


From jrkrideau at yahoo.ca  Fri Jan 26 17:48:38 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 26 Jan 2007 11:48:38 -0500 (EST)
Subject: [R] spss.get.  Warning with SPSS 14 dataset
Message-ID: <62400.73161.qm@web32808.mail.mud.yahoo.com>

I am using spss.get to import an SPSS database
"Data.sav", created with SPSS 14  :

df1 <- spss.get("C:/temp/Data.sav" , lowernames=TRUE,
            datevars = c("dateinte"))

I am getting this warning. I get the same warning with
read.spss.

Warning message:
C:/temp/Data.sav: Unrecognized record type 7, subtype
16 encountered in system file 

This is a stupid question but should I be worried
about it?  So far the data looks clean but it is not
my data base originally and I wondered if there is
anything specific that I should be checking for.

Thanks.


From mrennie at utm.utoronto.ca  Fri Jan 26 17:50:57 2007
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Fri, 26 Jan 2007 11:50:57 -0500
Subject: [R] plotting results from tapply
Message-ID: <6.1.0.6.0.20070126113039.01c63c78@mail.utm.utoronto.ca>


Hi, there

I'm trying to plot what is returned from a call to tapply, and can't figure 
out how to do it. My guess is that it has something to do with the 
inclusion of row names when you ask for the values you're interested in, 
but if anyone has any ideas on how to get it to work, that would be 
stellar.  Here's some example code:

y1<-rnorm(40, 2)
x1<-rep(1:2, each=20)
x2<-rep(1:2, each=10 times=2)

ex.dat<-data.frame(cbind(y1,x1,x2))

ex.dat$x1<-factor(ex.dat$x1, labels=c("A", "B"))
ex.dat$x2<-factor(ex.dat$x2, labels=c("C", "D"))

attach(ex.dat)

xbar<-tapply(ex.dat$y1, ex.dat[,-1], mean)
xbar

#values I'd like to plot:
row.names(xbar) #levels of x1
xbar[,1] #mean response of y1 for group C (x2) across x1

#plot mean response y1 for group C against x1 (i.e., using x2 as a grouping 
factor).
plot(row.names(xbar), xbar[,1], ylim=range[y1])

#This is where things break down. The error message states that I need 
"finite xlim values" but I haven't assigned anything to xlim. If I just 
plot the data:

plot(x1, y1)

#This works fine.

#And, I can get this to work:

stripchart(xbar[1,]~row.names(xbar), vert=T)

#However, I'd like to then add the values for the second set of means 
(i.e., mean values for group D against x1, or (xbar[,2])) to the plot.
#I tried following up the previous command with:

points(row.names(xbar), xbar[,2])

#But that returns an error as well (NAs introduced by coercion).



Any suggestions?

Cheers,

Mike

PS- some of you might suggest for me to use interaction.plot, since this is 
essentially what I'm building here. True, but I can't get error bars using 
interaction.plot. I'm therefore trying to build my own version where I can 
specify the inclusion of error bars. Presumably the interaction.plot has 
figured out how to do what I'm attempting, so I have some faith that I am 
on the right track....





Michael Rennie
Ph.D. Candidate, University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga, ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792
www.utm.utoronto.ca/~w3rennie


From helprhelp at gmail.com  Fri Jan 26 17:58:50 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Fri, 26 Jan 2007 11:58:50 -0500
Subject: [R] enumerates all possible combinations
In-Reply-To: <b39377d10701260807t6599a965sa622529ad218a4ac@mail.gmail.com>
References: <b39377d10701260807t6599a965sa622529ad218a4ac@mail.gmail.com>
Message-ID: <cdf817830701260858k4d16afacoa2100f3e17275461@mail.gmail.com>

a little improvement on ken knoblauch's method by adding the variable n

so,

n <- 3
expand.grid(lapply(1:n, function(i) (c(0,1))))

HTH,

weiwei


On 1/26/07, Zhiliang Ma <zhiliang.ma at gmail.com> wrote:
> Hi all R users,
> I want to create a matrix having n columns and 2^n rows, and all its entries
> are only 0 or 1. In each row, column i is 0 means dimension i is chosen, 0
> means not.  The matrix will contains all the possible combination of those n
> dimensions.
>
> Here is an example, if n = 3, the matrix will look like:
> 0 0 0
> 0 0 1
> 0 1 0
> 1 0 0
> 0 1 1
> 1 0 1
> 1 1 1
>
>
> I know I can use n "for" loops to do this, but is there a better way?
> Thanks,
> -zm
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From c.beale at macaulay.ac.uk  Fri Jan 26 18:01:46 2007
From: c.beale at macaulay.ac.uk (Colin Beale)
Date: Fri, 26 Jan 2007 17:01:46 +0000
Subject: [R] R crash with modified lmer code
Message-ID: <45BA33FA.1E3C.0035.0@macaulay.ac.uk>

Hi all,

I've now got a problem with some modified lmer code (function lmer1
pasted at end) - I've made only three changes to the lmer code (marked),
and I'm not really looking for comments on this function, but would like
to know why execution of the following commands that use it almost
invariably (but not quite predictably) leads to the R session
terminating.

Here's the command that almost always (if not the first time its run,
then certainly the second time) crashes R:

library(lme4)
source("M:/Rcode/lmer2.R") ##(Simply sourcing the modified lmer1 code
below)
set.seed(1)
dat <- data.frame(Y = rnorm(400), X1 = rnorm(400), X2 = rnorm(400),
         F1 = as.factor(sample(1:4, 400, replace = T)))
forSm <- Matrix(c(runif(1200)), ncol = 3, sparse = T)
matDim <- dim(forSm)
smoother <- as.factor(sample(1:4, matDim[1], replace = T))
test <- lmer1 (Y ~ X1 + X2 + (1|F1) + (1|smoother), data = dat,
rand_mat = forSm)


the sessionInfo (called after sourcing the code but before the crash)
is:

R version 2.4.1 (2006-12-18) 
i386-pc-mingw32 

locale:
LC_COLLATE=English_United Kingdom.1252;LC_CTYPE=English_United
Kingdom.1252;LC_MONETARY=English_United
Kingdom.1252;LC_NUMERIC=C;LC_TIME=English_United Kingdom.1252

attached base packages:
[1] "stats"     "graphics"  "grDevices" "datasets"  "tcltk"     "utils"
    "methods"   "base"     

other attached packages:
       lme4      Matrix     lattice    svSocket        svIO      R2HTML
     svMisc       svIDE 
"0.9975-10"  "0.9975-8"   "0.14-16"     "0.9-5"     "0.9-5"      "1.58"
    "0.9-5"     "0.9-5" 



and the modified lmer code (sourced in the above code) is:

lmer1 <- function (formula, data, family = gaussian, method = c("REML",

    "ML", "PQL", "Laplace", "AGQ"), control = list(), start = NULL, 
    subset, weights, na.action, offset, contrasts = NULL, model = TRUE,
 
    matDimI = matDim, rand_matI = rand_mat, ...) 
{

    method <- match.arg(method)
    formula <- as.formula(formula)
    if (length(formula) < 3) 
        stop("formula must be a two-sided formula")
    cv <- do.call("lmerControl", control)
    mc <- match.call()
    fr <- lmerFrames(mc, formula, data, contrasts)
    if (matDimI[1] != length(fr$Y))
        stop("forSmoothing is not a sensible length")                 
#insert
    Y <- fr$Y
    X <- fr$X
    weights <- fr$weights
    offset <- fr$offset
    mf <- fr$mf
    mt <- fr$mt
    if (is.character(family)) 
        family <- get(family, mode = "function", envir =
parent.frame())
    if (is.function(family)) 
        family <- family()
    if (is.null(family$family)) {
        print(family)
        stop("'family' not recognized")
    }
    fltype <- mkFltype(family)
    FL <- lmerFactorList(formula, mf, fltype)
    nFacs <- length(FL)                                          #My
insert
    cnames <- with(FL, c(lapply(Ztl, rownames), list(.fixed =
colnames(X))))
    nc <- with(FL, sapply(Ztl, nrow))
    Ztl <- with(FL, .Call(Ztl_sparse, fl, Ztl))
    Ztl[[nFacs]] <- t(rand_matI)                                  #My
insert
    Zt <- if (length(Ztl) == 1) 
        Ztl[[1]]
    else do.call("rbind", Ztl)
    fl <- FL$fl
    if (fltype < 0) {
        mer <- .Call(mer_create, fl, Zt, X, as.double(Y), method == 
            "REML", nc, cnames)
        if (!is.null(start)) 
            mer <- setOmega(mer, start)
        .Call(mer_ECMEsteps, mer, cv$niterEM, cv$EMverbose)
        LMEoptimize(mer) <- cv
        return(new("lmer", mer, frame = if (model) fr$mf else
data.frame(), 
            terms = mt, call = mc))
    }
    if (method %in% c("ML", "REML")) 
        method <- "Laplace"
    if (method == "AGQ") 
        stop("method = \"AGQ\" not yet implemented for supernodal
representation")
    if (method == "PQL") 
        cv$usePQL <- TRUE
    glmFit <- glm.fit(X, Y, weights = weights, offset = offset, 
        family = family, intercept = attr(mt, "intercept") > 
            0)
    Y <- as.double(glmFit$y)
    mer <- .Call(mer_create, fl, Zt, X, Y, 0, nc, cnames)
    if (!is.null(start)) 
        mer <- setOmega(mer, start)
    gVerb <- getOption("verbose")
    weights <- glmFit$prior.weights
    eta <- glmFit$linear.predictors
    linkinv <- quote(family$linkinv(eta))
    mu.eta <- quote(family$mu.eta(eta))
    mu <- family$linkinv(eta)
    variance <- quote(family$variance(mu))
    dev.resids <- quote(family$dev.resids(Y, mu, weights))
    LMEopt <- get("LMEoptimize<-")
    doLMEopt <- quote(LMEopt(x = mer, value = cv))
    if (family$family %in% c("binomial", "poisson")) 
        mer at devComp[8] <- -1
    mer at status["glmm"] <- as.integer(switch(method, PQL = 1, 
        Laplace = 2, AGQ = 3))
    GSpt <- .Call(glmer_init, environment(), fltype)
    if (cv$usePQL) {
        .Call(glmer_PQL, GSpt)
        PQLpars <- c(fixef(mer), .Call(mer_coef, mer, 2))
    }
    else {
        PQLpars <- c(coef(glmFit), .Call(mer_coef, mer, 2))
    }
    if (method == "PQL") {
        .Call(glmer_devLaplace, PQLpars, GSpt)
        .Call(glmer_finalize, GSpt)
        return(new("glmer", new("lmer", mer, frame = if (model) mf else
data.frame(), 
            terms = mt, call = match.call()), weights = weights, 
            family = family))
    }
    fixInd <- seq(ncol(X))
    const <- c(rep(FALSE, length(fixInd)),
unlist(lapply(mer at nc[seq(along = fl)], 
        function(k) 1:((k * (k + 1))/2) <= k)))
    devLaplace <- function(pars) .Call(glmer_devLaplace, pars, 
        GSpt)
    rel.tol <- abs(0.01/devLaplace(PQLpars))
    if (cv$msVerbose) 
        cat(paste("relative tolerance set to", rel.tol, "\n"))
    optimRes <- nlminb(PQLpars, devLaplace, lower = ifelse(const, 
        5e-10, -Inf), control = list(trace = cv$msVerbose, iter.max =
cv$msMaxIter, 
        rel.tol = rel.tol))
    .Call(glmer_finalize, GSpt)
    new("glmer", new("lmer", mer, frame = if (model) 
        mf
    else data.frame(), terms = mt, call = match.call()), weights =
weights, 
        family = family)
}
environment(lmer1) <- environment(lmer)


Thanks,

Colin



Dr. Colin Beale
Spatial Ecologist
The Macaulay Institute
Craigiebuckler
Aberdeen
AB15 8QH
UK

Tel: 01224 498245 ext. 2427
Fax: 01224 311556
Email: c.beale at macaulay.ac.uk 



-- 
Please note that the views expressed in this e-mail are those of the
sender and do not necessarily represent the views of the Macaulay
Institute. This email and any attachments are confidential and are
intended solely for the use of the recipient(s) to whom they are
addressed. If you are not the intended recipient, you should not read,
copy, disclose or rely on any information contained in this e-mail, and
we would ask you to contact the sender immediately and delete the email
from your system. Thank you.
Macaulay Institute and Associated Companies, Macaulay Drive,
Craigiebuckler, Aberdeen, AB15 8QH.


From bcarvalh at jhsph.edu  Fri Jan 26 18:15:27 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Fri, 26 Jan 2007 12:15:27 -0500
Subject: [R] strange behaviour with equality after simple subtraction
In-Reply-To: <mjbkr2tqg80p01u2g7i82hiebe3u4mb5h0@4ax.com>
References: <001801c74163$bc5c6cd0$3c2491c1@upf.edu>
	<mjbkr2tqg80p01u2g7i82hiebe3u4mb5h0@4ax.com>
Message-ID: <7031625D-9ED2-4CE7-8A85-E330623628FD@jhsph.edu>

In addition to Mike's comment:

 > x<-c(0.1,0.9)
 > 1-x[2]
[1] 0.1
 > x[1]==1-x[2]
[1] FALSE
 > all.equal(x[1], 1-x[2])
[1] TRUE

b

On Jan 26, 2007, at 11:40 AM, Mike Prager wrote:

> Not at all strange, an expected property of floating-point
> arithmetic and one of the most frequently asked questions here.
>
>> print(0.1, digits=17)
> [1] 0.1
>> print(1 - 0.9, digits=17)
> [1] 0.09999999999999998
>>
>
> A simple description of the issue is at
>
> http://docs.python.org/tut/node16.html
>
> In most cases, it suffices to test for approximate difference or
> relative difference. The former would look like this
>
> if (abs(x[1] - x[2]) < eps)) ...
>
> with "eps" set to something you think is an insignificant
> difference, say 1.0e-10.
>
>
> --  
> Mike Prager, NOAA, Beaufort, NC
> * Opinions expressed are personal and not represented otherwise.
> * Any use of tradenames does not constitute a NOAA endorsement.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From gunter.berton at gene.com  Fri Jan 26 18:16:44 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Fri, 26 Jan 2007 09:16:44 -0800
Subject: [R] strange behaviour with equality after simple subtraction
In-Reply-To: <mjbkr2tqg80p01u2g7i82hiebe3u4mb5h0@4ax.com>
Message-ID: <000c01c7416d$c16dd5b0$4d908980@gne.windows.gene.com>

FAQ on R 7.31. ?all.equal   ?identical  

Have you read these?

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mike Prager
Sent: Friday, January 26, 2007 8:41 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] strange behaviour with equality after simple subtraction

"martin sikora" <martin.sikora at upf.edu> wrote:

> today while trying to extract data from a list for subsequent analysis, i
> stumbled upon this funny behavior on my system:
> 
> > x<-c(0.1,0.9)
> 
> > 1-x[2]
> 
> [1] 0.1
> 
> > x[1]
> 
> [1] 0.1
> 
> > x[1]==1-x[2]
> 
> [1] FALSE
> 
> > x[1]>1-x[2]
> 
> [1] TRUE
> 

Not at all strange, an expected property of floating-point
arithmetic and one of the most frequently asked questions here.

> print(0.1, digits=17)
[1] 0.1
> print(1 - 0.9, digits=17)
[1] 0.09999999999999998
> 

A simple description of the issue is at

http://docs.python.org/tut/node16.html

In most cases, it suffices to test for approximate difference or
relative difference. The former would look like this

if (abs(x[1] - x[2]) < eps)) ...
 
with "eps" set to something you think is an insignificant
difference, say 1.0e-10.


-- 
Mike Prager, NOAA, Beaufort, NC
* Opinions expressed are personal and not represented otherwise.
* Any use of tradenames does not constitute a NOAA endorsement.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at comcast.net  Fri Jan 26 18:46:23 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 26 Jan 2007 11:46:23 -0600
Subject: [R] plotting results from tapply
In-Reply-To: <6.1.0.6.0.20070126113039.01c63c78@mail.utm.utoronto.ca>
References: <6.1.0.6.0.20070126113039.01c63c78@mail.utm.utoronto.ca>
Message-ID: <1169833583.4899.33.camel@localhost.localdomain>

On Fri, 2007-01-26 at 11:50 -0500, Michael Rennie wrote:
> Hi, there
> 
> I'm trying to plot what is returned from a call to tapply, and can't figure 
> out how to do it. My guess is that it has something to do with the 
> inclusion of row names when you ask for the values you're interested in, 
> but if anyone has any ideas on how to get it to work, that would be 
> stellar.  Here's some example code:
> 
> y1<-rnorm(40, 2)
> x1<-rep(1:2, each=20)
> x2<-rep(1:2, each=10 times=2)
> 
> ex.dat<-data.frame(cbind(y1,x1,x2))
> 
> ex.dat$x1<-factor(ex.dat$x1, labels=c("A", "B"))
> ex.dat$x2<-factor(ex.dat$x2, labels=c("C", "D"))
> 
> attach(ex.dat)
> 
> xbar<-tapply(ex.dat$y1, ex.dat[,-1], mean)
> xbar
> 
> #values I'd like to plot:
> row.names(xbar) #levels of x1
> xbar[,1] #mean response of y1 for group C (x2) across x1
> 
> #plot mean response y1 for group C against x1 (i.e., using x2 as a grouping 
> factor).
> plot(row.names(xbar), xbar[,1], ylim=range[y1])
> 
> #This is where things break down. The error message states that I need 
> "finite xlim values" but I haven't assigned anything to xlim. If I just 
> plot the data:
> 
> plot(x1, y1)
> 
> #This works fine.
> 
> #And, I can get this to work:
> 
> stripchart(xbar[1,]~row.names(xbar), vert=T)
> 
> #However, I'd like to then add the values for the second set of means 
> (i.e., mean values for group D against x1, or (xbar[,2])) to the plot.
> #I tried following up the previous command with:
> 
> points(row.names(xbar), xbar[,2])
> 
> #But that returns an error as well (NAs introduced by coercion).
> 
> 
> 
> Any suggestions?
> 
> Cheers,
> 
> Mike
> 
> PS- some of you might suggest for me to use interaction.plot, since this is 
> essentially what I'm building here. True, but I can't get error bars using 
> interaction.plot. I'm therefore trying to build my own version where I can 
> specify the inclusion of error bars. Presumably the interaction.plot has 
> figured out how to do what I'm attempting, so I have some faith that I am 
> on the right track....

Michael,

The problem is that when you are using the rownames for 'xbar', they are
a character vector:

> str(rownames(xbar))
 chr [1:2] "A" "B

When you attempt to use the same values from 'ex.dat$x1', they are
factors, which are being coerced to their numeric equivalents, so that
they can work as x axis values.

> str(ex.dat$x1)
 Factor w/ 2 levels "A","B": 1 1 1 1 1 1 1 1 1 1 ...

> as.numeric(ex.dat$x1)
 [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2
[35] 2 2 2 2 2 2



I might note using the following:

par(mfcol = c(1, 3))

with(ex.dat, plot(1:2, xbar[, 1], ylim = range(y1),
                  type = "b", col = "red",
                  lty = c("dashed", "solid"),
                  xaxt = "n", xlab = "x1",
                  ylab = "mean of y1"))

with(ex.dat, points(1:2, xbar[, 2], col = "blue",
                    type = "b"))

axis(1, at = 1:2, labels = c("A", "B"))


matplot(1:2, xbar, col = c("red", "blue"),
        pch = 21, type = "b", ylim = range(y1),
        lty = c("dashed", "solid"),
        xaxt = "n", xlab = "x1",
        ylab = "mean of y1")

axis(1, at = 1:2, labels = c("A", "B"))


with(ex.dat, interaction.plot(x1, x2, response = y1, 
                              type = "b", pch = 21,
                              col = c("red", "blue"),
                              ylim = range(ex.dat$y1),
                              legend = FALSE))



You get basically the same 3 plots, with the principal difference in
interaction.plot() being a different x axis range.

Using interaction.plot(), you can get the proper basic plot and then add
the CI's if you wish, since you know the x axis values of the mean
estimates, which is 1:NumberOfGroups (as in a boxplot).

interaction.plot() actually uses matplot() internally.

HTH,

Marc Schwartz


From josh.kalish at credit-suisse.com  Fri Jan 26 18:39:29 2007
From: josh.kalish at credit-suisse.com (Kalish, Josh)
Date: Fri, 26 Jan 2007 12:39:29 -0500
Subject: [R] Using tapply to create a new table
Message-ID: <FB9A739CA1DD3F4E81BF65BE4C6B0836458A4DA6@enyc19p32002.corpny.csfb.com>

All,

I'm sure that this is covered somewhere, but I can't seem to find a good explanation.  I have an existing table that contains information grouped by date.  This is as so:

Day		NumberOfCustomers		NumberOfComplaints
20060512	10040				40
20060513	32420				11
...


I also have a table at the detail level as so:

Day		Meal		PricePaid	UsedCupon
20060512	Fish		14		Y
20060512	Chicken	20		N
...

Is there a simple way to create summaries on the detail table and then join them into the first table above so that it looks like this:

Day		NumberOfCustomers		NumberOfComplaints	AveragePricePaid		NumberUsingCupon


I can do a tapply to get what I want from the detail table, but I can't figure out how to turn that into a table and join it back in.



Thanks,

Josh 

==============================================================================
Please access the attached hyperlink for an important electr...{{dropped}}


From marc_schwartz at comcast.net  Fri Jan 26 19:08:22 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 26 Jan 2007 12:08:22 -0600
Subject: [R] Using tapply to create a new table
In-Reply-To: <FB9A739CA1DD3F4E81BF65BE4C6B0836458A4DA6@enyc19p32002.corpny.csfb.com>
References: <FB9A739CA1DD3F4E81BF65BE4C6B0836458A4DA6@enyc19p32002.corpny.csfb.com>
Message-ID: <1169834902.4899.39.camel@localhost.localdomain>

On Fri, 2007-01-26 at 12:39 -0500, Kalish, Josh wrote:
> All,
> 
> I'm sure that this is covered somewhere, but I can't seem to find a
> good explanation.  I have an existing table that contains information
> grouped by date.  This is as so:
> 
> Day		NumberOfCustomers		NumberOfComplaints
> 20060512	10040				40
> 20060513	32420				11
> ...
> 
> 
> I also have a table at the detail level as so:
> 
> Day		Meal		PricePaid	UsedCupon
> 20060512	Fish		14		Y
> 20060512	Chicken	20		N
> ...
> 
> Is there a simple way to create summaries on the detail table and then
> join them into the first table above so that it looks like this:
> 
> Day		NumberOfCustomers		NumberOfComplaints	AveragePricePaid
> NumberUsingCupon
> 
> 
> I can do a tapply to get what I want from the detail table, but I
> can't figure out how to turn that into a table and join it back in.
> 
> 
> 
> Thanks,
> 
> Josh 

Skipping the steps of using tapply() or aggregate() to get the
summarized data from the second data frame, you would then use merge()
to perform a SQL-like 'join' operation:

> DF.1
       Day NumberOfCustomers NumberOfComplaints
1 20060512             10040                 40
2 20060513             32420                 11

> DF.2
       Day    Meal PricePaid UsedCupon
1 20060512    Fish        14         Y
2 20060512 Chicken        20         N

> merge(DF.1, DF.2, by = "Day")
       Day NumberOfCustomers NumberOfComplaints    Meal PricePaid
1 20060512             10040                 40    Fish        14
2 20060512             10040                 40 Chicken        20
  UsedCupon
1         Y
2         N


By default, only rows matching on the 'by' argument in both data frames
will be in the result. See the 'all.x' and 'all.y' arguments to handle
other scenarios of including non-matching rows.

See ?merge, which BTW:

  help.search("join")

would point you to, if you are familiar with the term from relational
data base operations.

HTH,

Marc Schwartz


From josh.kalish at credit-suisse.com  Fri Jan 26 19:21:18 2007
From: josh.kalish at credit-suisse.com (Kalish, Josh)
Date: Fri, 26 Jan 2007 13:21:18 -0500
Subject: [R] Using tapply to create a new table
Message-ID: <FB9A739CA1DD3F4E81BF65BE4C6B0836458A4DB1@enyc19p32002.corpny.csfb.com>

Marc, 

Thanks for pointing out the merge function.  That gets me part of the way there.  The only thing is that I can't get the tapply() results into a format that merge() will take.  For example:

merge( set1 , tapply( set2$f1 , set2$commonField, mean ) , by="commonField" )

Gives me  "Error in names... Unused arguments..."

I'm not sure what the result of a tapply() exactly is, but it doesn't seem to be a table.

Yeah, rank amateur questions...

Thanks,

Josh

-----Original Message-----
From: Marc Schwartz [mailto:marc_schwartz at comcast.net] 
Sent: Friday, January 26, 2007 1:08 PM
To: Kalish, Josh
Cc: 'r-help at stat.math.ethz.ch'
Subject: Re: [R] Using tapply to create a new table

On Fri, 2007-01-26 at 12:39 -0500, Kalish, Josh wrote:
> All,
> 
> I'm sure that this is covered somewhere, but I can't seem to find a 
> good explanation.  I have an existing table that contains information 
> grouped by date.  This is as so:
> 
> Day		NumberOfCustomers		NumberOfComplaints
> 20060512	10040				40
> 20060513	32420				11
> ...
> 
> 
> I also have a table at the detail level as so:
> 
> Day		Meal		PricePaid	UsedCupon
> 20060512	Fish		14		Y
> 20060512	Chicken	20		N
> ...
> 
> Is there a simple way to create summaries on the detail table and then 
> join them into the first table above so that it looks like this:
> 
> Day		NumberOfCustomers		NumberOfComplaints	AveragePricePaid
> NumberUsingCupon
> 
> 
> I can do a tapply to get what I want from the detail table, but I 
> can't figure out how to turn that into a table and join it back in.
> 
> 
> 
> Thanks,
> 
> Josh

Skipping the steps of using tapply() or aggregate() to get the summarized data from the second data frame, you would then use merge() to perform a SQL-like 'join' operation:

> DF.1
       Day NumberOfCustomers NumberOfComplaints
1 20060512             10040                 40
2 20060513             32420                 11

> DF.2
       Day    Meal PricePaid UsedCupon
1 20060512    Fish        14         Y
2 20060512 Chicken        20         N

> merge(DF.1, DF.2, by = "Day")
       Day NumberOfCustomers NumberOfComplaints    Meal PricePaid
1 20060512             10040                 40    Fish        14
2 20060512             10040                 40 Chicken        20
  UsedCupon
1         Y
2         N


By default, only rows matching on the 'by' argument in both data frames will be in the result. See the 'all.x' and 'all.y' arguments to handle other scenarios of including non-matching rows.

See ?merge, which BTW:

  help.search("join")

would point you to, if you are familiar with the term from relational data base operations.

HTH,

Marc Schwartz



==============================================================================
Please access the attached hyperlink for an important electr...{{dropped}}


From marc_schwartz at comcast.net  Fri Jan 26 19:47:04 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 26 Jan 2007 12:47:04 -0600
Subject: [R] Using tapply to create a new table
In-Reply-To: <FB9A739CA1DD3F4E81BF65BE4C6B0836458A4DB1@enyc19p32002.corpny.csfb.com>
References: <FB9A739CA1DD3F4E81BF65BE4C6B0836458A4DB1@enyc19p32002.corpny.csfb.com>
Message-ID: <1169837224.4899.49.camel@localhost.localdomain>

Josh,

As per the "Value" section of ?tapply, it "returns a single atomic value
for each cell".  This is easily viewed by using:

  tapply(set2$f1, set2$commonField, mean)

in a stand alone fashion or:

  str(tapply(set2$f1, set2$commonField, mean))

which will display the internal structure of the result. See ?str

To use merge() in the fashion you seem to want, you would want to use
aggregate() and not tapply().  The former returns a data frame, where
the "key" or "by" values will be part of the output.

See ?aggregate for more information.

I would also recommend that both for the readability of the code you
write and to help clarify for yourself, the objects that are returned
from each step, that you not nest the function calls as you have below.

There are times when it makes sense, but there are times when the code
would end up being a good candidate for an Obfuscated R contest.  :-)

HTH,

Marc

On Fri, 2007-01-26 at 13:21 -0500, Kalish, Josh wrote:
> Marc, 
> 
> Thanks for pointing out the merge function.  That gets me part of the
> way there.  The only thing is that I can't get the tapply() results
> into a format that merge() will take.  For example:
> 
> merge( set1 , tapply( set2$f1 , set2$commonField, mean ) ,
> by="commonField" )
> 
> Gives me  "Error in names... Unused arguments..."
> 
> I'm not sure what the result of a tapply() exactly is, but it doesn't
> seem to be a table.
> 
> Yeah, rank amateur questions...
> 
> Thanks,
> 
> Josh
> 
> -----Original Message-----
> From: Marc Schwartz [mailto:marc_schwartz at comcast.net] 
> Sent: Friday, January 26, 2007 1:08 PM
> To: Kalish, Josh
> Cc: 'r-help at stat.math.ethz.ch'
> Subject: Re: [R] Using tapply to create a new table
> 
> On Fri, 2007-01-26 at 12:39 -0500, Kalish, Josh wrote:
> > All,
> > 
> > I'm sure that this is covered somewhere, but I can't seem to find a 
> > good explanation.  I have an existing table that contains
> information 
> > grouped by date.  This is as so:
> > 
> > Day		NumberOfCustomers		NumberOfComplaints
> > 20060512	10040				40
> > 20060513	32420				11
> > ...
> > 
> > 
> > I also have a table at the detail level as so:
> > 
> > Day		Meal		PricePaid	UsedCupon
> > 20060512	Fish		14		Y
> > 20060512	Chicken	20		N
> > ...
> > 
> > Is there a simple way to create summaries on the detail table and
> then 
> > join them into the first table above so that it looks like this:
> > 
> > Day		NumberOfCustomers		NumberOfComplaints	AveragePricePaid
> > NumberUsingCupon
> > 
> > 
> > I can do a tapply to get what I want from the detail table, but I 
> > can't figure out how to turn that into a table and join it back in.
> > 
> > 
> > 
> > Thanks,
> > 
> > Josh
> 
> Skipping the steps of using tapply() or aggregate() to get the
> summarized data from the second data frame, you would then use merge()
> to perform a SQL-like 'join' operation:
> 
> > DF.1
>        Day NumberOfCustomers NumberOfComplaints
> 1 20060512             10040                 40
> 2 20060513             32420                 11
> 
> > DF.2
>        Day    Meal PricePaid UsedCupon
> 1 20060512    Fish        14         Y
> 2 20060512 Chicken        20         N
> 
> > merge(DF.1, DF.2, by = "Day")
>        Day NumberOfCustomers NumberOfComplaints    Meal PricePaid
> 1 20060512             10040                 40    Fish        14
> 2 20060512             10040                 40 Chicken        20
>   UsedCupon
> 1         Y
> 2         N
> 
> 
> By default, only rows matching on the 'by' argument in both data
> frames will be in the result. See the 'all.x' and 'all.y' arguments to
> handle other scenarios of including non-matching rows.
> 
> See ?merge, which BTW:
> 
>   help.search("join")
> 
> would point you to, if you are familiar with the term from relational
> data base operations.
> 
> HTH,
> 
> Marc Schwartz


From topkatz at msn.com  Fri Jan 26 20:21:06 2007
From: topkatz at msn.com (Talbot Katz)
Date: Fri, 26 Jan 2007 14:21:06 -0500
Subject: [R] Residual variance from rlm?
Message-ID: <BAY132-F25ACA675B6A16B700FB5F3AAA20@phx.gbl>

Hi.

This is a real basic question about results from rlm.  I want to compute the 
properly scaled residual variance.

Suppose M is my rlm result object; my example regression is against two 
variables, and based on 225 observations.
summary(M) tells me that
"Residual standard error: 0.0009401 on 222 degrees of freedom"
which I presume is the same result as
summary(M)$sigma:	0.0009401223
Then, summary(M)$sigma^2:	8.8383e-07

Is the value of summary(M)$sigma^2 the proper residual variance?  If so, I'd 
like to be able to replicate that from M$wresid and M$w, but I haven't been 
able to.  For example,
var(M$wresid*M$w) = sum((M$wresid*M$w)^2)/224		6.350269e-07
mean(M$wresid^2*M$w) = sum(M$wresid^2*M$w)/225		9.45235e-07
Note that sum(M$w)		205.8032
I was disappointed to find that M$df.residual was NA; however, summary(M)$df 
does return a vector:	3 222   3

I have tried a bunch of other combinations of M$wresid and M$w, but nothing 
I've tried comes out the same as summary(M)$sigma^2.

Again, is summary(M)$sigma^2 the proper residual variance?  If yes, can it 
be replicated from the M object?  If no, can I compute the proper value from 
the M object?

Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell


From christoph.heibl at gmx.net  Fri Jan 26 20:23:26 2007
From: christoph.heibl at gmx.net (Christoph Heibl)
Date: Fri, 26 Jan 2007 20:23:26 +0100
Subject: [R] =?iso-8859-1?q?Why_do_return_or_visible_don=B4t_return_my_obj?=
 =?iso-8859-1?q?ekt=3F?=
Message-ID: <BFB2D6E1-670E-43AC-BEF1-0F4AB24CD116@gmx.net>

Dear RRRRRrrrrrrrrlist!

I?ve got two lists which contain sets of DNA-sequences. They look  
something like this:

	List of 33
	$ Cunonia_atrorubens                 : chr [1:247] "t" "t" "n" "t" ...
	$ Cunonia_balansae                   : chr [1:254] "t" "c" "c" "c" ...
	$ Cunonia_capensis                   : chr [1:236] "v" "t" "c" "c" ...
	$ Cunonia_macrophylla                : chr [1:236] "t" "t" "a" "t" ...
	$ Pancheria_engleriana               : chr [1:315] "c" "t" "c" "c" ...
	[....]
	$ Brunellia_colombiana               : chr [1:336] "t" "t" "c" "t" ...
	$ Brunellia_oliveri                  : chr [1:368] "t" "a" "a" "t" ...
	$ Bauera_rubioides                   : chr [1:276] "t" "c" "t" "c" ...
	$ Bauera_sessiliflora                : chr [1:332] "t" "c" "t" "c" ...
	$ Davidsonia_pruriens_var._jeryseyana: chr [1:300] "t" "t" "c" "t" ...
	$ Davidsonia_sp._'johnsonii'         : chr [1:268] "c" "c" "c" "t" ...
	- attr(*, "species")= chr [1:33] "Cunonia_atrorubens"  
"Cunonia_balansae" "Cunonia_capensis" "Cunonia_macrophylla" ...


The names of each entry in first list match exactly the names of the  
entries in the second list and both list contain the same number of  
sequences (although not of the same length)
I want to concatenate these sequences by the following function:

	interleave <- function(seq1, seq2)
	{
		#create a list to contain concatenated sequences
		sequence <- list()			
		for (name in names(seq1)) 	# get names as indices
			{
			# concatenate sequences
			sequence[[name]] <- c(seq1[[name]], seq2[[name]])
			}
	# return concatenated sequences
	invisible(sequence)	
	}

I guess the function works in principle, because they commands do so  
if I enter them step by step.
But when I call the function it does not return my object "sequence".  
But why?
(This happened to me with some functions that I wrote, but not all.  
The error seems quite erratic : ) to me and I can?t figure out what  
to change)

Thanks a lot in advance!

Christoph


From marc_schwartz at comcast.net  Fri Jan 26 21:16:56 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 26 Jan 2007 14:16:56 -0600
Subject: [R]
	=?iso-8859-1?q?Why_do_return_or_visible_don=B4t_return=09my_o?=
	=?iso-8859-1?q?bjekt=3F?=
In-Reply-To: <BFB2D6E1-670E-43AC-BEF1-0F4AB24CD116@gmx.net>
References: <BFB2D6E1-670E-43AC-BEF1-0F4AB24CD116@gmx.net>
Message-ID: <1169842616.4899.69.camel@localhost.localdomain>

On Fri, 2007-01-26 at 20:23 +0100, Christoph Heibl wrote:
> Dear RRRRRrrrrrrrrlist!

Ruh-Ro....

;-)

> I?ve got two lists which contain sets of DNA-sequences. They look  
> something like this:
> 
> 	List of 33
> 	$ Cunonia_atrorubens                 : chr [1:247] "t" "t" "n" "t" ...
> 	$ Cunonia_balansae                   : chr [1:254] "t" "c" "c" "c" ...
> 	$ Cunonia_capensis                   : chr [1:236] "v" "t" "c" "c" ...
> 	$ Cunonia_macrophylla                : chr [1:236] "t" "t" "a" "t" ...
> 	$ Pancheria_engleriana               : chr [1:315] "c" "t" "c" "c" ...
> 	[....]
> 	$ Brunellia_colombiana               : chr [1:336] "t" "t" "c" "t" ...
> 	$ Brunellia_oliveri                  : chr [1:368] "t" "a" "a" "t" ...
> 	$ Bauera_rubioides                   : chr [1:276] "t" "c" "t" "c" ...
> 	$ Bauera_sessiliflora                : chr [1:332] "t" "c" "t" "c" ...
> 	$ Davidsonia_pruriens_var._jeryseyana: chr [1:300] "t" "t" "c" "t" ...
> 	$ Davidsonia_sp._'johnsonii'         : chr [1:268] "c" "c" "c" "t" ...
> 	- attr(*, "species")= chr [1:33] "Cunonia_atrorubens"  
> "Cunonia_balansae" "Cunonia_capensis" "Cunonia_macrophylla" ...
> 
> 
> The names of each entry in first list match exactly the names of the  
> entries in the second list and both list contain the same number of  
> sequences (although not of the same length)
> I want to concatenate these sequences by the following function:
> 
> 	interleave <- function(seq1, seq2)
> 	{
> 		#create a list to contain concatenated sequences
> 		sequence <- list()			
> 		for (name in names(seq1)) 	# get names as indices
> 			{
> 			# concatenate sequences
> 			sequence[[name]] <- c(seq1[[name]], seq2[[name]])
> 			}
> 	# return concatenated sequences
> 	invisible(sequence)	
> 	}
> 
> I guess the function works in principle, because they commands do so  
> if I enter them step by step.
> But when I call the function it does not return my object "sequence".  
> But why?
> (This happened to me with some functions that I wrote, but not all.  
> The error seems quite erratic : ) to me and I can?t figure out what  
> to change)
> 
> Thanks a lot in advance!
> 
> Christoph

invisible() does just what is says:

Details
This function can be useful when it is desired to have functions return
values which can be assigned, but which do not print when they are not
assigned.


You would have to assign the result of your interleave() function to
another object:

  Res <- interleave(..., ...)


However, there is an easier way to accomplish what you want:

L1 <- as.list(head(iris, 6))
L2 <- as.list(head(iris, 3))

> L1
$Sepal.Length
[1] 5.1 4.9 4.7 4.6 5.0 5.4

$Sepal.Width
[1] 3.5 3.0 3.2 3.1 3.6 3.9

$Petal.Length
[1] 1.4 1.4 1.3 1.5 1.4 1.7

$Petal.Width
[1] 0.2 0.2 0.2 0.2 0.2 0.4

$Species
[1] setosa setosa setosa setosa setosa setosa
Levels: setosa versicolor virginica


> L2
$Sepal.Length
[1] 5.1 4.9 4.7

$Sepal.Width
[1] 3.5 3.0 3.2

$Petal.Length
[1] 1.4 1.4 1.3

$Petal.Width
[1] 0.2 0.2 0.2

$Species
[1] setosa setosa setosa
Levels: setosa versicolor virginica


# Use mapply() to concatenate the two lists

Res <- mapply(c, L1, L2, SIMPLIFY = FALSE)



> Res
$Sepal.Length
[1] 5.1 4.9 4.7 4.6 5.0 5.4 5.1 4.9 4.7

$Sepal.Width
[1] 3.5 3.0 3.2 3.1 3.6 3.9 3.5 3.0 3.2

$Petal.Length
[1] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.4 1.3

$Petal.Width
[1] 0.2 0.2 0.2 0.2 0.2 0.4 0.2 0.2 0.2

$Species
[1] 1 1 1 1 1 1 1 1 1



See ?mapply

HTH,

Marc Schwartz


From jawegelin at ucdavis.edu  Fri Jan 26 21:26:04 2007
From: jawegelin at ucdavis.edu (Jacob Wegelin)
Date: Fri, 26 Jan 2007 12:26:04 -0800 (PST)
Subject: [R] bootstrap bca confidence intervals for large number of
 statistics in one model; library("boot")
Message-ID: <Pine.OSX.4.58.0701241839470.2545@jacob-wegelins-computer.local>


Sometimes one might like to obtain pointwise bootstrap bias-corrected,
accelerated (BCA) confidence intervals for a large number of statistics
computed from a single dataset.  For instance, one might like to get
(so as to plot graphically) bootstrap confidence bands for the fitted
values in a regression model.

(Example: Chiu S et al., Early Acceleration of Head Circumference in
Children with Fragile X Syndrome and Autism. Journal of Developmental &
Behavioral Pediatrics 2007;In press. In this paper we plot the
estimated trajectories of head circumference for two different
groups, computed by linear mixed-effects models, with confidence bands
obtained by bootstrap.)

Below is a toy example of how to do this using the "boot" library.
We obtain BCA CIs for all three regression parameters and for the fitted
values at 50 levels of the predictor.

set.seed(1234567)
x<-runif(150)
y<-2/3 + pi * x^2 + runif(length(x))/2
plot(x,y)
DAT<-data.frame(x,y)
NEWDATA<-data.frame(x=seq(min(x), max(x), length=50))
library('boot')
myfn<-function(data, whichrows) {
	TheseData<-data[whichrows,]
	thisLM<-lm( y~poly(x,2), data=TheseData)
	thisFit<-predict(thisLM, newdata=NEWDATA)
	c(
		coef(summary(thisLM))[,"Estimate"]
		, thisFit)
}
bootObj<-boot( data=DAT, statistic=myfn, R=1000 )
names(bootObj)
dim(bootObj$t)
sofar<-t(sapply( 1:ncol(bootObj$t), function(thiscolumn) boot.ci(bootObj, type='bca', index=thiscolumn)$bca[4:5] ))
colnames(sofar)<-c("lo", "hi")
NEWDATA<-cbind(NEWDATA, sofar[4:nrow(sofar),])
thecoefs<-sofar[1:3,]
lines( NEWDATA$x, NEWDATA$lo, col='red')
lines( NEWDATA$x, NEWDATA$hi, col='red')

Note: To get boot.ci to run with type='bca' it seems necessary to have a
large value of R.  Otherwise boot.ci returns an error, in my (limited)
experience.

Thanks in advance for any critiques.  (For instance, is there an easier or more elegant way?)

Jacob Wegelin


From helprhelp at gmail.com  Fri Jan 26 21:29:36 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Fri, 26 Jan 2007 15:29:36 -0500
Subject: [R]
	=?iso-8859-1?q?Why_do_return_or_visible_don=B4t_return_my_obj?=
	=?iso-8859-1?q?ekt=3F?=
In-Reply-To: <BFB2D6E1-670E-43AC-BEF1-0F4AB24CD116@gmx.net>
References: <BFB2D6E1-670E-43AC-BEF1-0F4AB24CD116@gmx.net>
Message-ID: <cdf817830701261229t18fcf6ffp197ae3dca617bafa@mail.gmail.com>

?invisible

if you return in this function with invisible, then you need to assign like this
> x0 <- lapply(1:3, function(i) (c("0","1")))
> names(x0) <- c("a", "b", "c")
> x0
$a
[1] "0" "1"

$b
[1] "0" "1"

$c
[1] "0" "1"

> x1 <- lapply(1:3, function(i) (c("2","3")))
> names(x1) <- c("a", "b", "c")

> x2 <- interleave(x0, x1)
> x2
$a
[1] "0" "1" "2" "3"

$b
[1] "0" "1" "2" "3"

$c
[1] "0" "1" "2" "3"


otherwise, you need to remove invisible when you return, like this
> interleave(x0, x1)
$a
[1] "0" "1" "2" "3"

$b
[1] "0" "1" "2" "3"

$c
[1] "0" "1" "2" "3"

then you also assign like x2 <- interleave(x0, x1)

HTH,

weiwei

On 1/26/07, Christoph Heibl <christoph.heibl at gmx.net> wrote:
> Dear RRRRRrrrrrrrrlist!
>
> I?ve got two lists which contain sets of DNA-sequences. They look
> something like this:
>
>         List of 33
>         $ Cunonia_atrorubens                 : chr [1:247] "t" "t" "n" "t" ...
>         $ Cunonia_balansae                   : chr [1:254] "t" "c" "c" "c" ...
>         $ Cunonia_capensis                   : chr [1:236] "v" "t" "c" "c" ...
>         $ Cunonia_macrophylla                : chr [1:236] "t" "t" "a" "t" ...
>         $ Pancheria_engleriana               : chr [1:315] "c" "t" "c" "c" ...
>         [....]
>         $ Brunellia_colombiana               : chr [1:336] "t" "t" "c" "t" ...
>         $ Brunellia_oliveri                  : chr [1:368] "t" "a" "a" "t" ...
>         $ Bauera_rubioides                   : chr [1:276] "t" "c" "t" "c" ...
>         $ Bauera_sessiliflora                : chr [1:332] "t" "c" "t" "c" ...
>         $ Davidsonia_pruriens_var._jeryseyana: chr [1:300] "t" "t" "c" "t" ...
>         $ Davidsonia_sp._'johnsonii'         : chr [1:268] "c" "c" "c" "t" ...
>         - attr(*, "species")= chr [1:33] "Cunonia_atrorubens"
> "Cunonia_balansae" "Cunonia_capensis" "Cunonia_macrophylla" ...
>
>
> The names of each entry in first list match exactly the names of the
> entries in the second list and both list contain the same number of
> sequences (although not of the same length)
> I want to concatenate these sequences by the following function:
>
>         interleave <- function(seq1, seq2)
>         {
>                 #create a list to contain concatenated sequences
>                 sequence <- list()
>                 for (name in names(seq1))       # get names as indices
>                         {
>                         # concatenate sequences
>                         sequence[[name]] <- c(seq1[[name]], seq2[[name]])
>                         }
>         # return concatenated sequences
>         invisible(sequence)
>         }
>
> I guess the function works in principle, because they commands do so
> if I enter them step by step.
> But when I call the function it does not return my object "sequence".
> But why?
> (This happened to me with some functions that I wrote, but not all.
> The error seems quite erratic : ) to me and I can?t figure out what
> to change)
>
> Thanks a lot in advance!
>
> Christoph
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From christoph.heibl at gmx.net  Fri Jan 26 21:40:54 2007
From: christoph.heibl at gmx.net (Christoph Heibl)
Date: Fri, 26 Jan 2007 21:40:54 +0100
Subject: [R]
 =?iso-8859-1?q?Why_do_return_or_visible_don=B4t_return_my_obj?=
 =?iso-8859-1?q?ekt=3F?=
In-Reply-To: <1169842616.4899.69.camel@localhost.localdomain>
References: <BFB2D6E1-670E-43AC-BEF1-0F4AB24CD116@gmx.net>
	<1169842616.4899.69.camel@localhost.localdomain>
Message-ID: <FBDE7525-382D-47BF-B329-CD0841BF6A28@gmx.net>

Dear Marc, dear Weiwei,

Thanks a lot!
As I now see me question was quite silly, but we?re always much  
smarter with hindsight...
  : )

Best,
Christoph




On 26.01.2007, at 21:16, Marc Schwartz wrote:

> On Fri, 2007-01-26 at 20:23 +0100, Christoph Heibl wrote:
>> Dear RRRRRrrrrrrrrlist!
>
> Ruh-Ro....
>
> ;-)
>
>> I?ve got two lists which contain sets of DNA-sequences. They look
>> something like this:
>>
>> 	List of 33
>> 	$ Cunonia_atrorubens                 : chr [1:247] "t" "t" "n"  
>> "t" ...
>> 	$ Cunonia_balansae                   : chr [1:254] "t" "c" "c"  
>> "c" ...
>> 	$ Cunonia_capensis                   : chr [1:236] "v" "t" "c"  
>> "c" ...
>> 	$ Cunonia_macrophylla                : chr [1:236] "t" "t" "a"  
>> "t" ...
>> 	$ Pancheria_engleriana               : chr [1:315] "c" "t" "c"  
>> "c" ...
>> 	[....]
>> 	$ Brunellia_colombiana               : chr [1:336] "t" "t" "c"  
>> "t" ...
>> 	$ Brunellia_oliveri                  : chr [1:368] "t" "a" "a"  
>> "t" ...
>> 	$ Bauera_rubioides                   : chr [1:276] "t" "c" "t"  
>> "c" ...
>> 	$ Bauera_sessiliflora                : chr [1:332] "t" "c" "t"  
>> "c" ...
>> 	$ Davidsonia_pruriens_var._jeryseyana: chr [1:300] "t" "t" "c"  
>> "t" ...
>> 	$ Davidsonia_sp._'johnsonii'         : chr [1:268] "c" "c" "c"  
>> "t" ...
>> 	- attr(*, "species")= chr [1:33] "Cunonia_atrorubens"
>> "Cunonia_balansae" "Cunonia_capensis" "Cunonia_macrophylla" ...
>>
>>
>> The names of each entry in first list match exactly the names of the
>> entries in the second list and both list contain the same number of
>> sequences (although not of the same length)
>> I want to concatenate these sequences by the following function:
>>
>> 	interleave <- function(seq1, seq2)
>> 	{
>> 		#create a list to contain concatenated sequences
>> 		sequence <- list()			
>> 		for (name in names(seq1)) 	# get names as indices
>> 			{
>> 			# concatenate sequences
>> 			sequence[[name]] <- c(seq1[[name]], seq2[[name]])
>> 			}
>> 	# return concatenated sequences
>> 	invisible(sequence)	
>> 	}
>>
>> I guess the function works in principle, because they commands do so
>> if I enter them step by step.
>> But when I call the function it does not return my object "sequence".
>> But why?
>> (This happened to me with some functions that I wrote, but not all.
>> The error seems quite erratic : ) to me and I can?t figure out what
>> to change)
>>
>> Thanks a lot in advance!
>>
>> Christoph
>
> invisible() does just what is says:
>
> Details
> This function can be useful when it is desired to have functions  
> return
> values which can be assigned, but which do not print when they are not
> assigned.
>
>
> You would have to assign the result of your interleave() function to
> another object:
>
>   Res <- interleave(..., ...)
>
>
> However, there is an easier way to accomplish what you want:
>
> L1 <- as.list(head(iris, 6))
> L2 <- as.list(head(iris, 3))
>
>> L1
> $Sepal.Length
> [1] 5.1 4.9 4.7 4.6 5.0 5.4
>
> $Sepal.Width
> [1] 3.5 3.0 3.2 3.1 3.6 3.9
>
> $Petal.Length
> [1] 1.4 1.4 1.3 1.5 1.4 1.7
>
> $Petal.Width
> [1] 0.2 0.2 0.2 0.2 0.2 0.4
>
> $Species
> [1] setosa setosa setosa setosa setosa setosa
> Levels: setosa versicolor virginica
>
>
>> L2
> $Sepal.Length
> [1] 5.1 4.9 4.7
>
> $Sepal.Width
> [1] 3.5 3.0 3.2
>
> $Petal.Length
> [1] 1.4 1.4 1.3
>
> $Petal.Width
> [1] 0.2 0.2 0.2
>
> $Species
> [1] setosa setosa setosa
> Levels: setosa versicolor virginica
>
>
> # Use mapply() to concatenate the two lists
>
> Res <- mapply(c, L1, L2, SIMPLIFY = FALSE)
>
>
>
>> Res
> $Sepal.Length
> [1] 5.1 4.9 4.7 4.6 5.0 5.4 5.1 4.9 4.7
>
> $Sepal.Width
> [1] 3.5 3.0 3.2 3.1 3.6 3.9 3.5 3.0 3.2
>
> $Petal.Length
> [1] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.4 1.3
>
> $Petal.Width
> [1] 0.2 0.2 0.2 0.2 0.2 0.4 0.2 0.2 0.2
>
> $Species
> [1] 1 1 1 1 1 1 1 1 1
>
>
>
> See ?mapply
>
> HTH,
>
> Marc Schwartz
>


From lalithaviswanath at yahoo.com  Fri Jan 26 21:43:13 2007
From: lalithaviswanath at yahoo.com (lalitha viswanath)
Date: Fri, 26 Jan 2007 12:43:13 -0800 (PST)
Subject: [R] unique/subset problem
In-Reply-To: <cdf817830701260826v5cc89a22i9218614f6819ee32@mail.gmail.com>
Message-ID: <712775.36727.qm@web43116.mail.sp1.yahoo.com>

Hi
I read in my dataset using
dt <read.table("filename")
calling unique(levels(dt$genome1))  yields the
following 

 "aero"      "aful"      "aquae"     "atum_D"   
"bbur"      "bhal"      "bmel"      "bsub"     
 [9] "buch"      "cace"      "ccre"      "cglu"     
"cjej"      "cper"      "cpneuA"    "cpneuC"   
[17] "cpneuJ"    "ctraM"     "ecoliO157" "hbsp"     
"hinf"      "hpyl"      "linn"      "llact"    
[25] "lmon"      "mgen"      "mjan"      "mlep"     
"mlot"      "mpneu"     "mpul"      "mthe"     
[33] "mtub"      "mtub_cdc"  "nost"      "pabyssi"  
"paer"      "paero"     "pmul"      "pyro"     
[41] "rcon"      "rpxx"      "saur_mu50" "saur_n315"
"sent"      "smel"      "spneu"     "spyo"     
[49] "ssol"      "stok"      "styp"      "synecho"  
"tacid"     "tmar"      "tpal"      "tvol"     
[57] "uure"      "vcho"      "xfas"      "ypes"     

It shows 60 genomes, which is correct.

I extracted a subset as follows
possible_relatives_subset <- subset(dt, Y < -5)
I am pasting the results below
     genome1   genome2 parameterX          Y
21       sent ecoliO157  0.00590 -200.633493
22       sent      paer  0.18603 -100.200570
27       styp ecoliO157  0.00484 -240.708645
28       styp      paer  0.18497 -30.250127
41       paer      sent  0.18603 -60.200570
44       paer      styp  0.18497 -80.250127
49       paer      hinf  0.18913 -90.056333
53       paer      vcho  0.18703 -10.153929
55       paer      pmul  0.18587 -100.208042
67       paer      buch  0.21485  -80.898667
70       paer      ypes  0.18460 -107.267454
82       paer      xfas  0.26268  -61.920552
95       hinf ecoliO157  0.07654 -163.018417
96       hinf      paer  0.18913 -10.056333
103      vcho ecoliO157  0.09518 -140.921153
104      vcho      paer  0.18703 -10.153929
107      pmul ecoliO157  0.07328 -165.215225
108      pmul      paer  0.18587 -10.208042
131      buch ecoliO157  0.15412 -11.746939
132      buch      paer  0.21485  -8.898667
137      ypes ecoliO157  0.02705 -19.171851
138      ypes      paer  0.18460 -10.267454
171 ecoliO157      sent  0.00590 -20.633493
174 ecoliO157      styp  0.00484 -20.708645
179 ecoliO157      hinf  0.07654 -6.018417
183 ecoliO157      vcho  0.09518 -14.921153
185 ecoliO157      pmul  0.07328 -6.215225
197 ecoliO157      buch  0.15412 -11.746939
200 ecoliO157      ypes  0.02705 -9.171851
211 ecoliO157      xfas  0.25833  -71.091552
217      xfas ecoliO157  0.25833  -75.091552
218      xfas      paer  0.26268  -64.920552

I think  even a cursory look will tell us that there
are not as many unique genomes in the subset results.
(around 8/10).
However when I do
unique(levels(possible_relatives_subset$genome1)), I
get

[1] "aero"      "aful"      "aquae"     "atum_D"   
"bbur"      "bhal"      "bmel"      "bsub"     
 [9] "buch"      "cace"      "ccre"      "cglu"     
"cjej"      "cper"      "cpneuA"    "cpneuC"   
[17] "cpneuJ"    "ctraM"     "ecoliO157" "hbsp"     
"hinf"      "hpyl"      "linn"      "llact"    
[25] "lmon"      "mgen"      "mjan"      "mlep"     
"mlot"      "mpneu"     "mpul"      "mthe"     
[33] "mtub"      "mtub_cdc"  "nost"      "pabyssi"  
"paer"      "paero"     "pmul"      "pyro"     
[41] "rcon"      "rpxx"      "saur_mu50" "saur_n315"
"sent"      "smel"      "spneu"     "spyo"     
[49] "ssol"      "stok"      "styp"      "synecho"  
"tacid"     "tmar"      "tpal"      "tvol"     
[57] "uure"      "vcho"      "xfas"      "ypes" 

Where am I going wrong?
I tried calling unique without the levels too, which
gives me the following response

[1] sent      styp      paer      hinf      vcho     
pmul      buch      ypes      ecoliO157 xfas     
60 Levels: aero aful aquae atum_D bbur bhal bmel bsub
buch cace ccre cglu cjej cper cpneuA ... ypes

--- Weiwei Shi <helprhelp at gmail.com> wrote:

> Then you need to provide more details about the
> calls you made and your dataset.
> For example, you can tell us by
> str(prunedrelatives, 1)
> 
> how did you call unique on prunedrelative and so on?
> I made a test
> data it gave me what you wanted (omitted here).
> 
> On 1/26/07, lalitha viswanath
> <lalithaviswanath at yahoo.com> wrote:
> > Hi
> > The pruned dataset has 8 unique genomes in it
> while
> > the dataset before pruning has 65 unique genomes
> in
> > it.
> > However calling unique on the pruned dataset seems
> to
> > return 65 no matter what.
> >
> > Any assistance in this matter would be
> appreciated.
> >
> > Thanks
> > Lalitha
> > --- Weiwei Shi <helprhelp at gmail.com> wrote:
> >
> > > Hi,
> > >
> > > Even you removed "many" genomes1 by setting
> score<
> > > -5; it is not
> > > necessary saying you changed the uniqueness.
> > >
> > > To check this, you can do like
> > > p0 <- unique(dataset[dataset$score< -5,
> "genome1"])
> > > # same as subset
> > > p1 <- unique(dataset[dataset$score>= -5,
> "genome1"])
> > >
> > > setdiff(p1, p0)
> > >
> > > if the output above has NULL, then it means even
> > > though you remove
> > > many genomes1, but it does not help changing the
> > > uniqueness.
> > >
> > > HTH,
> > >
> > > weiwei
> > >
> > >
> > >
> > > On 1/25/07, lalitha viswanath
> > > <lalithaviswanath at yahoo.com> wrote:
> > > > Hi
> > > > I am new to R programming and am using subset
> to
> > > > extract part of a data as follows
> > > >
> > > > names(dataset) =
> > > > c("genome1","genome2","dist","score");
> > > > prunedrelatives <- subset(dataset, score <
> -5);
> > > >
> > > > However when I use unique to find the number
> of
> > > unique
> > > > genomes now present in prunedrelatives I get
> > > results
> > > > identical to calling unique(dataset$genome1)
> > > although
> > > > subset has eliminated many genomes and
> records.
> > > >
> > > > I would greatly appreciate your input about
> using
> > > > "unique" correctly  in this regard.
> > > >
> > > > Thanks
> > > > Lalitha
> > > >
> > > >
> > > >
> > > >
> > >
> >
>
____________________________________________________________________________________
> > > > TV dinner still cooling?
> > > > Check out "Tonight's Picks" on Yahoo! TV.
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal,
> self-contained,
> > > reproducible code.
> > > >
> > >
> > >
> > > --
> > > Weiwei Shi, Ph.D
> > > Research Scientist
> > > GeneGO, Inc.
> > >
> > > "Did you always know?"
> > > "No, I did not. But I believed..."
> > > ---Matrix III
> > >
> >
> >
> >
> >
> >
>
____________________________________________________________________________________
> > Bored stiff? Loosen up...
> > Download and play hundreds of games for free on


> >
> 
> 
> -- 
> Weiwei Shi, Ph.D
> Research Scientist
> GeneGO, Inc.
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 



 
____________________________________________________________________________________
We won't tell. Get more on shows you hate to love


From josh.kalish at credit-suisse.com  Fri Jan 26 21:48:32 2007
From: josh.kalish at credit-suisse.com (Kalish, Josh)
Date: Fri, 26 Jan 2007 15:48:32 -0500
Subject: [R] Using tapply to create a new table
Message-ID: <FB9A739CA1DD3F4E81BF65BE4C6B0836458A4DBA@enyc19p32002.corpny.csfb.com>

Ok, Thanks.  I wasn't sure what the actual output of a tapply was. 

-----Original Message-----
From: Marc Schwartz [mailto:marc_schwartz at comcast.net] 
Sent: Friday, January 26, 2007 1:47 PM
To: Kalish, Josh
Cc: 'r-help at stat.math.ethz.ch'
Subject: Re: [R] Using tapply to create a new table

Josh,

As per the "Value" section of ?tapply, it "returns a single atomic value for each cell".  This is easily viewed by using:

  tapply(set2$f1, set2$commonField, mean)

in a stand alone fashion or:

  str(tapply(set2$f1, set2$commonField, mean))

which will display the internal structure of the result. See ?str

To use merge() in the fashion you seem to want, you would want to use
aggregate() and not tapply().  The former returns a data frame, where the "key" or "by" values will be part of the output.

See ?aggregate for more information.

I would also recommend that both for the readability of the code you write and to help clarify for yourself, the objects that are returned from each step, that you not nest the function calls as you have below.

There are times when it makes sense, but there are times when the code would end up being a good candidate for an Obfuscated R contest.  :-)

HTH,

Marc

On Fri, 2007-01-26 at 13:21 -0500, Kalish, Josh wrote:
> Marc,
> 
> Thanks for pointing out the merge function.  That gets me part of the 
> way there.  The only thing is that I can't get the tapply() results 
> into a format that merge() will take.  For example:
> 
> merge( set1 , tapply( set2$f1 , set2$commonField, mean ) , 
> by="commonField" )
> 
> Gives me  "Error in names... Unused arguments..."
> 
> I'm not sure what the result of a tapply() exactly is, but it doesn't 
> seem to be a table.
> 
> Yeah, rank amateur questions...
> 
> Thanks,
> 
> Josh
> 
> -----Original Message-----
> From: Marc Schwartz [mailto:marc_schwartz at comcast.net]
> Sent: Friday, January 26, 2007 1:08 PM
> To: Kalish, Josh
> Cc: 'r-help at stat.math.ethz.ch'
> Subject: Re: [R] Using tapply to create a new table
> 
> On Fri, 2007-01-26 at 12:39 -0500, Kalish, Josh wrote:
> > All,
> > 
> > I'm sure that this is covered somewhere, but I can't seem to find a 
> > good explanation.  I have an existing table that contains
> information
> > grouped by date.  This is as so:
> > 
> > Day		NumberOfCustomers		NumberOfComplaints
> > 20060512	10040				40
> > 20060513	32420				11
> > ...
> > 
> > 
> > I also have a table at the detail level as so:
> > 
> > Day		Meal		PricePaid	UsedCupon
> > 20060512	Fish		14		Y
> > 20060512	Chicken	20		N
> > ...
> > 
> > Is there a simple way to create summaries on the detail table and
> then
> > join them into the first table above so that it looks like this:
> > 
> > Day		NumberOfCustomers		NumberOfComplaints	AveragePricePaid
> > NumberUsingCupon
> > 
> > 
> > I can do a tapply to get what I want from the detail table, but I 
> > can't figure out how to turn that into a table and join it back in.
> > 
> > 
> > 
> > Thanks,
> > 
> > Josh
> 
> Skipping the steps of using tapply() or aggregate() to get the 
> summarized data from the second data frame, you would then use merge() 
> to perform a SQL-like 'join' operation:
> 
> > DF.1
>        Day NumberOfCustomers NumberOfComplaints
> 1 20060512             10040                 40
> 2 20060513             32420                 11
> 
> > DF.2
>        Day    Meal PricePaid UsedCupon
> 1 20060512    Fish        14         Y
> 2 20060512 Chicken        20         N
> 
> > merge(DF.1, DF.2, by = "Day")
>        Day NumberOfCustomers NumberOfComplaints    Meal PricePaid
> 1 20060512             10040                 40    Fish        14
> 2 20060512             10040                 40 Chicken        20
>   UsedCupon
> 1         Y
> 2         N
> 
> 
> By default, only rows matching on the 'by' argument in both data 
> frames will be in the result. See the 'all.x' and 'all.y' arguments to 
> handle other scenarios of including non-matching rows.
> 
> See ?merge, which BTW:
> 
>   help.search("join")
> 
> would point you to, if you are familiar with the term from relational 
> data base operations.
> 
> HTH,
> 
> Marc Schwartz



==============================================================================
Please access the attached hyperlink for an important electr...{{dropped}}


From helprhelp at gmail.com  Fri Jan 26 21:51:27 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Fri, 26 Jan 2007 15:51:27 -0500
Subject: [R] unique/subset problem
In-Reply-To: <712775.36727.qm@web43116.mail.sp1.yahoo.com>
References: <cdf817830701260826v5cc89a22i9218614f6819ee32@mail.gmail.com>
	<712775.36727.qm@web43116.mail.sp1.yahoo.com>
Message-ID: <cdf817830701261251q1550cfck1f7c67186f115a91@mail.gmail.com>

check
?read.table

and add "as.is=T" in the option. So you read string as character now
and avoid the factor things.

Then repeat your work.

For example
> x0 <- read.table("~/Documents/tox/noodles/four_sheets_orig/reg_r2.txt", sep="\t", nrows=10)
> str(x0,1)
`data.frame':	10 obs. of  7 variables:
 $ V1: Factor w/ 10 levels "-4086733916",..: 10 9 8 7 6 5 4 3 2 1
 $ V2: Factor w/ 10 levels "-1963744741",..: 10 8 7 4 5 6 3 9 1 2
 $ V3: Factor w/ 7 levels "-1687428658",..: 7 4 4 2 5 1 6 6 3 4
 $ V4: Factor w/ 2 levels "5","MECHANISM": 2 1 1 1 1 1 1 1 1 1
 $ V5: Factor w/ 2 levels "0","TYPE": 2 1 1 1 1 1 1 1 1 1
 $ V6: Factor w/ 2 levels "USER_","alexey": 1 2 2 2 2 2 2 2 2 2
 $ V7: Factor w/ 2 levels "3","TRUST": 2 1 1 1 1 1 1 1 1 1
> x0 <- read.table("~/Documents/tox/noodles/four_sheets_orig/reg_r2.txt", sep="\t", nrows=10, as.is=T)
> str(x0,1)
`data.frame':	10 obs. of  7 variables:
 $ V1: chr  "LINK_ID" "-4293537751" "-4247422653" "-4223137153" ...
 $ V2: chr  "ID1" "65259" "1020286" "-518245428" ...
 $ V3: chr  "ID2" "6436" "6436" "-2099509019" ...
 $ V4: chr  "MECHANISM" "5" "5" "5" ...
 $ V5: chr  "TYPE" "0" "0" "0" ...
 $ V6: chr  "USER_" "alexey" "alexey" "alexey" ...
 $ V7: chr  "TRUST" "3" "3" "3" ...

HTH,

weiwei

On 1/26/07, lalitha viswanath <lalithaviswanath at yahoo.com> wrote:
> Hi
> I read in my dataset using
> dt <read.table("filename")
> calling unique(levels(dt$genome1))  yields the
> following
>
>  "aero"      "aful"      "aquae"     "atum_D"
> "bbur"      "bhal"      "bmel"      "bsub"
>  [9] "buch"      "cace"      "ccre"      "cglu"
> "cjej"      "cper"      "cpneuA"    "cpneuC"
> [17] "cpneuJ"    "ctraM"     "ecoliO157" "hbsp"
> "hinf"      "hpyl"      "linn"      "llact"
> [25] "lmon"      "mgen"      "mjan"      "mlep"
> "mlot"      "mpneu"     "mpul"      "mthe"
> [33] "mtub"      "mtub_cdc"  "nost"      "pabyssi"
> "paer"      "paero"     "pmul"      "pyro"
> [41] "rcon"      "rpxx"      "saur_mu50" "saur_n315"
> "sent"      "smel"      "spneu"     "spyo"
> [49] "ssol"      "stok"      "styp"      "synecho"
> "tacid"     "tmar"      "tpal"      "tvol"
> [57] "uure"      "vcho"      "xfas"      "ypes"
>
> It shows 60 genomes, which is correct.
>
> I extracted a subset as follows
> possible_relatives_subset <- subset(dt, Y < -5)
> I am pasting the results below
>      genome1   genome2 parameterX          Y
> 21       sent ecoliO157  0.00590 -200.633493
> 22       sent      paer  0.18603 -100.200570
> 27       styp ecoliO157  0.00484 -240.708645
> 28       styp      paer  0.18497 -30.250127
> 41       paer      sent  0.18603 -60.200570
> 44       paer      styp  0.18497 -80.250127
> 49       paer      hinf  0.18913 -90.056333
> 53       paer      vcho  0.18703 -10.153929
> 55       paer      pmul  0.18587 -100.208042
> 67       paer      buch  0.21485  -80.898667
> 70       paer      ypes  0.18460 -107.267454
> 82       paer      xfas  0.26268  -61.920552
> 95       hinf ecoliO157  0.07654 -163.018417
> 96       hinf      paer  0.18913 -10.056333
> 103      vcho ecoliO157  0.09518 -140.921153
> 104      vcho      paer  0.18703 -10.153929
> 107      pmul ecoliO157  0.07328 -165.215225
> 108      pmul      paer  0.18587 -10.208042
> 131      buch ecoliO157  0.15412 -11.746939
> 132      buch      paer  0.21485  -8.898667
> 137      ypes ecoliO157  0.02705 -19.171851
> 138      ypes      paer  0.18460 -10.267454
> 171 ecoliO157      sent  0.00590 -20.633493
> 174 ecoliO157      styp  0.00484 -20.708645
> 179 ecoliO157      hinf  0.07654 -6.018417
> 183 ecoliO157      vcho  0.09518 -14.921153
> 185 ecoliO157      pmul  0.07328 -6.215225
> 197 ecoliO157      buch  0.15412 -11.746939
> 200 ecoliO157      ypes  0.02705 -9.171851
> 211 ecoliO157      xfas  0.25833  -71.091552
> 217      xfas ecoliO157  0.25833  -75.091552
> 218      xfas      paer  0.26268  -64.920552
>
> I think  even a cursory look will tell us that there
> are not as many unique genomes in the subset results.
> (around 8/10).
> However when I do
> unique(levels(possible_relatives_subset$genome1)), I
> get
>
> [1] "aero"      "aful"      "aquae"     "atum_D"
> "bbur"      "bhal"      "bmel"      "bsub"
>  [9] "buch"      "cace"      "ccre"      "cglu"
> "cjej"      "cper"      "cpneuA"    "cpneuC"
> [17] "cpneuJ"    "ctraM"     "ecoliO157" "hbsp"
> "hinf"      "hpyl"      "linn"      "llact"
> [25] "lmon"      "mgen"      "mjan"      "mlep"
> "mlot"      "mpneu"     "mpul"      "mthe"
> [33] "mtub"      "mtub_cdc"  "nost"      "pabyssi"
> "paer"      "paero"     "pmul"      "pyro"
> [41] "rcon"      "rpxx"      "saur_mu50" "saur_n315"
> "sent"      "smel"      "spneu"     "spyo"
> [49] "ssol"      "stok"      "styp"      "synecho"
> "tacid"     "tmar"      "tpal"      "tvol"
> [57] "uure"      "vcho"      "xfas"      "ypes"
>
> Where am I going wrong?
> I tried calling unique without the levels too, which
> gives me the following response
>
> [1] sent      styp      paer      hinf      vcho
> pmul      buch      ypes      ecoliO157 xfas
> 60 Levels: aero aful aquae atum_D bbur bhal bmel bsub
> buch cace ccre cglu cjej cper cpneuA ... ypes
>
> --- Weiwei Shi <helprhelp at gmail.com> wrote:
>
> > Then you need to provide more details about the
> > calls you made and your dataset.
> > For example, you can tell us by
> > str(prunedrelatives, 1)
> >
> > how did you call unique on prunedrelative and so on?
> > I made a test
> > data it gave me what you wanted (omitted here).
> >
> > On 1/26/07, lalitha viswanath
> > <lalithaviswanath at yahoo.com> wrote:
> > > Hi
> > > The pruned dataset has 8 unique genomes in it
> > while
> > > the dataset before pruning has 65 unique genomes
> > in
> > > it.
> > > However calling unique on the pruned dataset seems
> > to
> > > return 65 no matter what.
> > >
> > > Any assistance in this matter would be
> > appreciated.
> > >
> > > Thanks
> > > Lalitha
> > > --- Weiwei Shi <helprhelp at gmail.com> wrote:
> > >
> > > > Hi,
> > > >
> > > > Even you removed "many" genomes1 by setting
> > score<
> > > > -5; it is not
> > > > necessary saying you changed the uniqueness.
> > > >
> > > > To check this, you can do like
> > > > p0 <- unique(dataset[dataset$score< -5,
> > "genome1"])
> > > > # same as subset
> > > > p1 <- unique(dataset[dataset$score>= -5,
> > "genome1"])
> > > >
> > > > setdiff(p1, p0)
> > > >
> > > > if the output above has NULL, then it means even
> > > > though you remove
> > > > many genomes1, but it does not help changing the
> > > > uniqueness.
> > > >
> > > > HTH,
> > > >
> > > > weiwei
> > > >
> > > >
> > > >
> > > > On 1/25/07, lalitha viswanath
> > > > <lalithaviswanath at yahoo.com> wrote:
> > > > > Hi
> > > > > I am new to R programming and am using subset
> > to
> > > > > extract part of a data as follows
> > > > >
> > > > > names(dataset) =
> > > > > c("genome1","genome2","dist","score");
> > > > > prunedrelatives <- subset(dataset, score <
> > -5);
> > > > >
> > > > > However when I use unique to find the number
> > of
> > > > unique
> > > > > genomes now present in prunedrelatives I get
> > > > results
> > > > > identical to calling unique(dataset$genome1)
> > > > although
> > > > > subset has eliminated many genomes and
> > records.
> > > > >
> > > > > I would greatly appreciate your input about
> > using
> > > > > "unique" correctly  in this regard.
> > > > >
> > > > > Thanks
> > > > > Lalitha
> > > > >
> > > > >
> > > > >
> > > > >
> > > >
> > >
> >
> ____________________________________________________________________________________
> > > > > TV dinner still cooling?
> > > > > Check out "Tonight's Picks" on Yahoo! TV.
> > > > >
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > > > > and provide commented, minimal,
> > self-contained,
> > > > reproducible code.
> > > > >
> > > >
> > > >
> > > > --
> > > > Weiwei Shi, Ph.D
> > > > Research Scientist
> > > > GeneGO, Inc.
> > > >
> > > > "Did you always know?"
> > > > "No, I did not. But I believed..."
> > > > ---Matrix III
> > > >
> > >
> > >
> > >
> > >
> > >
> >
> ____________________________________________________________________________________
> > > Bored stiff? Loosen up...
> > > Download and play hundreds of games for free on
> > Yahoo! Games.
> > > http://games.yahoo.com/games/front
> > >
> >
> >
> > --
> > Weiwei Shi, Ph.D
> > Research Scientist
> > GeneGO, Inc.
> >
> > "Did you always know?"
> > "No, I did not. But I believed..."
> > ---Matrix III
> >
>
>
>
>
> ____________________________________________________________________________________
> We won't tell. Get more on shows you hate to love
> (and love to hate): Yahoo! TV's Guilty Pleasures list.
> http://tv.yahoo.com/collections/265
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From helprhelp at gmail.com  Fri Jan 26 21:53:40 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Fri, 26 Jan 2007 15:53:40 -0500
Subject: [R] unique/subset problem
In-Reply-To: <cdf817830701261251q1550cfck1f7c67186f115a91@mail.gmail.com>
References: <cdf817830701260826v5cc89a22i9218614f6819ee32@mail.gmail.com>
	<712775.36727.qm@web43116.mail.sp1.yahoo.com>
	<cdf817830701261251q1550cfck1f7c67186f115a91@mail.gmail.com>
Message-ID: <cdf817830701261253j53061ed2u5ef9fc3ac62f48@mail.gmail.com>

oh, i forgot, you can also convert factor into string like
dataset$genome1 <- as.character(dataset$genome1)

so you don't have to use
as.numeric(dataset$score) if you use "as.is=T" when you read.table

HTH,

weiwei

On 1/26/07, Weiwei Shi <helprhelp at gmail.com> wrote:
> check
> ?read.table
>
> and add "as.is=T" in the option. So you read string as character now
> and avoid the factor things.
>
> Then repeat your work.
>
> For example
> > x0 <- read.table("~/Documents/tox/noodles/four_sheets_orig/reg_r2.txt", sep="\t", nrows=10)
> > str(x0,1)
> `data.frame':   10 obs. of  7 variables:
>  $ V1: Factor w/ 10 levels "-4086733916",..: 10 9 8 7 6 5 4 3 2 1
>  $ V2: Factor w/ 10 levels "-1963744741",..: 10 8 7 4 5 6 3 9 1 2
>  $ V3: Factor w/ 7 levels "-1687428658",..: 7 4 4 2 5 1 6 6 3 4
>  $ V4: Factor w/ 2 levels "5","MECHANISM": 2 1 1 1 1 1 1 1 1 1
>  $ V5: Factor w/ 2 levels "0","TYPE": 2 1 1 1 1 1 1 1 1 1
>  $ V6: Factor w/ 2 levels "USER_","alexey": 1 2 2 2 2 2 2 2 2 2
>  $ V7: Factor w/ 2 levels "3","TRUST": 2 1 1 1 1 1 1 1 1 1
> > x0 <- read.table("~/Documents/tox/noodles/four_sheets_orig/reg_r2.txt", sep="\t", nrows=10, as.is=T)
> > str(x0,1)
> `data.frame':   10 obs. of  7 variables:
>  $ V1: chr  "LINK_ID" "-4293537751" "-4247422653" "-4223137153" ...
>  $ V2: chr  "ID1" "65259" "1020286" "-518245428" ...
>  $ V3: chr  "ID2" "6436" "6436" "-2099509019" ...
>  $ V4: chr  "MECHANISM" "5" "5" "5" ...
>  $ V5: chr  "TYPE" "0" "0" "0" ...
>  $ V6: chr  "USER_" "alexey" "alexey" "alexey" ...
>  $ V7: chr  "TRUST" "3" "3" "3" ...
>
> HTH,
>
> weiwei
>
> On 1/26/07, lalitha viswanath <lalithaviswanath at yahoo.com> wrote:
> > Hi
> > I read in my dataset using
> > dt <read.table("filename")
> > calling unique(levels(dt$genome1))  yields the
> > following
> >
> >  "aero"      "aful"      "aquae"     "atum_D"
> > "bbur"      "bhal"      "bmel"      "bsub"
> >  [9] "buch"      "cace"      "ccre"      "cglu"
> > "cjej"      "cper"      "cpneuA"    "cpneuC"
> > [17] "cpneuJ"    "ctraM"     "ecoliO157" "hbsp"
> > "hinf"      "hpyl"      "linn"      "llact"
> > [25] "lmon"      "mgen"      "mjan"      "mlep"
> > "mlot"      "mpneu"     "mpul"      "mthe"
> > [33] "mtub"      "mtub_cdc"  "nost"      "pabyssi"
> > "paer"      "paero"     "pmul"      "pyro"
> > [41] "rcon"      "rpxx"      "saur_mu50" "saur_n315"
> > "sent"      "smel"      "spneu"     "spyo"
> > [49] "ssol"      "stok"      "styp"      "synecho"
> > "tacid"     "tmar"      "tpal"      "tvol"
> > [57] "uure"      "vcho"      "xfas"      "ypes"
> >
> > It shows 60 genomes, which is correct.
> >
> > I extracted a subset as follows
> > possible_relatives_subset <- subset(dt, Y < -5)
> > I am pasting the results below
> >      genome1   genome2 parameterX          Y
> > 21       sent ecoliO157  0.00590 -200.633493
> > 22       sent      paer  0.18603 -100.200570
> > 27       styp ecoliO157  0.00484 -240.708645
> > 28       styp      paer  0.18497 -30.250127
> > 41       paer      sent  0.18603 -60.200570
> > 44       paer      styp  0.18497 -80.250127
> > 49       paer      hinf  0.18913 -90.056333
> > 53       paer      vcho  0.18703 -10.153929
> > 55       paer      pmul  0.18587 -100.208042
> > 67       paer      buch  0.21485  -80.898667
> > 70       paer      ypes  0.18460 -107.267454
> > 82       paer      xfas  0.26268  -61.920552
> > 95       hinf ecoliO157  0.07654 -163.018417
> > 96       hinf      paer  0.18913 -10.056333
> > 103      vcho ecoliO157  0.09518 -140.921153
> > 104      vcho      paer  0.18703 -10.153929
> > 107      pmul ecoliO157  0.07328 -165.215225
> > 108      pmul      paer  0.18587 -10.208042
> > 131      buch ecoliO157  0.15412 -11.746939
> > 132      buch      paer  0.21485  -8.898667
> > 137      ypes ecoliO157  0.02705 -19.171851
> > 138      ypes      paer  0.18460 -10.267454
> > 171 ecoliO157      sent  0.00590 -20.633493
> > 174 ecoliO157      styp  0.00484 -20.708645
> > 179 ecoliO157      hinf  0.07654 -6.018417
> > 183 ecoliO157      vcho  0.09518 -14.921153
> > 185 ecoliO157      pmul  0.07328 -6.215225
> > 197 ecoliO157      buch  0.15412 -11.746939
> > 200 ecoliO157      ypes  0.02705 -9.171851
> > 211 ecoliO157      xfas  0.25833  -71.091552
> > 217      xfas ecoliO157  0.25833  -75.091552
> > 218      xfas      paer  0.26268  -64.920552
> >
> > I think  even a cursory look will tell us that there
> > are not as many unique genomes in the subset results.
> > (around 8/10).
> > However when I do
> > unique(levels(possible_relatives_subset$genome1)), I
> > get
> >
> > [1] "aero"      "aful"      "aquae"     "atum_D"
> > "bbur"      "bhal"      "bmel"      "bsub"
> >  [9] "buch"      "cace"      "ccre"      "cglu"
> > "cjej"      "cper"      "cpneuA"    "cpneuC"
> > [17] "cpneuJ"    "ctraM"     "ecoliO157" "hbsp"
> > "hinf"      "hpyl"      "linn"      "llact"
> > [25] "lmon"      "mgen"      "mjan"      "mlep"
> > "mlot"      "mpneu"     "mpul"      "mthe"
> > [33] "mtub"      "mtub_cdc"  "nost"      "pabyssi"
> > "paer"      "paero"     "pmul"      "pyro"
> > [41] "rcon"      "rpxx"      "saur_mu50" "saur_n315"
> > "sent"      "smel"      "spneu"     "spyo"
> > [49] "ssol"      "stok"      "styp"      "synecho"
> > "tacid"     "tmar"      "tpal"      "tvol"
> > [57] "uure"      "vcho"      "xfas"      "ypes"
> >
> > Where am I going wrong?
> > I tried calling unique without the levels too, which
> > gives me the following response
> >
> > [1] sent      styp      paer      hinf      vcho
> > pmul      buch      ypes      ecoliO157 xfas
> > 60 Levels: aero aful aquae atum_D bbur bhal bmel bsub
> > buch cace ccre cglu cjej cper cpneuA ... ypes
> >
> > --- Weiwei Shi <helprhelp at gmail.com> wrote:
> >
> > > Then you need to provide more details about the
> > > calls you made and your dataset.
> > > For example, you can tell us by
> > > str(prunedrelatives, 1)
> > >
> > > how did you call unique on prunedrelative and so on?
> > > I made a test
> > > data it gave me what you wanted (omitted here).
> > >
> > > On 1/26/07, lalitha viswanath
> > > <lalithaviswanath at yahoo.com> wrote:
> > > > Hi
> > > > The pruned dataset has 8 unique genomes in it
> > > while
> > > > the dataset before pruning has 65 unique genomes
> > > in
> > > > it.
> > > > However calling unique on the pruned dataset seems
> > > to
> > > > return 65 no matter what.
> > > >
> > > > Any assistance in this matter would be
> > > appreciated.
> > > >
> > > > Thanks
> > > > Lalitha
> > > > --- Weiwei Shi <helprhelp at gmail.com> wrote:
> > > >
> > > > > Hi,
> > > > >
> > > > > Even you removed "many" genomes1 by setting
> > > score<
> > > > > -5; it is not
> > > > > necessary saying you changed the uniqueness.
> > > > >
> > > > > To check this, you can do like
> > > > > p0 <- unique(dataset[dataset$score< -5,
> > > "genome1"])
> > > > > # same as subset
> > > > > p1 <- unique(dataset[dataset$score>= -5,
> > > "genome1"])
> > > > >
> > > > > setdiff(p1, p0)
> > > > >
> > > > > if the output above has NULL, then it means even
> > > > > though you remove
> > > > > many genomes1, but it does not help changing the
> > > > > uniqueness.
> > > > >
> > > > > HTH,
> > > > >
> > > > > weiwei
> > > > >
> > > > >
> > > > >
> > > > > On 1/25/07, lalitha viswanath
> > > > > <lalithaviswanath at yahoo.com> wrote:
> > > > > > Hi
> > > > > > I am new to R programming and am using subset
> > > to
> > > > > > extract part of a data as follows
> > > > > >
> > > > > > names(dataset) =
> > > > > > c("genome1","genome2","dist","score");
> > > > > > prunedrelatives <- subset(dataset, score <
> > > -5);
> > > > > >
> > > > > > However when I use unique to find the number
> > > of
> > > > > unique
> > > > > > genomes now present in prunedrelatives I get
> > > > > results
> > > > > > identical to calling unique(dataset$genome1)
> > > > > although
> > > > > > subset has eliminated many genomes and
> > > records.
> > > > > >
> > > > > > I would greatly appreciate your input about
> > > using
> > > > > > "unique" correctly  in this regard.
> > > > > >
> > > > > > Thanks
> > > > > > Lalitha
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > >
> > > >
> > >
> > ____________________________________________________________________________________
> > > > > > TV dinner still cooling?
> > > > > > Check out "Tonight's Picks" on Yahoo! TV.
> > > > > >
> > > > > > ______________________________________________
> > > > > > R-help at stat.math.ethz.ch mailing list
> > > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > > PLEASE do read the posting guide
> > > > > http://www.R-project.org/posting-guide.html
> > > > > > and provide commented, minimal,
> > > self-contained,
> > > > > reproducible code.
> > > > > >
> > > > >
> > > > >
> > > > > --
> > > > > Weiwei Shi, Ph.D
> > > > > Research Scientist
> > > > > GeneGO, Inc.
> > > > >
> > > > > "Did you always know?"
> > > > > "No, I did not. But I believed..."
> > > > > ---Matrix III
> > > > >
> > > >
> > > >
> > > >
> > > >
> > > >
> > >
> > ____________________________________________________________________________________
> > > > Bored stiff? Loosen up...
> > > > Download and play hundreds of games for free on
> > > Yahoo! Games.
> > > > http://games.yahoo.com/games/front
> > > >
> > >
> > >
> > > --
> > > Weiwei Shi, Ph.D
> > > Research Scientist
> > > GeneGO, Inc.
> > >
> > > "Did you always know?"
> > > "No, I did not. But I believed..."
> > > ---Matrix III
> > >
> >
> >
> >
> >
> > ____________________________________________________________________________________
> > We won't tell. Get more on shows you hate to love
> > (and love to hate): Yahoo! TV's Guilty Pleasures list.
> > http://tv.yahoo.com/collections/265
> >
>
>
> --
> Weiwei Shi, Ph.D
> Research Scientist
> GeneGO, Inc.
>
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From MGilbert at jacqueswhitford.com  Fri Jan 26 22:07:09 2007
From: MGilbert at jacqueswhitford.com (Marianne Gilbert)
Date: Fri, 26 Jan 2007 17:07:09 -0400
Subject: [R] Form of the equation produced by a GLM with Poisson family
	and	log link function
Message-ID: <s5ba354c.033@dartmouth.jacqueswhitford.com>

Hi everyone,

My background is not math and I am trying to figure out exactly what equation to use to map a response variable in GIS based on the coefficients obtained from the GLM and the values of the independent variables in each grid cell of my study area. Most specifically, I want to know how to incorporate the Poisson family and log link function in the equation. I would really appreciate if someone could help me with this or direct me towards literature that would help me do this.

Here are the coefficients from my model:

Call:  glm(formula = count ~ slope + sst + chl + dist + ice, family = poisson,      data = dfo2003cc) 

Coefficients:
(Intercept)        slope          sst          chl         dist          ice  
 -6.884e-01   -6.740e-01    5.644e-01   -2.694e+00   -7.278e-05    1.044e-01  
Degrees of Freedom: 412 Total (i.e. Null);  407 Residual
Null Deviance:      544.7 
Residual Deviance: 413.2        AIC: 512.2 

Thanks in advance!  


Marianne Gilbert, M.Sc., R.P.Bio.
Environmental Scientist - Marine Biology
Jacques Whitford
4370 Dominion St., 5th Floor
Burnaby, B.C.
V5G 4L7, Canada
Tel. (604) 436-3014, ext. 253
Fax. (604) 436-3752

*****************************************************

This e-mail message, including attachments, is confidential ...{{dropped}}


From bgreen at dyson.brisnet.org.au  Fri Jan 26 22:26:45 2007
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Sat, 27 Jan 2007 07:26:45 +1000
Subject: [R] replicating the odds ratio from a published study
In-Reply-To: <45BA08C7.4060008@biostat.ku.dk>
References: <7.0.0.16.0.20070126130450.0195bf38@aghmed.fsnet.co.uk>
	<mailman.17.1169636407.29747.r-help@stat.math.ethz.ch>
	<5.1.0.14.0.20070126185720.00d09858@pop3.brisnet.org.au>
	<7.0.0.16.0.20070126130450.0195bf38@aghmed.fsnet.co.uk>
Message-ID: <5.1.0.14.0.20070127071119.00d64618@pop3.brisnet.org.au>

Peetr & Michael,

I now see my description may have confused the issue.  I do want to compare 
odds ratios across studies - in the sense that I want to create a table 
with the respective odds ratio for each study. I do not need to 
statistically test two sets of odds ratios.

What I want to do is ensure the method I use to compute an odds ratio is 
accurate and intended to check my method against published sources.

The paper I selected by Schanda et al (2004). Homicide and major mental 
disorders. Acta Psychiatr Scand, 11:98-107 reports a total sample of 1087. 
Odds ratios are reported separately for men and women. There were 961 men 
all of whom were convicted of homicide. Of these 961 men, 41 were diagnosed 
with schizophrenia. The unadjusted odds ratio is for this  group of 41 is 
cited as 6.52   (4.70-9.00).  They also report the general population aged 
over 15 with schizophrenia =20,109 and the total population =2,957,239.

Any further clarification is much appreciated,

regards

Bob Green





., At 02:57 PM 26/01/2007 +0100, Peter Dalgaard wrote:
>Michael Dewey wrote:
> > At 09:04 26/01/2007, Bob Green wrote:
> >
> >> I wanted to compare odds ratio across studies and tried to replicate
> >> the results from a study but have not been able to determine how to
> >> do this in R.
> >>
> >> The study reported a sample of 961 men, of whom 41 had a disorder.
> >> The reported raw odds ratio was 6.52 (4.70-9.00)
> >>
> >
> > For an odds ratio you require two odds from which you form the odds ratio.
> > You only have one odds.
> > Do you have another one lying around somewhere?
> >
>Alternatively, the odds ratio presumably compares two groups. If you
>know the group sizes, the two odds ratios may be reconstructed. If I
>make a wild guess that the groups are roughly equal, I might get
>
> > M <- cbind(c(460,460),c(6,35))
> > M
>      [,1] [,2]
>[1,]  460    6
>[2,]  460   35
> > fisher.test(M)
>
>         Fisher's Exact Test for Count Data
>
>data:  M
>p-value = 7.406e-06
>alternative hypothesis: true odds ratio is not equal to 1
>95 percent confidence interval:
>   2.393209 17.104976
>sample estimates:
>odds ratio
>   5.824317
>
>Judging by the c.i., the groups are probably *not* of similar size. I
>suppose that the high-incidence group is a bit smaller so that the count
>of advverse events is more similar. M <- cbind(c(803,117),c(21,20)) is a
>bit more like it, but your (Bob's) confidence interval is narrower even
>than this.
>
>--
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From lalithaviswanath at yahoo.com  Fri Jan 26 22:38:02 2007
From: lalithaviswanath at yahoo.com (lalitha viswanath)
Date: Fri, 26 Jan 2007 13:38:02 -0800 (PST)
Subject: [R] Package for phylogenetic tree analyses
Message-ID: <758285.71640.qm@web43103.mail.sp1.yahoo.com>

Hi
I am looking for a package that
1. reads in a phylogenetic tree in NEXUS format
2. given two members/nodes on the tree, can return the
distance between the two using the tree.

I came across the following packages on CRAN
ouch, ape, apTreeShape, phylgr all of which seem to
provide extensive range of functions for reading in a
Nexus-format tree and performing phylogenetic
analyses, tree comparisons etc, but none to the best
of my undestanding seem to provide a function obtain
distances (in terms of branch lengths) between two
nodes on a single tree.
I am working with just one tree and need a function to
return distances between various pairs of nodes on the
tree.

Is there any other package out there that has this
functionality?

Thanks for your responses to my earlier queries. As a
beginning R programmer, your responses have been of
utmost help and guidance.

Lalitha


 
____________________________________________________________________________________

Access over 1 million songs.


From bgreen at dyson.brisnet.org.au  Fri Jan 26 22:46:28 2007
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Sat, 27 Jan 2007 07:46:28 +1000
Subject: [R] replicating the odds ratio from a published study- post # 2
Message-ID: <5.1.0.14.0.20070127073847.00d4ed18@pop3.brisnet.org.au>

Peter & Michael,

I just came across the following on another mailing list and realized that 
my use (and the authors of the article use of the term 'odds ratio' ) is 
probably incorrect. I believe my interest is in the 'odds' of schizophrenia 
among the population of homicide, rather than a comparison of odds .

"Yours seems a good idea, Kevin, if you are only interested in computing 
the ODDS of disease (not the odds RATIO). The odds of disease
equal the probability of disease divided by the probability of non-disease, 
i.e. p/(1-p), where p is the proportion of cases at or above the cutoff
point. An odds RATIO is the ratio of two odds, e.g. the odds for vaccinated"

If it is the odds advice regarding the appropriate R script would be useful.

regards

Bob Green





., At 02:57 PM 26/01/2007 +0100, Peter Dalgaard wrote:
>Michael Dewey wrote:
> > At 09:04 26/01/2007, Bob Green wrote:
> >
> >> I wanted to compare odds ratio across studies and tried to replicate
> >> the results from a study but have not been able to determine how to
> >> do this in R.
> >>
> >> The study reported a sample of 961 men, of whom 41 had a disorder.
> >> The reported raw odds ratio was 6.52 (4.70-9.00)
> >>
> >
> > For an odds ratio you require two odds from which you form the odds ratio.
> > You only have one odds.
> > Do you have another one lying around somewhere?
> >
>Alternatively, the odds ratio presumably compares two groups. If you
>know the group sizes, the two odds ratios may be reconstructed. If I
>make a wild guess that the groups are roughly equal, I might get
>
> > M <- cbind(c(460,460),c(6,35))
> > M
>      [,1] [,2]
>[1,]  460    6
>[2,]  460   35
> > fisher.test(M)
>
>         Fisher's Exact Test for Count Data
>
>data:  M
>p-value = 7.406e-06
>alternative hypothesis: true odds ratio is not equal to 1
>95 percent confidence interval:
>   2.393209 17.104976
>sample estimates:
>odds ratio
>   5.824317
>
>Judging by the c.i., the groups are probably *not* of similar size. I
>suppose that the high-incidence group is a bit smaller so that the count
>of advverse events is more similar. M <- cbind(c(803,117),c(21,20)) is a
>bit more like it, but your (Bob's) confidence interval is narrower even
>than this.
>
>--
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From p.dalgaard at biostat.ku.dk  Fri Jan 26 23:01:11 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 26 Jan 2007 23:01:11 +0100
Subject: [R] replicating the odds ratio from a published study
In-Reply-To: <5.1.0.14.0.20070127071119.00d64618@pop3.brisnet.org.au>
References: <7.0.0.16.0.20070126130450.0195bf38@aghmed.fsnet.co.uk>	<mailman.17.1169636407.29747.r-help@stat.math.ethz.ch>	<5.1.0.14.0.20070126185720.00d09858@pop3.brisnet.org.au>	<7.0.0.16.0.20070126130450.0195bf38@aghmed.fsnet.co.uk>
	<5.1.0.14.0.20070127071119.00d64618@pop3.brisnet.org.au>
Message-ID: <45BA7A27.1030004@biostat.ku.dk>

Bob Green wrote:
> Peetr & Michael,
>
> I now see my description may have confused the issue.  I do want to compare 
> odds ratios across studies - in the sense that I want to create a table 
> with the respective odds ratio for each study. I do not need to 
> statistically test two sets of odds ratios.
>
> What I want to do is ensure the method I use to compute an odds ratio is 
> accurate and intended to check my method against published sources.
>
> The paper I selected by Schanda et al (2004). Homicide and major mental 
> disorders. Acta Psychiatr Scand, 11:98-107 reports a total sample of 1087. 
> Odds ratios are reported separately for men and women. There were 961 men 
> all of whom were convicted of homicide. Of these 961 men, 41 were diagnosed 
> with schizophrenia. The unadjusted odds ratio is for this  group of 41 is 
> cited as 6.52   (4.70-9.00).  They also report the general population aged 
> over 15 with schizophrenia =20,109 and the total population =2,957,239.
>
> Any further clarification is much appreciated,
>
>   
A fisher.test on the following matrix seems about right:
 > matrix(c(41,920,20109-41,2957239-20109-920),2)

     [,1]    [,2]
[1,]   41   20068
[2,]  920 2936210

 > fisher.test(matrix(c(41,920,20109-41,2957239-20109-920),2))

        Fisher's Exact Test for Count Data

data:  matrix(c(41, 920, 20109 - 41, 2957239 - 20109 - 920), 2)
p-value < 2.2e-16
alternative hypothesis: true odds ratio is not equal to 1
95 percent confidence interval:
 4.645663 8.918425
sample estimates:
odds ratio
  6.520379

The c.i. is not precisely the same as your source. This could be down to 
a different approximation (R's is based on the noncentral hypergeometric 
distribution), but the classical asymptotic formula gives

 > exp(log(41*2936210/920/20068)+qnorm(c(.025,.975))*sqrt(sum(1/M)))
[1] 4.767384 8.918216

which is closer, but still a bit narrower.


From rmailbox at justemail.net  Sat Jan 27 05:18:49 2007
From: rmailbox at justemail.net (Eric)
Date: Fri, 26 Jan 2007 20:18:49 -0800
Subject: [R] CGIwithR and visible output
	of	'invisible(capture.output(library(...)))'
In-Reply-To: <45B95EB9.4010503@anicca-vijja.de>
References: <45B95EB9.4010503@anicca-vijja.de>
Message-ID: <45BAD2A9.1000505@justemail.net>

library(R2HTML, verbose = FALSE)

Leo G?rtler wrote:
> Dear alltogether,
>
> I want to use CGIwithR in conjunction with R2HTML.
>
> A small example called 'test.R':
>
> #####
>
> #! /usr/bin/R
> invisible(capture.output(library(R2HTML)))
> HTML(summary(as.numeric(scanText(formData$numbers))), file=stdout())
>
> #####
>
> The script gets its input via 'CGIwithR.cgi' and contains the variable
> "numbers."
>
> The 'HTML' output (-> summary() in this example) goes well, but the
> loading of the R2HTML library is also printed (!) in the resulting html
> page.
>
> As I took the example from the original CGIwithR examples and I also
> read the manpages of invisible() and capture.output(), I do not find a
> logical reason why this still happens.
>
> Deleting the line with "invisible ... library().." and adding
> 'library(R2HTML)' to .Rprofile does not solve the problem.
>
> I use ubuntu edgy, R Version 2.3.1 (2006-06-01)
>
> THANKS!
>
> best wishes,
>
> leo
>
>
> PS: the output is (uncut)
>
> <---->
> MiniUnz 1.01b, demo of zLib + Unz package written by Gilles Vollant more
> info at http://www.winimage.com/zLibDll/unzip.html
> /usr/local/lib/R/site-library/R2HTML/output/R2HTMLstuff.zip opened
> extracting: ASCIIMathML.js extracting: factor.gif extracting:
> gridR2HTML.css extracting: gridR2HTML.js extracting: numeric.gif
> extracting: Pastel.css extracting: R2HTML.css extracting: R2HTMLlogo.gif
> creating directory: runtime/ creating directory: runtime/lib/
> extracting: runtime/lib/grid.js extracting: runtime/readme.txt creating
> directory: runtime/styles/ creating directory: runtime/styles/classic/
> extracting: runtime/styles/classic/gecko.xml extracting:
> runtime/styles/classic/grid.css extracting:
> runtime/styles/classic/grid.png extracting:
> runtime/styles/classic/icons.png extracting:
> runtime/styles/classic/loading.gif creating directory:
> runtime/styles/flat/ extracting: runtime/styles/flat/gecko.xml
> extracting: runtime/styles/flat/grid.css extracting:
> runtime/styles/flat/grid.png extracting: runtime/styles/flat/icons.png
> extracting: runtime/styles/flat/loading.gif creating directory:
> runtime/styles/xp/ extracting: runtime/styles/xp/gecko.xml extracting:
> runtime/styles/xp/grid.css extracting: runtime/styles/xp/grid.png
> extracting: runtime/styles/xp/icons.png extracting:
> runtime/styles/xp/loading.gif extracting: SciViews.css extracting:
> tablesort.htc
> </---->
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From leekell at gmail.com  Sat Jan 27 06:00:52 2007
From: leekell at gmail.com (Kelly Lee)
Date: Fri, 26 Jan 2007 21:00:52 -0800
Subject: [R] customizing covariance matrices
Message-ID: <45BADC84.1070907@gmail.com>

Hello R-users,
Does anyone know how to customize a corStruct object to be used in gls? 
I would either like to create the covariance matrix from scratch, or 
alter the diagonal elements of an existing corStruct object and pass 
that to gls.

Any ideas would be appreciated!


From pwang at berkeley.edu  Sat Jan 27 06:52:25 2007
From: pwang at berkeley.edu (Patrick Wang)
Date: Fri, 26 Jan 2007 21:52:25 -0800 (PST)
Subject: [R] Does inverse function exist in R
Message-ID: <63609.76.167.74.27.1169877145.squirrel@calmail.berkeley.edu>

Hi,

I wrote a simple derivative program

(ftest=deriv(y~x^2, c("x"), function(x){} ))

I put (ftest=deriv(y~x^2, c("x"), function(x){} ))(1) which return 2 which
is correct.

however, if I want the output to be 0 and hopefully a new inverse function
can take in
output (2) and return x=1. Can this be accomplished. Maybe output(0) can
return x=0

Thanks
pat


From ripley at stats.ox.ac.uk  Sat Jan 27 08:28:05 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 27 Jan 2007 07:28:05 +0000 (GMT)
Subject: [R] Form of the equation produced by a GLM with Poisson family
 and log link function
In-Reply-To: <s5ba354c.033@dartmouth.jacqueswhitford.com>
References: <s5ba354c.033@dartmouth.jacqueswhitford.com>
Message-ID: <Pine.LNX.4.64.0701270720180.26512@gannet.stats.ox.ac.uk>

Have you looked at the references on ?glm ?  They will all explain this to 
you (the first and last are simpler than the other two).

The short answer is that predictions from the linear part of your model 
are for the natural log of the mean response.  That is the default option 
for predict() on a glm fit, but there is also predict(fit, 
type="response") which predicts mean response.  The Poisson family is an 
assumption you have supplied about how the response counts vary about the 
mean, that they are Poisson and independent with a mwan predicted by the 
model (for the 'true' coefficients).

These are statistical (and not 'math') issues.

On Fri, 26 Jan 2007, Marianne Gilbert wrote:

> Hi everyone,
>
> My background is not math and I am trying to figure out exactly what 
> equation to use to map a response variable in GIS based on the 
> coefficients obtained from the GLM and the values of the independent 
> variables in each grid cell of my study area. Most specifically, I want 
> to know how to incorporate the Poisson family and log link function in 
> the equation. I would really appreciate if someone could help me with 
> this or direct me towards literature that would help me do this.
>
> Here are the coefficients from my model:
>
> Call:  glm(formula = count ~ slope + sst + chl + dist + ice, family = poisson,      data = dfo2003cc)
>
> Coefficients:
> (Intercept)        slope          sst          chl         dist          ice
> -6.884e-01   -6.740e-01    5.644e-01   -2.694e+00   -7.278e-05    1.044e-01
> Degrees of Freedom: 412 Total (i.e. Null);  407 Residual
> Null Deviance:      544.7
> Residual Deviance: 413.2        AIC: 512.2
>
> Thanks in advance!
>
>
> Marianne Gilbert, M.Sc., R.P.Bio.
> Environmental Scientist - Marine Biology
> Jacques Whitford
> 4370 Dominion St., 5th Floor
> Burnaby, B.C.
> V5G 4L7, Canada
> Tel. (604) 436-3014, ext. 253
> Fax. (604) 436-3752
>
> *****************************************************
>
> This e-mail message, including attachments, is confidential ...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From gallon.li at gmail.com  Sat Jan 27 10:53:46 2007
From: gallon.li at gmail.com (gallon li)
Date: Sat, 27 Jan 2007 17:53:46 +0800
Subject: [R] how to handle a longitudinal data
Message-ID: <54f7e7c30701270153l76786b13u9558b54566d4dab1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070127/07878385/attachment.pl 

From ccleland at optonline.net  Sat Jan 27 11:07:04 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Sat, 27 Jan 2007 05:07:04 -0500
Subject: [R] how to handle a longitudinal data
In-Reply-To: <54f7e7c30701270153l76786b13u9558b54566d4dab1@mail.gmail.com>
References: <54f7e7c30701270153l76786b13u9558b54566d4dab1@mail.gmail.com>
Message-ID: <45BB2448.9000405@optonline.net>

gallon li wrote:
> i have a data set with repeated measures on same people, structure like
> below:
> 
> id x1 x2 ...
> 001 10 20 ...
> 001 8 45 ...
> 001 4 2 ...
> 002 ....
> 002 ...
> 002 ....
> 002 ....
> 003 ....
> .......
> 
> what is the easist way to show how many observations for each subject id?

  If the data are in a data frame called df, do the following:

table(df$id)

  Also, the following is useful for seeing how many people have a
particular number of observations:

table(table(df$id))

> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From gallon.li at gmail.com  Sat Jan 27 11:12:32 2007
From: gallon.li at gmail.com (gallon li)
Date: Sat, 27 Jan 2007 18:12:32 +0800
Subject: [R] Fwd:  how to handle a longitudinal data
In-Reply-To: <54f7e7c30701270211r3a316e13o3d462a10e169d941@mail.gmail.com>
References: <54f7e7c30701270153l76786b13u9558b54566d4dab1@mail.gmail.com>
	<45BB2448.9000405@optonline.net>
	<54f7e7c30701270211r3a316e13o3d462a10e169d941@mail.gmail.com>
Message-ID: <54f7e7c30701270212g28293ebep7662d770e9fb8a5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070127/80d39cc2/attachment.pl 

From rdiaz02 at gmail.com  Sat Jan 27 11:21:40 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Sat, 27 Jan 2007 11:21:40 +0100
Subject: [R] Package for phylogenetic tree analyses
In-Reply-To: <758285.71640.qm@web43103.mail.sp1.yahoo.com>
References: <758285.71640.qm@web43103.mail.sp1.yahoo.com>
Message-ID: <624934630701270221jc4a9fa1vb40a9b8209e6dea4@mail.gmail.com>

Dear Lalitha,

On 1/26/07, lalitha viswanath <lalithaviswanath at yahoo.com> wrote:
> Hi
> I am looking for a package that
> 1. reads in a phylogenetic tree in NEXUS format
> 2. given two members/nodes on the tree, can return the
> distance between the two using the tree.
>
> I came across the following packages on CRAN
> ouch, ape, apTreeShape, phylgr all of which seem to
> provide extensive range of functions for reading in a
> Nexus-format tree and performing phylogenetic
> analyses, tree comparisons etc, but none to the best
> of my undestanding seem to provide a function obtain
> distances (in terms of branch lengths) between two
> nodes on a single tree.
> I am working with just one tree and need a function to
> return distances between various pairs of nodes on the
> tree.
>
> Is there any other package out there that has this
> functionality?


I've been away from that area for some years now, but certainly our
phylogr package will not do what you want. However, I think there are
various external (non R) programs that will do it, and that might be
all you need if this is just a sporadic use. The set of programs
distributed and maintained by Ted Garland (PDAP) did provide the type
of output you want (in the form of a matrix of distances). I am sure
there are others out there (I bet PHYLIP does it too).

HTH,

R.


>
> Thanks for your responses to my earlier queries. As a
> beginning R programmer, your responses have been of
> utmost help and guidance.
>
> Lalitha
>
>
>
> ____________________________________________________________________________________
>
> Access over 1 million songs.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From gallon.li at gmail.com  Sat Jan 27 11:34:40 2007
From: gallon.li at gmail.com (gallon li)
Date: Sat, 27 Jan 2007 18:34:40 +0800
Subject: [R] unequal number of observations for longitudinal data
Message-ID: <54f7e7c30701270234g75730e65g95d13a10965239f9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070127/eace85c1/attachment.pl 

From ccleland at optonline.net  Sat Jan 27 11:58:00 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Sat, 27 Jan 2007 05:58:00 -0500
Subject: [R] unequal number of observations for longitudinal data
In-Reply-To: <54f7e7c30701270234g75730e65g95d13a10965239f9@mail.gmail.com>
References: <54f7e7c30701270234g75730e65g95d13a10965239f9@mail.gmail.com>
Message-ID: <45BB3038.9020702@optonline.net>

gallon li wrote:
> i have a large longitudinal data set. The number of observations for each
> subject is not the same across the sample. The largest number of a subject
> is 5 and the smallest number is 1.
> 
> now i want to make each subject to have the same number of observations by
> filling zero, e.g., my original sample is
> 
> id x
> 001 10
> 001 30
> 001 20
> 002 10
> 002 20
> 002 40
> 002 80
> 002 70
> 003 20
> 003 40
> 004 ......
> 
> now i wish to make the data like
> 
>  id x
> 001 10
> 001 30
> 001 20
> 001 0
> 001 0
> 002 10
> 002 20
> 002 40
> 002 80
> 002 70
> 003 20
> 003 40
> 003 0
> 003 0
> 003 0
> 004 ......
> 
> so that each id has exactly 5 observations. is there a function which can
> allow me do this quickly?

  Filling in with zeros seems like a bad idea, but here is an approach
to filling in with NAs.  I will leave replacing the NAs with zeros to you.

df.long <- data.frame(id = c(1,1,1,2,2,2,2,2,3,3), x = runif(10),
                      time = c(1,2,5,1,2,3,4,5,2,4))

df.long
   id          x time
1   1 0.72888215    1
2   1 0.60893548    2
3   1 0.41347690    5
4   2 0.79388248    1
5   2 0.05810054    2
6   2 0.02451654    3
7   2 0.85464775    4
8   2 0.15970365    5
9   3 0.22856183    2
10  3 0.38291471    4

df.wide <- reshape(df, idvar = "id", v.names = "x", direction="wide")

df.wide
  id       x.1       x.2       x.5       x.3       x.4
1  1 0.6375135 0.1651258 0.3210223        NA        NA
4  2 0.9878134 0.8909020 0.9853269 0.7747615 0.3834130
9  3        NA 0.3586109        NA        NA 0.8310539

df.long2 <- reshape(df.wide, direction="long")

df.long2
    id time         x
1.1  1    1 0.6375135
2.1  2    1 0.9878134
3.1  3    1        NA
1.2  1    2 0.1651258
2.2  2    2 0.8909020
3.2  3    2 0.3586109
1.5  1    5 0.3210223
2.5  2    5 0.9853269
3.5  3    5        NA
1.3  1    3        NA
2.3  2    3 0.7747615
3.3  3    3        NA
1.4  1    4        NA
2.4  2    4 0.3834130
3.4  3    4 0.8310539

  This assumes that your data in the "long" format has a time variable.
 See the help page for reshape() for more details.

> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ggrothendieck at gmail.com  Sat Jan 27 12:15:54 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 27 Jan 2007 06:15:54 -0500
Subject: [R] unequal number of observations for longitudinal data
In-Reply-To: <54f7e7c30701270234g75730e65g95d13a10965239f9@mail.gmail.com>
References: <54f7e7c30701270234g75730e65g95d13a10965239f9@mail.gmail.com>
Message-ID: <971536df0701270315x3c963ea9q5b9ff8b7eaf69256@mail.gmail.com>

merge.zoo in the zoo package has an n-way merge supporting zero fill:

library(zoo)

DF <- data.frame(id = c(1, 1, 1, 2, 2, 2, 2, 2, 3, 3), x = c(10,
30, 20, 10, 20, 40, 80, 70, 20, 40))

as.data.frame(do.call(merge, c(lapply(unstack(DF, x ~ id), zoo), fill = 0)))

# last line can alternately be

f <- function(DF) zoo(DF$x)
as.data.frame(do.call(merge, c(by(DF, DF$id, f), fill = 0)))



On 1/27/07, gallon li <gallon.li at gmail.com> wrote:
> i have a large longitudinal data set. The number of observations for each
> subject is not the same across the sample. The largest number of a subject
> is 5 and the smallest number is 1.
>
> now i want to make each subject to have the same number of observations by
> filling zero, e.g., my original sample is
>
> id x
> 001 10
> 001 30
> 001 20
> 002 10
> 002 20
> 002 40
> 002 80
> 002 70
> 003 20
> 003 40
> 004 ......
>
> now i wish to make the data like
>
>  id x
> 001 10
> 001 30
> 001 20
> 001 0
> 001 0
> 002 10
> 002 20
> 002 40
> 002 80
> 002 70
> 003 20
> 003 40
> 003 0
> 003 0
> 003 0
> 004 ......
>
> so that each id has exactly 5 observations. is there a function which can
> allow me do this quickly?
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gallon.li at gmail.com  Sat Jan 27 12:22:04 2007
From: gallon.li at gmail.com (gallon li)
Date: Sat, 27 Jan 2007 19:22:04 +0800
Subject: [R] unequal number of observations for longitudinal data
In-Reply-To: <45BB3038.9020702@optonline.net>
References: <54f7e7c30701270234g75730e65g95d13a10965239f9@mail.gmail.com>
	<45BB3038.9020702@optonline.net>
Message-ID: <54f7e7c30701270322h7d74e1dk73b2cbe9317e81bf@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070127/c19b9402/attachment.pl 

From ccleland at optonline.net  Sat Jan 27 12:41:05 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Sat, 27 Jan 2007 06:41:05 -0500
Subject: [R] unequal number of observations for longitudinal data
In-Reply-To: <54f7e7c30701270322h7d74e1dk73b2cbe9317e81bf@mail.gmail.com>
References: <54f7e7c30701270234g75730e65g95d13a10965239f9@mail.gmail.com>
	<45BB3038.9020702@optonline.net>
	<54f7e7c30701270322h7d74e1dk73b2cbe9317e81bf@mail.gmail.com>
Message-ID: <45BB3A51.1000204@optonline.net>

gallon li wrote:
> Two questions:
> 
> 1. How do I replace "NA" with 0?

df.long2$x <- replace(df.long2$x, is.na(df.long2$x), 0)

?replace

> 2. How can I sort the observations by their id instead of by time? (actually
> i can see what you produced is automatically sorted by id; but in my case,
> the output data is sorted by time)

df.long2 <- df.long2[order(df.long2$id),]

or better ...

df.long2 <- df.long2[order(row.names(df.long2)),]

df.long2
    id time         x
1.1  1    1 0.6375135
1.2  1    2 0.1651258
1.3  1    3 0.0000000
1.4  1    4 0.0000000
1.5  1    5 0.3210223
2.1  2    1 0.9878134
2.2  2    2 0.8909020
2.3  2    3 0.7747615
2.4  2    4 0.3834130
2.5  2    5 0.9853269
3.1  3    1 0.0000000
3.2  3    2 0.3586109
3.3  3    3 0.0000000
3.4  3    4 0.8310539
3.5  3    5 0.0000000

R-FAQ 7.23 How can I sort the rows of a data frame?

http://finzi.psych.upenn.edu/R/doc/manual/R-FAQ.html

> On 1/27/07, Chuck Cleland <ccleland at optonline.net> wrote:
>> gallon li wrote:
>>> i have a large longitudinal data set. The number of observations for
>> each
>>> subject is not the same across the sample. The largest number of a
>> subject
>>> is 5 and the smallest number is 1.
>>>
>>> now i want to make each subject to have the same number of observations
>> by
>>> filling zero, e.g., my original sample is
>>>
>>> id x
>>> 001 10
>>> 001 30
>>> 001 20
>>> 002 10
>>> 002 20
>>> 002 40
>>> 002 80
>>> 002 70
>>> 003 20
>>> 003 40
>>> 004 ......
>>>
>>> now i wish to make the data like
>>>
>>>  id x
>>> 001 10
>>> 001 30
>>> 001 20
>>> 001 0
>>> 001 0
>>> 002 10
>>> 002 20
>>> 002 40
>>> 002 80
>>> 002 70
>>> 003 20
>>> 003 40
>>> 003 0
>>> 003 0
>>> 003 0
>>> 004 ......
>>>
>>> so that each id has exactly 5 observations. is there a function which
>> can
>>> allow me do this quickly?
>> Filling in with zeros seems like a bad idea, but here is an approach
>> to filling in with NAs.  I will leave replacing the NAs with zeros to you.
>>
>> df.long <- data.frame(id = c(1,1,1,2,2,2,2,2,3,3), x = runif(10),
>>                      time = c(1,2,5,1,2,3,4,5,2,4))
>>
>> df.long
>>   id          x time
>> 1   1 0.72888215    1
>> 2   1 0.60893548    2
>> 3   1 0.41347690    5
>> 4   2 0.79388248    1
>> 5   2 0.05810054    2
>> 6   2 0.02451654    3
>> 7   2 0.85464775    4
>> 8   2 0.15970365    5
>> 9   3 0.22856183    2
>> 10  3 0.38291471    4
>>
>> df.wide <- reshape(df, idvar = "id", v.names = "x", direction="wide")
>>
>> df.wide
>> id       x.1       x.2       x.5       x.3       x.4
>> 1  1 0.6375135 0.1651258 0.3210223        NA        NA
>> 4  2 0.9878134 0.8909020 0.9853269 0.7747615 0.3834130
>> 9  3        NA 0.3586109        NA        NA 0.8310539
>>
>> df.long2 <- reshape(df.wide, direction="long")
>>
>> df.long2
>>    id time         x
>> 1.1  1    1 0.6375135
>> 2.1  2    1 0.9878134
>> 3.1  3    1        NA
>> 1.2  1    2 0.1651258
>> 2.2  2    2 0.8909020
>> 3.2  3    2 0.3586109
>> 1.5  1    5 0.3210223
>> 2.5  2    5 0.9853269
>> 3.5  3    5        NA
>> 1.3  1    3        NA
>> 2.3  2    3 0.7747615
>> 3.3  3    3        NA
>> 1.4  1    4        NA
>> 2.4  2    4 0.3834130
>> 3.4  3    4 0.8310539
>>
>> This assumes that your data in the "long" format has a time variable.
>> See the help page for reshape() for more details.
>>
>>>       [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> --
>> Chuck Cleland, Ph.D.
>> NDRI, Inc.
>> 71 West 23rd Street, 8th floor
>> New York, NY 10010
>> tel: (212) 845-4495 (Tu, Th)
>> tel: (732) 512-0171 (M, W, F)
>> fax: (917) 438-0894
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From gallon.li at gmail.com  Sat Jan 27 12:54:13 2007
From: gallon.li at gmail.com (gallon li)
Date: Sat, 27 Jan 2007 19:54:13 +0800
Subject: [R] unequal number of observations for longitudinal data
In-Reply-To: <45BB3A51.1000204@optonline.net>
References: <54f7e7c30701270234g75730e65g95d13a10965239f9@mail.gmail.com>
	<45BB3038.9020702@optonline.net>
	<54f7e7c30701270322h7d74e1dk73b2cbe9317e81bf@mail.gmail.com>
	<45BB3A51.1000204@optonline.net>
Message-ID: <54f7e7c30701270354p1a8942c5o280e225b5242e02a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070127/f8bedd37/attachment.pl 

From phwang2000 at ucla.edu  Sat Jan 27 06:45:38 2007
From: phwang2000 at ucla.edu (phwang2000 at ucla.edu)
Date: Fri, 26 Jan 2007 21:45:38 -0800
Subject: [R] Does inverse function exist
Message-ID: <20070126214538.mg9j4ezb4440sk80@mail.ucla.edu>

Hi,

I wrote a simple derivative program

(ftest=deriv(y~x^2, c("x"), function(x){} ))

I put (ftest=deriv(y~x^2, c("x"), function(x){} ))(1) which return 2  
which is correct.

however, if I want the output to be 0 and hopefully a new inverse  
function can take in output (2) and return x=1. Can this be  
accomplished. Maybe output(0) can return x=0

Thanks
pat


From huxiaopengstat at gmail.com  Sat Jan 27 14:34:51 2007
From: huxiaopengstat at gmail.com (xiaopeng hu)
Date: Sat, 27 Jan 2007 21:34:51 +0800
Subject: [R] when i compile 2.4.1 in linux ,i got the message
Message-ID: <ffe0539f0701270534g258ac098y8ecdd80d05cd3cd0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070127/43d751ef/attachment.pl 

From leog at anicca-vijja.de  Sat Jan 27 14:39:16 2007
From: leog at anicca-vijja.de (=?ISO-8859-1?Q?Leo_G=FCrtler?=)
Date: Sat, 27 Jan 2007 14:39:16 +0100
Subject: [R] CGIwithR and visible output of
	'invisible(capture.output(library(...)))'
In-Reply-To: <45BAD2A9.1000505@justemail.net>
References: <45B95EB9.4010503@anicca-vijja.de> <45BAD2A9.1000505@justemail.net>
Message-ID: <45BB5604.5030109@anicca-vijja.de>

Eric schrieb:
> library(R2HTML, verbose = FALSE)
> 

Dear Eric,

thanks - I tried that, but that does not work for me. And it does not
depend whether R2HTML is loaded via script/ batch job (-> webserver,
cgi) or directly within R.

The same happens (of course) by using

require(R2HTML, quietly=TRUE)

Are there any further options I missed?
The problem seems to be that the file "R2HTMLstuff.zip" is extracted
every time (although this is not necessary) - how can I ged rid of that?

thanks,

best,

leo


----output----

> library(R2HTML, verbose=FALSE)
MiniUnz 1.01b, demo of zLib + Unz package written by Gilles Vollant
more info at http://www.winimage.com/zLibDll/unzip.html

/usr/local/lib/R/site-library/R2HTML/output/R2HTMLstuff.zip opened
 extracting: ASCIIMathML.js
 extracting: factor.gif
 extracting: gridR2HTML.css
 extracting: gridR2HTML.js
 extracting: numeric.gif
 extracting: Pastel.css
 extracting: R2HTML.css
 extracting: R2HTMLlogo.gif
creating directory: runtime/
creating directory: runtime/lib/
 extracting: runtime/lib/grid.js
 extracting: runtime/readme.txt
creating directory: runtime/styles/
creating directory: runtime/styles/classic/
 extracting: runtime/styles/classic/gecko.xml
 extracting: runtime/styles/classic/grid.css
 extracting: runtime/styles/classic/grid.png
 extracting: runtime/styles/classic/icons.png
 extracting: runtime/styles/classic/loading.gif
creating directory: runtime/styles/flat/
 extracting: runtime/styles/flat/gecko.xml
 extracting: runtime/styles/flat/grid.css
 extracting: runtime/styles/flat/grid.png
 extracting: runtime/styles/flat/icons.png
 extracting: runtime/styles/flat/loading.gif
creating directory: runtime/styles/xp/
 extracting: runtime/styles/xp/gecko.xml
 extracting: runtime/styles/xp/grid.css
 extracting: runtime/styles/xp/grid.png
 extracting: runtime/styles/xp/icons.png
 extracting: runtime/styles/xp/loading.gif
 extracting: SciViews.css
 extracting: tablesort.htc
>


From ripley at stats.ox.ac.uk  Sat Jan 27 14:44:10 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 27 Jan 2007 13:44:10 +0000 (GMT)
Subject: [R] Does inverse function exist
In-Reply-To: <20070126214538.mg9j4ezb4440sk80@mail.ucla.edu>
References: <20070126214538.mg9j4ezb4440sk80@mail.ucla.edu>
Message-ID: <Pine.LNX.4.64.0701271341200.7105@gannet.stats.ox.ac.uk>

Inverting a linear function is easy, and more generally functions need not 
be invertible.  But uniroot() is very helpful in finding an inverse of a 
monotonic function, and the idea is used in some of the qxxxxx functions.

On Fri, 26 Jan 2007, phwang2000 at ucla.edu wrote:

> Hi,
>
> I wrote a simple derivative program
>
> (ftest=deriv(y~x^2, c("x"), function(x){} ))
>
> I put (ftest=deriv(y~x^2, c("x"), function(x){} ))(1) which return 2
> which is correct.
>
> however, if I want the output to be 0 and hopefully a new inverse
> function can take in output (2) and return x=1. Can this be
> accomplished. Maybe output(0) can return x=0
>
> Thanks
> pat
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Sat Jan 27 14:57:21 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 27 Jan 2007 13:57:21 +0000 (GMT)
Subject: [R] CGIwithR and visible output of
	'invisible(capture.output(library(...)))'
In-Reply-To: <45BB5604.5030109@anicca-vijja.de>
References: <45B95EB9.4010503@anicca-vijja.de> <45BAD2A9.1000505@justemail.net>
	<45BB5604.5030109@anicca-vijja.de>
Message-ID: <Pine.LNX.4.64.0701271351200.7105@gannet.stats.ox.ac.uk>

This is done by .onLoad, so your settings are irrelevant.

We have no information here on your platform, version of R, R2HTML or 
anything else.  But almost certainly the problem is the command given by
getOption("unzip"), which looks like an unzip clone not respecting the -q 
flag.

Try using the 'real' unzip () instead, from http://www.info-zip.org/.


On Sat, 27 Jan 2007, Leo G?rtler wrote:

> Eric schrieb:
>> library(R2HTML, verbose = FALSE)
>>
>
> Dear Eric,
>
> thanks - I tried that, but that does not work for me. And it does not
> depend whether R2HTML is loaded via script/ batch job (-> webserver,
> cgi) or directly within R.
>
> The same happens (of course) by using
>
> require(R2HTML, quietly=TRUE)
>
> Are there any further options I missed?
> The problem seems to be that the file "R2HTMLstuff.zip" is extracted
> every time (although this is not necessary) - how can I ged rid of that?
>
> thanks,
>
> best,
>
> leo
>
>
> ----output----
>
>> library(R2HTML, verbose=FALSE)
> MiniUnz 1.01b, demo of zLib + Unz package written by Gilles Vollant
> more info at http://www.winimage.com/zLibDll/unzip.html
>
> /usr/local/lib/R/site-library/R2HTML/output/R2HTMLstuff.zip opened
> extracting: ASCIIMathML.js
> extracting: factor.gif
> extracting: gridR2HTML.css
> extracting: gridR2HTML.js
> extracting: numeric.gif
> extracting: Pastel.css
> extracting: R2HTML.css
> extracting: R2HTMLlogo.gif
> creating directory: runtime/
> creating directory: runtime/lib/
> extracting: runtime/lib/grid.js
> extracting: runtime/readme.txt
> creating directory: runtime/styles/
> creating directory: runtime/styles/classic/
> extracting: runtime/styles/classic/gecko.xml
> extracting: runtime/styles/classic/grid.css
> extracting: runtime/styles/classic/grid.png
> extracting: runtime/styles/classic/icons.png
> extracting: runtime/styles/classic/loading.gif
> creating directory: runtime/styles/flat/
> extracting: runtime/styles/flat/gecko.xml
> extracting: runtime/styles/flat/grid.css
> extracting: runtime/styles/flat/grid.png
> extracting: runtime/styles/flat/icons.png
> extracting: runtime/styles/flat/loading.gif
> creating directory: runtime/styles/xp/
> extracting: runtime/styles/xp/gecko.xml
> extracting: runtime/styles/xp/grid.css
> extracting: runtime/styles/xp/grid.png
> extracting: runtime/styles/xp/icons.png
> extracting: runtime/styles/xp/loading.gif
> extracting: SciViews.css
> extracting: tablesort.htc
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From bates at stat.wisc.edu  Sat Jan 27 16:04:38 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 27 Jan 2007 09:04:38 -0600
Subject: [R] Fwd: how to handle a longitudinal data
In-Reply-To: <54f7e7c30701270212g28293ebep7662d770e9fb8a5@mail.gmail.com>
References: <54f7e7c30701270153l76786b13u9558b54566d4dab1@mail.gmail.com>
	<45BB2448.9000405@optonline.net>
	<54f7e7c30701270211r3a316e13o3d462a10e169d941@mail.gmail.com>
	<54f7e7c30701270212g28293ebep7662d770e9fb8a5@mail.gmail.com>
Message-ID: <40e66e0b0701270704w63597458x35baf631d643d3e3@mail.gmail.com>

On 1/27/07, gallon li <gallon.li at gmail.com> wrote:
> actually there are too many subjects.
>
> is there a way that i can make a table such that it can tell me the
> frequency of number of observations?
>
> Like
>
> 3 4 5
> ------------
> 30 40 60
>
> which means there are 30 subjects which each has 3 obs., 40 which each has 4
> obs.....

Yes, and Chuck has already given you the answer to that question.  See
below where he describes

table(table(df$id))


>
>
>  On 1/27/07, Chuck Cleland <ccleland at optonline.net> wrote:
> >
> > gallon li wrote:
> > > i have a data set with repeated measures on same people, structure like
> > > below:
> > >
> > > id x1 x2 ...
> > > 001 10 20 ...
> > > 001 8 45 ...
> > > 001 4 2 ...
> > > 002 ....
> > > 002 ...
> > > 002 ....
> > > 002 ....
> > > 003 ....
> > > .......
> > >
> > > what is the easist way to show how many observations for each subject
> > id?
> >
> > If the data are in a data frame called df, do the following:
> >
> > table(df$id)
> >
> > Also, the following is useful for seeing how many people have a
> > particular number of observations:
> >
> > table(table(df$id))
> >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html>
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > --
> > Chuck Cleland, Ph.D.
> > NDRI, Inc.
> > 71 West 23rd Street, 8th floor
> > New York, NY 10010
> > tel: (212) 845-4495 (Tu, Th)
> > tel: (732) 512-0171 (M, W, F)
> > fax: (917) 438-0894
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From kubovy at virginia.edu  Sat Jan 27 16:22:04 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sat, 27 Jan 2007 10:22:04 -0500
Subject: [R] Unbalanced design help
Message-ID: <CDA62F3A-06BC-4D26-8761-D518AC1A7F3F@virginia.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070127/7f34169d/attachment.pl 

From vinodkgul at yahoo.com  Sat Jan 27 17:32:28 2007
From: vinodkgul at yahoo.com (vinod gullu)
Date: Sat, 27 Jan 2007 08:32:28 -0800 (PST)
Subject: [R] Canging the type of point points sizein plots..
In-Reply-To: <mailman.13.1169895604.29625.r-help@stat.math.ethz.ch>
Message-ID: <162647.22545.qm@web53807.mail.yahoo.com>

I want to plot variation of more than one variable in
single plot with different point types and points
sizes . Can someone help me to do that,

Thanks in advance.
vinod


 
____________________________________________________________________________________
Now that's room service!  Choose from over 150,000 hotels


From Dimitris.Rizopoulos at med.kuleuven.be  Sat Jan 27 18:22:42 2007
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Sat, 27 Jan 2007 18:22:42 +0100
Subject: [R] Canging the type of point points sizein plots..
In-Reply-To: <162647.22545.qm@web53807.mail.yahoo.com>
References: <162647.22545.qm@web53807.mail.yahoo.com>
Message-ID: <20070127182242.nrmrsfotltyoswgk@webmail4.kuleuven.be>

try this

x1 <- rnorm(10); y1 <- rnorm(10)
x2 <- rnorm(10); y2 <- rnorm(10)
x3 <- rnorm(10); y3 <- rnorm(10)
#################
plot(range(x1, x2, x3), range(y1, y2, y3), type = "n")
points(x1, y1, pch = 1, cex = 1, col = 1)
points(x2, y2, pch = 2, cex = 2, col = 2)
points(x3, y3, pch = 3, cex = 3, col = 3)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting vinod gullu <vinodkgul at yahoo.com>:

> I want to plot variation of more than one variable in
> single plot with different point types and points
> sizes . Can someone help me to do that,
>
> Thanks in advance.
> vinod
>
>
>
> ____________________________________________________________________________________
> Now that's room service!  Choose from over 150,000 hotels
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From neo27 at rakers.de  Sat Jan 27 20:31:43 2007
From: neo27 at rakers.de (Mark Hempelmann)
Date: Sat, 27 Jan 2007 20:31:43 +0100
Subject: [R] Conditional Selection of Columns for Tables
Message-ID: <45BBA89F.5040601@rakers.de>

Dear Wizards -

	Thank you so much for your help. That was exactly what I was looking 
for. Now, I have been working on conditional selection of columns in a 
data frame. My goal is to calculate the total revenue per sales 
representative per status in a table. I have come to a complete stop:

Let's say, we have a data.frame called df with several columns and a
number of rows:

df <- data.frame( nr=101:110, letter=LETTERS[1:10],
name=c(rep("eenie",3), rep("meenie",2), rep("miney",4),
   "moe"), revenue=round(runif(10, min=100, max=1000),0),
status=round(runif(10,min=1, max=3),0) )

gives
     nr letter   name revenue status
1  101      A  eenie     764      2
2  102      B  eenie     918      2
3  103      C  eenie     936      3
4  104      D meenie     770      2
5  105      E meenie     280      1
6  106      F  miney     172      2
7  107      G  miney     439      2
8  108      H  miney     607      1
9  109      I  miney     553      1
10 110      J    moe     322      2

where status means: 3=no deal, 2=pending, 1=good job.
now, we want the total revenue per sales representative per status in a
table.

sum( subset(df, name=="eenie", select=revenue) )

gives the total of eenie without status, but I would like to have sthg like:

status
1
name   revenue
eenie  1000
meenie 2000...

status
2
name   revenue
eenie  100
meenie 200...

Are these flat contingency tables? How can I get the results without
much hazzle in one list/ table? i did read the ?ftable and what I was 
able to derive so far is:

flat.df <- ftable(df[c("name", "revenue", "status")])

but I am unable to further agglomerate the data. hmpf.
Good God, what would I do without my R-help forum?

Thank you again
Cheers and a relaxing weekend
mark


From ggrothendieck at gmail.com  Sat Jan 27 21:18:41 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 27 Jan 2007 15:18:41 -0500
Subject: [R] Conditional Selection of Columns for Tables
In-Reply-To: <45BBA89F.5040601@rakers.de>
References: <45BBA89F.5040601@rakers.de>
Message-ID: <971536df0701271218t6f639a8brf170cb5b09d103b8@mail.gmail.com>

Try these:

aggregate(DF[4], DF[c(3,5)], sum)

xtabs(revenue ~ status + name, DF)

library(doBy)
summaryBy(revenue ~ status + name, data = DF, FUN = sum)


On 1/27/07, Mark Hempelmann <neo27 at rakers.de> wrote:
> Dear Wizards -
>
>        Thank you so much for your help. That was exactly what I was looking
> for. Now, I have been working on conditional selection of columns in a
> data frame. My goal is to calculate the total revenue per sales
> representative per status in a table. I have come to a complete stop:
>
> Let's say, we have a data.frame called df with several columns and a
> number of rows:
>
> df <- data.frame( nr=101:110, letter=LETTERS[1:10],
> name=c(rep("eenie",3), rep("meenie",2), rep("miney",4),
>   "moe"), revenue=round(runif(10, min=100, max=1000),0),
> status=round(runif(10,min=1, max=3),0) )
>
> gives
>     nr letter   name revenue status
> 1  101      A  eenie     764      2
> 2  102      B  eenie     918      2
> 3  103      C  eenie     936      3
> 4  104      D meenie     770      2
> 5  105      E meenie     280      1
> 6  106      F  miney     172      2
> 7  107      G  miney     439      2
> 8  108      H  miney     607      1
> 9  109      I  miney     553      1
> 10 110      J    moe     322      2
>
> where status means: 3=no deal, 2=pending, 1=good job.
> now, we want the total revenue per sales representative per status in a
> table.
>
> sum( subset(df, name=="eenie", select=revenue) )
>
> gives the total of eenie without status, but I would like to have sthg like:
>
> status
> 1
> name   revenue
> eenie  1000
> meenie 2000...
>
> status
> 2
> name   revenue
> eenie  100
> meenie 200...
>
> Are these flat contingency tables? How can I get the results without
> much hazzle in one list/ table? i did read the ?ftable and what I was
> able to derive so far is:
>
> flat.df <- ftable(df[c("name", "revenue", "status")])
>
> but I am unable to further agglomerate the data. hmpf.
> Good God, what would I do without my R-help forum?
>
> Thank you again
> Cheers and a relaxing weekend
> mark
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From albert.phillimore at imperial.ac.uk  Sat Jan 27 21:41:20 2007
From: albert.phillimore at imperial.ac.uk (Phillimore, Albert)
Date: Sat, 27 Jan 2007 20:41:20 -0000
Subject: [R] gsub regexp question
Message-ID: <1CA1A859632B1F4989CB23F90D0A78290218F3E0@icex4.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070127/941d4796/attachment.pl 

From leog at anicca-vijja.de  Sat Jan 27 22:12:47 2007
From: leog at anicca-vijja.de (=?ISO-8859-1?Q?Leo_G=FCrtler?=)
Date: Sat, 27 Jan 2007 22:12:47 +0100
Subject: [R] CGIwithR and visible output of
	'invisible(capture.output(library(...)))'
In-Reply-To: <Pine.LNX.4.64.0701271351200.7105@gannet.stats.ox.ac.uk>
References: <45B95EB9.4010503@anicca-vijja.de> <45BAD2A9.1000505@justemail.net>
	<45BB5604.5030109@anicca-vijja.de>
	<Pine.LNX.4.64.0701271351200.7105@gannet.stats.ox.ac.uk>
Message-ID: <45BBC04F.1060705@anicca-vijja.de>

Prof Brian Ripley schrieb:

Dear Prof Ripley,

thanks for your information, in my first email I mentioned that I use
Version 2.3.1 (2006-06-01) on ubuntu edgy. Sorry for leaving out the
version-no. 1.1.2.3-13ubuntu2, together with R2HTML 1.58, installed
directly from R via 'install.packages()'.

Yes - you're right, there is a .deb called 'zlib-bin' which is
responsible (binary called 'miniunzip'). The original 'unzip' you
mention is also present but not used, because ->

trying to deinstall 'zlib-bin' leads to unresolved dependencies with the
_whole_ 'r-base-core' on ubuntu.

This is totally unintelligible, because the man-page of miniunzip states

"It was written as a  demonstration  of  the zlib(3) library and
therefore lack many of the features of the unzip(1) program."

And - of course - there is no option like '-q' or '--q'.

What are my opportunities besides the need to contact the
ubuntu-maintainers to change the dependencies? There must be stated
somewhere that R should call miniunzip instead of unzip to decompress
files. Where can I find that option?

Thanks!

best,

leo





> This is done by .onLoad, so your settings are irrelevant.
> 
> We have no information here on your platform, version of R, R2HTML or
> anything else.  But almost certainly the problem is the command given by
> getOption("unzip"), which looks like an unzip clone not respecting the
> -q flag.
> 
> Try using the 'real' unzip () instead, from http://www.info-zip.org/.
> 
> 
> On Sat, 27 Jan 2007, Leo G?rtler wrote:
> 
>> Eric schrieb:
>>> library(R2HTML, verbose = FALSE)
>>>
>>
>> Dear Eric,
>>
>> thanks - I tried that, but that does not work for me. And it does not
>> depend whether R2HTML is loaded via script/ batch job (-> webserver,
>> cgi) or directly within R.
>>
>> The same happens (of course) by using
>>
>> require(R2HTML, quietly=TRUE)
>>
>> Are there any further options I missed?
>> The problem seems to be that the file "R2HTMLstuff.zip" is extracted
>> every time (although this is not necessary) - how can I ged rid of that?
>>
>> thanks,
>>
>> best,
>>
>> leo
>>
>>
>> ----output----
>>
>>> library(R2HTML, verbose=FALSE)
>> MiniUnz 1.01b, demo of zLib + Unz package written by Gilles Vollant
>> more info at http://www.winimage.com/zLibDll/unzip.html
>>
>> /usr/local/lib/R/site-library/R2HTML/output/R2HTMLstuff.zip opened
>> extracting: ASCIIMathML.js
>> extracting: factor.gif
>> extracting: gridR2HTML.css
>> extracting: gridR2HTML.js
>> extracting: numeric.gif
>> extracting: Pastel.css
>> extracting: R2HTML.css
>> extracting: R2HTMLlogo.gif
>> creating directory: runtime/
>> creating directory: runtime/lib/
>> extracting: runtime/lib/grid.js
>> extracting: runtime/readme.txt
>> creating directory: runtime/styles/
>> creating directory: runtime/styles/classic/
>> extracting: runtime/styles/classic/gecko.xml
>> extracting: runtime/styles/classic/grid.css
>> extracting: runtime/styles/classic/grid.png
>> extracting: runtime/styles/classic/icons.png
>> extracting: runtime/styles/classic/loading.gif
>> creating directory: runtime/styles/flat/
>> extracting: runtime/styles/flat/gecko.xml
>> extracting: runtime/styles/flat/grid.css
>> extracting: runtime/styles/flat/grid.png
>> extracting: runtime/styles/flat/icons.png
>> extracting: runtime/styles/flat/loading.gif
>> creating directory: runtime/styles/xp/
>> extracting: runtime/styles/xp/gecko.xml
>> extracting: runtime/styles/xp/grid.css
>> extracting: runtime/styles/xp/grid.png
>> extracting: runtime/styles/xp/icons.png
>> extracting: runtime/styles/xp/loading.gif
>> extracting: SciViews.css
>> extracting: tablesort.htc
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From skiadas at hanover.edu  Sat Jan 27 22:34:39 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Sat, 27 Jan 2007 16:34:39 -0500
Subject: [R] gsub regexp question
In-Reply-To: <1CA1A859632B1F4989CB23F90D0A78290218F3E0@icex4.ic.ac.uk>
References: <1CA1A859632B1F4989CB23F90D0A78290218F3E0@icex4.ic.ac.uk>
Message-ID: <202C544B-F41D-489C-80C1-E9D9F1113E90@hanover.edu>

On Jan 27, 2007, at 3:41 PM, Phillimore, Albert wrote:

> Dear R Users,
>
> I am trying to users gsub to remove multiple cases of square  
> brackets and their different contents in a character string. A  
> sample of such a string is shown below. However, I am having great  
> difficulty understanding regexp syntax. Any help is greatly  
> appreciated.
>
> Ally
>
> "tree STATE_286000 [&lnP=-12708.453945423369] = [&R] ((((((15 
> [&rate=0.009761226401396686]:7.040851727747465,17 
> [&rate=0.011500289631135564]:7.040851727747465) 
> [&rate=0.010986570567484494]:2.257049446900292,(18 
> [&rate=0.009123432243563103]:2.461289418776003,19 
> [&rate=0.00981822432115329]:2.461289418776003)"

Is this what you want? I tend to prefer perl regular expressions:

 > str <- "tree STATE_286000 [&lnP=-12708.453945423369] = [&R]  
((((((15[&rate=0.009761226401396686]:7.040851727747465,17 
[&rate=0.011500289631135564]:7.040851727747465) 
[&rate=0.010986570567484494]:2.257049446900292,(18 
[&rate=0.009123432243563103]:2.461289418776003,19 
[&rate=0.00981822432115329]:2.461289418776003)"
 > gsub("\\[[^\\]]+\\]","",str, perl=T)
[1] "tree STATE_286000  =   
((((((15:7.040851727747465,17:7.040851727747465):2.257049446900292, 
(18:2.461289418776003,19:2.461289418776003)"


As an explanation, \\[ and \\] match the two square brackets you  
want. We need to escape the brackets with the backslashes because  
they have a special meaning in perl regular expressions.

In perl regexps, "[....]" stands for "match a single character that  
is like what we have in the .... For instance [ab] will match an a or  
a b. [a-z] will match all lowercase characters. A ^ as a first  
character in there means "match all but what follows". for instance  
[^a-z] means match anything but lowercase characters. So [^\\]] means  
match any character but a closing bracket.

Finally the plus sign afterwards means: match at least one. So [^\\]] 
+ means "match any sequence of characters that does not contain a  
closing bracket. So the whole thing now matches an opening bracket,  
followed by all characters until a corresponding closing bracket.  
This will not work if you have nested pairs of brackets, [like [so]].  
That is a tad more delicate, and we can discuss it if you really need  
to deal with it.

Haris


From ripley at stats.ox.ac.uk  Sat Jan 27 22:48:40 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 27 Jan 2007 21:48:40 +0000 (GMT)
Subject: [R] CGIwithR and visible output of
	'invisible(capture.output(library(...)))'
In-Reply-To: <45BBC04F.1060705@anicca-vijja.de>
References: <45B95EB9.4010503@anicca-vijja.de> <45BAD2A9.1000505@justemail.net>
	<45BB5604.5030109@anicca-vijja.de>
	<Pine.LNX.4.64.0701271351200.7105@gannet.stats.ox.ac.uk>
	<45BBC04F.1060705@anicca-vijja.de>
Message-ID: <Pine.LNX.4.64.0701272142420.13549@gannet.stats.ox.ac.uk>

I would not try uninstalling anything.  The binary used is set via
getOption("unzip"), and that in turn is set at R configure time and 
recorded in R_HOME/etc/Renviron.  I have

R_UNZIPCMD=${R_UNZIPCMD-'/usr/bin/unzip'}

there.  So you can alter that or set the environment variable in some 
other way.

The alternative is to alter R2HTML.  R has zip.file.extract() which 
contains

         if (unzip != "internal") {
             if (!system(paste(unzip, "-o", file.path(path, zipname),
                 topic, "-d", tmpd, " > /dev/null")))
                 file <- file.path(tmpd, topic)
         }

and you will spot the crucial difference from R2HTML's myunzip().

Hope that helps!


On Sat, 27 Jan 2007, Leo G?rtler wrote:

> Prof Brian Ripley schrieb:
>
> Dear Prof Ripley,
>
> thanks for your information, in my first email I mentioned that I use
> Version 2.3.1 (2006-06-01) on ubuntu edgy. Sorry for leaving out the
> version-no. 1.1.2.3-13ubuntu2, together with R2HTML 1.58, installed
> directly from R via 'install.packages()'.
>
> Yes - you're right, there is a .deb called 'zlib-bin' which is
> responsible (binary called 'miniunzip'). The original 'unzip' you
> mention is also present but not used, because ->
>
> trying to deinstall 'zlib-bin' leads to unresolved dependencies with the
> _whole_ 'r-base-core' on ubuntu.
>
> This is totally unintelligible, because the man-page of miniunzip states
>
> "It was written as a  demonstration  of  the zlib(3) library and
> therefore lack many of the features of the unzip(1) program."
>
> And - of course - there is no option like '-q' or '--q'.
>
> What are my opportunities besides the need to contact the
> ubuntu-maintainers to change the dependencies? There must be stated
> somewhere that R should call miniunzip instead of unzip to decompress
> files. Where can I find that option?
>
> Thanks!
>
> best,
>
> leo
>
>
>
>
>
>> This is done by .onLoad, so your settings are irrelevant.
>>
>> We have no information here on your platform, version of R, R2HTML or
>> anything else.  But almost certainly the problem is the command given by
>> getOption("unzip"), which looks like an unzip clone not respecting the
>> -q flag.
>>
>> Try using the 'real' unzip () instead, from http://www.info-zip.org/.
>>
>>
>> On Sat, 27 Jan 2007, Leo G?rtler wrote:
>>
>>> Eric schrieb:
>>>> library(R2HTML, verbose = FALSE)
>>>>
>>>
>>> Dear Eric,
>>>
>>> thanks - I tried that, but that does not work for me. And it does not
>>> depend whether R2HTML is loaded via script/ batch job (-> webserver,
>>> cgi) or directly within R.
>>>
>>> The same happens (of course) by using
>>>
>>> require(R2HTML, quietly=TRUE)
>>>
>>> Are there any further options I missed?
>>> The problem seems to be that the file "R2HTMLstuff.zip" is extracted
>>> every time (although this is not necessary) - how can I ged rid of that?
>>>
>>> thanks,
>>>
>>> best,
>>>
>>> leo
>>>
>>>
>>> ----output----
>>>
>>>> library(R2HTML, verbose=FALSE)
>>> MiniUnz 1.01b, demo of zLib + Unz package written by Gilles Vollant
>>> more info at http://www.winimage.com/zLibDll/unzip.html
>>>
>>> /usr/local/lib/R/site-library/R2HTML/output/R2HTMLstuff.zip opened
>>> extracting: ASCIIMathML.js
>>> extracting: factor.gif
>>> extracting: gridR2HTML.css
>>> extracting: gridR2HTML.js
>>> extracting: numeric.gif
>>> extracting: Pastel.css
>>> extracting: R2HTML.css
>>> extracting: R2HTMLlogo.gif
>>> creating directory: runtime/
>>> creating directory: runtime/lib/
>>> extracting: runtime/lib/grid.js
>>> extracting: runtime/readme.txt
>>> creating directory: runtime/styles/
>>> creating directory: runtime/styles/classic/
>>> extracting: runtime/styles/classic/gecko.xml
>>> extracting: runtime/styles/classic/grid.css
>>> extracting: runtime/styles/classic/grid.png
>>> extracting: runtime/styles/classic/icons.png
>>> extracting: runtime/styles/classic/loading.gif
>>> creating directory: runtime/styles/flat/
>>> extracting: runtime/styles/flat/gecko.xml
>>> extracting: runtime/styles/flat/grid.css
>>> extracting: runtime/styles/flat/grid.png
>>> extracting: runtime/styles/flat/icons.png
>>> extracting: runtime/styles/flat/loading.gif
>>> creating directory: runtime/styles/xp/
>>> extracting: runtime/styles/xp/gecko.xml
>>> extracting: runtime/styles/xp/grid.css
>>> extracting: runtime/styles/xp/grid.png
>>> extracting: runtime/styles/xp/icons.png
>>> extracting: runtime/styles/xp/loading.gif
>>> extracting: SciViews.css
>>> extracting: tablesort.htc
>>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From edd at debian.org  Sat Jan 27 23:19:07 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 27 Jan 2007 16:19:07 -0600
Subject: [R] CGIwithR and visible output
	of	'invisible(capture.output(library(...)))'
In-Reply-To: <45BBC04F.1060705@anicca-vijja.de>
References: <45B95EB9.4010503@anicca-vijja.de> <45BAD2A9.1000505@justemail.net>
	<45BB5604.5030109@anicca-vijja.de>
	<Pine.LNX.4.64.0701271351200.7105@gannet.stats.ox.ac.uk>
	<45BBC04F.1060705@anicca-vijja.de>
Message-ID: <17851.53211.928000.409711@basebud.nulle.part>


On 27 January 2007 at 22:12, Leo G?rtler wrote:
| Prof Brian Ripley schrieb:
| 
| Dear Prof Ripley,
| 
| thanks for your information, in my first email I mentioned that I use
| Version 2.3.1 (2006-06-01) on ubuntu edgy. Sorry for leaving out the
| version-no. 1.1.2.3-13ubuntu2, together with R2HTML 1.58, installed
| directly from R via 'install.packages()'.

Ubuntu uses miniunzip because Ubunto copies Debian package verbatim.

And on Debian we have been using minizip/miniunzip since 'forever' as the
zip/unzip utilities were at one point restricted as far as their license and
copyright are concerned.

For R's use, we never know we needed to change to zip/unzip so we haven't.
Your report is the first in years I am aware of that demonstrates a
shortcoming in the mini variants. [ There was another problem once years ago
which lead me to write a patch for miniunzip to support '-d tempdir' when the
license made the real unzip unsuitable for the main archive. ]

| Yes - you're right, there is a .deb called 'zlib-bin' which is
| responsible (binary called 'miniunzip'). The original 'unzip' you
| mention is also present but not used, because ->
| 
| trying to deinstall 'zlib-bin' leads to unresolved dependencies with the
| _whole_ 'r-base-core' on ubuntu.

Well, yes. See above. Debian/Ubuntu configures R to use minizip/miniunzip, so
we need to make sure they are present. That's what the Depends is for, and
that's why you can't uninstall zlib-bin.

| This is totally unintelligible, because the man-page of miniunzip states
| 
| "It was written as a  demonstration  of  the zlib(3) library and
| therefore lack many of the features of the unzip(1) program."
| 
| And - of course - there is no option like '-q' or '--q'.

There are several options.  

-- First, you deal with it locally by defining 
	options("unzip"="/usr/bin/unzip")
   in ~/.Rprofile

   Obviously, this doesn't scale.

-- Second, you or I supply a patch to miniunzip to support -q. 

   Obviously, that requires work on our part.

-- Third, you hack a shell script unzip that uses miniunzip, but 
   suppresses output to /dev/null.

-- Fourth, you convince (or bribe) me to switch to unzip in Debian's R, and
   you wait til that trickles through to your next Ubuntu upgrade. 

   In the meantime, do 1) or 3).

In any event, the bug here is with CGIwithR as it assumes that unzip is the
real thing. That may be true, but isn't guaranteed.

| What are my opportunities besides the need to contact the
| ubuntu-maintainers to change the dependencies? There must be stated

There is no "ubuntu-maintainer". Ubuntu just recompiles my Debian package.

| somewhere that R should call miniunzip instead of unzip to decompress
| files. Where can I find that option?

That would be option 2 above. See   help("Startup")  in R.

Hope this helps,

Dirk

PS  Option 4 has now been implemented in my Debian R sources.

| Thanks!
| 
| best,
| 
| leo
| 
| 
| 
| 
| 
| > This is done by .onLoad, so your settings are irrelevant.
| > 
| > We have no information here on your platform, version of R, R2HTML or
| > anything else.  But almost certainly the problem is the command given by
| > getOption("unzip"), which looks like an unzip clone not respecting the
| > -q flag.
| > 
| > Try using the 'real' unzip () instead, from http://www.info-zip.org/.
| > 
| > 
| > On Sat, 27 Jan 2007, Leo G?rtler wrote:
| > 
| >> Eric schrieb:
| >>> library(R2HTML, verbose = FALSE)
| >>>
| >>
| >> Dear Eric,
| >>
| >> thanks - I tried that, but that does not work for me. And it does not
| >> depend whether R2HTML is loaded via script/ batch job (-> webserver,
| >> cgi) or directly within R.
| >>
| >> The same happens (of course) by using
| >>
| >> require(R2HTML, quietly=TRUE)
| >>
| >> Are there any further options I missed?
| >> The problem seems to be that the file "R2HTMLstuff.zip" is extracted
| >> every time (although this is not necessary) - how can I ged rid of that?
| >>
| >> thanks,
| >>
| >> best,
| >>
| >> leo
| >>
| >>
| >> ----output----
| >>
| >>> library(R2HTML, verbose=FALSE)
| >> MiniUnz 1.01b, demo of zLib + Unz package written by Gilles Vollant
| >> more info at http://www.winimage.com/zLibDll/unzip.html
| >>
| >> /usr/local/lib/R/site-library/R2HTML/output/R2HTMLstuff.zip opened
| >> extracting: ASCIIMathML.js
| >> extracting: factor.gif
| >> extracting: gridR2HTML.css
| >> extracting: gridR2HTML.js
| >> extracting: numeric.gif
| >> extracting: Pastel.css
| >> extracting: R2HTML.css
| >> extracting: R2HTMLlogo.gif
| >> creating directory: runtime/
| >> creating directory: runtime/lib/
| >> extracting: runtime/lib/grid.js
| >> extracting: runtime/readme.txt
| >> creating directory: runtime/styles/
| >> creating directory: runtime/styles/classic/
| >> extracting: runtime/styles/classic/gecko.xml
| >> extracting: runtime/styles/classic/grid.css
| >> extracting: runtime/styles/classic/grid.png
| >> extracting: runtime/styles/classic/icons.png
| >> extracting: runtime/styles/classic/loading.gif
| >> creating directory: runtime/styles/flat/
| >> extracting: runtime/styles/flat/gecko.xml
| >> extracting: runtime/styles/flat/grid.css
| >> extracting: runtime/styles/flat/grid.png
| >> extracting: runtime/styles/flat/icons.png
| >> extracting: runtime/styles/flat/loading.gif
| >> creating directory: runtime/styles/xp/
| >> extracting: runtime/styles/xp/gecko.xml
| >> extracting: runtime/styles/xp/grid.css
| >> extracting: runtime/styles/xp/grid.png
| >> extracting: runtime/styles/xp/icons.png
| >> extracting: runtime/styles/xp/loading.gif
| >> extracting: SciViews.css
| >> extracting: tablesort.htc
| >>>
| >>
| >> ______________________________________________
| >> R-help at stat.math.ethz.ch mailing list
| >> https://stat.ethz.ch/mailman/listinfo/r-help
| >> PLEASE do read the posting guide
| >> http://www.R-project.org/posting-guide.html
| >> and provide commented, minimal, self-contained, reproducible code.
| >>
| >
| 
| ______________________________________________
| R-help at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-help
| PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
| and provide commented, minimal, self-contained, reproducible code.

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From quesada at gmail.com  Sat Jan 27 23:42:34 2007
From: quesada at gmail.com (Jose Quesada)
Date: Sat, 27 Jan 2007 23:42:34 +0100
Subject: [R] %*% in Matrix objects
Message-ID: <21c05c7d0701271442q678fb864mda6be17d71ea7d87@mail.gmail.com>

Hi Martin,
Thanks for your detailed answer.

x <- Matrix(1:12, 3,4, sparse = TRUE)

>I hope that you are aware of the fact that it's not efficient at
>all to store a dense matrix (it has *no* 0 entry) as a sparse one..
>
>and your posting is indeed an incentive for the Matrix developers
>to improve that part ... ;-)
>

Yes, the toy example is not sparse but the actual data is, and very large; I'm
aware that coercing a dense matrix into the Sparse format is not leading to any
saving (on the contrary). I'm talking about a real application with large sparse
matrices; from now on, I'll post small examples using sparse matrices as well to
avoid confusion.

	Jose> so I tried

    Jose> x = matrix(1:12,3,4)
    Jose> x = as(x, "CsparseMatrix")
    Jose> xnorms  = sqrt(colSums(x^2))
    Jose> xnorms = as(xnorms, "CsparseMatrix")
    Jose> (xnormed = t(x) * (1/xnorms))

    Jose> But now, instead of a warning I get
    Jose> "Error: Matrices must have same dimensions in t(x) * (1/xnorms)"

>yes.  And the same happens with traditional matrices -- and well so:
>For arithmetic with matrices (traditional or "Matrices"),
>
>    A o B       (o in {"+", "*", "^", ....})
>    -----
>
>does require that matrices A and B are ``conformable'', i.e.,
>have exact same dimensions.
>
>Only when one of A or B is *not* a matrix,
>then the usual S-language recycling rules are applied,
>and that's what you were using in your first example
>(<Matrix> * <numeric>) above.
>

Right. So this means that the * operator is not overloaded in Matrix (that is,
if I use it, I'll get my Matrix coherced to matrix. Is that correct?
Does this mean that there is no easy way to do element-by-element
multiplication
without leaving the sparse Matrix format?

    Jose> I suspect I'm facing the drop=T as before...
> why??

Because when I got a row out of a Matrix object, the resulting vector is not of
class Matrix but numeric, and then (<Matrix> * <numeric>) is applied.

Last, I shouldn't consider myself the most standard user of the matrix package,
since my lineal algebra is really basic. But in any case, you should know that
your package is being enormously useful for me. Keep up the good work. And if I
can help by posting my very basic questions, I'm glad to help.



-- 
Cheers,
-Jose

--
Jose Quesada, PhD
Research fellow, Psychology Dept.
Sussex University, Brighton, UK
http://www.andrew.cmu.edu/~jquesada


From kubovy at virginia.edu  Sun Jan 28 02:13:15 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sat, 27 Jan 2007 20:13:15 -0500
Subject: [R] Adding lines to xYplot
Message-ID: <C9C884FB-A22E-42AF-BC6F-71034C830F80@virginia.edu>

I am using xYplot to plot data and CIs. How do I add several lines to  
the figure?
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From jholtman at gmail.com  Sun Jan 28 05:53:20 2007
From: jholtman at gmail.com (jim holtman)
Date: Sat, 27 Jan 2007 23:53:20 -0500
Subject: [R] Problems terminating RGui on Windows
Message-ID: <644e1f320701272053v700dc8c9l7ce349d9cdac1c15@mail.gmail.com>

I am running R2.4.1 under Windows.

> version
               _
platform       i386-pc-mingw32
arch           i386
os             mingw32
system         i386, mingw32
status
major          2
minor          4.1
year           2006
month          12
day            18
svn rev        40228
language       R
version.string R version 2.4.1 (2006-12-18)
>

Sometimes when I exit R I do it by right clicking on the Windows tab
for the group of R processes that are running (I am running in SDI
mode) so there is usually the GUI and a graphics window open.  When I
choose the option to close the group, the tab disappears indicating
that the processes have been exited, but I get the CPU going to 100%
utilization.

Opening the windows Task Manager I see that the RGui process is the
one consuming all the CPU.  I then "end" the process with the Task
Manager and everything is fine.  I have not noticed that problem when
either I "q()" or close the RGui window.

I was wondering if anyone else has seen the problem, or might there be
something wrong with my configuration.  I guess that I will just have
to be nice and do "q()" to get out of the session.

-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From qjing at sibs.ac.cn  Sun Jan 28 06:19:43 2007
From: qjing at sibs.ac.cn (qing)
Date: Sun, 28 Jan 2007 05:19:43 +0000 (UTC)
Subject: [R] Biostrings
Message-ID: <loom.20070128T060501-922@post.gmane.org>




Dear All?

I am a beginner in learning the package of Biostrings currently and for 
practice doing an example.

 library(Biostrings);
 dnaAlph<-new("BioPatternAlphabet",DNAAlphabet(),c(N="AGCT",
+ B="CGT",D="AGT",H="ACT",K="GT",M="AC",R="AG",S="CG",
+ V="ACG",W="AT",Y="CT"));
Error in getClass(Class, where = topenv(parent.frame())) : 
        "BioPatternAlphabet" is not a defined class

I am running R 2.4.1,  Wimdows XP, Biostrings_2.2.1
Any help or suggestions that you can provide will be greatly appreciated.

Qing


From bernd.weiss at uni-koeln.de  Sun Jan 28 06:40:46 2007
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Sun, 28 Jan 2007 06:40:46 +0100
Subject: [R] Adding lines to xYplot
In-Reply-To: <C9C884FB-A22E-42AF-BC6F-71034C830F80@virginia.edu>
References: <C9C884FB-A22E-42AF-BC6F-71034C830F80@virginia.edu>
Message-ID: <45BC456E.21683.426D9D@bernd.weiss.uni-koeln.de>

Am 27 Jan 2007 um 20:13 hat Michael Kubovy geschrieben:

To:             	"r-help at stat.math.ethz.ch list" <r-
help at stat.math.ethz.ch>
From:           	Michael Kubovy <kubovy at virginia.edu>
Date sent:      	Sat, 27 Jan 2007 20:13:15 -0500
Subject:        	[R] Adding lines to xYplot

> I am using xYplot to plot data and CIs. How do I add several lines to 
> the figure? 

It's not clear what you mean by "add several lines". Maybe 
<panel.abline> is what you are looking for? Another option might be 
to use <panel.xYplot(x,y,subscripts,type="b")>...

HTH,

Bernd


# Examples of plotting raw data
dfr <- expand.grid(month=1:12, 
continent=c('Europe','USA'),sex=c('female','male'))

set.seed(1)

dfr <- upData(dfr,  y=month/10 + 1*(sex=='female') + 
2*(continent=='Europe') + 
              runif(48,-.15,.15),
              lower=y - runif(48,.05,.15),
              upper=y + runif(48,.05,.15)
			  )

xYplot(Cbind(y,lower,upper) ~ month,
subset=sex=='male' & continent=='USA',
panel=function(x,y,subscripts){panel.xYplot(x,y,subscripts);
## add lines to the graph which denote means of x and y 
panel.abline(h=mean(y),v=mean(x))},data=dfr)


From pwang at berkeley.edu  Sun Jan 28 06:50:20 2007
From: pwang at berkeley.edu (Patrick Wang)
Date: Sat, 27 Jan 2007 21:50:20 -0800 (PST)
Subject: [R] ratio optimization with quadratic form
Message-ID: <62027.76.167.74.27.1169963420.squirrel@calmail.berkeley.edu>

Hi, All

I have a function
f(y) = (x'Ax + 2yx'a_i+y^2*a_ii)/(x'Bx + 2yx'b_i+y^2*b_ii),

where A and B are known square matrix, a_i are the ith row of matrix A,
a_ii are the ith row/column element of matrix A.

what can I do to find a y to minize/maximize the ratio?

Do optimize function? can I do a matrix decomposition, a more elegant
solution?

Thanks
Pat


From murdoch at stats.uwo.ca  Sun Jan 28 12:20:30 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 28 Jan 2007 06:20:30 -0500
Subject: [R] Problems terminating RGui on Windows
In-Reply-To: <644e1f320701272053v700dc8c9l7ce349d9cdac1c15@mail.gmail.com>
References: <644e1f320701272053v700dc8c9l7ce349d9cdac1c15@mail.gmail.com>
Message-ID: <45BC86FE.1050107@stats.uwo.ca>

On 1/27/2007 11:53 PM, jim holtman wrote:
> I am running R2.4.1 under Windows.
> 
>> version
>                _
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          4.1
> year           2006
> month          12
> day            18
> svn rev        40228
> language       R
> version.string R version 2.4.1 (2006-12-18)
> 
> Sometimes when I exit R I do it by right clicking on the Windows tab
> for the group of R processes that are running (I am running in SDI
> mode) so there is usually the GUI and a graphics window open.  When I
> choose the option to close the group, the tab disappears indicating
> that the processes have been exited, but I get the CPU going to 100%
> utilization.
> 
> Opening the windows Task Manager I see that the RGui process is the
> one consuming all the CPU.  I then "end" the process with the Task
> Manager and everything is fine.  I have not noticed that problem when
> either I "q()" or close the RGui window.
> 
> I was wondering if anyone else has seen the problem, or might there be
> something wrong with my configuration.  I guess that I will just have
> to be nice and do "q()" to get out of the session.

I haven't seen this before, but I don't usually close R that way.  I 
just tried it in R-devel (by starting R, opening 10 plot windows so the 
taskbar buttons collapsed into one, and clicking on "close group"), and 
I got a crash.

I'll look into it.

Duncan Murdoch


From murdoch at stats.uwo.ca  Sun Jan 28 12:34:27 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 28 Jan 2007 06:34:27 -0500
Subject: [R] Biostrings
In-Reply-To: <loom.20070128T060501-922@post.gmane.org>
References: <loom.20070128T060501-922@post.gmane.org>
Message-ID: <45BC8A43.5070004@stats.uwo.ca>

On 1/28/2007 12:19 AM, qing wrote:
> 
> 
> Dear All?
> 
> I am a beginner in learning the package of Biostrings currently and for 
> practice doing an example.
> 
>  library(Biostrings);
>  dnaAlph<-new("BioPatternAlphabet",DNAAlphabet(),c(N="AGCT",
> + B="CGT",D="AGT",H="ACT",K="GT",M="AC",R="AG",S="CG",
> + V="ACG",W="AT",Y="CT"));
> Error in getClass(Class, where = topenv(parent.frame())) : 
>         "BioPatternAlphabet" is not a defined class
> 
> I am running R 2.4.1,  Wimdows XP, Biostrings_2.2.1
> Any help or suggestions that you can provide will be greatly appreciated.

You should probably ask this on the bioconductor mailing list.  See

http://www.bioconductor.org/docs/mailList.html

for instructions.

Duncan Murdoch


From john.maindonald at anu.edu.au  Sun Jan 28 12:55:15 2007
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Sun, 28 Jan 2007 22:55:15 +1100
Subject: [R] [R-pkgs] New version of lme4 and new mailing list
	R-SIG-mixed-models
Message-ID: <C689310C-46B6-4BEA-8813-563FA2E91AEE@anu.edu.au>

Dear Douglas -
I have tried several fits that relate to ch 10 of the 2nd edition of  
Maindonald & Braun.  In two cases a fit that does not currently  
converge with lmer does now converge with lme2.  In one case where I  
took logs in order to get convergence (the data did not really demand  
it), I can now get away without taking logs.  Basically, none of the  
problems that I had to work around when I did the calculations for  
that chapter have surfaced.  An exercise that had been problematic  
now converges without apparent problem.

In one case CPU time was reduced by a factor of around 3.  In one  
case where I formerly could not get convergence, the reduction (to  
termination of the calculations) was by a factor of around 10.  The  
results seems similar, with a small increase in the loglikelihood  
(perhaps ~0.5; I have not checked very carefully) in some cases.  If  
you want more detail, I will provide it.

It looks a huge improvement.  I am sure that R users will be duly  
grateful for your efforts.
Regards
John Maindonald.


John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.

 > sessionInfo()
R version 2.4.1 Patched (2007-01-17 r40518)
i386-apple-darwin8.8.1

locale:
C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "datasets"  "utils"      
"methods"
[7] "base"

other attached packages:
        DAAG        MASS        lme4      Matrix     lattice
      "0.91"    "7.2-31" "0.9975-11"  "0.9975-8"   "0.14-16"


From info at aghmed.fsnet.co.uk  Sun Jan 28 14:57:27 2007
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Sun, 28 Jan 2007 13:57:27 +0000
Subject: [R] replicating the odds ratio from a published study
In-Reply-To: <45BA7A27.1030004@biostat.ku.dk>
References: <7.0.0.16.0.20070126130450.0195bf38@aghmed.fsnet.co.uk>
	<mailman.17.1169636407.29747.r-help@stat.math.ethz.ch>
	<5.1.0.14.0.20070126185720.00d09858@pop3.brisnet.org.au>
	<7.0.0.16.0.20070126130450.0195bf38@aghmed.fsnet.co.uk>
	<5.1.0.14.0.20070127071119.00d64618@pop3.brisnet.org.au>
	<45BA7A27.1030004@biostat.ku.dk>
Message-ID: <7.0.0.16.0.20070128135630.019c3798@aghmed.fsnet.co.uk>

At 22:01 26/01/2007, Peter Dalgaard wrote:
>Bob Green wrote:
>>Peetr & Michael,
>>
>>I now see my description may have confused the issue.  I do want to 
>>compare odds ratios across studies - in the sense that I want to 
>>create a table with the respective odds ratio for each study. I do 
>>not need to statistically test two sets of odds ratios.
>>
>>What I want to do is ensure the method I use to compute an odds 
>>ratio is accurate and intended to check my method against published sources.
>>
>>The paper I selected by Schanda et al (2004). Homicide and major 
>>mental disorders. Acta Psychiatr Scand, 11:98-107 reports a total 
>>sample of 1087. Odds ratios are reported separately for men and 
>>women. There were 961 men all of whom were convicted of homicide. 
>>Of these 961 men, 41 were diagnosed with schizophrenia. The 
>>unadjusted odds ratio is for this  group of 41 is cited as 
>>6.52   (4.70-9.00).  They also report the general population aged 
>>over 15 with schizophrenia =20,109 and the total population =2,957,239.

Looking at the paper (which is in volume 110 by the way) suggests 
that Peter's reading of the situation is correct and that is what the 
authors have done.

>>Any further clarification is much appreciated,
>>
>>
>A fisher.test on the following matrix seems about right:
> > matrix(c(41,920,20109-41,2957239-20109-920),2)
>
>     [,1]    [,2]
>[1,]   41   20068
>[2,]  920 2936210
>
> > fisher.test(matrix(c(41,920,20109-41,2957239-20109-920),2))
>
>        Fisher's Exact Test for Count Data
>
>data:  matrix(c(41, 920, 20109 - 41, 2957239 - 20109 - 920), 2)
>p-value < 2.2e-16
>alternative hypothesis: true odds ratio is not equal to 1
>95 percent confidence interval:
>4.645663 8.918425
>sample estimates:
>odds ratio
>  6.520379
>
>The c.i. is not precisely the same as your source. This could be 
>down to a different approximation (R's is based on the noncentral 
>hypergeometric distribution), but the classical asymptotic formula gives
>
> > exp(log(41*2936210/920/20068)+qnorm(c(.025,.975))*sqrt(sum(1/M)))
>[1] 4.767384 8.918216
>
>which is closer, but still a bit narrower.
>

Michael Dewey
http://www.aghmed.fsnet.co.uk


From weibullguy at charter.net  Sun Jan 28 15:31:40 2007
From: weibullguy at charter.net (Andrew Rowland)
Date: Sun, 28 Jan 2007 09:31:40 -0500
Subject: [R] when i compile 2.4.1 in linux ,i got the message
Message-ID: <45BCB3CC.6010600@charter.net>

configure: error: --with-readline=yes (default) and headers/libs are not
available

then i cannot "make".

why?

You need the development package for readline.  Depending on the 
distribution of Linux you use, this will be called readline-dev, 
readline-devel, or something of that nature.  Use your package manager 
to find and install it.


From karin.lagesen at medisin.uio.no  Sun Jan 28 18:49:13 2007
From: karin.lagesen at medisin.uio.no (Karin Lagesen)
Date: Sun, 28 Jan 2007 18:49:13 +0100
Subject: [R] gcrma and chip without mm values
Message-ID: <ypx6odojf3dy.fsf@uracil.uio.no>


I am working with a custom chip which does not have mm probes, 
but which does have negative control probes. I would like to 
see the results of gcrma adjustment of the probe values.

I have thus chosen all of the indices of the negative controls 
and am trying to use them as described in the gcrma package:

The indices are here:


> randomprobeindexes[1:5,]
       x   y
[1,] 114 117
[2,] 116 117
[3,] 118 117
[4,] 120 117
[5,] 122 117
>

I now use them to do gcrma:

> bg.adjust.gcrma(newdata, affinity.source="local", type="affinities", NCprobe=randomprobeindexes)
Adjusting for optical effect.Error in exprs(abatch)[Index, i] <- exprs(abatch)[Index, i] - min(exprs(abatch)[Index,  : 
        NAs are not allowed in subscripted assignments
> 

This is due to this:

> mm(newdata)[200:205,]
     dmso5.CEL mnng5.CEL mock5.CEL uv5.CEL
[1,]        NA        NA        NA      NA
[2,]        NA        NA        NA      NA
[3,]        NA        NA        NA      NA
[4,]        NA        NA        NA      NA
[5,]        NA        NA        NA      NA
[6,]        NA        NA        NA      NA
> 

And:

> mmindex(newdata)[200:202]
$`AFFX-yel006_5_copy5_at`
 [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA

$`AFFX-yel006_5_copy6_at`
 [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA

$`AFFX-yel006_M_copy1_at`
 [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA

> 

We do have some mm values on the chip, since there is always one
control set present which does have mm probes with it. This one
is required for affymetrix own purposes. Besides this there are
no mm values. which means that most of the mmindex list just
states na, and as shown above, so does the mm list too.

Now, my initial strategy was to try and replace all mm values 
with zero. gcrma does not use mm values when negative control
indices are given, so this should, afaik, be ok. However, I
am not able to do this, and I think this is due to me not 
having any mm indices to use for the replacement.

Does any of you have any tips on how to overcome this?

TIA,

Karin
-- 
Karin Lagesen, PhD student
karin.lagesen at medisin.uio.no
http://folk.uio.no/karinlag


From caobg at email.uc.edu  Sun Jan 28 18:59:58 2007
From: caobg at email.uc.edu (Baoqiang Cao)
Date: Sun, 28 Jan 2007 12:59:58 -0500 (EST)
Subject: [R] gcrma and chip without mm values
Message-ID: <20070128125958.ACA97936@mirapoint2.uc.edu>

Sorry for not being able to answer your question, but there is a specific place to seek help on bioconductor package problems: 

bioconductor at stat.math.ethz.ch

Good luck.

Best,
 Baoqiang

---- Original message ----
>Date: Sun, 28 Jan 2007 18:49:13 +0100
>From: Karin Lagesen <karin.lagesen at medisin.uio.no>  
>Subject: [R] gcrma and chip without mm values  
>To: r-help at stat.math.ethz.ch
>
>
>I am working with a custom chip which does not have mm probes, 
>but which does have negative control probes. I would like to 
>see the results of gcrma adjustment of the probe values.
>
>I have thus chosen all of the indices of the negative controls 
>and am trying to use them as described in the gcrma package:
>
>The indices are here:
>
>
>> randomprobeindexes[1:5,]
>       x   y
>[1,] 114 117
>[2,] 116 117
>[3,] 118 117
>[4,] 120 117
>[5,] 122 117
>>
>
>I now use them to do gcrma:
>
>> bg.adjust.gcrma(newdata, affinity.source="local", type="affinities", NCprobe=randomprobeindexes)
>Adjusting for optical effect.Error in exprs(abatch)[Index, i] <- exprs(abatch)[Index, i] - min(exprs(abatch)[Index,  : 
>        NAs are not allowed in subscripted assignments
>> 
>
>This is due to this:
>
>> mm(newdata)[200:205,]
>     dmso5.CEL mnng5.CEL mock5.CEL uv5.CEL
>[1,]        NA        NA        NA      NA
>[2,]        NA        NA        NA      NA
>[3,]        NA        NA        NA      NA
>[4,]        NA        NA        NA      NA
>[5,]        NA        NA        NA      NA
>[6,]        NA        NA        NA      NA
>> 
>
>And:
>
>> mmindex(newdata)[200:202]
>$`AFFX-yel006_5_copy5_at`
> [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
>
>$`AFFX-yel006_5_copy6_at`
> [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
>
>$`AFFX-yel006_M_copy1_at`
> [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
>
>> 
>
>We do have some mm values on the chip, since there is always one
>control set present which does have mm probes with it. This one
>is required for affymetrix own purposes. Besides this there are
>no mm values. which means that most of the mmindex list just
>states na, and as shown above, so does the mm list too.
>
>Now, my initial strategy was to try and replace all mm values 
>with zero. gcrma does not use mm values when negative control
>indices are given, so this should, afaik, be ok. However, I
>am not able to do this, and I think this is due to me not 
>having any mm indices to use for the replacement.
>
>Does any of you have any tips on how to overcome this?
>
>TIA,
>
>Karin
>-- 
>Karin Lagesen, PhD student
>karin.lagesen at medisin.uio.no
>http://folk.uio.no/karinlag
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From betty.health at gmail.com  Sun Jan 28 19:13:12 2007
From: betty.health at gmail.com (Betty Health)
Date: Sun, 28 Jan 2007 11:13:12 -0700
Subject: [R] help with RandomForest classwt option
Message-ID: <34eed650701281013h6616ec83k1391f4318c22c88c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070128/16f83856/attachment.pl 

From aiminy at iastate.edu  Sun Jan 28 19:30:15 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Sun, 28 Jan 2007 12:30:15 -0600
Subject: [R] question about nnet
Message-ID: <6.1.2.0.2.20070128122629.01ceb1b0@aiminy.mail.iastate.edu>

I use neural network to predict a continuous variable( omega in training).

But I get all "1" instead of real value.

Do you know why?

Aimin

Thanks

The following is code

 > m.nn.omega <- nnet(omega~aa_three+bas+bcu+aa_ss, aata=training,size=2)
# weights:  57
initial  value 97329662.256069
final  value 96367717.444383
converged
 > pr.nn.train<-predict(m.nn.omega,training,type="raw")
 > head(pr.nn.train)
   [,1]
1    1
2    1
3    1
4    1
5    1
6    1
 > head(training)
     pr aa_three aa_one aa_ss aa_pos    aas bas   ams bms        acu 
bcu     omega       y index
1 1acx      ALA      A     C      1 127.71   0 69.99   0 
-0.2498560   0  79.91470 outward  TRUE
2 1acx      PRO      P     C      2  68.55   0 55.44   0 
-0.0949008   0  76.60380 outward  TRUE
3 1acx      ALA      A     E      3  52.72   0 47.82   0 
-0.0396550   0  52.19970 outward  TRUE
4 1acx      PHE      F     E      4  22.62   0 31.21   0  0.1270330   0 
169.52500  inward  TRUE
5 1acx      SER      S     E      5  71.32   0 52.84   0 
-0.1312380   0   7.47528 outward  TRUE
6 1acx      VAL      V     E      6  12.92   0 22.40   0  0.1728390   0 
149.09400  inward  TRUE


From liuwensui at gmail.com  Sun Jan 28 20:54:46 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Sun, 28 Jan 2007 14:54:46 -0500
Subject: [R] question about nnet
In-Reply-To: <6.1.2.0.2.20070128122629.01ceb1b0@aiminy.mail.iastate.edu>
References: <6.1.2.0.2.20070128122629.01ceb1b0@aiminy.mail.iastate.edu>
Message-ID: <1115a2b00701281154w1122d63bmbc87de63a6dabe69@mail.gmail.com>

in nnet(), you should add linout = TRUE. The default setting is logistic output.

hth.

On 1/28/07, Aimin Yan <aiminy at iastate.edu> wrote:
> I use neural network to predict a continuous variable( omega in training).
>
> But I get all "1" instead of real value.
>
> Do you know why?
>
> Aimin
>
> Thanks
>
> The following is code
>
>  > m.nn.omega <- nnet(omega~aa_three+bas+bcu+aa_ss, aata=training,size=2)
> # weights:  57
> initial  value 97329662.256069
> final  value 96367717.444383
> converged
>  > pr.nn.train<-predict(m.nn.omega,training,type="raw")
>  > head(pr.nn.train)
>    [,1]
> 1    1
> 2    1
> 3    1
> 4    1
> 5    1
> 6    1
>  > head(training)
>      pr aa_three aa_one aa_ss aa_pos    aas bas   ams bms        acu
> bcu     omega       y index
> 1 1acx      ALA      A     C      1 127.71   0 69.99   0
> -0.2498560   0  79.91470 outward  TRUE
> 2 1acx      PRO      P     C      2  68.55   0 55.44   0
> -0.0949008   0  76.60380 outward  TRUE
> 3 1acx      ALA      A     E      3  52.72   0 47.82   0
> -0.0396550   0  52.19970 outward  TRUE
> 4 1acx      PHE      F     E      4  22.62   0 31.21   0  0.1270330   0
> 169.52500  inward  TRUE
> 5 1acx      SER      S     E      5  71.32   0 52.84   0
> -0.1312380   0   7.47528 outward  TRUE
> 6 1acx      VAL      V     E      6  12.92   0 22.40   0  0.1728390   0
> 149.09400  inward  TRUE
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From dylan.beaudette at gmail.com  Sun Jan 28 21:59:58 2007
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Sun, 28 Jan 2007 12:59:58 -0800
Subject: [R] extra panel arguments to plot.nmGroupedData {nlme}
Message-ID: <3c5546140701281259x5020764at573b886ec29d8a07@mail.gmail.com>

Greetings,


I have a groupedData (nmGroupedData) object created with the following syntax:

Soil <- groupedData(
  ksat ~ conc | soil_id/sar/rep,
  data=soil.data,
  labels=list(x='Solution Concentration', y='Saturated Hydraulic Conductivity'),
  units=list(x='(cmol_c)', y='(cm/s)')
)


the original data represents longitudinal observations in the form of:
'data.frame':	1197 obs. of  5 variables:
 $ soil_id: Factor w/ 19 levels "Arbuckle","Campbell",..: 16 16 16 16
16 16 16 16 16 16 ...
 $ sar    : int  12 12 12 12 12 12 12 6 6 6 ...
 $ conc   : num  500 100 50 10 5 1 0.003 500 100 50 ...
 $ rep    : Factor w/ 3 levels "C1","C2","C3": 1 1 1 1 1 1 1 1 1 1 ...
 $ ksat   : num  0.000214 0.000207 0.000198 0.000160 0.000108 ...

the default plotting behaviour of this groupedData object works as expected:
plot(
  Soil,
  collapse=1, inner=~sar, aspect='fill',
  scales=list( x=list(log=TRUE), y=list(log=TRUE))
)

... however, there is no way for me to alter the panels...

for example, attempting to add a horizontal line with a call to
panel.abline= ... :
plot(
  Soil,
  collapse=1, inner=~sar, aspect='fill',
  scales=list( x=list(log=TRUE), y=list(log=TRUE)),
  FUN=mean,
  panel= function(x,y, subscripts, groups) {
    panel.xyplot(x, y, type='o')
    panel.abline(h=0.005, col='black', lty=2)
    }
)

... results in an identical plot. Trying to re-create the results of
plot.nmGroupedData with a direct call to xyplot() has thus far been
unsucsessful- as I cannot figure out how to specifiy the original
formula in a meaningful way to xyplot: ksat ~ conc | soil_id/sar/rep .

Any tips on passing panel arguments to plot.nmGroupedData() would be
greatly appreciated.

Cheers,

Dylan


From bgreen at dyson.brisnet.org.au  Sun Jan 28 22:13:46 2007
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Mon, 29 Jan 2007 07:13:46 +1000
Subject: [R] replicating the odds ratio from a published study
In-Reply-To: <7.0.0.16.0.20070128135630.019c3798@aghmed.fsnet.co.uk>
References: <45BA7A27.1030004@biostat.ku.dk>
	<7.0.0.16.0.20070126130450.0195bf38@aghmed.fsnet.co.uk>
	<mailman.17.1169636407.29747.r-help@stat.math.ethz.ch>
	<5.1.0.14.0.20070126185720.00d09858@pop3.brisnet.org.au>
	<7.0.0.16.0.20070126130450.0195bf38@aghmed.fsnet.co.uk>
	<5.1.0.14.0.20070127071119.00d64618@pop3.brisnet.org.au>
	<45BA7A27.1030004@biostat.ku.dk>
Message-ID: <5.1.0.14.0.20070129063925.00d51340@pop3.brisnet.org.au>

Michael,

Thanks. Yes, clearly the volume number for the Schanda paper I cited is wrong.

Where things are a bit perplexing, is that I used the same method as Peter 
suggested on two papers by Eronen (referenced below). I can reproduce in R 
a similar odds ratio to the first published paper e.g OR = 9.7 (CI= 
7.4-12.6) whereas I obtained quite different results from the second 
published paper (Eronen 2) of OR =  10.0 (8.1-12.5). One reason why I 
wanted to work out the calculations was so I could analyse data from 
studies using the same method, for confirmation.

Now the additional issue, is that Woodward, who is also the author of an 
epidemiological text, says in a review that Eronen used wrong  formula in a 
1995 paper and indicates that this comment applies also to later studies - 
he stated the "they use methods designed for use with binomial data when 
they really have Poisson data. Consequently, they quote odds ratios when 
they really have relative rates and their confidence intervals are 
inaccurate".  Eronen1 cites the formula that was used for OR. Schanda sets 
out his table for odds ratio the same as Eronen1

For the present purpose, my primary question is: as you have now seen the 
Schanda paper, would you consider Schanda calculated odds or relative risk?

Also, when I tried the formula suggested by Peter (below) I obtained an 
error - do you know what M might be or the source of the error?

exp(log(41*2936210/920/20068)+qnorm(c(.025,.975))*sqrt(sum(1/M)))
Error in sum(1/M) : object "M" not found


 > eronen1 <-  as.table(matrix(c(58,852,13600-58,1947000-13600-852), ncol = 
2 , dimnames = list(group=c("scz", "nonscz"), who= c("sample", "population"))))
 > fisher.test(eronen1)


p-value < 2.2e-16
alternative hypothesis: true odds ratio is not equal to 1
95 percent confidence interval:
   7.309717 12.690087
sample estimates:
odds ratio
   9.713458

 > eronen2 <-  as.table(matrix(c(86,1302,13530-86,1933000-13530-1302), ncol 
= 2 , dimnames = list(group=c("scz", "nonscz"), who= c("sample", 
"population"))))
 > fisher.test(eronen2)

p-value < 2.2e-16
alternative hypothesis: true odds ratio is not equal to 1
95 percent confidence interval:
   7.481272 11.734136
sample estimates:
odds ratio
    9.42561

References

Eronen, M. et al. (1996 - 1) Mental disorders and homicidal behavior in 
Finland. Archives of General Psychiatry, 53, 497-501

Eronen, M et al (1996 - 2). Schizophrenia & homicidal 
behavior.  Schizophrenia Bulletin, 22, 83-89

Woodward, Mental disorder & homicide. Epidemiologia E Psichiatria Sociale, 
9, 171-189

Any comments are welcomed,

Bob

At 01:57 PM 28/01/2007 +0000, Michael Dewey wrote:
>At 22:01 26/01/2007, Peter Dalgaard wrote:
>>Bob Green wrote:
>>>Peetr & Michael,
>>>
>>>I now see my description may have confused the issue.  I do want to 
>>>compare odds ratios across studies - in the sense that I want to create 
>>>a table with the respective odds ratio for each study. I do not need to 
>>>statistically test two sets of odds ratios.
>>>
>>>What I want to do is ensure the method I use to compute an odds ratio is 
>>>accurate and intended to check my method against published sources.
>>>
>>>The paper I selected by Schanda et al (2004). Homicide and major mental 
>>>disorders. Acta Psychiatr Scand, 11:98-107 reports a total sample of 
>>>1087. Odds ratios are reported separately for men and women. There were 
>>>961 men all of whom were convicted of homicide. Of these 961 men, 41 
>>>were diagnosed with schizophrenia. The unadjusted odds ratio is for 
>>>this  group of 41 is cited as 6.52   (4.70-9.00).  They also report the 
>>>general population aged over 15 with schizophrenia =20,109 and the total 
>>>population =2,957,239.
>
>Looking at the paper (which is in volume 110 by the way) suggests that 
>Peter's reading of the situation is correct and that is what the authors 
>have done.
>
>>>Any further clarification is much appreciated,
>>>
>>A fisher.test on the following matrix seems about right:
>> > matrix(c(41,920,20109-41,2957239-20109-920),2)
>>
>>     [,1]    [,2]
>>[1,]   41   20068
>>[2,]  920 2936210
>>
>> > fisher.test(matrix(c(41,920,20109-41,2957239-20109-920),2))
>>
>>        Fisher's Exact Test for Count Data
>>
>>data:  matrix(c(41, 920, 20109 - 41, 2957239 - 20109 - 920), 2)
>>p-value < 2.2e-16
>>alternative hypothesis: true odds ratio is not equal to 1
>>95 percent confidence interval:
>>4.645663 8.918425
>>sample estimates:
>>odds ratio
>>  6.520379
>>
>>The c.i. is not precisely the same as your source. This could be down to 
>>a different approximation (R's is based on the noncentral hypergeometric 
>>distribution), but the classical asymptotic formula gives
>>
>> > exp(log(41*2936210/920/20068)+qnorm(c(.025,.975))*sqrt(sum(1/M)))
>>[1] 4.767384 8.918216
>>
>>which is closer, but still a bit narrower.
>
>Michael Dewey
>http://www.aghmed.fsnet.co.uk


From gzhu at peak6.com  Sun Jan 28 23:41:16 2007
From: gzhu at peak6.com (Geoffrey Zhu)
Date: Sun, 28 Jan 2007 16:41:16 -0600
Subject: [R] Inverse fuction of ecdf
Message-ID: <99F81FFD0EA54E4DA8D4F1BFE272F34103413EAB@ppi-mail1.chicago.peak6.net>

Hi Everyone,

I want to generate some random numbers according to some empirical
distribution. Therefore I am looking for the inverse of an empirical
cumulative distribution function. I haven't found any in R. Can anyone
give a pointer?

Thanks,
Geoffrey

_______________________________________________________=0A=
=0A=
=0A=
The information in this email or in any file attached hereto...{{dropped}}


From bcarvalh at jhsph.edu  Sun Jan 28 23:45:07 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Sun, 28 Jan 2007 17:45:07 -0500
Subject: [R] Inverse fuction of ecdf
In-Reply-To: <99F81FFD0EA54E4DA8D4F1BFE272F34103413EAB@ppi-mail1.chicago.peak6.net>
References: <99F81FFD0EA54E4DA8D4F1BFE272F34103413EAB@ppi-mail1.chicago.peak6.net>
Message-ID: <8025E6A4-BA29-466A-87A0-93F3EABF1120@jhsph.edu>

?quantile

b

On Jan 28, 2007, at 5:41 PM, Geoffrey Zhu wrote:

> Hi Everyone,
>
> I want to generate some random numbers according to some empirical
> distribution. Therefore I am looking for the inverse of an empirical
> cumulative distribution function. I haven't found any in R. Can anyone
> give a pointer?
>
> Thanks,
> Geoffrey


From aiminy at iastate.edu  Sun Jan 28 23:49:23 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Sun, 28 Jan 2007 16:49:23 -0600
Subject: [R] nnet question
Message-ID: <6.1.2.0.2.20070128164401.01d51bf8@aiminy.mail.iastate.edu>

Hello,
I use nnet to do prediction for a continuous variable.
after that, I calculate correlation coefficient between predicted value and 
real observation.

I run my code(see following) several time, but I get different correlation 
coefficient each time.

Anyone know why?

In addition, How to calculate prediction accuracy for prediction of 
continuous variable?

Aimin
thanks,


 > m.nn.omega <- nnet(omega~aa_three+bas+bcu+aa_ss, data=training, size=2, 
linout=TRUE)
# weights:  57
initial  value 89153525.582093
iter  10 value 15036439.951888
iter  20 value 15010796.121891
iter  30 value 15000761.804392
iter  40 value 14955839.294531
iter  50 value 14934746.564215
iter  60 value 14933978.758615
iter  70 value 14555668.381007
iter  80 value 14553072.231507
iter  90 value 14031071.223996
iter 100 value 13709055.312482
final  value 13709055.312482
stopped after 100 iterations
 > pr.nn.train<-predict(m.nn.omega,training)
 > corr.pr.nn.train<-round(cor(pr.nn.train,training$omega),2)
 > pr.nn.test<-predict(m.nn.omega,test)
 > corr.pr.nn.test<-round(cor(pr.nn.test,test$omega),2)
 > cat("correlation coefficient for train using neural 
network:",corr.pr.nn.train,"\n")
correlation coefficient for train using neural network: 0.32
 > cat("correlation coefficient for test using neural 
network:",corr.pr.nn.test,"\n")
correlation coefficient for test using neural network: 0.39


From gzhu at peak6.com  Mon Jan 29 00:06:04 2007
From: gzhu at peak6.com (Geoffrey Zhu)
Date: Sun, 28 Jan 2007 17:06:04 -0600
Subject: [R] Inverse fuction of ecdf
Message-ID: <99F81FFD0EA54E4DA8D4F1BFE272F34103413EAC@ppi-mail1.chicago.peak6.net>

Hi Benilton,

I tried this. It sort of works, but the results are not very
satisfactionary. The 3rd moment and higher do not match those of the
original by a large difference. Do you have any better way to do this?

Thanks,
Geoffrey 

-----Original Message-----
From: Benilton Carvalho [mailto:bcarvalh at jhsph.edu] 
Sent: Sunday, January 28, 2007 4:45 PM
To: Geoffrey Zhu
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Inverse fuction of ecdf

?quantile

b

On Jan 28, 2007, at 5:41 PM, Geoffrey Zhu wrote:

> Hi Everyone,
>
> I want to generate some random numbers according to some empirical 
> distribution. Therefore I am looking for the inverse of an empirical 
> cumulative distribution function. I haven't found any in R. Can anyone

> give a pointer?
>
> Thanks,
> Geoffrey



_______________________________________________________=0A=
=0A=
=0A=
The information in this email or in any file attached hereto...{{dropped}}


From roger at ysidro.econ.uiuc.edu  Mon Jan 29 00:19:01 2007
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Sun, 28 Jan 2007 17:19:01 -0600
Subject: [R] Inverse fuction of ecdf
In-Reply-To: <99F81FFD0EA54E4DA8D4F1BFE272F34103413EAC@ppi-mail1.chicago.peak6.net>
References: <99F81FFD0EA54E4DA8D4F1BFE272F34103413EAC@ppi-mail1.chicago.peak6.net>
Message-ID: <48671DE0-F9D4-4A54-A4AB-2C79B6EBE65D@ysidro.econ.uiuc.edu>

quantile() does some somewhat exotic interpolation --- if you are  
wanting to
match moments you need to be more explicit about how you are computing
moments for the two approaches...

On Jan 28, 2007, at 5:06 PM, Geoffrey Zhu wrote:

> Hi Benilton,
>
> I tried this. It sort of works, but the results are not very
> satisfactionary. The 3rd moment and higher do not match those of the
> original by a large difference. Do you have any better way to do this?
>
> Thanks,
> Geoffrey
>
> -----Original Message-----
> From: Benilton Carvalho [mailto:bcarvalh at jhsph.edu]
> Sent: Sunday, January 28, 2007 4:45 PM
> To: Geoffrey Zhu
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Inverse fuction of ecdf
>
> ?quantile
>
> b
>
> On Jan 28, 2007, at 5:41 PM, Geoffrey Zhu wrote:
>
>> Hi Everyone,
>>
>> I want to generate some random numbers according to some empirical
>> distribution. Therefore I am looking for the inverse of an empirical
>> cumulative distribution function. I haven't found any in R. Can  
>> anyone
>
>> give a pointer?
>>
>> Thanks,
>> Geoffrey
>
>
>
> _______________________________________________________=0A=
> =0A=
> =0A=
> The information in this email or in any file attached hereto... 
> {{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Mon Jan 29 00:23:44 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 28 Jan 2007 18:23:44 -0500
Subject: [R] Inverse fuction of ecdf
In-Reply-To: <99F81FFD0EA54E4DA8D4F1BFE272F34103413EAB@ppi-mail1.chicago.peak6.net>
References: <99F81FFD0EA54E4DA8D4F1BFE272F34103413EAB@ppi-mail1.chicago.peak6.net>
Message-ID: <45BD3080.2050208@stats.uwo.ca>

On 1/28/2007 5:41 PM, Geoffrey Zhu wrote:
> Hi Everyone,
> 
> I want to generate some random numbers according to some empirical
> distribution. Therefore I am looking for the inverse of an empirical
> cumulative distribution function. I haven't found any in R. Can anyone
> give a pointer?

sample() works fine if you have a sample to start from.  If you really 
need to start from an ecdf, you could generate x and prob args to 
sample() by looking inside the ecdf object.  For example:

x <- rbinom(100, 100, 0.2)
e <- ecdf(x)

Now either of these should give you what you want.

size <- 1000
sample(x, size, replace=TRUE)

or

vals <- get("x", environment(e))
probs <- diff(c(0, get("y", environment(e))))
sample(vals, size, replace=TRUE, prob=probs)

Duncan Murdoch


From ajkoch at postoffice.utas.edu.au  Mon Jan 29 01:34:51 2007
From: ajkoch at postoffice.utas.edu.au (Amy Koch)
Date: Mon, 29 Jan 2007 11:34:51 +1100
Subject: [R] comparing random forests and classification trees
Message-ID: <005101c7433d$4bab92f0$7128d983@geol.utas.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070129/a66fdde6/attachment.pl 

From kubovy at virginia.edu  Mon Jan 29 02:35:04 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sun, 28 Jan 2007 20:35:04 -0500
Subject: [R] lmer2 error under Mac OS X on PowerPC G5 but not on Dual-Core
	Intel Xeon
Message-ID: <7665FCCA-208B-4B07-9A95-A7559D0DD3C4@virginia.edu>

 > (fm1 <- lmer2(Reaction ~ Days + (Days|Subject), sleepstudy))
Error in as.double(start) : Calloc could not allocate (888475968 of  
4) memory
*************************
 > sessionInfo()
R version 2.4.1 (2006-12-18)
powerpc-apple-darwin8.8.0

locale:
C

attached base packages:
[1] "grid"      "datasets"  "stats"     "graphics"  "grDevices"  
"utils"     "methods"
[8] "base"

other attached packages:
         lme4       Matrix       xtable latticeExtra      lattice      
gridBase         MASS
"0.9975-11"   "0.9975-8"      "1.4-3"      "0.1-4"    "0.14-16"       
"0.4-3"     "7.2-31"
          JGR       iplots       JavaGD        rJava
     "1.4-15"      "1.0-5"      "0.3-5"     "0.4-13"
*************************
lmer runs the example w/o a problem

I just tried to run it on on Intel-based MacPro, and lmer2 ran  
without a hitch.
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From liuwensui at gmail.com  Mon Jan 29 02:38:52 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Sun, 28 Jan 2007 20:38:52 -0500
Subject: [R] nnet question
In-Reply-To: <6.1.2.0.2.20070128164401.01d51bf8@aiminy.mail.iastate.edu>
References: <6.1.2.0.2.20070128164401.01d51bf8@aiminy.mail.iastate.edu>
Message-ID: <1115a2b00701281738l1712a0eexaad23d4861eb104e@mail.gmail.com>

AM,
Don't worry. It is correct to get different correlation each time.
Unless you are very lucky, you will get different prediction for each
different training process, depending on your initial random state.

Take a look at Dr Ripley's MASS book, there are several excellent
examples on how to use nnet.

On 1/28/07, Aimin Yan <aiminy at iastate.edu> wrote:
> Hello,
> I use nnet to do prediction for a continuous variable.
> after that, I calculate correlation coefficient between predicted value and
> real observation.
>
> I run my code(see following) several time, but I get different correlation
> coefficient each time.
>
> Anyone know why?
>
> In addition, How to calculate prediction accuracy for prediction of
> continuous variable?
>
> Aimin
> thanks,
>
>
>  > m.nn.omega <- nnet(omega~aa_three+bas+bcu+aa_ss, data=training, size=2,
> linout=TRUE)
> # weights:  57
> initial  value 89153525.582093
> iter  10 value 15036439.951888
> iter  20 value 15010796.121891
> iter  30 value 15000761.804392
> iter  40 value 14955839.294531
> iter  50 value 14934746.564215
> iter  60 value 14933978.758615
> iter  70 value 14555668.381007
> iter  80 value 14553072.231507
> iter  90 value 14031071.223996
> iter 100 value 13709055.312482
> final  value 13709055.312482
> stopped after 100 iterations
>  > pr.nn.train<-predict(m.nn.omega,training)
>  > corr.pr.nn.train<-round(cor(pr.nn.train,training$omega),2)
>  > pr.nn.test<-predict(m.nn.omega,test)
>  > corr.pr.nn.test<-round(cor(pr.nn.test,test$omega),2)
>  > cat("correlation coefficient for train using neural
> network:",corr.pr.nn.train,"\n")
> correlation coefficient for train using neural network: 0.32
>  > cat("correlation coefficient for test using neural
> network:",corr.pr.nn.test,"\n")
> correlation coefficient for test using neural network: 0.39
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From liuwensui at gmail.com  Mon Jan 29 02:44:16 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Sun, 28 Jan 2007 20:44:16 -0500
Subject: [R] comparing random forests and classification trees
In-Reply-To: <005101c7433d$4bab92f0$7128d983@geol.utas.edu.au>
References: <005101c7433d$4bab92f0$7128d983@geol.utas.edu.au>
Message-ID: <1115a2b00701281744v4afbc78ah21d555b0033616b2@mail.gmail.com>

Amy,
If I were you, I will check the misclassification rates in both
training set and testing set from 2 models.


On 1/28/07, Amy Koch <ajkoch at postoffice.utas.edu.au> wrote:
> Hi,
>
> I have done an analysis using 'rpart' to construct a Classification Tree. I
> am wanting to retain the output in tree form so that it is easily
> interpretable. However, I am wanting to compare the 'accuracy' of the tree
> to a Random Forest to estimate how much predictive ability is lost by using
> one simple tree. My understanding is that the error automatically displayed
> by the two functions is calculated differently so it is therefore incorrect
> to use this as a comparison. Instead I have produced a table for both
> analyses comparing the observed and predicted response.
>
> E.g. table(data$dependent,predict(model,type="class"))
>
> I am looking for confirmation that (a) it is incorrect to compare the error
> estimates for the two techniques and (b) that comparing the
> misclassification rates is an appropriate method for comparing the two
> techniques.
>
> Thanks
>
> Amy
>
>
>
>
>
> Amelia Koch
>
> University of Tasmania
>
> School of Geography and Environmental Studies
>
> Private Bag 78 Hobart
>
> Tasmania, Australia 7001
>
> Ph: +61 3 6226 7454
>
> ajkoch at utas.edu.au
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From helprhelp at gmail.com  Mon Jan 29 03:16:58 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Sun, 28 Jan 2007 21:16:58 -0500
Subject: [R] help with RandomForest classwt option
In-Reply-To: <34eed650701281013h6616ec83k1391f4318c22c88c@mail.gmail.com>
References: <34eed650701281013h6616ec83k1391f4318c22c88c@mail.gmail.com>
Message-ID: <cdf817830701281816n50c768aasfa6ba4b0bac718a3@mail.gmail.com>

Dear Betty:

I could suggest 3 options:

1. try to use rf in fortran by following the linky below
http://www.stat.berkeley.edu/~breiman/RandomForests/cc_software.htm

2. make a wrapper function to do the down sampling by yourself

3. try to use cutoff in randomForest, which might help in your situation.

HTH,

weiwei

On 1/28/07, Betty Health <betty.health at gmail.com> wrote:
> Hello there,
>
> I am working on an extremely unbalanced two class classification problems. I
> wanna use "classwt" with "down sampling" together. By checking the rfNews()
> in R, it looks that classwt is not working yet. Then I looked at the
> software from Salford. I did not find the down sampling option.  I am
> wondering if you have any experience to deal with this problem. Do you know
> any method or softwares can handle this problem?
>
> Thank you very much!!
>
> Betty
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From helprhelp at gmail.com  Mon Jan 29 03:24:35 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Sun, 28 Jan 2007 21:24:35 -0500
Subject: [R] nnet question
In-Reply-To: <6.1.2.0.2.20070128164401.01d51bf8@aiminy.mail.iastate.edu>
References: <6.1.2.0.2.20070128164401.01d51bf8@aiminy.mail.iastate.edu>
Message-ID: <cdf817830701281824n3b19b08eod9cabee559ca6770@mail.gmail.com>

try to use
set.seed(10)
before you call nnet and see if you have the same correlation each time.

I did not try by myself, though.



On 1/28/07, Aimin Yan <aiminy at iastate.edu> wrote:
> Hello,
> I use nnet to do prediction for a continuous variable.
> after that, I calculate correlation coefficient between predicted value and
> real observation.
>
> I run my code(see following) several time, but I get different correlation
> coefficient each time.
>
> Anyone know why?
>
> In addition, How to calculate prediction accuracy for prediction of
> continuous variable?
>
> Aimin
> thanks,
>
>
>  > m.nn.omega <- nnet(omega~aa_three+bas+bcu+aa_ss, data=training, size=2,
> linout=TRUE)
> # weights:  57
> initial  value 89153525.582093
> iter  10 value 15036439.951888
> iter  20 value 15010796.121891
> iter  30 value 15000761.804392
> iter  40 value 14955839.294531
> iter  50 value 14934746.564215
> iter  60 value 14933978.758615
> iter  70 value 14555668.381007
> iter  80 value 14553072.231507
> iter  90 value 14031071.223996
> iter 100 value 13709055.312482
> final  value 13709055.312482
> stopped after 100 iterations
>  > pr.nn.train<-predict(m.nn.omega,training)
>  > corr.pr.nn.train<-round(cor(pr.nn.train,training$omega),2)
>  > pr.nn.test<-predict(m.nn.omega,test)
>  > corr.pr.nn.test<-round(cor(pr.nn.test,test$omega),2)
>  > cat("correlation coefficient for train using neural
> network:",corr.pr.nn.train,"\n")
> correlation coefficient for train using neural network: 0.32
>  > cat("correlation coefficient for test using neural
> network:",corr.pr.nn.test,"\n")
> correlation coefficient for test using neural network: 0.39
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From jporzak at gmail.com  Mon Jan 29 05:19:43 2007
From: jporzak at gmail.com (Jim Porzak)
Date: Sun, 28 Jan 2007 20:19:43 -0800
Subject: [R] help with RandomForest classwt option
In-Reply-To: <34eed650701281013h6616ec83k1391f4318c22c88c@mail.gmail.com>
References: <34eed650701281013h6616ec83k1391f4318c22c88c@mail.gmail.com>
Message-ID: <2a9c000c0701282019k162d368cr9fd8df4727bfeb40@mail.gmail.com>

See Andy's previous post on this.

-- 
HTH,
Jim Porzak
Loyalty Matrix Inc.
San Francisco, CA


===============
Liaw, Andy <andy_liaw at merck.com> 	Thu, Oct 27, 2005 at 8:37 AM
To: "David L. Van Brunt, Ph.D." <dlvanbrunt at gmail.com>, Gabor
Grothendieck <ggrothendieck at gmail.com>
Cc: r-help at stat.math.ethz.ch
"classwt" in the current version of the randomForest package doesn't work
too well.  (It's what was in version 3.x of the original Fortran code by
Breiman and Cutler, not the one in the new Fortran code.)  I'd advise
against using it.

"sampsize" and "strata" can be use in conjunction.  If "strata" is not
specified, the class labels will be used.  Take the iris data as an example:

randomForest(Species ~ ., iris, sampsize=c(10, 30, 10))

says to randomly draw 10, 30 and 10 from the three species (with
replacement) to grow each tree.  If you are unsure of the labels, use named
vector, e.g.,

randomForest(Species ~ ., iris,
            sampsize=c(setosa=10, versicolor=30, virginica=10))

Now, if you want the stratified sampling to be done using a different
variable than the class labels; e.g., for multi-centered clinical trial
data, you want to draw the same number of patients per center to grow each
tree (I'm just making things up, not that that necessarily makes any sense),
you can do something like:

randomForest(..., strata=center,
            sampsize=rep(min(table(center))), nlevels(center)))

which draws the same number of patients (minimum at any center) from each
center to grow each tree.

Hope that's clear.  Eventually all such things will be in the yet to be
written package vignette...

Andy


On 1/28/07, Betty Health <betty.health at gmail.com> wrote:
> Hello there,
>
> I am working on an extremely unbalanced two class classification problems. I
> wanna use "classwt" with "down sampling" together. By checking the rfNews()
> in R, it looks that classwt is not working yet. Then I looked at the
> software from Salford. I did not find the down sampling option.  I am
> wondering if you have any experience to deal with this problem. Do you know
> any method or softwares can handle this problem?
>
> Thank you very much!!
>
> Betty
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From btyner at stat.purdue.edu  Mon Jan 29 05:41:10 2007
From: btyner at stat.purdue.edu (Benjamin Tyner)
Date: Sun, 28 Jan 2007 23:41:10 -0500
Subject: [R] lattice: two grouping variables, one controls 'col',
	the other 'pch'
Message-ID: <45BD7AE6.3080309@stat.purdue.edu>

Say I have

library(lattice)

x<-runif(256)
y<-runif(256)
f<-gl(16,16)
g1<-rep(1:4,each=64)
g2<-rep(1:4,times=64)

plot<-xyplot(y~x|f,
             groups=g1,
             pch=as.character(1:4),
             panel=function(x,y,subscripts,groups,...){
                   panel.superpose(x,y,subscripts,groups,...)
             })            
            
print(plot)

Currently, both color and plotting symbol change with the grouping 
variable g1. What is the best way to have color change with g1, but 
plotting symbol change with g2 (or vice versa)?

Thanks,
Ben


From ggrothendieck at gmail.com  Mon Jan 29 06:18:28 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 29 Jan 2007 00:18:28 -0500
Subject: [R] lattice: two grouping variables, one controls 'col',
	the other 'pch'
In-Reply-To: <45BD7AE6.3080309@stat.purdue.edu>
References: <45BD7AE6.3080309@stat.purdue.edu>
Message-ID: <971536df0701282118p2b198716u46d12626b6109149@mail.gmail.com>

Try:

xyplot(y ~ x | f, pch = g1, col = g2, panel = function(x, y,
subscripts, ..., pch, col)
      panel.xyplot(x, y, ..., col = col[subscripts], pch = pch[subscripts])
)


On 1/28/07, Benjamin Tyner <btyner at stat.purdue.edu> wrote:
> Say I have
>
> library(lattice)
>
> x<-runif(256)
> y<-runif(256)
> f<-gl(16,16)
> g1<-rep(1:4,each=64)
> g2<-rep(1:4,times=64)
>
> plot<-xyplot(y~x|f,
>             groups=g1,
>             pch=as.character(1:4),
>             panel=function(x,y,subscripts,groups,...){
>                   panel.superpose(x,y,subscripts,groups,...)
>             })
>
> print(plot)
>
> Currently, both color and plotting symbol change with the grouping
> variable g1. What is the best way to have color change with g1, but
> plotting symbol change with g2 (or vice versa)?
>
> Thanks,
> Ben
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From suprtova at ucla.edu  Mon Jan 29 06:35:59 2007
From: suprtova at ucla.edu (Tova Fuller)
Date: Sun, 28 Jan 2007 21:35:59 -0800
Subject: [R] Hiding Quartz window in Mac OS X using Terminal
Message-ID: <A0AD5CE0-BC03-41C3-B3D1-0CFA612E62D4@ucla.edu>

Hello all,

I'm using R on Mac OS X, 10.4.8.  When I plot in the R gui app, there  
is never a problem with the quartz window.  However, using when I  
attempt to plot in terminal using quartz, the window is hidden.  I  
know this because if I reveal all open windows with expose, the  
quartz windows are shown, and appear to come from far left of my  
screen's display.  I'd like to be able to see, resize, etc. using the  
quartz device in terminal if this is in fact possible.

Thank you ahead of time.

Tova Fuller


From shubhak at ambaresearch.com  Mon Jan 29 07:47:12 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Mon, 29 Jan 2007 12:17:12 +0530
Subject: [R] Error in merging
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3E4BA2C@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070129/8c432aa1/attachment.ksh 

From maechler at stat.math.ethz.ch  Mon Jan 29 09:06:34 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 29 Jan 2007 09:06:34 +0100
Subject: [R] %*% in Matrix objects
In-Reply-To: <21c05c7d0701271442q678fb864mda6be17d71ea7d87@mail.gmail.com>
References: <21c05c7d0701271442q678fb864mda6be17d71ea7d87@mail.gmail.com>
Message-ID: <17853.43786.766138.593055@stat.math.ethz.ch>

>>>>> "Jose" == Jose Quesada <quesada at gmail.com>
>>>>>     on Sat, 27 Jan 2007 23:42:34 +0100 writes:

    Jose> Hi Martin, Thanks for your detailed answer.

    Jose> x <- Matrix(1:12, 3,4, sparse = TRUE)

    >> I hope that you are aware of the fact that it's not
    >> efficient at all to store a dense matrix (it has *no* 0
    >> entry) as a sparse one..
    >> 
    >> and your posting is indeed an incentive for the Matrix
    >> developers to improve that part ... ;-)
    >> 

    Jose> Yes, the toy example is not sparse but the actual data
    Jose> is, and very large; I'm aware that coercing a dense
    Jose> matrix into the Sparse format is not leading to any
    Jose> saving (on the contrary). I'm talking about a real
    Jose> application with large sparse matrices; from now on,
    Jose> I'll post small examples using sparse matrices as well
    Jose> to avoid confusion.

ok.

      Jose> so I tried

      Jose> x = matrix(1:12,3,4)
      Jose> x = as(x, "CsparseMatrix")
      Jose> xnorms  = sqrt(colSums(x^2))
      Jose> xnorms = as(xnorms, "CsparseMatrix")
      Jose> (xnormed = t(x) * (1/xnorms))

      Jose> But now, instead of a warning I get
      Jose> "Error: Matrices must have same dimensions in t(x) * (1/xnorms)"

    >> yes.  And the same happens with traditional matrices -- and well so:
    >> For arithmetic with matrices (traditional or "Matrices"),
    >> 
    >> A o B       (o in {"+", "*", "^", ....})
    >> -----
    >> 
    >> does require that matrices A and B are ``conformable'', i.e.,
    >> have exact same dimensions.
    >> 
    >> Only when one of A or B is *not* a matrix,
    >> then the usual S-language recycling rules are applied,
    >> and that's what you were using in your first example
    >> (<Matrix> * <numeric>) above.
    >> 

    Jose> Right. So this means that the * operator is not
    Jose> overloaded in Matrix (that is, if I use it, I'll get
    Jose> my Matrix coherced to matrix. Is that correct?

no.  The "*"  is overloaded (read on)

    Jose> Does this mean that there is no easy way to do element-by-element
    Jose> multiplication without leaving the sparse Matrix format?

No. There is an easy way:
If you multiply (or add or ..) two sparse matrices of matching dim(), the
result will be sparse. Also if use a "scalar" (length-1 vector)
with a Matrix, the result remains sparse (where appropriate) :

  > (x <- Matrix(c(0,1,0,0), 3,3))
  3 x 3 sparse Matrix of class "dtCMatrix"

  [1,] . . .
  [2,] 1 . .
  [3,] . 1 .
  Warning message:
  data length [4] is not a sub-multiple or multiple of the number of rows [3] in matrix 
  > (2 * x) + t(x)
  3 x 3 sparse Matrix of class "dgCMatrix"

  [1,] . 1 .
  [2,] 2 . 1
  [3,] . 2 .
  > ((2 * x) + t(x)) * t(x)
  3 x 3 sparse Matrix of class "dgCMatrix"

  [1,] . 1 .
  [2,] . . 1
  [3,] . . .


What you tried to do,  <sparse> * <vector-of-length-gt-1>, will
only result in a sparse matrix in the next version of the Matrix
package. 

    Jose> I suspect I'm facing the drop=T as before...
    >> why??

    Jose> Because when I got a row out of a Matrix object, the
    Jose> resulting vector is not of class Matrix but numeric,
    Jose> and then (<Matrix> * <numeric>) is applied.


    Jose> Last, I shouldn't consider myself the most standard
    Jose> user of the matrix package, since my lineal algebra is
    Jose> really basic. But in any case, you should know that
    Jose> your package is being enormously useful for me. Keep
    Jose> up the good work. And if I can help by posting my very
    Jose> basic questions, I'm glad to help.

Ok, thanks for the flowers :-)
Martin


From cusickb at hotmail.com  Sat Jan 27 23:59:31 2007
From: cusickb at hotmail.com (Brendan Cusick)
Date: Sat, 27 Jan 2007 17:59:31 -0500
Subject: [R] saving issue
Message-ID: <BAY115-F26A8231A9C1A950D70F67CA9A10@phx.gbl>

I apologize if this isn't the right forum for this question.  I'm a rookie R 
user trying to save a source file and i keep getting this error:

2007-01-27 18:02:12.195 R[175] *** -[NSBigMutableString 
writeToFile:options:error:]: selector not recognized [self = 0x68c8c20]
2007-01-27 18:02:12.211 R[175] Exception raised during posting of 
notification.  Ignored.  exception: *** -[NSBigMutableString 
writeToFile:options:error:]: selector not recognized [self = 0x68c8c20]

I'm using a mac with osx 10.2.  Any help would be greatly appreciated.

Thanks,
Brendan


From yli2008 at yahoo.com  Sun Jan 28 09:36:55 2007
From: yli2008 at yahoo.com (Yue)
Date: Sun, 28 Jan 2007 08:36:55 +0000 (UTC)
Subject: [R] reposTools
Message-ID: <loom.20070128T060501-922@post.gmane.org>
















Dear List,

I tested the example in the reposTools vignette:

library(reposTools);
Loading required package: tools
genRepos("Test 
Repository", "http://biowww.dfci.harvard.edu/~jgentry/","newRepos");
Error in rep.int(colnames(x), nr) : unimplemented type 'NULL' in 'rep'

Could someone help me out with this one?
I'd appreciate all help....

I am running R 2.4.1 windows XP.

YUE


From edd at debian.org  Sun Jan 28 01:33:27 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 27 Jan 2007 18:33:27 -0600
Subject: [R] CGIwithR and visible
	output	of	'invisible(capture.output(library(...)))'
In-Reply-To: <17851.53211.928000.409711@basebud.nulle.part>
References: <45B95EB9.4010503@anicca-vijja.de> <45BAD2A9.1000505@justemail.net>
	<45BB5604.5030109@anicca-vijja.de>
	<Pine.LNX.4.64.0701271351200.7105@gannet.stats.ox.ac.uk>
	<45BBC04F.1060705@anicca-vijja.de>
	<17851.53211.928000.409711@basebud.nulle.part>
Message-ID: <17851.61271.787457.87776@basebud.nulle.part>


On 27 January 2007 at 16:19, Dirk Eddelbuettel wrote:
| In any event, the bug here is with CGIwithR as it assumes that unzip is the
| real thing. That may be true, but isn't guaranteed.

Sorry, a correction: R2HTML is the one making the assumption about
options("unzip") being "unzip", not CGIwithR. Thanks to Duncan TL for
pointing this out in private mail.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From sebastian.weber at physik.tu-darmstadt.de  Mon Jan 29 09:52:25 2007
From: sebastian.weber at physik.tu-darmstadt.de (Sebastian Weber)
Date: Mon, 29 Jan 2007 09:52:25 +0100
Subject: [R] can not load my workspace any more
Message-ID: <1170060746.29288.5.camel@rock.kraft.de>

Hello everybody!

I've been working now for quite a while with my R envoirment. However,
today I tried to load it as usal, but I only get the error message

Error in methods:::mlistMetaName(mi, ns) : 
	the methods object name for 'plot' must include the name of the package
that contains the generic function, but there is no generic function of
this name

This happens directly after I start R on the command line and leaves me
again with my prompt such that I can not access my envoirment at the
moment. I already tried to start off with a clean R envoirment and
attach my envoirment with no sucsess. So how can I now access my
envoirment???

Greetings,

Sebastian Weber


From bonneu at cict.fr  Mon Jan 29 10:34:19 2007
From: bonneu at cict.fr (Florent Bonneu)
Date: Mon, 29 Jan 2007 10:34:19 +0100
Subject: [R] Adaptive kernel density estimation in a domain of R^2
Message-ID: <45BDBF9B.2030309@cict.fr>

Hello,
Is there exist a R Package (or R Code) of nonparametric density 
estimation with adaptive methods (k-nearest neighbors,...) for 
multivariate data ?  I have found the package "Locfit" but it is only 
for the univariate case.
Thank you.

-- 
Florent BONNEU
LSP
Universit? Paul  Sabatier
118 route de Narbonne
31062 TOULOUSE CEDEX 9

Bureau 15 b?t 1R1
T?l. 05 61 55 76 69
bonneu at cict.fr


From huxiaopengstat at gmail.com  Mon Jan 29 11:29:06 2007
From: huxiaopengstat at gmail.com (xiaopeng hu)
Date: Mon, 29 Jan 2007 18:29:06 +0800
Subject: [R] Does R support grid or parallel computing?
Message-ID: <ffe0539f0701290229s3111d821y5633e0ea8d4d84b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070129/1e50404c/attachment.ksh 

From rdiaz02 at gmail.com  Mon Jan 29 11:33:46 2007
From: rdiaz02 at gmail.com (Ramon Diaz-Uriarte)
Date: Mon, 29 Jan 2007 11:33:46 +0100
Subject: [R] Does R support grid or parallel computing?
In-Reply-To: <ffe0539f0701290229s3111d821y5633e0ea8d4d84b@mail.gmail.com>
References: <ffe0539f0701290229s3111d821y5633e0ea8d4d84b@mail.gmail.com>
Message-ID: <624934630701290233o3f97a7a8w3cc2227f50d359ca@mail.gmail.com>

Dear Xiaopeng,

There is certainly support for, among others, MPI and PVM; check
packages Rmpi, rpvm, snow, and papply, in CRAN.


Best,

R.

On 1/29/07, xiaopeng hu <huxiaopengstat at gmail.com> wrote:
> Does R support grid or parallel computing?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ramon Diaz-Uriarte
Statistical Computing Team
Structural Biology and Biocomputing Programme
Spanish National Cancer Centre (CNIO)
http://ligarto.org/rdiaz


From info at aghmed.fsnet.co.uk  Mon Jan 29 10:44:08 2007
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Mon, 29 Jan 2007 09:44:08 +0000
Subject: [R] replicating the odds ratio from a published study
In-Reply-To: <5.1.0.14.0.20070129063925.00d51340@pop3.brisnet.org.au>
References: <45BA7A27.1030004@biostat.ku.dk>
	<7.0.0.16.0.20070126130450.0195bf38@aghmed.fsnet.co.uk>
	<mailman.17.1169636407.29747.r-help@stat.math.ethz.ch>
	<5.1.0.14.0.20070126185720.00d09858@pop3.brisnet.org.au>
	<7.0.0.16.0.20070126130450.0195bf38@aghmed.fsnet.co.uk>
	<5.1.0.14.0.20070127071119.00d64618@pop3.brisnet.org.au>
	<45BA7A27.1030004@biostat.ku.dk>
	<5.1.0.14.0.20070129063925.00d51340@pop3.brisnet.org.au>
Message-ID: <7.0.0.16.0.20070129094019.019b73e0@aghmed.fsnet.co.uk>

At 21:13 28/01/2007, Bob Green wrote:
>Michael,
>
>Thanks. Yes, clearly the volume number for the Schanda paper I cited is wrong.
>
>Where things are a bit perplexing, is that I used the same method as 
>Peter suggested on two papers by Eronen (referenced below). I can 
>reproduce in R a similar odds ratio to the first published paper e.g 
>OR = 9.7 (CI= 7.4-12.6) whereas I obtained quite different results 
>from the second published paper (Eronen 2) of OR =  10.0 (8.1-12.5). 
>One reason why I wanted to work out the calculations was so I could 
>analyse data from studies using the same method, for confirmation.
>
>Now the additional issue, is that Woodward, who is also the author 
>of an epidemiological text, says in a review that Eronen used 
>wrong  formula in a 1995 paper and indicates that this comment 
>applies also to later studies - he stated the "they use methods 
>designed for use with binomial data when they really have Poisson 
>data. Consequently, they quote odds ratios when they really have 
>relative rates and their confidence intervals are 
>inaccurate".  Eronen1 cites the formula that was used for OR. 
>Schanda sets out his table for odds ratio the same as Eronen1

There do seem to be difficulties in what they are doing as they have 
not observed all the non-homicides, they estimate how many they are 
and then estimate the number of people with a given diagnosis using 
prevalence estimates from another study. I think you are moving 
towards writing an article criticising the statistical methods used 
in this whole field which I think is going beyond the resources of R-help.


>For the present purpose, my primary question is: as you have now 
>seen the Schanda paper, would you consider Schanda calculated odds 
>or relative risk?
>
>Also, when I tried the formula suggested by Peter (below) I obtained 
>an error - do you know what M might be or the source of the error?
>
>exp(log(41*2936210/920/20068)+qnorm(c(.025,.975))*sqrt(sum(1/M)))
>Error in sum(1/M) : object "M" not found
>
>
> > eronen1 <-  as.table(matrix(c(58,852,13600-58,1947000-13600-852), 
> ncol = 2 , dimnames = list(group=c("scz", "nonscz"), who= 
> c("sample", "population"))))
> > fisher.test(eronen1)
>
>
>p-value < 2.2e-16
>alternative hypothesis: true odds ratio is not equal to 1
>95 percent confidence interval:
>   7.309717 12.690087
>sample estimates:
>odds ratio
>   9.713458
>
> > eronen2 
> <-  as.table(matrix(c(86,1302,13530-86,1933000-13530-1302), ncol = 
> 2 , dimnames = list(group=c("scz", "nonscz"), who= c("sample", "population"))))
> > fisher.test(eronen2)
>
>p-value < 2.2e-16
>alternative hypothesis: true odds ratio is not equal to 1
>95 percent confidence interval:
>   7.481272 11.734136
>sample estimates:
>odds ratio
>    9.42561
>
>References
>
>Eronen, M. et al. (1996 - 1) Mental disorders and homicidal behavior 
>in Finland. Archives of General Psychiatry, 53, 497-501
>
>Eronen, M et al (1996 - 2). Schizophrenia & homicidal 
>behavior.  Schizophrenia Bulletin, 22, 83-89
>
>Woodward, Mental disorder & homicide. Epidemiologia E Psichiatria 
>Sociale, 9, 171-189
>
>Any comments are welcomed,
>
>Bob
>
>At 01:57 PM 28/01/2007 +0000, Michael Dewey wrote:
>>At 22:01 26/01/2007, Peter Dalgaard wrote:
>>>Bob Green wrote:
>>>>Peetr & Michael,
>>>>
>>>>I now see my description may have confused the issue.  I do want 
>>>>to compare odds ratios across studies - in the sense that I want 
>>>>to create a table with the respective odds ratio for each study. 
>>>>I do not need to statistically test two sets of odds ratios.
>>>>
>>>>What I want to do is ensure the method I use to compute an odds 
>>>>ratio is accurate and intended to check my method against published sources.
>>>>
>>>>The paper I selected by Schanda et al (2004). Homicide and major 
>>>>mental disorders. Acta Psychiatr Scand, 11:98-107 reports a total 
>>>>sample of 1087. Odds ratios are reported separately for men and 
>>>>women. There were 961 men all of whom were convicted of homicide. 
>>>>Of these 961 men, 41 were diagnosed with schizophrenia. The 
>>>>unadjusted odds ratio is for this  group of 41 is cited as 
>>>>6.52   (4.70-9.00).  They also report the general population aged 
>>>>over 15 with schizophrenia =20,109 and the total population =2,957,239.
>>
>>Looking at the paper (which is in volume 110 by the way) suggests 
>>that Peter's reading of the situation is correct and that is what 
>>the authors have done.
>>
>>>>Any further clarification is much appreciated,
>>>A fisher.test on the following matrix seems about right:
>>> > matrix(c(41,920,20109-41,2957239-20109-920),2)
>>>
>>>     [,1]    [,2]
>>>[1,]   41   20068
>>>[2,]  920 2936210
>>>
>>> > fisher.test(matrix(c(41,920,20109-41,2957239-20109-920),2))
>>>
>>>        Fisher's Exact Test for Count Data
>>>
>>>data:  matrix(c(41, 920, 20109 - 41, 2957239 - 20109 - 920), 2)
>>>p-value < 2.2e-16
>>>alternative hypothesis: true odds ratio is not equal to 1
>>>95 percent confidence interval:
>>>4.645663 8.918425
>>>sample estimates:
>>>odds ratio
>>>  6.520379
>>>
>>>The c.i. is not precisely the same as your source. This could be 
>>>down to a different approximation (R's is based on the noncentral 
>>>hypergeometric distribution), but the classical asymptotic formula gives
>>>
>>> > exp(log(41*2936210/920/20068)+qnorm(c(.025,.975))*sqrt(sum(1/M)))
>>>[1] 4.767384 8.918216
>>>
>>>which is closer, but still a bit narrower.
>>
>>Michael Dewey
>>http://www.aghmed.fsnet.co.uk
>

Michael Dewey
http://www.aghmed.fsnet.co.uk


From wl2776 at gmail.com  Mon Jan 29 12:52:48 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Mon, 29 Jan 2007 03:52:48 -0800 (PST)
Subject: [R] how to explore contents of R data file from command line?
Message-ID: <8688071.post@talk.nabble.com>


Dear all,

I have a directory with my research project, containing files 
.RData 
and 
inflow.RData

I am just curious, is there any way to explore contents of inflow.RData from
command line without affecting .RData and without copying inflow.RData to
another location?
I can see names and character attributes (of something in the file) in a 3rd
party raw file viewer.

-- 
View this message in context: http://www.nabble.com/how-to-explore-contents-of-R-data-file-from-command-line--tf3135483.html#a8688071
Sent from the R help mailing list archive at Nabble.com.


From wwwhsd at gmail.com  Mon Jan 29 13:21:03 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Mon, 29 Jan 2007 10:21:03 -0200
Subject: [R] how to explore contents of R data file from command line?
In-Reply-To: <8688071.post@talk.nabble.com>
References: <8688071.post@talk.nabble.com>
Message-ID: <da79af330701290421p5089e955j5ee4751cf3129119@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070129/fe8ab6a4/attachment.pl 

From kubovy at virginia.edu  Mon Jan 29 13:40:18 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Mon, 29 Jan 2007 07:40:18 -0500
Subject: [R] lmer2 error under Mac OS X on PowerPC G5 but not on
	Dual-Core Intel Xeon
In-Reply-To: <82D8B52C-E3EE-4B2F-AFBF-7897E889ECDB@jhsph.edu>
References: <7665FCCA-208B-4B07-9A95-A7559D0DD3C4@virginia.edu>
	<82D8B52C-E3EE-4B2F-AFBF-7897E889ECDB@jhsph.edu>
Message-ID: <2EF22041-6C3E-44DF-ABED-19AA08FA5BC4@virginia.edu>

On Jan 28, 2007, at 9:39 PM, Benilton Carvalho wrote:

> This seems to be due to the fact that you didn't have enough memory  
> when running lmer2.
>
> I might be wrong, but I think Calloc tries to get contiguous  
> memory, so this might the problem.
>
> If you are positive that you have enough memory, a gc() might help.

I have 2 GB memory on this machine. Should be enough, no?

 > gc()
           used (Mb) gc trigger (Mb) max used (Mb)
Ncells 1008175 27.0    1476915 39.5  1368491 36.6
Vcells  540055  4.2    1031040  7.9  1031026  7.9
 > (fm1 <- lmer2(Reaction ~ Days + (Days|Subject), sleepstudy))
Error in as.double(start) : Calloc could not allocate (903190944 of  
4) memory


> On Jan 28, 2007, at 8:35 PM, Michael Kubovy wrote:
>
>>> (fm1 <- lmer2(Reaction ~ Days + (Days|Subject), sleepstudy))
>> Error in as.double(start) : Calloc could not allocate (888475968 of
>> 4) memory
>> *************************
>>> sessionInfo()
>> R version 2.4.1 (2006-12-18)
>> powerpc-apple-darwin8.8.0
>>
>> locale:
>> C
>>
>> attached base packages:
>> [1] "grid"      "datasets"  "stats"     "graphics"  "grDevices"
>> "utils"     "methods"
>> [8] "base"
>>
>> other attached packages:
>>          lme4       Matrix       xtable latticeExtra      lattice
>> gridBase         MASS
>> "0.9975-11"   "0.9975-8"      "1.4-3"      "0.1-4"    "0.14-16"
>> "0.4-3"     "7.2-31"
>>           JGR       iplots       JavaGD        rJava
>>      "1.4-15"      "1.0-5"      "0.3-5"     "0.4-13"
>> *************************
>> lmer runs the example w/o a problem
>>
>> I just tried to run it on on Intel-based MacPro, and lmer2 ran
>> without a hitch.
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From wl2776 at gmail.com  Mon Jan 29 13:51:17 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Mon, 29 Jan 2007 04:51:17 -0800 (PST)
Subject: [R] how to explore contents of R data file from command line?
In-Reply-To: <da79af330701290421p5089e955j5ee4751cf3129119@mail.gmail.com>
References: <8688071.post@talk.nabble.com>
	<da79af330701290421p5089e955j5ee4751cf3129119@mail.gmail.com>
Message-ID: <8688767.post@talk.nabble.com>


Thank you, I have used this way already.
I would like to avoid copying files, as I have asked before.

I have found one more solution in Windows.

Just typing inflow.RData (the file name) in command line and pressing enter
runs Rgui, and it loads the file.
Then ls() and str() will give the insight of the file contents.
When exiting from R, one needs to say "No" to the R's question about saving
workspace image, as answering "Yes" deletes older .RData (default workspace
image).


Henrique Dallazuanna wrote:
> 
> You can try copy the file into another location and in the R:
> 
> load(file.choose())
> ls()
> 
> choose the file .RData
> 
> On 29/01/07, Vladimir Eremeev <wl2776 at gmail.com> wrote:
>> Dear all,
>>
>> I have a directory with my research project, containing files
>> .RData
>> and
>> inflow.RData
>>
>> I am just curious, is there any way to explore contents of
>> inflow.RDatafrom
>> command line without affecting .RData and 
>> without copying inflow.RData to
>> another location?
>> I can see names and character attributes (of something in the file) in a
>> 3rd
>> party raw file viewer.
> Henrique Dallazuanna
> 

-- 
View this message in context: http://www.nabble.com/how-to-explore-contents-of-R-data-file-from-command-line--tf3135483.html#a8688767
Sent from the R help mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Mon Jan 29 14:21:08 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 29 Jan 2007 08:21:08 -0500
Subject: [R] Error in merging
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3E4BA2C@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC3E4BA2C@BAN-MAILSRV03.Amba.com>
Message-ID: <971536df0701290521j3bff6bf7m2564d52d95b8d763@mail.gmail.com>

Please read the last line of every message to r-help.

On 1/29/07, Shubha Vishwanath Karanth <shubhak at ambaresearch.com> wrote:
> Hi R,
>
>
>
> The error I get if I try to merge the zoo objects, intra1, intra2, and
> intra3 are as follows:
>
>
>
> > z=merge(intra1,intra2,intra3)
>
> Error in dimnames(x) <- dn : length of 'dimnames' [2] not equal to array
> extent
>
>
>
> Does this error mean that when merged, two columns have the same column
> name and so we get this error and hence not able to merge? Need a
> concurrence...
>
>
>
> Thanks in advance...
>
> Shubha
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Mon Jan 29 14:46:07 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 29 Jan 2007 13:46:07 +0000 (GMT)
Subject: [R] how to explore contents of R data file from command line?
In-Reply-To: <8688767.post@talk.nabble.com>
References: <8688071.post@talk.nabble.com>
	<da79af330701290421p5089e955j5ee4751cf3129119@mail.gmail.com>
	<8688767.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.64.0701291344340.21657@gannet.stats.ox.ac.uk>

On Mon, 29 Jan 2007, Vladimir Eremeev wrote:

> Thank you, I have used this way already.
> I would like to avoid copying files, as I have asked before.

attach("inflow.RData")
ls(2)
...
detach(2)


>
> I have found one more solution in Windows.
>
> Just typing inflow.RData (the file name) in command line and pressing enter
> runs Rgui, and it loads the file.
> Then ls() and str() will give the insight of the file contents.
> When exiting from R, one needs to say "No" to the R's question about saving
> workspace image, as answering "Yes" deletes older .RData (default workspace
> image).
>
>
> Henrique Dallazuanna wrote:
>>
>> You can try copy the file into another location and in the R:
>>
>> load(file.choose())
>> ls()
>>
>> choose the file .RData
>>
>> On 29/01/07, Vladimir Eremeev <wl2776 at gmail.com> wrote:
>>> Dear all,
>>>
>>> I have a directory with my research project, containing files
>>> .RData
>>> and
>>> inflow.RData
>>>
>>> I am just curious, is there any way to explore contents of
>>> inflow.RDatafrom
>>> command line without affecting .RData and
>>> without copying inflow.RData to
>>> another location?
>>> I can see names and character attributes (of something in the file) in a
>>> 3rd
>>> party raw file viewer.
>> Henrique Dallazuanna
>>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From C.Rosa at lse.ac.uk  Mon Jan 29 16:06:01 2007
From: C.Rosa at lse.ac.uk (C.Rosa at lse.ac.uk)
Date: Mon, 29 Jan 2007 15:06:01 -0000
Subject: [R] Loop with string variable AND customizable "summary" output
Message-ID: <8760D6386F7F784693F05256139A1FFB70F70A@EXCHF3.lse.ac.uk>

Dear All,

I am using R for my research and I have two questions about it:

1) is it possible to create a loop using a string, instead of a numeric vector? I have in mind a specific problem:

Suppose you have 2 countries: UK, and USA, one dependent (y) and one independent variable (y) for each country (vale a dire: yUK, xUK, yUSA, xUSA) and you want to run automatically the following regressions:

 

for (i in c("UK","USA"))

output{i}<-summary(lm(y{i} ~ x{i}))

 

In other words, at the end I would like to have two objects as output: "outputUK" and "outputUSA", which contain respectively the results of the first and second regression (yUK on xUK and yUSA on xUSA). 

 

2) in STATA there is a very nice code ("outreg") to display nicely (and as the user wants to) your regression results.

Is there anything similar in R / R contributed packages? More precisely, I am thinking of something that is close in spirit to "summary" but it is also customizable. For example, suppose you want different Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 or a different format display (i.e. without "t value" column) implemented automatically (without manually editing it every time).

In alternative, if I was able to see it, I could modify the source code of the function "summary", but I am not able to see its (line by line) code. Any idea?

Or may be a customizable regression output already exists?

Thanks really a lot!

Carlo


From shubhak at ambaresearch.com  Mon Jan 29 16:20:49 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Mon, 29 Jan 2007 20:50:49 +0530
Subject: [R] Bayesian States Space Modeling
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3E4BC1A@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070129/04d26391/attachment.pl 

From Roger.Bivand at nhh.no  Mon Jan 29 16:31:01 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 29 Jan 2007 16:31:01 +0100 (CET)
Subject: [R] Loop with string variable AND customizable "summary" output
In-Reply-To: <8760D6386F7F784693F05256139A1FFB70F70A@EXCHF3.lse.ac.uk>
Message-ID: <Pine.LNX.4.44.0701291625520.2715-100000@reclus.nhh.no>

On Mon, 29 Jan 2007 C.Rosa at lse.ac.uk wrote:

> Dear All,
> 
> I am using R for my research and I have two questions about it:
> 
> 1) is it possible to create a loop using a string, instead of a numeric
> vector? I have in mind a specific problem:
> 
> Suppose you have 2 countries: UK, and USA, one dependent (y) and one
> independent variable (y) for each country (vale a dire: yUK, xUK, yUSA,
> xUSA) and you want to run automatically the following regressions:
> 
>  
> 
> for (i in c("UK","USA"))
> 
> output{i}<-summary(lm(y{i} ~ x{i}))
> 
>  
> 
> In other words, at the end I would like to have two objects as output:
> "outputUK" and "outputUSA", which contain respectively the results of
> the first and second regression (yUK on xUK and yUSA on xUSA).
> 

The input data could be reshaped as y, x, country, and subset= used in the 
lm() call. To assign to named objects see assign(), but consider using a 
named list instead, assigning to a list of the required length in turn, 
and giving the names from the defining vector. Then you'd get output$UK, 
etc.

>  
> 
> 2) in STATA there is a very nice code ("outreg") to display nicely (and
> as the user wants to) your regression results.
> 
> Is there anything similar in R / R contributed packages? More precisely,
> I am thinking of something that is close in spirit to "summary" but it
> is also customizable. For example, suppose you want different Signif.
> codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 or a different
> format display (i.e. without "t value" column) implemented automatically
> (without manually editing it every time).
> 
> In alternative, if I was able to see it, I could modify the source code
> of the function "summary", but I am not able to see its (line by line)
> code. Any idea?

Use a custom function on the output object from using the summary() method 
on the lm object (that is on the summary.lm object). Use str() to look at 
the summary.lm object to see what you want.

> 
> Or may be a customizable regression output already exists?
> 
> Thanks really a lot!
> 
> Carlo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From liuwensui at gmail.com  Mon Jan 29 16:39:45 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 29 Jan 2007 10:39:45 -0500
Subject: [R] Loop with string variable AND customizable "summary" output
In-Reply-To: <8760D6386F7F784693F05256139A1FFB70F70A@EXCHF3.lse.ac.uk>
References: <8760D6386F7F784693F05256139A1FFB70F70A@EXCHF3.lse.ac.uk>
Message-ID: <1115a2b00701290739t68a8250aqad63b4043ea1c3ee@mail.gmail.com>

Carlo,

try something like:

for (i in c("UK","USA"))
{
summ<-summary(lm(y ~ x), subset = (country = i))
assign(paste('output', i, sep = ''), summ);
}

(note: it is untested, sorry).

On 1/29/07, C.Rosa at lse.ac.uk <C.Rosa at lse.ac.uk> wrote:
> Dear All,
>
> I am using R for my research and I have two questions about it:
>
> 1) is it possible to create a loop using a string, instead of a numeric vector? I have in mind a specific problem:
>
> Suppose you have 2 countries: UK, and USA, one dependent (y) and one independent variable (y) for each country (vale a dire: yUK, xUK, yUSA, xUSA) and you want to run automatically the following regressions:
>
>
>
> for (i in c("UK","USA"))
>
> output{i}<-summary(lm(y{i} ~ x{i}))
>
>
>
> In other words, at the end I would like to have two objects as output: "outputUK" and "outputUSA", which contain respectively the results of the first and second regression (yUK on xUK and yUSA on xUSA).
>
>
>
> 2) in STATA there is a very nice code ("outreg") to display nicely (and as the user wants to) your regression results.
>
> Is there anything similar in R / R contributed packages? More precisely, I am thinking of something that is close in spirit to "summary" but it is also customizable. For example, suppose you want different Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 or a different format display (i.e. without "t value" column) implemented automatically (without manually editing it every time).
>
> In alternative, if I was able to see it, I could modify the source code of the function "summary", but I am not able to see its (line by line) code. Any idea?
>
> Or may be a customizable regression output already exists?
>
> Thanks really a lot!
>
> Carlo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From bcarvalh at jhsph.edu  Mon Jan 29 16:40:59 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Mon, 29 Jan 2007 10:40:59 -0500
Subject: [R] lmer2 error under Mac OS X on PowerPC G5 but not on
	Dual-Core Intel Xeon
In-Reply-To: <2EF22041-6C3E-44DF-ABED-19AA08FA5BC4@virginia.edu>
References: <7665FCCA-208B-4B07-9A95-A7559D0DD3C4@virginia.edu>
	<82D8B52C-E3EE-4B2F-AFBF-7897E889ECDB@jhsph.edu>
	<2EF22041-6C3E-44DF-ABED-19AA08FA5BC4@virginia.edu>
Message-ID: <5BB734C9-EBD7-49E7-8CC6-DD55AF76BB1C@jhsph.edu>

So, I decided to give it a try (and just now noticed that this is the  
example in lmer2)

I just gave it a try on a PPC G4 and it worked as expected. I'm  
copying R-sig-mac (sorry for the crosspost) as the experts there  
might give you a better suggestion.

 > fm1 <- lmer2(Reaction ~ Days + (Days|Subject), sleepstudy)
 > fm1
Linear mixed-effects model fit by REML
Formula: Reaction ~ Days + (Days | Subject)
    Data: sleepstudy
   AIC  BIC logLik MLdeviance REMLdeviance
1754 1770 -871.8       1752         1744
Random effects:
Groups   Name        Variance Std.Dev. Corr
Subject  (Intercept) 612.128  24.7412
           Days         35.049   5.9202  0.066
Residual             654.970  25.5924
Number of obs: 180, groups: Subject, 18

Fixed effects:
             Estimate Std. Error t value
(Intercept)  251.405      6.825   36.84
Days          10.467      1.545    6.77

Correlation of Fixed Effects:
      (Intr)
Days -0.137
 > sessionInfo()
R version 2.5.0 Under development (unstable) (2007-01-03 r40349)
powerpc-apple-darwin8.8.0

locale:
C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"   
"methods"
[7] "base"

other attached packages:
        lme4      Matrix     lattice
"0.9975-11"  "0.9975-8"   "0.14-16"


On Jan 29, 2007, at 7:40 AM, Michael Kubovy wrote:

> On Jan 28, 2007, at 9:39 PM, Benilton Carvalho wrote:
>
>> This seems to be due to the fact that you didn't have enough  
>> memory when running lmer2.
>>
>> I might be wrong, but I think Calloc tries to get contiguous  
>> memory, so this might the problem.
>>
>> If you are positive that you have enough memory, a gc() might help.
>
> I have 2 GB memory on this machine. Should be enough, no?
>
> > gc()
>           used (Mb) gc trigger (Mb) max used (Mb)
> Ncells 1008175 27.0    1476915 39.5  1368491 36.6
> Vcells  540055  4.2    1031040  7.9  1031026  7.9
> > (fm1 <- lmer2(Reaction ~ Days + (Days|Subject), sleepstudy))
> Error in as.double(start) : Calloc could not allocate (903190944 of  
> 4) memory
>
>
>> On Jan 28, 2007, at 8:35 PM, Michael Kubovy wrote:
>>
>>>> (fm1 <- lmer2(Reaction ~ Days + (Days|Subject), sleepstudy))
>>> Error in as.double(start) : Calloc could not allocate (888475968 of
>>> 4) memory
>>> *************************
>>>> sessionInfo()
>>> R version 2.4.1 (2006-12-18)
>>> powerpc-apple-darwin8.8.0
>>>
>>> locale:
>>> C
>>>
>>> attached base packages:
>>> [1] "grid"      "datasets"  "stats"     "graphics"  "grDevices"
>>> "utils"     "methods"
>>> [8] "base"
>>>
>>> other attached packages:
>>>          lme4       Matrix       xtable latticeExtra      lattice
>>> gridBase         MASS
>>> "0.9975-11"   "0.9975-8"      "1.4-3"      "0.1-4"    "0.14-16"
>>> "0.4-3"     "7.2-31"
>>>           JGR       iplots       JavaGD        rJava
>>>      "1.4-15"      "1.0-5"      "0.3-5"     "0.4-13"
>>> *************************
>>> lmer runs the example w/o a problem
>>>
>>> I just tried to run it on on Intel-based MacPro, and lmer2 ran
>>> without a hitch.
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
> Parcels:    Room 102        Gilmer Hall
>         McCormick Road    Charlottesville, VA 22903
> Office:    B011    +1-434-982-4729
> Lab:        B019    +1-434-982-4751
> Fax:        +1-434-982-4766
> WWW:    http://www.people.virginia.edu/~mk9y/
>


From wl2776 at gmail.com  Mon Jan 29 16:50:12 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Mon, 29 Jan 2007 07:50:12 -0800 (PST)
Subject: [R] Loop with string variable AND customizable "summary" output
In-Reply-To: <8760D6386F7F784693F05256139A1FFB70F70A@EXCHF3.lse.ac.uk>
References: <8760D6386F7F784693F05256139A1FFB70F70A@EXCHF3.lse.ac.uk>
Message-ID: <8691620.post@talk.nabble.com>



C.Rosa wrote:
> 
> Dear All,
> 
> I am using R for my research and I have two questions about it:
> 
> 1) is it possible to create a loop using a string, instead of a numeric
> vector? I have in mind a specific problem:
> 
> for (i in c("UK","USA"))
> 
> output{i}<-summary(lm(y{i} ~ x{i}))
> 
> In other words, at the end I would like to have two objects as output:
> "outputUK" and "outputUSA", which contain respectively the results of the
> first and second regression (yUK on xUK and yUSA on xUSA). 
> 

Consider R functions bquote, substitute, eval and parse.

Several examples are given somewhere in RNews
(http://cran.r-project.org/doc/Rnews/)
Unfortunately I don't remember exactly which issue, one of list members sent
me a link to the article several years ago, when I was studying similar
question.
 

C.Rosa wrote:
> 
> 2)  I am thinking of something that is close in spirit to "summary" but it
> is also customizable. For example, suppose you want different Signif.
> codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 or a different
> format display (i.e. without "t value" column) implemented automatically
> (without manually editing it every time).
> 
> In alternative, if I was able to see it, I could modify the source code of
> the function "summary", but I am not able to see its (line by line) code.
> Any idea?
> 

Stars and significance codes are printed with the symnum function.

To customize the summary, explore the result returned by the lm.
For example, 
  str(outputUK)

you will see, it is a list.
Then you will be able to reference its elements with $ (say, outputUK$coeff)

R is an object oriented language, and calls of the same function on
different objects usually invoke different functions (if a class has a
description of proper method). 
The R manuals contain very good description of this mechanism. 

Function methods gives you a list of all defined methods
For example
> methods(summary)
> methods(print)

If you are working with the lm results, you need to explore the function
print.summary.lm

> summary(outputUK)

invokes summary.lm function, as outputUK is the object of class "lm". 
This function produces the object of class "summary.lm"
Then this object is printed with the method print.summary.lm


-- 
View this message in context: http://www.nabble.com/-R--Loop-with-string-variable-AND-customizable-%22summary%22-output-tf3136358.html#a8691620
Sent from the R help mailing list archive at Nabble.com.


From wl2776 at gmail.com  Mon Jan 29 17:07:22 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Mon, 29 Jan 2007 08:07:22 -0800 (PST)
Subject: [R] Loop with string variable AND customizable "summary" output
In-Reply-To: <8760D6386F7F784693F05256139A1FFB70F70A@EXCHF3.lse.ac.uk>
References: <8760D6386F7F784693F05256139A1FFB70F70A@EXCHF3.lse.ac.uk>
Message-ID: <8692041.post@talk.nabble.com>


That is

C.Rosa wrote:
> 
> for (i in c("UK","USA"))
> output{i}<-summary(lm(y{i} ~ x{i}))
> 

for (i in c("UK","USA")) {
  lm.txt<-paste("output",i,"<-","lm(","y",i,"x",i,")",sep="") # 1. produce a
character string containing needed expression
  eval(parse(text=lm.txt))                                                 #
2. parse and evaluate it
}
-- 
View this message in context: http://www.nabble.com/-R--Loop-with-string-variable-AND-customizable-%22summary%22-output-tf3136358.html#a8692041
Sent from the R help mailing list archive at Nabble.com.


From wl2776 at gmail.com  Mon Jan 29 17:08:52 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Mon, 29 Jan 2007 08:08:52 -0800 (PST)
Subject: [R] sorry, I forgot the tilde
In-Reply-To: <8692041.post@talk.nabble.com>
References: <8760D6386F7F784693F05256139A1FFB70F70A@EXCHF3.lse.ac.uk>
	<8692041.post@talk.nabble.com>
Message-ID: <8692073.post@talk.nabble.com>




Vladimir Eremeev wrote:
> 
> That is
> 
> C.Rosa wrote:
>> 
>> for (i in c("UK","USA"))
>> output{i}<-summary(lm(y{i} ~ x{i}))
>> 
> 
> for (i in c("UK","USA")) {
>   lm.txt<-paste("output",i,"<-","lm(","y",i,"~","x",i,")",sep="") # 1.
> produce a character string containing needed expression
>   eval(parse(text=lm.txt))                                                
> # 2. parse and evaluate it
> }
> 

-- 
View this message in context: http://www.nabble.com/-R--Loop-with-string-variable-AND-customizable-%22summary%22-output-tf3136358.html#a8692073
Sent from the R help mailing list archive at Nabble.com.


From C.Rosa at lse.ac.uk  Mon Jan 29 17:22:38 2007
From: C.Rosa at lse.ac.uk (C.Rosa at lse.ac.uk)
Date: Mon, 29 Jan 2007 16:22:38 -0000
Subject: [R] Loop with string variable AND customizable "summary" output
References: <8760D6386F7F784693F05256139A1FFB70F70A@EXCHF3.lse.ac.uk>
	<1115a2b00701290739t68a8250aqad63b4043ea1c3ee@mail.gmail.com>
Message-ID: <8760D6386F7F784693F05256139A1FFB70F70F@EXCHF3.lse.ac.uk>

Dear All,
Thank you very much for your help!
Carlo

-----Original Message-----
From: Wensui Liu [mailto:liuwensui at gmail.com]
Sent: Mon 29/01/2007 15:39
To: Rosa,C
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Loop with string variable AND customizable "summary" output
 
Carlo,

try something like:

for (i in c("UK","USA"))
{
summ<-summary(lm(y ~ x), subset = (country = i))
assign(paste('output', i, sep = ''), summ);
}

(note: it is untested, sorry).

On 1/29/07, C.Rosa at lse.ac.uk <C.Rosa at lse.ac.uk> wrote:
> Dear All,
>
> I am using R for my research and I have two questions about it:
>
> 1) is it possible to create a loop using a string, instead of a numeric vector? I have in mind a specific problem:
>
> Suppose you have 2 countries: UK, and USA, one dependent (y) and one independent variable (y) for each country (vale a dire: yUK, xUK, yUSA, xUSA) and you want to run automatically the following regressions:
>
>
>
> for (i in c("UK","USA"))
>
> output{i}<-summary(lm(y{i} ~ x{i}))
>
>
>
> In other words, at the end I would like to have two objects as output: "outputUK" and "outputUSA", which contain respectively the results of the first and second regression (yUK on xUK and yUSA on xUSA).
>
>
>
> 2) in STATA there is a very nice code ("outreg") to display nicely (and as the user wants to) your regression results.
>
> Is there anything similar in R / R contributed packages? More precisely, I am thinking of something that is close in spirit to "summary" but it is also customizable. For example, suppose you want different Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 or a different format display (i.e. without "t value" column) implemented automatically (without manually editing it every time).
>
> In alternative, if I was able to see it, I could modify the source code of the function "summary", but I am not able to see its (line by line) code. Any idea?
>
> Or may be a customizable regression output already exists?
>
> Thanks really a lot!
>
> Carlo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From rmh at temple.edu  Mon Jan 29 17:25:47 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Mon, 29 Jan 2007 11:25:47 -0500 (EST)
Subject: [R] Fwd: Re:   LSD multiple comparison test
Message-ID: <20070129112547.BTD33671@po-d.temple.edu>

I am returning this to the R-help list.  Please keep followups on the list.

Yes, it can be done.  It is not currently easy because multcomp doesn't
have the syntax yet.  Making this easy is on Torsten's to-do list for the
multcomp package.

See the MMC.WoodEnergy example in the HH package.  The current version on CRAN
is HH_1.17.  Please see the discussion of this example in R-help:

https://stat.ethz.ch/pipermail/r-help/2007-January/123451.html

---- Original message ----
>Date: Mon, 29 Jan 2007 16:42:35 +0100 (CET)
>From: "Jorge Lampurlanes Castel" <jlampur at eagrof.UdL.es>  
>Subject: Re: [R]  LSD multiple comparison test  
>To: "Richard M. Heiberger" <rmh at temple.edu>
>
>Thank you very much for your useful advice.
>I do not found LSD but I am using Tukey test instead.
>
>In the model:
>
>  lm.1 <- lm(variable ~ BLOC + TIL * YEAR , data=selvanera)
>
>
>I found TIL*YEAR interaction significant. Then I am trying to compare
>means of the different levels of TIL inside every YEAR using:
>
>  mc.2 <- glht(lm.1, linfct = mcp(TIL*YEAR="Tukey"))
>  summary(mc.2, test = univariate())
>
>but it does not work.
>
>There is any way of doing it, like the SLICE option in PROC GLM (SAS)?
>
>Thanks a lot,
>
>Jorge
>
>> Look at the glht function in the multcomp package and the
>> MMC functions in the HH package.
>>
>> Rich
>>
>
>
>-- 
>**************************************************
>Jorge Lampurlan?s Castel
>Departament d'Enginyeria Agroforestal
>Escola T?cnica Superior d'Enginyeria Agr?ria
>Universitat de Lleida
>Avinguda Rovira Roure, 191
>25198-LLEIDA
>SPAIN
>
>Tl.: +34 973 70 25 37
>Fax.:+34 073 70 26 73
>e-mail: jlampur at eagrof.udl.es
>**************************************************
>


From ggrothendieck at gmail.com  Mon Jan 29 17:27:03 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 29 Jan 2007 11:27:03 -0500
Subject: [R] Loop with string variable AND customizable "summary" output
In-Reply-To: <8760D6386F7F784693F05256139A1FFB70F70A@EXCHF3.lse.ac.uk>
References: <8760D6386F7F784693F05256139A1FFB70F70A@EXCHF3.lse.ac.uk>
Message-ID: <971536df0701290827y58b29f4bh7ae436c3c4c861a2@mail.gmail.com>

Often you will find that if you arrange your data in a
desirable way in the first place everything becomes
easier.  What you really want is a data frame such
as the last three columns of the builtin data frame
CO2 where Treatment corresponds to country and
the two numeric variables correspond to your y and x.

Then its easy:

lapply(levels(CO2$Treatment), function(lev)
   lm(uptake ~ conc, CO2, subset = Treatment == lev))

The only problem with the above is that the Call: in the
output does not really tell you which level of Treatment
is being used since it literally shows
  "lm(uptake ~ conc, CO2, subset = Treatment == lev)"
each time.  To get around substitute the value of lev in.
Because R uses delayed evaluation you also need to force the
evaluation of lev prior to substituting it in:

lapply(levels(CO2$Treatment), function(lev) {
   lev <- force(lev)
   eval(substitute(lm(uptake ~ conc, CO2, subset = Treatment == lev)),
     list(lev = lev))
})


Now if you really want to do it the way you specified originally
try this.

Suppose we use attach to grab the variables
x1, x2, x3, x4, y1, y2, y3, y4 out of the builtin
anscombe data frame for purposes of getting
our hands on some sample data.   In your case
the variables would already be in the workspace
so the attach is not needed.

Then simply reconstruct the formula in fo.  You
could simply use lm(fo) but then the Call: in the
output of lm would literally read lm(fo) so its
better to use do.call:

# next line gives the variables x1, x2, x3, x4, y1, y2, y3, y4
# from the builtin ancombe data set.
# In your case such variables would already exist.
attach(anscombe)
lapply(1:4, function(i) {
   ynm <- paste("y", i, sep = "")
   xnm <- paste("x", i, sep = "")
   fo <- as.formula(paste(ynm, "~", xnm))
   do.call("lm", list(fo))
})
detach(anscombe)

Or if all the variables have the same length you could use
a form such as ancombe in the first place:

Actually this is not really a recommended way of
proceeding. You would be better off putting all
your variables in a data frame and using that.

lapply(1:4, function(i) {
    fo <- as.formula(paste(names(anscombe)[i+4], "~", names(anscombe)[i]))
    do.call("lm", list(fo, data = quote(anscombe)))
})

or

lapply(1:4, function(i) {
    fo <- y ~ x
    fo[[2]] <- as.name(names(anscombe)[i+4])
    fo[[3]] <- as.name(names(anscombe)[i])
    do.call("lm", list(fo, data = quote(anscombe)))
})



On 1/29/07, C.Rosa at lse.ac.uk <C.Rosa at lse.ac.uk> wrote:
> Dear All,
>
> I am using R for my research and I have two questions about it:
>
> 1) is it possible to create a loop using a string, instead of a numeric vector? I have in mind a specific problem:
>
> Suppose you have 2 countries: UK, and USA, one dependent (y) and one independent variable (y) for each country (vale a dire: yUK, xUK, yUSA, xUSA) and you want to run automatically the following regressions:
>
>
>
> for (i in c("UK","USA"))
>
> output{i}<-summary(lm(y{i} ~ x{i}))
>
>
>
> In other words, at the end I would like to have two objects as output: "outputUK" and "outputUSA", which contain respectively the results of the first and second regression (yUK on xUK and yUSA on xUSA).
>
>
>
> 2) in STATA there is a very nice code ("outreg") to display nicely (and as the user wants to) your regression results.
>
> Is there anything similar in R / R contributed packages? More precisely, I am thinking of something that is close in spirit to "summary" but it is also customizable. For example, suppose you want different Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 or a different format display (i.e. without "t value" column) implemented automatically (without manually editing it every time).
>
> In alternative, if I was able to see it, I could modify the source code of the function "summary", but I am not able to see its (line by line) code. Any idea?
>
> Or may be a customizable regression output already exists?
>
> Thanks really a lot!
>
> Carlo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From simon.urbanek at r-project.org  Mon Jan 29 17:28:39 2007
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Mon, 29 Jan 2007 11:28:39 -0500
Subject: [R] [R-SIG-Mac] lmer2 error under Mac OS X on PowerPC G5 but
	not on Dual-Core Intel Xeon
In-Reply-To: <5BB734C9-EBD7-49E7-8CC6-DD55AF76BB1C@jhsph.edu>
References: <7665FCCA-208B-4B07-9A95-A7559D0DD3C4@virginia.edu>
	<82D8B52C-E3EE-4B2F-AFBF-7897E889ECDB@jhsph.edu>
	<2EF22041-6C3E-44DF-ABED-19AA08FA5BC4@virginia.edu>
	<5BB734C9-EBD7-49E7-8CC6-DD55AF76BB1C@jhsph.edu>
Message-ID: <A0ABAC88-BCDE-40D9-B1E7-001702F32E17@r-project.org>

I'm not sure - what is the question here? It works for me on a both  
PowerPC G5 and Mac Pro (R 2.4.1 CRAN binary):

 > fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
 > fm1
Linear mixed-effects model fit by REML
Formula: Reaction ~ Days + (Days | Subject)
    Data: sleepstudy
   AIC  BIC logLik MLdeviance REMLdeviance
1754 1770 -871.8       1752         1744
Random effects:
Groups   Name        Variance Std.Dev. Corr
Subject  (Intercept) 610.835  24.7151
           Days         35.056   5.9208  0.067
Residual             655.066  25.5943
number of obs: 180, groups: Subject, 18

Fixed effects:
             Estimate Std. Error t value
(Intercept)  251.405      6.820   36.86
Days          10.467      1.546    6.77

Correlation of Fixed Effects:
      (Intr)
Days -0.137
 > gc()
          used (Mb) gc trigger (Mb) max used (Mb)
Ncells 966960 25.9    1368491 36.6  1265230 33.8
Vcells 523595  4.0    1031040  7.9   755843  5.8
 > sessionInfo()
R version 2.4.1 (2006-12-18)
i386-apple-darwin8.8.1

locale:
C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"   
"methods"
[7] "base"

other attached packages:
        lme4      Matrix     lattice
"0.9975-11"  "0.9975-8"   "0.14-16"

Cheers,
Simon

On Jan 29, 2007, at 10:40 AM, Benilton Carvalho wrote:

> So, I decided to give it a try (and just now noticed that this is the
> example in lmer2)
>
> I just gave it a try on a PPC G4 and it worked as expected. I'm
> copying R-sig-mac (sorry for the crosspost) as the experts there
> might give you a better suggestion.
>
>> fm1 <- lmer2(Reaction ~ Days + (Days|Subject), sleepstudy)
>> fm1
> Linear mixed-effects model fit by REML
> Formula: Reaction ~ Days + (Days | Subject)
>     Data: sleepstudy
>    AIC  BIC logLik MLdeviance REMLdeviance
> 1754 1770 -871.8       1752         1744
> Random effects:
> Groups   Name        Variance Std.Dev. Corr
> Subject  (Intercept) 612.128  24.7412
>            Days         35.049   5.9202  0.066
> Residual             654.970  25.5924
> Number of obs: 180, groups: Subject, 18
>
> Fixed effects:
>              Estimate Std. Error t value
> (Intercept)  251.405      6.825   36.84
> Days          10.467      1.545    6.77
>
> Correlation of Fixed Effects:
>       (Intr)
> Days -0.137
>> sessionInfo()
> R version 2.5.0 Under development (unstable) (2007-01-03 r40349)
> powerpc-apple-darwin8.8.0
>
> locale:
> C
>
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
> "methods"
> [7] "base"
>
> other attached packages:
>         lme4      Matrix     lattice
> "0.9975-11"  "0.9975-8"   "0.14-16"
>
>
> On Jan 29, 2007, at 7:40 AM, Michael Kubovy wrote:
>
>> On Jan 28, 2007, at 9:39 PM, Benilton Carvalho wrote:
>>
>>> This seems to be due to the fact that you didn't have enough
>>> memory when running lmer2.
>>>
>>> I might be wrong, but I think Calloc tries to get contiguous
>>> memory, so this might the problem.
>>>
>>> If you are positive that you have enough memory, a gc() might help.
>>
>> I have 2 GB memory on this machine. Should be enough, no?
>>
>>> gc()
>>           used (Mb) gc trigger (Mb) max used (Mb)
>> Ncells 1008175 27.0    1476915 39.5  1368491 36.6
>> Vcells  540055  4.2    1031040  7.9  1031026  7.9
>>> (fm1 <- lmer2(Reaction ~ Days + (Days|Subject), sleepstudy))
>> Error in as.double(start) : Calloc could not allocate (903190944 of
>> 4) memory
>>
>>
>>> On Jan 28, 2007, at 8:35 PM, Michael Kubovy wrote:
>>>
>>>>> (fm1 <- lmer2(Reaction ~ Days + (Days|Subject), sleepstudy))
>>>> Error in as.double(start) : Calloc could not allocate (888475968 of
>>>> 4) memory
>>>> *************************
>>>>> sessionInfo()
>>>> R version 2.4.1 (2006-12-18)
>>>> powerpc-apple-darwin8.8.0
>>>>
>>>> locale:
>>>> C
>>>>
>>>> attached base packages:
>>>> [1] "grid"      "datasets"  "stats"     "graphics"  "grDevices"
>>>> "utils"     "methods"
>>>> [8] "base"
>>>>
>>>> other attached packages:
>>>>          lme4       Matrix       xtable latticeExtra      lattice
>>>> gridBase         MASS
>>>> "0.9975-11"   "0.9975-8"      "1.4-3"      "0.1-4"    "0.14-16"
>>>> "0.4-3"     "7.2-31"
>>>>           JGR       iplots       JavaGD        rJava
>>>>      "1.4-15"      "1.0-5"      "0.3-5"     "0.4-13"
>>>> *************************
>>>> lmer runs the example w/o a problem
>>>>
>>>> I just tried to run it on on Intel-based MacPro, and lmer2 ran
>>>> without a hitch.
>> _____________________________
>> Professor Michael Kubovy
>> University of Virginia
>> Department of Psychology
>> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
>> Parcels:    Room 102        Gilmer Hall
>>         McCormick Road    Charlottesville, VA 22903
>> Office:    B011    +1-434-982-4729
>> Lab:        B019    +1-434-982-4751
>> Fax:        +1-434-982-4766
>> WWW:    http://www.people.virginia.edu/~mk9y/
>>
>
> _______________________________________________
> R-SIG-Mac mailing list
> R-SIG-Mac at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-mac
>
>


From ggrothendieck at gmail.com  Mon Jan 29 17:57:47 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 29 Jan 2007 11:57:47 -0500
Subject: [R] Loop with string variable AND customizable "summary" output
In-Reply-To: <971536df0701290827y58b29f4bh7ae436c3c4c861a2@mail.gmail.com>
References: <8760D6386F7F784693F05256139A1FFB70F70A@EXCHF3.lse.ac.uk>
	<971536df0701290827y58b29f4bh7ae436c3c4c861a2@mail.gmail.com>
Message-ID: <971536df0701290857l15d4f3fdrae0ee027630468b2@mail.gmail.com>

In thinking about this a bit more here is an even shorter one yet it
does show the level in the Call output.  See ?bquote

lapply(levels(CO2$Treatment), function(lev)
   eval(bquote(lm(uptake ~ conc, CO2, subset = Treatment == .(lev)))))


On 1/29/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Often you will find that if you arrange your data in a
> desirable way in the first place everything becomes
> easier.  What you really want is a data frame such
> as the last three columns of the builtin data frame
> CO2 where Treatment corresponds to country and
> the two numeric variables correspond to your y and x.
>
> Then its easy:
>
> lapply(levels(CO2$Treatment), function(lev)
>   lm(uptake ~ conc, CO2, subset = Treatment == lev))
>
> The only problem with the above is that the Call: in the
> output does not really tell you which level of Treatment
> is being used since it literally shows
>  "lm(uptake ~ conc, CO2, subset = Treatment == lev)"
> each time.  To get around substitute the value of lev in.
> Because R uses delayed evaluation you also need to force the
> evaluation of lev prior to substituting it in:
>
> lapply(levels(CO2$Treatment), function(lev) {
>   lev <- force(lev)
>   eval(substitute(lm(uptake ~ conc, CO2, subset = Treatment == lev)),
>     list(lev = lev))
> })
>
>
> Now if you really want to do it the way you specified originally
> try this.
>
> Suppose we use attach to grab the variables
> x1, x2, x3, x4, y1, y2, y3, y4 out of the builtin
> anscombe data frame for purposes of getting
> our hands on some sample data.   In your case
> the variables would already be in the workspace
> so the attach is not needed.
>
> Then simply reconstruct the formula in fo.  You
> could simply use lm(fo) but then the Call: in the
> output of lm would literally read lm(fo) so its
> better to use do.call:
>
> # next line gives the variables x1, x2, x3, x4, y1, y2, y3, y4
> # from the builtin ancombe data set.
> # In your case such variables would already exist.
> attach(anscombe)
> lapply(1:4, function(i) {
>   ynm <- paste("y", i, sep = "")
>   xnm <- paste("x", i, sep = "")
>   fo <- as.formula(paste(ynm, "~", xnm))
>   do.call("lm", list(fo))
> })
> detach(anscombe)
>
> Or if all the variables have the same length you could use
> a form such as ancombe in the first place:
>
> Actually this is not really a recommended way of
> proceeding. You would be better off putting all
> your variables in a data frame and using that.
>
> lapply(1:4, function(i) {
>    fo <- as.formula(paste(names(anscombe)[i+4], "~", names(anscombe)[i]))
>    do.call("lm", list(fo, data = quote(anscombe)))
> })
>
> or
>
> lapply(1:4, function(i) {
>    fo <- y ~ x
>    fo[[2]] <- as.name(names(anscombe)[i+4])
>    fo[[3]] <- as.name(names(anscombe)[i])
>    do.call("lm", list(fo, data = quote(anscombe)))
> })
>
>
>
> On 1/29/07, C.Rosa at lse.ac.uk <C.Rosa at lse.ac.uk> wrote:
> > Dear All,
> >
> > I am using R for my research and I have two questions about it:
> >
> > 1) is it possible to create a loop using a string, instead of a numeric vector? I have in mind a specific problem:
> >
> > Suppose you have 2 countries: UK, and USA, one dependent (y) and one independent variable (y) for each country (vale a dire: yUK, xUK, yUSA, xUSA) and you want to run automatically the following regressions:
> >
> >
> >
> > for (i in c("UK","USA"))
> >
> > output{i}<-summary(lm(y{i} ~ x{i}))
> >
> >
> >
> > In other words, at the end I would like to have two objects as output: "outputUK" and "outputUSA", which contain respectively the results of the first and second regression (yUK on xUK and yUSA on xUSA).
> >
> >
> >
> > 2) in STATA there is a very nice code ("outreg") to display nicely (and as the user wants to) your regression results.
> >
> > Is there anything similar in R / R contributed packages? More precisely, I am thinking of something that is close in spirit to "summary" but it is also customizable. For example, suppose you want different Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 or a different format display (i.e. without "t value" column) implemented automatically (without manually editing it every time).
> >
> > In alternative, if I was able to see it, I could modify the source code of the function "summary", but I am not able to see its (line by line) code. Any idea?
> >
> > Or may be a customizable regression output already exists?
> >
> > Thanks really a lot!
> >
> > Carlo
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From ripley at stats.ox.ac.uk  Mon Jan 29 18:02:57 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 29 Jan 2007 17:02:57 +0000 (GMT)
Subject: [R] lmer2 error under Mac OS X on PowerPC G5 but not on
 Dual-Core Intel Xeon
In-Reply-To: <2EF22041-6C3E-44DF-ABED-19AA08FA5BC4@virginia.edu>
References: <7665FCCA-208B-4B07-9A95-A7559D0DD3C4@virginia.edu>
	<82D8B52C-E3EE-4B2F-AFBF-7897E889ECDB@jhsph.edu>
	<2EF22041-6C3E-44DF-ABED-19AA08FA5BC4@virginia.edu>
Message-ID: <Pine.LNX.4.64.0701291347350.21657@gannet.stats.ox.ac.uk>

On Mon, 29 Jan 2007, Michael Kubovy wrote:

> On Jan 28, 2007, at 9:39 PM, Benilton Carvalho wrote:
>
>> This seems to be due to the fact that you didn't have enough memory
>> when running lmer2.
>>
>> I might be wrong, but I think Calloc tries to get contiguous
>> memory, so this might the problem.
>>
>> If you are positive that you have enough memory, a gc() might help.

Generally, it almost never helps.  R will itself do a gc() before giving 
up on memory allocation, and R is not allocating memory with Calloc, the 
OS services are being used.  Nothing is going to help allocating 3.6Gb on 
a 32-bit OS.  (One case where it might help is doing a large allocation 
with Calloc after freeing a large R object, on OSes where gc() will 
actually give memory back to the pool Calloc uses.)

What makes no sense is that it says using as.double gave an error message 
from Calloc.  No known as.double method calls Calloc.  This suggests that 
this is a symptom of memory corruption.

>
> I have 2 GB memory on this machine. Should be enough, no?
>
> > gc()
>           used (Mb) gc trigger (Mb) max used (Mb)
> Ncells 1008175 27.0    1476915 39.5  1368491 36.6
> Vcells  540055  4.2    1031040  7.9  1031026  7.9
> > (fm1 <- lmer2(Reaction ~ Days + (Days|Subject), sleepstudy))
> Error in as.double(start) : Calloc could not allocate (903190944 of
> 4) memory
>
>
>> On Jan 28, 2007, at 8:35 PM, Michael Kubovy wrote:
>>
>>>> (fm1 <- lmer2(Reaction ~ Days + (Days|Subject), sleepstudy))
>>> Error in as.double(start) : Calloc could not allocate (888475968 of
>>> 4) memory
>>> *************************
>>>> sessionInfo()
>>> R version 2.4.1 (2006-12-18)
>>> powerpc-apple-darwin8.8.0
>>>
>>> locale:
>>> C
>>>
>>> attached base packages:
>>> [1] "grid"      "datasets"  "stats"     "graphics"  "grDevices"
>>> "utils"     "methods"
>>> [8] "base"
>>>
>>> other attached packages:
>>>          lme4       Matrix       xtable latticeExtra      lattice
>>> gridBase         MASS
>>> "0.9975-11"   "0.9975-8"      "1.4-3"      "0.1-4"    "0.14-16"
>>> "0.4-3"     "7.2-31"
>>>           JGR       iplots       JavaGD        rJava
>>>      "1.4-15"      "1.0-5"      "0.3-5"     "0.4-13"
>>> *************************
>>> lmer runs the example w/o a problem
>>>
>>> I just tried to run it on on Intel-based MacPro, and lmer2 ran
>>> without a hitch.
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
> Parcels:    Room 102        Gilmer Hall
>         McCormick Road    Charlottesville, VA 22903
> Office:    B011    +1-434-982-4729
> Lab:        B019    +1-434-982-4751
> Fax:        +1-434-982-4766
> WWW:    http://www.people.virginia.edu/~mk9y/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ggrothendieck at gmail.com  Mon Jan 29 18:08:41 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 29 Jan 2007 12:08:41 -0500
Subject: [R] Loop with string variable AND customizable "summary" output
In-Reply-To: <971536df0701290857l15d4f3fdrae0ee027630468b2@mail.gmail.com>
References: <8760D6386F7F784693F05256139A1FFB70F70A@EXCHF3.lse.ac.uk>
	<971536df0701290827y58b29f4bh7ae436c3c4c861a2@mail.gmail.com>
	<971536df0701290857l15d4f3fdrae0ee027630468b2@mail.gmail.com>
Message-ID: <971536df0701290908n2dfc6a76nbd690a48e2376312@mail.gmail.com>

And yet one more.  This one does not use eval but uses do.call, quote
and bquote instead:

lapply(levels(CO2$Treatment), function(lev) do.call("lm",
     list(uptake ~ conc, quote(CO2), subset = bquote(Treatment == .(lev)))))


On 1/29/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> In thinking about this a bit more here is an even shorter one yet it
> does show the level in the Call output.  See ?bquote
>
> lapply(levels(CO2$Treatment), function(lev)
>   eval(bquote(lm(uptake ~ conc, CO2, subset = Treatment == .(lev)))))
>
>
> On 1/29/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Often you will find that if you arrange your data in a
> > desirable way in the first place everything becomes
> > easier.  What you really want is a data frame such
> > as the last three columns of the builtin data frame
> > CO2 where Treatment corresponds to country and
> > the two numeric variables correspond to your y and x.
> >
> > Then its easy:
> >
> > lapply(levels(CO2$Treatment), function(lev)
> >   lm(uptake ~ conc, CO2, subset = Treatment == lev))
> >
> > The only problem with the above is that the Call: in the
> > output does not really tell you which level of Treatment
> > is being used since it literally shows
> >  "lm(uptake ~ conc, CO2, subset = Treatment == lev)"
> > each time.  To get around substitute the value of lev in.
> > Because R uses delayed evaluation you also need to force the
> > evaluation of lev prior to substituting it in:
> >
> > lapply(levels(CO2$Treatment), function(lev) {
> >   lev <- force(lev)
> >   eval(substitute(lm(uptake ~ conc, CO2, subset = Treatment == lev)),
> >     list(lev = lev))
> > })
> >
> >
> > Now if you really want to do it the way you specified originally
> > try this.
> >
> > Suppose we use attach to grab the variables
> > x1, x2, x3, x4, y1, y2, y3, y4 out of the builtin
> > anscombe data frame for purposes of getting
> > our hands on some sample data.   In your case
> > the variables would already be in the workspace
> > so the attach is not needed.
> >
> > Then simply reconstruct the formula in fo.  You
> > could simply use lm(fo) but then the Call: in the
> > output of lm would literally read lm(fo) so its
> > better to use do.call:
> >
> > # next line gives the variables x1, x2, x3, x4, y1, y2, y3, y4
> > # from the builtin ancombe data set.
> > # In your case such variables would already exist.
> > attach(anscombe)
> > lapply(1:4, function(i) {
> >   ynm <- paste("y", i, sep = "")
> >   xnm <- paste("x", i, sep = "")
> >   fo <- as.formula(paste(ynm, "~", xnm))
> >   do.call("lm", list(fo))
> > })
> > detach(anscombe)
> >
> > Or if all the variables have the same length you could use
> > a form such as ancombe in the first place:
> >
> > Actually this is not really a recommended way of
> > proceeding. You would be better off putting all
> > your variables in a data frame and using that.
> >
> > lapply(1:4, function(i) {
> >    fo <- as.formula(paste(names(anscombe)[i+4], "~", names(anscombe)[i]))
> >    do.call("lm", list(fo, data = quote(anscombe)))
> > })
> >
> > or
> >
> > lapply(1:4, function(i) {
> >    fo <- y ~ x
> >    fo[[2]] <- as.name(names(anscombe)[i+4])
> >    fo[[3]] <- as.name(names(anscombe)[i])
> >    do.call("lm", list(fo, data = quote(anscombe)))
> > })
> >
> >
> >
> > On 1/29/07, C.Rosa at lse.ac.uk <C.Rosa at lse.ac.uk> wrote:
> > > Dear All,
> > >
> > > I am using R for my research and I have two questions about it:
> > >
> > > 1) is it possible to create a loop using a string, instead of a numeric vector? I have in mind a specific problem:
> > >
> > > Suppose you have 2 countries: UK, and USA, one dependent (y) and one independent variable (y) for each country (vale a dire: yUK, xUK, yUSA, xUSA) and you want to run automatically the following regressions:
> > >
> > >
> > >
> > > for (i in c("UK","USA"))
> > >
> > > output{i}<-summary(lm(y{i} ~ x{i}))
> > >
> > >
> > >
> > > In other words, at the end I would like to have two objects as output: "outputUK" and "outputUSA", which contain respectively the results of the first and second regression (yUK on xUK and yUSA on xUSA).
> > >
> > >
> > >
> > > 2) in STATA there is a very nice code ("outreg") to display nicely (and as the user wants to) your regression results.
> > >
> > > Is there anything similar in R / R contributed packages? More precisely, I am thinking of something that is close in spirit to "summary" but it is also customizable. For example, suppose you want different Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 or a different format display (i.e. without "t value" column) implemented automatically (without manually editing it every time).
> > >
> > > In alternative, if I was able to see it, I could modify the source code of the function "summary", but I am not able to see its (line by line) code. Any idea?
> > >
> > > Or may be a customizable regression output already exists?
> > >
> > > Thanks really a lot!
> > >
> > > Carlo
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
>


From mrennie at utm.utoronto.ca  Mon Jan 29 18:12:54 2007
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Mon, 29 Jan 2007 12:12:54 -0500
Subject: [R] plotting results from tapply;
 using indexing to  incorporate error bars
In-Reply-To: <1169833583.4899.33.camel@localhost.localdomain>
References: <6.1.0.6.0.20070126113039.01c63c78@mail.utm.utoronto.ca>
	<1169833583.4899.33.camel@localhost.localdomain>
Message-ID: <6.1.0.6.0.20070129120156.01e16cb0@mail.utm.utoronto.ca>


Hi, Mark

Thanks for the examples- this is great, and has helped me understand alot 
more what's going on in the plotting functions.

Now that I'm trying to work error bars into this, I was curious if someone 
might give me a hand indexing this properly so that I can format my error 
bars to match the formatting of the grouping variables that match the lines 
in the plot.

Here's a re-worked example from my first submission where I calculate 
standard errors for plotting on the figure, plot the error bars, and try to 
index them to match the appropriate lines. The values appear to be in the 
right place (I've turned on the "legend" option for the interaction plot to 
help verify this), however, my attempts at matching the colours on the bars 
to the colours on the lines fails miserably (as you'll see if you execute 
the code below). Is there any way to assign my colours to match them in a 
way that makes more sense?

Thanks for your help so far,

Mike

y1<-rnorm(40, 2)
x1<-rep(1:2, each=20)
x2<-rep(1:2, each=10, times=2)

ex.dat<-data.frame(cbind(y1,x1,x2))

ex.dat$x1<-factor(ex.dat$x1, labels=c("A", "B"))
ex.dat$x2<-factor(ex.dat$x2, labels=c("C", "D"))

attach(ex.dat)

xbar<-tapply(ex.dat$y1, ex.dat[,-1], mean)
xbar
s <- tapply(ex.dat$y1, ex.dat[,-1], sd)
n <- tapply(ex.dat$y1, ex.dat[,-1], length)
sem <- s/sqrt(n)
sem

row.names(xbar)
xbar[,1]



#from Marc Schwartz#

#par(mfcol = c(1, 3))


options(graphics.record = TRUE)

#using plot

with(ex.dat, plot(1:2, xbar[, 1], ylim = range(y1),
                   type = "b", col = "red",
                   lty = c("dashed", "solid"),
                   xaxt = "n", xlab = "x1",
                   ylab = "mean of y1"))


with(ex.dat, points(1:2, xbar[, 2], col = "blue",
                     type = "b"))


axis(1, at = 1:2, labels = c("A", "B"))


#using matplot

matplot(1:2, xbar, col = c("red", "blue"),
         pch = 21, type = "b", ylim = range(y1),
         lty = c("dashed", "solid"),
         xaxt = "n", xlab = "x1",
         ylab = "mean of y1")


axis(1, at = 1:2, labels = c("A", "B"))
arrows(1:2,xbar+sem, 1:2,xbar-sem, col = c("red", "blue"), angle=90, 
code=3, length=.1)

#using interaction.plot

with(ex.dat, interaction.plot(x1, x2, response = y1,
                               type = "b", pch = 21,
                               col = c("red", "blue"),
                               ylim = range(ex.dat$y1)))

arrows(1:2,xbar+sem, 1:2,xbar-sem, col = c("red", "blue"), angle=90, 
code=3, length=.05)

#as you can see, though the values for standard errors match the 
appropriate means, the assignment of colours does not.



At 12:46 PM 26/01/2007, Marc Schwartz wrote:
>On Fri, 2007-01-26 at 11:50 -0500, Michael Rennie wrote:
> > Hi, there
> >
> > I'm trying to plot what is returned from a call to tapply, and can't 
> figure
> > out how to do it. My guess is that it has something to do with the
> > inclusion of row names when you ask for the values you're interested in,
> > but if anyone has any ideas on how to get it to work, that would be
> > stellar.  Here's some example code:
> >
> > y1<-rnorm(40, 2)
> > x1<-rep(1:2, each=20)
> > x2<-rep(1:2, each=10 times=2)
> >
> > ex.dat<-data.frame(cbind(y1,x1,x2))
> >
> > ex.dat$x1<-factor(ex.dat$x1, labels=c("A", "B"))
> > ex.dat$x2<-factor(ex.dat$x2, labels=c("C", "D"))
> >
> > attach(ex.dat)
> >
> > xbar<-tapply(ex.dat$y1, ex.dat[,-1], mean)
> > xbar
> >
> > #values I'd like to plot:
> > row.names(xbar) #levels of x1
> > xbar[,1] #mean response of y1 for group C (x2) across x1
> >
> > #plot mean response y1 for group C against x1 (i.e., using x2 as a 
> grouping
> > factor).
> > plot(row.names(xbar), xbar[,1], ylim=range[y1])
> >
> > #This is where things break down. The error message states that I need
> > "finite xlim values" but I haven't assigned anything to xlim. If I just
> > plot the data:
> >
> > plot(x1, y1)
> >
> > #This works fine.
> >
> > #And, I can get this to work:
> >
> > stripchart(xbar[1,]~row.names(xbar), vert=T)
> >
> > #However, I'd like to then add the values for the second set of means
> > (i.e., mean values for group D against x1, or (xbar[,2])) to the plot.
> > #I tried following up the previous command with:
> >
> > points(row.names(xbar), xbar[,2])
> >
> > #But that returns an error as well (NAs introduced by coercion).
> >
> >
> >
> > Any suggestions?
> >
> > Cheers,
> >
> > Mike
> >
> > PS- some of you might suggest for me to use interaction.plot, since 
> this is
> > essentially what I'm building here. True, but I can't get error bars using
> > interaction.plot. I'm therefore trying to build my own version where I can
> > specify the inclusion of error bars. Presumably the interaction.plot has
> > figured out how to do what I'm attempting, so I have some faith that I am
> > on the right track....
>
>Michael,
>
>The problem is that when you are using the rownames for 'xbar', they are
>a character vector:
>
> > str(rownames(xbar))
>  chr [1:2] "A" "B
>
>When you attempt to use the same values from 'ex.dat$x1', they are
>factors, which are being coerced to their numeric equivalents, so that
>they can work as x axis values.
>
> > str(ex.dat$x1)
>  Factor w/ 2 levels "A","B": 1 1 1 1 1 1 1 1 1 1 ...
>
> > as.numeric(ex.dat$x1)
>  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2
>[35] 2 2 2 2 2 2
>
>
>
>I might note using the following:
>
>par(mfcol = c(1, 3))
>
>with(ex.dat, plot(1:2, xbar[, 1], ylim = range(y1),
>                   type = "b", col = "red",
>                   lty = c("dashed", "solid"),
>                   xaxt = "n", xlab = "x1",
>                   ylab = "mean of y1"))
>
>with(ex.dat, points(1:2, xbar[, 2], col = "blue",
>                     type = "b"))
>
>axis(1, at = 1:2, labels = c("A", "B"))
>
>
>matplot(1:2, xbar, col = c("red", "blue"),
>         pch = 21, type = "b", ylim = range(y1),
>         lty = c("dashed", "solid"),
>         xaxt = "n", xlab = "x1",
>         ylab = "mean of y1")
>
>axis(1, at = 1:2, labels = c("A", "B"))
>
>
>with(ex.dat, interaction.plot(x1, x2, response = y1,
>                               type = "b", pch = 21,
>                               col = c("red", "blue"),
>                               ylim = range(ex.dat$y1),
>                               legend = FALSE))
>
>
>
>You get basically the same 3 plots, with the principal difference in
>interaction.plot() being a different x axis range.
>
>Using interaction.plot(), you can get the proper basic plot and then add
>the CI's if you wish, since you know the x axis values of the mean
>estimates, which is 1:NumberOfGroups (as in a boxplot).
>
>interaction.plot() actually uses matplot() internally.
>
>HTH,
>
>Marc Schwartz

Michael Rennie
Ph.D. Candidate, University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga, ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792
www.utm.utoronto.ca/~w3rennie


From jhallman at frb.gov  Mon Jan 29 18:29:02 2007
From: jhallman at frb.gov (Jeffrey J. Hallman)
Date: 29 Jan 2007 12:29:02 -0500
Subject: [R] how to create daily / weekly ts object?
References: <1115a2b00701260755r63c4d49cuee4e704f85f7f136@mail.gmail.com>
Message-ID: <xmrodohn3mp.fsf@mralx1.rsma.frb.gov>

Look at the 'fame' package I recently put up. You don't need to have the FAME
database installed to use it.  Among other things, the package defines a class
tis (Time Indexed Series) that can handle weekly time series. 

"Wensui Liu" <liuwensui at gmail.com> writes:
> Monthly and Quarterly ts obj. is easy to understand. But I couldn't
> find an example in R manual how to create daily or weekly ts object.
> Could you please shed some light on it?
> I really appreciate it.

-- 
Jeff


From marc_schwartz at comcast.net  Mon Jan 29 18:42:26 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 29 Jan 2007 11:42:26 -0600
Subject: [R] plotting results from tapply;
	using indexing to	incorporate error bars
In-Reply-To: <6.1.0.6.0.20070129120156.01e16cb0@mail.utm.utoronto.ca>
References: <6.1.0.6.0.20070126113039.01c63c78@mail.utm.utoronto.ca>
	<1169833583.4899.33.camel@localhost.localdomain>
	<6.1.0.6.0.20070129120156.01e16cb0@mail.utm.utoronto.ca>
Message-ID: <1170092546.4841.7.camel@localhost.localdomain>

Mike,

Using interaction.plot():

with(ex.dat, interaction.plot(x1, x2, response = y1,
                               type = "b", pch = 21,
                               col = c("red", "blue"),
                               ylim = range(ex.dat$y1)))

arrows(1:2, xbar + sem, 1:2, xbar - sem, 
       col = rep(c("red", "blue"), each = 2),
       angle = 90, code = 3, length = .05)


Note that you need to draw two pairs of error bars (4). By specifying
only two colors in the call to arrows() as you do below, they will be
recycled as:

  red, blue, red, blue

This is defined in ?arrows:

   The graphical parameters 'col', 'lty' and 'lwd' can be vectors of
   length greater than one and will be recycled if necessary.

What you actually want is:

  red, red, blue, blue

which you get with:

> rep(c("red", "blue"), each = 2)
[1] "red"  "red"  "blue" "blue"


Try the above and it should work.

HTH,

Marc

On Mon, 2007-01-29 at 12:12 -0500, Michael Rennie wrote:
> Hi, Mark
> 
> Thanks for the examples- this is great, and has helped me understand alot 
> more what's going on in the plotting functions.
> 
> Now that I'm trying to work error bars into this, I was curious if someone 
> might give me a hand indexing this properly so that I can format my error 
> bars to match the formatting of the grouping variables that match the lines 
> in the plot.
> 
> Here's a re-worked example from my first submission where I calculate 
> standard errors for plotting on the figure, plot the error bars, and try to 
> index them to match the appropriate lines. The values appear to be in the 
> right place (I've turned on the "legend" option for the interaction plot to 
> help verify this), however, my attempts at matching the colours on the bars 
> to the colours on the lines fails miserably (as you'll see if you execute 
> the code below). Is there any way to assign my colours to match them in a 
> way that makes more sense?
> 
> Thanks for your help so far,
> 
> Mike
> 
> y1<-rnorm(40, 2)
> x1<-rep(1:2, each=20)
> x2<-rep(1:2, each=10, times=2)
> 
> ex.dat<-data.frame(cbind(y1,x1,x2))
> 
> ex.dat$x1<-factor(ex.dat$x1, labels=c("A", "B"))
> ex.dat$x2<-factor(ex.dat$x2, labels=c("C", "D"))
> 
> attach(ex.dat)
> 
> xbar<-tapply(ex.dat$y1, ex.dat[,-1], mean)
> xbar
> s <- tapply(ex.dat$y1, ex.dat[,-1], sd)
> n <- tapply(ex.dat$y1, ex.dat[,-1], length)
> sem <- s/sqrt(n)
> sem
> 
> row.names(xbar)
> xbar[,1]
> 
> 
> 
> #from Marc Schwartz#
> 
> #par(mfcol = c(1, 3))
> 
> 
> options(graphics.record = TRUE)
> 
> #using plot
> 
> with(ex.dat, plot(1:2, xbar[, 1], ylim = range(y1),
>                    type = "b", col = "red",
>                    lty = c("dashed", "solid"),
>                    xaxt = "n", xlab = "x1",
>                    ylab = "mean of y1"))
> 
> 
> with(ex.dat, points(1:2, xbar[, 2], col = "blue",
>                      type = "b"))
> 
> 
> axis(1, at = 1:2, labels = c("A", "B"))
> 
> 
> #using matplot
> 
> matplot(1:2, xbar, col = c("red", "blue"),
>          pch = 21, type = "b", ylim = range(y1),
>          lty = c("dashed", "solid"),
>          xaxt = "n", xlab = "x1",
>          ylab = "mean of y1")
> 
> 
> axis(1, at = 1:2, labels = c("A", "B"))
> arrows(1:2,xbar+sem, 1:2,xbar-sem, col = c("red", "blue"), angle=90, 
> code=3, length=.1)
> 
> #using interaction.plot
> 
> with(ex.dat, interaction.plot(x1, x2, response = y1,
>                                type = "b", pch = 21,
>                                col = c("red", "blue"),
>                                ylim = range(ex.dat$y1)))
> 
> arrows(1:2,xbar+sem, 1:2,xbar-sem, col = c("red", "blue"), angle=90, 
> code=3, length=.05)
> 
> #as you can see, though the values for standard errors match the 
> appropriate means, the assignment of colours does not.
> 
> 
> 
> At 12:46 PM 26/01/2007, Marc Schwartz wrote:
> >On Fri, 2007-01-26 at 11:50 -0500, Michael Rennie wrote:
> > > Hi, there
> > >
> > > I'm trying to plot what is returned from a call to tapply, and can't 
> > figure
> > > out how to do it. My guess is that it has something to do with the
> > > inclusion of row names when you ask for the values you're interested in,
> > > but if anyone has any ideas on how to get it to work, that would be
> > > stellar.  Here's some example code:
> > >
> > > y1<-rnorm(40, 2)
> > > x1<-rep(1:2, each=20)
> > > x2<-rep(1:2, each=10 times=2)
> > >
> > > ex.dat<-data.frame(cbind(y1,x1,x2))
> > >
> > > ex.dat$x1<-factor(ex.dat$x1, labels=c("A", "B"))
> > > ex.dat$x2<-factor(ex.dat$x2, labels=c("C", "D"))
> > >
> > > attach(ex.dat)
> > >
> > > xbar<-tapply(ex.dat$y1, ex.dat[,-1], mean)
> > > xbar
> > >
> > > #values I'd like to plot:
> > > row.names(xbar) #levels of x1
> > > xbar[,1] #mean response of y1 for group C (x2) across x1
> > >
> > > #plot mean response y1 for group C against x1 (i.e., using x2 as a 
> > grouping
> > > factor).
> > > plot(row.names(xbar), xbar[,1], ylim=range[y1])
> > >
> > > #This is where things break down. The error message states that I need
> > > "finite xlim values" but I haven't assigned anything to xlim. If I just
> > > plot the data:
> > >
> > > plot(x1, y1)
> > >
> > > #This works fine.
> > >
> > > #And, I can get this to work:
> > >
> > > stripchart(xbar[1,]~row.names(xbar), vert=T)
> > >
> > > #However, I'd like to then add the values for the second set of means
> > > (i.e., mean values for group D against x1, or (xbar[,2])) to the plot.
> > > #I tried following up the previous command with:
> > >
> > > points(row.names(xbar), xbar[,2])
> > >
> > > #But that returns an error as well (NAs introduced by coercion).
> > >
> > >
> > >
> > > Any suggestions?
> > >
> > > Cheers,
> > >
> > > Mike
> > >
> > > PS- some of you might suggest for me to use interaction.plot, since 
> > this is
> > > essentially what I'm building here. True, but I can't get error bars using
> > > interaction.plot. I'm therefore trying to build my own version where I can
> > > specify the inclusion of error bars. Presumably the interaction.plot has
> > > figured out how to do what I'm attempting, so I have some faith that I am
> > > on the right track....
> >
> >Michael,
> >
> >The problem is that when you are using the rownames for 'xbar', they are
> >a character vector:
> >
> > > str(rownames(xbar))
> >  chr [1:2] "A" "B
> >
> >When you attempt to use the same values from 'ex.dat$x1', they are
> >factors, which are being coerced to their numeric equivalents, so that
> >they can work as x axis values.
> >
> > > str(ex.dat$x1)
> >  Factor w/ 2 levels "A","B": 1 1 1 1 1 1 1 1 1 1 ...
> >
> > > as.numeric(ex.dat$x1)
> >  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2
> >[35] 2 2 2 2 2 2
> >
> >
> >
> >I might note using the following:
> >
> >par(mfcol = c(1, 3))
> >
> >with(ex.dat, plot(1:2, xbar[, 1], ylim = range(y1),
> >                   type = "b", col = "red",
> >                   lty = c("dashed", "solid"),
> >                   xaxt = "n", xlab = "x1",
> >                   ylab = "mean of y1"))
> >
> >with(ex.dat, points(1:2, xbar[, 2], col = "blue",
> >                     type = "b"))
> >
> >axis(1, at = 1:2, labels = c("A", "B"))
> >
> >
> >matplot(1:2, xbar, col = c("red", "blue"),
> >         pch = 21, type = "b", ylim = range(y1),
> >         lty = c("dashed", "solid"),
> >         xaxt = "n", xlab = "x1",
> >         ylab = "mean of y1")
> >
> >axis(1, at = 1:2, labels = c("A", "B"))
> >
> >
> >with(ex.dat, interaction.plot(x1, x2, response = y1,
> >                               type = "b", pch = 21,
> >                               col = c("red", "blue"),
> >                               ylim = range(ex.dat$y1),
> >                               legend = FALSE))
> >
> >
> >
> >You get basically the same 3 plots, with the principal difference in
> >interaction.plot() being a different x axis range.
> >
> >Using interaction.plot(), you can get the proper basic plot and then add
> >the CI's if you wish, since you know the x axis values of the mean
> >estimates, which is 1:NumberOfGroups (as in a boxplot).
> >
> >interaction.plot() actually uses matplot() internally.
> >
> >HTH,
> >
> >Marc Schwartz
> 
> Michael Rennie
> Ph.D. Candidate, University of Toronto at Mississauga
> 3359 Mississauga Rd. N.
> Mississauga, ON  L5L 1C6
> Ph: 905-828-5452  Fax: 905-828-3792
> www.utm.utoronto.ca/~w3rennie


From jlampur at eagrof.UdL.es  Mon Jan 29 18:58:36 2007
From: jlampur at eagrof.UdL.es (Jorge Lampurlanes Castel)
Date: Mon, 29 Jan 2007 18:58:36 +0100 (CET)
Subject: [R]  Multiple comparisons when interacction
Message-ID: <4082.217.124.30.94.1170093516.squirrel@correu.udl.es>

In the model:

  lm.1 <- lm(variable ~ BLOC + TIL * YEAR , data=selvanera)


I found TIL*YEAR interaction significant. Then I am trying to compare
means of the different levels of TIL inside every YEAR using:

  mc.2 <- glht(lm.1, linfct = mcp(TIL*YEAR="Tukey"))
  summary(mc.2, test = univariate())

but it does not work.

There is any way of doing this, like the SLICE option in PROC GLM (SAS)?

Thanks a lot,

Jorge



-- 
**************************************************
Jorge Lampurlan?s Castel
Departament d'Enginyeria Agroforestal
Escola T?cnica Superior d'Enginyeria Agr?ria
Universitat de Lleida
Avinguda Rovira Roure, 191
25198-LLEIDA
SPAIN

Tl.: +34 973 70 25 37
Fax.:+34 073 70 26 73
e-mail: jlampur at eagrof.udl.es


From gunter.berton at gene.com  Mon Jan 29 18:03:44 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Mon, 29 Jan 2007 09:03:44 -0800
Subject: [R] Loop with string variable AND customizable "summary" output
In-Reply-To: <8760D6386F7F784693F05256139A1FFB70F70F@EXCHF3.lse.ac.uk>
Message-ID: <004001c743c7$6ff4b710$4d908980@gne.windows.gene.com>

Prior answers are certainly correct, but this is where lists and lapply
shine:

result<-lapply(list(UK,USA),function(z)summary(lm(y~x,data=z)))

As in (nearly) all else, simplicity is a virtue.

If you prefer to keep the data sources as a character vector,dataNames,

result<-lapply(dataNames,function(z)summary(lm(y~x,data=get(z)))) 

should work. 

Note: both of these are untested for the general case where they might be
used within a function and may not find the right z unless you pay attention
to scope, especially in the get() construction.


Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of C.Rosa at lse.ac.uk
Sent: Monday, January 29, 2007 8:23 AM
To: liuwensui at gmail.com; bcarvalh at jhsph.edu; Roger.Bivand at nhh.no
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Loop with string variable AND customizable "summary" output

Dear All,
Thank you very much for your help!
Carlo

-----Original Message-----
From: Wensui Liu [mailto:liuwensui at gmail.com]
Sent: Mon 29/01/2007 15:39
To: Rosa,C
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Loop with string variable AND customizable "summary" output
 
Carlo,

try something like:

for (i in c("UK","USA"))
{
summ<-summary(lm(y ~ x), subset = (country = i))
assign(paste('output', i, sep = ''), summ);
}

(note: it is untested, sorry).

On 1/29/07, C.Rosa at lse.ac.uk <C.Rosa at lse.ac.uk> wrote:
> Dear All,
>
> I am using R for my research and I have two questions about it:
>
> 1) is it possible to create a loop using a string, instead of a numeric
vector? I have in mind a specific problem:
>
> Suppose you have 2 countries: UK, and USA, one dependent (y) and one
independent variable (y) for each country (vale a dire: yUK, xUK, yUSA,
xUSA) and you want to run automatically the following regressions:
>
>
>
> for (i in c("UK","USA"))
>
> output{i}<-summary(lm(y{i} ~ x{i}))
>
>
>
> In other words, at the end I would like to have two objects as output:
"outputUK" and "outputUSA", which contain respectively the results of the
first and second regression (yUK on xUK and yUSA on xUSA).
>
>
>
> 2) in STATA there is a very nice code ("outreg") to display nicely (and as
the user wants to) your regression results.
>
> Is there anything similar in R / R contributed packages? More precisely, I
am thinking of something that is close in spirit to "summary" but it is also
customizable. For example, suppose you want different Signif. codes:  0
'***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 or a different format display
(i.e. without "t value" column) implemented automatically (without manually
editing it every time).
>
> In alternative, if I was able to see it, I could modify the source code of
the function "summary", but I am not able to see its (line by line) code.
Any idea?
>
> Or may be a customizable regression output already exists?
>
> Thanks really a lot!
>
> Carlo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From charles.dupont at vanderbilt.edu  Mon Jan 29 18:11:19 2007
From: charles.dupont at vanderbilt.edu (Charles Dupont)
Date: Mon, 29 Jan 2007 11:11:19 -0600
Subject: [R] [R-pkgs] Hmisc Version 3.1-2 uploaded to CRAN repository
Message-ID: <45BE2AB7.8030107@vanderbilt.edu>

Hmisc 3.1-2 has been uploaded to the CRAN incoming directory.

Change log
3.2-1 1/25/2007:
       Hmisc function 'ecdf' has been renamed 'Ecdf' to deconflict it
       with the existing 'ecdf' function in base.

       Fixed Bug in format.df that would create numbers with many
       trailing zeros.

       Added arguments 'math.row.names' and 'math.col.names' to
       indicate that the row or col names should be wrapped in the
       latex math environment.

       Fixed problem with 'histbackback' function.

-- 
Charles Dupont	Computer System Analyst		School of Medicine
		Department of Biostatistics	Vanderbilt University

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From marc_schwartz at comcast.net  Mon Jan 29 19:24:08 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 29 Jan 2007 12:24:08 -0600
Subject: [R] Loop with string variable AND customizable "summary" output
In-Reply-To: <004001c743c7$6ff4b710$4d908980@gne.windows.gene.com>
References: <004001c743c7$6ff4b710$4d908980@gne.windows.gene.com>
Message-ID: <1170095048.4841.15.camel@localhost.localdomain>

Or, to throw yet another couple of possibilities into the mix:

lapply(split(YourDF, YourDF$country), 
       function(x) summary(lm(y ~ x, data = x))

and:

library(nlme)
summary(lmList(y ~ x | country, YourDF))


See ?split and help(lmList, package = nlme)

HTH,

Marc Schwartz

On Mon, 2007-01-29 at 09:03 -0800, Bert Gunter wrote:
> Prior answers are certainly correct, but this is where lists and lapply
> shine:
> 
> result<-lapply(list(UK,USA),function(z)summary(lm(y~x,data=z)))
> 
> As in (nearly) all else, simplicity is a virtue.
> 
> If you prefer to keep the data sources as a character vector,dataNames,
> 
> result<-lapply(dataNames,function(z)summary(lm(y~x,data=get(z)))) 
> 
> should work. 
> 
> Note: both of these are untested for the general case where they might be
> used within a function and may not find the right z unless you pay attention
> to scope, especially in the get() construction.
> 
> 
> Bert Gunter
> Genentech Nonclinical Statistics
> South San Francisco, CA 94404
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of C.Rosa at lse.ac.uk
> Sent: Monday, January 29, 2007 8:23 AM
> To: liuwensui at gmail.com; bcarvalh at jhsph.edu; Roger.Bivand at nhh.no
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Loop with string variable AND customizable "summary" output
> 
> Dear All,
> Thank you very much for your help!
> Carlo
> 
> -----Original Message-----
> From: Wensui Liu [mailto:liuwensui at gmail.com]
> Sent: Mon 29/01/2007 15:39
> To: Rosa,C
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Loop with string variable AND customizable "summary" output
>  
> Carlo,
> 
> try something like:
> 
> for (i in c("UK","USA"))
> {
> summ<-summary(lm(y ~ x), subset = (country = i))
> assign(paste('output', i, sep = ''), summ);
> }
> 
> (note: it is untested, sorry).
> 
> On 1/29/07, C.Rosa at lse.ac.uk <C.Rosa at lse.ac.uk> wrote:
> > Dear All,
> >
> > I am using R for my research and I have two questions about it:
> >
> > 1) is it possible to create a loop using a string, instead of a numeric
> vector? I have in mind a specific problem:
> >
> > Suppose you have 2 countries: UK, and USA, one dependent (y) and one
> independent variable (y) for each country (vale a dire: yUK, xUK, yUSA,
> xUSA) and you want to run automatically the following regressions:
> >
> >
> >
> > for (i in c("UK","USA"))
> >
> > output{i}<-summary(lm(y{i} ~ x{i}))
> >
> >
> >
> > In other words, at the end I would like to have two objects as output:
> "outputUK" and "outputUSA", which contain respectively the results of the
> first and second regression (yUK on xUK and yUSA on xUSA).
> >
> >
> >
> > 2) in STATA there is a very nice code ("outreg") to display nicely (and as
> the user wants to) your regression results.
> >
> > Is there anything similar in R / R contributed packages? More precisely, I
> am thinking of something that is close in spirit to "summary" but it is also
> customizable. For example, suppose you want different Signif. codes:  0
> '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 or a different format display
> (i.e. without "t value" column) implemented automatically (without manually
> editing it every time).
> >
> > In alternative, if I was able to see it, I could modify the source code of
> the function "summary", but I am not able to see its (line by line) code.
> Any idea?
> >
> > Or may be a customizable regression output already exists?
> >
> > Thanks really a lot!
> >
> > Carlo
> >


From octou at ipea.gov.br  Mon Jan 29 19:30:37 2007
From: octou at ipea.gov.br (Octavio Tourinho)
Date: Mon, 29 Jan 2007 16:30:37 -0200
Subject: [R] Problem with "readline" in compilatio of R for Solaris 11
 (Nevada) in x86
Message-ID: <45BE3D4D.3060408@ipea.gov.br>

Dear friends,
In configuring R 2.4.1 for Solaris 11, using SunStudio 11 compilers, I 
get the following error. 

checking readline/history.h usability... no
checking readline/history.h presence... no
checking for readline/history.h... no
checking readline/readline.h usability... no
checking readline/readline.h presence... no
checking for readline/readline.h... no
checking for rl_callback_read_char in -lreadline... no
checking for main in -lncurses... no
checking for main in -ltermcap... yes
checking for rl_callback_read_char in -lreadline... no
checking for history_truncate_file... no
configure: error: --with-readline=yes (default) and headers/libs are not 
available

I was not able to figure out what readline is about, whether it is 
optional or not (at some point of the script it seems to be a flag which 
can be true or false) and was therefore unable to debug it.

Thanks for any help you can provide

Octavio Tourinho
octavio.tourinho at terra.com.br
octou at ipea.gov.br


From r.nieuwenhuis at student.ru.nl  Mon Jan 29 18:59:50 2007
From: r.nieuwenhuis at student.ru.nl (Rense Nieuwenhuis)
Date: Mon, 29 Jan 2007 18:59:50 +0100
Subject: [R] Cronbach's alpha
In-Reply-To: <cdf817830701241344h381f4a80w9be5f07e1332d8d1@mail.gmail.com>
References: <2323A6D37908A847A7C32F1E3662C80E8D4573@dc1ex01.air.org>
	<cdf817830701241344h381f4a80w9be5f07e1332d8d1@mail.gmail.com>
Message-ID: <0A9C863C-6CAA-4558-8E8E-CBC97D0DAC0C@student.ru.nl>

Dear all,

on a practical level an alpha < 0 can be found, when a scale is  
constructed / evaluated consisting only a few items (say 5) and one  
of the items is coded in the wrong direction (values that should  
represent a high score wrongfully represent a low score).

Rense


On Jan 24, 2007, at 22:44 , Weiwei Shi wrote:

> Hi, there:
>
> I read that article (thanks Chucks, etc to point that out). Now I
> understand how those negatives are generated since my research subject
> "should" have negative convariance but they "are" measuring the same
> thing. So, I am confused about this "same" thing and about if it is
> proper to go ahead to use this measurement.
>
> To clear my point , I describe my idea here a little bit. My idea is
> to look for a way to assign a "statistic" or measurement to a set of
> variables to see if they "act" cohesively or coherently for an event.
> Instead of using simple correlation, which describes var/var
> correlation; I wanted to get a "total correlation" so that I can
> compare between setS of variables. Initially I "made" that word but
> google helps me find that statistic exists! So I read into it and post
> my original post on "total correlation". (Ben, you can find total
> correlation from wiki).
>
> I was suggested to use this alpha since it measures a "one latent
> construct", in which matches my idea about one event. I have a feeling
> it is like factor analysis; however, the grouping of variables has
> been fixed by domain knowledge.
>
> Sorry if it is off-list topic but I feel it is very interesting to  
> go ahead.
>
> Thanks,
>
> Weiwei
>
>
>
> On 1/24/07, Doran, Harold <HDoran at air.org> wrote:
>> Hi Dave
>>
>> We had a bit of an off list discussion on this. You're correct, it  
>> can
>> be negative IF the covariance among individual items is negative  
>> AND if
>> that covariance term is larger than the sum of the individual item
>> variances. Both of these conditions would be needed to make alpha go
>> negative.
>>
>> Psychometrically speaking, this introduces some question as to  
>> whether
>> the items are measuring the same latent trait. That is, if there is a
>> negative covariance among items, but those items are thought to  
>> measure
>> a common trait, then (I'm scratching my head) I think we have a
>> dimensionality issue.
>>
>>
>>
>>> -----Original Message-----
>>> From: r-help-bounces at stat.math.ethz.ch
>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dave Atkins
>>> Sent: Wednesday, January 24, 2007 4:08 PM
>>> To: R-help at stat.math.ethz.ch
>>> Subject: Re: [R] Cronbach's alpha
>>>
>>>
>>> Harold & Weiwei--
>>>
>>> Actually, alpha *can* go negative, which means that items are
>>> reliably different as opposed to reliably similar.  This
>>> happens when the sum of the covariances among items is
>>> negative.  See the ATS site below for a more thorough explanation:
>>>
>>> http://www.ats.ucla.edu/STAT/SPSS/library/negalpha.htm
>>>
>>> Hope that helps.
>>>
>>> cheers, Dave
>>> --
>>> Dave Atkins, PhD
>>> Assistant Professor in Clinical Psychology Fuller Graduate
>>> School of Psychology
>>> Email: datkins at fuller.edu
>>> Phone: 626.584.5554
>>>
>>>
>>> Weiwei
>>>
>>> Something is wrong. Coefficient alpha is bounded between 0 and 1, so
>>> negative values are outside the parameter space for a reliability
>>> statistic. Recall that reliability is the ratio of "true
>>> score" variance
>>> to "total score variance". That is
>>>
>>> var(t)/ var(t) + var(e)
>>>
>>> If all variance is true score variance, then var(e)=0 and the
>>> reliability is var(t)/var(t)=1. On the other hand, if all  
>>> variance is
>>> measurement error, then var(t) = 0 and reliability is 0.
>>>
>>> Here is a function I wrote to compute alpha along with an
>>> example. Maybe
>>> try recomputing your statistic using this function and see if you  
>>> get
>>> the same result.
>>>
>>> alpha <- function(columns){
>>>       k <- ncol(columns)
>>>       colVars <- apply(columns, 2, var)
>>>       total   <- var(apply(columns, 1, sum))
>>>       a <- (total - sum(colVars)) / total * (k/(k-1))
>>>       a
>>>       }
>>>
>>> data(LSAT, package='ltm')
>>>> alpha(LSAT)
>>> [1] 0.2949972
>>>
>>>
>>> Harold
>>>
>>>> -----Original Message-----
>>>> From: r-help-bounces at stat.math.ethz.ch
>>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
>>> Weiwei Shi
>>>> Sent: Wednesday, January 24, 2007 1:17 PM
>>>> To: R R
>>>> Subject: [R] Cronbach's alpha
>>>>
>>>> Dear Listers:
>>>>
>>>> I used cronbach{psy} to evaluate the internal consistency and
>>>> some set of variables gave me alpha=-1.1003, while other,
>>>> alpha=-0.2; alpha=0.89; and so on. I am interested in knowing
>>>> how to interpret 1. negative value 2. negative value less than -1.
>>>>
>>>> I also want to re-mention my previous question about how to
>>>> evaluate the consistency of a set of variables and about the
>>>> total correlation (my 2 cent to answer the question). Is
>>>> there any function in R to do that?
>>>>
>>>> Thank you very much!
>>>>
>>>>
>>>>
>>>> --
>>>> Weiwei Shi, Ph.D
>>>> Research Scientist
>>>> GeneGO, Inc.
>>>>
>>>> "Did you always know?"
>>>> "No, I did not. But I believed..."
>>>> ---Matrix III
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>> --
>>> Dave Atkins, PhD
>>> Assistant Professor in Clinical Psychology
>>> Fuller Graduate School of Psychology
>>> Email: datkins at fuller.edu
>>> Phone: 626.584.5554
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
> -- 
> Weiwei Shi, Ph.D
> Research Scientist
> GeneGO, Inc.
>
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Mon Jan 29 20:30:07 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 29 Jan 2007 14:30:07 -0500
Subject: [R] Loop with string variable AND customizable "summary" output
In-Reply-To: <1170095048.4841.15.camel@localhost.localdomain>
References: <004001c743c7$6ff4b710$4d908980@gne.windows.gene.com>
	<1170095048.4841.15.camel@localhost.localdomain>
Message-ID: <971536df0701291130m2ab0cfe4n3de73e9d22ab4931@mail.gmail.com>

Note that the nlme solution seems to give the same coefficients
but appears to use a single error term rather than one error
term per level of the conditioning variable and that would change various
other statistics relative to the other solutions should that matter.

> summary(lmList(uptake ~ conc | Treatment, CO2))
Call:
  Model: uptake ~ conc | Treatment
   Data: CO2

Coefficients:
   (Intercept)
           Estimate Std. Error  t value     Pr(>|t|)
nonchilled 22.01916    2.46416 8.935769 1.174616e-13
chilled    16.98142    2.46416 6.891361 1.146556e-09
   conc
             Estimate  Std. Error  t value     Pr(>|t|)
nonchilled 0.01982458 0.004692544 4.224699 6.292679e-05
chilled    0.01563659 0.004692544 3.332221 1.306259e-03

Residual standard error: 8.945667 on 80 degrees of freedom


On 1/29/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> Or, to throw yet another couple of possibilities into the mix:
>
> lapply(split(YourDF, YourDF$country),
>       function(x) summary(lm(y ~ x, data = x))
>
> and:
>
> library(nlme)
> summary(lmList(y ~ x | country, YourDF))
>
>
> See ?split and help(lmList, package = nlme)
>
> HTH,
>
> Marc Schwartz
>
> On Mon, 2007-01-29 at 09:03 -0800, Bert Gunter wrote:
> > Prior answers are certainly correct, but this is where lists and lapply
> > shine:
> >
> > result<-lapply(list(UK,USA),function(z)summary(lm(y~x,data=z)))
> >
> > As in (nearly) all else, simplicity is a virtue.
> >
> > If you prefer to keep the data sources as a character vector,dataNames,
> >
> > result<-lapply(dataNames,function(z)summary(lm(y~x,data=get(z))))
> >
> > should work.
> >
> > Note: both of these are untested for the general case where they might be
> > used within a function and may not find the right z unless you pay attention
> > to scope, especially in the get() construction.
> >
> >
> > Bert Gunter
> > Genentech Nonclinical Statistics
> > South San Francisco, CA 94404
> >
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of C.Rosa at lse.ac.uk
> > Sent: Monday, January 29, 2007 8:23 AM
> > To: liuwensui at gmail.com; bcarvalh at jhsph.edu; Roger.Bivand at nhh.no
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Loop with string variable AND customizable "summary" output
> >
> > Dear All,
> > Thank you very much for your help!
> > Carlo
> >
> > -----Original Message-----
> > From: Wensui Liu [mailto:liuwensui at gmail.com]
> > Sent: Mon 29/01/2007 15:39
> > To: Rosa,C
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Loop with string variable AND customizable "summary" output
> >
> > Carlo,
> >
> > try something like:
> >
> > for (i in c("UK","USA"))
> > {
> > summ<-summary(lm(y ~ x), subset = (country = i))
> > assign(paste('output', i, sep = ''), summ);
> > }
> >
> > (note: it is untested, sorry).
> >
> > On 1/29/07, C.Rosa at lse.ac.uk <C.Rosa at lse.ac.uk> wrote:
> > > Dear All,
> > >
> > > I am using R for my research and I have two questions about it:
> > >
> > > 1) is it possible to create a loop using a string, instead of a numeric
> > vector? I have in mind a specific problem:
> > >
> > > Suppose you have 2 countries: UK, and USA, one dependent (y) and one
> > independent variable (y) for each country (vale a dire: yUK, xUK, yUSA,
> > xUSA) and you want to run automatically the following regressions:
> > >
> > >
> > >
> > > for (i in c("UK","USA"))
> > >
> > > output{i}<-summary(lm(y{i} ~ x{i}))
> > >
> > >
> > >
> > > In other words, at the end I would like to have two objects as output:
> > "outputUK" and "outputUSA", which contain respectively the results of the
> > first and second regression (yUK on xUK and yUSA on xUSA).
> > >
> > >
> > >
> > > 2) in STATA there is a very nice code ("outreg") to display nicely (and as
> > the user wants to) your regression results.
> > >
> > > Is there anything similar in R / R contributed packages? More precisely, I
> > am thinking of something that is close in spirit to "summary" but it is also
> > customizable. For example, suppose you want different Signif. codes:  0
> > '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 or a different format display
> > (i.e. without "t value" column) implemented automatically (without manually
> > editing it every time).
> > >
> > > In alternative, if I was able to see it, I could modify the source code of
> > the function "summary", but I am not able to see its (line by line) code.
> > Any idea?
> > >
> > > Or may be a customizable regression output already exists?
> > >
> > > Thanks really a lot!
> > >
> > > Carlo
> > >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Kathy-Andree.Laplante-Albert at UQTR.CA  Mon Jan 29 20:34:21 2007
From: Kathy-Andree.Laplante-Albert at UQTR.CA (=?ISO-8859-1?B?S2F0aHktQW5kcullIA==?=Laplante-Albert)
Date: Mon, 29 Jan 2007 14:34:21 -0500
Subject: [R] overlay xyplot on contourplot in lattice in R
Message-ID: <1170099261.45be4c3ded32b@courriel.uqtr.ca>



Hi everybody, 

I want to do a contourplot in lattice with my raw data overlaid on it(xyplot)
which seemed to be a very easy thing to do.

I've tried it in many ways, but I haven't succeeded at obtaining it. 

let's say x, y, and z as the variables that comes from the data frame ''ex'':

therefore my countourplot is as followed :

contourplot(z ~ x * y, cuts=550)

I would like then to overalay on it xx and yy (same variables as x and y but in
another data frame let's say ''example'')

xyplot(example$xx, example$yy)

How can I do it?

It seems I could do it with the panel.contourplot and panel.xyplot options, but
until now I haven't suceeded. I found this command that maybe could help me : 


 # contourplot
contourplot(elev ~ longitude * latitude, data = interpgrid,
   panel = function(x, y, subscripts, ...) {
      panel.contourplot(x, y, subscripts, ...)
      panel.xyplot(ortann$longitude, ortann$latitude)
   } )

however, doesn't seem to work out, and I do not know what should I write for
subscripts. I do not understand the explanation in R.

Is anyone could help me? 

Thanks a lot in advance!

I use R, version 2.4.0.

Kathy-Andr?e Laplante-Albert
?tudiante ? la ma?trise
Groupe de Recherche sur les ?cosyst?mes Aquatiques
Universit? du Qu?bec ? Trois-Rivi?res
C.P. 500, Trois-Rivi?res (Qu?bec) Canada 

-------------------------------------------------
Courriel exp?di? via https://courriel.uqtr.ca


From dchan at GFC.STATE.GA.US  Mon Jan 29 20:42:45 2007
From: dchan at GFC.STATE.GA.US (Dan Chan)
Date: Mon, 29 Jan 2007 14:42:45 -0500
Subject: [R] Dynamic Variable
Message-ID: <43679C69FEEE9C40AC8876C0FF38EF100624E9D5@mailsvr01.gfc.state.ga.us>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070129/870b0e18/attachment.pl 

From hpages at fhcrc.org  Mon Jan 29 20:47:01 2007
From: hpages at fhcrc.org (Herve Pages)
Date: Mon, 29 Jan 2007 11:47:01 -0800
Subject: [R] Biostrings
In-Reply-To: <loom.20070128T060501-922@post.gmane.org>
References: <loom.20070128T060501-922@post.gmane.org>
Message-ID: <45BE4F35.8050602@fhcrc.org>

qing wrote:
> 
> 
> Dear All?
> 
> I am a beginner in learning the package of Biostrings currently and for 
> practice doing an example.
> 
>  library(Biostrings);
>  dnaAlph<-new("BioPatternAlphabet",DNAAlphabet(),c(N="AGCT",
> + B="CGT",D="AGT",H="ACT",K="GT",M="AC",R="AG",S="CG",
> + V="ACG",W="AT",Y="CT"));
> Error in getClass(Class, where = topenv(parent.frame())) : 
>         "BioPatternAlphabet" is not a defined class


Hi Qing,


"BioPatternAlphabet" was a class defined in Biostrings 1 (Biostrings
version 1.y.z). In Biostrings 2, the class system has changed and you don't
need to create an instance of the DNA alphabet anymore:

  - To create a DNAString object, just do:

      > mydna <- DNAString("AGG-HCNTT")
      > mydna
        9-letter "DNAString" object
      Value: AGG-HCNTT

  - To get the DNA alphabet, call alphabet() on a DNAString object:

      > alphabet(mydna)
       [1] "A" "C" "G" "T" "M" "R" "S" "V" "W" "Y" "H" "K" "D" "B" "N" "-"

    or, if you don't have a DNAString instance yet:

      > DNA_ALPHABET
       [1] "A" "C" "G" "T" "M" "R" "S" "V" "W" "Y" "H" "K" "D" "B" "N" "-"

    Note that the "DNA alphabet" is the "IUPAC Extended Genetic Alphabet".

  - To get the mapping between the DNA alphabet and the set of ambiguities
    associated to each "extended" letter:

      > IUPAC_CODE_MAP
           A      C      G      T      M      R      W      S      Y      K      V
         "A"    "C"    "G"    "T"   "AC"   "AG"   "AT"   "CG"   "CT"   "GT"  "ACG"
           H      D      B      N
       "ACT"  "AGT"  "CGT" "ACGT"

  - For more details, see:

      > ?DNAString

Cheers,

H.

PS: As Ducan said, the Bioconductor mailing list is a more appropriate place
to ask help about a Bioconductor package.



> 
> I am running R 2.4.1,  Wimdows XP, Biostrings_2.2.1
> Any help or suggestions that you can provide will be greatly appreciated.
> 
> Qing
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jkopecky at umich.edu  Mon Jan 29 20:52:24 2007
From: jkopecky at umich.edu (Jonathon Kopecky)
Date: Mon, 29 Jan 2007 14:52:24 -0500
Subject: [R] Need to fit a regression line using orthogonal residuals
Message-ID: <45BE5078.3070302@umich.edu>

I'm trying to fit a simple linear regression of just Y ~ X, but both X 
and Y are noisy.  Thus instead of fitting a standard linear model 
minimizing vertical residuals, I would like to minimize 
orthogonal/perpendicular residuals.  I have tried searching the 
R-packages, but have not found anything that seems suitable.  I'm not 
sure what these types of residuals are typically called (they seem to 
have many different names), so that may be my trouble.  I do not want to 
use Principal Components Analysis (as was answered to a previous 
questioner a few years ago), I just want to minimize the combined noise 
of my two variables.  Is there a way for me to do this in R?

Jonathon Kopecky
University of Michigan


From deepayan.sarkar at gmail.com  Mon Jan 29 20:54:41 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 29 Jan 2007 11:54:41 -0800
Subject: [R] extra panel arguments to plot.nmGroupedData {nlme}
In-Reply-To: <3c5546140701281259x5020764at573b886ec29d8a07@mail.gmail.com>
References: <3c5546140701281259x5020764at573b886ec29d8a07@mail.gmail.com>
Message-ID: <eb555e660701291154x601b2d50k448be11ea0abc11b@mail.gmail.com>

On 1/28/07, Dylan Beaudette <dylan.beaudette at gmail.com> wrote:
> Greetings,
>
>
> I have a groupedData (nmGroupedData) object created with the following syntax:
>
> Soil <- groupedData(
>   ksat ~ conc | soil_id/sar/rep,
>   data=soil.data,
>   labels=list(x='Solution Concentration', y='Saturated Hydraulic Conductivity'),
>   units=list(x='(cmol_c)', y='(cm/s)')
> )
>
>
> the original data represents longitudinal observations in the form of:
> 'data.frame':   1197 obs. of  5 variables:
>  $ soil_id: Factor w/ 19 levels "Arbuckle","Campbell",..: 16 16 16 16
> 16 16 16 16 16 16 ...
>  $ sar    : int  12 12 12 12 12 12 12 6 6 6 ...
>  $ conc   : num  500 100 50 10 5 1 0.003 500 100 50 ...
>  $ rep    : Factor w/ 3 levels "C1","C2","C3": 1 1 1 1 1 1 1 1 1 1 ...
>  $ ksat   : num  0.000214 0.000207 0.000198 0.000160 0.000108 ...
>
> the default plotting behaviour of this groupedData object works as expected:
> plot(
>   Soil,
>   collapse=1, inner=~sar, aspect='fill',
>   scales=list( x=list(log=TRUE), y=list(log=TRUE))
> )
>
> ... however, there is no way for me to alter the panels...
>
> for example, attempting to add a horizontal line with a call to
> panel.abline= ... :
> plot(
>   Soil,
>   collapse=1, inner=~sar, aspect='fill',
>   scales=list( x=list(log=TRUE), y=list(log=TRUE)),
>   FUN=mean,
>   panel= function(x,y, subscripts, groups) {
>     panel.xyplot(x, y, type='o')
>     panel.abline(h=0.005, col='black', lty=2)
>     }
> )
>
> ... results in an identical plot. Trying to re-create the results of
> plot.nmGroupedData with a direct call to xyplot() has thus far been
> unsucsessful- as I cannot figure out how to specifiy the original
> formula in a meaningful way to xyplot: ksat ~ conc | soil_id/sar/rep .

You might try something like

ksat ~ conc | soil_id:sar:rep

(also see ?interaction for a version with more control)

-Deepayan


From deepayan.sarkar at gmail.com  Mon Jan 29 21:01:01 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 29 Jan 2007 12:01:01 -0800
Subject: [R] overlay xyplot on contourplot in lattice in R
In-Reply-To: <1170099261.45be4c3ded32b@courriel.uqtr.ca>
References: <1170099261.45be4c3ded32b@courriel.uqtr.ca>
Message-ID: <eb555e660701291201x1720a7bu64f2925dc7b01bb4@mail.gmail.com>

On 1/29/07, Kathy-Andr?e Laplante-Albert
<Kathy-Andree.Laplante-Albert at uqtr.ca> wrote:
>
>
> Hi everybody,
>
> I want to do a contourplot in lattice with my raw data overlaid on it(xyplot)
> which seemed to be a very easy thing to do.
>
> I've tried it in many ways, but I haven't succeeded at obtaining it.
>
> let's say x, y, and z as the variables that comes from the data frame ''ex'':
>
> therefore my countourplot is as followed :
>
> contourplot(z ~ x * y, cuts=550)
>
> I would like then to overalay on it xx and yy (same variables as x and y but in
> another data frame let's say ''example'')
>
> xyplot(example$xx, example$yy)
>
> How can I do it?
>
> It seems I could do it with the panel.contourplot and panel.xyplot options, but
> until now I haven't suceeded. I found this command that maybe could help me :
>
>
>  # contourplot
> contourplot(elev ~ longitude * latitude, data = interpgrid,
>    panel = function(x, y, subscripts, ...) {
>       panel.contourplot(x, y, subscripts, ...)
>       panel.xyplot(ortann$longitude, ortann$latitude)
>    } )

This looks like something that should work. Could you explain why it
"doesn't seem to work out"?

-Deepayan

> however, doesn't seem to work out, and I do not know what should I write for
> subscripts. I do not understand the explanation in R.
>
> Is anyone could help me?
>
> Thanks a lot in advance!
>
> I use R, version 2.4.0.
>
> Kathy-Andr?e Laplante-Albert
> ?tudiante ? la ma?trise
> Groupe de Recherche sur les ?cosyst?mes Aquatiques
> Universit? du Qu?bec ? Trois-Rivi?res
> C.P. 500, Trois-Rivi?res (Qu?bec) Canada
>
> -------------------------------------------------
> Courriel exp?di? via https://courriel.uqtr.ca
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From marc_schwartz at comcast.net  Mon Jan 29 21:01:03 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 29 Jan 2007 14:01:03 -0600
Subject: [R] Loop with string variable AND customizable "summary" output
In-Reply-To: <971536df0701291130m2ab0cfe4n3de73e9d22ab4931@mail.gmail.com>
References: <004001c743c7$6ff4b710$4d908980@gne.windows.gene.com>
	<1170095048.4841.15.camel@localhost.localdomain>
	<971536df0701291130m2ab0cfe4n3de73e9d22ab4931@mail.gmail.com>
Message-ID: <1170100863.4841.40.camel@localhost.localdomain>

On Mon, 2007-01-29 at 14:30 -0500, Gabor Grothendieck wrote:
> Note that the nlme solution seems to give the same coefficients
> but appears to use a single error term rather than one error
> term per level of the conditioning variable and that would change various
> other statistics relative to the other solutions should that matter.
> 
> > summary(lmList(uptake ~ conc | Treatment, CO2))
> Call:
>   Model: uptake ~ conc | Treatment
>    Data: CO2
> 
> Coefficients:
>    (Intercept)
>            Estimate Std. Error  t value     Pr(>|t|)
> nonchilled 22.01916    2.46416 8.935769 1.174616e-13
> chilled    16.98142    2.46416 6.891361 1.146556e-09
>    conc
>              Estimate  Std. Error  t value     Pr(>|t|)
> nonchilled 0.01982458 0.004692544 4.224699 6.292679e-05
> chilled    0.01563659 0.004692544 3.332221 1.306259e-03
> 
> Residual standard error: 8.945667 on 80 degrees of freedom

<snip>

Gabor,

Thanks for noting that. There is a solution using 'pool = FALSE':

> summary(lmList(uptake ~ conc | Treatment, CO2, pool = FALSE))
Call:
  Model: uptake ~ conc | Treatment 
   Data: CO2 

Coefficients:
   (Intercept) 
           Estimate Std. Error   t value     Pr(>|t|)
nonchilled 22.01916   2.148475 10.248740 9.463480e-13
chilled    16.98142   2.743761  6.189103 2.562416e-07
   conc 
             Estimate  Std. Error  t value     Pr(>|t|)
nonchilled 0.01982458 0.004091379 4.845452 1.934996e-05
chilled    0.01563659 0.005224992 2.992653 4.721873e-03


I suppose that, while subtle, this could make this approach error prone
for those who (like me in this case) miss it...

Then of course, we get down to the format of the output, etc.

:-)

Thanks Gabor,

Marc


From jkopecky at umich.edu  Mon Jan 29 21:32:35 2007
From: jkopecky at umich.edu (Jonathon Kopecky)
Date: Mon, 29 Jan 2007 15:32:35 -0500
Subject: [R] [Fwd: Need to fit a regression line using orthogonal residuals]
Message-ID: <45BE59E3.8050007@umich.edu>

[Originally sent this to r-help at lists.R-projects.org, but in case that's 
the wrong list I'm re-posting.  Apologies if this becomes a re-post]
-------------- next part --------------
An embedded message was scrubbed...
From: Jonathon Kopecky <jkopecky at umich.edu>
Subject: Need to fit a regression line using orthogonal residuals
Date: Mon, 29 Jan 2007 14:52:24 -0500
Size: 1138
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070129/783036c5/attachment.mht 

From A.Robinson at ms.unimelb.edu.au  Mon Jan 29 21:46:58 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Tue, 30 Jan 2007 07:46:58 +1100
Subject: [R] [Fwd: Need to fit a regression line using orthogonal
	residuals]
In-Reply-To: <45BE59E3.8050007@umich.edu>
References: <45BE59E3.8050007@umich.edu>
Message-ID: <20070129204658.GQ13602@ms.unimelb.edu.au>

Hi Jonothan,

try the smatr package.  

I hope that this helps,

Andrew

On Mon, Jan 29, 2007 at 03:32:35PM -0500, Jonathon Kopecky wrote:
> [Originally sent this to r-help at lists.R-projects.org, but in case that's 
> the wrong list I'm re-posting.  Apologies if this becomes a re-post]

> Date: Mon, 29 Jan 2007 14:52:24 -0500
> From: Jonathon Kopecky <jkopecky at umich.edu>
> To: r-help at lists.R-project.org
> Subject: Need to fit a regression line using orthogonal residuals
> 
> I'm trying to fit a simple linear regression of just Y ~ X, but both X 
> and Y are noisy.  Thus instead of fitting a standard linear model 
> minimizing vertical residuals, I would like to minimize 
> orthogonal/perpendicular residuals.  I have tried searching the 
> R-packages, but have not found anything that seems suitable.  I'm not 
> sure what these types of residuals are typically called (they seem to 
> have many different names), so that may be my trouble.  I do not want to 
> use Principal Components Analysis (as was answered to a previous 
> questioner a few years ago), I just want to minimize the combined noise 
> of my two variables.  Is there a way for me to do this in R?
> 
> Jonathon Kopecky
> University of Michigan
> 

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From GPetris at uark.edu  Mon Jan 29 21:59:04 2007
From: GPetris at uark.edu (Giovanni Petris)
Date: Mon, 29 Jan 2007 14:59:04 -0600 (CST)
Subject: [R] Bayesian States Space Modeling
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3E4BC1A@BAN-MAILSRV03.Amba.com>
	(message from Shubha Vishwanath Karanth on Mon, 29 Jan 2007 20:50:49
	+0530)
References: <A36876D3F8A5734FA84A4338135E7CC3E4BC1A@BAN-MAILSRV03.Amba.com>
Message-ID: <200701292059.l0TKx4YC014204@definetti.ddns.uark.edu>


If you are interested in linear Gaussian State Space models, then
package dlm may be of interest.

Giovanni

> Date: Mon, 29 Jan 2007 20:50:49 +0530
> From: Shubha Vishwanath Karanth <shubhak at ambaresearch.com>
> Sender: r-help-bounces at stat.math.ethz.ch
> Precedence: list
> Thread-topic: Bayesian States Space Modeling
> Thread-index: AcdDuQ5nlhQW8jWNRcCUj+Tx1MEP8g==
> 
> Hi R,
> 
>  
> 
> What package of R can I use for Bayesian States Space Modeling? And any
> other supporting packages?
> 
>  
> 
> Thanks in advance,
> 
> Shubha
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 

Giovanni Petris  <GPetris at uark.edu>
Associate Professor
Department of Mathematical Sciences
University of Arkansas - Fayetteville, AR 72701
Ph: (479) 575-6324, 575-8630 (fax)
http://definetti.uark.edu/~gpetris/


From roland.rproject at gmail.com  Mon Jan 29 22:58:42 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Mon, 29 Jan 2007 16:58:42 -0500
Subject: [R] "Reversal" of Aggregation
Message-ID: <47c7c59e0701291358j77473b08m9e8fc9dc61f267b9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070129/db5fc859/attachment.pl 

From ripley at stats.ox.ac.uk  Mon Jan 29 23:05:07 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 29 Jan 2007 22:05:07 +0000 (GMT)
Subject: [R] Problem with "readline" in compilatio of R for Solaris 11
 (Nevada) in x86
In-Reply-To: <45BE3D4D.3060408@ipea.gov.br>
References: <45BE3D4D.3060408@ipea.gov.br>
Message-ID: <Pine.LNX.4.64.0701292203510.26820@gannet.stats.ox.ac.uk>

Please consult the R-admin manual, as the INSTALL file asked you to.
It explains this.

On Mon, 29 Jan 2007, Octavio Tourinho wrote:

> Dear friends,
> In configuring R 2.4.1 for Solaris 11, using SunStudio 11 compilers, I
> get the following error.
>
> checking readline/history.h usability... no
> checking readline/history.h presence... no
> checking for readline/history.h... no
> checking readline/readline.h usability... no
> checking readline/readline.h presence... no
> checking for readline/readline.h... no
> checking for rl_callback_read_char in -lreadline... no
> checking for main in -lncurses... no
> checking for main in -ltermcap... yes
> checking for rl_callback_read_char in -lreadline... no
> checking for history_truncate_file... no
> configure: error: --with-readline=yes (default) and headers/libs are not
> available
>
> I was not able to figure out what readline is about, whether it is
> optional or not (at some point of the script it seems to be a flag which
> can be true or false) and was therefore unable to debug it.
>
> Thanks for any help you can provide
>
> Octavio Tourinho
> octavio.tourinho at terra.com.br
> octou at ipea.gov.br
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From muenchen at utk.edu  Mon Jan 29 23:10:23 2007
From: muenchen at utk.edu (Muenchen, Robert A (Bob))
Date: Mon, 29 Jan 2007 17:10:23 -0500
Subject: [R] R for SAS & SPSS Users Document
Message-ID: <7270AEC73132194E8BC0EE06B35D93D86D3CE7@UTKFSVS3.utk.tennessee.edu>

Greetings,

I am pleased to announce the availability of the document, "R for SAS
and SPSS Users", at 
http://oit.utk.edu/scc/RforSAS&SPSSusers.doc .  It presents an
introductory view of R for people who already know SAS and/or SPSS.
Included are 27 programs written in all three languages (i.e. 81 total)
so that people can see how R works compared to the other two, task by
task.

I would appreciate it if folks with far more R expertise than I have
could review it and provide advice on ways to improve programming
examples or wording. The wording was challenging since the jargon used
by the three packages differs so much. I'm sure there is much room for
improvement.

Cheers,
Bob

=========================================================
Bob Muenchen (pronounced Min'-chen), Manager 
Statistical Consulting Center
U of TN Office of Information Technology
200 Stokely Management Center, Knoxville, TN 37996-0520
Voice: (865) 974-5230 
FAX: (865) 974-4810
Email: muenchen at utk.edu
Web: http://oit.utk.edu/scc, 
News: http://listserv.utk.edu/archives/statnews.html


From Achim.Zeileis at wu-wien.ac.at  Mon Jan 29 23:15:42 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 29 Jan 2007 23:15:42 +0100 (CET)
Subject: [R] "Reversal" of Aggregation
In-Reply-To: <47c7c59e0701291358j77473b08m9e8fc9dc61f267b9@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0701292312360.10150-100000@disco.wu-wien.ac.at>



On Mon, 29 Jan 2007, Roland Rau wrote:

> Dear all,
>
> given I have a data.frame in a format like this
>
> mydf <- data.frame(age=rep(1:3,5),
>                    year=c(rep(1996,3), rep(1997,3), rep(1998,3),
>                      rep(1999,3), rep(2000,3)),
>                    income=1:15)
> mydf
>
>
> Now I convert it to some 2D-frequency table like this:
> mymatrix <- tapply(X=mydf$income, INDEX=list(mydf$age, mydf$year),
>                    FUN=sum)
> mymatrix
>
>
> My question is:
> How can I go the opposite way, i.e. from 'mymatrix' to 'mydf'?
> Is there an elegant way?

You could do
  as.data.frame(as.table(mymatrix))
and then set appropriate column names. (The first two variables are also
coded as "factor"s which might or might not be what you want in this
example.)

Z


> Thanks,
> Roland
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From roland.rproject at gmail.com  Mon Jan 29 23:21:31 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Mon, 29 Jan 2007 17:21:31 -0500
Subject: [R] "Reversal" of Aggregation
In-Reply-To: <Pine.LNX.4.44.0701292312360.10150-100000@disco.wu-wien.ac.at>
References: <47c7c59e0701291358j77473b08m9e8fc9dc61f267b9@mail.gmail.com>
	<Pine.LNX.4.44.0701292312360.10150-100000@disco.wu-wien.ac.at>
Message-ID: <47c7c59e0701291421s3f0dca87ob9a6db3a4d12803e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070129/c474ffbf/attachment.pl 

From stefan.theussl at wu-wien.ac.at  Tue Jan 30 00:10:39 2007
From: stefan.theussl at wu-wien.ac.at (Stefan Theussl)
Date: Tue, 30 Jan 2007 00:10:39 +0100
Subject: [R] CRAN will be down for 15 minutes
Message-ID: <1170112239.45be7eefbb509@webmail.wu-wien.ac.at>

Dear R Community,

Due to maintenance work CRAN will not be available at 30th January around 10
o'clock in the morning CET.

Best regards,
Stefan Theussl


From ggrothendieck at gmail.com  Tue Jan 30 00:13:13 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 29 Jan 2007 18:13:13 -0500
Subject: [R] "Reversal" of Aggregation
In-Reply-To: <Pine.LNX.4.44.0701292312360.10150-100000@disco.wu-wien.ac.at>
References: <47c7c59e0701291358j77473b08m9e8fc9dc61f267b9@mail.gmail.com>
	<Pine.LNX.4.44.0701292312360.10150-100000@disco.wu-wien.ac.at>
Message-ID: <971536df0701291513w6dbc066dxed908d34085d5709@mail.gmail.com>

Or equivalently:

  as.data.frame.table(mymatrix)

On 1/29/07, Achim Zeileis <Achim.Zeileis at wu-wien.ac.at> wrote:
>
>
> On Mon, 29 Jan 2007, Roland Rau wrote:
>
> > Dear all,
> >
> > given I have a data.frame in a format like this
> >
> > mydf <- data.frame(age=rep(1:3,5),
> >                    year=c(rep(1996,3), rep(1997,3), rep(1998,3),
> >                      rep(1999,3), rep(2000,3)),
> >                    income=1:15)
> > mydf
> >
> >
> > Now I convert it to some 2D-frequency table like this:
> > mymatrix <- tapply(X=mydf$income, INDEX=list(mydf$age, mydf$year),
> >                    FUN=sum)
> > mymatrix
> >
> >
> > My question is:
> > How can I go the opposite way, i.e. from 'mymatrix' to 'mydf'?
> > Is there an elegant way?
>
> You could do
>  as.data.frame(as.table(mymatrix))
> and then set appropriate column names. (The first two variables are also
> coded as "factor"s which might or might not be what you want in this
> example.)
>
> Z
>
>
> > Thanks,
> > Roland
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jeff.breiwick at noaa.gov  Tue Jan 30 00:33:43 2007
From: jeff.breiwick at noaa.gov (J.M. Breiwick)
Date: Mon, 29 Jan 2007 15:33:43 -0800
Subject: [R] countour and poygon shading
Message-ID: <epm020$uv3$1@sea.gmane.org>

Hi,

I have a contour plot and I want to shade a polygon (the area below a line) 
but the polygon shading wipes out the contour lines. Does anybody know how 
to shade the polygon and still see the contour lines? Thanks.

Jeff


From betty.health at gmail.com  Tue Jan 30 00:41:08 2007
From: betty.health at gmail.com (Betty Health)
Date: Mon, 29 Jan 2007 16:41:08 -0700
Subject: [R] help with RandomForest classwt option
In-Reply-To: <cdf817830701281816n50c768aasfa6ba4b0bac718a3@mail.gmail.com>
References: <34eed650701281013h6616ec83k1391f4318c22c88c@mail.gmail.com>
	<cdf817830701281816n50c768aasfa6ba4b0bac718a3@mail.gmail.com>
Message-ID: <34eed650701291541p44c505aei368030cbe345af16@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070129/b530d153/attachment.pl 

From ggrothendieck at gmail.com  Tue Jan 30 00:53:15 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 29 Jan 2007 18:53:15 -0500
Subject: [R] countour and poygon shading
In-Reply-To: <epm020$uv3$1@sea.gmane.org>
References: <epm020$uv3$1@sea.gmane.org>
Message-ID: <971536df0701291553j3087b3ax2f20b1bd9f25d090@mail.gmail.com>

If you look at the examples in ?xyplot.zoo in the zoo package
there is an example of placing a rectangle behind a lattice plot.
Maybe that applies here too?

For classic graphics there is an example of displaying a
rectangle behind a plot here:
http://www.mayin.org/ajayshah/KB/R/html/g5.html

On 1/29/07, J.M. Breiwick <jeff.breiwick at noaa.gov> wrote:
> Hi,
>
> I have a contour plot and I want to shade a polygon (the area below a line)
> but the polygon shading wipes out the contour lines. Does anybody know how
> to shade the polygon and still see the contour lines? Thanks.


From helprhelp at gmail.com  Tue Jan 30 01:47:33 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Mon, 29 Jan 2007 19:47:33 -0500
Subject: [R] help with RandomForest classwt option
In-Reply-To: <34eed650701291541p44c505aei368030cbe345af16@mail.gmail.com>
References: <34eed650701281013h6616ec83k1391f4318c22c88c@mail.gmail.com>
	<cdf817830701281816n50c768aasfa6ba4b0bac718a3@mail.gmail.com>
	<34eed650701291541p44c505aei368030cbe345af16@mail.gmail.com>
Message-ID: <cdf817830701291647r49d339fat6192f7b38cf6f861@mail.gmail.com>

Hi, Betty:

1. Fortan code (http://www.stat.berkeley.edu/~breiman/RandomForests/cc_examples/prog.f)

	if(jclasswt.eq.0) then
		do j=1,nclass
			classwt(j)=1
		enddo
	endif
	if(jclasswt.eq.1) then
c		fill in classwt(j) for each j:
c		classwt(1)=1.
c		classwt(2)=10.

You need to set the jclasswt = 1 ( you can find by "search" through the codes).
then "uncomment" the last two lines. Here you go with classwt in
fortran. You can use this classwt for extremely-imbalanced
classification problem. Down-sampling is one possible choice for that
too but it is not directly implemented in rf. Check the following
paper, and it might help.
http://oz.berkeley.edu/users/chenchao/666.pdf

2. as to the wrapper function, the idea is that you can create a set
of samples by applying some sampling probilities to implement
down-sampling. Then build a rf model for each sample;
suppose you call rf in this way for each sample,
my.rf <- randomForest(...)

then you can access the oob scores and prediction scores by
my.rf$votes or my.rf$test$votes respectively.

then you can average those scores by yourself, it is just like a
simple meta-learning process but it does exactly what downsampling
plus rf does, though downsampling is not implemented.


3. classwt and cutoff are used at different places. The former is used
at two places: calculating the gini criteria and calculating the final
vote from the leaf. While cutoff is only used in the final voting. So
cutoff won't change the splitting while classwt can. However, since
the current R's rf cannot do classwt, you can try to use cutoff to see
if it helps in your case.

4. The fourth option is you can use my implementation of rf; But I did
not write a manual for that; and it cannot show your splitting yet.

HTH,

weiwei




On 1/29/07, Betty Health <betty.health at gmail.com> wrote:
> Thank you very much, Weiwei and Jim!
>
> Yeah, I did read the post by Andy, the contributor of this package. It seems
> that classwt is not implemented yet. For Weiwei's options, I have a few more
> questions. Thanks!
>
> "1. try to use rf in fortran by following the linky below
> http://www.stat.berkeley.edu/~breiman/RandomForests/cc_software.htm"
>
> I read the Fortran code briefly. But I did not find the options for down
> sampling. So does that mean I need to do down sampling myself?  Could you
> explain a little more about "2. make a wrapper function to do the down
> sampling by yourself"? You mean I can do it in R or in Fortran? Some links
> plz? I haven't done this before.
>
> Yeah, cut off did change for the final classification results. However from
> what I tried, they did not influence how the nodes are split. So I would go
> further in the above 2 options.
>
> Thank you again!
>
>  Betty
>
>
>
>
> On 1/28/07, Weiwei Shi <helprhelp at gmail.com> wrote:
> > Dear Betty:
> >
> > I could suggest 3 options:
> >
> > 1. try to use rf in fortran by following the linky below
> >
> http://www.stat.berkeley.edu/~breiman/RandomForests/cc_software.htm
> >
> > 2. make a wrapper function to do the down sampling by yourself
> >
> > 3. try to use cutoff in randomForest, which might help in your situation.
> >
> > HTH,
> >
> > weiwei
> >
> > On 1/28/07, Betty Health < betty.health at gmail.com> wrote:
> > > Hello there,
> > >
> > > I am working on an extremely unbalanced two class classification
> problems. I
> > > wanna use "classwt" with "down sampling" together. By checking the
> rfNews()
> > > in R, it looks that classwt is not working yet. Then I looked at the
> > > software from Salford. I did not find the down sampling option.  I am
> > > wondering if you have any experience to deal with this problem. Do you
> know
> > > any method or softwares can handle this problem?
> > >
> > > Thank you very much!!
> > >
> > > Betty
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
> > --
> > Weiwei Shi, Ph.D
> > Research Scientist
> > GeneGO, Inc.
> >
> > "Did you always know?"
> > "No, I did not. But I believed..."
> > ---Matrix III
> >
>
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From helprhelp at gmail.com  Tue Jan 30 02:16:51 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Mon, 29 Jan 2007 20:16:51 -0500
Subject: [R] help with RandomForest classwt option
In-Reply-To: <34eed650701291541p44c505aei368030cbe345af16@mail.gmail.com>
References: <34eed650701281013h6616ec83k1391f4318c22c88c@mail.gmail.com>
	<cdf817830701281816n50c768aasfa6ba4b0bac718a3@mail.gmail.com>
	<34eed650701291541p44c505aei368030cbe345af16@mail.gmail.com>
Message-ID: <cdf817830701291716r23627618jcb3eba5c36526551@mail.gmail.com>

The fifth option:

actually it might be the easiest way:
you "boost" your minority by like 10 fold (just repeat each minority
record 10 times). Then run rf on the boosted sample. The learning
process does not exactly behave like using classwt (setting classwt[2]
= 10 will exactly gives weight=10 in gini calculation), but it is
statistically similar.

Be careful of oob error though. it won't give you a correct estimation
of it since a sample can be used in training while its duplicates
could be used in out-of-bag. But if you care about the splitting, this
approach helps, IMHO.

HTH,

weiwei

On 1/29/07, Betty Health <betty.health at gmail.com> wrote:
> Thank you very much, Weiwei and Jim!
>
> Yeah, I did read the post by Andy, the contributor of this package. It seems
> that classwt is not implemented yet. For Weiwei's options, I have a few more
> questions. Thanks!
>
> "1. try to use rf in fortran by following the linky below
> http://www.stat.berkeley.edu/~breiman/RandomForests/cc_software.htm"
>
> I read the Fortran code briefly. But I did not find the options for down
> sampling. So does that mean I need to do down sampling myself?  Could you
> explain a little more about "2. make a wrapper function to do the down
> sampling by yourself"? You mean I can do it in R or in Fortran? Some links
> plz? I haven't done this before.
>
> Yeah, cut off did change for the final classification results. However from
> what I tried, they did not influence how the nodes are split. So I would go
> further in the above 2 options.
>
> Thank you again!
>
>  Betty
>
>
>
>
> On 1/28/07, Weiwei Shi <helprhelp at gmail.com> wrote:
> > Dear Betty:
> >
> > I could suggest 3 options:
> >
> > 1. try to use rf in fortran by following the linky below
> >
> http://www.stat.berkeley.edu/~breiman/RandomForests/cc_software.htm
> >
> > 2. make a wrapper function to do the down sampling by yourself
> >
> > 3. try to use cutoff in randomForest, which might help in your situation.
> >
> > HTH,
> >
> > weiwei
> >
> > On 1/28/07, Betty Health < betty.health at gmail.com> wrote:
> > > Hello there,
> > >
> > > I am working on an extremely unbalanced two class classification
> problems. I
> > > wanna use "classwt" with "down sampling" together. By checking the
> rfNews()
> > > in R, it looks that classwt is not working yet. Then I looked at the
> > > software from Salford. I did not find the down sampling option.  I am
> > > wondering if you have any experience to deal with this problem. Do you
> know
> > > any method or softwares can handle this problem?
> > >
> > > Thank you very much!!
> > >
> > > Betty
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
> > --
> > Weiwei Shi, Ph.D
> > Research Scientist
> > GeneGO, Inc.
> >
> > "Did you always know?"
> > "No, I did not. But I believed..."
> > ---Matrix III
> >
>
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From jiho.han at yahoo.com  Tue Jan 30 04:59:07 2007
From: jiho.han at yahoo.com (jiho.han)
Date: Mon, 29 Jan 2007 19:59:07 -0800 (PST)
Subject: [R] rbind-ing list
Message-ID: <8703373.post@talk.nabble.com>


hi, 

i have a list of data.frame that has same structure. i would like to know a
efficient way of rbind-ing it. 
right now, i write:


n = length(temp)     # 'temp' is a list of data.frames
temp2 = data.frame()
for (i in 1:n) temp2 = rbind( temp2, temp[[i]])
return(temp2)


but this is not an efficient way since we keeping overwriting temp2. i
wonder if there's faster way. 

thanks
-- 
View this message in context: http://www.nabble.com/rbind-ing-list-tf3140137.html#a8703373
Sent from the R help mailing list archive at Nabble.com.


From jholtman at gmail.com  Tue Jan 30 05:05:27 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 29 Jan 2007 23:05:27 -0500
Subject: [R] rbind-ing list
In-Reply-To: <8703373.post@talk.nabble.com>
References: <8703373.post@talk.nabble.com>
Message-ID: <644e1f320701292005o4e12b21fgfb7c2d4725754757@mail.gmail.com>

do.call(rbind, temp)

On 1/29/07, jiho.han <jiho.han at yahoo.com> wrote:
>
> hi,
>
> i have a list of data.frame that has same structure. i would like to know a
> efficient way of rbind-ing it.
> right now, i write:
>
>
> n = length(temp)     # 'temp' is a list of data.frames
> temp2 = data.frame()
> for (i in 1:n) temp2 = rbind( temp2, temp[[i]])
> return(temp2)
>
>
> but this is not an efficient way since we keeping overwriting temp2. i
> wonder if there's faster way.
>
> thanks
> --
> View this message in context: http://www.nabble.com/rbind-ing-list-tf3140137.html#a8703373
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From betty.health at gmail.com  Tue Jan 30 05:59:24 2007
From: betty.health at gmail.com (Betty Health)
Date: Mon, 29 Jan 2007 21:59:24 -0700
Subject: [R] help with RandomForest classwt option
In-Reply-To: <cdf817830701291647r49d339fat6192f7b38cf6f861@mail.gmail.com>
References: <34eed650701281013h6616ec83k1391f4318c22c88c@mail.gmail.com>
	<cdf817830701281816n50c768aasfa6ba4b0bac718a3@mail.gmail.com>
	<34eed650701291541p44c505aei368030cbe345af16@mail.gmail.com>
	<cdf817830701291647r49d339fat6192f7b38cf6f861@mail.gmail.com>
Message-ID: <34eed650701292059s3fa7e9e4p770a5c4cc909ebed@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070129/6edfaaa6/attachment.pl 

From cyau at tulane.edu  Tue Jan 30 07:12:28 2007
From: cyau at tulane.edu (C. Lillian Yau)
Date: Tue, 30 Jan 2007 00:12:28 -0600
Subject: [R] Error message when building a package
Message-ID: <C1E43DEC.688%cyau@tulane.edu>

I'm trying to build a package. The machine is PowerPC G4 with Mac OS 10.4.8,
and I'm using R2.4.1.

I get "R CMD build roots" working, and it created roots.tar.gz. But I get
the following message when I run "R CMD INSTALL -l ../myrlibrary
roots.tar.gz"

======================================================================
* Installing *source* package 'roots' ...
** libs
** arch - ppc
gcc-4.0 -arch ppc -std=gnu99
-I/Library/Frameworks/R.framework/Resources/include
-I/Library/Frameworks/R.framework/Resources/include/ppc
-I/usr/local/include    -fPIC  -g -O2 -c fifrt.c -o fifrt.o
gfortran-4.0 -arch ppc   -fPIC  -g -O2 -c fthrt.f -o fthrt.o
gcc-4.0 -arch ppc -std=gnu99 -dynamiclib -Wl,-macosx_version_min -Wl,10.3
-undefined dynamic_lookup -single_module -multiply_defined suppress
-L/usr/local/lib -o roots.so fifrt.o fthrt.o  -lgfortran -lgcc_s
-lSystemStubs -lmx -lSystem
-L/Library/Frameworks/R.framework/Resources/lib/ppc -lR -dylib_file
libRblas.dylib:/Library/Frameworks/R.framework/Resources/lib/ppc/libRblas.dy
lib
/usr/bin/libtool: unknown option character `m' in: -macosx_version_min
Usage: /usr/bin/libtool -static [-] file [...] [-filelist
listfile[,dirname]] [-arch_only arch] [-sacLT]
Usage: /usr/bin/libtool -dynamic [-] file [...] [-filelist
listfile[,dirname]] [-arch_only arch] [-o output] [-install_name name]
[-compatibility_version #] [-current_version #] [-seg1addr 0x#]
[-segs_read_only_addr 0x#] [-segs_read_write_addr 0x#] [-seg_addr_table
<filename>] [-seg_addr_table_filename <file_system_path>] [-all_load]
[-noall_load]
make: *** [roots.so] Error 1
chmod: /../myrlibrary/roots/libs/ppc/*: No such file or directory
ERROR: compilation failed for package 'roots'
** Removing '/../myrlibrary/roots'
============================================================================

One thing I can see from above is the message "/usr/bin/libtool: unknown
option character `m' in: -macosx_version_min", which I got when compiling
other c programs to be called by R.

How can I fix it or get around it? I appreciate your time and help very
much.

Lillian Yau


From petr.pikal at precheza.cz  Tue Jan 30 07:35:50 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 30 Jan 2007 07:35:50 +0100
Subject: [R] Adding lines to xYplot
In-Reply-To: <C9C884FB-A22E-42AF-BC6F-71034C830F80@virginia.edu>
Message-ID: <45BEF556.29112.404B4A@localhost>

Hi

here is another approach based on Gabor Grothendieck's idea to add 
lines to existing lattice plot.

# based on Gabor Grothendieck's code suggestion
# adds straight lines to panels in lattice plots

addLine<- function(...) {
tcL <- trellis.currentLayout()
for(i in 1:nrow(tcL))
  for(j in 1:ncol(tcL))
    if (tcL[i,j] > 0) {
        trellis.focus("panel", j, i, highlight = FALSE)
        panel.abline(...)
        trellis.unfocus()
        }
}

HTH
Petr



On 27 Jan 2007 at 20:13, Michael Kubovy wrote:

To:             	"r-help at stat.math.ethz.ch list" <r-help at stat.math.ethz.ch>
From:           	Michael Kubovy <kubovy at virginia.edu>
Date sent:      	Sat, 27 Jan 2007 20:13:15 -0500
Subject:        	[R] Adding lines to xYplot

> I am using xYplot to plot data and CIs. How do I add several lines to 
> the figure? _____________________________ Professor Michael Kubovy
> University of Virginia Department of Psychology USPS:     P.O.Box
> 400400    Charlottesville, VA 22904-4400 Parcels:    Room 102       
> Gilmer Hall
>          McCormick Road    Charlottesville, VA 22903
> Office:    B011    +1-434-982-4729
> Lab:        B019    +1-434-982-4751
> Fax:        +1-434-982-4766
> WWW:    http://www.people.virginia.edu/~mk9y/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From ripley at stats.ox.ac.uk  Tue Jan 30 08:21:40 2007
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Tue, 30 Jan 2007 07:21:40 +0000 (GMT Standard Time)
Subject: [R] Error message when building a package
In-Reply-To: <C1E43DEC.688%cyau@tulane.edu>
References: <C1E43DEC.688%cyau@tulane.edu>
Message-ID: <Pine.WNT.4.64.0701300720070.3776@Petrel>

Please update your Xcode tools.  According to

http://cran.r-project.org/bin/macosx/RMacOSX-FAQ.html

2.2 or later is needed, and 2.4.1 is current.


On Tue, 30 Jan 2007, C. Lillian Yau wrote:

> I'm trying to build a package. The machine is PowerPC G4 with Mac OS 10.4.8,
> and I'm using R2.4.1.
>
> I get "R CMD build roots" working, and it created roots.tar.gz. But I get
> the following message when I run "R CMD INSTALL -l ../myrlibrary
> roots.tar.gz"
>
> ======================================================================
> * Installing *source* package 'roots' ...
> ** libs
> ** arch - ppc
> gcc-4.0 -arch ppc -std=gnu99
> -I/Library/Frameworks/R.framework/Resources/include
> 
-I/Library/Frameworks/R.framework/Resources/include/ppc
> -I/usr/local/include    -fPIC  -g -O2 -c fifrt.c -o fifrt.o
> gfortran-4.0 -arch ppc   -fPIC  -g -O2 -c fthrt.f -o fthrt.o
> gcc-4.0 -arch ppc -std=gnu99 -dynamiclib -Wl,-macosx_version_min -Wl,10.3
> -undefined dynamic_lookup -single_module -multiply_defined suppress
> -L/usr/local/lib -o roots.so fifrt.o fthrt.o  -lgfortran -lgcc_s
> -lSystemStubs -lmx -lSystem
> -L/Library/Frameworks/R.framework/Resources/lib/ppc -lR -dylib_file
> libRblas.dylib:/Library/Frameworks/R.framework/Resources/lib/ppc/libRblas.dy
> lib
> /usr/bin/libtool: unknown option character `m' in: -macosx_version_min

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From shubhak at ambaresearch.com  Tue Jan 30 08:20:32 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Tue, 30 Jan 2007 12:50:32 +0530
Subject: [R] Appending zoo objects
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3E4BEE1@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070130/979dd234/attachment.pl 

From stat700004 at yahoo.co.in  Tue Jan 30 09:45:12 2007
From: stat700004 at yahoo.co.in (stat stat)
Date: Tue, 30 Jan 2007 08:45:12 +0000 (GMT)
Subject: [R] Finding the Min.
Message-ID: <910855.70666.qm@web7615.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070130/ed7a9fb4/attachment.pl 

From wl2776 at gmail.com  Tue Jan 30 09:58:14 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Tue, 30 Jan 2007 00:58:14 -0800 (PST)
Subject: [R] R syntax highlighting for FAR manager's plugin Colorer
Message-ID: <8706070.post@talk.nabble.com>


Does anyone use FAR manager?
If yes, does that one use the Colorer plugin with the FAR?
And, if yes, does that one have a file for Colorer, describing R syntax? :)
-- 
View this message in context: http://www.nabble.com/R-syntax-highlighting-for-FAR-manager%27s-plugin-Colorer-tf3141231.html#a8706070
Sent from the R help mailing list archive at Nabble.com.


From javalkon at hytti.uku.fi  Tue Jan 30 10:00:54 2007
From: javalkon at hytti.uku.fi (Jarimatti Valkonen)
Date: Tue, 30 Jan 2007 11:00:54 +0200
Subject: [R] Finding the Min.
In-Reply-To: <910855.70666.qm@web7615.mail.in.yahoo.com>
References: <910855.70666.qm@web7615.mail.in.yahoo.com>
Message-ID: <45BF0946.80004@hytti.uku.fi>

stat stat wrote:

>   I want to see at which row minimum value of the second column occures.
>    
>   Therefore I made the following loop:

[snip while-loop]

> 
>   Is there any more effective way to do that in terms of time consumption?

I don't know about timing, but I understand that loops are somewhat 
slow. Assuming that x is the first column and y is the second column:

	D <- data.frame( x = 1:10, y = rnorm(10) )
	D$x[ which.min(D$y) ]

See the help of which.min for more info.

-Jarimatti


From Bernhard_Pfaff at fra.invesco.com  Tue Jan 30 10:02:38 2007
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Tue, 30 Jan 2007 09:02:38 -0000
Subject: [R] Finding the Min.
In-Reply-To: <910855.70666.qm@web7615.mail.in.yahoo.com>
References: <910855.70666.qm@web7615.mail.in.yahoo.com>
Message-ID: <E4A9111DA23BA048B9A46686BF727CF461BF03@DEFRAXMB01.corp.amvescap.net>

?which.min

>
>Dear all R users,
>   
>  Suppose I have a dataset like that, data = 
>   
>      1         1.957759e-09
>    2         1.963619e-09
>    3         1.962807e-09
>    4         1.973951e-09
>    5         1.983401e-09
>    6         1.990894e-09
>    7         2.000935e-09
>    8         1.998391e-09
>    9         1.973322e-09
>   10         1.983202e-09
>   
>  I want to see at which row minimum value of the second 
>column occures.
>   
>  Therefore I made the following loop:
>   
>  i=1
>  while (min(data[,2]) != data[i,2]) 
>                         {
>                          i = i+1
>                         }
>                  
>  > i
>[1] 1
>
>  Is there any more effective way to do that in terms of time 
>consumption?
>   
>  Thanks
>  stat
>
> 				
>---------------------------------
> Here's a new way to find what you're looking for - Yahoo! Answers 
>	[[alternative HTML version deleted]]
>
>
*****************************************************************
Confidentiality Note: The information contained in this mess...{{dropped}}


From shubhak at ambaresearch.com  Tue Jan 30 10:47:07 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Tue, 30 Jan 2007 15:17:07 +0530
Subject: [R] Rbind for appending zoo objects
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3E4BF94@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070130/49be0d6d/attachment.pl 

From ggrothendieck at gmail.com  Tue Jan 30 10:52:09 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 30 Jan 2007 04:52:09 -0500
Subject: [R] Appending zoo objects
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3E4BEE1@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC3E4BEE1@BAN-MAILSRV03.Amba.com>
Message-ID: <971536df0701300152o6acb3b11kc81856e653a18bd1@mail.gmail.com>

You can use rbind.zoo if the times do not overlap.

On 1/30/07, Shubha Vishwanath Karanth <shubhak at ambaresearch.com> wrote:
> Hi everybody,
>
>
>
> How do we append (note: it's not merging) two zoo objects with the same
> column names?
>
>
>
> Thanks,
>
> Shubha
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jim at bitwrit.com.au  Tue Jan 30 11:00:35 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Tue, 30 Jan 2007 21:00:35 +1100
Subject: [R] countour and poygon shading
In-Reply-To: <epm020$uv3$1@sea.gmane.org>
References: <epm020$uv3$1@sea.gmane.org>
Message-ID: <45BF1743.8000702@bitwrit.com.au>

J.M. Breiwick wrote:
> Hi,
> 
> I have a contour plot and I want to shade a polygon (the area below a line) 
> but the polygon shading wipes out the contour lines. Does anybody know how 
> to shade the polygon and still see the contour lines? Thanks.
> 
Hi Jeff,
If you're talking about polygon.shadow in the plotrix package, you have 
to call that function before you draw the overlying figure and anything 
else that would lie within the shadow.

Jim


From ggrothendieck at gmail.com  Tue Jan 30 11:10:05 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 30 Jan 2007 05:10:05 -0500
Subject: [R] Rbind for appending zoo objects
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3E4BF94@BAN-MAILSRV03.Amba.com>
References: <A36876D3F8A5734FA84A4338135E7CC3E4BF94@BAN-MAILSRV03.Amba.com>
Message-ID: <971536df0701300210x6c00dcb0kaab74ba22a4697e2@mail.gmail.com>

That is how rbind in the core of R works too and rbind.zoo works the same.

Try:

rbind(y1, y2[, colnames(y1)])


On 1/30/07, Shubha Vishwanath Karanth <shubhak at ambaresearch.com> wrote:
> Hi R,
>
>
>
> y1 <- zoo(matrix(1:10, ncol = 2), 1:5)
>
> colnames(y1)=c("a","b")
>
> y2 <- zoo(matrix(rnorm(10), ncol = 2), 6:10)
>
> colnames(y2)=c("b","a")
>
>
>
> > y1
>
>  a  b
>
> 1 1  6
>
> 2 2  7
>
> 3 3  8
>
> 4 4  9
>
> 5 5 10
>
>
>
> > y2
>
>            b          a
>
> 6   0.9070204  0.3527630
>
> 7   1.2405943  0.8275001
>
> 8  -0.1690653 -0.1724976
>
> 9  -0.6905223 -1.1127670
>
> 10  0.3776210  0.4208908
>
>
>
> Now, I have to append these two zoo objects, y1 and y2. So, I do as
> follows:
>
>
>
> > rbind(y2,y1)
>
>            b          a
>
> 1   1.0000000  6.0000000
>
> 2   2.0000000  7.0000000
>
> 3   3.0000000  8.0000000
>
> 4   4.0000000  9.0000000
>
> 5   5.0000000 10.0000000
>
> 6   0.9070204  0.3527630
>
> 7   1.2405943  0.8275001
>
> 8  -0.1690653 -0.1724976
>
> 9  -0.6905223 -1.1127670
>
> 10  0.3776210  0.4208908
>
> >
>
>
>
> The doubts I get are as follows:
>
> 1.      The above rbind function for the zoo objects doesn't take care
> of the column names while merging. Example: Column 'a' of y1 is appended
> with column 'b' of y2. Why is this so? How do I get rid of this?
> 2.      In the rbind function, I have given y2 first and then y1. But in
> the appended data, I see the data corresponding to y1 first and then of
> y2. Is this because of ordering of the index elements of the zoo
> objects?
>
>
>
> Or, is there any other better function to append zoo objects?
>
>
>
> Thanks in advance,
>
> Shubha
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From pburns at pburns.seanet.com  Tue Jan 30 11:41:12 2007
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Tue, 30 Jan 2007 10:41:12 +0000
Subject: [R] rbind-ing list
In-Reply-To: <644e1f320701292005o4e12b21fgfb7c2d4725754757@mail.gmail.com>
References: <8703373.post@talk.nabble.com>
	<644e1f320701292005o4e12b21fgfb7c2d4725754757@mail.gmail.com>
Message-ID: <45BF20C8.5090404@pburns.seanet.com>

The original poster is correct that the loop can be
astoundingly inefficient.  The issue is not so much
overwriting a variable, but increasing its size as it
is overwritten.

In cases where 'do.call' won't do, then a good approach
is to create the object with its final size and then
subscript into it to replace parts of it on each iteration.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

jim holtman wrote:

>do.call(rbind, temp)
>
>On 1/29/07, jiho.han <jiho.han at yahoo.com> wrote:
>  
>
>>hi,
>>
>>i have a list of data.frame that has same structure. i would like to know a
>>efficient way of rbind-ing it.
>>right now, i write:
>>
>>
>>n = length(temp)     # 'temp' is a list of data.frames
>>temp2 = data.frame()
>>for (i in 1:n) temp2 = rbind( temp2, temp[[i]])
>>return(temp2)
>>
>>
>>but this is not an efficient way since we keeping overwriting temp2. i
>>wonder if there's faster way.
>>
>>thanks
>>--
>>View this message in context: http://www.nabble.com/rbind-ing-list-tf3140137.html#a8703373
>>Sent from the R help mailing list archive at Nabble.com.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
>>    
>>
>
>
>  
>


From edholdgate at yahoo.com  Tue Jan 30 02:49:28 2007
From: edholdgate at yahoo.com (Ed Holdgate)
Date: Mon, 29 Jan 2007 17:49:28 -0800 (PST)
Subject: [R] How to find series of small numbers in a big vector?
Message-ID: <38661.37380.qm@web53801.mail.yahoo.com>


Hello:

I have a vector with 120,000 reals
between 0.00000 and 0.9999

They are not sorted but the vector index is the 
time-order of my measurements, and therefore
cannot be lost.

How do I use R to find the starting and ending 
index of ANY and ALL the "series" or "sequences" 
in that vector where ever there are 5 or more  
members in a row between 0.021 and 0.029 ?

For example:

search_range <- c (0.021, 0.029) # inclusive searching
search_length <- 5   # find ALL series of 5 members within search_range
my_data <- c(0.900, 0.900, 0.900, 0.900, 0.900,
             0.900, 0.900, 0.900, 0.900, 0.900,
             0.900, 0.028, 0.024, 0.027, 0.023,
             0.022, 0.900, 0.900, 0.900, 0.900,
             0.900, 0.900, 0.024, 0.029, 0.023,
             0.025, 0.026, 0.900, 0.900, 0.900,
             0.900, 0.900, 0.900, 0.900, 0.900,
             0.900, 0.900, 0.900, 0.900, 0.022,
             0.023, 0.025, 0.333, 0.027, 0.028,
             0.900, 0.900, 0.900, 0.900, 0.900)

I seek the R program to report: 
start_index of 12 and an end_index of 16
-- and also --
start_index of 23 and an end_index of 27
because that is were there happens to be
search_length numbers within my search_range.

It should _not_ report the series at start_index 40
because that 0.333 in there violates the search_range.

I could brute-force hard-code an R program, but
perhaps an expert can give me a tip for an
easy, elegant existing function or a tactic 
to approach?

Execution speed or algorithm performance is not, 
for me in this case, important.  Rather, I
seek an easy R solution to find the time windows 
(starting & ending indicies) where 5 or more 
small numbers in my search_range were measured 
all in a row.

Advice welcome and many thanks in advance.

Ed Holdgate


From Achim.Zeileis at wu-wien.ac.at  Tue Jan 30 12:03:46 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 30 Jan 2007 12:03:46 +0100 (CET)
Subject: [R] Rbind for appending zoo objects
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3E4BF94@BAN-MAILSRV03.Amba.com>
Message-ID: <Pine.LNX.4.44.0701301201300.10150-100000@disco.wu-wien.ac.at>

On Tue, 30 Jan 2007, Shubha Vishwanath Karanth wrote:

> 1.	The above rbind function for the zoo objects doesn't take care
> of the column names while merging. Example: Column 'a' of y1 is appended
> with column 'b' of y2. Why is this so?

We do not check the column names at all. This should probably be changed.

> How do I get rid of this?

Sort the columns by hand before calling rbind().

> 2.	In the rbind function, I have given y2 first and then y1. But in
> the appended data, I see the data corresponding to y1 first and then of
> y2. Is this because of ordering of the index elements of the zoo
> objects?

Yes, recall that the second "o" in "zoo" means "ordered".

Best,
Z


From fabascal at cnb.uam.es  Tue Jan 30 12:27:53 2007
From: fabascal at cnb.uam.es (Federico Abascal)
Date: Tue, 30 Jan 2007 12:27:53 +0100
Subject: [R] how to join two arrays using their column names intersection
Message-ID: <45BF2BB9.1060408@cnb.uam.es>

Dear all,

I have a problem that may be someone of you can help. I am a newbie and
do not find how to do it in manuals.

I have two arrays, for example:

ar1 <- array(data=c(1:16),dim=c(4,4))
ar2 <- array(data=c(1:16),dim=c(4,4))
colnames(ar1)<-c("A","B","D","E")
colnames(ar2)<-c("C","A","E","B")

> ar1
     A B  D  E
[1,] 1 5  9 13
[2,] 2 6 10 14
[3,] 3 7 11 15
[4,] 4 8 12 16
> ar2
     C A  E  B
[1,] 1 5  9 13
[2,] 2 6 10 14
[3,] 3 7 11 15
[4,] 4 8 12 16


I would like to join both arrays only for the columns present in both
ar1 and ar2 (the intersection). I would like to obtain this:
     A B  E
[1,] 1 5 13
[2,] 2 6 14
[3,] 3 7 15
[4,] 4 8 16
[5,] 5 13  9
[6,] 6 14 10
[7,] 7 15 11
[8,] 8 16 12

(rows 5-8 correspond to ar2)

I have tried several things but I am unable to accomplish it. Any
experienced user could give me some advice, please?

I have another question: how can I sort the columns of an array
according to its column names (for ar2, change CAEB to ABCE)?

Thanks in advance!
Federico


From wl2776 at gmail.com  Tue Jan 30 12:34:34 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Tue, 30 Jan 2007 03:34:34 -0800 (PST)
Subject: [R] How to find series of small numbers in a big vector?
In-Reply-To: <38661.37380.qm@web53801.mail.yahoo.com>
References: <38661.37380.qm@web53801.mail.yahoo.com>
Message-ID: <8707709.post@talk.nabble.com>


I would try using na.contiguos from package stats.

R.utils has seqToIntervals.defaul, 
which "Gets all contigous intervals of a vector of indices".

(I didn't use the latter, help.search("contiguous") gave me that name).
-- 
View this message in context: http://www.nabble.com/-R--How-to-find-series-of-small-numbers-in-a-big-vector--tf3141691.html#a8707709
Sent from the R help mailing list archive at Nabble.com.


From j.zutt at tudelft.nl  Tue Jan 30 12:53:49 2007
From: j.zutt at tudelft.nl (Jonne Zutt)
Date: Tue, 30 Jan 2007 12:53:49 +0100
Subject: [R] How to find series of small numbers in a big vector?
In-Reply-To: <38661.37380.qm@web53801.mail.yahoo.com>
References: <38661.37380.qm@web53801.mail.yahoo.com>
Message-ID: <1170158029.24658.34.camel@dutiih.st.ewi.tudelft.nl>

I suggest the following appraoch

This gives TRUE for all data within the search_range
        A1 = my_data > search_range[1] & my_data < search_range[2]

which() gives us the indices
        A2 = which(A1)

and diff() the gaps between those intervals
        A3 = diff(A2)

Hence, if A3 > search_length, we have enough consecutive numbers within
the search range

Finally, this is what you wanted to know?

        A2[ which(A3 > search_length) ]


On Mon, 2007-01-29 at 17:49 -0800, Ed Holdgate wrote:
> Hello:
> 
> I have a vector with 120,000 reals
> between 0.00000 and 0.9999
> 
> They are not sorted but the vector index is the 
> time-order of my measurements, and therefore
> cannot be lost.
> 
> How do I use R to find the starting and ending 
> index of ANY and ALL the "series" or "sequences" 
> in that vector where ever there are 5 or more  
> members in a row between 0.021 and 0.029 ?
> 
> For example:
> 
> search_range <- c (0.021, 0.029) # inclusive searching
> search_length <- 5   # find ALL series of 5 members within search_range
> my_data <- c(0.900, 0.900, 0.900, 0.900, 0.900,
>              0.900, 0.900, 0.900, 0.900, 0.900,
>              0.900, 0.028, 0.024, 0.027, 0.023,
>              0.022, 0.900, 0.900, 0.900, 0.900,
>              0.900, 0.900, 0.024, 0.029, 0.023,
>              0.025, 0.026, 0.900, 0.900, 0.900,
>              0.900, 0.900, 0.900, 0.900, 0.900,
>              0.900, 0.900, 0.900, 0.900, 0.022,
>              0.023, 0.025, 0.333, 0.027, 0.028,
>              0.900, 0.900, 0.900, 0.900, 0.900)
> 
> I seek the R program to report: 
> start_index of 12 and an end_index of 16
> -- and also --
> start_index of 23 and an end_index of 27
> because that is were there happens to be
> search_length numbers within my search_range.
> 
> It should _not_ report the series at start_index 40
> because that 0.333 in there violates the search_range.
> 
> I could brute-force hard-code an R program, but
> perhaps an expert can give me a tip for an
> easy, elegant existing function or a tactic 
> to approach?
> 
> Execution speed or algorithm performance is not, 
> for me in this case, important.  Rather, I
> seek an easy R solution to find the time windows 
> (starting & ending indicies) where 5 or more 
> small numbers in my search_range were measured 
> all in a row.
> 
> Advice welcome and many thanks in advance.
> 
> Ed Holdgate
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From fabascal at cnb.uam.es  Tue Jan 30 12:56:13 2007
From: fabascal at cnb.uam.es (Federico Abascal)
Date: Tue, 30 Jan 2007 12:56:13 +0100
Subject: [R] how to join two arrays using their column names intersection
In-Reply-To: <45BF2BB9.1060408@cnb.uam.es>
References: <45BF2BB9.1060408@cnb.uam.es>
Message-ID: <45BF325D.5070507@cnb.uam.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070130/82d904e5/attachment.pl 

From wl2776 at gmail.com  Tue Jan 30 13:01:06 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Tue, 30 Jan 2007 04:01:06 -0800 (PST)
Subject: [R] how to join two arrays using their column names intersection
In-Reply-To: <45BF2BB9.1060408@cnb.uam.es>
References: <45BF2BB9.1060408@cnb.uam.es>
Message-ID: <8708210.post@talk.nabble.com>


dfr1<-merge(ar1,ar2,all=TRUE)
result<-as.matrix(dfr1[apply(dfr1,2,function(x)!any(is.na(x)))])


Federico Abascal wrote:
> 
> Dear all,
> 
> I have a problem that may be someone of you can help. I am a newbie and
> do not find how to do it in manuals.
> 
> I have two arrays, for example:
> 
> ar1 <- array(data=c(1:16),dim=c(4,4))
> ar2 <- array(data=c(1:16),dim=c(4,4))
> colnames(ar1)<-c("A","B","D","E")
> colnames(ar2)<-c("C","A","E","B")
> 
>> ar1
>      A B  D  E
> [1,] 1 5  9 13
> [2,] 2 6 10 14
> [3,] 3 7 11 15
> [4,] 4 8 12 16
>> ar2
>      C A  E  B
> [1,] 1 5  9 13
> [2,] 2 6 10 14
> [3,] 3 7 11 15
> [4,] 4 8 12 16
> 
> 
> I would like to join both arrays only for the columns present in both
> ar1 and ar2 (the intersection). I would like to obtain this:
>      A B  E
> [1,] 1 5 13
> [2,] 2 6 14
> [3,] 3 7 15
> [4,] 4 8 16
> [5,] 5 13  9
> [6,] 6 14 10
> [7,] 7 15 11
> [8,] 8 16 12
> 
> (rows 5-8 correspond to ar2)
> 
> I have tried several things but I am unable to accomplish it. Any
> experienced user could give me some advice, please?
> 
> I have another question: how can I sort the columns of an array
> according to its column names (for ar2, change CAEB to ABCE)?
> 
> Thanks in advance!
> Federico
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/-R--how-to-join-two-arrays-using-their-column-names-intersection-tf3141828.html#a8708210
Sent from the R help mailing list archive at Nabble.com.


From wl2776 at gmail.com  Tue Jan 30 13:07:52 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Tue, 30 Jan 2007 04:07:52 -0800 (PST)
Subject: [R] about sorting
In-Reply-To: <45BF2BB9.1060408@cnb.uam.es>
References: <45BF2BB9.1060408@cnb.uam.es>
Message-ID: <8708259.post@talk.nabble.com>


see ?order
ar2[,order(colnames(ar2))]


Federico Abascal wrote:
> 
> I have another question: how can I sort the columns of an array
> according to its column names (for ar2, change CAEB to ABCE)?
> 

-- 
View this message in context: http://www.nabble.com/-R--how-to-join-two-arrays-using-their-column-names-intersection-tf3141828.html#a8708259
Sent from the R help mailing list archive at Nabble.com.


From info at aghmed.fsnet.co.uk  Tue Jan 30 13:19:47 2007
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Tue, 30 Jan 2007 12:19:47 +0000
Subject: [R] How to find series of small numbers in a big vector?
In-Reply-To: <38661.37380.qm@web53801.mail.yahoo.com>
References: <38661.37380.qm@web53801.mail.yahoo.com>
Message-ID: <7.0.0.16.0.20070130121859.019a1b08@aghmed.fsnet.co.uk>

At 01:49 30/01/2007, Ed Holdgate wrote:

>Hello:
>
>I have a vector with 120,000 reals
>between 0.00000 and 0.9999
>
>They are not sorted but the vector index is the
>time-order of my measurements, and therefore
>cannot be lost.
>
>How do I use R to find the starting and ending
>index of ANY and ALL the "series" or "sequences"
>in that vector where ever there are 5 or more
>members in a row between 0.021 and 0.029 ?

You could look at rle which codes into runs


>For example:
>
>search_range <- c (0.021, 0.029) # inclusive searching
>search_length <- 5   # find ALL series of 5 members within search_range
>my_data <- c(0.900, 0.900, 0.900, 0.900, 0.900,
>              0.900, 0.900, 0.900, 0.900, 0.900,
>              0.900, 0.028, 0.024, 0.027, 0.023,
>              0.022, 0.900, 0.900, 0.900, 0.900,
>              0.900, 0.900, 0.024, 0.029, 0.023,
>              0.025, 0.026, 0.900, 0.900, 0.900,
>              0.900, 0.900, 0.900, 0.900, 0.900,
>              0.900, 0.900, 0.900, 0.900, 0.022,
>              0.023, 0.025, 0.333, 0.027, 0.028,
>              0.900, 0.900, 0.900, 0.900, 0.900)
>
>I seek the R program to report:
>start_index of 12 and an end_index of 16
>-- and also --
>start_index of 23 and an end_index of 27
>because that is were there happens to be
>search_length numbers within my search_range.
>
>It should _not_ report the series at start_index 40
>because that 0.333 in there violates the search_range.
>
>I could brute-force hard-code an R program, but
>perhaps an expert can give me a tip for an
>easy, elegant existing function or a tactic
>to approach?
>
>Execution speed or algorithm performance is not,
>for me in this case, important.  Rather, I
>seek an easy R solution to find the time windows
>(starting & ending indicies) where 5 or more
>small numbers in my search_range were measured
>all in a row.
>
>Advice welcome and many thanks in advance.
>
>Ed Holdgate

Michael Dewey
http://www.aghmed.fsnet.co.uk


From michellang at gmail.com  Tue Jan 30 13:22:43 2007
From: michellang at gmail.com (michel lang)
Date: Tue, 30 Jan 2007 13:22:43 +0100
Subject: [R] Permutation problem
Message-ID: <441a8bc70701300422n6734edcclbda634472f7adf1d@mail.gmail.com>

Dear R-Users,

I need a matrix containing line by line all possible permutations with
length 'i'  of the integers 1:N, given the restriction that the
integers in each row have to be in ascending order.

For example: N = 5, length i = 3, should result in a matrix like this:
1  2  3
1  2  4
1  2  5
1  3  4
1  3  5
1  4  5
2  3  4
2  3  5
2  4  5
3  4  5

I'm grateful for any advice on how to proceed,
  Michel Lang


From jholtman at gmail.com  Tue Jan 30 13:23:38 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 30 Jan 2007 07:23:38 -0500
Subject: [R] How to find series of small numbers in a big vector?
In-Reply-To: <1170158029.24658.34.camel@dutiih.st.ewi.tudelft.nl>
References: <38661.37380.qm@web53801.mail.yahoo.com>
	<1170158029.24658.34.camel@dutiih.st.ewi.tudelft.nl>
Message-ID: <644e1f320701300423r364c898ah6195f4395c0739f0@mail.gmail.com>

You can use 'rle'

> search_range <- c (0.021, 0.029) # inclusive searching
> search_length <- 5   # find ALL series of 5 members within search_range
> my_data <- c(0.900, 0.900, 0.900, 0.900, 0.900,
+             0.900, 0.900, 0.900, 0.900, 0.900,
+             0.900, 0.028, 0.024, 0.027, 0.023,
+             0.022, 0.900, 0.900, 0.900, 0.900,
+             0.900, 0.900, 0.024, 0.029, 0.023,
+             0.025, 0.026, 0.900, 0.900, 0.900,
+             0.900, 0.900, 0.900, 0.900, 0.900,
+             0.900, 0.900, 0.900, 0.900, 0.022,
+             0.023, 0.025, 0.333, 0.027, 0.028,
+             0.900, 0.900, 0.900, 0.900, 0.900)
> # create vector of values within range
> series <- (my_data >= search_range[1]) & (my_data <= search_range[2])
> # determine the 'runs'
> runs <- rle(series)
> # find runs that meet criteria
> long_runs <- which((runs$lengths >= search_length) & (runs$values))
> # create dataframe of indices
> series <- data.frame(start=cumsum(runs$lengths)[long_runs] - runs$lengths[long_runs] + 1,
+     end=cumsum(runs$lengths)[long_runs])
> series
  start end
1    12  16
2    23  27
>


On 1/30/07, Jonne Zutt <j.zutt at tudelft.nl> wrote:
> I suggest the following appraoch
>
> This gives TRUE for all data within the search_range
>        A1 = my_data > search_range[1] & my_data < search_range[2]
>
> which() gives us the indices
>        A2 = which(A1)
>
> and diff() the gaps between those intervals
>        A3 = diff(A2)
>
> Hence, if A3 > search_length, we have enough consecutive numbers within
> the search range
>
> Finally, this is what you wanted to know?
>
>        A2[ which(A3 > search_length) ]
>
>
> On Mon, 2007-01-29 at 17:49 -0800, Ed Holdgate wrote:
> > Hello:
> >
> > I have a vector with 120,000 reals
> > between 0.00000 and 0.9999
> >
> > They are not sorted but the vector index is the
> > time-order of my measurements, and therefore
> > cannot be lost.
> >
> > How do I use R to find the starting and ending
> > index of ANY and ALL the "series" or "sequences"
> > in that vector where ever there are 5 or more
> > members in a row between 0.021 and 0.029 ?
> >
> > For example:
> >
> > search_range <- c (0.021, 0.029) # inclusive searching
> > search_length <- 5   # find ALL series of 5 members within search_range
> > my_data <- c(0.900, 0.900, 0.900, 0.900, 0.900,
> >              0.900, 0.900, 0.900, 0.900, 0.900,
> >              0.900, 0.028, 0.024, 0.027, 0.023,
> >              0.022, 0.900, 0.900, 0.900, 0.900,
> >              0.900, 0.900, 0.024, 0.029, 0.023,
> >              0.025, 0.026, 0.900, 0.900, 0.900,
> >              0.900, 0.900, 0.900, 0.900, 0.900,
> >              0.900, 0.900, 0.900, 0.900, 0.022,
> >              0.023, 0.025, 0.333, 0.027, 0.028,
> >              0.900, 0.900, 0.900, 0.900, 0.900)
> >
> > I seek the R program to report:
> > start_index of 12 and an end_index of 16
> > -- and also --
> > start_index of 23 and an end_index of 27
> > because that is were there happens to be
> > search_length numbers within my search_range.
> >
> > It should _not_ report the series at start_index 40
> > because that 0.333 in there violates the search_range.
> >
> > I could brute-force hard-code an R program, but
> > perhaps an expert can give me a tip for an
> > easy, elegant existing function or a tactic
> > to approach?
> >
> > Execution speed or algorithm performance is not,
> > for me in this case, important.  Rather, I
> > seek an easy R solution to find the time windows
> > (starting & ending indicies) where 5 or more
> > small numbers in my search_range were measured
> > all in a row.
> >
> > Advice welcome and many thanks in advance.
> >
> > Ed Holdgate
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jholtman at gmail.com  Tue Jan 30 13:30:46 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 30 Jan 2007 07:30:46 -0500
Subject: [R] how to join two arrays using their column names intersection
In-Reply-To: <45BF325D.5070507@cnb.uam.es>
References: <45BF2BB9.1060408@cnb.uam.es> <45BF325D.5070507@cnb.uam.es>
Message-ID: <644e1f320701300430u389d382cj4e98b091654cbf53@mail.gmail.com>

> ar1 <- array(data=c(1:16),dim=c(4,4))
> ar2 <- array(data=c(1:16),dim=c(4,4))
> colnames(ar1)<-c("A","B","D","E")
> colnames(ar2)<-c("C","A","E","B")
>
> ar1
     A B  D  E
[1,] 1 5  9 13
[2,] 2 6 10 14
[3,] 3 7 11 15
[4,] 4 8 12 16
> ar2
     C A  E  B
[1,] 1 5  9 13
[2,] 2 6 10 14
[3,] 3 7 11 15
[4,] 4 8 12 16
> # get the common names between the matrices
> same <- intersect(colnames(ar1), colnames(ar2))
> same
[1] "A" "B" "E"
> # join them together
> rbind(ar1[,same], ar2[,same])
     A  B  E
[1,] 1  5 13
[2,] 2  6 14
[3,] 3  7 15
[4,] 4  8 16
[5,] 5 13  9
[6,] 6 14 10
[7,] 7 15 11
[8,] 8 16 12
>


On 1/30/07, Federico Abascal <fabascal at cnb.uam.es> wrote:
> I have found something that partially works. Just to illustrate what am
> I looking for:
>
> for(a in colnames(ar1)) {if(a %in% colnames(ar2)) { cat(a, "\t",
> ar1[,a],"\t",ar2[,a],"\n")}}
> A        1 2 3 4         5 6 7 8
> B        5 6 7 8         13 14 15 16
> E        13 14 15 16     9 10 11 12
>
> I think I can use something similar to obtain the desired results, but I
> am curious to know if there is a better way to do it.
> Federico
>
>
> Federico Abascal wrote:
> > Dear all,
> >
> > I have a problem that may be someone of you can help. I am a newbie and
> > do not find how to do it in manuals.
> >
> > I have two arrays, for example:
> >
> > ar1 <- array(data=c(1:16),dim=c(4,4))
> > ar2 <- array(data=c(1:16),dim=c(4,4))
> > colnames(ar1)<-c("A","B","D","E")
> > colnames(ar2)<-c("C","A","E","B")
> >
> >
> >> ar1
> >>
> >      A B  D  E
> > [1,] 1 5  9 13
> > [2,] 2 6 10 14
> > [3,] 3 7 11 15
> > [4,] 4 8 12 16
> >
> >> ar2
> >>
> >      C A  E  B
> > [1,] 1 5  9 13
> > [2,] 2 6 10 14
> > [3,] 3 7 11 15
> > [4,] 4 8 12 16
> >
> >
> > I would like to join both arrays only for the columns present in both
> > ar1 and ar2 (the intersection). I would like to obtain this:
> >      A B  E
> > [1,] 1 5 13
> > [2,] 2 6 14
> > [3,] 3 7 15
> > [4,] 4 8 16
> > [5,] 5 13  9
> > [6,] 6 14 10
> > [7,] 7 15 11
> > [8,] 8 16 12
> >
> > (rows 5-8 correspond to ar2)
> >
> > I have tried several things but I am unable to accomplish it. Any
> > experienced user could give me some advice, please?
> >
> > I have another question: how can I sort the columns of an array
> > according to its column names (for ar2, change CAEB to ABCE)?
> >
> > Thanks in advance!
> > Federico
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jholtman at gmail.com  Tue Jan 30 13:37:51 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 30 Jan 2007 07:37:51 -0500
Subject: [R] Permutation problem
In-Reply-To: <441a8bc70701300422n6734edcclbda634472f7adf1d@mail.gmail.com>
References: <441a8bc70701300422n6734edcclbda634472f7adf1d@mail.gmail.com>
Message-ID: <644e1f320701300437w4e548ba0ja1003a244aa87e0a@mail.gmail.com>

> help.search('combination')
> ?combn
> combn(1:5,3)
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    1    1    1    1    1    1    2    2    2     3
[2,]    2    2    2    3    3    4    3    3    4     4
[3,]    3    4    5    4    5    5    4    5    5     5
>

On 1/30/07, michel lang <michellang at gmail.com> wrote:
> Dear R-Users,
>
> I need a matrix containing line by line all possible permutations with
> length 'i'  of the integers 1:N, given the restriction that the
> integers in each row have to be in ascending order.
>
> For example: N = 5, length i = 3, should result in a matrix like this:
> 1  2  3
> 1  2  4
> 1  2  5
> 1  3  4
> 1  3  5
> 1  4  5
> 2  3  4
> 2  3  5
> 2  4  5
> 3  4  5
>
> I'm grateful for any advice on how to proceed,
>  Michel Lang
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From peter.robinson at charite.de  Tue Jan 30 14:03:25 2007
From: peter.robinson at charite.de (Peter Robinson)
Date: Tue, 30 Jan 2007 14:03:25 +0100
Subject: [R] Adding Scale to image
Message-ID: <45BF421D.7040103@charite.de>

Dear List,

I have used the image() function to show a heat plot of a matrix of data 
whose intensity  is color-coded. I have two questions that I have not 
been able to solve by using the help system or google.

1) How can one add a scale/legend that shows what numerical values a 
given color corresponds to?
2) How can one "flip" the rows (i.e., change the order of the rows in 
the image plot)?

Thanks very much in advance, Peter

-- 
Dr. med. Peter N. Robinson, MSc.
Institut f?r Medizinische Genetik
Universit?tsklinikum Charite
Humboldt-Universit?t
Augustenburger Platz 1
13353 Berlin
Germany
voice: 49-30-450569124
fax:   49-30-450569915
email: peter.robinson at charite.de
http://www.charite.de/ch/medgen/robinson


From r.hankin at noc.soton.ac.uk  Tue Jan 30 13:59:34 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 30 Jan 2007 12:59:34 +0000
Subject: [R] Permutation problem
In-Reply-To: <441a8bc70701300422n6734edcclbda634472f7adf1d@mail.gmail.com>
References: <441a8bc70701300422n6734edcclbda634472f7adf1d@mail.gmail.com>
Message-ID: <BCCC9044-FA22-4621-AFDD-F6B40D09870B@soc.soton.ac.uk>

Hi


 > library(gtools)
 > combinations(5,3)
       [,1] [,2] [,3]
[1,]    1    2    3
[2,]    1    2    4
[3,]    1    2    5
[4,]    1    3    4
[5,]    1    3    5
[6,]    1    4    5
[7,]    2    3    4
[8,]    2    3    5
[9,]    2    4    5
[10,]    3    4    5
 >






On 30 Jan 2007, at 12:22, michel lang wrote:

> Dear R-Users,
>
> I need a matrix containing line by line all possible permutations with
> length 'i'  of the integers 1:N, given the restriction that the
> integers in each row have to be in ascending order.
>
> For example: N = 5, length i = 3, should result in a matrix like this:
> 1  2  3
> 1  2  4
> 1  2  5
> 1  3  4
> 1  3  5
> 1  4  5
> 2  3  4
> 2  3  5
> 2  4  5
> 3  4  5
>
> I'm grateful for any advice on how to proceed,
>   Michel Lang
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From mcardeal at ufba.br  Tue Jan 30 14:03:43 2007
From: mcardeal at ufba.br (Mauricio Cardeal)
Date: Tue, 30 Jan 2007 10:03:43 -0300
Subject: [R] sequencial frequency table
Message-ID: <45BF422F.6010302@ufba.br>

Hi.

I have the data frame xyz below and I?d like to perform a sequencial 
frequency table using some unique function instead of making 3 different 
one-by-one table (table x, table y and table z). But I?can?t figure out 
the correct way.

x <- c(2,3,2,4)
y <- c(3,1,1,1)
z <- c(4,2,1,4)
xyz <- as.data.frame(cbind(x,y,z))

freq <- function(x){
table(x)
}

freq(xyz)

Thank you,
Mauricio


From wl2776 at gmail.com  Tue Jan 30 14:39:04 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Tue, 30 Jan 2007 05:39:04 -0800 (PST)
Subject: [R] Adding Scale to image
In-Reply-To: <45BF421D.7040103@charite.de>
References: <45BF421D.7040103@charite.de>
Message-ID: <8709150.post@talk.nabble.com>



Dr. med. Peter Robinson wrote:
> 
> I have used the image() function to show a heat plot of a matrix of data 
> whose intensity  is color-coded. I have two questions that I have not 
> been able to solve by using the help system or google.
> 
> 1) How can one add a scale/legend that shows what numerical values a 
> given color corresponds to?
> 
by plotting one more sequence of points
For example

layout(matrix(c(1,2)),heights=c(0.9,0.1))
image( blah blah blah )
plot.new()
plot.window(xlim=c(0,100),ylim=c(0,1))
points(x=0:100,y=rep(1,101),col=heat.colors(101),pch=15,cex=2)


Dr. med. Peter Robinson wrote:
> 
> 2) How can one "flip" the rows (i.e., change the order of the rows in 
> the image plot)?
> 

By flipping the source matrix, displayed with "image"

image(matrix.result[nrow(matrix.result):1,])
-- 
View this message in context: http://www.nabble.com/-R--Adding-Scale-to-image-tf3142258.html#a8709150
Sent from the R help mailing list archive at Nabble.com.


From dusa.adrian at gmail.com  Tue Jan 30 15:29:15 2007
From: dusa.adrian at gmail.com (Adrian DUSA)
Date: Tue, 30 Jan 2007 16:29:15 +0200
Subject: [R] jump in sequence
Message-ID: <be5487e70701300629y38728e4k65e47cff6da5e4cf@mail.gmail.com>

Dear list,

This should be a simple one, I just cannot see it.
I need to generate a sequence of the form:
4  5  6 13 14 15 22 23 24

That is: starting with 4, make a 3 numbers sequence, jump 6, then another 3
and so on.
I can create a whole vector with:
myvec <- rep(rep(c(F, T, F), rep(3, 3)), 3)

Then see which are TRUE:
which(myvec)
[1]  4  5  6 13 14 15 22 23 24


I'd like to avoid creating the whole vector if possible; for very large ones
it can be time consuming. There should be a way to only create the proper
indexes...

Thanks for any hint,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From epistat at gmail.com  Tue Jan 30 15:29:47 2007
From: epistat at gmail.com (zhijie zhang)
Date: Tue, 30 Jan 2007 22:29:47 +0800
Subject: [R] R and S-Plus got the different results of principal component
	analysis from SAS, why?
Message-ID: <2fc17e30701300629r209c683o5e0dd1c840a0278d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070130/66b25936/attachment.pl 

From P.Dalgaard at biostat.ku.dk  Tue Jan 30 15:38:52 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 30 Jan 2007 15:38:52 +0100
Subject: [R] jump in sequence
In-Reply-To: <be5487e70701300629y38728e4k65e47cff6da5e4cf@mail.gmail.com>
References: <be5487e70701300629y38728e4k65e47cff6da5e4cf@mail.gmail.com>
Message-ID: <45BF587C.5030406@biostat.ku.dk>

Adrian DUSA wrote:
> Dear list,
>
> This should be a simple one, I just cannot see it.
> I need to generate a sequence of the form:
> 4  5  6 13 14 15 22 23 24
>
> That is: starting with 4, make a 3 numbers sequence, jump 6, then another 3
> and so on.
> I can create a whole vector with:
> myvec <- rep(rep(c(F, T, F), rep(3, 3)), 3)
>
> Then see which are TRUE:
> which(myvec)
> [1]  4  5  6 13 14 15 22 23 24
>
>
> I'd like to avoid creating the whole vector if possible; for very large ones
> it can be time consuming. There should be a way to only create the proper
> indexes...
>
> Thanks for any hint,
> Adrian
>   
Is this it?

> as.vector(outer(0:2,seq(4,22,9),"+"))
[1]  4  5  6 13 14 15 22 23 24

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From bcarvalh at jhsph.edu  Tue Jan 30 15:43:01 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Tue, 30 Jan 2007 09:43:01 -0500
Subject: [R] jump in sequence
In-Reply-To: <be5487e70701300629y38728e4k65e47cff6da5e4cf@mail.gmail.com>
References: <be5487e70701300629y38728e4k65e47cff6da5e4cf@mail.gmail.com>
Message-ID: <5DE2BD72-AFCD-4286-9766-D0EBFDDC994D@jhsph.edu>

is it sth like:

as.integer(sapply(seq(4, 22, by=9), seq, length.out=3))

you're looking for?

b

On Jan 30, 2007, at 9:29 AM, Adrian DUSA wrote:

> Dear list,
>
> This should be a simple one, I just cannot see it.
> I need to generate a sequence of the form:
> 4  5  6 13 14 15 22 23 24
>
> That is: starting with 4, make a 3 numbers sequence, jump 6, then  
> another 3
> and so on.
> I can create a whole vector with:
> myvec <- rep(rep(c(F, T, F), rep(3, 3)), 3)
>
> Then see which are TRUE:
> which(myvec)
> [1]  4  5  6 13 14 15 22 23 24
>
>
> I'd like to avoid creating the whole vector if possible; for very  
> large ones
> it can be time consuming. There should be a way to only create the  
> proper
> indexes...
>
> Thanks for any hint,
> Adrian
>
> -- 
> Adrian Dusa
> Romanian Social Data Archive
> 1, Schitu Magureanu Bd
> 050025 Bucharest sector 5
> Romania
> Tel./Fax: +40 21 3126618 \
>           +40 21 3120210 / int.101
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From martin.becker at mx.uni-saarland.de  Tue Jan 30 15:46:11 2007
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Tue, 30 Jan 2007 15:46:11 +0100
Subject: [R] jump in sequence
In-Reply-To: <be5487e70701300629y38728e4k65e47cff6da5e4cf@mail.gmail.com>
References: <be5487e70701300629y38728e4k65e47cff6da5e4cf@mail.gmail.com>
Message-ID: <45BF5A33.4030802@mx.uni-saarland.de>

Adrian DUSA wrote:
> Dear list,
>
> This should be a simple one, I just cannot see it.
> I need to generate a sequence of the form:
> 4  5  6 13 14 15 22 23 24
>
> That is: starting with 4, make a 3 numbers sequence, jump 6, then another 3
> and so on.
> I can create a whole vector with:
> myvec <- rep(rep(c(F, T, F), rep(3, 3)), 3)
>
> Then see which are TRUE:
> which(myvec)
> [1]  4  5  6 13 14 15 22 23 24
>
>
> I'd like to avoid creating the whole vector if possible; for very large ones
> it can be time consuming. There should be a way to only create the proper
> indexes...
>
>   
Maybe:

n=10
3+rep(1:3,times=n)+9*rep(0:(n-1),each=3)

> Thanks for any hint,
> Adrian
>
>   

Regards,
  Martin


From dusa.adrian at gmail.com  Tue Jan 30 15:46:49 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Tue, 30 Jan 2007 16:46:49 +0200
Subject: [R] jump in sequence
In-Reply-To: <45BF587C.5030406@biostat.ku.dk>
References: <be5487e70701300629y38728e4k65e47cff6da5e4cf@mail.gmail.com>
	<45BF587C.5030406@biostat.ku.dk>
Message-ID: <200701301646.49352.dusa.adrian@gmail.com>

On Tuesday 30 January 2007 16:38, Peter Dalgaard wrote:
> >[...snip...]
> Is this it?
>
> > as.vector(outer(0:2,seq(4,22,9),"+"))
>
> [1]  4  5  6 13 14 15 22 23 24

Indeed it is :))
Thanks,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From P.Dalgaard at biostat.ku.dk  Tue Jan 30 15:47:26 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 30 Jan 2007 15:47:26 +0100
Subject: [R] R and S-Plus got the different results of principal
 component analysis from SAS, why?
In-Reply-To: <2fc17e30701300629r209c683o5e0dd1c840a0278d@mail.gmail.com>
References: <2fc17e30701300629r209c683o5e0dd1c840a0278d@mail.gmail.com>
Message-ID: <45BF5A7E.90602@biostat.ku.dk>

zhijie zhang wrote:
> Dear Rusers,
>
>   I have met a difficult problem on explaining the differences of principal
> component analysis(PCA) between R,S-PLUS and SAS/STATA/SPSS, which wasn't
> met before.
>
>   Althought they have got the same eigenvalues, their coeffiecients were
> different.
>
>   First, I list my results from R,S-PLUS and SAS/STATA/SPSS, and then show
> the original dataset, hoping sb. to try and explain it.
>
>   SAS,STATA,and SPSS have the same results, so i put them together. From
> their results, we see that the absolute values of coeffiecient are same, but
> PC1,PC2,PC4,PC5 and PC6 in R have the opposite sign on the coeffiecnts
> contrast with SAS, and PC4,PC5 in S-PLUS have the opposite sign on the
> coeffiecnts contrast with SAS. Curiously, I got the same results amont all
> these software using my another dataset.
>
> *
Principal components are only *defined* up to sign changes (as the help
page for prcomp says quite explicitly!!!!)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From marc_schwartz at comcast.net  Tue Jan 30 15:48:44 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 30 Jan 2007 08:48:44 -0600
Subject: [R] sequencial frequency table
In-Reply-To: <45BF422F.6010302@ufba.br>
References: <45BF422F.6010302@ufba.br>
Message-ID: <1170168524.5001.125.camel@localhost.localdomain>

On Tue, 2007-01-30 at 10:03 -0300, Mauricio Cardeal wrote:
> Hi.
> 
> I have the data frame xyz below and I?d like to perform a sequencial 
> frequency table using some unique function instead of making 3 different 
> one-by-one table (table x, table y and table z). But I?can?t figure out 
> the correct way.
> 
> x <- c(2,3,2,4)
> y <- c(3,1,1,1)
> z <- c(4,2,1,4)
> xyz <- as.data.frame(cbind(x,y,z))
> 
> freq <- function(x){
> table(x)
> }
> 
> freq(xyz)
> 
> Thank you,
> Mauricio

Is that what you want?

> lapply(xyz, table)
$x

2 3 4 
2 1 1 

$y

1 3 
3 1 

$z

1 2 4 
1 1 2 


See ?lapply if so.


HTH,

Marc Schwartz


From r.hankin at noc.soton.ac.uk  Tue Jan 30 16:05:18 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 30 Jan 2007 15:05:18 +0000
Subject: [R] jump in sequence
In-Reply-To: <be5487e70701300629y38728e4k65e47cff6da5e4cf@mail.gmail.com>
References: <be5487e70701300629y38728e4k65e47cff6da5e4cf@mail.gmail.com>
Message-ID: <D3AB681B-72E1-4605-A259-B6FBF9A9A53F@soc.soton.ac.uk>


 > f <- function(n){as.vector(sweep(matrix(4:6,nrow=3,ncol=n),2,seq 
(from=0,by=9,len=n),"+"))}
 > f(10)
[1]  4  5  6 13 14 15 22 23 24 31 32 33 40 41 42 49 50 51 58 59 60 67  
68 69 76 77 78 85 86 87
 >

HTH

rksh


On 30 Jan 2007, at 14:29, Adrian DUSA wrote:

> Dear list,
>
> This should be a simple one, I just cannot see it.
> I need to generate a sequence of the form:
> 4  5  6 13 14 15 22 23 24
>
> That is: starting with 4, make a 3 numbers sequence, jump 6, then  
> another 3
> and so on.
> I can create a whole vector with:
> myvec <- rep(rep(c(F, T, F), rep(3, 3)), 3)
>
> Then see which are TRUE:
> which(myvec)
> [1]  4  5  6 13 14 15 22 23 24
>
>
> I'd like to avoid creating the whole vector if possible; for very  
> large ones
> it can be time consuming. There should be a way to only create the  
> proper
> indexes...
>
> Thanks for any hint,
> Adrian
>
> -- 
> Adrian Dusa
> Romanian Social Data Archive
> 1, Schitu Magureanu Bd
> 050025 Bucharest sector 5
> Romania
> Tel./Fax: +40 21 3126618 \
>           +40 21 3120210 / int.101
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From gavin.simpson at ucl.ac.uk  Tue Jan 30 16:08:28 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 30 Jan 2007 15:08:28 +0000
Subject: [R] R and S-Plus got the different results of
	principal	component analysis from SAS, why?
In-Reply-To: <2fc17e30701300629r209c683o5e0dd1c840a0278d@mail.gmail.com>
References: <2fc17e30701300629r209c683o5e0dd1c840a0278d@mail.gmail.com>
Message-ID: <1170169708.9629.6.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2007-01-30 at 22:29 +0800, zhijie zhang wrote:
> Dear Rusers,
> 
>   I have met a difficult problem on explaining the differences of principal
> component analysis(PCA) between R,S-PLUS and SAS/STATA/SPSS, which wasn't
> met before.
> 
>   Althought they have got the same eigenvalues, their coeffiecients were
> different.

Only up to rounding of printed results and their signs, which are
arbitrary. The latter is covered in both the help pages for the two main
PCA functions in R. Please read the Notes section of ?prcomp
and/or ?princomp (you don't say which you used in R).

G

> 
>   First, I list my results from R,S-PLUS and SAS/STATA/SPSS, and then show
> the original dataset, hoping sb. to try and explain it.
> 
>   SAS,STATA,and SPSS have the same results, so i put them together. From
> their results, we see that the absolute values of coeffiecient are same, but
> PC1,PC2,PC4,PC5 and PC6 in R have the opposite sign on the coeffiecnts
> contrast with SAS, and PC4,PC5 in S-PLUS have the opposite sign on the
> coeffiecnts contrast with SAS. Curiously, I got the same results amont all
> these software using my another dataset.
> 
> *R's results of PCA:*
> 
>       *PC1*         *PC2*          PC3        *PC4*        *PC5 *        *
> PC6*
> 
> X1 -0.5152569  0.20264489 -0.2338786  0.2350876 -0.2033335 -0.736298528
> 
> X2 -0.5197856  0.08989351 -0.2068260  0.3737667 -0.3187746  0.661548469
> 
> X3 -0.5148033  0.15820613 -0.0590627 -0.3210113  0.7693052  0.107616466
> 
> X4 -0.3535798  0.08105168  0.7317188 -0.4350752 -0.3790772  0.003088541
> 
> X5 -0.1868691 -0.67517084 -0.4397442 -0.5119015 -0.2314833 -0.014886524
> 
> X6 -0.1984241 -0.68073489  0.4126112  0.5006500  0.2606219 -0.091682326
> 
> 
> 
> pca<-read.csv('D:\pca.csv',sep=',',header=T)
> 
> attach(pca)
> 
> pcacomp <- prcomp(pca[,-1], retx=TRUE, center=TRUE,scale.= TRUE,tol=0.0001)
> 
> 
> 
> *S-Plus's results of PCA:*
> 
>      pc1     pc2      pc3     *pc4    pc5*     pc6
> 
> X1  0.5153 -0.2026 -0.2339  0.2351 -0.2033  0.7363
> 
> X2  0.5198 -0.0899 -0.2068  0.3738 -0.3188 -0.6615
> 
> X3  0.5148 -0.1582 -0.0591 -0.3210  0.7693 -0.1076
> 
> X4  0.3536 -0.0811  0.7317 -0.4351 -0.3791 -0.0031
> 
> X5  0.1869  0.6752 -0.4397 -0.5119 -0.2315  0.0149
> 
> X6  0.1984  0.6807  0.4126  0.5007  0.2606  0.0917
> 
> 
> 
> *SAS/STATA/SPSS's results of PCA:*
> 
>      PC1       PC2     PC3      PC4      PC5       PC6
> 
> X1 0.515257 -.202645 -.233879 -.235088 0.203334 0.736299
> 
> X2 0.519786 -.089894 -.206826 -.373767 0.318775 -.661548
> 
> X3 0.514803 -.158206 -.059063 0.321011 -.769305 -.107616
> 
> X4 0.353580 -.081052 0.731719 0.435075 0.379077 -.003089
> 
> X5 0.186869 0.675171 -.439744 0.511902 0.231483 0.014887
> 
> X6 0.198424 0.680735 0.412611 -.500650 -.260622 0.091682
> 
> 
> 
> My dataset used in the above results is :
> 
>    X1
> 
> X2
> 
> X3
> 
> X4
> 
> X5
> 
> X6
> 
> 173.28
> 
> 93.62
> 
> 60.1
> 
> 86.72
> 
> 38.97
> 
> 27.51
> 
> 172.09
> 
> 92.83
> 
> 60.38
> 
> 87.39
> 
> 38.62
> 
> 27.82
> 
> 171.46
> 
> 92.73
> 
> 59.74
> 
> 85.59
> 
> 38.83
> 
> 27.46
> 
> 170.08
> 
> 92.25
> 
> 58.04
> 
> 85.92
> 
> 38.33
> 
> 27.29
> 
> 170.61
> 
> 92.36
> 
> 59.67
> 
> 87.46
> 
> 38.38
> 
> 27.14
> 
> 171.69
> 
> 92.85
> 
> 59.44
> 
> 87.45
> 
> 38.19
> 
> 27.1
> 
> 171.46
> 
> 92.93
> 
> 58.7
> 
> 87.06
> 
> 38.58
> 
> 27.36
> 
> 171.6
> 
> 93.28
> 
> 59.75
> 
> 88.03
> 
> 38.68
> 
> 27.22
> 
> 171.6
> 
> 92.26
> 
> 60.5
> 
> 87.63
> 
> 38.79
> 
> 26.63
> 
> 171.16
> 
> 92.62
> 
> 58.72
> 
> 87.11
> 
> 38.19
> 
> 27.18
> 
> 170.04
> 
> 92.17
> 
> 56.95
> 
> 88.08
> 
> 38.24
> 
> 27.65
> 
> 170.27
> 
> 91.94
> 
> 56
> 
> 84.52
> 
> 37.16
> 
> 26.81
> 
> 170.61
> 
> 92.5
> 
> 57.34
> 
> 85.61
> 
> 38.52
> 
> 27.36
> 
> 171.39
> 
> 92.44
> 
> 58.92
> 
> 85.37
> 
> 38.83
> 
> 26.47
> 
> 171.83
> 
> 92.79
> 
> 56.85
> 
> 85.35
> 
> 38.58
> 
> 27.03
> 
> 171.36
> 
> 92.53
> 
> 58.39
> 
> 87.09
> 
> 38.23
> 
> 27.04
> 
> 171.24
> 
> 92.61
> 
> 57.69
> 
> 83.98
> 
> 39.04
> 
> 27.07
> 
> 170.49
> 
> 92.03
> 
> 57.56
> 
> 87.18
> 
> 38.54
> 
> 27.57
> 
> 169.43
> 
> 91.67
> 
> 55.22
> 
> 83.87
> 
> 38.41
> 
> 26.6
> 
> 168.57
> 
> 91.4
> 
> 55.96
> 
> 83.02
> 
> 38.74
> 
> 26.97
> 
> 170.43
> 
> 92.38
> 
> 57.87
> 
> 84.87
> 
> 38.78
> 
> 27.37
> 
> 169.88
> 
> 91.89
> 
> 56.87
> 
> 86.34
> 
> 38.37
> 
> 27.19
> 
> 167.94
> 
> 90.91
> 
> 55.97
> 
> 86.77
> 
> 38.17
> 
> 27.16
> 
> 168.82
> 
> 91.3
> 
> 56.07
> 
> 85.87
> 
> 37.61
> 
> 26.67
> 
> 168.02
> 
> 91.26
> 
> 55.28
> 
> 85.63
> 
> 39.66
> 
> 28.07
> 
> 167.87
> 
> 90.96
> 
> 55.79
> 
> 84.92
> 
> 38.2
> 
> 26.53
> 
> 168.15
> 
> 91.5
> 
> 54.56
> 
> 84.81
> 
> 38.44
> 
> 27.38
> 
> 168.99
> 
> 91.52
> 
> 55.11
> 
> 86.23
> 
> 38.3
> 
> 27.11
> 
>  Any help or suggestions are greatly appreciated.
> 
> 
> -- 
> With Kind Regards,
> 
> oooO:::::::::
> (..):::::::::
> :\.(:::Oooo::
> ::\_)::(..)::
> :::::::)./:::
> ::::::(_/::::
> :::::::::::::
> [***********************************************************************]
> Zhi Jie,Zhang ,PHD
> Tel:86-21-54237149   epistat at gmail.com
> Dept. of Epidemiology,school of public health,Fudan University
> Address:No. 138 Yi Xue Yuan Road,Shanghai,China
> Postcode:200032
> [***********************************************************************]
> oooO:::::::::
> (..):::::::::
> :\.(:::Oooo::
> ::\_)::(..)::
> :::::::)./:::
> ::::::(_/::::
> :::::::::::::
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From jiho.han at yahoo.com  Tue Jan 30 16:11:52 2007
From: jiho.han at yahoo.com (jiho.han)
Date: Tue, 30 Jan 2007 07:11:52 -0800 (PST)
Subject: [R] rbind-ing list
In-Reply-To: <8703373.post@talk.nabble.com>
References: <8703373.post@talk.nabble.com>
Message-ID: <8709194.post@talk.nabble.com>


thanks for all those who added great comments (and solutions) to my problem!!
thanks-

jh



jiho.han wrote:
> 
> hi, 
> 
> i have a list of data.frame that has same structure. i would like to know
> a efficient way of rbind-ing it. 
> right now, i write:
> 
> 
> n = length(temp)     # 'temp' is a list of data.frames
> temp2 = data.frame()
> for (i in 1:n) temp2 = rbind( temp2, temp[[i]])
> return(temp2)
> 
> 
> but this is not an efficient way since we keeping overwriting temp2. i
> wonder if there's faster way. 
> 
> thanks
> 

-- 
View this message in context: http://www.nabble.com/rbind-ing-list-tf3140137.html#a8709194
Sent from the R help mailing list archive at Nabble.com.


From shubhak at ambaresearch.com  Tue Jan 30 16:25:11 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Tue, 30 Jan 2007 20:55:11 +0530
Subject: [R] R packages
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3EB96B9@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070130/e981d007/attachment.pl 

From osklyar at ebi.ac.uk  Tue Jan 30 16:27:30 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Tue, 30 Jan 2007 15:27:30 +0000
Subject: [R] Step-by-step guide for using C/C++ in R;
	WAS: [Rd] Speed of for loops
In-Reply-To: <20070130144606.GA3241@pu100877.student.princeton.edu>
References: <op.tmyqilgbffxks9@norisk> <45BF36E1.7050200@ebi.ac.uk>
	<20070130144606.GA3241@pu100877.student.princeton.edu>
Message-ID: <45BF63E2.3080804@ebi.ac.uk>

I know this should not go to [Rd], but the original post was there and 
the replies as well.

Thank you all who expressed interest in the "Step-by-step guide for 
using C/C++ in R"! Answering some of you, yes it is by me and was 
written to assist other group members to start adding c/c++ code to 
their R coding.

You can now download it from:

http://www.ebi.ac.uk/~osklyar/kb/CtoRinterfacingPrimer.pdf

I would also appreciate your comments if you find it useful or not, or 
maybe what can be added or modified. But not on the list, directly to my 
email please.

Best wishes,
Oleg

Tamas K Papp wrote:
> On Tue, Jan 30, 2007 at 12:15:29PM +0000, Oleg Sklyar wrote:
> 
>> magnitude using c-functions for "complex" vector indexing operations. If 
>> you need instructions, I can send you a very nice "Step-by-step guide 
>> for using C/C++ in R" which goes beyond "Writing R Extensions" document.
> 
> Hi Oleg,
> 
> Can you please post this guide online?  I think that many people would
> be interested in reading it, incl. me.
> 
> Tamas

-- 
Dr Oleg Sklyar * EBI/EMBL, Cambridge CB10 1SD, England * +44-1223-494466


From BEN at SSANET.COM  Tue Jan 30 16:38:28 2007
From: BEN at SSANET.COM (Ben Fairbank)
Date: Tue, 30 Jan 2007 09:38:28 -0600
Subject: [R] jump in sequence
References: <be5487e70701300629y38728e4k65e47cff6da5e4cf@mail.gmail.com>
Message-ID: <CA612484A337C6479EA341DF9EEE14AC05DAD7BB@hercules.ssainfo>

Or perhaps

rep(4:6,3)+9*rep(0:2,rep(3,3)) 

with changes as necessary for longer sequences

Ben Fairbank


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Adrian DUSA
Sent: Tuesday, January 30, 2007 8:29 AM
To: r-help at stat.math.ethz.ch
Subject: [R] jump in sequence

Dear list,

This should be a simple one, I just cannot see it.
I need to generate a sequence of the form:
4  5  6 13 14 15 22 23 24

That is: starting with 4, make a 3 numbers sequence, jump 6, then
another 3
and so on.
I can create a whole vector with:
myvec <- rep(rep(c(F, T, F), rep(3, 3)), 3)

Then see which are TRUE:
which(myvec)
[1]  4  5  6 13 14 15 22 23 24


I'd like to avoid creating the whole vector if possible; for very large
ones
it can be time consuming. There should be a way to only create the
proper
indexes...

Thanks for any hint,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From btyner at stat.purdue.edu  Tue Jan 30 16:54:52 2007
From: btyner at stat.purdue.edu (Benjamin Tyner)
Date: Tue, 30 Jan 2007 10:54:52 -0500
Subject: [R] lattice: two grouping variables, one controls 'col',
 the other 'pch'
In-Reply-To: <971536df0701282118p2b198716u46d12626b6109149@mail.gmail.com>
References: <45BD7AE6.3080309@stat.purdue.edu>
	<971536df0701282118p2b198716u46d12626b6109149@mail.gmail.com>
Message-ID: <45BF6A4C.6040402@stat.purdue.edu>

Thanks, that does the trick. I guess with two grouping variables it is 
best to bypass 'groups' entirely.'

Ben

Gabor Grothendieck wrote:
> Try:
>
> xyplot(y ~ x | f, pch = g1, col = g2, panel = function(x, y,
> subscripts, ..., pch, col)
>      panel.xyplot(x, y, ..., col = col[subscripts], pch = 
> pch[subscripts])
> )
>
>
> On 1/28/07, Benjamin Tyner <btyner at stat.purdue.edu> wrote:
>> Say I have
>>
>> library(lattice)
>>
>> x<-runif(256)
>> y<-runif(256)
>> f<-gl(16,16)
>> g1<-rep(1:4,each=64)
>> g2<-rep(1:4,times=64)
>>
>> plot<-xyplot(y~x|f,
>>             groups=g1,
>>             pch=as.character(1:4),
>>             panel=function(x,y,subscripts,groups,...){
>>                   panel.superpose(x,y,subscripts,groups,...)
>>             })
>>
>> print(plot)
>>
>> Currently, both color and plotting symbol change with the grouping
>> variable g1. What is the best way to have color change with g1, but
>> plotting symbol change with g2 (or vice versa)?
>>
>> Thanks,
>> Ben
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


From btyner at stat.purdue.edu  Tue Jan 30 17:09:01 2007
From: btyner at stat.purdue.edu (Benjamin Tyner)
Date: Tue, 30 Jan 2007 11:09:01 -0500
Subject: [R] lattice: how to get 'lattice.theme'
Message-ID: <45BF6D9D.9070204@stat.purdue.edu>

I'm using lattice 0.14. As of version 0.5 the Changes says there is a 
global list called 'lattice.theme'. How can I access this? I have tried 
many ways, including

options(lattice.theme)
lattice.getOption("lattice.theme")
get("lattice.theme", envir = .LatticeEnv)
getFromNamespace("lattice.theme", "lattice")
getAnywhere("lattice.theme")

(both before and after a call to trellis.device)

Thanks,
Ben


From ggrothendieck at gmail.com  Tue Jan 30 17:27:59 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 30 Jan 2007 11:27:59 -0500
Subject: [R] lattice: how to get 'lattice.theme'
In-Reply-To: <45BF6D9D.9070204@stat.purdue.edu>
References: <45BF6D9D.9070204@stat.purdue.edu>
Message-ID: <971536df0701300827q3ecaee9axc5f6387baa6c8ad0@mail.gmail.com>

Try:

lattice.theme <- get("lattice.theme", envir = lattice:::.LatticeEnv)



On 1/30/07, Benjamin Tyner <btyner at stat.purdue.edu> wrote:
> I'm using lattice 0.14. As of version 0.5 the Changes says there is a
> global list called 'lattice.theme'. How can I access this? I have tried
> many ways, including
>
> options(lattice.theme)
> lattice.getOption("lattice.theme")
> get("lattice.theme", envir = .LatticeEnv)
> getFromNamespace("lattice.theme", "lattice")
> getAnywhere("lattice.theme")
>
> (both before and after a call to trellis.device)
>
> Thanks,
> Ben
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From btyner at stat.purdue.edu  Tue Jan 30 17:40:44 2007
From: btyner at stat.purdue.edu (Benjamin Tyner)
Date: Tue, 30 Jan 2007 11:40:44 -0500
Subject: [R] lattice: how to get 'lattice.theme'
References: 45BF6D9D.9070204@stat.purdue.edu
Message-ID: <45BF750C.8020802@stat.purdue.edu>

Thanks...what I really want is to be able to access the value of the 
'color' argument the user gave in their call to trellis.device(). I 
mistakenly assumed it would be stored in 'lattice.theme' but this does 
not seem to be the case?

Ben


From ggrothendieck at gmail.com  Tue Jan 30 17:51:22 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 30 Jan 2007 11:51:22 -0500
Subject: [R] lattice: how to get 'lattice.theme'
In-Reply-To: <45BF750C.8020802@stat.purdue.edu>
References: <45BF750C.8020802@stat.purdue.edu>
Message-ID: <971536df0701300851t644f5a43pe954b899c27af618@mail.gmail.com>

I think there is an entry for each device in lattice.theme and the
color would be in the device's entry.  Check the source of
trellis.device to be sure.

On 1/30/07, Benjamin Tyner <btyner at stat.purdue.edu> wrote:
> Thanks...what I really want is to be able to access the value of the
> 'color' argument the user gave in their call to trellis.device(). I
> mistakenly assumed it would be stored in 'lattice.theme' but this does
> not seem to be the case?
>
> Ben


From muenchen at utk.edu  Tue Jan 30 17:59:10 2007
From: muenchen at utk.edu (Muenchen, Robert A (Bob))
Date: Tue, 30 Jan 2007 11:59:10 -0500
Subject: [R] spss.get.  Warning with SPSS 14 dataset
In-Reply-To: <62400.73161.qm@web32808.mail.mud.yahoo.com>
References: <62400.73161.qm@web32808.mail.mud.yahoo.com>
Message-ID: <7270AEC73132194E8BC0EE06B35D93D86D3EF2@UTKFSVS3.utk.tennessee.edu>

Here's a warning about that:
http://tolstoy.newcastle.edu.au/R/help/04/12/8827.html 

Bob

=========================================================
  Bob Muenchen (pronounced Min'-chen), Manager  
  Statistical Consulting Center
  U of TN Office of Information Technology
  200 Stokely Management Center, Knoxville, TN 37996-0520
  Voice: (865) 974-5230  
  FAX:   (865) 974-4810
  Email: muenchen at utk.edu
  Web:   http://oit.utk.edu/scc, 
  News:  http://listserv.utk.edu/archives/statnews.html
=========================================================


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of John Kane
> Sent: Friday, January 26, 2007 11:49 AM
> To: R R-help
> Subject: [R] spss.get. Warning with SPSS 14 dataset
> 
> I am using spss.get to import an SPSS database
> "Data.sav", created with SPSS 14  :
> 
> df1 <- spss.get("C:/temp/Data.sav" , lowernames=TRUE,
>             datevars = c("dateinte"))
> 
> I am getting this warning. I get the same warning with
> read.spss.
> 
> Warning message:
> C:/temp/Data.sav: Unrecognized record type 7, subtype
> 16 encountered in system file
> 
> This is a stupid question but should I be worried
> about it?  So far the data looks clean but it is not
> my data base originally and I wondered if there is
> anything specific that I should be checking for.
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From EULOTTERYONLINE.ORG at faria-instruments.ru  Tue Jan 30 18:55:00 2007
From: EULOTTERYONLINE.ORG at faria-instruments.ru (Lottery Coordinator.)
Date: Tue, 30 Jan 2007 20:55:00 +0300
Subject: [R] FINAL WINNING NOTIFICATION...CONTACT THE CLAIMS AGENT
Message-ID: <E1HBxC8-000M8o-9Z@www.faria-instruments.ru>

FROM THE DESK OF THE DIRECTOR INTERNATIONAL PRIZE AWARD DEPT.
Attn Lucky Winner, 
WINNING NOTIFICATION FOR CATEGORY "A" WINNER ONLY 
We are pleased to inform you of the result of the last final annual 
draw of our Lottery International Programs. 
The online cyber lotto draws was conducted from an exclusive list of 
25,000,000 e-mail addresses of individual and corporate bodies picked by 
an advanced automated random computer search from theinternet. No 
tickets were sold.
CONGRATULATIONS!!!
After this automated computer ballot, your e-mail address emerged as a 
winner in the category "A" with the following numbers attached Ref 
Number: PW 9590 ES 9414,Batch Number: 573881545-NL/2006 and Ticket Number: PP 3502 /8707-01
You are therefore to receive a cash prize of $2.5million (Two Million 
Five Hundred Thousand United States Dollars) from the total payout 
CONGRATULATIONS!!! 
Your prize award has been insured with your e-mail address and will be 
transferred to you upon meeting our requirements, statutory obligations, verifications, validations and satisfactory report.
To file in for the processing of your prize winnings, you are advised 
to contact our Certified and Accredited claims agent for category "A" 
winners with the information below:
************************************* 
Name:  Bernard PEIJS
Phone: +0031 616 941 146
Email: awards_007 at yahoo.de 
*************************************
You are advice to provide him with the following information: 
Names:
Telephone/Fax number:
Nationality:
Age:
NOTE: All winnings must be claimed not later than 14 days, thereafter 
unclaimed funds would be included in the next stake. Remember to quote 
your reference information in all correspondence.You are to keep all 
lotto information confidential, especially your reference and ticket 
numbers. 
(This is important as a case of double claims will not be entertained).Members of the affiliate agencies are automatically not allowed to participate in this program.
Furthermore, should there be any change of address do inform our agent 
as soon as possible.
Congratulations once more from our members of staff and thank you for 
being part of our promotional program.
Yours Faithfully,
Walter Jones.
Lottery Coordinator.
Thank you and congratulations!!!
This email may contain information which is confidential and/or privileged. The information is intended solely for the use of the individual or entity named above. If you are not the intended recipient, be aware that any disclosure, copying, distribution or use of  the contents is prohibited. If you have received this electronic transmission in error,please notify the sender by telephone or return email and delete the material from your computer.


From EULOTTERYONLINE.ORG at faria-instruments.ru  Tue Jan 30 18:55:00 2007
From: EULOTTERYONLINE.ORG at faria-instruments.ru (Lottery Coordinator.)
Date: Tue, 30 Jan 2007 20:55:00 +0300
Subject: [R] FINAL WINNING NOTIFICATION...CONTACT THE CLAIMS AGENT
Message-ID: <E1HBxC8-000M8o-9Z@www.faria-instruments.ru>

FROM THE DESK OF THE DIRECTOR INTERNATIONAL PRIZE AWARD DEPT.
Attn Lucky Winner, 
WINNING NOTIFICATION FOR CATEGORY "A" WINNER ONLY 
We are pleased to inform you of the result of the last final annual 
draw of our Lottery International Programs. 
The online cyber lotto draws was conducted from an exclusive list of 
25,000,000 e-mail addresses of individual and corporate bodies picked by 
an advanced automated random computer search from theinternet. No 
tickets were sold.
CONGRATULATIONS!!!
After this automated computer ballot, your e-mail address emerged as a 
winner in the category "A" with the following numbers attached Ref 
Number: PW 9590 ES 9414,Batch Number: 573881545-NL/2006 and Ticket Number: PP 3502 /8707-01
You are therefore to receive a cash prize of $2.5million (Two Million 
Five Hundred Thousand United States Dollars) from the total payout 
CONGRATULATIONS!!! 
Your prize award has been insured with your e-mail address and will be 
transferred to you upon meeting our requirements, statutory obligations, verifications, validations and satisfactory report.
To file in for the processing of your prize winnings, you are advised 
to contact our Certified and Accredited claims agent for category "A" 
winners with the information below:
************************************* 
Name:  Bernard PEIJS
Phone: +0031 616 941 146
Email: awards_007 at yahoo.de 
*************************************
You are advice to provide him with the following information: 
Names:
Telephone/Fax number:
Nationality:
Age:
NOTE: All winnings must be claimed not later than 14 days, thereafter 
unclaimed funds would be included in the next stake. Remember to quote 
your reference information in all correspondence.You are to keep all 
lotto information confidential, especially your reference and ticket 
numbers. 
(This is important as a case of double claims will not be entertained).Members of the affiliate agencies are automatically not allowed to participate in this program.
Furthermore, should there be any change of address do inform our agent 
as soon as possible.
Congratulations once more from our members of staff and thank you for 
being part of our promotional program.
Yours Faithfully,
Walter Jones.
Lottery Coordinator.
Thank you and congratulations!!!
This email may contain information which is confidential and/or privileged. The information is intended solely for the use of the individual or entity named above. If you are not the intended recipient, be aware that any disclosure, copying, distribution or use of  the contents is prohibited. If you have received this electronic transmission in error,please notify the sender by telephone or return email and delete the material from your computer.


From jlampur at eagrof.UdL.es  Tue Jan 30 19:27:23 2007
From: jlampur at eagrof.UdL.es (Jorge Lampurlanes Castel)
Date: Tue, 30 Jan 2007 19:27:23 +0100 (CET)
Subject: [R] Multiple comparisons when interacction
In-Reply-To: <3238.217.124.30.115.1170181507.squirrel@correu.udl.es>
References: <20070129112547.BTD33671@po-d.temple.edu>
	<3238.217.124.30.115.1170181507.squirrel@correu.udl.es>
Message-ID: <3262.217.124.30.115.1170181643.squirrel@correu.udl.es>

As I understand from the WoodEnergy example in package HH, You are
proposing to compute a separate lm for each level of YEAR factor to
compare TIL means.
This is the way I used to do this kind of analysis.

But now, it is also possible, with PROC GLM, to adjust only the general
model (variable ~ BLOC + TIL * YEAR ) and then to compare TIL means inside
every YEAR with the SLICE option.

I am not statistician but I think the difference is that, in the first
approach, we use a different error term for every YEAR, whereas in the
second approach, we use the same error term for all comparisons.

Perhaps the differences between both approaches are negligible. If not, it
is possible to do this analysis in R?

Thanks a lot.

-- 
**************************************************
Jorge Lampurlan?s Castel
Departament d'Enginyeria Agroforestal
Escola T?cnica Superior d'Enginyeria Agr?ria
Universitat de Lleida
Avinguda Rovira Roure, 191
25198-LLEIDA
SPAIN

Tl.: +34 973 70 25 37
Fax.:+34 073 70 26 73
e-mail: jlampur at eagrof.udl.es


From england at cs.umn.edu  Tue Jan 30 20:32:07 2007
From: england at cs.umn.edu (Darin A. England)
Date: Tue, 30 Jan 2007 13:32:07 -0600
Subject: [R] comparing random forests and classification trees
In-Reply-To: <005101c7433d$4bab92f0$7128d983@geol.utas.edu.au>
References: <005101c7433d$4bab92f0$7128d983@geol.utas.edu.au>
Message-ID: <20070130193206.GA5553@cs.umn.edu>

Amy,

I have also had this issue with randomForest, that is, you lose the
ability to explain the classifier in a simple way to
non-specialists (everyone can understand the single decision tree.)
As far as comparing the accuracy of the two, I think that you are
correct in comparing them by the actual vs predicted tables.
randomForest reports this as the confusion matrix, and it also
reports the out-of-bag error, which I think you are referring to. I
would not compare the rf out-of-bag error with the rpart relative
error (or cross-validated error if you are doing cross validation.)

So, for what it's worth I think you are correct. Also, do you know
about ctree in the "party" package? If you want to retain the
explanatory power of a single tree and have a nice accurate
classifier, I have found ctree to work quite well.

HTH,

Darin

On Mon, Jan 29, 2007 at 11:34:51AM +1100, Amy Koch wrote:
> Hi,
> 
> I have done an analysis using 'rpart' to construct a Classification Tree. I
> am wanting to retain the output in tree form so that it is easily
> interpretable. However, I am wanting to compare the 'accuracy' of the tree
> to a Random Forest to estimate how much predictive ability is lost by using
> one simple tree. My understanding is that the error automatically displayed
> by the two functions is calculated differently so it is therefore incorrect
> to use this as a comparison. Instead I have produced a table for both
> analyses comparing the observed and predicted response. 
> 
> E.g. table(data$dependent,predict(model,type="class"))
> 
> I am looking for confirmation that (a) it is incorrect to compare the error
> estimates for the two techniques and (b) that comparing the
> misclassification rates is an appropriate method for comparing the two
> techniques.
> 
> Thanks
> 
> Amy
> 
>  
> 
>  
> 
> Amelia Koch
> 
> University of Tasmania
> 
> School of Geography and Environmental Studies
> 
> Private Bag 78 Hobart
> 
> Tasmania, Australia 7001
> 
> Ph: +61 3 6226 7454
> 
> ajkoch at utas.edu.au
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From h.wickham at gmail.com  Tue Jan 30 19:05:07 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 30 Jan 2007 12:05:07 -0600
Subject: [R] lattice: two grouping variables, one controls 'col',
	the other 'pch'
In-Reply-To: <45BD7AE6.3080309@stat.purdue.edu>
References: <45BD7AE6.3080309@stat.purdue.edu>
Message-ID: <f8e6ff050701301005x7ca0e27ak9de43bd2d045e22b@mail.gmail.com>

On 1/28/07, Benjamin Tyner <btyner at stat.purdue.edu> wrote:
> Say I have
>
> library(lattice)
>
> x<-runif(256)
> y<-runif(256)
> f<-gl(16,16)
> g1<-rep(1:4,each=64)
> g2<-rep(1:4,times=64)
>
> plot<-xyplot(y~x|f,
>              groups=g1,
>              pch=as.character(1:4),
>              panel=function(x,y,subscripts,groups,...){
>                    panel.superpose(x,y,subscripts,groups,...)
>              })
>
> print(plot)
>
> Currently, both color and plotting symbol change with the grouping
> variable g1. What is the best way to have color change with g1, but
> plotting symbol change with g2 (or vice versa)?

Another option is to use ggplot:

install.packages("ggplot")
library(ggplot)
qplot(x, y, facet=. ~ f, size=g1, colour=g2)


From jlampur at eagrof.UdL.es  Tue Jan 30 18:57:44 2007
From: jlampur at eagrof.UdL.es (Jorge Lampurlanes Castel)
Date: Tue, 30 Jan 2007 18:57:44 +0100 (CET)
Subject: [R] Multiple comparisons when interacction
Message-ID: <3181.217.124.30.115.1170179864.squirrel@correu.udl.es>

Yes, it can be done.  It is not currently easy because multcomp doesn't
have the syntax yet.  Making this easy is on Torsten's to-do list for the
multcomp package.

See the MMC.WoodEnergy example in the HH package.  The current version on
CRAN
is HH_1.17.  Please see the discussion of this example in R-help:

https://stat.ethz.ch/pipermail/r-help/2007-January/123451.html

Rich


From deepayan.sarkar at gmail.com  Tue Jan 30 20:48:42 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 30 Jan 2007 13:48:42 -0600
Subject: [R] lattice: how to get 'lattice.theme'
In-Reply-To: <45BF750C.8020802@stat.purdue.edu>
References: <45BF750C.8020802@stat.purdue.edu>
Message-ID: <eb555e660701301148t1c886ff5y165fb93a8a30882@mail.gmail.com>

On 1/30/07, Benjamin Tyner <btyner at stat.purdue.edu> wrote:
> Thanks...what I really want is to be able to access the value of the
> 'color' argument the user gave in their call to trellis.device(). I
> mistakenly assumed it would be stored in 'lattice.theme' but this does
> not seem to be the case?

It is not stored anywhere, only used inside trellis.device (as an
argument to canonical.theme).

Deepayan


From dkaplan at education.wisc.edu  Tue Jan 30 21:08:31 2007
From: dkaplan at education.wisc.edu (David Kaplan)
Date: Tue, 30 Jan 2007 14:08:31 -0600
Subject: [R] complex samping designs
Message-ID: <45BFA5BF.6030402@education.wisc.edu>

Hi all,

Is there a package available in R that can be used in conjunction with 
statistical procedures to weight observations for unequal probability of 
selection?

Thanks in advance.

David


-- 
===========================================================================
David Kaplan, Ph.D.
Professor
Department of Educational Psychology
University of Wisconsin - Madison
Educational Sciences, Room, 1061
1025 W. Johnson Street
Madison, WI 53706

email: dkaplan at education.wisc.edu
homepage: http://www.education.wisc.edu/edpsych/facstaff/kaplan/kaplan.htm
Phone: 608-262-0836


From ccleland at optonline.net  Tue Jan 30 21:13:39 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 30 Jan 2007 15:13:39 -0500
Subject: [R] complex samping designs
In-Reply-To: <45BFA5BF.6030402@education.wisc.edu>
References: <45BFA5BF.6030402@education.wisc.edu>
Message-ID: <45BFA6F3.9060206@optonline.net>

David Kaplan wrote:
> Hi all,
> 
> Is there a package available in R that can be used in conjunction with 
> statistical procedures to weight observations for unequal probability of 
> selection?

http://cran.r-project.org/doc/packages/survey.pdf

RSiteSearch("complex survey")

> Thanks in advance.
> 
> David

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From johanfaux at yahoo.com  Tue Jan 30 21:17:23 2007
From: johanfaux at yahoo.com (johan Faux)
Date: Tue, 30 Jan 2007 12:17:23 -0800 (PST)
Subject: [R] silent loading of packages
Message-ID: <560967.58255.qm@web56208.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070130/b1375e29/attachment.ksh 

From dkaplan at education.wisc.edu  Tue Jan 30 21:21:45 2007
From: dkaplan at education.wisc.edu (David Kaplan)
Date: Tue, 30 Jan 2007 14:21:45 -0600
Subject: [R] complex samping designs
In-Reply-To: <45BFA6F3.9060206@optonline.net>
References: <45BFA5BF.6030402@education.wisc.edu>
	<45BFA6F3.9060206@optonline.net>
Message-ID: <45BFA8D9.9070700@education.wisc.edu>

This is a lifesaver.  Thanks!!!

David


===========================================================================
David Kaplan, Ph.D.
Professor
Department of Educational Psychology
University of Wisconsin - Madison
Educational Sciences, Room, 1061
1025 W. Johnson Street
Madison, WI 53706

email: dkaplan at education.wisc.edu
homepage: http://www.education.wisc.edu/edpsych/facstaff/kaplan/kaplan.htm
Phone: 608-262-0836
===========================================================================

Chuck Cleland wrote:
> David Kaplan wrote:
>> Hi all,
>>
>> Is there a package available in R that can be used in conjunction with 
>> statistical procedures to weight observations for unequal probability of 
>> selection?
> 
> http://cran.r-project.org/doc/packages/survey.pdf
> 
> RSiteSearch("complex survey")
> 
>> Thanks in advance.
>>
>> David
>


From jrkrideau at yahoo.ca  Tue Jan 30 21:27:24 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 30 Jan 2007 15:27:24 -0500 (EST)
Subject: [R] Simple Date problems with cbind
Message-ID: <395531.15349.qm@web32809.mail.mud.yahoo.com>

I am clearly misunderstanding something about dates
and my reading of the help and RSiteSearch have not
turned up anything. 

I have a variable of class "Date" and I want to add
include it in a data.frame. However when do a cbind
the date var is coerced into a numeric. 

However when I tried to create a example I also seem
to be doing something wrong as I cannot seem even  to
create a date class var even when I try to copy an
example from the Help

Example from Help
dates <- c("02/27/92", "02/27/92", "01/14/92",
"02/28/92", "02/01/92")
max <-as.Date(dates, "%m/%d/%y")
max
class(max)

Results
> dates <- c("02/27/92", "02/27/92", "01/14/92",
"02/28/92", "02/01/92")
> max <-as.Date(dates, "%m/%d/%y")
> max
[1] "1992-02-27" "1992-02-27" "1992-01-14"
"1992-02-28" "1992-02-01"
> class(max)
[1] "Date"


My example

cc <- c("2005/01/24" ,"2006/01/23" ,"2006/01/23",
"2006/01/23")
xx <- as.Date(cc, "%y/%m/%d")
xx
class(xx)

Results
> cc <- c("2005/01/24" ,"2006/01/23" ,"2006/01/23",
"2006/01/23")
> xx <- as.Date(cc, "%y/%m/%d")
> xx
[1] NA NA NA NA
> class(xx)
[1] "Date"

And on to the cbind problem

jj <- 1:5
cbind(jj,max)

   jj  max
[1,]  1 8092
[2,]  2 8092
[3,]  3 8048
[4,]  4 8093
[5,]  5 8066

I have tried various as.Date etc approcaes

It is probably something blindingly simple but can
anyone suggest something?

Thanks


From ggrothendieck at gmail.com  Tue Jan 30 21:37:32 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 30 Jan 2007 15:37:32 -0500
Subject: [R] silent loading of packages
In-Reply-To: <560967.58255.qm@web56208.mail.re3.yahoo.com>
References: <560967.58255.qm@web56208.mail.re3.yahoo.com>
Message-ID: <971536df0701301237k4fa74ab5t2114dc28bca11bdd@mail.gmail.com>

Try this.  We first load the dependent packages to avoid the loading message:

library(splines)
library(stats4)
library(VGAM, warn.conflicts = FALSE)

On 1/30/07, johan Faux <johanfaux at yahoo.com> wrote:
> I would like to turn off all the messages during
> library(aPackage) or
> require(aPackage)
>
> I tried different commands: invisible, capture.output, sink but none of them is working.
>
> For example, loading VGAM, gives a lot of unnecessary messages:
>
> > library(VGAM)
>
> Attaching package: 'VGAM'
>
>
>        The following object(s) are masked from package:splines :
>
>         bs
>
>        The following object(s) are masked from package:splines :
>
>         ns
>
>
>        The following object(s) are masked from package:boot :
>
>         logit
>
>        The following object(s) are masked from package:boot :
>
>         simplex
>
>
>        The following object(s) are masked from package:stats :
>
>         glm
>
>        The following object(s) are masked from package:stats :
>
>         lm
>
>        The following object(s) are masked from package:stats :
>
>         poly
>
>        The following object(s) are masked from package:stats :
>
>         predict.glm
>
>        The following object(s) are masked from package:stats :
>
>         predict.lm
>
>        The following object(s) are masked from package:stats :
>
>         predict.mlm
>
>
>        The following object(s) are masked from package:base :
>
>         scale.default
>
>
>
> Any hint/help will be appreciated.
>
>
>
>
> ---------------------------------
> Expecting? Get great news right away with email Auto-Check.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tplate at acm.org  Tue Jan 30 21:42:54 2007
From: tplate at acm.org (Tony Plate)
Date: Tue, 30 Jan 2007 13:42:54 -0700
Subject: [R] Simple Date problems with cbind
In-Reply-To: <395531.15349.qm@web32809.mail.mud.yahoo.com>
References: <395531.15349.qm@web32809.mail.mud.yahoo.com>
Message-ID: <45BFADCE.50800@acm.org>


> It is probably something blindingly simple but can
> anyone suggest something?

You need to use the format code "%Y" for 4-digits years.
You need to create a data frame using 'data.frame()' (cbind() creates a 
matrix when given just vectors).

 > as.Date(c("2005/01/24" ,"2006/01/23" ,"2006/01/23"), "%Y/%m/%d")
[1] "2005-01-24" "2006-01-23" "2006-01-23"
 > data.frame(int=1:3, date=as.Date(c("2005/01/24" ,"2006/01/23" 
,"2006/01/23"), "%Y/%m/%d"))
   int       date
1   1 2005-01-24
2   2 2006-01-23
3   3 2006-01-23
 > (x <- data.frame(int=1:3, date=as.Date(c("2005/01/24" ,"2006/01/23" 
,"2006/01/23"), "%Y/%m/%d")))
   int       date
1   1 2005-01-24
2   2 2006-01-23
3   3 2006-01-23
 > class(x)
[1] "data.frame"
 > sapply(x, class)
       int      date
"integer"    "Date"
 >

-- Tony Plate


From marc_schwartz at comcast.net  Tue Jan 30 21:47:03 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 30 Jan 2007 14:47:03 -0600
Subject: [R] Simple Date problems with cbind
In-Reply-To: <395531.15349.qm@web32809.mail.mud.yahoo.com>
References: <395531.15349.qm@web32809.mail.mud.yahoo.com>
Message-ID: <1170190023.4908.26.camel@localhost.localdomain>

On Tue, 2007-01-30 at 15:27 -0500, John Kane wrote:
> I am clearly misunderstanding something about dates
> and my reading of the help and RSiteSearch have not
> turned up anything. 
> 
> I have a variable of class "Date" and I want to add
> include it in a data.frame. However when do a cbind
> the date var is coerced into a numeric. 
> 
> However when I tried to create a example I also seem
> to be doing something wrong as I cannot seem even  to
> create a date class var even when I try to copy an
> example from the Help
> 
> Example from Help
> dates <- c("02/27/92", "02/27/92", "01/14/92",
> "02/28/92", "02/01/92")
> max <-as.Date(dates, "%m/%d/%y")
> max
> class(max)
> 
> Results
> > dates <- c("02/27/92", "02/27/92", "01/14/92",
> "02/28/92", "02/01/92")
> > max <-as.Date(dates, "%m/%d/%y")
> > max
> [1] "1992-02-27" "1992-02-27" "1992-01-14"
> "1992-02-28" "1992-02-01"
> > class(max)
> [1] "Date"
> 
> 
> My example
> 
> cc <- c("2005/01/24" ,"2006/01/23" ,"2006/01/23",
> "2006/01/23")
> xx <- as.Date(cc, "%y/%m/%d")
> xx
> class(xx)


You need to use a capital "Y" for a four digit year...

See ?strftime for more information on date formats.


> Results
> > cc <- c("2005/01/24" ,"2006/01/23" ,"2006/01/23",
> "2006/01/23")
> > xx <- as.Date(cc, "%y/%m/%d")
> > xx
> [1] NA NA NA NA
> > class(xx)
> [1] "Date"


> cc <- c("2005/01/24" ,"2006/01/23" ,"2006/01/23",
          "2006/01/23")

xx <- as.Date(cc, "%Y/%m/%d")

> xx
[1] "2005-01-24" "2006-01-23" "2006-01-23" "2006-01-23"

> class(xx)
[1] "Date"



> And on to the cbind problem
> 
> jj <- 1:5
> cbind(jj,max)
> 
>    jj  max
> [1,]  1 8092
> [2,]  2 8092
> [3,]  3 8048
> [4,]  4 8093
> [5,]  5 8066
> 
> I have tried various as.Date etc approcaes
> 
> It is probably something blindingly simple but can
> anyone suggest something?
> 
> Thanks

In this case, you are trying to cbind() a numeric vector and a Date
vector into a matrix.  Since a matrix may only have one data type, the
Date vector will be coerced to numeric.

If you want mixed data types, you need to create a data frame:

jj <- 1:4
DF <- data.frame(jj, xx)

> DF
  jj         xx
1  1 2005-01-24
2  2 2006-01-23
3  3 2006-01-23
4  4 2006-01-23

> str(DF)
'data.frame':   4 obs. of  2 variables:
 $ jj: int  1 2 3 4
 $ xx:Class 'Date'  num [1:4] 12807 13171 13171 13171


Alternatively, create an initial data frame with 'jj' and then cbind()
'xx':

JJ <- data.frame(jj)

> str(JJ)
'data.frame':   4 obs. of  1 variable:
 $ jj: int  1 2 3 4

DF <- cbind(JJ, xx)

> str(DF)
'data.frame':   4 obs. of  2 variables:
 $ jj: int  1 2 3 4
 $ xx:Class 'Date'  num [1:4] 12807 13171 13171 13171


Once you create the initial data frame, cbind() will then use the
appropriate approach based upon the first argument already being a data
frame.

HTH,

Marc Schwartz


From tom.boonen.maiden at gmail.com  Tue Jan 30 21:47:55 2007
From: tom.boonen.maiden at gmail.com (Tom Boonen)
Date: Tue, 30 Jan 2007 15:47:55 -0500
Subject: [R] change plotting symbol for groups in trellis graph
Message-ID: <cc088e260701301247k6a7e4b6ewba868b3a4ad3cb8d@mail.gmail.com>

Hi,

how can I change the plotting symbol for the groups in a trellis panel dotplot.

My graph is similar to:

library(trellis)
dotplot(variety ~ yield | site, data = barley, groups = year,
             key = simpleKey(levels(barley$year), space = "right"),
             xlab = "Barley Yield (bushels/acre) ",
             aspect=0.5, layout = c(1,6), ylab=NULL)

I'd like to plot the different years with different symbols (eg. one
with pch=19 the other with pch=21 or so). Thank you very much for your
help.

Tom


From Andrew_Duba at wustl.edu  Tue Jan 30 22:17:03 2007
From: Andrew_Duba at wustl.edu (Andrew_Duba at wustl.edu)
Date: Tue, 30 Jan 2007 15:17:03 -0600
Subject: [R] Solaris 10 compilation issue
Message-ID: <OF59918CF7.CFBC616D-ON86257273.0072FDFC-86257273.0074EB5B@aismail.wustl.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070130/8ea90ee9/attachment.ksh 

From aduba at artsci.wustl.edu  Tue Jan 30 22:26:02 2007
From: aduba at artsci.wustl.edu (andrew duba)
Date: Tue, 30 Jan 2007 15:26:02 -0600
Subject: [R] R Compiling issue
Message-ID: <001601c744b5$41a266c0$2e5f14ac@cfspc046>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070130/a368e14f/attachment.ksh 

From aduba at ascc1.artsci.wustl.edu  Tue Jan 30 22:47:07 2007
From: aduba at ascc1.artsci.wustl.edu (Andrew John Duba)
Date: Tue, 30 Jan 2007 15:47:07 -0600 (CST)
Subject: [R] Issue with compiling R on solaris 10
Message-ID: <Pine.GSO.4.58.0701301545160.18688@ascc1.artsci.wustl.edu>

I am trying to compile R-2.4.1 in 64-bit on Solaris 10 running on AMD
hardware.  I am trying to do this with Sun Studio 11.

My config.site looks like this:

#! /bin/sh
AR="/usr/ccs/bin/ar"

TEX="/usr/local/teTeX/bin/i386-pc-solaris2.10/tex"
LATEX="/usr/local/teTeX/bin/i386-pc-solaris2.10/latex"
PDFTEX="/usr/local/teTeX/bin/i386-pc-solaris2.10/pdftex"
PDFLATEX="/usr/local/teTeX/bin/i386-pc-solaris2.10/pdflatex"
DVIPS="/usr/local/teTeX/bin/i386-pc-solaris2.10/dvips"
MAKEINFO="/usr/local/teTeX/bin/i386-pc-solaris2.10/makeinfo"

UCOMPILER="-xarch=amd64"
UFLAGS="-xO5 -xlibmil"

CC="/opt/SUNWspro/bin/cc $UCOMPILER"
CFLAGS="$UFLAGS -xc99=%all -xlibmieee"
F77="/opt/SUNWspro/bin/f95 $UCOMPILER"
FFLAGS="$UFLAGS -fsimple=0"
CXX="/opt/SUNWspro/bin/CC $UCOMPILER"
CXXFLAGS="$UFLAGS -xlibmieee"
FC="/opt/SUNWspro/bin/f95 $UCOMPILER"
FCFLAGS="$UFLAGS -fsimple=0 -xlang=c99"
LD="/usr/ccs/bin/amd64/ld"
LDFLAGS="-L/usr/local/lib/64 -L/usr/sfw/lib/64 -L/lib/amd64
-R/usr/local/lib/64 -R/usr/sfw/lib/64 -R/lib/amd64"

R_BROWSER="/usr/sfw/bin/mozilla"

MAIN_LD="/opt/SUNWspro/bin/cc -xarch=amd64"
LIBnn=amd64


The ./configure runs without incident but make breaks down here

/opt/SUNWspro/bin/f95 -xarch=amd64   -xO5 -xlibmil -fsimple=0 -c xxxpr.f
-o xxxpr.o
/opt/SUNWspro/bin/cc -xarch=amd64 -I../../src/extra/zlib
-I../../src/extra/bzip2 -I../../src/extra/pcre  -I. -I../../src/include
-I../../src/include -I/usr/local/include -DHAVE_CONFIG_H
-D__NO_MATH_INLINES  -xO5 -xlibmil -xc99=%all -xlibmieee -c mkdtemp.c -o
mkdtemp.o
/opt/SUNWspro/bin/cc -xarch=amd64  -L/usr/local/lib/64 -L/usr/sfw/lib/64
-L/lib/amd64 -R/usr/local/lib/64 -R/usr/sfw/lib/64 -R/lib/amd64 -o R.bin
Rmain.o CConverters.o CommandLineArgs.o Rdynload.o Renviron.o RNG.o
apply.o arithmetic.o apse.o array.o attrib.o base.o bind.o builtin.o
character.o coerce.o colors.o complex.o connections.o context.o cov.o
cum.o dcf.o datetime.o debug.o deparse.o deriv.o dotcode.o dounzip.o
dstruct.o duplicate.o engine.o envir.o errors.o eval.o format.o fourier.o
gevents.o gram.o gram-ex.o graphics.o identical.o internet.o iosupport.o
lapack.o list.o localecharset.o logic.o main.o mapply.o match.o memory.o
model.o names.o objects.o optim.o optimize.o options.o par.o paste.o
pcre.o platform.o plot.o plot3d.o plotmath.o print.o printarray.o
printvector.o printutils.o qsort.o random.o regex.o registration.o relop.o
rlocale.o saveload.o scan.o seq.o serialize.o size.o sort.o source.o
split.o sprintf.o startup.o subassign.o subscript.o subset.o summary.o
sysutils.o unique.o util.o version.o vfonts.o xxxpr.o  mkdtemp.o
../unix/libunix.a ../appl/libappl.a ../nmath/libnmath.a -L../../lib
-lRblas -R/usr/local/lib/64 -R/usr/sfw/lib/64 -R/lib/amd64
-R/opt/SUNWspro/lib/amd64 -L/opt/SUNWspro/lib/amd64
-L/opt/SUNWspro/prod/lib/amd64 -L/usr/lib/amd64 -lfui -lfai -lfsu
-lsunmath -lmtsk -lm  ../extra/zlib/libz.a ../extra/bzip2/libbz2.a
../extra/pcre/libpcre.a  ../extra/intl/libintl.a -lreadline -ltermcap
-lnsl -lsocket -ldl -lm
Undefined                       first referenced
 symbol                             in file
cg_                                 registration.o
ch_                                 registration.o
rg_                                 registration.o
rs_                                 registration.o
bincount                            registration.o
inflate                             connections.o
Rf_df                               arithmetic.o
Rf_dt                               arithmetic.o
Rf_pf                               arithmetic.o
Rf_pt                               arithmetic.o
Rf_qf                               arithmetic.o
Rf_qt                               arithmetic.o
Rf_rf                               random.o
Rf_rt                               random.o
chol_                               registration.o
gzeof                               connections.o
do_syssleep                         names.o
R_EditFiles                         platform.o
R_getProcTime                       memory.o
R_ExpandFileName                    Renviron.o
R_OpenInitFile                      main.o
R_getClockIncrement                 eval.o
R_ReadConsole                       connections.o
Rf_initialize_R                     Rmain.o
call_dqags                          registration.o
call_dqagi                          registration.o
deflateEnd                          connections.o
ptr_R_EditFile                      ../unix/libunix.a(edit.o)
inflateEnd                          connections.o
Rf_lchoose                          arithmetic.o
R_ChooseFile                        platform.o
lminfl_                             registration.o
deflateInit2_                       connections.o
N01_kind                            RNG.o
do_machine                          names.o
libintl_dcigettext                  ../extra/intl/libintl.a(dcgettext.o)
R_max_col                           registration.o
R_ClearerrConsole                   connections.o
pcre_maketables                     character.o
spline_eval                         registration.o
spline_coef                         registration.o
R_Busy                              main.o
R_approx                            registration.o
Rf_dnf                              arithmetic.o
Rf_dnt                              arithmetic.o
Rf_pnf                              arithmetic.o
Rf_pnt                              arithmetic.o
Rf_qnf                              arithmetic.o
Rf_qnt                              arithmetic.o
BZ2_bzRead                          connections.o
Rf_choose                           arithmetic.o
Brent_fmin                          optimize.o
Rf_dbinom                           arithmetic.o
Rf_dchisq                           arithmetic.o
R_cumsum                            registration.o
Rf_dgamma                           arithmetic.o
Rf_dhyper                           arithmetic.o
Rf_dlogis                           arithmetic.o
Rf_dlnorm                           arithmetic.o
Rf_dnbeta                           arithmetic.o
Rf_dnorm4                           arithmetic.o
fft_factor                          fourier.o
Rf_fround                           arithmetic.o
Rf_ftrunc                           arithmetic.o
libintl_gettextparse                ../extra/intl/libintl.a(plural-exp.o)
Rf_pbinom                           arithmetic.o
Rf_pchisq                           arithmetic.o
Rf_pgamma                           arithmetic.o
Rf_phyper                           arithmetic.o
Rf_plogis                           arithmetic.o
Rf_plnorm                           arithmetic.o
Rf_pnbeta                           arithmetic.o
Rf_pnorm5                           arithmetic.o
Rf_qbinom                           arithmetic.o
R_pretty                            registration.o
R_cpolyroot                         complex.o
Rf_qchisq                           arithmetic.o
Rf_ptukey                           arithmetic.o
Rf_qgamma                           arithmetic.o
Rf_qhyper                           arithmetic.o
Rf_qlogis                           arithmetic.o
Rf_qlnorm                           arithmetic.o
Rf_qnbeta                           arithmetic.o
Rf_qnorm5                           arithmetic.o
bakslv                              registration.o
Rf_rbinom                           random.o
Rf_rchisq                           random.o
Rf_qtukey                           arithmetic.o
Rf_rgamma                           random.o
Rf_rhyper                           random.o
Rf_rlogis                           random.o
Rf_rlnorm                           random.o
R_rowsum                            registration.o
str_signif                          registration.o
Rf_pcauchy                          arithmetic.o
dchdc_                              registration.o
do_system                           names.o
dpbfa_                              registration.o
dpbsl_                              registration.o
dpoco_                              registration.o
dpodi_                              registration.o
dpofa_                              registration.o
dposl_                              registration.o
dqrdc_                              registration.o
dqrls_                              registration.o
dqrsl_                              registration.o
dsvdc_                              registration.o
dtrco_                              registration.o
dtrsl_                              registration.o
wilcox_free                         registration.o
fdhess                              optimize.o
R_ShowFiles                         platform.o
gzgetc                              connections.o
gzopen                              connections.o
gzread                              connections.o
gzseek                              connections.o
gztell                              connections.o
BZ2_bzReadOpen                      connections.o
R_zeroin                            optimize.o
ptr_R_savehistory                   ../unix/libunix.a(stubs.o)
loglin                              registration.o
lowess                              registration.o
machar                              platform.o
ptr_R_loadhistory                   ../unix/libunix.a(stubs.o)
Rf_pnchisq                          arithmetic.o
Rf_pnbinom                          arithmetic.o
optif9                              optimize.o
rcont2                              random.o
R_pretty0                           engine.o
setulb                              optim.o
Rf_qcauchy                          arithmetic.o
UsingReadline                       platform.o
Rf_pwilcox                          arithmetic.o
find_interv_vec                     registration.o
Rf_qnchisq                          arithmetic.o
Rf_qnbinom                          arithmetic.o
Rf_rcauchy                          random.o
Rf_trigamma                         arithmetic.o
massdist                            registration.o
pcre_info                           pcre.o
pcre_exec                           character.o
Rf_beta                             arithmetic.o
Rf_dexp                             arithmetic.o
Rf_pexp                             arithmetic.o
Rf_qexp                             arithmetic.o
Rf_rexp                             random.o
Rf_sign                             arithmetic.o
libintl_nl_domain_bindings          ../extra/intl/libintl.a(bindtextdom.o)
libintl_nl_current_default_domain   ../extra/intl/libintl.a(textdomain.o)
Rf_qwilcox                          arithmetic.o
libintl_nl_default_default_domain   ../extra/intl/libintl.a(textdomain.o)
BZ2_bzWriteOpen                     connections.o
bincode                             registration.o
R_CleanTempDir                      main.o
do_proctime                         names.o
norm_rand                           optim.o
Rf_rnchisq                          random.o
Rf_rnbinom                          random.o
R_running_as_main_program           Rmain.o
Rf_psigamma                         arithmetic.o
ch2inv_                             registration.o
BZ2_bzWrite                         connections.o
inflateInit2_                       connections.o
Rf_rwilcox                          random.o
R_CleanUp                           errors.o
_nl_find_msg                        ../extra/intl/libintl.a(loadmsgcat.o)
Rf_dsignrank                        arithmetic.o
Rf_psignrank                        arithmetic.o
Rf_rsignrank                        random.o
Rf_qsignrank                        arithmetic.o
Rf_dcauchy                          arithmetic.o
deflate                             connections.o
R_Suicide                           Renviron.o
stemleaf                            registration.o
Rf_digamma                          arithmetic.o
do_sysinfo                          names.o
Rf_dnchisq                          arithmetic.o
Rf_dnbinom                          arithmetic.o
pcre_study                          character.o
R_ShowMessage                       CommandLineArgs.o
dqrdc2_                             registration.o
ptr_R_addhistory                    ../unix/libunix.a(stubs.o)
Rf_InitFunctionHashing              Rdynload.o
R_FlushConsole                      errors.o
Rf_gamma_cody                       arithmetic.o
Rf_dwilcox                          arithmetic.o
deflateInit_                        ../extra/zlib/libz.a(compress.o)
rmultinom                           random.o
Rf_dbeta                            arithmetic.o
Rf_dgeom                            arithmetic.o
Rf_dpois                            arithmetic.o
Rf_dunif                            arithmetic.o
Rf_fmin2                            engine.o
Rf_fmax2                            engine.o
Rf_fprec                            arithmetic.o
Rf_fsign                            complex.o
Rf_imin2                            scan.o
Rf_imax2                            bind.o
Rf_lbeta                            arithmetic.o
Rf_pbeta                            arithmetic.o
Rf_pgeom                            arithmetic.o
Rf_ppois                            arithmetic.o
Rf_qbeta                            arithmetic.o
Rf_punif                            arithmetic.o
Rf_qgeom                            arithmetic.o
Rf_qpois                            arithmetic.o
Rf_rbeta                            random.o
Rf_qunif                            arithmetic.o
Rf_rgeom                            random.o
Rf_rnorm                            random.o
Rf_rpois                            random.o
Rf_runif                            random.o
Rf_bessel_i                         arithmetic.o
Rf_bessel_j                         arithmetic.o
Rf_bessel_k                         arithmetic.o
Rf_bessel_y                         arithmetic.o
Rf_lgammafn                         arithmetic.o
BM_norm_keep                        RNG.o
Rf_gammafn                          arithmetic.o
inflateInit_                        ../extra/zlib/libz.a(uncompr.o)
fft_work                            fourier.o
Rf_pweibull                         arithmetic.o
Rf_qweibull                         arithmetic.o
Rf_rweibull                         random.o
Rf_dweibull                         arithmetic.o
libintl_nl_default_dirname          ../extra/intl/libintl.a(bindtextdom.o)
R_WriteConsole                      printutils.o
R_ResetConsole                      errors.o
BZ2_bzWriteClose                    connections.o
gzclose                             connections.o
signrank_free                       registration.o
gzwrite                             connections.o
pcre_compile                        character.o
BZ2_bzReadClose                     connections.o
ld: fatal: Symbol referencing errors. No output written to R.bin
make[3]: *** [R.bin] Error 1
make[3]: Leaving directory `/home/aduba/source/R-2.4.1/src/main'
make[2]: *** [R] Error 2
make[2]: Leaving directory `/home/aduba/source/R-2.4.1/src/main'
make[1]: *** [R] Error 1
make[1]: Leaving directory `/home/aduba/source/R-2.4.1/src'
make: *** [R] Error 1

Any suggestions?

Thanks in advance for your help.


From ggrothendieck at gmail.com  Tue Jan 30 22:55:12 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 30 Jan 2007 16:55:12 -0500
Subject: [R] change plotting symbol for groups in trellis graph
In-Reply-To: <cc088e260701301247k6a7e4b6ewba868b3a4ad3cb8d@mail.gmail.com>
References: <cc088e260701301247k6a7e4b6ewba868b3a4ad3cb8d@mail.gmail.com>
Message-ID: <971536df0701301355j77b8d05oabce276b61fa2bf7@mail.gmail.com>

Try this:

library(lattice)
dotplot(variety ~ yield | site, data = barley, groups = year,
             auto.key = list(space = "right"),
             xlab = "Barley Yield (bushels/acre) ",
             aspect=0.5, layout = c(1,6), ylab=NULL,
             par.settings = list(superpose.symbol = list(pch = c(19, 21))))



On 1/30/07, Tom Boonen <tom.boonen.maiden at gmail.com> wrote:
> Hi,
>
> how can I change the plotting symbol for the groups in a trellis panel dotplot.
>
> My graph is similar to:
>
> library(trellis)
> dotplot(variety ~ yield | site, data = barley, groups = year,
>             key = simpleKey(levels(barley$year), space = "right"),
>             xlab = "Barley Yield (bushels/acre) ",
>             aspect=0.5, layout = c(1,6), ylab=NULL)
>
> I'd like to plot the different years with different symbols (eg. one
> with pch=19 the other with pch=21 or so). Thank you very much for your
> help.
>
> Tom
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From aduba at ascc1.artsci.wustl.edu  Tue Jan 30 23:07:39 2007
From: aduba at ascc1.artsci.wustl.edu (Andrew John Duba)
Date: Tue, 30 Jan 2007 16:07:39 -0600 (CST)
Subject: [R] Difficulty with compiling R-2.4.1 on solaris 10
Message-ID: <Pine.GSO.4.58.0701301600460.3888@ascc1.artsci.wustl.edu>

I am trying to compile R-2.4.1 in 64-bit on Solaris 10 running on AMD
hardware.  I am trying to do this with Sun Studio 11.

My config.site looks like this:

#! /bin/sh
AR="/usr/ccs/bin/ar"

TEX="/usr/local/teTeX/bin/i386-pc-solaris2.10/tex"
LATEX="/usr/local/teTeX/bin/i386-pc-solaris2.10/latex"
PDFTEX="/usr/local/teTeX/bin/i386-pc-solaris2.10/pdftex"
PDFLATEX="/usr/local/teTeX/bin/i386-pc-solaris2.10/pdflatex"
DVIPS="/usr/local/teTeX/bin/i386-pc-solaris2.10/dvips"
MAKEINFO="/usr/local/teTeX/bin/i386-pc-solaris2.10/makeinfo"

UCOMPILER="-xarch=amd64"
UFLAGS="-xO5 -xlibmil"

CC="/opt/SUNWspro/bin/cc $UCOMPILER"
CFLAGS="$UFLAGS -xc99=%all -xlibmieee"
F77="/opt/SUNWspro/bin/f95 $UCOMPILER"
FFLAGS="$UFLAGS -fsimple=0"
CXX="/opt/SUNWspro/bin/CC $UCOMPILER"
CXXFLAGS="$UFLAGS -xlibmieee"
FC="/opt/SUNWspro/bin/f95 $UCOMPILER"
FCFLAGS="$UFLAGS -fsimple=0 -xlang=c99"
LD="/usr/ccs/bin/amd64/ld"
LDFLAGS="-L/usr/local/lib/64 -L/usr/sfw/lib/64 -L/lib/amd64
-R/usr/local/lib/64 -R/usr/sfw/lib/64 -R/lib/amd64"

R_BROWSER="/usr/sfw/bin/mozilla"

MAIN_LD="/opt/SUNWspro/bin/cc -xarch=amd64"
LIBnn=amd64


The ./configure runs without incident but make breaks down and the linker
complains about undefined symbols.  The symbols that the linker complains
about are in Rmath.h.  I have tried adding the folder with Rmath.h to the
linkers path but that has not helped.  Does anyone have any suggestions?

If someone needs it below is an excript from the make session

Thanks in advance for your help.

/opt/SUNWspro/bin/f95 -xarch=amd64   -xO5 -xlibmil -fsimple=0 -c xxxpr.f
-o xxxpr.o
/opt/SUNWspro/bin/cc -xarch=amd64 -I../../src/extra/zlib
-I../../src/extra/bzip2 -I../../src/extra/pcre  -I. -I../../src/include
-I../../src/include -I/usr/local/include -DHAVE_CONFIG_H
-D__NO_MATH_INLINES  -xO5 -xlibmil -xc99=%all -xlibmieee -c mkdtemp.c -o
mkdtemp.o
/opt/SUNWspro/bin/cc -xarch=amd64  -L/usr/local/lib/64 -L/usr/sfw/lib/64
-L/lib/amd64 -R/usr/local/lib/64 -R/usr/sfw/lib/64 -R/lib/amd64 -o R.bin
Rmain.o CConverters.o CommandLineArgs.o Rdynload.o Renviron.o RNG.o
apply.o arithmetic.o apse.o array.o attrib.o base.o bind.o builtin.o
character.o coerce.o colors.o complex.o connections.o context.o cov.o
cum.o dcf.o datetime.o debug.o deparse.o deriv.o dotcode.o dounzip.o
dstruct.o duplicate.o engine.o envir.o errors.o eval.o format.o fourier.o
gevents.o gram.o gram-ex.o graphics.o identical.o internet.o iosupport.o
lapack.o list.o localecharset.o logic.o main.o mapply.o match.o memory.o
model.o names.o objects.o optim.o optimize.o options.o par.o paste.o
pcre.o platform.o plot.o plot3d.o plotmath.o print.o printarray.o
printvector.o printutils.o qsort.o random.o regex.o registration.o relop.o
rlocale.o saveload.o scan.o seq.o serialize.o size.o sort.o source.o
split.o sprintf.o startup.o subassign.o subscript.o subset.o summary.o
sysutils.o unique.o util.o version.o vfonts.o xxxpr.o  mkdtemp.o
../unix/libunix.a ../appl/libappl.a ../nmath/libnmath.a -L../../lib
-lRblas -R/usr/local/lib/64 -R/usr/sfw/lib/64 -R/lib/amd64
-R/opt/SUNWspro/lib/amd64 -L/opt/SUNWspro/lib/amd64
-L/opt/SUNWspro/prod/lib/amd64 -L/usr/lib/amd64 -lfui -lfai -lfsu
-lsunmath -lmtsk -lm  ../extra/zlib/libz.a ../extra/bzip2/libbz2.a
../extra/pcre/libpcre.a  ../extra/intl/libintl.a -lreadline -ltermcap
-lnsl -lsocket -ldl -lm
Undefined                       first referenced
 symbol                             in file
cg_                                 registration.o
ch_                                 registration.o
rg_                                 registration.o
rs_                                 registration.o
bincount                            registration.o
inflate                             connections.o
Rf_df                               arithmetic.o
Rf_dt                               arithmetic.o
Rf_pf                               arithmetic.o
Rf_pt                               arithmetic.o
Rf_qf                               arithmetic.o
Rf_qt                               arithmetic.o
Rf_rf                               random.o
Rf_rt                               random.o
chol_                               registration.o
gzeof                               connections.o
do_syssleep                         names.o
R_EditFiles                         platform.o
R_getProcTime                       memory.o
R_ExpandFileName                    Renviron.o
R_OpenInitFile                      main.o
R_getClockIncrement                 eval.o
R_ReadConsole                       connections.o
Rf_initialize_R                     Rmain.o
call_dqags                          registration.o
call_dqagi                          registration.o
deflateEnd                          connections.o
ptr_R_EditFile                      ../unix/libunix.a(edit.o)
inflateEnd                          connections.o
Rf_lchoose                          arithmetic.o
R_ChooseFile                        platform.o
lminfl_                             registration.o
deflateInit2_                       connections.o
N01_kind                            RNG.o
do_machine                          names.o
libintl_dcigettext                  ../extra/intl/libintl.a(dcgettext.o)
R_max_col                           registration.o
R_ClearerrConsole                   connections.o
pcre_maketables                     character.o
spline_eval                         registration.o
spline_coef                         registration.o
R_Busy                              main.o
R_approx                            registration.o
Rf_dnf                              arithmetic.o
Rf_dnt                              arithmetic.o
Rf_pnf                              arithmetic.o
Rf_pnt                              arithmetic.o
Rf_qnf                              arithmetic.o
Rf_qnt                              arithmetic.o
BZ2_bzRead                          connections.o
Rf_choose                           arithmetic.o
Brent_fmin                          optimize.o
Rf_dbinom                           arithmetic.o
Rf_dchisq                           arithmetic.o
R_cumsum                            registration.o
Rf_dgamma                           arithmetic.o
Rf_dhyper                           arithmetic.o
Rf_dlogis                           arithmetic.o
Rf_dlnorm                           arithmetic.o
Rf_dnbeta                           arithmetic.o
Rf_dnorm4                           arithmetic.o
fft_factor                          fourier.o
Rf_fround                           arithmetic.o
Rf_ftrunc                           arithmetic.o
libintl_gettextparse                ../extra/intl/libintl.a(plural-exp.o)
Rf_pbinom                           arithmetic.o
Rf_pchisq                           arithmetic.o
Rf_pgamma                           arithmetic.o
Rf_phyper                           arithmetic.o
Rf_plogis                           arithmetic.o
Rf_plnorm                           arithmetic.o
Rf_pnbeta                           arithmetic.o
Rf_pnorm5                           arithmetic.o
Rf_qbinom                           arithmetic.o
R_pretty                            registration.o
R_cpolyroot                         complex.o
Rf_qchisq                           arithmetic.o
Rf_ptukey                           arithmetic.o
Rf_qgamma                           arithmetic.o
Rf_qhyper                           arithmetic.o
Rf_qlogis                           arithmetic.o
Rf_qlnorm                           arithmetic.o
Rf_qnbeta                           arithmetic.o
Rf_qnorm5                           arithmetic.o
bakslv                              registration.o
Rf_rbinom                           random.o
Rf_rchisq                           random.o
Rf_qtukey                           arithmetic.o
Rf_rgamma                           random.o
Rf_rhyper                           random.o
Rf_rlogis                           random.o
Rf_rlnorm                           random.o
R_rowsum                            registration.o
str_signif                          registration.o
Rf_pcauchy                          arithmetic.o
dchdc_                              registration.o
do_system                           names.o
dpbfa_                              registration.o
dpbsl_                              registration.o
dpoco_                              registration.o
dpodi_                              registration.o
dpofa_                              registration.o
dposl_                              registration.o
dqrdc_                              registration.o
dqrls_                              registration.o
dqrsl_                              registration.o
dsvdc_                              registration.o
dtrco_                              registration.o
dtrsl_                              registration.o
wilcox_free                         registration.o
fdhess                              optimize.o
R_ShowFiles                         platform.o
gzgetc                              connections.o
gzopen                              connections.o
gzread                              connections.o
gzseek                              connections.o
gztell                              connections.o
BZ2_bzReadOpen                      connections.o
R_zeroin                            optimize.o
ptr_R_savehistory                   ../unix/libunix.a(stubs.o)
loglin                              registration.o
lowess                              registration.o
machar                              platform.o
ptr_R_loadhistory                   ../unix/libunix.a(stubs.o)
Rf_pnchisq                          arithmetic.o
Rf_pnbinom                          arithmetic.o
optif9                              optimize.o
rcont2                              random.o
R_pretty0                           engine.o
setulb                              optim.o
Rf_qcauchy                          arithmetic.o
UsingReadline                       platform.o
Rf_pwilcox                          arithmetic.o
find_interv_vec                     registration.o
Rf_qnchisq                          arithmetic.o
Rf_qnbinom                          arithmetic.o
Rf_rcauchy                          random.o
Rf_trigamma                         arithmetic.o
massdist                            registration.o
pcre_info                           pcre.o
pcre_exec                           character.o
Rf_beta                             arithmetic.o
Rf_dexp                             arithmetic.o
Rf_pexp                             arithmetic.o
Rf_qexp                             arithmetic.o
Rf_rexp                             random.o
Rf_sign                             arithmetic.o
libintl_nl_domain_bindings          ../extra/intl/libintl.a(bindtextdom.o)
libintl_nl_current_default_domain   ../extra/intl/libintl.a(textdomain.o)
Rf_qwilcox                          arithmetic.o
libintl_nl_default_default_domain   ../extra/intl/libintl.a(textdomain.o)
BZ2_bzWriteOpen                     connections.o
bincode                             registration.o
R_CleanTempDir                      main.o
do_proctime                         names.o
norm_rand                           optim.o
Rf_rnchisq                          random.o
Rf_rnbinom                          random.o
R_running_as_main_program           Rmain.o
Rf_psigamma                         arithmetic.o
ch2inv_                             registration.o
BZ2_bzWrite                         connections.o
inflateInit2_                       connections.o
Rf_rwilcox                          random.o
R_CleanUp                           errors.o
_nl_find_msg                        ../extra/intl/libintl.a(loadmsgcat.o)
Rf_dsignrank                        arithmetic.o
Rf_psignrank                        arithmetic.o
Rf_rsignrank                        random.o
Rf_qsignrank                        arithmetic.o
Rf_dcauchy                          arithmetic.o
deflate                             connections.o
R_Suicide                           Renviron.o
stemleaf                            registration.o
Rf_digamma                          arithmetic.o
do_sysinfo                          names.o
Rf_dnchisq                          arithmetic.o
Rf_dnbinom                          arithmetic.o
pcre_study                          character.o
R_ShowMessage                       CommandLineArgs.o
dqrdc2_                             registration.o
ptr_R_addhistory                    ../unix/libunix.a(stubs.o)
Rf_InitFunctionHashing              Rdynload.o
R_FlushConsole                      errors.o
Rf_gamma_cody                       arithmetic.o
Rf_dwilcox                          arithmetic.o
deflateInit_                        ../extra/zlib/libz.a(compress.o)
rmultinom                           random.o
Rf_dbeta                            arithmetic.o
Rf_dgeom                            arithmetic.o
Rf_dpois                            arithmetic.o
Rf_dunif                            arithmetic.o
Rf_fmin2                            engine.o
Rf_fmax2                            engine.o
Rf_fprec                            arithmetic.o
Rf_fsign                            complex.o
Rf_imin2                            scan.o
Rf_imax2                            bind.o
Rf_lbeta                            arithmetic.o
Rf_pbeta                            arithmetic.o
Rf_pgeom                            arithmetic.o
Rf_ppois                            arithmetic.o
Rf_qbeta                            arithmetic.o
Rf_punif                            arithmetic.o
Rf_qgeom                            arithmetic.o
Rf_qpois                            arithmetic.o
Rf_rbeta                            random.o
Rf_qunif                            arithmetic.o
Rf_rgeom                            random.o
Rf_rnorm                            random.o
Rf_rpois                            random.o
Rf_runif                            random.o
Rf_bessel_i                         arithmetic.o
Rf_bessel_j                         arithmetic.o
Rf_bessel_k                         arithmetic.o
Rf_bessel_y                         arithmetic.o
Rf_lgammafn                         arithmetic.o
BM_norm_keep                        RNG.o
Rf_gammafn                          arithmetic.o
inflateInit_                        ../extra/zlib/libz.a(uncompr.o)
fft_work                            fourier.o
Rf_pweibull                         arithmetic.o
Rf_qweibull                         arithmetic.o
Rf_rweibull                         random.o
Rf_dweibull                         arithmetic.o
libintl_nl_default_dirname          ../extra/intl/libintl.a(bindtextdom.o)
R_WriteConsole                      printutils.o
R_ResetConsole                      errors.o
BZ2_bzWriteClose                    connections.o
gzclose                             connections.o
signrank_free                       registration.o
gzwrite                             connections.o
pcre_compile                        character.o
BZ2_bzReadClose                     connections.o
ld: fatal: Symbol referencing errors. No output written to R.bin
make[3]: *** [R.bin] Error 1
make[3]: Leaving directory `/home/aduba/source/R-2.4.1/src/main'
make[2]: *** [R] Error 2
make[2]: Leaving directory `/home/aduba/source/R-2.4.1/src/main'
make[1]: *** [R] Error 1
make[1]: Leaving directory `/home/aduba/source/R-2.4.1/src'
make: *** [R] Error 1


From ripley at stats.ox.ac.uk  Tue Jan 30 23:09:41 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 30 Jan 2007 22:09:41 +0000 (GMT)
Subject: [R] Solaris 10 compilation issue
In-Reply-To: <OF59918CF7.CFBC616D-ON86257273.0072FDFC-86257273.0074EB5B@aismail.wustl.edu>
References: <OF59918CF7.CFBC616D-ON86257273.0072FDFC-86257273.0074EB5B@aismail.wustl.edu>
Message-ID: <Pine.LNX.4.64.0701302203330.24341@gannet.stats.ox.ac.uk>

We really don't want this three times!

It looks like the archives (.a) you are making are broken, possibly 
because your path does not include the approriate Solaris tools such as 
ranlib.  To take the first example, cg_ should be in src/appl/libappl.a.

There is a pre-compiled version of R-2.4.1 available on SunFreeware, which 
might save you some pain.


On Tue, 30 Jan 2007, Andrew_Duba at wustl.edu wrote:

> I am trying to compile R-2.4.1 in 64-bit on Solaris 10 running on AMD
> hardware.  I am trying to do this with Sun Studio 11.
>
> My config.site looks like this:
>
> #! /bin/sh
> AR="/usr/ccs/bin/ar"
>
> TEX="/usr/local/teTeX/bin/i386-pc-solaris2.10/tex"
> LATEX="/usr/local/teTeX/bin/i386-pc-solaris2.10/latex"
> PDFTEX="/usr/local/teTeX/bin/i386-pc-solaris2.10/pdftex"
> PDFLATEX="/usr/local/teTeX/bin/i386-pc-solaris2.10/pdflatex"
> DVIPS="/usr/local/teTeX/bin/i386-pc-solaris2.10/dvips"
> MAKEINFO="/usr/local/teTeX/bin/i386-pc-solaris2.10/makeinfo"
>
> UCOMPILER="-xarch=amd64"
> UFLAGS="-xO5 -xlibmil"
>
> CC="/opt/SUNWspro/bin/cc $UCOMPILER"
> CFLAGS="$UFLAGS -xc99=%all -xlibmieee"
> F77="/opt/SUNWspro/bin/f95 $UCOMPILER"
> FFLAGS="$UFLAGS -fsimple=0"
> CXX="/opt/SUNWspro/bin/CC $UCOMPILER"
> CXXFLAGS="$UFLAGS -xlibmieee"
> FC="/opt/SUNWspro/bin/f95 $UCOMPILER"
> FCFLAGS="$UFLAGS -fsimple=0 -xlang=c99"
> LD="/usr/ccs/bin/amd64/ld"
> LDFLAGS="-L/usr/local/lib/64 -L/usr/sfw/lib/64 -L/lib/amd64
> -R/usr/local/lib/64 -R/usr/sfw/lib/64 -R/lib/amd64"
>
> R_BROWSER="/usr/sfw/bin/mozilla"
>
> MAIN_LD="/opt/SUNWspro/bin/cc -xarch=amd64"
> LIBnn=amd64
>
>
> The ./configure runs without incident but make breaks down here
>
> /opt/SUNWspro/bin/f95 -xarch=amd64   -xO5 -xlibmil -fsimple=0 -c xxxpr.f
> -o xxxpr.o
> /opt/SUNWspro/bin/cc -xarch=amd64 -I../../src/extra/zlib
> -I../../src/extra/bzip2 -I../../src/extra/pcre  -I. -I../../src/include
> -I../../src/include -I/usr/local/include -DHAVE_CONFIG_H
> -D__NO_MATH_INLINES  -xO5 -xlibmil -xc99=%all -xlibmieee -c mkdtemp.c -o
> mkdtemp.o
> /opt/SUNWspro/bin/cc -xarch=amd64  -L/usr/local/lib/64 -L/usr/sfw/lib/64
> -L/lib/amd64 -R/usr/local/lib/64 -R/usr/sfw/lib/64 -R/lib/amd64 -o R.bin
> Rmain.o CConverters.o CommandLineArgs.o Rdynload.o Renviron.o RNG.o
> apply.o arithmetic.o apse.o array.o attrib.o base.o bind.o builtin.o
> character.o coerce.o colors.o complex.o connections.o context.o cov.o
> cum.o dcf.o datetime.o debug.o deparse.o deriv.o dotcode.o dounzip.o
> dstruct.o duplicate.o engine.o envir.o errors.o eval.o format.o fourier.o
> gevents.o gram.o gram-ex.o graphics.o identical.o internet.o iosupport.o
> lapack.o list.o localecharset.o logic.o main.o mapply.o match.o memory.o
> model.o names.o objects.o optim.o optimize.o options.o par.o paste.o
> pcre.o platform.o plot.o plot3d.o plotmath.o print.o printarray.o
> printvector.o printutils.o qsort.o random.o regex.o registration.o relop.o
> rlocale.o saveload.o scan.o seq.o serialize.o size.o sort.o source.o
> split.o sprintf.o startup.o subassign.o subscript.o subset.o summary.o
> sysutils.o unique.o util.o version.o vfonts.o xxxpr.o  mkdtemp.o
> ../unix/libunix.a ../appl/libappl.a ../nmath/libnmath.a -L../../lib
> -lRblas -R/usr/local/lib/64 -R/usr/sfw/lib/64 -R/lib/amd64
> -R/opt/SUNWspro/lib/amd64 -L/opt/SUNWspro/lib/amd64
> -L/opt/SUNWspro/prod/lib/amd64 -L/usr/lib/amd64 -lfui -lfai -lfsu
> -lsunmath -lmtsk -lm  ../extra/zlib/libz.a ../extra/bzip2/libbz2.a
> ../extra/pcre/libpcre.a  ../extra/intl/libintl.a -lreadline -ltermcap
> -lnsl -lsocket -ldl -lm
> Undefined                       first referenced
> symbol                             in file
> cg_                                 registration.o
> ch_                                 registration.o
> rg_                                 registration.o
> rs_                                 registration.o
> bincount                            registration.o
> inflate                             connections.o
> Rf_df                               arithmetic.o
> Rf_dt                               arithmetic.o
> Rf_pf                               arithmetic.o
> Rf_pt                               arithmetic.o
> Rf_qf                               arithmetic.o
> Rf_qt                               arithmetic.o
> Rf_rf                               random.o
> Rf_rt                               random.o
> chol_                               registration.o
> gzeof                               connections.o
> do_syssleep                         names.o
> R_EditFiles                         platform.o
> R_getProcTime                       memory.o
> R_ExpandFileName                    Renviron.o
> R_OpenInitFile                      main.o
> R_getClockIncrement                 eval.o
> R_ReadConsole                       connections.o
> Rf_initialize_R                     Rmain.o
> call_dqags                          registration.o
> call_dqagi                          registration.o
> deflateEnd                          connections.o
> ptr_R_EditFile                      ../unix/libunix.a(edit.o)
> inflateEnd                          connections.o
> Rf_lchoose                          arithmetic.o
> R_ChooseFile                        platform.o
> lminfl_                             registration.o
> deflateInit2_                       connections.o
> N01_kind                            RNG.o
> do_machine                          names.o
> libintl_dcigettext                  ../extra/intl/libintl.a(dcgettext.o)
> R_max_col                           registration.o
> R_ClearerrConsole                   connections.o
> pcre_maketables                     character.o
> spline_eval                         registration.o
> spline_coef                         registration.o
> R_Busy                              main.o
> R_approx                            registration.o
> Rf_dnf                              arithmetic.o
> Rf_dnt                              arithmetic.o
> Rf_pnf                              arithmetic.o
> Rf_pnt                              arithmetic.o
> Rf_qnf                              arithmetic.o
> Rf_qnt                              arithmetic.o
> BZ2_bzRead                          connections.o
> Rf_choose                           arithmetic.o
> Brent_fmin                          optimize.o
> Rf_dbinom                           arithmetic.o
> Rf_dchisq                           arithmetic.o
> R_cumsum                            registration.o
> Rf_dgamma                           arithmetic.o
> Rf_dhyper                           arithmetic.o
> Rf_dlogis                           arithmetic.o
> Rf_dlnorm                           arithmetic.o
> Rf_dnbeta                           arithmetic.o
> Rf_dnorm4                           arithmetic.o
> fft_factor                          fourier.o
> Rf_fround                           arithmetic.o
> Rf_ftrunc                           arithmetic.o
> libintl_gettextparse                ../extra/intl/libintl.a(plural-exp.o)
> Rf_pbinom                           arithmetic.o
> Rf_pchisq                           arithmetic.o
> Rf_pgamma                           arithmetic.o
> Rf_phyper                           arithmetic.o
> Rf_plogis                           arithmetic.o
> Rf_plnorm                           arithmetic.o
> Rf_pnbeta                           arithmetic.o
> Rf_pnorm5                           arithmetic.o
> Rf_qbinom                           arithmetic.o
> R_pretty                            registration.o
> R_cpolyroot                         complex.o
> Rf_qchisq                           arithmetic.o
> Rf_ptukey                           arithmetic.o
> Rf_qgamma                           arithmetic.o
> Rf_qhyper                           arithmetic.o
> Rf_qlogis                           arithmetic.o
> Rf_qlnorm                           arithmetic.o
> Rf_qnbeta                           arithmetic.o
> Rf_qnorm5                           arithmetic.o
> bakslv                              registration.o
> Rf_rbinom                           random.o
> Rf_rchisq                           random.o
> Rf_qtukey                           arithmetic.o
> Rf_rgamma                           random.o
> Rf_rhyper                           random.o
> Rf_rlogis                           random.o
> Rf_rlnorm                           random.o
> R_rowsum                            registration.o
> str_signif                          registration.o
> Rf_pcauchy                          arithmetic.o
> dchdc_                              registration.o
> do_system                           names.o
> dpbfa_                              registration.o
> dpbsl_                              registration.o
> dpoco_                              registration.o
> dpodi_                              registration.o
> dpofa_                              registration.o
> dposl_                              registration.o
> dqrdc_                              registration.o
> dqrls_                              registration.o
> dqrsl_                              registration.o
> dsvdc_                              registration.o
> dtrco_                              registration.o
> dtrsl_                              registration.o
> wilcox_free                         registration.o
> fdhess                              optimize.o
> R_ShowFiles                         platform.o
> gzgetc                              connections.o
> gzopen                              connections.o
> gzread                              connections.o
> gzseek                              connections.o
> gztell                              connections.o
> BZ2_bzReadOpen                      connections.o
> R_zeroin                            optimize.o
> ptr_R_savehistory                   ../unix/libunix.a(stubs.o)
> loglin                              registration.o
> lowess                              registration.o
> machar                              platform.o
> ptr_R_loadhistory                   ../unix/libunix.a(stubs.o)
> Rf_pnchisq                          arithmetic.o
> Rf_pnbinom                          arithmetic.o
> optif9                              optimize.o
> rcont2                              random.o
> R_pretty0                           engine.o
> setulb                              optim.o
> Rf_qcauchy                          arithmetic.o
> UsingReadline                       platform.o
> Rf_pwilcox                          arithmetic.o
> find_interv_vec                     registration.o
> Rf_qnchisq                          arithmetic.o
> Rf_qnbinom                          arithmetic.o
> Rf_rcauchy                          random.o
> Rf_trigamma                         arithmetic.o
> massdist                            registration.o
> pcre_info                           pcre.o
> pcre_exec                           character.o
> Rf_beta                             arithmetic.o
> Rf_dexp                             arithmetic.o
> Rf_pexp                             arithmetic.o
> Rf_qexp                             arithmetic.o
> Rf_rexp                             random.o
> Rf_sign                             arithmetic.o
> libintl_nl_domain_bindings          ../extra/intl/libintl.a(bindtextdom.o)
> libintl_nl_current_default_domain   ../extra/intl/libintl.a(textdomain.o)
> Rf_qwilcox                          arithmetic.o
> libintl_nl_default_default_domain   ../extra/intl/libintl.a(textdomain.o)
> BZ2_bzWriteOpen                     connections.o
> bincode                             registration.o
> R_CleanTempDir                      main.o
> do_proctime                         names.o
> norm_rand                           optim.o
> Rf_rnchisq                          random.o
> Rf_rnbinom                          random.o
> R_running_as_main_program           Rmain.o
> Rf_psigamma                         arithmetic.o
> ch2inv_                             registration.o
> BZ2_bzWrite                         connections.o
> inflateInit2_                       connections.o
> Rf_rwilcox                          random.o
> R_CleanUp                           errors.o
> _nl_find_msg                        ../extra/intl/libintl.a(loadmsgcat.o)
> Rf_dsignrank                        arithmetic.o
> Rf_psignrank                        arithmetic.o
> Rf_rsignrank                        random.o
> Rf_qsignrank                        arithmetic.o
> Rf_dcauchy                          arithmetic.o
> deflate                             connections.o
> R_Suicide                           Renviron.o
> stemleaf                            registration.o
> Rf_digamma                          arithmetic.o
> do_sysinfo                          names.o
> Rf_dnchisq                          arithmetic.o
> Rf_dnbinom                          arithmetic.o
> pcre_study                          character.o
> R_ShowMessage                       CommandLineArgs.o
> dqrdc2_                             registration.o
> ptr_R_addhistory                    ../unix/libunix.a(stubs.o)
> Rf_InitFunctionHashing              Rdynload.o
> R_FlushConsole                      errors.o
> Rf_gamma_cody                       arithmetic.o
> Rf_dwilcox                          arithmetic.o
> deflateInit_                        ../extra/zlib/libz.a(compress.o)
> rmultinom                           random.o
> Rf_dbeta                            arithmetic.o
> Rf_dgeom                            arithmetic.o
> Rf_dpois                            arithmetic.o
> Rf_dunif                            arithmetic.o
> Rf_fmin2                            engine.o
> Rf_fmax2                            engine.o
> Rf_fprec                            arithmetic.o
> Rf_fsign                            complex.o
> Rf_imin2                            scan.o
> Rf_imax2                            bind.o
> Rf_lbeta                            arithmetic.o
> Rf_pbeta                            arithmetic.o
> Rf_pgeom                            arithmetic.o
> Rf_ppois                            arithmetic.o
> Rf_qbeta                            arithmetic.o
> Rf_punif                            arithmetic.o
> Rf_qgeom                            arithmetic.o
> Rf_qpois                            arithmetic.o
> Rf_rbeta                            random.o
> Rf_qunif                            arithmetic.o
> Rf_rgeom                            random.o
> Rf_rnorm                            random.o
> Rf_rpois                            random.o
> Rf_runif                            random.o
> Rf_bessel_i                         arithmetic.o
> Rf_bessel_j                         arithmetic.o
> Rf_bessel_k                         arithmetic.o
> Rf_bessel_y                         arithmetic.o
> Rf_lgammafn                         arithmetic.o
> BM_norm_keep                        RNG.o
> Rf_gammafn                          arithmetic.o
> inflateInit_                        ../extra/zlib/libz.a(uncompr.o)
> fft_work                            fourier.o
> Rf_pweibull                         arithmetic.o
> Rf_qweibull                         arithmetic.o
> Rf_rweibull                         random.o
> Rf_dweibull                         arithmetic.o
> libintl_nl_default_dirname          ../extra/intl/libintl.a(bindtextdom.o)
> R_WriteConsole                      printutils.o
> R_ResetConsole                      errors.o
> BZ2_bzWriteClose                    connections.o
> gzclose                             connections.o
> signrank_free                       registration.o
> gzwrite                             connections.o
> pcre_compile                        character.o
> BZ2_bzReadClose                     connections.o
> ld: fatal: Symbol referencing errors. No output written to R.bin
> make[3]: *** [R.bin] Error 1
> make[3]: Leaving directory `/home/aduba/source/R-2.4.1/src/main'
> make[2]: *** [R] Error 2
> make[2]: Leaving directory `/home/aduba/source/R-2.4.1/src/main'
> make[1]: *** [R] Error 1
> make[1]: Leaving directory `/home/aduba/source/R-2.4.1/src'
> make: *** [R] Error 1
>
> Any suggestions?
>
> Thanks in advance for your help.
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From GPetris at uark.edu  Tue Jan 30 23:17:06 2007
From: GPetris at uark.edu (Giovanni Petris)
Date: Tue, 30 Jan 2007 16:17:06 -0600 (CST)
Subject: [R] R packages
In-Reply-To: <A36876D3F8A5734FA84A4338135E7CC3EB96B9@BAN-MAILSRV03.Amba.com>
	(message from Shubha Vishwanath Karanth on Tue, 30 Jan 2007 20:55:11
	+0530)
References: <A36876D3F8A5734FA84A4338135E7CC3EB96B9@BAN-MAILSRV03.Amba.com>
Message-ID: <200701302217.l0UMH6vd015274@definetti.ddns.uark.edu>


Have you tried a search of CRAN? Seaching r-help archives may result
in useful information as well. 

As formulated, the questions are difficult to answer. There are
packages that use MCMC, Gibbs Sampling, Metropolis Hastings for
specific classes of models.

Best,
Giovanni Petris

> Date: Tue, 30 Jan 2007 20:55:11 +0530
> From: Shubha Vishwanath Karanth <shubhak at ambaresearch.com>
> Sender: r-help-bounces at stat.math.ethz.ch
> Precedence: list
> Thread-topic: R packages
> Thread-index: AcdEgtTn1ik2yhc/Qvav5FLBJea68Q==
> 
> Hi,
> 
>  
> 
> Do any body know which packages of R I need to go for the below topics?
> 
>  
> 
> 1.	Monte Carlo Markov chain (MCMC)
> 2.	Gibbs Sampling
> 3.	Metropolis Hastings
> 
>  
> 
> Thanks in advance...
> 
> Shubha
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From mkimpel at iupui.edu  Tue Jan 30 23:23:45 2007
From: mkimpel at iupui.edu (Kimpel, Mark William)
Date: Tue, 30 Jan 2007 17:23:45 -0500
Subject: [R] regexpr and parsing question
In-Reply-To: <971536df0701301355j77b8d05oabce276b61fa2bf7@mail.gmail.com>
References: <cc088e260701301247k6a7e4b6ewba868b3a4ad3cb8d@mail.gmail.com>
	<971536df0701301355j77b8d05oabce276b61fa2bf7@mail.gmail.com>
Message-ID: <836F00680EECD340A96AD34ECFF3B534B4AC88@iu-mssg-mbx106.ads.iu.edu>

The main problem I am trying to solve it this:

I am importing a tab delimited file whose first line contains only one
column, which is a descriptor of the form "col_1 col_2 col_3", i.e. the
colnames are not tab delineated but are separated by whitespace. I would
like to parse this first line and make such that it becomes the colnames
of the rest of the file, which I am reading into R using read.delim().
The file is so huge that I must do this in R.

My first question is this: What is the best way to accomplish what I
want to do?

My other questions revolve around some failed attempts on my part to
solve the problem on my own using regular expressions. I thought that
perhaps I could change the first line to "c("col_1", "col_2", "col_3")
using gsub. I was having trouble figuring out how R uses the backslash
character because I know that sometimes the backslash one would use in
Perl needs to be a double backslash in R.

Here is a sample of what I tried and what I got:

a<-"col_1 col_2 col_3"

> gsub("\\s", " " , a) 

[1] "col_1 col_2 col_3"

> gsub("\\s", "\\s" , a) 

[1] "col_1scol_2scol_3"

As you can see, it looks like R is taking a regular expression for
"pattern", but not taking it for "replacement". Why is this?

Assuming that I did want to solve my original problem with gsub and then
turn the string into an R object, how would I get gsub to return
"c("col_1", "col_2", "col_3") using my original string?

Finally, is there a way to declare a string as a regular expression so
that R sees it the same way other languages, such as Perl do, i.e. make
the backslash be interpreted the same way? For someone who is just
learning regular expressions as I am, it is very frustrating to read
about them in references and then have to translate what I've learned
into R syntax. I was thinking that instead of enclosing the string in
"", one could use THIS.IS.A.REGULAR.EXPRESSION(), similar to the way we
use I() in formulae.

These are a bunch of questions, but obviously I have a lot to learn!

Thanks,

Mark

Mark W. Kimpel MD 

 

(317) 490-5129 Work, & Mobile

 

(317) 663-0513 Home (no voice mail please)

1-(317)-536-2730 FAX


From ripley at stats.ox.ac.uk  Tue Jan 30 23:24:25 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 30 Jan 2007 22:24:25 +0000 (GMT)
Subject: [R] silent loading of packages
In-Reply-To: <560967.58255.qm@web56208.mail.re3.yahoo.com>
References: <560967.58255.qm@web56208.mail.re3.yahoo.com>
Message-ID: <Pine.LNX.4.64.0701302213020.24341@gannet.stats.ox.ac.uk>

It depends on the 'message'. In this case

> library(VGAM, warn.conflicts=FALSE)
> suppressMessages(library(VGAM))

both work.  (How did you manage to miss the first?)

In general, it depends on whether the 'message' is a message in the sense 
of message() or produced some other way.  sink() would work, but these are 
*messages*, so how did you use it?


On Tue, 30 Jan 2007, johan Faux wrote:

> I would like to turn off all the messages during
> library(aPackage) or
> require(aPackage)
>
> I tried different commands: invisible, capture.output, sink but none of them is working.
>
> For example, loading VGAM, gives a lot of unnecessary messages:
>
>> library(VGAM)
>
> Attaching package: 'VGAM'
>
>
>        The following object(s) are masked from package:splines :
>
>         bs
>
>        The following object(s) are masked from package:splines :
>
>         ns
>
>
>        The following object(s) are masked from package:boot :
>
>         logit
>
>        The following object(s) are masked from package:boot :
>
>         simplex
>
>
>        The following object(s) are masked from package:stats :
>
>         glm
>
>        The following object(s) are masked from package:stats :
>
>         lm
>
>        The following object(s) are masked from package:stats :
>
>         poly
>
>        The following object(s) are masked from package:stats :
>
>         predict.glm
>
>        The following object(s) are masked from package:stats :
>
>         predict.lm
>
>        The following object(s) are masked from package:stats :
>
>         predict.mlm
>
>
>        The following object(s) are masked from package:base :
>
>         scale.default
>
>
>
> Any hint/help will be appreciated.
>
>
>
>
> ---------------------------------
> Expecting? Get great news right away with email Auto-Check.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Achim.Zeileis at wu-wien.ac.at  Tue Jan 30 23:33:41 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 30 Jan 2007 23:33:41 +0100 (CET)
Subject: [R] R packages
In-Reply-To: <200701302217.l0UMH6vd015274@definetti.ddns.uark.edu>
Message-ID: <Pine.LNX.4.44.0701302332340.10150-100000@disco.wu-wien.ac.at>

Also have a look at the excellent Bayesian CRAN task view
  http://CRAN.R-project.org/src/contrib/Views/Bayesian.html
Z

On Tue, 30 Jan 2007, Giovanni Petris wrote:

> Have you tried a search of CRAN? Seaching r-help archives may result
> in useful information as well.
>
> As formulated, the questions are difficult to answer. There are
> packages that use MCMC, Gibbs Sampling, Metropolis Hastings for
> specific classes of models.
>
> Best,
> Giovanni Petris
>
> > Date: Tue, 30 Jan 2007 20:55:11 +0530
> > From: Shubha Vishwanath Karanth <shubhak at ambaresearch.com>
> > Sender: r-help-bounces at stat.math.ethz.ch
> > Precedence: list
> > Thread-topic: R packages
> > Thread-index: AcdEgtTn1ik2yhc/Qvav5FLBJea68Q==
> >
> > Hi,
> >
> >
> >
> > Do any body know which packages of R I need to go for the below topics?
> >
> >
> >
> > 1.	Monte Carlo Markov chain (MCMC)
> > 2.	Gibbs Sampling
> > 3.	Metropolis Hastings
> >
> >
> >
> > Thanks in advance...
> >
> > Shubha
> >
> >
> > 	[[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From p.dalgaard at biostat.ku.dk  Tue Jan 30 23:35:06 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 30 Jan 2007 23:35:06 +0100
Subject: [R] Difficulty with compiling R-2.4.1 on solaris 10
In-Reply-To: <Pine.GSO.4.58.0701301600460.3888@ascc1.artsci.wustl.edu>
References: <Pine.GSO.4.58.0701301600460.3888@ascc1.artsci.wustl.edu>
Message-ID: <45BFC81A.8010404@biostat.ku.dk>

Andrew John Duba wrote:
> I am trying to compile R-2.4.1 in 64-bit on Solaris 10 running on AMD
> hardware.  I am trying to do this with Sun Studio 11.
>
>   
Oi!! We heard you the first three times. And you are still on the wrong 
list (this belongs on r-devel.)

Spamming the list will only annoy the people you are trying to get to 
help you....

Unfortunately, Solaris users are becoming scarce, and those with 
commercial compilers even more.
Some of the missing symbols, e.g. Rf_qt, are supposed to sit inside 
libnmath.a, so checking whether that looks OK (use the 'nm' command, I 
think) could be helpful.

> My config.site looks like this:
>
> #! /bin/sh
> AR="/usr/ccs/bin/ar"
>
> TEX="/usr/local/teTeX/bin/i386-pc-solaris2.10/tex"
> LATEX="/usr/local/teTeX/bin/i386-pc-solaris2.10/latex"
> PDFTEX="/usr/local/teTeX/bin/i386-pc-solaris2.10/pdftex"
> PDFLATEX="/usr/local/teTeX/bin/i386-pc-solaris2.10/pdflatex"
> DVIPS="/usr/local/teTeX/bin/i386-pc-solaris2.10/dvips"
> MAKEINFO="/usr/local/teTeX/bin/i386-pc-solaris2.10/makeinfo"
>
> UCOMPILER="-xarch=amd64"
> UFLAGS="-xO5 -xlibmil"
>
> CC="/opt/SUNWspro/bin/cc $UCOMPILER"
> CFLAGS="$UFLAGS -xc99=%all -xlibmieee"
> F77="/opt/SUNWspro/bin/f95 $UCOMPILER"
> FFLAGS="$UFLAGS -fsimple=0"
> CXX="/opt/SUNWspro/bin/CC $UCOMPILER"
> CXXFLAGS="$UFLAGS -xlibmieee"
> FC="/opt/SUNWspro/bin/f95 $UCOMPILER"
> FCFLAGS="$UFLAGS -fsimple=0 -xlang=c99"
> LD="/usr/ccs/bin/amd64/ld"
> LDFLAGS="-L/usr/local/lib/64 -L/usr/sfw/lib/64 -L/lib/amd64
> -R/usr/local/lib/64 -R/usr/sfw/lib/64 -R/lib/amd64"
>
> R_BROWSER="/usr/sfw/bin/mozilla"
>
> MAIN_LD="/opt/SUNWspro/bin/cc -xarch=amd64"
> LIBnn=amd64
>
>
> The ./configure runs without incident but make breaks down and the linker
> complains about undefined symbols.  The symbols that the linker complains
> about are in Rmath.h.  I have tried adding the folder with Rmath.h to the
> linkers path but that has not helped.  Does anyone have any suggestions?
>
> If someone needs it below is an excript from the make session
>
> Thanks in advance for your help.
>
> /opt/SUNWspro/bin/f95 -xarch=amd64   -xO5 -xlibmil -fsimple=0 -c xxxpr.f
> -o xxxpr.o
> /opt/SUNWspro/bin/cc -xarch=amd64 -I../../src/extra/zlib
> -I../../src/extra/bzip2 -I../../src/extra/pcre  -I. -I../../src/include
> -I../../src/include -I/usr/local/include -DHAVE_CONFIG_H
> -D__NO_MATH_INLINES  -xO5 -xlibmil -xc99=%all -xlibmieee -c mkdtemp.c -o
> mkdtemp.o
> /opt/SUNWspro/bin/cc -xarch=amd64  -L/usr/local/lib/64 -L/usr/sfw/lib/64
> -L/lib/amd64 -R/usr/local/lib/64 -R/usr/sfw/lib/64 -R/lib/amd64 -o R.bin
> Rmain.o CConverters.o CommandLineArgs.o Rdynload.o Renviron.o RNG.o
> apply.o arithmetic.o apse.o array.o attrib.o base.o bind.o builtin.o
> character.o coerce.o colors.o complex.o connections.o context.o cov.o
> cum.o dcf.o datetime.o debug.o deparse.o deriv.o dotcode.o dounzip.o
> dstruct.o duplicate.o engine.o envir.o errors.o eval.o format.o fourier.o
> gevents.o gram.o gram-ex.o graphics.o identical.o internet.o iosupport.o
> lapack.o list.o localecharset.o logic.o main.o mapply.o match.o memory.o
> model.o names.o objects.o optim.o optimize.o options.o par.o paste.o
> pcre.o platform.o plot.o plot3d.o plotmath.o print.o printarray.o
> printvector.o printutils.o qsort.o random.o regex.o registration.o relop.o
> rlocale.o saveload.o scan.o seq.o serialize.o size.o sort.o source.o
> split.o sprintf.o startup.o subassign.o subscript.o subset.o summary.o
> sysutils.o unique.o util.o version.o vfonts.o xxxpr.o  mkdtemp.o
> ../unix/libunix.a ../appl/libappl.a ../nmath/libnmath.a -L../../lib
> -lRblas -R/usr/local/lib/64 -R/usr/sfw/lib/64 -R/lib/amd64
> -R/opt/SUNWspro/lib/amd64 -L/opt/SUNWspro/lib/amd64
> -L/opt/SUNWspro/prod/lib/amd64 -L/usr/lib/amd64 -lfui -lfai -lfsu
> -lsunmath -lmtsk -lm  ../extra/zlib/libz.a ../extra/bzip2/libbz2.a
> ../extra/pcre/libpcre.a  ../extra/intl/libintl.a -lreadline -ltermcap
> -lnsl -lsocket -ldl -lm
> Undefined                       first referenced
>  symbol                             in file
> cg_                                 registration.o
> ch_                                 registration.o
> rg_                                 registration.o
> rs_                                 registration.o
> bincount                            registration.o
> inflate                             connections.o
> Rf_df                               arithmetic.o
> Rf_dt                               arithmetic.o
> Rf_pf                               arithmetic.o
> Rf_pt                               arithmetic.o
> Rf_qf                               arithmetic.o
> Rf_qt                               arithmetic.o
> Rf_rf                               random.o
> Rf_rt                               random.o
> chol_                               registration.o
> gzeof                               connections.o
> do_syssleep                         names.o
> R_EditFiles                         platform.o
> R_getProcTime                       memory.o
> R_ExpandFileName                    Renviron.o
> R_OpenInitFile                      main.o
> R_getClockIncrement                 eval.o
> R_ReadConsole                       connections.o
> Rf_initialize_R                     Rmain.o
> call_dqags                          registration.o
> call_dqagi                          registration.o
> deflateEnd                          connections.o
> ptr_R_EditFile                      ../unix/libunix.a(edit.o)
> inflateEnd                          connections.o
> Rf_lchoose                          arithmetic.o
> R_ChooseFile                        platform.o
> lminfl_                             registration.o
> deflateInit2_                       connections.o
> N01_kind                            RNG.o
> do_machine                          names.o
> libintl_dcigettext                  ../extra/intl/libintl.a(dcgettext.o)
> R_max_col                           registration.o
> R_ClearerrConsole                   connections.o
> pcre_maketables                     character.o
> spline_eval                         registration.o
> spline_coef                         registration.o
> R_Busy                              main.o
> R_approx                            registration.o
> Rf_dnf                              arithmetic.o
> Rf_dnt                              arithmetic.o
> Rf_pnf                              arithmetic.o
> Rf_pnt                              arithmetic.o
> Rf_qnf                              arithmetic.o
> Rf_qnt                              arithmetic.o
> BZ2_bzRead                          connections.o
> Rf_choose                           arithmetic.o
> Brent_fmin                          optimize.o
> Rf_dbinom                           arithmetic.o
> Rf_dchisq                           arithmetic.o
> R_cumsum                            registration.o
> Rf_dgamma                           arithmetic.o
> Rf_dhyper                           arithmetic.o
> Rf_dlogis                           arithmetic.o
> Rf_dlnorm                           arithmetic.o
> Rf_dnbeta                           arithmetic.o
> Rf_dnorm4                           arithmetic.o
> fft_factor                          fourier.o
> Rf_fround                           arithmetic.o
> Rf_ftrunc                           arithmetic.o
> libintl_gettextparse                ../extra/intl/libintl.a(plural-exp.o)
> Rf_pbinom                           arithmetic.o
> Rf_pchisq                           arithmetic.o
> Rf_pgamma                           arithmetic.o
> Rf_phyper                           arithmetic.o
> Rf_plogis                           arithmetic.o
> Rf_plnorm                           arithmetic.o
> Rf_pnbeta                           arithmetic.o
> Rf_pnorm5                           arithmetic.o
> Rf_qbinom                           arithmetic.o
> R_pretty                            registration.o
> R_cpolyroot                         complex.o
> Rf_qchisq                           arithmetic.o
> Rf_ptukey                           arithmetic.o
> Rf_qgamma                           arithmetic.o
> Rf_qhyper                           arithmetic.o
> Rf_qlogis                           arithmetic.o
> Rf_qlnorm                           arithmetic.o
> Rf_qnbeta                           arithmetic.o
> Rf_qnorm5                           arithmetic.o
> bakslv                              registration.o
> Rf_rbinom                           random.o
> Rf_rchisq                           random.o
> Rf_qtukey                           arithmetic.o
> Rf_rgamma                           random.o
> Rf_rhyper                           random.o
> Rf_rlogis                           random.o
> Rf_rlnorm                           random.o
> R_rowsum                            registration.o
> str_signif                          registration.o
> Rf_pcauchy                          arithmetic.o
> dchdc_                              registration.o
> do_system                           names.o
> dpbfa_                              registration.o
> dpbsl_                              registration.o
> dpoco_                              registration.o
> dpodi_                              registration.o
> dpofa_                              registration.o
> dposl_                              registration.o
> dqrdc_                              registration.o
> dqrls_                              registration.o
> dqrsl_                              registration.o
> dsvdc_                              registration.o
> dtrco_                              registration.o
> dtrsl_                              registration.o
> wilcox_free                         registration.o
> fdhess                              optimize.o
> R_ShowFiles                         platform.o
> gzgetc                              connections.o
> gzopen                              connections.o
> gzread                              connections.o
> gzseek                              connections.o
> gztell                              connections.o
> BZ2_bzReadOpen                      connections.o
> R_zeroin                            optimize.o
> ptr_R_savehistory                   ../unix/libunix.a(stubs.o)
> loglin                              registration.o
> lowess                              registration.o
> machar                              platform.o
> ptr_R_loadhistory                   ../unix/libunix.a(stubs.o)
> Rf_pnchisq                          arithmetic.o
> Rf_pnbinom                          arithmetic.o
> optif9                              optimize.o
> rcont2                              random.o
> R_pretty0                           engine.o
> setulb                              optim.o
> Rf_qcauchy                          arithmetic.o
> UsingReadline                       platform.o
> Rf_pwilcox                          arithmetic.o
> find_interv_vec                     registration.o
> Rf_qnchisq                          arithmetic.o
> Rf_qnbinom                          arithmetic.o
> Rf_rcauchy                          random.o
> Rf_trigamma                         arithmetic.o
> massdist                            registration.o
> pcre_info                           pcre.o
> pcre_exec                           character.o
> Rf_beta                             arithmetic.o
> Rf_dexp                             arithmetic.o
> Rf_pexp                             arithmetic.o
> Rf_qexp                             arithmetic.o
> Rf_rexp                             random.o
> Rf_sign                             arithmetic.o
> libintl_nl_domain_bindings          ../extra/intl/libintl.a(bindtextdom.o)
> libintl_nl_current_default_domain   ../extra/intl/libintl.a(textdomain.o)
> Rf_qwilcox                          arithmetic.o
> libintl_nl_default_default_domain   ../extra/intl/libintl.a(textdomain.o)
> BZ2_bzWriteOpen                     connections.o
> bincode                             registration.o
> R_CleanTempDir                      main.o
> do_proctime                         names.o
> norm_rand                           optim.o
> Rf_rnchisq                          random.o
> Rf_rnbinom                          random.o
> R_running_as_main_program           Rmain.o
> Rf_psigamma                         arithmetic.o
> ch2inv_                             registration.o
> BZ2_bzWrite                         connections.o
> inflateInit2_                       connections.o
> Rf_rwilcox                          random.o
> R_CleanUp                           errors.o
> _nl_find_msg                        ../extra/intl/libintl.a(loadmsgcat.o)
> Rf_dsignrank                        arithmetic.o
> Rf_psignrank                        arithmetic.o
> Rf_rsignrank                        random.o
> Rf_qsignrank                        arithmetic.o
> Rf_dcauchy                          arithmetic.o
> deflate                             connections.o
> R_Suicide                           Renviron.o
> stemleaf                            registration.o
> Rf_digamma                          arithmetic.o
> do_sysinfo                          names.o
> Rf_dnchisq                          arithmetic.o
> Rf_dnbinom                          arithmetic.o
> pcre_study                          character.o
> R_ShowMessage                       CommandLineArgs.o
> dqrdc2_                             registration.o
> ptr_R_addhistory                    ../unix/libunix.a(stubs.o)
> Rf_InitFunctionHashing              Rdynload.o
> R_FlushConsole                      errors.o
> Rf_gamma_cody                       arithmetic.o
> Rf_dwilcox                          arithmetic.o
> deflateInit_                        ../extra/zlib/libz.a(compress.o)
> rmultinom                           random.o
> Rf_dbeta                            arithmetic.o
> Rf_dgeom                            arithmetic.o
> Rf_dpois                            arithmetic.o
> Rf_dunif                            arithmetic.o
> Rf_fmin2                            engine.o
> Rf_fmax2                            engine.o
> Rf_fprec                            arithmetic.o
> Rf_fsign                            complex.o
> Rf_imin2                            scan.o
> Rf_imax2                            bind.o
> Rf_lbeta                            arithmetic.o
> Rf_pbeta                            arithmetic.o
> Rf_pgeom                            arithmetic.o
> Rf_ppois                            arithmetic.o
> Rf_qbeta                            arithmetic.o
> Rf_punif                            arithmetic.o
> Rf_qgeom                            arithmetic.o
> Rf_qpois                            arithmetic.o
> Rf_rbeta                            random.o
> Rf_qunif                            arithmetic.o
> Rf_rgeom                            random.o
> Rf_rnorm                            random.o
> Rf_rpois                            random.o
> Rf_runif                            random.o
> Rf_bessel_i                         arithmetic.o
> Rf_bessel_j                         arithmetic.o
> Rf_bessel_k                         arithmetic.o
> Rf_bessel_y                         arithmetic.o
> Rf_lgammafn                         arithmetic.o
> BM_norm_keep                        RNG.o
> Rf_gammafn                          arithmetic.o
> inflateInit_                        ../extra/zlib/libz.a(uncompr.o)
> fft_work                            fourier.o
> Rf_pweibull                         arithmetic.o
> Rf_qweibull                         arithmetic.o
> Rf_rweibull                         random.o
> Rf_dweibull                         arithmetic.o
> libintl_nl_default_dirname          ../extra/intl/libintl.a(bindtextdom.o)
> R_WriteConsole                      printutils.o
> R_ResetConsole                      errors.o
> BZ2_bzWriteClose                    connections.o
> gzclose                             connections.o
> signrank_free                       registration.o
> gzwrite                             connections.o
> pcre_compile                        character.o
> BZ2_bzReadClose                     connections.o
> ld: fatal: Symbol referencing errors. No output written to R.bin
> make[3]: *** [R.bin] Error 1
> make[3]: Leaving directory `/home/aduba/source/R-2.4.1/src/main'
> make[2]: *** [R] Error 2
> make[2]: Leaving directory `/home/aduba/source/R-2.4.1/src/main'
> make[1]: *** [R] Error 1
> make[1]: Leaving directory `/home/aduba/source/R-2.4.1/src'
> make: *** [R] Error 1
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Wed Jan 31 00:16:25 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 30 Jan 2007 18:16:25 -0500
Subject: [R] regexpr and parsing question
In-Reply-To: <836F00680EECD340A96AD34ECFF3B534B4AC88@iu-mssg-mbx106.ads.iu.edu>
References: <cc088e260701301247k6a7e4b6ewba868b3a4ad3cb8d@mail.gmail.com>
	<971536df0701301355j77b8d05oabce276b61fa2bf7@mail.gmail.com>
	<836F00680EECD340A96AD34ECFF3B534B4AC88@iu-mssg-mbx106.ads.iu.edu>
Message-ID: <971536df0701301516p1d7d9f51y13a58362fbf8ada3@mail.gmail.com>

Both spaces and tabs are whitespace so this
should be good enough (unless you can
have empty fields):

read.table("myfile.dat", header = TRUE)

See the sep= argument in ?read.table .

Although I don't think you really need this, here are
some regular expressions for processing a header
into the form you asked for.  The first line places
quotes around the names, the second one inserts
commas and the last one adds c( and ).

s <- gsub('(\\S+)', '"\\1"', 'col1 col2 col3')
s <- gsub("(\\S+) ", "\\1, ", s)
sub("(.*)", "c(\\1)", s)


On 1/30/07, Kimpel, Mark William <mkimpel at iupui.edu> wrote:
> The main problem I am trying to solve it this:
>
> I am importing a tab delimited file whose first line contains only one
> column, which is a descriptor of the form "col_1 col_2 col_3", i.e. the
> colnames are not tab delineated but are separated by whitespace. I would
> like to parse this first line and make such that it becomes the colnames
> of the rest of the file, which I am reading into R using read.delim().
> The file is so huge that I must do this in R.
>
> My first question is this: What is the best way to accomplish what I
> want to do?
>
> My other questions revolve around some failed attempts on my part to
> solve the problem on my own using regular expressions. I thought that
> perhaps I could change the first line to "c("col_1", "col_2", "col_3")
> using gsub. I was having trouble figuring out how R uses the backslash
> character because I know that sometimes the backslash one would use in
> Perl needs to be a double backslash in R.
>
> Here is a sample of what I tried and what I got:
>
> a<-"col_1 col_2 col_3"
>
> > gsub("\\s", " " , a)
>
> [1] "col_1 col_2 col_3"
>
> > gsub("\\s", "\\s" , a)
>
> [1] "col_1scol_2scol_3"
>
> As you can see, it looks like R is taking a regular expression for
> "pattern", but not taking it for "replacement". Why is this?
>
> Assuming that I did want to solve my original problem with gsub and then
> turn the string into an R object, how would I get gsub to return
> "c("col_1", "col_2", "col_3") using my original string?
>
> Finally, is there a way to declare a string as a regular expression so
> that R sees it the same way other languages, such as Perl do, i.e. make
> the backslash be interpreted the same way? For someone who is just
> learning regular expressions as I am, it is very frustrating to read
> about them in references and then have to translate what I've learned
> into R syntax. I was thinking that instead of enclosing the string in
> "", one could use THIS.IS.A.REGULAR.EXPRESSION(), similar to the way we
> use I() in formulae.
>
> These are a bunch of questions, but obviously I have a lot to learn!
>
> Thanks,
>
> Mark
>
> Mark W. Kimpel MD
>
>
>
> (317) 490-5129 Work, & Mobile
>
>
>
> (317) 663-0513 Home (no voice mail please)
>
> 1-(317)-536-2730 FAX
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From marc_schwartz at comcast.net  Wed Jan 31 00:21:44 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 30 Jan 2007 17:21:44 -0600
Subject: [R] regexpr and parsing question
In-Reply-To: <836F00680EECD340A96AD34ECFF3B534B4AC88@iu-mssg-mbx106.ads.iu.edu>
References: <cc088e260701301247k6a7e4b6ewba868b3a4ad3cb8d@mail.gmail.com>
	<971536df0701301355j77b8d05oabce276b61fa2bf7@mail.gmail.com>
	<836F00680EECD340A96AD34ECFF3B534B4AC88@iu-mssg-mbx106.ads.iu.edu>
Message-ID: <1170199304.4908.85.camel@localhost.localdomain>

On Tue, 2007-01-30 at 17:23 -0500, Kimpel, Mark William wrote:
> The main problem I am trying to solve it this:
> 
> I am importing a tab delimited file whose first line contains only one
> column, which is a descriptor of the form "col_1 col_2 col_3", i.e. the
> colnames are not tab delineated but are separated by whitespace. I would
> like to parse this first line and make such that it becomes the colnames
> of the rest of the file, which I am reading into R using read.delim().
> The file is so huge that I must do this in R.
> 
> My first question is this: What is the best way to accomplish what I
> want to do?

Mark,

The first thing that comes to mind is a two pass approach on the file:

First pass: (using example file with your first line)

# Get the first line into a vector to set the colnames for the DF
# during the second pass
ColNames <- unlist(read.table("test.txt", nrow = 1, as.is = TRUE))

> str(ColNames)
 Named chr [1:3] "col_1" "col_2" "col_3"
 - attr(*, "names")= chr [1:3] "V1" "V2" "V3"


Second pass:

# Now read the rest of the file, skipping the first line
DF <- read.delim("test.txt", skip = 1, col.names = ColNames)


I believe that should get you the full data set and set the colnames
based upon the first line. This should pretty much obviate the need for
everything below here.

> My other questions revolve around some failed attempts on my part to
> solve the problem on my own using regular expressions. I thought that
> perhaps I could change the first line to "c("col_1", "col_2", "col_3")
> using gsub. I was having trouble figuring out how R uses the backslash
> character because I know that sometimes the backslash one would use in
> Perl needs to be a double backslash in R.

You would not want to change the first line as you have it above, as it
would not be parsed properly using read.table() family functions.

> Here is a sample of what I tried and what I got:
> 
> a<-"col_1 col_2 col_3"
> 
> > gsub("\\s", " " , a) 
> 
> [1] "col_1 col_2 col_3"
> 
> > gsub("\\s", "\\s" , a) 
> 
> [1] "col_1scol_2scol_3"
> 
> As you can see, it looks like R is taking a regular expression for
> "pattern", but not taking it for "replacement". Why is this?

There are various settings for how regex are interpreted by/within R.
See ?grep and note the various arguments to the functions there and how
they impact R's behavior here.

Also, note that there is a difference (to further complicate your
life...) between the characters that R displays by default using print()
and how they are displayed using cat(). See below.

> a
[1] "col_1 col_2 col_3"

> gsub(" ", ", " , a)
[1] "col_1, col_2, col_3"

or to get you to your vector statement above:

Note the result here:

> paste("c(\"", gsub(" ", "\", \"" , a), "\")", sep = "")
[1] "c(\"col_1\", \"col_2\", \"col_3\")"


Now see how it displays when the escaped double quote chars are
interpreted properly using cat():

> cat(paste("c(\"", gsub(" ", "\", \"" , a), "\")", sep = ""), "\n")
c("col_1", "col_2", "col_3") 


> Assuming that I did want to solve my original problem with gsub and then
> turn the string into an R object, how would I get gsub to return
> "c("col_1", "col_2", "col_3") using my original string?

Again, note the two pass solution above.  It's easier, unless you would
want to consider using awk/sed from a CLI, which I generally avoid at
all costs...

> Finally, is there a way to declare a string as a regular expression so
> that R sees it the same way other languages, such as Perl do, i.e. make
> the backslash be interpreted the same way? For someone who is just
> learning regular expressions as I am, it is very frustrating to read
> about them in references and then have to translate what I've learned
> into R syntax. I was thinking that instead of enclosing the string in
> "", one could use THIS.IS.A.REGULAR.EXPRESSION(), similar to the way we
> use I() in formulae.

Part of the challenge is noting the different behaviors of regex within
R and how that behavior is affected by the aforementioned arguments.
Also, noting how the output is displayed within R relative to the
interpretation of escaped characters as is seen above.

> These are a bunch of questions, but obviously I have a lot to learn!
> 
> Thanks,
> 
> Mark


HTH,

Marc Schwartz


From davidkat at davidkatzconsulting.com  Wed Jan 31 00:31:47 2007
From: davidkat at davidkatzconsulting.com (davidkat at davidkatzconsulting.com)
Date: Tue, 30 Jan 2007 15:31:47 -0800
Subject: [R] SparseM and Stepwise Problem
Message-ID: <45BF64E3.3335.1D54E65@davidkat.davidkatzconsulting.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070130/3feb841b/attachment.pl 

From AKniss at uwyo.edu  Wed Jan 31 00:44:09 2007
From: AKniss at uwyo.edu (Andrew R. Kniss)
Date: Tue, 30 Jan 2007 16:44:09 -0700
Subject: [R] lme : Error in y[revOrder] - Fitted : non-conformable arrays
Message-ID: <9F8B441CE67B1F49933DDB55BD0381AC01D8A308@TELEGRAPH1.uwyo.edu>

Greetings R-helpers,
I am attempting to fit an lme() while specifying a correlation
structure, but I'm getting into trouble long before I get to that point.
I am receiving the error:

Error in y[revOrder] - Fitted : non-conformable arrays

It doesn't seem to matter how simple or complex the model I specify is,
it always gives this same error message.  This makes me suspect
something is wrong with my data set, but I'm not sure where to begin
looking. I can run through the Pinheiro & Bates example on p.240 in a
simplified form, so I know that everything with my installation (R
v2.4.1; nlme v3.1-79; Windows XP) is working fine.  However if I
construct a groupedData object similar in form to the Ovary example, I
will receive the same error, regardless of the model I specify.  I can
plot the groupedData object and get a figure very similar to Fig. 5.10
in Pinheiro & Bates.  I have provided summarized output with a very
simple lme() model, and the exact message.  Any ideas on why the error
is showing up and how I might correct it?

#### Attempting to use my own data ####
> groupedData(CHEAL~rel.year|blk,data=seed.cb[,c(5,1,7)])->cheal.grp
> summary(cheal.grp)
  blk        rel.year     CHEAL      
 I  :36   Min.   :0   Min.   :15.96  
 II :36   1st Qu.:2   1st Qu.:56.70  
 III:36   Median :4   Median :70.76  
 IV :36   Mean   :4   Mean   :68.98  
          3rd Qu.:6   3rd Qu.:82.63  
          Max.   :8   Max.   :99.13  
> lme(CHEAL~rel.year,data=cheal.grp)->cheal.lme
Error in y[revOrder] - Fitted : non-conformable arrays


#### Everything works fine using the Pinheiro & Bates example
#### on page 240 (in a simplified form)
> summary(Ovary)
      Mare          Time           follicles    
 8      : 31   Min.   :-0.1667   Min.   : 1.00  
 4      : 29   1st Qu.: 0.1667   1st Qu.: 8.00  
 5      : 29   Median : 0.5000   Median :12.00  
 1      : 29   Mean   : 0.5000   Mean   :12.04  
 6      : 29   3rd Qu.: 0.8333   3rd Qu.:15.00  
 10     : 29   Max.   : 1.1667   Max.   :25.00  
 (Other):132 
> lme(follicles~Time,data=Ovary)->ovary.lme
> summary(ovary.lme)
Linear mixed-effects model fit by REML
 Data: Ovary 
       AIC      BIC    logLik
  1748.103 1770.445 -868.0515

Random effects:
 Formula: ~Time | Mare
 Structure: General positive-definite
            StdDev   Corr  
(Intercept) 2.682827 (Intr)
Time        3.787545 -0.159
Residual    3.783400       

Fixed effects: follicles ~ Time 
               Value Std.Error  DF   t-value p-value
(Intercept) 10.86191 0.8828363 296 12.303430  0.0000
Time         2.28347 1.2719903 296  1.795194  0.0736
 Correlation: 
     (Intr)
Time -0.27 

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.74071048 -0.62051954 -0.05461339  0.70634887  2.70641284 

Number of Observations: 308
Number of Groups: 11 
>




Andrew Kniss
University of Wyoming
Department 3354
1000 E. University Ave.
Laramie WY, 82071

Email:   akniss at uwyo.edu
Office:  (307) 766-3949
Fax:     (307) 766-5549


From ggrothendieck at gmail.com  Wed Jan 31 00:52:11 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 30 Jan 2007 18:52:11 -0500
Subject: [R] regexpr and parsing question
In-Reply-To: <971536df0701301516p1d7d9f51y13a58362fbf8ada3@mail.gmail.com>
References: <cc088e260701301247k6a7e4b6ewba868b3a4ad3cb8d@mail.gmail.com>
	<971536df0701301355j77b8d05oabce276b61fa2bf7@mail.gmail.com>
	<836F00680EECD340A96AD34ECFF3B534B4AC88@iu-mssg-mbx106.ads.iu.edu>
	<971536df0701301516p1d7d9f51y13a58362fbf8ada3@mail.gmail.com>
Message-ID: <971536df0701301552j1a72a087x68ae3a85bfd115fb@mail.gmail.com>

And here is an alternative to the regular expressions (although again
I don't think you really need any of this):

> capture.output(dput(strsplit("col1 col2 col3", " ")[[1]]))
[1] "c(\"col1\", \"col2\", \"col3\")"

On 1/30/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Both spaces and tabs are whitespace so this
> should be good enough (unless you can
> have empty fields):
>
> read.table("myfile.dat", header = TRUE)
>
> See the sep= argument in ?read.table .
>
> Although I don't think you really need this, here are
> some regular expressions for processing a header
> into the form you asked for.  The first line places
> quotes around the names, the second one inserts
> commas and the last one adds c( and ).
>
> s <- gsub('(\\S+)', '"\\1"', 'col1 col2 col3')
> s <- gsub("(\\S+) ", "\\1, ", s)
> sub("(.*)", "c(\\1)", s)
>
>
> On 1/30/07, Kimpel, Mark William <mkimpel at iupui.edu> wrote:
> > The main problem I am trying to solve it this:
> >
> > I am importing a tab delimited file whose first line contains only one
> > column, which is a descriptor of the form "col_1 col_2 col_3", i.e. the
> > colnames are not tab delineated but are separated by whitespace. I would
> > like to parse this first line and make such that it becomes the colnames
> > of the rest of the file, which I am reading into R using read.delim().
> > The file is so huge that I must do this in R.
> >
> > My first question is this: What is the best way to accomplish what I
> > want to do?
> >
> > My other questions revolve around some failed attempts on my part to
> > solve the problem on my own using regular expressions. I thought that
> > perhaps I could change the first line to "c("col_1", "col_2", "col_3")
> > using gsub. I was having trouble figuring out how R uses the backslash
> > character because I know that sometimes the backslash one would use in
> > Perl needs to be a double backslash in R.
> >
> > Here is a sample of what I tried and what I got:
> >
> > a<-"col_1 col_2 col_3"
> >
> > > gsub("\\s", " " , a)
> >
> > [1] "col_1 col_2 col_3"
> >
> > > gsub("\\s", "\\s" , a)
> >
> > [1] "col_1scol_2scol_3"
> >
> > As you can see, it looks like R is taking a regular expression for
> > "pattern", but not taking it for "replacement". Why is this?
> >
> > Assuming that I did want to solve my original problem with gsub and then
> > turn the string into an R object, how would I get gsub to return
> > "c("col_1", "col_2", "col_3") using my original string?
> >
> > Finally, is there a way to declare a string as a regular expression so
> > that R sees it the same way other languages, such as Perl do, i.e. make
> > the backslash be interpreted the same way? For someone who is just
> > learning regular expressions as I am, it is very frustrating to read
> > about them in references and then have to translate what I've learned
> > into R syntax. I was thinking that instead of enclosing the string in
> > "", one could use THIS.IS.A.REGULAR.EXPRESSION(), similar to the way we
> > use I() in formulae.
> >
> > These are a bunch of questions, but obviously I have a lot to learn!
> >
> > Thanks,
> >
> > Mark
> >
> > Mark W. Kimpel MD
> >
> >
> >
> > (317) 490-5129 Work, & Mobile
> >
> >
> >
> > (317) 663-0513 Home (no voice mail please)
> >
> > 1-(317)-536-2730 FAX
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From Bill.Venables at csiro.au  Wed Jan 31 02:36:47 2007
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Wed, 31 Jan 2007 11:36:47 +1000
Subject: [R] Need to fit a regression line using orthogonal residuals
References: <45BE5078.3070302@umich.edu>
Message-ID: <B998A44C8986644EA8029CFE6396A924840CB9@exqld2-bne.qld.csiro.au>

Jonathon Kopecky asks:

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jonathon Kopecky
Sent: Tuesday, 30 January 2007 5:52 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Need to fit a regression line using orthogonal residuals

I'm trying to fit a simple linear regression of just Y ~ X, but both X
and Y are noisy.  Thus instead of fitting a standard linear model
minimizing vertical residuals, I would like to minimize
orthogonal/perpendicular residuals.  I have tried searching the
R-packages, but have not found anything that seems suitable.  I'm not
sure what these types of residuals are typically called (they seem to
have many different names), so that may be my trouble.  I do not want to
use Principal Components Analysis (as was answered to a previous
questioner a few years ago), I just want to minimize the combined noise
of my two variables.  Is there a way for me to do this in R?
[WNV] There's always a way if you are prepared to program it.  Your
question is a bit like asking "Is there a way to do this in Fortran?"
The most direct way to do it is to define a function that gives you the
sum of the perpendicular distances and minimise it using, say, optim().
E.g.
	ppdis <- function(b, x, y) sum((y - b[1] - b[2]*x)^2/(1+b[2]^2))
	b0 <- lsfit(x, y)$coef  # initial value
	op <- optim(b0, ppdis, method = "BFGS", x=x, y=y)
	op  # now to check the results
	plot(x, y, asp = 1)  # why 'asp = 1'?? exercise
	abline(b0, col = "red")
	abline(op$par, col = "blue")
There are a couple of things about this you should be aware of, though
First, this is just a fiddly way of finding the first principal
component, so your desire not to use Principal Component Analysis is
somewhat thwarted, as it must be.
Second, the result is sensitive to scale - if you change the scales of
either x or y, e.g. from lbs to kilograms, the answer is different.
This also means that unless your measurement units for x and y are
comparable it's hard to see how the result can make much sense.  A
related issue is that you have to take some care when plotting the
result or orthogonal distances will not appear to be orthogonal.
Third, the resulting line is not optimal for either predicting y for a
new x or x from a new y.  It's hard to see why it is ever of much
interest.
Bill Venables.


Jonathon Kopecky
University of Michigan

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ecjbosu at aol.com  Wed Jan 31 02:52:59 2007
From: ecjbosu at aol.com (Joe W. Byers)
Date: Tue, 30 Jan 2007 19:52:59 -0600
Subject: [R] Install RMySQL with R 2.4.0
In-Reply-To: <459D502A.5020103@utulsa.edu>
References: <mailman.9.1162724403.23649.r-help@stat.math.ethz.ch>	<001c01c700e9$60b875f0$0ac0a8c0@MightyMini>	<eit4k8$7j3$1@sea.gmane.org>	<eit7hn$l73$1@sea.gmane.org>	<45525FC9.1040504@cs.odu.edu>
	<459D502A.5020103@utulsa.edu>
Message-ID: <eposps$9jm$1@sea.gmane.org>

This is a test to get my new subscription upgraded.  forgive me.

thank you
Joe

Joe Byers wrote:
> All,
> 
> I am glad all of you have benefited from the posting of the RMySQL 5-10 
> zip file on my university website.  I am asking for some help from the 
> group, I am leaving the university at the end of the semester and I need 
> a place to post this file until I get settled in my new position. 
> Anyone that can help me or us out with this. It would be greatly 
> appreciated.
> 
> Thank you
> Joe
> 
> 
> 
> Frank McCown wrote:
>> Joe Byers wrote:
>>> All,
>>>
>>> After staring at this error message for an hour or so yesterday and this 
>>> morning.  I decided to try something else.  Low and behold trying to 
>>> build the package in cygwin causes R to try and build under linux/unix 
>>> not windows.  I went to the command prompt and was able to build the 
>>> package.
>>>
>>> Download the RMySQL...tar.gz file and unzip somewhere like drive:/projects
>>>
>>> Several notes
>>> 1.  make sure you have mysql directories on your computer somewhere with 
>>>   the subdirs of include, bin, and lib.  You can just copy these from 
>>> you actual server unless you want to install them.  I used d:/mysql/...
>>> 2.  Modify configure.win in RMySQL and Makevars.win ins RMySQL/src to 
>>> have the mysql directories from (1)
>>> 3.  Copy and paste this script to a batch file and execute
>>> **************
>>> Rem build without --docs=normal tries to build chm help on windows this 
>>> bombs
>>> Rem if a zip program not installed the zip file will not be built
>>> Rem go find the temp directory where R built the package and copy to 
>>> ../R/library
>>> Rem temp directory will look something like C:\Temp\Rinst32098657\RMySQL
>>>
>>> Rem if R bin directory in the path this will run otherwise add the 
>>> drive:\Dir1\R\bin to the command
>>> Rcmd build --binary \projects\RMySQL --docs=normal
>>> ***********************
>>> 4. Note that I have --docs=normal in the command line. This is needed to 
>>>   get the package built.  Windows packages now default to chm files and 
>>> RMySQL does not have any windows chm help files.  All txt, html, and 
>>> latex help are built with this option.
>>> 5.  I am not sure where the RMySQL...zip file is stored, I think in 
>>> ...R\Bin.  I just copied the files from the temp\RinstXXXXXX\to the 
>>> ...\R\library to install.
>>>
>>> This may or may  not work for you, it did for me.
>>>
>>> I will try and update my website www.cba.utulsa.edu/byersj Research and 
>>> Analytics section to include a link to the RMySQL zip file for others to 
>>> download.
>>>
>>> Good Luck
>>> Joe
>>
>> Joe,
>>
>> Thanks for telling us how you got RMySQL installed.  Would you mind 
>> posting the dll files so the rest of us wouldn't have to recompile anything?
>>
>> Thanks,
>> Frank
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From roger at ysidro.econ.uiuc.edu  Wed Jan 31 03:05:34 2007
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Tue, 30 Jan 2007 20:05:34 -0600
Subject: [R] SparseM and Stepwise Problem
In-Reply-To: <45BF64E3.3335.1D54E65@davidkat.davidkatzconsulting.com>
References: <45BF64E3.3335.1D54E65@davidkat.davidkatzconsulting.com>
Message-ID: <77809A6A-D974-4370-B01D-46B2DA2FF584@ysidro.econ.uiuc.edu>

One simple possibility  -- if you can generate the X matrix in dense  
form is
the coercion

	X <- as.matrix.csr(X)

Unfortunately, there is no current way to go from a formula to a  
sparse X
matrix  without  passing through a dense version of X first.   
Otherwise you
need to use new() to define the X matrix directly.  This is usually  
not that
difficult, but it depends on the model....



url:    www.econ.uiuc.edu/~roger                Roger Koenker
email   rkoenker at uiuc.edu                       Department of Economics
vox:    217-333-4558                            University of Illinois
fax:    217-244-6678                            Champaign, IL 61820


On Jan 30, 2007, at 5:31 PM, davidkat at davidkatzconsulting.com wrote:

> I'm trying to use stepAIC on sparse matrices, and I need some help.
> The documentation for slm.fit suggests:
> slm.fit and slm.wfit call slm.fit.csr to do Cholesky decomposition  
> and then
> backsolve to obtain the least squares estimated coefficients. These  
> functions can be
> called directly if the user is willing to specify the design matrix  
> in matrix.csr form.
> This is often advantageous in large problems to reduce memory  
> requirements.
> I need some help or a reference that will show how to create the  
> design matrix from
> data in matrix.csr form.
> Thanks for any help.
>
>
> -- 
> David Katz
>  www.davidkatzconsulting.com
>    541 482-1137
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From hb at stat.berkeley.edu  Wed Jan 31 03:18:56 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Tue, 30 Jan 2007 18:18:56 -0800
Subject: [R] Need to fit a regression line using orthogonal residuals
In-Reply-To: <B998A44C8986644EA8029CFE6396A924840CB9@exqld2-bne.qld.csiro.au>
References: <45BE5078.3070302@umich.edu>
	<B998A44C8986644EA8029CFE6396A924840CB9@exqld2-bne.qld.csiro.au>
Message-ID: <59d7961d0701301818x7732a6fbr33a9f0506e860b29@mail.gmail.com>

The iwpca() in Bioconductor package aroma.light takes a matrix of
column vectors and "fits an R-dimensional hyperplane using iterative
re-weighted PCA".  From ?iwpca.matrix:

"Arguments:
X 	N-times-K matrix where N is the number of observations and K is the
number of dimensions.

Details:
This method uses weighted principal component analysis (WPCA) to fit a
R-dimensional hyperplane through the data with initial internal
weights all equal. At each iteration the internal weights are
recalculated based on the "residuals". If method=="L1", the internal
weights are 1 / sum(abs(r) + reps). This is the same as
method=function(r) 1/sum(abs(r)+reps). The "residuals" are orthogonal
Euclidean distance of the principal components R,R+1,...,K. In each
iteration before doing WPCA, the internal weighted are multiplied by
the weights given by argument w, if specified."

Thus, in your case you want to do:

X <- cbind(x,y)
fit <- iwpca(X)

There are different ways of robustifying the estimate, cf. argument
'method'. For heteroscedastic noise, fitting in L1 is convenient since
there is no bandwidth parameter.

Hope this helps

Henrik

On 1/30/07, Bill.Venables at csiro.au <Bill.Venables at csiro.au> wrote:
> Jonathon Kopecky asks:
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jonathon Kopecky
> Sent: Tuesday, 30 January 2007 5:52 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Need to fit a regression line using orthogonal residuals
>
> I'm trying to fit a simple linear regression of just Y ~ X, but both X
> and Y are noisy.  Thus instead of fitting a standard linear model
> minimizing vertical residuals, I would like to minimize
> orthogonal/perpendicular residuals.  I have tried searching the
> R-packages, but have not found anything that seems suitable.  I'm not
> sure what these types of residuals are typically called (they seem to
> have many different names), so that may be my trouble.  I do not want to
> use Principal Components Analysis (as was answered to a previous
> questioner a few years ago), I just want to minimize the combined noise
> of my two variables.  Is there a way for me to do this in R?
> [WNV] There's always a way if you are prepared to program it.  Your
> question is a bit like asking "Is there a way to do this in Fortran?"
> The most direct way to do it is to define a function that gives you the
> sum of the perpendicular distances and minimise it using, say, optim().
> E.g.
>         ppdis <- function(b, x, y) sum((y - b[1] - b[2]*x)^2/(1+b[2]^2))
>         b0 <- lsfit(x, y)$coef  # initial value
>         op <- optim(b0, ppdis, method = "BFGS", x=x, y=y)
>         op  # now to check the results
>         plot(x, y, asp = 1)  # why 'asp = 1'?? exercise
>         abline(b0, col = "red")
>         abline(op$par, col = "blue")
> There are a couple of things about this you should be aware of, though
> First, this is just a fiddly way of finding the first principal
> component, so your desire not to use Principal Component Analysis is
> somewhat thwarted, as it must be.
> Second, the result is sensitive to scale - if you change the scales of
> either x or y, e.g. from lbs to kilograms, the answer is different.
> This also means that unless your measurement units for x and y are
> comparable it's hard to see how the result can make much sense.  A
> related issue is that you have to take some care when plotting the
> result or orthogonal distances will not appear to be orthogonal.
> Third, the resulting line is not optimal for either predicting y for a
> new x or x from a new y.  It's hard to see why it is ever of much
> interest.
> Bill Venables.
>
>
> Jonathon Kopecky
> University of Michigan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From timh at insightful.com  Wed Jan 31 04:11:31 2007
From: timh at insightful.com (Tim Hesterberg)
Date: 30 Jan 2007 19:11:31 -0800
Subject: [R] Bootstrapping Confidence Intervals for Medians
Message-ID: <SEWINEXCH00cekh8SjC00000163@sewinexch00.insightful.com>

This is in followup to an earlier message about bootstrap confidence
intervals for the difference in two medians.  I'll touch on three
topics:
* bootstrap theory for one vs two samples
* special case of median
* bootstrap small samples


ONE VS TWO SAMPLES

Brian Ripley wrote (the complete posting is below):
>The 'standard' theory of bootstrap confidence intervals as implemented in 
>e.g. package 'boot' is for a single-sample problem (and it would be 
>pushing its justification very hard to use this for n=9).  But you have 
>two samples, and haven't told us how you intend to bootstrap.  I guess you 
>mean a stratified bootstrap, sampling with replacement independently from 
>observations 1-4 and 5-9.  I don't know of theory for bootstrap confidence 
>intervals from that scenario: do you?

Much of the bootstrap theory was developed for a single sample,
but carries over fairly easily to two samples or other stratified
sampling.  Davison and Hinkley do talk about stratified sampling
in their book, and Canty implemented that in the 'boot' package.
S+Resample also supports stratified sampling, with a front-end
function 'bootstrap2' to make the two-sample case easier.


SPECIAL CASE OF MEDIANS

>Beyond this, there are considerable problems with bootstrapping medians in 
>small samples as the median is a non-smooth function of the data and the 
>bootstrap samples take very few values.  See for example the galaxies 
>dataset as discussed in MASS4.  For the stratified bootstrapping I 
>referred to, there are only a handful of possible values of each of the 
>medians and so the bootstrap distribution is a highly non-uniform one on a 
>few values.  E.g.

Bootstrapping medians does present special difficulties.  Particularly
with a single sample and n odd, the bootstrap distribution is often a
very inaccurate estimate for the sampling distribution; it takes on
only a few values.

Curiously, however, bootstrap percentile intervals for a single median
are not bad.  They are similar to classical nonparametric intervals
based on order statistics, and use either the same or an adjacent
order statistic; where they differ, the classical interval is
conservative (one-sided non-coverage less than 0.025, often much
less), and the bootstrap interval is one order statistic narrower, with
closer to the nominal coverage.


BOOTSTRAPPING SMALL VS MEDIUM SAMPLES

People tend to think of bootstrapping for small samples, where they
don't trust the central limit theorem, like n=9.  That is misguided.
They should use the bootstrap in medium size samples, where
they shouldn't trust the central limit theorem, like perhaps n=20
(depending on the application) to n=1000.  

For very small samples the data distribution does not reliably reflect
the shape of the population, and it is better to impose parametric
assumptions.  In other words, with very small samples you can't accurately
estimate skewness, so it may be better to use classical methods that just
assume that skewness is zero.

Conversely, for medium size samples one should not just assume that
skewness is zero, but instead use bootstrap or other methods that
allow for skewness.  Yes, n=1000 is a medium size sample -- the effect
of skewness on confidence intervals and tests decreases very slowly,
with size or coverage errors of O(1/sqrt(n)) for classical t tests and
confidence intervals.

One beauty of the bootstrap is that it provides a nice way to estimate
what size errors you make by assuming sampling distributions are normal --
by creating a bootstrap distribution and seeing how non-normal it is.
It provides a nice alternative to the pre-computer rule of using
normal methods if n >= 30.

Tim Hesterberg

========================================================
| Tim Hesterberg       Senior Research Scientist       |
| timh at insightful.com  Insightful Corp.                |
| (206)802-2319        1700 Westlake Ave. N, Suite 500 |
| (206)283-8691 (fax)  Seattle, WA 98109-3044, U.S.A.  |
|                      www.insightful.com/Hesterberg   |
========================================================
Download S+Resample from www.insightful.com/downloads/libraries
Bootstrap short courses:  May 21-22 Philadelphia, Oct 10-11 San Francisco.
                          2-3 May UK, 3-4 Oct UK.
(I do talk more about these issues in the short courses.)

Brian Ripley wrote:
>On Sat, 6 Jan 2007, gilbertg at musc.edu wrote:
>
>> I apologize for this post. I am new to R (two days) and I have tried and 
>> tried to calculated confidence intervals for medians. Can someone help 
>> me?
>
>Later, you say you want a confidence interval for a difference in medians, 
>not the same thing.
>
>For medians, see MASS4 section 5.7 for worked examples and discussion of 
>the pitfalls.
>
>> Here is my data:
>>
>> institution1
>> 0.21
>> 0.16
>> 0.32
>> 0.69
>> 1.15
>> 0.9
>> 0.87
>> 0.87
>> 0.73
>>
>> The first four observations compose group 1 and observations 5 through 9 
>> compose group 2. I would like to create a bootstrapped 90% confidence 
>> interval on the difference of the medians (n2-n1). I have successfully 
>> calculated a permutation test.
>>
>> This shouldn't be as difficult as I am making it, would someone please 
>> enlighten me?
>
>
>It seems to me to be much more difficult than you have made it.  We need 
>to know exactly what you mean by
>
>> a bootstrapped 90% confidence interval on the difference of the medians
>
>The 'standard' theory of bootstrap confidence intervals as implemented in 
>e.g. package 'boot' is for a single-sample problem (and it would be 
>pushing its justification very hard to use this for n=9).  But you have 
>two samples, and haven't told us how you intend to bootstrap.  I guess you 
>mean a stratified bootstrap, sampling with replacement independently from 
>observations 1-4 and 5-9.  I don't know of theory for bootstrap confidence 
>intervals from that scenario: do you?
>
>Beyond this, there are considerable problems with bootstrapping medians in 
>small samples as the median is a non-smooth function of the data and the 
>bootstrap samples take very few values.  See for example the galaxies 
>dataset as discussed in MASS4.  For the stratified bootstrapping I 
>referred to, there are only a handful of possible values of each of the 
>medians and so the bootstrap distribution is a highly non-uniform one on a 
>few values.  E.g.
>
>x <- c(0.21, 0.16, 0.32, 0.69)
>y <- c(1.15, 0.9, 0.87, 0.87, 0.73)
>z <- numeric(10000)
>for(i in seq_len(10000))
>z[i] <- median(sample(x, replace=TRUE)) - median(sample(y, replace=TRUE))
>
>  -0.99 -0.965  -0.94  -0.91 -0.885  -0.83  -0.74 -0.725 -0.715  -0.71
>     33     70     83     27    134     64    129     16    259    317
>   -0.7  -0.69 -0.685  -0.66 -0.645 -0.635  -0.63 -0.605  -0.58  -0.57
>     43    370    711   1064     70    538    455   1388    424     29
>  -0.55 -0.545  -0.52  -0.49 -0.475 -0.465  -0.46  -0.45 -0.445  -0.42
>    905     57     79     41     54    119     28    183    146    436
>  -0.41 -0.395 -0.365 -0.305  -0.28 -0.225  -0.21  -0.18  -0.04
>    100    290    759     13     34     64    117    323     28
>
>You could use that table to give 'basic' or 'percentile' confidence 
>intervals, if you have reason to believe in them.
>
>
>> Greg Gilbert, Faculty Research Associate
>> Department of Biostatistics, Bioinformatics, & Epidemiology
>> Medical University of South Carolina
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jingy1 at ucla.edu  Wed Jan 31 05:23:13 2007
From: jingy1 at ucla.edu (Chen, Xiao)
Date: Tue, 30 Jan 2007 20:23:13 -0800
Subject: [R] help with function "system" and MS-DOS command TYPE
Message-ID: <43F64E86355A744E9D51506B6C6783B9019DB9FD@EM2.ad.ucla.edu>

Greetings -

I have a quick question that I hope someone will have a quick answer. I
have tried to use the R function "system" with the MS-DOS command "type"
to display the full content of a text file. But it always returns with a
message saying the text file is not found. I could accomplish the same
task with the "cat" command which is one of the unix-like commands that
I have installed on my windows machine. But I would like to know how it
would work with the "type" command.  

Here is what I have tried:

> zz<-file("d:/work/test/test.txt", "w")
> cat("this is a test\n", file=zz)
> close(zz)
> system("cat test.txt", show.output.on.console=T)
this is a test
> system("type test.txt", show.output.on.console=T)
test.txt not found
>


Thanks much for any help that you could offer. 

Best, 

Xiao Chen
Statistical Consulting Group
UCLA Academic Technology Services
http://www.ats.ucla.edu/stat/


From ripley at stats.ox.ac.uk  Wed Jan 31 08:56:34 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 31 Jan 2007 07:56:34 +0000 (GMT)
Subject: [R] Need to fit a regression line using orthogonal residuals
In-Reply-To: <B998A44C8986644EA8029CFE6396A924840CB9@exqld2-bne.qld.csiro.au>
References: <45BE5078.3070302@umich.edu>
	<B998A44C8986644EA8029CFE6396A924840CB9@exqld2-bne.qld.csiro.au>
Message-ID: <Pine.LNX.4.64.0701310737460.30339@gannet.stats.ox.ac.uk>

Just to pick up

> Third, the resulting line is not optimal for either predicting y for a
> new x or x from a new y.  It's hard to see why it is ever of much
> interest.

It is not a regression (and hence the subject line was misleading), but it 
does come up in errors-in-variables problems.  Suppose you have two sets 
of measurements of the same quantity with the same variance of measurement 
error and you want a line calibrating set 2 to set 1.  Then the optimal 
(in the sense of MLE, for example) line is this one, and it is symmetrical 
in the two sets.

Now those are rather specific assumptions but they do come up in some 
problems in physics and analytical chemistry, and the result goes back to 
the 19th century.  In the 1980s I implemented a version which allowed for 
unequal (but known) heteroskedastic error variances which is quite popular 
in analytical chemistry.

The literature is patchy:  Fuller's `Measurement Error Models' covers the 
general area, and I recall this being in Sprent's (1969) book
`Models in Regression and Related Topics'.

See also the thread starting at

http://tolstoy.newcastle.edu.au/R/help/00a/0285.html

almost 7 years ago.  If that is the thread Jonathon Kopecky refers to (how 
are we to know?) then he is misquoting me: I said it was the same thing as 
using the first principal component, not an alternative proposal.


On Wed, 31 Jan 2007, Bill.Venables at csiro.au wrote:

> Jonathon Kopecky asks:
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jonathon Kopecky
> Sent: Tuesday, 30 January 2007 5:52 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Need to fit a regression line using orthogonal residuals
>
> I'm trying to fit a simple linear regression of just Y ~ X, but both X
> and Y are noisy.  Thus instead of fitting a standard linear model
> minimizing vertical residuals, I would like to minimize
> orthogonal/perpendicular residuals.  I have tried searching the
> R-packages, but have not found anything that seems suitable.  I'm not
> sure what these types of residuals are typically called (they seem to
> have many different names), so that may be my trouble.  I do not want to
> use Principal Components Analysis (as was answered to a previous
> questioner a few years ago), I just want to minimize the combined noise
> of my two variables.  Is there a way for me to do this in R?

> [WNV] There's always a way if you are prepared to program it.  Your
> question is a bit like asking "Is there a way to do this in Fortran?"
> The most direct way to do it is to define a function that gives you the
> sum of the perpendicular distances and minimise it using, say, optim().
> E.g.
> 	ppdis <- function(b, x, y) sum((y - b[1] - b[2]*x)^2/(1+b[2]^2))
> 	b0 <- lsfit(x, y)$coef  # initial value
> 	op <- optim(b0, ppdis, method = "BFGS", x=x, y=y)
> 	op  # now to check the results
> 	plot(x, y, asp = 1)  # why 'asp = 1'?? exercise
> 	abline(b0, col = "red")
> 	abline(op$par, col = "blue")
> There are a couple of things about this you should be aware of, though
> First, this is just a fiddly way of finding the first principal
> component, so your desire not to use Principal Component Analysis is
> somewhat thwarted, as it must be.
> Second, the result is sensitive to scale - if you change the scales of
> either x or y, e.g. from lbs to kilograms, the answer is different.
> This also means that unless your measurement units for x and y are
> comparable it's hard to see how the result can make much sense.  A
> related issue is that you have to take some care when plotting the
> result or orthogonal distances will not appear to be orthogonal.
> Third, the resulting line is not optimal for either predicting y for a
> new x or x from a new y.  It's hard to see why it is ever of much
> interest.
> Bill Venables.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Johannatxkl at 1webhighway.com  Wed Jan 31 08:38:51 2007
From: Johannatxkl at 1webhighway.com (Hanna)
Date: Wed, 31 Jan 2007 05:38:51 -0200
Subject: [R] Mit der Webseite www.stat.math.ethz.ch wirst Du nie viele
	Besucher bekommen!
Message-ID: <2F8E0C9B.FA4D519@1webhighway.com>


From pifbugs at hotmail.fr  Tue Jan 30 13:53:45 2007
From: pifbugs at hotmail.fr (pif pif)
Date: Tue, 30 Jan 2007 13:53:45 +0100
Subject: [R] Read multiple text files and combine into single file?
Message-ID: <BAY123-F26CC060420DE9A5A389E6BC6A60@phx.gbl>

Hi everyone!

I have 1000 text files that contain data in a standard format separated by 
semi colons. However, each file also has single header (first) and footer 
(final) lines.  The text files are all named with a sample number, which do 
not follow a pattern.  I need to delete the header and footer lines and 
combine all files into a single (or possibly several) large files.
Does anyone know how this could be done automatically using R?

best regards,

Ton


From p.nimda at gmail.com  Tue Jan 30 14:09:31 2007
From: p.nimda at gmail.com (Peter Nimda)
Date: Tue, 30 Jan 2007 14:09:31 +0100
Subject: [R] any implementations for adaptive modeling of time series?
Message-ID: <c5d31fc20701300509l777edf2dp5a8e50cf6bd8c6e0@mail.gmail.com>

Hallo,

my noisy time series represent a fading signal comprising of long
enough parts with a simple trend inside of each such a part.
Transition from one part into another is always a non-smooth
and very sharp/acute. In other words I have a piecewise
polynomial noisy curve asymptotically converging to the
biased constant, points between pieces are non-differentiable.

I am looking for implementations of models adequate for such a
data. Are there any possibilities to adapt the ARIMA or
MCMC?

Many thanks in advance for any help/URLs


From usstata at 126.com  Wed Jan 31 09:12:52 2007
From: usstata at 126.com (usstata)
Date: Wed, 31 Jan 2007 16:12:52 +0800
Subject: [R] How to print the objects in the memory
Message-ID: <45C04F78.09FD54.07207@m5-141.126.com>

Hi,all:

	May be a  pointless question
	
	a <- 1:10
	b <- matrix(1:8,nrow = 4)
	c <- letters[4:8]  
	????
	
	> ls()
	[1] "a"      "b"      "c"  

	ls() can print the names of the objects in the memory ,
	
	but I want to get the result :
> a
[1]  1  2  3  4  5  6  7  8  9 10
> b
     [,1] [,2]
[1,]    1    5
[2,]    2    6
[3,]    3    7
[4,]    4    8
> c
[1] "d" "e" "f" "g" "h"
????

 		I try the command print(noquote(ls()))	which it can't help
		
	
Best regards
usstata


From Reinecke at consultic.com  Wed Jan 31 09:30:26 2007
From: Reinecke at consultic.com (Michael Reinecke)
Date: Wed, 31 Jan 2007 09:30:26 +0100
Subject: [R] mca-graphics: all elements overlapping in the help-example for
	multiple correspondence analysis
Message-ID: <D1A363788EC8F946A56DAF95C0FBE7CF2991F5@sbs2003.CMI.local>

Dear all,
 
I tried out the example in the help document for mca (the multiple correspondence analysis of the MASS package):
 
farms.mca <- mca(farms, abbrev=TRUE)
farms.mca
plot(farms.mca)
 
But the graphic that I get seems unfeasible to me: I cannot recognize the numbers (printed in black) because they are all overlapping and concealing each other. I don ?t dare using my own data, which consist of several hundred cases - I guess I won ?t see anything.
 
How can I solve this? Thank you for any idea!
 
Michael


From r.mueller at oeko-sorpe.de  Wed Jan 31 10:01:34 2007
From: r.mueller at oeko-sorpe.de (Richard =?iso-8859-1?q?M=FCller?=)
Date: Wed, 31 Jan 2007 10:01:34 +0100
Subject: [R] can't find mailing list commands
Message-ID: <200701311001.34137.r.mueller@oeko-sorpe.de>

Sorry for inconvenience - but since I can't find any hints on this list at 
stat.ethz.ch, I'll ask here: How do I unsubscribe from this list?
Thanks for answering - Richard
-- 
Richard M?ller - Am Spring 9 - D-58802 Balve-Eisborn
www.oeko-sorpe.de


From wl2776 at gmail.com  Wed Jan 31 10:00:24 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Wed, 31 Jan 2007 01:00:24 -0800 (PST)
Subject: [R] How to print the objects in the memory
In-Reply-To: <45C04F78.09FD54.07207@m5-141.126.com>
References: <45C04F78.09FD54.07207@m5-141.126.com>
Message-ID: <8725398.post@talk.nabble.com>


1. ? ls.str()

2. My favourite "eval(parse(text= ... ))" :)

sapply(ls(),FUN=function(x)eval(parse(text=x)))

-- 
View this message in context: http://www.nabble.com/-R--How-to-print-the-objects-in-the-memory-tf3147472.html#a8725398
Sent from the R help mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Wed Jan 31 10:19:21 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 31 Jan 2007 09:19:21 +0000 (GMT)
Subject: [R] mca-graphics: all elements overlapping in the help-example
 for multiple correspondence analysis
In-Reply-To: <D1A363788EC8F946A56DAF95C0FBE7CF2991F5@sbs2003.CMI.local>
References: <D1A363788EC8F946A56DAF95C0FBE7CF2991F5@sbs2003.CMI.local>
Message-ID: <Pine.LNX.4.64.0701310907001.9130@gannet.stats.ox.ac.uk>

On Wed, 31 Jan 2007, Michael Reinecke wrote:

> Dear all,
>
> I tried out the example in the help document for mca (the multiple correspondence analysis of the MASS package):
>
> farms.mca <- mca(farms, abbrev=TRUE)
> farms.mca
> plot(farms.mca)
>
> But the graphic that I get seems unfeasible to me: I cannot recognize 
> the numbers (printed in black) because they are all overlapping and 
> concealing each other. I don ?t dare using my own data, which consist of 
> several hundred cases - I guess I won ?t see anything.
>
> How can I solve this? Thank you for any idea!

Some levels do overplot, as they are identical (this is an unusual 
example).  But as you see in the book, not many, and you can 
adjust pointsize of your device or 'cex' to mitigate the problem.

Plotting the rows is optional: see the help page.  I would not recommend 
plotting rows for several hundred cases.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From wraff at titus.u-strasbg.fr  Wed Jan 31 10:31:56 2007
From: wraff at titus.u-strasbg.fr (Wolfgang Raffelsberger)
Date: Wed, 31 Jan 2007 10:31:56 +0100
Subject: [R] problem with compilation of R on Solaris 10 in x86
Message-ID: <45C0620C.5090501@igbmc.u-strasbg.fr>

Dear List,
we're trying to install R on Solaris10 on a x86 (amd64).

During the installation we pass successfully the ./configure but we get 
an error through the built-in function "_isnan" which we see existing in 
/lib

When passing the command "make" we get :

gcc -std=gnu99 -I../../src/extra/zlib -I../../src/extra/bzip2 
-I../../src/extra/pcre   -I. -I../../src/include -I../../src/include 
-I/usr/local/include -DHAVE_CONFIG_H -D__NO_MATH_INLINES  -g -O2 -c 
serialize.c -o serialize.o
In file included from ../../src/include/Rinternals.h:921,
                 from ../../src/include/Defn.h:72,
                 from serialize.c:29:
../../src/include/Rinlinedfuns.h: In function `fmin2_int':
../../src/include/Rinlinedfuns.h:602: warning: implicit declaration of 
function `__builtin_isnan'

[... and ...]
gcc -std=gnu99  -L/usr/local/lib -o R.bin Rmain.o CConverters.o 
CommandLineArgs.o  Rdynload.o Renviron.o RNG.o  apply.o arithmetic.o 
apse.o array.o attrib.o  base.o bind.o builtin.o  character.o coerce.o 
colors.o complex.o connections.o context.o  cov.o cum.o  dcf.o 
datetime.o debug.o deparse.o deriv.o  dotcode.o dounzip.o dstruct.o 
duplicate.o  engine.o envir.o errors.o eval.o  format.o fourier.o  
gevents.o gram.o gram-ex.o graphics.o  identical.o internet.o 
iosupport.o  lapack.o list.o localecharset.o logic.o  main.o mapply.o 
match.o memory.o model.o  names.o  objects.o optim.o optimize.o 
options.o  par.o paste.o pcre.o platform.o  plot.o plot3d.o plotmath.o  
print.o printarray.o printvector.o printutils.o qsort.o  random.o 
regex.o registration.o relop.o rlocale.o  saveload.o scan.o seq.o 
serialize.o size.o sort.o source.o split.o  sprintf.o startup.o 
subassign.o subscript.o subset.o summary.o sysutils.o  unique.o util.o  
version.o vfonts.o xxxpr.o  mkdtemp.o ../unix/libun!
ix.a ../appl/libappl.a ../nmath/libnmath.a -L../../lib -lRblas 
-L/opt/sfw/lib/gcc/i386-pc-solaris2.10/3.4.2 -L/usr/ccs/bin 
-L/opt/sfw/lib -lg2c -lm -lgcc_s  ../extra/zlib/libz.a  
../extra/bzip2/libbz2.a ../extra/pcre/libpcre.a  
../extra/intl/libintl.a  -lreadline -lncurses -lnsl -lsocket -ldl -lm
Undefined                       first referenced
 symbol                             in file
__builtin_isnan                     arithmetic.o
ld: fatal: Symbol referencing errors. No output written to R.bin
collect2: ld returned 1 exit status


Thank's in advance for all hints how we could overcome this problem,
Wolfgang

 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . .

Wolfgang Raffelsberger, PhD
Laboratoire de BioInformatique et G?nomique Int?grative
IGBMC
1 rue Laurent Fries,  67404 Illkirch  Strasbourg,  France
Tel (+33) 388 65 3300         Fax (+33) 388 65 3276
http://www-bio3d-igbmc.u-strasbg.fr/~wraff
wolfgang.raffelsberger at igbmc.u-strasbg.fr


From ripley at stats.ox.ac.uk  Wed Jan 31 10:43:40 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 31 Jan 2007 09:43:40 +0000 (GMT)
Subject: [R] How to print the objects in the memory
In-Reply-To: <8725398.post@talk.nabble.com>
References: <45C04F78.09FD54.07207@m5-141.126.com>
	<8725398.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.64.0701310927180.12547@gannet.stats.ox.ac.uk>

On Wed, 31 Jan 2007, Vladimir Eremeev wrote:

[replying to something else, without any indication or context.  That 
'something' was about how to print all objects.]

>
> 1. ? ls.str()
>
> 2. My favourite "eval(parse(text= ... ))" :)
>
> sapply(ls(),FUN=function(x)eval(parse(text=x)))

Note

> library(fortune)
> fortune("parse")

If the answer is parse() you should usually rethink the question.
    -- Thomas Lumley
       R-help (February 2005)

Here the answer is 'get', as ls.str uses via utils:::print.ls_str. You 
need to be careful to get the correct object of the given name, and using 
your favourite will not in general do so.

When you have a character string containing the name of a variable, 
eval(as.name(x)) is more direct than parsing (but less direct that get(x) 
).  And since there is no reason that the results are all suitable for 
simplifying to a vector, lapply is safer.  E.g.

> xx <- pi
> y <- "test it"
> sapply(ls(), function(x) eval(parse(text=x)))
                 xx                  y
"3.14159265358979"          "test it"

is not what is wanted.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From maechler at stat.math.ethz.ch  Wed Jan 31 10:46:13 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 31 Jan 2007 10:46:13 +0100
Subject: [R] can't find mailing list commands
In-Reply-To: <200701311001.34137.r.mueller@oeko-sorpe.de>
References: <200701311001.34137.r.mueller@oeko-sorpe.de>
Message-ID: <17856.25957.810032.306194@stat.math.ethz.ch>

Hmm,
I thought that learning to read was pretty wide-spread in your
country...

>>>>> "Richard" == Richard M?ller <r.mueller at oeko-sorpe.de>
>>>>>     on Wed, 31 Jan 2007 10:01:34 +0100 writes:

    Richard> Sorry for inconvenience - but since I can't find
    Richard> any hints on this list at stat.ethz.ch, I'll ask here:
    Richard>  How do I unsubscribe from this list? 
    
    Richard> ______________________________________________
    Richard> R-help at stat.math.ethz.ch mailing list
    Richard> https://stat.ethz.ch/mailman/listinfo/r-help 
	     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
why do you think this does *not* mention 'unsubscribe' ???????

    Richard> PLEASE do read the posting guide ...........

yes, indeed.

Spamming > 3000 readers of R-help with your problem is rather
impolite. I apologize for doing the same with my answer, but
maybe it prevents similar questsion at least for the very near
future..

Martin Maechler, ETH Zurich


From christoph.heibl at gmx.net  Wed Jan 31 11:00:42 2007
From: christoph.heibl at gmx.net (Christoph Heibl)
Date: Wed, 31 Jan 2007 11:00:42 +0100
Subject: [R] Check for presence of object in group of objects
Message-ID: <476F2591-26D2-40FF-B159-DA0E4CD9C4F3@gmx.net>

Dear List,

must be simple, but i got stuck on this...

I?ve been trying around for some time, but I could not find a  
straigthforward way to do this:
How can one ask if a certain object is element of a certain group of  
objects, e.g stored in a vector?

x <- c("a", "b", "c")
# is there a simple function which returns:
is "a" part of x?
TRUE
is "d" part of x?
FALSE

Thanks a lot
Christoph


From jim at bitwrit.com.au  Wed Jan 31 11:05:09 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Wed, 31 Jan 2007 21:05:09 +1100
Subject: [R] Adding Scale to image
In-Reply-To: <45BF421D.7040103@charite.de>
References: <45BF421D.7040103@charite.de>
Message-ID: <45C069D5.4080505@bitwrit.com.au>

Peter Robinson wrote:
> Dear List,
> 
> I have used the image() function to show a heat plot of a matrix of data 
> whose intensity  is color-coded. I have two questions that I have not 
> been able to solve by using the help system or google.
> 
> 1) How can one add a scale/legend that shows what numerical values a 
> given color corresponds to?

You can use the gradient.rect function in the plotrix package to display 
a rectangular bar with a specified number of color "slices" in it. For 
an example of how to make this into a color legend, see the 
color2D.matplot function in the same package. If this is of interest to 
R users, I could include a color.legend function in plotrix that would 
do it all.

Jim


From ccleland at optonline.net  Wed Jan 31 11:05:16 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 31 Jan 2007 05:05:16 -0500
Subject: [R] Check for presence of object in group of objects
In-Reply-To: <476F2591-26D2-40FF-B159-DA0E4CD9C4F3@gmx.net>
References: <476F2591-26D2-40FF-B159-DA0E4CD9C4F3@gmx.net>
Message-ID: <45C069DC.5030201@optonline.net>

Christoph Heibl wrote:
> Dear List,
> 
> must be simple, but i got stuck on this...
> 
> I?ve been trying around for some time, but I could not find a  
> straigthforward way to do this:
> How can one ask if a certain object is element of a certain group of  
> objects, e.g stored in a vector?
> 
> x <- c("a", "b", "c")
> # is there a simple function which returns:
> is "a" part of x?
> TRUE
> is "d" part of x?
> FALSE

x <- c("a", "b", "c")

"a" %in% x
[1] TRUE

"d" %in% x
[1] FALSE

?is.element

> Thanks a lot
> Christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From Thierry.ONKELINX at inbo.be  Wed Jan 31 11:16:56 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 31 Jan 2007 11:16:56 +0100
Subject: [R] Check for presence of object in group of objects
In-Reply-To: <476F2591-26D2-40FF-B159-DA0E4CD9C4F3@gmx.net>
Message-ID: <2E9C414912813E4EB981326983E0A1040286E756@inboexch.inbo.be>

You need %in%

"a" %in% x
"d" %in% x

----------------------------------------------------------------------------

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics, methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of uncertainties, a surgery of suppositions. ~M.J.Moroney

-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch [mailto:r-help-bounces op stat.math.ethz.ch] Namens Christoph Heibl
Verzonden: woensdag 31 januari 2007 11:01
Aan: r-help op stat.math.ethz.ch
Onderwerp: [R] Check for presence of object in group of objects

Dear List,

must be simple, but i got stuck on this...

I?ve been trying around for some time, but I could not find a  
straigthforward way to do this:
How can one ask if a certain object is element of a certain group of  
objects, e.g stored in a vector?

x <- c("a", "b", "c")
# is there a simple function which returns:
is "a" part of x?
TRUE
is "d" part of x?
FALSE

Thanks a lot
Christoph

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jon.clayden at gmail.com  Wed Jan 31 12:03:07 2007
From: jon.clayden at gmail.com (Jon Clayden)
Date: Wed, 31 Jan 2007 11:03:07 +0000
Subject: [R] readBin is much slower for raw input than for a file
In-Reply-To: <68e086180701260325m58269f70w6bfd583c4e9c5aa3@mail.gmail.com>
References: <68e086180701260325m58269f70w6bfd583c4e9c5aa3@mail.gmail.com>
Message-ID: <68e086180701310303r4776eb02s99986a9112629f@mail.gmail.com>

This hasn't generated any feedback after a few days on R-devel, so I'm
forwarding it to R-help in case anyone here has any ideas...

Thanks,
Jon

---------- Forwarded message ----------
From: Jon Clayden <jon.clayden at gmail.com>
Date: 26-Jan-2007 11:25
Subject: readBin is much slower for raw input than for a file
To: r-devel at r-project.org


Dear all,

I'm trying to write an efficient binary file reader for a file type
that is made up of several fields of variable length, and so requires
many small reads. Doing this on the file directly using a sequence of
readBin() calls is a bit too slow for my needs, so I tried buffering
the file into a raw vector and reading from that ("loc" is the
equivalent of the file pointer):

fileSize <- file.info(fileName)$size
connection <- file(fileName, "rb")
bytes <- readBin(connection, "raw", n=fileSize)
loc <- 0
close(connection)

--

# within a custom read function:
if (loc == 0)
    data <- readBin(bytes, what, n, size, ...)
else if (loc > 0)
    data <- readBin(bytes[-(1:loc)], what, n, size, ...)

However, this method runs almost 10 times slower for me than the
sequence of file reads did. The initial call to readBin() - for
reading in the file - is very quick, but running Rprof shows that the
vast majority of the run time in doing the full parse is spent in
readBin, so it does seem to be that that's slowing things down. Can
anyone shed any light on why this is?

I'm not expecting miracles here - and I realise that writing the whole
read routine in C would be much quicker - but surely reading from a
raw vector should work out faster than reading from a file? The system
is R-2.4.1/Linux, Xeon 3.2 GHz, 2 GiB RAM; typical file size is 44
KiB.

Thanks in advance,
Jon


From wl2776 at gmail.com  Wed Jan 31 12:30:36 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Wed, 31 Jan 2007 03:30:36 -0800 (PST)
Subject: [R] Adding Scale to image
In-Reply-To: <45BF421D.7040103@charite.de>
References: <45BF421D.7040103@charite.de>
Message-ID: <8727371.post@talk.nabble.com>


"image.plot" from "fields" plots an image and a legend (or scale)


Dr. med. Peter Robinson wrote:
> 
> I have used the image() function to show a heat plot of a matrix of data 
> whose intensity  is color-coded. 
> 
> 1) How can one add a scale/legend that shows what numerical values a 
> given color corresponds to?
> 

-- 
View this message in context: http://www.nabble.com/-R--Adding-Scale-to-image-tf3142258.html#a8727371
Sent from the R help mailing list archive at Nabble.com.


From wl2776 at gmail.com  Wed Jan 31 12:36:23 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Wed, 31 Jan 2007 03:36:23 -0800 (PST)
Subject: [R] help with function "system" and MS-DOS command TYPE
In-Reply-To: <43F64E86355A744E9D51506B6C6783B9019DB9FD@EM2.ad.ucla.edu>
References: <43F64E86355A744E9D51506B6C6783B9019DB9FD@EM2.ad.ucla.edu>
Message-ID: <8727428.post@talk.nabble.com>



Chen, Xiao wrote:
> 
> Greetings -
> 
> I have a quick question that I hope someone will have a quick answer. I
> have tried to use the R function "system" with the MS-DOS command "type"
> to display the full content of a text file. But it always returns with a
> message saying the text file is not found. I could accomplish the same 
> task with the "cat" command which is one of the unix-like commands that 
> I have installed on my windows machine. But I would like to know how it 
> would work with the "type" command.  
> 
> Here is what I have tried:
> 
>> zz<-file("d:/work/test/test.txt", "w")
>> cat("this is a test\n", file=zz)
>> close(zz)
>> system("cat test.txt", show.output.on.console=T)
> this is a test
>> system("type test.txt", show.output.on.console=T)
> test.txt not found
> 

type is an internal command, rather than an executable file

use
system("cmd /c type test.txt", show.output.on.console=T)
or
system("command /c type test.txt", show.output.on.console=T)

-- 
View this message in context: http://www.nabble.com/-R--help-with-function-%22system%22-and-MS-DOS-command-TYPE-tf3146629.html#a8727428
Sent from the R help mailing list archive at Nabble.com.


From andza at osi.lv  Wed Jan 31 12:50:30 2007
From: andza at osi.lv (Andris Jankevics)
Date: Wed, 31 Jan 2007 13:50:30 +0200
Subject: [R] Interactive plots with R
Message-ID: <200701311350.30775.andza@osi.lv>

Hi,

I wrote some simple rpanel package script for visual spectral data comparison. 
At this example i have a three samples and i want to zoom through x and y 
axis to compare differences between samples. With my script below I can zoom 
to some data region and add some other spetra to the plot, through text input 
field.
But I can't figure out, how to change axis scaling for all displayed spectra, 
because only first spectra are redrawed after axis scaling.
Or maybe I need to study samples and documentation for tcltk package?

My script:

DATAPPM=seq(0,11,0.004)
DATAPPM=data.frame(DATAPPM,DATAPPM,DATAPPM)
DATAS=data.frame 
(S1=sample(1:3260,2751),S2=sample(1:3260,2751),S3=sample(1:3260,2751))

library (rpanel)
pplotNMR <- function (panel) {
	with (panel,{
	VECP <- DATAPPM[,sample]
	VECS <- DATAS[,sample]	
	names (VECP) <- rownames(DATAPPM)
	names (VECS) <- rownames(DATAS)
	SEL <- names(VECP[VECP>MIN & VECP<MAX])
	VECP <- VECP[SEL]
	VECS <- VECS[SEL]
	plot (VECP,VECS,type="l",xlim=c(MAX,MIN),xlab="PPM",ylab="intensity",
	ylim=c(YMIN,YMAX))
	panel})
	}

pplotNMRadd<- function (panel) {
	with (panel, {
	sample <- as.numeric (sample.text)
	VECP <- DATAPPM[,sample]
	VECS <- DATAS[,sample]	
	names (VECP) <- rownames(DATAPPM)
	names (VECS) <- rownames(DATAS)
	SEL <- names(VECP[VECP>MIN & VECP<MAX])
	VECP <- VECP[SEL]
	VECS <- VECS[SEL]
	points (VECP,VECS,type="l",col=sample) 
	panel })
	}
	

panel <- rp.control (title="NMR 
control",MIN=-2,MAX=12,sample=1,YMAX=max(apply(DATAS,2,max)),YMIN=min(apply(DATAS,2,min)),sample.text=1)
rp.slider(panel,MIN,from=-2,to=12,showvalue=TRUE,action=pplotNMR,title="ppm 
min")
rp.slider(panel,MAX,from=-2,to=12,showvalue=TRUE,action=pplotNMR,title="ppm 
max")
rp.slider(panel,YMAX,from=YMIN,to=max(apply(DATAS,2,max)),showvalue=TRUE,action=pplotNMR,title="Intesity")
rp.doublebutton (panel,YMAX,step=10,action=pplotNMR,title="intensity, 
step=10",showvalue=TRUE)
rp.textentry (panel, sample.text, action=pplotNMRadd, title="Add sample to 
plot:")



Thanks and regards,
Andris Jankevics


From bxc at steno.dk  Wed Jan 31 13:18:50 2007
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Wed, 31 Jan 2007 13:18:50 +0100
Subject: [R] how to join two arrays using their column names intersection
In-Reply-To: <45BF2BB9.1060408@cnb.uam.es>
Message-ID: <40D3930AC1C8EA469E39536E5BC8083503B58863@EXDKBA021.corp.novocorp.net>

Here is a workable solution:

df1 <- data.frame(ar1)
df2 <- data.frame(ar2)
cmn <- intersect(names(df1),names(df2))
rbind(df1[,cmn],df2[,cmn])

Best
Bendix
______________________________________________

Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2-4
DK-2820 Gentofte
Denmark
+45 44 43 87 38 (direct)
+45 30 75 87 38 (mobile)
+45 44 43 73 13 (fax)
bxc at steno.dk   http://www.biostat.ku.dk/~bxc
______________________________________________

> -----Original Message-----
> From: Federico Abascal [mailto:fabascal at cnb.uam.es] 
> Sent: Tuesday, January 30, 2007 12:28 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] how to join two arrays using their column names 
> intersection
> 
> Dear all,
> 
> I have a problem that may be someone of you can help. I am a 
> newbie and do not find how to do it in manuals.
> 
> I have two arrays, for example:
> 
> ar1 <- array(data=c(1:16),dim=c(4,4))
> ar2 <- array(data=c(1:16),dim=c(4,4))
> colnames(ar1)<-c("A","B","D","E")
> colnames(ar2)<-c("C","A","E","B")
> 
> > ar1
>      A B  D  E
> [1,] 1 5  9 13
> [2,] 2 6 10 14
> [3,] 3 7 11 15
> [4,] 4 8 12 16
> > ar2
>      C A  E  B
> [1,] 1 5  9 13
> [2,] 2 6 10 14
> [3,] 3 7 11 15
> [4,] 4 8 12 16
> 
> 
> I would like to join both arrays only for the columns present in both
> ar1 and ar2 (the intersection). I would like to obtain this:
>      A B  E
> [1,] 1 5 13
> [2,] 2 6 14
> [3,] 3 7 15
> [4,] 4 8 16
> [5,] 5 13  9
> [6,] 6 14 10
> [7,] 7 15 11
> [8,] 8 16 12
> 
> (rows 5-8 correspond to ar2)
> 
> I have tried several things but I am unable to accomplish it. 
> Any experienced user could give me some advice, please?
> 
> I have another question: how can I sort the columns of an 
> array according to its column names (for ar2, change CAEB to ABCE)?
> 
> Thanks in advance!
> Federico
> 
> 
>


From ripley at stats.ox.ac.uk  Wed Jan 31 13:27:09 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 31 Jan 2007 12:27:09 +0000 (GMT)
Subject: [R] problem with compilation of R on Solaris 10 in x86
In-Reply-To: <45C0620C.5090501@igbmc.u-strasbg.fr>
References: <45C0620C.5090501@igbmc.u-strasbg.fr>
Message-ID: <Pine.LNX.4.64.0701311212480.16374@gannet.stats.ox.ac.uk>

This has come up several times, all with long obselete versions of gcc. 
R does not use __builtin_isnan, but gcc does in its own (rather than 
Solaris') header files.  So it is an issue with the compiler, and note 
that 3.4.2 is an old (Sept 2004) version of the compiler.

If you are happy to use SunFreeware, they offer a pre-built R (apart for 
some unstated reason for the foreign package) at

http://www.sunfreeware.com/programlistintel10.html#R

and so it would appear that others have succeeded with more recent 
compilers.


On Wed, 31 Jan 2007, Wolfgang Raffelsberger wrote:

> Dear List,
> we're trying to install R on Solaris10 on a x86 (amd64).
>
> During the installation we pass successfully the ./configure but we get
> an error through the built-in function "_isnan" which we see existing in
> /lib
>
> When passing the command "make" we get :
>
> gcc -std=gnu99 -I../../src/extra/zlib -I../../src/extra/bzip2
> -I../../src/extra/pcre   -I. -I../../src/include -I../../src/include
> -I/usr/local/include -DHAVE_CONFIG_H -D__NO_MATH_INLINES  -g -O2 -c
> serialize.c -o serialize.o
> In file included from ../../src/include/Rinternals.h:921,
>                 from ../../src/include/Defn.h:72,
>                 from serialize.c:29:
> ../../src/include/Rinlinedfuns.h: In function `fmin2_int':
> ../../src/include/Rinlinedfuns.h:602: warning: implicit declaration of
> function `__builtin_isnan'
>
> [... and ...]
> gcc -std=gnu99  -L/usr/local/lib -o R.bin Rmain.o CConverters.o
> CommandLineArgs.o  Rdynload.o Renviron.o RNG.o  apply.o arithmetic.o
> apse.o array.o attrib.o  base.o bind.o builtin.o  character.o coerce.o
> colors.o complex.o connections.o context.o  cov.o cum.o  dcf.o
> datetime.o debug.o deparse.o deriv.o  dotcode.o dounzip.o dstruct.o
> duplicate.o  engine.o envir.o errors.o eval.o  format.o fourier.o
> gevents.o gram.o gram-ex.o graphics.o  identical.o internet.o
> iosupport.o  lapack.o list.o localecharset.o logic.o  main.o mapply.o
> match.o memory.o model.o  names.o  objects.o optim.o optimize.o
> options.o  par.o paste.o pcre.o platform.o  plot.o plot3d.o plotmath.o
> print.o printarray.o printvector.o printutils.o qsort.o  random.o
> regex.o registration.o relop.o rlocale.o  saveload.o scan.o seq.o
> serialize.o size.o sort.o source.o split.o  sprintf.o startup.o
> subassign.o subscript.o subset.o summary.o sysutils.o  unique.o util.o
> version.o vfonts.o xxxpr.o  mkdtemp.o ../unix/libun!
> ix.a ../appl/libappl.a ../nmath/libnmath.a -L../../lib -lRblas
> -L/opt/sfw/lib/gcc/i386-pc-solaris2.10/3.4.2 -L/usr/ccs/bin
> -L/opt/sfw/lib -lg2c -lm -lgcc_s  ../extra/zlib/libz.a
> ../extra/bzip2/libbz2.a ../extra/pcre/libpcre.a
> ../extra/intl/libintl.a  -lreadline -lncurses -lnsl -lsocket -ldl -lm
> Undefined                       first referenced
> symbol                             in file
> __builtin_isnan                     arithmetic.o
> ld: fatal: Symbol referencing errors. No output written to R.bin
> collect2: ld returned 1 exit status
>
>
> Thank's in advance for all hints how we could overcome this problem,
> Wolfgang
>
>
>
> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
> . . . . .
>
> Wolfgang Raffelsberger, PhD
> Laboratoire de BioInformatique et G?nomique Int?grative
> IGBMC
> 1 rue Laurent Fries,  67404 Illkirch  Strasbourg,  France
> Tel (+33) 388 65 3300         Fax (+33) 388 65 3276
> http://www-bio3d-igbmc.u-strasbg.fr/~wraff
> wolfgang.raffelsberger at igbmc.u-strasbg.fr
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From rvaradhan at jhmi.edu  Wed Jan 31 13:41:39 2007
From: rvaradhan at jhmi.edu (RAVI VARADHAN)
Date: Wed, 31 Jan 2007 07:41:39 -0500
Subject: [R] Looking for a faster code
Message-ID: <f450eb5a2632.45c04833@johnshopkins.edu>

Hi,

I have two matrices A (m x 2) and B (n x 2), where m and n are large integers (on the order of 10^4).  I am looking for an efficient way to create another matrix, W (m x n), which can be defined as follows:

for (i in 1:m){
for (j in 1:n) {
W[i,j] <- g(A[i,], B[j,])
} }

I have tried the following and it works okay, but I am sure that I can do even better:

for (i in 1:m) {
W[i,] <- apply(B, 1, y=A[i,], function(x,y) g(y,x))
}

How can I do this in a faster manner (for example, I feel that I should be able to use "outer")?  

Thanks for any suggestions.

Best,
Ravi.


From jholtman at gmail.com  Wed Jan 31 14:12:09 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 31 Jan 2007 08:12:09 -0500
Subject: [R] readBin is much slower for raw input than for a file
In-Reply-To: <68e086180701310303r4776eb02s99986a9112629f@mail.gmail.com>
References: <68e086180701260325m58269f70w6bfd583c4e9c5aa3@mail.gmail.com>
	<68e086180701310303r4776eb02s99986a9112629f@mail.gmail.com>
Message-ID: <644e1f320701310512j1f1c6cdfu5ede81e5a6bd6103@mail.gmail.com>

I think your problem is subsetting the raw vector: you are deleteing
from the head -- a lot of copying going on.  Instead just subset and
extract the vector length of interest:

> # within a custom read function:
> if (loc == 0)
>    data <- readBin(bytes, what, n, size, ...)
> else if (loc > 0)
>    data <- readBin(bytes[(1:size) + loc, what, n, size, ...)



On 1/31/07, Jon Clayden <jon.clayden at gmail.com> wrote:
> This hasn't generated any feedback after a few days on R-devel, so I'm
> forwarding it to R-help in case anyone here has any ideas...
>
> Thanks,
> Jon
>
> ---------- Forwarded message ----------
> From: Jon Clayden <jon.clayden at gmail.com>
> Date: 26-Jan-2007 11:25
> Subject: readBin is much slower for raw input than for a file
> To: r-devel at r-project.org
>
>
> Dear all,
>
> I'm trying to write an efficient binary file reader for a file type
> that is made up of several fields of variable length, and so requires
> many small reads. Doing this on the file directly using a sequence of
> readBin() calls is a bit too slow for my needs, so I tried buffering
> the file into a raw vector and reading from that ("loc" is the
> equivalent of the file pointer):
>
> fileSize <- file.info(fileName)$size
> connection <- file(fileName, "rb")
> bytes <- readBin(connection, "raw", n=fileSize)
> loc <- 0
> close(connection)
>
> --
>
> # within a custom read function:
> if (loc == 0)
>    data <- readBin(bytes, what, n, size, ...)
> else if (loc > 0)
>    data <- readBin(bytes[-(1:loc)], what, n, size, ...)
>
> However, this method runs almost 10 times slower for me than the
> sequence of file reads did. The initial call to readBin() - for
> reading in the file - is very quick, but running Rprof shows that the
> vast majority of the run time in doing the full parse is spent in
> readBin, so it does seem to be that that's slowing things down. Can
> anyone shed any light on why this is?
>
> I'm not expecting miracles here - and I realise that writing the whole
> read routine in C would be much quicker - but surely reading from a
> raw vector should work out faster than reading from a file? The system
> is R-2.4.1/Linux, Xeon 3.2 GHz, 2 GiB RAM; typical file size is 44
> KiB.
>
> Thanks in advance,
> Jon
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jholtman at gmail.com  Wed Jan 31 14:22:39 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 31 Jan 2007 08:22:39 -0500
Subject: [R] help with function "system" and MS-DOS command TYPE
In-Reply-To: <43F64E86355A744E9D51506B6C6783B9019DB9FD@EM2.ad.ucla.edu>
References: <43F64E86355A744E9D51506B6C6783B9019DB9FD@EM2.ad.ucla.edu>
Message-ID: <644e1f320701310522t61bf64e4p80d691f0645aa077@mail.gmail.com>

Try 'file.show'

On 1/30/07, Chen, Xiao <jingy1 at ucla.edu> wrote:
> Greetings -
>
> I have a quick question that I hope someone will have a quick answer. I
> have tried to use the R function "system" with the MS-DOS command "type"
> to display the full content of a text file. But it always returns with a
> message saying the text file is not found. I could accomplish the same
> task with the "cat" command which is one of the unix-like commands that
> I have installed on my windows machine. But I would like to know how it
> would work with the "type" command.
>
> Here is what I have tried:
>
> > zz<-file("d:/work/test/test.txt", "w")
> > cat("this is a test\n", file=zz)
> > close(zz)
> > system("cat test.txt", show.output.on.console=T)
> this is a test
> > system("type test.txt", show.output.on.console=T)
> test.txt not found
> >
>
>
> Thanks much for any help that you could offer.
>
> Best,
>
> Xiao Chen
> Statistical Consulting Group
> UCLA Academic Technology Services
> http://www.ats.ucla.edu/stat/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jholtman at gmail.com  Wed Jan 31 14:32:25 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 31 Jan 2007 08:32:25 -0500
Subject: [R] Read multiple text files and combine into single file?
In-Reply-To: <BAY123-F26CC060420DE9A5A389E6BC6A60@phx.gbl>
References: <BAY123-F26CC060420DE9A5A389E6BC6A60@phx.gbl>
Message-ID: <644e1f320701310532y2e5dd210tef08869197b87d4d@mail.gmail.com>

I think something like this should work:

outFile <- file("output.txt", "w")
for (i in ListOfFileNames){
    x <- readLines(i)
    writeLines(x[2:(length(x)-1)], outFile)
}
close(outFile)



On 1/30/07, pif pif <pifbugs at hotmail.fr> wrote:
> Hi everyone!
>
> I have 1000 text files that contain data in a standard format separated by
> semi colons. However, each file also has single header (first) and footer
> (final) lines.  The text files are all named with a sample number, which do
> not follow a pattern.  I need to delete the header and footer lines and
> combine all files into a single (or possibly several) large files.
> Does anyone know how this could be done automatically using R?
>
> best regards,
>
> Ton
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jrkrideau at yahoo.ca  Wed Jan 31 14:32:42 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 31 Jan 2007 08:32:42 -0500 (EST)
Subject: [R] Fwd: Re:  Simple Date problems with cbind
Message-ID: <653719.60010.qm@web32809.mail.mud.yahoo.com>


--- John Kane <jrkrideau at yahoo.ca> wrote:

> Date: Wed, 31 Jan 2007 08:32:03 -0500 (EST)
> From: John Kane <jrkrideau at yahoo.ca>
> Subject: Re: [R] Simple Date problems with cbind
> To: marc_schwartz at comcast.net
> 
> My thanks to Marc, Tony Plate, Phil Spector & Mark
> Leeds.  
> 
> I totally missed the significance of the Y not y
> issue
> even  after reading ?strftime.  
> 
> Just after posting I realised why my date was
> getting
> coerced to a numeric. Thanks for the various
> alternatives for creating the data.frame that gets
> around this.
> 
> john
> 
> 
> --- Marc Schwartz <marc_schwartz at comcast.net> wrote:
> 
> > On Tue, 2007-01-30 at 15:27 -0500, John Kane
> wrote:
> > > I am clearly misunderstanding something about
> > dates
> > > and my reading of the help and RSiteSearch have
> > not
> > > turned up anything. 
> > > 
> > > I have a variable of class "Date" and I want to
> > add
> > > include it in a data.frame. However when do a
> > cbind
> > > the date var is coerced into a numeric. 
> > > 
> > > However when I tried to create a example I also
> > seem
> > > to be doing something wrong as I cannot seem
> even 
> > to
> > > create a date class var even when I try to copy
> an
> > > example from the Help
> > > 
> > > Example from Help
> > > dates <- c("02/27/92", "02/27/92", "01/14/92",
> > > "02/28/92", "02/01/92")
> > > max <-as.Date(dates, "%m/%d/%y")
> > > max
> > > class(max)
> > > 
> > > Results
> > > > dates <- c("02/27/92", "02/27/92", "01/14/92",
> > > "02/28/92", "02/01/92")
> > > > max <-as.Date(dates, "%m/%d/%y")
> > > > max
> > > [1] "1992-02-27" "1992-02-27" "1992-01-14"
> > > "1992-02-28" "1992-02-01"
> > > > class(max)
> > > [1] "Date"
> > > 
> > > 
> > > My example
> > > 
> > > cc <- c("2005/01/24" ,"2006/01/23"
> ,"2006/01/23",
> > > "2006/01/23")
> > > xx <- as.Date(cc, "%y/%m/%d")
> > > xx
> > > class(xx)
> > 
> > 
> > You need to use a capital "Y" for a four digit
> > year...
> > 
> > See ?strftime for more information on date
> formats.
> > 
> > 
> > > Results
> > > > cc <- c("2005/01/24" ,"2006/01/23"
> > ,"2006/01/23",
> > > "2006/01/23")
> > > > xx <- as.Date(cc, "%y/%m/%d")
> > > > xx
> > > [1] NA NA NA NA
> > > > class(xx)
> > > [1] "Date"
> > 
> > 
> > > cc <- c("2005/01/24" ,"2006/01/23"
> ,"2006/01/23",
> >           "2006/01/23")
> > 
> > xx <- as.Date(cc, "%Y/%m/%d")
> > 
> > > xx
> > [1] "2005-01-24" "2006-01-23" "2006-01-23"
> > "2006-01-23"
> > 
> > > class(xx)
> > [1] "Date"
> > 
> > 
> > 
> > > And on to the cbind problem
> > > 
> > > jj <- 1:5
> > > cbind(jj,max)
> > > 
> > >    jj  max
> > > [1,]  1 8092
> > > [2,]  2 8092
> > > [3,]  3 8048
> > > [4,]  4 8093
> > > [5,]  5 8066
> > > 
> > > I have tried various as.Date etc approcaes
> > > 
> > > It is probably something blindingly simple but
> can
> > > anyone suggest something?
> > > 
> > > Thanks
> > 
> > In this case, you are trying to cbind() a numeric
> > vector and a Date
> > vector into a matrix.  Since a matrix may only
> have
> > one data type, the
> > Date vector will be coerced to numeric.
> > 
> > If you want mixed data types, you need to create a
> > data frame:
> > 
> > jj <- 1:4
> > DF <- data.frame(jj, xx)
> > 
> > > DF
> >   jj         xx
> > 1  1 2005-01-24
> > 2  2 2006-01-23
> > 3  3 2006-01-23
> > 4  4 2006-01-23
> > 
> > > str(DF)
> > 'data.frame':   4 obs. of  2 variables:
> >  $ jj: int  1 2 3 4
> >  $ xx:Class 'Date'  num [1:4] 12807 13171 13171
> > 13171
> > 
> > 
> > Alternatively, create an initial data frame with
> > 'jj' and then cbind()
> > 'xx':
> > 
> > JJ <- data.frame(jj)
> > 
> > > str(JJ)
> > 'data.frame':   4 obs. of  1 variable:
> >  $ jj: int  1 2 3 4
> > 
> > DF <- cbind(JJ, xx)
> > 
> > > str(DF)
> > 'data.frame':   4 obs. of  2 variables:
> >  $ jj: int  1 2 3 4
> >  $ xx:Class 'Date'  num [1:4] 12807 13171 13171
> > 13171
> > 
> > 
> > Once you create the initial data frame, cbind()
> will
> > then use the
> > appropriate approach based upon the first argument
> > already being a data
> > frame.
> > 
> > HTH,
> > 
> > Marc Schwartz
> > 
> > 
> > 
> 
> 
> __________________________________________________
> Do You Yahoo!?

> protection around 
> http://mail.yahoo.com 
>


From tobias.sing at gmail.com  Wed Jan 31 11:53:43 2007
From: tobias.sing at gmail.com (Tobias Sing)
Date: Wed, 31 Jan 2007 11:53:43 +0100
Subject: [R] [R-pkgs] ROCR 1.0-2
Message-ID: <c3ca233a0701310253h458e88eam5d0a85ace4098fcc@mail.gmail.com>

Dear useRs,

an update of the ROCR package is available on CRAN.

ROCR helps in evaluating the performance of scoring classifiers using
ROC graphs, precision/recall plots, lift charts and many other
performance metrics.
For further information check http://rocr.bioinf.mpi-sb.mpg.de and
http://bioinformatics.oxfordjournals.org/cgi/reprint/21/20/3940

NEWS:
- added an optional parameter 'fpr.stop' to the performance measure 'auc',
  allowing to calculate the partial area under the ROC curve
  up to the false positive rate given by 'fpr.stop'.

- fixed bug in 'prediction' function which caused ROCR to halt
  in the context of a custom label.ordering (thanks to Roberto Perdisci
  for pointing out)

As usual, any feedback is more than welcome!
- Tobias


-- 
Tobias Sing
Computational Biology and Applied Algorithmics
Max Planck Institute for Informatics
Saarbrucken, Germany
Phone: +49 681 9325 315
Fax: +49 681 9325 399
http://www.tobiassing.net

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From ripley at stats.ox.ac.uk  Wed Jan 31 14:57:53 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 31 Jan 2007 13:57:53 +0000 (GMT)
Subject: [R] help with function "system" and MS-DOS command TYPE
In-Reply-To: <8727428.post@talk.nabble.com>
References: <43F64E86355A744E9D51506B6C6783B9019DB9FD@EM2.ad.ucla.edu>
	<8727428.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.64.0701311350280.17807@gannet.stats.ox.ac.uk>

On Wed, 31 Jan 2007, Vladimir Eremeev wrote:

> Chen, Xiao wrote:
>>
>> Greetings -
>>
>> I have a quick question that I hope someone will have a quick answer. I
>> have tried to use the R function "system" with the MS-DOS command "type"
>> to display the full content of a text file. But it always returns with a
>> message saying the text file is not found. I could accomplish the same
>> task with the "cat" command which is one of the unix-like commands that
>> I have installed on my windows machine. But I would like to know how it
>> would work with the "type" command.
>>
>> Here is what I have tried:
>>
>>> zz<-file("d:/work/test/test.txt", "w")
>>> cat("this is a test\n", file=zz)
>>> close(zz)
>>> system("cat test.txt", show.output.on.console=T)
>> this is a test
>>> system("type test.txt", show.output.on.console=T)
>> test.txt not found
>>
>
> type is an internal command, rather than an executable file
>
> use
> system("cmd /c type test.txt", show.output.on.console=T)
> or
> system("command /c type test.txt", show.output.on.console=T)

or use the user-friendly version shell(), as it says on the help page for 
system() (together with a portable version of the above).

Please can people who answer questions do their homework as the posting 
guide requests, and point to the definitive documentation.  The archives 
of these lists are a public resource, and often searched to pick up
pieces of misinformation.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jporzak at gmail.com  Wed Jan 31 15:55:53 2007
From: jporzak at gmail.com (Jim Porzak)
Date: Wed, 31 Jan 2007 06:55:53 -0800
Subject: [R] comparing random forests and classification trees
In-Reply-To: <20070130193206.GA5553@cs.umn.edu>
References: <005101c7433d$4bab92f0$7128d983@geol.utas.edu.au>
	<20070130193206.GA5553@cs.umn.edu>
Message-ID: <2a9c000c0701310655i65c350akf5213cdfefa9b318@mail.gmail.com>

Amy, et al,

I agree with you and the group that comparing test set classification
errors between the two methods is the way to go.

On interpretation, I find the partial dependence plots from
randomForest are useful - especially when talking to clients about
what the forest "means". See slides 32 to 38 in my recent DMA
presentation below for some examples. (When looking at the plots for
continuous variables, it's really important to pay attention to the
decile rug plot on the x-axis so as to not get distracted by the edges
which apply to a small part of the population)

I would argue that, except for simple "text book" examples, a full
classification tree is not all that easy to interpret. Sure, anyone
can walk through each branch, the over all meaning gets lost in the
trees.

http://loyaltymatrix.com/JimPorzak_RFwithR_DMAAC_Jan07_webinar.pdf


On 1/30/07, Darin A. England <england at cs.umn.edu> wrote:
> Amy,
>
> I have also had this issue with randomForest, that is, you lose the
> ability to explain the classifier in a simple way to
> non-specialists (everyone can understand the single decision tree.)
> As far as comparing the accuracy of the two, I think that you are
> correct in comparing them by the actual vs predicted tables.
> randomForest reports this as the confusion matrix, and it also
> reports the out-of-bag error, which I think you are referring to. I
> would not compare the rf out-of-bag error with the rpart relative
> error (or cross-validated error if you are doing cross validation.)
>
> So, for what it's worth I think you are correct. Also, do you know
> about ctree in the "party" package? If you want to retain the
> explanatory power of a single tree and have a nice accurate
> classifier, I have found ctree to work quite well.
>
> HTH,
>
> Darin
>
> On Mon, Jan 29, 2007 at 11:34:51AM +1100, Amy Koch wrote:
> > Hi,
> >
> > I have done an analysis using 'rpart' to construct a Classification Tree. I
> > am wanting to retain the output in tree form so that it is easily
> > interpretable. However, I am wanting to compare the 'accuracy' of the tree
> > to a Random Forest to estimate how much predictive ability is lost by using
> > one simple tree. My understanding is that the error automatically displayed
> > by the two functions is calculated differently so it is therefore incorrect
> > to use this as a comparison. Instead I have produced a table for both
> > analyses comparing the observed and predicted response.
> >
> > E.g. table(data$dependent,predict(model,type="class"))
> >
> > I am looking for confirmation that (a) it is incorrect to compare the error
> > estimates for the two techniques and (b) that comparing the
> > misclassification rates is an appropriate method for comparing the two
> > techniques.
> >
> > Thanks
> >
> > Amy
> >
> >
> >
> >
> >
> > Amelia Koch
> >
> > University of Tasmania
> >
> > School of Geography and Environmental Studies
> >
> > Private Bag 78 Hobart
> >
> > Tasmania, Australia 7001
> >
> > Ph: +61 3 6226 7454
> >
> > ajkoch at utas.edu.au
> >
> >
> >
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
HTH,
Jim Porzak
Loyalty Matrix Inc.
San Francisco, CA


From mckellercran at gmail.com  Wed Jan 31 16:18:42 2007
From: mckellercran at gmail.com (Matthew Keller)
Date: Wed, 31 Jan 2007 10:18:42 -0500
Subject: [R] How to print the objects in the memory
In-Reply-To: <45C04F78.09FD54.07207@m5-141.126.com>
References: <45C04F78.09FD54.07207@m5-141.126.com>
Message-ID: <3f547caa0701310718h5de2cedfj32df451bcba617fb@mail.gmail.com>

Hi usstata,

I think this will get you what you want:

mget(ls(),globalenv())


On 1/31/07, usstata <usstata at 126.com> wrote:
> Hi,all:
>
>        May be a  pointless question
>
>        a <- 1:10
>        b <- matrix(1:8,nrow = 4)
>        c <- letters[4:8]
>        ??
>
>        > ls()
>        [1] "a"      "b"      "c"
>
>        ls() can print the names of the objects in the memory ,
>
>        but I want to get the result :
> > a
> [1]  1  2  3  4  5  6  7  8  9 10
> > b
>     [,1] [,2]
> [1,]    1    5
> [2,]    2    6
> [3,]    3    7
> [4,]    4    8
> > c
> [1] "d" "e" "f" "g" "h"
> ??
>
>                I try the command print(noquote(ls()))  which it can't help
>
>
> Best regards
> usstata
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From dcn208 at nyu.edu  Wed Jan 31 16:22:21 2007
From: dcn208 at nyu.edu (Damion Colin Nero)
Date: Wed, 31 Jan 2007 10:22:21 -0500
Subject: [R] Comparing two data matrices of unequal size using rcor.test
Message-ID: <d5a2df939db6.45c06ddd@mail.nyu.edu>

I am using the rcor.test function available in the ltm package and
wanted to know if there was a way to use this function to compute the
correlation of two datasets that are not of equal length (similar to the
base cor function).  I was thinking an apply function like by might work
but so far I have had no luck getting it to work.I was wondering if
anyone could give me some general advice on this. 


Damion Nero
Plant Molecular Biology Lab
Department of Biology 
New York University


From davidr at rhotrading.com  Wed Jan 31 17:04:26 2007
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Wed, 31 Jan 2007 10:04:26 -0600
Subject: [R] Need to fit a regression line using orthogonal residuals
Message-ID: <F9F2A641C593D7408925574C05A7BE7720BDE2@rhopost.rhotrading.com>

This problem also comes up in financial hedging problems,
but usually the 'errors' need not be of comparable size, so Errors in
Variables or Total Least Squares might be used.

David L. Reiner
Rho Trading Securities, LLC
Chicago  IL  60605
312-362-4963

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof Brian Ripley
Sent: Wednesday, January 31, 2007 1:57 AM
To: Bill.Venables at csiro.au
Cc: jkopecky at umich.edu; r-help at stat.math.ethz.ch
Subject: Re: [R] Need to fit a regression line using orthogonal
residuals

Just to pick up

> Third, the resulting line is not optimal for either predicting y for a
> new x or x from a new y.  It's hard to see why it is ever of much
> interest.

It is not a regression (and hence the subject line was misleading), but
it 
does come up in errors-in-variables problems.  Suppose you have two sets

of measurements of the same quantity with the same variance of
measurement 
error and you want a line calibrating set 2 to set 1.  Then the optimal 
(in the sense of MLE, for example) line is this one, and it is
symmetrical 
in the two sets.

Now those are rather specific assumptions but they do come up in some 
problems in physics and analytical chemistry, and the result goes back
to 
the 19th century.  In the 1980s I implemented a version which allowed
for 
unequal (but known) heteroskedastic error variances which is quite
popular 
in analytical chemistry.

The literature is patchy:  Fuller's `Measurement Error Models' covers
the 
general area, and I recall this being in Sprent's (1969) book
`Models in Regression and Related Topics'.

See also the thread starting at

http://tolstoy.newcastle.edu.au/R/help/00a/0285.html

almost 7 years ago.  If that is the thread Jonathon Kopecky refers to
(how 
are we to know?) then he is misquoting me: I said it was the same thing
as 
using the first principal component, not an alternative proposal.


On Wed, 31 Jan 2007, Bill.Venables at csiro.au wrote:

> Jonathon Kopecky asks:
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jonathon
Kopecky
> Sent: Tuesday, 30 January 2007 5:52 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Need to fit a regression line using orthogonal residuals
>
> I'm trying to fit a simple linear regression of just Y ~ X, but both X
> and Y are noisy.  Thus instead of fitting a standard linear model
> minimizing vertical residuals, I would like to minimize
> orthogonal/perpendicular residuals.  I have tried searching the
> R-packages, but have not found anything that seems suitable.  I'm not
> sure what these types of residuals are typically called (they seem to
> have many different names), so that may be my trouble.  I do not want
to
> use Principal Components Analysis (as was answered to a previous
> questioner a few years ago), I just want to minimize the combined
noise
> of my two variables.  Is there a way for me to do this in R?

> [WNV] There's always a way if you are prepared to program it.  Your
> question is a bit like asking "Is there a way to do this in Fortran?"
> The most direct way to do it is to define a function that gives you
the
> sum of the perpendicular distances and minimise it using, say,
optim().
> E.g.
> 	ppdis <- function(b, x, y) sum((y - b[1] - b[2]*x)^2/(1+b[2]^2))
> 	b0 <- lsfit(x, y)$coef  # initial value
> 	op <- optim(b0, ppdis, method = "BFGS", x=x, y=y)
> 	op  # now to check the results
> 	plot(x, y, asp = 1)  # why 'asp = 1'?? exercise
> 	abline(b0, col = "red")
> 	abline(op$par, col = "blue")
> There are a couple of things about this you should be aware of, though
> First, this is just a fiddly way of finding the first principal
> component, so your desire not to use Principal Component Analysis is
> somewhat thwarted, as it must be.
> Second, the result is sensitive to scale - if you change the scales of
> either x or y, e.g. from lbs to kilograms, the answer is different.
> This also means that unless your measurement units for x and y are
> comparable it's hard to see how the result can make much sense.  A
> related issue is that you have to take some care when plotting the
> result or orthogonal distances will not appear to be orthogonal.
> Third, the resulting line is not optimal for either predicting y for a
> new x or x from a new y.  It's hard to see why it is ever of much
> interest.
> Bill Venables.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From HStevens at MUOhio.edu  Wed Jan 31 17:56:54 2007
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Wed, 31 Jan 2007 11:56:54 -0500
Subject: [R] Wiki for Graphics tips for MacOS X
Message-ID: <E9E8F95E-7610-4D61-ABF5-B8CEA952F1A3@MUOhio.edu>

Hi Folks,
I just finished (the first draft of) a Wiki document to explains  
options for producing graphics to incorporate in MS Word.
Thanks to all those who provided input, and who provided the wiki space.

Here is the direct link.

http://wiki.r-project.org/rwiki/doku.php?id=tips:using- 
platform:macosx-graphics

Cheers,
Hank

Dr. Hank Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/

"E Pluribus Unum"


From mpiktas at gmail.com  Wed Jan 31 18:03:41 2007
From: mpiktas at gmail.com (Vaidotas Zemlys)
Date: Wed, 31 Jan 2007 19:03:41 +0200
Subject: [R] features of save and save.image (unexpected file sizes)
Message-ID: <e47808320701310903h3a253dcfj33ea6aa859392661@mail.gmail.com>

Hi,

Today I came upon unexpected R behaviour. I did some modelling and the
result was R object,  about 28MB size (nested list, with matrixes as
list elements). When I was saving the session with save.image, the
resulting .RData file was 300MB. There were no other large objects:

> sum(sapply(ls(),function(x)eval(parse(text=paste("object.size(",x,")",sep=""))))/1024^2)
[1] 30.10540

The interesting thing, then I removed the large object with rm,
save.image again produced  .RData file with 300MB size. Only after
rm(list=ls()) I got normal sized .RData file. I used dump for dumping
my object, the resulting dump file was 72 MB in size. So I assume that
R was saving some large object which was not visible to me directly,
using ls(). Is there a way to find such objects, and discard them
before saving? I use R 2.4.1 on Ubuntu 6.06, through Emacs 23.0 and
ESS 5.3.1.

Vaidotas Zemlys
--
Doctorate student, http://www.mif.vu.lt/katedros/eka/katedra/zemlys.php
Vilnius University


From ggrothendieck at gmail.com  Wed Jan 31 18:11:42 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 31 Jan 2007 12:11:42 -0500
Subject: [R] Wiki for Graphics tips for MacOS X
In-Reply-To: <E9E8F95E-7610-4D61-ABF5-B8CEA952F1A3@MUOhio.edu>
References: <E9E8F95E-7610-4D61-ABF5-B8CEA952F1A3@MUOhio.edu>
Message-ID: <971536df0701310911g597d73afiede5947437e35498@mail.gmail.com>

To get the best results you need to transfer it using vector
graphics rather than bitmapped graphics:

http://www.stc-saz.org/resources/0203_graphics.pdf

There are a number of variations described here (see
entire thread).  Its for UNIX and Windows but I think
it would likely work similarly on Mac and Windows:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/32297.html

On 1/31/07, Martin Henry H. Stevens <HStevens at muohio.edu> wrote:
> Hi Folks,
> I just finished (the first draft of) a Wiki document to explains
> options for producing graphics to incorporate in MS Word.
> Thanks to all those who provided input, and who provided the wiki space.
>
> Here is the direct link.
>
> http://wiki.r-project.org/rwiki/doku.php?id=tips:using-
> platform:macosx-graphics
>
> Cheers,
> Hank
>
> Dr. Hank Stevens, Assistant Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
>
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/~stevenmh/
> http://www.muohio.edu/ecology/
> http://www.muohio.edu/botany/
>
> "E Pluribus Unum"
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From amnakhan493 at gmail.com  Wed Jan 31 18:23:17 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Wed, 31 Jan 2007 09:23:17 -0800
Subject: [R] Help to solve matrices
Message-ID: <3ffd3bb60701310923m258307f1s550881795133fe19@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070131/718e05ce/attachment.pl 

From amnakhan493 at gmail.com  Wed Jan 31 18:25:35 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Wed, 31 Jan 2007 09:25:35 -0800
Subject: [R] Regression Line Plot
Message-ID: <3ffd3bb60701310925j4bd4582dhe749f4bc7af400d3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070131/407f3c4a/attachment.pl 

From jingy1 at ucla.edu  Wed Jan 31 18:39:45 2007
From: jingy1 at ucla.edu (Chen, Xiao)
Date: Wed, 31 Jan 2007 09:39:45 -0800
Subject: [R] help with function "system" and MS-DOS command TYPE
Message-ID: <43F64E86355A744E9D51506B6C6783B9019DBA05@EM2.ad.ucla.edu>

Dear R-Help,

Thanks much. 

I have received very good advice from a couple of experts.  R-help is
just wonderful!

I have combined all the solutions that I got and they are shown below:

zz<-file("d:/work/test/test.txt", "w") 
cat("this is a test\n", file=zz)
close(zz)

setwd("d:/work/test")
shell("type test.txt") 
system("cat test.txt", show.output.on.console=T)
file.show("d:/work/test/test.txt") #opens a new window
file.show("d:/work/test/test.txt", pager="console") #displays on console

Thanks again for all the help. 

Best regards,

Xiao Chen
Statistical Consulting Group
UCLA Academic Technology Services
http://www.ats.ucla.edu/stat/

-----Original Message-----
From: Chen, Xiao 
Sent: Tuesday, January 30, 2007 8:23 PM
To: 'r-help at stat.math.ethz.ch'
Subject: help with function "system" and MS-DOS command TYPE

Greetings -

I have a quick question that I hope someone will have a quick answer. I
have tried to use the R function "system" with the MS-DOS command "type"
to display the full content of a text file. But it always returns with a
message saying the text file is not found. I could accomplish the same
task with the "cat" command which is one of the unix-like commands that
I have installed on my windows machine. But I would like to know how it
would work with the "type" command.  

Here is what I have tried:

> zz<-file("d:/work/test/test.txt", "w")
> cat("this is a test\n", file=zz)
> close(zz)
> system("cat test.txt", show.output.on.console=T)
this is a test
> system("type test.txt", show.output.on.console=T)
test.txt not found
>


Thanks much for any help that you could offer. 

Best, 

Xiao Chen
Statistical Consulting Group
UCLA Academic Technology Services
http://www.ats.ucla.edu/stat/


From marc_schwartz at comcast.net  Wed Jan 31 18:50:26 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 31 Jan 2007 11:50:26 -0600
Subject: [R] Regression Line Plot
In-Reply-To: <3ffd3bb60701310925j4bd4582dhe749f4bc7af400d3@mail.gmail.com>
References: <3ffd3bb60701310925j4bd4582dhe749f4bc7af400d3@mail.gmail.com>
Message-ID: <1170265826.4894.25.camel@localhost.localdomain>

On Wed, 2007-01-31 at 09:25 -0800, amna khan wrote:
> Sir I am not finding the function to plot least square regression line on
> type="o" plot of two variables.
> guid me in this regard.

Did you want something like this:

 x <- 1:50
 y <- rnorm(50)
 plot(x, y, type = "o")
 abline(lm(y ~ x))

See ?abline if so.

Alternatively, see ?predict.lm for additional possibilities.

HTH,

Marc Schwartz


From julien at no-log.org  Wed Jan 31 09:19:40 2007
From: julien at no-log.org (Julien Barnier)
Date: Wed, 31 Jan 2007 09:19:40 +0100
Subject: [R] R for SAS & SPSS Users Document
References: <7270AEC73132194E8BC0EE06B35D93D86D3CE7@UTKFSVS3.utk.tennessee.edu>
Message-ID: <87d54vbobn.fsf@ens-lsh.fr>

Hi,

> I am pleased to announce the availability of the document, "R for SAS
> and SPSS Users", at 
> http://oit.utk.edu/scc/RforSAS&SPSSusers.doc

I've looked at the document and printed it. I think it will be very
useful to me, even if I will use it "the reverse way" : learn how to
use SAS from R...

As I am far from an R expert, I will not be able to give you good
advices on R code. But maybe you would have had more comments on your
tutorial if you had given the link to the PDF version instead of the
MSWord one :

http://oit.utk.edu/scc/RforSAS&SPSSusers.pdf

Thanks again for your document,

-- 
Julien


From ripley at stats.ox.ac.uk  Wed Jan 31 19:06:04 2007
From: ripley at stats.ox.ac.uk (Professor Brian Ripley)
Date: Wed, 31 Jan 2007 18:06:04 +0000
Subject: [R] features of save and save.image (unexpected file sizes)
In-Reply-To: <e47808320701310903h3a253dcfj33ea6aa859392661@mail.gmail.com>
References: <e47808320701310903h3a253dcfj33ea6aa859392661@mail.gmail.com>
Message-ID: <45C0DA8C.2020902@stats.ox.ac.uk>

Two comments:

1) ls() does not list all the objects: it has all.names argument.

2) save.image() does not just save the objects in the workspace, it also
saves any environments they may have.  Having a function with a
large environment is the usual cause of a large saved image.

(And finally, a compressed binary representation from save.image is
nor comparable sizewise with an ASCII version from dump.)

Vaidotas Zemlys wrote:
> Hi,
> 
> Today I came upon unexpected R behaviour. I did some modelling and the
> result was R object,  about 28MB size (nested list, with matrixes as
> list elements). When I was saving the session with save.image, the
> resulting .RData file was 300MB. There were no other large objects:
> 
>> sum(sapply(ls(),function(x)eval(parse(text=paste("object.size(",x,")",sep=""))))/1024^2)
> [1] 30.10540
> 
> The interesting thing, then I removed the large object with rm,
> save.image again produced  .RData file with 300MB size. Only after
> rm(list=ls()) I got normal sized .RData file. I used dump for dumping
> my object, the resulting dump file was 72 MB in size. So I assume that
> R was saving some large object which was not visible to me directly,
> using ls(). Is there a way to find such objects, and discard them
> before saving? I use R 2.4.1 on Ubuntu 6.06, through Emacs 23.0 and
> ESS 5.3.1.
> 
> Vaidotas Zemlys
> --
> Doctorate student, http://www.mif.vu.lt/katedros/eka/katedra/zemlys.php
> Vilnius University

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From klijunjie at gmail.com  Wed Jan 31 19:16:41 2007
From: klijunjie at gmail.com (=?GB2312?B?wO6/ob3c?=)
Date: Thu, 1 Feb 2007 02:16:41 +0800
Subject: [R] A GSE data in the web of ncbi, GSE3524 cannot be open correctly
Message-ID: <dff718fc0701311016t2054ed7bo85df7e33e0b60782@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070201/cb335570/attachment.pl 

From timh at insightful.com  Wed Jan 31 20:00:29 2007
From: timh at insightful.com (Tim Hesterberg)
Date: 31 Jan 2007 11:00:29 -0800
Subject: [R] bootstrap syntax + bootstrap for teaching
In-Reply-To: <Pine.OSX.4.58.0701241839470.2545@jacob-wegelins-computer.local>
	(message from Jacob Wegelin on Fri, 26 Jan 2007 12:26:04 -0800 (PST))
References: <Pine.OSX.4.58.0701241839470.2545@jacob-wegelins-computer.local>
Message-ID: <SEWINEXCH00zUSxvjPk0000016f@sewinexch00.insightful.com>

Previous subject:
bootstrap bca confidence intervals for large number of statistics in one model; library("boot")

Jacob Wegelin asked for an easier way to do many bootstrap confidence
intervals for regression output.
The syntax would be easier with S+Resample, example below.
You create an ordinary lm object, then do e.g.
	boot <- bootstrap(fit, c(coef(fit), predict(fit, newdata=NEWDATA)))
	limits.bca(boot)


---- RESAMPLING FOR TEACHING ----

I would encourage people to consider using S+Resample for teaching.
There is a free student version of S+, and S+Resample is easier for
students -- easier syntax (in general, not just this example), plus a
menu interface.  There are free chapters and packages you can use to
supplement a statistics course with resampling.  See:
http://www.insightful.com/Hesterberg/bootstrap/#Reference.introStat

I'll give a workshop on resampling for teaching at the USCOTS conference
(U.S. Conference On Teaching Statistics) on May 16.
http://www.causeweb.org/uscots
http://www.causeweb.org/workshop/hesterberg/


set.seed(0)
x <- runif(150)
y <- 2/3 + pi * x^2 + runif(length(x))/2
plot(x,y)
DAT <- data.frame(x,y)
NEWDATA <- data.frame(x=seq(min(x), max(x), length=50))
library(resample)
fit <- lm(y ~ x + x^2, data=DAT)
boot <- bootstrap(fit, c(coef(fit), predict(fit, newdata = NEWDATA)))
CIs <- limits.bca(boot)
lines(NEWDATA$x, CIs[4:53,1], col="red")
lines(NEWDATA$x, CIs[4:53,4], col="red")
CIs[1:3,]

I used x + x^2 in the model rather than poly(x,2), because poly is
data dependent, so regression coefficients cannot be used for new
data, and confidence intervals for the coefficients are not meaningful.

Comment - the default is 1000 bootstrap samples; that isn't really
enough for BCa intervals, because the BCa calculations magnify the
Monte Carlo standard error by roughly a factor of two.

Jacob Wegelin wrote:
>Sometimes one might like to obtain pointwise bootstrap bias-corrected,
>accelerated (BCA) confidence intervals for a large number of statistics
>computed from a single dataset.  For instance, one might like to get
>(so as to plot graphically) bootstrap confidence bands for the fitted
>values in a regression model.
>
>(Example: Chiu S et al., Early Acceleration of Head Circumference in
>Children with Fragile X Syndrome and Autism. Journal of Developmental &
>Behavioral Pediatrics 2007;In press. In this paper we plot the
>estimated trajectories of head circumference for two different
>groups, computed by linear mixed-effects models, with confidence bands
>obtained by bootstrap.)
>
>Below is a toy example of how to do this using the "boot" library.
>We obtain BCA CIs for all three regression parameters and for the fitted
>values at 50 levels of the predictor.
>
>set.seed(1234567)
>x<-runif(150)
>y<-2/3 + pi * x^2 + runif(length(x))/2
>plot(x,y)
>DAT<-data.frame(x,y)
>NEWDATA<-data.frame(x=seq(min(x), max(x), length=50))
>library('boot')
>myfn<-function(data, whichrows) {
>	TheseData<-data[whichrows,]
>	thisLM<-lm( y~poly(x,2), data=TheseData)
>	thisFit<-predict(thisLM, newdata=NEWDATA)
>	c(
>		coef(summary(thisLM))[,"Estimate"]
>		, thisFit)
>}
>bootObj<-boot( data=DAT, statistic=myfn, R=1000 )
>names(bootObj)
>dim(bootObj$t)
>sofar<-t(sapply( 1:ncol(bootObj$t), function(thiscolumn) boot.ci(bootObj, type='bca', index=thiscolumn)$bca[4:5] ))
>colnames(sofar)<-c("lo", "hi")
>NEWDATA<-cbind(NEWDATA, sofar[4:nrow(sofar),])
>thecoefs<-sofar[1:3,]
>lines( NEWDATA$x, NEWDATA$lo, col='red')
>lines( NEWDATA$x, NEWDATA$hi, col='red')
>
>Note: To get boot.ci to run with type='bca' it seems necessary to have a
>large value of R.  Otherwise boot.ci returns an error, in my (limited)
>experience.
>
>Thanks in advance for any critiques.  (For instance, is there an easier or more elegant way?)

Caveat - I helped create S+Resample.

========================================================
| Tim Hesterberg       Senior Research Scientist       |
| timh at insightful.com  Insightful Corp.                |
| (206)802-2319        1700 Westlake Ave. N, Suite 500 |
| (206)283-8691 (fax)  Seattle, WA 98109-3044, U.S.A.  |
|                      www.insightful.com/Hesterberg   |
========================================================
Download S+Resample from www.insightful.com/downloads/libraries

Bootstrap short courses:  May 21-22 Philadelphia, Oct 10-11 San Francisco.
                          2-3 May UK, 3-4 Oct UK.
		http://www.insightful.com/services/training.asp

Workshop on resampling for teaching:  May 16 Ohio State 
		http://www.causeweb.org/workshop/hesterberg/


From justin_bem at yahoo.fr  Wed Jan 31 20:21:24 2007
From: justin_bem at yahoo.fr (justin bem)
Date: Wed, 31 Jan 2007 20:21:24 +0100 (CET)
Subject: [R] RE :  Help to solve matrices
In-Reply-To: <3ffd3bb60701310923m258307f1s550881795133fe19@mail.gmail.com>
Message-ID: <331869.45125.qm@web23004.mail.ird.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070131/5f59dee2/attachment.pl 

From konrad.hammel at prilink.com  Wed Jan 31 20:35:45 2007
From: konrad.hammel at prilink.com (Konrad)
Date: Wed, 31 Jan 2007 14:35:45 -0500
Subject: [R] Quick Question about R
Message-ID: <00a801c7456f$0148e7b0$9c01a8c0@compaqgzlehbfc>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070131/f2630d98/attachment.pl 

From marc_schwartz at comcast.net  Wed Jan 31 20:38:53 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 31 Jan 2007 13:38:53 -0600
Subject: [R] RE :  Help to solve matrices
In-Reply-To: <331869.45125.qm@web23004.mail.ird.yahoo.com>
References: <331869.45125.qm@web23004.mail.ird.yahoo.com>
Message-ID: <1170272333.4894.36.camel@localhost.localdomain>

Justin,

Did you actually run that code?  :-)

> a<-array(c(3,4,1),dim=c(3,1))
> b<-a%*%a
Error in a %*% a : non-conformable arguments

I suspect that Amina wants:

a <- array(c(3, 4, 1), dim=c(3, 1))
b <- t(a) %*% a

> solve(b)
           [,1]
[1,] 0.03846154


or perhaps:

> solve(crossprod(a))
           [,1]
[1,] 0.03846154


Amina, note that %o% is an 'outer' product, not an 'inner' product.

See ?outer and ?"%*%"

HTH,

Marc Schwartz


On Wed, 2007-01-31 at 20:21 +0100, justin bem wrote:
> >a<-array(c(3,4,1),dim=c(3,1))
> >b<-a%*%a
> >solve(b)
> 
> amna khan <amnakhan493 at gmail.com> a crit : Sir I am new user of R. I
> am facing problems in solving matrices.
> for example
> >a<-array(c(3,4,1),dim=c(3,1))
> >b<-a%o%a
> >b
> 
> , , 1, 1
> 
>      [,1]
> [1,]    9
> [2,]   12
> [3,]    3
> 
> , , 2, 1
> 
>      [,1]
> [1,]   12
> [2,]   16
> [3,]    4
> 
> , , 3, 1
> 
>      [,1]
> [1,]    3
> [2,]    4
> [3,]    1
> >solve(b)
> Error in solve.default(b) : only square matrices can be inverted
> >solve(b,a)
> Error in solve.default(b, a) : 'b' must be compatible with 'a'
> Sir I request you to help me in  this regard.


From bcarvalh at jhsph.edu  Wed Jan 31 20:42:33 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Wed, 31 Jan 2007 14:42:33 -0500
Subject: [R] Quick Question about R
In-Reply-To: <00a801c7456f$0148e7b0$9c01a8c0@compaqgzlehbfc>
References: <00a801c7456f$0148e7b0$9c01a8c0@compaqgzlehbfc>
Message-ID: <1F8211E4-CA57-4097-B0FE-4FCB802DE204@jhsph.edu>

suppressWarnings(a <- as.numeric(c(1, 2, pi, "a", 9, "z")))

b

On Jan 31, 2007, at 2:35 PM, Konrad wrote:

> Hello,
> Is there a way to convert a character to a number with out getting  
> a warning?  I have a vector that has both numbers and letters in it  
> and I need to convert it to only numbers.  At the moment I'm using  
> as.numeric but it is generating a warning when it converts a  
> letter.  Is there another function out there that will do what I  
> need or is there a way to turn off the warnings as I don't want the  
> warning to be displayed to the end user?
>
> Konrad Hammel
> Engineer
> Prilink LTD
> konrad.hammel at prilink.com
> 905.305.1096
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ccleland at optonline.net  Wed Jan 31 20:43:19 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 31 Jan 2007 14:43:19 -0500
Subject: [R] Quick Question about R
In-Reply-To: <00a801c7456f$0148e7b0$9c01a8c0@compaqgzlehbfc>
References: <00a801c7456f$0148e7b0$9c01a8c0@compaqgzlehbfc>
Message-ID: <45C0F157.3090502@optonline.net>

Konrad wrote:
> Hello,
> Is there a way to convert a character to a number with out getting a warning?  I have a vector that has both numbers and letters in it and I need to convert it to only numbers.  At the moment I'm using as.numeric but it is generating a warning when it converts a letter.  Is there another function out there that will do what I need or is there a way to turn off the warnings as I don't want the warning to be displayed to the end user?

?options

options(warn = -1)

as.numeric(c("1","A","2"))
[1]  1 NA  2

options(warn = 0) # return to default

> Konrad Hammel
> Engineer
> Prilink LTD
> konrad.hammel at prilink.com
> 905.305.1096
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From marc_schwartz at comcast.net  Wed Jan 31 20:48:59 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 31 Jan 2007 13:48:59 -0600
Subject: [R] Quick Question about R
In-Reply-To: <00a801c7456f$0148e7b0$9c01a8c0@compaqgzlehbfc>
References: <00a801c7456f$0148e7b0$9c01a8c0@compaqgzlehbfc>
Message-ID: <1170272939.4894.42.camel@localhost.localdomain>

On Wed, 2007-01-31 at 14:35 -0500, Konrad wrote:
> Hello,
> Is there a way to convert a character to a number with out getting a
> warning?  I have a vector that has both numbers and letters in it and
> I need to convert it to only numbers.  At the moment I'm using
> as.numeric but it is generating a warning when it converts a letter.
> Is there another function out there that will do what I need or is
> there a way to turn off the warnings as I don't want the warning to be
> displayed to the end user?

If you provide an example of your data, we can offer more definitive
assistance, especially if the formats are in a regular pattern.

However, you might want to look at:

?gsub  (and perhaps ?regex)
?strsplit
?substr
?strtrim

One quick example:

> V <- paste(sample(c(LETTERS, 0:9), 20), collapse = "")

> V
[1] "D7YL1N5U2H8QCS03RXMZ"

> gsub("[A-Z]", "", V)
[1] "7152803"

> as.numeric(gsub("[A-Z]", "", V))
[1] 7152803


HTH,

Marc Schwartz


From ted.harding at nessie.mcc.ac.uk  Wed Jan 31 20:59:36 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 31 Jan 2007 19:59:36 -0000 (GMT)
Subject: [R] Quick Question about R
In-Reply-To: <00a801c7456f$0148e7b0$9c01a8c0@compaqgzlehbfc>
Message-ID: <XFMail.070131195936.ted.harding@nessie.mcc.ac.uk>

On 31-Jan-07 Konrad wrote:
> Hello,
> Is there a way to convert a character to a number with out getting a
> warning?  I have a vector that has both numbers and letters in it and I
> need to convert it to only numbers.  At the moment I'm using as.numeric
> but it is generating a warning when it converts a letter.  Is there
> another function out there that will do what I need or is there a way
> to turn off the warnings as I don't want the warning to be displayed to
> the end user?

Have a look at the entries "warn" and "warning.expression" under
?options

In particular:

> options()$warn
[1] 0

> x <- c(1,2,"C",4,"E",6,"G")
> as.numeric(x)
[1]  1  2 NA  4 NA  6 NA
Warning message: 
NAs introduced by coercion 

> old.warn <- options()$warn
> options(warn = -1)
> as.numeric(x)
[1]  1  2 NA  4 NA  6 NA

> options(warn = old.warn)
> as.numeric(x)
[1]  1  2 NA  4 NA  6 NA
Warning message: 
NAs introduced by coercion 
> 

Note that options(warn = -1) turns off all warning messages everywhere,
so you may want to bracket your calls to as.numeric() in the way shown
above, to avoid intefering with other cases where you may want the
user to see the warning.

Hoping this helps,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 31-Jan-07                                       Time: 19:59:33
------------------------------ XFMail ------------------------------


From peteoutside at yahoo.com  Wed Jan 31 21:01:36 2007
From: peteoutside at yahoo.com (Pete Cap)
Date: Wed, 31 Jan 2007 12:01:36 -0800 (PST)
Subject: [R] Random Sampling pointers?
Message-ID: <540643.55472.qm@web52401.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070131/5c409056/attachment.pl 

From Ted.Harding at manchester.ac.uk  Wed Jan 31 21:02:29 2007
From: Ted.Harding at manchester.ac.uk ( (Ted Harding))
Date: Wed, 31 Jan 2007 20:02:29 -0000 (GMT)
Subject: [R] Quick Question about R
In-Reply-To: <1F8211E4-CA57-4097-B0FE-4FCB802DE204@jhsph.edu>
Message-ID: <XFMail.070131200229.Ted.Harding@manchester.ac.uk>

On 31-Jan-07 Benilton Carvalho wrote:
> suppressWarnings(a <- as.numeric(c(1, 2, pi, "a", 9, "z")))

Of course! Much better!
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 31-Jan-07                                       Time: 20:02:25
------------------------------ XFMail ------------------------------


From marc_schwartz at comcast.net  Wed Jan 31 21:36:25 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 31 Jan 2007 14:36:25 -0600
Subject: [R] Quick Question about R
In-Reply-To: <XFMail.070131200229.Ted.Harding@manchester.ac.uk>
References: <XFMail.070131200229.Ted.Harding@manchester.ac.uk>
Message-ID: <1170275785.4894.49.camel@localhost.localdomain>

On Wed, 2007-01-31 at 20:02 +0000, Ted.Harding at manchester.ac.uk wrote:
> On 31-Jan-07 Benilton Carvalho wrote:
> > suppressWarnings(a <- as.numeric(c(1, 2, pi, "a", 9, "z")))
> 
> Of course! Much better!
> Ted.

In the context of my prior reply:

# Bear in mind that the above vector is of class character, not mixed...
> str(c(1, 2, pi, "a", 9, "z"))
 chr [1:6] "1" "2" "3.14159265358979" "a" "9" "z"

> class(c(1, 2, pi, "a", 9, "z"))
[1] "character"


Thus:

> grep("[0-9\\.]", Vec, value = TRUE)
[1] "1"                "2"                "3.14159265358979"
[4] "9"               
 
> as.numeric(grep("[0-9\\.]", Vec, value = TRUE))
[1] 1.000000 2.000000 3.141593 9.000000

or:

> gsub("[A-Za-z]", "", Vec)
[1] "1"                "2"                "3.14159265358979"
[4] ""                 "9"                ""                
 
> as.numeric(gsub("[A-Za-z]", "", Vec))
[1] 1.000000 2.000000 3.141593       NA 9.000000       NA


The latter solution returns the NA's for non-convertible elements,
whereas the former, only the resultant numerics.

My preference is to be proactive in managing the data rather than
disabling warnings.  :-)

HTH,

Marc Schwartz


From mckellercran at gmail.com  Wed Jan 31 22:33:26 2007
From: mckellercran at gmail.com (Matthew Keller)
Date: Wed, 31 Jan 2007 16:33:26 -0500
Subject: [R] what is the purpose of an error message in uniroot?
Message-ID: <3f547caa0701311333o4d6886a2pf796a3c04aa173a3@mail.gmail.com>

Hi all,

This is probably a blindingly obvious question: Why does it matter in
the uniroot function whether the f() values at the end points that you
supply are of the same sign?

For example:

f <- function(x,y) {y-x^2+1}

#this gives a warning
uniroot(f,interval=c(-5,5),y=0)
Error in uniroot(f, interval=c(-5, 5), y = 0) : f() values at end
points not of opposite sign

#this doesn't give a warning
uniroot(f,interval=c(.1,5),y=0)
$root
[1] 1

$f.root
[1] 1.054e-05

$iter
[1] 9

$estim.prec
[1] 6.104e-05

If I comment out the two lines of script in the uniroot function that
produce this warning and create a new function, call it uniroot2,
everything works as I'd like. But for didactic purposes, why did the
creators of uniroot want the f() values at endpoints to be of opposite
sign?

Thanks in advance,

Matt


From rolf at math.unb.ca  Wed Jan 31 23:05:38 2007
From: rolf at math.unb.ca (rolf at math.unb.ca)
Date: Wed, 31 Jan 2007 18:05:38 -0400 (AST)
Subject: [R] what is the purpose of an error message in uniroot?
Message-ID: <200701312205.l0VM5cLI005328@weisner.math.unb.ca>

mckellercran at gmail.com wrote:

> This is probably a blindingly obvious question:

        Yes, it is.

> Why does it matter in the uniroot function whether the f() values at
> the end points that you supply are of the same sign?

        Plot some graphs.

        Think about the *name* of the function --- *uni*root.

        Does that ring any bells?

        And how do you know there *is* a root in the interval
        in question?  Try your ``uniroot2'' on f(x) = 1+x^2
        and the interval [-5,5].

        To belabour the point --- if the f() values are of the
        same sign, then there are 0, or 2, or 4, or ....
        roots in the interval in question.

        The ***only chance*** you have of there being a unique
        root is if the f() values are of opposite sign.

        The algorithm used and the precision estimates returned
        presumably depend on the change of sign.  You can get
        answers --- sometimes --- if the change of sign is not
        present, but the results could be seriously misleading.

        Without the opposite sign requirement the user will often
        wind up trying to do something impossible or getting
        results about which he/she is deluded.

                                cheers,

                                        Rolf Turner
                                        rolf at math.unb.ca

P. S.  If the f() values are of the same sign, uniroot() DOES
NOT give a warning!  It gives an error.

                                        R. T.


From mkimpel at iupui.edu  Wed Jan 31 23:38:52 2007
From: mkimpel at iupui.edu (Kimpel, Mark William)
Date: Wed, 31 Jan 2007 17:38:52 -0500
Subject: [R] possible spam alert
Message-ID: <836F00680EECD340A96AD34ECFF3B534B4AD0C@iu-mssg-mbx106.ads.iu.edu>

The last two times I have originated message threads on R or
Bioconductor I have received the message included below from someone
named Patrick Connolly. Both times I was the originator of the message
thread and used what I thought was a unique subject line that explained
as best I could what my question was. Patrick seems to be implying that
I am abusing the R and BioC help newsgroups in this fashion. 

When I emailed him to give me a specific example, he did not reply. The
most recent thread that he seems concerned about was to the R list and
was entitled "regexpr and parsing question" . I believe the previous
post of mine that he had problems with was to the BioC list but I can't
remember its subject.

Is this spam?

If I am doing this correctly, you should see the subject "possible spam
alert" in the subject header of THIS message.

Would the moderators of the lists please check and see if I am doing
some wrong and, if not, inform Mr. Connolly that I am not. If others
have received this message in error, it is possible it is spam and users
should be alerted.

Thanks,

Mark

Mark W. Kimpel MD 

 

 

Official Business Address:

 

Department of Psychiatry

Indiana University School of Medicine

PR M116

Institute of Psychiatric Research

791 Union Drive

Indianapolis, IN 46202

 
This is a request to anyone who starts a new subject to begin with a new
message and NOT reply to an existing one.  If your mail client is any
good, it's very simple to set up an alias (mine is simply 'r') so that
the tedious task of typing 'r-help at stat.math.ethz.ch' is unnecessary and
it's quicker than scrolling through an address book.
It's also quicker than deleting the previous subject.

Most mornings, I have over a screenful of messages mostly from R-help
and it's very useful to have them threaded.  However, the usefulness of
threading is lost when posters reply to a message and then change the
subject instead of creating a new message.

People who don't have a mail client that can display email in threads
are probably unaware that this sort of thing can happen in ones that do:


    37 N   25 Jan Luis Silva              ( 34) [R] plot/screen
    38 N   25 Jan Uwe Ligges              ( 55) `-> 
    39 N   25 Jan Fernando Henrique Ferra ( 20) [R] Plotting coloured
histograms
->  40 N   26 Jan Mohamed A. Kerasha      ( 12) |->[R] Distributions.
    41 N   26 Jan ripley at stats.ox.ac.uk   ( 26) | |->
    42     26 Jan Qin Xin                 (  9) | `->[R] how could I add
legends
    43     27 Jan Ko-Kang Kevin Wang      ( 31) |   `->
    44 N   26 Jan Remigijus Lapinskas     ( 32) |->Re: [R] Plotting
coloured his
    45 N   26 Jan Damon Wischik           (125) `-> 
    46 N   25 Jan Rex_Bryan at urscorp.com   ( 10) [R] plotting primatives,
ellipse
    47 N   25 Jan Uwe Ligges              ( 19) `->   


As Martin Maechler explained some time ago, it also screws up the
archives for a similar reason.

Your cooperation will be greatly appreciated.

best

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.

   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From octou at ipea.gov.br  Wed Jan 31 22:35:13 2007
From: octou at ipea.gov.br (Octavio Tourinho)
Date: Wed, 31 Jan 2007 19:35:13 -0200
Subject: [R] Problems installing R-2.4.1 on Solaris 11 x-86 from source:
 error in "gmake" after successful "configure"
Message-ID: <45C10B91.7020804@ipea.gov.br>

Dear friends,
I am trying to install R-2.4.1 from source on Solaris 11 x-86. 64 bits, 
running on Sun Ultra-20 workstation, and using the SunStudio 11 compilers.

I was able to "configure" R correctly, but received an error in "gmake", 
aparently related to bzip2 which I have been unable to debug.
The messages are listed below.
The configure.log and configure.status files are attached.

Any help would be sincerely appreciated.

Octavio Tourinho

=============================================
R is now configured for i386-pc-solaris2.11

 Source directory:          .
 Installation directory:    /usr/local

 C compiler:                gcc -std=gnu99 -D__NO_MATH_INLINES -g -O2
 Fortran 77 compiler:       g77  -g -O2

 C++ compiler:              g++  -g -O2
 Fortran 90/95 compiler:    f95 -g

 Interfaces supported:      X11, tcltk
 External libraries:        readline
 Additional capabilities:   PNG, JPEG, NLS
 Options enabled:           shared BLAS, R profiling

 Recommended packages:      yes

configure: WARNING: you cannot build DVI versions of the R manuals
configure: WARNING: you cannot build PDF versions of the R manuals
# gmake
gmake[1]: Entering directory `/usr/local/R-2.4.1/m4'
gmake[1]: Nothing to be done for `R'.
gmake[1]: Leaving directory `/usr/local/R-2.4.1/m4'
gmake[1]: Entering directory `/usr/local/R-2.4.1/tools'
gmake[1]: Nothing to be done for `R'.
gmake[1]: Leaving directory `/usr/local/R-2.4.1/tools'
gmake[1]: Entering directory `/usr/local/R-2.4.1/doc'
gmake[2]: Entering directory `/usr/local/R-2.4.1/doc/html'
gmake[3]: Entering directory `/usr/local/R-2.4.1/doc/html/search'
gmake[3]: Leaving directory `/usr/local/R-2.4.1/doc/html/search'
gmake[2]: Leaving directory `/usr/local/R-2.4.1/doc/html'
gmake[2]: Entering directory `/usr/local/R-2.4.1/doc/manual'
gmake[2]: Nothing to be done for `R'.
gmake[2]: Leaving directory `/usr/local/R-2.4.1/doc/manual'
gmake[1]: Leaving directory `/usr/local/R-2.4.1/doc'
gmake[1]: Entering directory `/usr/local/R-2.4.1/etc'
gmake[1]: Leaving directory `/usr/local/R-2.4.1/etc'
gmake[1]: Entering directory `/usr/local/R-2.4.1/share'
gmake[1]: Leaving directory `/usr/local/R-2.4.1/share'
gmake[1]: Entering directory `/usr/local/R-2.4.1/src'
gmake[2]: Entering directory `/usr/local/R-2.4.1/src/scripts'
creating src/scripts/R.fe
gmake[3]: Entering directory `/usr/local/R-2.4.1/src/scripts'
gmake[3]: Leaving directory `/usr/local/R-2.4.1/src/scripts'
gmake[2]: Leaving directory `/usr/local/R-2.4.1/src/scripts'
gmake[2]: Entering directory `/usr/local/R-2.4.1/src/include'
config.status: creating src/include/config.h
config.status: src/include/config.h is unchanged
Rmath.h is unchanged
gmake[3]: Entering directory `/usr/local/R-2.4.1/src/include/R_ext'
gmake[3]: Nothing to be done for `R'.
gmake[3]: Leaving directory `/usr/local/R-2.4.1/src/include/R_ext'
gmake[2]: Leaving directory `/usr/local/R-2.4.1/src/include'
gmake[2]: Entering directory `/usr/local/R-2.4.1/src/extra'
gmake[3]: Entering directory `/usr/local/R-2.4.1/src/extra/blas'
gmake[4]: Entering directory `/usr/local/R-2.4.1/src/extra/blas'
gmake[4]: `libRblas.so' is up to date.
gmake[4]: Leaving directory `/usr/local/R-2.4.1/src/extra/blas'
gmake[4]: Entering directory `/usr/local/R-2.4.1/src/extra/blas'
/usr/local/R-2.4.1/lib/libRblas.so is unchanged
gmake[4]: Leaving directory `/usr/local/R-2.4.1/src/extra/blas'
gmake[3]: Leaving directory `/usr/local/R-2.4.1/src/extra/blas'
gmake[3]: Entering directory `/usr/local/R-2.4.1/src/extra/bzip2'
gmake[4]: Entering directory `/usr/local/R-2.4.1/src/extra/bzip2'
gmake[4]: Leaving directory `/usr/local/R-2.4.1/src/extra/bzip2'
gmake[4]: Entering directory `/usr/local/R-2.4.1/src/extra/bzip2'
rm -f libbz2.a
false cr libbz2.a blocksort.o bzlib.o compress.o crctable.o decompress.o 
huffman.o randtable.o
gmake[4]: *** [libbz2.a] Error 1
gmake[4]: Leaving directory `/usr/local/R-2.4.1/src/extra/bzip2'
gmake[3]: *** [R] Error 2
gmake[3]: Leaving directory `/usr/local/R-2.4.1/src/extra/bzip2'
gmake[2]: *** [R] Error 1
gmake[2]: Leaving directory `/usr/local/R-2.4.1/src/extra'
gmake[1]: *** [R] Error 1
gmake[1]: Leaving directory `/usr/local/R-2.4.1/src'
gmake: *** [R] Error 1
============================================================
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: config.status
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070131/38ae49ed/attachment.pl 

From wvanwie at few.vu.nl  Wed Jan 31 16:03:03 2007
From: wvanwie at few.vu.nl (wvanwie at few.vu.nl)
Date: Wed, 31 Jan 2007 16:03:03 +0100
Subject: [R] Estimation of discrete unimodal density
Message-ID: <1170255783.45c0afa798f66@www.few.vu.nl>

Dear All,


A method for the estimation is univariate unimodal densities (with unknown 
mode) is described in "Statistical Inference under Order Restrictions" by 
Barlow et al.. Would anyone know whether there is an R-implementation 
(preferably with reference) for the estimation of univariate discrete unimodal 
densities (with unknown mode)? Thanks in advance for your help.

Kind regards,


Wessel van Wieringen


From benoit at ebi.ac.uk  Wed Jan 31 17:33:22 2007
From: benoit at ebi.ac.uk (Benoit Ballester)
Date: Wed, 31 Jan 2007 16:33:22 +0000
Subject: [R] R for bioinformatics
Message-ID: <45C0C4D2.9080908@ebi.ac.uk>

Hi,

I was wondering if someone could tell me more about this book, (if it's 
a good or bad one).
I can't find it, as it seems that O'Reilly doesn't publish any more.

Thanks,

Ben


-- 
Benoit Ballester


