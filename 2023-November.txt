From dw|n@em|u@ @end|ng |rom comc@@t@net  Wed Nov  1 07:08:37 2023
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Tue, 31 Oct 2023 23:08:37 -0700
Subject: [R] running crossvalidation many times MSE for Lasso regression
In-Reply-To: <CAGxFJbQ26vOFLgHbSiqObOz+amAY94xVMx0d8UUTg8OPaLLdjg@mail.gmail.com>
References: <1823742611.1400069.1698006976815.ref@mail.yahoo.com>
 <1823742611.1400069.1698006976815@mail.yahoo.com>
 <CAGxFJbQ26vOFLgHbSiqObOz+amAY94xVMx0d8UUTg8OPaLLdjg@mail.gmail.com>
Message-ID: <32DB26CD-B94E-40FF-BCC1-D85221D466A7@comcast.net>



> On Oct 22, 2023, at 4:01 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> No error message shown Please include the error message so that it is
> not necessary to rerun your code. This might enable someone to see the
> problem without running the code (e.g. downloading packages, etc.)
> 
> -- Bert
> 
> On Sun, Oct 22, 2023 at 1:36?PM varin sacha via R-help
> <r-help at r-project.org> wrote:
>> 
>> Dear R-experts,
>> 
>> Here below my R code with an error message. Can somebody help me to fix this error?
>> Really appreciate your help.
>> 
>> Best,
>> 
>> ############################################################
>> # MSE CROSSVALIDATION Lasso regression
>> 
>> library(glmnet)
>> 
>> 
>> x1=c(34,35,12,13,15,37,65,45,47,67,87,45,46,39,87,98,67,51,10,30,65,34,57,68,98,86,45,65,34,78,98,123,202,231,154,21,34,26,56,78,99,83,46,58,91)
>> x2=c(1,3,2,4,5,6,7,3,8,9,10,11,12,1,3,4,2,3,4,5,4,6,8,7,9,4,3,6,7,9,8,4,7,6,1,3,2,5,6,8,7,1,1,2,9)
>> y=c(2,6,5,4,6,7,8,10,11,2,3,1,3,5,4,6,5,3.4,5.6,-2.4,-5.4,5,3,6,5,-3,-5,3,2,-1,-8,5,8,6,9,4,5,-3,-7,-9,-9,8,7,1,2)
>> T=data.frame(y,x1,x2)
>> 
>> z=matrix(c(x1,x2), ncol=2)
>> cv_model=glmnet(z,y,alpha=1)
>> best_lambda=cv_model$lambda.min
>> best_lambda
>> 
>> 
>> # Create a list to store the results
>> lst<-list()
>> 
>> # This statement does the repetitions (looping)
>> for(i in 1 :1000) {
>> 
>> n=45
>> 
>> p=0.667
>> 
>> sam=sample(1 :n,floor(p*n),replace=FALSE)
>> 
>> Training =T [sam,]
>> Testing = T [-sam,]
>> 
>> test1=matrix(c(Testing$x1,Testing$x2),ncol=2)
>> 
>> predictLasso=predict(cv_model, newx=test1)
>> 
>> 
>> ypred=predict(predictLasso,newdata=test1)

The error I got was:

Error in UseMethod("predict") : 
  no applicable method for 'predict' applied to an object of class "c('matrix', 'array', 'double', 'numeric')"


I'm not sure why the name of the object was cv_model since it was not created as a cross-validation result.

The loops called predict() twice and it was the second call that produced the error since the predictLasso object was not a glmnet classed object.

If the OP had left out the second use of predict and then subtracted predictLasso from the y vector a result would have appeared

y=T[-sam,]$y
MSE = mean((y-predictLasso)^2)
...
> mean(unlist(lst))
[1] 23.39621

Whether this is meaningful is hard to tell. It also makes the fundamental error of overwriting the original data object `y` with another intermediate result.

-- 
David
>> y=T[-sam,]$y
>> 
>> MSE = mean((y-ypred)^2)
>> MSE
>> lst[i]<-MSE
>> }
>> mean(unlist(lst))
>> ##################################################################
>> 
>> 
>> 
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From k|mem|||@029 @end|ng |rom gm@||@com  Wed Nov  1 14:06:27 2023
From: k|mem|||@029 @end|ng |rom gm@||@com (Kim Emilia)
Date: Wed, 1 Nov 2023 22:06:27 +0900
Subject: [R] How can I remove my packages from rdrr.io?
Message-ID: <CAFmxOoHUE1Hr4NkUWXNAxjk-Xjrqv30WbqrFJTM99ukpu8MjMA@mail.gmail.com>

Hello all,

I would like to take down my packages posted/created on the website rdrr.io.
[https://rdrr.io/] Is there any way to take down packages from the website?
It would be appreciated if you suggested/offered a way to remove the
package from the website.

Thank you.

	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Wed Nov  1 14:40:49 2023
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Wed, 1 Nov 2023 16:40:49 +0300
Subject: [R] How can I remove my packages from rdrr.io?
In-Reply-To: <CAFmxOoHUE1Hr4NkUWXNAxjk-Xjrqv30WbqrFJTM99ukpu8MjMA@mail.gmail.com>
References: <CAFmxOoHUE1Hr4NkUWXNAxjk-Xjrqv30WbqrFJTM99ukpu8MjMA@mail.gmail.com>
Message-ID: <20231101164049.2176bcce@arachnoid>

? Wed, 1 Nov 2023 22:06:27 +0900
Kim Emilia <kimemilia029 at gmail.com> ?????:

> I would like to take down my packages posted/created on the website
> rdrr.io.

I think it's unlikely to find people affiliated with rdrr.io here on
the R-help mailing list. Have you tried contacting the website author
via the links at the bottom of the page? They reference both the
author's e-mail address and the dedicated GitHub issue repo.

-- 
Best regards,
Ivan


From bbo|ker @end|ng |rom gm@||@com  Wed Nov  1 14:38:50 2023
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 1 Nov 2023 09:38:50 -0400
Subject: [R] How can I remove my packages from rdrr.io?
In-Reply-To: <CAFmxOoHUE1Hr4NkUWXNAxjk-Xjrqv30WbqrFJTM99ukpu8MjMA@mail.gmail.com>
References: <CAFmxOoHUE1Hr4NkUWXNAxjk-Xjrqv30WbqrFJTM99ukpu8MjMA@mail.gmail.com>
Message-ID: <92071c91-8946-42cc-88dc-bbe4c3d070f5@gmail.com>

   There is a github site with an issues list: 
https://github.com/rdrr-io/rdrr-issues/issues

   It looks like people have successfully requested removal in the past, 
e.g. https://github.com/rdrr-io/rdrr-issues/issues/113

On 2023-11-01 9:06 a.m., Kim Emilia wrote:
> Hello all,
> 
> I would like to take down my packages posted/created on the website rdrr.io.
> [https://rdrr.io/] Is there any way to take down packages from the website?
> It would be appreciated if you suggested/offered a way to remove the
> package from the website.
> 
> Thank you.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dw|n@em|u@ @end|ng |rom comc@@t@net  Wed Nov  1 14:42:52 2023
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Wed, 1 Nov 2023 06:42:52 -0700
Subject: [R] How can I remove my packages from rdrr.io?
In-Reply-To: <CAFmxOoHUE1Hr4NkUWXNAxjk-Xjrqv30WbqrFJTM99ukpu8MjMA@mail.gmail.com>
References: <CAFmxOoHUE1Hr4NkUWXNAxjk-Xjrqv30WbqrFJTM99ukpu8MjMA@mail.gmail.com>
Message-ID: <EBF36825-7994-47EC-BB61-90E862F115DD@comcast.net>



> On Nov 1, 2023, at 6:06 AM, Kim Emilia <kimemilia029 at gmail.com> wrote:
> 
> Hello all,
> 
> I would like to take down my packages posted/created on the website rdrr.io.
> [https://rdrr.io/] Is there any way to take down packages from the website?
> It would be appreciated if you suggested/offered a way to remove the
> package from the website.
> 

The website you are concerned about is not maintained by the R-project. A but of link-following suggested to me that the person you need to be addressing this to has a "personal" webpage at:  https://ianhowson.com/

There will be many other sites that hold most or all of the extensive list of CRAN packages, although it is certainly true that rrio.io has become a favorite of Google. 

-- 
David
> Thank you.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pd@|gd @end|ng |rom gm@||@com  Thu Nov  2 08:28:19 2023
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Thu, 2 Nov 2023 08:28:19 +0100
Subject: [R] weights vs. offset (negative binomial regression)
In-Reply-To: <acfecb0b-b3b1-4074-ae8a-ccfc27ce3346@gmail.com>
References: <CAPFqpxHHhbkVhiJTV8GKMmsRNgbjogHrXrb8d3sZmJm22B53Cw@mail.gmail.com>
 <322cd6d4-3223-4cbe-b958-e8fd8f38f9cb@gmail.com>
 <CAPFqpxHp09pQoF+hfhMYPt-Kv-=ahtbKF==uQ7iNn6708x5yoA@mail.gmail.com>
 <acfecb0b-b3b1-4074-ae8a-ccfc27ce3346@gmail.com>
Message-ID: <3372ED8B-9C57-44D5-8C7E-EA13CDF8670D@gmail.com>

I think it is more clear-cut than so, at least if the Poisson situation is something to go by. 

There, you can do either of these and get equivalent results

> fit.lung <- glm(cases ~ age + city,  offset=log(pop), 
+                 family=poisson, data=lungcancer)
> fit.lung2 <- glm(cases/pop ~ age + city,  weights=pop, 
+                 family=poisson, data=lungcancer)
There were 12 warnings (use warnings() to see them)

(Except for the warnings about non-integer responses, which have annoyed some epidemiologists trying to work with non-log link fuctions.)

The point is that you need to convert to rates on the LHS and then compensate for the fact that this has a smaller variance when the population is larger. Counts on the LHS combined with weights wouldn't be right. So I would expect that the weighted version of the OP's code should model Catch/Effort, although I'm not quite sure how glm.nb reacts to non-integer responses.

-pd


> On 31 Oct 2023, at 17:59 , Ben Bolker <bbolker at gmail.com> wrote:
> 
>  [Please keep r-help in the cc: list]
> 
>  I don't quite know how to interpret the difference between specifying effort as an offset vs. as weights; I would have to spend more time thinking about it/working through it than I have available at the moment.
> 
>   I don't know that specifying effort as weights is *wrong*, but I don't know that it's right or what it is doing: if I were the reviewer of a paper (for example) I would require you to explain what the difference is and convince me that it was appropriate. (Furthermore, "I want to do it this way because it gives me significant effects" is automatically suspicious.)
> 
>  This would be a good question for CrossValidated (https://stats.stackexchange.com), you could try posting it there (I would be interested in the answer!)
> 
>  cheers
>    Ben Bolker
> 
> 
> On 2023-10-30 8:19 p.m., ??? wrote:
>> Dear Mr. Bolker,
>> Thank you for the fast response.
>> I also know that a poisson (or negative binomial ) regression of glm is  generally modelled using an offset variable.
>> In this case, when a weights term instead of the offset is used, this gave me significant coefficients of covariance.
>> I understand that the weights function for exponential family distributions in glm affects the variance of response variable.
>> I was just wondering whether my first model is a completely wrong model and the use of offset variable is valid in the case that
>> response variable is  not proportional to offset variable such as my dataset.
>> Sincerely,
>> Joon-Taek
>> 2023? 10? 29? (?) ?? 3:25, Ben Bolker <bbolker at gmail.com <mailto:bbolker at gmail.com>>?? ??:
>>        Using an offset of log(Effort) as in your second model is the more
>>    standard way to approach this problem; it corresponds to assuming that
>>    catch is strictly proportional to effort. Adding log(Effort) as a
>>    covariate (as illustrated below) tests whether a power-law model (catch
>>    propto (Effort)^(b+1), b!=0) is a better description of the data.  (In
>>    this case it is not, although the confidence intervals on b are very
>>    wide, indicating that we have very little information -- this is not
>>    surprising since the proportional range of effort is very small
>>    (246-258) in this data set.
>>        In general you should *not* check overdispersion of the raw data
>>    (i.e., the *marginal distribution* of the data, you should check
>>    overdispersion of a fitted (e.g. Poisson) model, as below.
>>        cheers
>>         Ben Bolker
>>    edata <- data.frame(Catch, Effort, xx1, xx2, xx3)
>>    ## graphical exploration
>>    library(ggplot2); theme_set(theme_bw())
>>    library(tidyr)
>>    edata_long <- edata |> pivot_longer(names_to="var", cols =-c("Catch",
>>    "Effort"))
>>    ggplot(edata_long, aes(value, Catch)) +
>>          geom_point(alpha = 0.2, aes(size = Effort)) +
>>          facet_wrap(~var, scale="free_x") +
>>          geom_smooth(method = "glm", method.args = list(family =
>>    "quasipoisson"))
>>    #
>>    library(MASS)
>>    g1 <- glm.nb(Catch~xx1+xx2+xx3+offset(log(Effort)), data=edata)
>>    g2 <- update(g1, . ~ . + log(Effort))
>>    g0 <- glm(Catch~xx1+xx2+xx3+offset(log(Effort)), data=edata,
>>                family = poisson)
>>    performance::check_overdispersion(g0)
>>    summary(g1)
>>    summary(g2)
>>    options(digits = 3)
>>    confint(g2)
>>    summary(g1)
>>    On 2023-10-28 3:30 a.m., ??? wrote:
>>     > Colleagues,
>>     >
>>     >
>>     >
>>     > I have a dataset that includes five variables.
>>     >
>>     > - Catch: the catch number counted in some species (ind.)
>>     >
>>     > - Effort: fishing effort (the number of fishing vessels)
>>     >
>>     > - xx1, xx2, xx3: some environmental factors
>>     >
>>     > As an overdispersion test on the ?Catch? variable, I modeled with
>>    negative
>>     > binomial distribution using a GLM. The ?Effort? variable showed a
>>    gradually
>>     > decreasing trend during the study period. I was able to get the
>>    results I
>>     > wanted when considered ?Effort? function as a weights function in the
>>     > negative binomial regression as follows:
>>     >
>>     >
>>     >
>>     > library(qcc)
>>     >
>>     >
>>    Catch=c(25,2,7,6,75,5,1,4,66,15,9,25,40,8,7,4,36,11,1,14,141,9,74,38,126,3)
>>     >
>>     >
>>    Effort=c(258,258,258,258,258,258,258,254,252,252,252,252,252,252,252,252,252,252,252,248,246,246,246,246,246,246)
>>     >
>>     >
>>    xx1=c(0.8,0.5,1.2,0.5,1.1,1.1,1.0,0.6,0.9,0.5,1.2,0.6,1.2,0.7,1.0,0.6,1.6,0.7,0.8,0.6,1.7,0.9,1.1,0.5,1.4,0.5)
>>     >
>>     >
>>    xx2=c(1.7,1.6,2.7,2.6,1.5,1.5,2.8,2.5,1.7,1.9,2.2,2.4,1.6,1.4,3.0,2.4,1.4,1.5,2.2,2.3,1.7,1.7,1.9,1.9,1.4,1.4)
>>     >
>>     >
>>    xx3=c(188,40,2,10,210,102,117,14,141,28,48,15,220,115,10,14,320,20,3,10,400,150,145,160,460,66)
>>     >
>>     > #
>>     >
>>     > edata <- data.frame(Catch, Effort, xx1, xx2, xx3)
>>     >
>>     > #
>>     >
>>     > qcc.overdispersion.test(edata$Catch, type="poisson")
>>     >
>>     > #
>>     >
>>     > summary(glm.nb(Catch~xx1+xx2+xx3, weights=Effort, data=edata))
>>     >
>>     > summary(glm.nb(Catch~xx1+xx2+xx3+offset(log(Effort)), data=edata))
>>     >
>>     >
>>     >
>>     > I am not sure the application of the weights function to the negative
>>     > binomial regression is correct. Also I wonder if there is a
>>    better way
>>     > doing this. Can anyone help?
>>     >
>>     >       [[alternative HTML version deleted]]
>>     >
>>     > ______________________________________________
>>     > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>>    -- To UNSUBSCRIBE and more, see
>>     > https://stat.ethz.ch/mailman/listinfo/r-help
>>    <https://stat.ethz.ch/mailman/listinfo/r-help>
>>     > PLEASE do read the posting guide
>>    http://www.R-project.org/posting-guide.html
>>    <http://www.R-project.org/posting-guide.html>
>>     > and provide commented, minimal, self-contained, reproducible code.
>>    ______________________________________________
>>    R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>>    To UNSUBSCRIBE and more, see
>>    https://stat.ethz.ch/mailman/listinfo/r-help
>>    <https://stat.ethz.ch/mailman/listinfo/r-help>
>>    PLEASE do read the posting guide
>>    http://www.R-project.org/posting-guide.html
>>    <http://www.R-project.org/posting-guide.html>
>>    and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From kry|ov@r00t @end|ng |rom gm@||@com  Thu Nov  2 12:27:12 2023
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Thu, 2 Nov 2023 14:27:12 +0300
Subject: [R] Bug in print for data frames?
In-Reply-To: <a52edbd7-3291-4c99-b42e-5ff9aeaefdc9@app.fastmail.com>
References: <a52edbd7-3291-4c99-b42e-5ff9aeaefdc9@app.fastmail.com>
Message-ID: <20231102142712.16321aed@arachnoid>

? Wed, 25 Oct 2023 09:18:26 +0300
"Christian Asseburg" <rhelp at moin.fi> ?????:

> > str(x)  
> 'data.frame':   1 obs. of  3 variables:
>  $ A: num 1
>  $ B: num 1
>  $ C:'data.frame':      1 obs. of  1 variable:
>   ..$ A: num 1
> 
> Why does the print(x) not show "C" as the name of the third element?

Interesting problem.

print.data.frame() calls format.data.frame() to prepare its argument
for printing, which in turn calls as.data.frame.list() to reconstruct a
data.frame from the formatted arguments, which in turn uses
data.frame() to actually construct the object.

data.frame() is able to return combined column names, but only if the
inner data.frame has more than one column:

names(data.frame(A = 1:3, B = data.frame(C = 4:6, D = 7:9)))
# [1] "A"   "B.C" "B.D"
names(data.frame(A = 1:3, B = data.frame(C = 4:6)))
# [1] "A" "C"

This matches the behaviour documented in ?data.frame:

>> For a named or unnamed matrix/list/data frame argument that contains
>> a single column, the column name in the result is the column name in
>> the argument.

Still, changing the presentational code like print.data.frame() or
format.data.frame() could be safe. I've tried writing a patch for
format.data.frame(), but it looks clumsy and breaks regression tests
(that do actually check capture.output()):

--- src/library/base/R/format.R (revision 85459)
+++ src/library/base/R/format.R (working copy)
@@ -243,8 +243,16 @@
     if(!nc) return(x) # 0 columns: evade problems, notably for nrow() > 0
     nr <- .row_names_info(x, 2L)
     rval <- vector("list", nc)
-    for(i in seq_len(nc))
+    for(i in seq_len(nc)) {
        rval[[i]] <- format(x[[i]], ..., justify = justify)
+       # avoid data.frame(foo = data.frame(bar = ...)) overwriting
+       # the single column name
+       if (
+           identical(ncol(rval[[i]]), 1L) &&
+           !is.null(colnames(rval[[i]])) &&
+           colnames(rval[[i]]) != ''
+       ) colnames(rval[[i]]) <- paste(names(x)[[i]], colnames(rval[[i]]), sep = '.')
+    }
     lens <- vapply(rval, NROW, 1)
     if(any(lens != nr)) { # corrupt data frame, must have at least one column
        warning("corrupt data frame: columns will be truncated or
        padded with NAs")

Is it worth changing the behaviour of {print,format}.data.frame() (and
fixing the regression tests to accept the new behaviour), or would that
break too much?

-- 
Best regards,
Ivan


From ro@||n@ump @end|ng |rom gm@||@com  Fri Nov  3 00:23:49 2023
From: ro@||n@ump @end|ng |rom gm@||@com (roslinazairimah zakaria)
Date: Fri, 3 Nov 2023 07:23:49 +0800
Subject: [R] Sum data according to date in sequence
Message-ID: <CANTvJZL-jS=mWPWSJXqWVvxZfEVcvHPNhqQu4O9eEQj7ru8qRw@mail.gmail.com>

Dear all,

I have this set of data. I would like to sum the EnergykWh according date
sequences.

> head(dt1,20)                   StationName      date  time EnergykWh
1  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 12:09  4.680496
2  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 19:50  6.272414
3  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 20:22  1.032782
4  PALO ALTO CA / CAMBRIDGE #1 1/15/2016  8:25 11.004884
5  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 14:23 10.096824
6  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 18:17  6.658797
7  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 21:46  4.808874
8  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 10:19  1.469384
9  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 12:12  2.996239
10 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 14:12  0.303222
11 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 16:22  4.988339
12 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:16  8.131804
13 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:19  0.117156
14 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 20:24  3.285669
15 PALO ALTO CA / CAMBRIDGE #1 1/17/2016  9:54  1.175608
16 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 12:16  3.677487
17 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 13:53  1.068393
18 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 19:03  8.820755
19 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 22:00  8.138583
20 PALO ALTO CA / CAMBRIDGE #1 1/18/2016  8:58  9.057500

I have tried this:
library(dplyr)
sums <- dt1 %>%
  group_by(date) %>%
  summarise(EnergykWh = sum(EnergykWh))

head(sums,20)

The date is not by daily sequence but by year sequence.

> head(sums,20)# A tibble: 20 ? 2
   date      EnergykWh
   <chr>         <dbl> 1 1/1/2017     25.3   2 1/1/2018     61.0   3
1/1/2019      0.627 4 1/1/2020     10.7   5 1/10/2017    69.4   6
1/10/2018    54.5   7 1/10/2019    49.1   8 1/10/2020    45.9   9
1/11/2017    73.9  10 1/11/2018    53.3  11 1/11/2019    93.5  12
1/11/2020    66.7  13 1/12/2017    78.6  14 1/12/2018    42.2  15
1/12/2019    22.7  16 1/12/2020    80.9  17 1/13/2017    85.6  18
1/13/2018    46.4  19 1/13/2019    40.0  20 1/13/2020   121.



Thank you very much for any help given.


-- 
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From cry@n @end|ng |rom b|ngh@mton@edu  Fri Nov  3 01:08:40 2023
From: cry@n @end|ng |rom b|ngh@mton@edu (Christopher W. Ryan)
Date: Thu, 2 Nov 2023 20:08:40 -0400
Subject: [R] Sum data according to date in sequence
In-Reply-To: <CANTvJZL-jS=mWPWSJXqWVvxZfEVcvHPNhqQu4O9eEQj7ru8qRw@mail.gmail.com>
References: <CANTvJZL-jS=mWPWSJXqWVvxZfEVcvHPNhqQu4O9eEQj7ru8qRw@mail.gmail.com>
Message-ID: <38d92f12-49cb-752e-83c2-12db9fe41bc7@binghamton.edu>

date appears to be a character variable, and R is treating it as such.

str(dt1)

might give you some insight.  Or the dplyr equivalent

glimpse(dt1)

I think R did what you asked, but if you want to be able to order
records by date, in temporal order, you need to tell R that it is a date:

library(dplyr)
sums <- dt1 %>%
    mutate(realDate = as.Date(date, format = "%m/%d/%Y") %>%
    group_by(realDate) %>%
    summarise(EnergykWh = sum(EnergykWh))


--Chris Ryan


roslinazairimah zakaria wrote:
> Dear all,
> 
> I have this set of data. I would like to sum the EnergykWh according date
> sequences.
> 
>> head(dt1,20)                   StationName      date  time EnergykWh
> 1  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 12:09  4.680496
> 2  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 19:50  6.272414
> 3  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 20:22  1.032782
> 4  PALO ALTO CA / CAMBRIDGE #1 1/15/2016  8:25 11.004884
> 5  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 14:23 10.096824
> 6  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 18:17  6.658797
> 7  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 21:46  4.808874
> 8  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 10:19  1.469384
> 9  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 12:12  2.996239
> 10 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 14:12  0.303222
> 11 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 16:22  4.988339
> 12 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:16  8.131804
> 13 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:19  0.117156
> 14 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 20:24  3.285669
> 15 PALO ALTO CA / CAMBRIDGE #1 1/17/2016  9:54  1.175608
> 16 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 12:16  3.677487
> 17 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 13:53  1.068393
> 18 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 19:03  8.820755
> 19 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 22:00  8.138583
> 20 PALO ALTO CA / CAMBRIDGE #1 1/18/2016  8:58  9.057500
> 
> I have tried this:
> library(dplyr)
> sums <- dt1 %>%
>   group_by(date) %>%
>   summarise(EnergykWh = sum(EnergykWh))
> 
> head(sums,20)
> 
> The date is not by daily sequence but by year sequence.
> 
>> head(sums,20)# A tibble: 20 ? 2
>    date      EnergykWh
>    <chr>         <dbl> 1 1/1/2017     25.3   2 1/1/2018     61.0   3
> 1/1/2019      0.627 4 1/1/2020     10.7   5 1/10/2017    69.4   6
> 1/10/2018    54.5   7 1/10/2019    49.1   8 1/10/2020    45.9   9
> 1/11/2017    73.9  10 1/11/2018    53.3  11 1/11/2019    93.5  12
> 1/11/2020    66.7  13 1/12/2017    78.6  14 1/12/2018    42.2  15
> 1/12/2019    22.7  16 1/12/2020    80.9  17 1/13/2017    85.6  18
> 1/13/2018    46.4  19 1/13/2019    40.0  20 1/13/2020   121.
> 
> 
> 
> Thank you very much for any help given.
> 
>


From jho|tm@n @end|ng |rom gm@||@com  Fri Nov  3 01:10:08 2023
From: jho|tm@n @end|ng |rom gm@||@com (jim holtman)
Date: Thu, 2 Nov 2023 17:10:08 -0700
Subject: [R] Sum data according to date in sequence
In-Reply-To: <CANTvJZL-jS=mWPWSJXqWVvxZfEVcvHPNhqQu4O9eEQj7ru8qRw@mail.gmail.com>
References: <CANTvJZL-jS=mWPWSJXqWVvxZfEVcvHPNhqQu4O9eEQj7ru8qRw@mail.gmail.com>
Message-ID: <CAAxdm-74kKpsFMG1qUJmNxV8hA2LyDyjCjyVwiKz3KqvuUQgNQ@mail.gmail.com>

How about send a 'dput' of some sample data.  My guess is that your date is
'character' and not 'Date'.

Thanks

Jim Holtman
*Data Munger Guru*


*What is the problem that you are trying to solve?Tell me what you want to
do, not how you want to do it.*


On Thu, Nov 2, 2023 at 4:24?PM roslinazairimah zakaria <roslinaump at gmail.com>
wrote:

> Dear all,
>
> I have this set of data. I would like to sum the EnergykWh according date
> sequences.
>
> > head(dt1,20)                   StationName      date  time EnergykWh
> 1  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 12:09  4.680496
> 2  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 19:50  6.272414
> 3  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 20:22  1.032782
> 4  PALO ALTO CA / CAMBRIDGE #1 1/15/2016  8:25 11.004884
> 5  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 14:23 10.096824
> 6  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 18:17  6.658797
> 7  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 21:46  4.808874
> 8  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 10:19  1.469384
> 9  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 12:12  2.996239
> 10 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 14:12  0.303222
> 11 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 16:22  4.988339
> 12 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:16  8.131804
> 13 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:19  0.117156
> 14 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 20:24  3.285669
> 15 PALO ALTO CA / CAMBRIDGE #1 1/17/2016  9:54  1.175608
> 16 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 12:16  3.677487
> 17 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 13:53  1.068393
> 18 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 19:03  8.820755
> 19 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 22:00  8.138583
> 20 PALO ALTO CA / CAMBRIDGE #1 1/18/2016  8:58  9.057500
>
> I have tried this:
> library(dplyr)
> sums <- dt1 %>%
>   group_by(date) %>%
>   summarise(EnergykWh = sum(EnergykWh))
>
> head(sums,20)
>
> The date is not by daily sequence but by year sequence.
>
> > head(sums,20)# A tibble: 20 ? 2
>    date      EnergykWh
>    <chr>         <dbl> 1 1/1/2017     25.3   2 1/1/2018     61.0   3
> 1/1/2019      0.627 4 1/1/2020     10.7   5 1/10/2017    69.4   6
> 1/10/2018    54.5   7 1/10/2019    49.1   8 1/10/2020    45.9   9
> 1/11/2017    73.9  10 1/11/2018    53.3  11 1/11/2019    93.5  12
> 1/11/2020    66.7  13 1/12/2017    78.6  14 1/12/2018    42.2  15
> 1/12/2019    22.7  16 1/12/2020    80.9  17 1/13/2017    85.6  18
> 1/13/2018    46.4  19 1/13/2019    40.0  20 1/13/2020   121.
>
>
>
> Thank you very much for any help given.
>
>
> --
> *Roslinazairimah Zakaria*
> *Tel: +609-5492370; Fax. No.+609-5492766*
>
> *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
> roslinaump at gmail.com <roslinaump at gmail.com>*
> Faculty of Industrial Sciences & Technology
> University Malaysia Pahang
> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ro@||n@ump @end|ng |rom gm@||@com  Fri Nov  3 02:49:57 2023
From: ro@||n@ump @end|ng |rom gm@||@com (roslinazairimah zakaria)
Date: Fri, 3 Nov 2023 09:49:57 +0800
Subject: [R] Sum data according to date in sequence
In-Reply-To: <CAAxdm-74kKpsFMG1qUJmNxV8hA2LyDyjCjyVwiKz3KqvuUQgNQ@mail.gmail.com>
References: <CANTvJZL-jS=mWPWSJXqWVvxZfEVcvHPNhqQu4O9eEQj7ru8qRw@mail.gmail.com>
 <CAAxdm-74kKpsFMG1qUJmNxV8hA2LyDyjCjyVwiKz3KqvuUQgNQ@mail.gmail.com>
Message-ID: <CANTvJZLZDkdT68sU0a-E35v_vAUtDvWWUcMV0x840XSM1KQDvA@mail.gmail.com>

Hi all,

This is the data:

> dput(head(dt1,20))structure(list(StationName = c("PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1"), date = c("1/14/2016", "1/14/2016",
"1/14/2016", "1/15/2016", "1/15/2016", "1/15/2016", "1/15/2016",
"1/16/2016", "1/16/2016", "1/16/2016", "1/16/2016", "1/16/2016",
"1/16/2016", "1/16/2016", "1/17/2016", "1/17/2016", "1/17/2016",
"1/17/2016", "1/17/2016", "1/18/2016"), time = c("12:09", "19:50",
"20:22", "8:25", "14:23", "18:17", "21:46", "10:19", "12:12",
"14:12", "16:22", "19:16", "19:19", "20:24", "9:54", "12:16",
"13:53", "19:03", "22:00", "8:58"), EnergykWh = c(4.680496, 6.272414,
1.032782, 11.004884, 10.096824, 6.658797, 4.808874, 1.469384,
2.996239, 0.303222, 4.988339, 8.131804, 0.117156, 3.285669, 1.175608,
3.677487, 1.068393, 8.820755, 8.138583, 9.0575)), row.names = c(NA,
20L), class = "data.frame")


I would like to sum EnergykW data by the date. E.g. all values for
EnergykWh on 1/14/2016


On Fri, Nov 3, 2023 at 8:10?AM jim holtman <jholtman at gmail.com> wrote:

> How about send a 'dput' of some sample data.  My guess is that your date
> is 'character' and not 'Date'.
>
> Thanks
>
> Jim Holtman
> *Data Munger Guru*
>
>
> *What is the problem that you are trying to solve?Tell me what you want to
> do, not how you want to do it.*
>
>
> On Thu, Nov 2, 2023 at 4:24?PM roslinazairimah zakaria <
> roslinaump at gmail.com> wrote:
>
>> Dear all,
>>
>> I have this set of data. I would like to sum the EnergykWh according date
>> sequences.
>>
>> > head(dt1,20)                   StationName      date  time EnergykWh
>> 1  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 12:09  4.680496
>> 2  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 19:50  6.272414
>> 3  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 20:22  1.032782
>> 4  PALO ALTO CA / CAMBRIDGE #1 1/15/2016  8:25 11.004884
>> 5  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 14:23 10.096824
>> 6  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 18:17  6.658797
>> 7  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 21:46  4.808874
>> 8  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 10:19  1.469384
>> 9  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 12:12  2.996239
>> 10 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 14:12  0.303222
>> 11 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 16:22  4.988339
>> 12 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:16  8.131804
>> 13 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:19  0.117156
>> 14 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 20:24  3.285669
>> 15 PALO ALTO CA / CAMBRIDGE #1 1/17/2016  9:54  1.175608
>> 16 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 12:16  3.677487
>> 17 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 13:53  1.068393
>> 18 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 19:03  8.820755
>> 19 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 22:00  8.138583
>> 20 PALO ALTO CA / CAMBRIDGE #1 1/18/2016  8:58  9.057500
>>
>> I have tried this:
>> library(dplyr)
>> sums <- dt1 %>%
>>   group_by(date) %>%
>>   summarise(EnergykWh = sum(EnergykWh))
>>
>> head(sums,20)
>>
>> The date is not by daily sequence but by year sequence.
>>
>> > head(sums,20)# A tibble: 20 ? 2
>>    date      EnergykWh
>>    <chr>         <dbl> 1 1/1/2017     25.3   2 1/1/2018     61.0   3
>> 1/1/2019      0.627 4 1/1/2020     10.7   5 1/10/2017    69.4   6
>> 1/10/2018    54.5   7 1/10/2019    49.1   8 1/10/2020    45.9   9
>> 1/11/2017    73.9  10 1/11/2018    53.3  11 1/11/2019    93.5  12
>> 1/11/2020    66.7  13 1/12/2017    78.6  14 1/12/2018    42.2  15
>> 1/12/2019    22.7  16 1/12/2020    80.9  17 1/13/2017    85.6  18
>> 1/13/2018    46.4  19 1/13/2019    40.0  20 1/13/2020   121.
>>
>>
>>
>> Thank you very much for any help given.
>>
>>
>> --
>> *Roslinazairimah Zakaria*
>> *Tel: +609-5492370; Fax. No.+609-5492766*
>>
>> *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
>> roslinaump at gmail.com <roslinaump at gmail.com>*
>> Faculty of Industrial Sciences & Technology
>> University Malaysia Pahang
>> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

-- 
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From ro@||n@ump @end|ng |rom gm@||@com  Fri Nov  3 04:04:23 2023
From: ro@||n@ump @end|ng |rom gm@||@com (roslinazairimah zakaria)
Date: Fri, 3 Nov 2023 11:04:23 +0800
Subject: [R] Sum data according to date in sequence
In-Reply-To: <CANTvJZL-jS=mWPWSJXqWVvxZfEVcvHPNhqQu4O9eEQj7ru8qRw@mail.gmail.com>
References: <CANTvJZL-jS=mWPWSJXqWVvxZfEVcvHPNhqQu4O9eEQj7ru8qRw@mail.gmail.com>
Message-ID: <CANTvJZL96njE2sQ+VwGYU57N1S4Q2Uz__4zVCVC3=canAPsedg@mail.gmail.com>

Thank you very much for your help. It is very much appreciated.

On Fri, Nov 3, 2023 at 7:23?AM roslinazairimah zakaria <roslinaump at gmail.com>
wrote:

> Dear all,
>
> I have this set of data. I would like to sum the EnergykWh according date
> sequences.
>
> > head(dt1,20)                   StationName      date  time EnergykWh
> 1  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 12:09  4.680496
> 2  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 19:50  6.272414
> 3  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 20:22  1.032782
> 4  PALO ALTO CA / CAMBRIDGE #1 1/15/2016  8:25 11.004884
> 5  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 14:23 10.096824
> 6  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 18:17  6.658797
> 7  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 21:46  4.808874
> 8  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 10:19  1.469384
> 9  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 12:12  2.996239
> 10 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 14:12  0.303222
> 11 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 16:22  4.988339
> 12 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:16  8.131804
> 13 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:19  0.117156
> 14 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 20:24  3.285669
> 15 PALO ALTO CA / CAMBRIDGE #1 1/17/2016  9:54  1.175608
> 16 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 12:16  3.677487
> 17 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 13:53  1.068393
> 18 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 19:03  8.820755
> 19 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 22:00  8.138583
> 20 PALO ALTO CA / CAMBRIDGE #1 1/18/2016  8:58  9.057500
>
> I have tried this:
> library(dplyr)
> sums <- dt1 %>%
>   group_by(date) %>%
>   summarise(EnergykWh = sum(EnergykWh))
>
> head(sums,20)
>
> The date is not by daily sequence but by year sequence.
>
> > head(sums,20)# A tibble: 20 ? 2
>    date      EnergykWh
>    <chr>         <dbl> 1 1/1/2017     25.3   2 1/1/2018     61.0   3 1/1/2019      0.627 4 1/1/2020     10.7   5 1/10/2017    69.4   6 1/10/2018    54.5   7 1/10/2019    49.1   8 1/10/2020    45.9   9 1/11/2017    73.9  10 1/11/2018    53.3  11 1/11/2019    93.5  12 1/11/2020    66.7  13 1/12/2017    78.6  14 1/12/2018    42.2  15 1/12/2019    22.7  16 1/12/2020    80.9  17 1/13/2017    85.6  18 1/13/2018    46.4  19 1/13/2019    40.0  20 1/13/2020   121.
>
>
>
> Thank you very much for any help given.
>
>
> --
> *Roslinazairimah Zakaria*
> *Tel: +609-5492370; Fax. No.+609-5492766*
>
> *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
> roslinaump at gmail.com <roslinaump at gmail.com>*
> Faculty of Industrial Sciences & Technology
> University Malaysia Pahang
> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>


-- 
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From ro@||n@ump @end|ng |rom gm@||@com  Fri Nov  3 10:51:20 2023
From: ro@||n@ump @end|ng |rom gm@||@com (roslinazairimah zakaria)
Date: Fri, 3 Nov 2023 17:51:20 +0800
Subject: [R] Sum data according to date in sequence
In-Reply-To: <CANTvJZL-jS=mWPWSJXqWVvxZfEVcvHPNhqQu4O9eEQj7ru8qRw@mail.gmail.com>
References: <CANTvJZL-jS=mWPWSJXqWVvxZfEVcvHPNhqQu4O9eEQj7ru8qRw@mail.gmail.com>
Message-ID: <CANTvJZ+S2rKDzQt9sZhTb26rS7-+mmZbxo1H6DH7yxM1rNjKZA@mail.gmail.com>

Hi,
I tried this:
# extract date from the time stamp
dt1 <- cbind(as.Date(dt$EndDate, format="%m/%d/%Y"), dt$EnergykWh)
head(dt1)
colnames(dt1) <- c("date", "EnergykWh")
and
my dt1 becomes these, the dates are replace by numbers.

dt1 <- cbind(as.Date(dt$EndDate, format="%m/%d/%Y"), dt$EnergykWh)
dput(head(dt1))
colnames(dt1) <- c("date", "EnergykWh")
dput(head(dt1))


> dput(head(dt1))structure(c(16814, 16814, 16814, 16815, 16815, 16815, 4.680496,
6.272414, 1.032782, 11.004884, 10.096824, 6.658797), dim = c(6L,
2L), dimnames = list(NULL, c("date", "EnergykWh")))

Then I tried this:
library(dplyr)
dt1 %>%
  group_by(date) %>%
  summarise(EnergykWh.sum = sum(EnergykWh))
and got this errors

dt1 %>%+   group_by(date) %>%+   summarise(EnergykWh.sum =
sum(EnergykWh))Error in UseMethod("group_by") :
  no applicable method for 'group_by' applied to an object of class
"c('matrix', 'array', 'double', 'numeric')"



On Fri, Nov 3, 2023 at 7:23?AM roslinazairimah zakaria <roslinaump at gmail.com>
wrote:

> Dear all,
>
> I have this set of data. I would like to sum the EnergykWh according date
> sequences.
>
> > head(dt1,20)                   StationName      date  time EnergykWh
> 1  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 12:09  4.680496
> 2  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 19:50  6.272414
> 3  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 20:22  1.032782
> 4  PALO ALTO CA / CAMBRIDGE #1 1/15/2016  8:25 11.004884
> 5  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 14:23 10.096824
> 6  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 18:17  6.658797
> 7  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 21:46  4.808874
> 8  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 10:19  1.469384
> 9  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 12:12  2.996239
> 10 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 14:12  0.303222
> 11 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 16:22  4.988339
> 12 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:16  8.131804
> 13 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:19  0.117156
> 14 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 20:24  3.285669
> 15 PALO ALTO CA / CAMBRIDGE #1 1/17/2016  9:54  1.175608
> 16 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 12:16  3.677487
> 17 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 13:53  1.068393
> 18 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 19:03  8.820755
> 19 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 22:00  8.138583
> 20 PALO ALTO CA / CAMBRIDGE #1 1/18/2016  8:58  9.057500
>
> I have tried this:
> library(dplyr)
> sums <- dt1 %>%
>   group_by(date) %>%
>   summarise(EnergykWh = sum(EnergykWh))
>
> head(sums,20)
>
> The date is not by daily sequence but by year sequence.
>
> > head(sums,20)# A tibble: 20 ? 2
>    date      EnergykWh
>    <chr>         <dbl> 1 1/1/2017     25.3   2 1/1/2018     61.0   3 1/1/2019      0.627 4 1/1/2020     10.7   5 1/10/2017    69.4   6 1/10/2018    54.5   7 1/10/2019    49.1   8 1/10/2020    45.9   9 1/11/2017    73.9  10 1/11/2018    53.3  11 1/11/2019    93.5  12 1/11/2020    66.7  13 1/12/2017    78.6  14 1/12/2018    42.2  15 1/12/2019    22.7  16 1/12/2020    80.9  17 1/13/2017    85.6  18 1/13/2018    46.4  19 1/13/2019    40.0  20 1/13/2020   121.
>
>
>
> Thank you very much for any help given.
>
>
> --
> *Roslinazairimah Zakaria*
> *Tel: +609-5492370; Fax. No.+609-5492766*
>
> *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
> roslinaump at gmail.com <roslinaump at gmail.com>*
> Faculty of Industrial Sciences & Technology
> University Malaysia Pahang
> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>


-- 
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From mkz@m@n@m @end|ng |rom gm@||@com  Fri Nov  3 01:40:49 2023
From: mkz@m@n@m @end|ng |rom gm@||@com (Md. Kamruzzaman)
Date: Fri, 3 Nov 2023 11:10:49 +1030
Subject: [R] I need to create new variables based on two numeric variables
 and one dichotomize conditional category variables.
Message-ID: <CAGbxoeGjsxZKQ6qijEMq-X-5doqnQQS1jjPDDrGT6hH5xWqOKQ@mail.gmail.com>

Hello Everyone,
I have three variables: Waist circumference (WC), serum triglyceride (TG)
level and gender. Waist circumference and serum triglyceride is numeric and
gender (male and female) is categorical. From these three variables, I want
to calculate the "Lipid Accumulation Product (LAP) Index". The equation to
calculate LAP is different for male and females. I am giving both equations
below.

LAP for male = (WC-65)*TG
LAP for female = (WC-58)*TG

My question is 'how can I calculate the LAP and create a single new column?

Your cooperation will be highly appreciated.

Thanks in advance.

With Regards

*--------------------------------*

*Md Kamruzzaman*

*PhD **Research Fellow (**Medicine**)*
Discipline of Medicine and Centre of Research Excellence in Translating
Nutritional Science to Good Health
Adelaide Medical School | Faculty of Health and Medical Sciences
The University of Adelaide
Adelaide SA 5005

	[[alternative HTML version deleted]]


From pd@|gd @end|ng |rom gm@||@com  Fri Nov  3 13:12:47 2023
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Fri, 3 Nov 2023 13:12:47 +0100
Subject: [R] Bug in print for data frames?
In-Reply-To: <54e89da3-c9fe-4a61-b59b-ec225cb8b1ca@gmail.com>
References: <a52edbd7-3291-4c99-b42e-5ff9aeaefdc9@app.fastmail.com>
 <54e89da3-c9fe-4a61-b59b-ec225cb8b1ca@gmail.com>
Message-ID: <0146352C-8CCC-4AF8-A4DA-EC5B5C7978BE@gmail.com>

It's still kind of weird; embedded 2-column data frames print differently than 1-column ones:

> d <- data.frame(a=1, b=I(data.frame(d=1,e=2)))
> d
  a b.d b.e
1 1   1   2
> str(d)
'data.frame':	1 obs. of  2 variables:
 $ a: num 1
 $ b:Classes 'AsIs' and 'data.frame':	1 obs. of  2 variables:
  ..$ d: num 1
  ..$ e: num 2
> names(d)
[1] "a" "b"
> d <- data.frame(a=1, b=I(data.frame(d=1)))
> d
  a d
1 1 1
> str(d)
'data.frame':	1 obs. of  2 variables:
 $ a: num 1
 $ b:Classes 'AsIs' and 'data.frame':	1 obs. of  1 variable:
  ..$ d: num 1
> names(d)
[1] "a" "b"

It is happening inside format.data.frame() or as.data.frame.list() but I can't figure out the logic at this point.

-pd


> On 26 Oct 2023, at 10:55 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
> On 25/10/2023 2:18 a.m., Christian Asseburg wrote:
>> Hi! I came across this unexpected behaviour in R. First I thought it was a bug in the assignment operator <- but now I think it's maybe a bug in the way data frames are being printed. What do you think?
>> Using R 4.3.1:
>>> x <- data.frame(A = 1, B = 2, C = 3)
>>> y <- data.frame(A = 1)
>>> x
>>   A B C
>> 1 1 2 3
>>> x$B <- y$A # works as expected
>>> x
>>   A B C
>> 1 1 1 3
>>> x$C <- y[1] # makes C disappear
>>> x
>>   A B A
>> 1 1 1 1
>>> str(x)
>> 'data.frame':   1 obs. of  3 variables:
>>  $ A: num 1
>>  $ B: num 1
>>  $ C:'data.frame':      1 obs. of  1 variable:
>>   ..$ A: num 1
>> Why does the print(x) not show "C" as the name of the third element? I did mess up the data frame (and this was a mistake on my part), but finding the bug was harder because print(x) didn't show the C any longer.
> 
> y[1] is a dataframe with one column, i.e. it is identical to y.  To get the result you expected, you should have used y[[1]], to extract column 1.
> 
> Since dataframes are lists, you can assign them as columns of other dataframes, and you'll create a single column in the result whose rows are the columns of the dataframe you're assigning.  This means that
> 
> x$C <- y[1]
> 
> replaces the C column of x with a dataframe.  It retains the name C (you can see this if you print names(x) ), but since the column contains a dataframe, it chooses to use the column name of y when printing.
> 
> If you try
> 
> x$D <- x
> 
> you'll see it generate new names when printing, but the names within x remain as A, B, C, D.
> 
> This is a situation where tibbles do a better job than dataframes:  if you created x and y as tibbles instead of dataframes and executed your code, you'd see this:
> 
>  library(tibble)
>  x <- tibble(A = 1, B = 2, C = 3)
>  y <- tibble(A = 1)
>  x$C <- y[1]
>  x
>  #> # A tibble: 1 ? 3
>  #>       A     B   C$A
>  #>   <dbl> <dbl> <dbl>
>  #> 1     1     2     1
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From j|ox @end|ng |rom mcm@@ter@c@  Fri Nov  3 13:44:34 2023
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Fri, 3 Nov 2023 08:44:34 -0400
Subject: [R] [R-pkgs] new cv package: cross-validation of regression models
Message-ID: <e64f1971-9441-4779-88ed-153712cfbac3@mcmaster.ca>

Georges Monette and I would like to announce a new package, cv, now on 
CRAN, which implements cross-validation of regression models.

Some of the functions supplied by the package:

-   cv() is a generic function with a default method and computationally 
efficient "lm" and "glm" methods, along with a method for a list of 
competing models. There are also experimental "merMod", "lme", and 
"glmmTMB" methods for mixed-effects models. cv() supports parallel 
computations.

-   mse() (mean-squared error) and BayesRule() are cross-validation 
criteria ("cost functions"), suitable for use with cv().

-   cvSelect() cross-validates a selection procedure for a regression 
model. cvSelect() also supports parallel computations.

-   selectStepAIC() is a model-selection procedure, suitable for use 
with cvSelect(), based on the stepAIC() function in the MASS package.

-   selectTrans() is a procedure for selecting predictor and response 
transformations in regression, also suitable for use with cvSelect(), 
based on the powerTransform() function in the car package.

For additional information on using the cv package, see the 
"Cross-validation of regression models" vignette, in the package and at 
<https://cran.r-project.org/web/packages/cv/vignettes/cv.html>. The cv 
package is designed to be extensible to other classes of regression 
models and other model-selection procedures; for details, see the 
"Extending the cv package" vignette, also in the package and at 
<https://cran.r-project.org/web/packages/cv/vignettes/cv-extend.html>.

Comments and suggestions would be appreciated. Bug reports and problems 
can be filed at <https://github.com/gmonette/cv/issues>.

Thank you for your attention,
  John and Georges


-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://www.john-fox.ca/

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From bgunter@4567 @end|ng |rom gm@||@com  Fri Nov  3 15:43:35 2023
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 3 Nov 2023 07:43:35 -0700
Subject: [R] 
 I need to create new variables based on two numeric variables
 and one dichotomize conditional category variables.
In-Reply-To: <CAGbxoeGjsxZKQ6qijEMq-X-5doqnQQS1jjPDDrGT6hH5xWqOKQ@mail.gmail.com>
References: <CAGbxoeGjsxZKQ6qijEMq-X-5doqnQQS1jjPDDrGT6hH5xWqOKQ@mail.gmail.com>
Message-ID: <CAGxFJbS41wwYMDxRDE6s=WNEVchyAQv4uppMge2dFRdMnraCWQ@mail.gmail.com>

Well, something like:

LAP <- ifelse(gender =='male', (WC-65)*TG, (WC-58)*TG)

The exact code depends on whether your variables are in a data frame or
list or whatever, which you failed to specify. If so, ?with  may be useful.

Cheers,
Bert



On Fri, Nov 3, 2023 at 3:43?AM Md. Kamruzzaman <mkzaman.m at gmail.com> wrote:

> Hello Everyone,
> I have three variables: Waist circumference (WC), serum triglyceride (TG)
> level and gender. Waist circumference and serum triglyceride is numeric and
> gender (male and female) is categorical. From these three variables, I want
> to calculate the "Lipid Accumulation Product (LAP) Index". The equation to
> calculate LAP is different for male and females. I am giving both equations
> below.
>
> LAP for male = (WC-65)*TG
> LAP for female = (WC-58)*TG
>
> My question is 'how can I calculate the LAP and create a single new column?
>
> Your cooperation will be highly appreciated.
>
> Thanks in advance.
>
> With Regards
>
> *--------------------------------*
>
> *Md Kamruzzaman*
>
> *PhD **Research Fellow (**Medicine**)*
> Discipline of Medicine and Centre of Research Excellence in Translating
> Nutritional Science to Good Health
> Adelaide Medical School | Faculty of Health and Medical Sciences
> The University of Adelaide
> Adelaide SA 5005
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri Nov  3 16:15:59 2023
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 03 Nov 2023 08:15:59 -0700
Subject: [R] Sum data according to date in sequence
In-Reply-To: <CANTvJZ+S2rKDzQt9sZhTb26rS7-+mmZbxo1H6DH7yxM1rNjKZA@mail.gmail.com>
References: <CANTvJZL-jS=mWPWSJXqWVvxZfEVcvHPNhqQu4O9eEQj7ru8qRw@mail.gmail.com>
 <CANTvJZ+S2rKDzQt9sZhTb26rS7-+mmZbxo1H6DH7yxM1rNjKZA@mail.gmail.com>
Message-ID: <0735CADE-B135-4CD2-BEF8-AD0EF1A4997C@dcn.davis.ca.us>

Cbind is not a very good tool for adding columns to a data frame. Either use explicit column referencing like

dt1$x <- new_data_vector

but you do have to make sure the new data vector has the same number of values and in the same order as the other data in dt1. The transition from multiple records per day to one record per day would make the starting data and ending data incompatible.

library(dplyr)

dt1 <- structure(list(StationName = c("PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
"PALO ALTO CA / CAMBRIDGE #1"), date = c("1/14/2016", "1/14/2016",
"1/14/2016", "1/15/2016", "1/15/2016", "1/15/2016", "1/15/2016",
"1/16/2016", "1/16/2016", "1/16/2016", "1/16/2016", "1/16/2016",
"1/16/2016", "1/16/2016", "1/17/2016", "1/17/2016", "1/17/2016",
"1/17/2016", "1/17/2016", "1/18/2016"), time = c("12:09", "19:50",
"20:22", "8:25", "14:23", "18:17", "21:46", "10:19", "12:12",
"14:12", "16:22", "19:16", "19:19", "20:24", "9:54", "12:16",
"13:53", "19:03", "22:00", "8:58"), EnergykWh = c(4.680496, 6.272414,
1.032782, 11.004884, 10.096824, 6.658797, 4.808874, 1.469384,
2.996239, 0.303222, 4.988339, 8.131804, 0.117156, 3.285669, 1.175608,
3.677487, 1.068393, 8.820755, 8.138583, 9.0575)), row.names = c(NA,
20L), class = "data.frame")

ans <- (
  dt1
  %>% mutate(
    Dt = as.Date( date, format = "%m/%d/%Y" )
  )
  %>% group_by( StationName, Dt )
  %>% summarize( DailyEnergykWh = sum( EnergykWh ) )
)


On November 3, 2023 2:51:20 AM PDT, roslinazairimah zakaria <roslinaump at gmail.com> wrote:
>Hi,
>I tried this:
># extract date from the time stamp
>dt1 <- cbind(as.Date(dt$EndDate, format="%m/%d/%Y"), dt$EnergykWh)
>head(dt1)
>colnames(dt1) <- c("date", "EnergykWh")
>and
>my dt1 becomes these, the dates are replace by numbers.
>
>dt1 <- cbind(as.Date(dt$EndDate, format="%m/%d/%Y"), dt$EnergykWh)
>dput(head(dt1))
>colnames(dt1) <- c("date", "EnergykWh")
>dput(head(dt1))
>
>
>> dput(head(dt1))structure(c(16814, 16814, 16814, 16815, 16815, 16815, 4.680496,
>6.272414, 1.032782, 11.004884, 10.096824, 6.658797), dim = c(6L,
>2L), dimnames = list(NULL, c("date", "EnergykWh")))
>
>Then I tried this:
>library(dplyr)
>dt1 %>%
>  group_by(date) %>%
>  summarise(EnergykWh.sum = sum(EnergykWh))
>and got this errors
>
>dt1 %>%+   group_by(date) %>%+   summarise(EnergykWh.sum =
>sum(EnergykWh))Error in UseMethod("group_by") :
>  no applicable method for 'group_by' applied to an object of class
>"c('matrix', 'array', 'double', 'numeric')"
>
>
>
>On Fri, Nov 3, 2023 at 7:23?AM roslinazairimah zakaria <roslinaump at gmail.com>
>wrote:
>
>> Dear all,
>>
>> I have this set of data. I would like to sum the EnergykWh according date
>> sequences.
>>
>> > head(dt1,20)                   StationName      date  time EnergykWh
>> 1  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 12:09  4.680496
>> 2  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 19:50  6.272414
>> 3  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 20:22  1.032782
>> 4  PALO ALTO CA / CAMBRIDGE #1 1/15/2016  8:25 11.004884
>> 5  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 14:23 10.096824
>> 6  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 18:17  6.658797
>> 7  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 21:46  4.808874
>> 8  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 10:19  1.469384
>> 9  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 12:12  2.996239
>> 10 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 14:12  0.303222
>> 11 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 16:22  4.988339
>> 12 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:16  8.131804
>> 13 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:19  0.117156
>> 14 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 20:24  3.285669
>> 15 PALO ALTO CA / CAMBRIDGE #1 1/17/2016  9:54  1.175608
>> 16 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 12:16  3.677487
>> 17 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 13:53  1.068393
>> 18 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 19:03  8.820755
>> 19 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 22:00  8.138583
>> 20 PALO ALTO CA / CAMBRIDGE #1 1/18/2016  8:58  9.057500
>>
>> I have tried this:
>> library(dplyr)
>> sums <- dt1 %>%
>>   group_by(date) %>%
>>   summarise(EnergykWh = sum(EnergykWh))
>>
>> head(sums,20)
>>
>> The date is not by daily sequence but by year sequence.
>>
>> > head(sums,20)# A tibble: 20 ? 2
>>    date      EnergykWh
>>    <chr>         <dbl> 1 1/1/2017     25.3   2 1/1/2018     61.0   3 1/1/2019      0.627 4 1/1/2020     10.7   5 1/10/2017    69.4   6 1/10/2018    54.5   7 1/10/2019    49.1   8 1/10/2020    45.9   9 1/11/2017    73.9  10 1/11/2018    53.3  11 1/11/2019    93.5  12 1/11/2020    66.7  13 1/12/2017    78.6  14 1/12/2018    42.2  15 1/12/2019    22.7  16 1/12/2020    80.9  17 1/13/2017    85.6  18 1/13/2018    46.4  19 1/13/2019    40.0  20 1/13/2020   121.
>>
>>
>>
>> Thank you very much for any help given.
>>
>>
>> --
>> *Roslinazairimah Zakaria*
>> *Tel: +609-5492370; Fax. No.+609-5492766*
>>
>> *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
>> roslinaump at gmail.com <roslinaump at gmail.com>*
>> Faculty of Industrial Sciences & Technology
>> University Malaysia Pahang
>> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>>
>
>

-- 
Sent from my phone. Please excuse my brevity.


From JH@rm@e @end|ng |rom roku@com  Fri Nov  3 16:55:57 2023
From: JH@rm@e @end|ng |rom roku@com (Jorgen Harmse)
Date: Fri, 3 Nov 2023 15:55:57 +0000
Subject: [R] 
 I need to create new variables based on two numeric variables
 and one dichotomize conditional category variables.
In-Reply-To: <mailman.370481.1.1699009201.56788.r-help@r-project.org>
References: <mailman.370481.1.1699009201.56788.r-help@r-project.org>
Message-ID: <LV3PR01MB86736581F1E69742E545FD59DCA5A@LV3PR01MB8673.prod.exchangelabs.com>

df$LAP <- with(df, ifelse(G=='male', (WC-65)*TG, (WC-58)*TG))

That will do both calculations and merge the two vectors appropriately. It will use extra memory, but it should be much faster than a 'for' loop.

Regards,
Jorgen Harmse.

------------------------------

Message: 8
Date: Fri, 3 Nov 2023 11:10:49 +1030
From: "Md. Kamruzzaman" <mkzaman.m at gmail.com>
To: r-help at r-project.org
Subject: [R] I need to create new variables based on two numeric
        variables and one dichotomize conditional category variables.
Message-ID:
        <CAGbxoeGjsxZKQ6qijEMq-X-5doqnQQS1jjPDDrGT6hH5xWqOKQ at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

Hello Everyone,
I have three variables: Waist circumference (WC), serum triglyceride (TG)
level and gender. Waist circumference and serum triglyceride is numeric and
gender (male and female) is categorical. From these three variables, I want
to calculate the "Lipid Accumulation Product (LAP) Index". The equation to
calculate LAP is different for male and females. I am giving both equations
below.

LAP for male = (WC-65)*TG
LAP for female = (WC-58)*TG

My question is 'how can I calculate the LAP and create a single new column?

Your cooperation will be highly appreciated.

Thanks in advance.

With Regards

*--------------------------------*

*Md Kamruzzaman*

*PhD **Research Fellow (**Medicine**)*
Discipline of Medicine and Centre of Research Excellence in Translating
Nutritional Science to Good Health
Adelaide Medical School | Faculty of Health and Medical Sciences
The University of Adelaide
Adelaide SA 5005

        [[alternative HTML version deleted]]



	[[alternative HTML version deleted]]


From jho|tm@n @end|ng |rom gm@||@com  Fri Nov  3 16:58:18 2023
From: jho|tm@n @end|ng |rom gm@||@com (jim holtman)
Date: Fri, 3 Nov 2023 08:58:18 -0700
Subject: [R] Sum data according to date in sequence
In-Reply-To: <CANTvJZ+S2rKDzQt9sZhTb26rS7-+mmZbxo1H6DH7yxM1rNjKZA@mail.gmail.com>
References: <CANTvJZL-jS=mWPWSJXqWVvxZfEVcvHPNhqQu4O9eEQj7ru8qRw@mail.gmail.com>
 <CANTvJZ+S2rKDzQt9sZhTb26rS7-+mmZbxo1H6DH7yxM1rNjKZA@mail.gmail.com>
Message-ID: <CAAxdm-7mHhR32n-dUxSK7d2pk9o2Frq1Cj_AorW2Q6C5PTbedA@mail.gmail.com>

Is this what you are after?

library(tidyverse)


library(lubridate)

input <- structure(list(StationName = c("PALO ALTO CA / CAMBRIDGE #1",
  "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
  "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
  "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
  "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
  "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
  "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
  "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
  "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
  "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
  "PALO ALTO CA / CAMBRIDGE #1"), date = c("1/14/2016", "1/14/2016",
   "1/14/2016", "1/15/2016", "1/15/2016", "1/15/2016", "1/15/2016",
   "1/16/2016", "1/16/2016", "1/16/2016", "1/16/2016", "1/16/2016",
   "1/16/2016", "1/16/2016", "1/17/2016", "1/17/2016", "1/17/2016",
   "1/17/2016", "1/17/2016", "1/18/2016"), time = c("12:09", "19:50",
  "20:22", "8:25", "14:23", "18:17", "21:46", "10:19", "12:12",
  "14:12", "16:22", "19:16", "19:19", "20:24", "9:54", "12:16",
  "13:53", "19:03", "22:00", "8:58"),
  EnergykWh = c(4.680496, 6.272414,
  1.032782, 11.004884, 10.096824, 6.658797, 4.808874, 1.469384,
  2.996239, 0.303222, 4.988339, 8.131804, 0.117156, 3.285669, 1.175608,
  3.677487, 1.068393, 8.820755, 8.138583, 9.0575)),
  row.names = c(NA, 20L), class = "data.frame")
# convert date from character to Date
byDate <- input |>
  mutate(newdate = mdy(date)) |>
  group_by(newdate) |>
  summarise(total = sum(EnergykWh))

byDate

## # A tibble: 5 ? 2
##   newdate    total
##   <date>     <dbl>
## 1 2016-01-14 12.0
## 2 2016-01-15 32.6
## 3 2016-01-16 21.3
## 4 2016-01-17 22.9
## 5 2016-01-18  9.06


Thanks

Jim Holtman
*Data Munger Guru*


*What is the problem that you are trying to solve?Tell me what you want to
do, not how you want to do it.*


On Fri, Nov 3, 2023 at 2:51?AM roslinazairimah zakaria <roslinaump at gmail.com>
wrote:

> Hi,
> I tried this:
> # extract date from the time stamp
> dt1 <- cbind(as.Date(dt$EndDate, format="%m/%d/%Y"), dt$EnergykWh)
> head(dt1)
> colnames(dt1) <- c("date", "EnergykWh")
> and
> my dt1 becomes these, the dates are replace by numbers.
>
> dt1 <- cbind(as.Date(dt$EndDate, format="%m/%d/%Y"), dt$EnergykWh)
> dput(head(dt1))
> colnames(dt1) <- c("date", "EnergykWh")
> dput(head(dt1))
>
>
> > dput(head(dt1))structure(c(16814, 16814, 16814, 16815, 16815, 16815,
> 4.680496,
> 6.272414, 1.032782, 11.004884, 10.096824, 6.658797), dim = c(6L,
> 2L), dimnames = list(NULL, c("date", "EnergykWh")))
>
> Then I tried this:
> library(dplyr)
> dt1 %>%
>   group_by(date) %>%
>   summarise(EnergykWh.sum = sum(EnergykWh))
> and got this errors
>
> dt1 %>%+   group_by(date) %>%+   summarise(EnergykWh.sum =
> sum(EnergykWh))Error in UseMethod("group_by") :
>   no applicable method for 'group_by' applied to an object of class
> "c('matrix', 'array', 'double', 'numeric')"
>
>
>
> On Fri, Nov 3, 2023 at 7:23?AM roslinazairimah zakaria <
> roslinaump at gmail.com>
> wrote:
>
> > Dear all,
> >
> > I have this set of data. I would like to sum the EnergykWh according date
> > sequences.
> >
> > > head(dt1,20)                   StationName      date  time EnergykWh
> > 1  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 12:09  4.680496
> > 2  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 19:50  6.272414
> > 3  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 20:22  1.032782
> > 4  PALO ALTO CA / CAMBRIDGE #1 1/15/2016  8:25 11.004884
> > 5  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 14:23 10.096824
> > 6  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 18:17  6.658797
> > 7  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 21:46  4.808874
> > 8  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 10:19  1.469384
> > 9  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 12:12  2.996239
> > 10 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 14:12  0.303222
> > 11 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 16:22  4.988339
> > 12 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:16  8.131804
> > 13 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:19  0.117156
> > 14 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 20:24  3.285669
> > 15 PALO ALTO CA / CAMBRIDGE #1 1/17/2016  9:54  1.175608
> > 16 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 12:16  3.677487
> > 17 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 13:53  1.068393
> > 18 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 19:03  8.820755
> > 19 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 22:00  8.138583
> > 20 PALO ALTO CA / CAMBRIDGE #1 1/18/2016  8:58  9.057500
> >
> > I have tried this:
> > library(dplyr)
> > sums <- dt1 %>%
> >   group_by(date) %>%
> >   summarise(EnergykWh = sum(EnergykWh))
> >
> > head(sums,20)
> >
> > The date is not by daily sequence but by year sequence.
> >
> > > head(sums,20)# A tibble: 20 ? 2
> >    date      EnergykWh
> >    <chr>         <dbl> 1 1/1/2017     25.3   2 1/1/2018     61.0   3
> 1/1/2019      0.627 4 1/1/2020     10.7   5 1/10/2017    69.4   6
> 1/10/2018    54.5   7 1/10/2019    49.1   8 1/10/2020    45.9   9
> 1/11/2017    73.9  10 1/11/2018    53.3  11 1/11/2019    93.5  12
> 1/11/2020    66.7  13 1/12/2017    78.6  14 1/12/2018    42.2  15
> 1/12/2019    22.7  16 1/12/2020    80.9  17 1/13/2017    85.6  18
> 1/13/2018    46.4  19 1/13/2019    40.0  20 1/13/2020   121.
> >
> >
> >
> > Thank you very much for any help given.
> >
> >
> > --
> > *Roslinazairimah Zakaria*
> > *Tel: +609-5492370; Fax. No.+609-5492766*
> >
> > *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
> > roslinaump at gmail.com <roslinaump at gmail.com>*
> > Faculty of Industrial Sciences & Technology
> > University Malaysia Pahang
> > Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
> >
>
>
> --
> *Roslinazairimah Zakaria*
> *Tel: +609-5492370; Fax. No.+609-5492766*
>
> *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
> roslinaump at gmail.com <roslinaump at gmail.com>*
> Faculty of Industrial Sciences & Technology
> University Malaysia Pahang
> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ro@||n@ump @end|ng |rom gm@||@com  Fri Nov  3 21:21:40 2023
From: ro@||n@ump @end|ng |rom gm@||@com (roslinazairimah zakaria)
Date: Sat, 4 Nov 2023 04:21:40 +0800
Subject: [R] Sum data according to date in sequence
In-Reply-To: <CAAxdm-7mHhR32n-dUxSK7d2pk9o2Frq1Cj_AorW2Q6C5PTbedA@mail.gmail.com>
References: <CANTvJZL-jS=mWPWSJXqWVvxZfEVcvHPNhqQu4O9eEQj7ru8qRw@mail.gmail.com>
 <CANTvJZ+S2rKDzQt9sZhTb26rS7-+mmZbxo1H6DH7yxM1rNjKZA@mail.gmail.com>
 <CAAxdm-7mHhR32n-dUxSK7d2pk9o2Frq1Cj_AorW2Q6C5PTbedA@mail.gmail.com>
Message-ID: <CANTvJZK6ze4ghswTxtk1btUfE1JOSOoKA+48dy492oL6iaeZ_g@mail.gmail.com>

Hi Jim,

Yes, that is exactly what I am trying to do. Once I get that, I want to
plot time series data.
Thank you very much Jim.

On Fri, Nov 3, 2023 at 11:58?PM jim holtman <jholtman at gmail.com> wrote:

> Is this what you are after?
>
> library(tidyverse)
>
>
> library(lubridate)
>
> input <- structure(list(StationName = c("PALO ALTO CA / CAMBRIDGE #1",
>   "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
>   "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
>   "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
>   "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
>   "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
>   "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
>   "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
>   "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
>   "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
>   "PALO ALTO CA / CAMBRIDGE #1"), date = c("1/14/2016", "1/14/2016",
>    "1/14/2016", "1/15/2016", "1/15/2016", "1/15/2016", "1/15/2016",
>    "1/16/2016", "1/16/2016", "1/16/2016", "1/16/2016", "1/16/2016",
>    "1/16/2016", "1/16/2016", "1/17/2016", "1/17/2016", "1/17/2016",
>    "1/17/2016", "1/17/2016", "1/18/2016"), time = c("12:09", "19:50",
>   "20:22", "8:25", "14:23", "18:17", "21:46", "10:19", "12:12",
>   "14:12", "16:22", "19:16", "19:19", "20:24", "9:54", "12:16",
>   "13:53", "19:03", "22:00", "8:58"),
>   EnergykWh = c(4.680496, 6.272414,
>   1.032782, 11.004884, 10.096824, 6.658797, 4.808874, 1.469384,
>   2.996239, 0.303222, 4.988339, 8.131804, 0.117156, 3.285669, 1.175608,
>   3.677487, 1.068393, 8.820755, 8.138583, 9.0575)),
>   row.names = c(NA, 20L), class = "data.frame")
> # convert date from character to Date
> byDate <- input |>
>   mutate(newdate = mdy(date)) |>
>   group_by(newdate) |>
>   summarise(total = sum(EnergykWh))
>
> byDate
>
> ## # A tibble: 5 ? 2
> ##   newdate    total
> ##   <date>     <dbl>
> ## 1 2016-01-14 12.0
> ## 2 2016-01-15 32.6
> ## 3 2016-01-16 21.3
> ## 4 2016-01-17 22.9
> ## 5 2016-01-18  9.06
>
>
> Thanks
>
> Jim Holtman
> *Data Munger Guru*
>
>
> *What is the problem that you are trying to solve?Tell me what you want to
> do, not how you want to do it.*
>
>
> On Fri, Nov 3, 2023 at 2:51?AM roslinazairimah zakaria <
> roslinaump at gmail.com> wrote:
>
>> Hi,
>> I tried this:
>> # extract date from the time stamp
>> dt1 <- cbind(as.Date(dt$EndDate, format="%m/%d/%Y"), dt$EnergykWh)
>> head(dt1)
>> colnames(dt1) <- c("date", "EnergykWh")
>> and
>> my dt1 becomes these, the dates are replace by numbers.
>>
>> dt1 <- cbind(as.Date(dt$EndDate, format="%m/%d/%Y"), dt$EnergykWh)
>> dput(head(dt1))
>> colnames(dt1) <- c("date", "EnergykWh")
>> dput(head(dt1))
>>
>>
>> > dput(head(dt1))structure(c(16814, 16814, 16814, 16815, 16815, 16815,
>> 4.680496,
>> 6.272414, 1.032782, 11.004884, 10.096824, 6.658797), dim = c(6L,
>> 2L), dimnames = list(NULL, c("date", "EnergykWh")))
>>
>> Then I tried this:
>> library(dplyr)
>> dt1 %>%
>>   group_by(date) %>%
>>   summarise(EnergykWh.sum = sum(EnergykWh))
>> and got this errors
>>
>> dt1 %>%+   group_by(date) %>%+   summarise(EnergykWh.sum =
>> sum(EnergykWh))Error in UseMethod("group_by") :
>>   no applicable method for 'group_by' applied to an object of class
>> "c('matrix', 'array', 'double', 'numeric')"
>>
>>
>>
>> On Fri, Nov 3, 2023 at 7:23?AM roslinazairimah zakaria <
>> roslinaump at gmail.com>
>> wrote:
>>
>> > Dear all,
>> >
>> > I have this set of data. I would like to sum the EnergykWh according
>> date
>> > sequences.
>> >
>> > > head(dt1,20)                   StationName      date  time EnergykWh
>> > 1  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 12:09  4.680496
>> > 2  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 19:50  6.272414
>> > 3  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 20:22  1.032782
>> > 4  PALO ALTO CA / CAMBRIDGE #1 1/15/2016  8:25 11.004884
>> > 5  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 14:23 10.096824
>> > 6  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 18:17  6.658797
>> > 7  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 21:46  4.808874
>> > 8  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 10:19  1.469384
>> > 9  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 12:12  2.996239
>> > 10 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 14:12  0.303222
>> > 11 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 16:22  4.988339
>> > 12 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:16  8.131804
>> > 13 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:19  0.117156
>> > 14 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 20:24  3.285669
>> > 15 PALO ALTO CA / CAMBRIDGE #1 1/17/2016  9:54  1.175608
>> > 16 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 12:16  3.677487
>> > 17 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 13:53  1.068393
>> > 18 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 19:03  8.820755
>> > 19 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 22:00  8.138583
>> > 20 PALO ALTO CA / CAMBRIDGE #1 1/18/2016  8:58  9.057500
>> >
>> > I have tried this:
>> > library(dplyr)
>> > sums <- dt1 %>%
>> >   group_by(date) %>%
>> >   summarise(EnergykWh = sum(EnergykWh))
>> >
>> > head(sums,20)
>> >
>> > The date is not by daily sequence but by year sequence.
>> >
>> > > head(sums,20)# A tibble: 20 ? 2
>> >    date      EnergykWh
>> >    <chr>         <dbl> 1 1/1/2017     25.3   2 1/1/2018     61.0   3
>> 1/1/2019      0.627 4 1/1/2020     10.7   5 1/10/2017    69.4   6
>> 1/10/2018    54.5   7 1/10/2019    49.1   8 1/10/2020    45.9   9
>> 1/11/2017    73.9  10 1/11/2018    53.3  11 1/11/2019    93.5  12
>> 1/11/2020    66.7  13 1/12/2017    78.6  14 1/12/2018    42.2  15
>> 1/12/2019    22.7  16 1/12/2020    80.9  17 1/13/2017    85.6  18
>> 1/13/2018    46.4  19 1/13/2019    40.0  20 1/13/2020   121.
>> >
>> >
>> >
>> > Thank you very much for any help given.
>> >
>> >
>> > --
>> > *Roslinazairimah Zakaria*
>> > *Tel: +609-5492370; Fax. No.+609-5492766*
>> >
>> > *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
>> > roslinaump at gmail.com <roslinaump at gmail.com>*
>> > Faculty of Industrial Sciences & Technology
>> > University Malaysia Pahang
>> > Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>> >
>>
>>
>> --
>> *Roslinazairimah Zakaria*
>> *Tel: +609-5492370; Fax. No.+609-5492766*
>>
>> *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
>> roslinaump at gmail.com <roslinaump at gmail.com>*
>> Faculty of Industrial Sciences & Technology
>> University Malaysia Pahang
>> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

-- 
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From @vi@e@gross m@iii@g oii gm@ii@com  Fri Nov  3 22:11:53 2023
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Fri, 3 Nov 2023 17:11:53 -0400
Subject: [R] 
 I need to create new variables based on two numeric variables
 and one dichotomize conditional category variables.
In-Reply-To: <LV3PR01MB86736581F1E69742E545FD59DCA5A@LV3PR01MB8673.prod.exchangelabs.com>
References: <mailman.370481.1.1699009201.56788.r-help@r-project.org>
 <LV3PR01MB86736581F1E69742E545FD59DCA5A@LV3PR01MB8673.prod.exchangelabs.com>
Message-ID: <002d01da0e9a$5f1d2930$1d577b90$@gmail.com>

Just a minor point in the suggested solution:

df$LAP <- with(df, ifelse(G=='male', (WC-65)*TG, (WC-58)*TG))

since WC and TG are not conditional, would this be a slight improvement?

df$LAP <- with(df, TG*(WC - ifelse(G=='male', 65, 58)))



-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Jorgen Harmse via
R-help
Sent: Friday, November 3, 2023 11:56 AM
To: r-help at r-project.org; mkzaman.m at gmail.com
Subject: Re: [R] I need to create new variables based on two numeric
variables and one dichotomize conditional category variables.

df$LAP <- with(df, ifelse(G=='male', (WC-65)*TG, (WC-58)*TG))

That will do both calculations and merge the two vectors appropriately. It
will use extra memory, but it should be much faster than a 'for' loop.

Regards,
Jorgen Harmse.

------------------------------

Message: 8
Date: Fri, 3 Nov 2023 11:10:49 +1030
From: "Md. Kamruzzaman" <mkzaman.m at gmail.com>
To: r-help at r-project.org
Subject: [R] I need to create new variables based on two numeric
        variables and one dichotomize conditional category variables.
Message-ID:
        <CAGbxoeGjsxZKQ6qijEMq-X-5doqnQQS1jjPDDrGT6hH5xWqOKQ at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

Hello Everyone,
I have three variables: Waist circumference (WC), serum triglyceride (TG)
level and gender. Waist circumference and serum triglyceride is numeric and
gender (male and female) is categorical. From these three variables, I want
to calculate the "Lipid Accumulation Product (LAP) Index". The equation to
calculate LAP is different for male and females. I am giving both equations
below.

LAP for male = (WC-65)*TG
LAP for female = (WC-58)*TG

My question is 'how can I calculate the LAP and create a single new column?

Your cooperation will be highly appreciated.

Thanks in advance.

With Regards

*--------------------------------*

*Md Kamruzzaman*

*PhD **Research Fellow (**Medicine**)*
Discipline of Medicine and Centre of Research Excellence in Translating
Nutritional Science to Good Health
Adelaide Medical School | Faculty of Health and Medical Sciences
The University of Adelaide
Adelaide SA 5005

        [[alternative HTML version deleted]]



	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From JH@rm@e @end|ng |rom roku@com  Fri Nov  3 23:26:38 2023
From: JH@rm@e @end|ng |rom roku@com (Jorgen Harmse)
Date: Fri, 3 Nov 2023 22:26:38 +0000
Subject: [R] [EXTERNAL] RE: I need to create new variables based on two
 numeric variables and one dichotomize conditional category variables.
In-Reply-To: <002d01da0e9a$5f1d2930$1d577b90$@gmail.com>
References: <mailman.370481.1.1699009201.56788.r-help@r-project.org>
 <LV3PR01MB86736581F1E69742E545FD59DCA5A@LV3PR01MB8673.prod.exchangelabs.com>
 <002d01da0e9a$5f1d2930$1d577b90$@gmail.com>
Message-ID: <LV3PR01MB867383C6F83525C671FE74E0DCA5A@LV3PR01MB8673.prod.exchangelabs.com>

Yes, that will halve the number of multiplications.

If you?re looking for such optimisations then you can also consider ifelse(G=='male', 65L, 58L). That will definitely use less time & memory if WC is integer, but the trade-offs are more complicated if WC is floating point.

Regards,
Jorgen Harmse.



From: avi.e.gross at gmail.com <avi.e.gross at gmail.com>
Date: Friday, November 3, 2023 at 16:12
To: Jorgen Harmse <JHarmse at roku.com>, r-help at r-project.org <r-help at r-project.org>, mkzaman.m at gmail.com <mkzaman.m at gmail.com>
Subject: [EXTERNAL] RE: [R] I need to create new variables based on two numeric variables and one dichotomize conditional category variables.
Just a minor point in the suggested solution:

df$LAP <- with(df, ifelse(G=='male', (WC-65)*TG, (WC-58)*TG))

since WC and TG are not conditional, would this be a slight improvement?

df$LAP <- with(df, TG*(WC - ifelse(G=='male', 65, 58)))



-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Jorgen Harmse via
R-help
Sent: Friday, November 3, 2023 11:56 AM
To: r-help at r-project.org; mkzaman.m at gmail.com
Subject: Re: [R] I need to create new variables based on two numeric
variables and one dichotomize conditional category variables.

df$LAP <- with(df, ifelse(G=='male', (WC-65)*TG, (WC-58)*TG))

That will do both calculations and merge the two vectors appropriately. It
will use extra memory, but it should be much faster than a 'for' loop.

Regards,
Jorgen Harmse.

------------------------------

Message: 8
Date: Fri, 3 Nov 2023 11:10:49 +1030
From: "Md. Kamruzzaman" <mkzaman.m at gmail.com>
To: r-help at r-project.org
Subject: [R] I need to create new variables based on two numeric
        variables and one dichotomize conditional category variables.
Message-ID:
        <CAGbxoeGjsxZKQ6qijEMq-X-5doqnQQS1jjPDDrGT6hH5xWqOKQ at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

Hello Everyone,
I have three variables: Waist circumference (WC), serum triglyceride (TG)
level and gender. Waist circumference and serum triglyceride is numeric and
gender (male and female) is categorical. From these three variables, I want
to calculate the "Lipid Accumulation Product (LAP) Index". The equation to
calculate LAP is different for male and females. I am giving both equations
below.

LAP for male = (WC-65)*TG
LAP for female = (WC-58)*TG

My question is 'how can I calculate the LAP and create a single new column?

Your cooperation will be highly appreciated.

Thanks in advance.

With Regards

*--------------------------------*

*Md Kamruzzaman*

*PhD **Research Fellow (**Medicine**)*
Discipline of Medicine and Centre of Research Excellence in Translating
Nutritional Science to Good Health
Adelaide Medical School | Faculty of Health and Medical Sciences
The University of Adelaide
Adelaide SA 5005

        [[alternative HTML version deleted]]



        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From @vi@e@gross m@iii@g oii gm@ii@com  Sat Nov  4 06:08:03 2023
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Sat, 4 Nov 2023 01:08:03 -0400
Subject: [R] [EXTERNAL] RE: I need to create new variables based on two
 numeric variables and one dichotomize conditional category variables.
In-Reply-To: <LV3PR01MB867383C6F83525C671FE74E0DCA5A@LV3PR01MB8673.prod.exchangelabs.com>
References: <mailman.370481.1.1699009201.56788.r-help@r-project.org>
 <LV3PR01MB86736581F1E69742E545FD59DCA5A@LV3PR01MB8673.prod.exchangelabs.com>
 <002d01da0e9a$5f1d2930$1d577b90$@gmail.com>
 <LV3PR01MB867383C6F83525C671FE74E0DCA5A@LV3PR01MB8673.prod.exchangelabs.com>
Message-ID: <019a01da0edc$e41c39e0$ac54ada0$@gmail.com>

To be fair, Jordan, I think R has some optimizations so that the arguments
in some cases are NOT evaluated until needed. So only one or the other
choice ever gets evaluated for each row. My suggestion merely has
typographic implications and some aspects of clarity and minor amounts of
less memory and parsing needed. 
 
But ifelse() is currently implemented somewhat too complexly for my taste.
Just type "ifelse" at the prompt and you will see many lines of code that
handle various scenarios.
 
If you KNOW you have a certain situation such as a data.frame with multiple
rows and are sure a simpler solution works, there may well be faster ways to
do this. Obviously you could write a function that can be called once per
line and returns the answer, or a vectorized version that returns a vector
of 65 and 58 entries. Or you could add a few lines of code creating a
vector, perhaps as a temporary new column that looks like:
 
logic_male <- df$G == "male"
 
age[logic_male] <- 65
age(!logic_male] <- 58
 
Then use the age column in a formula directly as it contains the part of the
ifelse needed. You can then delete "age" whether stand-alone or as a column.
 
What is more efficient depends on your data.
 
Do note though that an advantage of using ifelse() is when you have nested
conditions which cannot trivially be written out along the lines above, but
I find that sometimes such nested expressions may be easier to read using
other techniques such as the dplyr function case_when().
Here is an example of code where some entries are NA or not categorized:
 
library(tidyverse)
WC <- 100
TG <- 2
Gender <- c("male", "female", "no comment", NA, "female")
 
result <- TG * (WC -
                  case_when(
                    is.na(Gender) ~NA,
                    Gender == "male" ~ 65,
                    Gender == "female" ~ 58,
                    .default  = NA
                  ))
 
The result for the above example is a result:
 
> result
[1] 70 84 NA NA 84
 
 
If you later want to add categories such as "transgender" with a value of 61
or have other numbers for groups like "Hispanic male", you can amend the
instructions as long as you put your conditions in an order so that they are
tried until one of them matches, or it takes the default. Yes, in a sense
the above is doable using a deeply nested ifelse() but easier for me to read
and write and evaluate. It may not be more efficient or may be as some of
dplyr is compiled code.
 
Please note some here prefer discussions about base-R functionality and some
have qualms about the tidyverse for various reasons. I don't and find much
of their functionality more easy to use.
 
 
 
From: Jorgen Harmse <JHarmse at roku.com> 
Sent: Friday, November 3, 2023 6:27 PM
To: avi.e.gross at gmail.com; r-help at r-project.org; mkzaman.m at gmail.com
Subject: Re: [EXTERNAL] RE: [R] I need to create new variables based on two
numeric variables and one dichotomize conditional category variables.
 
Yes, that will halve the number of multiplications.
 
If you're looking for such optimisations then you can also consider
ifelse(G=='male', 65L, 58L). That will definitely use less time & memory if
WC is integer, but the trade-offs are more complicated if WC is floating
point.
 
Regards,
Jorgen Harmse.


 
From: avi.e.gross at gmail.com <mailto:avi.e.gross at gmail.com>
<avi.e.gross at gmail.com <mailto:avi.e.gross at gmail.com> >
Date: Friday, November 3, 2023 at 16:12
To: Jorgen Harmse <JHarmse at roku.com <mailto:JHarmse at roku.com> >,
r-help at r-project.org <mailto:r-help at r-project.org>  <r-help at r-project.org
<mailto:r-help at r-project.org> >, mkzaman.m at gmail.com
<mailto:mkzaman.m at gmail.com>  <mkzaman.m at gmail.com
<mailto:mkzaman.m at gmail.com> >
Subject: [EXTERNAL] RE: [R] I need to create new variables based on two
numeric variables and one dichotomize conditional category variables.
Just a minor point in the suggested solution:

df$LAP <- with(df, ifelse(G=='male', (WC-65)*TG, (WC-58)*TG))

since WC and TG are not conditional, would this be a slight improvement?

df$LAP <- with(df, TG*(WC - ifelse(G=='male', 65, 58)))



-----Original Message-----
From: R-help <r-help-bounces at r-project.org
<mailto:r-help-bounces at r-project.org> > On Behalf Of Jorgen Harmse via
R-help
Sent: Friday, November 3, 2023 11:56 AM
To: r-help at r-project.org <mailto:r-help at r-project.org> ; mkzaman.m at gmail.com
<mailto:mkzaman.m at gmail.com> 
Subject: Re: [R] I need to create new variables based on two numeric
variables and one dichotomize conditional category variables.

df$LAP <- with(df, ifelse(G=='male', (WC-65)*TG, (WC-58)*TG))

That will do both calculations and merge the two vectors appropriately. It
will use extra memory, but it should be much faster than a 'for' loop.

Regards,
Jorgen Harmse.

------------------------------

Message: 8
Date: Fri, 3 Nov 2023 11:10:49 +1030
From: "Md. Kamruzzaman" <mkzaman.m at gmail.com <mailto:mkzaman.m at gmail.com> >
To: r-help at r-project.org <mailto:r-help at r-project.org> 
Subject: [R] I need to create new variables based on two numeric
        variables and one dichotomize conditional category variables.
Message-ID:
        <CAGbxoeGjsxZKQ6qijEMq-X-5doqnQQS1jjPDDrGT6hH5xWqOKQ at mail.gmail.com
<mailto:CAGbxoeGjsxZKQ6qijEMq-X-5doqnQQS1jjPDDrGT6hH5xWqOKQ at mail.gmail.com>
>
Content-Type: text/plain; charset="utf-8"

Hello Everyone,
I have three variables: Waist circumference (WC), serum triglyceride (TG)
level and gender. Waist circumference and serum triglyceride is numeric and
gender (male and female) is categorical. From these three variables, I want
to calculate the "Lipid Accumulation Product (LAP) Index". The equation to
calculate LAP is different for male and females. I am giving both equations
below.

LAP for male = (WC-65)*TG
LAP for female = (WC-58)*TG

My question is 'how can I calculate the LAP and create a single new column?

Your cooperation will be highly appreciated.

Thanks in advance.

With Regards

*--------------------------------*

*Md Kamruzzaman*

*PhD **Research Fellow (**Medicine**)*
Discipline of Medicine and Centre of Research Excellence in Translating
Nutritional Science to Good Health
Adelaide Medical School | Faculty of Health and Medical Sciences
The University of Adelaide
Adelaide SA 5005

        [[alternative HTML version deleted]]



        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To
UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From @|e@@@ndro@pug||@| @end|ng |rom gm@||@com  Fri Nov  3 17:20:43 2023
From: @|e@@@ndro@pug||@| @end|ng |rom gm@||@com (Alessandro Puglisi)
Date: Fri, 3 Nov 2023 17:20:43 +0100
Subject: [R] Adding columns to a tibble based on a value in a different
 tibble
Message-ID: <CA+MLcG12KWOwGeR_fPNbazHCNAktmvcgvJ8TKVQdB1xbJ5jsQw@mail.gmail.com>

Hi everyone,

I have a tibble with various ids and associated information.

I need to add a new column to this tibble that retrieves a specific 'y'
value from a different tibble that has some of the mentioned ids in the
first column and a 'y' value in the second one. If the id, and so the 'y'
value is found, it will be included; otherwise, 'NA' will be used.

Could you please help me?

Thanks,
Alessandro

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sat Nov  4 15:35:18 2023
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 4 Nov 2023 07:35:18 -0700
Subject: [R] Adding columns to a tibble based on a value in a different
 tibble
In-Reply-To: <CA+MLcG12KWOwGeR_fPNbazHCNAktmvcgvJ8TKVQdB1xbJ5jsQw@mail.gmail.com>
References: <CA+MLcG12KWOwGeR_fPNbazHCNAktmvcgvJ8TKVQdB1xbJ5jsQw@mail.gmail.com>
Message-ID: <CAGxFJbT+X6Gj-KUjZUYcGmiGn71-YmOkguH=rg=PTaSXq-=wtQ@mail.gmail.com>

I think a simple reproducible example ("reprex") may be necessary for you
to get a useful reply. Questions with vague specifications such as yours
often result in going round and round with attempts to clarify what you
mean without a satisfactory answer. Clarification at the outset with a
reprex may save you and others a lot of frustration.

Cheers,
Bert

On Sat, Nov 4, 2023 at 1:41?AM Alessandro Puglisi <
alessandro.puglisi at gmail.com> wrote:

> Hi everyone,
>
> I have a tibble with various ids and associated information.
>
> I need to add a new column to this tibble that retrieves a specific 'y'
> value from a different tibble that has some of the mentioned ids in the
> first column and a 'y' value in the second one. If the id, and so the 'y'
> value is found, it will be included; otherwise, 'NA' will be used.
>
> Could you please help me?
>
> Thanks,
> Alessandro
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @vi@e@gross m@iii@g oii gm@ii@com  Sat Nov  4 16:25:13 2023
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Sat, 4 Nov 2023 11:25:13 -0400
Subject: [R] Adding columns to a tibble based on a value in a different
 tibble
In-Reply-To: <CAGxFJbT+X6Gj-KUjZUYcGmiGn71-YmOkguH=rg=PTaSXq-=wtQ@mail.gmail.com>
References: <CA+MLcG12KWOwGeR_fPNbazHCNAktmvcgvJ8TKVQdB1xbJ5jsQw@mail.gmail.com>
 <CAGxFJbT+X6Gj-KUjZUYcGmiGn71-YmOkguH=rg=PTaSXq-=wtQ@mail.gmail.com>
Message-ID: <003e01da0f33$1b45f810$51d1e830$@gmail.com>

Yes, Bert. At first glance I thought it was one of the merge/joins and then wondered at the wording that made it sound like the ids may not be one per column.

IFF the need is the simpler case, it is a straightforward enough and common need. An example might make it clear enough so actual code can be shared as compared to talking about a first and second tibble.

Here is one reference to consider:

https://r4ds.hadley.nz/joins.html#:~:text=dplyr%20provides%20six%20join%20functions,is%20primarily%20determined%20by%20x%20.


A left_join may be what works, and of course more basic R includes the merge() function:

https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/merge

If the column were to contain multiple ID, that changes things and a more complex approach could be needed.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Bert Gunter
Sent: Saturday, November 4, 2023 10:35 AM
To: Alessandro Puglisi <alessandro.puglisi at gmail.com>
Cc: r-help at r-project.org
Subject: Re: [R] Adding columns to a tibble based on a value in a different tibble

I think a simple reproducible example ("reprex") may be necessary for you
to get a useful reply. Questions with vague specifications such as yours
often result in going round and round with attempts to clarify what you
mean without a satisfactory answer. Clarification at the outset with a
reprex may save you and others a lot of frustration.

Cheers,
Bert

On Sat, Nov 4, 2023 at 1:41?AM Alessandro Puglisi <
alessandro.puglisi at gmail.com> wrote:

> Hi everyone,
>
> I have a tibble with various ids and associated information.
>
> I need to add a new column to this tibble that retrieves a specific 'y'
> value from a different tibble that has some of the mentioned ids in the
> first column and a 'y' value in the second one. If the id, and so the 'y'
> value is found, it will be included; otherwise, 'NA' will be used.
>
> Could you please help me?
>
> Thanks,
> Alessandro
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat Nov  4 17:56:20 2023
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 4 Nov 2023 16:56:20 +0000
Subject: [R] Sum data according to date in sequence
In-Reply-To: <CANTvJZLZDkdT68sU0a-E35v_vAUtDvWWUcMV0x840XSM1KQDvA@mail.gmail.com>
References: <CANTvJZL-jS=mWPWSJXqWVvxZfEVcvHPNhqQu4O9eEQj7ru8qRw@mail.gmail.com>
 <CAAxdm-74kKpsFMG1qUJmNxV8hA2LyDyjCjyVwiKz3KqvuUQgNQ@mail.gmail.com>
 <CANTvJZLZDkdT68sU0a-E35v_vAUtDvWWUcMV0x840XSM1KQDvA@mail.gmail.com>
Message-ID: <a9c4f560-d376-4f65-bdeb-33e7e2e80484@sapo.pt>

?s 01:49 de 03/11/2023, roslinazairimah zakaria escreveu:
> Hi all,
> 
> This is the data:
> 
>> dput(head(dt1,20))structure(list(StationName = c("PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1"), date = c("1/14/2016", "1/14/2016",
> "1/14/2016", "1/15/2016", "1/15/2016", "1/15/2016", "1/15/2016",
> "1/16/2016", "1/16/2016", "1/16/2016", "1/16/2016", "1/16/2016",
> "1/16/2016", "1/16/2016", "1/17/2016", "1/17/2016", "1/17/2016",
> "1/17/2016", "1/17/2016", "1/18/2016"), time = c("12:09", "19:50",
> "20:22", "8:25", "14:23", "18:17", "21:46", "10:19", "12:12",
> "14:12", "16:22", "19:16", "19:19", "20:24", "9:54", "12:16",
> "13:53", "19:03", "22:00", "8:58"), EnergykWh = c(4.680496, 6.272414,
> 1.032782, 11.004884, 10.096824, 6.658797, 4.808874, 1.469384,
> 2.996239, 0.303222, 4.988339, 8.131804, 0.117156, 3.285669, 1.175608,
> 3.677487, 1.068393, 8.820755, 8.138583, 9.0575)), row.names = c(NA,
> 20L), class = "data.frame")
> 
> 
> I would like to sum EnergykW data by the date. E.g. all values for
> EnergykWh on 1/14/2016
> 
> 
> On Fri, Nov 3, 2023 at 8:10?AM jim holtman <jholtman at gmail.com> wrote:
> 
>> How about send a 'dput' of some sample data.  My guess is that your date
>> is 'character' and not 'Date'.
>>
>> Thanks
>>
>> Jim Holtman
>> *Data Munger Guru*
>>
>>
>> *What is the problem that you are trying to solve?Tell me what you want to
>> do, not how you want to do it.*
>>
>>
>> On Thu, Nov 2, 2023 at 4:24?PM roslinazairimah zakaria <
>> roslinaump at gmail.com> wrote:
>>
>>> Dear all,
>>>
>>> I have this set of data. I would like to sum the EnergykWh according date
>>> sequences.
>>>
>>>> head(dt1,20)                   StationName      date  time EnergykWh
>>> 1  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 12:09  4.680496
>>> 2  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 19:50  6.272414
>>> 3  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 20:22  1.032782
>>> 4  PALO ALTO CA / CAMBRIDGE #1 1/15/2016  8:25 11.004884
>>> 5  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 14:23 10.096824
>>> 6  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 18:17  6.658797
>>> 7  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 21:46  4.808874
>>> 8  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 10:19  1.469384
>>> 9  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 12:12  2.996239
>>> 10 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 14:12  0.303222
>>> 11 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 16:22  4.988339
>>> 12 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:16  8.131804
>>> 13 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:19  0.117156
>>> 14 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 20:24  3.285669
>>> 15 PALO ALTO CA / CAMBRIDGE #1 1/17/2016  9:54  1.175608
>>> 16 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 12:16  3.677487
>>> 17 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 13:53  1.068393
>>> 18 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 19:03  8.820755
>>> 19 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 22:00  8.138583
>>> 20 PALO ALTO CA / CAMBRIDGE #1 1/18/2016  8:58  9.057500
>>>
>>> I have tried this:
>>> library(dplyr)
>>> sums <- dt1 %>%
>>>    group_by(date) %>%
>>>    summarise(EnergykWh = sum(EnergykWh))
>>>
>>> head(sums,20)
>>>
>>> The date is not by daily sequence but by year sequence.
>>>
>>>> head(sums,20)# A tibble: 20 ? 2
>>>     date      EnergykWh
>>>     <chr>         <dbl> 1 1/1/2017     25.3   2 1/1/2018     61.0   3
>>> 1/1/2019      0.627 4 1/1/2020     10.7   5 1/10/2017    69.4   6
>>> 1/10/2018    54.5   7 1/10/2019    49.1   8 1/10/2020    45.9   9
>>> 1/11/2017    73.9  10 1/11/2018    53.3  11 1/11/2019    93.5  12
>>> 1/11/2020    66.7  13 1/12/2017    78.6  14 1/12/2018    42.2  15
>>> 1/12/2019    22.7  16 1/12/2020    80.9  17 1/13/2017    85.6  18
>>> 1/13/2018    46.4  19 1/13/2019    40.0  20 1/13/2020   121.
>>>
>>>
>>>
>>> Thank you very much for any help given.
>>>
>>>
>>> --
>>> *Roslinazairimah Zakaria*
>>> *Tel: +609-5492370; Fax. No.+609-5492766*
>>>
>>> *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
>>> roslinaump at gmail.com <roslinaump at gmail.com>*
>>> Faculty of Industrial Sciences & Technology
>>> University Malaysia Pahang
>>> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
> 
Hello,

Here are two solutions.

1. Base R

Though I don't coerce the date column to class "Date", it seems to work.


aggregate(EnergykWh ~ date, dt1, sum)
#>        date EnergykWh
#> 1 1/14/2016  11.98569
#> 2 1/15/2016  32.56938
#> 3 1/16/2016  21.29181
#> 4 1/17/2016  22.88083
#> 5 1/18/2016   9.05750


2. Package dplyr.
First column date is coerced from class "character" to class "Date".
Then the grouped sums are computed.


suppressPackageStartupMessages(
   library(dplyr)
)

dt1 %>%
   mutate(date = as.Date(date, "%m/%d/%Y")) %>%
   summarise(EnergykWh = sum(EnergykWh), .by = date)
#>         date EnergykWh
#> 1 2016-01-14  11.98569
#> 2 2016-01-15  32.56938
#> 3 2016-01-16  21.29181
#> 4 2016-01-17  22.88083
#> 5 2016-01-18   9.05750


As you can see, the results are the same.

Also, this exact problem is one of the most asked on StackOverflow. 
Maybe you could try searching there for a solution. My code above is 
also exactly the code in [1], though I had already this answer written. 
I only checked after :(.


[1] 
https://stackoverflow.com/questions/61548758/r-how-sum-values-by-group-by-date


Hope this helps,

Rui Barradas



-- 
Este e-mail foi analisado pelo software antiv?rus AVG para verificar a presen?a de v?rus.
www.avg.com


From mkz@m@n@m @end|ng |rom gm@||@com  Sat Nov  4 10:54:49 2023
From: mkz@m@n@m @end|ng |rom gm@||@com (Md. Kamruzzaman)
Date: Sat, 4 Nov 2023 20:24:49 +1030
Subject: [R] 
 I need to create new variables based on two numeric variables
 and one dichotomize conditional category variables.
In-Reply-To: <CAGxFJbS41wwYMDxRDE6s=WNEVchyAQv4uppMge2dFRdMnraCWQ@mail.gmail.com>
References: <CAGbxoeGjsxZKQ6qijEMq-X-5doqnQQS1jjPDDrGT6hH5xWqOKQ@mail.gmail.com>
 <CAGxFJbS41wwYMDxRDE6s=WNEVchyAQv4uppMge2dFRdMnraCWQ@mail.gmail.com>
Message-ID: <CAGbxoeEin+pM0Z1_9v0cBV38PeV31POpHejTW-0NycYSHu0J0g@mail.gmail.com>

Thanks Everyone,
My variables are in a dataframe with multiple other variables.

Thanks

-------------------------

*Md Kamruzzaman*


On Sat, Nov 4, 2023 at 1:13?AM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Well, something like:
>
> LAP <- ifelse(gender =='male', (WC-65)*TG, (WC-58)*TG)
>
> The exact code depends on whether your variables are in a data frame or
> list or whatever, which you failed to specify. If so, ?with  may be useful.
>
> Cheers,
> Bert
>
>
>
> On Fri, Nov 3, 2023 at 3:43?AM Md. Kamruzzaman <mkzaman.m at gmail.com>
> wrote:
>
>> Hello Everyone,
>> I have three variables: Waist circumference (WC), serum triglyceride (TG)
>> level and gender. Waist circumference and serum triglyceride is numeric
>> and
>> gender (male and female) is categorical. From these three variables, I
>> want
>> to calculate the "Lipid Accumulation Product (LAP) Index". The equation to
>> calculate LAP is different for male and females. I am giving both
>> equations
>> below.
>>
>> LAP for male = (WC-65)*TG
>> LAP for female = (WC-58)*TG
>>
>> My question is 'how can I calculate the LAP and create a single new
>> column?
>>
>> Your cooperation will be highly appreciated.
>>
>> Thanks in advance.
>>
>> With Regards
>>
>> *--------------------------------*
>>
>> *Md Kamruzzaman*
>>
>> *PhD **Research Fellow (**Medicine**)*
>> Discipline of Medicine and Centre of Research Excellence in Translating
>> Nutritional Science to Good Health
>> Adelaide Medical School | Faculty of Health and Medical Sciences
>> The University of Adelaide
>> Adelaide SA 5005
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From @vi@e@gross m@iii@g oii gm@ii@com  Sat Nov  4 18:47:53 2023
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Sat, 4 Nov 2023 13:47:53 -0400
Subject: [R] Sum data according to date in sequence
In-Reply-To: <a9c4f560-d376-4f65-bdeb-33e7e2e80484@sapo.pt>
References: <CANTvJZL-jS=mWPWSJXqWVvxZfEVcvHPNhqQu4O9eEQj7ru8qRw@mail.gmail.com>
 <CAAxdm-74kKpsFMG1qUJmNxV8hA2LyDyjCjyVwiKz3KqvuUQgNQ@mail.gmail.com>
 <CANTvJZLZDkdT68sU0a-E35v_vAUtDvWWUcMV0x840XSM1KQDvA@mail.gmail.com>
 <a9c4f560-d376-4f65-bdeb-33e7e2e80484@sapo.pt>
Message-ID: <008501da0f47$09624220$1c26c660$@gmail.com>

There may be a point to consider about the field containing dates in the request below. Yes, much code will "work" just fine if the column  are is seen as text as you can group by that too. The results will perhaps not be in the order by row that you expected but you can do your re-sorting perhaps even more efficiently after your summarise() either by converting the fewer remaining rows to a form of date or by transforming the text dates into an order of year/month/date that then sorts properly in forward or reverse order as needed. 

Converting lots of rows to date is not a cheap process and grouping by that more complex date data structure may be harder. Heck, it may even make sense to use the text form of dates organized as a factor as the grouping becomes sort of pre-done.

The above comments are not saying any other solutions offered are wrong but simply discussing whether, especially for larger data sets, there are ways that could be more efficient.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Rui Barradas
Sent: Saturday, November 4, 2023 12:56 PM
To: roslinazairimah zakaria <roslinaump at gmail.com>; jim holtman <jholtman at gmail.com>
Cc: r-help mailing list <r-help at r-project.org>
Subject: Re: [R] Sum data according to date in sequence

?s 01:49 de 03/11/2023, roslinazairimah zakaria escreveu:
> Hi all,
> 
> This is the data:
> 
>> dput(head(dt1,20))structure(list(StationName = c("PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> "PALO ALTO CA / CAMBRIDGE #1"), date = c("1/14/2016", "1/14/2016",
> "1/14/2016", "1/15/2016", "1/15/2016", "1/15/2016", "1/15/2016",
> "1/16/2016", "1/16/2016", "1/16/2016", "1/16/2016", "1/16/2016",
> "1/16/2016", "1/16/2016", "1/17/2016", "1/17/2016", "1/17/2016",
> "1/17/2016", "1/17/2016", "1/18/2016"), time = c("12:09", "19:50",
> "20:22", "8:25", "14:23", "18:17", "21:46", "10:19", "12:12",
> "14:12", "16:22", "19:16", "19:19", "20:24", "9:54", "12:16",
> "13:53", "19:03", "22:00", "8:58"), EnergykWh = c(4.680496, 6.272414,
> 1.032782, 11.004884, 10.096824, 6.658797, 4.808874, 1.469384,
> 2.996239, 0.303222, 4.988339, 8.131804, 0.117156, 3.285669, 1.175608,
> 3.677487, 1.068393, 8.820755, 8.138583, 9.0575)), row.names = c(NA,
> 20L), class = "data.frame")
> 
> 
> I would like to sum EnergykW data by the date. E.g. all values for
> EnergykWh on 1/14/2016
> 
> 
> On Fri, Nov 3, 2023 at 8:10?AM jim holtman <jholtman at gmail.com> wrote:
> 
>> How about send a 'dput' of some sample data.  My guess is that your date
>> is 'character' and not 'Date'.
>>
>> Thanks
>>
>> Jim Holtman
>> *Data Munger Guru*
>>
>>
>> *What is the problem that you are trying to solve?Tell me what you want to
>> do, not how you want to do it.*
>>
>>
>> On Thu, Nov 2, 2023 at 4:24?PM roslinazairimah zakaria <
>> roslinaump at gmail.com> wrote:
>>
>>> Dear all,
>>>
>>> I have this set of data. I would like to sum the EnergykWh according date
>>> sequences.
>>>
>>>> head(dt1,20)                   StationName      date  time EnergykWh
>>> 1  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 12:09  4.680496
>>> 2  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 19:50  6.272414
>>> 3  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 20:22  1.032782
>>> 4  PALO ALTO CA / CAMBRIDGE #1 1/15/2016  8:25 11.004884
>>> 5  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 14:23 10.096824
>>> 6  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 18:17  6.658797
>>> 7  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 21:46  4.808874
>>> 8  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 10:19  1.469384
>>> 9  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 12:12  2.996239
>>> 10 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 14:12  0.303222
>>> 11 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 16:22  4.988339
>>> 12 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:16  8.131804
>>> 13 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:19  0.117156
>>> 14 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 20:24  3.285669
>>> 15 PALO ALTO CA / CAMBRIDGE #1 1/17/2016  9:54  1.175608
>>> 16 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 12:16  3.677487
>>> 17 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 13:53  1.068393
>>> 18 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 19:03  8.820755
>>> 19 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 22:00  8.138583
>>> 20 PALO ALTO CA / CAMBRIDGE #1 1/18/2016  8:58  9.057500
>>>
>>> I have tried this:
>>> library(dplyr)
>>> sums <- dt1 %>%
>>>    group_by(date) %>%
>>>    summarise(EnergykWh = sum(EnergykWh))
>>>
>>> head(sums,20)
>>>
>>> The date is not by daily sequence but by year sequence.
>>>
>>>> head(sums,20)# A tibble: 20 ? 2
>>>     date      EnergykWh
>>>     <chr>         <dbl> 1 1/1/2017     25.3   2 1/1/2018     61.0   3
>>> 1/1/2019      0.627 4 1/1/2020     10.7   5 1/10/2017    69.4   6
>>> 1/10/2018    54.5   7 1/10/2019    49.1   8 1/10/2020    45.9   9
>>> 1/11/2017    73.9  10 1/11/2018    53.3  11 1/11/2019    93.5  12
>>> 1/11/2020    66.7  13 1/12/2017    78.6  14 1/12/2018    42.2  15
>>> 1/12/2019    22.7  16 1/12/2020    80.9  17 1/13/2017    85.6  18
>>> 1/13/2018    46.4  19 1/13/2019    40.0  20 1/13/2020   121.
>>>
>>>
>>>
>>> Thank you very much for any help given.
>>>
>>>
>>> --
>>> *Roslinazairimah Zakaria*
>>> *Tel: +609-5492370; Fax. No.+609-5492766*
>>>
>>> *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
>>> roslinaump at gmail.com <roslinaump at gmail.com>*
>>> Faculty of Industrial Sciences & Technology
>>> University Malaysia Pahang
>>> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
> 
Hello,

Here are two solutions.

1. Base R

Though I don't coerce the date column to class "Date", it seems to work.


aggregate(EnergykWh ~ date, dt1, sum)
#>        date EnergykWh
#> 1 1/14/2016  11.98569
#> 2 1/15/2016  32.56938
#> 3 1/16/2016  21.29181
#> 4 1/17/2016  22.88083
#> 5 1/18/2016   9.05750


2. Package dplyr.
First column date is coerced from class "character" to class "Date".
Then the grouped sums are computed.


suppressPackageStartupMessages(
   library(dplyr)
)

dt1 %>%
   mutate(date = as.Date(date, "%m/%d/%Y")) %>%
   summarise(EnergykWh = sum(EnergykWh), .by = date)
#>         date EnergykWh
#> 1 2016-01-14  11.98569
#> 2 2016-01-15  32.56938
#> 3 2016-01-16  21.29181
#> 4 2016-01-17  22.88083
#> 5 2016-01-18   9.05750


As you can see, the results are the same.

Also, this exact problem is one of the most asked on StackOverflow. 
Maybe you could try searching there for a solution. My code above is 
also exactly the code in [1], though I had already this answer written. 
I only checked after :(.


[1] 
https://stackoverflow.com/questions/61548758/r-how-sum-values-by-group-by-date


Hope this helps,

Rui Barradas



-- 
Este e-mail foi analisado pelo software antiv?rus AVG para verificar a presen?a de v?rus.
www.avg.com

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ro@||n@ump @end|ng |rom gm@||@com  Sun Nov  5 00:05:40 2023
From: ro@||n@ump @end|ng |rom gm@||@com (roslinazairimah zakaria)
Date: Sun, 5 Nov 2023 07:05:40 +0800
Subject: [R] Sum data according to date in sequence
In-Reply-To: <a9c4f560-d376-4f65-bdeb-33e7e2e80484@sapo.pt>
References: <CANTvJZL-jS=mWPWSJXqWVvxZfEVcvHPNhqQu4O9eEQj7ru8qRw@mail.gmail.com>
 <CAAxdm-74kKpsFMG1qUJmNxV8hA2LyDyjCjyVwiKz3KqvuUQgNQ@mail.gmail.com>
 <CANTvJZLZDkdT68sU0a-E35v_vAUtDvWWUcMV0x840XSM1KQDvA@mail.gmail.com>
 <a9c4f560-d376-4f65-bdeb-33e7e2e80484@sapo.pt>
Message-ID: <CANTvJZJFRKNwJbyT0q_F8uBt1+AmS68s8wnrJsKz93isXnARYA@mail.gmail.com>

Hi all,
Thank you very much.
I learn a lot from your suggested solution.



On Sun, Nov 5, 2023 at 12:56?AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> ?s 01:49 de 03/11/2023, roslinazairimah zakaria escreveu:
> > Hi all,
> >
> > This is the data:
> >
> >> dput(head(dt1,20))structure(list(StationName = c("PALO ALTO CA /
> CAMBRIDGE #1",
> > "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> > "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> > "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> > "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> > "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> > "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> > "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> > "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> > "PALO ALTO CA / CAMBRIDGE #1", "PALO ALTO CA / CAMBRIDGE #1",
> > "PALO ALTO CA / CAMBRIDGE #1"), date = c("1/14/2016", "1/14/2016",
> > "1/14/2016", "1/15/2016", "1/15/2016", "1/15/2016", "1/15/2016",
> > "1/16/2016", "1/16/2016", "1/16/2016", "1/16/2016", "1/16/2016",
> > "1/16/2016", "1/16/2016", "1/17/2016", "1/17/2016", "1/17/2016",
> > "1/17/2016", "1/17/2016", "1/18/2016"), time = c("12:09", "19:50",
> > "20:22", "8:25", "14:23", "18:17", "21:46", "10:19", "12:12",
> > "14:12", "16:22", "19:16", "19:19", "20:24", "9:54", "12:16",
> > "13:53", "19:03", "22:00", "8:58"), EnergykWh = c(4.680496, 6.272414,
> > 1.032782, 11.004884, 10.096824, 6.658797, 4.808874, 1.469384,
> > 2.996239, 0.303222, 4.988339, 8.131804, 0.117156, 3.285669, 1.175608,
> > 3.677487, 1.068393, 8.820755, 8.138583, 9.0575)), row.names = c(NA,
> > 20L), class = "data.frame")
> >
> >
> > I would like to sum EnergykW data by the date. E.g. all values for
> > EnergykWh on 1/14/2016
> >
> >
> > On Fri, Nov 3, 2023 at 8:10?AM jim holtman <jholtman at gmail.com> wrote:
> >
> >> How about send a 'dput' of some sample data.  My guess is that your date
> >> is 'character' and not 'Date'.
> >>
> >> Thanks
> >>
> >> Jim Holtman
> >> *Data Munger Guru*
> >>
> >>
> >> *What is the problem that you are trying to solve?Tell me what you want
> to
> >> do, not how you want to do it.*
> >>
> >>
> >> On Thu, Nov 2, 2023 at 4:24?PM roslinazairimah zakaria <
> >> roslinaump at gmail.com> wrote:
> >>
> >>> Dear all,
> >>>
> >>> I have this set of data. I would like to sum the EnergykWh according
> date
> >>> sequences.
> >>>
> >>>> head(dt1,20)                   StationName      date  time EnergykWh
> >>> 1  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 12:09  4.680496
> >>> 2  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 19:50  6.272414
> >>> 3  PALO ALTO CA / CAMBRIDGE #1 1/14/2016 20:22  1.032782
> >>> 4  PALO ALTO CA / CAMBRIDGE #1 1/15/2016  8:25 11.004884
> >>> 5  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 14:23 10.096824
> >>> 6  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 18:17  6.658797
> >>> 7  PALO ALTO CA / CAMBRIDGE #1 1/15/2016 21:46  4.808874
> >>> 8  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 10:19  1.469384
> >>> 9  PALO ALTO CA / CAMBRIDGE #1 1/16/2016 12:12  2.996239
> >>> 10 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 14:12  0.303222
> >>> 11 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 16:22  4.988339
> >>> 12 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:16  8.131804
> >>> 13 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 19:19  0.117156
> >>> 14 PALO ALTO CA / CAMBRIDGE #1 1/16/2016 20:24  3.285669
> >>> 15 PALO ALTO CA / CAMBRIDGE #1 1/17/2016  9:54  1.175608
> >>> 16 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 12:16  3.677487
> >>> 17 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 13:53  1.068393
> >>> 18 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 19:03  8.820755
> >>> 19 PALO ALTO CA / CAMBRIDGE #1 1/17/2016 22:00  8.138583
> >>> 20 PALO ALTO CA / CAMBRIDGE #1 1/18/2016  8:58  9.057500
> >>>
> >>> I have tried this:
> >>> library(dplyr)
> >>> sums <- dt1 %>%
> >>>    group_by(date) %>%
> >>>    summarise(EnergykWh = sum(EnergykWh))
> >>>
> >>> head(sums,20)
> >>>
> >>> The date is not by daily sequence but by year sequence.
> >>>
> >>>> head(sums,20)# A tibble: 20 ? 2
> >>>     date      EnergykWh
> >>>     <chr>         <dbl> 1 1/1/2017     25.3   2 1/1/2018     61.0   3
> >>> 1/1/2019      0.627 4 1/1/2020     10.7   5 1/10/2017    69.4   6
> >>> 1/10/2018    54.5   7 1/10/2019    49.1   8 1/10/2020    45.9   9
> >>> 1/11/2017    73.9  10 1/11/2018    53.3  11 1/11/2019    93.5  12
> >>> 1/11/2020    66.7  13 1/12/2017    78.6  14 1/12/2018    42.2  15
> >>> 1/12/2019    22.7  16 1/12/2020    80.9  17 1/13/2017    85.6  18
> >>> 1/13/2018    46.4  19 1/13/2019    40.0  20 1/13/2020   121.
> >>>
> >>>
> >>>
> >>> Thank you very much for any help given.
> >>>
> >>>
> >>> --
> >>> *Roslinazairimah Zakaria*
> >>> *Tel: +609-5492370; Fax. No.+609-5492766*
> >>>
> >>> *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
> >>> roslinaump at gmail.com <roslinaump at gmail.com>*
> >>> Faculty of Industrial Sciences & Technology
> >>> University Malaysia Pahang
> >>> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
> >>>
> >>>          [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >
> Hello,
>
> Here are two solutions.
>
> 1. Base R
>
> Though I don't coerce the date column to class "Date", it seems to work.
>
>
> aggregate(EnergykWh ~ date, dt1, sum)
> #>        date EnergykWh
> #> 1 1/14/2016  11.98569
> #> 2 1/15/2016  32.56938
> #> 3 1/16/2016  21.29181
> #> 4 1/17/2016  22.88083
> #> 5 1/18/2016   9.05750
>
>
> 2. Package dplyr.
> First column date is coerced from class "character" to class "Date".
> Then the grouped sums are computed.
>
>
> suppressPackageStartupMessages(
>    library(dplyr)
> )
>
> dt1 %>%
>    mutate(date = as.Date(date, "%m/%d/%Y")) %>%
>    summarise(EnergykWh = sum(EnergykWh), .by = date)
> #>         date EnergykWh
> #> 1 2016-01-14  11.98569
> #> 2 2016-01-15  32.56938
> #> 3 2016-01-16  21.29181
> #> 4 2016-01-17  22.88083
> #> 5 2016-01-18   9.05750
>
>
> As you can see, the results are the same.
>
> Also, this exact problem is one of the most asked on StackOverflow.
> Maybe you could try searching there for a solution. My code above is
> also exactly the code in [1], though I had already this answer written.
> I only checked after :(.
>
>
> [1]
>
> https://stackoverflow.com/questions/61548758/r-how-sum-values-by-group-by-date
>
>
> Hope this helps,
>
> Rui Barradas
>
>
>
> --
> Este e-mail foi analisado pelo software antiv?rus AVG para verificar a
> presen?a de v?rus.
> www.avg.com
>


-- 
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From po|c1410 @end|ng |rom gm@||@com  Sun Nov  5 00:14:05 2023
From: po|c1410 @end|ng |rom gm@||@com (CALUM POLWART)
Date: Sat, 4 Nov 2023 23:14:05 +0000
Subject: [R] 
 I need to create new variables based on two numeric variables
 and one dichotomize conditional category variables.
In-Reply-To: <002d01da0e9a$5f1d2930$1d577b90$@gmail.com>
References: <mailman.370481.1.1699009201.56788.r-help@r-project.org>
 <LV3PR01MB86736581F1E69742E545FD59DCA5A@LV3PR01MB8673.prod.exchangelabs.com>
 <002d01da0e9a$5f1d2930$1d577b90$@gmail.com>
Message-ID: <CA+etgPmfSeWLs2t4Grjz98ypc4XeiSOGUPCBqeB=NR2JBxK0pg@mail.gmail.com>

I might have factored the gender.

I'm not sure it would in any way be quicker.  But might be to some extent
easier to develop variations of. And is sort of what factors should be
doing...

# make dummy data
gender <- c("Male", "Female", "Male", "Female")
WC <- c(70,60,75,65)
TG <- c(0.9, 1.1, 1.2, 1.0)
myDf <- data.frame( gender, WC, TG )

# label a factor
myDf$GF <- factor(myDf$gender, labels= c("Male"=65, "Female"=58))

# do the maths
myDf$LAP <- (myDf$WC - as.numeric(myDf$GF))* myDf$TG

#show results
head(myDf)

gender WC  TG GF  LAP
1   Male 70 0.9 58 61.2
2 Female 60 1.1 65 64.9
3   Male 75 1.2 58 87.6
4 Female 65 1.0 65 64.0


(Reality: I'd have probably used case_when in tidy to create a new numeric
column)





The equation to
> calculate LAP is different for male and females. I am giving both equations
> below.
>
> LAP for male = (WC-65)*TG
> LAP for female = (WC-58)*TG
>
> My question is 'how can I calculate the LAP and create a single new column?
>
>

	[[alternative HTML version deleted]]


From @vi@e@gross m@iii@g oii gm@ii@com  Sun Nov  5 05:27:16 2023
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Sun, 5 Nov 2023 00:27:16 -0400
Subject: [R] 
 I need to create new variables based on two numeric variables
 and one dichotomize conditional category variables.
In-Reply-To: <CA+etgPmfSeWLs2t4Grjz98ypc4XeiSOGUPCBqeB=NR2JBxK0pg@mail.gmail.com>
References: <mailman.370481.1.1699009201.56788.r-help@r-project.org>
 <LV3PR01MB86736581F1E69742E545FD59DCA5A@LV3PR01MB8673.prod.exchangelabs.com>
 <002d01da0e9a$5f1d2930$1d577b90$@gmail.com>
 <CA+etgPmfSeWLs2t4Grjz98ypc4XeiSOGUPCBqeB=NR2JBxK0pg@mail.gmail.com>
Message-ID: <006301da0fa0$5cbe99b0$163bcd10$@gmail.com>

There are many techniques Callum and yours is an interesting twist I had not considered. 
 
Yes, you can specify what integer a factor uses to represent things but not what I meant. Of course your trick does not work for some other forms of data like real numbers in double format. There is a cost to converting a column to a factor that is recouped best if it speeds things up multiple times.
 
The point I was making was that when you will be using group_by, especially if done many times, it might speed things up if the column is already a normal factor, perhaps just indexed from 1 onward. My guess is that underneath the covers, some programs implicitly do such a factor conversion if needed. An example might be aspects of the ggplot program where you may get a mysterious order of presentation in the graph unless you create a factor with the order you wish to have used and avoid it making one invisibly.
 
From: CALUM POLWART <polc1410 at gmail.com> 
Sent: Saturday, November 4, 2023 7:14 PM
To: avi.e.gross at gmail.com
Cc: Jorgen Harmse <JHarmse at roku.com>; r-help at r-project.org; mkzaman.m at gmail.com
Subject: Re: [R] I need to create new variables based on two numeric variables and one dichotomize conditional category variables.
 
I might have factored the gender.
 
I'm not sure it would in any way be quicker.  But might be to some extent easier to develop variations of. And is sort of what factors should be doing... 
 
# make dummy data
gender <- c("Male", "Female", "Male", "Female")
WC <- c(70,60,75,65)
TG <- c(0.9, 1.1, 1.2, 1.0)
myDf <- data.frame( gender, WC, TG )
 
# label a factor
myDf$GF <- factor(myDf$gender, labels= c("Male"=65, "Female"=58))
 
# do the maths
myDf$LAP <- (myDf$WC - as.numeric(myDf$GF))* myDf$TG
 
#show results
head(myDf)
 
gender WC  TG GF  LAP
1   Male 70 0.9 58 61.2
2 Female 60 1.1 65 64.9
3   Male 75 1.2 58 87.6
4 Female 65 1.0 65 64.0
 
 
(Reality: I'd have probably used case_when in tidy to create a new numeric column)
 
 
 
 
The equation to
calculate LAP is different for male and females. I am giving both equations
below.

LAP for male = (WC-65)*TG
LAP for female = (WC-58)*TG

My question is 'how can I calculate the LAP and create a single new column?

	[[alternative HTML version deleted]]


From po|c1410 @end|ng |rom gm@||@com  Sun Nov  5 09:27:17 2023
From: po|c1410 @end|ng |rom gm@||@com (CALUM POLWART)
Date: Sun, 5 Nov 2023 08:27:17 +0000
Subject: [R] 
 I need to create new variables based on two numeric variables
 and one dichotomize conditional category variables.
In-Reply-To: <006301da0fa0$5cbe99b0$163bcd10$@gmail.com>
References: <mailman.370481.1.1699009201.56788.r-help@r-project.org>
 <LV3PR01MB86736581F1E69742E545FD59DCA5A@LV3PR01MB8673.prod.exchangelabs.com>
 <002d01da0e9a$5f1d2930$1d577b90$@gmail.com>
 <CA+etgPmfSeWLs2t4Grjz98ypc4XeiSOGUPCBqeB=NR2JBxK0pg@mail.gmail.com>
 <006301da0fa0$5cbe99b0$163bcd10$@gmail.com>
Message-ID: <CA+etgPkANBQ5ndDfWP3nEEWk_3wd3JB-eeqvJDLwHRWREY4imQ@mail.gmail.com>

In this case I think I've made the labels the number and so a double is
possible. But it wasn't what I actually set out to do!  (I'm a tidy fab so
would have to engage the brain in base too much)

When I thought 'I think I might say that's a factor (sometimes in medical
equations gender is shown as a "F" and described as a "factor". Not our
factors... But still a factor).

My programming is never efficient!

But I was planning for a scenario for where gender is not a binary concept,
adding unknowns, trans etc. and if the data set is very large then
obviously storing as factors is less memory intensive, but processing may
be more intensive.








On Sun, 5 Nov 2023, 04:27 , <avi.e.gross at gmail.com> wrote:

> There are many techniques Callum and yours is an interesting twist I had
> not considered.
>
> Yes, you can specify what integer a factor uses to represent things but
> not what I meant. Of course your trick does not work for some other forms
> of data like real numbers in double format. There is a cost to converting a
> column to a factor that is recouped best if it speeds things up multiple
> times.
>
> The point I was making was that when you will be using group_by,
> especially if done many times, it might speed things up if the column is
> already a normal factor, perhaps just indexed from 1 onward. My guess is
> that underneath the covers, some programs implicitly do such a factor
> conversion if needed. An example might be aspects of the ggplot program
> where you may get a mysterious order of presentation in the graph unless
> you create a factor with the order you wish to have used and avoid it
> making one invisibly.
>
> From: CALUM POLWART <polc1410 at gmail.com>
> Sent: Saturday, November 4, 2023 7:14 PM
> To: avi.e.gross at gmail.com
> Cc: Jorgen Harmse <JHarmse at roku.com>; r-help at r-project.org;
> mkzaman.m at gmail.com
> Subject: Re: [R] I need to create new variables based on two numeric
> variables and one dichotomize conditional category variables.
>
> I might have factored the gender.
>
> I'm not sure it would in any way be quicker.  But might be to some extent
> easier to develop variations of. And is sort of what factors should be
> doing...
>
> # make dummy data
> gender <- c("Male", "Female", "Male", "Female")
> WC <- c(70,60,75,65)
> TG <- c(0.9, 1.1, 1.2, 1.0)
> myDf <- data.frame( gender, WC, TG )
>
> # label a factor
> myDf$GF <- factor(myDf$gender, labels= c("Male"=65, "Female"=58))
>
> # do the maths
> myDf$LAP <- (myDf$WC - as.numeric(myDf$GF))* myDf$TG
>
> #show results
> head(myDf)
>
> gender WC  TG GF  LAP
> 1   Male 70 0.9 58 61.2
> 2 Female 60 1.1 65 64.9
> 3   Male 75 1.2 58 87.6
> 4 Female 65 1.0 65 64.0
>
>
> (Reality: I'd have probably used case_when in tidy to create a new numeric
> column)
>
>
>
>
> The equation to
> calculate LAP is different for male and females. I am giving both equations
> below.
>
> LAP for male = (WC-65)*TG
> LAP for female = (WC-58)*TG
>
> My question is 'how can I calculate the LAP and create a single new column?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r@oknz @end|ng |rom gm@||@com  Mon Nov  6 00:45:01 2023
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Mon, 6 Nov 2023 12:45:01 +1300
Subject: [R] strptime with +03:00 zone designator
Message-ID: <CABcYAd+CTsSRuay3uE0S-k5k+uLvjZX=ZWuy2P6sCbFzkbFmtg@mail.gmail.com>

I have some data that includes timestamps like this:
2017-02-28T13:35:00+03:00
The documentation for strptime says that %z expects
an offset like 0300.  I don't see any way in the documentation
to get it to accept +hh:mm with a colon separator, and
everything I tried gave me NA as the answer.

Section 4.2.5.1 of ISO 8601:2004(E) allows both the
absence of colons in +hh[mm] (basic format) and the
presence of colons in +hh:mm (extended format).
Again in section 4.2.5.2 where a zone offset is combined
with a time of day: if you have hh:mm:ss you are using
extended format and the offset MUST have a colon; if
you have hhmmss you are using basic format and the
offset MUST NOT have a colon.  And again in section
4.3.2 (complete representations of date and time of day).
If you use hyphens and colons in the date and time part
you MUST have a colon in the zone designator.

So I am dealing with timestamps in strict ISO 8601
complete extended representation, and it is rather
frustrating that strptime doesn't deal with it simply.

The simplest thing would be for R's own version of
strptime to allow an optional colon between the hour
digits and the minute digits of a zone designator.

I'm about to clone the data source and edit it to
remove the colons, but is there something obvious
I am missing?


From roy@mende|@@ohn @end|ng |rom no@@@gov  Mon Nov  6 00:56:47 2023
From: roy@mende|@@ohn @end|ng |rom no@@@gov (Roy Mendelssohn - NOAA Federal)
Date: Sun, 5 Nov 2023 15:56:47 -0800
Subject: [R] strptime with +03:00 zone designator
In-Reply-To: <CABcYAd+CTsSRuay3uE0S-k5k+uLvjZX=ZWuy2P6sCbFzkbFmtg@mail.gmail.com>
References: <CABcYAd+CTsSRuay3uE0S-k5k+uLvjZX=ZWuy2P6sCbFzkbFmtg@mail.gmail.com>
Message-ID: <23A4C381-E483-4F6A-AAB6-86330E047CF1@noaa.gov>

what if you try lubridate::as_datetime('2017-02-28T13:35:00+03:00?)

-Roy


> On Nov 5, 2023, at 3:45 PM, Richard O'Keefe <raoknz at gmail.com> wrote:
> 
> I have some data that includes timestamps like this:
> 2017-02-28T13:35:00+03:00
> The documentation for strptime says that %z expects
> an offset like 0300.  I don't see any way in the documentation
> to get it to accept +hh:mm with a colon separator, and
> everything I tried gave me NA as the answer.
> 
> Section 4.2.5.1 of ISO 8601:2004(E) allows both the
> absence of colons in +hh[mm] (basic format) and the
> presence of colons in +hh:mm (extended format).
> Again in section 4.2.5.2 where a zone offset is combined
> with a time of day: if you have hh:mm:ss you are using
> extended format and the offset MUST have a colon; if
> you have hhmmss you are using basic format and the
> offset MUST NOT have a colon.  And again in section
> 4.3.2 (complete representations of date and time of day).
> If you use hyphens and colons in the date and time part
> you MUST have a colon in the zone designator.
> 
> So I am dealing with timestamps in strict ISO 8601
> complete extended representation, and it is rather
> frustrating that strptime doesn't deal with it simply.
> 
> The simplest thing would be for R's own version of
> strptime to allow an optional colon between the hour
> digits and the minute digits of a zone designator.
> 
> I'm about to clone the data source and edit it to
> remove the colons, but is there something obvious
> I am missing?
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Nov  6 01:07:38 2023
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sun, 05 Nov 2023 16:07:38 -0800
Subject: [R] strptime with +03:00 zone designator
In-Reply-To: <CABcYAd+CTsSRuay3uE0S-k5k+uLvjZX=ZWuy2P6sCbFzkbFmtg@mail.gmail.com>
References: <CABcYAd+CTsSRuay3uE0S-k5k+uLvjZX=ZWuy2P6sCbFzkbFmtg@mail.gmail.com>
Message-ID: <AB45F9E7-4802-473B-ACC9-F732A67A6B58@dcn.davis.ca.us>

I usually just use a regex to strip the colon.

On November 5, 2023 3:45:01 PM PST, Richard O'Keefe <raoknz at gmail.com> wrote:
>I have some data that includes timestamps like this:
>2017-02-28T13:35:00+03:00
>The documentation for strptime says that %z expects
>an offset like 0300.  I don't see any way in the documentation
>to get it to accept +hh:mm with a colon separator, and
>everything I tried gave me NA as the answer.
>
>Section 4.2.5.1 of ISO 8601:2004(E) allows both the
>absence of colons in +hh[mm] (basic format) and the
>presence of colons in +hh:mm (extended format).
>Again in section 4.2.5.2 where a zone offset is combined
>with a time of day: if you have hh:mm:ss you are using
>extended format and the offset MUST have a colon; if
>you have hhmmss you are using basic format and the
>offset MUST NOT have a colon.  And again in section
>4.3.2 (complete representations of date and time of day).
>If you use hyphens and colons in the date and time part
>you MUST have a colon in the zone designator.
>
>So I am dealing with timestamps in strict ISO 8601
>complete extended representation, and it is rather
>frustrating that strptime doesn't deal with it simply.
>
>The simplest thing would be for R's own version of
>strptime to allow an optional colon between the hour
>digits and the minute digits of a zone designator.
>
>I'm about to clone the data source and edit it to
>remove the colons, but is there something obvious
>I am missing?
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From jho|tm@n @end|ng |rom gm@||@com  Mon Nov  6 01:18:00 2023
From: jho|tm@n @end|ng |rom gm@||@com (jim holtman)
Date: Sun, 5 Nov 2023 16:18:00 -0800
Subject: [R] strptime with +03:00 zone designator
In-Reply-To: <CABcYAd+CTsSRuay3uE0S-k5k+uLvjZX=ZWuy2P6sCbFzkbFmtg@mail.gmail.com>
References: <CABcYAd+CTsSRuay3uE0S-k5k+uLvjZX=ZWuy2P6sCbFzkbFmtg@mail.gmail.com>
Message-ID: <CAAxdm-666SwXzBgKJEn-HWphspbWDb2ycyVzdAbvD+oX16xRVA@mail.gmail.com>

try using 'lubridate'

> library(lubridate)Attaching package: ?lubridate?

The following objects are masked from ?package:base?:

    date, intersect, setdiff, union
> x <- "2017-02-28T13:35:00+03:00"> ymd_hms(x)[1] "2017-02-28 10:35:00 UTC"

>



Thanks

Jim Holtman
*Data Munger Guru*


*What is the problem that you are trying to solve?Tell me what you want to
do, not how you want to do it.*


On Sun, Nov 5, 2023 at 3:45?PM Richard O'Keefe <raoknz at gmail.com> wrote:

> I have some data that includes timestamps like this:
> 2017-02-28T13:35:00+03:00
> The documentation for strptime says that %z expects
> an offset like 0300.  I don't see any way in the documentation
> to get it to accept +hh:mm with a colon separator, and
> everything I tried gave me NA as the answer.
>
> Section 4.2.5.1 of ISO 8601:2004(E) allows both the
> absence of colons in +hh[mm] (basic format) and the
> presence of colons in +hh:mm (extended format).
> Again in section 4.2.5.2 where a zone offset is combined
> with a time of day: if you have hh:mm:ss you are using
> extended format and the offset MUST have a colon; if
> you have hhmmss you are using basic format and the
> offset MUST NOT have a colon.  And again in section
> 4.3.2 (complete representations of date and time of day).
> If you use hyphens and colons in the date and time part
> you MUST have a colon in the zone designator.
>
> So I am dealing with timestamps in strict ISO 8601
> complete extended representation, and it is rather
> frustrating that strptime doesn't deal with it simply.
>
> The simplest thing would be for R's own version of
> strptime to allow an optional colon between the hour
> digits and the minute digits of a zone designator.
>
> I'm about to clone the data source and edit it to
> remove the colons, but is there something obvious
> I am missing?
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r@oknz @end|ng |rom gm@||@com  Mon Nov  6 06:37:12 2023
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Mon, 6 Nov 2023 18:37:12 +1300
Subject: [R] strptime with +03:00 zone designator
In-Reply-To: <CAAxdm-666SwXzBgKJEn-HWphspbWDb2ycyVzdAbvD+oX16xRVA@mail.gmail.com>
References: <CABcYAd+CTsSRuay3uE0S-k5k+uLvjZX=ZWuy2P6sCbFzkbFmtg@mail.gmail.com>
 <CAAxdm-666SwXzBgKJEn-HWphspbWDb2ycyVzdAbvD+oX16xRVA@mail.gmail.com>
Message-ID: <CABcYAdJpMibwW7Qjm_0A2D+1H=ddvow6S12RaKRUx-frR210tg@mail.gmail.com>

OK, so the consensus is
(1) One cannot make strptime accept ISO8601-compliant zone designators
(2) The lubridate package can
(3) Or one can hack away with regex.
Lubridate it is, then.

But I do regard strptime's inability to process ISO8601-compliant zone
designators as a bug.


On Mon, 6 Nov 2023 at 13:18, jim holtman <jholtman at gmail.com> wrote:

> try using 'lubridate'
>
> > library(lubridate)Attaching package: ?lubridate?
>
> The following objects are masked from ?package:base?:
>
>     date, intersect, setdiff, union
> > x <- "2017-02-28T13:35:00+03:00"> ymd_hms(x)[1] "2017-02-28 10:35:00 UTC"
>
> >
>
>
>
> Thanks
>
> Jim Holtman
> *Data Munger Guru*
>
>
> *What is the problem that you are trying to solve?Tell me what you want to
> do, not how you want to do it.*
>
>
> On Sun, Nov 5, 2023 at 3:45?PM Richard O'Keefe <raoknz at gmail.com> wrote:
>
>> I have some data that includes timestamps like this:
>> 2017-02-28T13:35:00+03:00
>> The documentation for strptime says that %z expects
>> an offset like 0300.  I don't see any way in the documentation
>> to get it to accept +hh:mm with a colon separator, and
>> everything I tried gave me NA as the answer.
>>
>> Section 4.2.5.1 of ISO 8601:2004(E) allows both the
>> absence of colons in +hh[mm] (basic format) and the
>> presence of colons in +hh:mm (extended format).
>> Again in section 4.2.5.2 where a zone offset is combined
>> with a time of day: if you have hh:mm:ss you are using
>> extended format and the offset MUST have a colon; if
>> you have hhmmss you are using basic format and the
>> offset MUST NOT have a colon.  And again in section
>> 4.3.2 (complete representations of date and time of day).
>> If you use hyphens and colons in the date and time part
>> you MUST have a colon in the zone designator.
>>
>> So I am dealing with timestamps in strict ISO 8601
>> complete extended representation, and it is rather
>> frustrating that strptime doesn't deal with it simply.
>>
>> The simplest thing would be for R's own version of
>> strptime to allow an optional colon between the hour
>> digits and the minute digits of a zone designator.
>>
>> I'm about to clone the data source and edit it to
>> remove the colons, but is there something obvious
>> I am missing?
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From r@oknz @end|ng |rom gm@||@com  Mon Nov  6 06:37:34 2023
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Mon, 6 Nov 2023 18:37:34 +1300
Subject: [R] strptime with +03:00 zone designator
In-Reply-To: <CABcYAdJpMibwW7Qjm_0A2D+1H=ddvow6S12RaKRUx-frR210tg@mail.gmail.com>
References: <CABcYAd+CTsSRuay3uE0S-k5k+uLvjZX=ZWuy2P6sCbFzkbFmtg@mail.gmail.com>
 <CAAxdm-666SwXzBgKJEn-HWphspbWDb2ycyVzdAbvD+oX16xRVA@mail.gmail.com>
 <CABcYAdJpMibwW7Qjm_0A2D+1H=ddvow6S12RaKRUx-frR210tg@mail.gmail.com>
Message-ID: <CABcYAd+y6v7YTOtE0QrDciNNSpGZb22MFqsWA=xB+e1-q-QcLg@mail.gmail.com>

Thanks to all who replied.

On Mon, 6 Nov 2023 at 18:37, Richard O'Keefe <raoknz at gmail.com> wrote:

> OK, so the consensus is
> (1) One cannot make strptime accept ISO8601-compliant zone designators
> (2) The lubridate package can
> (3) Or one can hack away with regex.
> Lubridate it is, then.
>
> But I do regard strptime's inability to process ISO8601-compliant zone
> designators as a bug.
>
>
> On Mon, 6 Nov 2023 at 13:18, jim holtman <jholtman at gmail.com> wrote:
>
>> try using 'lubridate'
>>
>> > library(lubridate)Attaching package: ?lubridate?
>>
>> The following objects are masked from ?package:base?:
>>
>>     date, intersect, setdiff, union
>> > x <- "2017-02-28T13:35:00+03:00"> ymd_hms(x)[1] "2017-02-28 10:35:00 UTC"
>>
>> >
>>
>>
>>
>> Thanks
>>
>> Jim Holtman
>> *Data Munger Guru*
>>
>>
>> *What is the problem that you are trying to solve?Tell me what you want
>> to do, not how you want to do it.*
>>
>>
>> On Sun, Nov 5, 2023 at 3:45?PM Richard O'Keefe <raoknz at gmail.com> wrote:
>>
>>> I have some data that includes timestamps like this:
>>> 2017-02-28T13:35:00+03:00
>>> The documentation for strptime says that %z expects
>>> an offset like 0300.  I don't see any way in the documentation
>>> to get it to accept +hh:mm with a colon separator, and
>>> everything I tried gave me NA as the answer.
>>>
>>> Section 4.2.5.1 of ISO 8601:2004(E) allows both the
>>> absence of colons in +hh[mm] (basic format) and the
>>> presence of colons in +hh:mm (extended format).
>>> Again in section 4.2.5.2 where a zone offset is combined
>>> with a time of day: if you have hh:mm:ss you are using
>>> extended format and the offset MUST have a colon; if
>>> you have hhmmss you are using basic format and the
>>> offset MUST NOT have a colon.  And again in section
>>> 4.3.2 (complete representations of date and time of day).
>>> If you use hyphens and colons in the date and time part
>>> you MUST have a colon in the zone designator.
>>>
>>> So I am dealing with timestamps in strict ISO 8601
>>> complete extended representation, and it is rather
>>> frustrating that strptime doesn't deal with it simply.
>>>
>>> The simplest thing would be for R's own version of
>>> strptime to allow an optional colon between the hour
>>> digits and the minute digits of a zone designator.
>>>
>>> I'm about to clone the data source and edit it to
>>> remove the colons, but is there something obvious
>>> I am missing?
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>

	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Mon Nov  6 10:01:53 2023
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Mon, 6 Nov 2023 10:01:53 +0100
Subject: [R] strptime with +03:00 zone designator
In-Reply-To: <CABcYAd+y6v7YTOtE0QrDciNNSpGZb22MFqsWA=xB+e1-q-QcLg@mail.gmail.com>
References: <CABcYAd+CTsSRuay3uE0S-k5k+uLvjZX=ZWuy2P6sCbFzkbFmtg@mail.gmail.com>
 <CAAxdm-666SwXzBgKJEn-HWphspbWDb2ycyVzdAbvD+oX16xRVA@mail.gmail.com>
 <CABcYAdJpMibwW7Qjm_0A2D+1H=ddvow6S12RaKRUx-frR210tg@mail.gmail.com>
 <CABcYAd+y6v7YTOtE0QrDciNNSpGZb22MFqsWA=xB+e1-q-QcLg@mail.gmail.com>
Message-ID: <25928.43905.917277.897930@stat.math.ethz.ch>

>>>>> Richard O'Keefe 
>>>>>     on Mon, 6 Nov 2023 18:37:34 +1300 writes:

    > Thanks to all who replied.  On Mon, 6 Nov 2023 at 18:37,
    > Richard O'Keefe <raoknz at gmail.com> wrote:

    >> OK, so the consensus is (1) One cannot make strptime
    >> accept ISO8601-compliant zone designators (2) The
    >> lubridate package can (3) Or one can hack away with
    >> regex.  Lubridate it is, then.
    >> 
    >> But I do regard strptime's inability to process
    >> ISO8601-compliant zone designators as a bug.

Did you try to submit it to R's bugzilla?

It's the first time I hear of this "Feature" of the ISO
standard, but then I'm not at all a timezone, and even less an
ISO standard expert.

Best,
Martin


    >> On Mon, 6 Nov 2023 at 13:18, jim holtman
    >> <jholtman at gmail.com> wrote:
    >> 
    >>> try using 'lubridate'
    >>> 
    >>> > library(lubridate)Attaching package: ?lubridate?
    >>> 
    >>> The following objects are masked from ?package:base?:
    >>> 
    >>> date, intersect, setdiff, union > x <-
    >>> "2017-02-28T13:35:00+03:00"> ymd_hms(x)[1] "2017-02-28
    >>> 10:35:00 UTC"
    >>> 
    >>> >
    >>> 
    >>> 
    >>> 
    >>> Thanks
    >>> 
    >>> Jim Holtman *Data Munger Guru*
    >>> 
    >>> 
    >>> *What is the problem that you are trying to solve?Tell
    >>> me what you want to do, not how you want to do it.*
    >>> 
    >>> 
    >>> On Sun, Nov 5, 2023 at 3:45?PM Richard O'Keefe
    >>> <raoknz at gmail.com> wrote:
    >>> 
    >>>> I have some data that includes timestamps like this:
    >>>> 2017-02-28T13:35:00+03:00 The documentation for
    >>>> strptime says that %z expects an offset like 0300.  I
    >>>> don't see any way in the documentation to get it to
    >>>> accept +hh:mm with a colon separator, and everything I
    >>>> tried gave me NA as the answer.
    >>>> 
    >>>> Section 4.2.5.1 of ISO 8601:2004(E) allows both the
    >>>> absence of colons in +hh[mm] (basic format) and the
    >>>> presence of colons in +hh:mm (extended format).  Again
    >>>> in section 4.2.5.2 where a zone offset is combined with
    >>>> a time of day: if you have hh:mm:ss you are using
    >>>> extended format and the offset MUST have a colon; if
    >>>> you have hhmmss you are using basic format and the
    >>>> offset MUST NOT have a colon.  And again in section
    >>>> 4.3.2 (complete representations of date and time of
    >>>> day).  If you use hyphens and colons in the date and
    >>>> time part you MUST have a colon in the zone designator.
    >>>> 
    >>>> So I am dealing with timestamps in strict ISO 8601
    >>>> complete extended representation, and it is rather
    >>>> frustrating that strptime doesn't deal with it simply.
    >>>> 
    >>>> The simplest thing would be for R's own version of
    >>>> strptime to allow an optional colon between the hour
    >>>> digits and the minute digits of a zone designator.
    >>>> 
    >>>> I'm about to clone the data source and edit it to
    >>>> remove the colons, but is there something obvious I am
    >>>> missing?
    >>>> 
    >>>> ______________________________________________
    >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    >>>> more, see https://stat.ethz.ch/mailman/listinfo/r-help
    >>>> PLEASE do read the posting guide
    >>>> http://www.R-project.org/posting-guide.html and provide
    >>>> commented, minimal, self-contained, reproducible code.
    >>>> 
    >>> 

    > 	[[alternative HTML version deleted]]

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    > more, see https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide
    > http://www.R-project.org/posting-guide.html and provide
    > commented, minimal, self-contained, reproducible code.


From th||eu @end|ng |rom @tudent@ethz@ch  Sun Nov  5 14:35:39 2023
From: th||eu @end|ng |rom @tudent@ethz@ch (Leu  Thierry)
Date: Sun, 5 Nov 2023 13:35:39 +0000
Subject: [R] Cryptic error for mscmt function
Message-ID: <33ec052e26e042688f68e59e6f80fd99@student.ethz.ch>

Hi everyone,


I am trying to conduct a synthetic control analysis using the MSCMT package. However, when trying to run it I get a very cryptic error message saying  "Error in lst[[nam]][intersect(tim, rownames(lst[[nam]])), cols, drop = FALSE]: subscript out of bounds". Does anyone know what this means and why I receive this error? I attached the code & dataset used in the attachment. Thanks a lot!


Best regards

Thierry


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon Nov  6 14:53:14 2023
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 6 Nov 2023 13:53:14 +0000
Subject: [R] Cryptic error for mscmt function
In-Reply-To: <33ec052e26e042688f68e59e6f80fd99@student.ethz.ch>
References: <33ec052e26e042688f68e59e6f80fd99@student.ethz.ch>
Message-ID: <72a74b31-c95b-4a07-ac36-824bb4d738d6@sapo.pt>

?s 13:35 de 05/11/2023, Leu Thierry escreveu:
> Hi everyone,
> 
> 
> I am trying to conduct a synthetic control analysis using the MSCMT package. However, when trying to run it I get a very cryptic error message saying  "Error in lst[[nam]][intersect(tim, rownames(lst[[nam]])), cols, drop = FALSE]: subscript out of bounds". Does anyone know what this means and why I receive this error? I attached the code & dataset used in the attachment. Thanks a lot!
> 
> 
> Best regards
> 
> Thierry
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
Hello,

No attachment came through the filters, can you resend in plain text or 
if it was a .R file, rename it .txt?

See [1], section General Instructions for more on this

[1] https://www.r-project.org/mail.html#instructions

Hope this helps,

Rui Barradas

-- 
Este e-mail foi analisado pelo software antiv?rus AVG para verificar a presen?a de v?rus.
www.avg.com


From kry|ov@r00t @end|ng |rom gm@||@com  Mon Nov  6 15:08:36 2023
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Mon, 6 Nov 2023 17:08:36 +0300
Subject: [R] Cryptic error for mscmt function
In-Reply-To: <33ec052e26e042688f68e59e6f80fd99@student.ethz.ch>
References: <33ec052e26e042688f68e59e6f80fd99@student.ethz.ch>
Message-ID: <20231106170836.18b083d0@Tarkus>

? Sun, 5 Nov 2023 13:35:39 +0000
"Leu  Thierry" <thileu at student.ethz.ch> ?????:

> However, when trying to run it I get a very cryptic error message
> saying  "Error in lst[[nam]][intersect(tim, rownames(lst[[nam]])),
> cols, drop = FALSE]: subscript out of bounds".

Without a way to reproduce the error, I can offer a few bits of generic
advice:

1. Use traceback() to find out where the error happens. You can then
type the name of the function at the R prompt to read its source code
(although most likely without the comments).

2. Even better, set options(error = recover) before running your code
and have the debugger launched at the point where the error happens.
Use the debugger (see help(browser) to look at every variable and find
out why indeed lst[[nam]] doesn't seem to contain rows with names
intersect(tim, rownames(lst[[nam]])) and/or columns with names `cols`.

3. See the free book The R Inferno
<https://www.burns-stat.com/documents/books/the-r-inferno/> for more
advice on debugging R code.

-- 
Best regards,
Ivan


From JH@rm@e @end|ng |rom roku@com  Mon Nov  6 17:22:32 2023
From: JH@rm@e @end|ng |rom roku@com (Jorgen Harmse)
Date: Mon, 6 Nov 2023 16:22:32 +0000
Subject: [R] [EXTERNAL] Re: I need to create new variables based on two
 numeric variables and one dichotomize conditional category variables.
In-Reply-To: <CA+etgPmfSeWLs2t4Grjz98ypc4XeiSOGUPCBqeB=NR2JBxK0pg@mail.gmail.com>
References: <mailman.370481.1.1699009201.56788.r-help@r-project.org>
 <LV3PR01MB86736581F1E69742E545FD59DCA5A@LV3PR01MB8673.prod.exchangelabs.com>
 <002d01da0e9a$5f1d2930$1d577b90$@gmail.com>
 <CA+etgPmfSeWLs2t4Grjz98ypc4XeiSOGUPCBqeB=NR2JBxK0pg@mail.gmail.com>
Message-ID: <LV3PR01MB86730118F2D4F630A8E7FA97DCAAA@LV3PR01MB8673.prod.exchangelabs.com>

That?s ingenious, but I would hesitate to rely on a specific mapping between strings and integers. (I usually read data frames with stringsAsFactors=FALSE or coerce to character later: I don?t think it takes more memory.) Maybe create another column with the coefficients. What if gender is part of another formula?

Regards,
Jorgen Harmse.

From: CALUM POLWART <polc1410 at gmail.com>
Date: Saturday, November 4, 2023 at 18:23
To: avi.e.gross at gmail.com <avi.e.gross at gmail.com>
Cc: Jorgen Harmse <JHarmse at roku.com>, r-help at r-project.org <r-help at r-project.org>, mkzaman.m at gmail.com <mkzaman.m at gmail.com>
Subject: [EXTERNAL] Re: [R] I need to create new variables based on two numeric variables and one dichotomize conditional category variables.
I might have factored the gender.

I'm not sure it would in any way be quicker.  But might be to some extent easier to develop variations of. And is sort of what factors should be doing...

# make dummy data
gender <- c("Male", "Female", "Male", "Female")
WC <- c(70,60,75,65)
TG <- c(0.9, 1.1, 1.2, 1.0)
myDf <- data.frame( gender, WC, TG )

# label a factor
myDf$GF <- factor(myDf$gender, labels= c("Male"=65, "Female"=58))

# do the maths
myDf$LAP <- (myDf$WC - as.numeric(myDf$GF))* myDf$TG

#show results
head(myDf)

gender WC  TG GF  LAP
1   Male 70 0.9 58 61.2
2 Female 60 1.1 65 64.9
3   Male 75 1.2 58 87.6
4 Female 65 1.0 65 64.0


(Reality: I'd have probably used case_when in tidy to create a new numeric column)




The equation to
calculate LAP is different for male and females. I am giving both equations
below.

LAP for male = (WC-65)*TG
LAP for female = (WC-58)*TG

My question is 'how can I calculate the LAP and create a single new column?

	[[alternative HTML version deleted]]


From JH@rm@e @end|ng |rom roku@com  Mon Nov  6 17:52:14 2023
From: JH@rm@e @end|ng |rom roku@com (Jorgen Harmse)
Date: Mon, 6 Nov 2023 16:52:14 +0000
Subject: [R] 
 I need to create new variables based on two numeric variables
 and one dichotomize conditional category
In-Reply-To: <mailman.370492.1.1699095602.29524.r-help@r-project.org>
References: <mailman.370492.1.1699095602.29524.r-help@r-project.org>
Message-ID: <LV3PR01MB8673934678078681ACBD4646DCAAA@LV3PR01MB8673.prod.exchangelabs.com>

Avi: Thank you for checking. I think the optimization is limited. If test is all TRUE or all FALSE then at most one vector is evaluated. Anything beyond that would be very complicated. (Inspect the two expressions and verify that both specify elementwise computations. Then use indexing to shrink the input properly. Take into account all recycling rules for binary operations.)


> ifelse(0:1, log(-1:0), 1:2)

Warning in log(-1:0) : NaNs produced

[1]    1 -Inf

> ifelse(c(FALSE,FALSE), log(-1:0), 1:2)

[1] 1 2

I agree that nested ifelse is cumbersome. I wrote a function to address that:


#' Nested conditional element selection

#'

#' \code{ifelses(test1,yes1,test2,yes2,....,no)} is shorthand for

#' \code{ifelse(test1,yes1,ifelse(test2,yes2,....,no....))}. The inputs should

#' not be named.

#'

#' @param test1 usually \code{test} for the outer call to \code{\link{ifelse}}

#' @param yes1 \code{yes} for the outer call to \code{ifelse}

#' @param ... usually the \code{(test,yes)} for nested calls followed by \code{no}

#' for the innermost call to \code{ifelse}

#'

#' @note There must be an odd number of inputs. If there is exactly one input then it is

#' returned (unless it is named \code{yes1}): this supports the recursive implementation.

#'

#' @return a vector with entries from \code{yes1} where \code{test1} is \code{TRUE}, else from

#' \code{yes2} where \code{test2} is \code{TRUE}, ..., and from \code{no} where none of

#' the conditions holds

#'

#' @export



ifelses <- function(test1,yes1,...)

{ if (missing(test1))

  { if (!missing(yes1) || length(L <- list(...)) != 1L)

      stop("Wrong number of arguments or confusing argument names.")

    return(L[[1L]])

  }

  if (missing(yes1))

  { if (length(L <- list(...)) != 0L)

      stop("Wrong number of arguments or confusing argument names.")

    return(test1)

  }

  return( ifelse(test1, yes1, ifelses(...)) )

}

Regards,
Jorgen Harmse (not Jordan).

------------------------------

Message: 10
Date: Sat, 4 Nov 2023 01:08:03 -0400
From: <avi.e.gross at gmail.com>
To: "'Jorgen Harmse'" <JHarmse at roku.com>
Cc: <r-help at r-project.org>
Subject: Re: [R] [EXTERNAL] RE: I need to create new variables based
        on two numeric variables and one dichotomize conditional category
        variables.
Message-ID: <019a01da0edc$e41c39e0$ac54ada0$@gmail.com>
Content-Type: text/plain; charset="utf-8"

To be fair, Jordan, I think R has some optimizations so that the arguments
in some cases are NOT evaluated until needed. So only one or the other
choice ever gets evaluated for each row. My suggestion merely has
typographic implications and some aspects of clarity and minor amounts of
less memory and parsing needed.

But ifelse() is currently implemented somewhat too complexly for my taste.
Just type "ifelse" at the prompt and you will see many lines of code that
handle various scenarios.

?

If you later want to add categories such as ?transgender? with a value of 61 or have other numbers for groups like ?Hispanic male?, you can amend the instructions as long as you put your conditions in an order so that they are tried until one of them matches, or it takes the default. Yes, in a sense the above is doable using a deeply nested ifelse() but easier for me to read and write and evaluate. It may not be more efficient or may be as some of dplyr is compiled code.






	[[alternative HTML version deleted]]


From tr|ng @end|ng |rom gvdnet@dk  Mon Nov  6 17:53:49 2023
From: tr|ng @end|ng |rom gvdnet@dk (Troels Ring)
Date: Mon, 6 Nov 2023 17:53:49 +0100
Subject: [R] non-linear regression and root finding
Message-ID: <eb487c2a-4b9d-4000-af86-f99de576d106@gvdnet.dk>

Dear friends - I have a function for the charge in a fluid (water) 
buffered with HEPES and otherwise only containing Na and Cl so that [Na] 
- [Cl] = SID (strong ion difference) goes from -1 mM to 1 mM. With known 
SID and total HEPES concentration I can calculate accurately the pH if I 
know 3 pK values for HEPES by finding the single root with uniroot

Now, the problem is that there is some disagreement in the literature 
what is the correct value for the 3 pKs. I know the 3 pK values have the 
relationship pK1 < pK2 <- pK3 and for the most common formulation of 
HEPES I know the charge on fully protonated species is 2. Hence I can 
generate a huge number of pK values from uniform distribution by taking 
sort(runif(3,-1,9) and make sure there are no ties and then run all 
those triplets of pK values to find pH values and thereby find the 
lowest deviation vis a vis measured values. That works but requires many 
triplets and is not satisfying. Hence I wonder if I could somehow have 
non linear regression to find the 3 pK values. Below is HEPESFUNC which 
delivers charge in the fluid for known pKs, HEPTOT and SID. Is it 
possible to have root-finding in the formula with nls? (I know the 
precision asked for is extreme but it has worked well in really many 
applications).

All best wishes

Troels Ring, MD
Aalborg, Denmark


HEPESFUNC <-
 ? function(H,SID,HEPTOT,pK1,pK2,pK3) {
 ??? XX <- (H^3/(10^-pK3*10^-pK2*10^-pK1)+H^2/(10^-pK3*10^-pK2)+H/(10^-pK3))
 ??? IV <- HEPTOT/(1+XX)
 ??? I <- IV*H^3/(10^-pK3*10^-pK2*10^-pK1)
 ??? II <- IV*H^2/(10^-pK3*10^-pK2)
 ??? III <- IV*H/10^-pK3
 ??? H - kw/H + SID + I*2 + II - IV
 ? }


HEPTOT <- 0.050
SID <- c(-seq(10,1)*1e-4,0,seq(1,10)*1e-4)

pHobs? <- c(4.63,4.68,4.72,4.77,4.83,4.9,4.96,5.04,5.12,5.21,5.3,
 ??????????? 5.39,5.48,5.55,5.63,5.69,5.74,5.8,5.85,5.89,5.93)

pK1 <- -1; pK2 <- 3; pK3 <- 7.55 # literature values
pK3 <- 7.6; pK2 <- 2.96 # values eye-balled to be better

pH <- c()
for (i in 1:length(SID)) {
 ? pH[i] <- -log10(uniroot(HEPESFUNC,c(1e-20,1),tol=1e-20,maxiter=1e4,
 ?????????????????????? HEPTOT=HEPTOT,SID = SID[i],
 ?????????????????????? pK1=pK1,pK2=pK2,pK3=pK3)$root)
}

plot(SID,pH)
points(SID,pHobs,col="red")


From jo@h@m@u|r|ch @end|ng |rom gm@||@com  Mon Nov  6 18:07:22 2023
From: jo@h@m@u|r|ch @end|ng |rom gm@||@com (Joshua Ulrich)
Date: Mon, 6 Nov 2023 11:07:22 -0600
Subject: [R] strptime with +03:00 zone designator
In-Reply-To: <25928.43905.917277.897930@stat.math.ethz.ch>
References: <CABcYAd+CTsSRuay3uE0S-k5k+uLvjZX=ZWuy2P6sCbFzkbFmtg@mail.gmail.com>
 <CAAxdm-666SwXzBgKJEn-HWphspbWDb2ycyVzdAbvD+oX16xRVA@mail.gmail.com>
 <CABcYAdJpMibwW7Qjm_0A2D+1H=ddvow6S12RaKRUx-frR210tg@mail.gmail.com>
 <CABcYAd+y6v7YTOtE0QrDciNNSpGZb22MFqsWA=xB+e1-q-QcLg@mail.gmail.com>
 <25928.43905.917277.897930@stat.math.ethz.ch>
Message-ID: <CAPPM_gQejVfnPjeCF7cUw9b22mHsnHcMh90rk9K9=ciY+dUpHg@mail.gmail.com>

On Mon, Nov 6, 2023 at 3:02?AM Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
>
> >>>>> Richard O'Keefe
> >>>>>     on Mon, 6 Nov 2023 18:37:34 +1300 writes:
>
>     > Thanks to all who replied.  On Mon, 6 Nov 2023 at 18:37,
>     > Richard O'Keefe <raoknz at gmail.com> wrote:
>
>     >> OK, so the consensus is (1) One cannot make strptime
>     >> accept ISO8601-compliant zone designators (2) The
>     >> lubridate package can (3) Or one can hack away with
>     >> regex.  Lubridate it is, then.
>     >>
>     >> But I do regard strptime's inability to process
>     >> ISO8601-compliant zone designators as a bug.
>
> Did you try to submit it to R's bugzilla?
>
> It's the first time I hear of this "Feature" of the ISO
> standard, but then I'm not at all a timezone, and even less an
> ISO standard expert.
>
FWIW, the timezone offset format (%z) is handled in
src/main/Rstrptime.h around line 987. There's a comment that only the
RFC 822 form is recognized (+/-HHMM). The fix may be as simple as
ignoring the ':' in the while() loop.

> Best,
> Martin

>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From kry|ov@r00t @end|ng |rom gm@||@com  Mon Nov  6 19:19:02 2023
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Mon, 6 Nov 2023 21:19:02 +0300
Subject: [R] non-linear regression and root finding
In-Reply-To: <eb487c2a-4b9d-4000-af86-f99de576d106@gvdnet.dk>
References: <eb487c2a-4b9d-4000-af86-f99de576d106@gvdnet.dk>
Message-ID: <20231106211902.375d3b0a@Tarkus>

? Mon, 6 Nov 2023 17:53:49 +0100
Troels Ring <tring at gvdnet.dk> ?????:

> Hence I wonder if I could somehow have non linear regression to find
> the 3 pK values. Below is HEPESFUNC which delivers charge in the
> fluid for known pKs, HEPTOT and SID. Is it possible to have
> root-finding in the formula with nls?

Sure. Just reformulate the problem in terms of a function that takes a
vector of predictors (your independent variable SID) and the desired
parameters (pK1, pK2, pK3) as separate arguments and then returns
predicted values of the dependent variable (to compare against pHobs):

kw <- 1e-14 # I'm assuming
pHm <- Vectorize(function(SID, pK1, pK2, pK3)
 -log10(uniroot(HEPESFUNC,c(1e-20,1),tol=1e-20,maxiter=1e4,
        HEPTOT=HEPTOT,SID = SID, pK1=pK1,pK2=pK2,pK3=pK3)$root))

(Yes, Vectorize() doesn't make the function any faster, but I don't see
an easy way to rewrite this function to make it truly vectorised.)

Unfortunately, nls() seems to take a step somewhere where crossprod()
of the Jacobian of the model function cannot be inverted and fails with
the message "Singular gradient". I wish that R could have a more
reliable built-in nonlinear least squares solver. (I could also be
holding it wrong.) Meanwhile, we have excellent CRAN packages nlsr and
minpack.lm:

minpack.lm::nlsLM(
 pHobs ~ pHm(SID, pK1, pK2, pK3),
 data.frame(pHobs = pHobs, SID = SID),
 start = c(pK1 = pK1, pK2 = pK2, pK3 = pK3),
 # the following is also needed to avoid MINPACK failing to fit
 lower = rep(-1, 3), upper = rep(9, 3)
)
# Nonlinear regression model
#   model: pHobs ~ pHm(SID, pK1, pK2, pK3)
#    data: data.frame(pHobs = pHobs, SID = SID)
#    pK1     pK2    pK3
# -1.000   2.966  7.606
#  residual sum-of-squares: 0.001195
#
# Number of iterations to convergence: 15 
# Achieved convergence tolerance: 1.49e-08

(Unfortunately, your code seemed to have a lot of non-breakable spaces,
which confuse R's parser and make it harder to copy&paste it.)

I think that your function can also be presented as a degree-5
polynomial in H, so it should also be possible to use polyroot() to
obtain your solutions in a more exact manner.

-- 
Best regards,
Ivan


From pro|jcn@@h @end|ng |rom gm@||@com  Mon Nov  6 19:36:21 2023
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Mon, 6 Nov 2023 13:36:21 -0500
Subject: [R] non-linear regression and root finding
In-Reply-To: <eb487c2a-4b9d-4000-af86-f99de576d106@gvdnet.dk>
References: <eb487c2a-4b9d-4000-af86-f99de576d106@gvdnet.dk>
Message-ID: <87c26e00-7bd0-4929-b866-9fc0e6c6a5ba@gmail.com>

Your script is missing something (in particular kw).

I presume you are trying to estimate the pK values. You may have more success
with package nlsr than nls(). nlsr::nlxb() tries to get the Jacobian of the
model specified by a formula and do so by applying symbolic or automatic
differentiation. The multi expression function would probably not work.
(That is one advantage of nls(), but it has an unstabilized solver and unless
carefully called will use a simple forward derivative approximation.)
There is also nlsr::nlfb() that can use functions for the residuals and
jacobian. Your function is messy, but likely the jacobian can be developed
with a bit of work.

Some of the symbolic derivative features of R can help here. Alternatively,
there are several numerical approximations in nlsr and the vignette "Intro
to nlsr" explains how to use these. Note that simple forward and backward
approximations are generally not worth using, but central approximation is
quite good.

The solver in the nlsr package allows of bounds on the parameters. Since you
want them ordered, you may want to transform

    pK1 < pK2 <- pK3

to  pk1, deltapk2, deltapk3 so pK2 = pk1+deltapk2
and pk3 = pk2 + deltapk3 = pk1 + deltapk2 + deltapk3
and all values bounded below by 0 or possibly some small numbers to
keep the paramters apart.

I won't pretend any of this is trivial. It is VERY easy to make small errors
that still allow for output that is disastrously incorrect. If it is possible
to get a single "formula" as one expression even if spread over multiple lines,
then nlxb() might be able to handle it.


J Nash (maintainer of nlsr and optimx)

On 2023-11-06 11:53, Troels Ring wrote:

....
> HEPESFUNC <-
>  ? function(H,SID,HEPTOT,pK1,pK2,pK3) {
>  ??? XX <- (H^3/(10^-pK3*10^-pK2*10^-pK1)+H^2/(10^-pK3*10^-pK2)+H/(10^-pK3))
>  ??? IV <- HEPTOT/(1+XX)
>  ??? I <- IV*H^3/(10^-pK3*10^-pK2*10^-pK1)
>  ??? II <- IV*H^2/(10^-pK3*10^-pK2)
>  ??? III <- IV*H/10^-pK3
>  ??? H - kw/H + SID + I*2 + II - IV
>  ? }
> 
> 
> HEPTOT <- 0.050
> SID <- c(-seq(10,1)*1e-4,0,seq(1,10)*1e-4)
> 
> pHobs? <- c(4.63,4.68,4.72,4.77,4.83,4.9,4.96,5.04,5.12,5.21,5.3,
>  ??????????? 5.39,5.48,5.55,5.63,5.69,5.74,5.8,5.85,5.89,5.93)
> 
> pK1 <- -1; pK2 <- 3; pK3 <- 7.55 # literature values
> pK3 <- 7.6; pK2 <- 2.96 # values eye-balled to be better
> 
> pH <- c()
> for (i in 1:length(SID)) {
>  ? pH[i] <- -log10(uniroot(HEPESFUNC,c(1e-20,1),tol=1e-20,maxiter=1e4,
>  ?????????????????????? HEPTOT=HEPTOT,SID = SID[i],
>  ?????????????????????? pK1=pK1,pK2=pK2,pK3=pK3)$root)
> }
> 
> plot(SID,pH)
> points(SID,pHobs,col="red")


From tr|ng @end|ng |rom gvdnet@dk  Mon Nov  6 20:43:10 2023
From: tr|ng @end|ng |rom gvdnet@dk (Troels Ring)
Date: Mon, 6 Nov 2023 20:43:10 +0100
Subject: [R] non-linear regression and root finding
In-Reply-To: <20231106211902.375d3b0a@Tarkus>
References: <eb487c2a-4b9d-4000-af86-f99de576d106@gvdnet.dk>
 <20231106211902.375d3b0a@Tarkus>
Message-ID: <7990e957-6f37-4d2c-9ff0-d586f30798c6@gvdnet.dk>

Thanks a lot! This was amazing. I'm not sure I see how the conditiion 
pK1 < pK2 < pK3 is enforced? - it comes from the derivation via 
generalized Henderson-Hasselbalch but perhaps it is not really 
necessary. Anyway, the use of Vectorize did the trick!

Best wishes
Troels

Den 06-11-2023 kl. 19:19 skrev Ivan Krylov:
> ? Mon, 6 Nov 2023 17:53:49 +0100
> Troels Ring <tring at gvdnet.dk> ?????:
>
>> Hence I wonder if I could somehow have non linear regression to find
>> the 3 pK values. Below is HEPESFUNC which delivers charge in the
>> fluid for known pKs, HEPTOT and SID. Is it possible to have
>> root-finding in the formula with nls?
> Sure. Just reformulate the problem in terms of a function that takes a
> vector of predictors (your independent variable SID) and the desired
> parameters (pK1, pK2, pK3) as separate arguments and then returns
> predicted values of the dependent variable (to compare against pHobs):
>
> kw <- 1e-14 # I'm assuming
> pHm <- Vectorize(function(SID, pK1, pK2, pK3)
>   -log10(uniroot(HEPESFUNC,c(1e-20,1),tol=1e-20,maxiter=1e4,
>          HEPTOT=HEPTOT,SID = SID, pK1=pK1,pK2=pK2,pK3=pK3)$root))
>
> (Yes, Vectorize() doesn't make the function any faster, but I don't see
> an easy way to rewrite this function to make it truly vectorised.)
>
> Unfortunately, nls() seems to take a step somewhere where crossprod()
> of the Jacobian of the model function cannot be inverted and fails with
> the message "Singular gradient". I wish that R could have a more
> reliable built-in nonlinear least squares solver. (I could also be
> holding it wrong.) Meanwhile, we have excellent CRAN packages nlsr and
> minpack.lm:
>
> minpack.lm::nlsLM(
>   pHobs ~ pHm(SID, pK1, pK2, pK3),
>   data.frame(pHobs = pHobs, SID = SID),
>   start = c(pK1 = pK1, pK2 = pK2, pK3 = pK3),
>   # the following is also needed to avoid MINPACK failing to fit
>   lower = rep(-1, 3), upper = rep(9, 3)
> )
> # Nonlinear regression model
> #   model: pHobs ~ pHm(SID, pK1, pK2, pK3)
> #    data: data.frame(pHobs = pHobs, SID = SID)
> #    pK1     pK2    pK3
> # -1.000   2.966  7.606
> #  residual sum-of-squares: 0.001195
> #
> # Number of iterations to convergence: 15
> # Achieved convergence tolerance: 1.49e-08
>
> (Unfortunately, your code seemed to have a lot of non-breakable spaces,
> which confuse R's parser and make it harder to copy&paste it.)
>
> I think that your function can also be presented as a degree-5
> polynomial in H, so it should also be possible to use polyroot() to
> obtain your solutions in a more exact manner.
>


From pro|jcn@@h @end|ng |rom gm@||@com  Mon Nov  6 20:55:39 2023
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Mon, 6 Nov 2023 14:55:39 -0500
Subject: [R] non-linear regression and root finding
In-Reply-To: <7990e957-6f37-4d2c-9ff0-d586f30798c6@gvdnet.dk>
References: <eb487c2a-4b9d-4000-af86-f99de576d106@gvdnet.dk>
 <20231106211902.375d3b0a@Tarkus>
 <7990e957-6f37-4d2c-9ff0-d586f30798c6@gvdnet.dk>
Message-ID: <67c5ad87-6653-4d3d-be42-ba2e7aef08f7@gmail.com>

I won't send to list, but just to the two of you, as I don't have
anything to add at this time. However, I'm wondering if this approach
is worth writing up, at least as a vignette or blog post. It does need
a shorter example and some explanation of the "why" and some testing
perhaps.

If there's interest, I'll be happy to join in. And my own posting suggests
how the ordering is enforced by bounding the "delta" parameters from below.

Note that nls() can only handle bounds in the "port" algorithm, and the
man page rather pours cold water on using that algorithm.

Best, JN


On 2023-11-06 14:43, Troels Ring wrote:
> Thanks a lot! This was amazing. I'm not sure I see how the conditiion pK1 < pK2 < pK3 is enforced? - it comes from the 
> derivation via generalized Henderson-Hasselbalch but perhaps it is not really necessary. Anyway, the use of Vectorize 
> did the trick!
> 
> Best wishes
> Troels
> 
> Den 06-11-2023 kl. 19:19 skrev Ivan Krylov:
>> ? Mon, 6 Nov 2023 17:53:49 +0100
>> Troels Ring <tring at gvdnet.dk> ?????:
>>
>>> Hence I wonder if I could somehow have non linear regression to find
>>> the 3 pK values. Below is HEPESFUNC which delivers charge in the
>>> fluid for known pKs, HEPTOT and SID. Is it possible to have
>>> root-finding in the formula with nls?
>> Sure. Just reformulate the problem in terms of a function that takes a
>> vector of predictors (your independent variable SID) and the desired
>> parameters (pK1, pK2, pK3) as separate arguments and then returns
>> predicted values of the dependent variable (to compare against pHobs):
>>
>> kw <- 1e-14 # I'm assuming
>> pHm <- Vectorize(function(SID, pK1, pK2, pK3)
>> ? -log10(uniroot(HEPESFUNC,c(1e-20,1),tol=1e-20,maxiter=1e4,
>> ???????? HEPTOT=HEPTOT,SID = SID, pK1=pK1,pK2=pK2,pK3=pK3)$root))
>>
>> (Yes, Vectorize() doesn't make the function any faster, but I don't see
>> an easy way to rewrite this function to make it truly vectorised.)
>>
>> Unfortunately, nls() seems to take a step somewhere where crossprod()
>> of the Jacobian of the model function cannot be inverted and fails with
>> the message "Singular gradient". I wish that R could have a more
>> reliable built-in nonlinear least squares solver. (I could also be
>> holding it wrong.) Meanwhile, we have excellent CRAN packages nlsr and
>> minpack.lm:
>>
>> minpack.lm::nlsLM(
>> ? pHobs ~ pHm(SID, pK1, pK2, pK3),
>> ? data.frame(pHobs = pHobs, SID = SID),
>> ? start = c(pK1 = pK1, pK2 = pK2, pK3 = pK3),
>> ? # the following is also needed to avoid MINPACK failing to fit
>> ? lower = rep(-1, 3), upper = rep(9, 3)
>> )
>> # Nonlinear regression model
>> #?? model: pHobs ~ pHm(SID, pK1, pK2, pK3)
>> #??? data: data.frame(pHobs = pHobs, SID = SID)
>> #??? pK1???? pK2??? pK3
>> # -1.000?? 2.966? 7.606
>> #? residual sum-of-squares: 0.001195
>> #
>> # Number of iterations to convergence: 15
>> # Achieved convergence tolerance: 1.49e-08
>>
>> (Unfortunately, your code seemed to have a lot of non-breakable spaces,
>> which confuse R's parser and make it harder to copy&paste it.)
>>
>> I think that your function can also be presented as a degree-5
>> polynomial in H, so it should also be possible to use polyroot() to
>> obtain your solutions in a more exact manner.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Mon Nov  6 22:16:33 2023
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Mon, 6 Nov 2023 15:16:33 -0600
Subject: [R] understanding predict.lm
Message-ID: <40f8154c-a5d9-44e1-9221-f8dcd4d7f427@effectivedefense.org>

Hello, All:


	  I am unable to manually replicate predict.lm, specifically comparing 
se.fit with (fit[,3]-fit[,2]): I think their ratio should be 
2*qnorm((1-level)/2), and that's not what I'm getting.


	  Consider the following slight modification of the first example in 
help('predict.lm'):


set.seed(1)
x <- rnorm(15)
y <- x + rnorm(15)
predict(lm(y ~ x))
new <- data.frame(x = seq(-3, 3, 0.5))
predict(lm(y ~ x), new, se.fit = TRUE)
pred.w.plim <- predict(lm(y ~ x), new, interval = "prediction",
                        se.fit = TRUE)
pred.w.clim <- predict(lm(y ~ x), new, interval = "confidence",
                        se.fit = TRUE)

(z.confInt <- with(pred.w.clim, (fit[,3]-fit[,2])/se.fit))
pnorm(-z.confInt/2)

s.pred <- sqrt(with(pred.w.plim,
                     se.fit^2+residual.scale^2))
(z.predInt <- with(pred.w.plim, (fit[,3]-fit[,2])/s.pred))
pnorm(-z.predInt/2)


	  ** This gives me 0.01537207. I do not understand why it's not 0.025 
with level = 0.95.


	  Can someone help me understand this?
	  Thanks,
	  Spencer Graves


From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Tue Nov  7 00:27:46 2023
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Mon, 6 Nov 2023 17:27:46 -0600
Subject: [R] understanding predict.lm
In-Reply-To: <093d2b57-076d-4a9f-9aa3-9ad83c101af6@mcmaster.ca>
References: <40f8154c-a5d9-44e1-9221-f8dcd4d7f427@effectivedefense.org>
 <093d2b57-076d-4a9f-9aa3-9ad83c101af6@mcmaster.ca>
Message-ID: <932599e8-903f-4d23-b548-6d4a5c034b9b@effectivedefense.org>

Doh! Thanks very much. sg


On 11/6/23 5:17 PM, John Fox wrote:
> Dear Spencer,
> 
> You need the t distribution with correct df, not the standard-normal 
> distribution:
> 
>  > pt(-z.confInt/2, df=13)
>  ??? 1???? 2???? 3???? 4???? 5???? 6???? 7???? 8???? 9??? 10??? 11
> 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025
>  ?? 12??? 13
> 0.025 0.025
> 
>  > pt(-z.predInt/2, df=13)
>  ??? 1???? 2???? 3???? 4???? 5???? 6???? 7???? 8???? 9??? 10??? 11
> 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025
>  ?? 12??? 13
> 0.025 0.025
> 
> I hope this helps,
>  ?John


From @tyen @end|ng |rom ntu@edu@tw  Tue Nov  7 02:09:33 2023
From: @tyen @end|ng |rom ntu@edu@tw (Steven Yen)
Date: Tue, 7 Nov 2023 09:09:33 +0800
Subject: [R] Concordance and Kendall's tau in copula
Message-ID: <f33ad3c9-fc61-4b8f-b8fb-2ca33453a078@ntu.edu.tw>

Dear

I estimate a sample selection model using the Clayton copula and Burr 
and Gaussian marginal. I need to derive ther Kendall'sw tau from the 
concordance coefficient by integration. I came across a way to do that 
in R long time ago but cannot find it again. Can somewone tell me what 
to read and what to use? Thank you.

Steven Yen


From berw|n@tur|@ch @end|ng |rom gm@||@com  Tue Nov  7 05:10:08 2023
From: berw|n@tur|@ch @end|ng |rom gm@||@com (Berwin A Turlach)
Date: Tue, 7 Nov 2023 12:10:08 +0800
Subject: [R] non-linear regression and root finding
In-Reply-To: <7990e957-6f37-4d2c-9ff0-d586f30798c6@gvdnet.dk>
References: <eb487c2a-4b9d-4000-af86-f99de576d106@gvdnet.dk>
 <20231106211902.375d3b0a@Tarkus>
 <7990e957-6f37-4d2c-9ff0-d586f30798c6@gvdnet.dk>
Message-ID: <20231107121008.3dcc71e6@dep59159.uniwa.uwa.edu.au>

G'day Troels,

On Mon, 6 Nov 2023 20:43:10 +0100
Troels Ring <tring at gvdnet.dk> wrote:

> Thanks a lot! This was amazing. I'm not sure I see how the conditiion
> pK1 < pK2 < pK3 is enforced? 

One way of enforcing such constraints (well, in finite computer
arithemtic only "<=" can be enforced) is to rewrite the parameters as:

pK1 = exp(theta1)       ## only if pK1 > 0
pK2 = pK1 + exp(theta2)
pK3 = pk2 + exp(theta3)

And then use your optimiser to optimise over theta1, theta2 and theta3.

There might be better approaches depending on the specific problem.

Cheers,

	Berwin


From tr|ng @end|ng |rom gvdnet@dk  Tue Nov  7 07:14:02 2023
From: tr|ng @end|ng |rom gvdnet@dk (Troels Ring)
Date: Tue, 7 Nov 2023 07:14:02 +0100
Subject: [R] non-linear regression and root finding
In-Reply-To: <20231107121008.3dcc71e6@dep59159.uniwa.uwa.edu.au>
References: <eb487c2a-4b9d-4000-af86-f99de576d106@gvdnet.dk>
 <20231106211902.375d3b0a@Tarkus>
 <7990e957-6f37-4d2c-9ff0-d586f30798c6@gvdnet.dk>
 <20231107121008.3dcc71e6@dep59159.uniwa.uwa.edu.au>
Message-ID: <d258f5c5-9ae3-4bf5-9f6c-cccb64de37a1@gvdnet.dk>

Thanks a lot, Berwin. Unfortunately, pK1 may well be negative and as I 
understand the literature it may be poorly defined as such, and also 
seems to be at a boundary, since when lower is set to say rep(-4,3) pK1 
is returned as -4 while pK2 and pK3 are undisturbed. Perhaps the point 
is that pK1 is not carrying any information at the pH around 5. Fair 
enough, I guess. Only, I believe I need stick to including all three pK 
values to be in agreement with the molecular information about HEPES - 
although even this is contentious. Be as it may, I wonder if not your 
method might work if only we KNOW that pK1 is either positive OR 
negative, in which case we have pK1 = -exp(theta1)?

Best wishes
Troels

Den 07-11-2023 kl. 05:10 skrev Berwin A Turlach:
> G'day Troels,
>
> On Mon, 6 Nov 2023 20:43:10 +0100
> Troels Ring <tring at gvdnet.dk> wrote:
>
>> Thanks a lot! This was amazing. I'm not sure I see how the conditiion
>> pK1 < pK2 < pK3 is enforced?
> One way of enforcing such constraints (well, in finite computer
> arithemtic only "<=" can be enforced) is to rewrite the parameters as:
>
> pK1 = exp(theta1)       ## only if pK1 > 0
> pK2 = pK1 + exp(theta2)
> pK3 = pk2 + exp(theta3)
>
> And then use your optimiser to optimise over theta1, theta2 and theta3.
>
> There might be better approaches depending on the specific problem.
>
> Cheers,
>
> 	Berwin


From j|ox @end|ng |rom mcm@@ter@c@  Tue Nov  7 00:17:21 2023
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Mon, 6 Nov 2023 18:17:21 -0500
Subject: [R] understanding predict.lm
In-Reply-To: <40f8154c-a5d9-44e1-9221-f8dcd4d7f427@effectivedefense.org>
References: <40f8154c-a5d9-44e1-9221-f8dcd4d7f427@effectivedefense.org>
Message-ID: <093d2b57-076d-4a9f-9aa3-9ad83c101af6@mcmaster.ca>

Dear Spencer,

You need the t distribution with correct df, not the standard-normal 
distribution:

 > pt(-z.confInt/2, df=13)
     1     2     3     4     5     6     7     8     9    10    11
0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025
    12    13
0.025 0.025

 > pt(-z.predInt/2, df=13)
     1     2     3     4     5     6     7     8     9    10    11
0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025
    12    13
0.025 0.025

I hope this helps,
  John
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://www.john-fox.ca/

On 2023-11-06 4:16 p.m., Spencer Graves wrote:
> Caution: External email.
> 
> 
> Hello, All:
> 
> 
>  ???????? I am unable to manually replicate predict.lm, specifically 
> comparing
> se.fit with (fit[,3]-fit[,2]): I think their ratio should be
> 2*qnorm((1-level)/2), and that's not what I'm getting.
> 
> 
>  ???????? Consider the following slight modification of the first 
> example in
> help('predict.lm'):
> 
> 
> set.seed(1)
> x <- rnorm(15)
> y <- x + rnorm(15)
> predict(lm(y ~ x))
> new <- data.frame(x = seq(-3, 3, 0.5))
> predict(lm(y ~ x), new, se.fit = TRUE)
> pred.w.plim <- predict(lm(y ~ x), new, interval = "prediction",
>  ?????????????????????? se.fit = TRUE)
> pred.w.clim <- predict(lm(y ~ x), new, interval = "confidence",
>  ?????????????????????? se.fit = TRUE)
> 
> (z.confInt <- with(pred.w.clim, (fit[,3]-fit[,2])/se.fit))
> pnorm(-z.confInt/2)
> 
> s.pred <- sqrt(with(pred.w.plim,
>  ??????????????????? se.fit^2+residual.scale^2))
> (z.predInt <- with(pred.w.plim, (fit[,3]-fit[,2])/s.pred))
> pnorm(-z.predInt/2)
> 
> 
>  ???????? ** This gives me 0.01537207. I do not understand why it's not 
> 0.025
> with level = 0.95.
> 
> 
>  ???????? Can someone help me understand this?
>  ???????? Thanks,
>  ???????? Spencer Graves
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From berw|n@tur|@ch @end|ng |rom gm@||@com  Tue Nov  7 08:53:03 2023
From: berw|n@tur|@ch @end|ng |rom gm@||@com (Berwin A Turlach)
Date: Tue, 7 Nov 2023 15:53:03 +0800
Subject: [R] non-linear regression and root finding
In-Reply-To: <d258f5c5-9ae3-4bf5-9f6c-cccb64de37a1@gvdnet.dk>
References: <eb487c2a-4b9d-4000-af86-f99de576d106@gvdnet.dk>
 <20231106211902.375d3b0a@Tarkus>
 <7990e957-6f37-4d2c-9ff0-d586f30798c6@gvdnet.dk>
 <20231107121008.3dcc71e6@dep59159.uniwa.uwa.edu.au>
 <d258f5c5-9ae3-4bf5-9f6c-cccb64de37a1@gvdnet.dk>
Message-ID: <20231107155303.0d27333e@dep59159.uniwa.uwa.edu.au>

G'day Troels,

On Tue, 7 Nov 2023 07:14:02 +0100
Troels Ring <tring at gvdnet.dk> wrote:

> Be as it may, I wonder if not your method might work if only we KNOW
> that pK1 is either positive OR negative, in which case we have pK1 =
> -exp(theta1)?

If pK1 can be either negative or positive (or 0 :-) ), and it is just
the ordering that you want to have enforced, then I would try the
parameterisation:

pK1 = pK1
pK2 = pK1 + exp(theta2)
pK3 = pk2 + exp(theta3)

and optimise over pK1, theta2 and theta3.  

As long as you want to know the estimates only.  

Asking for standard errors of the original estimates would open another
can of worms. :-)

Cheers,

	Berwin


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Tue Nov  7 10:03:58 2023
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Tue, 7 Nov 2023 10:03:58 +0100
Subject: [R] Concordance and Kendall's tau in copula
In-Reply-To: <f33ad3c9-fc61-4b8f-b8fb-2ca33453a078@ntu.edu.tw>
References: <f33ad3c9-fc61-4b8f-b8fb-2ca33453a078@ntu.edu.tw>
Message-ID: <25929.64894.937088.581822@stat.math.ethz.ch>

>>>>> Steven Yen 
>>>>>     on Tue, 7 Nov 2023 09:09:33 +0800 writes:

    > Dear
    > I estimate a sample selection model using the Clayton copula and Burr 
    > and Gaussian marginal. I need to derive ther Kendall'sw tau from the 
    > concordance coefficient by integration. I came across a way to do that 
    > in R long time ago but cannot find it again. Can somewone tell me what 
    > to read and what to use? Thank you.

    > Steven Yen


I think you can estimate your model relatively easily using our
package {copula}  and the function  fitMvdc()

   https://search.r-project.org/CRAN/refmans/copula/html/fitMvdc.html

MVDC := Multivariate Variate Distribution {built from} Copula

To solve the question you asked --- but would not need to answer if
using fitMvdc(),
you can use  e.g.,

    > iTau(claytonCopula(), tau = 1.4)
    [1] -7

or look up the formulas for tau() or its inverse 'iTau':

  > copClayton at tau
  function (theta) 
  {
      theta/(theta + 2)
  }

  > copClayton at iTau
  function (tau) 
  {
      2 * tau/(1 - tau)
  }

  > 

Best regards,
Martin

{and yes, consider getting our 'useR! Springer series book, as
 it's the only "real" book, I've been a coauthor.. https://copula.r-forge.r-project.org/book/ }

--
Martin Maechler
ETH Zurich  and  R Core team


From cry@n @end|ng |rom b|ngh@mton@edu  Tue Nov  7 17:01:50 2023
From: cry@n @end|ng |rom b|ngh@mton@edu (Christopher Ryan)
Date: Tue, 7 Nov 2023 11:01:50 -0500
Subject: [R] make a lattice dotplot with symbol size proportional to a
 variable in the plotted dataframe
Message-ID: <CAM+rpY=Mn9+osm2Ct4fNkDOKXP2zLDEa+tTkz0WB6rgzytKi5Q@mail.gmail.com>

Hello. My question is in the subject line. Using R 4.1.3 on Windows 10.
Commented MWE below. Thanks.

--Chris Ryan



library(dplyr)
library(lattice)

## fabricate a dataframe
dd <- data.frame(agency = sample(LETTERS, size = 5),
total = sample(100:200, size = 5),
las = sample(20:40, size = 5))
dd <- dd %>% mutate(proportion = las/total, bubble = total/100)


## attempt to make a dotplot with symbol size proportional
## to the variable named total

dotplot(agency ~ proportion, pch = 16, cex = bubble, data = dd)
##  object 'bubble' not found

dotplot(agency ~ proportion, pch = 16, cex = dd$bubble, data = dd)
## works



## also works in two commands
external.bubble <- dd$bubble
dotplot(agency ~ proportion, pch = 16, cex = external.bubble, data = dd)



## but how to chain it with pipes, dplyr-style,
## modifying the dataframe and then
## using the modified version in dotplot, all in one chain?

dd %>% mutate(new.proportion = las/total, new.bubble = total/100) %>%
    dotplot(agency ~ new.proportion, pch = 16, cex = new.bubble, data = .)
## object 'new.bubble' not found


dd %>% mutate(new.proportion = las/total, new.bubble = total/100) %>%
     dotplot(agency ~ new.proportion, pch = 16, cex = .$new.bubble, data =
.)
## the .$new.bubble syntax seems to work, but I've never
## used or seen that before, and it seems weird.
## Is there a "proper" syntax?

	[[alternative HTML version deleted]]


From deep@y@n@@@rk@r @end|ng |rom gm@||@com  Tue Nov  7 17:25:10 2023
From: deep@y@n@@@rk@r @end|ng |rom gm@||@com (Deepayan Sarkar)
Date: Tue, 7 Nov 2023 11:25:10 -0500
Subject: [R] make a lattice dotplot with symbol size proportional to a
 variable in the plotted dataframe
In-Reply-To: <CAM+rpY=Mn9+osm2Ct4fNkDOKXP2zLDEa+tTkz0WB6rgzytKi5Q@mail.gmail.com>
References: <CAM+rpY=Mn9+osm2Ct4fNkDOKXP2zLDEa+tTkz0WB6rgzytKi5Q@mail.gmail.com>
Message-ID: <CADfFDC4bns4YY7SC4mFQOsY68WQj=VFSPJAjHOU223km0vDq7w@mail.gmail.com>

Handling NSE in these kinds of examples is a pain in lattice. I would
suggest using with() and dropping the data argument for simple examples,
e.g.,

dd |> mutate(new.proportion = las/total, new.bubble = total/100) |>
    with(dotplot(agency ~ new.proportion, pch = 16, cex = new.bubble))

But if you care about multi-panel plots, you also need to be careful about
making sure that the 'cex' values get split properly. This is done
generally using the 'subscripts' argument provided to panel functions, so
something like this should be safer:

panel.bubble <- function(x, y, cex, ..., subscripts) {
    panel.dotplot(x, y, cex = cex[subscripts], ...)
}

dd |> mutate(new.proportion = las/total, new.bubble = total/100) |>
    with(dotplot(agency ~ new.proportion, pch = 16,
                 cex = new.bubble, panel = panel.bubble))

Best,
-Deepayan

On Tue, 7 Nov 2023 at 11:03, Christopher Ryan via R-help <
r-help at r-project.org> wrote:

> Hello. My question is in the subject line. Using R 4.1.3 on Windows 10.
> Commented MWE below. Thanks.
>
> --Chris Ryan
>
>
>
> library(dplyr)
> library(lattice)
>
> ## fabricate a dataframe
> dd <- data.frame(agency = sample(LETTERS, size = 5),
> total = sample(100:200, size = 5),
> las = sample(20:40, size = 5))
> dd <- dd %>% mutate(proportion = las/total, bubble = total/100)
>
>
> ## attempt to make a dotplot with symbol size proportional
> ## to the variable named total
>
> dotplot(agency ~ proportion, pch = 16, cex = bubble, data = dd)
> ##  object 'bubble' not found
>
> dotplot(agency ~ proportion, pch = 16, cex = dd$bubble, data = dd)
> ## works
>
>
>
> ## also works in two commands
> external.bubble <- dd$bubble
> dotplot(agency ~ proportion, pch = 16, cex = external.bubble, data = dd)
>
>
>
> ## but how to chain it with pipes, dplyr-style,
> ## modifying the dataframe and then
> ## using the modified version in dotplot, all in one chain?
>
> dd %>% mutate(new.proportion = las/total, new.bubble = total/100) %>%
>     dotplot(agency ~ new.proportion, pch = 16, cex = new.bubble, data = .)
> ## object 'new.bubble' not found
>
>
> dd %>% mutate(new.proportion = las/total, new.bubble = total/100) %>%
>      dotplot(agency ~ new.proportion, pch = 16, cex = .$new.bubble, data =
> .)
> ## the .$new.bubble syntax seems to work, but I've never
> ## used or seen that before, and it seems weird.
> ## Is there a "proper" syntax?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Tue Nov  7 22:51:23 2023
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Wed, 8 Nov 2023 00:51:23 +0300
Subject: [R] non-linear regression and root finding
In-Reply-To: <67c5ad87-6653-4d3d-be42-ba2e7aef08f7@gmail.com>
References: <eb487c2a-4b9d-4000-af86-f99de576d106@gvdnet.dk>
 <20231106211902.375d3b0a@Tarkus>
 <7990e957-6f37-4d2c-9ff0-d586f30798c6@gvdnet.dk>
 <67c5ad87-6653-4d3d-be42-ba2e7aef08f7@gmail.com>
Message-ID: <20231108005123.3dd0edb0@Tarkus>

On Mon, 6 Nov 2023 14:55:39 -0500
J C Nash <profjcnash at gmail.com> wrote:

> However, I'm wondering if this approach is worth writing up, at least
> as a vignette or blog post. It does need a shorter example and some
> explanation of the "why" and some testing perhaps.

Do you mean using this problem as a basis to illustrate ordering
constraints on parameters? Weird constraints do come up every now and
then in regression problems. I could definitely offer my help with at
least some of the text.

> If there's interest, I'll be happy to join in. And my own posting
> suggests how the ordering is enforced by bounding the "delta"
> parameters from below.

I have just tried nlsr::nlxb for a slightly larger dataset shared by
Troels off-list, and it worked great with the delta parameters as you
suggested, thank you!

It's interesting that nlxb and nlsLM give slightly different answers,
differing in 0.5 pK units for pK1 and (pK2-pK1) but not (pK3-pK2). Then
again, they both agree that the standard error for pK1 and (pK2-pK1) is
very large, so perhaps the problem is just very ill-conditioned.

-- 
Best regards,
Ivan


From cry@n @end|ng |rom b|ngh@mton@edu  Wed Nov  8 16:56:13 2023
From: cry@n @end|ng |rom b|ngh@mton@edu (Christopher W. Ryan)
Date: Wed, 8 Nov 2023 10:56:13 -0500
Subject: [R] make a lattice dotplot with symbol size proportional to a
 variable in the plotted dataframe
In-Reply-To: <CADfFDC4bns4YY7SC4mFQOsY68WQj=VFSPJAjHOU223km0vDq7w@mail.gmail.com>
References: <CAM+rpY=Mn9+osm2Ct4fNkDOKXP2zLDEa+tTkz0WB6rgzytKi5Q@mail.gmail.com>
 <CADfFDC4bns4YY7SC4mFQOsY68WQj=VFSPJAjHOU223km0vDq7w@mail.gmail.com>
Message-ID: <c479de42-706a-ca64-f330-7a4f49aa9b6b@binghamton.edu>

Very helpful, Deepayan, and educational. Thank you.

What does NSE stand for?

Thanks,
Chris

Deepayan Sarkar wrote:
> 
> --Chris Ryan


From bbo|ker @end|ng |rom gm@||@com  Wed Nov  8 16:58:59 2023
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 8 Nov 2023 10:58:59 -0500
Subject: [R] make a lattice dotplot with symbol size proportional to a
 variable in the plotted dataframe
In-Reply-To: <c479de42-706a-ca64-f330-7a4f49aa9b6b@binghamton.edu>
References: <CAM+rpY=Mn9+osm2Ct4fNkDOKXP2zLDEa+tTkz0WB6rgzytKi5Q@mail.gmail.com>
 <CADfFDC4bns4YY7SC4mFQOsY68WQj=VFSPJAjHOU223km0vDq7w@mail.gmail.com>
 <c479de42-706a-ca64-f330-7a4f49aa9b6b@binghamton.edu>
Message-ID: <eccf2ede-8fb0-42b6-a9b4-700b30bfcdc7@gmail.com>

   Non-standard evaluation

On 2023-11-08 10:56 a.m., Christopher W. Ryan via R-help wrote:
> Very helpful, Deepayan, and educational. Thank you.
> 
> What does NSE stand for?
> 
> Thanks,
> Chris
> 
> Deepayan Sarkar wrote:
>>
>> --Chris Ryan
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From deep@y@n@@@rk@r @end|ng |rom gm@||@com  Wed Nov  8 17:02:22 2023
From: deep@y@n@@@rk@r @end|ng |rom gm@||@com (Deepayan Sarkar)
Date: Wed, 8 Nov 2023 11:02:22 -0500
Subject: [R] make a lattice dotplot with symbol size proportional to a
 variable in the plotted dataframe
In-Reply-To: <c479de42-706a-ca64-f330-7a4f49aa9b6b@binghamton.edu>
References: <CAM+rpY=Mn9+osm2Ct4fNkDOKXP2zLDEa+tTkz0WB6rgzytKi5Q@mail.gmail.com>
 <CADfFDC4bns4YY7SC4mFQOsY68WQj=VFSPJAjHOU223km0vDq7w@mail.gmail.com>
 <c479de42-706a-ca64-f330-7a4f49aa9b6b@binghamton.edu>
Message-ID: <CADfFDC4SpQmBtwVmOi0FAw2MT7z+5icqpp9fUDcYpAkcWw6WCQ@mail.gmail.com>

On Wed, 8 Nov 2023 at 10:56, Christopher W. Ryan via R-help
<r-help at r-project.org> wrote:
>
> Very helpful, Deepayan, and educational. Thank you.
>
> What does NSE stand for?

Non-standard evaluation, used widely in formula-interface functions as
well as the tidyverse. with() in my example is a less nuanced version
of this. See

http://adv-r.had.co.nz/Computing-on-the-language.html

https://developer.r-project.org/nonstandard-eval.pdf

Best,
-Deepayan


> Thanks,
> Chris
>
> Deepayan Sarkar wrote:
> >
> > --Chris Ryan
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Wed Nov  8 17:52:40 2023
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 8 Nov 2023 08:52:40 -0800
Subject: [R] make a lattice dotplot with symbol size proportional to a
 variable in the plotted dataframe
In-Reply-To: <CADfFDC4SpQmBtwVmOi0FAw2MT7z+5icqpp9fUDcYpAkcWw6WCQ@mail.gmail.com>
References: <CAM+rpY=Mn9+osm2Ct4fNkDOKXP2zLDEa+tTkz0WB6rgzytKi5Q@mail.gmail.com>
 <CADfFDC4bns4YY7SC4mFQOsY68WQj=VFSPJAjHOU223km0vDq7w@mail.gmail.com>
 <c479de42-706a-ca64-f330-7a4f49aa9b6b@binghamton.edu>
 <CADfFDC4SpQmBtwVmOi0FAw2MT7z+5icqpp9fUDcYpAkcWw6WCQ@mail.gmail.com>
Message-ID: <CAGxFJbS3vt1_e0cQeF-aa83-YcYWReX7Z+CNinJc9rM7XD2skw@mail.gmail.com>

... and see also Section 3.5, Scope of Variables in the "R Language
Definition" manual that ships with R.

Cheers,
Bert

On Wed, Nov 8, 2023 at 8:06?AM Deepayan Sarkar <deepayan.sarkar at gmail.com>
wrote:

> On Wed, 8 Nov 2023 at 10:56, Christopher W. Ryan via R-help
> <r-help at r-project.org> wrote:
> >
> > Very helpful, Deepayan, and educational. Thank you.
> >
> > What does NSE stand for?
>
> Non-standard evaluation, used widely in formula-interface functions as
> well as the tidyverse. with() in my example is a less nuanced version
> of this. See
>
> http://adv-r.had.co.nz/Computing-on-the-language.html
>
> https://developer.r-project.org/nonstandard-eval.pdf
>
> Best,
> -Deepayan
>
>
> > Thanks,
> > Chris
> >
> > Deepayan Sarkar wrote:
> > >
> > > --Chris Ryan
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From hwborcher@ @end|ng |rom gm@||@com  Thu Nov  9 06:42:51 2023
From: hwborcher@ @end|ng |rom gm@||@com (Hans W)
Date: Thu, 9 Nov 2023 06:42:51 +0100
Subject: [R] Dependency errors for package pracma
Message-ID: <CAML4n3N+UgaiFjM1uH_QFBqcLT=-+4zU2yH64OFbsh6up-yjMA@mail.gmail.com>

I tried to update my package {pracma} on CRAN from 2.4.2 (2022-09-21)
to version 2.4.4 (2023-11-08). This package reverse depends / imports
/ suggests on 350 packages on CRAN and 25 packages on Bioconductor.

The only changes are small corrections on some help files, a new
function for stereographic projection, and `gcd` and `Lcm` require
integer inputs now (these functions are not used in the packages
below).

I received a dependency report saying that
    *** Changes to worse in reverse dependencies ***
    celltrackR, geostatsp, gmvjoint, hypr, randnet

Example: geostatsp suggests pracma, but uses only the function 'trapz'
that has not changed for years and years.
I cannot check this package, as probably other packages (from
Bioconductor ?) are needed to install it.

Example: gmvjoint imports pracma and uses 'grad', 'hessian', and
'nearest_spd'; these functions have not changed for years.
On my system, gmvjoint gets checked without ERRORs !

What should I do? Frankly, I do not have the time to check these
packages or to test almost 400 packages before uploading to CRAN.
Okay, I can leave it as is and wait until it gets thrown off CRAN
(because of some new syntax checks, e.g.). I will not mind much. Are
there better alternatives?

Thanks, Hans Werner


From crown11||@me @end|ng |rom gm@||@com  Wed Nov  8 11:33:15 2023
From: crown11||@me @end|ng |rom gm@||@com (Crown Flame)
Date: Wed, 8 Nov 2023 16:03:15 +0530
Subject: [R] Problem in R code
Message-ID: <CAHoPVsd77gWDPFFCdTFPsZrGXjX0gskwef7eyBDTr+YFxMXhWw@mail.gmail.com>

Good afternoon,
I have been working on my thesis project on the topic "Urban Heat Island
Pattern in India". To achieve the results I am applying a* two-dimensional
Gaussian fit* on an LST raster of 1 km spatial resolution but I am facing
two errors in the following code.

library(raster)
LST <- raster("D:/Celsius_Day/MOD_01.tif")
gaussian2d <- function(x, y, mu_x, mu_y, sigma_x, sigma_y, amp)
{
  exp(-((x - mu_x)^2/(2*sigma_x^2) + (y - mu_y)^2/(2*sigma_y^2)))*amp
}

#define a function for the sum of squared errors between the data and the
Gaussian
sse <- function(p)
{ mu_x <- p
mu_y <- p
sigma_x <- p
sigma_y <- p
amp <- p[5]
fitted <- gaussian2d(x, y, mu_x, mu_y, sigma_x, sigma_y, amp)
sum((z - fitted)^2)
}

#loop over 8 cities
cities <-
c("Delhi","Jaipur","Kolkata","Mumbai","Pune","Hyderabad","Bangalore","Chennai")
lon <-
c(77.219934,75.793261,88.365394,72.900361,73.875199,78.47476,77.602114,80.192181)
lat <-
c(28.589256,26.892024,22.619754,19.110629,18.50269,17.422973,12.974087,13.044415)
results <- data.frame() #A data frame to store the results
for(i in 1:8)
{
  LST_city <- extract(LST, c(lon[i],lat[i]), fun = mean, buffer = 10000,
na.rm = TRUE)   #error
}

# Fit a 2D Gaussian surface to the LST data
x <- coordinates(LST)[,1]
y <- coordinates(LST)[,2]
z <- values(LST)
mu_x0 <- mean(x)
mu_y0 <- mean(y)
sigma_x0 <- sd(x)/2
sigma_y0 <- sd(y)/2
amp0 <- max(z)
opt <- optim(c(mu_x0, mu_y0, sigma_x0, sigma_y0, amp0), sse)     #error 2

#Calculate the footprint of SUHI effect (FP) by the Gaussian surface
FP <- which(gaussian2d(x, y, opt$par, opt$par, opt$par, opt$par,
opt$par[5]) >= threshold)

#store the results for each city in the data frame
results <- rbind(results, data.frame(city=cities[i], FP=mean(FP)))

#print the results
results


The two errors are in the row which are defining the variables "LST_city"
and "opt".
The first error is:
Error in .cellValues(x, y, ...) : unused arguments (fun =
new("standardGeneric", .Data = function (x, ...) standardGeneric("mean"),
generic = "mean", package = "base", group = list(), valueClass =
character(0), signature = "x", default = new("derivedDefaultMethod", .Data
= function (x, ...) UseMethod("mean"), target = new("signature", .Data =
"ANY", names = "x", package = "methods"), defined = new("signature", .Data
= "ANY", names = "x", package = "methods"), generic = "mean"), skeleton =
(new("derivedDefaultMethod", .Data = function (x, ...) UseMethod("mean"),
target = new("signature", .Data = "ANY", names = "x", package = "methods"),
defined = new("signature", .Data = "ANY", names = "x", package =
"methods"), generic = "mean"))(x, ...)), buffer = 20000, na.rm = TRUE)


The second error is:

Error in optim(c(mu_x0, mu_y0, sigma_x0, sigma_y0, amp0), sse) :
  non-finite value supplied by optim



What could be the possible reason behind these errors and most
importantly how can I get rid of these errors?



Thank you


Regards

DD

	[[alternative HTML version deleted]]


From ||gge@ @end|ng |rom @t@t|@t|k@tu-dortmund@de  Thu Nov  9 09:59:12 2023
From: ||gge@ @end|ng |rom @t@t|@t|k@tu-dortmund@de (Uwe Ligges)
Date: Thu, 9 Nov 2023 09:59:12 +0100
Subject: [R] Dependency errors for package pracma
In-Reply-To: <CAML4n3N+UgaiFjM1uH_QFBqcLT=-+4zU2yH64OFbsh6up-yjMA@mail.gmail.com>
References: <CAML4n3N+UgaiFjM1uH_QFBqcLT=-+4zU2yH64OFbsh6up-yjMA@mail.gmail.com>
Message-ID: <f8899feb-eff0-07d7-cefa-b298ec1a0b8a@statistik.tu-dortmund.de>



On 09.11.2023 06:42, Hans W wrote:
> I tried to update my package {pracma} on CRAN from 2.4.2 (2022-09-21)
> to version 2.4.4 (2023-11-08). This package reverse depends / imports
> / suggests on 350 packages on CRAN and 25 packages on Bioconductor.
> 
> The only changes are small corrections on some help files, a new
> function for stereographic projection, and `gcd` and `Lcm` require
> integer inputs now (these functions are not used in the packages
> below).
> 
> I received a dependency report saying that
>      *** Changes to worse in reverse dependencies ***
>      celltrackR, geostatsp, gmvjoint, hypr, randnet

Most are from a recent Matrix update, hypr might not, so please check 
the above and tell us whether pracma casuses one of the issues (hypr?) 
and whether the maintainer has been informed in advance.

> 
> Example: geostatsp suggests pracma, but uses only the function 'trapz'
> that has not changed for years and years.
> I cannot check this package, as probably other packages (from
> Bioconductor ?) are needed to install it.
> 
> Example: gmvjoint imports pracma and uses 'grad', 'hessian', and
> 'nearest_spd'; these functions have not changed for years.
> On my system, gmvjoint gets checked without ERRORs !
> 
> What should I do? Frankly, I do not have the time to check these
> packages or to test almost 400 packages before uploading to CRAN.

The CRAN policies point you to functionality so that you can simply run 
these checks autiomatically on your machine. Let me read it for you:

"For a package update, please check that any packages depending on this 
one still pass R CMD check: it is especially expected that you will have 
checked your own packages. Reverse dependencies can conveniently be 
checked using tools::check_packages_in_dir(reverse = list()), and 
changes in check status subsequently be analyzed using 
tools::check_packages_in_dir_changes(). A listing of the reverse 
dependencies of the current version can be found on the CRAN web page 
for the package, or be obtained via tools::package_dependencies(reverse 
= TRUE). If possible, check reverse strong dependencies, reverse 
suggests and the recursive strong dependencies of these (by 
tools::package_dependencies(reverse = TRUE, which = "most", recursive = 
"strong")). "



Best,
Uwe Ligges


> Okay, I can leave it as is and wait until it gets thrown off CRAN
> (because of some new syntax checks, e.g.). I will not mind much. Are
> there better alternatives?
> 
> Thanks, Hans Werner
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From |@go@g|ne @end|ng |rom @jd@e@  Thu Nov  9 11:59:44 2023
From: |@go@g|ne @end|ng |rom @jd@e@ (=?iso-8859-1?Q?Iago_Gin=E9_V=E1zquez?=)
Date: Thu, 9 Nov 2023 10:59:44 +0000
Subject: [R] Why Rprofile.site is not built with manual installation of R
 devel in linux?
Message-ID: <AM6PR02MB44230F37BC7E7A139581925F94AFA@AM6PR02MB4423.eurprd02.prod.outlook.com>

Hi all,

I downloaded R-devel as explicited in https://developer.r-project.org/SVNtips.html
Then, I tried to install it through instructions in https://cran.r-project.org/doc/manuals/r-devel/R-admin.html#Installation
(taking into account also https://stat.ethz.ch/pipermail/r-devel/2016-May/072777.html)
So:
export REPOS=https://svn.r-project.org/R
export RTOP=~ #adjust as necessary
cd $RTOP
svn co $REPOS/trunk r-devel/R
cd r-devel/R
tools/rsync-recommended
mkdir ../build-R
 cd ../build-R
 ../R/configure --prefix=/where/you/want/R/to/go
 make
 make check
make install
make install-tests
cd tests
## followed by one of
../bin/R CMD make check
../bin/R CMD make check-devel

And here I get the following error
checking package 'base'
Error in file(filename, "r", encoding = encoding) :
  cannot open the connection
Calls: source -> file
In addition: Warning message:
In file(filename, "r", encoding = encoding) :
  cannot open file '.../etc/Rprofile.site': No such file or directory
Execution halted

where the dots ... specify the path to the build-R folder where R-devel was built. And I check the etc folder and indeed there is no the Rprofile.site
-rw-r--r-- 1 iago iago  209 Nov  9 08:27 javaconf
-rw-r--r-- 1 iago iago  770 Nov  9 08:35 ldpaths
-rw-r--r-- 1 iago iago 6672 Nov  9 08:35 Makeconf
-rw-r--r-- 1 iago iago 3336 Nov  9 08:27 Makefile
-rw-r--r-- 1 iago iago 1853 Nov  9 08:27 Renviron
-rw-r--r-- 1 iago iago 1173 Nov  9 08:32 repositories

I note that make install installed R in the path I specified in  ../R/configure --prefix=/where/you/want/R/to/go
however
    1. make install-tests installed the tests folder in build-R .
    2.  In the installed R in /where/you/want/R/to/go, there is no even etc folder, there are only the folders bin, lib and share.

Am I  skipping some step? I am on Debain 12.

Thank you!


Iago

	[[alternative HTML version deleted]]


From hwborcher@ @end|ng |rom gm@||@com  Thu Nov  9 12:22:52 2023
From: hwborcher@ @end|ng |rom gm@||@com (Hans W)
Date: Thu, 9 Nov 2023 12:22:52 +0100
Subject: [R] Dependency errors for package pracma
Message-ID: <CAML4n3PuLgFRyUdQb-24CvZo11fTEp0j7yckGLEHQrVErkdvKA@mail.gmail.com>

What really interests me:

With all those strict checking procedures, how is it possible that the
new 'Matrix' version got accepted on CRAN?

I think this happened twice to me before, and it takes a lot of time
to check package dependencies that turn out to be not dependent --
more time than checking dependencies that are real.


From kry|ov@r00t @end|ng |rom gm@||@com  Thu Nov  9 12:59:46 2023
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Thu, 9 Nov 2023 14:59:46 +0300
Subject: [R] Problem in R code
In-Reply-To: <CAHoPVsd77gWDPFFCdTFPsZrGXjX0gskwef7eyBDTr+YFxMXhWw@mail.gmail.com>
References: <CAHoPVsd77gWDPFFCdTFPsZrGXjX0gskwef7eyBDTr+YFxMXhWw@mail.gmail.com>
Message-ID: <20231109145946.775b142a@arachnoid>

? Wed, 8 Nov 2023 16:03:15 +0530
Crown Flame <crown11flame at gmail.com> ?????:

> for(i in 1:8)
> {
>   LST_city <- extract(LST, c(lon[i],lat[i]), fun = mean, buffer =
> 10000, na.rm = TRUE)   #error
> }

Three things you might need to change:

1. You are trying to assign the output of extract() to the same
variable LST_city in all 8 iterations of the loop. You will probably
need to make it a list and assign to the list elements, e.g.
LST_city[[i]] <- ...

It will also help to learn about lapply() and other functions that
encapsulate loops, although that is more of a matter of taste.

2. You are giving a single vector c(lon[i], lat[i]) to extract() as the
y=... argument. According to help(extract), this is interpreted as
_numbers_ of cells inside x, not coordinates. You should probably
construct a spPolygons() object and use that as the `y` argument to
extract().

3. The description of the 'raster' package says that it's been
superseded by the 'terra' package. You don't have to rewrite all your
code now, but it may be beneficial to check whether 'terra' can
accomplish what you want.

> Error in optim(c(mu_x0, mu_y0, sigma_x0, sigma_y0, amp0), sse) :
>   non-finite value supplied by optim

This must be self-explanatory: your `sse` function returns something
that is not a real number. Try options(error = recover) to see which
arguments it is given when it fails in such a manner. See the free book
"The R Inferno" for more R debugging tips
<https://www.burns-stat.com/documents/books/the-r-inferno/>. Consider
using the 'optimx' package which contains optimisation algorithms that
might work better for you.

-- 
Best regards,
Ivan


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Thu Nov  9 14:27:37 2023
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Thu, 9 Nov 2023 14:27:37 +0100
Subject: [R] Dependency errors for package pracma
In-Reply-To: <CAML4n3PuLgFRyUdQb-24CvZo11fTEp0j7yckGLEHQrVErkdvKA@mail.gmail.com>
References: <CAML4n3PuLgFRyUdQb-24CvZo11fTEp0j7yckGLEHQrVErkdvKA@mail.gmail.com>
Message-ID: <25932.56905.565988.452850@stat.math.ethz.ch>

>>>>> Hans W 
>>>>>     on Thu, 9 Nov 2023 12:22:52 +0100 writes:

    > What really interests me:
    > With all those strict checking procedures, how is it possible that the
    > new 'Matrix' version got accepted on CRAN?

There > 2000 reverse dependencies for Matrix.

We have had some (in themselves small) inconsistency fixes some
of which make Matrix 'Matrices' more similar to base R matrices.

In any case we pre-tested all reverse dependencies and found 7 or 8
out of 22'000 CRAN+Bioc packages which needed to adapt to the
Matrix changes.  The package maintainer of the 7 packages were
told in advance that they should (very slightly) adapt their
code (sometimes only there *tests*) and mostly were even handed
out a *patch* to apply to the respective package for updating it.
One or two of these packages were updated on CRAN in time, the
other ~ 5 were not... this is  ~ 1 in 4000 packages.  So the
decision was made to release (from us pkg maintainers) and to
accept (from the CRAN team).

Also, to be fair, in our tests we did miss one package,
but then quickly told the maintainer, again providing him with a
complete patch file to update the package (working both with old
and new Matrix).

    > I think this happened twice to me before, and it takes a lot of time
    > to check package dependencies that turn out to be not dependent --
    > more time than checking dependencies that are real.

With Matrix there is some extra effort, because it also exports
a C API (so other packages can have  'LinkingTo: Matrix') and
this time it was necessary for CRAN to additionally re-install
those linking-reverse-dependent packages .. on all platforms, in
time, etc. a somewhat brittle task all with a CRAN check system
that simultaneously runs other package installations and checks.

I think you were slightly unlucky in the timing of your package
checks/submission.

Best regards,
Martin


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Nov  9 21:08:07 2023
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 09 Nov 2023 12:08:07 -0800
Subject: [R] 
 Why Rprofile.site is not built with manual installation of R
 devel in linux?
In-Reply-To: <AM6PR02MB44230F37BC7E7A139581925F94AFA@AM6PR02MB4423.eurprd02.prod.outlook.com>
References: <AM6PR02MB44230F37BC7E7A139581925F94AFA@AM6PR02MB4423.eurprd02.prod.outlook.com>
Message-ID: <205A4E17-50C5-45F0-8D91-F400C1B89E6E@dcn.davis.ca.us>

No clue. Tip: R-devel is the mailing list for anything related to development versions of R. Off-topic here.

On November 9, 2023 2:59:44 AM PST, "Iago Gin? V?zquez" <iago.gine at sjd.es> wrote:
>Hi all,
>
>I downloaded R-devel as explicited in https://developer.r-project.org/SVNtips.html
>Then, I tried to install it through instructions in https://cran.r-project.org/doc/manuals/r-devel/R-admin.html#Installation
>(taking into account also https://stat.ethz.ch/pipermail/r-devel/2016-May/072777.html)
>So:
>export REPOS=https://svn.r-project.org/R
>export RTOP=~ #adjust as necessary
>cd $RTOP
>svn co $REPOS/trunk r-devel/R
>cd r-devel/R
>tools/rsync-recommended
>mkdir ../build-R
> cd ../build-R
> ../R/configure --prefix=/where/you/want/R/to/go
> make
> make check
>make install
>make install-tests
>cd tests
>## followed by one of
>../bin/R CMD make check
>../bin/R CMD make check-devel
>
>And here I get the following error
>checking package 'base'
>Error in file(filename, "r", encoding = encoding) :
>  cannot open the connection
>Calls: source -> file
>In addition: Warning message:
>In file(filename, "r", encoding = encoding) :
>  cannot open file '.../etc/Rprofile.site': No such file or directory
>Execution halted
>
>where the dots ... specify the path to the build-R folder where R-devel was built. And I check the etc folder and indeed there is no the Rprofile.site
>-rw-r--r-- 1 iago iago  209 Nov  9 08:27 javaconf
>-rw-r--r-- 1 iago iago  770 Nov  9 08:35 ldpaths
>-rw-r--r-- 1 iago iago 6672 Nov  9 08:35 Makeconf
>-rw-r--r-- 1 iago iago 3336 Nov  9 08:27 Makefile
>-rw-r--r-- 1 iago iago 1853 Nov  9 08:27 Renviron
>-rw-r--r-- 1 iago iago 1173 Nov  9 08:32 repositories
>
>I note that make install installed R in the path I specified in  ../R/configure --prefix=/where/you/want/R/to/go
>however
>    1. make install-tests installed the tests folder in build-R .
>    2.  In the installed R in /where/you/want/R/to/go, there is no even etc folder, there are only the folders bin, lib and share.
>
>Am I  skipping some step? I am on Debain 12.
>
>Thank you!
>
>
>Iago
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From r@oknz @end|ng |rom gm@||@com  Fri Nov 10 09:01:20 2023
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Fri, 10 Nov 2023 21:01:20 +1300
Subject: [R] strptime with +03:00 zone designator
In-Reply-To: <25928.43905.917277.897930@stat.math.ethz.ch>
References: <CABcYAd+CTsSRuay3uE0S-k5k+uLvjZX=ZWuy2P6sCbFzkbFmtg@mail.gmail.com>
 <CAAxdm-666SwXzBgKJEn-HWphspbWDb2ycyVzdAbvD+oX16xRVA@mail.gmail.com>
 <CABcYAdJpMibwW7Qjm_0A2D+1H=ddvow6S12RaKRUx-frR210tg@mail.gmail.com>
 <CABcYAd+y6v7YTOtE0QrDciNNSpGZb22MFqsWA=xB+e1-q-QcLg@mail.gmail.com>
 <25928.43905.917277.897930@stat.math.ethz.ch>
Message-ID: <CABcYAd+pvgu1RJVvOZSCxoovL8Xrmt=-AET-cKhbEgDeVR4O6w@mail.gmail.com>

(1) I hadn't tried to submit a bug report because (a) I wasn't sure
that it was an actual limitation in strptime() and not my ignorance or
stupidity and (b) I didn't actually know how to submit a bug report.
> bug.report(package = "base")
brings up a web page.  Clicking on "File a Bug" takes me to another page,
https://bugs.r-project.org/enter_bug.cgi
which says "
Bugzilla needs a legitimate login and password to continue
" and I don't have a login or a password for this.  The process is
more hassle than I really wanted to go through. Oh well, I'd better do
the thing properly.

(2) The relevant aspect of the ISO 8601 standard has been stable since
1988.  There are two ways to display any time thingy, a basic format
(where NONE of the optional separators are present) and an extended
format (where ALL of the optional separators are present).  There's no
half-and-half form.

On Mon, 6 Nov 2023 at 22:01, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
>
> >>>>> Richard O'Keefe
> >>>>>     on Mon, 6 Nov 2023 18:37:34 +1300 writes:
>
>     > Thanks to all who replied.  On Mon, 6 Nov 2023 at 18:37,
>     > Richard O'Keefe <raoknz at gmail.com> wrote:
>
>     >> OK, so the consensus is (1) One cannot make strptime
>     >> accept ISO8601-compliant zone designators (2) The
>     >> lubridate package can (3) Or one can hack away with
>     >> regex.  Lubridate it is, then.
>     >>
>     >> But I do regard strptime's inability to process
>     >> ISO8601-compliant zone designators as a bug.
>
> Did you try to submit it to R's bugzilla?
>
> It's the first time I hear of this "Feature" of the ISO
> standard, but then I'm not at all a timezone, and even less an
> ISO standard expert.
>
> Best,
> Martin
>
>
>     >> On Mon, 6 Nov 2023 at 13:18, jim holtman
>     >> <jholtman at gmail.com> wrote:
>     >>
>     >>> try using 'lubridate'
>     >>>
>     >>> > library(lubridate)Attaching package: ?lubridate?
>     >>>
>     >>> The following objects are masked from ?package:base?:
>     >>>
>     >>> date, intersect, setdiff, union > x <-
>     >>> "2017-02-28T13:35:00+03:00"> ymd_hms(x)[1] "2017-02-28
>     >>> 10:35:00 UTC"
>     >>>
>     >>> >
>     >>>
>     >>>
>     >>>
>     >>> Thanks
>     >>>
>     >>> Jim Holtman *Data Munger Guru*
>     >>>
>     >>>
>     >>> *What is the problem that you are trying to solve?Tell
>     >>> me what you want to do, not how you want to do it.*
>     >>>
>     >>>
>     >>> On Sun, Nov 5, 2023 at 3:45?PM Richard O'Keefe
>     >>> <raoknz at gmail.com> wrote:
>     >>>
>     >>>> I have some data that includes timestamps like this:
>     >>>> 2017-02-28T13:35:00+03:00 The documentation for
>     >>>> strptime says that %z expects an offset like 0300.  I
>     >>>> don't see any way in the documentation to get it to
>     >>>> accept +hh:mm with a colon separator, and everything I
>     >>>> tried gave me NA as the answer.
>     >>>>
>     >>>> Section 4.2.5.1 of ISO 8601:2004(E) allows both the
>     >>>> absence of colons in +hh[mm] (basic format) and the
>     >>>> presence of colons in +hh:mm (extended format).  Again
>     >>>> in section 4.2.5.2 where a zone offset is combined with
>     >>>> a time of day: if you have hh:mm:ss you are using
>     >>>> extended format and the offset MUST have a colon; if
>     >>>> you have hhmmss you are using basic format and the
>     >>>> offset MUST NOT have a colon.  And again in section
>     >>>> 4.3.2 (complete representations of date and time of
>     >>>> day).  If you use hyphens and colons in the date and
>     >>>> time part you MUST have a colon in the zone designator.
>     >>>>
>     >>>> So I am dealing with timestamps in strict ISO 8601
>     >>>> complete extended representation, and it is rather
>     >>>> frustrating that strptime doesn't deal with it simply.
>     >>>>
>     >>>> The simplest thing would be for R's own version of
>     >>>> strptime to allow an optional colon between the hour
>     >>>> digits and the minute digits of a zone designator.
>     >>>>
>     >>>> I'm about to clone the data source and edit it to
>     >>>> remove the colons, but is there something obvious I am
>     >>>> missing?
>     >>>>
>     >>>> ______________________________________________
>     >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>     >>>> more, see https://stat.ethz.ch/mailman/listinfo/r-help
>     >>>> PLEASE do read the posting guide
>     >>>> http://www.R-project.org/posting-guide.html and provide
>     >>>> commented, minimal, self-contained, reproducible code.
>     >>>>
>     >>>
>
>     >   [[alternative HTML version deleted]]
>
>     > ______________________________________________
>     > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>     > more, see https://stat.ethz.ch/mailman/listinfo/r-help
>     > PLEASE do read the posting guide
>     > http://www.R-project.org/posting-guide.html and provide
>     > commented, minimal, self-contained, reproducible code.


From |kw@|mmo @end|ng |rom gm@||@com  Fri Nov 10 09:43:40 2023
From: |kw@|mmo @end|ng |rom gm@||@com (Iris Simmons)
Date: Fri, 10 Nov 2023 03:43:40 -0500
Subject: [R] Calling Emacs Lisp Code/Function from R
Message-ID: <CADNULg_3vytsPFkWJTuXgwC9HNtZ7sJFcX=TX=kqHvSvoZjFgw@mail.gmail.com>

Hi,


I'm using R in Emacs and I'm interested in programatically knowing the
details of all opened buffers; details such a buffer name, size, mode,
and possibly associated filename. I've been able to write such a
function in Emacs Lisp, but now I'd like to be able to call that
function from R, or if that's not possible then calling it from C
would be fine.

Does anyone know if this is possible? And if so, could you perhaps
direct me towards an existing R package or program that calls an Emacs
Lisp function from R that I could use as a guide?


Thank you!
    Iris


From murdoch@dunc@n @end|ng |rom gm@||@com  Fri Nov 10 11:18:12 2023
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Fri, 10 Nov 2023 05:18:12 -0500
Subject: [R] Calling Emacs Lisp Code/Function from R
In-Reply-To: <CADNULg_3vytsPFkWJTuXgwC9HNtZ7sJFcX=TX=kqHvSvoZjFgw@mail.gmail.com>
References: <CADNULg_3vytsPFkWJTuXgwC9HNtZ7sJFcX=TX=kqHvSvoZjFgw@mail.gmail.com>
Message-ID: <6b5a51f6-af79-4382-97bd-4185f9dcd6f8@gmail.com>

I'm not an Emacs user, but the ESS-help mailing list (see 
ess.r-project.org) might be able to help with this.

Duncan Murdoch

On 10/11/2023 3:43 a.m., Iris Simmons wrote:
> Hi,
> 
> 
> I'm using R in Emacs and I'm interested in programatically knowing the
> details of all opened buffers; details such a buffer name, size, mode,
> and possibly associated filename. I've been able to write such a
> function in Emacs Lisp, but now I'd like to be able to call that
> function from R, or if that's not possible then calling it from C
> would be fine.
> 
> Does anyone know if this is possible? And if so, could you perhaps
> direct me towards an existing R package or program that calls an Emacs
> Lisp function from R that I could use as a guide?
> 
> 
> Thank you!
>      Iris
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m@rt|n@@@gregory @end|ng |rom |c|oud@com  Fri Nov 10 12:12:39 2023
From: m@rt|n@@@gregory @end|ng |rom |c|oud@com (Martin Gregory)
Date: Fri, 10 Nov 2023 12:12:39 +0100
Subject: [R] Calling Emacs Lisp Code/Function from R
In-Reply-To: <6b5a51f6-af79-4382-97bd-4185f9dcd6f8@gmail.com>
References: <CADNULg_3vytsPFkWJTuXgwC9HNtZ7sJFcX=TX=kqHvSvoZjFgw@mail.gmail.com>
 <6b5a51f6-af79-4382-97bd-4185f9dcd6f8@gmail.com>
Message-ID: <392e4973-651f-425d-a0a2-2828d2d3347f@icloud.com>

Hi,

if you run a server in your Emacs session you can use emacsclient to 
send a lisp call to the server. There's an example here:

https://emacs.stackexchange.com/questions/54156/how-can-i-query-emacs-from-a-separate-process/54161#54161

Regards,
Martin Gregory

On 11/10/23 11:18, Duncan Murdoch wrote:
> I'm not an Emacs user, but the ESS-help mailing list (see 
> ess.r-project.org) might be able to help with this.
> 
> Duncan Murdoch
> 
> On 10/11/2023 3:43 a.m., Iris Simmons wrote:
>> Hi,
>>
>>
>> I'm using R in Emacs and I'm interested in programatically knowing the
>> details of all opened buffers; details such a buffer name, size, mode,
>> and possibly associated filename. I've been able to write such a
>> function in Emacs Lisp, but now I'd like to be able to call that
>> function from R, or if that's not possible then calling it from C
>> would be fine.
>>
>> Does anyone know if this is possible? And if so, could you perhaps
>> direct me towards an existing R package or program that calls an Emacs
>> Lisp function from R that I could use as a guide?
>>
>>
>> Thank you!
>> ???? Iris
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From |kw@|mmo @end|ng |rom gm@||@com  Sun Nov 12 09:56:24 2023
From: |kw@|mmo @end|ng |rom gm@||@com (Iris Simmons)
Date: Sun, 12 Nov 2023 03:56:24 -0500
Subject: [R] Calling Emacs Lisp Code/Function from R
In-Reply-To: <392e4973-651f-425d-a0a2-2828d2d3347f@icloud.com>
References: <CADNULg_3vytsPFkWJTuXgwC9HNtZ7sJFcX=TX=kqHvSvoZjFgw@mail.gmail.com>
 <6b5a51f6-af79-4382-97bd-4185f9dcd6f8@gmail.com>
 <392e4973-651f-425d-a0a2-2828d2d3347f@icloud.com>
Message-ID: <CADNULg-fVWh4W6Zt_yvTRvz0QsRS=O1h9vtz06G6OJr-EtBtvQ@mail.gmail.com>

Hi,


Thank you Duncan, I will check with that other mailing list to see if
they can guide me.
And thank you Martin, I was able to implement what I wanted using the
example you sent.

On Fri, Nov 10, 2023 at 6:55?AM Martin Gregory via R-help
<r-help at r-project.org> wrote:
>
> Hi,
>
> if you run a server in your Emacs session you can use emacsclient to
> send a lisp call to the server. There's an example here:
>
> https://emacs.stackexchange.com/questions/54156/how-can-i-query-emacs-from-a-separate-process/54161#54161
>
> Regards,
> Martin Gregory
>
> On 11/10/23 11:18, Duncan Murdoch wrote:
> > I'm not an Emacs user, but the ESS-help mailing list (see
> > ess.r-project.org) might be able to help with this.
> >
> > Duncan Murdoch
> >
> > On 10/11/2023 3:43 a.m., Iris Simmons wrote:
> >> Hi,
> >>
> >>
> >> I'm using R in Emacs and I'm interested in programatically knowing the
> >> details of all opened buffers; details such a buffer name, size, mode,
> >> and possibly associated filename. I've been able to write such a
> >> function in Emacs Lisp, but now I'd like to be able to call that
> >> function from R, or if that's not possible then calling it from C
> >> would be fine.
> >>
> >> Does anyone know if this is possible? And if so, could you perhaps
> >> direct me towards an existing R package or program that calls an Emacs
> >> Lisp function from R that I could use as a guide?
> >>
> >>
> >> Thank you!
> >>      Iris
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


