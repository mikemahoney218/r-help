From neh@@bo|ogn@90 @end|ng |rom gm@||@com  Fri Jul  1 00:03:35 2022
From: neh@@bo|ogn@90 @end|ng |rom gm@||@com (Neha gupta)
Date: Fri, 1 Jul 2022 00:03:35 +0200
Subject: [R] Ranger could not work with caret
In-Reply-To: <0777f610-e580-c6e0-bb55-645bdbf32a0b@sapo.pt>
References: <CA+nrPnuc9+-R65CgJq_qNbuRTj491Jfv7YdbRgqd1Tom6ww=9Q@mail.gmail.com>
 <CA+nrPnvG5a26MFJJWQtuE9ZkCjHxqSb5gR7igr2kiBVGkQjUXw@mail.gmail.com>
 <0777f610-e580-c6e0-bb55-645bdbf32a0b@sapo.pt>
Message-ID: <CA+nrPnu+awO6q=2eVpYom9yi7JT9FgAh0k0mcT=-9eLj1H7qAg@mail.gmail.com>

Ok, the data is pasted below

But on the same data (everything the same) and with other models like RF,
SVM etc, it works fine.


> dput(head(tr, 30))
structure(list(recordnumber = c(0, 0.02, 0.04, 0.06, 0.07, 0.08,
0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.16, 0.17, 0.18, 0.23, 0.24,
0.25, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.35, 0.36, 0.37, 0.38,
0.4, 0.41), projectname = structure(c(1L, 1L, 1L, 1L, 2L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
4L, 4L, 4L, 4L, 4L, 4L, 5L, 6L), levels = c("de", "erb", "gal",
"X", "hst", "slp", "spl", "Y"), class = "factor"), cat2 = structure(c(3L,
3L, 3L, 3L, 3L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L,
9L, 11L, 5L, 4L, 6L, 8L, 3L, 9L, 9L, 9L, 9L, 6L, 7L), levels =
c("Avionics",
"application_ground", "avionicsmonitoring", "batchdataprocessing",
"communications", "datacapture", "launchprocessing", "missionplanning",
"monitor_control", "operatingsystem", "realdataprocessing", "science",
"simulation", "utility"), class = "factor"), forg = structure(c(2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), levels = c("f",
"g"), class = "factor"), center = structure(c(2L, 2L, 2L, 2L,
2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 6L), levels = c("1", "2",
"3", "4", "5", "6"), class = "factor"), year = c(0.5, 0.5, 0.5,
0.5, 0.6875, 0.5625, 0.5625, 0.8125, 0.5625, 0.875, 0.5625, 0.75,
0.5625, 0.8125, 0.75, 0.9375, 0.9375, 0.9375, 0.6875, 0.6875,
0.6875, 0.6875, 0.875, 1, 0.9375, 0.9375, 0.9375, 0.9375, 0.5625,
0.25), mode = structure(c(3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L), levels = c("embedded", "organic", "semidetached"
), class = "factor"), rely = structure(c(4L, 4L, 4L, 4L, 4L,
4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 3L, 3L, 3L, 3L,
3L, 4L, 4L, 4L, 3L, 3L, 3L, 3L, 4L), levels = c("vl", "l", "n",
"h", "vh", "xh"), class = "factor"), data = structure(c(2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L,
5L, 5L, 5L, 5L, 2L, 2L, 3L, 3L, 3L, 3L, 4L, 2L), levels = c("vl",
"l", "n", "h", "vh", "xh"), class = "factor"), cplx = structure(c(4L,
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 4L,
3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L), levels = c("vl",
"l", "n", "h", "vh", "xh"), class = "factor"), time = structure(c(3L,
3L, 3L, 3L, 3L, 6L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L,
3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 5L, 3L), levels = c("vl",
"l", "n", "h", "vh", "xh"), class = "factor"), stor = structure(c(3L,
3L, 3L, 3L, 3L, 6L, 3L, 3L, 3L, 3L, 3L, 3L, 6L, 3L, 3L, 3L, 3L,
3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L), levels = c("vl",
"l", "n", "h", "vh", "xh"), class = "factor"), virt = structure(c(2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 4L, 2L, 2L, 2L, 2L, 3L, 3L,
3L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 2L, 2L), levels = c("vl",
"l", "n", "h", "vh", "xh"), class = "factor"), turn = structure(c(2L,
2L, 2L, 2L, 2L, 4L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L,
3L, 4L, 4L, 4L, 4L, 2L, 2L, 3L, 3L, 3L, 3L, 4L, 2L), levels = c("vl",
"l", "n", "h", "vh", "xh"), class = "factor"), acap = structure(c(3L,
3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 3L,
3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L), levels = c("vl",
"l", "n", "h", "vh", "xh"), class = "factor"), aexp = structure(c(3L,
3L, 3L, 3L, 3L, 4L, 5L, 5L, 5L, 5L, 4L, 5L, 5L, 4L, 5L, 4L, 4L,
4L, 4L, 4L, 4L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), levels = c("vl",
"l", "n", "h", "vh", "xh"), class = "factor"), pcap = structure(c(3L,
3L, 3L, 3L, 3L, 4L, 5L, 4L, 5L, 3L, 4L, 4L, 5L, 4L, 4L, 4L, 4L,
4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L, 4L, 4L), levels = c("vl",
"l", "n", "h", "vh", "xh"), class = "factor"), vexp = structure(c(3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 2L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 2L, 3L), levels = c("vl",
"l", "n", "h", "vh", "xh"), class = "factor"), lexp = structure(c(4L,
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 2L, 1L, 4L, 4L, 4L, 4L, 3L, 3L,
3L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 3L, 4L, 3L, 4L, 3L), levels = c("vl",
"l", "n", "h", "vh", "xh"), class = "factor"), modp = structure(c(4L,
4L, 4L, 4L, 4L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 5L, 5L, 5L, 5L, 4L, 4L, 3L, 3L, 4L, 3L, 4L, 4L), levels = c("vl",
"l", "n", "h", "vh", "xh"), class = "factor"), tool = structure(c(3L,
3L, 3L, 3L, 3L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 4L, 3L, 3L, 1L), levels = c("vl",
"l", "n", "h", "vh", "xh"), class = "factor"), sced = structure(c(2L,
2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
3L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 2L, 3L), levels = c("vl",
"l", "n", "h", "vh", "xh"), class = "factor"), equivphyskloc = c(0.025534,
0.006945, 0.008988, 0.002655, 0.067102, 0.006741, 0.019508, 0.005209,
0.101215, 0.010622, 0.101215, 0.019508, 0.152283, 0.031253, 0.014401,
0.014401, 0.037892, 0.009294, 0.015729, 0.012154, 0.032377, 0.035339,
0.004698, 0.009703, 0.00572, 0.012358, 0.091002, 0.007252, 0.180778,
0.307527), act_effort = c(117.6, 31.2, 25.2, 10.8, 352.8, 72,
72, 24, 360, 36, 215, 48, 324, 60, 48, 90, 210, 48, 82, 62, 170,
192, 18, 50, 42, 60, 444, 42, 1248, 2400)), row.names = c(1L,
3L, 5L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 17L, 18L, 19L,
24L, 25L, 26L, 29L, 30L, 31L, 32L, 33L, 34L, 36L, 37L, 38L, 39L,
41L, 42L), class = "data.frame")



On Thu, Jun 30, 2022 at 11:28 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> Please post data in dput format, without it it's difficult to tell.
> If I substitute
>
> mpg for act_effort
> mtcars for tr
>
> keeping everything else, I don't get any errors.
> And the error message says clearly that the error is in tr (data).
>
> Can you post the output of dput(head(tr, 30))?
>
> Rui Barradas
>
>
> ?s 19:32 de 30/06/2022, Neha gupta escreveu:
> > I posted it for the second time as I didn't get any response from group
> > members. I am not sure if some problem is with the question.
> >
> >
> >
> > I cannot run the "ranger" model with caret. I am only using the farff and
> > caret libraries and the following code:
> >
> > boot <- trainControl(method = "cv", number=10)
> >
> > c1 <-train(act_effort ~ ., data = tr,
> >                method = "ranger",
> >                 tuneLength = 5,
> >                metric = "MAE",
> >                preProc = c("center", "scale", "nzv"),
> >                trControl = boot)
> >
> > The error I get is the repeating of the following message until I
> interrupt
> > it.
> >
> > Error: mtry can not be larger than number of variables in data. Ranger
> will
> > EXIT now.
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From n@re@h_gurbux@n| @end|ng |rom hotm@||@com  Fri Jul  1 13:07:37 2022
From: n@re@h_gurbux@n| @end|ng |rom hotm@||@com (Naresh Gurbuxani)
Date: Fri, 1 Jul 2022 07:07:37 -0400
Subject: [R] split apply on multiple variables
Message-ID: <BL0PR01MB4036C50C9F934D49026D6592FABD9@BL0PR01MB4036.prod.exchangelabs.com>


I am looking for a more general solution to below exercise.

Thanks,
Naresh

library(plyr)
mydf <- data.frame(
date = rep(seq.Date(from = as.Date("2022-06-01"), by = 1, length.out =
10), 4),
account = c(rep("ABC", 20), rep("XYZ", 20)),
client = c(rep("P", 10), rep("Q", 10), rep("R", 10), rep("S", 10)),
profit = round(runif(40, 2, 5), 2), sales = round(runif(40, 10, 20), 2))

mydf.split <- split(mydf, mydf$account)

# if there are 10 variables like sales, profit, etc., need 10 lines
myres <- lapply(mydf.split, function(df) {
sales.ts <- aggregate(sales ~ date, FUN = sum, data = df) #one step for both?
profit.ts <- aggregate(profit ~ date, FUN = sum, data = df)
merge(profit.ts, sales.ts, by = "date")})

myres.df <- ldply(myres)


From petr@p|k@| @end|ng |rom prechez@@cz  Fri Jul  1 13:17:06 2022
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Fri, 1 Jul 2022 11:17:06 +0000
Subject: [R] split apply on multiple variables
In-Reply-To: <BL0PR01MB4036C50C9F934D49026D6592FABD9@BL0PR01MB4036.prod.exchangelabs.com>
References: <BL0PR01MB4036C50C9F934D49026D6592FABD9@BL0PR01MB4036.prod.exchangelabs.com>
Message-ID: <933483ad18b64e448852fa7f05666924@SRVEXCHCM1302.precheza.cz>

Hi

Maybe
lapply(mydf.split, function(x) aggregate(x[,4:5], list(x$date), sum))

For your particular case, but lacking overall genarality.

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Naresh Gurbuxani
> Sent: Friday, July 1, 2022 1:08 PM
> To: r-help at r-project.org
> Subject: [R] split apply on multiple variables
> 
> 
> I am looking for a more general solution to below exercise.
> 
> Thanks,
> Naresh
> 
> library(plyr)
> mydf <- data.frame(
> date = rep(seq.Date(from = as.Date("2022-06-01"), by = 1, length.out =
> 10), 4),
> account = c(rep("ABC", 20), rep("XYZ", 20)),
> client = c(rep("P", 10), rep("Q", 10), rep("R", 10), rep("S", 10)),
> profit = round(runif(40, 2, 5), 2), sales = round(runif(40, 10, 20), 2))
> 
> mydf.split <- split(mydf, mydf$account)
> 
> # if there are 10 variables like sales, profit, etc., need 10 lines
> myres <- lapply(mydf.split, function(df) {
> sales.ts <- aggregate(sales ~ date, FUN = sum, data = df) #one step for
both?
> profit.ts <- aggregate(profit ~ date, FUN = sum, data = df)
> merge(profit.ts, sales.ts, by = "date")})
> 
> myres.df <- ldply(myres)
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From n@re@h_gurbux@n| @end|ng |rom hotm@||@com  Fri Jul  1 13:19:18 2022
From: n@re@h_gurbux@n| @end|ng |rom hotm@||@com (Naresh Gurbuxani)
Date: Fri, 1 Jul 2022 11:19:18 +0000
Subject: [R] split apply on multiple variables
In-Reply-To: <BL0PR01MB4036C50C9F934D49026D6592FABD9@BL0PR01MB4036.prod.exchangelabs.com>
References: <BL0PR01MB4036C50C9F934D49026D6592FABD9@BL0PR01MB4036.prod.exchangelabs.com>
Message-ID: <BL0PR01MB4036199CCFCDF1D2A2D00E7FFABD9@BL0PR01MB4036.prod.exchangelabs.com>

I have solved this problem. 

ddply(mydf, c("date", "account"), function(df) {
profit = sum(profit); 
sales = sum(sales); 
data.frame(profit = profit, sales = sales)}) 


From: R-help <r-help-bounces at r-project.org> on behalf of Naresh Gurbuxani <naresh_gurbuxani at hotmail.com>
Sent: Friday, July 1, 2022 7:07 AM
To: r-help at r-project.org <r-help at R-project.org>
Subject: [R] split apply on multiple variables 
?

I am looking for a more general solution to below exercise.

Thanks,
Naresh

library(plyr)
mydf <- data.frame(
date = rep(seq.Date(from = as.Date("2022-06-01"), by = 1, length.out =
10), 4),
account = c(rep("ABC", 20), rep("XYZ", 20)),
client = c(rep("P", 10), rep("Q", 10), rep("R", 10), rep("S", 10)),
profit = round(runif(40, 2, 5), 2), sales = round(runif(40, 10, 20), 2))

mydf.split <- split(mydf, mydf$account)

# if there are 10 variables like sales, profit, etc., need 10 lines
myres <- lapply(mydf.split, function(df) {
sales.ts <- aggregate(sales ~ date, FUN = sum, data = df) #one step for both?
profit.ts <- aggregate(profit ~ date, FUN = sum, data = df)
merge(profit.ts, sales.ts, by = "date")})

myres.df <- ldply(myres)

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri Jul  1 17:24:29 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 1 Jul 2022 16:24:29 +0100
Subject: [R] Ranger could not work with caret
In-Reply-To: <CA+nrPnu+awO6q=2eVpYom9yi7JT9FgAh0k0mcT=-9eLj1H7qAg@mail.gmail.com>
References: <CA+nrPnuc9+-R65CgJq_qNbuRTj491Jfv7YdbRgqd1Tom6ww=9Q@mail.gmail.com>
 <CA+nrPnvG5a26MFJJWQtuE9ZkCjHxqSb5gR7igr2kiBVGkQjUXw@mail.gmail.com>
 <0777f610-e580-c6e0-bb55-645bdbf32a0b@sapo.pt>
 <CA+nrPnu+awO6q=2eVpYom9yi7JT9FgAh0k0mcT=-9eLj1H7qAg@mail.gmail.com>
Message-ID: <85d6347a-21f6-36ff-a92b-f0aae2c45749@sapo.pt>

Hello,

The error is in Ranger parameter mtry becoming greater than the number 
of variables (columns).
mtry can be set manually in caret::train argument tuneGrid. But for 
random forests you must also set the split rule and the minimum node.


library(caret)
library(farff)

boot <- trainControl(method = "cv", number = 10)

# set the maximum mtry manually to ncol(tr)
# this creates a sequence of mtry values
mtry <- var_seq(ncol(tr), len = 3)  # 3 is the default value
mtry
#  [1]  2 13 24
#[1]  2 13 24

splitrule <- c("variance", "extratrees")
min.node.size <- 1:10
mtrygrid <- expand.grid(mtry, splitrule, min.node.size)
names(mtrygrid) <- c("mtry", "splitrule", "min.node.size")

c1 <- train(act_effort ~ ., data = tr,
            method = "ranger",
            tuneLength = 5,
            metric = "MAE",
            preProc = c("center", "scale", "nzv"),
            tuneGrid = mtrygrid,
            trControl = boot)
c1
#  Random Forest
#
#  30 samples
#  23 predictors
#
#  Pre-processing: centered (48), scaled (48), remove (58)
#  Resampling: Cross-Validated (10 fold)
#  Summary of sample sizes: 28, 27, 27, 28, 27, 27, ...
#  Resampling results across tuning parameters:
#
#    mtry  splitrule   min.node.size  RMSE      Rsquared   MAE
#     2    variance     1             256.6391  0.8103759  186.3609
#     2    variance     2             249.7120  0.8628109  183.6696
#     2    variance     3             258.8240  0.8284449  189.0712
#
# [...omit...]
#
#    13    extratrees  10             254.9569  0.8918014  191.2524
#    24    variance     1             177.7188  0.9458652  112.2800
#    24    variance     2             172.6826  0.9204287  108.5943
#    24    variance     3             172.9954  0.9271006  109.2554
#    24    variance     4             172.2467  0.9523067  110.0776
#    24    variance     5             175.2485  0.9283317  112.8798
#    24    variance     6             177.9285  0.9369881  115.8970
#    24    variance     7             180.5959  0.9485035  117.5816
#    24    variance     8             178.8037  0.9358033  117.8725
#    24    variance     9             176.5849  0.9210959  117.0055
#    24    variance    10             178.6439  0.9257969  119.8035
#    24    extratrees   1             219.1368  0.8801770  141.0720
#    24    extratrees   2             216.1900  0.8550002  140.9263
#    24    extratrees   3             212.4138  0.8979379  141.4282
#    24    extratrees   4             218.2631  0.9121471  146.2908
#    24    extratrees   5             212.5679  0.9279598  144.2715
#    24    extratrees   6             218.9856  0.9141754  152.2099
#    24    extratrees   7             222.8540  0.9412682  152.4614
#    24    extratrees   8             228.1156  0.9423414  161.8456
#    24    extratrees   9             226.6182  0.9408306  160.5264
#    24    extratrees  10             226.9280  0.9429413  165.6878
#
#  MAE was used to select the optimal model using the smallest value.
#  The final values used for the model were mtry = 24, splitrule = variance
#   and min.node.size = 2.
plot(c1)



Hope this helps,

Rui Barradas


?s 23:03 de 30/06/2022, Neha gupta escreveu:
> Ok, the data is pasted below
> 
> But on the same data (everything the same) and with other models like 
> RF, SVM etc, it works fine.
> 
>  > dput(head(tr, 30))
> structure(list(recordnumber = c(0, 0.02, 0.04, 0.06, 0.07, 0.08,
> 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.16, 0.17, 0.18, 0.23, 0.24,
> 0.25, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.35, 0.36, 0.37, 0.38,
> 0.4, 0.41), projectname = structure(c(1L, 1L, 1L, 1L, 2L, 3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
> 4L, 4L, 4L, 4L, 4L, 4L, 5L, 6L), levels = c("de", "erb", "gal",
> "X", "hst", "slp", "spl", "Y"), class = "factor"), cat2 = structure(c(3L,
> 3L, 3L, 3L, 3L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L,
> 9L, 11L, 5L, 4L, 6L, 8L, 3L, 9L, 9L, 9L, 9L, 6L, 7L), levels = 
> c("Avionics",
> "application_ground", "avionicsmonitoring", "batchdataprocessing",
> "communications", "datacapture", "launchprocessing", "missionplanning",
> "monitor_control", "operatingsystem", "realdataprocessing", "science",
> "simulation", "utility"), class = "factor"), forg = structure(c(2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), levels = c("f",
> "g"), class = "factor"), center = structure(c(2L, 2L, 2L, 2L,
> 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 6L), levels = c("1", "2",
> "3", "4", "5", "6"), class = "factor"), year = c(0.5, 0.5, 0.5,
> 0.5, 0.6875, 0.5625, 0.5625, 0.8125, 0.5625, 0.875, 0.5625, 0.75,
> 0.5625, 0.8125, 0.75, 0.9375, 0.9375, 0.9375, 0.6875, 0.6875,
> 0.6875, 0.6875, 0.875, 1, 0.9375, 0.9375, 0.9375, 0.9375, 0.5625,
> 0.25), mode = structure(c(3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 3L, 3L, 3L, 3L), levels = c("embedded", "organic", "semidetached"
> ), class = "factor"), rely = structure(c(4L, 4L, 4L, 4L, 4L,
> 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 3L, 3L, 3L, 3L,
> 3L, 4L, 4L, 4L, 3L, 3L, 3L, 3L, 4L), levels = c("vl", "l", "n",
> "h", "vh", "xh"), class = "factor"), data = structure(c(2L, 2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L,
> 5L, 5L, 5L, 5L, 2L, 2L, 3L, 3L, 3L, 3L, 4L, 2L), levels = c("vl",
> "l", "n", "h", "vh", "xh"), class = "factor"), cplx = structure(c(4L,
> 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 4L,
> 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L), levels = c("vl",
> "l", "n", "h", "vh", "xh"), class = "factor"), time = structure(c(3L,
> 3L, 3L, 3L, 3L, 6L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L,
> 3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 5L, 3L), levels = c("vl",
> "l", "n", "h", "vh", "xh"), class = "factor"), stor = structure(c(3L,
> 3L, 3L, 3L, 3L, 6L, 3L, 3L, 3L, 3L, 3L, 3L, 6L, 3L, 3L, 3L, 3L,
> 3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L), levels = c("vl",
> "l", "n", "h", "vh", "xh"), class = "factor"), virt = structure(c(2L,
> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 4L, 2L, 2L, 2L, 2L, 3L, 3L,
> 3L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 2L, 2L), levels = c("vl",
> "l", "n", "h", "vh", "xh"), class = "factor"), turn = structure(c(2L,
> 2L, 2L, 2L, 2L, 4L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L,
> 3L, 4L, 4L, 4L, 4L, 2L, 2L, 3L, 3L, 3L, 3L, 4L, 2L), levels = c("vl",
> "l", "n", "h", "vh", "xh"), class = "factor"), acap = structure(c(3L,
> 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 3L,
> 3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L), levels = c("vl",
> "l", "n", "h", "vh", "xh"), class = "factor"), aexp = structure(c(3L,
> 3L, 3L, 3L, 3L, 4L, 5L, 5L, 5L, 5L, 4L, 5L, 5L, 4L, 5L, 4L, 4L,
> 4L, 4L, 4L, 4L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), levels = c("vl",
> "l", "n", "h", "vh", "xh"), class = "factor"), pcap = structure(c(3L,
> 3L, 3L, 3L, 3L, 4L, 5L, 4L, 5L, 3L, 4L, 4L, 5L, 4L, 4L, 4L, 4L,
> 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L, 4L, 4L), levels = c("vl",
> "l", "n", "h", "vh", "xh"), class = "factor"), vexp = structure(c(3L,
> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 2L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 2L, 3L), levels = c("vl",
> "l", "n", "h", "vh", "xh"), class = "factor"), lexp = structure(c(4L,
> 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 2L, 1L, 4L, 4L, 4L, 4L, 3L, 3L,
> 3L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 3L, 4L, 3L, 4L, 3L), levels = c("vl",
> "l", "n", "h", "vh", "xh"), class = "factor"), modp = structure(c(4L,
> 4L, 4L, 4L, 4L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 5L, 5L, 5L, 5L, 4L, 4L, 3L, 3L, 4L, 3L, 4L, 4L), levels = c("vl",
> "l", "n", "h", "vh", "xh"), class = "factor"), tool = structure(c(3L,
> 3L, 3L, 3L, 3L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 4L, 3L, 3L, 1L), levels = c("vl",
> "l", "n", "h", "vh", "xh"), class = "factor"), sced = structure(c(2L,
> 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> 3L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 2L, 3L), levels = c("vl",
> "l", "n", "h", "vh", "xh"), class = "factor"), equivphyskloc = c(0.025534,
> 0.006945, 0.008988, 0.002655, 0.067102, 0.006741, 0.019508, 0.005209,
> 0.101215, 0.010622, 0.101215, 0.019508, 0.152283, 0.031253, 0.014401,
> 0.014401, 0.037892, 0.009294, 0.015729, 0.012154, 0.032377, 0.035339,
> 0.004698, 0.009703, 0.00572, 0.012358, 0.091002, 0.007252, 0.180778,
> 0.307527), act_effort = c(117.6, 31.2, 25.2, 10.8, 352.8, 72,
> 72, 24, 360, 36, 215, 48, 324, 60, 48, 90, 210, 48, 82, 62, 170,
> 192, 18, 50, 42, 60, 444, 42, 1248, 2400)), row.names = c(1L,
> 3L, 5L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 17L, 18L, 19L,
> 24L, 25L, 26L, 29L, 30L, 31L, 32L, 33L, 34L, 36L, 37L, 38L, 39L,
> 41L, 42L), class = "data.frame")
> 
> 
> 
> On Thu, Jun 30, 2022 at 11:28 PM Rui Barradas <ruipbarradas at sapo.pt 
> <mailto:ruipbarradas at sapo.pt>> wrote:
> 
>     Hello,
> 
>     Please post data in dput format, without it it's difficult to tell.
>     If I substitute
> 
>     mpg for act_effort
>     mtcars for tr
> 
>     keeping everything else, I don't get any errors.
>     And the error message says clearly that the error is in tr (data).
> 
>     Can you post the output of dput(head(tr, 30))?
> 
>     Rui Barradas
> 
> 
>     ?s 19:32 de 30/06/2022, Neha gupta escreveu:
>      > I posted it for the second time as I didn't get any response from
>     group
>      > members. I am not sure if some problem is with the question.
>      >
>      >
>      >
>      > I cannot run the "ranger" model with caret. I am only using the
>     farff and
>      > caret libraries and the following code:
>      >
>      > boot <- trainControl(method = "cv", number=10)
>      >
>      > c1 <-train(act_effort ~ ., data = tr,
>      >? ? ? ? ? ? ? ? method = "ranger",
>      >? ? ? ? ? ? ? ? ?tuneLength = 5,
>      >? ? ? ? ? ? ? ? metric = "MAE",
>      >? ? ? ? ? ? ? ? preProc = c("center", "scale", "nzv"),
>      >? ? ? ? ? ? ? ? trControl = boot)
>      >
>      > The error I get is the repeating of the following message until I
>     interrupt
>      > it.
>      >
>      > Error: mtry can not be larger than number of variables in data.
>     Ranger will
>      > EXIT now.
>      >
>      >? ? ? ?[[alternative HTML version deleted]]
>      >
>      > ______________________________________________
>      > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>     -- To UNSUBSCRIBE and more, see
>      > https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>      > PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>      > and provide commented, minimal, self-contained, reproducible code.
>


From neh@@bo|ogn@90 @end|ng |rom gm@||@com  Fri Jul  1 18:18:36 2022
From: neh@@bo|ogn@90 @end|ng |rom gm@||@com (Neha gupta)
Date: Fri, 1 Jul 2022 18:18:36 +0200
Subject: [R] Ranger could not work with caret
In-Reply-To: <85d6347a-21f6-36ff-a92b-f0aae2c45749@sapo.pt>
References: <CA+nrPnuc9+-R65CgJq_qNbuRTj491Jfv7YdbRgqd1Tom6ww=9Q@mail.gmail.com>
 <CA+nrPnvG5a26MFJJWQtuE9ZkCjHxqSb5gR7igr2kiBVGkQjUXw@mail.gmail.com>
 <0777f610-e580-c6e0-bb55-645bdbf32a0b@sapo.pt>
 <CA+nrPnu+awO6q=2eVpYom9yi7JT9FgAh0k0mcT=-9eLj1H7qAg@mail.gmail.com>
 <85d6347a-21f6-36ff-a92b-f0aae2c45749@sapo.pt>
Message-ID: <CA+nrPnuzNEhv88j4LR0SXXRMqcSCWQGmef_-BsWJO8ubN1Oe9Q@mail.gmail.com>

Thank you so much for your help. I hope it will work.

However, why the same error doesn't arise when I am using rf. They both
have the same parameters and it's default values.

Best regards

On Friday, July 1, 2022, Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> The error is in Ranger parameter mtry becoming greater than the number of
> variables (columns).
> mtry can be set manually in caret::train argument tuneGrid. But for random
> forests you must also set the split rule and the minimum node.
>
>
> library(caret)
> library(farff)
>
> boot <- trainControl(method = "cv", number = 10)
>
> # set the maximum mtry manually to ncol(tr)
> # this creates a sequence of mtry values
> mtry <- var_seq(ncol(tr), len = 3)  # 3 is the default value
> mtry
> #  [1]  2 13 24
> #[1]  2 13 24
>
> splitrule <- c("variance", "extratrees")
> min.node.size <- 1:10
> mtrygrid <- expand.grid(mtry, splitrule, min.node.size)
> names(mtrygrid) <- c("mtry", "splitrule", "min.node.size")
>
> c1 <- train(act_effort ~ ., data = tr,
>            method = "ranger",
>            tuneLength = 5,
>            metric = "MAE",
>            preProc = c("center", "scale", "nzv"),
>            tuneGrid = mtrygrid,
>            trControl = boot)
> c1
> #  Random Forest
> #
> #  30 samples
> #  23 predictors
> #
> #  Pre-processing: centered (48), scaled (48), remove (58)
> #  Resampling: Cross-Validated (10 fold)
> #  Summary of sample sizes: 28, 27, 27, 28, 27, 27, ...
> #  Resampling results across tuning parameters:
> #
> #    mtry  splitrule   min.node.size  RMSE      Rsquared   MAE
> #     2    variance     1             256.6391  0.8103759  186.3609
> #     2    variance     2             249.7120  0.8628109  183.6696
> #     2    variance     3             258.8240  0.8284449  189.0712
> #
> # [...omit...]
> #
> #    13    extratrees  10             254.9569  0.8918014  191.2524
> #    24    variance     1             177.7188  0.9458652  112.2800
> #    24    variance     2             172.6826  0.9204287  108.5943
> #    24    variance     3             172.9954  0.9271006  109.2554
> #    24    variance     4             172.2467  0.9523067  110.0776
> #    24    variance     5             175.2485  0.9283317  112.8798
> #    24    variance     6             177.9285  0.9369881  115.8970
> #    24    variance     7             180.5959  0.9485035  117.5816
> #    24    variance     8             178.8037  0.9358033  117.8725
> #    24    variance     9             176.5849  0.9210959  117.0055
> #    24    variance    10             178.6439  0.9257969  119.8035
> #    24    extratrees   1             219.1368  0.8801770  141.0720
> #    24    extratrees   2             216.1900  0.8550002  140.9263
> #    24    extratrees   3             212.4138  0.8979379  141.4282
> #    24    extratrees   4             218.2631  0.9121471  146.2908
> #    24    extratrees   5             212.5679  0.9279598  144.2715
> #    24    extratrees   6             218.9856  0.9141754  152.2099
> #    24    extratrees   7             222.8540  0.9412682  152.4614
> #    24    extratrees   8             228.1156  0.9423414  161.8456
> #    24    extratrees   9             226.6182  0.9408306  160.5264
> #    24    extratrees  10             226.9280  0.9429413  165.6878
> #
> #  MAE was used to select the optimal model using the smallest value.
> #  The final values used for the model were mtry = 24, splitrule = variance
> #   and min.node.size = 2.
> plot(c1)
>
>
>
> Hope this helps,
>
> Rui Barradas
>
>
> ?s 23:03 de 30/06/2022, Neha gupta escreveu:
>
>> Ok, the data is pasted below
>>
>> But on the same data (everything the same) and with other models like RF,
>> SVM etc, it works fine.
>>
>>  > dput(head(tr, 30))
>> structure(list(recordnumber = c(0, 0.02, 0.04, 0.06, 0.07, 0.08,
>> 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.16, 0.17, 0.18, 0.23, 0.24,
>> 0.25, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.35, 0.36, 0.37, 0.38,
>> 0.4, 0.41), projectname = structure(c(1L, 1L, 1L, 1L, 2L, 3L,
>> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>> 4L, 4L, 4L, 4L, 4L, 4L, 5L, 6L), levels = c("de", "erb", "gal",
>> "X", "hst", "slp", "spl", "Y"), class = "factor"), cat2 = structure(c(3L,
>> 3L, 3L, 3L, 3L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L,
>> 9L, 11L, 5L, 4L, 6L, 8L, 3L, 9L, 9L, 9L, 9L, 6L, 7L), levels =
>> c("Avionics",
>> "application_ground", "avionicsmonitoring", "batchdataprocessing",
>> "communications", "datacapture", "launchprocessing", "missionplanning",
>> "monitor_control", "operatingsystem", "realdataprocessing", "science",
>> "simulation", "utility"), class = "factor"), forg = structure(c(2L,
>> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), levels = c("f",
>> "g"), class = "factor"), center = structure(c(2L, 2L, 2L, 2L,
>> 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L,
>> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 6L), levels = c("1", "2",
>> "3", "4", "5", "6"), class = "factor"), year = c(0.5, 0.5, 0.5,
>> 0.5, 0.6875, 0.5625, 0.5625, 0.8125, 0.5625, 0.875, 0.5625, 0.75,
>> 0.5625, 0.8125, 0.75, 0.9375, 0.9375, 0.9375, 0.6875, 0.6875,
>> 0.6875, 0.6875, 0.875, 1, 0.9375, 0.9375, 0.9375, 0.9375, 0.5625,
>> 0.25), mode = structure(c(3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>> 3L, 3L, 3L, 3L, 3L), levels = c("embedded", "organic", "semidetached"
>> ), class = "factor"), rely = structure(c(4L, 4L, 4L, 4L, 4L,
>> 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 3L, 3L, 3L, 3L,
>> 3L, 4L, 4L, 4L, 3L, 3L, 3L, 3L, 4L), levels = c("vl", "l", "n",
>> "h", "vh", "xh"), class = "factor"), data = structure(c(2L, 2L,
>> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L,
>> 5L, 5L, 5L, 5L, 2L, 2L, 3L, 3L, 3L, 3L, 4L, 2L), levels = c("vl",
>> "l", "n", "h", "vh", "xh"), class = "factor"), cplx = structure(c(4L,
>> 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 4L,
>> 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L), levels = c("vl",
>> "l", "n", "h", "vh", "xh"), class = "factor"), time = structure(c(3L,
>> 3L, 3L, 3L, 3L, 6L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L,
>> 3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 5L, 3L), levels = c("vl",
>> "l", "n", "h", "vh", "xh"), class = "factor"), stor = structure(c(3L,
>> 3L, 3L, 3L, 3L, 6L, 3L, 3L, 3L, 3L, 3L, 3L, 6L, 3L, 3L, 3L, 3L,
>> 3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L), levels = c("vl",
>> "l", "n", "h", "vh", "xh"), class = "factor"), virt = structure(c(2L,
>> 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 4L, 2L, 2L, 2L, 2L, 3L, 3L,
>> 3L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 2L, 2L), levels = c("vl",
>> "l", "n", "h", "vh", "xh"), class = "factor"), turn = structure(c(2L,
>> 2L, 2L, 2L, 2L, 4L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L,
>> 3L, 4L, 4L, 4L, 4L, 2L, 2L, 3L, 3L, 3L, 3L, 4L, 2L), levels = c("vl",
>> "l", "n", "h", "vh", "xh"), class = "factor"), acap = structure(c(3L,
>> 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 3L,
>> 3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L), levels = c("vl",
>> "l", "n", "h", "vh", "xh"), class = "factor"), aexp = structure(c(3L,
>> 3L, 3L, 3L, 3L, 4L, 5L, 5L, 5L, 5L, 4L, 5L, 5L, 4L, 5L, 4L, 4L,
>> 4L, 4L, 4L, 4L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), levels = c("vl",
>> "l", "n", "h", "vh", "xh"), class = "factor"), pcap = structure(c(3L,
>> 3L, 3L, 3L, 3L, 4L, 5L, 4L, 5L, 3L, 4L, 4L, 5L, 4L, 4L, 4L, 4L,
>> 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L, 4L, 4L), levels = c("vl",
>> "l", "n", "h", "vh", "xh"), class = "factor"), vexp = structure(c(3L,
>> 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 2L, 3L, 3L, 3L, 3L, 3L, 3L,
>> 3L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 2L, 3L), levels = c("vl",
>> "l", "n", "h", "vh", "xh"), class = "factor"), lexp = structure(c(4L,
>> 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 2L, 1L, 4L, 4L, 4L, 4L, 3L, 3L,
>> 3L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 3L, 4L, 3L, 4L, 3L), levels = c("vl",
>> "l", "n", "h", "vh", "xh"), class = "factor"), modp = structure(c(4L,
>> 4L, 4L, 4L, 4L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>> 3L, 5L, 5L, 5L, 5L, 4L, 4L, 3L, 3L, 4L, 3L, 4L, 4L), levels = c("vl",
>> "l", "n", "h", "vh", "xh"), class = "factor"), tool = structure(c(3L,
>> 3L, 3L, 3L, 3L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>> 3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 4L, 3L, 3L, 1L), levels = c("vl",
>> "l", "n", "h", "vh", "xh"), class = "factor"), sced = structure(c(2L,
>> 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>> 3L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 2L, 3L), levels = c("vl",
>> "l", "n", "h", "vh", "xh"), class = "factor"), equivphyskloc = c(0.025534,
>> 0.006945, 0.008988, 0.002655, 0.067102, 0.006741, 0.019508, 0.005209,
>> 0.101215, 0.010622, 0.101215, 0.019508, 0.152283, 0.031253, 0.014401,
>> 0.014401, 0.037892, 0.009294, 0.015729, 0.012154, 0.032377, 0.035339,
>> 0.004698, 0.009703, 0.00572, 0.012358, 0.091002, 0.007252, 0.180778,
>> 0.307527), act_effort = c(117.6, 31.2, 25.2, 10.8, 352.8, 72,
>> 72, 24, 360, 36, 215, 48, 324, 60, 48, 90, 210, 48, 82, 62, 170,
>> 192, 18, 50, 42, 60, 444, 42, 1248, 2400)), row.names = c(1L,
>> 3L, 5L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 17L, 18L, 19L,
>> 24L, 25L, 26L, 29L, 30L, 31L, 32L, 33L, 34L, 36L, 37L, 38L, 39L,
>> 41L, 42L), class = "data.frame")
>>
>>
>>
>> On Thu, Jun 30, 2022 at 11:28 PM Rui Barradas <ruipbarradas at sapo.pt
>> <mailto:ruipbarradas at sapo.pt>> wrote:
>>
>>     Hello,
>>
>>     Please post data in dput format, without it it's difficult to tell.
>>     If I substitute
>>
>>     mpg for act_effort
>>     mtcars for tr
>>
>>     keeping everything else, I don't get any errors.
>>     And the error message says clearly that the error is in tr (data).
>>
>>     Can you post the output of dput(head(tr, 30))?
>>
>>     Rui Barradas
>>
>>
>>     ?s 19:32 de 30/06/2022, Neha gupta escreveu:
>>      > I posted it for the second time as I didn't get any response from
>>     group
>>      > members. I am not sure if some problem is with the question.
>>      >
>>      >
>>      >
>>      > I cannot run the "ranger" model with caret. I am only using the
>>     farff and
>>      > caret libraries and the following code:
>>      >
>>      > boot <- trainControl(method = "cv", number=10)
>>      >
>>      > c1 <-train(act_effort ~ ., data = tr,
>>      >                method = "ranger",
>>      >                 tuneLength = 5,
>>      >                metric = "MAE",
>>      >                preProc = c("center", "scale", "nzv"),
>>      >                trControl = boot)
>>      >
>>      > The error I get is the repeating of the following message until I
>>     interrupt
>>      > it.
>>      >
>>      > Error: mtry can not be larger than number of variables in data.
>>     Ranger will
>>      > EXIT now.
>>      >
>>      >       [[alternative HTML version deleted]]
>>      >
>>      > ______________________________________________
>>      > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>>     -- To UNSUBSCRIBE and more, see
>>      > https://stat.ethz.ch/mailman/listinfo/r-help
>>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>>      > PLEASE do read the posting guide
>>     http://www.R-project.org/posting-guide.html
>>     <http://www.R-project.org/posting-guide.html>
>>      > and provide commented, minimal, self-contained, reproducible code.
>>
>>

	[[alternative HTML version deleted]]


From neh@@bo|ogn@90 @end|ng |rom gm@||@com  Fri Jul  1 21:18:54 2022
From: neh@@bo|ogn@90 @end|ng |rom gm@||@com (Neha gupta)
Date: Fri, 1 Jul 2022 21:18:54 +0200
Subject: [R] Ranger could not work with caret
In-Reply-To: <b47d6a9c-7072-a819-3ba6-ab9c00c226e7@sapo.pt>
References: <CA+nrPnuc9+-R65CgJq_qNbuRTj491Jfv7YdbRgqd1Tom6ww=9Q@mail.gmail.com>
 <CA+nrPnvG5a26MFJJWQtuE9ZkCjHxqSb5gR7igr2kiBVGkQjUXw@mail.gmail.com>
 <0777f610-e580-c6e0-bb55-645bdbf32a0b@sapo.pt>
 <CA+nrPnu+awO6q=2eVpYom9yi7JT9FgAh0k0mcT=-9eLj1H7qAg@mail.gmail.com>
 <85d6347a-21f6-36ff-a92b-f0aae2c45749@sapo.pt>
 <CA+nrPnuzNEhv88j4LR0SXXRMqcSCWQGmef_-BsWJO8ubN1Oe9Q@mail.gmail.com>
 <b47d6a9c-7072-a819-3ba6-ab9c00c226e7@sapo.pt>
Message-ID: <CA+nrPns0Cjk8F=yW6eCAuPr4Y+-K+dSP4P=QipbbCpxFY=PSHw@mail.gmail.com>

@Rui Barradas <ruipbarradas at sapo.pt>

Thank you again for the useful explanation.

Best regards

On Fri, Jul 1, 2022 at 8:26 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> The error doesn't arise in randomForest because rf has a function tuneRF
> that looks for the best mtry (best relative to OOB error estimate). And
> it's this value that it uses.
>
> The question's code gives Ranger errors but it also gives R warnings:
>
> Warning messages:
> 1: model fit failed for Fold01: mtry=48, min.node.size=5,
> splitrule=variance Error in ranger::ranger(dependent.variable.name =
> ".outcome", data = x,  :
>    User interrupt or internal error.
>
>
> As you can see, mtry=48 is the double of ncol(tr) when should *never* be
> greater than the number of variables in the data set. Why it is using
> this value, I don't know. Function bug? Ask the package maintainer?
>
> And, by the way, package caret does or can do a grid search for optimal
> parameter values. If that is giving errors and you are calling rf
> directly why bother whith caret's error? Use the original function. Here
> is an example with tuneRF. Setting argument doBest to TRUE you'll have
> both the optimal value for mtry and the fitted random forest. 2 in 1.
>
>
> library(randomForest)
> #  randomForest 4.7-1.1
> #  Type rfNews() to see new features/changes/bug fixes.
>
> c2 <- tuneRF(
>    x = tr[-ncol(tr)],
>    y = tr$act_effort,
>    mtryStart = ncol(tr)/2,
>    doBest = TRUE
> )
> #  mtry = 12  OOB error = 139920.7
> #  Searching left ...
> #  mtry = 6     OOB error = 170909.3
> #  -0.2214729 0.05
> #  Searching right ...
> #  mtry = 23    OOB error = 128566.7
> #  0.08114586 0.05
>
> c2
> #
> #  Call:
> #   randomForest(x = x, y = y, mtry = res[which.min(res[, 2]), 1])
> #                 Type of random forest: regression
> #                       Number of trees: 500
> #  No. of variables tried at each split: 23
> #
> #            Mean of squared residuals: 129734.8
> #                      % Var explained: 39.98
>
>
> Hope this helps,
>
> Rui Barradas
>
>
>
> ?s 17:18 de 01/07/2022, Neha gupta escreveu:
> > Thank you so much for your help. I hope it will work.
> >
> > However, why the same error doesn't arise when I am using rf. They both
> > have the same parameters and it's default values.
> >
> > Best regards
> >
> > On Friday, July 1, 2022, Rui Barradas <ruipbarradas at sapo.pt
> > <mailto:ruipbarradas at sapo.pt>> wrote:
> >
> >     Hello,
> >
> >     The error is in Ranger parameter mtry becoming greater than the
> >     number of variables (columns).
> >     mtry can be set manually in caret::train argument tuneGrid. But for
> >     random forests you must also set the split rule and the minimum node.
> >
> >
> >     library(caret)
> >     library(farff)
> >
> >     boot <- trainControl(method = "cv", number = 10)
> >
> >     # set the maximum mtry manually to ncol(tr)
> >     # this creates a sequence of mtry values
> >     mtry <- var_seq(ncol(tr), len = 3)  # 3 is the default value
> >     mtry
> >     #  [1]  2 13 24
> >     #[1]  2 13 24
> >
> >     splitrule <- c("variance", "extratrees")
> >     min.node.size <- 1:10
> >     mtrygrid <- expand.grid(mtry, splitrule, min.node.size)
> >     names(mtrygrid) <- c("mtry", "splitrule", "min.node.size")
> >
> >     c1 <- train(act_effort ~ ., data = tr,
> >                 method = "ranger",
> >                 tuneLength = 5,
> >                 metric = "MAE",
> >                 preProc = c("center", "scale", "nzv"),
> >                 tuneGrid = mtrygrid,
> >                 trControl = boot)
> >     c1
> >     #  Random Forest
> >     #
> >     #  30 samples
> >     #  23 predictors
> >     #
> >     #  Pre-processing: centered (48), scaled (48), remove (58)
> >     #  Resampling: Cross-Validated (10 fold)
> >     #  Summary of sample sizes: 28, 27, 27, 28, 27, 27, ...
> >     #  Resampling results across tuning parameters:
> >     #
> >     #    mtry  splitrule   min.node.size  RMSE      Rsquared   MAE
> >     #     2    variance     1             256.6391  0.8103759  186.3609
> >     #     2    variance     2             249.7120  0.8628109  183.6696
> >     #     2    variance     3             258.8240  0.8284449  189.0712
> >     #
> >     # [...omit...]
> >     #
> >     #    13    extratrees  10             254.9569  0.8918014  191.2524
> >     #    24    variance     1             177.7188  0.9458652  112.2800
> >     #    24    variance     2             172.6826  0.9204287  108.5943
> >     #    24    variance     3             172.9954  0.9271006  109.2554
> >     #    24    variance     4             172.2467  0.9523067  110.0776
> >     #    24    variance     5             175.2485  0.9283317  112.8798
> >     #    24    variance     6             177.9285  0.9369881  115.8970
> >     #    24    variance     7             180.5959  0.9485035  117.5816
> >     #    24    variance     8             178.8037  0.9358033  117.8725
> >     #    24    variance     9             176.5849  0.9210959  117.0055
> >     #    24    variance    10             178.6439  0.9257969  119.8035
> >     #    24    extratrees   1             219.1368  0.8801770  141.0720
> >     #    24    extratrees   2             216.1900  0.8550002  140.9263
> >     #    24    extratrees   3             212.4138  0.8979379  141.4282
> >     #    24    extratrees   4             218.2631  0.9121471  146.2908
> >     #    24    extratrees   5             212.5679  0.9279598  144.2715
> >     #    24    extratrees   6             218.9856  0.9141754  152.2099
> >     #    24    extratrees   7             222.8540  0.9412682  152.4614
> >     #    24    extratrees   8             228.1156  0.9423414  161.8456
> >     #    24    extratrees   9             226.6182  0.9408306  160.5264
> >     #    24    extratrees  10             226.9280  0.9429413  165.6878
> >     #
> >     #  MAE was used to select the optimal model using the smallest value.
> >     #  The final values used for the model were mtry = 24, splitrule =
> >     variance
> >     #   and min.node.size = 2.
> >     plot(c1)
> >
> >
> >
> >     Hope this helps,
> >
> >     Rui Barradas
> >
> >
> >     ?s 23:03 de 30/06/2022, Neha gupta escreveu:
> >
> >         Ok, the data is pasted below
> >
> >         But on the same data (everything the same) and with other models
> >         like RF, SVM etc, it works fine.
> >
> >           > dput(head(tr, 30))
> >         structure(list(recordnumber = c(0, 0.02, 0.04, 0.06, 0.07, 0.08,
> >         0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.16, 0.17, 0.18, 0.23, 0.24,
> >         0.25, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.35, 0.36, 0.37, 0.38,
> >         0.4, 0.41), projectname = structure(c(1L, 1L, 1L, 1L, 2L, 3L,
> >         3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
> >         4L, 4L, 4L, 4L, 4L, 4L, 5L, 6L), levels = c("de", "erb", "gal",
> >         "X", "hst", "slp", "spl", "Y"), class = "factor"), cat2 =
> >         structure(c(3L,
> >         3L, 3L, 3L, 3L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L,
> >         9L, 11L, 5L, 4L, 6L, 8L, 3L, 9L, 9L, 9L, 9L, 6L, 7L), levels =
> >         c("Avionics",
> >         "application_ground", "avionicsmonitoring",
> "batchdataprocessing",
> >         "communications", "datacapture", "launchprocessing",
> >         "missionplanning",
> >         "monitor_control", "operatingsystem", "realdataprocessing",
> >         "science",
> >         "simulation", "utility"), class = "factor"), forg =
> structure(c(2L,
> >         2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> >         2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), levels =
> c("f",
> >         "g"), class = "factor"), center = structure(c(2L, 2L, 2L, 2L,
> >         2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L,
> >         2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 6L), levels = c("1", "2",
> >         "3", "4", "5", "6"), class = "factor"), year = c(0.5, 0.5, 0.5,
> >         0.5, 0.6875, 0.5625, 0.5625, 0.8125, 0.5625, 0.875, 0.5625, 0.75,
> >         0.5625, 0.8125, 0.75, 0.9375, 0.9375, 0.9375, 0.6875, 0.6875,
> >         0.6875, 0.6875, 0.875, 1, 0.9375, 0.9375, 0.9375, 0.9375, 0.5625,
> >         0.25), mode = structure(c(3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> >         3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> >         3L, 3L, 3L, 3L, 3L), levels = c("embedded", "organic",
> >         "semidetached"
> >         ), class = "factor"), rely = structure(c(4L, 4L, 4L, 4L, 4L,
> >         4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 3L, 3L, 3L, 3L,
> >         3L, 4L, 4L, 4L, 3L, 3L, 3L, 3L, 4L), levels = c("vl", "l", "n",
> >         "h", "vh", "xh"), class = "factor"), data = structure(c(2L, 2L,
> >         2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L,
> >         5L, 5L, 5L, 5L, 2L, 2L, 3L, 3L, 3L, 3L, 4L, 2L), levels = c("vl",
> >         "l", "n", "h", "vh", "xh"), class = "factor"), cplx =
> >         structure(c(4L,
> >         4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 4L,
> >         3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L), levels =
> >         c("vl",
> >         "l", "n", "h", "vh", "xh"), class = "factor"), time =
> >         structure(c(3L,
> >         3L, 3L, 3L, 3L, 6L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L,
> >         3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 5L, 3L), levels =
> >         c("vl",
> >         "l", "n", "h", "vh", "xh"), class = "factor"), stor =
> >         structure(c(3L,
> >         3L, 3L, 3L, 3L, 6L, 3L, 3L, 3L, 3L, 3L, 3L, 6L, 3L, 3L, 3L, 3L,
> >         3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L), levels =
> >         c("vl",
> >         "l", "n", "h", "vh", "xh"), class = "factor"), virt =
> >         structure(c(2L,
> >         2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 4L, 2L, 2L, 2L, 2L, 3L, 3L,
> >         3L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 2L, 2L), levels =
> >         c("vl",
> >         "l", "n", "h", "vh", "xh"), class = "factor"), turn =
> >         structure(c(2L,
> >         2L, 2L, 2L, 2L, 4L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L,
> >         3L, 4L, 4L, 4L, 4L, 2L, 2L, 3L, 3L, 3L, 3L, 4L, 2L), levels =
> >         c("vl",
> >         "l", "n", "h", "vh", "xh"), class = "factor"), acap =
> >         structure(c(3L,
> >         3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 3L,
> >         3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L), levels =
> >         c("vl",
> >         "l", "n", "h", "vh", "xh"), class = "factor"), aexp =
> >         structure(c(3L,
> >         3L, 3L, 3L, 3L, 4L, 5L, 5L, 5L, 5L, 4L, 5L, 5L, 4L, 5L, 4L, 4L,
> >         4L, 4L, 4L, 4L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), levels =
> >         c("vl",
> >         "l", "n", "h", "vh", "xh"), class = "factor"), pcap =
> >         structure(c(3L,
> >         3L, 3L, 3L, 3L, 4L, 5L, 4L, 5L, 3L, 4L, 4L, 5L, 4L, 4L, 4L, 4L,
> >         4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L, 4L, 4L), levels =
> >         c("vl",
> >         "l", "n", "h", "vh", "xh"), class = "factor"), vexp =
> >         structure(c(3L,
> >         3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 2L, 3L, 3L, 3L, 3L, 3L, 3L,
> >         3L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 2L, 3L), levels =
> >         c("vl",
> >         "l", "n", "h", "vh", "xh"), class = "factor"), lexp =
> >         structure(c(4L,
> >         4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 2L, 1L, 4L, 4L, 4L, 4L, 3L, 3L,
> >         3L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 3L, 4L, 3L, 4L, 3L), levels =
> >         c("vl",
> >         "l", "n", "h", "vh", "xh"), class = "factor"), modp =
> >         structure(c(4L,
> >         4L, 4L, 4L, 4L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> >         3L, 5L, 5L, 5L, 5L, 4L, 4L, 3L, 3L, 4L, 3L, 4L, 4L), levels =
> >         c("vl",
> >         "l", "n", "h", "vh", "xh"), class = "factor"), tool =
> >         structure(c(3L,
> >         3L, 3L, 3L, 3L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> >         3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 4L, 3L, 3L, 1L), levels =
> >         c("vl",
> >         "l", "n", "h", "vh", "xh"), class = "factor"), sced =
> >         structure(c(2L,
> >         2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
> >         3L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 2L, 3L), levels =
> >         c("vl",
> >         "l", "n", "h", "vh", "xh"), class = "factor"), equivphyskloc =
> >         c(0.025534,
> >         0.006945, 0.008988, 0.002655, 0.067102, 0.006741, 0.019508,
> >         0.005209,
> >         0.101215, 0.010622, 0.101215, 0.019508, 0.152283, 0.031253,
> >         0.014401,
> >         0.014401, 0.037892, 0.009294, 0.015729, 0.012154, 0.032377,
> >         0.035339,
> >         0.004698, 0.009703, 0.00572, 0.012358, 0.091002, 0.007252,
> 0.180778,
> >         0.307527), act_effort = c(117.6, 31.2, 25.2, 10.8, 352.8, 72,
> >         72, 24, 360, 36, 215, 48, 324, 60, 48, 90, 210, 48, 82, 62, 170,
> >         192, 18, 50, 42, 60, 444, 42, 1248, 2400)), row.names = c(1L,
> >         3L, 5L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 17L, 18L, 19L,
> >         24L, 25L, 26L, 29L, 30L, 31L, 32L, 33L, 34L, 36L, 37L, 38L, 39L,
> >         41L, 42L), class = "data.frame")
> >
> >
> >
> >         On Thu, Jun 30, 2022 at 11:28 PM Rui Barradas
> >         <ruipbarradas at sapo.pt <mailto:ruipbarradas at sapo.pt>
> >         <mailto:ruipbarradas at sapo.pt <mailto:ruipbarradas at sapo.pt>>>
> wrote:
> >
> >              Hello,
> >
> >              Please post data in dput format, without it it's difficult
> >         to tell.
> >              If I substitute
> >
> >              mpg for act_effort
> >              mtcars for tr
> >
> >              keeping everything else, I don't get any errors.
> >              And the error message says clearly that the error is in tr
> >         (data).
> >
> >              Can you post the output of dput(head(tr, 30))?
> >
> >              Rui Barradas
> >
> >
> >              ?s 19:32 de 30/06/2022, Neha gupta escreveu:
> >               > I posted it for the second time as I didn't get any
> >         response from
> >              group
> >               > members. I am not sure if some problem is with the
> question.
> >               >
> >               >
> >               >
> >               > I cannot run the "ranger" model with caret. I am only
> >         using the
> >              farff and
> >               > caret libraries and the following code:
> >               >
> >               > boot <- trainControl(method = "cv", number=10)
> >               >
> >               > c1 <-train(act_effort ~ ., data = tr,
> >               >                method = "ranger",
> >               >                 tuneLength = 5,
> >               >                metric = "MAE",
> >               >                preProc = c("center", "scale", "nzv"),
> >               >                trControl = boot)
> >               >
> >               > The error I get is the repeating of the following
> >         message until I
> >              interrupt
> >               > it.
> >               >
> >               > Error: mtry can not be larger than number of variables
> >         in data.
> >              Ranger will
> >               > EXIT now.
> >               >
> >               >       [[alternative HTML version deleted]]
> >               >
> >               > ______________________________________________
> >               > R-help at r-project.org <mailto:R-help at r-project.org>
> >         <mailto:R-help at r-project.org <mailto:R-help at r-project.org>>
> >         mailing list
> >              -- To UNSUBSCRIBE and more, see
> >               > https://stat.ethz.ch/mailman/listinfo/r-help
> >         <https://stat.ethz.ch/mailman/listinfo/r-help>
> >              <https://stat.ethz.ch/mailman/listinfo/r-help
> >         <https://stat.ethz.ch/mailman/listinfo/r-help>>
> >               > PLEASE do read the posting guide
> >         http://www.R-project.org/posting-guide.html
> >         <http://www.R-project.org/posting-guide.html>
> >              <http://www.R-project.org/posting-guide.html
> >         <http://www.R-project.org/posting-guide.html>>
> >               > and provide commented, minimal, self-contained,
> >         reproducible code.
> >
>

	[[alternative HTML version deleted]]


From peter@|@ng|e|der @end|ng |rom gm@||@com  Sat Jul  2 05:30:47 2022
From: peter@|@ng|e|der @end|ng |rom gm@||@com (Peter Langfelder)
Date: Fri, 1 Jul 2022 20:30:47 -0700
Subject: [R] Subsetting a vector using an index with all missing values
Message-ID: <CA+hbrhUmU4Bsu7HV-rjepBkcvLzyqe3rB7nrtHBKyiZiNkLg8g@mail.gmail.com>

Hi all,

I stumbled on subsetting behavior that seems counterintuitive and
perhaps is a bug. Here's a simple example:

> x = 1:10
> x[ rep(NA, 3)]
 [1] NA NA NA NA NA NA NA NA NA NA

I would have expected 3 NAs (the length of the index), not 10 (all
values in x). Looked at the documentation for the subsetting operator
`[` but found nothing indicating that if the index contains all
missing data, the result is the entire vector.

I can work around the issue for a general 'index' using a somewhat
clunky but straightforward construct along the lines of

> index = rep(NA, 3)
> x[c(1, index)][-1]
[1] NA NA NA

but I'm wondering if the behaviour above is intended.

Thanks,

Peter


From w||||@mwdun|@p @end|ng |rom gm@||@com  Sat Jul  2 07:01:44 2022
From: w||||@mwdun|@p @end|ng |rom gm@||@com (Bill Dunlap)
Date: Fri, 1 Jul 2022 22:01:44 -0700
Subject: [R] Subsetting a vector using an index with all missing values
In-Reply-To: <CA+hbrhUmU4Bsu7HV-rjepBkcvLzyqe3rB7nrtHBKyiZiNkLg8g@mail.gmail.com>
References: <CA+hbrhUmU4Bsu7HV-rjepBkcvLzyqe3rB7nrtHBKyiZiNkLg8g@mail.gmail.com>
Message-ID: <CAHqSRuQUBBsDFxNUHBYmBkoDmUG1FOY_aoX9oVS9=afuX35J+Q@mail.gmail.com>

This has to do with the mode of the subscript - logical subscripts are
repeated to the length of x and integer/numeric ones are not.  NA is
logical, NA_integer_ is integer, so we get

> x <- 1:10
> x[ rep(NA_integer_, 3) ]
[1] NA NA NA
> x[ rep(NA, 3) ]
 [1] NA NA NA NA NA NA NA NA NA NA

-Bill


On Fri, Jul 1, 2022 at 8:31 PM Peter Langfelder <peter.langfelder at gmail.com>
wrote:

> Hi all,
>
> I stumbled on subsetting behavior that seems counterintuitive and
> perhaps is a bug. Here's a simple example:
>
> > x = 1:10
> > x[ rep(NA, 3)]
>  [1] NA NA NA NA NA NA NA NA NA NA
>
> I would have expected 3 NAs (the length of the index), not 10 (all
> values in x). Looked at the documentation for the subsetting operator
> `[` but found nothing indicating that if the index contains all
> missing data, the result is the entire vector.
>
> I can work around the issue for a general 'index' using a somewhat
> clunky but straightforward construct along the lines of
>
> > index = rep(NA, 3)
> > x[c(1, index)][-1]
> [1] NA NA NA
>
> but I'm wondering if the behaviour above is intended.
>
> Thanks,
>
> Peter
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From peter@|@ng|e|der @end|ng |rom gm@||@com  Sat Jul  2 08:18:56 2022
From: peter@|@ng|e|der @end|ng |rom gm@||@com (Peter Langfelder)
Date: Fri, 1 Jul 2022 23:18:56 -0700
Subject: [R] Subsetting a vector using an index with all missing values
In-Reply-To: <CAHqSRuQUBBsDFxNUHBYmBkoDmUG1FOY_aoX9oVS9=afuX35J+Q@mail.gmail.com>
References: <CA+hbrhUmU4Bsu7HV-rjepBkcvLzyqe3rB7nrtHBKyiZiNkLg8g@mail.gmail.com>
 <CAHqSRuQUBBsDFxNUHBYmBkoDmUG1FOY_aoX9oVS9=afuX35J+Q@mail.gmail.com>
Message-ID: <CA+hbrhX6tH-1R3k_rLZF+KMz--hJN8tRhJm7s5unbJd9Uob9gw@mail.gmail.com>

Ah, thanks, that makes sense.

Peter

On Fri, Jul 1, 2022 at 10:01 PM Bill Dunlap <williamwdunlap at gmail.com> wrote:
>
> This has to do with the mode of the subscript - logical subscripts are repeated to the length of x and integer/numeric ones are not.  NA is logical, NA_integer_ is integer, so we get
>
> > x <- 1:10
> > x[ rep(NA_integer_, 3) ]
> [1] NA NA NA
> > x[ rep(NA, 3) ]
>  [1] NA NA NA NA NA NA NA NA NA NA
>
> -Bill
>
>
> On Fri, Jul 1, 2022 at 8:31 PM Peter Langfelder <peter.langfelder at gmail.com> wrote:
>>
>> Hi all,
>>
>> I stumbled on subsetting behavior that seems counterintuitive and
>> perhaps is a bug. Here's a simple example:
>>
>> > x = 1:10
>> > x[ rep(NA, 3)]
>>  [1] NA NA NA NA NA NA NA NA NA NA
>>
>> I would have expected 3 NAs (the length of the index), not 10 (all
>> values in x). Looked at the documentation for the subsetting operator
>> `[` but found nothing indicating that if the index contains all
>> missing data, the result is the entire vector.
>>
>> I can work around the issue for a general 'index' using a somewhat
>> clunky but straightforward construct along the lines of
>>
>> > index = rep(NA, 3)
>> > x[c(1, index)][-1]
>> [1] NA NA NA
>>
>> but I'm wondering if the behaviour above is intended.
>>
>> Thanks,
>>
>> Peter
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From tebert @end|ng |rom u||@edu  Sat Jul  2 16:47:22 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Sat, 2 Jul 2022 14:47:22 +0000
Subject: [R] Subsetting a vector using an index with all missing values
In-Reply-To: <CA+hbrhX6tH-1R3k_rLZF+KMz--hJN8tRhJm7s5unbJd9Uob9gw@mail.gmail.com>
References: <CA+hbrhUmU4Bsu7HV-rjepBkcvLzyqe3rB7nrtHBKyiZiNkLg8g@mail.gmail.com>
 <CAHqSRuQUBBsDFxNUHBYmBkoDmUG1FOY_aoX9oVS9=afuX35J+Q@mail.gmail.com>
 <CA+hbrhX6tH-1R3k_rLZF+KMz--hJN8tRhJm7s5unbJd9Uob9gw@mail.gmail.com>
Message-ID: <BN6PR2201MB155374BFB4ECF44564E8AFB6CFBC9@BN6PR2201MB1553.namprd22.prod.outlook.com>

That nicely explains the difference in outcome between 
x[rep(TRUE,3)]
x[rep("TRUE",3)]


I do not quite get it.
x<-1:10
x[rep(x<2,3)]
[1] 1 NA NA
The length is three

but
x[rep(x>2,3)]
[1] 3  4  5  6  7  8  9  10  NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
The length is 24

Tim
-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Peter Langfelder
Sent: Saturday, July 2, 2022 2:19 AM
To: Bill Dunlap <williamwdunlap at gmail.com>
Cc: r-help <r-help at r-project.org>
Subject: Re: [R] Subsetting a vector using an index with all missing values

[External Email]

Ah, thanks, that makes sense.

Peter

On Fri, Jul 1, 2022 at 10:01 PM Bill Dunlap <williamwdunlap at gmail.com> wrote:
>
> This has to do with the mode of the subscript - logical subscripts are 
> repeated to the length of x and integer/numeric ones are not.  NA is 
> logical, NA_integer_ is integer, so we get
>
> > x <- 1:10
> > x[ rep(NA_integer_, 3) ]
> [1] NA NA NA
> > x[ rep(NA, 3) ]
>  [1] NA NA NA NA NA NA NA NA NA NA
>
> -Bill
>
>
> On Fri, Jul 1, 2022 at 8:31 PM Peter Langfelder <peter.langfelder at gmail.com> wrote:
>>
>> Hi all,
>>
>> I stumbled on subsetting behavior that seems counterintuitive and 
>> perhaps is a bug. Here's a simple example:
>>
>> > x = 1:10
>> > x[ rep(NA, 3)]
>>  [1] NA NA NA NA NA NA NA NA NA NA
>>
>> I would have expected 3 NAs (the length of the index), not 10 (all 
>> values in x). Looked at the documentation for the subsetting operator 
>> `[` but found nothing indicating that if the index contains all 
>> missing data, the result is the entire vector.
>>
>> I can work around the issue for a general 'index' using a somewhat 
>> clunky but straightforward construct along the lines of
>>
>> > index = rep(NA, 3)
>> > x[c(1, index)][-1]
>> [1] NA NA NA
>>
>> but I'm wondering if the behaviour above is intended.
>>
>> Thanks,
>>
>> Peter
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
>> lman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVe
>> AsRzsn7AkP-g&m=5kixibKMuZiVbTFd2D5fSZBYO3aFODtFyW96wUN-oC5gJtbOYJ9G0j
>> 6zoo-P6z4W&s=WGGoTTZ6ENtmckv7K_B0OepH04TDjbiNp0D6IbdqpAg&e=
>> PLEASE do read the posting guide 
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
>> rg_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kV
>> eAsRzsn7AkP-g&m=5kixibKMuZiVbTFd2D5fSZBYO3aFODtFyW96wUN-oC5gJtbOYJ9G0
>> j6zoo-P6z4W&s=JErkxZzuGa2y8pjLddJY5u_vDIbjw4tX1vzkb8LAe98&e=
>> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=5kixibKMuZiVbTFd2D5fSZBYO3aFODtFyW96wUN-oC5gJtbOYJ9G0j6zoo-P6z4W&s=WGGoTTZ6ENtmckv7K_B0OepH04TDjbiNp0D6IbdqpAg&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=5kixibKMuZiVbTFd2D5fSZBYO3aFODtFyW96wUN-oC5gJtbOYJ9G0j6zoo-P6z4W&s=JErkxZzuGa2y8pjLddJY5u_vDIbjw4tX1vzkb8LAe98&e=
and provide commented, minimal, self-contained, reproducible code.


From w||||@mwdun|@p @end|ng |rom gm@||@com  Sat Jul  2 17:24:46 2022
From: w||||@mwdun|@p @end|ng |rom gm@||@com (Bill Dunlap)
Date: Sat, 2 Jul 2022 08:24:46 -0700
Subject: [R] Subsetting a vector using an index with all missing values
In-Reply-To: <BN6PR2201MB155374BFB4ECF44564E8AFB6CFBC9@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CA+hbrhUmU4Bsu7HV-rjepBkcvLzyqe3rB7nrtHBKyiZiNkLg8g@mail.gmail.com>
 <CAHqSRuQUBBsDFxNUHBYmBkoDmUG1FOY_aoX9oVS9=afuX35J+Q@mail.gmail.com>
 <CA+hbrhX6tH-1R3k_rLZF+KMz--hJN8tRhJm7s5unbJd9Uob9gw@mail.gmail.com>
 <BN6PR2201MB155374BFB4ECF44564E8AFB6CFBC9@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <CAHqSRuTLzAmB6tBJJVkGF94uudBD1Nq-f5pnFRC5eBKM4kNW3w@mail.gmail.com>

Perhaps it should be an error if the length of a logical subscript is
bigger than the dimension it is subscripting.  Currently in that case, x is
extended (with NA or NULL) to the length of the logical subscript.  I doubt
this is desired very often.

> dput((1:3)[c(FALSE,FALSE,FALSE,TRUE,TRUE)])
c(NA_integer_, NA_integer_)
> dput((1:3)[c(FALSE,FALSE,TRUE,FALSE,TRUE)])
c(3L, NA)

-Bill

On Sat, Jul 2, 2022 at 7:49 AM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:

> That nicely explains the difference in outcome between
> x[rep(TRUE,3)]
> x[rep("TRUE",3)]
>
>
> I do not quite get it.
> x<-1:10
> x[rep(x<2,3)]
> [1] 1 NA NA
> The length is three
>
> but
> x[rep(x>2,3)]
> [1] 3  4  5  6  7  8  9  10  NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
> NA
> The length is 24
>
> Tim
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Peter Langfelder
> Sent: Saturday, July 2, 2022 2:19 AM
> To: Bill Dunlap <williamwdunlap at gmail.com>
> Cc: r-help <r-help at r-project.org>
> Subject: Re: [R] Subsetting a vector using an index with all missing values
>
> [External Email]
>
> Ah, thanks, that makes sense.
>
> Peter
>
> On Fri, Jul 1, 2022 at 10:01 PM Bill Dunlap <williamwdunlap at gmail.com>
> wrote:
> >
> > This has to do with the mode of the subscript - logical subscripts are
> > repeated to the length of x and integer/numeric ones are not.  NA is
> > logical, NA_integer_ is integer, so we get
> >
> > > x <- 1:10
> > > x[ rep(NA_integer_, 3) ]
> > [1] NA NA NA
> > > x[ rep(NA, 3) ]
> >  [1] NA NA NA NA NA NA NA NA NA NA
> >
> > -Bill
> >
> >
> > On Fri, Jul 1, 2022 at 8:31 PM Peter Langfelder <
> peter.langfelder at gmail.com> wrote:
> >>
> >> Hi all,
> >>
> >> I stumbled on subsetting behavior that seems counterintuitive and
> >> perhaps is a bug. Here's a simple example:
> >>
> >> > x = 1:10
> >> > x[ rep(NA, 3)]
> >>  [1] NA NA NA NA NA NA NA NA NA NA
> >>
> >> I would have expected 3 NAs (the length of the index), not 10 (all
> >> values in x). Looked at the documentation for the subsetting operator
> >> `[` but found nothing indicating that if the index contains all
> >> missing data, the result is the entire vector.
> >>
> >> I can work around the issue for a general 'index' using a somewhat
> >> clunky but straightforward construct along the lines of
> >>
> >> > index = rep(NA, 3)
> >> > x[c(1, index)][-1]
> >> [1] NA NA NA
> >>
> >> but I'm wondering if the behaviour above is intended.
> >>
> >> Thanks,
> >>
> >> Peter
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
> >> lman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVe
> >> AsRzsn7AkP-g&m=5kixibKMuZiVbTFd2D5fSZBYO3aFODtFyW96wUN-oC5gJtbOYJ9G0j
> >> 6zoo-P6z4W&s=WGGoTTZ6ENtmckv7K_B0OepH04TDjbiNp0D6IbdqpAg&e=
> >> PLEASE do read the posting guide
> >> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
> >> rg_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kV
> >> eAsRzsn7AkP-g&m=5kixibKMuZiVbTFd2D5fSZBYO3aFODtFyW96wUN-oC5gJtbOYJ9G0
> >> j6zoo-P6z4W&s=JErkxZzuGa2y8pjLddJY5u_vDIbjw4tX1vzkb8LAe98&e=
> >> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=5kixibKMuZiVbTFd2D5fSZBYO3aFODtFyW96wUN-oC5gJtbOYJ9G0j6zoo-P6z4W&s=WGGoTTZ6ENtmckv7K_B0OepH04TDjbiNp0D6IbdqpAg&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=5kixibKMuZiVbTFd2D5fSZBYO3aFODtFyW96wUN-oC5gJtbOYJ9G0j6zoo-P6z4W&s=JErkxZzuGa2y8pjLddJY5u_vDIbjw4tX1vzkb8LAe98&e=
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Sat Jul  2 18:11:52 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Sat, 2 Jul 2022 21:11:52 +0500
Subject: [R] A humble request
Message-ID: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>

Dear Experts,
I cannot find a package "wmtsa" for my R version "R 4.2.0". Kindly help me
to find it or share the link with me.
Although I tried the old version of "wmtsa" but failed.
Thank you for your precious time.

Regards
Muhammad Zubair Chishti

	[[alternative HTML version deleted]]


From rhur||n @end|ng |rom gwdg@de  Sat Jul  2 18:24:05 2022
From: rhur||n @end|ng |rom gwdg@de (Rainer Hurling)
Date: Sat, 2 Jul 2022 18:24:05 +0200
Subject: [R] A humble request
In-Reply-To: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
References: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
Message-ID: <351d54b9-c39c-34f2-a0d3-31def36c01e9@gwdg.de>

Hi Muhammad,

Am 02.07.22 um 18:11 schrieb Muhammad Zubair Chishti:
> Dear Experts,
> I cannot find a package "wmtsa" for my R version "R 4.2.0". Kindly help me
> to find it or share the link with me.
> Although I tried the old version of "wmtsa" but failed.
> Thank you for your precious time.
> 
> Regards
> Muhammad Zubair Chishti
> 
> 	[[alternative HTML version deleted]]

AFAIK there is no package wmtsa anymore. It has been archived on 
2020-06-09 [1], the latest version was wmtsa_2.0-3.tar.gz from 2017-12-06.

[1] https://cran.r-project.org/src/contrib/Archive/wmtsa/

HTH,
Rainer


From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Sat Jul  2 18:31:50 2022
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Sat, 2 Jul 2022 11:31:50 -0500
Subject: [R] A humble request
In-Reply-To: <351d54b9-c39c-34f2-a0d3-31def36c01e9@gwdg.de>
References: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
 <351d54b9-c39c-34f2-a0d3-31def36c01e9@gwdg.de>
Message-ID: <93b1b40a-544d-bff5-24e0-1bc0ce29f5b7@effectivedefense.org>

If I understand correctly the rules for "archived" packages, anyone is 
free to download them, compile them locally -- AND resubmit any such 
package with themselves as the maintainer:  They were archived, because 
they would no longer pass CRAN checks, and the designated maintainer 
failed to respond to the notice of problems with newer versions of R. 
If anything I've said here is wrong or poorly nuanced, I trust someone 
knowledgeable will correct me.  Spencer Graves


On 7/2/22 11:24 AM, Rainer Hurling wrote:
> Hi Muhammad,
> 
> Am 02.07.22 um 18:11 schrieb Muhammad Zubair Chishti:
>> Dear Experts,
>> I cannot find a package "wmtsa" for my R version "R 4.2.0". Kindly 
>> help me
>> to find it or share the link with me.
>> Although I tried the old version of "wmtsa" but failed.
>> Thank you for your precious time.
>>
>> Regards
>> Muhammad Zubair Chishti
>>
>> ????[[alternative HTML version deleted]]
> 
> AFAIK there is no package wmtsa anymore. It has been archived on 
> 2020-06-09 [1], the latest version was wmtsa_2.0-3.tar.gz from 2017-12-06.
> 
> [1] https://cran.r-project.org/src/contrib/Archive/wmtsa/
> 
> HTH,
> Rainer
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Sat Jul  2 18:32:00 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Sat, 2 Jul 2022 21:32:00 +0500
Subject: [R] A humble request
In-Reply-To: <CAMfKi3JH-pSwKAsrQ51koEBN5Jx8Sfk2NKv4n0KgLiaMxHsYDw@mail.gmail.com>
References: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
 <351d54b9-c39c-34f2-a0d3-31def36c01e9@gwdg.de>
 <CAMfKi3JH-pSwKAsrQ51koEBN5Jx8Sfk2NKv4n0KgLiaMxHsYDw@mail.gmail.com>
Message-ID: <CAMfKi3Kao4U61nAfXWxjifTr_8K6110Ue3AbYKcOub05=NSLRw@mail.gmail.com>

Dear Respected Sir,
Thank you for the quick response. However, the link your shared provides
the old version of wmtsa that is not working in R 4.2.0.
Can you please tell me any other solution for this issue?

Regards
Muhammad Zubair Chishti

On Sat, 2 Jul 2022, 21:30 Muhammad Zubair Chishti, <mzchishti at eco.qau.edu.pk>
wrote:

> Dear Respected Sir,
> Thank you for the quick response. However, the link your shared provides
> the old version of wmtsa that is not working in R 4.2.0.
> Can you please tell me any other solution for this issue?
>
> Regards
> Muhammad Zubair Chishti
>
> On Sat, 2 Jul 2022, 21:24 Rainer Hurling, <rhurlin at gwdg.de> wrote:
>
>> Hi Muhammad,
>>
>> Am 02.07.22 um 18:11 schrieb Muhammad Zubair Chishti:
>> > Dear Experts,
>> > I cannot find a package "wmtsa" for my R version "R 4.2.0". Kindly help
>> me
>> > to find it or share the link with me.
>> > Although I tried the old version of "wmtsa" but failed.
>> > Thank you for your precious time.
>> >
>> > Regards
>> > Muhammad Zubair Chishti
>> >
>> >       [[alternative HTML version deleted]]
>>
>> AFAIK there is no package wmtsa anymore. It has been archived on
>> 2020-06-09 [1], the latest version was wmtsa_2.0-3.tar.gz from 2017-12-06.
>>
>> [1] https://cran.r-project.org/src/contrib/Archive/wmtsa/
>>
>> HTH,
>> Rainer
>>
>

	[[alternative HTML version deleted]]


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Sat Jul  2 18:39:20 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Sat, 2 Jul 2022 21:39:20 +0500
Subject: [R] A humble request
In-Reply-To: <93b1b40a-544d-bff5-24e0-1bc0ce29f5b7@effectivedefense.org>
References: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
 <351d54b9-c39c-34f2-a0d3-31def36c01e9@gwdg.de>
 <93b1b40a-544d-bff5-24e0-1bc0ce29f5b7@effectivedefense.org>
Message-ID: <CAMfKi3+a+9yQfnOpZidot-6OHhRdguU-UGGVo4aV-72mDM57RQ@mail.gmail.com>

Dear Respected Professor,
You are right. However, a student like me who doesn't know much R requires
specific helping packages to install. Therefore, I requested here to
experts to give the solution for my issue.

I hope that I will get the solution from this platform.

Regards
Muhammad Zubair Chishti

On Sat, 2 Jul 2022, 21:34 Spencer Graves, <
spencer.graves at effectivedefense.org> wrote:

> If I understand correctly the rules for "archived" packages, anyone is
> free to download them, compile them locally -- AND resubmit any such
> package with themselves as the maintainer:  They were archived, because
> they would no longer pass CRAN checks, and the designated maintainer
> failed to respond to the notice of problems with newer versions of R.
> If anything I've said here is wrong or poorly nuanced, I trust someone
> knowledgeable will correct me.  Spencer Graves
>
>
> On 7/2/22 11:24 AM, Rainer Hurling wrote:
> > Hi Muhammad,
> >
> > Am 02.07.22 um 18:11 schrieb Muhammad Zubair Chishti:
> >> Dear Experts,
> >> I cannot find a package "wmtsa" for my R version "R 4.2.0". Kindly
> >> help me
> >> to find it or share the link with me.
> >> Although I tried the old version of "wmtsa" but failed.
> >> Thank you for your precious time.
> >>
> >> Regards
> >> Muhammad Zubair Chishti
> >>
> >>     [[alternative HTML version deleted]]
> >
> > AFAIK there is no package wmtsa anymore. It has been archived on
> > 2020-06-09 [1], the latest version was wmtsa_2.0-3.tar.gz from
> 2017-12-06.
> >
> > [1] https://cran.r-project.org/src/contrib/Archive/wmtsa/
> >
> > HTH,
> > Rainer
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Sat Jul  2 18:49:00 2022
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Sat, 2 Jul 2022 11:49:00 -0500
Subject: [R] A humble request
In-Reply-To: <CAMfKi3+a+9yQfnOpZidot-6OHhRdguU-UGGVo4aV-72mDM57RQ@mail.gmail.com>
References: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
 <351d54b9-c39c-34f2-a0d3-31def36c01e9@gwdg.de>
 <93b1b40a-544d-bff5-24e0-1bc0ce29f5b7@effectivedefense.org>
 <CAMfKi3+a+9yQfnOpZidot-6OHhRdguU-UGGVo4aV-72mDM57RQ@mail.gmail.com>
Message-ID: <02586896-14cc-f5c1-de7a-e7a98ffe80dd@effectivedefense.org>

	  Are you familiar with the "debug" function?


	  You can download the package, source the function you want to use. 
If it doesn't run or doesn't seem to give a sensible answer, debug(fun) 
followed by fun(arg1, arg2, arg3) will start the function then stop and 
invite you to walk through the function line by line.  You can look at 
what it does, and change it as you like.  Doing so should help you learn 
R while also making it easier for you to figure out how to make the 
function do what you want.


	  Hope this helps.
	  Spencer Graves


On 7/2/22 11:39 AM, Muhammad Zubair Chishti wrote:
> Dear Respected Professor,
> You are right. However, a student like me who doesn't know much R 
> requires specific helping packages to install. Therefore, I requested 
> here to experts to give the solution for my issue.
> 
> I hope that I will get the solution from this platform.
> 
> Regards
> Muhammad Zubair Chishti
> 
> On Sat, 2 Jul 2022, 21:34 Spencer Graves, 
> <spencer.graves at effectivedefense.org 
> <mailto:spencer.graves at effectivedefense.org>> wrote:
> 
>     If I understand correctly the rules for "archived" packages, anyone is
>     free to download them, compile them locally -- AND resubmit any such
>     package with themselves as the maintainer:? They were archived, because
>     they would no longer pass CRAN checks, and the designated maintainer
>     failed to respond to the notice of problems with newer versions of R.
>     If anything I've said here is wrong or poorly nuanced, I trust someone
>     knowledgeable will correct me.? Spencer Graves
> 
> 
>     On 7/2/22 11:24 AM, Rainer Hurling wrote:
>      > Hi Muhammad,
>      >
>      > Am 02.07.22 um 18:11 schrieb Muhammad Zubair Chishti:
>      >> Dear Experts,
>      >> I cannot find a package "wmtsa" for my R version "R 4.2.0". Kindly
>      >> help me
>      >> to find it or share the link with me.
>      >> Although I tried the old version of "wmtsa" but failed.
>      >> Thank you for your precious time.
>      >>
>      >> Regards
>      >> Muhammad Zubair Chishti
>      >>
>      >> ????[[alternative HTML version deleted]]
>      >
>      > AFAIK there is no package wmtsa anymore. It has been archived on
>      > 2020-06-09 [1], the latest version was wmtsa_2.0-3.tar.gz from
>     2017-12-06.
>      >
>      > [1] https://cran.r-project.org/src/contrib/Archive/wmtsa/
>     <https://cran.r-project.org/src/contrib/Archive/wmtsa/>
>      >
>      > HTH,
>      > Rainer
>      >
>      > ______________________________________________
>      > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>     -- To UNSUBSCRIBE and more, see
>      > https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>      > PLEASE do read the posting guide
>      > http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>      > and provide commented, minimal, self-contained, reproducible code.
> 
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>     and provide commented, minimal, self-contained, reproducible code.
>


From tebert @end|ng |rom u||@edu  Sat Jul  2 19:23:10 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Sat, 2 Jul 2022 17:23:10 +0000
Subject: [R] A humble request
In-Reply-To: <CAMfKi3+a+9yQfnOpZidot-6OHhRdguU-UGGVo4aV-72mDM57RQ@mail.gmail.com>
References: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
 <351d54b9-c39c-34f2-a0d3-31def36c01e9@gwdg.de>
 <93b1b40a-544d-bff5-24e0-1bc0ce29f5b7@effectivedefense.org>
 <CAMfKi3+a+9yQfnOpZidot-6OHhRdguU-UGGVo4aV-72mDM57RQ@mail.gmail.com>
Message-ID: <BN6PR2201MB1553A058DE80C56C92E5B20FCFBC9@BN6PR2201MB1553.namprd22.prod.outlook.com>

Would package versions help? It would allow you to install a specific version of wmtsa into R 4.2.
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Muhammad Zubair Chishti
Sent: Saturday, July 2, 2022 12:39 PM
To: Spencer Graves <spencer.graves at effectivedefense.org>; r-help at r-project.org
Subject: Re: [R] A humble request

[External Email]

Dear Respected Professor,
You are right. However, a student like me who doesn't know much R requires specific helping packages to install. Therefore, I requested here to experts to give the solution for my issue.

I hope that I will get the solution from this platform.

Regards
Muhammad Zubair Chishti

On Sat, 2 Jul 2022, 21:34 Spencer Graves, < spencer.graves at effectivedefense.org> wrote:

> If I understand correctly the rules for "archived" packages, anyone is 
> free to download them, compile them locally -- AND resubmit any such 
> package with themselves as the maintainer:  They were archived, 
> because they would no longer pass CRAN checks, and the designated 
> maintainer failed to respond to the notice of problems with newer versions of R.
> If anything I've said here is wrong or poorly nuanced, I trust someone 
> knowledgeable will correct me.  Spencer Graves
>
>
> On 7/2/22 11:24 AM, Rainer Hurling wrote:
> > Hi Muhammad,
> >
> > Am 02.07.22 um 18:11 schrieb Muhammad Zubair Chishti:
> >> Dear Experts,
> >> I cannot find a package "wmtsa" for my R version "R 4.2.0". Kindly 
> >> help me to find it or share the link with me.
> >> Although I tried the old version of "wmtsa" but failed.
> >> Thank you for your precious time.
> >>
> >> Regards
> >> Muhammad Zubair Chishti
> >>
> >>     [[alternative HTML version deleted]]
> >
> > AFAIK there is no package wmtsa anymore. It has been archived on
> > 2020-06-09 [1], the latest version was wmtsa_2.0-3.tar.gz from
> 2017-12-06.
> >
> > [1] 
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dprojec
> > t.org_src_contrib_Archive_wmtsa_&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r
> > =9PEhQh2kVeAsRzsn7AkP-g&m=dWy_4gLGVAfQXtlFKz60hApURH4b1S-aO2lzWIDMTy
> > ABcOJ1gS2DXUhqww0npRxq&s=dgdXo5a7Pf0lxEB5T1DNMCBEJICza3NUIVXlGNYnwA4
> > &e=
> >
> > HTH,
> > Rainer
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_ma
> > ilman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2k
> > VeAsRzsn7AkP-g&m=dWy_4gLGVAfQXtlFKz60hApURH4b1S-aO2lzWIDMTyABcOJ1gS2
> > DXUhqww0npRxq&s=CuJJJWS5D5eyTOWy9Wfp3klxqALm4jzyFByUpvlj178&e=
> > PLEASE do read the posting guide
> > https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.
> > org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2
> > kVeAsRzsn7AkP-g&m=dWy_4gLGVAfQXtlFKz60hApURH4b1S-aO2lzWIDMTyABcOJ1gS
> > 2DXUhqww0npRxq&s=Q6r9x0BVDfQ_WvryQqz7NpwFr5qSH1VrW792A0VAfgY&e=
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=dWy_4gLGVAfQXtlFKz60hApURH4b1S-aO2lzWIDMTyABcOJ1gS2DXUhqw
> w0npRxq&s=CuJJJWS5D5eyTOWy9Wfp3klxqALm4jzyFByUpvlj178&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=dWy_4gLGVAfQXtlFKz60hApURH4b1S-aO2lzWIDMTyABcOJ1gS2DXUhq
> ww0npRxq&s=Q6r9x0BVDfQ_WvryQqz7NpwFr5qSH1VrW792A0VAfgY&e=
> and provide commented, minimal, self-contained, reproducible code.
>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=dWy_4gLGVAfQXtlFKz60hApURH4b1S-aO2lzWIDMTyABcOJ1gS2DXUhqww0npRxq&s=CuJJJWS5D5eyTOWy9Wfp3klxqALm4jzyFByUpvlj178&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=dWy_4gLGVAfQXtlFKz60hApURH4b1S-aO2lzWIDMTyABcOJ1gS2DXUhqww0npRxq&s=Q6r9x0BVDfQ_WvryQqz7NpwFr5qSH1VrW792A0VAfgY&e=
and provide commented, minimal, self-contained, reproducible code.


From dw|n@em|u@ @end|ng |rom comc@@t@net  Sat Jul  2 19:30:42 2022
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Sat, 2 Jul 2022 10:30:42 -0700
Subject: [R] A humble request
In-Reply-To: <CAMfKi3Kao4U61nAfXWxjifTr_8K6110Ue3AbYKcOub05=NSLRw@mail.gmail.com>
References: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
 <351d54b9-c39c-34f2-a0d3-31def36c01e9@gwdg.de>
 <CAMfKi3JH-pSwKAsrQ51koEBN5Jx8Sfk2NKv4n0KgLiaMxHsYDw@mail.gmail.com>
 <CAMfKi3Kao4U61nAfXWxjifTr_8K6110Ue3AbYKcOub05=NSLRw@mail.gmail.com>
Message-ID: <5E592054-A142-4947-AB68-DE13626CF98C@comcast.net>



> On Jul 2, 2022, at 9:32 AM, Muhammad Zubair Chishti <mzchishti at eco.qau.edu.pk> wrote:
> 
> Dear Respected Sir,
> Thank you for the quick response. However, the link your shared provides
> the old version of wmtsa that is not working in R 4.2.0.
> Can you please tell me any other solution for this issue?

Your response suggests that you do not understand how one installs a source package for R. You probably need to educate yourself about how to compile packages from source. Since the process is different for Linux, MacOS, and Windows, you would need to pay attention to the specific requirements of your unnamed OS.

You might find as I did that the wmtsa package has some dependencies that are also archived, namely pkg:splus2R (provided generously by Insightful for many years but apparently no longer) and pkg:ifultools. They both have incompatibilities that prevent compilation from source with my Mac setup. I did find a mirror that has a binary version of splus2R that did install and load.

install.packages("splus2R", repo="https://mirrors.vcea.wsu.edu/r-cran/", type="binary", dependencies=TRUE)

So maybe part of your PhD training should be learning how to compile packages and address changes in the compilers being used as R evolves. That is beyond my capabilities for offering assistance. Alternately you might find a forum or a consultant with similar interests to yours. Again, I can offer no assistance because I have never used pkg:wmtsa and don't know what it provided when it was compatible with the CRAN universe.

-- 
David
> 
> Regards
> Muhammad Zubair Chishti
> 
> On Sat, 2 Jul 2022, 21:30 Muhammad Zubair Chishti, <mzchishti at eco.qau.edu.pk>
> wrote:
> 
>> Dear Respected Sir,
>> Thank you for the quick response. However, the link your shared provides
>> the old version of wmtsa that is not working in R 4.2.0.
>> Can you please tell me any other solution for this issue?
>> 
>> Regards
>> Muhammad Zubair Chishti
>> 
>> On Sat, 2 Jul 2022, 21:24 Rainer Hurling, <rhurlin at gwdg.de> wrote:
>> 
>>> Hi Muhammad,
>>> 
>>> Am 02.07.22 um 18:11 schrieb Muhammad Zubair Chishti:
>>>> Dear Experts,
>>>> I cannot find a package "wmtsa" for my R version "R 4.2.0". Kindly help
>>> me
>>>> to find it or share the link with me.
>>>> Although I tried the old version of "wmtsa" but failed.
>>>> Thank you for your precious time.
>>>> 
>>>> Regards
>>>> Muhammad Zubair Chishti
>>>> 
>>>> [[alternative HTML version deleted]]
>>> 
>>> AFAIK there is no package wmtsa anymore. It has been archived on
>>> 2020-06-09 [1], the latest version was wmtsa_2.0-3.tar.gz from 2017-12-06.
>>> 
>>> [1] https://cran.r-project.org/src/contrib/Archive/wmtsa/
>>> 
>>> HTH,
>>> Rainer
>>> 
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Sat Jul  2 19:42:26 2022
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Sat, 2 Jul 2022 12:42:26 -0500
Subject: [R] A humble request
In-Reply-To: <5E592054-A142-4947-AB68-DE13626CF98C@comcast.net>
References: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
 <351d54b9-c39c-34f2-a0d3-31def36c01e9@gwdg.de>
 <CAMfKi3JH-pSwKAsrQ51koEBN5Jx8Sfk2NKv4n0KgLiaMxHsYDw@mail.gmail.com>
 <CAMfKi3Kao4U61nAfXWxjifTr_8K6110Ue3AbYKcOub05=NSLRw@mail.gmail.com>
 <5E592054-A142-4947-AB68-DE13626CF98C@comcast.net>
Message-ID: <f027b508-f1e6-b90a-7d18-d8668b2b7049@effectivedefense.org>



On 7/2/22 12:30 PM, David Winsemius wrote:
> 
> 
>> On Jul 2, 2022, at 9:32 AM, Muhammad Zubair Chishti <mzchishti at eco.qau.edu.pk> wrote:
>>
>> Dear Respected Sir,
>> Thank you for the quick response. However, the link your shared provides
>> the old version of wmtsa that is not working in R 4.2.0.
>> Can you please tell me any other solution for this issue?
> 
> Your response suggests that you do not understand how one installs a source package for R. You probably need to educate yourself about how to compile packages from source. Since the process is different for Linux, MacOS, and Windows, you would need to pay attention to the specific requirements of your unnamed OS.


BUT you do not need to compile the entire package to use a function in 
it:  You only need to source that particular function ... and any others 
it calls that you don't already have ;-)  That simplifies the task 
greatly.  With luck, Muhammad Zubair Chishti can source only the desired 
functions, and they will run and do what is desired with no further 
effort.  If not, Muhammad can be guided by the error message(s):  If 
they complain that a certain function is not available, Muhammad can 
find it and source it.  If the problem is more subtle, Muhammad can use 
"debug", as I earlier suggested.  Spencer Graves

> 
> You might find as I did that the wmtsa package has some dependencies that are also archived, namely pkg:splus2R (provided generously by Insightful for many years but apparently no longer) and pkg:ifultools. They both have incompatibilities that prevent compilation from source with my Mac setup. I did find a mirror that has a binary version of splus2R that did install and load.
> 
> install.packages("splus2R", repo="https://mirrors.vcea.wsu.edu/r-cran/", type="binary", dependencies=TRUE)
> 
> So maybe part of your PhD training should be learning how to compile packages and address changes in the compilers being used as R evolves. That is beyond my capabilities for offering assistance. Alternately you might find a forum or a consultant with similar interests to yours. Again, I can offer no assistance because I have never used pkg:wmtsa and don't know what it provided when it was compatible with the CRAN universe.
>


From @k@h@y_e4 @end|ng |rom hotm@||@com  Sat Jul  2 20:03:43 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Sat, 2 Jul 2022 18:03:43 +0000
Subject: [R] is.na with lists....
Message-ID: <PU4P216MB15680637B861C03D24236F5BC8BC9@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear memebers,
                            I have list of stock data OHLCData for 500 stocks, 15 of whom are NA's. The following is the code:

require(quantmod)
getOHLCData <- function(NSESym) {
  OHLCData1 <- list()
  for(i in 1:500){
    OHLCData1[[i]] <- tryCatch(getSymbols(NSESym[i], auto.assign=FALSE),
                          error = function (e) {print(i); return(NA)})

  }
  return(OHLCData1)
}

 OHLCData <- getOHLCData(NSESym)

however, when I check for is.na, I get the following:

length(OHLCData)
[1] 500
> length(is.na(OHLCData))
[1] 500

length(is.na(OHLCData)) should return 15. Whats going wrong? I assume is.na returns TRUE if there is an NA.

Yours sincerely,
AKSHAY M KULKARNI

	[[alternative HTML version deleted]]


From @kw@|mmo @end|ng |rom gm@||@com  Sat Jul  2 20:06:05 2022
From: @kw@|mmo @end|ng |rom gm@||@com (Andrew Simmons)
Date: Sat, 2 Jul 2022 14:06:05 -0400
Subject: [R] is.na with lists....
In-Reply-To: <PU4P216MB15680637B861C03D24236F5BC8BC9@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <PU4P216MB15680637B861C03D24236F5BC8BC9@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <CAPcHnpTOc8UXjRys5y_yJs5spX8vWvStGgJy0Pw11ph_Zb6T_Q@mail.gmail.com>

It's supposed to match the length. Perhaps you meant to use which(is.na())?

On Sat, Jul 2, 2022, 14:04 akshay kulkarni <akshay_e4 at hotmail.com> wrote:

> Dear memebers,
>                             I have list of stock data OHLCData for 500
> stocks, 15 of whom are NA's. The following is the code:
>
> require(quantmod)
> getOHLCData <- function(NSESym) {
>   OHLCData1 <- list()
>   for(i in 1:500){
>     OHLCData1[[i]] <- tryCatch(getSymbols(NSESym[i], auto.assign=FALSE),
>                           error = function (e) {print(i); return(NA)})
>
>   }
>   return(OHLCData1)
> }
>
>  OHLCData <- getOHLCData(NSESym)
>
> however, when I check for is.na, I get the following:
>
> length(OHLCData)
> [1] 500
> > length(is.na(OHLCData))
> [1] 500
>
> length(is.na(OHLCData)) should return 15. Whats going wrong? I assume
> is.na returns TRUE if there is an NA.
>
> Yours sincerely,
> AKSHAY M KULKARNI
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @v|gro@@ @end|ng |rom ver|zon@net  Sat Jul  2 20:17:44 2022
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Sat, 2 Jul 2022 18:17:44 +0000 (UTC)
Subject: [R] is.na with lists....
In-Reply-To: <CAPcHnpTOc8UXjRys5y_yJs5spX8vWvStGgJy0Pw11ph_Zb6T_Q@mail.gmail.com>
References: <PU4P216MB15680637B861C03D24236F5BC8BC9@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <CAPcHnpTOc8UXjRys5y_yJs5spX8vWvStGgJy0Pw11ph_Zb6T_Q@mail.gmail.com>
Message-ID: <1075482367.53300.1656785864970@mail.yahoo.com>

People often use sum() to count how many boolean values are true, not length).

Sent from the all new AOL app for Android 
 
  On Sat, Jul 2, 2022 at 2:14 PM, Andrew Simmons<akwsimmo at gmail.com> wrote:   It's supposed to match the length. Perhaps you meant to use which(is.na())?

On Sat, Jul 2, 2022, 14:04 akshay kulkarni <akshay_e4 at hotmail.com> wrote:

> Dear memebers,
>? ? ? ? ? ? ? ? ? ? ? ? ? ? I have list of stock data OHLCData for 500
> stocks, 15 of whom are NA's. The following is the code:
>
> require(quantmod)
> getOHLCData <- function(NSESym) {
>? OHLCData1 <- list()
>? for(i in 1:500){
>? ? OHLCData1[[i]] <- tryCatch(getSymbols(NSESym[i], auto.assign=FALSE),
>? ? ? ? ? ? ? ? ? ? ? ? ? error = function (e) {print(i); return(NA)})
>
>? }
>? return(OHLCData1)
> }
>
>? OHLCData <- getOHLCData(NSESym)
>
> however, when I check for is.na, I get the following:
>
> length(OHLCData)
> [1] 500
> > length(is.na(OHLCData))
> [1] 500
>
> length(is.na(OHLCData)) should return 15. Whats going wrong? I assume
> is.na returns TRUE if there is an NA.
>
> Yours sincerely,
> AKSHAY M KULKARNI
>
>? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

??? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
  

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Sat Jul  2 20:18:08 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Sat, 2 Jul 2022 18:18:08 +0000
Subject: [R] is.na with lists....
In-Reply-To: <CAPcHnpTOc8UXjRys5y_yJs5spX8vWvStGgJy0Pw11ph_Zb6T_Q@mail.gmail.com>
References: <PU4P216MB15680637B861C03D24236F5BC8BC9@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <CAPcHnpTOc8UXjRys5y_yJs5spX8vWvStGgJy0Pw11ph_Zb6T_Q@mail.gmail.com>
Message-ID: <PU4P216MB15689E9C400B6095B9A47830C8BC9@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear Andrew,
                      It's working! Thanks... I thought the following

> length(is.na(c(1,2,3,NA)))

would return

[1] 1

But got to know the difference ... thanks.

Yours sincerely,
AKSHAY M KULKARNI
________________________________
From: Andrew Simmons <akwsimmo at gmail.com>
Sent: Saturday, July 2, 2022 11:36 PM
To: akshay kulkarni <akshay_e4 at hotmail.com>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] is.na with lists....

It's supposed to match the length. Perhaps you meant to use which(is.na<http://is.na>())?

On Sat, Jul 2, 2022, 14:04 akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>> wrote:
Dear memebers,
                            I have list of stock data OHLCData for 500 stocks, 15 of whom are NA's. The following is the code:

require(quantmod)
getOHLCData <- function(NSESym) {
  OHLCData1 <- list()
  for(i in 1:500){
    OHLCData1[[i]] <- tryCatch(getSymbols(NSESym[i], auto.assign=FALSE),
                          error = function (e) {print(i); return(NA)})

  }
  return(OHLCData1)
}

 OHLCData <- getOHLCData(NSESym)

however, when I check for is.na<http://is.na>, I get the following:

length(OHLCData)
[1] 500
> length(is.na<http://is.na>(OHLCData))
[1] 500

length(is.na<http://is.na>(OHLCData)) should return 15. Whats going wrong? I assume is.na<http://is.na> returns TRUE if there is an NA.

Yours sincerely,
AKSHAY M KULKARNI

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From j@de@shod@@ m@iii@g oii googiem@ii@com  Sat Jul  2 20:20:49 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Sat, 2 Jul 2022 19:20:49 +0100
Subject: [R] mgcv: estimate concurvity vs worst concurvity in GAMs
Message-ID: <CANg3_k9XPzN2JJxOC0oBHiqaohnkOrsOCUbNFfkiNVr_H_egxg@mail.gmail.com>

Dear list members,

I was wondering if someone could explain (in conceptual terms) how to
interpret *estimate* concurvity in a GAM implemented with mgcv and how
it differs from *worst* concurvity (as obtained through mgcv's
concurvity function). I understand that concurvity is the
non-parametric analogue of collinearity in GAMs and that it represents
the extent to which a smooth term can be approximated by one or more
of the other smooth terms in the model.

It seems to be common practice to base one's course of action on the
worst concurvity estimate (as e.g. advised in Noam Ross' course on
GAMs). However, the mgcv help page for concurvity states that worst
concurvity is a "fairly pessimistic measure, as it looks at the worst
case irrespective of data", whereas estimate concurvity "does not
suffer from the pessimism or potential for over-optimism of the
previous two measures, but is less easy to understand".

Worst concurvity is extremely high in my GAMs, whereas estimate
concurvity is much lower (see below), so I am unsure as to whether I
should deal with the concurvity. I should stress that the aim of my
model is to gain an understanding of the relationship between
variables, rather than pure prediction performance.

For those interested, concurvity values are for a GAM with number of
daily deaths as response variable, and a smooth of time, a smooth of a
heat variable (wbgt_mean) and a smooth of precipitation as predictors,
the latter one being a potential confounder and wbgt_mean being the
variable of interest.  Heat and precipitation are modelled as having
distributed lag (6 days), set up as 7 column matrices as per Simon
Wood's book on GAMs (2017, p. 352). The model is as follows:

c1b <- gam(deaths_ip~s(time, k=200) + te(wbgt_mean, lag, k=c(12, 4)) +
te(precip_daily_total, lag, k=c(12, 4)), data = dat, family = nb,
method = 'REML', select = TRUE)

Let's ignore the issue of time decomposition for the moment, since
none of the many ways I've tried reduced concurvity much.

Depending on whether I have to deal with concurvity or not, my models
and their interpretation will look very different. As a potential
solution for high concurvity, I developed alternative models using a
detrended measure of heat as a predictor (by using residuals from a
GAM with heat as response and time as a predictor). Same for
precipitation. This does reduce concurvity substantially, but it
severely reduces the practical application/ interpretation of the
results, so I'd rather not take this route if I can avoid it.
(Modelling with an autoregressive term, as helpfully suggested in
response to a previous post, did not help to reduce concurvity
either).

Below is the output from the concurvity function with argument
full=TRUE. Explanations of estimate vs worst concurvity will be very
gratefully received!

                      para          s(time)       te(wbgt_mean,lag)
   te(precip_daily_total,lag)
worst         0.957257     0.96533049         0.9811214
  0.9749704
observed   0.957257     0.03825656         0.7652984                  0.8568042
estimate    0.957257     0.04334243         0.4197013                  0.5975567

And with argument full= FALSE:

$worst
                                        para
s(time)          te(wbgt_mean,lag)        te(precip_daily_total,lag)
para                                 1.000000e+00    6.033833e-17
  0.04891485                  0.6677235
s(time)                             6.033871e-17    1.000000e+00
 0.96109743                  0.7784443
te(wbgt_mean,lag)          4.006085e-02    9.521445e-01
1.00000000                  0.6941748
te(precip_daily_total,lag) 6.677235e-01    7.784443e-01
0.70026895                  1.0000000

$observed
                                        para
s(time)             te(wbgt_mean,lag)       te(precip_daily_total,lag)
para                                 1.000000e+00    2.203611e-27
5.898608e-33                  0.5790756
s(time)                             6.033871e-17    1.000000e+00
7.485690e-01                  0.2652631
te(wbgt_mean,lag)          4.006085e-02    1.928856e-02
1.000000e+00                  0.1902533
te(precip_daily_total,lag) 6.677235e-01    1.914191e-02
3.076595e-01                  1.0000000

$estimate
                                            para
s(time)              te(wbgt_mean,lag)
te(precip_daily_total,lag)
para                                 1.000000e+00   4.023819e-23
6.199709e-33                  0.2582767
s(time)                             6.033871e-17    1.000000e+00
4.031365e-01                  0.3198069
te(wbgt_mean,lag)          4.006085e-02    2.519282e-02
1.000000e+00                  0.2498455
te(precip_daily_total,lag) 6.677235e-01    1.644668e-02
1.119781e-01                  1.0000000


From @k@h@y_e4 @end|ng |rom hotm@||@com  Sat Jul  2 20:19:44 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Sat, 2 Jul 2022 18:19:44 +0000
Subject: [R] is.na with lists....
In-Reply-To: <1075482367.53300.1656785864970@mail.yahoo.com>
References: <PU4P216MB15680637B861C03D24236F5BC8BC9@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <CAPcHnpTOc8UXjRys5y_yJs5spX8vWvStGgJy0Pw11ph_Zb6T_Q@mail.gmail.com>
 <1075482367.53300.1656785864970@mail.yahoo.com>
Message-ID: <PU4P216MB156814C6CFCB44F2E4DDBF6FC8BC9@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear AVi,
                 Thanks ....got to know the difference...should have used sum() instead of length()...

Yours sincerely,
AKSHAY M KULKARNI
________________________________
From: Avi Gross <avigross at verizon.net>
Sent: Saturday, July 2, 2022 11:47 PM
To: akwsimmo at gmail.com <akwsimmo at gmail.com>; akshay kulkarni <akshay_e4 at hotmail.com>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] is.na with lists....

People often use sum() to count how many boolean values are true, not length).

Sent from the all new AOL app for Android<https://play.google.com/store/apps/details?id=com.aol.mobile.aolapp>

On Sat, Jul 2, 2022 at 2:14 PM, Andrew Simmons
<akwsimmo at gmail.com> wrote:
It's supposed to match the length. Perhaps you meant to use which(is.na())?

On Sat, Jul 2, 2022, 14:04 akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>> wrote:

> Dear memebers,
>                            I have list of stock data OHLCData for 500
> stocks, 15 of whom are NA's. The following is the code:
>
> require(quantmod)
> getOHLCData <- function(NSESym) {
>  OHLCData1 <- list()
>  for(i in 1:500){
>    OHLCData1[[i]] <- tryCatch(getSymbols(NSESym[i], auto.assign=FALSE),
>                          error = function (e) {print(i); return(NA)})
>
>  }
>  return(OHLCData1)
> }
>
>  OHLCData <- getOHLCData(NSESym)
>
> however, when I check for is.na, I get the following:
>
> length(OHLCData)
> [1] 500
> > length(is.na(OHLCData))
> [1] 500
>
> length(is.na(OHLCData)) should return 15. Whats going wrong? I assume
> is.na returns TRUE if there is an NA.
>
> Yours sincerely,
> AKSHAY M KULKARNI
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

>

    [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From @v|gro@@ @end|ng |rom ver|zon@net  Sat Jul  2 23:40:34 2022
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Sat, 2 Jul 2022 21:40:34 +0000 (UTC)
Subject: [R] Subsetting a vector using an index with all missing values
In-Reply-To: <CAHqSRuTLzAmB6tBJJVkGF94uudBD1Nq-f5pnFRC5eBKM4kNW3w@mail.gmail.com>
References: <CA+hbrhUmU4Bsu7HV-rjepBkcvLzyqe3rB7nrtHBKyiZiNkLg8g@mail.gmail.com>
 <CAHqSRuQUBBsDFxNUHBYmBkoDmUG1FOY_aoX9oVS9=afuX35J+Q@mail.gmail.com>
 <CA+hbrhX6tH-1R3k_rLZF+KMz--hJN8tRhJm7s5unbJd9Uob9gw@mail.gmail.com>
 <BN6PR2201MB155374BFB4ECF44564E8AFB6CFBC9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAHqSRuTLzAmB6tBJJVkGF94uudBD1Nq-f5pnFRC5eBKM4kNW3w@mail.gmail.com>
Message-ID: <18568172.601714.1656798034855@mail.yahoo.com>

Actually, Bill, I suspect there is a not uncommon use when you want a briefer logical vector to be broadcast or re-used as often as needed.?

The example below, if it can be read, uses an abbreviated set of boolean vectors to get the odd elements of another vector, then the even ones, and then the ones not divisible by three because it gets recycled. I suggest there are many variants like this in use, albeit a better design for some things might have been to add a flag to allow such expansion and otherwise consider it an error.

> test <- 11:20

> test[c(TRUE, FALSE)]

[1] 11 13 15 17 19

> test[c(FALSE, TRUE)]

[1] 12 14 16 18 20

> test[c(FALSE, TRUE, TRUE)]

[1] 12 13 15 16 18 19



-----Original Message-----
From: Bill Dunlap <williamwdunlap at gmail.com>
To: Ebert,Timothy Aaron <tebert at ufl.edu>
Cc: r-help <r-help at r-project.org>
Sent: Sat, Jul 2, 2022 11:24 am
Subject: Re: [R] Subsetting a vector using an index with all missing values

Perhaps it should be an error if the length of a logical subscript is
bigger than the dimension it is subscripting.? Currently in that case, x is
extended (with NA or NULL) to the length of the logical subscript.? I doubt
this is desired very often.

> dput((1:3)[c(FALSE,FALSE,FALSE,TRUE,TRUE)])
c(NA_integer_, NA_integer_)
> dput((1:3)[c(FALSE,FALSE,TRUE,FALSE,TRUE)])
c(3L, NA)

-Bill

On Sat, Jul 2, 2022 at 7:49 AM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:

> That nicely explains the difference in outcome between
> x[rep(TRUE,3)]
> x[rep("TRUE",3)]
>
>
> I do not quite get it.
> x<-1:10
> x[rep(x<2,3)]
> [1] 1 NA NA
> The length is three
>
> but
> x[rep(x>2,3)]
> [1] 3? 4? 5? 6? 7? 8? 9? 10? NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
> NA
> The length is 24
>
> Tim
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Peter Langfelder
> Sent: Saturday, July 2, 2022 2:19 AM
> To: Bill Dunlap <williamwdunlap at gmail.com>
> Cc: r-help <r-help at r-project.org>
> Subject: Re: [R] Subsetting a vector using an index with all missing values
>
> [External Email]
>
> Ah, thanks, that makes sense.
>
> Peter
>
> On Fri, Jul 1, 2022 at 10:01 PM Bill Dunlap <williamwdunlap at gmail.com>
> wrote:
> >
> > This has to do with the mode of the subscript - logical subscripts are
> > repeated to the length of x and integer/numeric ones are not.? NA is
> > logical, NA_integer_ is integer, so we get
> >
> > > x <- 1:10
> > > x[ rep(NA_integer_, 3) ]
> > [1] NA NA NA
> > > x[ rep(NA, 3) ]
> >? [1] NA NA NA NA NA NA NA NA NA NA
> >
> > -Bill
> >
> >
> > On Fri, Jul 1, 2022 at 8:31 PM Peter Langfelder <
> peter.langfelder at gmail.com> wrote:
> >>
> >> Hi all,
> >>
> >> I stumbled on subsetting behavior that seems counterintuitive and
> >> perhaps is a bug. Here's a simple example:
> >>
> >> > x = 1:10
> >> > x[ rep(NA, 3)]
> >>? [1] NA NA NA NA NA NA NA NA NA NA
> >>
> >> I would have expected 3 NAs (the length of the index), not 10 (all
> >> values in x). Looked at the documentation for the subsetting operator
> >> `[` but found nothing indicating that if the index contains all
> >> missing data, the result is the entire vector.
> >>
> >> I can work around the issue for a general 'index' using a somewhat
> >> clunky but straightforward construct along the lines of
> >>
> >> > index = rep(NA, 3)
> >> > x[c(1, index)][-1]
> >> [1] NA NA NA
> >>
> >> but I'm wondering if the behaviour above is intended.
> >>
> >> Thanks,
> >>
> >> Peter
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
> >> lman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVe
> >> AsRzsn7AkP-g&m=5kixibKMuZiVbTFd2D5fSZBYO3aFODtFyW96wUN-oC5gJtbOYJ9G0j
> >> 6zoo-P6z4W&s=WGGoTTZ6ENtmckv7K_B0OepH04TDjbiNp0D6IbdqpAg&e=
> >> PLEASE do read the posting guide
> >> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
> >> rg_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kV
> >> eAsRzsn7AkP-g&m=5kixibKMuZiVbTFd2D5fSZBYO3aFODtFyW96wUN-oC5gJtbOYJ9G0
> >> j6zoo-P6z4W&s=JErkxZzuGa2y8pjLddJY5u_vDIbjw4tX1vzkb8LAe98&e=
> >> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=5kixibKMuZiVbTFd2D5fSZBYO3aFODtFyW96wUN-oC5gJtbOYJ9G0j6zoo-P6z4W&s=WGGoTTZ6ENtmckv7K_B0OepH04TDjbiNp0D6IbdqpAg&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=5kixibKMuZiVbTFd2D5fSZBYO3aFODtFyW96wUN-oC5gJtbOYJ9G0j6zoo-P6z4W&s=JErkxZzuGa2y8pjLddJY5u_vDIbjw4tX1vzkb8LAe98&e=
> and provide commented, minimal, self-contained, reproducible code.
>

??? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From neh@@bo|ogn@90 @end|ng |rom gm@||@com  Sat Jul  2 23:58:35 2022
From: neh@@bo|ogn@90 @end|ng |rom gm@||@com (Neha gupta)
Date: Sat, 2 Jul 2022 23:58:35 +0200
Subject: [R] Ranger could not work with caret
In-Reply-To: <CA+nrPns0Cjk8F=yW6eCAuPr4Y+-K+dSP4P=QipbbCpxFY=PSHw@mail.gmail.com>
References: <CA+nrPnuc9+-R65CgJq_qNbuRTj491Jfv7YdbRgqd1Tom6ww=9Q@mail.gmail.com>
 <CA+nrPnvG5a26MFJJWQtuE9ZkCjHxqSb5gR7igr2kiBVGkQjUXw@mail.gmail.com>
 <0777f610-e580-c6e0-bb55-645bdbf32a0b@sapo.pt>
 <CA+nrPnu+awO6q=2eVpYom9yi7JT9FgAh0k0mcT=-9eLj1H7qAg@mail.gmail.com>
 <85d6347a-21f6-36ff-a92b-f0aae2c45749@sapo.pt>
 <CA+nrPnuzNEhv88j4LR0SXXRMqcSCWQGmef_-BsWJO8ubN1Oe9Q@mail.gmail.com>
 <b47d6a9c-7072-a819-3ba6-ab9c00c226e7@sapo.pt>
 <CA+nrPns0Cjk8F=yW6eCAuPr4Y+-K+dSP4P=QipbbCpxFY=PSHw@mail.gmail.com>
Message-ID: <CA+nrPnuuzCnSAkrd35aQRkLGkA_2zEMEQtv=BeU58uA1Dg4ypA@mail.gmail.com>

@Rui Barradas <ruipbarradas at sapo.pt>

I tried the code according to your comments and it works. However, when I
try it for another dataset with a different number of input features, it
again shows the same error message. I tried it with different types of
datasets and the same error appeared.

Best regards

On Fri, Jul 1, 2022 at 9:18 PM Neha gupta <neha.bologna90 at gmail.com> wrote:

> @Rui Barradas <ruipbarradas at sapo.pt>
>
> Thank you again for the useful explanation.
>
> Best regards
>
> On Fri, Jul 1, 2022 at 8:26 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
>> Hello,
>>
>> The error doesn't arise in randomForest because rf has a function tuneRF
>> that looks for the best mtry (best relative to OOB error estimate). And
>> it's this value that it uses.
>>
>> The question's code gives Ranger errors but it also gives R warnings:
>>
>> Warning messages:
>> 1: model fit failed for Fold01: mtry=48, min.node.size=5,
>> splitrule=variance Error in ranger::ranger(dependent.variable.name =
>> ".outcome", data = x,  :
>>    User interrupt or internal error.
>>
>>
>> As you can see, mtry=48 is the double of ncol(tr) when should *never* be
>> greater than the number of variables in the data set. Why it is using
>> this value, I don't know. Function bug? Ask the package maintainer?
>>
>> And, by the way, package caret does or can do a grid search for optimal
>> parameter values. If that is giving errors and you are calling rf
>> directly why bother whith caret's error? Use the original function. Here
>> is an example with tuneRF. Setting argument doBest to TRUE you'll have
>> both the optimal value for mtry and the fitted random forest. 2 in 1.
>>
>>
>> library(randomForest)
>> #  randomForest 4.7-1.1
>> #  Type rfNews() to see new features/changes/bug fixes.
>>
>> c2 <- tuneRF(
>>    x = tr[-ncol(tr)],
>>    y = tr$act_effort,
>>    mtryStart = ncol(tr)/2,
>>    doBest = TRUE
>> )
>> #  mtry = 12  OOB error = 139920.7
>> #  Searching left ...
>> #  mtry = 6     OOB error = 170909.3
>> #  -0.2214729 0.05
>> #  Searching right ...
>> #  mtry = 23    OOB error = 128566.7
>> #  0.08114586 0.05
>>
>> c2
>> #
>> #  Call:
>> #   randomForest(x = x, y = y, mtry = res[which.min(res[, 2]), 1])
>> #                 Type of random forest: regression
>> #                       Number of trees: 500
>> #  No. of variables tried at each split: 23
>> #
>> #            Mean of squared residuals: 129734.8
>> #                      % Var explained: 39.98
>>
>>
>> Hope this helps,
>>
>> Rui Barradas
>>
>>
>>
>> ?s 17:18 de 01/07/2022, Neha gupta escreveu:
>> > Thank you so much for your help. I hope it will work.
>> >
>> > However, why the same error doesn't arise when I am using rf. They both
>> > have the same parameters and it's default values.
>> >
>> > Best regards
>> >
>> > On Friday, July 1, 2022, Rui Barradas <ruipbarradas at sapo.pt
>> > <mailto:ruipbarradas at sapo.pt>> wrote:
>> >
>> >     Hello,
>> >
>> >     The error is in Ranger parameter mtry becoming greater than the
>> >     number of variables (columns).
>> >     mtry can be set manually in caret::train argument tuneGrid. But for
>> >     random forests you must also set the split rule and the minimum
>> node.
>> >
>> >
>> >     library(caret)
>> >     library(farff)
>> >
>> >     boot <- trainControl(method = "cv", number = 10)
>> >
>> >     # set the maximum mtry manually to ncol(tr)
>> >     # this creates a sequence of mtry values
>> >     mtry <- var_seq(ncol(tr), len = 3)  # 3 is the default value
>> >     mtry
>> >     #  [1]  2 13 24
>> >     #[1]  2 13 24
>> >
>> >     splitrule <- c("variance", "extratrees")
>> >     min.node.size <- 1:10
>> >     mtrygrid <- expand.grid(mtry, splitrule, min.node.size)
>> >     names(mtrygrid) <- c("mtry", "splitrule", "min.node.size")
>> >
>> >     c1 <- train(act_effort ~ ., data = tr,
>> >                 method = "ranger",
>> >                 tuneLength = 5,
>> >                 metric = "MAE",
>> >                 preProc = c("center", "scale", "nzv"),
>> >                 tuneGrid = mtrygrid,
>> >                 trControl = boot)
>> >     c1
>> >     #  Random Forest
>> >     #
>> >     #  30 samples
>> >     #  23 predictors
>> >     #
>> >     #  Pre-processing: centered (48), scaled (48), remove (58)
>> >     #  Resampling: Cross-Validated (10 fold)
>> >     #  Summary of sample sizes: 28, 27, 27, 28, 27, 27, ...
>> >     #  Resampling results across tuning parameters:
>> >     #
>> >     #    mtry  splitrule   min.node.size  RMSE      Rsquared   MAE
>> >     #     2    variance     1             256.6391  0.8103759  186.3609
>> >     #     2    variance     2             249.7120  0.8628109  183.6696
>> >     #     2    variance     3             258.8240  0.8284449  189.0712
>> >     #
>> >     # [...omit...]
>> >     #
>> >     #    13    extratrees  10             254.9569  0.8918014  191.2524
>> >     #    24    variance     1             177.7188  0.9458652  112.2800
>> >     #    24    variance     2             172.6826  0.9204287  108.5943
>> >     #    24    variance     3             172.9954  0.9271006  109.2554
>> >     #    24    variance     4             172.2467  0.9523067  110.0776
>> >     #    24    variance     5             175.2485  0.9283317  112.8798
>> >     #    24    variance     6             177.9285  0.9369881  115.8970
>> >     #    24    variance     7             180.5959  0.9485035  117.5816
>> >     #    24    variance     8             178.8037  0.9358033  117.8725
>> >     #    24    variance     9             176.5849  0.9210959  117.0055
>> >     #    24    variance    10             178.6439  0.9257969  119.8035
>> >     #    24    extratrees   1             219.1368  0.8801770  141.0720
>> >     #    24    extratrees   2             216.1900  0.8550002  140.9263
>> >     #    24    extratrees   3             212.4138  0.8979379  141.4282
>> >     #    24    extratrees   4             218.2631  0.9121471  146.2908
>> >     #    24    extratrees   5             212.5679  0.9279598  144.2715
>> >     #    24    extratrees   6             218.9856  0.9141754  152.2099
>> >     #    24    extratrees   7             222.8540  0.9412682  152.4614
>> >     #    24    extratrees   8             228.1156  0.9423414  161.8456
>> >     #    24    extratrees   9             226.6182  0.9408306  160.5264
>> >     #    24    extratrees  10             226.9280  0.9429413  165.6878
>> >     #
>> >     #  MAE was used to select the optimal model using the smallest
>> value.
>> >     #  The final values used for the model were mtry = 24, splitrule =
>> >     variance
>> >     #   and min.node.size = 2.
>> >     plot(c1)
>> >
>> >
>> >
>> >     Hope this helps,
>> >
>> >     Rui Barradas
>> >
>> >
>> >     ?s 23:03 de 30/06/2022, Neha gupta escreveu:
>> >
>> >         Ok, the data is pasted below
>> >
>> >         But on the same data (everything the same) and with other models
>> >         like RF, SVM etc, it works fine.
>> >
>> >           > dput(head(tr, 30))
>> >         structure(list(recordnumber = c(0, 0.02, 0.04, 0.06, 0.07, 0.08,
>> >         0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.16, 0.17, 0.18, 0.23, 0.24,
>> >         0.25, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.35, 0.36, 0.37, 0.38,
>> >         0.4, 0.41), projectname = structure(c(1L, 1L, 1L, 1L, 2L, 3L,
>> >         3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
>> >         4L, 4L, 4L, 4L, 4L, 4L, 5L, 6L), levels = c("de", "erb", "gal",
>> >         "X", "hst", "slp", "spl", "Y"), class = "factor"), cat2 =
>> >         structure(c(3L,
>> >         3L, 3L, 3L, 3L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L,
>> >         9L, 11L, 5L, 4L, 6L, 8L, 3L, 9L, 9L, 9L, 9L, 6L, 7L), levels =
>> >         c("Avionics",
>> >         "application_ground", "avionicsmonitoring",
>> "batchdataprocessing",
>> >         "communications", "datacapture", "launchprocessing",
>> >         "missionplanning",
>> >         "monitor_control", "operatingsystem", "realdataprocessing",
>> >         "science",
>> >         "simulation", "utility"), class = "factor"), forg =
>> structure(c(2L,
>> >         2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>> >         2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), levels =
>> c("f",
>> >         "g"), class = "factor"), center = structure(c(2L, 2L, 2L, 2L,
>> >         2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L,
>> >         2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 6L), levels = c("1", "2",
>> >         "3", "4", "5", "6"), class = "factor"), year = c(0.5, 0.5, 0.5,
>> >         0.5, 0.6875, 0.5625, 0.5625, 0.8125, 0.5625, 0.875, 0.5625,
>> 0.75,
>> >         0.5625, 0.8125, 0.75, 0.9375, 0.9375, 0.9375, 0.6875, 0.6875,
>> >         0.6875, 0.6875, 0.875, 1, 0.9375, 0.9375, 0.9375, 0.9375,
>> 0.5625,
>> >         0.25), mode = structure(c(3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>> >         3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>> >         3L, 3L, 3L, 3L, 3L), levels = c("embedded", "organic",
>> >         "semidetached"
>> >         ), class = "factor"), rely = structure(c(4L, 4L, 4L, 4L, 4L,
>> >         4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 3L, 3L, 3L, 3L,
>> >         3L, 4L, 4L, 4L, 3L, 3L, 3L, 3L, 4L), levels = c("vl", "l", "n",
>> >         "h", "vh", "xh"), class = "factor"), data = structure(c(2L, 2L,
>> >         2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L,
>> >         5L, 5L, 5L, 5L, 2L, 2L, 3L, 3L, 3L, 3L, 4L, 2L), levels =
>> c("vl",
>> >         "l", "n", "h", "vh", "xh"), class = "factor"), cplx =
>> >         structure(c(4L,
>> >         4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 4L,
>> >         3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L), levels =
>> >         c("vl",
>> >         "l", "n", "h", "vh", "xh"), class = "factor"), time =
>> >         structure(c(3L,
>> >         3L, 3L, 3L, 3L, 6L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L,
>> >         3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 5L, 3L), levels =
>> >         c("vl",
>> >         "l", "n", "h", "vh", "xh"), class = "factor"), stor =
>> >         structure(c(3L,
>> >         3L, 3L, 3L, 3L, 6L, 3L, 3L, 3L, 3L, 3L, 3L, 6L, 3L, 3L, 3L, 3L,
>> >         3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L), levels =
>> >         c("vl",
>> >         "l", "n", "h", "vh", "xh"), class = "factor"), virt =
>> >         structure(c(2L,
>> >         2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 4L, 2L, 2L, 2L, 2L, 3L, 3L,
>> >         3L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 2L, 2L), levels =
>> >         c("vl",
>> >         "l", "n", "h", "vh", "xh"), class = "factor"), turn =
>> >         structure(c(2L,
>> >         2L, 2L, 2L, 2L, 4L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L,
>> >         3L, 4L, 4L, 4L, 4L, 2L, 2L, 3L, 3L, 3L, 3L, 4L, 2L), levels =
>> >         c("vl",
>> >         "l", "n", "h", "vh", "xh"), class = "factor"), acap =
>> >         structure(c(3L,
>> >         3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 3L,
>> >         3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L), levels =
>> >         c("vl",
>> >         "l", "n", "h", "vh", "xh"), class = "factor"), aexp =
>> >         structure(c(3L,
>> >         3L, 3L, 3L, 3L, 4L, 5L, 5L, 5L, 5L, 4L, 5L, 5L, 4L, 5L, 4L, 4L,
>> >         4L, 4L, 4L, 4L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), levels =
>> >         c("vl",
>> >         "l", "n", "h", "vh", "xh"), class = "factor"), pcap =
>> >         structure(c(3L,
>> >         3L, 3L, 3L, 3L, 4L, 5L, 4L, 5L, 3L, 4L, 4L, 5L, 4L, 4L, 4L, 4L,
>> >         4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 3L, 4L, 4L), levels =
>> >         c("vl",
>> >         "l", "n", "h", "vh", "xh"), class = "factor"), vexp =
>> >         structure(c(3L,
>> >         3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 2L, 3L, 3L, 3L, 3L, 3L, 3L,
>> >         3L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 2L, 3L), levels =
>> >         c("vl",
>> >         "l", "n", "h", "vh", "xh"), class = "factor"), lexp =
>> >         structure(c(4L,
>> >         4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 2L, 1L, 4L, 4L, 4L, 4L, 3L, 3L,
>> >         3L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 3L, 4L, 3L, 4L, 3L), levels =
>> >         c("vl",
>> >         "l", "n", "h", "vh", "xh"), class = "factor"), modp =
>> >         structure(c(4L,
>> >         4L, 4L, 4L, 4L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>> >         3L, 5L, 5L, 5L, 5L, 4L, 4L, 3L, 3L, 4L, 3L, 4L, 4L), levels =
>> >         c("vl",
>> >         "l", "n", "h", "vh", "xh"), class = "factor"), tool =
>> >         structure(c(3L,
>> >         3L, 3L, 3L, 3L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>> >         3L, 5L, 5L, 5L, 5L, 3L, 3L, 3L, 3L, 4L, 3L, 3L, 1L), levels =
>> >         c("vl",
>> >         "l", "n", "h", "vh", "xh"), class = "factor"), sced =
>> >         structure(c(2L,
>> >         2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>> >         3L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 2L, 3L), levels =
>> >         c("vl",
>> >         "l", "n", "h", "vh", "xh"), class = "factor"), equivphyskloc =
>> >         c(0.025534,
>> >         0.006945, 0.008988, 0.002655, 0.067102, 0.006741, 0.019508,
>> >         0.005209,
>> >         0.101215, 0.010622, 0.101215, 0.019508, 0.152283, 0.031253,
>> >         0.014401,
>> >         0.014401, 0.037892, 0.009294, 0.015729, 0.012154, 0.032377,
>> >         0.035339,
>> >         0.004698, 0.009703, 0.00572, 0.012358, 0.091002, 0.007252,
>> 0.180778,
>> >         0.307527), act_effort = c(117.6, 31.2, 25.2, 10.8, 352.8, 72,
>> >         72, 24, 360, 36, 215, 48, 324, 60, 48, 90, 210, 48, 82, 62, 170,
>> >         192, 18, 50, 42, 60, 444, 42, 1248, 2400)), row.names = c(1L,
>> >         3L, 5L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 17L, 18L, 19L,
>> >         24L, 25L, 26L, 29L, 30L, 31L, 32L, 33L, 34L, 36L, 37L, 38L, 39L,
>> >         41L, 42L), class = "data.frame")
>> >
>> >
>> >
>> >         On Thu, Jun 30, 2022 at 11:28 PM Rui Barradas
>> >         <ruipbarradas at sapo.pt <mailto:ruipbarradas at sapo.pt>
>> >         <mailto:ruipbarradas at sapo.pt <mailto:ruipbarradas at sapo.pt>>>
>> wrote:
>> >
>> >              Hello,
>> >
>> >              Please post data in dput format, without it it's difficult
>> >         to tell.
>> >              If I substitute
>> >
>> >              mpg for act_effort
>> >              mtcars for tr
>> >
>> >              keeping everything else, I don't get any errors.
>> >              And the error message says clearly that the error is in tr
>> >         (data).
>> >
>> >              Can you post the output of dput(head(tr, 30))?
>> >
>> >              Rui Barradas
>> >
>> >
>> >              ?s 19:32 de 30/06/2022, Neha gupta escreveu:
>> >               > I posted it for the second time as I didn't get any
>> >         response from
>> >              group
>> >               > members. I am not sure if some problem is with the
>> question.
>> >               >
>> >               >
>> >               >
>> >               > I cannot run the "ranger" model with caret. I am only
>> >         using the
>> >              farff and
>> >               > caret libraries and the following code:
>> >               >
>> >               > boot <- trainControl(method = "cv", number=10)
>> >               >
>> >               > c1 <-train(act_effort ~ ., data = tr,
>> >               >                method = "ranger",
>> >               >                 tuneLength = 5,
>> >               >                metric = "MAE",
>> >               >                preProc = c("center", "scale", "nzv"),
>> >               >                trControl = boot)
>> >               >
>> >               > The error I get is the repeating of the following
>> >         message until I
>> >              interrupt
>> >               > it.
>> >               >
>> >               > Error: mtry can not be larger than number of variables
>> >         in data.
>> >              Ranger will
>> >               > EXIT now.
>> >               >
>> >               >       [[alternative HTML version deleted]]
>> >               >
>> >               > ______________________________________________
>> >               > R-help at r-project.org <mailto:R-help at r-project.org>
>> >         <mailto:R-help at r-project.org <mailto:R-help at r-project.org>>
>> >         mailing list
>> >              -- To UNSUBSCRIBE and more, see
>> >               > https://stat.ethz.ch/mailman/listinfo/r-help
>> >         <https://stat.ethz.ch/mailman/listinfo/r-help>
>> >              <https://stat.ethz.ch/mailman/listinfo/r-help
>> >         <https://stat.ethz.ch/mailman/listinfo/r-help>>
>> >               > PLEASE do read the posting guide
>> >         http://www.R-project.org/posting-guide.html
>> >         <http://www.R-project.org/posting-guide.html>
>> >              <http://www.R-project.org/posting-guide.html
>> >         <http://www.R-project.org/posting-guide.html>>
>> >               > and provide commented, minimal, self-contained,
>> >         reproducible code.
>> >
>>
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun Jul  3 01:22:27 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 3 Jul 2022 02:22:27 +0300
Subject: [R] Subsetting a vector using an index with all missing values
In-Reply-To: <18568172.601714.1656798034855@mail.yahoo.com>
References: <CA+hbrhUmU4Bsu7HV-rjepBkcvLzyqe3rB7nrtHBKyiZiNkLg8g@mail.gmail.com>
 <CAHqSRuQUBBsDFxNUHBYmBkoDmUG1FOY_aoX9oVS9=afuX35J+Q@mail.gmail.com>
 <CA+hbrhX6tH-1R3k_rLZF+KMz--hJN8tRhJm7s5unbJd9Uob9gw@mail.gmail.com>
 <BN6PR2201MB155374BFB4ECF44564E8AFB6CFBC9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAHqSRuTLzAmB6tBJJVkGF94uudBD1Nq-f5pnFRC5eBKM4kNW3w@mail.gmail.com>
 <18568172.601714.1656798034855@mail.yahoo.com>
Message-ID: <CAGxFJbR0pVx1TQ-T9aHfOMONq_WnVogOCDD3sOz=BwY+chdwdQ@mail.gmail.com>

Reread Bill's comment. It referred to logical indices being **longer** than
the vectors they subscript. You have it the other way round.

Bert

On Sun, Jul 3, 2022, 12:40 AM Avi Gross via R-help <r-help at r-project.org>
wrote:

> Actually, Bill, I suspect there is a not uncommon use when you want a
> briefer logical vector to be broadcast or re-used as often as needed.
>
> The example below, if it can be read, uses an abbreviated set of boolean
> vectors to get the odd elements of another vector, then the even ones, and
> then the ones not divisible by three because it gets recycled. I suggest
> there are many variants like this in use, albeit a better design for some
> things might have been to add a flag to allow such expansion and otherwise
> consider it an error.
>
> > test <- 11:20
>
> > test[c(TRUE, FALSE)]
>
> [1] 11 13 15 17 19
>
> > test[c(FALSE, TRUE)]
>
> [1] 12 14 16 18 20
>
> > test[c(FALSE, TRUE, TRUE)]
>
> [1] 12 13 15 16 18 19
>
>
>
> -----Original Message-----
> From: Bill Dunlap <williamwdunlap at gmail.com>
> To: Ebert,Timothy Aaron <tebert at ufl.edu>
> Cc: r-help <r-help at r-project.org>
> Sent: Sat, Jul 2, 2022 11:24 am
> Subject: Re: [R] Subsetting a vector using an index with all missing values
>
> Perhaps it should be an error if the length of a logical subscript is
> bigger than the dimension it is subscripting.  Currently in that case, x is
> extended (with NA or NULL) to the length of the logical subscript.  I doubt
> this is desired very often.
>
> > dput((1:3)[c(FALSE,FALSE,FALSE,TRUE,TRUE)])
> c(NA_integer_, NA_integer_)
> > dput((1:3)[c(FALSE,FALSE,TRUE,FALSE,TRUE)])
> c(3L, NA)
>
> -Bill
>
> On Sat, Jul 2, 2022 at 7:49 AM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>
> > That nicely explains the difference in outcome between
> > x[rep(TRUE,3)]
> > x[rep("TRUE",3)]
> >
> >
> > I do not quite get it.
> > x<-1:10
> > x[rep(x<2,3)]
> > [1] 1 NA NA
> > The length is three
> >
> > but
> > x[rep(x>2,3)]
> > [1] 3  4  5  6  7  8  9  10  NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
> > NA
> > The length is 24
> >
> > Tim
> > -----Original Message-----
> > From: R-help <r-help-bounces at r-project.org> On Behalf Of Peter
> Langfelder
> > Sent: Saturday, July 2, 2022 2:19 AM
> > To: Bill Dunlap <williamwdunlap at gmail.com>
> > Cc: r-help <r-help at r-project.org>
> > Subject: Re: [R] Subsetting a vector using an index with all missing
> values
> >
> > [External Email]
> >
> > Ah, thanks, that makes sense.
> >
> > Peter
> >
> > On Fri, Jul 1, 2022 at 10:01 PM Bill Dunlap <williamwdunlap at gmail.com>
> > wrote:
> > >
> > > This has to do with the mode of the subscript - logical subscripts are
> > > repeated to the length of x and integer/numeric ones are not.  NA is
> > > logical, NA_integer_ is integer, so we get
> > >
> > > > x <- 1:10
> > > > x[ rep(NA_integer_, 3) ]
> > > [1] NA NA NA
> > > > x[ rep(NA, 3) ]
> > >  [1] NA NA NA NA NA NA NA NA NA NA
> > >
> > > -Bill
> > >
> > >
> > > On Fri, Jul 1, 2022 at 8:31 PM Peter Langfelder <
> > peter.langfelder at gmail.com> wrote:
> > >>
> > >> Hi all,
> > >>
> > >> I stumbled on subsetting behavior that seems counterintuitive and
> > >> perhaps is a bug. Here's a simple example:
> > >>
> > >> > x = 1:10
> > >> > x[ rep(NA, 3)]
> > >>  [1] NA NA NA NA NA NA NA NA NA NA
> > >>
> > >> I would have expected 3 NAs (the length of the index), not 10 (all
> > >> values in x). Looked at the documentation for the subsetting operator
> > >> `[` but found nothing indicating that if the index contains all
> > >> missing data, the result is the entire vector.
> > >>
> > >> I can work around the issue for a general 'index' using a somewhat
> > >> clunky but straightforward construct along the lines of
> > >>
> > >> > index = rep(NA, 3)
> > >> > x[c(1, index)][-1]
> > >> [1] NA NA NA
> > >>
> > >> but I'm wondering if the behaviour above is intended.
> > >>
> > >> Thanks,
> > >>
> > >> Peter
> > >>
> > >> ______________________________________________
> > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
> > >> lman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVe
> > >> AsRzsn7AkP-g&m=5kixibKMuZiVbTFd2D5fSZBYO3aFODtFyW96wUN-oC5gJtbOYJ9G0j
> > >> 6zoo-P6z4W&s=WGGoTTZ6ENtmckv7K_B0OepH04TDjbiNp0D6IbdqpAg&e=
> > >> PLEASE do read the posting guide
> > >> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
> > >> rg_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kV
> > >> eAsRzsn7AkP-g&m=5kixibKMuZiVbTFd2D5fSZBYO3aFODtFyW96wUN-oC5gJtbOYJ9G0
> > >> j6zoo-P6z4W&s=JErkxZzuGa2y8pjLddJY5u_vDIbjw4tX1vzkb8LAe98&e=
> > >> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=5kixibKMuZiVbTFd2D5fSZBYO3aFODtFyW96wUN-oC5gJtbOYJ9G0j6zoo-P6z4W&s=WGGoTTZ6ENtmckv7K_B0OepH04TDjbiNp0D6IbdqpAg&e=
> > PLEASE do read the posting guide
> >
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=5kixibKMuZiVbTFd2D5fSZBYO3aFODtFyW96wUN-oC5gJtbOYJ9G0j6zoo-P6z4W&s=JErkxZzuGa2y8pjLddJY5u_vDIbjw4tX1vzkb8LAe98&e=
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>     [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r@oknz @end|ng |rom gm@||@com  Sun Jul  3 05:24:41 2022
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Sun, 3 Jul 2022 15:24:41 +1200
Subject: [R] A humble request
In-Reply-To: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
References: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
Message-ID: <CABcYAdJ7E6aoC7ZYEx2i5Mx9VHRd+S8xMDCmsvbeCy++bTcosA@mail.gmail.com>

Can we start a step back please?
wmtsa stands for
  Wavelet Methods for Time Series Analysis.

OK, so you have some time series data,
and for some reason you want to analyse
your data using wavelets.  No worries.
But does it have to be THIS unmaintained
package?

Why not visit

https://CRAN.R-project.org/view=TimeSeries
and search for "wavelets" in the text?
Oh heck, I might as well do it for you.
<snip>
*Wavelet methods* : The wavelets
<https://cran.r-project.org/web/packages/wavelets/index.html> package
includes computing wavelet filters, wavelet transforms and multiresolution
analyses. Multiresolution forecasting using wavelets is also implemented in
mrf <https://cran.r-project.org/web/packages/mrf/index.html>. WaveletComp
<https://cran.r-project.org/web/packages/WaveletComp/index.html> provides
some tools for wavelet-based analysis of univariate and bivariate time
series including cross-wavelets, phase-difference and significance tests.
biwavelet <https://cran.r-project.org/web/packages/biwavelet/index.html> is
a port of the WTC Matlab package for univariate and bivariate wavelet
analyses. mvLSW <https://cran.r-project.org/web/packages/mvLSW/index.html>
provides tools for multivariate locally stationary wavelet processes.
LSWPlib <https://cran.r-project.org/web/packages/LSWPlib/index.html>
contains functions for simulation and spectral estimation of locally
stationary wavelet packet processes. Tests of white noise using wavelets
are provided by hwwntest
<https://cran.r-project.org/web/packages/hwwntest/index.html>. Wavelet
scalogram tools are contained in wavScalogram
<https://cran.r-project.org/web/packages/wavScalogram/index.html>. Further
wavelet methods can be found in the packages rwt
<https://cran.r-project.org/web/packages/rwt/index.html>, waveslim
<https://cran.r-project.org/web/packages/waveslim/index.html>, wavethresh
<https://cran.r-project.org/web/packages/wavethresh/index.html>.
</snip>

Presumably there is a reason that nobody else has
bothered to continue maintaining wmtsa.  Perhaps
one of those other wavelets + time series packages
can do what you need?


On Sun, 3 Jul 2022 at 04:12, Muhammad Zubair Chishti <
mzchishti at eco.qau.edu.pk> wrote:

> Dear Experts,
> I cannot find a package "wmtsa" for my R version "R 4.2.0". Kindly help me
> to find it or share the link with me.
> Although I tried the old version of "wmtsa" but failed.
> Thank you for your precious time.
>
> Regards
> Muhammad Zubair Chishti
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r@oknz @end|ng |rom gm@||@com  Sun Jul  3 06:11:25 2022
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Sun, 3 Jul 2022 16:11:25 +1200
Subject: [R] A humble request
In-Reply-To: <CABcYAdJ7E6aoC7ZYEx2i5Mx9VHRd+S8xMDCmsvbeCy++bTcosA@mail.gmail.com>
References: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
 <CABcYAdJ7E6aoC7ZYEx2i5Mx9VHRd+S8xMDCmsvbeCy++bTcosA@mail.gmail.com>
Message-ID: <CABcYAd+f9yupeiztYWL4BDF=BOQf0Bn_ayRYUw0udmScGyxuig@mail.gmail.com>

A postscript to my previous comment.
I used to supervise PhD students.
Let me advise you to write this issue up as a draft section for your thesis.
1. Why I wanted to use the wmtsa package.
2. Why I didn't.
3. How I went about selecting a replacement.
4. What I chose and why that's the right choice.
5. How the analyses I wanted to do are done in
   package X and what difference it makes.

Off the top of my head, the only reasons for struggling to use an old
package are to try to replicate someone else's results and/or to try to use
their software (built atop the dead package) with new data.  Well, if you
get different results, that's interesting too, and then it's time to work
harder to resurrect the dead package.

Speaking of which, an easier route might be to set up a separate
environment running an old version of R that *can* run the old code and the
old code's dependencies.  In fact trying to use the same versions that the
work you're trying to reproduce used might make a lot of sense.

Overall, I think selecting an alternative package
that *is* currently maintained is the best use of your time, but your
supervisor should be able to help you with that.  Selecting appropriate
packages is part of doing research, after all, and demonstrating
that you can do it is all to the good, no?


On Sun, 3 Jul 2022 at 15:24, Richard O'Keefe <raoknz at gmail.com> wrote:

> Can we start a step back please?
> wmtsa stands for
>   Wavelet Methods for Time Series Analysis.
>
> OK, so you have some time series data,
> and for some reason you want to analyse
> your data using wavelets.  No worries.
> But does it have to be THIS unmaintained
> package?
>
> Why not visit
>
> https://CRAN.R-project.org/view=TimeSeries
> and search for "wavelets" in the text?
> Oh heck, I might as well do it for you.
> <snip>
> *Wavelet methods* : The wavelets
> <https://cran.r-project.org/web/packages/wavelets/index.html> package
> includes computing wavelet filters, wavelet transforms and multiresolution
> analyses. Multiresolution forecasting using wavelets is also implemented in
> mrf <https://cran.r-project.org/web/packages/mrf/index.html>. WaveletComp
> <https://cran.r-project.org/web/packages/WaveletComp/index.html> provides
> some tools for wavelet-based analysis of univariate and bivariate time
> series including cross-wavelets, phase-difference and significance tests.
> biwavelet <https://cran.r-project.org/web/packages/biwavelet/index.html>
> is a port of the WTC Matlab package for univariate and bivariate wavelet
> analyses. mvLSW <https://cran.r-project.org/web/packages/mvLSW/index.html>
> provides tools for multivariate locally stationary wavelet processes.
> LSWPlib <https://cran.r-project.org/web/packages/LSWPlib/index.html>
> contains functions for simulation and spectral estimation of locally
> stationary wavelet packet processes. Tests of white noise using wavelets
> are provided by hwwntest
> <https://cran.r-project.org/web/packages/hwwntest/index.html>. Wavelet
> scalogram tools are contained in wavScalogram
> <https://cran.r-project.org/web/packages/wavScalogram/index.html>.
> Further wavelet methods can be found in the packages rwt
> <https://cran.r-project.org/web/packages/rwt/index.html>, waveslim
> <https://cran.r-project.org/web/packages/waveslim/index.html>, wavethresh
> <https://cran.r-project.org/web/packages/wavethresh/index.html>.
> </snip>
>
> Presumably there is a reason that nobody else has
> bothered to continue maintaining wmtsa.  Perhaps
> one of those other wavelets + time series packages
> can do what you need?
>
>
> On Sun, 3 Jul 2022 at 04:12, Muhammad Zubair Chishti <
> mzchishti at eco.qau.edu.pk> wrote:
>
>> Dear Experts,
>> I cannot find a package "wmtsa" for my R version "R 4.2.0". Kindly help me
>> to find it or share the link with me.
>> Although I tried the old version of "wmtsa" but failed.
>> Thank you for your precious time.
>>
>> Regards
>> Muhammad Zubair Chishti
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Sun Jul  3 08:00:26 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Sun, 3 Jul 2022 11:00:26 +0500
Subject: [R] A humble request
In-Reply-To: <CABcYAd+f9yupeiztYWL4BDF=BOQf0Bn_ayRYUw0udmScGyxuig@mail.gmail.com>
References: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
 <CABcYAdJ7E6aoC7ZYEx2i5Mx9VHRd+S8xMDCmsvbeCy++bTcosA@mail.gmail.com>
 <CABcYAd+f9yupeiztYWL4BDF=BOQf0Bn_ayRYUw0udmScGyxuig@mail.gmail.com>
Message-ID: <CAMfKi3+e+gsv6f5ko33e7FCSWYt4a=iwgtoq=MeMtgP+Y8uA8Q@mail.gmail.com>

Dear Respected Experts and specifically Professor Richard O'Keefe,
Thank you so much for your precious time and generous help. However, the
problem is still there and I am just unable to resolve it due to the lack
of expertise in R. Still, the hope is there. I believe that this platform
can help me.

Regards
Muhammad Zubair Chishti
School of Business,
Zhengzhou University, Henan, China
My Google scholar link:
https://scholar.google.com/citations?hl=en&user=YPqNJMwAAAAJ
My ReseachGate Profile:
https://www.researchgate.net/profile/Muhammad-Chishti


On Sun, Jul 3, 2022 at 9:11 AM Richard O'Keefe <raoknz at gmail.com> wrote:

> A postscript to my previous comment.
> I used to supervise PhD students.
> Let me advise you to write this issue up as a draft section for your
> thesis.
> 1. Why I wanted to use the wmtsa package.
> 2. Why I didn't.
> 3. How I went about selecting a replacement.
> 4. What I chose and why that's the right choice.
> 5. How the analyses I wanted to do are done in
>    package X and what difference it makes.
>
> Off the top of my head, the only reasons for struggling to use an old
> package are to try to replicate someone else's results and/or to try to use
> their software (built atop the dead package) with new data.  Well, if you
> get different results, that's interesting too, and then it's time to work
> harder to resurrect the dead package.
>
> Speaking of which, an easier route might be to set up a separate
> environment running an old version of R that *can* run the old code and the
> old code's dependencies.  In fact trying to use the same versions that the
> work you're trying to reproduce used might make a lot of sense.
>
> Overall, I think selecting an alternative package
> that *is* currently maintained is the best use of your time, but your
> supervisor should be able to help you with that.  Selecting appropriate
> packages is part of doing research, after all, and demonstrating
> that you can do it is all to the good, no?
>
>
> On Sun, 3 Jul 2022 at 15:24, Richard O'Keefe <raoknz at gmail.com> wrote:
>
>> Can we start a step back please?
>> wmtsa stands for
>>   Wavelet Methods for Time Series Analysis.
>>
>> OK, so you have some time series data,
>> and for some reason you want to analyse
>> your data using wavelets.  No worries.
>> But does it have to be THIS unmaintained
>> package?
>>
>> Why not visit
>>
>> https://CRAN.R-project.org/view=TimeSeries
>> and search for "wavelets" in the text?
>> Oh heck, I might as well do it for you.
>> <snip>
>> *Wavelet methods* : The wavelets
>> <https://cran.r-project.org/web/packages/wavelets/index.html> package
>> includes computing wavelet filters, wavelet transforms and multiresolution
>> analyses. Multiresolution forecasting using wavelets is also implemented in
>> mrf <https://cran.r-project.org/web/packages/mrf/index.html>. WaveletComp
>> <https://cran.r-project.org/web/packages/WaveletComp/index.html>
>> provides some tools for wavelet-based analysis of univariate and bivariate
>> time series including cross-wavelets, phase-difference and significance
>> tests. biwavelet
>> <https://cran.r-project.org/web/packages/biwavelet/index.html> is a port
>> of the WTC Matlab package for univariate and bivariate wavelet analyses.
>> mvLSW <https://cran.r-project.org/web/packages/mvLSW/index.html>
>> provides tools for multivariate locally stationary wavelet processes.
>> LSWPlib <https://cran.r-project.org/web/packages/LSWPlib/index.html>
>> contains functions for simulation and spectral estimation of locally
>> stationary wavelet packet processes. Tests of white noise using wavelets
>> are provided by hwwntest
>> <https://cran.r-project.org/web/packages/hwwntest/index.html>. Wavelet
>> scalogram tools are contained in wavScalogram
>> <https://cran.r-project.org/web/packages/wavScalogram/index.html>.
>> Further wavelet methods can be found in the packages rwt
>> <https://cran.r-project.org/web/packages/rwt/index.html>, waveslim
>> <https://cran.r-project.org/web/packages/waveslim/index.html>, wavethresh
>> <https://cran.r-project.org/web/packages/wavethresh/index.html>.
>> </snip>
>>
>> Presumably there is a reason that nobody else has
>> bothered to continue maintaining wmtsa.  Perhaps
>> one of those other wavelets + time series packages
>> can do what you need?
>>
>>
>> On Sun, 3 Jul 2022 at 04:12, Muhammad Zubair Chishti <
>> mzchishti at eco.qau.edu.pk> wrote:
>>
>>> Dear Experts,
>>> I cannot find a package "wmtsa" for my R version "R 4.2.0". Kindly help
>>> me
>>> to find it or share the link with me.
>>> Although I tried the old version of "wmtsa" but failed.
>>> Thank you for your precious time.
>>>
>>> Regards
>>> Muhammad Zubair Chishti
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>

	[[alternative HTML version deleted]]


From @kw@|mmo @end|ng |rom gm@||@com  Sun Jul  3 08:28:50 2022
From: @kw@|mmo @end|ng |rom gm@||@com (Andrew Simmons)
Date: Sun, 3 Jul 2022 02:28:50 -0400
Subject: [R] A humble request
In-Reply-To: <CAMfKi3+e+gsv6f5ko33e7FCSWYt4a=iwgtoq=MeMtgP+Y8uA8Q@mail.gmail.com>
References: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
 <CABcYAdJ7E6aoC7ZYEx2i5Mx9VHRd+S8xMDCmsvbeCy++bTcosA@mail.gmail.com>
 <CABcYAd+f9yupeiztYWL4BDF=BOQf0Bn_ayRYUw0udmScGyxuig@mail.gmail.com>
 <CAMfKi3+e+gsv6f5ko33e7FCSWYt4a=iwgtoq=MeMtgP+Y8uA8Q@mail.gmail.com>
Message-ID: <CAPcHnpR8AgPwx=CNCJrJ9CmqzqG7oF2i_5PWWj9JgYtq8t1_AQ@mail.gmail.com>

It seems like this package was archived because package "ifultools"
was archived. I tried installing "ifultools" from source, but it has a
seriously large amount of C compilation issues. The main issue seems
to be that variable PROBLEM was never defined anywhere, and it is
unclear what its definition should be. Unfortunately, as far as I can
tell, this issue is unfixable. If you want to install "wmtsa", you'll
have to use an older version of R. Otherwise, you can use one of the
other wavelet analysis packages that Richard O'Keefe mentioned.

On Sun, Jul 3, 2022 at 2:01 AM Muhammad Zubair Chishti
<mzchishti at eco.qau.edu.pk> wrote:
>
> Dear Respected Experts and specifically Professor Richard O'Keefe,
> Thank you so much for your precious time and generous help. However, the
> problem is still there and I am just unable to resolve it due to the lack
> of expertise in R. Still, the hope is there. I believe that this platform
> can help me.
>
> Regards
> Muhammad Zubair Chishti
> School of Business,
> Zhengzhou University, Henan, China
> My Google scholar link:
> https://scholar.google.com/citations?hl=en&user=YPqNJMwAAAAJ
> My ReseachGate Profile:
> https://www.researchgate.net/profile/Muhammad-Chishti
>
>
> On Sun, Jul 3, 2022 at 9:11 AM Richard O'Keefe <raoknz at gmail.com> wrote:
>
> > A postscript to my previous comment.
> > I used to supervise PhD students.
> > Let me advise you to write this issue up as a draft section for your
> > thesis.
> > 1. Why I wanted to use the wmtsa package.
> > 2. Why I didn't.
> > 3. How I went about selecting a replacement.
> > 4. What I chose and why that's the right choice.
> > 5. How the analyses I wanted to do are done in
> >    package X and what difference it makes.
> >
> > Off the top of my head, the only reasons for struggling to use an old
> > package are to try to replicate someone else's results and/or to try to use
> > their software (built atop the dead package) with new data.  Well, if you
> > get different results, that's interesting too, and then it's time to work
> > harder to resurrect the dead package.
> >
> > Speaking of which, an easier route might be to set up a separate
> > environment running an old version of R that *can* run the old code and the
> > old code's dependencies.  In fact trying to use the same versions that the
> > work you're trying to reproduce used might make a lot of sense.
> >
> > Overall, I think selecting an alternative package
> > that *is* currently maintained is the best use of your time, but your
> > supervisor should be able to help you with that.  Selecting appropriate
> > packages is part of doing research, after all, and demonstrating
> > that you can do it is all to the good, no?
> >
> >
> > On Sun, 3 Jul 2022 at 15:24, Richard O'Keefe <raoknz at gmail.com> wrote:
> >
> >> Can we start a step back please?
> >> wmtsa stands for
> >>   Wavelet Methods for Time Series Analysis.
> >>
> >> OK, so you have some time series data,
> >> and for some reason you want to analyse
> >> your data using wavelets.  No worries.
> >> But does it have to be THIS unmaintained
> >> package?
> >>
> >> Why not visit
> >>
> >> https://CRAN.R-project.org/view=TimeSeries
> >> and search for "wavelets" in the text?
> >> Oh heck, I might as well do it for you.
> >> <snip>
> >> *Wavelet methods* : The wavelets
> >> <https://cran.r-project.org/web/packages/wavelets/index.html> package
> >> includes computing wavelet filters, wavelet transforms and multiresolution
> >> analyses. Multiresolution forecasting using wavelets is also implemented in
> >> mrf <https://cran.r-project.org/web/packages/mrf/index.html>. WaveletComp
> >> <https://cran.r-project.org/web/packages/WaveletComp/index.html>
> >> provides some tools for wavelet-based analysis of univariate and bivariate
> >> time series including cross-wavelets, phase-difference and significance
> >> tests. biwavelet
> >> <https://cran.r-project.org/web/packages/biwavelet/index.html> is a port
> >> of the WTC Matlab package for univariate and bivariate wavelet analyses.
> >> mvLSW <https://cran.r-project.org/web/packages/mvLSW/index.html>
> >> provides tools for multivariate locally stationary wavelet processes.
> >> LSWPlib <https://cran.r-project.org/web/packages/LSWPlib/index.html>
> >> contains functions for simulation and spectral estimation of locally
> >> stationary wavelet packet processes. Tests of white noise using wavelets
> >> are provided by hwwntest
> >> <https://cran.r-project.org/web/packages/hwwntest/index.html>. Wavelet
> >> scalogram tools are contained in wavScalogram
> >> <https://cran.r-project.org/web/packages/wavScalogram/index.html>.
> >> Further wavelet methods can be found in the packages rwt
> >> <https://cran.r-project.org/web/packages/rwt/index.html>, waveslim
> >> <https://cran.r-project.org/web/packages/waveslim/index.html>, wavethresh
> >> <https://cran.r-project.org/web/packages/wavethresh/index.html>.
> >> </snip>
> >>
> >> Presumably there is a reason that nobody else has
> >> bothered to continue maintaining wmtsa.  Perhaps
> >> one of those other wavelets + time series packages
> >> can do what you need?
> >>
> >>
> >> On Sun, 3 Jul 2022 at 04:12, Muhammad Zubair Chishti <
> >> mzchishti at eco.qau.edu.pk> wrote:
> >>
> >>> Dear Experts,
> >>> I cannot find a package "wmtsa" for my R version "R 4.2.0". Kindly help
> >>> me
> >>> to find it or share the link with me.
> >>> Although I tried the old version of "wmtsa" but failed.
> >>> Thank you for your precious time.
> >>>
> >>> Regards
> >>> Muhammad Zubair Chishti
> >>>
> >>>         [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Sun Jul  3 13:24:52 2022
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Sun, 3 Jul 2022 06:24:52 -0500
Subject: [R] A humble request
In-Reply-To: <CAPcHnpR8AgPwx=CNCJrJ9CmqzqG7oF2i_5PWWj9JgYtq8t1_AQ@mail.gmail.com>
References: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
 <CABcYAdJ7E6aoC7ZYEx2i5Mx9VHRd+S8xMDCmsvbeCy++bTcosA@mail.gmail.com>
 <CABcYAd+f9yupeiztYWL4BDF=BOQf0Bn_ayRYUw0udmScGyxuig@mail.gmail.com>
 <CAMfKi3+e+gsv6f5ko33e7FCSWYt4a=iwgtoq=MeMtgP+Y8uA8Q@mail.gmail.com>
 <CAPcHnpR8AgPwx=CNCJrJ9CmqzqG7oF2i_5PWWj9JgYtq8t1_AQ@mail.gmail.com>
Message-ID: <a82dbde5-0358-ca87-339f-fdd8f403b251@effectivedefense.org>

Muhammad Zubair Chishti:


	  What specifically have you tried?  What were the results?


	  Please "provide commented, minimal, self-contained, reproducible 
code", per "the posting guide 
http://www.R-project.org/posting-guide.html", as indicated in at the end 
of each email in this thread.


	  And don't overlook the suggestion I made:  Download the archived 
package.  Do NOT try to compile it. Instead source only the function you 
want, try to run it.  With luck, it will work.  If it doesn't, you will 
get a diagnostic that can help you take the next step.


	  Spencer


On 7/3/22 1:28 AM, Andrew Simmons wrote:
> It seems like this package was archived because package "ifultools"
> was archived. I tried installing "ifultools" from source, but it has a
> seriously large amount of C compilation issues. The main issue seems
> to be that variable PROBLEM was never defined anywhere, and it is
> unclear what its definition should be. Unfortunately, as far as I can
> tell, this issue is unfixable. If you want to install "wmtsa", you'll
> have to use an older version of R. Otherwise, you can use one of the
> other wavelet analysis packages that Richard O'Keefe mentioned.
> 
> On Sun, Jul 3, 2022 at 2:01 AM Muhammad Zubair Chishti
> <mzchishti at eco.qau.edu.pk> wrote:
>>
>> Dear Respected Experts and specifically Professor Richard O'Keefe,
>> Thank you so much for your precious time and generous help. However, the
>> problem is still there and I am just unable to resolve it due to the lack
>> of expertise in R. Still, the hope is there. I believe that this platform
>> can help me.
>>
>> Regards
>> Muhammad Zubair Chishti
>> School of Business,
>> Zhengzhou University, Henan, China
>> My Google scholar link:
>> https://scholar.google.com/citations?hl=en&user=YPqNJMwAAAAJ
>> My ReseachGate Profile:
>> https://www.researchgate.net/profile/Muhammad-Chishti
>>
>>
>> On Sun, Jul 3, 2022 at 9:11 AM Richard O'Keefe <raoknz at gmail.com> wrote:
>>
>>> A postscript to my previous comment.
>>> I used to supervise PhD students.
>>> Let me advise you to write this issue up as a draft section for your
>>> thesis.
>>> 1. Why I wanted to use the wmtsa package.
>>> 2. Why I didn't.
>>> 3. How I went about selecting a replacement.
>>> 4. What I chose and why that's the right choice.
>>> 5. How the analyses I wanted to do are done in
>>>     package X and what difference it makes.
>>>
>>> Off the top of my head, the only reasons for struggling to use an old
>>> package are to try to replicate someone else's results and/or to try to use
>>> their software (built atop the dead package) with new data.  Well, if you
>>> get different results, that's interesting too, and then it's time to work
>>> harder to resurrect the dead package.
>>>
>>> Speaking of which, an easier route might be to set up a separate
>>> environment running an old version of R that *can* run the old code and the
>>> old code's dependencies.  In fact trying to use the same versions that the
>>> work you're trying to reproduce used might make a lot of sense.
>>>
>>> Overall, I think selecting an alternative package
>>> that *is* currently maintained is the best use of your time, but your
>>> supervisor should be able to help you with that.  Selecting appropriate
>>> packages is part of doing research, after all, and demonstrating
>>> that you can do it is all to the good, no?
>>>
>>>
>>> On Sun, 3 Jul 2022 at 15:24, Richard O'Keefe <raoknz at gmail.com> wrote:
>>>
>>>> Can we start a step back please?
>>>> wmtsa stands for
>>>>    Wavelet Methods for Time Series Analysis.
>>>>
>>>> OK, so you have some time series data,
>>>> and for some reason you want to analyse
>>>> your data using wavelets.  No worries.
>>>> But does it have to be THIS unmaintained
>>>> package?
>>>>
>>>> Why not visit
>>>>
>>>> https://CRAN.R-project.org/view=TimeSeries
>>>> and search for "wavelets" in the text?
>>>> Oh heck, I might as well do it for you.
>>>> <snip>
>>>> *Wavelet methods* : The wavelets
>>>> <https://cran.r-project.org/web/packages/wavelets/index.html> package
>>>> includes computing wavelet filters, wavelet transforms and multiresolution
>>>> analyses. Multiresolution forecasting using wavelets is also implemented in
>>>> mrf <https://cran.r-project.org/web/packages/mrf/index.html>. WaveletComp
>>>> <https://cran.r-project.org/web/packages/WaveletComp/index.html>
>>>> provides some tools for wavelet-based analysis of univariate and bivariate
>>>> time series including cross-wavelets, phase-difference and significance
>>>> tests. biwavelet
>>>> <https://cran.r-project.org/web/packages/biwavelet/index.html> is a port
>>>> of the WTC Matlab package for univariate and bivariate wavelet analyses.
>>>> mvLSW <https://cran.r-project.org/web/packages/mvLSW/index.html>
>>>> provides tools for multivariate locally stationary wavelet processes.
>>>> LSWPlib <https://cran.r-project.org/web/packages/LSWPlib/index.html>
>>>> contains functions for simulation and spectral estimation of locally
>>>> stationary wavelet packet processes. Tests of white noise using wavelets
>>>> are provided by hwwntest
>>>> <https://cran.r-project.org/web/packages/hwwntest/index.html>. Wavelet
>>>> scalogram tools are contained in wavScalogram
>>>> <https://cran.r-project.org/web/packages/wavScalogram/index.html>.
>>>> Further wavelet methods can be found in the packages rwt
>>>> <https://cran.r-project.org/web/packages/rwt/index.html>, waveslim
>>>> <https://cran.r-project.org/web/packages/waveslim/index.html>, wavethresh
>>>> <https://cran.r-project.org/web/packages/wavethresh/index.html>.
>>>> </snip>
>>>>
>>>> Presumably there is a reason that nobody else has
>>>> bothered to continue maintaining wmtsa.  Perhaps
>>>> one of those other wavelets + time series packages
>>>> can do what you need?
>>>>
>>>>
>>>> On Sun, 3 Jul 2022 at 04:12, Muhammad Zubair Chishti <
>>>> mzchishti at eco.qau.edu.pk> wrote:
>>>>
>>>>> Dear Experts,
>>>>> I cannot find a package "wmtsa" for my R version "R 4.2.0". Kindly help
>>>>> me
>>>>> to find it or share the link with me.
>>>>> Although I tried the old version of "wmtsa" but failed.
>>>>> Thank you for your precious time.
>>>>>
>>>>> Regards
>>>>> Muhammad Zubair Chishti
>>>>>
>>>>>          [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkr|de@u @end|ng |rom gm@||@com  Sun Jul  3 14:38:13 2022
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Sun, 3 Jul 2022 08:38:13 -0400
Subject: [R] A humble request
In-Reply-To: <a82dbde5-0358-ca87-339f-fdd8f403b251@effectivedefense.org>
References: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
 <CABcYAdJ7E6aoC7ZYEx2i5Mx9VHRd+S8xMDCmsvbeCy++bTcosA@mail.gmail.com>
 <CABcYAd+f9yupeiztYWL4BDF=BOQf0Bn_ayRYUw0udmScGyxuig@mail.gmail.com>
 <CAMfKi3+e+gsv6f5ko33e7FCSWYt4a=iwgtoq=MeMtgP+Y8uA8Q@mail.gmail.com>
 <CAPcHnpR8AgPwx=CNCJrJ9CmqzqG7oF2i_5PWWj9JgYtq8t1_AQ@mail.gmail.com>
 <a82dbde5-0358-ca87-339f-fdd8f403b251@effectivedefense.org>
Message-ID: <CAKZQJMBtz0h+HKz9cGpz7rRt1-MLX4vi1Rxu3TDpteZc6-VuAQ@mail.gmail.com>

Spenser,
the idea to source the fonction makes sense but since tho OP is a very
new beginner perhaps you could point him towards code showing him how
to do this?  I have never done this AFAIR, and while I suspect a few
minutes googling would show me how, it is likely to be more difficult
for a noobie.

On Sun, 3 Jul 2022 at 07:25, Spencer Graves
<spencer.graves at effectivedefense.org> wrote:
>
> Muhammad Zubair Chishti:
>
>
>           What specifically have you tried?  What were the results?
>
>
>           Please "provide commented, minimal, self-contained, reproducible
> code", per "the posting guide
> http://www.R-project.org/posting-guide.html", as indicated in at the end
> of each email in this thread.
>
>
>           And don't overlook the suggestion I made:  Download the archived
> package.  Do NOT try to compile it. Instead source only the function you
> want, try to run it.  With luck, it will work.  If it doesn't, you will
> get a diagnostic that can help you take the next step.
>
>
>           Spencer
>
>
> On 7/3/22 1:28 AM, Andrew Simmons wrote:
> > It seems like this package was archived because package "ifultools"
> > was archived. I tried installing "ifultools" from source, but it has a
> > seriously large amount of C compilation issues. The main issue seems
> > to be that variable PROBLEM was never defined anywhere, and it is
> > unclear what its definition should be. Unfortunately, as far as I can
> > tell, this issue is unfixable. If you want to install "wmtsa", you'll
> > have to use an older version of R. Otherwise, you can use one of the
> > other wavelet analysis packages that Richard O'Keefe mentioned.
> >
> > On Sun, Jul 3, 2022 at 2:01 AM Muhammad Zubair Chishti
> > <mzchishti at eco.qau.edu.pk> wrote:
> >>
> >> Dear Respected Experts and specifically Professor Richard O'Keefe,
> >> Thank you so much for your precious time and generous help. However, the
> >> problem is still there and I am just unable to resolve it due to the lack
> >> of expertise in R. Still, the hope is there. I believe that this platform
> >> can help me.
> >>
> >> Regards
> >> Muhammad Zubair Chishti
> >> School of Business,
> >> Zhengzhou University, Henan, China
> >> My Google scholar link:
> >> https://scholar.google.com/citations?hl=en&user=YPqNJMwAAAAJ
> >> My ReseachGate Profile:
> >> https://www.researchgate.net/profile/Muhammad-Chishti
> >>
> >>
> >> On Sun, Jul 3, 2022 at 9:11 AM Richard O'Keefe <raoknz at gmail.com> wrote:
> >>
> >>> A postscript to my previous comment.
> >>> I used to supervise PhD students.
> >>> Let me advise you to write this issue up as a draft section for your
> >>> thesis.
> >>> 1. Why I wanted to use the wmtsa package.
> >>> 2. Why I didn't.
> >>> 3. How I went about selecting a replacement.
> >>> 4. What I chose and why that's the right choice.
> >>> 5. How the analyses I wanted to do are done in
> >>>     package X and what difference it makes.
> >>>
> >>> Off the top of my head, the only reasons for struggling to use an old
> >>> package are to try to replicate someone else's results and/or to try to use
> >>> their software (built atop the dead package) with new data.  Well, if you
> >>> get different results, that's interesting too, and then it's time to work
> >>> harder to resurrect the dead package.
> >>>
> >>> Speaking of which, an easier route might be to set up a separate
> >>> environment running an old version of R that *can* run the old code and the
> >>> old code's dependencies.  In fact trying to use the same versions that the
> >>> work you're trying to reproduce used might make a lot of sense.
> >>>
> >>> Overall, I think selecting an alternative package
> >>> that *is* currently maintained is the best use of your time, but your
> >>> supervisor should be able to help you with that.  Selecting appropriate
> >>> packages is part of doing research, after all, and demonstrating
> >>> that you can do it is all to the good, no?
> >>>
> >>>
> >>> On Sun, 3 Jul 2022 at 15:24, Richard O'Keefe <raoknz at gmail.com> wrote:
> >>>
> >>>> Can we start a step back please?
> >>>> wmtsa stands for
> >>>>    Wavelet Methods for Time Series Analysis.
> >>>>
> >>>> OK, so you have some time series data,
> >>>> and for some reason you want to analyse
> >>>> your data using wavelets.  No worries.
> >>>> But does it have to be THIS unmaintained
> >>>> package?
> >>>>
> >>>> Why not visit
> >>>>
> >>>> https://CRAN.R-project.org/view=TimeSeries
> >>>> and search for "wavelets" in the text?
> >>>> Oh heck, I might as well do it for you.
> >>>> <snip>
> >>>> *Wavelet methods* : The wavelets
> >>>> <https://cran.r-project.org/web/packages/wavelets/index.html> package
> >>>> includes computing wavelet filters, wavelet transforms and multiresolution
> >>>> analyses. Multiresolution forecasting using wavelets is also implemented in
> >>>> mrf <https://cran.r-project.org/web/packages/mrf/index.html>. WaveletComp
> >>>> <https://cran.r-project.org/web/packages/WaveletComp/index.html>
> >>>> provides some tools for wavelet-based analysis of univariate and bivariate
> >>>> time series including cross-wavelets, phase-difference and significance
> >>>> tests. biwavelet
> >>>> <https://cran.r-project.org/web/packages/biwavelet/index.html> is a port
> >>>> of the WTC Matlab package for univariate and bivariate wavelet analyses.
> >>>> mvLSW <https://cran.r-project.org/web/packages/mvLSW/index.html>
> >>>> provides tools for multivariate locally stationary wavelet processes.
> >>>> LSWPlib <https://cran.r-project.org/web/packages/LSWPlib/index.html>
> >>>> contains functions for simulation and spectral estimation of locally
> >>>> stationary wavelet packet processes. Tests of white noise using wavelets
> >>>> are provided by hwwntest
> >>>> <https://cran.r-project.org/web/packages/hwwntest/index.html>. Wavelet
> >>>> scalogram tools are contained in wavScalogram
> >>>> <https://cran.r-project.org/web/packages/wavScalogram/index.html>.
> >>>> Further wavelet methods can be found in the packages rwt
> >>>> <https://cran.r-project.org/web/packages/rwt/index.html>, waveslim
> >>>> <https://cran.r-project.org/web/packages/waveslim/index.html>, wavethresh
> >>>> <https://cran.r-project.org/web/packages/wavethresh/index.html>.
> >>>> </snip>
> >>>>
> >>>> Presumably there is a reason that nobody else has
> >>>> bothered to continue maintaining wmtsa.  Perhaps
> >>>> one of those other wavelets + time series packages
> >>>> can do what you need?
> >>>>
> >>>>
> >>>> On Sun, 3 Jul 2022 at 04:12, Muhammad Zubair Chishti <
> >>>> mzchishti at eco.qau.edu.pk> wrote:
> >>>>
> >>>>> Dear Experts,
> >>>>> I cannot find a package "wmtsa" for my R version "R 4.2.0". Kindly help
> >>>>> me
> >>>>> to find it or share the link with me.
> >>>>> Although I tried the old version of "wmtsa" but failed.
> >>>>> Thank you for your precious time.
> >>>>>
> >>>>> Regards
> >>>>> Muhammad Zubair Chishti
> >>>>>
> >>>>>          [[alternative HTML version deleted]]
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide
> >>>>> http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>
> >>>>
> >>
> >>          [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
John Kane
Kingston ON Canada


From r@oknz @end|ng |rom gm@||@com  Sun Jul  3 14:44:50 2022
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Mon, 4 Jul 2022 00:44:50 +1200
Subject: [R] A humble request
In-Reply-To: <CAMfKi3+e+gsv6f5ko33e7FCSWYt4a=iwgtoq=MeMtgP+Y8uA8Q@mail.gmail.com>
References: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
 <CABcYAdJ7E6aoC7ZYEx2i5Mx9VHRd+S8xMDCmsvbeCy++bTcosA@mail.gmail.com>
 <CABcYAd+f9yupeiztYWL4BDF=BOQf0Bn_ayRYUw0udmScGyxuig@mail.gmail.com>
 <CAMfKi3+e+gsv6f5ko33e7FCSWYt4a=iwgtoq=MeMtgP+Y8uA8Q@mail.gmail.com>
Message-ID: <CABcYAdLt-NdtwLGGA5woupaV+gLVk7XR-3sU_Zd8AzvxZDRDhA@mail.gmail.com>

This is why research students have supervisors.
A research degree is training in doing research.
When you get stuck, your supervisor (or co-supervisor) is supposed to help
you unstick yourself.
Asking random strangers can never be as good as asking someone who already
understands whatever it is you are actually trying to do.
Supervisors have knowledge and experience relevant to the research their
students are doing.  It's not always *much* more.  In fact, I once
"supervised" a student who had years of industrial experience in his topic,
when I knew just the basics of the subject.  He taught me a lot.
But one thing a supervisor ought to be able to do for you is to ask the
random strangers in a much clearer way.  He should be able to explain what
you are trying to do.
If, for example, he knows that you are trying to replicate someone else's
work as a prelude to extending it, he has the status to ask THEM what
package they recommend now.

You seem to be insisting that this one package is what you need and no
other, but you haven't told us WHY it has to be this package.
- We do not know what kind of data you have.
- We do not know why you think a wavelet analysis is appropriate.
  (I don't really understand wavelets and what they can do.  It is a big
gap in my education, but there are so many others it's near the bottom of
the list.)
- We do not yet have any reason to believe that a wavelet analysis IS a
particularly good way to deal with your data.
- We do not know what you want to learn from the data.
- We do not know whether your literature survey has turned up any other
papers doing something similar.
The metaphor I'm about to offer is a bit hyperbolic, but I hope it gets the
idea across.
Student: I want to get the HCN from the Poisons Cabinet here but the key I
have doesn't work.
Helpful random stranger: I tell you what, why not join a lock sports club
[they exist], buy the Genesis kit [real product] of lockpicking tools from
Covert Instruments [real company], and learn how to pick the lock?
REALLY helpful random stranger: What the hell do you want HCN for?

Other people have made it clear that you personally are NOT going to get
wmtsa working with reasonable effort, and nobody is paying any of us to
resurrect a dead package.  At SOME point you are going to have to describe
very clearly, as if to an intelligent 14-year-old,
- what kind of data you have
- what physical or social processes generated that data
- what kind of insight you think can be obtained from it
- what the raw data look like
- some crude "exploratory" statistics on the data
- what kinds of models other people with similar data have used
- what specific methods of analysis you intend to use/did use
- how you can tell whether the answers make sense
- what you are going to do with the answers
All of this needs to be part of the thesis.  You are going to have to write
it SOME time.  If not now, when?  If not you, who?

When you understand what you want to do well enough to explain it, you will
be able to ask much better questions, of the form
"I wanted to use wmtsa to do <X> to <Y>.
 What's a good package for that these days?
 Is there a textbook about doing <X> with R?"

As a former supervisor, I hope, oh how much I hope, that you were not
planning to just load wmtsa, turn the crank on a script you found down the
back of a couch, and believe whatever came out.  I've known Masters and
even PhD students do that.  Not mine, at least not for long.  I also hope
that you have a good supervisor and a good working relationship.  Speaking
from experience, one thing that really ANNOYS a good supervisor is a
student who stays stuck instead of asking for advice.  (I've been that
student.  I was much stupider then.)  It makes the supervisor feel
disrespected.



On Sun, 3 Jul 2022 at 18:00, Muhammad Zubair Chishti <
mzchishti at eco.qau.edu.pk> wrote:

> Dear Respected Experts and specifically Professor Richard O'Keefe,
> Thank you so much for your precious time and generous help. However, the
> problem is still there and I am just unable to resolve it due to the lack
> of expertise in R. Still, the hope is there. I believe that this platform
> can help me.
>
> Regards
> Muhammad Zubair Chishti
> School of Business,
> Zhengzhou University, Henan, China
> My Google scholar link:
> https://scholar.google.com/citations?hl=en&user=YPqNJMwAAAAJ
> My ReseachGate Profile:
> https://www.researchgate.net/profile/Muhammad-Chishti
>
>
> On Sun, Jul 3, 2022 at 9:11 AM Richard O'Keefe <raoknz at gmail.com> wrote:
>
>> A postscript to my previous comment.
>> I used to supervise PhD students.
>> Let me advise you to write this issue up as a draft section for your
>> thesis.
>> 1. Why I wanted to use the wmtsa package.
>> 2. Why I didn't.
>> 3. How I went about selecting a replacement.
>> 4. What I chose and why that's the right choice.
>> 5. How the analyses I wanted to do are done in
>>    package X and what difference it makes.
>>
>> Off the top of my head, the only reasons for struggling to use an old
>> package are to try to replicate someone else's results and/or to try to use
>> their software (built atop the dead package) with new data.  Well, if you
>> get different results, that's interesting too, and then it's time to work
>> harder to resurrect the dead package.
>>
>> Speaking of which, an easier route might be to set up a separate
>> environment running an old version of R that *can* run the old code and the
>> old code's dependencies.  In fact trying to use the same versions that the
>> work you're trying to reproduce used might make a lot of sense.
>>
>> Overall, I think selecting an alternative package
>> that *is* currently maintained is the best use of your time, but your
>> supervisor should be able to help you with that.  Selecting appropriate
>> packages is part of doing research, after all, and demonstrating
>> that you can do it is all to the good, no?
>>
>>
>> On Sun, 3 Jul 2022 at 15:24, Richard O'Keefe <raoknz at gmail.com> wrote:
>>
>>> Can we start a step back please?
>>> wmtsa stands for
>>>   Wavelet Methods for Time Series Analysis.
>>>
>>> OK, so you have some time series data,
>>> and for some reason you want to analyse
>>> your data using wavelets.  No worries.
>>> But does it have to be THIS unmaintained
>>> package?
>>>
>>> Why not visit
>>>
>>> https://CRAN.R-project.org/view=TimeSeries
>>> and search for "wavelets" in the text?
>>> Oh heck, I might as well do it for you.
>>> <snip>
>>> *Wavelet methods* : The wavelets
>>> <https://cran.r-project.org/web/packages/wavelets/index.html> package
>>> includes computing wavelet filters, wavelet transforms and multiresolution
>>> analyses. Multiresolution forecasting using wavelets is also implemented in
>>> mrf <https://cran.r-project.org/web/packages/mrf/index.html>.
>>> WaveletComp
>>> <https://cran.r-project.org/web/packages/WaveletComp/index.html>
>>> provides some tools for wavelet-based analysis of univariate and bivariate
>>> time series including cross-wavelets, phase-difference and significance
>>> tests. biwavelet
>>> <https://cran.r-project.org/web/packages/biwavelet/index.html> is a
>>> port of the WTC Matlab package for univariate and bivariate wavelet
>>> analyses. mvLSW
>>> <https://cran.r-project.org/web/packages/mvLSW/index.html> provides
>>> tools for multivariate locally stationary wavelet processes. LSWPlib
>>> <https://cran.r-project.org/web/packages/LSWPlib/index.html> contains
>>> functions for simulation and spectral estimation of locally stationary
>>> wavelet packet processes. Tests of white noise using wavelets are provided
>>> by hwwntest
>>> <https://cran.r-project.org/web/packages/hwwntest/index.html>. Wavelet
>>> scalogram tools are contained in wavScalogram
>>> <https://cran.r-project.org/web/packages/wavScalogram/index.html>.
>>> Further wavelet methods can be found in the packages rwt
>>> <https://cran.r-project.org/web/packages/rwt/index.html>, waveslim
>>> <https://cran.r-project.org/web/packages/waveslim/index.html>,
>>> wavethresh
>>> <https://cran.r-project.org/web/packages/wavethresh/index.html>.
>>> </snip>
>>>
>>> Presumably there is a reason that nobody else has
>>> bothered to continue maintaining wmtsa.  Perhaps
>>> one of those other wavelets + time series packages
>>> can do what you need?
>>>
>>>
>>> On Sun, 3 Jul 2022 at 04:12, Muhammad Zubair Chishti <
>>> mzchishti at eco.qau.edu.pk> wrote:
>>>
>>>> Dear Experts,
>>>> I cannot find a package "wmtsa" for my R version "R 4.2.0". Kindly help
>>>> me
>>>> to find it or share the link with me.
>>>> Although I tried the old version of "wmtsa" but failed.
>>>> Thank you for your precious time.
>>>>
>>>> Regards
>>>> Muhammad Zubair Chishti
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>

	[[alternative HTML version deleted]]


From r@oknz @end|ng |rom gm@||@com  Sun Jul  3 15:15:18 2022
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Mon, 4 Jul 2022 01:15:18 +1200
Subject: [R] A humble request
In-Reply-To: <CAKZQJMBtz0h+HKz9cGpz7rRt1-MLX4vi1Rxu3TDpteZc6-VuAQ@mail.gmail.com>
References: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
 <CABcYAdJ7E6aoC7ZYEx2i5Mx9VHRd+S8xMDCmsvbeCy++bTcosA@mail.gmail.com>
 <CABcYAd+f9yupeiztYWL4BDF=BOQf0Bn_ayRYUw0udmScGyxuig@mail.gmail.com>
 <CAMfKi3+e+gsv6f5ko33e7FCSWYt4a=iwgtoq=MeMtgP+Y8uA8Q@mail.gmail.com>
 <CAPcHnpR8AgPwx=CNCJrJ9CmqzqG7oF2i_5PWWj9JgYtq8t1_AQ@mail.gmail.com>
 <a82dbde5-0358-ca87-339f-fdd8f403b251@effectivedefense.org>
 <CAKZQJMBtz0h+HKz9cGpz7rRt1-MLX4vi1Rxu3TDpteZc6-VuAQ@mail.gmail.com>
Message-ID: <CABcYAdJUebN26gH_14By+GQ0MrGXrQXEyjZTje2ZY5P=Rr7=2w@mail.gmail.com>

I'm not quite sure what "source the fonction" means.
The wmtsa package is available from the archive as a
compressed archive wmtsa_2.0-3.tar.gz.
% tar xf wmtsa_2.0-3.tar.gz
creates a directory wmtsa/ with subdirectories
wmtsa/man and wmtsa/R plus some other files.
The documentation is in wmtsa/man/*.md
The source code is in wmtsa/R/*.R
You look at either using whatever text editor you feel like.
Looking at wav_xform.R I see hundreds of lines of code
that only a mother could love, with a good table of contents
but no really informative comments anywhere.
I am reminded, as too often, that there are people who develop packages for
R, and there are software engineers, but there are precious few software
engineers developing packages for R.
As it happens, I *am* a software engineer (amongst other things),
and I do have a tolerably good knowledge of base R, and a high tolerance
for looking things up in the documentation.  But I would have to be paid
quite a large sum of money before I would spend any time on this
code-base.  There are other currently maintained packages that might be
able to do the job, so it would be a waste of my time.

My advice is
  DON'T spend any time looking at this code.
  An R beginning WON'T understand it.
  DO ask your supervisor help you to select an alternative,
  and if you can't figure that out between you,
  ASK a more informative question.



On Mon, 4 Jul 2022 at 00:38, John Kane <jrkrideau at gmail.com> wrote:

> Spenser,
> the idea to source the fonction makes sense but since tho OP is a very
> new beginner perhaps you could point him towards code showing him how
> to do this?  I have never done this AFAIR, and while I suspect a few
> minutes googling would show me how, it is likely to be more difficult
> for a noobie.
>
> On Sun, 3 Jul 2022 at 07:25, Spencer Graves
> <spencer.graves at effectivedefense.org> wrote:
> >
> > Muhammad Zubair Chishti:
> >
> >
> >           What specifically have you tried?  What were the results?
> >
> >
> >           Please "provide commented, minimal, self-contained,
> reproducible
> > code", per "the posting guide
> > http://www.R-project.org/posting-guide.html", as indicated in at the end
> > of each email in this thread.
> >
> >
> >           And don't overlook the suggestion I made:  Download the
> archived
> > package.  Do NOT try to compile it. Instead source only the function you
> > want, try to run it.  With luck, it will work.  If it doesn't, you will
> > get a diagnostic that can help you take the next step.
> >
> >
> >           Spencer
> >
> >
> > On 7/3/22 1:28 AM, Andrew Simmons wrote:
> > > It seems like this package was archived because package "ifultools"
> > > was archived. I tried installing "ifultools" from source, but it has a
> > > seriously large amount of C compilation issues. The main issue seems
> > > to be that variable PROBLEM was never defined anywhere, and it is
> > > unclear what its definition should be. Unfortunately, as far as I can
> > > tell, this issue is unfixable. If you want to install "wmtsa", you'll
> > > have to use an older version of R. Otherwise, you can use one of the
> > > other wavelet analysis packages that Richard O'Keefe mentioned.
> > >
> > > On Sun, Jul 3, 2022 at 2:01 AM Muhammad Zubair Chishti
> > > <mzchishti at eco.qau.edu.pk> wrote:
> > >>
> > >> Dear Respected Experts and specifically Professor Richard O'Keefe,
> > >> Thank you so much for your precious time and generous help. However,
> the
> > >> problem is still there and I am just unable to resolve it due to the
> lack
> > >> of expertise in R. Still, the hope is there. I believe that this
> platform
> > >> can help me.
> > >>
> > >> Regards
> > >> Muhammad Zubair Chishti
> > >> School of Business,
> > >> Zhengzhou University, Henan, China
> > >> My Google scholar link:
> > >> https://scholar.google.com/citations?hl=en&user=YPqNJMwAAAAJ
> > >> My ReseachGate Profile:
> > >> https://www.researchgate.net/profile/Muhammad-Chishti
> > >>
> > >>
> > >> On Sun, Jul 3, 2022 at 9:11 AM Richard O'Keefe <raoknz at gmail.com>
> wrote:
> > >>
> > >>> A postscript to my previous comment.
> > >>> I used to supervise PhD students.
> > >>> Let me advise you to write this issue up as a draft section for your
> > >>> thesis.
> > >>> 1. Why I wanted to use the wmtsa package.
> > >>> 2. Why I didn't.
> > >>> 3. How I went about selecting a replacement.
> > >>> 4. What I chose and why that's the right choice.
> > >>> 5. How the analyses I wanted to do are done in
> > >>>     package X and what difference it makes.
> > >>>
> > >>> Off the top of my head, the only reasons for struggling to use an old
> > >>> package are to try to replicate someone else's results and/or to try
> to use
> > >>> their software (built atop the dead package) with new data.  Well,
> if you
> > >>> get different results, that's interesting too, and then it's time to
> work
> > >>> harder to resurrect the dead package.
> > >>>
> > >>> Speaking of which, an easier route might be to set up a separate
> > >>> environment running an old version of R that *can* run the old code
> and the
> > >>> old code's dependencies.  In fact trying to use the same versions
> that the
> > >>> work you're trying to reproduce used might make a lot of sense.
> > >>>
> > >>> Overall, I think selecting an alternative package
> > >>> that *is* currently maintained is the best use of your time, but your
> > >>> supervisor should be able to help you with that.  Selecting
> appropriate
> > >>> packages is part of doing research, after all, and demonstrating
> > >>> that you can do it is all to the good, no?
> > >>>
> > >>>
> > >>> On Sun, 3 Jul 2022 at 15:24, Richard O'Keefe <raoknz at gmail.com>
> wrote:
> > >>>
> > >>>> Can we start a step back please?
> > >>>> wmtsa stands for
> > >>>>    Wavelet Methods for Time Series Analysis.
> > >>>>
> > >>>> OK, so you have some time series data,
> > >>>> and for some reason you want to analyse
> > >>>> your data using wavelets.  No worries.
> > >>>> But does it have to be THIS unmaintained
> > >>>> package?
> > >>>>
> > >>>> Why not visit
> > >>>>
> > >>>> https://CRAN.R-project.org/view=TimeSeries
> > >>>> and search for "wavelets" in the text?
> > >>>> Oh heck, I might as well do it for you.
> > >>>> <snip>
> > >>>> *Wavelet methods* : The wavelets
> > >>>> <https://cran.r-project.org/web/packages/wavelets/index.html>
> package
> > >>>> includes computing wavelet filters, wavelet transforms and
> multiresolution
> > >>>> analyses. Multiresolution forecasting using wavelets is also
> implemented in
> > >>>> mrf <https://cran.r-project.org/web/packages/mrf/index.html>.
> WaveletComp
> > >>>> <https://cran.r-project.org/web/packages/WaveletComp/index.html>
> > >>>> provides some tools for wavelet-based analysis of univariate and
> bivariate
> > >>>> time series including cross-wavelets, phase-difference and
> significance
> > >>>> tests. biwavelet
> > >>>> <https://cran.r-project.org/web/packages/biwavelet/index.html> is
> a port
> > >>>> of the WTC Matlab package for univariate and bivariate wavelet
> analyses.
> > >>>> mvLSW <https://cran.r-project.org/web/packages/mvLSW/index.html>
> > >>>> provides tools for multivariate locally stationary wavelet
> processes.
> > >>>> LSWPlib <https://cran.r-project.org/web/packages/LSWPlib/index.html
> >
> > >>>> contains functions for simulation and spectral estimation of locally
> > >>>> stationary wavelet packet processes. Tests of white noise using
> wavelets
> > >>>> are provided by hwwntest
> > >>>> <https://cran.r-project.org/web/packages/hwwntest/index.html>.
> Wavelet
> > >>>> scalogram tools are contained in wavScalogram
> > >>>> <https://cran.r-project.org/web/packages/wavScalogram/index.html>.
> > >>>> Further wavelet methods can be found in the packages rwt
> > >>>> <https://cran.r-project.org/web/packages/rwt/index.html>, waveslim
> > >>>> <https://cran.r-project.org/web/packages/waveslim/index.html>,
> wavethresh
> > >>>> <https://cran.r-project.org/web/packages/wavethresh/index.html>.
> > >>>> </snip>
> > >>>>
> > >>>> Presumably there is a reason that nobody else has
> > >>>> bothered to continue maintaining wmtsa.  Perhaps
> > >>>> one of those other wavelets + time series packages
> > >>>> can do what you need?
> > >>>>
> > >>>>
> > >>>> On Sun, 3 Jul 2022 at 04:12, Muhammad Zubair Chishti <
> > >>>> mzchishti at eco.qau.edu.pk> wrote:
> > >>>>
> > >>>>> Dear Experts,
> > >>>>> I cannot find a package "wmtsa" for my R version "R 4.2.0". Kindly
> help
> > >>>>> me
> > >>>>> to find it or share the link with me.
> > >>>>> Although I tried the old version of "wmtsa" but failed.
> > >>>>> Thank you for your precious time.
> > >>>>>
> > >>>>> Regards
> > >>>>> Muhammad Zubair Chishti
> > >>>>>
> > >>>>>          [[alternative HTML version deleted]]
> > >>>>>
> > >>>>> ______________________________________________
> > >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>>>> PLEASE do read the posting guide
> > >>>>> http://www.R-project.org/posting-guide.html
> > >>>>> and provide commented, minimal, self-contained, reproducible code.
> > >>>>>
> > >>>>
> > >>
> > >>          [[alternative HTML version deleted]]
> > >>
> > >> ______________________________________________
> > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > >> and provide commented, minimal, self-contained, reproducible code.
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> John Kane
> Kingston ON Canada
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Sun Jul  3 17:48:03 2022
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Sun, 3 Jul 2022 10:48:03 -0500
Subject: [R] A humble request
In-Reply-To: <CABcYAdJUebN26gH_14By+GQ0MrGXrQXEyjZTje2ZY5P=Rr7=2w@mail.gmail.com>
References: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
 <CABcYAdJ7E6aoC7ZYEx2i5Mx9VHRd+S8xMDCmsvbeCy++bTcosA@mail.gmail.com>
 <CABcYAd+f9yupeiztYWL4BDF=BOQf0Bn_ayRYUw0udmScGyxuig@mail.gmail.com>
 <CAMfKi3+e+gsv6f5ko33e7FCSWYt4a=iwgtoq=MeMtgP+Y8uA8Q@mail.gmail.com>
 <CAPcHnpR8AgPwx=CNCJrJ9CmqzqG7oF2i_5PWWj9JgYtq8t1_AQ@mail.gmail.com>
 <a82dbde5-0358-ca87-339f-fdd8f403b251@effectivedefense.org>
 <CAKZQJMBtz0h+HKz9cGpz7rRt1-MLX4vi1Rxu3TDpteZc6-VuAQ@mail.gmail.com>
 <CABcYAdJUebN26gH_14By+GQ0MrGXrQXEyjZTje2ZY5P=Rr7=2w@mail.gmail.com>
Message-ID: <3f807c4c-10cd-277f-47ef-e0c42547bdcc@effectivedefense.org>

Hi, Richard:  Thanks for the question.


	  "Source the function" means the following.


	  1.  Go to "https://cran.r-project.org/" and download the most recent 
version.  To do that, I found "archived" under "Related Directories".  I 
clicked on that and then searched for "wmtsa".  To the right of that, it 
says it was "Last Modified" "2020-06-09 07:24".  Note that "Last 
Modified", because you can use that to find recent documentation, etc., 
from "archive.org" shortly prior to that date.


	  2.  Click on "wmtsa".  That takes you to "Index of 
/src/contrib/Archive/wmtsa", which includes 11 different versions of the 
source code for that package from "wmtsa_1.0-1.tar.gz 2007-09-23 21:54 
139K" to "wmtsa_2.0-3.tar.gz 2017-12-06 12:14 151K" Click on the last 
one to download it.  This downloaded a file by that name to the 
"Downloads" folder in my computer.


	  3.  Create a directory for that file.  I have a directory called "R" 
on my computer.  I just created a new directory called "wmtsa" within 
that "R" directory.  Then I moved that file into that directory and 
unzip it by double clicking on it.  As a result, I now have a directory 
"~R/wmtsa/wmtsa".


	  4.  To compile that package, I routinely in a Terminal in R (called 
something else in Windows), I "cd" until I get to "~R/wmtsa".  Then I 
run "R CMD check "wmtsa_2.0-3.tar.gz".  I just did that.  It said, 
"ERROR  Packages required but not available: 'splus2R', 'ifultools'". 
In R, I was able to download the first using 
"install.packages('splus2R')".  When I tried 
"install.packages('ifultools')", I got a message, "package ?ifultools? 
is not available for this version of R".


	  5.  However, the inner "wmtsa" directory contains, among other 
things, directories called "man" and "R".  The latter directory contains 
files with names like "wav_boot.R", ..., "wav_xform.R".  I'm assuming 
you have documentation that tells you which function(s) you want to run. 
  Let's say you want to run "wavBestBasis".  If you are lucky, there 
will be a file with a name like "wavBestBasis.R".  That doesn't work in 
this case.  So instead I did a cd into that ~R/wmtsa/wmtsa/R".  Then I 
did "grep 'wavBestBasis' *.R".  I got the following:


wav_xform.R:##    wavBestBasis
wav_xform.R:# wavBestBasis
wav_xform.R:"wavBestBasis" <- function(costs)
Let's say it says you want to start with a f


	  6.  That says that "wavBestBasis" is in file "wav_xform.R".  I opened 
that file in RStudio (I use the free version) then clicked on the 
"Source" button in the upper right.


	  7.  Also, in the directory "~R/wmtsa/wmtsa/man" I found a file called 
"wavBestBasis.Rd".  I opened that in R.  To learn how to read that, you 
can type "help.start()", which will open a menu that includes, "Writing 
R Extensions".  Click that.  That tells you how to write (and read) a 
*.Rd file.  That file "wavBestBasis.Rd" contains and "\examples" 
section.  The first line in that section is "W <- 
wavDWPT(diff(atomclock), n.level=6)".  I ran that, and got:


Error in diff(atomclock) : object 'atomclock' not found


	  I won't discuss how to find "atomclick".  If you need that, you can 
ask again.


	  Hope this helps.
	  Spencer


On 7/3/22 8:15 AM, Richard O'Keefe wrote:
> I'm not quite sure what "source the fonction" means.
> The wmtsa package is available from the archive as a
> compressed archive wmtsa_2.0-3.tar.gz.
> % tar xf wmtsa_2.0-3.tar.gz
> creates a directory wmtsa/ with subdirectories
> wmtsa/man and wmtsa/R plus some other files.
> The documentation is in wmtsa/man/*.md
> The source code is in wmtsa/R/*.R
> You look at either using whatever text editor you feel like.
> Looking at wav_xform.R I see hundreds of lines of code
> that only a mother could love, with a good table of contents
> but no really informative comments anywhere.
> I am reminded, as too often, that there are people who develop packages 
> for R, and there are software engineers, but there are precious few 
> software engineers developing packages for R.
> As it happens, I *am* a software engineer (amongst other things),
> and I do have a tolerably good knowledge of base R, and a high tolerance 
> for looking things up in the documentation.? But I would have to be paid 
> quite a large sum of money before I would spend any time on this 
> code-base.? There are other currently maintained packages that might be 
> able to do the job, so it would be a waste of my time.
> 
> My advice is
>  ? DON'T spend any time looking at this code.
>  ? An R beginning WON'T understand it.
>  ? DO ask your supervisor help you to select an alternative,
>  ? and if you can't figure that out between you,
>  ? ASK a more informative question.
> 
> 
> 
> On Mon, 4 Jul 2022 at 00:38, John Kane <jrkrideau at gmail.com 
> <mailto:jrkrideau at gmail.com>> wrote:
> 
>     Spenser,
>     the idea to source the fonction makes sense but since tho OP is a very
>     new beginner perhaps you could point him towards code showing him how
>     to do this?? I have never done this AFAIR, and while I suspect a few
>     minutes googling would show me how, it is likely to be more difficult
>     for a noobie.
> 
>     On Sun, 3 Jul 2022 at 07:25, Spencer Graves
>     <spencer.graves at effectivedefense.org
>     <mailto:spencer.graves at effectivedefense.org>> wrote:
>      >
>      > Muhammad Zubair Chishti:
>      >
>      >
>      >? ? ? ? ? ?What specifically have you tried?? What were the results?
>      >
>      >
>      >? ? ? ? ? ?Please "provide commented, minimal, self-contained,
>     reproducible
>      > code", per "the posting guide
>      > http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>", as indicated in at
>     the end
>      > of each email in this thread.
>      >
>      >
>      >? ? ? ? ? ?And don't overlook the suggestion I made:? Download the
>     archived
>      > package.? Do NOT try to compile it. Instead source only the
>     function you
>      > want, try to run it.? With luck, it will work.? If it doesn't,
>     you will
>      > get a diagnostic that can help you take the next step.
>      >
>      >
>      >? ? ? ? ? ?Spencer
>      >
>      >
>      > On 7/3/22 1:28 AM, Andrew Simmons wrote:
>      > > It seems like this package was archived because package "ifultools"
>      > > was archived. I tried installing "ifultools" from source, but
>     it has a
>      > > seriously large amount of C compilation issues. The main issue
>     seems
>      > > to be that variable PROBLEM was never defined anywhere, and it is
>      > > unclear what its definition should be. Unfortunately, as far as
>     I can
>      > > tell, this issue is unfixable. If you want to install "wmtsa",
>     you'll
>      > > have to use an older version of R. Otherwise, you can use one
>     of the
>      > > other wavelet analysis packages that Richard O'Keefe mentioned.
>      > >
>      > > On Sun, Jul 3, 2022 at 2:01 AM Muhammad Zubair Chishti
>      > > <mzchishti at eco.qau.edu.pk <mailto:mzchishti at eco.qau.edu.pk>> wrote:
>      > >>
>      > >> Dear Respected Experts and specifically Professor Richard O'Keefe,
>      > >> Thank you so much for your precious time and generous help.
>     However, the
>      > >> problem is still there and I am just unable to resolve it due
>     to the lack
>      > >> of expertise in R. Still, the hope is there. I believe that
>     this platform
>      > >> can help me.
>      > >>
>      > >> Regards
>      > >> Muhammad Zubair Chishti
>      > >> School of Business,
>      > >> Zhengzhou University, Henan, China
>      > >> My Google scholar link:
>      > >> https://scholar.google.com/citations?hl=en&user=YPqNJMwAAAAJ
>     <https://scholar.google.com/citations?hl=en&user=YPqNJMwAAAAJ>
>      > >> My ReseachGate Profile:
>      > >> https://www.researchgate.net/profile/Muhammad-Chishti
>     <https://www.researchgate.net/profile/Muhammad-Chishti>
>      > >>
>      > >>
>      > >> On Sun, Jul 3, 2022 at 9:11 AM Richard O'Keefe
>     <raoknz at gmail.com <mailto:raoknz at gmail.com>> wrote:
>      > >>
>      > >>> A postscript to my previous comment.
>      > >>> I used to supervise PhD students.
>      > >>> Let me advise you to write this issue up as a draft section
>     for your
>      > >>> thesis.
>      > >>> 1. Why I wanted to use the wmtsa package.
>      > >>> 2. Why I didn't.
>      > >>> 3. How I went about selecting a replacement.
>      > >>> 4. What I chose and why that's the right choice.
>      > >>> 5. How the analyses I wanted to do are done in
>      > >>>? ? ?package X and what difference it makes.
>      > >>>
>      > >>> Off the top of my head, the only reasons for struggling to
>     use an old
>      > >>> package are to try to replicate someone else's results and/or
>     to try to use
>      > >>> their software (built atop the dead package) with new data. 
>     Well, if you
>      > >>> get different results, that's interesting too, and then it's
>     time to work
>      > >>> harder to resurrect the dead package.
>      > >>>
>      > >>> Speaking of which, an easier route might be to set up a separate
>      > >>> environment running an old version of R that *can* run the
>     old code and the
>      > >>> old code's dependencies.? In fact trying to use the same
>     versions that the
>      > >>> work you're trying to reproduce used might make a lot of sense.
>      > >>>
>      > >>> Overall, I think selecting an alternative package
>      > >>> that *is* currently maintained is the best use of your time,
>     but your
>      > >>> supervisor should be able to help you with that.? Selecting
>     appropriate
>      > >>> packages is part of doing research, after all, and demonstrating
>      > >>> that you can do it is all to the good, no?
>      > >>>
>      > >>>
>      > >>> On Sun, 3 Jul 2022 at 15:24, Richard O'Keefe
>     <raoknz at gmail.com <mailto:raoknz at gmail.com>> wrote:
>      > >>>
>      > >>>> Can we start a step back please?
>      > >>>> wmtsa stands for
>      > >>>>? ? Wavelet Methods for Time Series Analysis.
>      > >>>>
>      > >>>> OK, so you have some time series data,
>      > >>>> and for some reason you want to analyse
>      > >>>> your data using wavelets.? No worries.
>      > >>>> But does it have to be THIS unmaintained
>      > >>>> package?
>      > >>>>
>      > >>>> Why not visit
>      > >>>>
>      > >>>> https://CRAN.R-project.org/view=TimeSeries
>     <https://CRAN.R-project.org/view=TimeSeries>
>      > >>>> and search for "wavelets" in the text?
>      > >>>> Oh heck, I might as well do it for you.
>      > >>>> <snip>
>      > >>>> *Wavelet methods* : The wavelets
>      > >>>> <https://cran.r-project.org/web/packages/wavelets/index.html
>     <https://cran.r-project.org/web/packages/wavelets/index.html>> package
>      > >>>> includes computing wavelet filters, wavelet transforms and
>     multiresolution
>      > >>>> analyses. Multiresolution forecasting using wavelets is also
>     implemented in
>      > >>>> mrf <https://cran.r-project.org/web/packages/mrf/index.html
>     <https://cran.r-project.org/web/packages/mrf/index.html>>. WaveletComp
>      > >>>>
>     <https://cran.r-project.org/web/packages/WaveletComp/index.html
>     <https://cran.r-project.org/web/packages/WaveletComp/index.html>>
>      > >>>> provides some tools for wavelet-based analysis of univariate
>     and bivariate
>      > >>>> time series including cross-wavelets, phase-difference and
>     significance
>      > >>>> tests. biwavelet
>      > >>>>
>     <https://cran.r-project.org/web/packages/biwavelet/index.html
>     <https://cran.r-project.org/web/packages/biwavelet/index.html>> is a
>     port
>      > >>>> of the WTC Matlab package for univariate and bivariate
>     wavelet analyses.
>      > >>>> mvLSW
>     <https://cran.r-project.org/web/packages/mvLSW/index.html
>     <https://cran.r-project.org/web/packages/mvLSW/index.html>>
>      > >>>> provides tools for multivariate locally stationary wavelet
>     processes.
>      > >>>> LSWPlib
>     <https://cran.r-project.org/web/packages/LSWPlib/index.html
>     <https://cran.r-project.org/web/packages/LSWPlib/index.html>>
>      > >>>> contains functions for simulation and spectral estimation of
>     locally
>      > >>>> stationary wavelet packet processes. Tests of white noise
>     using wavelets
>      > >>>> are provided by hwwntest
>      > >>>> <https://cran.r-project.org/web/packages/hwwntest/index.html
>     <https://cran.r-project.org/web/packages/hwwntest/index.html>>. Wavelet
>      > >>>> scalogram tools are contained in wavScalogram
>      > >>>>
>     <https://cran.r-project.org/web/packages/wavScalogram/index.html
>     <https://cran.r-project.org/web/packages/wavScalogram/index.html>>.
>      > >>>> Further wavelet methods can be found in the packages rwt
>      > >>>> <https://cran.r-project.org/web/packages/rwt/index.html
>     <https://cran.r-project.org/web/packages/rwt/index.html>>, waveslim
>      > >>>> <https://cran.r-project.org/web/packages/waveslim/index.html
>     <https://cran.r-project.org/web/packages/waveslim/index.html>>,
>     wavethresh
>      > >>>>
>     <https://cran.r-project.org/web/packages/wavethresh/index.html
>     <https://cran.r-project.org/web/packages/wavethresh/index.html>>.
>      > >>>> </snip>
>      > >>>>
>      > >>>> Presumably there is a reason that nobody else has
>      > >>>> bothered to continue maintaining wmtsa.? Perhaps
>      > >>>> one of those other wavelets + time series packages
>      > >>>> can do what you need?
>      > >>>>
>      > >>>>
>      > >>>> On Sun, 3 Jul 2022 at 04:12, Muhammad Zubair Chishti <
>      > >>>> mzchishti at eco.qau.edu.pk <mailto:mzchishti at eco.qau.edu.pk>>
>     wrote:
>      > >>>>
>      > >>>>> Dear Experts,
>      > >>>>> I cannot find a package "wmtsa" for my R version "R 4.2.0".
>     Kindly help
>      > >>>>> me
>      > >>>>> to find it or share the link with me.
>      > >>>>> Although I tried the old version of "wmtsa" but failed.
>      > >>>>> Thank you for your precious time.
>      > >>>>>
>      > >>>>> Regards
>      > >>>>> Muhammad Zubair Chishti
>      > >>>>>
>      > >>>>>? ? ? ? ? [[alternative HTML version deleted]]
>      > >>>>>
>      > >>>>> ______________________________________________
>      > >>>>> R-help at r-project.org <mailto:R-help at r-project.org> mailing
>     list -- To UNSUBSCRIBE and more, see
>      > >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>      > >>>>> PLEASE do read the posting guide
>      > >>>>> http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>      > >>>>> and provide commented, minimal, self-contained,
>     reproducible code.
>      > >>>>>
>      > >>>>
>      > >>
>      > >>? ? ? ? ? [[alternative HTML version deleted]]
>      > >>
>      > >> ______________________________________________
>      > >> R-help at r-project.org <mailto:R-help at r-project.org> mailing
>     list -- To UNSUBSCRIBE and more, see
>      > >> https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>      > >> PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>      > >> and provide commented, minimal, self-contained, reproducible code.
>      > >
>      > > ______________________________________________
>      > > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>     -- To UNSUBSCRIBE and more, see
>      > > https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>      > > PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>      > > and provide commented, minimal, self-contained, reproducible code.
>      >
>      > ______________________________________________
>      > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>     -- To UNSUBSCRIBE and more, see
>      > https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>      > PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>      > and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
>     -- 
>     John Kane
>     Kingston ON Canada
> 
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>     and provide commented, minimal, self-contained, reproducible code.
>


From |u||@dmtru @end|ng |rom gm@||@com  Tue Jul  5 11:00:03 2022
From: |u||@dmtru @end|ng |rom gm@||@com (Iulia Dumitru)
Date: Tue, 5 Jul 2022 12:00:03 +0300
Subject: [R] Where to find the source code for survey::SE
Message-ID: <F27D7C26-CABD-497C-890F-9AF07B8A1ED0@gmail.com>

Hello! I want to port some functionality from the Survey package to Julia and I am stuck on standard error calculation. How is the standard error calculated in the Survey package?

I searched in the can/survey repo on GitHub and I couldn?t find the source code of the SE function. I also looked through Thomas Lumley?s book Complex Surveys A Guide to Analysis Using R and from there I understood that the standard error is calculated by first calculating the Horvitz-Thompson variance estimator and then taking the square root of that, but I get completely different results.

This is the R code I used:

> library(survey)
> data(api)
> dclus1 <- svydesign(id = ~dnum, weights = ~pw, data = apiclus1, fpc = ~fpc)
> svyby(~api00, by = ~cname, design = dclus1, svymean)

And this is what I obtain:

                  cname    api00           se
Alameda         Alameda 669.0000 1.264044e-15
Fresno           Fresno 472.0000 0.000000e+00
Kern               Kern 452.5000 0.000000e+00
Los Angeles Los Angeles 647.2667 1.724959e+01
Mendocino     Mendocino 623.2500 0.000000e+00
Merced           Merced 519.2500 0.000000e+00
Orange           Orange 710.5625 0.000000e+00
Plumas           Plumas 709.5556 1.248655e-13
San Diego     San Diego 659.4364 2.296800e+00
San Joaquin San Joaquin 551.1892 1.354175e-13
Santa Clara Santa Clara 732.0769 1.577292e+01


With Julia this is the result I get:

11?3 DataFrame
 Row ? cname        mean     se       
     ? String15     Float64  Float64  
??????????????????????????????????????
   1 ? Alameda      669.0     6469.11
   2 ? Fresno       472.0     7832.61
   3 ? Kern         452.5    10703.1
   4 ? Los Angeles  647.267   5232.24
   5 ? Mendocino    623.25   10364.8
   6 ? Merced       519.25    8616.22
   7 ? Orange       710.563   5534.39
   8 ? Plumas       709.556   7673.15
   9 ? San Diego    659.436   1882.06
  10 ? San Joaquin  551.189   2254.24
  11 ? Santa Clara  732.077   4000.03

And just to have the full picture, this is my code for the Horvitz-Thompson variance estimator:

function hte(y, pw, n)
	p = 1 .- (1 .- 1 ./ pw) .^ n

	first_sum = sum((1 .- p) ./ (p .^ 2) .* y .^ 2)
	second_sum = 0

	for i in 1:length(p)
		for j in 1:length(p)
			if i == j
				continue
			end


			p_ij = p[i] + p[j] - (1 - (1 - 1 / pw[i] - 1 / pw[j]) ^ n)
			second_sum += (p_ij - p[i] * p[j]) / (p[i] * p[j]) * y[i] * y[j]
		end
	end

	return first_sum + second_sum
end

The standard error in the resulted data frame above is just the square root of the hte result.
	[[alternative HTML version deleted]]


From @jd@m|co @end|ng |rom gm@||@com  Tue Jul  5 14:36:22 2022
From: @jd@m|co @end|ng |rom gm@||@com (Anthony Damico)
Date: Tue, 5 Jul 2022 08:36:22 -0400
Subject: [R] Where to find the source code for survey::SE
In-Reply-To: <F27D7C26-CABD-497C-890F-9AF07B8A1ED0@gmail.com>
References: <F27D7C26-CABD-497C-890F-9AF07B8A1ED0@gmail.com>
Message-ID: <CAOwvMDxkXYiJrfMFtrC5je3zTVu9eyJ2vXGAh++gQOt0AWb7Wg@mail.gmail.com>

hi, are you looking for the code in survey:::svymean.survey.design2   ?
running `debug(svymean)` and `debug(svyrecvar)` might help you step through
the calculations..



On Tue, Jul 5, 2022 at 8:29 AM Iulia Dumitru <iuliadmtru at gmail.com> wrote:

> Hello! I want to port some functionality from the Survey package to Julia
> and I am stuck on standard error calculation. How is the standard error
> calculated in the Survey package?
>
> I searched in the can/survey repo on GitHub and I couldn?t find the source
> code of the SE function. I also looked through Thomas Lumley?s book Complex
> Surveys A Guide to Analysis Using R and from there I understood that the
> standard error is calculated by first calculating the Horvitz-Thompson
> variance estimator and then taking the square root of that, but I get
> completely different results.
>
> This is the R code I used:
>
> > library(survey)
> > data(api)
> > dclus1 <- svydesign(id = ~dnum, weights = ~pw, data = apiclus1, fpc =
> ~fpc)
> > svyby(~api00, by = ~cname, design = dclus1, svymean)
>
> And this is what I obtain:
>
>                   cname    api00           se
> Alameda         Alameda 669.0000 1.264044e-15
> Fresno           Fresno 472.0000 0.000000e+00
> Kern               Kern 452.5000 0.000000e+00
> Los Angeles Los Angeles 647.2667 1.724959e+01
> Mendocino     Mendocino 623.2500 0.000000e+00
> Merced           Merced 519.2500 0.000000e+00
> Orange           Orange 710.5625 0.000000e+00
> Plumas           Plumas 709.5556 1.248655e-13
> San Diego     San Diego 659.4364 2.296800e+00
> San Joaquin San Joaquin 551.1892 1.354175e-13
> Santa Clara Santa Clara 732.0769 1.577292e+01
>
>
> With Julia this is the result I get:
>
> 11?3 DataFrame
>  Row ? cname        mean     se
>      ? String15     Float64  Float64
> ??????????????????????????????????????
>    1 ? Alameda      669.0     6469.11
>    2 ? Fresno       472.0     7832.61
>    3 ? Kern         452.5    10703.1
>    4 ? Los Angeles  647.267   5232.24
>    5 ? Mendocino    623.25   10364.8
>    6 ? Merced       519.25    8616.22
>    7 ? Orange       710.563   5534.39
>    8 ? Plumas       709.556   7673.15
>    9 ? San Diego    659.436   1882.06
>   10 ? San Joaquin  551.189   2254.24
>   11 ? Santa Clara  732.077   4000.03
>
> And just to have the full picture, this is my code for the
> Horvitz-Thompson variance estimator:
>
> function hte(y, pw, n)
>         p = 1 .- (1 .- 1 ./ pw) .^ n
>
>         first_sum = sum((1 .- p) ./ (p .^ 2) .* y .^ 2)
>         second_sum = 0
>
>         for i in 1:length(p)
>                 for j in 1:length(p)
>                         if i == j
>                                 continue
>                         end
>
>
>                         p_ij = p[i] + p[j] - (1 - (1 - 1 / pw[i] - 1 /
> pw[j]) ^ n)
>                         second_sum += (p_ij - p[i] * p[j]) / (p[i] * p[j])
> * y[i] * y[j]
>                 end
>         end
>
>         return first_sum + second_sum
> end
>
> The standard error in the resulted data frame above is just the square
> root of the hte result.
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Wed Jul  6 18:29:28 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Wed, 6 Jul 2022 16:29:28 +0000
Subject: [R] byte coding compiling.....
Message-ID: <SL2P216MB1561B2FC1E68A2ADC116E3A9C8809@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>

Dear members,
                          I am using pbmclapply, the progress bar version of mclapply, from the parallel package. The point is, pbmclapply is three times faster than mclapply, and I think the most probable reason would be that pbmclapply is byte code compiled (I can think of no other reason).

I know the cmpfun function from compiler package. If I do:

> cmpfun(mclapply)

will the job be done? The point is mclapply may look for other functions in the parallel package. So I have to compile the whole package right? How do you do that? or in general, how do you byte code compile a whole package?

Thanking you,
Yours sincerely,
AKSHAY M KULKARNI

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Wed Jul  6 19:02:47 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 6 Jul 2022 20:02:47 +0300
Subject: [R] byte coding compiling.....
In-Reply-To: <SL2P216MB1561B2FC1E68A2ADC116E3A9C8809@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB1561B2FC1E68A2ADC116E3A9C8809@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
Message-ID: <CAGxFJbQgh3P7314CJY5ZpwyGpkJt7w7mfZGVTtLKs0Bze=XKAQ@mail.gmail.com>

Unlikely

See here:

https://www.r-bloggers.com/2017/08/how-to-make-best-use-of-the-byte-compiler-in-r/

Byte code compilation should be automatic in both cases, as I understand
it. Of course, I could be wrong due to special features of parallel
programming, etc.

A reprex might be helpful here.

Cheers,
Bert


On Wed, Jul 6, 2022, 7:29 PM akshay kulkarni <akshay_e4 at hotmail.com> wrote:

> Dear members,
>                           I am using pbmclapply, the progress bar version
> of mclapply, from the parallel package. The point is, pbmclapply is three
> times faster than mclapply, and I think the most probable reason would be
> that pbmclapply is byte code compiled (I can think of no other reason).
>
> I know the cmpfun function from compiler package. If I do:
>
> > cmpfun(mclapply)
>
> will the job be done? The point is mclapply may look for other functions
> in the parallel package. So I have to compile the whole package right? How
> do you do that? or in general, how do you byte code compile a whole package?
>
> Thanking you,
> Yours sincerely,
> AKSHAY M KULKARNI
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From j@de@shod@@ m@iii@g oii googiem@ii@com  Wed Jul  6 19:22:09 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Wed, 6 Jul 2022 18:22:09 +0100
Subject: [R] Space between axis title and tick labels in persp plot in R
 (using vis.gam)
Message-ID: <CANg3_k_6XXX5qFsbQAd49h2Ty7sPUCD5Xqp4zHJf1W=AXy2N=g@mail.gmail.com>

Dear list,

I am making a perspective plot of my generalised additive model (GAM)
named a1b, using vis.gam() in mgcv, which in turn makes use of the
persp function in base R.

Code is as follows:

library(mgcv)
vis.gam(x = a1b,
        view = c("wbgt_max", "lag"),
        plot.type = "persp",
        xlab = "max WBGT (?C)",
        ylab = "lag (days)",
        zlab = "deaths",
        theta = 60,
        phi = 15,
        r = sqrt(3),
        d = 1,
        type = "response",
        ticktype = "detailed")

The plot can be found here:
https://stackoverflow.com/questions/72884763/space-between-axis-title-and-tick-labels-in-persp-plot-in-r-using-vis-gam

On the x and z-axes, the axis title and axis labels are plotted over each other.


I would like to:

1) increase the space between the axis titles and the tick labels for
the x and z- axes (WBGT and deaths) and
2) increase the space between the ticks and the tick labels.

I've looked up similar posts on StackOverflow, which suggest adding
the following code to the plot (and change the values for the
currently stated defaults):

par(mgp=c(3,1,0))

like

library(mgcv)
par(mgp=c(20,20,20))
vis.gam(x = a1b,
        view = c("wbgt_max", "lag"),
        etc.

However, this changes nothing at all to the plot. (I tried with c(20,
20, 20) , smaller values and larger ones).

Would be grateful for any suggestions!

Jade


From @k@h@y_e4 @end|ng |rom hotm@||@com  Wed Jul  6 19:54:18 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Wed, 6 Jul 2022 17:54:18 +0000
Subject: [R] byte coding compiling.....
In-Reply-To: <CAGxFJbQgh3P7314CJY5ZpwyGpkJt7w7mfZGVTtLKs0Bze=XKAQ@mail.gmail.com>
References: <SL2P216MB1561B2FC1E68A2ADC116E3A9C8809@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <CAGxFJbQgh3P7314CJY5ZpwyGpkJt7w7mfZGVTtLKs0Bze=XKAQ@mail.gmail.com>
Message-ID: <PU4P216MB1568D049849A4F1EC8A0BAEFC8809@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear Bert,
                 Thanks for your reply...

So
> cmpfun(mclapply)

should  do the job right?

By the by, how can I give a reprex? Reprex of the code that I am giving to mclapply (as FUN argument)?

Yours sincerely,
AKSHAY M KULKARNI
________________________________
From: Bert Gunter <bgunter.4567 at gmail.com>
Sent: Wednesday, July 6, 2022 10:32 PM
To: akshay kulkarni <akshay_e4 at hotmail.com>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] byte coding compiling.....

Unlikely

See here:
 https://www.r-bloggers.com/2017/08/how-to-make-best-use-of-the-byte-compiler-in-r/

Byte code compilation should be automatic in both cases, as I understand it. Of course, I could be wrong due to special features of parallel  programming, etc.

A reprex might be helpful here.

Cheers,
Bert


On Wed, Jul 6, 2022, 7:29 PM akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>> wrote:
Dear members,
                          I am using pbmclapply, the progress bar version of mclapply, from the parallel package. The point is, pbmclapply is three times faster than mclapply, and I think the most probable reason would be that pbmclapply is byte code compiled (I can think of no other reason).

I know the cmpfun function from compiler package. If I do:

> cmpfun(mclapply)

will the job be done? The point is mclapply may look for other functions in the parallel package. So I have to compile the whole package right? How do you do that? or in general, how do you byte code compile a whole package?

Thanking you,
Yours sincerely,
AKSHAY M KULKARNI

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Jul  6 20:24:16 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 06 Jul 2022 11:24:16 -0700
Subject: [R] byte coding compiling.....
In-Reply-To: <PU4P216MB1568D049849A4F1EC8A0BAEFC8809@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB1561B2FC1E68A2ADC116E3A9C8809@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <CAGxFJbQgh3P7314CJY5ZpwyGpkJt7w7mfZGVTtLKs0Bze=XKAQ@mail.gmail.com>
 <PU4P216MB1568D049849A4F1EC8A0BAEFC8809@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <9DDCCA81-D032-47FF-B47F-54698AA5F503@dcn.davis.ca.us>

A reprex is a minimal example that performs as you described. It is up to you to trim your code down to eliminate irrelevant computation while not eliminating the described behavior. You may discover something about your own code in the process that answers your question, or you will be able to demonstrate the issue, or we may still not be able to reproduce if it is related to your system.

On July 6, 2022 10:54:18 AM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>Dear Bert,
>                 Thanks for your reply...
>
>So
>> cmpfun(mclapply)
>
>should  do the job right?
>
>By the by, how can I give a reprex? Reprex of the code that I am giving to mclapply (as FUN argument)?
>
>Yours sincerely,
>AKSHAY M KULKARNI
>________________________________
>From: Bert Gunter <bgunter.4567 at gmail.com>
>Sent: Wednesday, July 6, 2022 10:32 PM
>To: akshay kulkarni <akshay_e4 at hotmail.com>
>Cc: R help Mailing list <r-help at r-project.org>
>Subject: Re: [R] byte coding compiling.....
>
>Unlikely
>
>See here:
> https://www.r-bloggers.com/2017/08/how-to-make-best-use-of-the-byte-compiler-in-r/
>
>Byte code compilation should be automatic in both cases, as I understand it. Of course, I could be wrong due to special features of parallel  programming, etc.
>
>A reprex might be helpful here.
>
>Cheers,
>Bert
>
>
>On Wed, Jul 6, 2022, 7:29 PM akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>> wrote:
>Dear members,
>                          I am using pbmclapply, the progress bar version of mclapply, from the parallel package. The point is, pbmclapply is three times faster than mclapply, and I think the most probable reason would be that pbmclapply is byte code compiled (I can think of no other reason).
>
>I know the cmpfun function from compiler package. If I do:
>
>> cmpfun(mclapply)
>
>will the job be done? The point is mclapply may look for other functions in the parallel package. So I have to compile the whole package right? How do you do that? or in general, how do you byte code compile a whole package?
>
>Thanking you,
>Yours sincerely,
>AKSHAY M KULKARNI
>
>        [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From ||gge@ @end|ng |rom @t@t|@t|k@tu-dortmund@de  Thu Jul  7 12:38:19 2022
From: ||gge@ @end|ng |rom @t@t|@t|k@tu-dortmund@de (Uwe Ligges)
Date: Thu, 7 Jul 2022 12:38:19 +0200
Subject: [R] byte coding compiling.....
In-Reply-To: <PU4P216MB1568D049849A4F1EC8A0BAEFC8809@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB1561B2FC1E68A2ADC116E3A9C8809@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <CAGxFJbQgh3P7314CJY5ZpwyGpkJt7w7mfZGVTtLKs0Bze=XKAQ@mail.gmail.com>
 <PU4P216MB1568D049849A4F1EC8A0BAEFC8809@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <925b6245-0fb2-b2b0-8a3e-832ca145afff@statistik.tu-dortmund.de>



On 06.07.2022 19:54, akshay kulkarni wrote:
> Dear Bert,
>                   Thanks for your reply...
> 
> So
>> cmpfun(mclapply)

mclapply is already byte compiled as it is in a package.

You may want to
cmpfun(yourFunction)
the function that you use in the mclapply call.

Best,
Uwe Ligges


> 
> should  do the job right?
> 
> By the by, how can I give a reprex? Reprex of the code that I am giving to mclapply (as FUN argument)?
> 
> Yours sincerely,
> AKSHAY M KULKARNI
> ________________________________
> From: Bert Gunter <bgunter.4567 at gmail.com>
> Sent: Wednesday, July 6, 2022 10:32 PM
> To: akshay kulkarni <akshay_e4 at hotmail.com>
> Cc: R help Mailing list <r-help at r-project.org>
> Subject: Re: [R] byte coding compiling.....
> 
> Unlikely
> 
> See here:
>   https://www.r-bloggers.com/2017/08/how-to-make-best-use-of-the-byte-compiler-in-r/
> 
> Byte code compilation should be automatic in both cases, as I understand it. Of course, I could be wrong due to special features of parallel  programming, etc.
> 
> A reprex might be helpful here.
> 
> Cheers,
> Bert
> 
> 
> On Wed, Jul 6, 2022, 7:29 PM akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>> wrote:
> Dear members,
>                            I am using pbmclapply, the progress bar version of mclapply, from the parallel package. The point is, pbmclapply is three times faster than mclapply, and I think the most probable reason would be that pbmclapply is byte code compiled (I can think of no other reason).
> 
> I know the cmpfun function from compiler package. If I do:
> 
>> cmpfun(mclapply)
> 
> will the job be done? The point is mclapply may look for other functions in the parallel package. So I have to compile the whole package right? How do you do that? or in general, how do you byte code compile a whole package?
> 
> Thanking you,
> Yours sincerely,
> AKSHAY M KULKARNI
> 
>          [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ||gge@ @end|ng |rom @t@t|@t|k@tu-dortmund@de  Thu Jul  7 12:42:28 2022
From: ||gge@ @end|ng |rom @t@t|@t|k@tu-dortmund@de (Uwe Ligges)
Date: Thu, 7 Jul 2022 12:42:28 +0200
Subject: [R] Space between axis title and tick labels in persp plot in R
 (using vis.gam)
In-Reply-To: <CANg3_k_6XXX5qFsbQAd49h2Ty7sPUCD5Xqp4zHJf1W=AXy2N=g@mail.gmail.com>
References: <CANg3_k_6XXX5qFsbQAd49h2Ty7sPUCD5Xqp4zHJf1W=AXy2N=g@mail.gmail.com>
Message-ID: <ccfb3e47-05bd-f564-5b0d-5510f862bf41@statistik.tu-dortmund.de>

R does not provide a native 3D coordinate syytem in base graphics. 
Therefore, persp() is rather a hack that plots everything based on its 
internal code into the device.
Labels are not in the marhings of the 2D coordinate system, hence the 
related par() are not functional.

I'd suggest to add the axis titles manually after the plot and keep the 
rest unless you want to look fpr alternatives to persp().

Best,
Uwe Ligges

On 06.07.2022 19:22, jade.shodan--- via R-help wrote:
> Dear list,
> 
> I am making a perspective plot of my generalised additive model (GAM)
> named a1b, using vis.gam() in mgcv, which in turn makes use of the
> persp function in base R.
> 
> Code is as follows:
> 
> library(mgcv)
> vis.gam(x = a1b,
>          view = c("wbgt_max", "lag"),
>          plot.type = "persp",
>          xlab = "max WBGT (?C)",
>          ylab = "lag (days)",
>          zlab = "deaths",
>          theta = 60,
>          phi = 15,
>          r = sqrt(3),
>          d = 1,
>          type = "response",
>          ticktype = "detailed")
> 
> The plot can be found here:
> https://stackoverflow.com/questions/72884763/space-between-axis-title-and-tick-labels-in-persp-plot-in-r-using-vis-gam
> 
> On the x and z-axes, the axis title and axis labels are plotted over each other.
> 
> 
> I would like to:
> 
> 1) increase the space between the axis titles and the tick labels for
> the x and z- axes (WBGT and deaths) and
> 2) increase the space between the ticks and the tick labels.
> 
> I've looked up similar posts on StackOverflow, which suggest adding
> the following code to the plot (and change the values for the
> currently stated defaults):
> 
> par(mgp=c(3,1,0))
> 
> like
> 
> library(mgcv)
> par(mgp=c(20,20,20))
> vis.gam(x = a1b,
>          view = c("wbgt_max", "lag"),
>          etc.
> 
> However, this changes nothing at all to the plot. (I tried with c(20,
> 20, 20) , smaller values and larger ones).
> 
> Would be grateful for any suggestions!
> 
> Jade
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j@de@shod@@ m@iii@g oii googiem@ii@com  Thu Jul  7 12:57:23 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Thu, 7 Jul 2022 11:57:23 +0100
Subject: [R] Space between axis title and tick labels in persp plot in R
 (using vis.gam)
In-Reply-To: <ccfb3e47-05bd-f564-5b0d-5510f862bf41@statistik.tu-dortmund.de>
References: <CANg3_k_6XXX5qFsbQAd49h2Ty7sPUCD5Xqp4zHJf1W=AXy2N=g@mail.gmail.com>
 <ccfb3e47-05bd-f564-5b0d-5510f862bf41@statistik.tu-dortmund.de>
Message-ID: <CANg3_k-jOTFkJBx+1Npx+reQqRp3TFAqbj4qbibRdjQA7Vke4A@mail.gmail.com>

Thanks for the response Uwe. It's really helpful! I can spend my time
on doing more useful stuff now :-)

On Thu, 7 Jul 2022 at 11:42, Uwe Ligges <ligges at statistik.tu-dortmund.de> wrote:
>
> R does not provide a native 3D coordinate syytem in base graphics.
> Therefore, persp() is rather a hack that plots everything based on its
> internal code into the device.
> Labels are not in the marhings of the 2D coordinate system, hence the
> related par() are not functional.
>
> I'd suggest to add the axis titles manually after the plot and keep the
> rest unless you want to look fpr alternatives to persp().
>
> Best,
> Uwe Ligges
>
> On 06.07.2022 19:22, jade.shodan--- via R-help wrote:
> > Dear list,
> >
> > I am making a perspective plot of my generalised additive model (GAM)
> > named a1b, using vis.gam() in mgcv, which in turn makes use of the
> > persp function in base R.
> >
> > Code is as follows:
> >
> > library(mgcv)
> > vis.gam(x = a1b,
> >          view = c("wbgt_max", "lag"),
> >          plot.type = "persp",
> >          xlab = "max WBGT (?C)",
> >          ylab = "lag (days)",
> >          zlab = "deaths",
> >          theta = 60,
> >          phi = 15,
> >          r = sqrt(3),
> >          d = 1,
> >          type = "response",
> >          ticktype = "detailed")
> >
> > The plot can be found here:
> > https://stackoverflow.com/questions/72884763/space-between-axis-title-and-tick-labels-in-persp-plot-in-r-using-vis-gam
> >
> > On the x and z-axes, the axis title and axis labels are plotted over each other.
> >
> >
> > I would like to:
> >
> > 1) increase the space between the axis titles and the tick labels for
> > the x and z- axes (WBGT and deaths) and
> > 2) increase the space between the ticks and the tick labels.
> >
> > I've looked up similar posts on StackOverflow, which suggest adding
> > the following code to the plot (and change the values for the
> > currently stated defaults):
> >
> > par(mgp=c(3,1,0))
> >
> > like
> >
> > library(mgcv)
> > par(mgp=c(20,20,20))
> > vis.gam(x = a1b,
> >          view = c("wbgt_max", "lag"),
> >          etc.
> >
> > However, this changes nothing at all to the plot. (I tried with c(20,
> > 20, 20) , smaller values and larger ones).
> >
> > Would be grateful for any suggestions!
> >
> > Jade
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From mjrizzo m@iii@g oii y@hoo@com  Thu Jul  7 15:54:47 2022
From: mjrizzo m@iii@g oii y@hoo@com (mjrizzo m@iii@g oii y@hoo@com)
Date: Thu, 7 Jul 2022 13:54:47 +0000 (UTC)
Subject: [R] RODBC package issues with DB2 ODBC drivers and Windows R 4.2.0
In-Reply-To: <450681330.3178427.1657201876029@mail.yahoo.com>
References: <450681330.3178427.1657201876029.ref@mail.yahoo.com>
 <450681330.3178427.1657201876029@mail.yahoo.com>
Message-ID: <1915480031.2714301.1657202087397@mail.yahoo.com>

 Hello,

I have been looking for help with an issue I am having with the RODBC package .? I recently upgraded to R 4.2.0 from R 4.1.1 in Windows 10 Enterprise Build 19044.? I use the RODBC package along with the IBM DB2 ODBC driver version 11.01.331.368 to access DB2 databases.? Using the RODBC package in 4.1.1, everything works as it should.? I am able to connect to the DB2 databases without any problems.??
After upgrading to R 4.2.0 and upgrading the RODBC package, I haven't been able to connect at all, and I receive the following error message: "ERROR: state IM004, code 0, message [Microsoft] [ODBC Driver Manager] Driver's SQLAllocHandle on SQL_HANDLE_ENV failed".??
When I searched the web for anyone else experiencing?the same issue, there were a few mentions of the same problem but did not mention any remedies.? Has anyone else experienced any issues with using the DB2 ODBC driver in R 4.2.0?? I downgraded back to R 4.1.1 and everything worked fine once again.
Any help would be appreciated.
  
	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Jul  7 17:38:49 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 7 Jul 2022 15:38:49 +0000
Subject: [R] byte coding compiling.....
In-Reply-To: <925b6245-0fb2-b2b0-8a3e-832ca145afff@statistik.tu-dortmund.de>
References: <SL2P216MB1561B2FC1E68A2ADC116E3A9C8809@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <CAGxFJbQgh3P7314CJY5ZpwyGpkJt7w7mfZGVTtLKs0Bze=XKAQ@mail.gmail.com>
 <PU4P216MB1568D049849A4F1EC8A0BAEFC8809@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <925b6245-0fb2-b2b0-8a3e-832ca145afff@statistik.tu-dortmund.de>
Message-ID: <PU4P216MB156808C6AF30820D91E7E559C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear Uwe,
                  I have attached the info from the parallel package description from my Rstudio IDE:

Package: parallel
Version: 4.1.2
Priority: base
Title: Support for Parallel computation in R
Author: R Core Team
Maintainer: R Core Team <do-use-Contact-address at r-project.org>
Contact: R-help mailing list <r-help at r-project.org>
Description: Support for parallel computation, including by forking
   (taken from package multicore), by sockets (taken from package snow)
   and random-number generation.
License: Part of R 4.1.2
Imports: tools, compiler
Suggests: methods
Enhances: snow, nws, Rmpi
NeedsCompilation: yes
Built: R 4.1.2; x86_64-w64-mingw32; 2021-11-01 18:38:05 UTC; windows

It says: NeedsCompilation: yes

How about it?

Yours sincerely,
AKSHAY M KULKARNI

________________________________
From: Uwe Ligges <ligges at statistik.tu-dortmund.de>
Sent: Thursday, July 7, 2022 4:08 PM
To: akshay kulkarni <akshay_e4 at hotmail.com>; Bert Gunter <bgunter.4567 at gmail.com>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] byte coding compiling.....



On 06.07.2022 19:54, akshay kulkarni wrote:
> Dear Bert,
>                   Thanks for your reply...
>
> So
>> cmpfun(mclapply)

mclapply is already byte compiled as it is in a package.

You may want to
cmpfun(yourFunction)
the function that you use in the mclapply call.

Best,
Uwe Ligges


>
> should  do the job right?
>
> By the by, how can I give a reprex? Reprex of the code that I am giving to mclapply (as FUN argument)?
>
> Yours sincerely,
> AKSHAY M KULKARNI
> ________________________________
> From: Bert Gunter <bgunter.4567 at gmail.com>
> Sent: Wednesday, July 6, 2022 10:32 PM
> To: akshay kulkarni <akshay_e4 at hotmail.com>
> Cc: R help Mailing list <r-help at r-project.org>
> Subject: Re: [R] byte coding compiling.....
>
> Unlikely
>
> See here:
>   https://www.r-bloggers.com/2017/08/how-to-make-best-use-of-the-byte-compiler-in-r/
>
> Byte code compilation should be automatic in both cases, as I understand it. Of course, I could be wrong due to special features of parallel  programming, etc.
>
> A reprex might be helpful here.
>
> Cheers,
> Bert
>
>
> On Wed, Jul 6, 2022, 7:29 PM akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>> wrote:
> Dear members,
>                            I am using pbmclapply, the progress bar version of mclapply, from the parallel package. The point is, pbmclapply is three times faster than mclapply, and I think the most probable reason would be that pbmclapply is byte code compiled (I can think of no other reason).
>
> I know the cmpfun function from compiler package. If I do:
>
>> cmpfun(mclapply)
>
> will the job be done? The point is mclapply may look for other functions in the parallel package. So I have to compile the whole package right? How do you do that? or in general, how do you byte code compile a whole package?
>
> Thanking you,
> Yours sincerely,
> AKSHAY M KULKARNI
>
>          [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Jul  7 18:38:09 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 7 Jul 2022 17:38:09 +0100
Subject: [R] byte coding compiling.....
In-Reply-To: <PU4P216MB156808C6AF30820D91E7E559C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB1561B2FC1E68A2ADC116E3A9C8809@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <CAGxFJbQgh3P7314CJY5ZpwyGpkJt7w7mfZGVTtLKs0Bze=XKAQ@mail.gmail.com>
 <PU4P216MB1568D049849A4F1EC8A0BAEFC8809@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <925b6245-0fb2-b2b0-8a3e-832ca145afff@statistik.tu-dortmund.de>
 <PU4P216MB156808C6AF30820D91E7E559C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <43e4c9ca-a622-9c6b-a197-52c991d130cc@sapo.pt>

Hello,

Sorry to intrude in this thread but what NeedsCompilation means is that 
the package contains C, C++ or Fortran native code in a directory named src.

 From the documentation, Writing R Extensions, section 1.1.1 The 
DESCRIPTION file:


The ?NeedsCompilation? field should be set to "yes" if the package 
contains native code which needs to be compiled, otherwise "no" (when 
the package could be installed from source on any platform without 
additional tools). This is used by install.packages(type = "both") in R 
 >= 2.15.2 on platforms where binary packages are the norm: it is 
normally set by R CMD build or the repository assuming compilation is 
required if and only if the package has a src directory.


And package parallel does have a src directory with C code.


[1] 
https://cran.r-project.org/doc/manuals/r-release/R-exts.html#The-DESCRIPTION-file


Hope this helps,

Rui Barradas

?s 16:38 de 07/07/2022, akshay kulkarni escreveu:
> Dear Uwe,
>                    I have attached the info from the parallel package description from my Rstudio IDE:
> 
> Package: parallel
> Version: 4.1.2
> Priority: base
> Title: Support for Parallel computation in R
> Author: R Core Team
> Maintainer: R Core Team <do-use-Contact-address at r-project.org>
> Contact: R-help mailing list <r-help at r-project.org>
> Description: Support for parallel computation, including by forking
>     (taken from package multicore), by sockets (taken from package snow)
>     and random-number generation.
> License: Part of R 4.1.2
> Imports: tools, compiler
> Suggests: methods
> Enhances: snow, nws, Rmpi
> NeedsCompilation: yes
> Built: R 4.1.2; x86_64-w64-mingw32; 2021-11-01 18:38:05 UTC; windows
> 
> It says: NeedsCompilation: yes
> 
> How about it?
> 
> Yours sincerely,
> AKSHAY M KULKARNI
> 
> ________________________________
> From: Uwe Ligges <ligges at statistik.tu-dortmund.de>
> Sent: Thursday, July 7, 2022 4:08 PM
> To: akshay kulkarni <akshay_e4 at hotmail.com>; Bert Gunter <bgunter.4567 at gmail.com>
> Cc: R help Mailing list <r-help at r-project.org>
> Subject: Re: [R] byte coding compiling.....
> 
> 
> 
> On 06.07.2022 19:54, akshay kulkarni wrote:
>> Dear Bert,
>>                    Thanks for your reply...
>>
>> So
>>> cmpfun(mclapply)
> 
> mclapply is already byte compiled as it is in a package.
> 
> You may want to
> cmpfun(yourFunction)
> the function that you use in the mclapply call.
> 
> Best,
> Uwe Ligges
> 
> 
>>
>> should  do the job right?
>>
>> By the by, how can I give a reprex? Reprex of the code that I am giving to mclapply (as FUN argument)?
>>
>> Yours sincerely,
>> AKSHAY M KULKARNI
>> ________________________________
>> From: Bert Gunter <bgunter.4567 at gmail.com>
>> Sent: Wednesday, July 6, 2022 10:32 PM
>> To: akshay kulkarni <akshay_e4 at hotmail.com>
>> Cc: R help Mailing list <r-help at r-project.org>
>> Subject: Re: [R] byte coding compiling.....
>>
>> Unlikely
>>
>> See here:
>>    https://www.r-bloggers.com/2017/08/how-to-make-best-use-of-the-byte-compiler-in-r/
>>
>> Byte code compilation should be automatic in both cases, as I understand it. Of course, I could be wrong due to special features of parallel  programming, etc.
>>
>> A reprex might be helpful here.
>>
>> Cheers,
>> Bert
>>
>>
>> On Wed, Jul 6, 2022, 7:29 PM akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>> wrote:
>> Dear members,
>>                             I am using pbmclapply, the progress bar version of mclapply, from the parallel package. The point is, pbmclapply is three times faster than mclapply, and I think the most probable reason would be that pbmclapply is byte code compiled (I can think of no other reason).
>>
>> I know the cmpfun function from compiler package. If I do:
>>
>>> cmpfun(mclapply)
>>
>> will the job be done? The point is mclapply may look for other functions in the parallel package. So I have to compile the whole package right? How do you do that? or in general, how do you byte code compile a whole package?
>>
>> Thanking you,
>> Yours sincerely,
>> AKSHAY M KULKARNI
>>
>>           [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Jul  7 18:40:17 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 07 Jul 2022 09:40:17 -0700
Subject: [R] byte coding compiling.....
In-Reply-To: <PU4P216MB156808C6AF30820D91E7E559C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB1561B2FC1E68A2ADC116E3A9C8809@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <CAGxFJbQgh3P7314CJY5ZpwyGpkJt7w7mfZGVTtLKs0Bze=XKAQ@mail.gmail.com>
 <PU4P216MB1568D049849A4F1EC8A0BAEFC8809@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <925b6245-0fb2-b2b0-8a3e-832ca145afff@statistik.tu-dortmund.de>
 <PU4P216MB156808C6AF30820D91E7E559C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <52AEDAC3-F513-4247-B315-CE06BFA92AB1@dcn.davis.ca.us>

That item refers to the package having some compiled language (e.g. C, C++, Fortran, etc) components. The very fact that it got installed confirms that compilation occurred... it would not be usable otherwise.

On July 7, 2022 8:38:49 AM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>Dear Uwe,
>                  I have attached the info from the parallel package description from my Rstudio IDE:
>
>Package: parallel
>Version: 4.1.2
>Priority: base
>Title: Support for Parallel computation in R
>Author: R Core Team
>Maintainer: R Core Team <do-use-Contact-address at r-project.org>
>Contact: R-help mailing list <r-help at r-project.org>
>Description: Support for parallel computation, including by forking
>   (taken from package multicore), by sockets (taken from package snow)
>   and random-number generation.
>License: Part of R 4.1.2
>Imports: tools, compiler
>Suggests: methods
>Enhances: snow, nws, Rmpi
>NeedsCompilation: yes
>Built: R 4.1.2; x86_64-w64-mingw32; 2021-11-01 18:38:05 UTC; windows
>
>It says: NeedsCompilation: yes
>
>How about it?
>
>Yours sincerely,
>AKSHAY M KULKARNI
>
>________________________________
>From: Uwe Ligges <ligges at statistik.tu-dortmund.de>
>Sent: Thursday, July 7, 2022 4:08 PM
>To: akshay kulkarni <akshay_e4 at hotmail.com>; Bert Gunter <bgunter.4567 at gmail.com>
>Cc: R help Mailing list <r-help at r-project.org>
>Subject: Re: [R] byte coding compiling.....
>
>
>
>On 06.07.2022 19:54, akshay kulkarni wrote:
>> Dear Bert,
>>                   Thanks for your reply...
>>
>> So
>>> cmpfun(mclapply)
>
>mclapply is already byte compiled as it is in a package.
>
>You may want to
>cmpfun(yourFunction)
>the function that you use in the mclapply call.
>
>Best,
>Uwe Ligges
>
>
>>
>> should  do the job right?
>>
>> By the by, how can I give a reprex? Reprex of the code that I am giving to mclapply (as FUN argument)?
>>
>> Yours sincerely,
>> AKSHAY M KULKARNI
>> ________________________________
>> From: Bert Gunter <bgunter.4567 at gmail.com>
>> Sent: Wednesday, July 6, 2022 10:32 PM
>> To: akshay kulkarni <akshay_e4 at hotmail.com>
>> Cc: R help Mailing list <r-help at r-project.org>
>> Subject: Re: [R] byte coding compiling.....
>>
>> Unlikely
>>
>> See here:
>>   https://www.r-bloggers.com/2017/08/how-to-make-best-use-of-the-byte-compiler-in-r/
>>
>> Byte code compilation should be automatic in both cases, as I understand it. Of course, I could be wrong due to special features of parallel  programming, etc.
>>
>> A reprex might be helpful here.
>>
>> Cheers,
>> Bert
>>
>>
>> On Wed, Jul 6, 2022, 7:29 PM akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>> wrote:
>> Dear members,
>>                            I am using pbmclapply, the progress bar version of mclapply, from the parallel package. The point is, pbmclapply is three times faster than mclapply, and I think the most probable reason would be that pbmclapply is byte code compiled (I can think of no other reason).
>>
>> I know the cmpfun function from compiler package. If I do:
>>
>>> cmpfun(mclapply)
>>
>> will the job be done? The point is mclapply may look for other functions in the parallel package. So I have to compile the whole package right? How do you do that? or in general, how do you byte code compile a whole package?
>>
>> Thanking you,
>> Yours sincerely,
>> AKSHAY M KULKARNI
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>        [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Jul  7 18:48:22 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 7 Jul 2022 16:48:22 +0000
Subject: [R] byte coding compiling.....
In-Reply-To: <52AEDAC3-F513-4247-B315-CE06BFA92AB1@dcn.davis.ca.us>
References: <SL2P216MB1561B2FC1E68A2ADC116E3A9C8809@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <CAGxFJbQgh3P7314CJY5ZpwyGpkJt7w7mfZGVTtLKs0Bze=XKAQ@mail.gmail.com>
 <PU4P216MB1568D049849A4F1EC8A0BAEFC8809@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <925b6245-0fb2-b2b0-8a3e-832ca145afff@statistik.tu-dortmund.de>
 <PU4P216MB156808C6AF30820D91E7E559C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <52AEDAC3-F513-4247-B315-CE06BFA92AB1@dcn.davis.ca.us>
Message-ID: <PU4P216MB1568D5C1C17BA6D81CC08550C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear Jeff,
                THen why is pbmclapply 3 time faster than mclappy? In the package description of pbmclapply it says: "Needs compilation no". When i ran my code iwith pbmclapply, I did not compile my code. So I do presume that the speed up must in some way connected to this "NeedsCompilation" field. Any thoughts on that?

Yours sincerely
AKSHAY M KULKARNI
________________________________
From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Sent: Thursday, July 7, 2022 10:10 PM
To: r-help at r-project.org <r-help at r-project.org>; akshay kulkarni <akshay_e4 at hotmail.com>; Uwe Ligges <ligges at statistik.tu-dortmund.de>; Bert Gunter <bgunter.4567 at gmail.com>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] byte coding compiling.....

That item refers to the package having some compiled language (e.g. C, C++, Fortran, etc) components. The very fact that it got installed confirms that compilation occurred... it would not be usable otherwise.

On July 7, 2022 8:38:49 AM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>Dear Uwe,
>                  I have attached the info from the parallel package description from my Rstudio IDE:
>
>Package: parallel
>Version: 4.1.2
>Priority: base
>Title: Support for Parallel computation in R
>Author: R Core Team
>Maintainer: R Core Team <do-use-Contact-address at r-project.org>
>Contact: R-help mailing list <r-help at r-project.org>
>Description: Support for parallel computation, including by forking
>   (taken from package multicore), by sockets (taken from package snow)
>   and random-number generation.
>License: Part of R 4.1.2
>Imports: tools, compiler
>Suggests: methods
>Enhances: snow, nws, Rmpi
>NeedsCompilation: yes
>Built: R 4.1.2; x86_64-w64-mingw32; 2021-11-01 18:38:05 UTC; windows
>
>It says: NeedsCompilation: yes
>
>How about it?
>
>Yours sincerely,
>AKSHAY M KULKARNI
>
>________________________________
>From: Uwe Ligges <ligges at statistik.tu-dortmund.de>
>Sent: Thursday, July 7, 2022 4:08 PM
>To: akshay kulkarni <akshay_e4 at hotmail.com>; Bert Gunter <bgunter.4567 at gmail.com>
>Cc: R help Mailing list <r-help at r-project.org>
>Subject: Re: [R] byte coding compiling.....
>
>
>
>On 06.07.2022 19:54, akshay kulkarni wrote:
>> Dear Bert,
>>                   Thanks for your reply...
>>
>> So
>>> cmpfun(mclapply)
>
>mclapply is already byte compiled as it is in a package.
>
>You may want to
>cmpfun(yourFunction)
>the function that you use in the mclapply call.
>
>Best,
>Uwe Ligges
>
>
>>
>> should  do the job right?
>>
>> By the by, how can I give a reprex? Reprex of the code that I am giving to mclapply (as FUN argument)?
>>
>> Yours sincerely,
>> AKSHAY M KULKARNI
>> ________________________________
>> From: Bert Gunter <bgunter.4567 at gmail.com>
>> Sent: Wednesday, July 6, 2022 10:32 PM
>> To: akshay kulkarni <akshay_e4 at hotmail.com>
>> Cc: R help Mailing list <r-help at r-project.org>
>> Subject: Re: [R] byte coding compiling.....
>>
>> Unlikely
>>
>> See here:
>>   https://www.r-bloggers.com/2017/08/how-to-make-best-use-of-the-byte-compiler-in-r/
>>
>> Byte code compilation should be automatic in both cases, as I understand it. Of course, I could be wrong due to special features of parallel  programming, etc.
>>
>> A reprex might be helpful here.
>>
>> Cheers,
>> Bert
>>
>>
>> On Wed, Jul 6, 2022, 7:29 PM akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>> wrote:
>> Dear members,
>>                            I am using pbmclapply, the progress bar version of mclapply, from the parallel package. The point is, pbmclapply is three times faster than mclapply, and I think the most probable reason would be that pbmclapply is byte code compiled (I can think of no other reason).
>>
>> I know the cmpfun function from compiler package. If I do:
>>
>>> cmpfun(mclapply)
>>
>> will the job be done? The point is mclapply may look for other functions in the parallel package. So I have to compile the whole package right? How do you do that? or in general, how do you byte code compile a whole package?
>>
>> Thanking you,
>> Yours sincerely,
>> AKSHAY M KULKARNI
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>        [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>       [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Jul  7 19:23:19 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 07 Jul 2022 10:23:19 -0700
Subject: [R] byte coding compiling.....
In-Reply-To: <PU4P216MB1568D5C1C17BA6D81CC08550C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB1561B2FC1E68A2ADC116E3A9C8809@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <CAGxFJbQgh3P7314CJY5ZpwyGpkJt7w7mfZGVTtLKs0Bze=XKAQ@mail.gmail.com>
 <PU4P216MB1568D049849A4F1EC8A0BAEFC8809@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <925b6245-0fb2-b2b0-8a3e-832ca145afff@statistik.tu-dortmund.de>
 <PU4P216MB156808C6AF30820D91E7E559C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <52AEDAC3-F513-4247-B315-CE06BFA92AB1@dcn.davis.ca.us>
 <PU4P216MB1568D5C1C17BA6D81CC08550C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <1D1E6315-8288-49C8-A15D-ECBD4E060CFE@dcn.davis.ca.us>

I don't know why one is faster. You have not provided a reprex (the "reprex" package may be of help to you with that). However, as Rui confirmed, you are completely wrong as to the NeedsCompilation setting in the package... that is a wild goose chase.

On July 7, 2022 9:48:22 AM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>Dear Jeff,
>                THen why is pbmclapply 3 time faster than mclappy? In the package description of pbmclapply it says: "Needs compilation no". When i ran my code iwith pbmclapply, I did not compile my code. So I do presume that the speed up must in some way connected to this "NeedsCompilation" field. Any thoughts on that?
>
>Yours sincerely
>AKSHAY M KULKARNI
>________________________________
>From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>Sent: Thursday, July 7, 2022 10:10 PM
>To: r-help at r-project.org <r-help at r-project.org>; akshay kulkarni <akshay_e4 at hotmail.com>; Uwe Ligges <ligges at statistik.tu-dortmund.de>; Bert Gunter <bgunter.4567 at gmail.com>
>Cc: R help Mailing list <r-help at r-project.org>
>Subject: Re: [R] byte coding compiling.....
>
>That item refers to the package having some compiled language (e.g. C, C++, Fortran, etc) components. The very fact that it got installed confirms that compilation occurred... it would not be usable otherwise.
>
>On July 7, 2022 8:38:49 AM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>>Dear Uwe,
>>                  I have attached the info from the parallel package description from my Rstudio IDE:
>>
>>Package: parallel
>>Version: 4.1.2
>>Priority: base
>>Title: Support for Parallel computation in R
>>Author: R Core Team
>>Maintainer: R Core Team <do-use-Contact-address at r-project.org>
>>Contact: R-help mailing list <r-help at r-project.org>
>>Description: Support for parallel computation, including by forking
>>   (taken from package multicore), by sockets (taken from package snow)
>>   and random-number generation.
>>License: Part of R 4.1.2
>>Imports: tools, compiler
>>Suggests: methods
>>Enhances: snow, nws, Rmpi
>>NeedsCompilation: yes
>>Built: R 4.1.2; x86_64-w64-mingw32; 2021-11-01 18:38:05 UTC; windows
>>
>>It says: NeedsCompilation: yes
>>
>>How about it?
>>
>>Yours sincerely,
>>AKSHAY M KULKARNI
>>
>>________________________________
>>From: Uwe Ligges <ligges at statistik.tu-dortmund.de>
>>Sent: Thursday, July 7, 2022 4:08 PM
>>To: akshay kulkarni <akshay_e4 at hotmail.com>; Bert Gunter <bgunter.4567 at gmail.com>
>>Cc: R help Mailing list <r-help at r-project.org>
>>Subject: Re: [R] byte coding compiling.....
>>
>>
>>
>>On 06.07.2022 19:54, akshay kulkarni wrote:
>>> Dear Bert,
>>>                   Thanks for your reply...
>>>
>>> So
>>>> cmpfun(mclapply)
>>
>>mclapply is already byte compiled as it is in a package.
>>
>>You may want to
>>cmpfun(yourFunction)
>>the function that you use in the mclapply call.
>>
>>Best,
>>Uwe Ligges
>>
>>
>>>
>>> should  do the job right?
>>>
>>> By the by, how can I give a reprex? Reprex of the code that I am giving to mclapply (as FUN argument)?
>>>
>>> Yours sincerely,
>>> AKSHAY M KULKARNI
>>> ________________________________
>>> From: Bert Gunter <bgunter.4567 at gmail.com>
>>> Sent: Wednesday, July 6, 2022 10:32 PM
>>> To: akshay kulkarni <akshay_e4 at hotmail.com>
>>> Cc: R help Mailing list <r-help at r-project.org>
>>> Subject: Re: [R] byte coding compiling.....
>>>
>>> Unlikely
>>>
>>> See here:
>>>   https://www.r-bloggers.com/2017/08/how-to-make-best-use-of-the-byte-compiler-in-r/
>>>
>>> Byte code compilation should be automatic in both cases, as I understand it. Of course, I could be wrong due to special features of parallel  programming, etc.
>>>
>>> A reprex might be helpful here.
>>>
>>> Cheers,
>>> Bert
>>>
>>>
>>> On Wed, Jul 6, 2022, 7:29 PM akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>> wrote:
>>> Dear members,
>>>                            I am using pbmclapply, the progress bar version of mclapply, from the parallel package. The point is, pbmclapply is three times faster than mclapply, and I think the most probable reason would be that pbmclapply is byte code compiled (I can think of no other reason).
>>>
>>> I know the cmpfun function from compiler package. If I do:
>>>
>>>> cmpfun(mclapply)
>>>
>>> will the job be done? The point is mclapply may look for other functions in the parallel package. So I have to compile the whole package right? How do you do that? or in general, how do you byte code compile a whole package?
>>>
>>> Thanking you,
>>> Yours sincerely,
>>> AKSHAY M KULKARNI
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>        [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>       [[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>--
>Sent from my phone. Please excuse my brevity.

-- 
Sent from my phone. Please excuse my brevity.


From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Jul  7 19:35:05 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 7 Jul 2022 17:35:05 +0000
Subject: [R] byte coding compiling.....
In-Reply-To: <1D1E6315-8288-49C8-A15D-ECBD4E060CFE@dcn.davis.ca.us>
References: <SL2P216MB1561B2FC1E68A2ADC116E3A9C8809@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <CAGxFJbQgh3P7314CJY5ZpwyGpkJt7w7mfZGVTtLKs0Bze=XKAQ@mail.gmail.com>
 <PU4P216MB1568D049849A4F1EC8A0BAEFC8809@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <925b6245-0fb2-b2b0-8a3e-832ca145afff@statistik.tu-dortmund.de>
 <PU4P216MB156808C6AF30820D91E7E559C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <52AEDAC3-F513-4247-B315-CE06BFA92AB1@dcn.davis.ca.us>
 <PU4P216MB1568D5C1C17BA6D81CC08550C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <1D1E6315-8288-49C8-A15D-ECBD4E060CFE@dcn.davis.ca.us>
Message-ID: <PU4P216MB156867C60B04746F1138F503C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear Jeff,
               Thanks for the reply....can you please refer some online resources for  getting the reason why pbmclapply is more faster? There should be some reason right?  If a reprex is essential, I will provide when the code is ready(I am still working on it). Thanks in advance...

Yours sincerely,
AKSHAY M KULKARNI

________________________________
From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Sent: Thursday, July 7, 2022 10:53 PM
To: akshay kulkarni <akshay_e4 at hotmail.com>; r-help at r-project.org <r-help at r-project.org>; Uwe Ligges <ligges at statistik.tu-dortmund.de>; Bert Gunter <bgunter.4567 at gmail.com>
Subject: Re: [R] byte coding compiling.....

I don't know why one is faster. You have not provided a reprex (the "reprex" package may be of help to you with that). However, as Rui confirmed, you are completely wrong as to the NeedsCompilation setting in the package... that is a wild goose chase.

On July 7, 2022 9:48:22 AM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>Dear Jeff,
>                THen why is pbmclapply 3 time faster than mclappy? In the package description of pbmclapply it says: "Needs compilation no". When i ran my code iwith pbmclapply, I did not compile my code. So I do presume that the speed up must in some way connected to this "NeedsCompilation" field. Any thoughts on that?
>
>Yours sincerely
>AKSHAY M KULKARNI
>________________________________
>From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>Sent: Thursday, July 7, 2022 10:10 PM
>To: r-help at r-project.org <r-help at r-project.org>; akshay kulkarni <akshay_e4 at hotmail.com>; Uwe Ligges <ligges at statistik.tu-dortmund.de>; Bert Gunter <bgunter.4567 at gmail.com>
>Cc: R help Mailing list <r-help at r-project.org>
>Subject: Re: [R] byte coding compiling.....
>
>That item refers to the package having some compiled language (e.g. C, C++, Fortran, etc) components. The very fact that it got installed confirms that compilation occurred... it would not be usable otherwise.
>
>On July 7, 2022 8:38:49 AM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>>Dear Uwe,
>>                  I have attached the info from the parallel package description from my Rstudio IDE:
>>
>>Package: parallel
>>Version: 4.1.2
>>Priority: base
>>Title: Support for Parallel computation in R
>>Author: R Core Team
>>Maintainer: R Core Team <do-use-Contact-address at r-project.org>
>>Contact: R-help mailing list <r-help at r-project.org>
>>Description: Support for parallel computation, including by forking
>>   (taken from package multicore), by sockets (taken from package snow)
>>   and random-number generation.
>>License: Part of R 4.1.2
>>Imports: tools, compiler
>>Suggests: methods
>>Enhances: snow, nws, Rmpi
>>NeedsCompilation: yes
>>Built: R 4.1.2; x86_64-w64-mingw32; 2021-11-01 18:38:05 UTC; windows
>>
>>It says: NeedsCompilation: yes
>>
>>How about it?
>>
>>Yours sincerely,
>>AKSHAY M KULKARNI
>>
>>________________________________
>>From: Uwe Ligges <ligges at statistik.tu-dortmund.de>
>>Sent: Thursday, July 7, 2022 4:08 PM
>>To: akshay kulkarni <akshay_e4 at hotmail.com>; Bert Gunter <bgunter.4567 at gmail.com>
>>Cc: R help Mailing list <r-help at r-project.org>
>>Subject: Re: [R] byte coding compiling.....
>>
>>
>>
>>On 06.07.2022 19:54, akshay kulkarni wrote:
>>> Dear Bert,
>>>                   Thanks for your reply...
>>>
>>> So
>>>> cmpfun(mclapply)
>>
>>mclapply is already byte compiled as it is in a package.
>>
>>You may want to
>>cmpfun(yourFunction)
>>the function that you use in the mclapply call.
>>
>>Best,
>>Uwe Ligges
>>
>>
>>>
>>> should  do the job right?
>>>
>>> By the by, how can I give a reprex? Reprex of the code that I am giving to mclapply (as FUN argument)?
>>>
>>> Yours sincerely,
>>> AKSHAY M KULKARNI
>>> ________________________________
>>> From: Bert Gunter <bgunter.4567 at gmail.com>
>>> Sent: Wednesday, July 6, 2022 10:32 PM
>>> To: akshay kulkarni <akshay_e4 at hotmail.com>
>>> Cc: R help Mailing list <r-help at r-project.org>
>>> Subject: Re: [R] byte coding compiling.....
>>>
>>> Unlikely
>>>
>>> See here:
>>>   https://www.r-bloggers.com/2017/08/how-to-make-best-use-of-the-byte-compiler-in-r/
>>>
>>> Byte code compilation should be automatic in both cases, as I understand it. Of course, I could be wrong due to special features of parallel  programming, etc.
>>>
>>> A reprex might be helpful here.
>>>
>>> Cheers,
>>> Bert
>>>
>>>
>>> On Wed, Jul 6, 2022, 7:29 PM akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>> wrote:
>>> Dear members,
>>>                            I am using pbmclapply, the progress bar version of mclapply, from the parallel package. The point is, pbmclapply is three times faster than mclapply, and I think the most probable reason would be that pbmclapply is byte code compiled (I can think of no other reason).
>>>
>>> I know the cmpfun function from compiler package. If I do:
>>>
>>>> cmpfun(mclapply)
>>>
>>> will the job be done? The point is mclapply may look for other functions in the parallel package. So I have to compile the whole package right? How do you do that? or in general, how do you byte code compile a whole package?
>>>
>>> Thanking you,
>>> Yours sincerely,
>>> AKSHAY M KULKARNI
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>        [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>       [[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>--
>Sent from my phone. Please excuse my brevity.

--
Sent from my phone. Please excuse my brevity.

	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Thu Jul  7 19:45:33 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Thu, 7 Jul 2022 13:45:33 -0400
Subject: [R] byte coding compiling.....
In-Reply-To: <6478_1657212557_267GnG9M004296_PU4P216MB1568D5C1C17BA6D81CC08550C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB1561B2FC1E68A2ADC116E3A9C8809@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <CAGxFJbQgh3P7314CJY5ZpwyGpkJt7w7mfZGVTtLKs0Bze=XKAQ@mail.gmail.com>
 <PU4P216MB1568D049849A4F1EC8A0BAEFC8809@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <925b6245-0fb2-b2b0-8a3e-832ca145afff@statistik.tu-dortmund.de>
 <PU4P216MB156808C6AF30820D91E7E559C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <52AEDAC3-F513-4247-B315-CE06BFA92AB1@dcn.davis.ca.us>
 <6478_1657212557_267GnG9M004296_PU4P216MB1568D5C1C17BA6D81CC08550C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <c9dd9d3e-2e11-2f79-9abf-d56931253af0@mcmaster.ca>

Dear Akshay,

All of this seems very confused, and you don't appear to attend to 
people's attempts to help you:

(1) All packages on CRAN are byte-compiled.

(2) The NeedsCompilation field in the package DESCRIPTION file refers 
not to byte-compilation but to the presence of C, C++, or Fortran code 
in the package, which needs to be compiled in the traditional sense.

(3) Oddly, the current version 1.5.1 of the pbmcapply package (not 
pbmclapply, which is a function in pbmcapply) does include C code and 
therefore sets NeedsCompilation to true (see 
<https://cran.r-project.org/web/packages/pbmcapply/index.html>).

(4) Most oddly of all, the pbmclapply() function calls the mclappy() 
function in the parallel package and indeed is described in ?pbmclapply 
as "a wrapper around the mclapply function." Simply look at the code for 
pbmclapply(). It is therefore very curious that pbmclapply() is faster 
than mclapply(), and in the absence of a reproducible example 
illustrating this phenomenon, I doubt that anyone will be able to tell 
you why. It might also help to know something about your system, as 
reported by Sys.info(). (My apologies if you've already reported that.)

Best,
  John


On 2022-07-07 12:48 p.m., akshay kulkarni wrote:
> Dear Jeff,
>                  THen why is pbmclapply 3 time faster than mclappy? In the package description of pbmclapply it says: "Needs compilation no". When i ran my code iwith pbmclapply, I did not compile my code. So I do presume that the speed up must in some way connected to this "NeedsCompilation" field. Any thoughts on that?
> 
> Yours sincerely
> AKSHAY M KULKARNI
> ________________________________
> From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> Sent: Thursday, July 7, 2022 10:10 PM
> To: r-help at r-project.org <r-help at r-project.org>; akshay kulkarni <akshay_e4 at hotmail.com>; Uwe Ligges <ligges at statistik.tu-dortmund.de>; Bert Gunter <bgunter.4567 at gmail.com>
> Cc: R help Mailing list <r-help at r-project.org>
> Subject: Re: [R] byte coding compiling.....
> 
> That item refers to the package having some compiled language (e.g. C, C++, Fortran, etc) components. The very fact that it got installed confirms that compilation occurred... it would not be usable otherwise.
> 
> On July 7, 2022 8:38:49 AM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>> Dear Uwe,
>>                   I have attached the info from the parallel package description from my Rstudio IDE:
>>
>> Package: parallel
>> Version: 4.1.2
>> Priority: base
>> Title: Support for Parallel computation in R
>> Author: R Core Team
>> Maintainer: R Core Team <do-use-Contact-address at r-project.org>
>> Contact: R-help mailing list <r-help at r-project.org>
>> Description: Support for parallel computation, including by forking
>>    (taken from package multicore), by sockets (taken from package snow)
>>    and random-number generation.
>> License: Part of R 4.1.2
>> Imports: tools, compiler
>> Suggests: methods
>> Enhances: snow, nws, Rmpi
>> NeedsCompilation: yes
>> Built: R 4.1.2; x86_64-w64-mingw32; 2021-11-01 18:38:05 UTC; windows
>>
>> It says: NeedsCompilation: yes
>>
>> How about it?
>>
>> Yours sincerely,
>> AKSHAY M KULKARNI
>>
>> ________________________________
>> From: Uwe Ligges <ligges at statistik.tu-dortmund.de>
>> Sent: Thursday, July 7, 2022 4:08 PM
>> To: akshay kulkarni <akshay_e4 at hotmail.com>; Bert Gunter <bgunter.4567 at gmail.com>
>> Cc: R help Mailing list <r-help at r-project.org>
>> Subject: Re: [R] byte coding compiling.....
>>
>>
>>
>> On 06.07.2022 19:54, akshay kulkarni wrote:
>>> Dear Bert,
>>>                    Thanks for your reply...
>>>
>>> So
>>>> cmpfun(mclapply)
>>
>> mclapply is already byte compiled as it is in a package.
>>
>> You may want to
>> cmpfun(yourFunction)
>> the function that you use in the mclapply call.
>>
>> Best,
>> Uwe Ligges
>>
>>
>>>
>>> should  do the job right?
>>>
>>> By the by, how can I give a reprex? Reprex of the code that I am giving to mclapply (as FUN argument)?
>>>
>>> Yours sincerely,
>>> AKSHAY M KULKARNI
>>> ________________________________
>>> From: Bert Gunter <bgunter.4567 at gmail.com>
>>> Sent: Wednesday, July 6, 2022 10:32 PM
>>> To: akshay kulkarni <akshay_e4 at hotmail.com>
>>> Cc: R help Mailing list <r-help at r-project.org>
>>> Subject: Re: [R] byte coding compiling.....
>>>
>>> Unlikely
>>>
>>> See here:
>>>    https://www.r-bloggers.com/2017/08/how-to-make-best-use-of-the-byte-compiler-in-r/
>>>
>>> Byte code compilation should be automatic in both cases, as I understand it. Of course, I could be wrong due to special features of parallel  programming, etc.
>>>
>>> A reprex might be helpful here.
>>>
>>> Cheers,
>>> Bert
>>>
>>>
>>> On Wed, Jul 6, 2022, 7:29 PM akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>> wrote:
>>> Dear members,
>>>                             I am using pbmclapply, the progress bar version of mclapply, from the parallel package. The point is, pbmclapply is three times faster than mclapply, and I think the most probable reason would be that pbmclapply is byte code compiled (I can think of no other reason).
>>>
>>> I know the cmpfun function from compiler package. If I do:
>>>
>>>> cmpfun(mclapply)
>>>
>>> will the job be done? The point is mclapply may look for other functions in the parallel package. So I have to compile the whole package right? How do you do that? or in general, how do you byte code compile a whole package?
>>>
>>> Thanking you,
>>> Yours sincerely,
>>> AKSHAY M KULKARNI
>>>
>>>           [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>        [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Sent from my phone. Please excuse my brevity.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/


From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Jul  7 19:55:14 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 7 Jul 2022 17:55:14 +0000
Subject: [R] byte coding compiling.....
In-Reply-To: <c9dd9d3e-2e11-2f79-9abf-d56931253af0@mcmaster.ca>
References: <SL2P216MB1561B2FC1E68A2ADC116E3A9C8809@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <CAGxFJbQgh3P7314CJY5ZpwyGpkJt7w7mfZGVTtLKs0Bze=XKAQ@mail.gmail.com>
 <PU4P216MB1568D049849A4F1EC8A0BAEFC8809@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <925b6245-0fb2-b2b0-8a3e-832ca145afff@statistik.tu-dortmund.de>
 <PU4P216MB156808C6AF30820D91E7E559C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <52AEDAC3-F513-4247-B315-CE06BFA92AB1@dcn.davis.ca.us>
 <6478_1657212557_267GnG9M004296_PU4P216MB1568D5C1C17BA6D81CC08550C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <c9dd9d3e-2e11-2f79-9abf-d56931253af0@mcmaster.ca>
Message-ID: <PU4P216MB1568DB457E4342F519C1BA67C8839@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear John
                 Thanks for the reply....I will give the reprex when it is ready...

Yours sincerely,
AKSHAY M KULKARNI
________________________________
From: John Fox <jfox at mcmaster.ca>
Sent: Thursday, July 7, 2022 11:15 PM
To: akshay kulkarni <akshay_e4 at hotmail.com>
Cc: r-help at r-project.org <r-help at r-project.org>; Uwe Ligges <ligges at statistik.tu-dortmund.de>; Jeff Newmiller <jdnewmil at dcn.davis.ca.us>; Bert Gunter <bgunter.4567 at gmail.com>
Subject: Re: [R] byte coding compiling.....

Dear Akshay,

All of this seems very confused, and you don't appear to attend to
people's attempts to help you:

(1) All packages on CRAN are byte-compiled.

(2) The NeedsCompilation field in the package DESCRIPTION file refers
not to byte-compilation but to the presence of C, C++, or Fortran code
in the package, which needs to be compiled in the traditional sense.

(3) Oddly, the current version 1.5.1 of the pbmcapply package (not
pbmclapply, which is a function in pbmcapply) does include C code and
therefore sets NeedsCompilation to true (see
<https://cran.r-project.org/web/packages/pbmcapply/index.html>).

(4) Most oddly of all, the pbmclapply() function calls the mclappy()
function in the parallel package and indeed is described in ?pbmclapply
as "a wrapper around the mclapply function." Simply look at the code for
pbmclapply(). It is therefore very curious that pbmclapply() is faster
than mclapply(), and in the absence of a reproducible example
illustrating this phenomenon, I doubt that anyone will be able to tell
you why. It might also help to know something about your system, as
reported by Sys.info(). (My apologies if you've already reported that.)

Best,
  John


On 2022-07-07 12:48 p.m., akshay kulkarni wrote:
> Dear Jeff,
>                  THen why is pbmclapply 3 time faster than mclappy? In the package description of pbmclapply it says: "Needs compilation no". When i ran my code iwith pbmclapply, I did not compile my code. So I do presume that the speed up must in some way connected to this "NeedsCompilation" field. Any thoughts on that?
>
> Yours sincerely
> AKSHAY M KULKARNI
> ________________________________
> From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> Sent: Thursday, July 7, 2022 10:10 PM
> To: r-help at r-project.org <r-help at r-project.org>; akshay kulkarni <akshay_e4 at hotmail.com>; Uwe Ligges <ligges at statistik.tu-dortmund.de>; Bert Gunter <bgunter.4567 at gmail.com>
> Cc: R help Mailing list <r-help at r-project.org>
> Subject: Re: [R] byte coding compiling.....
>
> That item refers to the package having some compiled language (e.g. C, C++, Fortran, etc) components. The very fact that it got installed confirms that compilation occurred... it would not be usable otherwise.
>
> On July 7, 2022 8:38:49 AM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>> Dear Uwe,
>>                   I have attached the info from the parallel package description from my Rstudio IDE:
>>
>> Package: parallel
>> Version: 4.1.2
>> Priority: base
>> Title: Support for Parallel computation in R
>> Author: R Core Team
>> Maintainer: R Core Team <do-use-Contact-address at r-project.org>
>> Contact: R-help mailing list <r-help at r-project.org>
>> Description: Support for parallel computation, including by forking
>>    (taken from package multicore), by sockets (taken from package snow)
>>    and random-number generation.
>> License: Part of R 4.1.2
>> Imports: tools, compiler
>> Suggests: methods
>> Enhances: snow, nws, Rmpi
>> NeedsCompilation: yes
>> Built: R 4.1.2; x86_64-w64-mingw32; 2021-11-01 18:38:05 UTC; windows
>>
>> It says: NeedsCompilation: yes
>>
>> How about it?
>>
>> Yours sincerely,
>> AKSHAY M KULKARNI
>>
>> ________________________________
>> From: Uwe Ligges <ligges at statistik.tu-dortmund.de>
>> Sent: Thursday, July 7, 2022 4:08 PM
>> To: akshay kulkarni <akshay_e4 at hotmail.com>; Bert Gunter <bgunter.4567 at gmail.com>
>> Cc: R help Mailing list <r-help at r-project.org>
>> Subject: Re: [R] byte coding compiling.....
>>
>>
>>
>> On 06.07.2022 19:54, akshay kulkarni wrote:
>>> Dear Bert,
>>>                    Thanks for your reply...
>>>
>>> So
>>>> cmpfun(mclapply)
>>
>> mclapply is already byte compiled as it is in a package.
>>
>> You may want to
>> cmpfun(yourFunction)
>> the function that you use in the mclapply call.
>>
>> Best,
>> Uwe Ligges
>>
>>
>>>
>>> should  do the job right?
>>>
>>> By the by, how can I give a reprex? Reprex of the code that I am giving to mclapply (as FUN argument)?
>>>
>>> Yours sincerely,
>>> AKSHAY M KULKARNI
>>> ________________________________
>>> From: Bert Gunter <bgunter.4567 at gmail.com>
>>> Sent: Wednesday, July 6, 2022 10:32 PM
>>> To: akshay kulkarni <akshay_e4 at hotmail.com>
>>> Cc: R help Mailing list <r-help at r-project.org>
>>> Subject: Re: [R] byte coding compiling.....
>>>
>>> Unlikely
>>>
>>> See here:
>>>    https://www.r-bloggers.com/2017/08/how-to-make-best-use-of-the-byte-compiler-in-r/
>>>
>>> Byte code compilation should be automatic in both cases, as I understand it. Of course, I could be wrong due to special features of parallel  programming, etc.
>>>
>>> A reprex might be helpful here.
>>>
>>> Cheers,
>>> Bert
>>>
>>>
>>> On Wed, Jul 6, 2022, 7:29 PM akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>> wrote:
>>> Dear members,
>>>                             I am using pbmclapply, the progress bar version of mclapply, from the parallel package. The point is, pbmclapply is three times faster than mclapply, and I think the most probable reason would be that pbmclapply is byte code compiled (I can think of no other reason).
>>>
>>> I know the cmpfun function from compiler package. If I do:
>>>
>>>> cmpfun(mclapply)
>>>
>>> will the job be done? The point is mclapply may look for other functions in the parallel package. So I have to compile the whole package right? How do you do that? or in general, how do you byte code compile a whole package?
>>>
>>> Thanking you,
>>> Yours sincerely,
>>> AKSHAY M KULKARNI
>>>
>>>           [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>        [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
--
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/


	[[alternative HTML version deleted]]


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Fri Jul  8 18:56:52 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Fri, 8 Jul 2022 21:56:52 +0500
Subject: [R] Please guide
Message-ID: <CAMfKi3KtWtuH-Zc6qhDcrN8bwhEeTiD9PbZB_m1L_XbejCR0sQ@mail.gmail.com>

*Dear Experts,*
*Greetings from Pakistan*.
*When I run the following code in R*
library(frequencyConnectedness)
library(readxl)
##Add data here##
Data <- read_excel("Data_oil_agri.xlsx", sheet = "Sheet1")
Data <- Data[,2:22]
Data=na.omit(Data)
Bnames=colnames(Data)

lags=VARselect(Data, lag.max = 12, type = "const")  ## "const", "trend",
"both", "none"
p=lags$selection[[3]]

est <- VAR(Data, p = p, type = "const")
sp <- spilloverDY12(est, n.ahead = 10, no.corr = F)

params_est = list(p = p, type = "const")
sp_roll <- spilloverRollingDY12(Data, n.ahead = 10, no.corr = F, "VAR",
params_est = params_est, window = 260)
bounds <- c(pi+0.00001, pi/5, 0)
sp_bk <- spilloverBK12(est, n.ahead = 100, no.corr = F, partition = bounds)

sp_bk_roll <- spilloverRollingBK12(Data, n.ahead = 100, no.corr = F,
func_est = "VAR", params_est = params_est, window = 260, partition = bounds)

**Till now the code works well. After that, when I run the following:*

con      = as.matrix(read_excel("DY_Table.xlsx")[,-1])*100

*I face the following error:*
*Error in* *`vectbl_as_col_location()`:*
! Can't negate columns past the end.
? Location 1 doesn't exist.
? There are only 0 columns.
Run `rlang::last_error()` to see where the error occurred.

Kindly please guide me.

Regards
Muhammad Zubair Chishti
Ph.D. Student
School of Business,
Zhengzhou University, Henan, China.
My Google scholar link:
https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
My ResearchGate link: https://www.researchgate.net/profile/Muhammad-Chishti

	[[alternative HTML version deleted]]


From tg@77m @end|ng |rom y@hoo@com  Sat Jul  9 17:11:50 2022
From: tg@77m @end|ng |rom y@hoo@com (Thomas Subia)
Date: Sat, 9 Jul 2022 15:11:50 +0000 (UTC)
Subject: [R] Geom ribbon
References: <67192773.4080716.1657379510887.ref@mail.yahoo.com>
Message-ID: <67192773.4080716.1657379510887@mail.yahoo.com>

Colleagues,
I am plotting tolerance ratios by location and dateMy reproducible code is:
library(ggplot2)
library(cowplot)

# tolerance ratio 
day1 <- runif(3)
day2 <- runif(3)
day3 <-runif(3)
location <- c("A","B","C")

df <- data.frame(location,day1,day2,day3)

# plot of tolerance ratio by location and day
myplot <-ggplot(df,aes(location,day1))+geom_point()+
? geom_point(aes(location,day2))+
? ylab("Tolerance ratio")+
? xlab("Location")+
? theme_cowplot()+
? ggtitle("Tolerance ratio by location and day")
I need to shade the areas between the following y data

Tolerance
Ratio > 0.6??? ????????????????????area shaded = red??? ????Unacceptable
0.3 <= Ratio <= 0.6?????? ????light red???????????????????? Inadequate
0.25 <= Ratio < 0.3??????? ????yellow?????????????????????? Marginal0.2 <= Ratio < 0.25??????? ????light green???????????????? Good
Ratio < 0.2?????????????????????? ????green??????????????????????? Great
I believe that using geom_ribbon may work for this.
Any suggestions for this would be appreciated.
All the best,
Thomas Subia


	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Sat Jul  9 18:46:49 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Sat, 9 Jul 2022 16:46:49 +0000
Subject: [R] printing with bothe print and cat...
Message-ID: <PU4P216MB1568F36F3D77861C985FFF5AC8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear members,
                         I have the following code:

testprint <- function() {

  for(i in 1:5) {for(j in 1:5)
  {cat(j)}
    print(i)}
}

And the output is:

> testprint()
12345[1] 1
12345[1] 2
12345[1] 3
12345[1] 4
12345[1] 5

Any idea on how to remove the [1] from the output, and give spaces in the cat output? The desired output is:

1 2 3 4 5      1
1 2 3 4 5      2
1 2 3 4 5      3
1 2 3 4 5      4
1 2 3 4 5      5
 Many thanks in advance.....

THanking you,
Yours  sincreely,
AKSHAY M KULKARNI

	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Sat Jul  9 18:58:00 2022
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Sat, 9 Jul 2022 09:58:00 -0700
Subject: [R] printing with bothe print and cat...
In-Reply-To: <PU4P216MB1568F36F3D77861C985FFF5AC8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <PU4P216MB1568F36F3D77861C985FFF5AC8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <4FCF8673-8308-4A08-B1EC-AA4EA6D0C2A0@comcast.net>

Skip the for loops:

cat(paste( seq(1:5), ?    ?, 1:5) )

? 
David

Sent from my iPhone

> On Jul 9, 2022, at 9:47 AM, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
> 
> ?Dear members,
>                         I have the following code:
> 
> testprint <- function() {
> 
>  for(i in 1:5) {for(j in 1:5)
>  {cat(j)}
>    print(i)}
> }
> 
> And the output is:
> 
>> testprint()
> 12345[1] 1
> 12345[1] 2
> 12345[1] 3
> 12345[1] 4
> 12345[1] 5
> 
> Any idea on how to remove the [1] from the output, and give spaces in the cat output? The desired output is:
> 
> 1 2 3 4 5      1
> 1 2 3 4 5      2
> 1 2 3 4 5      3
> 1 2 3 4 5      4
> 1 2 3 4 5      5
> Many thanks in advance.....
> 
> THanking you,
> Yours  sincreely,
> AKSHAY M KULKARNI
> 
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rmh @end|ng |rom temp|e@edu  Sat Jul  9 19:01:08 2022
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Sat, 9 Jul 2022 17:01:08 +0000
Subject: [R] [External]  printing with bothe print and cat...
In-Reply-To: <PU4P216MB1568F36F3D77861C985FFF5AC8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <PU4P216MB1568F36F3D77861C985FFF5AC8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <BL1PR11MB52397F72056AFB2E3416E558D2859@BL1PR11MB5239.namprd11.prod.outlook.com>

for(i in 1:5) {
       for(j in 1:5) cat(j, " ")
        cat(" ", i, "\n")}

print() puts out the [1], so don?t use print().
cat puts out only what you tell it.  So if you want a space, you must include the space.

From: R-help <r-help-bounces at r-project.org> on behalf of akshay kulkarni <akshay_e4 at hotmail.com>
Date: Saturday, July 09, 2022 at 12:47
To: R help Mailing list <r-help at r-project.org>
Subject: [External] [R] printing with bothe print and cat...
Dear members,
                         I have the following code:

testprint <- function() {

  for(i in 1:5) {for(j in 1:5)
  {cat(j)}
    print(i)}
}

And the output is:

> testprint()
12345[1] 1
12345[1] 2
12345[1] 3
12345[1] 4
12345[1] 5

Any idea on how to remove the [1] from the output, and give spaces in the cat output? The desired output is:

1 2 3 4 5      1
1 2 3 4 5      2
1 2 3 4 5      3
1 2 3 4 5      4
1 2 3 4 5      5
 Many thanks in advance.....

THanking you,
Yours  sincreely,
AKSHAY M KULKARNI

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Crmh%40temple.edu%7C9409e175bc7045c094c408da61caa961%7C716e81efb52244738e3110bd02ccf6e5%7C0%7C0%7C637929820309666159%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=dKfATg8gOlTDpLqxUFgUSJ1l8%2Bnr3qNakRxPZcFSRdE%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Crmh%40temple.edu%7C9409e175bc7045c094c408da61caa961%7C716e81efb52244738e3110bd02ccf6e5%7C0%7C0%7C637929820309666159%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=cFyypcyUFM3eRJWnJ%2FFzUkcc%2FtEjAYl4rJzOkPo29z0%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Sat Jul  9 19:00:44 2022
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Sat, 9 Jul 2022 10:00:44 -0700
Subject: [R] printing with bothe print and cat...
In-Reply-To: <4FCF8673-8308-4A08-B1EC-AA4EA6D0C2A0@comcast.net>
References: <4FCF8673-8308-4A08-B1EC-AA4EA6D0C2A0@comcast.net>
Message-ID: <84345DCC-D8FD-464B-BDE4-353385D7B6F8@comcast.net>

If spaces needed. In first sequences then 

paste( 1:5, collapse=? ?)

Sent from my iPhone

> On Jul 9, 2022, at 9:59 AM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> ?Skip the for loops:
> 
> cat(paste( seq(1:5), ?    ?, 1:5) )
> 
> ? 
> David
> 
> Sent from my iPhone
> 
>> On Jul 9, 2022, at 9:47 AM, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>> 
>> ?Dear members,
>>                        I have the following code:
>> 
>> testprint <- function() {
>> 
>> for(i in 1:5) {for(j in 1:5)
>> {cat(j)}
>>   print(i)}
>> }
>> 
>> And the output is:
>> 
>>> testprint()
>> 12345[1] 1
>> 12345[1] 2
>> 12345[1] 3
>> 12345[1] 4
>> 12345[1] 5
>> 
>> Any idea on how to remove the [1] from the output, and give spaces in the cat output? The desired output is:
>> 
>> 1 2 3 4 5      1
>> 1 2 3 4 5      2
>> 1 2 3 4 5      3
>> 1 2 3 4 5      4
>> 1 2 3 4 5      5
>> Many thanks in advance.....
>> 
>> THanking you,
>> Yours  sincreely,
>> AKSHAY M KULKARNI
>> 
>>   [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From @k@h@y_e4 @end|ng |rom hotm@||@com  Sat Jul  9 19:02:20 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Sat, 9 Jul 2022 17:02:20 +0000
Subject: [R] printing with bothe print and cat...
In-Reply-To: <4FCF8673-8308-4A08-B1EC-AA4EA6D0C2A0@comcast.net>
References: <PU4P216MB1568F36F3D77861C985FFF5AC8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <4FCF8673-8308-4A08-B1EC-AA4EA6D0C2A0@comcast.net>
Message-ID: <PU4P216MB15689EED85F17F711D8340A6C8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear David,
                    THe code given was a reprex, and unfortunately, I cannot skip the for loops: Its a very big  web scraping code. Any alternative?

Many thanks in advance...

Yours sincrely,
AKSHAY M KULKARNI

________________________________
From: David Winsemius <dwinsemius at comcast.net>
Sent: Saturday, July 9, 2022 10:28 PM
To: akshay kulkarni <akshay_e4 at hotmail.com>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] printing with bothe print and cat...

Skip the for loops:

cat(paste( seq(1:5), ?    ?, 1:5) )

?
David

Sent from my iPhone

> On Jul 9, 2022, at 9:47 AM, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>
> ?Dear members,
>                         I have the following code:
>
> testprint <- function() {
>
>  for(i in 1:5) {for(j in 1:5)
>  {cat(j)}
>    print(i)}
> }
>
> And the output is:
>
>> testprint()
> 12345[1] 1
> 12345[1] 2
> 12345[1] 3
> 12345[1] 4
> 12345[1] 5
>
> Any idea on how to remove the [1] from the output, and give spaces in the cat output? The desired output is:
>
> 1 2 3 4 5      1
> 1 2 3 4 5      2
> 1 2 3 4 5      3
> 1 2 3 4 5      4
> 1 2 3 4 5      5
> Many thanks in advance.....
>
> THanking you,
> Yours  sincreely,
> AKSHAY M KULKARNI
>
>    [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Sat Jul  9 19:09:32 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Sat, 9 Jul 2022 17:09:32 +0000
Subject: [R] printing with bothe print and cat...
In-Reply-To: <84345DCC-D8FD-464B-BDE4-353385D7B6F8@comcast.net>
References: <4FCF8673-8308-4A08-B1EC-AA4EA6D0C2A0@comcast.net>
 <84345DCC-D8FD-464B-BDE4-353385D7B6F8@comcast.net>
Message-ID: <PU4P216MB1568DABBAC3E074B8C2AFDACC8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear David,
                       Thanks ...

Yours sinecrely,
AKSHAY M KULKARNI
________________________________
From: David Winsemius <dwinsemius at comcast.net>
Sent: Saturday, July 9, 2022 10:30 PM
To: akshay kulkarni <akshay_e4 at hotmail.com>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] printing with bothe print and cat...

If spaces needed. In first sequences then

paste( 1:5, collapse=? ?)

Sent from my iPhone

> On Jul 9, 2022, at 9:59 AM, David Winsemius <dwinsemius at comcast.net> wrote:
>
> ?Skip the for loops:
>
> cat(paste( seq(1:5), ?    ?, 1:5) )
>
> ?
> David
>
> Sent from my iPhone
>
>> On Jul 9, 2022, at 9:47 AM, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>>
>> ?Dear members,
>>                        I have the following code:
>>
>> testprint <- function() {
>>
>> for(i in 1:5) {for(j in 1:5)
>> {cat(j)}
>>   print(i)}
>> }
>>
>> And the output is:
>>
>>> testprint()
>> 12345[1] 1
>> 12345[1] 2
>> 12345[1] 3
>> 12345[1] 4
>> 12345[1] 5
>>
>> Any idea on how to remove the [1] from the output, and give spaces in the cat output? The desired output is:
>>
>> 1 2 3 4 5      1
>> 1 2 3 4 5      2
>> 1 2 3 4 5      3
>> 1 2 3 4 5      4
>> 1 2 3 4 5      5
>> Many thanks in advance.....
>>
>> THanking you,
>> Yours  sincreely,
>> AKSHAY M KULKARNI
>>
>>   [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Sat Jul  9 19:08:48 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Sat, 9 Jul 2022 17:08:48 +0000
Subject: [R] [External]  printing with bothe print and cat...
In-Reply-To: <BL1PR11MB52397F72056AFB2E3416E558D2859@BL1PR11MB5239.namprd11.prod.outlook.com>
References: <PU4P216MB1568F36F3D77861C985FFF5AC8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <BL1PR11MB52397F72056AFB2E3416E558D2859@BL1PR11MB5239.namprd11.prod.outlook.com>
Message-ID: <PU4P216MB1568F7BDE04894E4124410A9C8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear Richard,
                      Thanks ....
yours sincerely
AKSHAY M KULKARNI
________________________________
From: Richard M. Heiberger <rmh at temple.edu>
Sent: Saturday, July 9, 2022 10:31 PM
To: akshay kulkarni <akshay_e4 at hotmail.com>; R help Mailing list <r-help at r-project.org>
Subject: Re: [External] [R] printing with bothe print and cat...


for(i in 1:5) {

       for(j in 1:5) cat(j, " ")

        cat(" ", i, "\n")}



print() puts out the [1], so don?t use print().

cat puts out only what you tell it.  So if you want a space, you must include the space.



From: R-help <r-help-bounces at r-project.org> on behalf of akshay kulkarni <akshay_e4 at hotmail.com>
Date: Saturday, July 09, 2022 at 12:47
To: R help Mailing list <r-help at r-project.org>
Subject: [External] [R] printing with bothe print and cat...

Dear members,
                         I have the following code:

testprint <- function() {

  for(i in 1:5) {for(j in 1:5)
  {cat(j)}
    print(i)}
}

And the output is:

> testprint()
12345[1] 1
12345[1] 2
12345[1] 3
12345[1] 4
12345[1] 5

Any idea on how to remove the [1] from the output, and give spaces in the cat output? The desired output is:

1 2 3 4 5      1
1 2 3 4 5      2
1 2 3 4 5      3
1 2 3 4 5      4
1 2 3 4 5      5
 Many thanks in advance.....

THanking you,
Yours  sincreely,
AKSHAY M KULKARNI

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Crmh%40temple.edu%7C9409e175bc7045c094c408da61caa961%7C716e81efb52244738e3110bd02ccf6e5%7C0%7C0%7C637929820309666159%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=dKfATg8gOlTDpLqxUFgUSJ1l8%2Bnr3qNakRxPZcFSRdE%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Crmh%40temple.edu%7C9409e175bc7045c094c408da61caa961%7C716e81efb52244738e3110bd02ccf6e5%7C0%7C0%7C637929820309666159%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=cFyypcyUFM3eRJWnJ%2FFzUkcc%2FtEjAYl4rJzOkPo29z0%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From j@de@shod@@ m@iii@g oii googiem@ii@com  Sat Jul  9 19:27:38 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Sat, 9 Jul 2022 18:27:38 +0100
Subject: [R] Geom ribbon
In-Reply-To: <67192773.4080716.1657379510887@mail.yahoo.com>
References: <67192773.4080716.1657379510887.ref@mail.yahoo.com>
 <67192773.4080716.1657379510887@mail.yahoo.com>
Message-ID: <CANg3_k-92vy6c7CB7kSfhpM2x_NWMzUvJOihmGdYxkdBY7=3Ag@mail.gmail.com>

I'm not sure if geom_ribbon works with categorical data. It didn't
work for me, so I have coded location as a numeric, which works. You
can then manuall re-label the tick marks, as per the code below.
Others may be able to add to the code to add a legend, or propose a
different solution altogether, which may involve having to restructure
your data frame (I'm not an expert in R/ ggplot).

Below is some code. It's not elegant, but it seems to do at least part
of the job...

# tolerance ratio
day1 <- runif(3)
day2 <- runif(3)
day3 <-runif(3)
location <- c(1,2,3)

df <- data.frame(location,day1,day2,day3)
max <- 1.0  # find the maximum y-value in your data. Here, I've
assigned 1 for ease
ggplot()+
  geom_ribbon(aes(ymin = 0, ymax = 0.199999999, x = location), fill = "green") +
  geom_ribbon(aes(ymin = 0.2, ymax = 0.249999999, x = location), fill
= "light green") +
  geom_ribbon(aes(ymin = 0.25, ymax = 0.299999999, x = location), fill
= "yellow") +
  geom_ribbon(aes(ymin = 0.3, ymax = 0.6, x = location), fill =
"orange") +   # "light red " is not recognised as a colour
  geom_ribbon(aes(ymin = 0.6000001, ymax = max, x = location), fill = "red") +
  geom_point(data = df, aes(location, day1))+
  geom_point(aes(location,day2))+
   scale_x_discrete(limits=c("A","B","C"))+
  ylab("Tolerance ratio")+
  xlab("Location")+
  ggtitle("Tolerance ratio by location and day")


Hope this provides at least a start!

Jade

On Sat, 9 Jul 2022 at 16:13, Thomas Subia via R-help
<r-help at r-project.org> wrote:
>
> Colleagues,
> I am plotting tolerance ratios by location and dateMy reproducible code is:
> library(ggplot2)
> library(cowplot)
>
> # tolerance ratio
> day1 <- runif(3)
> day2 <- runif(3)
> day3 <-runif(3)
> location <- c("A","B","C")
>
> df <- data.frame(location,day1,day2,day3)
>
> # plot of tolerance ratio by location and day
> myplot <-ggplot(df,aes(location,day1))+geom_point()+
>   geom_point(aes(location,day2))+
>   ylab("Tolerance ratio")+
>   xlab("Location")+
>   theme_cowplot()+
>   ggtitle("Tolerance ratio by location and day")
> I need to shade the areas between the following y data
>
> Tolerance
> Ratio > 0.6                        area shaded = red        Unacceptable
> 0.3 <= Ratio <= 0.6           light red                     Inadequate
> 0.25 <= Ratio < 0.3            yellow                       Marginal0.2 <= Ratio < 0.25            light green                 Good
> Ratio < 0.2                           green                        Great
> I believe that using geom_ribbon may work for this.
> Any suggestions for this would be appreciated.
> All the best,
> Thomas Subia
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tebert @end|ng |rom u||@edu  Sat Jul  9 19:33:54 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Sat, 9 Jul 2022 17:33:54 +0000
Subject: [R] printing with bothe print and cat...
In-Reply-To: <PU4P216MB15689EED85F17F711D8340A6C8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <PU4P216MB1568F36F3D77861C985FFF5AC8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <4FCF8673-8308-4A08-B1EC-AA4EA6D0C2A0@comcast.net>
 <PU4P216MB15689EED85F17F711D8340A6C8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <BN6PR2201MB155324CBC9C8C91B89C32FF1CF859@BN6PR2201MB1553.namprd22.prod.outlook.com>

Here is an alternative that makes a dataframe and then prints the dataframe.
n_rows <- 5
n_cols <- 5
p_mat<-matrix(0,nrow=n_rows,ncol=n_cols)
for(i in 1:n_rows) {
  for(j in 1:n_cols){
    p_mat[i,j]<-(j)
  }
}
p_df <- data.frame(p_mat,1:n_rows)
print(p_df)


Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of akshay kulkarni
Sent: Saturday, July 9, 2022 1:02 PM
To: David Winsemius <dwinsemius at comcast.net>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] printing with bothe print and cat...

[External Email]

Dear David,
                    THe code given was a reprex, and unfortunately, I cannot skip the for loops: Its a very big  web scraping code. Any alternative?

Many thanks in advance...

Yours sincrely,
AKSHAY M KULKARNI

________________________________
From: David Winsemius <dwinsemius at comcast.net>
Sent: Saturday, July 9, 2022 10:28 PM
To: akshay kulkarni <akshay_e4 at hotmail.com>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] printing with bothe print and cat...

Skip the for loops:

cat(paste( seq(1:5), ?    ?, 1:5) )

?
David

Sent from my iPhone

> On Jul 9, 2022, at 9:47 AM, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>
> ?Dear members,
>                         I have the following code:
>
> testprint <- function() {
>
>  for(i in 1:5) {for(j in 1:5)
>  {cat(j)}
>    print(i)}
> }
>
> And the output is:
>
>> testprint()
> 12345[1] 1
> 12345[1] 2
> 12345[1] 3
> 12345[1] 4
> 12345[1] 5
>
> Any idea on how to remove the [1] from the output, and give spaces in the cat output? The desired output is:
>
> 1 2 3 4 5      1
> 1 2 3 4 5      2
> 1 2 3 4 5      3
> 1 2 3 4 5      4
> 1 2 3 4 5      5
> Many thanks in advance.....
>
> THanking you,
> Yours  sincreely,
> AKSHAY M KULKARNI
>
>    [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIGaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=lceoyahibi7acrpoFW6uDxJ4j_QtB0NYjb7GNZu_e9WpZDZ4hdpYpgvtx
> sQZbqGe&s=m5EK_gr2QGFthC1Q_GqiGjSV05AZSmV-T1kQpq5KlUM&e=
> PLEASE do read the posting guide 
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIGaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=lceoyahibi7acrpoFW6uDxJ4j_QtB0NYjb7GNZu_e9WpZDZ4hdpYpgvt
> xsQZbqGe&s=EdP6JP8fQVHC-xOJfowZ7VlJOlqWwnJBjAi8sZR2Els&e=
> and provide commented, minimal, self-contained, reproducible code.


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIGaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=lceoyahibi7acrpoFW6uDxJ4j_QtB0NYjb7GNZu_e9WpZDZ4hdpYpgvtxsQZbqGe&s=m5EK_gr2QGFthC1Q_GqiGjSV05AZSmV-T1kQpq5KlUM&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIGaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=lceoyahibi7acrpoFW6uDxJ4j_QtB0NYjb7GNZu_e9WpZDZ4hdpYpgvtxsQZbqGe&s=EdP6JP8fQVHC-xOJfowZ7VlJOlqWwnJBjAi8sZR2Els&e=
and provide commented, minimal, self-contained, reproducible code.

From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Sat Jul  9 19:40:29 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Sat, 9 Jul 2022 22:40:29 +0500
Subject: [R] Please guide
In-Reply-To: <CAMfKi3KtWtuH-Zc6qhDcrN8bwhEeTiD9PbZB_m1L_XbejCR0sQ@mail.gmail.com>
References: <CAMfKi3KtWtuH-Zc6qhDcrN8bwhEeTiD9PbZB_m1L_XbejCR0sQ@mail.gmail.com>
Message-ID: <CAMfKi3JaV5jGukKwsvqfeD5rOqu-A7FBhA5PjF+=RFKTgNzbcw@mail.gmail.com>

Dear Experts,
A kind reminder. Please help me.


On Fri, 8 Jul 2022, 21:56 Muhammad Zubair Chishti, <mzchishti at eco.qau.edu.pk>
wrote:

>
> *Dear Experts,*
> *Greetings from Pakistan*.
> *When I run the following code in R*
> library(frequencyConnectedness)
> library(readxl)
> ##Add data here##
> Data <- read_excel("Data_oil_agri.xlsx", sheet = "Sheet1")
> Data <- Data[,2:22]
> Data=na.omit(Data)
> Bnames=colnames(Data)
>
> lags=VARselect(Data, lag.max = 12, type = "const")  ## "const", "trend",
> "both", "none"
> p=lags$selection[[3]]
>
> est <- VAR(Data, p = p, type = "const")
> sp <- spilloverDY12(est, n.ahead = 10, no.corr = F)
>
> params_est = list(p = p, type = "const")
> sp_roll <- spilloverRollingDY12(Data, n.ahead = 10, no.corr = F, "VAR",
> params_est = params_est, window = 260)
> bounds <- c(pi+0.00001, pi/5, 0)
> sp_bk <- spilloverBK12(est, n.ahead = 100, no.corr = F, partition = bounds)
>
> sp_bk_roll <- spilloverRollingBK12(Data, n.ahead = 100, no.corr = F,
> func_est = "VAR", params_est = params_est, window = 260, partition = bounds)
>
> **Till now the code works well. After that, when I run the following:*
>
> con      = as.matrix(read_excel("DY_Table.xlsx")[,-1])*100
>
> *I face the following error:*
> *Error in* *`vectbl_as_col_location()`:*
> ! Can't negate columns past the end.
> ? Location 1 doesn't exist.
> ? There are only 0 columns.
> Run `rlang::last_error()` to see where the error occurred.
>
> Kindly please guide me.
>
> Regards
> Muhammad Zubair Chishti
> Ph.D. Student
> School of Business,
> Zhengzhou University, Henan, China.
> My Google scholar link:
> https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
> My ResearchGate link:
> https://www.researchgate.net/profile/Muhammad-Chishti
>

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Sat Jul  9 19:40:52 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Sat, 9 Jul 2022 17:40:52 +0000
Subject: [R] printing with bothe print and cat...
In-Reply-To: <BN6PR2201MB155324CBC9C8C91B89C32FF1CF859@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <PU4P216MB1568F36F3D77861C985FFF5AC8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <4FCF8673-8308-4A08-B1EC-AA4EA6D0C2A0@comcast.net>
 <PU4P216MB15689EED85F17F711D8340A6C8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <BN6PR2201MB155324CBC9C8C91B89C32FF1CF859@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <PU4P216MB1568A309F94B33EC574B43BCC8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear Tim,
                   Many thanks...

Yours sincerely
AKSHAYM KULKARNI
________________________________
From: Ebert,Timothy Aaron <tebert at ufl.edu>
Sent: Saturday, July 9, 2022 11:03 PM
To: akshay kulkarni <akshay_e4 at hotmail.com>; David Winsemius <dwinsemius at comcast.net>
Cc: R help Mailing list <r-help at r-project.org>
Subject: RE: [R] printing with bothe print and cat...

Here is an alternative that makes a dataframe and then prints the dataframe.
n_rows <- 5
n_cols <- 5
p_mat<-matrix(0,nrow=n_rows,ncol=n_cols)
for(i in 1:n_rows) {
  for(j in 1:n_cols){
    p_mat[i,j]<-(j)
  }
}
p_df <- data.frame(p_mat,1:n_rows)
print(p_df)


Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of akshay kulkarni
Sent: Saturday, July 9, 2022 1:02 PM
To: David Winsemius <dwinsemius at comcast.net>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] printing with bothe print and cat...

[External Email]

Dear David,
                    THe code given was a reprex, and unfortunately, I cannot skip the for loops: Its a very big  web scraping code. Any alternative?

Many thanks in advance...

Yours sincrely,
AKSHAY M KULKARNI

________________________________
From: David Winsemius <dwinsemius at comcast.net>
Sent: Saturday, July 9, 2022 10:28 PM
To: akshay kulkarni <akshay_e4 at hotmail.com>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] printing with bothe print and cat...

Skip the for loops:

cat(paste( seq(1:5), ?    ?, 1:5) )

?
David

Sent from my iPhone

> On Jul 9, 2022, at 9:47 AM, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>
> ?Dear members,
>                         I have the following code:
>
> testprint <- function() {
>
>  for(i in 1:5) {for(j in 1:5)
>  {cat(j)}
>    print(i)}
> }
>
> And the output is:
>
>> testprint()
> 12345[1] 1
> 12345[1] 2
> 12345[1] 3
> 12345[1] 4
> 12345[1] 5
>
> Any idea on how to remove the [1] from the output, and give spaces in the cat output? The desired output is:
>
> 1 2 3 4 5      1
> 1 2 3 4 5      2
> 1 2 3 4 5      3
> 1 2 3 4 5      4
> 1 2 3 4 5      5
> Many thanks in advance.....
>
> THanking you,
> Yours  sincreely,
> AKSHAY M KULKARNI
>
>    [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIGaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=lceoyahibi7acrpoFW6uDxJ4j_QtB0NYjb7GNZu_e9WpZDZ4hdpYpgvtx
> sQZbqGe&s=m5EK_gr2QGFthC1Q_GqiGjSV05AZSmV-T1kQpq5KlUM&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIGaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=lceoyahibi7acrpoFW6uDxJ4j_QtB0NYjb7GNZu_e9WpZDZ4hdpYpgvt
> xsQZbqGe&s=EdP6JP8fQVHC-xOJfowZ7VlJOlqWwnJBjAi8sZR2Els&e=
> and provide commented, minimal, self-contained, reproducible code.


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIGaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=lceoyahibi7acrpoFW6uDxJ4j_QtB0NYjb7GNZu_e9WpZDZ4hdpYpgvtxsQZbqGe&s=m5EK_gr2QGFthC1Q_GqiGjSV05AZSmV-T1kQpq5KlUM&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIGaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=lceoyahibi7acrpoFW6uDxJ4j_QtB0NYjb7GNZu_e9WpZDZ4hdpYpgvtxsQZbqGe&s=EdP6JP8fQVHC-xOJfowZ7VlJOlqWwnJBjAi8sZR2Els&e=
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Sat Jul  9 19:45:12 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Sat, 9 Jul 2022 22:45:12 +0500
Subject: [R] A humble request
In-Reply-To: <3f807c4c-10cd-277f-47ef-e0c42547bdcc@effectivedefense.org>
References: <CAMfKi3+=X6pGJp+5Mt=zZYL870xs0+w5gTsNsKi9o4=_jPt6kA@mail.gmail.com>
 <CABcYAdJ7E6aoC7ZYEx2i5Mx9VHRd+S8xMDCmsvbeCy++bTcosA@mail.gmail.com>
 <CABcYAd+f9yupeiztYWL4BDF=BOQf0Bn_ayRYUw0udmScGyxuig@mail.gmail.com>
 <CAMfKi3+e+gsv6f5ko33e7FCSWYt4a=iwgtoq=MeMtgP+Y8uA8Q@mail.gmail.com>
 <CAPcHnpR8AgPwx=CNCJrJ9CmqzqG7oF2i_5PWWj9JgYtq8t1_AQ@mail.gmail.com>
 <a82dbde5-0358-ca87-339f-fdd8f403b251@effectivedefense.org>
 <CAKZQJMBtz0h+HKz9cGpz7rRt1-MLX4vi1Rxu3TDpteZc6-VuAQ@mail.gmail.com>
 <CABcYAdJUebN26gH_14By+GQ0MrGXrQXEyjZTje2ZY5P=Rr7=2w@mail.gmail.com>
 <3f807c4c-10cd-277f-47ef-e0c42547bdcc@effectivedefense.org>
Message-ID: <CAMfKi3KqSj+iTt0s6xdimJrXng6XncNAsAPx=ikKA8krnrMSzg@mail.gmail.com>

Dear Respected Experts,
Thank you so much for your precious time and generous support. My issue is
resolved now.

Regards

On Sun, 3 Jul 2022, 20:48 Spencer Graves, <
spencer.graves at effectivedefense.org> wrote:

> Hi, Richard:  Thanks for the question.
>
>
>           "Source the function" means the following.
>
>
>           1.  Go to "https://cran.r-project.org/" and download the most
> recent
> version.  To do that, I found "archived" under "Related Directories".  I
> clicked on that and then searched for "wmtsa".  To the right of that, it
> says it was "Last Modified" "2020-06-09 07:24".  Note that "Last
> Modified", because you can use that to find recent documentation, etc.,
> from "archive.org" shortly prior to that date.
>
>
>           2.  Click on "wmtsa".  That takes you to "Index of
> /src/contrib/Archive/wmtsa", which includes 11 different versions of the
> source code for that package from "wmtsa_1.0-1.tar.gz 2007-09-23 21:54
> 139K" to "wmtsa_2.0-3.tar.gz 2017-12-06 12:14 151K" Click on the last
> one to download it.  This downloaded a file by that name to the
> "Downloads" folder in my computer.
>
>
>           3.  Create a directory for that file.  I have a directory called
> "R"
> on my computer.  I just created a new directory called "wmtsa" within
> that "R" directory.  Then I moved that file into that directory and
> unzip it by double clicking on it.  As a result, I now have a directory
> "~R/wmtsa/wmtsa".
>
>
>           4.  To compile that package, I routinely in a Terminal in R
> (called
> something else in Windows), I "cd" until I get to "~R/wmtsa".  Then I
> run "R CMD check "wmtsa_2.0-3.tar.gz".  I just did that.  It said,
> "ERROR  Packages required but not available: 'splus2R', 'ifultools'".
> In R, I was able to download the first using
> "install.packages('splus2R')".  When I tried
> "install.packages('ifultools')", I got a message, "package ?ifultools?
> is not available for this version of R".
>
>
>           5.  However, the inner "wmtsa" directory contains, among other
> things, directories called "man" and "R".  The latter directory contains
> files with names like "wav_boot.R", ..., "wav_xform.R".  I'm assuming
> you have documentation that tells you which function(s) you want to run.
>   Let's say you want to run "wavBestBasis".  If you are lucky, there
> will be a file with a name like "wavBestBasis.R".  That doesn't work in
> this case.  So instead I did a cd into that ~R/wmtsa/wmtsa/R".  Then I
> did "grep 'wavBestBasis' *.R".  I got the following:
>
>
> wav_xform.R:##    wavBestBasis
> wav_xform.R:# wavBestBasis
> wav_xform.R:"wavBestBasis" <- function(costs)
> Let's say it says you want to start with a f
>
>
>           6.  That says that "wavBestBasis" is in file "wav_xform.R".  I
> opened
> that file in RStudio (I use the free version) then clicked on the
> "Source" button in the upper right.
>
>
>           7.  Also, in the directory "~R/wmtsa/wmtsa/man" I found a file
> called
> "wavBestBasis.Rd".  I opened that in R.  To learn how to read that, you
> can type "help.start()", which will open a menu that includes, "Writing
> R Extensions".  Click that.  That tells you how to write (and read) a
> *.Rd file.  That file "wavBestBasis.Rd" contains and "\examples"
> section.  The first line in that section is "W <-
> wavDWPT(diff(atomclock), n.level=6)".  I ran that, and got:
>
>
> Error in diff(atomclock) : object 'atomclock' not found
>
>
>           I won't discuss how to find "atomclick".  If you need that, you
> can
> ask again.
>
>
>           Hope this helps.
>           Spencer
>
>
> On 7/3/22 8:15 AM, Richard O'Keefe wrote:
> > I'm not quite sure what "source the fonction" means.
> > The wmtsa package is available from the archive as a
> > compressed archive wmtsa_2.0-3.tar.gz.
> > % tar xf wmtsa_2.0-3.tar.gz
> > creates a directory wmtsa/ with subdirectories
> > wmtsa/man and wmtsa/R plus some other files.
> > The documentation is in wmtsa/man/*.md
> > The source code is in wmtsa/R/*.R
> > You look at either using whatever text editor you feel like.
> > Looking at wav_xform.R I see hundreds of lines of code
> > that only a mother could love, with a good table of contents
> > but no really informative comments anywhere.
> > I am reminded, as too often, that there are people who develop packages
> > for R, and there are software engineers, but there are precious few
> > software engineers developing packages for R.
> > As it happens, I *am* a software engineer (amongst other things),
> > and I do have a tolerably good knowledge of base R, and a high tolerance
> > for looking things up in the documentation.  But I would have to be paid
> > quite a large sum of money before I would spend any time on this
> > code-base.  There are other currently maintained packages that might be
> > able to do the job, so it would be a waste of my time.
> >
> > My advice is
> >    DON'T spend any time looking at this code.
> >    An R beginning WON'T understand it.
> >    DO ask your supervisor help you to select an alternative,
> >    and if you can't figure that out between you,
> >    ASK a more informative question.
> >
> >
> >
> > On Mon, 4 Jul 2022 at 00:38, John Kane <jrkrideau at gmail.com
> > <mailto:jrkrideau at gmail.com>> wrote:
> >
> >     Spenser,
> >     the idea to source the fonction makes sense but since tho OP is a
> very
> >     new beginner perhaps you could point him towards code showing him how
> >     to do this?  I have never done this AFAIR, and while I suspect a few
> >     minutes googling would show me how, it is likely to be more difficult
> >     for a noobie.
> >
> >     On Sun, 3 Jul 2022 at 07:25, Spencer Graves
> >     <spencer.graves at effectivedefense.org
> >     <mailto:spencer.graves at effectivedefense.org>> wrote:
> >      >
> >      > Muhammad Zubair Chishti:
> >      >
> >      >
> >      >           What specifically have you tried?  What were the
> results?
> >      >
> >      >
> >      >           Please "provide commented, minimal, self-contained,
> >     reproducible
> >      > code", per "the posting guide
> >      > http://www.R-project.org/posting-guide.html
> >     <http://www.R-project.org/posting-guide.html>", as indicated in at
> >     the end
> >      > of each email in this thread.
> >      >
> >      >
> >      >           And don't overlook the suggestion I made:  Download the
> >     archived
> >      > package.  Do NOT try to compile it. Instead source only the
> >     function you
> >      > want, try to run it.  With luck, it will work.  If it doesn't,
> >     you will
> >      > get a diagnostic that can help you take the next step.
> >      >
> >      >
> >      >           Spencer
> >      >
> >      >
> >      > On 7/3/22 1:28 AM, Andrew Simmons wrote:
> >      > > It seems like this package was archived because package
> "ifultools"
> >      > > was archived. I tried installing "ifultools" from source, but
> >     it has a
> >      > > seriously large amount of C compilation issues. The main issue
> >     seems
> >      > > to be that variable PROBLEM was never defined anywhere, and it
> is
> >      > > unclear what its definition should be. Unfortunately, as far as
> >     I can
> >      > > tell, this issue is unfixable. If you want to install "wmtsa",
> >     you'll
> >      > > have to use an older version of R. Otherwise, you can use one
> >     of the
> >      > > other wavelet analysis packages that Richard O'Keefe mentioned.
> >      > >
> >      > > On Sun, Jul 3, 2022 at 2:01 AM Muhammad Zubair Chishti
> >      > > <mzchishti at eco.qau.edu.pk <mailto:mzchishti at eco.qau.edu.pk>>
> wrote:
> >      > >>
> >      > >> Dear Respected Experts and specifically Professor Richard
> O'Keefe,
> >      > >> Thank you so much for your precious time and generous help.
> >     However, the
> >      > >> problem is still there and I am just unable to resolve it due
> >     to the lack
> >      > >> of expertise in R. Still, the hope is there. I believe that
> >     this platform
> >      > >> can help me.
> >      > >>
> >      > >> Regards
> >      > >> Muhammad Zubair Chishti
> >      > >> School of Business,
> >      > >> Zhengzhou University, Henan, China
> >      > >> My Google scholar link:
> >      > >> https://scholar.google.com/citations?hl=en&user=YPqNJMwAAAAJ
> >     <https://scholar.google.com/citations?hl=en&user=YPqNJMwAAAAJ>
> >      > >> My ReseachGate Profile:
> >      > >> https://www.researchgate.net/profile/Muhammad-Chishti
> >     <https://www.researchgate.net/profile/Muhammad-Chishti>
> >      > >>
> >      > >>
> >      > >> On Sun, Jul 3, 2022 at 9:11 AM Richard O'Keefe
> >     <raoknz at gmail.com <mailto:raoknz at gmail.com>> wrote:
> >      > >>
> >      > >>> A postscript to my previous comment.
> >      > >>> I used to supervise PhD students.
> >      > >>> Let me advise you to write this issue up as a draft section
> >     for your
> >      > >>> thesis.
> >      > >>> 1. Why I wanted to use the wmtsa package.
> >      > >>> 2. Why I didn't.
> >      > >>> 3. How I went about selecting a replacement.
> >      > >>> 4. What I chose and why that's the right choice.
> >      > >>> 5. How the analyses I wanted to do are done in
> >      > >>>     package X and what difference it makes.
> >      > >>>
> >      > >>> Off the top of my head, the only reasons for struggling to
> >     use an old
> >      > >>> package are to try to replicate someone else's results and/or
> >     to try to use
> >      > >>> their software (built atop the dead package) with new data.
> >     Well, if you
> >      > >>> get different results, that's interesting too, and then it's
> >     time to work
> >      > >>> harder to resurrect the dead package.
> >      > >>>
> >      > >>> Speaking of which, an easier route might be to set up a
> separate
> >      > >>> environment running an old version of R that *can* run the
> >     old code and the
> >      > >>> old code's dependencies.  In fact trying to use the same
> >     versions that the
> >      > >>> work you're trying to reproduce used might make a lot of
> sense.
> >      > >>>
> >      > >>> Overall, I think selecting an alternative package
> >      > >>> that *is* currently maintained is the best use of your time,
> >     but your
> >      > >>> supervisor should be able to help you with that.  Selecting
> >     appropriate
> >      > >>> packages is part of doing research, after all, and
> demonstrating
> >      > >>> that you can do it is all to the good, no?
> >      > >>>
> >      > >>>
> >      > >>> On Sun, 3 Jul 2022 at 15:24, Richard O'Keefe
> >     <raoknz at gmail.com <mailto:raoknz at gmail.com>> wrote:
> >      > >>>
> >      > >>>> Can we start a step back please?
> >      > >>>> wmtsa stands for
> >      > >>>>    Wavelet Methods for Time Series Analysis.
> >      > >>>>
> >      > >>>> OK, so you have some time series data,
> >      > >>>> and for some reason you want to analyse
> >      > >>>> your data using wavelets.  No worries.
> >      > >>>> But does it have to be THIS unmaintained
> >      > >>>> package?
> >      > >>>>
> >      > >>>> Why not visit
> >      > >>>>
> >      > >>>> https://CRAN.R-project.org/view=TimeSeries
> >     <https://CRAN.R-project.org/view=TimeSeries>
> >      > >>>> and search for "wavelets" in the text?
> >      > >>>> Oh heck, I might as well do it for you.
> >      > >>>> <snip>
> >      > >>>> *Wavelet methods* : The wavelets
> >      > >>>> <https://cran.r-project.org/web/packages/wavelets/index.html
> >     <https://cran.r-project.org/web/packages/wavelets/index.html>>
> package
> >      > >>>> includes computing wavelet filters, wavelet transforms and
> >     multiresolution
> >      > >>>> analyses. Multiresolution forecasting using wavelets is also
> >     implemented in
> >      > >>>> mrf <https://cran.r-project.org/web/packages/mrf/index.html
> >     <https://cran.r-project.org/web/packages/mrf/index.html>>.
> WaveletComp
> >      > >>>>
> >     <https://cran.r-project.org/web/packages/WaveletComp/index.html
> >     <https://cran.r-project.org/web/packages/WaveletComp/index.html>>
> >      > >>>> provides some tools for wavelet-based analysis of univariate
> >     and bivariate
> >      > >>>> time series including cross-wavelets, phase-difference and
> >     significance
> >      > >>>> tests. biwavelet
> >      > >>>>
> >     <https://cran.r-project.org/web/packages/biwavelet/index.html
> >     <https://cran.r-project.org/web/packages/biwavelet/index.html>> is a
> >     port
> >      > >>>> of the WTC Matlab package for univariate and bivariate
> >     wavelet analyses.
> >      > >>>> mvLSW
> >     <https://cran.r-project.org/web/packages/mvLSW/index.html
> >     <https://cran.r-project.org/web/packages/mvLSW/index.html>>
> >      > >>>> provides tools for multivariate locally stationary wavelet
> >     processes.
> >      > >>>> LSWPlib
> >     <https://cran.r-project.org/web/packages/LSWPlib/index.html
> >     <https://cran.r-project.org/web/packages/LSWPlib/index.html>>
> >      > >>>> contains functions for simulation and spectral estimation of
> >     locally
> >      > >>>> stationary wavelet packet processes. Tests of white noise
> >     using wavelets
> >      > >>>> are provided by hwwntest
> >      > >>>> <https://cran.r-project.org/web/packages/hwwntest/index.html
> >     <https://cran.r-project.org/web/packages/hwwntest/index.html>>.
> Wavelet
> >      > >>>> scalogram tools are contained in wavScalogram
> >      > >>>>
> >     <https://cran.r-project.org/web/packages/wavScalogram/index.html
> >     <https://cran.r-project.org/web/packages/wavScalogram/index.html>>.
> >      > >>>> Further wavelet methods can be found in the packages rwt
> >      > >>>> <https://cran.r-project.org/web/packages/rwt/index.html
> >     <https://cran.r-project.org/web/packages/rwt/index.html>>, waveslim
> >      > >>>> <https://cran.r-project.org/web/packages/waveslim/index.html
> >     <https://cran.r-project.org/web/packages/waveslim/index.html>>,
> >     wavethresh
> >      > >>>>
> >     <https://cran.r-project.org/web/packages/wavethresh/index.html
> >     <https://cran.r-project.org/web/packages/wavethresh/index.html>>.
> >      > >>>> </snip>
> >      > >>>>
> >      > >>>> Presumably there is a reason that nobody else has
> >      > >>>> bothered to continue maintaining wmtsa.  Perhaps
> >      > >>>> one of those other wavelets + time series packages
> >      > >>>> can do what you need?
> >      > >>>>
> >      > >>>>
> >      > >>>> On Sun, 3 Jul 2022 at 04:12, Muhammad Zubair Chishti <
> >      > >>>> mzchishti at eco.qau.edu.pk <mailto:mzchishti at eco.qau.edu.pk>>
> >     wrote:
> >      > >>>>
> >      > >>>>> Dear Experts,
> >      > >>>>> I cannot find a package "wmtsa" for my R version "R 4.2.0".
> >     Kindly help
> >      > >>>>> me
> >      > >>>>> to find it or share the link with me.
> >      > >>>>> Although I tried the old version of "wmtsa" but failed.
> >      > >>>>> Thank you for your precious time.
> >      > >>>>>
> >      > >>>>> Regards
> >      > >>>>> Muhammad Zubair Chishti
> >      > >>>>>
> >      > >>>>>          [[alternative HTML version deleted]]
> >      > >>>>>
> >      > >>>>> ______________________________________________
> >      > >>>>> R-help at r-project.org <mailto:R-help at r-project.org> mailing
> >     list -- To UNSUBSCRIBE and more, see
> >      > >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >     <https://stat.ethz.ch/mailman/listinfo/r-help>
> >      > >>>>> PLEASE do read the posting guide
> >      > >>>>> http://www.R-project.org/posting-guide.html
> >     <http://www.R-project.org/posting-guide.html>
> >      > >>>>> and provide commented, minimal, self-contained,
> >     reproducible code.
> >      > >>>>>
> >      > >>>>
> >      > >>
> >      > >>          [[alternative HTML version deleted]]
> >      > >>
> >      > >> ______________________________________________
> >      > >> R-help at r-project.org <mailto:R-help at r-project.org> mailing
> >     list -- To UNSUBSCRIBE and more, see
> >      > >> https://stat.ethz.ch/mailman/listinfo/r-help
> >     <https://stat.ethz.ch/mailman/listinfo/r-help>
> >      > >> PLEASE do read the posting guide
> >     http://www.R-project.org/posting-guide.html
> >     <http://www.R-project.org/posting-guide.html>
> >      > >> and provide commented, minimal, self-contained, reproducible
> code.
> >      > >
> >      > > ______________________________________________
> >      > > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
> >     -- To UNSUBSCRIBE and more, see
> >      > > https://stat.ethz.ch/mailman/listinfo/r-help
> >     <https://stat.ethz.ch/mailman/listinfo/r-help>
> >      > > PLEASE do read the posting guide
> >     http://www.R-project.org/posting-guide.html
> >     <http://www.R-project.org/posting-guide.html>
> >      > > and provide commented, minimal, self-contained, reproducible
> code.
> >      >
> >      > ______________________________________________
> >      > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
> >     -- To UNSUBSCRIBE and more, see
> >      > https://stat.ethz.ch/mailman/listinfo/r-help
> >     <https://stat.ethz.ch/mailman/listinfo/r-help>
> >      > PLEASE do read the posting guide
> >     http://www.R-project.org/posting-guide.html
> >     <http://www.R-project.org/posting-guide.html>
> >      > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> >     --
> >     John Kane
> >     Kingston ON Canada
> >
> >     ______________________________________________
> >     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
> >     To UNSUBSCRIBE and more, see
> >     https://stat.ethz.ch/mailman/listinfo/r-help
> >     <https://stat.ethz.ch/mailman/listinfo/r-help>
> >     PLEASE do read the posting guide
> >     http://www.R-project.org/posting-guide.html
> >     <http://www.R-project.org/posting-guide.html>
> >     and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat Jul  9 20:48:10 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 09 Jul 2022 11:48:10 -0700
Subject: [R] Please guide
In-Reply-To: <CAMfKi3JaV5jGukKwsvqfeD5rOqu-A7FBhA5PjF+=RFKTgNzbcw@mail.gmail.com>
References: <CAMfKi3KtWtuH-Zc6qhDcrN8bwhEeTiD9PbZB_m1L_XbejCR0sQ@mail.gmail.com>
 <CAMfKi3JaV5jGukKwsvqfeD5rOqu-A7FBhA5PjF+=RFKTgNzbcw@mail.gmail.com>
Message-ID: <FBBC8CF8-E55A-4DAC-B327-5622A2C85210@dcn.davis.ca.us>

This is a mailing list whose members are random people with an interest in R. We are not a tech support call center. If we have time, and you invest some effort into making it easier for us to reproduce your problem, then someone may respond... but no guarantees. Your current efforts are missing key elements that impede our efforts and there isn't much here to persuade many people to dig further. You may garner more responses if you make the following changes to your approach:

1) Make your example reproducible. We need access to the data you are working with, or at least a small fake similar data set. We don't have your Excel files... dead end for us. But see next point about files. Also, verify that your code can start from a fresh R environment and demonstrate your problem... the reprex package may be useful for this.

2) Make your example minimal. If you are having trouble with data importing, give us the data file or something similar that gives you the same error. If you are having trouble with data calculations or manipulations, then use dput [1] to give us a snippet of the data you are referring to in the troublesome line of code... don't give us a data file and a bunch of code that isn't giving you trouble before the error code... we don't want to wade through it.

3) Remove garbage from your posts (formatting). Look at the extra asterisks in your message as we see it [2]. We cannot run this code without guessing which symbols belong and which don't. I know you did not put them there intentionally, but neither did you disable HTML formatting in your email client when you composed the message, and that was responsible for the extra mess. The Posting Guide warns you to post plain text, and there are way too many email client programs out there for us to be able to tell you how to alter this setting... but it is essential that you do it if you want to encourage responses here.

I considered not responding at all due to these deficiencies, but figured I would spell out the problems you have missed in the Posting Guide once. Mailing lists are tough ways to get help... but the expertise here is kind of rare. You decide whether to make things easy or hard for yourself.

As for the "con =" line below, it seems to have nothing at all to do with the code or data referred to earlier. That would be a hint for point 2 above. Try calling the read_excel function alone without the rest of the indexing and matrix conversion. Only when the result you get from that step makes sense should you try indexing it or converting it. Maybe you need a "sheet=" argument?

---
[1]  http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

[2] https://stat.ethz.ch/pipermail/r-help/2022-July/475212.html

On July 9, 2022 10:40:29 AM PDT, Muhammad Zubair Chishti <mzchishti at eco.qau.edu.pk> wrote:
>Dear Experts,
>A kind reminder. Please help me.
>
>
>On Fri, 8 Jul 2022, 21:56 Muhammad Zubair Chishti, <mzchishti at eco.qau.edu.pk>
>wrote:
>
>>
>> *Dear Experts,*
>> *Greetings from Pakistan*.
>> *When I run the following code in R*
>> library(frequencyConnectedness)
>> library(readxl)
>> ##Add data here##
>> Data <- read_excel("Data_oil_agri.xlsx", sheet = "Sheet1")
>> Data <- Data[,2:22]
>> Data=na.omit(Data)
>> Bnames=colnames(Data)
>>
>> lags=VARselect(Data, lag.max = 12, type = "const")  ## "const", "trend",
>> "both", "none"
>> p=lags$selection[[3]]
>>
>> est <- VAR(Data, p = p, type = "const")
>> sp <- spilloverDY12(est, n.ahead = 10, no.corr = F)
>>
>> params_est = list(p = p, type = "const")
>> sp_roll <- spilloverRollingDY12(Data, n.ahead = 10, no.corr = F, "VAR",
>> params_est = params_est, window = 260)
>> bounds <- c(pi+0.00001, pi/5, 0)
>> sp_bk <- spilloverBK12(est, n.ahead = 100, no.corr = F, partition = bounds)
>>
>> sp_bk_roll <- spilloverRollingBK12(Data, n.ahead = 100, no.corr = F,
>> func_est = "VAR", params_est = params_est, window = 260, partition = bounds)
>>
>> **Till now the code works well. After that, when I run the following:*
>>
>> con      = as.matrix(read_excel("DY_Table.xlsx")[,-1])*100
>>
>> *I face the following error:*
>> *Error in* *`vectbl_as_col_location()`:*
>> ! Can't negate columns past the end.
>> ? Location 1 doesn't exist.
>> ? There are only 0 columns.
>> Run `rlang::last_error()` to see where the error occurred.
>>
>> Kindly please guide me.
>>
>> Regards
>> Muhammad Zubair Chishti
>> Ph.D. Student
>> School of Business,
>> Zhengzhou University, Henan, China.
>> My Google scholar link:
>> https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
>> My ResearchGate link:
>> https://www.researchgate.net/profile/Muhammad-Chishti
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat Jul  9 21:09:30 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 9 Jul 2022 20:09:30 +0100
Subject: [R] printing with bothe print and cat...
In-Reply-To: <PU4P216MB1568F36F3D77861C985FFF5AC8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <PU4P216MB1568F36F3D77861C985FFF5AC8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <fcd2f864-6a16-01d1-013b-38bbc2f8b53a@sapo.pt>

Hello,

Like this?


testprint <- function() {
   for(i in 1:5) {
     for(j in 1:5) {
       cat(j, "")
     }
     cat("\t", i, "\n")
   }
}

testprint()
#> 1 2 3 4 5     1
#> 1 2 3 4 5     2
#> 1 2 3 4 5     3
#> 1 2 3 4 5     4
#> 1 2 3 4 5     5


Hope this helps,

Rui Barradas

?s 17:46 de 09/07/2022, akshay kulkarni escreveu:
> Dear members,
>                           I have the following code:
> 
> testprint <- function() {
> 
>    for(i in 1:5) {for(j in 1:5)
>    {cat(j)}
>      print(i)}
> }
> 
> And the output is:
> 
>> testprint()
> 12345[1] 1
> 12345[1] 2
> 12345[1] 3
> 12345[1] 4
> 12345[1] 5
> 
> Any idea on how to remove the [1] from the output, and give spaces in the cat output? The desired output is:
> 
> 1 2 3 4 5      1
> 1 2 3 4 5      2
> 1 2 3 4 5      3
> 1 2 3 4 5      4
> 1 2 3 4 5      5
>   Many thanks in advance.....
> 
> THanking you,
> Yours  sincreely,
> AKSHAY M KULKARNI
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tebert @end|ng |rom u||@edu  Sat Jul  9 22:04:43 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Sat, 9 Jul 2022 20:04:43 +0000
Subject: [R] printing with bothe print and cat...
In-Reply-To: <PU4P216MB1568A309F94B33EC574B43BCC8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <PU4P216MB1568F36F3D77861C985FFF5AC8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <4FCF8673-8308-4A08-B1EC-AA4EA6D0C2A0@comcast.net>
 <PU4P216MB15689EED85F17F711D8340A6C8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <BN6PR2201MB155324CBC9C8C91B89C32FF1CF859@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <PU4P216MB1568A309F94B33EC574B43BCC8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <BN6PR2201MB1553C647A64AB9E0ECAD418DCF859@BN6PR2201MB1553.namprd22.prod.outlook.com>

Here is an alternative without for loops

n_rows <- 5
n_cols <- 5
p_df <- data.frame(matrix( rep(1:n_rows, n_cols), nrow=n_rows, byrow=TRUE), line_n=1:n_rows)
print(p_df)


If n_rows always equals n_cols then you could combine the first two lines

n_rows <- n_cols <- 5
p_df <- data.frame(matrix( rep(1:n_rows, n_cols), nrow=n_rows, byrow=TRUE), line_n=1:n_rows)
n_cols <- n_cols+1
print(p_df)

You might also notice that as.numeric(rownames(p_df)) returns the same values as line_n.


Tim
From: akshay kulkarni <akshay_e4 at hotmail.com>
Sent: Saturday, July 9, 2022 1:41 PM
To: Ebert,Timothy Aaron <tebert at ufl.edu>; David Winsemius <dwinsemius at comcast.net>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] printing with bothe print and cat...

[External Email]
Dear Tim,
                   Many thanks...

Yours sincerely
AKSHAYM KULKARNI
________________________________
From: Ebert,Timothy Aaron <tebert at ufl.edu<mailto:tebert at ufl.edu>>
Sent: Saturday, July 9, 2022 11:03 PM
To: akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>>; David Winsemius <dwinsemius at comcast.net<mailto:dwinsemius at comcast.net>>
Cc: R help Mailing list <r-help at r-project.org<mailto:r-help at r-project.org>>
Subject: RE: [R] printing with bothe print and cat...

Here is an alternative that makes a dataframe and then prints the dataframe.
n_rows <- 5
n_cols <- 5
p_mat<-matrix(0,nrow=n_rows,ncol=n_cols)
for(i in 1:n_rows) {
  for(j in 1:n_cols){
    p_mat[i,j]<-(j)
  }
}
p_df <- data.frame(p_mat,1:n_rows)
print(p_df)


Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>> On Behalf Of akshay kulkarni
Sent: Saturday, July 9, 2022 1:02 PM
To: David Winsemius <dwinsemius at comcast.net<mailto:dwinsemius at comcast.net>>
Cc: R help Mailing list <r-help at r-project.org<mailto:r-help at r-project.org>>
Subject: Re: [R] printing with bothe print and cat...

[External Email]

Dear David,
                    THe code given was a reprex, and unfortunately, I cannot skip the for loops: Its a very big  web scraping code. Any alternative?

Many thanks in advance...

Yours sincrely,
AKSHAY M KULKARNI

________________________________
From: David Winsemius <dwinsemius at comcast.net<mailto:dwinsemius at comcast.net>>
Sent: Saturday, July 9, 2022 10:28 PM
To: akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>>
Cc: R help Mailing list <r-help at r-project.org<mailto:r-help at r-project.org>>
Subject: Re: [R] printing with bothe print and cat...

Skip the for loops:

cat(paste( seq(1:5), ?    ?, 1:5) )

?
David

Sent from my iPhone

> On Jul 9, 2022, at 9:47 AM, akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>> wrote:
>
> ?Dear members,
>                         I have the following code:
>
> testprint <- function() {
>
>  for(i in 1:5) {for(j in 1:5)
>  {cat(j)}
>    print(i)}
> }
>
> And the output is:
>
>> testprint()
> 12345[1] 1
> 12345[1] 2
> 12345[1] 3
> 12345[1] 4
> 12345[1] 5
>
> Any idea on how to remove the [1] from the output, and give spaces in the cat output? The desired output is:
>
> 1 2 3 4 5      1
> 1 2 3 4 5      2
> 1 2 3 4 5      3
> 1 2 3 4 5      4
> 1 2 3 4 5      5
> Many thanks in advance.....
>
> THanking you,
> Yours  sincreely,
> AKSHAY M KULKARNI
>
>    [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIGaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=lceoyahibi7acrpoFW6uDxJ4j_QtB0NYjb7GNZu_e9WpZDZ4hdpYpgvtx
> sQZbqGe&s=m5EK_gr2QGFthC1Q_GqiGjSV05AZSmV-T1kQpq5KlUM&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIGaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=lceoyahibi7acrpoFW6uDxJ4j_QtB0NYjb7GNZu_e9WpZDZ4hdpYpgvt
> xsQZbqGe&s=EdP6JP8fQVHC-xOJfowZ7VlJOlqWwnJBjAi8sZR2Els&e=
> and provide commented, minimal, self-contained, reproducible code.


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIGaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=lceoyahibi7acrpoFW6uDxJ4j_QtB0NYjb7GNZu_e9WpZDZ4hdpYpgvtxsQZbqGe&s=m5EK_gr2QGFthC1Q_GqiGjSV05AZSmV-T1kQpq5KlUM&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIGaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=lceoyahibi7acrpoFW6uDxJ4j_QtB0NYjb7GNZu_e9WpZDZ4hdpYpgvtxsQZbqGe&s=EdP6JP8fQVHC-xOJfowZ7VlJOlqWwnJBjAi8sZR2Els&e=
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Sun Jul 10 11:27:44 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Sun, 10 Jul 2022 09:27:44 +0000
Subject: [R] printing with bothe print and cat...
In-Reply-To: <fcd2f864-6a16-01d1-013b-38bbc2f8b53a@sapo.pt>
References: <PU4P216MB1568F36F3D77861C985FFF5AC8859@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <fcd2f864-6a16-01d1-013b-38bbc2f8b53a@sapo.pt>
Message-ID: <PU4P216MB156886ADF2234EDA7C7D196FC8849@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear Rui,
Many thanks...

Yours
Akshay M Kulkarni

________________________________
From: Rui Barradas <ruipbarradas at sapo.pt>
Sent: Sunday, July 10, 2022 12:39 AM
To: akshay kulkarni <akshay_e4 at hotmail.com>; R help Mailing list <r-help at r-project.org>
Subject: Re: [R] printing with bothe print and cat...

Hello,

Like this?


testprint <- function() {
   for(i in 1:5) {
     for(j in 1:5) {
       cat(j, "")
     }
     cat("\t", i, "\n")
   }
}

testprint()
#> 1 2 3 4 5     1
#> 1 2 3 4 5     2
#> 1 2 3 4 5     3
#> 1 2 3 4 5     4
#> 1 2 3 4 5     5


Hope this helps,

Rui Barradas

?s 17:46 de 09/07/2022, akshay kulkarni escreveu:
> Dear members,
>                           I have the following code:
>
> testprint <- function() {
>
>    for(i in 1:5) {for(j in 1:5)
>    {cat(j)}
>      print(i)}
> }
>
> And the output is:
>
>> testprint()
> 12345[1] 1
> 12345[1] 2
> 12345[1] 3
> 12345[1] 4
> 12345[1] 5
>
> Any idea on how to remove the [1] from the output, and give spaces in the cat output? The desired output is:
>
> 1 2 3 4 5      1
> 1 2 3 4 5      2
> 1 2 3 4 5      3
> 1 2 3 4 5      4
> 1 2 3 4 5      5
>   Many thanks in advance.....
>
> THanking you,
> Yours  sincreely,
> AKSHAY M KULKARNI
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Sun Jul 10 17:41:59 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Sun, 10 Jul 2022 15:41:59 +0000
Subject: [R] rmoving dates from an xts object...
Message-ID: <PU4P216MB1568885A7CD531D6620DC8CDC8849@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear members,
                         I have OHLC data of 500 stocks: OHLCData and dates. These are of xts object. I want to do the following:


  1.  I want to remove a contiguous set of dates from the xts object. for example, I want to remove data from the OHLC data from "2022-20-7"
  2.   to "2018-2-2", continuously.
  3.

       2. I want to remove a set of dates, which are not contiguous.

Any idea on how to accomplish it? I can write an intricate for loop, but any other method? Does an xts object behave like an atomic vector : OHLCData[[i]][-dates[[i]]] ?

Many thanks in advance....

THanking you,
Yours sincerely,
AKSHAY M KULKARNI

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun Jul 10 18:35:42 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 10 Jul 2022 19:35:42 +0300
Subject: [R] rmoving dates from an xts object...
In-Reply-To: <PU4P216MB1568885A7CD531D6620DC8CDC8849@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <PU4P216MB1568885A7CD531D6620DC8CDC8849@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <CAGxFJbR2Q8XuVW3-MsbmCNBauvh+xRuDSB_rL=2Fm04rWgGYhw@mail.gmail.com>

Time to do your homework:

https://rpubs.com/odenipinedo/manipulating-time-series-data-with-xts-and-zoo-in-R

Bert

On Sun, Jul 10, 2022, 6:42 PM akshay kulkarni <akshay_e4 at hotmail.com> wrote:

> Dear members,
>                          I have OHLC data of 500 stocks: OHLCData and
> dates. These are of xts object. I want to do the following:
>
>
>   1.  I want to remove a contiguous set of dates from the xts object. for
> example, I want to remove data from the OHLC data from "2022-20-7"
>   2.   to "2018-2-2", continuously.
>   3.
>
>        2. I want to remove a set of dates, which are not contiguous.
>
> Any idea on how to accomplish it? I can write an intricate for loop, but
> any other method? Does an xts object behave like an atomic vector :
> OHLCData[[i]][-dates[[i]]] ?
>
> Many thanks in advance....
>
> THanking you,
> Yours sincerely,
> AKSHAY M KULKARNI
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ggrothend|eck @end|ng |rom gm@||@com  Sun Jul 10 20:04:29 2022
From: ggrothend|eck @end|ng |rom gm@||@com (Gabor Grothendieck)
Date: Sun, 10 Jul 2022 14:04:29 -0400
Subject: [R] rmoving dates from an xts object...
In-Reply-To: <PU4P216MB1568885A7CD531D6620DC8CDC8849@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <PU4P216MB1568885A7CD531D6620DC8CDC8849@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <CAP01uRk-YSd-5CUCL9-19XsDnKgaZC8GUG7V5gis4fNPizjAEA@mail.gmail.com>

Look at the examples at the end of ?xts for more info.

  library(quantmod)
  getSymbols("AAPL")

  class(AAPL)
  ## [1] "xts" "zoo"

  range(time(AAPL))
  ## [1] "2007-01-03" "2022-07-08"

  # everything up to indicated date
  a1 <- AAPL["/2018-02-01"]

  # remove non consecutive dates
  d <- as.Date(c("2022-07-01", "2022-07-06")) # dates to remove
  a2 <- AAPL[ ! time(AAPL) %in% d ]

On Sun, Jul 10, 2022 at 11:42 AM akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>
> Dear members,
>                          I have OHLC data of 500 stocks: OHLCData and dates. These are of xts object. I want to do the following:
>
>
>   1.  I want to remove a contiguous set of dates from the xts object. for example, I want to remove data from the OHLC data from "2022-20-7"
>   2.   to "2018-2-2", continuously.
>   3.
>
>        2. I want to remove a set of dates, which are not contiguous.
>
> Any idea on how to accomplish it? I can write an intricate for loop, but any other method? Does an xts object behave like an atomic vector : OHLCData[[i]][-dates[[i]]] ?
>
> Many thanks in advance....
>
> THanking you,
> Yours sincerely,
> AKSHAY M KULKARNI
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From @tyen @end|ng |rom ntu@edu@tw  Mon Jul 11 03:08:45 2022
From: @tyen @end|ng |rom ntu@edu@tw (Steven T. Yen)
Date: Mon, 11 Jul 2022 09:08:45 +0800
Subject: [R] grep
Message-ID: <6ea1f979-3485-7031-97c9-775d78f4fd37@ntu.edu.tw>

Dear, Below, jj contains character strings starting with ?z.? and ?x.?. 
I want to grep all that contain either ?z.? or ?x.?. I had to grep ?z.? 
and ?x.? separately and then tack the result together. Is there a 
convenient grep option that would grep strings with either ?z.? or ?x.?. 
Thank you!

 > jj<-names(v$est); jj
 ?[1] "z.one"???? "z.liberal" "z.conserv" "z.dem"???? "z.rep" "z.realinc"
 ?[7] "x.one"???? "x.liberal" "x.conserv" "x.dem"???? "x.rep" "x.realinc"
[13] "mu1_1"???? "mu2_1"???? "rho"
 > j1<-grep("z.",jj,value=TRUE); j1
[1] "z.one"???? "z.liberal" "z.conserv" "z.dem"???? "z.rep" "z.realinc"
 > j2<-grep("x.",jj,value=TRUE); j2
[1] "x.one"???? "x.liberal" "x.conserv" "x.dem"???? "x.rep" "x.realinc"
 > j<-c(j1,j2); j
 ?[1] "z.one"???? "z.liberal" "z.conserv" "z.dem"???? "z.rep" "z.realinc"
 ?[7] "x.one"???? "x.liberal" "x.conserv" "x.dem"???? "x.rep" "x.realinc"


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Jul 11 03:30:09 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sun, 10 Jul 2022 18:30:09 -0700
Subject: [R] grep
In-Reply-To: <6ea1f979-3485-7031-97c9-775d78f4fd37@ntu.edu.tw>
References: <6ea1f979-3485-7031-97c9-775d78f4fd37@ntu.edu.tw>
Message-ID: <6BA0FA52-C207-475C-BB50-5870EAEE1AE2@dcn.davis.ca.us>

grep( "^(z|x)\\.", jj, value = TRUE )

or

grep( r"(^(z|x)\.)", jj, value = TRUE )


On July 10, 2022 6:08:45 PM PDT, "Steven T. Yen" <styen at ntu.edu.tw> wrote:
>Dear, Below, jj contains character strings starting with ?z.? and ?x.?. I want to grep all that contain either ?z.? or ?x.?. I had to grep ?z.? and ?x.? separately and then tack the result together. Is there a convenient grep option that would grep strings with either ?z.? or ?x.?. Thank you!
>
>> jj<-names(v$est); jj
>?[1] "z.one"???? "z.liberal" "z.conserv" "z.dem"???? "z.rep" "z.realinc"
>?[7] "x.one"???? "x.liberal" "x.conserv" "x.dem"???? "x.rep" "x.realinc"
>[13] "mu1_1"???? "mu2_1"???? "rho"
>> j1<-grep("z.",jj,value=TRUE); j1
>[1] "z.one"???? "z.liberal" "z.conserv" "z.dem"???? "z.rep" "z.realinc"
>> j2<-grep("x.",jj,value=TRUE); j2
>[1] "x.one"???? "x.liberal" "x.conserv" "x.dem"???? "x.rep" "x.realinc"
>> j<-c(j1,j2); j
>?[1] "z.one"???? "z.liberal" "z.conserv" "z.dem"???? "z.rep" "z.realinc"
>?[7] "x.one"???? "x.liberal" "x.conserv" "x.dem"???? "x.rep" "x.realinc"
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @tyen @end|ng |rom ntu@edu@tw  Mon Jul 11 03:43:54 2022
From: @tyen @end|ng |rom ntu@edu@tw (Steven T. Yen)
Date: Mon, 11 Jul 2022 09:43:54 +0800
Subject: [R] grep
In-Reply-To: <6BA0FA52-C207-475C-BB50-5870EAEE1AE2@dcn.davis.ca.us>
References: <6ea1f979-3485-7031-97c9-775d78f4fd37@ntu.edu.tw>
 <6BA0FA52-C207-475C-BB50-5870EAEE1AE2@dcn.davis.ca.us>
Message-ID: <506d74e3-8fd8-3b5a-883e-ae75f96bafeb@ntu.edu.tw>

Thanks Jeff. It works. If there is a good reference I should read 
(besides ? grep) I's be glad to have it.

On 7/11/2022 9:30 AM, Jeff Newmiller wrote:
> grep( "^(z|x)\\.", jj, value = TRUE )
>
> or
>
> grep( r"(^(z|x)\.)", jj, value = TRUE )
>
>
> On July 10, 2022 6:08:45 PM PDT, "Steven T. Yen" <styen at ntu.edu.tw> wrote:
>> Dear, Below, jj contains character strings starting with ?z.? and ?x.?. I want to grep all that contain either ?z.? or ?x.?. I had to grep ?z.? and ?x.? separately and then tack the result together. Is there a convenient grep option that would grep strings with either ?z.? or ?x.?. Thank you!
>>
>>> jj<-names(v$est); jj
>>  ?[1] "z.one"???? "z.liberal" "z.conserv" "z.dem"???? "z.rep" "z.realinc"
>>  ?[7] "x.one"???? "x.liberal" "x.conserv" "x.dem"???? "x.rep" "x.realinc"
>> [13] "mu1_1"???? "mu2_1"???? "rho"
>>> j1<-grep("z.",jj,value=TRUE); j1
>> [1] "z.one"???? "z.liberal" "z.conserv" "z.dem"???? "z.rep" "z.realinc"
>>> j2<-grep("x.",jj,value=TRUE); j2
>> [1] "x.one"???? "x.liberal" "x.conserv" "x.dem"???? "x.rep" "x.realinc"
>>> j<-c(j1,j2); j
>>  ?[1] "z.one"???? "z.liberal" "z.conserv" "z.dem"???? "z.rep" "z.realinc"
>>  ?[7] "x.one"???? "x.liberal" "x.conserv" "x.dem"???? "x.rep" "x.realinc"
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From @kw@|mmo @end|ng |rom gm@||@com  Mon Jul 11 04:32:04 2022
From: @kw@|mmo @end|ng |rom gm@||@com (Andrew Simmons)
Date: Sun, 10 Jul 2022 22:32:04 -0400
Subject: [R] grep
In-Reply-To: <506d74e3-8fd8-3b5a-883e-ae75f96bafeb@ntu.edu.tw>
References: <6ea1f979-3485-7031-97c9-775d78f4fd37@ntu.edu.tw>
 <6BA0FA52-C207-475C-BB50-5870EAEE1AE2@dcn.davis.ca.us>
 <506d74e3-8fd8-3b5a-883e-ae75f96bafeb@ntu.edu.tw>
Message-ID: <CAPcHnpTc1AZoV3DrPMJWMxyMp+Pisdh9rMym+V+RcW32huZiEg@mail.gmail.com>

?regex is a nice starting point, it's got plenty of details on meta
characters and characters classes, if you need more advanced stuff you'll
probably have to look at the perl regex documentation, I believe it's
linked in ?regex.

I might try something like

grep("^[xz]\\.")

or

grep("^[xz][.]")

or if you'd consider a different function,

startsWith(jj, "x.") | startsWith(jj, "z.")

On Sun, Jul 10, 2022, 21:49 Steven T. Yen <styen at ntu.edu.tw> wrote:

> Thanks Jeff. It works. If there is a good reference I should read
> (besides ? grep) I's be glad to have it.
>
> On 7/11/2022 9:30 AM, Jeff Newmiller wrote:
> > grep( "^(z|x)\\.", jj, value = TRUE )
> >
> > or
> >
> > grep( r"(^(z|x)\.)", jj, value = TRUE )
> >
> >
> > On July 10, 2022 6:08:45 PM PDT, "Steven T. Yen" <styen at ntu.edu.tw>
> wrote:
> >> Dear, Below, jj contains character strings starting with ?z.? and ?x.?.
> I want to grep all that contain either ?z.? or ?x.?. I had to grep ?z.? and
> ?x.? separately and then tack the result together. Is there a convenient
> grep option that would grep strings with either ?z.? or ?x.?. Thank you!
> >>
> >>> jj<-names(v$est); jj
> >>   [1] "z.one"     "z.liberal" "z.conserv" "z.dem"     "z.rep"
> "z.realinc"
> >>   [7] "x.one"     "x.liberal" "x.conserv" "x.dem"     "x.rep"
> "x.realinc"
> >> [13] "mu1_1"     "mu2_1"     "rho"
> >>> j1<-grep("z.",jj,value=TRUE); j1
> >> [1] "z.one"     "z.liberal" "z.conserv" "z.dem"     "z.rep" "z.realinc"
> >>> j2<-grep("x.",jj,value=TRUE); j2
> >> [1] "x.one"     "x.liberal" "x.conserv" "x.dem"     "x.rep" "x.realinc"
> >>> j<-c(j1,j2); j
> >>   [1] "z.one"     "z.liberal" "z.conserv" "z.dem"     "z.rep"
> "z.realinc"
> >>   [7] "x.one"     "x.liberal" "x.conserv" "x.dem"     "x.rep"
> "x.realinc"
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @tyen @end|ng |rom ntu@edu@tw  Mon Jul 11 05:11:51 2022
From: @tyen @end|ng |rom ntu@edu@tw (Steven T. Yen)
Date: Mon, 11 Jul 2022 11:11:51 +0800
Subject: [R] grep
In-Reply-To: <CAPcHnpTc1AZoV3DrPMJWMxyMp+Pisdh9rMym+V+RcW32huZiEg@mail.gmail.com>
References: <6ea1f979-3485-7031-97c9-775d78f4fd37@ntu.edu.tw>
 <6BA0FA52-C207-475C-BB50-5870EAEE1AE2@dcn.davis.ca.us>
 <506d74e3-8fd8-3b5a-883e-ae75f96bafeb@ntu.edu.tw>
 <CAPcHnpTc1AZoV3DrPMJWMxyMp+Pisdh9rMym+V+RcW32huZiEg@mail.gmail.com>
Message-ID: <7eb93b95-a3eb-0c61-46d1-299df6169752@ntu.edu.tw>

Thank you all.

On 7/11/2022 10:32 AM, Andrew Simmons wrote:
> ?regex is a nice starting point, it's got plenty of details on meta 
> characters and characters classes, if you need more advanced stuff 
> you'll probably have to look at the perl regex documentation, I 
> believe it's linked in ?regex.
>
> I might try something like
>
> grep("^[xz]\\.")
>
> or
>
> grep("^[xz][.]")
>
> or if you'd consider a different function,
>
> startsWith(jj, "x.") | startsWith(jj, "z.")
>
> On Sun, Jul 10, 2022, 21:49 Steven T. Yen <styen at ntu.edu.tw> wrote:
>
>     Thanks Jeff. It works. If there is a good reference I should read
>     (besides ? grep) I's be glad to have it.
>
>     On 7/11/2022 9:30 AM, Jeff Newmiller wrote:
>     > grep( "^(z|x)\\.", jj, value = TRUE )
>     >
>     > or
>     >
>     > grep( r"(^(z|x)\.)", jj, value = TRUE )
>     >
>     >
>     > On July 10, 2022 6:08:45 PM PDT, "Steven T. Yen"
>     <styen at ntu.edu.tw> wrote:
>     >> Dear, Below, jj contains character strings starting with ?z.?
>     and ?x.?. I want to grep all that contain either ?z.? or ?x.?. I
>     had to grep ?z.? and ?x.? separately and then tack the result
>     together. Is there a convenient grep option that would grep
>     strings with either ?z.? or ?x.?. Thank you!
>     >>
>     >>> jj<-names(v$est); jj
>     >>? ?[1] "z.one"???? "z.liberal" "z.conserv" "z.dem" "z.rep"
>     "z.realinc"
>     >>? ?[7] "x.one"???? "x.liberal" "x.conserv" "x.dem" "x.rep"
>     "x.realinc"
>     >> [13] "mu1_1"???? "mu2_1"???? "rho"
>     >>> j1<-grep("z.",jj,value=TRUE); j1
>     >> [1] "z.one"???? "z.liberal" "z.conserv" "z.dem" "z.rep" "z.realinc"
>     >>> j2<-grep("x.",jj,value=TRUE); j2
>     >> [1] "x.one"???? "x.liberal" "x.conserv" "x.dem" "x.rep" "x.realinc"
>     >>> j<-c(j1,j2); j
>     >>? ?[1] "z.one"???? "z.liberal" "z.conserv" "z.dem" "z.rep"
>     "z.realinc"
>     >>? ?[7] "x.one"???? "x.liberal" "x.conserv" "x.dem" "x.rep"
>     "x.realinc"
>     >>
>     >> ______________________________________________
>     >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>     >> https://stat.ethz.ch/mailman/listinfo/r-help
>     >> PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>     >> and provide commented, minimal, self-contained, reproducible code.
>
>     ______________________________________________
>     R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>     and provide commented, minimal, self-contained, reproducible code.
>
	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Mon Jul 11 05:21:23 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Sun, 10 Jul 2022 23:21:23 -0400
Subject: [R] grep
In-Reply-To: <11401_1657504157_26B1nDH8013179_506d74e3-8fd8-3b5a-883e-ae75f96bafeb@ntu.edu.tw>
References: <6ea1f979-3485-7031-97c9-775d78f4fd37@ntu.edu.tw>
 <6BA0FA52-C207-475C-BB50-5870EAEE1AE2@dcn.davis.ca.us>
 <11401_1657504157_26B1nDH8013179_506d74e3-8fd8-3b5a-883e-ae75f96bafeb@ntu.edu.tw>
Message-ID: <8ec7ee8d-5f02-75f6-28c2-61475f2f8b13@mcmaster.ca>

Dear Steven,

Beyond ?regex, the Wikipedia article on regular expressions 
<https://en.wikipedia.org/wiki/Regular_expression> is quite helpful and 
not too long.

I hope this helps,
  John

On 2022-07-10 9:43 p.m., Steven T. Yen wrote:
> Thanks Jeff. It works. If there is a good reference I should read 
> (besides ? grep) I's be glad to have it.
> 
> On 7/11/2022 9:30 AM, Jeff Newmiller wrote:
>> grep( "^(z|x)\\.", jj, value = TRUE )
>>
>> or
>>
>> grep( r"(^(z|x)\.)", jj, value = TRUE )
>>
>>
>> On July 10, 2022 6:08:45 PM PDT, "Steven T. Yen" <styen at ntu.edu.tw> 
>> wrote:
>>> Dear, Below, jj contains character strings starting with ?z.? and 
>>> ?x.?. I want to grep all that contain either ?z.? or ?x.?. I had to 
>>> grep ?z.? and ?x.? separately and then tack the result together. Is 
>>> there a convenient grep option that would grep strings with either 
>>> ?z.? or ?x.?. Thank you!
>>>
>>>> jj<-names(v$est); jj
>>> ??[1] "z.one"???? "z.liberal" "z.conserv" "z.dem"???? "z.rep" 
>>> "z.realinc"
>>> ??[7] "x.one"???? "x.liberal" "x.conserv" "x.dem"???? "x.rep" 
>>> "x.realinc"
>>> [13] "mu1_1"???? "mu2_1"???? "rho"
>>>> j1<-grep("z.",jj,value=TRUE); j1
>>> [1] "z.one"???? "z.liberal" "z.conserv" "z.dem"???? "z.rep" "z.realinc"
>>>> j2<-grep("x.",jj,value=TRUE); j2
>>> [1] "x.one"???? "x.liberal" "x.conserv" "x.dem"???? "x.rep" "x.realinc"
>>>> j<-c(j1,j2); j
>>> ??[1] "z.one"???? "z.liberal" "z.conserv" "z.dem"???? "z.rep" 
>>> "z.realinc"
>>> ??[7] "x.one"???? "x.liberal" "x.conserv" "x.dem"???? "x.rep" 
>>> "x.realinc"
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide 
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Mon Jul 11 05:50:23 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Mon, 11 Jul 2022 08:50:23 +0500
Subject: [R] Please guide
In-Reply-To: <FBBC8CF8-E55A-4DAC-B327-5622A2C85210@dcn.davis.ca.us>
References: <CAMfKi3KtWtuH-Zc6qhDcrN8bwhEeTiD9PbZB_m1L_XbejCR0sQ@mail.gmail.com>
 <CAMfKi3JaV5jGukKwsvqfeD5rOqu-A7FBhA5PjF+=RFKTgNzbcw@mail.gmail.com>
 <FBBC8CF8-E55A-4DAC-B327-5622A2C85210@dcn.davis.ca.us>
Message-ID: <CAMfKi3KcKCEKdMTb0o_0+w6eNsuhk7WFOmTBdRG+bYNmr2Y5QA@mail.gmail.com>

Dear Respected Professor,
You are right. I strongly agree with you. Besides, Thank you so much for
your kind reply. As you said, the data file mentioned in the above code is
attached herewith. Kindly consider my request.

 Data_oil_agri.xlsx
<https://drive.google.com/file/d/1heMlPfUtB-RthKYv4jRM3PGFI8CJ4fck/view?usp=drive_web>

Regards
Muhammad Zubair Chishti

On Sat, Jul 9, 2022 at 11:48 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> This is a mailing list whose members are random people with an interest in
> R. We are not a tech support call center. If we have time, and you invest
> some effort into making it easier for us to reproduce your problem, then
> someone may respond... but no guarantees. Your current efforts are missing
> key elements that impede our efforts and there isn't much here to persuade
> many people to dig further. You may garner more responses if you make the
> following changes to your approach:
>
> 1) Make your example reproducible. We need access to the data you are
> working with, or at least a small fake similar data set. We don't have your
> Excel files... dead end for us. But see next point about files. Also,
> verify that your code can start from a fresh R environment and demonstrate
> your problem... the reprex package may be useful for this.
>
> 2) Make your example minimal. If you are having trouble with data
> importing, give us the data file or something similar that gives you the
> same error. If you are having trouble with data calculations or
> manipulations, then use dput [1] to give us a snippet of the data you are
> referring to in the troublesome line of code... don't give us a data file
> and a bunch of code that isn't giving you trouble before the error code...
> we don't want to wade through it.
>
> 3) Remove garbage from your posts (formatting). Look at the extra
> asterisks in your message as we see it [2]. We cannot run this code without
> guessing which symbols belong and which don't. I know you did not put them
> there intentionally, but neither did you disable HTML formatting in your
> email client when you composed the message, and that was responsible for
> the extra mess. The Posting Guide warns you to post plain text, and there
> are way too many email client programs out there for us to be able to tell
> you how to alter this setting... but it is essential that you do it if you
> want to encourage responses here.
>
> I considered not responding at all due to these deficiencies, but figured
> I would spell out the problems you have missed in the Posting Guide once.
> Mailing lists are tough ways to get help... but the expertise here is kind
> of rare. You decide whether to make things easy or hard for yourself.
>
> As for the "con =" line below, it seems to have nothing at all to do with
> the code or data referred to earlier. That would be a hint for point 2
> above. Try calling the read_excel function alone without the rest of the
> indexing and matrix conversion. Only when the result you get from that step
> makes sense should you try indexing it or converting it. Maybe you need a
> "sheet=" argument?
>
> ---
> [1]
> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
>
> [2] https://stat.ethz.ch/pipermail/r-help/2022-July/475212.html
>
> On July 9, 2022 10:40:29 AM PDT, Muhammad Zubair Chishti <
> mzchishti at eco.qau.edu.pk> wrote:
> >Dear Experts,
> >A kind reminder. Please help me.
> >
> >
> >On Fri, 8 Jul 2022, 21:56 Muhammad Zubair Chishti, <
> mzchishti at eco.qau.edu.pk>
> >wrote:
> >
> >>
> >> *Dear Experts,*
> >> *Greetings from Pakistan*.
> >> *When I run the following code in R*
> >> library(frequencyConnectedness)
> >> library(readxl)
> >> ##Add data here##
> >> Data <- read_excel("Data_oil_agri.xlsx", sheet = "Sheet1")
> >> Data <- Data[,2:22]
> >> Data=na.omit(Data)
> >> Bnames=colnames(Data)
> >>
> >> lags=VARselect(Data, lag.max = 12, type = "const")  ## "const", "trend",
> >> "both", "none"
> >> p=lags$selection[[3]]
> >>
> >> est <- VAR(Data, p = p, type = "const")
> >> sp <- spilloverDY12(est, n.ahead = 10, no.corr = F)
> >>
> >> params_est = list(p = p, type = "const")
> >> sp_roll <- spilloverRollingDY12(Data, n.ahead = 10, no.corr = F, "VAR",
> >> params_est = params_est, window = 260)
> >> bounds <- c(pi+0.00001, pi/5, 0)
> >> sp_bk <- spilloverBK12(est, n.ahead = 100, no.corr = F, partition =
> bounds)
> >>
> >> sp_bk_roll <- spilloverRollingBK12(Data, n.ahead = 100, no.corr = F,
> >> func_est = "VAR", params_est = params_est, window = 260, partition =
> bounds)
> >>
> >> **Till now the code works well. After that, when I run the following:*
> >>
> >> con      = as.matrix(read_excel("DY_Table.xlsx")[,-1])*100
> >>
> >> *I face the following error:*
> >> *Error in* *`vectbl_as_col_location()`:*
> >> ! Can't negate columns past the end.
> >> ? Location 1 doesn't exist.
> >> ? There are only 0 columns.
> >> Run `rlang::last_error()` to see where the error occurred.
> >>
> >> Kindly please guide me.
> >>
> >> Regards
> >> Muhammad Zubair Chishti
> >> Ph.D. Student
> >> School of Business,
> >> Zhengzhou University, Henan, China.
> >> My Google scholar link:
> >> https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
> >> My ResearchGate link:
> >> https://www.researchgate.net/profile/Muhammad-Chishti
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Jul 11 06:30:53 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sun, 10 Jul 2022 21:30:53 -0700
Subject: [R] Please guide
In-Reply-To: <CAMfKi3KcKCEKdMTb0o_0+w6eNsuhk7WFOmTBdRG+bYNmr2Y5QA@mail.gmail.com>
References: <CAMfKi3KtWtuH-Zc6qhDcrN8bwhEeTiD9PbZB_m1L_XbejCR0sQ@mail.gmail.com>
 <CAMfKi3JaV5jGukKwsvqfeD5rOqu-A7FBhA5PjF+=RFKTgNzbcw@mail.gmail.com>
 <FBBC8CF8-E55A-4DAC-B327-5622A2C85210@dcn.davis.ca.us>
 <CAMfKi3KcKCEKdMTb0o_0+w6eNsuhk7WFOmTBdRG+bYNmr2Y5QA@mail.gmail.com>
Message-ID: <4E7E5B02-9471-40DC-A660-17244531A029@dcn.davis.ca.us>

1) Thank you for trying to be respectful, but calling me a professor is not a good way to do that. I have no PhD, nor do I work for a university. A simple "Jeff" or if you prefer "Mr. Newmiller" will be fine 

2) Because you replied to me, I can see your file, but no-one else on the list can... most attachments are removed by the mailing list. Do read the Posting Guide and try to embed data with dput rather than attaching things. Again, you limit the number of people who can help you by failing to use the list properly.

3) The name of the file you attached suggests that this is not the one that was causing you trouble. Your code references two different filenames.

On July 10, 2022 8:50:23 PM PDT, Muhammad Zubair Chishti <mzchishti at eco.qau.edu.pk> wrote:
>Dear Respected Professor,
>You are right. I strongly agree with you. Besides, Thank you so much for
>your kind reply. As you said, the data file mentioned in the above code is
>attached herewith. Kindly consider my request.
>
> Data_oil_agri.xlsx
><https://drive.google.com/file/d/1heMlPfUtB-RthKYv4jRM3PGFI8CJ4fck/view?usp=drive_web>
>
>Regards
>Muhammad Zubair Chishti
>
>On Sat, Jul 9, 2022 at 11:48 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>wrote:
>
>> This is a mailing list whose members are random people with an interest in
>> R. We are not a tech support call center. If we have time, and you invest
>> some effort into making it easier for us to reproduce your problem, then
>> someone may respond... but no guarantees. Your current efforts are missing
>> key elements that impede our efforts and there isn't much here to persuade
>> many people to dig further. You may garner more responses if you make the
>> following changes to your approach:
>>
>> 1) Make your example reproducible. We need access to the data you are
>> working with, or at least a small fake similar data set. We don't have your
>> Excel files... dead end for us. But see next point about files. Also,
>> verify that your code can start from a fresh R environment and demonstrate
>> your problem... the reprex package may be useful for this.
>>
>> 2) Make your example minimal. If you are having trouble with data
>> importing, give us the data file or something similar that gives you the
>> same error. If you are having trouble with data calculations or
>> manipulations, then use dput [1] to give us a snippet of the data you are
>> referring to in the troublesome line of code... don't give us a data file
>> and a bunch of code that isn't giving you trouble before the error code...
>> we don't want to wade through it.
>>
>> 3) Remove garbage from your posts (formatting). Look at the extra
>> asterisks in your message as we see it [2]. We cannot run this code without
>> guessing which symbols belong and which don't. I know you did not put them
>> there intentionally, but neither did you disable HTML formatting in your
>> email client when you composed the message, and that was responsible for
>> the extra mess. The Posting Guide warns you to post plain text, and there
>> are way too many email client programs out there for us to be able to tell
>> you how to alter this setting... but it is essential that you do it if you
>> want to encourage responses here.
>>
>> I considered not responding at all due to these deficiencies, but figured
>> I would spell out the problems you have missed in the Posting Guide once.
>> Mailing lists are tough ways to get help... but the expertise here is kind
>> of rare. You decide whether to make things easy or hard for yourself.
>>
>> As for the "con =" line below, it seems to have nothing at all to do with
>> the code or data referred to earlier. That would be a hint for point 2
>> above. Try calling the read_excel function alone without the rest of the
>> indexing and matrix conversion. Only when the result you get from that step
>> makes sense should you try indexing it or converting it. Maybe you need a
>> "sheet=" argument?
>>
>> ---
>> [1]
>> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
>>
>> [2] https://stat.ethz.ch/pipermail/r-help/2022-July/475212.html
>>
>> On July 9, 2022 10:40:29 AM PDT, Muhammad Zubair Chishti <
>> mzchishti at eco.qau.edu.pk> wrote:
>> >Dear Experts,
>> >A kind reminder. Please help me.
>> >
>> >
>> >On Fri, 8 Jul 2022, 21:56 Muhammad Zubair Chishti, <
>> mzchishti at eco.qau.edu.pk>
>> >wrote:
>> >
>> >>
>> >> *Dear Experts,*
>> >> *Greetings from Pakistan*.
>> >> *When I run the following code in R*
>> >> library(frequencyConnectedness)
>> >> library(readxl)
>> >> ##Add data here##
>> >> Data <- read_excel("Data_oil_agri.xlsx", sheet = "Sheet1")
>> >> Data <- Data[,2:22]
>> >> Data=na.omit(Data)
>> >> Bnames=colnames(Data)
>> >>
>> >> lags=VARselect(Data, lag.max = 12, type = "const")  ## "const", "trend",
>> >> "both", "none"
>> >> p=lags$selection[[3]]
>> >>
>> >> est <- VAR(Data, p = p, type = "const")
>> >> sp <- spilloverDY12(est, n.ahead = 10, no.corr = F)
>> >>
>> >> params_est = list(p = p, type = "const")
>> >> sp_roll <- spilloverRollingDY12(Data, n.ahead = 10, no.corr = F, "VAR",
>> >> params_est = params_est, window = 260)
>> >> bounds <- c(pi+0.00001, pi/5, 0)
>> >> sp_bk <- spilloverBK12(est, n.ahead = 100, no.corr = F, partition =
>> bounds)
>> >>
>> >> sp_bk_roll <- spilloverRollingBK12(Data, n.ahead = 100, no.corr = F,
>> >> func_est = "VAR", params_est = params_est, window = 260, partition =
>> bounds)
>> >>
>> >> **Till now the code works well. After that, when I run the following:*
>> >>
>> >> con      = as.matrix(read_excel("DY_Table.xlsx")[,-1])*100
>> >>
>> >> *I face the following error:*
>> >> *Error in* *`vectbl_as_col_location()`:*
>> >> ! Can't negate columns past the end.
>> >> ? Location 1 doesn't exist.
>> >> ? There are only 0 columns.
>> >> Run `rlang::last_error()` to see where the error occurred.
>> >>
>> >> Kindly please guide me.
>> >>
>> >> Regards
>> >> Muhammad Zubair Chishti
>> >> Ph.D. Student
>> >> School of Business,
>> >> Zhengzhou University, Henan, China.
>> >> My Google scholar link:
>> >> https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
>> >> My ResearchGate link:
>> >> https://www.researchgate.net/profile/Muhammad-Chishti
>> >>
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> >______________________________________________
>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>

-- 
Sent from my phone. Please excuse my brevity.


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Mon Jul 11 06:45:02 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Mon, 11 Jul 2022 09:45:02 +0500
Subject: [R] Please guide
In-Reply-To: <4E7E5B02-9471-40DC-A660-17244531A029@dcn.davis.ca.us>
References: <CAMfKi3KtWtuH-Zc6qhDcrN8bwhEeTiD9PbZB_m1L_XbejCR0sQ@mail.gmail.com>
 <CAMfKi3JaV5jGukKwsvqfeD5rOqu-A7FBhA5PjF+=RFKTgNzbcw@mail.gmail.com>
 <FBBC8CF8-E55A-4DAC-B327-5622A2C85210@dcn.davis.ca.us>
 <CAMfKi3KcKCEKdMTb0o_0+w6eNsuhk7WFOmTBdRG+bYNmr2Y5QA@mail.gmail.com>
 <4E7E5B02-9471-40DC-A660-17244531A029@dcn.davis.ca.us>
Message-ID: <CAMfKi3Lo_s9pnnCpKWWpCpKXcYfhvP46+RKLBm5+z6=X-Q1H0w@mail.gmail.com>

Dear Jeff,
1) Being a PhD student, I am obliged to pay deep homage to all experts like
you on this forum who provide free knowledge.

2) Since the data is too large and I cannot cut it, I have no other
suitable option to send my full data.

3) This is the same file that I used in the codes mentioned in my
first email. The codes work well while using the data file I sent you till
this code line:

*sp_bk_roll <- spilloverRollingBK12(Data, n.ahead = 100, no.corr = F,
func_est = "VAR", params_est = params_est, window = 260, partition =
bounds)*

**Till now the code works well. After that, when I run the following:*

con      = as.matrix(read_excel("DY_Table.xlsx")[,-1])*100

*I face the following error:*
*Error in* *`vectbl_as_col_location()`:*
! Can't negate columns past the end.
? Location 1 doesn't exist.
? There are only 0 columns.
Run `rlang::last_error()` to see where the error occurred.

Now kindly check my issue.

Regards
Chishti

On Mon, Jul 11, 2022 at 9:30 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> 1) Thank you for trying to be respectful, but calling me a professor is
> not a good way to do that. I have no PhD, nor do I work for a university. A
> simple "Jeff" or if you prefer "Mr. Newmiller" will be fine
>
> 2) Because you replied to me, I can see your file, but no-one else on the
> list can... most attachments are removed by the mailing list. Do read the
> Posting Guide and try to embed data with dput rather than attaching things.
> Again, you limit the number of people who can help you by failing to use
> the list properly.
>
> 3) The name of the file you attached suggests that this is not the one
> that was causing you trouble. Your code references two different filenames.
>
> On July 10, 2022 8:50:23 PM PDT, Muhammad Zubair Chishti <
> mzchishti at eco.qau.edu.pk> wrote:
> >Dear Respected Professor,
> >You are right. I strongly agree with you. Besides, Thank you so much for
> >your kind reply. As you said, the data file mentioned in the above code is
> >attached herewith. Kindly consider my request.
> >
> > Data_oil_agri.xlsx
> ><
> https://drive.google.com/file/d/1heMlPfUtB-RthKYv4jRM3PGFI8CJ4fck/view?usp=drive_web
> >
> >
> >Regards
> >Muhammad Zubair Chishti
> >
> >On Sat, Jul 9, 2022 at 11:48 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> >wrote:
> >
> >> This is a mailing list whose members are random people with an interest
> in
> >> R. We are not a tech support call center. If we have time, and you
> invest
> >> some effort into making it easier for us to reproduce your problem, then
> >> someone may respond... but no guarantees. Your current efforts are
> missing
> >> key elements that impede our efforts and there isn't much here to
> persuade
> >> many people to dig further. You may garner more responses if you make
> the
> >> following changes to your approach:
> >>
> >> 1) Make your example reproducible. We need access to the data you are
> >> working with, or at least a small fake similar data set. We don't have
> your
> >> Excel files... dead end for us. But see next point about files. Also,
> >> verify that your code can start from a fresh R environment and
> demonstrate
> >> your problem... the reprex package may be useful for this.
> >>
> >> 2) Make your example minimal. If you are having trouble with data
> >> importing, give us the data file or something similar that gives you the
> >> same error. If you are having trouble with data calculations or
> >> manipulations, then use dput [1] to give us a snippet of the data you
> are
> >> referring to in the troublesome line of code... don't give us a data
> file
> >> and a bunch of code that isn't giving you trouble before the error
> code...
> >> we don't want to wade through it.
> >>
> >> 3) Remove garbage from your posts (formatting). Look at the extra
> >> asterisks in your message as we see it [2]. We cannot run this code
> without
> >> guessing which symbols belong and which don't. I know you did not put
> them
> >> there intentionally, but neither did you disable HTML formatting in your
> >> email client when you composed the message, and that was responsible for
> >> the extra mess. The Posting Guide warns you to post plain text, and
> there
> >> are way too many email client programs out there for us to be able to
> tell
> >> you how to alter this setting... but it is essential that you do it if
> you
> >> want to encourage responses here.
> >>
> >> I considered not responding at all due to these deficiencies, but
> figured
> >> I would spell out the problems you have missed in the Posting Guide
> once.
> >> Mailing lists are tough ways to get help... but the expertise here is
> kind
> >> of rare. You decide whether to make things easy or hard for yourself.
> >>
> >> As for the "con =" line below, it seems to have nothing at all to do
> with
> >> the code or data referred to earlier. That would be a hint for point 2
> >> above. Try calling the read_excel function alone without the rest of the
> >> indexing and matrix conversion. Only when the result you get from that
> step
> >> makes sense should you try indexing it or converting it. Maybe you need
> a
> >> "sheet=" argument?
> >>
> >> ---
> >> [1]
> >>
> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
> >>
> >> [2] https://stat.ethz.ch/pipermail/r-help/2022-July/475212.html
> >>
> >> On July 9, 2022 10:40:29 AM PDT, Muhammad Zubair Chishti <
> >> mzchishti at eco.qau.edu.pk> wrote:
> >> >Dear Experts,
> >> >A kind reminder. Please help me.
> >> >
> >> >
> >> >On Fri, 8 Jul 2022, 21:56 Muhammad Zubair Chishti, <
> >> mzchishti at eco.qau.edu.pk>
> >> >wrote:
> >> >
> >> >>
> >> >> *Dear Experts,*
> >> >> *Greetings from Pakistan*.
> >> >> *When I run the following code in R*
> >> >> library(frequencyConnectedness)
> >> >> library(readxl)
> >> >> ##Add data here##
> >> >> Data <- read_excel("Data_oil_agri.xlsx", sheet = "Sheet1")
> >> >> Data <- Data[,2:22]
> >> >> Data=na.omit(Data)
> >> >> Bnames=colnames(Data)
> >> >>
> >> >> lags=VARselect(Data, lag.max = 12, type = "const")  ## "const",
> "trend",
> >> >> "both", "none"
> >> >> p=lags$selection[[3]]
> >> >>
> >> >> est <- VAR(Data, p = p, type = "const")
> >> >> sp <- spilloverDY12(est, n.ahead = 10, no.corr = F)
> >> >>
> >> >> params_est = list(p = p, type = "const")
> >> >> sp_roll <- spilloverRollingDY12(Data, n.ahead = 10, no.corr = F,
> "VAR",
> >> >> params_est = params_est, window = 260)
> >> >> bounds <- c(pi+0.00001, pi/5, 0)
> >> >> sp_bk <- spilloverBK12(est, n.ahead = 100, no.corr = F, partition =
> >> bounds)
> >> >>
> >> >> sp_bk_roll <- spilloverRollingBK12(Data, n.ahead = 100, no.corr = F,
> >> >> func_est = "VAR", params_est = params_est, window = 260, partition =
> >> bounds)
> >> >>
> >> >> **Till now the code works well. After that, when I run the
> following:*
> >> >>
> >> >> con      = as.matrix(read_excel("DY_Table.xlsx")[,-1])*100
> >> >>
> >> >> *I face the following error:*
> >> >> *Error in* *`vectbl_as_col_location()`:*
> >> >> ! Can't negate columns past the end.
> >> >> ? Location 1 doesn't exist.
> >> >> ? There are only 0 columns.
> >> >> Run `rlang::last_error()` to see where the error occurred.
> >> >>
> >> >> Kindly please guide me.
> >> >>
> >> >> Regards
> >> >> Muhammad Zubair Chishti
> >> >> Ph.D. Student
> >> >> School of Business,
> >> >> Zhengzhou University, Henan, China.
> >> >> My Google scholar link:
> >> >> https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
> >> >> My ResearchGate link:
> >> >> https://www.researchgate.net/profile/Muhammad-Chishti
> >> >>
> >> >
> >> >       [[alternative HTML version deleted]]
> >> >
> >> >______________________________________________
> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >https://stat.ethz.ch/mailman/listinfo/r-help
> >> >PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> >and provide commented, minimal, self-contained, reproducible code.
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From @v|gro@@ @end|ng |rom ver|zon@net  Mon Jul 11 07:00:13 2022
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Mon, 11 Jul 2022 05:00:13 +0000 (UTC)
Subject: [R] Please guide
In-Reply-To: <CAMfKi3KcKCEKdMTb0o_0+w6eNsuhk7WFOmTBdRG+bYNmr2Y5QA@mail.gmail.com>
References: <CAMfKi3KtWtuH-Zc6qhDcrN8bwhEeTiD9PbZB_m1L_XbejCR0sQ@mail.gmail.com>
 <CAMfKi3JaV5jGukKwsvqfeD5rOqu-A7FBhA5PjF+=RFKTgNzbcw@mail.gmail.com>
 <FBBC8CF8-E55A-4DAC-B327-5622A2C85210@dcn.davis.ca.us>
 <CAMfKi3KcKCEKdMTb0o_0+w6eNsuhk7WFOmTBdRG+bYNmr2Y5QA@mail.gmail.com>
Message-ID: <1774255781.2354028.1657515613298@mail.yahoo.com>

I am sure if Jeff Newmiller is a Professor, he may well be respected but he has not said he is nor most of us. He specifically said the people here are random people with an interest in R. A few may indeed be professors or once taught? like I did ages ago but few of us relate particularly to be called an honorific and unlike in some cultures, many simply get a bit annoyed at such phrases and some move on and won't reply.

May I suggest that on this forum, we appreciate it is some asks a properly formatted question which shows attempts to solve the problem and supplies a minimum of details with NONE of the details in an attached file which we will not see even if sent repeatedly.

I think you may have received replies and have not followed through on them.

Let me try ONCE, and if a reply refers to me being a professor I will just press DELETE and go on with my life.

Your error message seems clear. It says that what you asked for has NO columns.?


Did you type what it asked for IMMEDIATELY??

rlang::last_error()

The above might return MORE INFO and you have not shared that here. Is it possible it would have been helpful?

Now consider NOT running complex code in one long line when trying to see what failed. This line is too complex:

as.matrix(read_excel("DY_Table.xlsx")[,-1])*100


Try this:

temp <-?read_excel("DY_Table.xlsx")

See what it returns If the file exists in the current directory/folder and is spelled exactly that way and has only one tab, maybe it does.?But if that fails, DEBUG that before moving on. You need to look at temp and see if it is a valid data.frame.

So ONLY if it looks OK, do you move on to the next idea. What do you think temp[, -1] means??Your code seems to want all rows kept and I think the you want the FIRST column removed, leaving all the rest.?Note if the data only has ONE column, you are left with no columns. If the data already is empty or nonsense,trying to do the [, -1] indexing will fail, perhaps with that complaint.
So, if the above still worked as in you typed head(temp[, -1]) then continue, else go FIX what you already have till it works.

You now seem to want to change a data.frame into a matrix. Note that will really only work if everything is numeric. I mean?if it has any character data, everything will be character and probably not useful.?
So these two lines let you examine:
temp <- temp[, -1]

temp <- as.matrix(temp)

Any error messages so far? I highly doubt the next line generates that message, but as noted, the contents better be numeric.

temp <- temp * 100

When you get all those steps working, feel free to resume your original code:

con? ? ? = as.matrix(read_excel("DY_Table.xlsx")[,-1])*100


Just FYI, although "=" is a legal use in that line, many people use "<-" when writing in R.

And note again, we have no idea what is in what you call a DATA file except that a file whose name?ends in .XSLX is not what we think of as a data file. It is a newer format for an EXCEL SPREADSHEET?as compared to a .CSV file that has simpler comma separated values in a text compatible file. To read data?from EXCEL is much more complex and you may need to tell it which of the tabbed sheets to use and where?in that sheet to look for data. It often makes a good guess of looking at the first sheet and if it finds what looks?like columns of data and NOTHING ELSE on that sheet, brings it in. Chances are your data is not set up?for that default and you have several options. One is to open the file in EXCEL and clean it so it fits, and another?is to save a sheet, or a region, as a new .CSV file and change your code to use a function like read.csv() with?appropriate options.

Again, politeness here is wasted and volunteers have no obligation to you. If the above is helpful, great. If not,so be it. Ask your actual Professor who presumably will love being respected and speak your language well?and be able to see your data.
Avi


-----Original Message-----
From: Muhammad Zubair Chishti <mzchishti at eco.qau.edu.pk>
To: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>; r-help at r-project.org
Sent: Sun, Jul 10, 2022 11:50 pm
Subject: Re: [R] Please guide

Dear Respected Professor,
You are right. I strongly agree with you. Besides, Thank you so much for
your kind reply. As you said, the data file mentioned in the above code is
attached herewith. Kindly consider my request.

 Data_oil_agri.xlsx
<https://drive.google.com/file/d/1heMlPfUtB-RthKYv4jRM3PGFI8CJ4fck/view?usp=drive_web>

Regards
Muhammad Zubair Chishti

On Sat, Jul 9, 2022 at 11:48 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> This is a mailing list whose members are random people with an interest in
> R. We are not a tech support call center. If we have time, and you invest
> some effort into making it easier for us to reproduce your problem, then
> someone may respond... but no guarantees. Your current efforts are missing
> key elements that impede our efforts and there isn't much here to persuade
> many people to dig further. You may garner more responses if you make the
> following changes to your approach:
>
> 1) Make your example reproducible. We need access to the data you are
> working with, or at least a small fake similar data set. We don't have your
> Excel files... dead end for us. But see next point about files. Also,
> verify that your code can start from a fresh R environment and demonstrate
> your problem... the reprex package may be useful for this.
>
> 2) Make your example minimal. If you are having trouble with data
> importing, give us the data file or something similar that gives you the
> same error. If you are having trouble with data calculations or
> manipulations, then use dput [1] to give us a snippet of the data you are
> referring to in the troublesome line of code... don't give us a data file
> and a bunch of code that isn't giving you trouble before the error code...
> we don't want to wade through it.
>
> 3) Remove garbage from your posts (formatting). Look at the extra
> asterisks in your message as we see it [2]. We cannot run this code without
> guessing which symbols belong and which don't. I know you did not put them
> there intentionally, but neither did you disable HTML formatting in your
> email client when you composed the message, and that was responsible for
> the extra mess. The Posting Guide warns you to post plain text, and there
> are way too many email client programs out there for us to be able to tell
> you how to alter this setting... but it is essential that you do it if you
> want to encourage responses here.
>
> I considered not responding at all due to these deficiencies, but figured
> I would spell out the problems you have missed in the Posting Guide once.
> Mailing lists are tough ways to get help... but the expertise here is kind
> of rare. You decide whether to make things easy or hard for yourself.
>
> As for the "con =" line below, it seems to have nothing at all to do with
> the code or data referred to earlier. That would be a hint for point 2
> above. Try calling the read_excel function alone without the rest of the
> indexing and matrix conversion. Only when the result you get from that step
> makes sense should you try indexing it or converting it. Maybe you need a
> "sheet=" argument?
>
> ---
> [1]
> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
>
> [2] https://stat.ethz.ch/pipermail/r-help/2022-July/475212.html
>
> On July 9, 2022 10:40:29 AM PDT, Muhammad Zubair Chishti <
> mzchishti at eco.qau.edu.pk> wrote:
> >Dear Experts,
> >A kind reminder. Please help me.
> >
> >
> >On Fri, 8 Jul 2022, 21:56 Muhammad Zubair Chishti, <
> mzchishti at eco.qau.edu.pk>
> >wrote:
> >
> >>
> >> *Dear Experts,*
> >> *Greetings from Pakistan*.
> >> *When I run the following code in R*
> >> library(frequencyConnectedness)
> >> library(readxl)
> >> ##Add data here##
> >> Data <- read_excel("Data_oil_agri.xlsx", sheet = "Sheet1")
> >> Data <- Data[,2:22]
> >> Data=na.omit(Data)
> >> Bnames=colnames(Data)
> >>
> >> lags=VARselect(Data, lag.max = 12, type = "const")? ## "const", "trend",
> >> "both", "none"
> >> p=lags$selection[[3]]
> >>
> >> est <- VAR(Data, p = p, type = "const")
> >> sp <- spilloverDY12(est, n.ahead = 10, no.corr = F)
> >>
> >> params_est = list(p = p, type = "const")
> >> sp_roll <- spilloverRollingDY12(Data, n.ahead = 10, no.corr = F, "VAR",
> >> params_est = params_est, window = 260)
> >> bounds <- c(pi+0.00001, pi/5, 0)
> >> sp_bk <- spilloverBK12(est, n.ahead = 100, no.corr = F, partition =
> bounds)
> >>
> >> sp_bk_roll <- spilloverRollingBK12(Data, n.ahead = 100, no.corr = F,
> >> func_est = "VAR", params_est = params_est, window = 260, partition =
> bounds)
> >>
> >> **Till now the code works well. After that, when I run the following:*
> >>
> >> con? ? ? = as.matrix(read_excel("DY_Table.xlsx")[,-1])*100
> >>
> >> *I face the following error:*
> >> *Error in* *`vectbl_as_col_location()`:*
> >> ! Can't negate columns past the end.
> >> ? Location 1 doesn't exist.
> >> ? There are only 0 columns.
> >> Run `rlang::last_error()` to see where the error occurred.
> >>
> >> Kindly please guide me.
> >>
> >> Regards
> >> Muhammad Zubair Chishti
> >> Ph.D. Student
> >> School of Business,
> >> Zhengzhou University, Henan, China.
> >> My Google scholar link:
> >> https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
> >> My ResearchGate link:
> >> https://www.researchgate.net/profile/Muhammad-Chishti
> >>
> >
> >? ? ? [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>

??? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From @v|gro@@ @end|ng |rom ver|zon@net  Mon Jul 11 07:21:39 2022
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Mon, 11 Jul 2022 05:21:39 +0000 (UTC)
Subject: [R] Please guide
In-Reply-To: <CAMfKi3Lo_s9pnnCpKWWpCpKXcYfhvP46+RKLBm5+z6=X-Q1H0w@mail.gmail.com>
References: <CAMfKi3KtWtuH-Zc6qhDcrN8bwhEeTiD9PbZB_m1L_XbejCR0sQ@mail.gmail.com>
 <CAMfKi3JaV5jGukKwsvqfeD5rOqu-A7FBhA5PjF+=RFKTgNzbcw@mail.gmail.com>
 <FBBC8CF8-E55A-4DAC-B327-5622A2C85210@dcn.davis.ca.us>
 <CAMfKi3KcKCEKdMTb0o_0+w6eNsuhk7WFOmTBdRG+bYNmr2Y5QA@mail.gmail.com>
 <4E7E5B02-9471-40DC-A660-17244531A029@dcn.davis.ca.us>
 <CAMfKi3Lo_s9pnnCpKWWpCpKXcYfhvP46+RKLBm5+z6=X-Q1H0w@mail.gmail.com>
Message-ID: <2145635986.2520338.1657516899844@mail.yahoo.com>

Cultures differ and we can put up with it but the feedback you get is that plenty of parts of the wider?world are not like Pakistan. We simply do not invent honorifics for people who we do not know for sure?has earned them, and even then, many people do not want or need a title used when not in that job but?just here helping out. If you want to keep calling us Professors, feel free. You have been notified many of?us are not impressed and may act accordingly. I know people with more than one Doctorate who do not?feel any need to use the titles.


But enough of that. Your level of understanding what you are doing is a bigger issue as you may?not know enough to be easy to help. My guess is you got some code from someone and are trying?to adapt it without knowing much. That works sometimes but maybe not now.

Anyone there can probably show you how to NOT SEND a huge EXCEL file because your problem probably?will happen if you make a copy of your file and edit it and save just the first few rows and toss the rest. If you?did that and your code still failed, you could cut and paste a small sample of the data, or even better do as?Jeff suggested, albeit in this case, he needs to consider that if you are not reading in the data properly, how?can you use dput() on an in-memory data structure and send us the output.


As Jeff pointed out, the part of your code you shared looked at two files and I assumed the error you reported?was only for the second. But did you notice for the other file you did it in steps:

Data <- read_excel("Data_oil_agri.xlsx", sheet = "Sheet1")

You specifically asked for Sheet1 and the second time did not. Does that matter? Maybe not. But it is not consistent.?You also removed the first column from that file another way:

Data <- Data[,2:22]


Just FYI, there is an easier way to remove a column by setting it to NULL as in

Data$ColNAME <- NULL? # if you know the name of the column

or: Data[,1] <- NULL
You do not need to know how many columns you have BUT the above only works if you properly?have read in something. Your code for the other file probably did not read it in properly.
I suggest again making your code more like the above rather than one long complex one,?so you can catch where it broke. My guess is earlier before you asked it to remove a?non-existent column.



-----Original Message-----
From: Muhammad Zubair Chishti <mzchishti at eco.qau.edu.pk>
To: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>; r-help at r-project.org
Sent: Mon, Jul 11, 2022 12:45 am
Subject: Re: [R] Please guide

Dear Jeff,
1) Being a PhD student, I am obliged to pay deep homage to all experts like
you on this forum who provide free knowledge.

2) Since the data is too large and I cannot cut it, I have no other
suitable option to send my full data.

3) This is the same file that I used in the codes mentioned in my
first email. The codes work well while using the data file I sent you till
this code line:

*sp_bk_roll <- spilloverRollingBK12(Data, n.ahead = 100, no.corr = F,
func_est = "VAR", params_est = params_est, window = 260, partition =
bounds)*

**Till now the code works well. After that, when I run the following:*

con? ? ? = as.matrix(read_excel("DY_Table.xlsx")[,-1])*100

*I face the following error:*
*Error in* *`vectbl_as_col_location()`:*
! Can't negate columns past the end.
? Location 1 doesn't exist.
? There are only 0 columns.
Run `rlang::last_error()` to see where the error occurred.

Now kindly check my issue.

Regards
Chishti

On Mon, Jul 11, 2022 at 9:30 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> 1) Thank you for trying to be respectful, but calling me a professor is
> not a good way to do that. I have no PhD, nor do I work for a university. A
> simple "Jeff" or if you prefer "Mr. Newmiller" will be fine
>
> 2) Because you replied to me, I can see your file, but no-one else on the
> list can... most attachments are removed by the mailing list. Do read the
> Posting Guide and try to embed data with dput rather than attaching things.
> Again, you limit the number of people who can help you by failing to use
> the list properly.
>
> 3) The name of the file you attached suggests that this is not the one
> that was causing you trouble. Your code references two different filenames.
>
> On July 10, 2022 8:50:23 PM PDT, Muhammad Zubair Chishti <
> mzchishti at eco.qau.edu.pk> wrote:
> >Dear Respected Professor,
> >You are right. I strongly agree with you. Besides, Thank you so much for
> >your kind reply. As you said, the data file mentioned in the above code is
> >attached herewith. Kindly consider my request.
> >
> > Data_oil_agri.xlsx
> ><
> https://drive.google.com/file/d/1heMlPfUtB-RthKYv4jRM3PGFI8CJ4fck/view?usp=drive_web
> >
> >
> >Regards
> >Muhammad Zubair Chishti
> >
> >On Sat, Jul 9, 2022 at 11:48 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> >wrote:
> >
> >> This is a mailing list whose members are random people with an interest
> in
> >> R. We are not a tech support call center. If we have time, and you
> invest
> >> some effort into making it easier for us to reproduce your problem, then
> >> someone may respond... but no guarantees. Your current efforts are
> missing
> >> key elements that impede our efforts and there isn't much here to
> persuade
> >> many people to dig further. You may garner more responses if you make
> the
> >> following changes to your approach:
> >>
> >> 1) Make your example reproducible. We need access to the data you are
> >> working with, or at least a small fake similar data set. We don't have
> your
> >> Excel files... dead end for us. But see next point about files. Also,
> >> verify that your code can start from a fresh R environment and
> demonstrate
> >> your problem... the reprex package may be useful for this.
> >>
> >> 2) Make your example minimal. If you are having trouble with data
> >> importing, give us the data file or something similar that gives you the
> >> same error. If you are having trouble with data calculations or
> >> manipulations, then use dput [1] to give us a snippet of the data you
> are
> >> referring to in the troublesome line of code... don't give us a data
> file
> >> and a bunch of code that isn't giving you trouble before the error
> code...
> >> we don't want to wade through it.
> >>
> >> 3) Remove garbage from your posts (formatting). Look at the extra
> >> asterisks in your message as we see it [2]. We cannot run this code
> without
> >> guessing which symbols belong and which don't. I know you did not put
> them
> >> there intentionally, but neither did you disable HTML formatting in your
> >> email client when you composed the message, and that was responsible for
> >> the extra mess. The Posting Guide warns you to post plain text, and
> there
> >> are way too many email client programs out there for us to be able to
> tell
> >> you how to alter this setting... but it is essential that you do it if
> you
> >> want to encourage responses here.
> >>
> >> I considered not responding at all due to these deficiencies, but
> figured
> >> I would spell out the problems you have missed in the Posting Guide
> once.
> >> Mailing lists are tough ways to get help... but the expertise here is
> kind
> >> of rare. You decide whether to make things easy or hard for yourself.
> >>
> >> As for the "con =" line below, it seems to have nothing at all to do
> with
> >> the code or data referred to earlier. That would be a hint for point 2
> >> above. Try calling the read_excel function alone without the rest of the
> >> indexing and matrix conversion. Only when the result you get from that
> step
> >> makes sense should you try indexing it or converting it. Maybe you need
> a
> >> "sheet=" argument?
> >>
> >> ---
> >> [1]
> >>
> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
> >>
> >> [2] https://stat.ethz.ch/pipermail/r-help/2022-July/475212.html
> >>
> >> On July 9, 2022 10:40:29 AM PDT, Muhammad Zubair Chishti <
> >> mzchishti at eco.qau.edu.pk> wrote:
> >> >Dear Experts,
> >> >A kind reminder. Please help me.
> >> >
> >> >
> >> >On Fri, 8 Jul 2022, 21:56 Muhammad Zubair Chishti, <
> >> mzchishti at eco.qau.edu.pk>
> >> >wrote:
> >> >
> >> >>
> >> >> *Dear Experts,*
> >> >> *Greetings from Pakistan*.
> >> >> *When I run the following code in R*
> >> >> library(frequencyConnectedness)
> >> >> library(readxl)
> >> >> ##Add data here##
> >> >> Data <- read_excel("Data_oil_agri.xlsx", sheet = "Sheet1")
> >> >> Data <- Data[,2:22]
> >> >> Data=na.omit(Data)
> >> >> Bnames=colnames(Data)
> >> >>
> >> >> lags=VARselect(Data, lag.max = 12, type = "const")? ## "const",
> "trend",
> >> >> "both", "none"
> >> >> p=lags$selection[[3]]
> >> >>
> >> >> est <- VAR(Data, p = p, type = "const")
> >> >> sp <- spilloverDY12(est, n.ahead = 10, no.corr = F)
> >> >>
> >> >> params_est = list(p = p, type = "const")
> >> >> sp_roll <- spilloverRollingDY12(Data, n.ahead = 10, no.corr = F,
> "VAR",
> >> >> params_est = params_est, window = 260)
> >> >> bounds <- c(pi+0.00001, pi/5, 0)
> >> >> sp_bk <- spilloverBK12(est, n.ahead = 100, no.corr = F, partition =
> >> bounds)
> >> >>
> >> >> sp_bk_roll <- spilloverRollingBK12(Data, n.ahead = 100, no.corr = F,
> >> >> func_est = "VAR", params_est = params_est, window = 260, partition =
> >> bounds)
> >> >>
> >> >> **Till now the code works well. After that, when I run the
> following:*
> >> >>
> >> >> con? ? ? = as.matrix(read_excel("DY_Table.xlsx")[,-1])*100
> >> >>
> >> >> *I face the following error:*
> >> >> *Error in* *`vectbl_as_col_location()`:*
> >> >> ! Can't negate columns past the end.
> >> >> ? Location 1 doesn't exist.
> >> >> ? There are only 0 columns.
> >> >> Run `rlang::last_error()` to see where the error occurred.
> >> >>
> >> >> Kindly please guide me.
> >> >>
> >> >> Regards
> >> >> Muhammad Zubair Chishti
> >> >> Ph.D. Student
> >> >> School of Business,
> >> >> Zhengzhou University, Henan, China.
> >> >> My Google scholar link:
> >> >> https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
> >> >> My ResearchGate link:
> >> >> https://www.researchgate.net/profile/Muhammad-Chishti
> >> >>
> >> >
> >> >? ? ? [[alternative HTML version deleted]]
> >> >
> >> >______________________________________________
> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >https://stat.ethz.ch/mailman/listinfo/r-help
> >> >PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> >and provide commented, minimal, self-contained, reproducible code.
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>

??? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From pd@|gd @end|ng |rom gm@||@com  Mon Jul 11 09:28:05 2022
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Mon, 11 Jul 2022 09:28:05 +0200
Subject: [R] Cumulative probability from binomial distribution
In-Reply-To: <CA+dpOJmELwQoVJXnJS9yKoZ8kBmY8N3oZqLw43bwsDSYHGv54g@mail.gmail.com>
References: <CA+dpOJmELwQoVJXnJS9yKoZ8kBmY8N3oZqLw43bwsDSYHGv54g@mail.gmail.com>
Message-ID: <BC87D684-B0E8-4989-90BF-4E4428BA9C26@gmail.com>

It means that the probability of an outcome less than or equal to 0.10 is 0.6. 

0 is the only possible such outcome but that is not a problem for the definition, and it is fairly common to use cumulative distribution functions for discrete outcomes defined as right-continuous step functions on the real line.

-pd

> On 30 Jun 2022, at 15:08 , Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
> 
> Hi,
> 
> I have the below output.
> 
>> pbinom(0.10, 1, 0.40)
> 
> [1] 0.6
> 
> I am curious what it means to serve a fraction as the first argument
> in pbinom()?
> 
> Thanks for your time
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From r@oknz @end|ng |rom gm@||@com  Mon Jul 11 12:05:21 2022
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Mon, 11 Jul 2022 22:05:21 +1200
Subject: [R] Please guide
In-Reply-To: <CAMfKi3KtWtuH-Zc6qhDcrN8bwhEeTiD9PbZB_m1L_XbejCR0sQ@mail.gmail.com>
References: <CAMfKi3KtWtuH-Zc6qhDcrN8bwhEeTiD9PbZB_m1L_XbejCR0sQ@mail.gmail.com>
Message-ID: <CABcYAdK__66+oL6-=zxC9w5+aKVQsvsyFyfkwbnArHsf_Ut=bA@mail.gmail.com>

(1) Your sample code refers to a file DY_Table.xlsx
    but the file you attached to a later message is
    called Data_oil_agri.xlsx and I find it hard to
    believe that they are the same file.

(2) gmail offered me two different ways to download
    the file, but neither of them worked.  Fortunately,
    the third way worked.

(3) Here's what I get with Data_oil_agri.xlsx
> d<-read_excel("Data_oil_agri.xlsx", sheet=1)
> str(d)

tibble [3,870 x 22] (S3: tbl_df/tbl/data.frame)
    So that's a table with 3870 rows and 22 columns.
    But your code got this error message:
! Can't negate columns past the end.
? Location 1 doesn't exist.
? There are only 0 columns.
Run `rlang::last_error()` to see where the error occurred.
    This is why I wonder if you really sent the right file.

    The error message told you to do something.
    WHAT DID YOU SEE WHEN YOU ENTERED THE COMMAND
    rlang::last_error()
    ?
When you are reading from an .xls or .xlsx file, it is always
a good idea to be explicit about which sheet you want.

In another message, you wrote something that seems very strange.
"Since the data is too large and I cannot cut it".
It's an Excel spreadsheet!  (Or rather, three sheets.)
OF COURSE YOU CAN CUT IT!  That's like saying
"Because this banana is on my plate and I have a knife,
 I cannot cut it."
If you do not have Excel (and why would you?  I don't myself)
you could use LibreOffice or Google Sheets or WPS Office
or even Python
https://stackabuse.com/reading-and-writing-excel-files-in-python-with-the-pandas-library/
Libre Office and Google Sheets have worked well for me.

These things you must do.
(A) Check that DY_Table.xlsx and Data_oil_agri.xlsx
    are the same.  If they are not, make sure you are
    using the right one.
(B) Open the file in a spreadsheet program and see
    what is actually there.  Write down on a piece of
    paper how many sheets there are, what the sheet
    you want is called, and how many rows and columns it has.
    What are the first few columns called?
(C) Start up a fresh R session and use read_excel to read
    the right sheet from the right file.
    Use head() on the result.  Does it have the right
    number of columns?  Do they have the right names?
    Does the result have the right number of rows?
(D) Try the rest of the failing line.
    x <- as.matrix(the.tibble[,-1])*100
(E) If anything goes wrong, DO WHAT THE ERROR MESSAGE SAYS
    TO DO.

Got that?

By the way, I'm old school.  I love the assignment arrow <-
and find the abuse of "=" confusing and ugly.  But you just
taught me that there is something even uglier and more
confusing, and that's randomly switching between them.  Pick
one and stick to it.

Oh yeah, one final point.  The line where you say your code
has trouble doesn't seem to be connected in any way to the
preceding lines.  We didn't need to see them.



On Sat, 9 Jul 2022 at 04:57, Muhammad Zubair Chishti <
mzchishti at eco.qau.edu.pk> wrote:

> *Dear Experts,*
> *Greetings from Pakistan*.
> *When I run the following code in R*
> library(frequencyConnectedness)
> library(readxl)
> ##Add data here##
> Data <- read_excel("Data_oil_agri.xlsx", sheet = "Sheet1")
> Data <- Data[,2:22]
> Data=na.omit(Data)
> Bnames=colnames(Data)
>
> lags=VARselect(Data, lag.max = 12, type = "const")  ## "const", "trend",
> "both", "none"
> p=lags$selection[[3]]
>
> est <- VAR(Data, p = p, type = "const")
> sp <- spilloverDY12(est, n.ahead = 10, no.corr = F)
>
> params_est = list(p = p, type = "const")
> sp_roll <- spilloverRollingDY12(Data, n.ahead = 10, no.corr = F, "VAR",
> params_est = params_est, window = 260)
> bounds <- c(pi+0.00001, pi/5, 0)
> sp_bk <- spilloverBK12(est, n.ahead = 100, no.corr = F, partition = bounds)
>
> sp_bk_roll <- spilloverRollingBK12(Data, n.ahead = 100, no.corr = F,
> func_est = "VAR", params_est = params_est, window = 260, partition =
> bounds)
>
> **Till now the code works well. After that, when I run the following:*
>
> con      = as.matrix(read_excel("DY_Table.xlsx")[,-1])*100
>
> *I face the following error:*
> *Error in* *`vectbl_as_col_location()`:*
> ! Can't negate columns past the end.
> ? Location 1 doesn't exist.
> ? There are only 0 columns.
> Run `rlang::last_error()` to see where the error occurred.
>
> Kindly please guide me.
>
> Regards
> Muhammad Zubair Chishti
> Ph.D. Student
> School of Business,
> Zhengzhou University, Henan, China.
> My Google scholar link:
> https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
> My ResearchGate link:
> https://www.researchgate.net/profile/Muhammad-Chishti
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ur@ @end|ng |rom k|e|nho|derm@nn@de  Mon Jul 11 12:26:51 2022
From: ur@ @end|ng |rom k|e|nho|derm@nn@de (Urs Kleinholdermann)
Date: Mon, 11 Jul 2022 12:26:51 +0200
Subject: [R] Cairo does not work on install of R 4.2.0 from source
Message-ID: <7d83fa41-abc3-45d9-7beb-90d52a9c63dc@kleinholdermann.de>

Dear R community

I'm running R on Ubuntu 20.04.4 LTS (GNU/Linux 5.4.0-121-generic 
x86_64). The standard install was R version 3.6.3 which I kept and where 
all my graphics tasks work fine. Now, since I needed some packages which 
don't work with this version of R, I installed R version 4.2.0 into /opt 
from source using checkinstall. With this version of R graphics using 
the normal "plot" command still get displayed, although with a different 
font as in R 3.6.3 (more coarse and monotype-ish, I'd say). When I want 
to save the plot using grDevices::savePlot however I get the folloing error:

Error in grDevices::savePlot("tmp.png") :
 ? can only copy from 'X11(type="*cairo")' devices

As I said, both versions of R are installed on the same machine, thus 
the necessary libraries should be available, I think. Can someone point 
me to the background of the problem?

Thanks a lot!

Urs


From kry|ov@r00t @end|ng |rom gm@||@com  Mon Jul 11 13:07:10 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Mon, 11 Jul 2022 14:07:10 +0300
Subject: [R] Cairo does not work on install of R 4.2.0 from source
In-Reply-To: <7d83fa41-abc3-45d9-7beb-90d52a9c63dc@kleinholdermann.de>
References: <7d83fa41-abc3-45d9-7beb-90d52a9c63dc@kleinholdermann.de>
Message-ID: <20220711140710.3fdbd0fd@arachnoid>

? Mon, 11 Jul 2022 12:26:51 +0200
Urs Kleinholdermann <urs at kleinholdermann.de> ?????:

> As I said, both versions of R are installed on the same machine, thus 
> the necessary libraries should be available, I think. 

See the configure.log. You do have the necessary libraries but not the
header files (in the lib*-dev packages) required to compile code to be
linked against them.

Does it help if you `sudo apt build-dep r-base` before compiling R from
source?

-- 
Best regards,
Ivan


From n@re@h_gurbux@n| @end|ng |rom hotm@||@com  Mon Jul 11 15:14:15 2022
From: n@re@h_gurbux@n| @end|ng |rom hotm@||@com (Naresh Gurbuxani)
Date: Mon, 11 Jul 2022 09:14:15 -0400
Subject: [R] Reference factors inside split
Message-ID: <BL0PR01MB40366DA100BDD4D2C909C68AFA879@BL0PR01MB4036.prod.exchangelabs.com>


I want to split my dataframe according to a list of factors.  Then, in
the resulting list, I want to reference the factors used in split.  Is
it possible?

Thanks,
Naresh

mydf <- data.frame(
date = rep(seq.Date(from = as.Date("2022-06-01"), by = 1, length.out =
10), 4),
account = c(rep("ABC", 20), rep("XYZ", 20)),
client = c(rep("P", 10), rep("Q", 10), rep("R", 10), rep("S", 10)),
profit = round(runif(40, 2, 5), 2), sale = round(runif(40, 10, 20), 2))

account.names <- data.frame(account = c("ABC", "DEF", "XYZ"),
corp = c("ABC Corporation", "DEF LLC", "XYZ Incorporated"))

mydf.split <- split(mydf, mydf$account)

# This does not work
myplots <- lapply(mydf.split, function(df) {
myts <- aggregate(sales ~ date, FUN = sum, data = df)
xyplot(sales ~ date, data = myts, main = account)})

# This works, but may have a large overhead
mydf <- merge(mydf, account.names, by = "account", all.x = TRUE)
mydf.split <- split(mydf, mydf$account)
myplots <- lapply(mydf.split, function(df) {
myts <- aggregate(sale ~ date, FUN = sum, data = df)
xyplot(sale ~ date, data = myts, main = unique(myts$corp))})

# Now I can print one plot at a time
myplots[["ABC"]]
myplots[["XYZ"]]


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Mon Jul 11 15:16:59 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Mon, 11 Jul 2022 18:16:59 +0500
Subject: [R] Please guide
In-Reply-To: <CABcYAdK__66+oL6-=zxC9w5+aKVQsvsyFyfkwbnArHsf_Ut=bA@mail.gmail.com>
References: <CAMfKi3KtWtuH-Zc6qhDcrN8bwhEeTiD9PbZB_m1L_XbejCR0sQ@mail.gmail.com>
 <CABcYAdK__66+oL6-=zxC9w5+aKVQsvsyFyfkwbnArHsf_Ut=bA@mail.gmail.com>
Message-ID: <CAMfKi3J8oxrDxg3gvtp=xJifoMzcAjEGzDZJYOijLG6n1JY7pw@mail.gmail.com>

Dear Respected Sir,
My exact question is how to develop "DY_Table.xlsx" for the following code:
(While considering [,-1])*100)
con      = as.matrix(read_excel("DY_Table.xlsx")[,-1])*100
I asked that code writer about it, he just reply to write the table. So, I
need to know how to write the table for the code mentioned above?
I hope that you got my question now.

Regards
Chishti

On Mon, Jul 11, 2022 at 3:05 PM Richard O'Keefe <raoknz at gmail.com> wrote:

> (1) Your sample code refers to a file DY_Table.xlsx
>     but the file you attached to a later message is
>     called Data_oil_agri.xlsx and I find it hard to
>     believe that they are the same file.
>
> (2) gmail offered me two different ways to download
>     the file, but neither of them worked.  Fortunately,
>     the third way worked.
>
> (3) Here's what I get with Data_oil_agri.xlsx
> > d<-read_excel("Data_oil_agri.xlsx", sheet=1)
> > str(d)
>
> tibble [3,870 x 22] (S3: tbl_df/tbl/data.frame)
>     So that's a table with 3870 rows and 22 columns.
>     But your code got this error message:
> ! Can't negate columns past the end.
> ? Location 1 doesn't exist.
> ? There are only 0 columns.
> Run `rlang::last_error()` to see where the error occurred.
>     This is why I wonder if you really sent the right file.
>
>     The error message told you to do something.
>     WHAT DID YOU SEE WHEN YOU ENTERED THE COMMAND
>     rlang::last_error()
>     ?
> When you are reading from an .xls or .xlsx file, it is always
> a good idea to be explicit about which sheet you want.
>
> In another message, you wrote something that seems very strange.
> "Since the data is too large and I cannot cut it".
> It's an Excel spreadsheet!  (Or rather, three sheets.)
> OF COURSE YOU CAN CUT IT!  That's like saying
> "Because this banana is on my plate and I have a knife,
>  I cannot cut it."
> If you do not have Excel (and why would you?  I don't myself)
> you could use LibreOffice or Google Sheets or WPS Office
> or even Python
> https://stackabuse.com/reading-and-writing-excel-files-in-python-with-the-pandas-library/
> Libre Office and Google Sheets have worked well for me.
>
> These things you must do.
> (A) Check that DY_Table.xlsx and Data_oil_agri.xlsx
>     are the same.  If they are not, make sure you are
>     using the right one.
> (B) Open the file in a spreadsheet program and see
>     what is actually there.  Write down on a piece of
>     paper how many sheets there are, what the sheet
>     you want is called, and how many rows and columns it has.
>     What are the first few columns called?
> (C) Start up a fresh R session and use read_excel to read
>     the right sheet from the right file.
>     Use head() on the result.  Does it have the right
>     number of columns?  Do they have the right names?
>     Does the result have the right number of rows?
> (D) Try the rest of the failing line.
>     x <- as.matrix(the.tibble[,-1])*100
> (E) If anything goes wrong, DO WHAT THE ERROR MESSAGE SAYS
>     TO DO.
>
> Got that?
>
> By the way, I'm old school.  I love the assignment arrow <-
> and find the abuse of "=" confusing and ugly.  But you just
> taught me that there is something even uglier and more
> confusing, and that's randomly switching between them.  Pick
> one and stick to it.
>
> Oh yeah, one final point.  The line where you say your code
> has trouble doesn't seem to be connected in any way to the
> preceding lines.  We didn't need to see them.
>
>
>
> On Sat, 9 Jul 2022 at 04:57, Muhammad Zubair Chishti <
> mzchishti at eco.qau.edu.pk> wrote:
>
>> *Dear Experts,*
>> *Greetings from Pakistan*.
>> *When I run the following code in R*
>> library(frequencyConnectedness)
>> library(readxl)
>> ##Add data here##
>> Data <- read_excel("Data_oil_agri.xlsx", sheet = "Sheet1")
>> Data <- Data[,2:22]
>> Data=na.omit(Data)
>> Bnames=colnames(Data)
>>
>> lags=VARselect(Data, lag.max = 12, type = "const")  ## "const", "trend",
>> "both", "none"
>> p=lags$selection[[3]]
>>
>> est <- VAR(Data, p = p, type = "const")
>> sp <- spilloverDY12(est, n.ahead = 10, no.corr = F)
>>
>> params_est = list(p = p, type = "const")
>> sp_roll <- spilloverRollingDY12(Data, n.ahead = 10, no.corr = F, "VAR",
>> params_est = params_est, window = 260)
>> bounds <- c(pi+0.00001, pi/5, 0)
>> sp_bk <- spilloverBK12(est, n.ahead = 100, no.corr = F, partition =
>> bounds)
>>
>> sp_bk_roll <- spilloverRollingBK12(Data, n.ahead = 100, no.corr = F,
>> func_est = "VAR", params_est = params_est, window = 260, partition =
>> bounds)
>>
>> **Till now the code works well. After that, when I run the following:*
>>
>> con      = as.matrix(read_excel("DY_Table.xlsx")[,-1])*100
>>
>> *I face the following error:*
>> *Error in* *`vectbl_as_col_location()`:*
>> ! Can't negate columns past the end.
>> ? Location 1 doesn't exist.
>> ? There are only 0 columns.
>> Run `rlang::last_error()` to see where the error occurred.
>>
>> Kindly please guide me.
>>
>> Regards
>> Muhammad Zubair Chishti
>> Ph.D. Student
>> School of Business,
>> Zhengzhou University, Henan, China.
>> My Google scholar link:
>> https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
>> My ResearchGate link:
>> https://www.researchgate.net/profile/Muhammad-Chishti
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From r@oknz @end|ng |rom gm@||@com  Mon Jul 11 15:29:32 2022
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Tue, 12 Jul 2022 01:29:32 +1200
Subject: [R] Please guide
In-Reply-To: <CAMfKi3J8oxrDxg3gvtp=xJifoMzcAjEGzDZJYOijLG6n1JY7pw@mail.gmail.com>
References: <CAMfKi3KtWtuH-Zc6qhDcrN8bwhEeTiD9PbZB_m1L_XbejCR0sQ@mail.gmail.com>
 <CABcYAdK__66+oL6-=zxC9w5+aKVQsvsyFyfkwbnArHsf_Ut=bA@mail.gmail.com>
 <CAMfKi3J8oxrDxg3gvtp=xJifoMzcAjEGzDZJYOijLG6n1JY7pw@mail.gmail.com>
Message-ID: <CABcYAdJSHSE5-4yjU3xtnDoPYJhYFD3ZZX8nWxBG+W5xwiyHwg@mail.gmail.com>

ARGH!
Now I understand less than ever!
Previously you said that you HAD a file and that
you got an error message when you tried to read
and process it.  You showed us the line of code.
You sent us what was supposed to be a copy of the file!

Now you say that
 - is not your code
 - the person who did write the code told you
   "just write the table"
 - you want us to tell you how to write the table.

Nobody is paying me for my time.  In fact if I hadn't
had to run the dishes through the dishwasher a second
time I wouldn't even be awake to do it.

We do not even know
 - what the code is supposed to do
 - what information is supposed to be in the table
 - what table you are actually talking about
   * is it the one of the sheets in the XLSX file?
   * is it the tibble that read_excel creates?
 - what you mean by "write the table"

It sounds as though there is someone well placed to help
you who refuses to do so.  Actually, there is someone
else who can help you, and does not seem to be interested
in doing so, and that is you yourself.  I gave you a list
of things to do.  Did you do them?


On Tue, 12 Jul 2022 at 01:17, Muhammad Zubair Chishti <
mzchishti at eco.qau.edu.pk> wrote:

> Dear Respected Sir,
> My exact question is how to develop "DY_Table.xlsx" for the following
> code: (While considering [,-1])*100)
> con      = as.matrix(read_excel("DY_Table.xlsx")[,-1])*100
> I asked that code writer about it, he just reply to write the table. So, I
> need to know how to write the table for the code mentioned above?
> I hope that you got my question now.
>
> Regards
> Chishti
>
> On Mon, Jul 11, 2022 at 3:05 PM Richard O'Keefe <raoknz at gmail.com> wrote:
>
>> (1) Your sample code refers to a file DY_Table.xlsx
>>     but the file you attached to a later message is
>>     called Data_oil_agri.xlsx and I find it hard to
>>     believe that they are the same file.
>>
>> (2) gmail offered me two different ways to download
>>     the file, but neither of them worked.  Fortunately,
>>     the third way worked.
>>
>> (3) Here's what I get with Data_oil_agri.xlsx
>> > d<-read_excel("Data_oil_agri.xlsx", sheet=1)
>> > str(d)
>>
>> tibble [3,870 x 22] (S3: tbl_df/tbl/data.frame)
>>     So that's a table with 3870 rows and 22 columns.
>>     But your code got this error message:
>> ! Can't negate columns past the end.
>> ? Location 1 doesn't exist.
>> ? There are only 0 columns.
>> Run `rlang::last_error()` to see where the error occurred.
>>     This is why I wonder if you really sent the right file.
>>
>>     The error message told you to do something.
>>     WHAT DID YOU SEE WHEN YOU ENTERED THE COMMAND
>>     rlang::last_error()
>>     ?
>> When you are reading from an .xls or .xlsx file, it is always
>> a good idea to be explicit about which sheet you want.
>>
>> In another message, you wrote something that seems very strange.
>> "Since the data is too large and I cannot cut it".
>> It's an Excel spreadsheet!  (Or rather, three sheets.)
>> OF COURSE YOU CAN CUT IT!  That's like saying
>> "Because this banana is on my plate and I have a knife,
>>  I cannot cut it."
>> If you do not have Excel (and why would you?  I don't myself)
>> you could use LibreOffice or Google Sheets or WPS Office
>> or even Python
>> https://stackabuse.com/reading-and-writing-excel-files-in-python-with-the-pandas-library/
>> Libre Office and Google Sheets have worked well for me.
>>
>> These things you must do.
>> (A) Check that DY_Table.xlsx and Data_oil_agri.xlsx
>>     are the same.  If they are not, make sure you are
>>     using the right one.
>> (B) Open the file in a spreadsheet program and see
>>     what is actually there.  Write down on a piece of
>>     paper how many sheets there are, what the sheet
>>     you want is called, and how many rows and columns it has.
>>     What are the first few columns called?
>> (C) Start up a fresh R session and use read_excel to read
>>     the right sheet from the right file.
>>     Use head() on the result.  Does it have the right
>>     number of columns?  Do they have the right names?
>>     Does the result have the right number of rows?
>> (D) Try the rest of the failing line.
>>     x <- as.matrix(the.tibble[,-1])*100
>> (E) If anything goes wrong, DO WHAT THE ERROR MESSAGE SAYS
>>     TO DO.
>>
>> Got that?
>>
>> By the way, I'm old school.  I love the assignment arrow <-
>> and find the abuse of "=" confusing and ugly.  But you just
>> taught me that there is something even uglier and more
>> confusing, and that's randomly switching between them.  Pick
>> one and stick to it.
>>
>> Oh yeah, one final point.  The line where you say your code
>> has trouble doesn't seem to be connected in any way to the
>> preceding lines.  We didn't need to see them.
>>
>>
>>
>> On Sat, 9 Jul 2022 at 04:57, Muhammad Zubair Chishti <
>> mzchishti at eco.qau.edu.pk> wrote:
>>
>>> *Dear Experts,*
>>> *Greetings from Pakistan*.
>>> *When I run the following code in R*
>>> library(frequencyConnectedness)
>>> library(readxl)
>>> ##Add data here##
>>> Data <- read_excel("Data_oil_agri.xlsx", sheet = "Sheet1")
>>> Data <- Data[,2:22]
>>> Data=na.omit(Data)
>>> Bnames=colnames(Data)
>>>
>>> lags=VARselect(Data, lag.max = 12, type = "const")  ## "const", "trend",
>>> "both", "none"
>>> p=lags$selection[[3]]
>>>
>>> est <- VAR(Data, p = p, type = "const")
>>> sp <- spilloverDY12(est, n.ahead = 10, no.corr = F)
>>>
>>> params_est = list(p = p, type = "const")
>>> sp_roll <- spilloverRollingDY12(Data, n.ahead = 10, no.corr = F, "VAR",
>>> params_est = params_est, window = 260)
>>> bounds <- c(pi+0.00001, pi/5, 0)
>>> sp_bk <- spilloverBK12(est, n.ahead = 100, no.corr = F, partition =
>>> bounds)
>>>
>>> sp_bk_roll <- spilloverRollingBK12(Data, n.ahead = 100, no.corr = F,
>>> func_est = "VAR", params_est = params_est, window = 260, partition =
>>> bounds)
>>>
>>> **Till now the code works well. After that, when I run the following:*
>>>
>>> con      = as.matrix(read_excel("DY_Table.xlsx")[,-1])*100
>>>
>>> *I face the following error:*
>>> *Error in* *`vectbl_as_col_location()`:*
>>> ! Can't negate columns past the end.
>>> ? Location 1 doesn't exist.
>>> ? There are only 0 columns.
>>> Run `rlang::last_error()` to see where the error occurred.
>>>
>>> Kindly please guide me.
>>>
>>> Regards
>>> Muhammad Zubair Chishti
>>> Ph.D. Student
>>> School of Business,
>>> Zhengzhou University, Henan, China.
>>> My Google scholar link:
>>> https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
>>> My ResearchGate link:
>>> https://www.researchgate.net/profile/Muhammad-Chishti
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>

	[[alternative HTML version deleted]]


From btupper @end|ng |rom b|ge|ow@org  Mon Jul 11 16:00:22 2022
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Mon, 11 Jul 2022 10:00:22 -0400
Subject: [R] Reference factors inside split
In-Reply-To: <BL0PR01MB40366DA100BDD4D2C909C68AFA879@BL0PR01MB4036.prod.exchangelabs.com>
References: <BL0PR01MB40366DA100BDD4D2C909C68AFA879@BL0PR01MB4036.prod.exchangelabs.com>
Message-ID: <CALrbzg1sC1x3K8yxdwb=y1YeQkDKjwwApaF4eO1-inzdAeDRCg@mail.gmail.com>

Hi,

The grouping variable is removed from the subgroups when you split.
Instead of iterating over the elements of the split list, you can
iterate over the **names** of the elements.  In your case the account
name is the grouping variable.


##start

library(lattice)
mydf <- data.frame(
  date = rep(seq.Date(from = as.Date("2022-06-01"), by = 1, length.out =
                        10), 4),
  account = c(rep("ABC", 20), rep("XYZ", 20)),
  client = c(rep("P", 10), rep("Q", 10), rep("R", 10), rep("S", 10)),
  profit = round(runif(40, 2, 5), 2), sale = round(runif(40, 10, 20), 2))

account.names <- data.frame(account = c("ABC", "DEF", "XYZ"),
                            corp = c("ABC Corporation", "DEF LLC",
"XYZ Incorporated"))

mydf.split <- split(mydf, mydf$account)

myplots <- sapply(names(mydf.split),
  function(name, x = NULL) {
    df <- x[[name]]
    myts <- aggregate(sale ~ date, FUN = sum, data = df)
    xyplot(sale ~ date, data = myts, main = name)
  }, x = mydf.split, USE.NAMES = TRUE, simplify = FALSE)

myplots[["ABC"]]
myplots[["XYZ"]]

## end

Does that help?

On Mon, Jul 11, 2022 at 9:14 AM Naresh Gurbuxani
<naresh_gurbuxani at hotmail.com> wrote:
>
>
> I want to split my dataframe according to a list of factors.  Then, in
> the resulting list, I want to reference the factors used in split.  Is
> it possible?
>
> Thanks,
> Naresh
>
> mydf <- data.frame(
> date = rep(seq.Date(from = as.Date("2022-06-01"), by = 1, length.out =
> 10), 4),
> account = c(rep("ABC", 20), rep("XYZ", 20)),
> client = c(rep("P", 10), rep("Q", 10), rep("R", 10), rep("S", 10)),
> profit = round(runif(40, 2, 5), 2), sale = round(runif(40, 10, 20), 2))
>
> account.names <- data.frame(account = c("ABC", "DEF", "XYZ"),
> corp = c("ABC Corporation", "DEF LLC", "XYZ Incorporated"))
>
> mydf.split <- split(mydf, mydf$account)
>
> # This does not work
> myplots <- lapply(mydf.split, function(df) {
> myts <- aggregate(sales ~ date, FUN = sum, data = df)
> xyplot(sales ~ date, data = myts, main = account)})
>
> # This works, but may have a large overhead
> mydf <- merge(mydf, account.names, by = "account", all.x = TRUE)
> mydf.split <- split(mydf, mydf$account)
> myplots <- lapply(mydf.split, function(df) {
> myts <- aggregate(sale ~ date, FUN = sum, data = df)
> xyplot(sale ~ date, data = myts, main = unique(myts$corp))})
>
> # Now I can print one plot at a time
> myplots[["ABC"]]
> myplots[["XYZ"]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Ben Tupper (he/him)
Bigelow Laboratory for Ocean Science
East Boothbay, Maine
http://www.bigelow.org/
https://eco.bigelow.org


From n@re@h_gurbux@n| @end|ng |rom hotm@||@com  Mon Jul 11 16:33:31 2022
From: n@re@h_gurbux@n| @end|ng |rom hotm@||@com (Naresh Gurbuxani)
Date: Mon, 11 Jul 2022 14:33:31 +0000
Subject: [R] Reference factors inside split
In-Reply-To: <CALrbzg1sC1x3K8yxdwb=y1YeQkDKjwwApaF4eO1-inzdAeDRCg@mail.gmail.com>
References: <BL0PR01MB40366DA100BDD4D2C909C68AFA879@BL0PR01MB4036.prod.exchangelabs.com>
 <CALrbzg1sC1x3K8yxdwb=y1YeQkDKjwwApaF4eO1-inzdAeDRCg@mail.gmail.com>
Message-ID: <BL0PR01MB4036193D2F64E3509AFFAAF3FA879@BL0PR01MB4036.prod.exchangelabs.com>

This is what I was looking for.  Thanks for your quick response and elegant solution.

Naresh

Sent from my iPhone

> On Jul 11, 2022, at 10:00 AM, Ben Tupper <btupper at bigelow.org> wrote:
> 
> ?Hi,
> 
> The grouping variable is removed from the subgroups when you split.
> Instead of iterating over the elements of the split list, you can
> iterate over the **names** of the elements.  In your case the account
> name is the grouping variable.
> 
> 
> ##start
> 
> library(lattice)
> mydf <- data.frame(
>  date = rep(seq.Date(from = as.Date("2022-06-01"), by = 1, length.out =
>                        10), 4),
>  account = c(rep("ABC", 20), rep("XYZ", 20)),
>  client = c(rep("P", 10), rep("Q", 10), rep("R", 10), rep("S", 10)),
>  profit = round(runif(40, 2, 5), 2), sale = round(runif(40, 10, 20), 2))
> 
> account.names <- data.frame(account = c("ABC", "DEF", "XYZ"),
>                            corp = c("ABC Corporation", "DEF LLC",
> "XYZ Incorporated"))
> 
> mydf.split <- split(mydf, mydf$account)
> 
> myplots <- sapply(names(mydf.split),
>  function(name, x = NULL) {
>    df <- x[[name]]
>    myts <- aggregate(sale ~ date, FUN = sum, data = df)
>    xyplot(sale ~ date, data = myts, main = name)
>  }, x = mydf.split, USE.NAMES = TRUE, simplify = FALSE)
> 
> myplots[["ABC"]]
> myplots[["XYZ"]]
> 
> ## end
> 
> Does that help?
> 
>> On Mon, Jul 11, 2022 at 9:14 AM Naresh Gurbuxani
>> <naresh_gurbuxani at hotmail.com> wrote:
>> 
>> 
>> I want to split my dataframe according to a list of factors.  Then, in
>> the resulting list, I want to reference the factors used in split.  Is
>> it possible?
>> 
>> Thanks,
>> Naresh
>> 
>> mydf <- data.frame(
>> date = rep(seq.Date(from = as.Date("2022-06-01"), by = 1, length.out =
>> 10), 4),
>> account = c(rep("ABC", 20), rep("XYZ", 20)),
>> client = c(rep("P", 10), rep("Q", 10), rep("R", 10), rep("S", 10)),
>> profit = round(runif(40, 2, 5), 2), sale = round(runif(40, 10, 20), 2))
>> 
>> account.names <- data.frame(account = c("ABC", "DEF", "XYZ"),
>> corp = c("ABC Corporation", "DEF LLC", "XYZ Incorporated"))
>> 
>> mydf.split <- split(mydf, mydf$account)
>> 
>> # This does not work
>> myplots <- lapply(mydf.split, function(df) {
>> myts <- aggregate(sales ~ date, FUN = sum, data = df)
>> xyplot(sales ~ date, data = myts, main = account)})
>> 
>> # This works, but may have a large overhead
>> mydf <- merge(mydf, account.names, by = "account", all.x = TRUE)
>> mydf.split <- split(mydf, mydf$account)
>> myplots <- lapply(mydf.split, function(df) {
>> myts <- aggregate(sale ~ date, FUN = sum, data = df)
>> xyplot(sale ~ date, data = myts, main = unique(myts$corp))})
>> 
>> # Now I can print one plot at a time
>> myplots[["ABC"]]
>> myplots[["XYZ"]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
> -- 
> Ben Tupper (he/him)
> Bigelow Laboratory for Ocean Science
> East Boothbay, Maine
> http://www.bigelow.org/
> https://eco.bigelow.org

From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Mon Jul 11 18:37:06 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Mon, 11 Jul 2022 21:37:06 +0500
Subject: [R] Please guide
In-Reply-To: <CAMfKi3KxugKNZnZC0b4qhVSX8bfbQHXP+jLJzKic5+42a5eBrw@mail.gmail.com>
References: <CAMfKi3KtWtuH-Zc6qhDcrN8bwhEeTiD9PbZB_m1L_XbejCR0sQ@mail.gmail.com>
 <CABcYAdK__66+oL6-=zxC9w5+aKVQsvsyFyfkwbnArHsf_Ut=bA@mail.gmail.com>
 <CAMfKi3J8oxrDxg3gvtp=xJifoMzcAjEGzDZJYOijLG6n1JY7pw@mail.gmail.com>
 <CABcYAdJSHSE5-4yjU3xtnDoPYJhYFD3ZZX8nWxBG+W5xwiyHwg@mail.gmail.com>
 <CAMfKi3KxugKNZnZC0b4qhVSX8bfbQHXP+jLJzKic5+42a5eBrw@mail.gmail.com>
Message-ID: <CAMfKi3LZf=gh=9QkaeGs0b69isy+CtXbXWoWZ=_68RQO=a5WXg@mail.gmail.com>

Dear Respected Sir,
Accept my deepest apologies for not explaining my issue clearly and taking
your precious time.
I hope that my this mistake will not discourage you to help others in
future.
Again, sorry for that.

Regards
Muhammad Zubair Chishti
PhD student, School of Business,
Zhengzhou University, Henan, China

On Mon, 11 Jul 2022, 21:36 Muhammad Zubair Chishti, <
mzchishti at eco.qau.edu.pk> wrote:

> Dear Respected Sir,
> Accept my deepest apologies for not explaining my issue clearly and taking
> your precious time.
> I hope that my this mistake will not discourage you to help others in
> future.
> Again, sorry for that.
>
> Regards
> Muhammad Zubair Chishti
> PhD student, School of Business,
> Zhengzhou University, Henan, China
>
> On Mon, 11 Jul 2022, 18:29 Richard O'Keefe, <raoknz at gmail.com> wrote:
>
>> ARGH!
>> Now I understand less than ever!
>> Previously you said that you HAD a file and that
>> you got an error message when you tried to read
>> and process it.  You showed us the line of code.
>> You sent us what was supposed to be a copy of the file!
>>
>> Now you say that
>>  - is not your code
>>  - the person who did write the code told you
>>    "just write the table"
>>  - you want us to tell you how to write the table.
>>
>> Nobody is paying me for my time.  In fact if I hadn't
>> had to run the dishes through the dishwasher a second
>> time I wouldn't even be awake to do it.
>>
>> We do not even know
>>  - what the code is supposed to do
>>  - what information is supposed to be in the table
>>  - what table you are actually talking about
>>    * is it the one of the sheets in the XLSX file?
>>    * is it the tibble that read_excel creates?
>>  - what you mean by "write the table"
>>
>> It sounds as though there is someone well placed to help
>> you who refuses to do so.  Actually, there is someone
>> else who can help you, and does not seem to be interested
>> in doing so, and that is you yourself.  I gave you a list
>> of things to do.  Did you do them?
>>
>>
>> On Tue, 12 Jul 2022 at 01:17, Muhammad Zubair Chishti <
>> mzchishti at eco.qau.edu.pk> wrote:
>>
>>> Dear Respected Sir,
>>> My exact question is how to develop "DY_Table.xlsx" for the following
>>> code: (While considering [,-1])*100)
>>> con      = as.matrix(read_excel("DY_Table.xlsx")[,-1])*100
>>> I asked that code writer about it, he just reply to write the table. So,
>>> I need to know how to write the table for the code mentioned above?
>>> I hope that you got my question now.
>>>
>>> Regards
>>> Chishti
>>>
>>> On Mon, Jul 11, 2022 at 3:05 PM Richard O'Keefe <raoknz at gmail.com>
>>> wrote:
>>>
>>>> (1) Your sample code refers to a file DY_Table.xlsx
>>>>     but the file you attached to a later message is
>>>>     called Data_oil_agri.xlsx and I find it hard to
>>>>     believe that they are the same file.
>>>>
>>>> (2) gmail offered me two different ways to download
>>>>     the file, but neither of them worked.  Fortunately,
>>>>     the third way worked.
>>>>
>>>> (3) Here's what I get with Data_oil_agri.xlsx
>>>> > d<-read_excel("Data_oil_agri.xlsx", sheet=1)
>>>> > str(d)
>>>>
>>>> tibble [3,870 x 22] (S3: tbl_df/tbl/data.frame)
>>>>     So that's a table with 3870 rows and 22 columns.
>>>>     But your code got this error message:
>>>> ! Can't negate columns past the end.
>>>> ? Location 1 doesn't exist.
>>>> ? There are only 0 columns.
>>>> Run `rlang::last_error()` to see where the error occurred.
>>>>     This is why I wonder if you really sent the right file.
>>>>
>>>>     The error message told you to do something.
>>>>     WHAT DID YOU SEE WHEN YOU ENTERED THE COMMAND
>>>>     rlang::last_error()
>>>>     ?
>>>> When you are reading from an .xls or .xlsx file, it is always
>>>> a good idea to be explicit about which sheet you want.
>>>>
>>>> In another message, you wrote something that seems very strange.
>>>> "Since the data is too large and I cannot cut it".
>>>> It's an Excel spreadsheet!  (Or rather, three sheets.)
>>>> OF COURSE YOU CAN CUT IT!  That's like saying
>>>> "Because this banana is on my plate and I have a knife,
>>>>  I cannot cut it."
>>>> If you do not have Excel (and why would you?  I don't myself)
>>>> you could use LibreOffice or Google Sheets or WPS Office
>>>> or even Python
>>>> https://stackabuse.com/reading-and-writing-excel-files-in-python-with-the-pandas-library/
>>>> Libre Office and Google Sheets have worked well for me.
>>>>
>>>> These things you must do.
>>>> (A) Check that DY_Table.xlsx and Data_oil_agri.xlsx
>>>>     are the same.  If they are not, make sure you are
>>>>     using the right one.
>>>> (B) Open the file in a spreadsheet program and see
>>>>     what is actually there.  Write down on a piece of
>>>>     paper how many sheets there are, what the sheet
>>>>     you want is called, and how many rows and columns it has.
>>>>     What are the first few columns called?
>>>> (C) Start up a fresh R session and use read_excel to read
>>>>     the right sheet from the right file.
>>>>     Use head() on the result.  Does it have the right
>>>>     number of columns?  Do they have the right names?
>>>>     Does the result have the right number of rows?
>>>> (D) Try the rest of the failing line.
>>>>     x <- as.matrix(the.tibble[,-1])*100
>>>> (E) If anything goes wrong, DO WHAT THE ERROR MESSAGE SAYS
>>>>     TO DO.
>>>>
>>>> Got that?
>>>>
>>>> By the way, I'm old school.  I love the assignment arrow <-
>>>> and find the abuse of "=" confusing and ugly.  But you just
>>>> taught me that there is something even uglier and more
>>>> confusing, and that's randomly switching between them.  Pick
>>>> one and stick to it.
>>>>
>>>> Oh yeah, one final point.  The line where you say your code
>>>> has trouble doesn't seem to be connected in any way to the
>>>> preceding lines.  We didn't need to see them.
>>>>
>>>>
>>>>
>>>> On Sat, 9 Jul 2022 at 04:57, Muhammad Zubair Chishti <
>>>> mzchishti at eco.qau.edu.pk> wrote:
>>>>
>>>>> *Dear Experts,*
>>>>> *Greetings from Pakistan*.
>>>>> *When I run the following code in R*
>>>>> library(frequencyConnectedness)
>>>>> library(readxl)
>>>>> ##Add data here##
>>>>> Data <- read_excel("Data_oil_agri.xlsx", sheet = "Sheet1")
>>>>> Data <- Data[,2:22]
>>>>> Data=na.omit(Data)
>>>>> Bnames=colnames(Data)
>>>>>
>>>>> lags=VARselect(Data, lag.max = 12, type = "const")  ## "const",
>>>>> "trend",
>>>>> "both", "none"
>>>>> p=lags$selection[[3]]
>>>>>
>>>>> est <- VAR(Data, p = p, type = "const")
>>>>> sp <- spilloverDY12(est, n.ahead = 10, no.corr = F)
>>>>>
>>>>> params_est = list(p = p, type = "const")
>>>>> sp_roll <- spilloverRollingDY12(Data, n.ahead = 10, no.corr = F, "VAR",
>>>>> params_est = params_est, window = 260)
>>>>> bounds <- c(pi+0.00001, pi/5, 0)
>>>>> sp_bk <- spilloverBK12(est, n.ahead = 100, no.corr = F, partition =
>>>>> bounds)
>>>>>
>>>>> sp_bk_roll <- spilloverRollingBK12(Data, n.ahead = 100, no.corr = F,
>>>>> func_est = "VAR", params_est = params_est, window = 260, partition =
>>>>> bounds)
>>>>>
>>>>> **Till now the code works well. After that, when I run the following:*
>>>>>
>>>>> con      = as.matrix(read_excel("DY_Table.xlsx")[,-1])*100
>>>>>
>>>>> *I face the following error:*
>>>>> *Error in* *`vectbl_as_col_location()`:*
>>>>> ! Can't negate columns past the end.
>>>>> ? Location 1 doesn't exist.
>>>>> ? There are only 0 columns.
>>>>> Run `rlang::last_error()` to see where the error occurred.
>>>>>
>>>>> Kindly please guide me.
>>>>>
>>>>> Regards
>>>>> Muhammad Zubair Chishti
>>>>> Ph.D. Student
>>>>> School of Business,
>>>>> Zhengzhou University, Henan, China.
>>>>> My Google scholar link:
>>>>> https://scholar.google.com/citationshl=en&user=YPqNJMwAAAAJ
>>>>> My ResearchGate link:
>>>>> https://www.researchgate.net/profile/Muhammad-Chishti
>>>>>
>>>>>         [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>

	[[alternative HTML version deleted]]


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Mon Jul 11 18:38:06 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Mon, 11 Jul 2022 21:38:06 +0500
Subject: [R] Please guide
In-Reply-To: <865356.1657556979@apollo2.minshall.org>
References: <CAMfKi3KtWtuH-Zc6qhDcrN8bwhEeTiD9PbZB_m1L_XbejCR0sQ@mail.gmail.com>
 <865356.1657556979@apollo2.minshall.org>
Message-ID: <CAMfKi3L+KRO+B3kLb8xd_58ycsXHazyoqv0T3reL+hL4LTX9Bg@mail.gmail.com>

Dear Respected Sir,
Thank you for kind and informative email. I will definitely follow your
instructions in my next mail.

Regards
Muhammad Zubair Chishti

On Mon, 11 Jul 2022, 21:29 Greg Minshall, <minshall at umich.edu> wrote:

> Muhammad,
>
> sorry for some of the flak you are getting (about honorifics).
>
> but, one thing you could do which i think would improve your
> interactions with the list is learn how to configure your e-mail program
> to *NOT* send HTML e-mail when sending to the list.
>
> if it is configured correctly, your e-mails to the list, that you
> receive as copies *from* the list, will *not* have this line in them:
> ----
> >       [[alternative HTML version deleted]]
> ----
>
> the problem is that the people on the list see the plain text version of
> your e-mail.  when composing HTML e-mail, you see a nicely formatted
> HTML version, but *we* see an often poorly formatted plain text version.
> it's often very frustrating trying to parse this version.
>
> if you compose your e-mail in plain text, *you* will see the plain text
> version, and can clean it up so that it is more legible to the rest of
> us.
>
> you have lots of other things to learn -- congratulations for that;
> learning is the best!
>
> good luck!
>
> cheers, Greg
>

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Tue Jul 12 19:03:50 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Tue, 12 Jul 2022 17:03:50 +0000
Subject: [R] rmoving dates from an xts object...
In-Reply-To: <CAGxFJbR2Q8XuVW3-MsbmCNBauvh+xRuDSB_rL=2Fm04rWgGYhw@mail.gmail.com>
References: <PU4P216MB1568885A7CD531D6620DC8CDC8849@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <CAGxFJbR2Q8XuVW3-MsbmCNBauvh+xRuDSB_rL=2Fm04rWgGYhw@mail.gmail.com>
Message-ID: <PU4P216MB1568BFFEB87D0F69B3D23304C8869@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

dear Bert,
                 Went through it........thanks a lot...

Yours sincerely,
AKSHAY M KULKARNI
________________________________
From: Bert Gunter <bgunter.4567 at gmail.com>
Sent: Sunday, July 10, 2022 10:05 PM
To: akshay kulkarni <akshay_e4 at hotmail.com>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] rmoving dates from an xts object...

Time to do your homework:

https://rpubs.com/odenipinedo/manipulating-time-series-data-with-xts-and-zoo-in-R

Bert

On Sun, Jul 10, 2022, 6:42 PM akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>> wrote:
Dear members,
                         I have OHLC data of 500 stocks: OHLCData and dates. These are of xts object. I want to do the following:


  1.  I want to remove a contiguous set of dates from the xts object. for example, I want to remove data from the OHLC data from "2022-20-7"
  2.   to "2018-2-2", continuously.
  3.

       2. I want to remove a set of dates, which are not contiguous.

Any idea on how to accomplish it? I can write an intricate for loop, but any other method? Does an xts object behave like an atomic vector : OHLCData[[i]][-dates[[i]]] ?

Many thanks in advance....

THanking you,
Yours sincerely,
AKSHAY M KULKARNI

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Tue Jul 12 19:04:46 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Tue, 12 Jul 2022 17:04:46 +0000
Subject: [R] rmoving dates from an xts object...
In-Reply-To: <CAP01uRk-YSd-5CUCL9-19XsDnKgaZC8GUG7V5gis4fNPizjAEA@mail.gmail.com>
References: <PU4P216MB1568885A7CD531D6620DC8CDC8849@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <CAP01uRk-YSd-5CUCL9-19XsDnKgaZC8GUG7V5gis4fNPizjAEA@mail.gmail.com>
Message-ID: <PU4P216MB1568BCD4260BAEA822F3E87FC8869@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

dear Gabor,
                   THanks a lot....

Yours sincerely,
AKSHAYM KULKARNI
________________________________
From: Gabor Grothendieck <ggrothendieck at gmail.com>
Sent: Sunday, July 10, 2022 11:34 PM
To: akshay kulkarni <akshay_e4 at hotmail.com>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] rmoving dates from an xts object...

Look at the examples at the end of ?xts for more info.

  library(quantmod)
  getSymbols("AAPL")

  class(AAPL)
  ## [1] "xts" "zoo"

  range(time(AAPL))
  ## [1] "2007-01-03" "2022-07-08"

  # everything up to indicated date
  a1 <- AAPL["/2018-02-01"]

  # remove non consecutive dates
  d <- as.Date(c("2022-07-01", "2022-07-06")) # dates to remove
  a2 <- AAPL[ ! time(AAPL) %in% d ]

On Sun, Jul 10, 2022 at 11:42 AM akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>
> Dear members,
>                          I have OHLC data of 500 stocks: OHLCData and dates. These are of xts object. I want to do the following:
>
>
>   1.  I want to remove a contiguous set of dates from the xts object. for example, I want to remove data from the OHLC data from "2022-20-7"
>   2.   to "2018-2-2", continuously.
>   3.
>
>        2. I want to remove a set of dates, which are not contiguous.
>
> Any idea on how to accomplish it? I can write an intricate for loop, but any other method? Does an xts object behave like an atomic vector : OHLCData[[i]][-dates[[i]]] ?
>
> Many thanks in advance....
>
> THanking you,
> Yours sincerely,
> AKSHAY M KULKARNI
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



--
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com

	[[alternative HTML version deleted]]


From ccont|ngency @end|ng |rom gm@||@com  Wed Jul 13 01:36:06 2022
From: ccont|ngency @end|ng |rom gm@||@com (core_contingency)
Date: Tue, 12 Jul 2022 16:36:06 -0700
Subject: [R] Does the function "c" have a character limit?
Message-ID: <61a4af67-18a8-f363-8068-54fd1bc4d53d@gmail.com>

To Whom it May Concern,

I am creating a vector with the base R function "c", with many arguments 
as shown below:

 ???? $ R
 ???? > MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2", "ACTN1", 
"ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", "AJUBA", 
"ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", "ANXA6", 
"APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", "ARMCX2", 
"ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4", "ATP6V0E1", 
"ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", "BOC", 
"BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", "CALD1", "CALU", 
"CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", "CD44", "CD59", 
"CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", "CKAP4", "CLIC4", 
"CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", "COL1A1", "COL27A1", 
"COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2", "COL6A1", "COL6A2", 
"COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2", "CREG1", 
"CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", "CTNNA1", "CTSB", 
"CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", "CYP26A1", "CYR61", 
"DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", "DLX1", "DLX2", 
"DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", "DPY19L1", "DSE", 
"DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2", "EGFR", "EGR1", 
"EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", "EMILIN1", "EMP1", 
"ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1", "ETS1", "EVA1A", 
"EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1", "FAM120A", 
"FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", "FBN1", "FBN2", "FGFR1", 
"FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2", "FMOD", "FN1", "FNDC3B", 
"FSTL1", "FUCA2", "FZD1", "FZD2", "FZD7", "GABRR1", "GALNT10", "GAS1", 
"GAS2", "GDF15", "GJA1", "GNAI1", "GNG12", "GNS", "GORAB", "GPC6", 
"GPR137B", "GPX8", "GRN", "GSN", "HES1", "HEXB", "HIBADH", "HIPK3", 
"HIST1H2AC", "HIST1H2BK", "HLA-A", "HLA-B", "HLA-C", "HLA-F", "HLX", 
"HNMT", "HOMER1", "HS3ST3A1", "HSP90B1", "HSPA5", "HSPB1", "HTRA1", 
"HYOU1", "ID1", "ID3", "IFI16", "IFITM2", "IFITM3", "IGF2R", "IGFBP5", 
"IGFBP6", "IL13RA1", "IL6ST", "INSIG1", "IQGAP2", "ITGA10", "ITGA4", 
"ITGAV", "ITGB1", "ITM2B", "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", 
"KANK2", "KCNK2", "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", 
"KIAA1462", "KIF13A", "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", 
"LAMB1", "LAMC1", "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", 
"LGALS1", "LHFP", "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", 
"LMNA", "LOXL2", "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", 
"MAGT1", "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1", 
"MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", "MOB1A", 
"MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP", "NANS", 
"NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2", "NOTCH2", 
"NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1", "OGFRL1", 
"OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", "PALLD", "PAPSS2", 
"PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", "PDGFC", "PDIA3", 
"PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", "PHLDA3", "PHLDB2", 
"PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", "PLK2", "PLOD2", 
"PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", "PLXDC2", "POLR2L", 
"PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", "PRDM6", "PRDX4", 
"PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", "PTGFRN", "PTN", "PTPN14", 
"PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", "PYGL", "QKI", "QSOX1", 
"RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B", "RBMS1", "RCN1", "RECK", 
"REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ", "RIN2", "RIT1", 
"RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", "S1PR3", "SASH1", "SCPEP1", 
"SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4", "SEC14L1", "SEL1L3", "SEMA3C", 
"SEMA3F", "SEPT10", "SERPINE2", "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", 
"SH3BGRL", "SHC1", "SHROOM3", "SIX1", "SIX4", "SKIL", "SLC16A4", 
"SLC30A1", "SLC30A7", "SLC35F5", "SLC38A2", "SLC38A6", "SLC39A14", 
"SMAD3", "SNAI2", "SNAP23", "SOSTDC1", "SOX9", "SPARC", "SPARCL1", 
"SPATA20", "SPCS3", "SPRED1", "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", 
"SRPX", "SSBP4", "SSR1", "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", 
"SUCLG2", "SURF4", "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", 
"TFPI", "TGFB1I1", "TGFBR2", "THBS1", "TIMP1", "TJP1")
 ???? +

For some reason, the R console does not display a ">" symbol, indicating 
that it has completed the function, but displays a "+" symbol instead, 
which indicates that the function is still waiting for more input. 
However, I believe that my syntax is correct. If I shorten my command by 
a few characters by removing the last entry, "TJP1":

 ???? $ R
 ???? > MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2", "ACTN1", 
"ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", "AJUBA", 
"ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", "ANXA6", 
"APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", "ARMCX2", 
"ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4", "ATP6V0E1", 
"ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", "BOC", 
"BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", "CALD1", "CALU", 
"CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", "CD44", "CD59", 
"CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", "CKAP4", "CLIC4", 
"CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", "COL1A1", "COL27A1", 
"COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2", "COL6A1", "COL6A2", 
"COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2", "CREG1", 
"CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", "CTNNA1", "CTSB", 
"CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", "CYP26A1", "CYR61", 
"DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", "DLX1", "DLX2", 
"DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", "DPY19L1", "DSE", 
"DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2", "EGFR", "EGR1", 
"EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", "EMILIN1", "EMP1", 
"ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1", "ETS1", "EVA1A", 
"EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1", "FAM120A", 
"FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", "FBN1", "FBN2", "FGFR1", 
"FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2", "FMOD", "FN1", "FNDC3B", 
"FSTL1", "FUCA2", "FZD1", "FZD2", "FZD7", "GABRR1", "GALNT10", "GAS1", 
"GAS2", "GDF15", "GJA1", "GNAI1", "GNG12", "GNS", "GORAB", "GPC6", 
"GPR137B", "GPX8", "GRN", "GSN", "HES1", "HEXB", "HIBADH", "HIPK3", 
"HIST1H2AC", "HIST1H2BK", "HLA-A", "HLA-B", "HLA-C", "HLA-F", "HLX", 
"HNMT", "HOMER1", "HS3ST3A1", "HSP90B1", "HSPA5", "HSPB1", "HTRA1", 
"HYOU1", "ID1", "ID3", "IFI16", "IFITM2", "IFITM3", "IGF2R", "IGFBP5", 
"IGFBP6", "IL13RA1", "IL6ST", "INSIG1", "IQGAP2", "ITGA10", "ITGA4", 
"ITGAV", "ITGB1", "ITM2B", "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", 
"KANK2", "KCNK2", "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", 
"KIAA1462", "KIF13A", "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", 
"LAMB1", "LAMC1", "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", 
"LGALS1", "LHFP", "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", 
"LMNA", "LOXL2", "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", 
"MAGT1", "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1", 
"MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", "MOB1A", 
"MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP", "NANS", 
"NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2", "NOTCH2", 
"NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1", "OGFRL1", 
"OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", "PALLD", "PAPSS2", 
"PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", "PDGFC", "PDIA3", 
"PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", "PHLDA3", "PHLDB2", 
"PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", "PLK2", "PLOD2", 
"PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", "PLXDC2", "POLR2L", 
"PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", "PRDM6", "PRDX4", 
"PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", "PTGFRN", "PTN", "PTPN14", 
"PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", "PYGL", "QKI", "QSOX1", 
"RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B", "RBMS1", "RCN1", "RECK", 
"REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ", "RIN2", "RIT1", 
"RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", "S1PR3", "SASH1", "SCPEP1", 
"SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4", "SEC14L1", "SEL1L3", "SEMA3C", 
"SEMA3F", "SEPT10", "SERPINE2", "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", 
"SH3BGRL", "SHC1", "SHROOM3", "SIX1", "SIX4", "SKIL", "SLC16A4", 
"SLC30A1", "SLC30A7", "SLC35F5", "SLC38A2", "SLC38A6", "SLC39A14", 
"SMAD3", "SNAI2", "SNAP23", "SOSTDC1", "SOX9", "SPARC", "SPARCL1", 
"SPATA20", "SPCS3", "SPRED1", "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", 
"SRPX", "SSBP4", "SSR1", "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", 
"SUCLG2", "SURF4", "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", 
"TFPI", "TGFB1I1", "TGFBR2", "THBS1", "TIMP1")
 ???? >

The function now works, and returns a ">" symbol, indicating that the 
function completed successfully. The ls() function proves it:

 ???? $ R
 ???? > ls()
 ???? [1] "MES"

Is this a bug in the base R "c" function? It seems like the "c" function 
can only accept so many characters before it fails.

Thank you for your time,
core_contingency


-------------- next part --------------
A non-text attachment was scrubbed...
Name: OpenPGP_signature
Type: application/pgp-signature
Size: 236 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220712/53cd0593/attachment.sig>

From dw|n@em|u@ @end|ng |rom comc@@t@net  Wed Jul 13 08:26:41 2022
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Tue, 12 Jul 2022 23:26:41 -0700
Subject: [R] Does the function "c" have a character limit?
In-Reply-To: <61a4af67-18a8-f363-8068-54fd1bc4d53d@gmail.com>
References: <61a4af67-18a8-f363-8068-54fd1bc4d53d@gmail.com>
Message-ID: <14BEE7E3-2E2F-46AF-9181-6FE7AA2F78B6@comcast.net>

I think the restriction is not specific to `c` but rather is a limitation on the length of expressions. My foggy memory is that the limit is in the 450-500 character vicinity. Pretty sure it?s been discussed here in the past. 

? 
David. 

Sent from my iPhone

> On Jul 12, 2022, at 11:13 PM, core_contingency <ccontingency at gmail.com> wrote:
> 
> ?To Whom it May Concern,
> 
> I am creating a vector with the base R function "c", with many arguments as shown below:
> 
>      $ R
>      > MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2", "ACTN1", "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", "AJUBA", "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", "ANXA6", "APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", "ARMCX2", "ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4", "ATP6V0E1", "ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", "BOC", "BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", "CALD1", "CALU", "CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", "CD44", "CD59", "CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", "CKAP4", "CLIC4", "CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", "COL1A1", "COL27A1", "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2", "COL6A1", "COL6A2", "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2", "CREG1", "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", "CTNNA1", "CTSB", "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", "CYP26A1", "CYR61", "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", "DLX1", "DLX2", "DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", "DPY19L1", "DSE", "DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2", "EGFR", "EGR1", "EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", "EMILIN1", "EMP1", "ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1", "ETS1", "EVA1A", "EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1", "FAM120A", "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", "FBN1", "FBN2", "FGFR1", "FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2", "FMOD", "FN1", "FNDC3B", "FSTL1", "FUCA2", "FZD1", "FZD2", "FZD7", "GABRR1", "GALNT10", "GAS1", "GAS2", "GDF15", "GJA1", "GNAI1", "GNG12", "GNS", "GORAB", "GPC6", "GPR137B", "GPX8", "GRN", "GSN", "HES1", "HEXB", "HIBADH", "HIPK3", "HIST1H2AC", "HIST1H2BK", "HLA-A", "HLA-B", "HLA-C", "HLA-F", "HLX", "HNMT", "HOMER1", "HS3ST3A1", "HSP90B1", "HSPA5", "HSPB1", "HTRA1", "HYOU1", "ID1", "ID3", "IFI16", "IFITM2", "IFITM3", "IGF2R", "IGFBP5", "IGFBP6", "IL13RA1", "IL6ST", "INSIG1", "IQGAP2", "ITGA10", "ITGA4", "ITGAV", "ITGB1", "ITM2B", "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", "KANK2", "KCNK2", "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", "KIAA1462", "KIF13A", "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", "LAMB1", "LAMC1", "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", "LGALS1", "LHFP", "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", "LMNA", "LOXL2", "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", "MAGT1", "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1", "MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", "MOB1A", "MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP", "NANS", "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2", "NOTCH2", "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1", "OGFRL1", "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", "PALLD", "PAPSS2", "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", "PDGFC", "PDIA3", "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", "PHLDA3", "PHLDB2", "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", "PLK2", "PLOD2", "PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", "PLXDC2", "POLR2L", "PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", "PRDM6", "PRDX4", "PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", "PTGFRN", "PTN", "PTPN14", "PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", "PYGL", "QKI", "QSOX1", "RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B", "RBMS1", "RCN1", "RECK", "REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ", "RIN2", "RIT1", "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", "S1PR3", "SASH1", "SCPEP1", "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4", "SEC14L1", "SEL1L3", "SEMA3C", "SEMA3F", "SEPT10", "SERPINE2", "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", "SH3BGRL", "SHC1", "SHROOM3", "SIX1", "SIX4", "SKIL", "SLC16A4", "SLC30A1", "SLC30A7", "SLC35F5", "SLC38A2", "SLC38A6", "SLC39A14", "SMAD3", "SNAI2", "SNAP23", "SOSTDC1", "SOX9", "SPARC", "SPARCL1", "SPATA20", "SPCS3", "SPRED1", "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", "SRPX", "SSBP4", "SSR1", "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", "SUCLG2", "SURF4", "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", "TFPI", "TGFB1I1", "TGFBR2", "THBS1", "TIMP1", "TJP1")
>      +
> 
> For some reason, the R console does not display a ">" symbol, indicating that it has completed the function, but displays a "+" symbol instead, which indicates that the function is still waiting for more input. However, I believe that my syntax is correct. If I shorten my command by a few characters by removing the last entry, "TJP1":
> 
>      $ R
>      > MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2", "ACTN1", "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", "AJUBA", "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", "ANXA6", "APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", "ARMCX2", "ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4", "ATP6V0E1", "ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", "BOC", "BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", "CALD1", "CALU", "CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", "CD44", "CD59", "CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", "CKAP4", "CLIC4", "CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", "COL1A1", "COL27A1", "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2", "COL6A1", "COL6A2", "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2", "CREG1", "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", "CTNNA1", "CTSB", "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", "CYP26A1", "CYR61", "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", "DLX1", "DLX2", "DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", "DPY19L1", "DSE", "DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2", "EGFR", "EGR1", "EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", "EMILIN1", "EMP1", "ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1", "ETS1", "EVA1A", "EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1", "FAM120A", "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", "FBN1", "FBN2", "FGFR1", "FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2", "FMOD", "FN1", "FNDC3B", "FSTL1", "FUCA2", "FZD1", "FZD2", "FZD7", "GABRR1", "GALNT10", "GAS1", "GAS2", "GDF15", "GJA1", "GNAI1", "GNG12", "GNS", "GORAB", "GPC6", "GPR137B", "GPX8", "GRN", "GSN", "HES1", "HEXB", "HIBADH", "HIPK3", "HIST1H2AC", "HIST1H2BK", "HLA-A", "HLA-B", "HLA-C", "HLA-F", "HLX", "HNMT", "HOMER1", "HS3ST3A1", "HSP90B1", "HSPA5", "HSPB1", "HTRA1", "HYOU1", "ID1", "ID3", "IFI16", "IFITM2", "IFITM3", "IGF2R", "IGFBP5", "IGFBP6", "IL13RA1", "IL6ST", "INSIG1", "IQGAP2", "ITGA10", "ITGA4", "ITGAV", "ITGB1", "ITM2B", "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", "KANK2", "KCNK2", "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", "KIAA1462", "KIF13A", "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", "LAMB1", "LAMC1", "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", "LGALS1", "LHFP", "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", "LMNA", "LOXL2", "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", "MAGT1", "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1", "MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", "MOB1A", "MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP", "NANS", "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2", "NOTCH2", "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1", "OGFRL1", "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", "PALLD", "PAPSS2", "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", "PDGFC", "PDIA3", "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", "PHLDA3", "PHLDB2", "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", "PLK2", "PLOD2", "PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", "PLXDC2", "POLR2L", "PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", "PRDM6", "PRDX4", "PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", "PTGFRN", "PTN", "PTPN14", "PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", "PYGL", "QKI", "QSOX1", "RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B", "RBMS1", "RCN1", "RECK", "REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ", "RIN2", "RIT1", "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", "S1PR3", "SASH1", "SCPEP1", "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4", "SEC14L1", "SEL1L3", "SEMA3C", "SEMA3F", "SEPT10", "SERPINE2", "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", "SH3BGRL", "SHC1", "SHROOM3", "SIX1", "SIX4", "SKIL", "SLC16A4", "SLC30A1", "SLC30A7", "SLC35F5", "SLC38A2", "SLC38A6", "SLC39A14", "SMAD3", "SNAI2", "SNAP23", "SOSTDC1", "SOX9", "SPARC", "SPARCL1", "SPATA20", "SPCS3", "SPRED1", "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", "SRPX", "SSBP4", "SSR1", "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", "SUCLG2", "SURF4", "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", "TFPI", "TGFB1I1", "TGFBR2", "THBS1", "TIMP1")
>      >
> 
> The function now works, and returns a ">" symbol, indicating that the function completed successfully. The ls() function proves it:
> 
>      $ R
>      > ls()
>      [1] "MES"
> 
> Is this a bug in the base R "c" function? It seems like the "c" function can only accept so many characters before it fails.
> 
> Thank you for your time,
> core_contingency
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From dw|n@em|u@ @end|ng |rom comc@@t@net  Wed Jul 13 08:39:09 2022
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Tue, 12 Jul 2022 23:39:09 -0700
Subject: [R] Does the function "c" have a character limit?
In-Reply-To: <14BEE7E3-2E2F-46AF-9181-6FE7AA2F78B6@comcast.net>
References: <61a4af67-18a8-f363-8068-54fd1bc4d53d@gmail.com>
 <14BEE7E3-2E2F-46AF-9181-6FE7AA2F78B6@comcast.net>
Message-ID: <29cbe589-3d32-ba59-b016-6e4608007cba@comcast.net>


On 7/12/22 23:26, David Winsemius wrote:
> I think the restriction is not specific to `c` but rather is a limitation on the length of expressions. My foggy memory is that the limit is in the 450-500 character vicinity. Pretty sure it?s been discussed here in the past.


One way to get around this limitation is to create a .r file and source 
the material. I just tested with your code and it succeeded.


-- 

David.

>
> ?
> David.
>
> Sent from my iPhone
>
>> On Jul 12, 2022, at 11:13 PM, core_contingency <ccontingency at gmail.com> wrote:
>>
>> ?To Whom it May Concern,
>>
>> I am creating a vector with the base R function "c", with many arguments as shown below:
>>
>>       $ R
>>       > MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2", "ACTN1", "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", "AJUBA", "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", "ANXA6", "APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", "ARMCX2", "ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4", "ATP6V0E1", "ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", "BOC", "BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", "CALD1", "CALU", "CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", "CD44", "CD59", "CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", "CKAP4", "CLIC4", "CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", "COL1A1", "COL27A1", "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2", "COL6A1", "COL6A2", "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2", "CREG1", "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", "CTNNA1", "CTSB", "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", "CYP26A1", "CYR61", "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", "DLX1", "DLX2", "DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", "DPY19L1", "DSE", "DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2", "EGFR", "EGR1", "EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", "EMILIN1", "EMP1", "ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1", "ETS1", "EVA1A", "EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1", "FAM120A", "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", "FBN1", "FBN2", "FGFR1", "FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2", "FMOD", "FN1", "FNDC3B", "FSTL1", "FUCA2", "FZD1", "FZD2", "FZD7", "GABRR1", "GALNT10", "GAS1", "GAS2", "GDF15", "GJA1", "GNAI1", "GNG12", "GNS", "GORAB", "GPC6", "GPR137B", "GPX8", "GRN", "GSN", "HES1", "HEXB", "HIBADH", "HIPK3", "HIST1H2AC", "HIST1H2BK", "HLA-A", "HLA-B", "HLA-C", "HLA-F", "HLX", "HNMT", "HOMER1", "HS3ST3A1", "HSP90B1", "HSPA5", "HSPB1", "HTRA1", "HYOU1", "ID1", "ID3", "IFI16", "IFITM2", "IFITM3", "IGF2R", "IGFBP5", "IGFBP6", "IL13RA1", "IL6ST", "INSIG1", "IQGAP2", "ITGA10", "ITGA4", "ITGAV", "ITGB1", "ITM2B", "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", "KANK2", "KCNK2", "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", "KIAA1462", "KIF13A", "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", "LAMB1", "LAMC1", "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", "LGALS1", "LHFP", "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", "LMNA", "LOXL2", "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", "MAGT1", "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1", "MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", "MOB1A", "MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP", "NANS", "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2", "NOTCH2", "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1", "OGFRL1", "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", "PALLD", "PAPSS2", "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", "PDGFC", "PDIA3", "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", "PHLDA3", "PHLDB2", "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", "PLK2", "PLOD2", "PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", "PLXDC2", "POLR2L", "PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", "PRDM6", "PRDX4", "PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", "PTGFRN", "PTN", "PTPN14", "PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", "PYGL", "QKI", "QSOX1", "RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B", "RBMS1", "RCN1", "RECK", "REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ", "RIN2", "RIT1", "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", "S1PR3", "SASH1", "SCPEP1", "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4", "SEC14L1", "SEL1L3", "SEMA3C", "SEMA3F", "SEPT10", "SERPINE2", "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", "SH3BGRL", "SHC1", "SHROOM3", "SIX1", "SIX4", "SKIL", "SLC16A4", "SLC30A1", "SLC30A7", "SLC35F5", "SLC38A2", "SLC38A6", "SLC39A14", "SMAD3", "SNAI2", "SNAP23", "SOSTDC1", "SOX9", "SPARC", "SPARCL1", "SPATA20", "SPCS3", "SPRED1", "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", "SRPX", "SSBP4", "SSR1", "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", "SUCLG2", "SURF4", "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", "TFPI", "TGFB1I1", "TGFBR2", "THBS1", "TIMP1", "TJP1")
>>       +
>>
>> For some reason, the R console does not display a ">" symbol, indicating that it has completed the function, but displays a "+" symbol instead, which indicates that the function is still waiting for more input. However, I believe that my syntax is correct. If I shorten my command by a few characters by removing the last entry, "TJP1":
>>
>>       $ R
>>       > MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2", "ACTN1", "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", "AJUBA", "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", "ANXA6", "APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", "ARMCX2", "ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4", "ATP6V0E1", "ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", "BOC", "BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", "CALD1", "CALU", "CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", "CD44", "CD59", "CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", "CKAP4", "CLIC4", "CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", "COL1A1", "COL27A1", "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2", "COL6A1", "COL6A2", "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2", "CREG1", "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", "CTNNA1", "CTSB", "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", "CYP26A1", "CYR61", "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", "DLX1", "DLX2", "DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", "DPY19L1", "DSE", "DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2", "EGFR", "EGR1", "EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", "EMILIN1", "EMP1", "ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1", "ETS1", "EVA1A", "EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1", "FAM120A", "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", "FBN1", "FBN2", "FGFR1", "FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2", "FMOD", "FN1", "FNDC3B", "FSTL1", "FUCA2", "FZD1", "FZD2", "FZD7", "GABRR1", "GALNT10", "GAS1", "GAS2", "GDF15", "GJA1", "GNAI1", "GNG12", "GNS", "GORAB", "GPC6", "GPR137B", "GPX8", "GRN", "GSN", "HES1", "HEXB", "HIBADH", "HIPK3", "HIST1H2AC", "HIST1H2BK", "HLA-A", "HLA-B", "HLA-C", "HLA-F", "HLX", "HNMT", "HOMER1", "HS3ST3A1", "HSP90B1", "HSPA5", "HSPB1", "HTRA1", "HYOU1", "ID1", "ID3", "IFI16", "IFITM2", "IFITM3", "IGF2R", "IGFBP5", "IGFBP6", "IL13RA1", "IL6ST", "INSIG1", "IQGAP2", "ITGA10", "ITGA4", "ITGAV", "ITGB1", "ITM2B", "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", "KANK2", "KCNK2", "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", "KIAA1462", "KIF13A", "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", "LAMB1", "LAMC1", "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", "LGALS1", "LHFP", "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", "LMNA", "LOXL2", "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", "MAGT1", "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1", "MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", "MOB1A", "MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP", "NANS", "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2", "NOTCH2", "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1", "OGFRL1", "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", "PALLD", "PAPSS2", "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", "PDGFC", "PDIA3", "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", "PHLDA3", "PHLDB2", "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", "PLK2", "PLOD2", "PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", "PLXDC2", "POLR2L", "PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", "PRDM6", "PRDX4", "PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", "PTGFRN", "PTN", "PTPN14", "PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", "PYGL", "QKI", "QSOX1", "RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B", "RBMS1", "RCN1", "RECK", "REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ", "RIN2", "RIT1", "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", "S1PR3", "SASH1", "SCPEP1", "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4", "SEC14L1", "SEL1L3", "SEMA3C", "SEMA3F", "SEPT10", "SERPINE2", "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", "SH3BGRL", "SHC1", "SHROOM3", "SIX1", "SIX4", "SKIL", "SLC16A4", "SLC30A1", "SLC30A7", "SLC35F5", "SLC38A2", "SLC38A6", "SLC39A14", "SMAD3", "SNAI2", "SNAP23", "SOSTDC1", "SOX9", "SPARC", "SPARCL1", "SPATA20", "SPCS3", "SPRED1", "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", "SRPX", "SSBP4", "SSR1", "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", "SUCLG2", "SURF4", "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", "TFPI", "TGFB1I1", "TGFBR2", "THBS1", "TIMP1")
>>       >
>>
>> The function now works, and returns a ">" symbol, indicating that the function completed successfully. The ls() function proves it:
>>
>>       $ R
>>       > ls()
>>       [1] "MES"
>>
>> Is this a bug in the base R "c" function? It seems like the "c" function can only accept so many characters before it fails.
>>
>> Thank you for your time,
>> core_contingency
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From kry|ov@r00t @end|ng |rom gm@||@com  Wed Jul 13 09:12:19 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Wed, 13 Jul 2022 10:12:19 +0300
Subject: [R] Does the function "c" have a character limit?
In-Reply-To: <61a4af67-18a8-f363-8068-54fd1bc4d53d@gmail.com>
References: <61a4af67-18a8-f363-8068-54fd1bc4d53d@gmail.com>
Message-ID: <20220713101219.4c6755ca@Tarkus>

On Tue, 12 Jul 2022 16:36:06 -0700
core_contingency <ccontingency at gmail.com> wrote:

> For some reason, the R console does not display a ">" symbol,
> indicating that it has completed the function, but displays a "+"
> symbol instead, which indicates that the function is still waiting
> for more input.

If I copy&paste from your message into an R session, it seems to work
for me. I think the reason for that is because what was a single long
line has been wrapped into a lot of shorter lines.

-- 
Best regards,
Ivan


From k|mmo@e|o @end|ng |rom utu@||  Wed Jul 13 10:06:29 2022
From: k|mmo@e|o @end|ng |rom utu@|| (Kimmo Elo)
Date: Wed, 13 Jul 2022 08:06:29 +0000
Subject: [R] Does the function "c" have a character limit?
In-Reply-To: <29cbe589-3d32-ba59-b016-6e4608007cba@comcast.net>
References: <61a4af67-18a8-f363-8068-54fd1bc4d53d@gmail.com>
 <14BEE7E3-2E2F-46AF-9181-6FE7AA2F78B6@comcast.net>
 <29cbe589-3d32-ba59-b016-6e4608007cba@comcast.net>
Message-ID: <4ffcd4f1a5fc6810b3a342af2dcd1d01b65d2ad1.camel@utu.fi>

Hi!

Cannot reproduce this either with my R 4.2.1 on Linux. Just copy-pasted 
your expression and it works just fine.

Best,
Kimmo

ti, 2022-07-12 kello 23:39 -0700, David Winsemius kirjoitti:
> On 7/12/22 23:26, David Winsemius wrote:
> > I think the restriction is not specific to `c` but rather is a
> > limitation on the length of expressions. My foggy memory is that
> > the limit is in the 450-500 character vicinity. Pretty sure it?s
> > been discussed here in the past.
> 
> One way to get around this limitation is to create a .r file and
> source 
> the material. I just tested with your code and it succeeded.
> 
> 
> -- 
> 
> David.
> 
> > ?
> > David.
> > 
> > Sent from my iPhone
> > 
> > > On Jul 12, 2022, at 11:13 PM, core_contingency <
> > > ccontingency at gmail.com> wrote:
> > > 
> > > ?To Whom it May Concern,
> > > 
> > > I am creating a vector with the base R function "c", with many
> > > arguments as shown below:
> > > 
> > >       $ R
> > >       > MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2",
> > > "ACTN1", "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6",
> > > "AEBP1", "AJUBA", "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1",
> > > "ANXA2", "ANXA5", "ANXA6", "APOE", "APP", "ARHGAP1", "ARHGEF40",
> > > "ARL1", "ARL4A", "ARMCX2", "ARPC1B", "ASPH", "ATP10D", "ATP1B1",
> > > "ATP2B1", "ATP2B4", "ATP6V0E1", "ATP8B2", "ATXN1", "B2M", "BAG3",
> > > "BGN", "BMP5", "BNC2", "BOC", "BTN3A2", "C1orf198", "C1orf54",
> > > "C4orf32", "C6orf120", "CALD1", "CALU", "CAPN2", "CAPN6", "CBFB",
> > > "CBLB", "CCDC80", "CD164", "CD44", "CD59", "CD63", "CDH11",
> > > "CETN2", "CFH", "CFI", "CILP", "CKAP4", "CLIC4", "CMTM3",
> > > "CMTM6", "CNN3", "COL11A1", "COL12A1", "COL1A1", "COL27A1",
> > > "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2", "COL6A1",
> > > "COL6A2", "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2",
> > > "CREG1", "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2",
> > > "CTNNA1", "CTSB", "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1",
> > > "CYP26A1", "CYR61", "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3",
> > > "DLC1", "DLX1", "DLX2", "DMD", "DNAJC1", "DNAJC10", "DNAJC3",
> > > "DNM3OS", "DPY19L1", "DSE", "DUSP14", "DUSP5", "DUSP6", "EDEM1",
> > > "EDNRA", "EFEMP2", "EGFR", "EGR1", "EGR3", "EHD2", "ELAVL1",
> > > "ELF1", "ELK3", "ELK4", "EMILIN1", "EMP1", "ENAH", "EPHA3",
> > > "EPS8", "ERBIN", "ERLIN1", "ERRFI1", "ETS1", "EVA1A", "EXT1",
> > > "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1", "FAM120A",
> > > "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", "FBN1", "FBN2",
> > > "FGFR1", "FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2", "FMOD",
> > > "FN1", "FNDC3B", "FSTL1", "FUCA2", "FZD1", "FZD2", "FZD7",
> > > "GABRR1", "GALNT10", "GAS1", "GAS2", "GDF15", "GJA1", "GNAI1",
> > > "GNG12", "GNS", "GORAB", "GPC6", "GPR137B", "GPX8", "GRN", "GSN",
> > > "HES1", "HEXB", "HIBADH", "HIPK3", "HIST1H2AC", "HIST1H2BK",
> > > "HLA-A", "HLA-B", "HLA-C", "HLA-F", "HLX", "HNMT", "HOMER1",
> > > "HS3ST3A1", "HSP90B1", "HSPA5", "HSPB1", "HTRA1", "HYOU1", "ID1",
> > > "ID3", "IFI16", "IFITM2", "IFITM3", "IGF2R", "IGFBP5", "IGFBP6",
> > > "IL13RA1", "IL6ST", "INSIG1", "IQGAP2", "ITGA10", "ITGA4",
> > > "ITGAV", "ITGB1", "ITM2B", "ITM2C", "ITPR1", "ITPRIPL2", "JAK1",
> > > "JAM3", "KANK2", "KCNK2", "KCTD12", "KDELC2", "KDELR2", "KDELR3",
> > > "KDM5B", "KIAA1462", "KIF13A", "KIRREL", "KLF10", "KLF4", "KLF6",
> > > "L3HYPDH", "LAMB1", "LAMC1", "LAMP1", "LAPTM4A", "LASP1",
> > > "LATS2", "LEPROT", "LGALS1", "LHFP", "LHX8", "LIFR", "LIPA",
> > > "LITAF", "LIX1L", "LMAN1", "LMNA", "LOXL2", "LPP", "LRP10",
> > > "LRRC17", "LRRC8C", "LTBP1", "LUZP1", "MAGT1", "MAML2", "MAN2A1",
> > > "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1", "MEOX2", "MEST",
> > > "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", "MOB1A", "MRC2",
> > > "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP", "NANS",
> > > "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2", "NOTCH2",
> > > "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1", "OGFRL1",
> > > "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", "PALLD",
> > > "PAPSS2", "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B",
> > > "PDGFC", "PDIA3", "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1",
> > > "PHLDA3", "PHLDB2", "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2",
> > > "PLEKHH2", "PLK2", "PLOD2", "PLOD3", "PLPP1", "PLS3", "PLSCR1",
> > > "PLSCR4", "PLXDC2", "POLR2L", "PON2", "POSTN", "PPIB", "PPIC",
> > > "PPT1", "PRCP", "PRDM6", "PRDX4", "PRDX6", "PROM1", "PRRX1",
> > > "PTBP1", "PTGER4", "PTGFRN", "PTN", "PTPN14", "PTPRG", "PTPRK",
> > > "PTRF", "PXDC1", "PXDN", "PYGL", "QKI", "QSOX1", "RAB13",
> > > "RAB29", "RAB31", "RAP1A", "RAP1B", "RBMS1", "RCN1", "RECK",
> > > "REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ", "RIN2", "RIT1",
> > > "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", "S1PR3", "SASH1",
> > > "SCPEP1", "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4", "SEC14L1",
> > > "SEL1L3", "SEMA3C", "SEMA3F", "SEPT10", "SERPINE2", "SERPINH1",
> > > "SFT2D1", "SFT2D2", "SGK1", "SH3BGRL", "SHC1", "SHROOM3", "SIX1",
> > > "SIX4", "SKIL", "SLC16A4", "SLC30A1", "SLC30A7", "SLC35F5",
> > > "SLC38A2", "SLC38A6", "SLC39A14", "SMAD3", "SNAI2", "SNAP23",
> > > "SOSTDC1", "SOX9", "SPARC", "SPARCL1", "SPATA20", "SPCS3",
> > > "SPRED1", "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", "SRPX",
> > > "SSBP4", "SSR1", "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L",
> > > "SUCLG2", "SURF4", "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2",
> > > "TFE3", "TFPI", "TGFB1I1", "TGFBR2", "THBS1", "TIMP1", "TJP1")
> > >       +
> > > 
> > > For some reason, the R console does not display a ">" symbol,
> > > indicating that it has completed the function, but displays a "+"
> > > symbol instead, which indicates that the function is still
> > > waiting for more input. However, I believe that my syntax is
> > > correct. If I shorten my command by a few characters by removing
> > > the last entry, "TJP1":
> > > 
> > >       $ R
> > >       > MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2",
> > > "ACTN1", "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6",
> > > "AEBP1", "AJUBA", "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1",
> > > "ANXA2", "ANXA5", "ANXA6", "APOE", "APP", "ARHGAP1", "ARHGEF40",
> > > "ARL1", "ARL4A", "ARMCX2", "ARPC1B", "ASPH", "ATP10D", "ATP1B1",
> > > "ATP2B1", "ATP2B4", "ATP6V0E1", "ATP8B2", "ATXN1", "B2M", "BAG3",
> > > "BGN", "BMP5", "BNC2", "BOC", "BTN3A2", "C1orf198", "C1orf54",
> > > "C4orf32", "C6orf120", "CALD1", "CALU", "CAPN2", "CAPN6", "CBFB",
> > > "CBLB", "CCDC80", "CD164", "CD44", "CD59", "CD63", "CDH11",
> > > "CETN2", "CFH", "CFI", "CILP", "CKAP4", "CLIC4", "CMTM3",
> > > "CMTM6", "CNN3", "COL11A1", "COL12A1", "COL1A1", "COL27A1",
> > > "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2", "COL6A1",
> > > "COL6A2", "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2",
> > > "CREG1", "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2",
> > > "CTNNA1", "CTSB", "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1",
> > > "CYP26A1", "CYR61", "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3",
> > > "DLC1", "DLX1", "DLX2", "DMD", "DNAJC1", "DNAJC10", "DNAJC3",
> > > "DNM3OS", "DPY19L1", "DSE", "DUSP14", "DUSP5", "DUSP6", "EDEM1",
> > > "EDNRA", "EFEMP2", "EGFR", "EGR1", "EGR3", "EHD2", "ELAVL1",
> > > "ELF1", "ELK3", "ELK4", "EMILIN1", "EMP1", "ENAH", "EPHA3",
> > > "EPS8", "ERBIN", "ERLIN1", "ERRFI1", "ETS1", "EVA1A", "EXT1",
> > > "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1", "FAM120A",
> > > "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", "FBN1", "FBN2",
> > > "FGFR1", "FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2", "FMOD",
> > > "FN1", "FNDC3B", "FSTL1", "FUCA2", "FZD1", "FZD2", "FZD7",
> > > "GABRR1", "GALNT10", "GAS1", "GAS2", "GDF15", "GJA1", "GNAI1",
> > > "GNG12", "GNS", "GORAB", "GPC6", "GPR137B", "GPX8", "GRN", "GSN",
> > > "HES1", "HEXB", "HIBADH", "HIPK3", "HIST1H2AC", "HIST1H2BK",
> > > "HLA-A", "HLA-B", "HLA-C", "HLA-F", "HLX", "HNMT", "HOMER1",
> > > "HS3ST3A1", "HSP90B1", "HSPA5", "HSPB1", "HTRA1", "HYOU1", "ID1",
> > > "ID3", "IFI16", "IFITM2", "IFITM3", "IGF2R", "IGFBP5", "IGFBP6",
> > > "IL13RA1", "IL6ST", "INSIG1", "IQGAP2", "ITGA10", "ITGA4",
> > > "ITGAV", "ITGB1", "ITM2B", "ITM2C", "ITPR1", "ITPRIPL2", "JAK1",
> > > "JAM3", "KANK2", "KCNK2", "KCTD12", "KDELC2", "KDELR2", "KDELR3",
> > > "KDM5B", "KIAA1462", "KIF13A", "KIRREL", "KLF10", "KLF4", "KLF6",
> > > "L3HYPDH", "LAMB1", "LAMC1", "LAMP1", "LAPTM4A", "LASP1",
> > > "LATS2", "LEPROT", "LGALS1", "LHFP", "LHX8", "LIFR", "LIPA",
> > > "LITAF", "LIX1L", "LMAN1", "LMNA", "LOXL2", "LPP", "LRP10",
> > > "LRRC17", "LRRC8C", "LTBP1", "LUZP1", "MAGT1", "MAML2", "MAN2A1",
> > > "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1", "MEOX2", "MEST",
> > > "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", "MOB1A", "MRC2",
> > > "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP", "NANS",
> > > "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2", "NOTCH2",
> > > "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1", "OGFRL1",
> > > "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", "PALLD",
> > > "PAPSS2", "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B",
> > > "PDGFC", "PDIA3", "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1",
> > > "PHLDA3", "PHLDB2", "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2",
> > > "PLEKHH2", "PLK2", "PLOD2", "PLOD3", "PLPP1", "PLS3", "PLSCR1",
> > > "PLSCR4", "PLXDC2", "POLR2L", "PON2", "POSTN", "PPIB", "PPIC",
> > > "PPT1", "PRCP", "PRDM6", "PRDX4", "PRDX6", "PROM1", "PRRX1",
> > > "PTBP1", "PTGER4", "PTGFRN", "PTN", "PTPN14", "PTPRG", "PTPRK",
> > > "PTRF", "PXDC1", "PXDN", "PYGL", "QKI", "QSOX1", "RAB13",
> > > "RAB29", "RAB31", "RAP1A", "RAP1B", "RBMS1", "RCN1", "RECK",
> > > "REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ", "RIN2", "RIT1",
> > > "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", "S1PR3", "SASH1",
> > > "SCPEP1", "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4", "SEC14L1",
> > > "SEL1L3", "SEMA3C", "SEMA3F", "SEPT10", "SERPINE2", "SERPINH1",
> > > "SFT2D1", "SFT2D2", "SGK1", "SH3BGRL", "SHC1", "SHROOM3", "SIX1",
> > > "SIX4", "SKIL", "SLC16A4", "SLC30A1", "SLC30A7", "SLC35F5",
> > > "SLC38A2", "SLC38A6", "SLC39A14", "SMAD3", "SNAI2", "SNAP23",
> > > "SOSTDC1", "SOX9", "SPARC", "SPARCL1", "SPATA20", "SPCS3",
> > > "SPRED1", "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", "SRPX",
> > > "SSBP4", "SSR1", "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L",
> > > "SUCLG2", "SURF4", "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2",
> > > "TFE3", "TFPI", "TGFB1I1", "TGFBR2", "THBS1", "TIMP1")
> > >       >
> > > 
> > > The function now works, and returns a ">" symbol, indicating that
> > > the function completed successfully. The ls() function proves it:
> > > 
> > >       $ R
> > >       > ls()
> > >       [1] "MES"
> > > 
> > > Is this a bug in the base R "c" function? It seems like the "c"
> > > function can only accept so many characters before it fails.
> > > 
> > > Thank you for your time,
> > > core_contingency
> > > 
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide 
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible
> > > code.
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed Jul 13 12:36:05 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 13 Jul 2022 11:36:05 +0100
Subject: [R] Does the function "c" have a character limit?
In-Reply-To: <61a4af67-18a8-f363-8068-54fd1bc4d53d@gmail.com>
References: <61a4af67-18a8-f363-8068-54fd1bc4d53d@gmail.com>
Message-ID: <75c7fbeb-8aa1-ab91-b164-11c2a18e176d@sapo.pt>

Hello,

This is documented behavior.
 From R-intro, last line of section 1.8 [1].


Command lines entered at the console are limited4 to about 4095 bytes 
(not characters).


The number 4 in limited4 is a footnote link:


some of the consoles will not allow you to enter more, and amongst those 
which do some will silently discard the excess and some will use it as 
the start of the next line.



Prof. Ripley called the r-devel mailing list's attention to this in 
August 2006 when the limit was 1024 [2], it was then increased to the 
current 4095. I remember seeing a limit of 2048 (?) but couldn't find where.


Try creating a file with your command as only content, then run


x <- readLines("rhelp.txt")
nchar(x)
# [1] 4096


You are above the limit by 1 byte.
Standard solutions are to break the command line, in your case into at 
least 2 lines, or to source the command from file, like David proposed.


[1] 
https://cran.r-project.org/doc/manuals/R-intro.html#R-commands_003b-case-sensitivity-etc
[2] https://stat.ethz.ch/pipermail/r-devel/2006-August/038985.html


Hope this helps,

Rui Barradas


?s 00:36 de 13/07/2022, core_contingency escreveu:
> To Whom it May Concern,
> 
> I am creating a vector with the base R function "c", with many arguments 
> as shown below:
> 
>  ???? $ R
>  ???? > MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2", "ACTN1", 
> "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", "AJUBA", 
> "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", "ANXA6", 
> "APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", "ARMCX2", 
> "ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4", "ATP6V0E1", 
> "ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", "BOC", 
> "BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", "CALD1", "CALU", 
> "CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", "CD44", "CD59", 
> "CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", "CKAP4", "CLIC4", 
> "CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", "COL1A1", "COL27A1", 
> "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2", "COL6A1", "COL6A2", 
> "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2", "CREG1", 
> "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", "CTNNA1", "CTSB", 
> "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", "CYP26A1", "CYR61", 
> "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", "DLX1", "DLX2", 
> "DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", "DPY19L1", "DSE", 
> "DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2", "EGFR", "EGR1", 
> "EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", "EMILIN1", "EMP1", 
> "ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1", "ETS1", "EVA1A", 
> "EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1", "FAM120A", 
> "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", "FBN1", "FBN2", "FGFR1", 
> "FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2", "FMOD", "FN1", "FNDC3B", 
> "FSTL1", "FUCA2", "FZD1", "FZD2", "FZD7", "GABRR1", "GALNT10", "GAS1", 
> "GAS2", "GDF15", "GJA1", "GNAI1", "GNG12", "GNS", "GORAB", "GPC6", 
> "GPR137B", "GPX8", "GRN", "GSN", "HES1", "HEXB", "HIBADH", "HIPK3", 
> "HIST1H2AC", "HIST1H2BK", "HLA-A", "HLA-B", "HLA-C", "HLA-F", "HLX", 
> "HNMT", "HOMER1", "HS3ST3A1", "HSP90B1", "HSPA5", "HSPB1", "HTRA1", 
> "HYOU1", "ID1", "ID3", "IFI16", "IFITM2", "IFITM3", "IGF2R", "IGFBP5", 
> "IGFBP6", "IL13RA1", "IL6ST", "INSIG1", "IQGAP2", "ITGA10", "ITGA4", 
> "ITGAV", "ITGB1", "ITM2B", "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", 
> "KANK2", "KCNK2", "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", 
> "KIAA1462", "KIF13A", "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", 
> "LAMB1", "LAMC1", "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", 
> "LGALS1", "LHFP", "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", 
> "LMNA", "LOXL2", "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", 
> "MAGT1", "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1", 
> "MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", "MOB1A", 
> "MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP", "NANS", 
> "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2", "NOTCH2", 
> "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1", "OGFRL1", 
> "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", "PALLD", "PAPSS2", 
> "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", "PDGFC", "PDIA3", 
> "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", "PHLDA3", "PHLDB2", 
> "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", "PLK2", "PLOD2", 
> "PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", "PLXDC2", "POLR2L", 
> "PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", "PRDM6", "PRDX4", 
> "PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", "PTGFRN", "PTN", "PTPN14", 
> "PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", "PYGL", "QKI", "QSOX1", 
> "RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B", "RBMS1", "RCN1", "RECK", 
> "REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ", "RIN2", "RIT1", 
> "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", "S1PR3", "SASH1", "SCPEP1", 
> "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4", "SEC14L1", "SEL1L3", "SEMA3C", 
> "SEMA3F", "SEPT10", "SERPINE2", "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", 
> "SH3BGRL", "SHC1", "SHROOM3", "SIX1", "SIX4", "SKIL", "SLC16A4", 
> "SLC30A1", "SLC30A7", "SLC35F5", "SLC38A2", "SLC38A6", "SLC39A14", 
> "SMAD3", "SNAI2", "SNAP23", "SOSTDC1", "SOX9", "SPARC", "SPARCL1", 
> "SPATA20", "SPCS3", "SPRED1", "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", 
> "SRPX", "SSBP4", "SSR1", "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", 
> "SUCLG2", "SURF4", "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", 
> "TFPI", "TGFB1I1", "TGFBR2", "THBS1", "TIMP1", "TJP1")
>  ???? +
> 
> For some reason, the R console does not display a ">" symbol, indicating 
> that it has completed the function, but displays a "+" symbol instead, 
> which indicates that the function is still waiting for more input. 
> However, I believe that my syntax is correct. If I shorten my command by 
> a few characters by removing the last entry, "TJP1":
> 
>  ???? $ R
>  ???? > MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2", "ACTN1", 
> "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", "AJUBA", 
> "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", "ANXA6", 
> "APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", "ARMCX2", 
> "ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4", "ATP6V0E1", 
> "ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", "BOC", 
> "BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", "CALD1", "CALU", 
> "CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", "CD44", "CD59", 
> "CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", "CKAP4", "CLIC4", 
> "CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", "COL1A1", "COL27A1", 
> "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2", "COL6A1", "COL6A2", 
> "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2", "CREG1", 
> "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", "CTNNA1", "CTSB", 
> "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", "CYP26A1", "CYR61", 
> "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", "DLX1", "DLX2", 
> "DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", "DPY19L1", "DSE", 
> "DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2", "EGFR", "EGR1", 
> "EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", "EMILIN1", "EMP1", 
> "ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1", "ETS1", "EVA1A", 
> "EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1", "FAM120A", 
> "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", "FBN1", "FBN2", "FGFR1", 
> "FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2", "FMOD", "FN1", "FNDC3B", 
> "FSTL1", "FUCA2", "FZD1", "FZD2", "FZD7", "GABRR1", "GALNT10", "GAS1", 
> "GAS2", "GDF15", "GJA1", "GNAI1", "GNG12", "GNS", "GORAB", "GPC6", 
> "GPR137B", "GPX8", "GRN", "GSN", "HES1", "HEXB", "HIBADH", "HIPK3", 
> "HIST1H2AC", "HIST1H2BK", "HLA-A", "HLA-B", "HLA-C", "HLA-F", "HLX", 
> "HNMT", "HOMER1", "HS3ST3A1", "HSP90B1", "HSPA5", "HSPB1", "HTRA1", 
> "HYOU1", "ID1", "ID3", "IFI16", "IFITM2", "IFITM3", "IGF2R", "IGFBP5", 
> "IGFBP6", "IL13RA1", "IL6ST", "INSIG1", "IQGAP2", "ITGA10", "ITGA4", 
> "ITGAV", "ITGB1", "ITM2B", "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", 
> "KANK2", "KCNK2", "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", 
> "KIAA1462", "KIF13A", "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", 
> "LAMB1", "LAMC1", "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", 
> "LGALS1", "LHFP", "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", 
> "LMNA", "LOXL2", "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", 
> "MAGT1", "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1", 
> "MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", "MOB1A", 
> "MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP", "NANS", 
> "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2", "NOTCH2", 
> "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1", "OGFRL1", 
> "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", "PALLD", "PAPSS2", 
> "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", "PDGFC", "PDIA3", 
> "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", "PHLDA3", "PHLDB2", 
> "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", "PLK2", "PLOD2", 
> "PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", "PLXDC2", "POLR2L", 
> "PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", "PRDM6", "PRDX4", 
> "PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", "PTGFRN", "PTN", "PTPN14", 
> "PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", "PYGL", "QKI", "QSOX1", 
> "RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B", "RBMS1", "RCN1", "RECK", 
> "REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ", "RIN2", "RIT1", 
> "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", "S1PR3", "SASH1", "SCPEP1", 
> "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4", "SEC14L1", "SEL1L3", "SEMA3C", 
> "SEMA3F", "SEPT10", "SERPINE2", "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", 
> "SH3BGRL", "SHC1", "SHROOM3", "SIX1", "SIX4", "SKIL", "SLC16A4", 
> "SLC30A1", "SLC30A7", "SLC35F5", "SLC38A2", "SLC38A6", "SLC39A14", 
> "SMAD3", "SNAI2", "SNAP23", "SOSTDC1", "SOX9", "SPARC", "SPARCL1", 
> "SPATA20", "SPCS3", "SPRED1", "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", 
> "SRPX", "SSBP4", "SSR1", "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", 
> "SUCLG2", "SURF4", "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", 
> "TFPI", "TGFB1I1", "TGFBR2", "THBS1", "TIMP1")
>  ???? >
> 
> The function now works, and returns a ">" symbol, indicating that the 
> function completed successfully. The ls() function proves it:
> 
>  ???? $ R
>  ???? > ls()
>  ???? [1] "MES"
> 
> Is this a bug in the base R "c" function? It seems like the "c" function 
> can only accept so many characters before it fails.
> 
> Thank you for your time,
> core_contingency
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tebert @end|ng |rom u||@edu  Wed Jul 13 15:20:34 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Wed, 13 Jul 2022 13:20:34 +0000
Subject: [R] Does the function "c" have a character limit?
In-Reply-To: <75c7fbeb-8aa1-ab91-b164-11c2a18e176d@sapo.pt>
References: <61a4af67-18a8-f363-8068-54fd1bc4d53d@gmail.com>
 <75c7fbeb-8aa1-ab91-b164-11c2a18e176d@sapo.pt>
Message-ID: <BN6PR2201MB155355656C6FD9DC0BD729C1CF899@BN6PR2201MB1553.namprd22.prod.outlook.com>

The limits to the size of vectors, matrices, data frames, lists, or other data structure does not have a simple answer.
1) 2^31 - 1  is the maximum number of rows. https://stackoverflow.com/questions/5233769/practical-limits-of-r-data-frame#:~:text=The%20number%20is%202%5E31,start%20collecting%20several%20of%20them.
2) help(Memory) suggests that the default limit for all variables is 6 Mb. The help page tells you how to change this.
Neither of these two factors have any bearing on this problem except that your vector is not close to these limits.

I got the same result you did when I entered your vector into my system (R 4.2 in RStudio, on 64 bit Windows). I shortened it by removing the first entry and it works.

I can copy the entire line into Microsoft Word, and count the number of characters (including spaces) and I get 4089. There were seven characters in the first entry including the comma and space. If I add seven spaces between MES and the equal sign I get the original outcome. So the limit is on the number of characters in the line. You can get more entries by shortening each entry, or fewer if each entry was longer.

As others have suggested, I would break the line into two pieces and then combine the pieces.

Tim



-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Rui Barradas
Sent: Wednesday, July 13, 2022 6:36 AM
To: core_contingency <ccontingency at gmail.com>; r-help at r-project.org
Subject: Re: [R] Does the function "c" have a character limit?

[External Email]

Hello,

This is documented behavior.
 From R-intro, last line of section 1.8 [1].


Command lines entered at the console are limited4 to about 4095 bytes (not characters).


The number 4 in limited4 is a footnote link:


some of the consoles will not allow you to enter more, and amongst those which do some will silently discard the excess and some will use it as the start of the next line.



Prof. Ripley called the r-devel mailing list's attention to this in August 2006 when the limit was 1024 [2], it was then increased to the current 4095. I remember seeing a limit of 2048 (?) but couldn't find where.


Try creating a file with your command as only content, then run


x <- readLines("rhelp.txt")
nchar(x)
# [1] 4096


You are above the limit by 1 byte.
Standard solutions are to break the command line, in your case into at least 2 lines, or to source the command from file, like David proposed.


[1]
https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dproject.org_doc_manuals_R-2Dintro.html-23R-2Dcommands-5F003b-2Dcase-2Dsensitivity-2Detc&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=g5cbrpfyHaIN9sXycd_-f2iDsOcbuzLe2u3KjvQNm-0&e=
[2] https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_pipermail_r-2Ddevel_2006-2DAugust_038985.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=B1YSca31vHlpy9WJG8o0MBTh7bX4v7M61eWfQ4tmdog&e=


Hope this helps,

Rui Barradas


?s 00:36 de 13/07/2022, core_contingency escreveu:
> To Whom it May Concern,
>
> I am creating a vector with the base R function "c", with many 
> arguments as shown below:
>
>       $ R
>       > MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2", "ACTN1", 
> "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", "AJUBA", 
> "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", "ANXA6", 
> "APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", "ARMCX2", 
> "ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4", "ATP6V0E1", 
> "ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", "BOC", 
> "BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", "CALD1", 
> "CALU", "CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", "CD44", 
> "CD59", "CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", "CKAP4", 
> "CLIC4", "CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", "COL1A1", 
> "COL27A1", "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2", "COL6A1", 
> "COL6A2", "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2", 
> "CREG1", "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", "CTNNA1", 
> "CTSB", "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", "CYP26A1", 
> "CYR61", "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", "DLX1", 
> "DLX2", "DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", "DPY19L1", 
> "DSE", "DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2", "EGFR", 
> "EGR1", "EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", "EMILIN1", 
> "EMP1", "ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1", "ETS1", 
> "EVA1A", "EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1", 
> "FAM120A", "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", "FBN1", 
> "FBN2", "FGFR1", "FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2", 
> "FMOD", "FN1", "FNDC3B", "FSTL1", "FUCA2", "FZD1", "FZD2", "FZD7", 
> "GABRR1", "GALNT10", "GAS1", "GAS2", "GDF15", "GJA1", "GNAI1", 
> "GNG12", "GNS", "GORAB", "GPC6", "GPR137B", "GPX8", "GRN", "GSN", 
> "HES1", "HEXB", "HIBADH", "HIPK3", "HIST1H2AC", "HIST1H2BK", "HLA-A", 
> "HLA-B", "HLA-C", "HLA-F", "HLX", "HNMT", "HOMER1", "HS3ST3A1", 
> "HSP90B1", "HSPA5", "HSPB1", "HTRA1", "HYOU1", "ID1", "ID3", "IFI16", 
> "IFITM2", "IFITM3", "IGF2R", "IGFBP5", "IGFBP6", "IL13RA1", "IL6ST", 
> "INSIG1", "IQGAP2", "ITGA10", "ITGA4", "ITGAV", "ITGB1", "ITM2B", 
> "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", "KANK2", "KCNK2", 
> "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", "KIAA1462", "KIF13A", 
> "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", "LAMB1", "LAMC1", 
> "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", "LGALS1", "LHFP", 
> "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", "LMNA", "LOXL2", 
> "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", "MAGT1", 
> "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1", 
> "MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", "MOB1A", 
> "MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP", 
> "NANS", "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2", 
> "NOTCH2", "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1", 
> "OGFRL1", "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", "PALLD", 
> "PAPSS2", "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", "PDGFC", 
> "PDIA3", "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", "PHLDA3", 
> "PHLDB2", "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", "PLK2", 
> "PLOD2", "PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", "PLXDC2", 
> "POLR2L", "PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", "PRDM6", 
> "PRDX4", "PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", "PTGFRN", 
> "PTN", "PTPN14", "PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", "PYGL", 
> "QKI", "QSOX1", "RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B", "RBMS1", 
> "RCN1", "RECK", "REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ", 
> "RIN2", "RIT1", "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", "S1PR3", 
> "SASH1", "SCPEP1", "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4", 
> "SEC14L1", "SEL1L3", "SEMA3C", "SEMA3F", "SEPT10", "SERPINE2", 
> "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", "SH3BGRL", "SHC1", "SHROOM3", 
> "SIX1", "SIX4", "SKIL", "SLC16A4", "SLC30A1", "SLC30A7", "SLC35F5", 
> "SLC38A2", "SLC38A6", "SLC39A14", "SMAD3", "SNAI2", "SNAP23", 
> "SOSTDC1", "SOX9", "SPARC", "SPARCL1", "SPATA20", "SPCS3", "SPRED1", 
> "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", "SRPX", "SSBP4", "SSR1", 
> "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", "SUCLG2", "SURF4", "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", "TFPI", "TGFB1I1", "TGFBR2", "THBS1", "TIMP1", "TJP1")
>       +
>
> For some reason, the R console does not display a ">" symbol, 
> indicating that it has completed the function, but displays a "+" 
> symbol instead, which indicates that the function is still waiting for more input.
> However, I believe that my syntax is correct. If I shorten my command 
> by a few characters by removing the last entry, "TJP1":
>
>       $ R
>       > MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2", "ACTN1", 
> "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", "AJUBA", 
> "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", "ANXA6", 
> "APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", "ARMCX2", 
> "ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4", "ATP6V0E1", 
> "ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", "BOC", 
> "BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", "CALD1", 
> "CALU", "CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", "CD44", 
> "CD59", "CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", "CKAP4", 
> "CLIC4", "CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", "COL1A1", 
> "COL27A1", "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2", "COL6A1", 
> "COL6A2", "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2", 
> "CREG1", "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", "CTNNA1", 
> "CTSB", "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", "CYP26A1", 
> "CYR61", "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", "DLX1", 
> "DLX2", "DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", "DPY19L1", 
> "DSE", "DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2", "EGFR", 
> "EGR1", "EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", "EMILIN1", 
> "EMP1", "ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1", "ETS1", 
> "EVA1A", "EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1", 
> "FAM120A", "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", "FBN1", 
> "FBN2", "FGFR1", "FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2", 
> "FMOD", "FN1", "FNDC3B", "FSTL1", "FUCA2", "FZD1", "FZD2", "FZD7", 
> "GABRR1", "GALNT10", "GAS1", "GAS2", "GDF15", "GJA1", "GNAI1", 
> "GNG12", "GNS", "GORAB", "GPC6", "GPR137B", "GPX8", "GRN", "GSN", 
> "HES1", "HEXB", "HIBADH", "HIPK3", "HIST1H2AC", "HIST1H2BK", "HLA-A", 
> "HLA-B", "HLA-C", "HLA-F", "HLX", "HNMT", "HOMER1", "HS3ST3A1", 
> "HSP90B1", "HSPA5", "HSPB1", "HTRA1", "HYOU1", "ID1", "ID3", "IFI16", 
> "IFITM2", "IFITM3", "IGF2R", "IGFBP5", "IGFBP6", "IL13RA1", "IL6ST", 
> "INSIG1", "IQGAP2", "ITGA10", "ITGA4", "ITGAV", "ITGB1", "ITM2B", 
> "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", "KANK2", "KCNK2", 
> "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", "KIAA1462", "KIF13A", 
> "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", "LAMB1", "LAMC1", 
> "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", "LGALS1", "LHFP", 
> "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", "LMNA", "LOXL2", 
> "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", "MAGT1", 
> "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1", 
> "MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", "MOB1A", 
> "MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP", 
> "NANS", "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2", 
> "NOTCH2", "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1", 
> "OGFRL1", "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", "PALLD", 
> "PAPSS2", "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", "PDGFC", 
> "PDIA3", "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", "PHLDA3", 
> "PHLDB2", "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", "PLK2", 
> "PLOD2", "PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", "PLXDC2", 
> "POLR2L", "PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", "PRDM6", 
> "PRDX4", "PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", "PTGFRN", 
> "PTN", "PTPN14", "PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", "PYGL", 
> "QKI", "QSOX1", "RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B", "RBMS1", 
> "RCN1", "RECK", "REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ", 
> "RIN2", "RIT1", "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", "S1PR3", 
> "SASH1", "SCPEP1", "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4", 
> "SEC14L1", "SEL1L3", "SEMA3C", "SEMA3F", "SEPT10", "SERPINE2", 
> "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", "SH3BGRL", "SHC1", "SHROOM3", 
> "SIX1", "SIX4", "SKIL", "SLC16A4", "SLC30A1", "SLC30A7", "SLC35F5", 
> "SLC38A2", "SLC38A6", "SLC39A14", "SMAD3", "SNAI2", "SNAP23", 
> "SOSTDC1", "SOX9", "SPARC", "SPARCL1", "SPATA20", "SPCS3", "SPRED1", 
> "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", "SRPX", "SSBP4", "SSR1", 
> "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", "SUCLG2", "SURF4", "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", "TFPI", "TGFB1I1", "TGFBR2", "THBS1", "TIMP1")
>       >
>
> The function now works, and returns a ">" symbol, indicating that the 
> function completed successfully. The ls() function proves it:
>
>       $ R
>       > ls()
>       [1] "MES"
>
> Is this a bug in the base R "c" function? It seems like the "c" 
> function can only accept so many characters before it fails.
>
> Thank you for your time,
> core_contingency
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD
> 2XzMc9x&s=x5SJbPFsoqRiJYh7Y5B0QDKio2Wy4Je38lBBi99AbAE&e=
> PLEASE do read the posting guide 
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0C
> D2XzMc9x&s=lCG5D1ItMs8G_wkshvm4nBaVq4Ehy_zyq5mnp2zpt2Y&e=
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=x5SJbPFsoqRiJYh7Y5B0QDKio2Wy4Je38lBBi99AbAE&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=lCG5D1ItMs8G_wkshvm4nBaVq4Ehy_zyq5mnp2zpt2Y&e=
and provide commented, minimal, self-contained, reproducible code.

From no@p@m @end|ng |rom ||@@e@NA  Wed Jul 13 15:40:43 2022
From: no@p@m @end|ng |rom ||@@e@NA (Dr Eberhard Lisse)
Date: Wed, 13 Jul 2022 15:40:43 +0200
Subject: [R] How to parse a really silly date with lubridate
Message-ID: <tami12$11b8$1@ciao.gmane.io>


Hi,

I have data file which generated by an otherwise very nice (diabetes
log) app, but exports dates really silly.

After reading the enclosed mwe.csv into R like so

	 MWE <- read_delim('mwe.csv', delim = ';') %>%
		select(Date) %>%
		print()


this comes out as:

	 # A tibble: 2 ? 1
	Date
	<chr>
	 1 9. Jul 2022 at 11:39
	 2 10. Jul 2022 at 01:58


No matter what I try I am not able to parse this inside R to get at
proper dates (I have loaded tidyverse and lubridate).

I can easily do somethig

	 csvq  -d ';' -t '%e. %b %Y at %H:%i' \
		'SELECT Date as oridate,
			DATETIME_FORMAT(Date, "%Y-%m-%d %H:%m") AS date
		FROM mwe'

	 +-----------------------+------------------+
	 |        oridate        |       date       |
	 +-----------------------+------------------+
	 | 9. Jul 2022 at 11:39  | 2022-07-09 11:07 |
	 | 10. Jul 2022 at 01:58 | 2022-07-10 01:07 |
	 +-----------------------+------------------+

and hence could easily do something like

	 csvq  -d ';' -t '%e. %b %Y at %H:%i' \
	  'ALTER mwe
	  SET Date = DATETIME_FORMAT(Date, "%Y-%m-%d %H:%m")'

but would rather like to be able to do it inside R and would therefor
appreciate any advice in this regard.


greetings, el

-- 
To email me replace 'nospam' with 'el'

From r@oknz @end|ng |rom gm@||@com  Wed Jul 13 15:50:04 2022
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Thu, 14 Jul 2022 01:50:04 +1200
Subject: [R] Does the function "c" have a character limit?
In-Reply-To: <BN6PR2201MB155355656C6FD9DC0BD729C1CF899@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <61a4af67-18a8-f363-8068-54fd1bc4d53d@gmail.com>
 <75c7fbeb-8aa1-ab91-b164-11c2a18e176d@sapo.pt>
 <BN6PR2201MB155355656C6FD9DC0BD729C1CF899@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <CABcYAdKRhzPSBxa+DgBuYVT78JQFcK3Tm68+tPLcKFZPrfCkwg@mail.gmail.com>

Breaking up the *line* doesn't mean breaking up the *command*.
For example,
x <- c(
    "FOOBAR", # 1
    ...
    "FOOBAR", # 4999
    "UGGLE")
works fine, with source(..), with "R -f ...", and other ways.
Each *line* is short, but it's still one *command*.

I'd probably put that much data in a file, myself,
but R doesn't mind.


On Thu, 14 Jul 2022 at 01:21, Ebert,Timothy Aaron <tebert at ufl.edu> wrote:

> The limits to the size of vectors, matrices, data frames, lists, or other
> data structure does not have a simple answer.
> 1) 2^31 - 1  is the maximum number of rows.
> https://stackoverflow.com/questions/5233769/practical-limits-of-r-data-frame#:~:text=The%20number%20is%202%5E31,start%20collecting%20several%20of%20them
> .
> 2) help(Memory) suggests that the default limit for all variables is 6 Mb.
> The help page tells you how to change this.
> Neither of these two factors have any bearing on this problem except that
> your vector is not close to these limits.
>
> I got the same result you did when I entered your vector into my system (R
> 4.2 in RStudio, on 64 bit Windows). I shortened it by removing the first
> entry and it works.
>
> I can copy the entire line into Microsoft Word, and count the number of
> characters (including spaces) and I get 4089. There were seven characters
> in the first entry including the comma and space. If I add seven spaces
> between MES and the equal sign I get the original outcome. So the limit is
> on the number of characters in the line. You can get more entries by
> shortening each entry, or fewer if each entry was longer.
>
> As others have suggested, I would break the line into two pieces and then
> combine the pieces.
>
> Tim
>
>
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Rui Barradas
> Sent: Wednesday, July 13, 2022 6:36 AM
> To: core_contingency <ccontingency at gmail.com>; r-help at r-project.org
> Subject: Re: [R] Does the function "c" have a character limit?
>
> [External Email]
>
> Hello,
>
> This is documented behavior.
>  From R-intro, last line of section 1.8 [1].
>
>
> Command lines entered at the console are limited4 to about 4095 bytes (not
> characters).
>
>
> The number 4 in limited4 is a footnote link:
>
>
> some of the consoles will not allow you to enter more, and amongst those
> which do some will silently discard the excess and some will use it as the
> start of the next line.
>
>
>
> Prof. Ripley called the r-devel mailing list's attention to this in August
> 2006 when the limit was 1024 [2], it was then increased to the current
> 4095. I remember seeing a limit of 2048 (?) but couldn't find where.
>
>
> Try creating a file with your command as only content, then run
>
>
> x <- readLines("rhelp.txt")
> nchar(x)
> # [1] 4096
>
>
> You are above the limit by 1 byte.
> Standard solutions are to break the command line, in your case into at
> least 2 lines, or to source the command from file, like David proposed.
>
>
> [1]
>
> https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dproject.org_doc_manuals_R-2Dintro.html-23R-2Dcommands-5F003b-2Dcase-2Dsensitivity-2Detc&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=g5cbrpfyHaIN9sXycd_-f2iDsOcbuzLe2u3KjvQNm-0&e=
> [2]
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_pipermail_r-2Ddevel_2006-2DAugust_038985.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=B1YSca31vHlpy9WJG8o0MBTh7bX4v7M61eWfQ4tmdog&e=
>
>
> Hope this helps,
>
> Rui Barradas
>
>
> ?s 00:36 de 13/07/2022, core_contingency escreveu:
> > To Whom it May Concern,
> >
> > I am creating a vector with the base R function "c", with many
> > arguments as shown below:
> >
> >       $ R
> >       > MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2", "ACTN1",
> > "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", "AJUBA",
> > "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", "ANXA6",
> > "APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", "ARMCX2",
> > "ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4", "ATP6V0E1",
> > "ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", "BOC",
> > "BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", "CALD1",
> > "CALU", "CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", "CD44",
> > "CD59", "CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", "CKAP4",
> > "CLIC4", "CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", "COL1A1",
> > "COL27A1", "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2", "COL6A1",
> > "COL6A2", "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2",
> > "CREG1", "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", "CTNNA1",
> > "CTSB", "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", "CYP26A1",
> > "CYR61", "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", "DLX1",
> > "DLX2", "DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", "DPY19L1",
> > "DSE", "DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2", "EGFR",
> > "EGR1", "EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", "EMILIN1",
> > "EMP1", "ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1", "ETS1",
> > "EVA1A", "EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1",
> > "FAM120A", "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", "FBN1",
> > "FBN2", "FGFR1", "FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2",
> > "FMOD", "FN1", "FNDC3B", "FSTL1", "FUCA2", "FZD1", "FZD2", "FZD7",
> > "GABRR1", "GALNT10", "GAS1", "GAS2", "GDF15", "GJA1", "GNAI1",
> > "GNG12", "GNS", "GORAB", "GPC6", "GPR137B", "GPX8", "GRN", "GSN",
> > "HES1", "HEXB", "HIBADH", "HIPK3", "HIST1H2AC", "HIST1H2BK", "HLA-A",
> > "HLA-B", "HLA-C", "HLA-F", "HLX", "HNMT", "HOMER1", "HS3ST3A1",
> > "HSP90B1", "HSPA5", "HSPB1", "HTRA1", "HYOU1", "ID1", "ID3", "IFI16",
> > "IFITM2", "IFITM3", "IGF2R", "IGFBP5", "IGFBP6", "IL13RA1", "IL6ST",
> > "INSIG1", "IQGAP2", "ITGA10", "ITGA4", "ITGAV", "ITGB1", "ITM2B",
> > "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", "KANK2", "KCNK2",
> > "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", "KIAA1462", "KIF13A",
> > "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", "LAMB1", "LAMC1",
> > "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", "LGALS1", "LHFP",
> > "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", "LMNA", "LOXL2",
> > "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", "MAGT1",
> > "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1",
> > "MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", "MOB1A",
> > "MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP",
> > "NANS", "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2",
> > "NOTCH2", "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1",
> > "OGFRL1", "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", "PALLD",
> > "PAPSS2", "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", "PDGFC",
> > "PDIA3", "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", "PHLDA3",
> > "PHLDB2", "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", "PLK2",
> > "PLOD2", "PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", "PLXDC2",
> > "POLR2L", "PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", "PRDM6",
> > "PRDX4", "PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", "PTGFRN",
> > "PTN", "PTPN14", "PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", "PYGL",
> > "QKI", "QSOX1", "RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B", "RBMS1",
> > "RCN1", "RECK", "REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ",
> > "RIN2", "RIT1", "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", "S1PR3",
> > "SASH1", "SCPEP1", "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4",
> > "SEC14L1", "SEL1L3", "SEMA3C", "SEMA3F", "SEPT10", "SERPINE2",
> > "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", "SH3BGRL", "SHC1", "SHROOM3",
> > "SIX1", "SIX4", "SKIL", "SLC16A4", "SLC30A1", "SLC30A7", "SLC35F5",
> > "SLC38A2", "SLC38A6", "SLC39A14", "SMAD3", "SNAI2", "SNAP23",
> > "SOSTDC1", "SOX9", "SPARC", "SPARCL1", "SPATA20", "SPCS3", "SPRED1",
> > "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", "SRPX", "SSBP4", "SSR1",
> > "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", "SUCLG2", "SURF4", "SVIL",
> "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", "TFPI", "TGFB1I1", "TGFBR2",
> "THBS1", "TIMP1", "TJP1")
> >       +
> >
> > For some reason, the R console does not display a ">" symbol,
> > indicating that it has completed the function, but displays a "+"
> > symbol instead, which indicates that the function is still waiting for
> more input.
> > However, I believe that my syntax is correct. If I shorten my command
> > by a few characters by removing the last entry, "TJP1":
> >
> >       $ R
> >       > MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2", "ACTN1",
> > "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", "AJUBA",
> > "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", "ANXA6",
> > "APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", "ARMCX2",
> > "ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4", "ATP6V0E1",
> > "ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", "BOC",
> > "BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", "CALD1",
> > "CALU", "CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", "CD44",
> > "CD59", "CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", "CKAP4",
> > "CLIC4", "CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", "COL1A1",
> > "COL27A1", "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2", "COL6A1",
> > "COL6A2", "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2",
> > "CREG1", "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", "CTNNA1",
> > "CTSB", "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", "CYP26A1",
> > "CYR61", "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", "DLX1",
> > "DLX2", "DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", "DPY19L1",
> > "DSE", "DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2", "EGFR",
> > "EGR1", "EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", "EMILIN1",
> > "EMP1", "ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1", "ETS1",
> > "EVA1A", "EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1",
> > "FAM120A", "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", "FBN1",
> > "FBN2", "FGFR1", "FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2",
> > "FMOD", "FN1", "FNDC3B", "FSTL1", "FUCA2", "FZD1", "FZD2", "FZD7",
> > "GABRR1", "GALNT10", "GAS1", "GAS2", "GDF15", "GJA1", "GNAI1",
> > "GNG12", "GNS", "GORAB", "GPC6", "GPR137B", "GPX8", "GRN", "GSN",
> > "HES1", "HEXB", "HIBADH", "HIPK3", "HIST1H2AC", "HIST1H2BK", "HLA-A",
> > "HLA-B", "HLA-C", "HLA-F", "HLX", "HNMT", "HOMER1", "HS3ST3A1",
> > "HSP90B1", "HSPA5", "HSPB1", "HTRA1", "HYOU1", "ID1", "ID3", "IFI16",
> > "IFITM2", "IFITM3", "IGF2R", "IGFBP5", "IGFBP6", "IL13RA1", "IL6ST",
> > "INSIG1", "IQGAP2", "ITGA10", "ITGA4", "ITGAV", "ITGB1", "ITM2B",
> > "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", "KANK2", "KCNK2",
> > "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", "KIAA1462", "KIF13A",
> > "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", "LAMB1", "LAMC1",
> > "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", "LGALS1", "LHFP",
> > "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", "LMNA", "LOXL2",
> > "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", "MAGT1",
> > "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1",
> > "MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", "MOB1A",
> > "MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP",
> > "NANS", "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2",
> > "NOTCH2", "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1",
> > "OGFRL1", "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", "PALLD",
> > "PAPSS2", "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", "PDGFC",
> > "PDIA3", "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", "PHLDA3",
> > "PHLDB2", "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", "PLK2",
> > "PLOD2", "PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", "PLXDC2",
> > "POLR2L", "PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", "PRDM6",
> > "PRDX4", "PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", "PTGFRN",
> > "PTN", "PTPN14", "PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", "PYGL",
> > "QKI", "QSOX1", "RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B", "RBMS1",
> > "RCN1", "RECK", "REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ",
> > "RIN2", "RIT1", "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", "S1PR3",
> > "SASH1", "SCPEP1", "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4",
> > "SEC14L1", "SEL1L3", "SEMA3C", "SEMA3F", "SEPT10", "SERPINE2",
> > "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", "SH3BGRL", "SHC1", "SHROOM3",
> > "SIX1", "SIX4", "SKIL", "SLC16A4", "SLC30A1", "SLC30A7", "SLC35F5",
> > "SLC38A2", "SLC38A6", "SLC39A14", "SMAD3", "SNAI2", "SNAP23",
> > "SOSTDC1", "SOX9", "SPARC", "SPARCL1", "SPATA20", "SPCS3", "SPRED1",
> > "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", "SRPX", "SSBP4", "SSR1",
> > "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", "SUCLG2", "SURF4", "SVIL",
> "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", "TFPI", "TGFB1I1", "TGFBR2",
> "THBS1", "TIMP1")
> >       >
> >
> > The function now works, and returns a ">" symbol, indicating that the
> > function completed successfully. The ls() function proves it:
> >
> >       $ R
> >       > ls()
> >       [1] "MES"
> >
> > Is this a bug in the base R "c" function? It seems like the "c"
> > function can only accept so many characters before it fails.
> >
> > Thank you for your time,
> > core_contingency
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> > man_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> > Rzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD
> > 2XzMc9x&s=x5SJbPFsoqRiJYh7Y5B0QDKio2Wy4Je38lBBi99AbAE&e=
> > PLEASE do read the posting guide
> > https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> > g_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> > sRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0C
> > D2XzMc9x&s=lCG5D1ItMs8G_wkshvm4nBaVq4Ehy_zyq5mnp2zpt2Y&e=
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=x5SJbPFsoqRiJYh7Y5B0QDKio2Wy4Je38lBBi99AbAE&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=lCG5D1ItMs8G_wkshvm4nBaVq4Ehy_zyq5mnp2zpt2Y&e=
> and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Wed Jul 13 15:52:41 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Wed, 13 Jul 2022 16:52:41 +0300
Subject: [R] How to parse a really silly date with lubridate
In-Reply-To: <tami12$11b8$1@ciao.gmane.io>
References: <tami12$11b8$1@ciao.gmane.io>
Message-ID: <20220713165241.064bf6a0@arachnoid>

? Wed, 13 Jul 2022 15:40:43 +0200
Dr Eberhard Lisse <nospam at lisse.NA> ?????:

> 	 1 9. Jul 2022 at 11:39
> 	 2 10. Jul 2022 at 01:58

Don't know about lubridate, but the following seems to work:

Sys.setlocale('LC_TIME', 'C')
strptime(
 c('9. Jul 2022 at 11:39', '10. Jul 2022 at 01:58'),
 '%d. %b %Y at %H:%M'
)

(Use Sys.getlocale() and on.exit() to restore the previous locale
state if you need it.)

-- 
Best regards,
Ivan


From btupper @end|ng |rom b|ge|ow@org  Wed Jul 13 16:03:02 2022
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Wed, 13 Jul 2022 10:03:02 -0400
Subject: [R] How to parse a really silly date with lubridate
In-Reply-To: <tami12$11b8$1@ciao.gmane.io>
References: <tami12$11b8$1@ciao.gmane.io>
Message-ID: <CALrbzg3wM8r3UkxyDE-j8j0JTarewNrK-0Z5U7HfXsBeLPFrqg@mail.gmail.com>

Does this do the trick?

s = c("9. Jul 2022 at 11:39", "10. Jul 2022 at 01:58")
as.POSIXct(s, format = "%d. %b %Y at %H:%M")
as.POSIXct(s, format = "%d. %b %Y at %H:%M", tz = "UTC")


On Wed, Jul 13, 2022 at 9:41 AM Dr Eberhard Lisse <nospam at lisse.na> wrote:
>
>
> Hi,
>
> I have data file which generated by an otherwise very nice (diabetes
> log) app, but exports dates really silly.
>
> After reading the enclosed mwe.csv into R like so
>
>          MWE <- read_delim('mwe.csv', delim = ';') %>%
>                 select(Date) %>%
>                 print()
>
>
> this comes out as:
>
>          # A tibble: 2 ? 1
>         Date
>         <chr>
>          1 9. Jul 2022 at 11:39
>          2 10. Jul 2022 at 01:58
>
>
> No matter what I try I am not able to parse this inside R to get at
> proper dates (I have loaded tidyverse and lubridate).
>
> I can easily do somethig
>
>          csvq  -d ';' -t '%e. %b %Y at %H:%i' \
>                 'SELECT Date as oridate,
>                         DATETIME_FORMAT(Date, "%Y-%m-%d %H:%m") AS date
>                 FROM mwe'
>
>          +-----------------------+------------------+
>          |        oridate        |       date       |
>          +-----------------------+------------------+
>          | 9. Jul 2022 at 11:39  | 2022-07-09 11:07 |
>          | 10. Jul 2022 at 01:58 | 2022-07-10 01:07 |
>          +-----------------------+------------------+
>
> and hence could easily do something like
>
>          csvq  -d ';' -t '%e. %b %Y at %H:%i' \
>           'ALTER mwe
>           SET Date = DATETIME_FORMAT(Date, "%Y-%m-%d %H:%m")'
>
> but would rather like to be able to do it inside R and would therefor
> appreciate any advice in this regard.
>
>
> greetings, el
>
> --
> To email me replace 'nospam' with 'el'
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



--
Ben Tupper (he/him)
Bigelow Laboratory for Ocean Science
East Boothbay, Maine
http://www.bigelow.org/
https://eco.bigelow.org


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed Jul 13 16:42:13 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 13 Jul 2022 15:42:13 +0100
Subject: [R] Does the function "c" have a character limit?
In-Reply-To: <CABcYAdKRhzPSBxa+DgBuYVT78JQFcK3Tm68+tPLcKFZPrfCkwg@mail.gmail.com>
References: <61a4af67-18a8-f363-8068-54fd1bc4d53d@gmail.com>
 <75c7fbeb-8aa1-ab91-b164-11c2a18e176d@sapo.pt>
 <BN6PR2201MB155355656C6FD9DC0BD729C1CF899@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CABcYAdKRhzPSBxa+DgBuYVT78JQFcK3Tm68+tPLcKFZPrfCkwg@mail.gmail.com>
Message-ID: <320a723b-06e5-575b-db12-00b1a8856b29@sapo.pt>

Hello,

The original command has spaces in it. If they are removed its total 
length goes down to 3649 bytes (or characters, in this case it's the 
same). With this length the command is expected to work without further 
worries. And it does, I have just tried it.


MES=c("A2M","ABRACL","ACADVL","ACAP2","ACTA2","ACTN1","ADAM19","ADAM9","ADAMTS5","ADGRE5","ADGRG6","AEBP1","AJUBA","ALDH1A3","AMMECR1","ANTXR1","ANXA1","ANXA2","ANXA5","ANXA6","APOE","APP","ARHGAP1","ARHGEF40","ARL1","ARL4A","ARMCX2","ARPC1B","ASPH","ATP10D","ATP1B1","ATP2B1","ATP2B4","ATP6V0E1","ATP8B2","ATXN1","B2M","BAG3","BGN","BMP5","BNC2","BOC","BTN3A2","C1orf198","C1orf54","C4orf32","C6orf120","CALD1","CALU","CAPN2","CAPN6","CBFB","CBLB","CCDC80","CD164","CD44","CD59","CD63","CDH11","CETN2","CFH","CFI","CILP","CKAP4","CLIC4","CMTM3","CMTM6","CNN3","COL11A1","COL12A1","COL1A1","COL27A1","COL3A1","COL4A1","COL4A2","COL5A1","COL5A2","COL6A1","COL6A2","COL6A3","COPA","CPED1","CPS1","CRABP2","CREB3L2","CREG1","CRELD2","CRISPLD1","CRTAP","CSRP1","CTDSP2","CTNNA1","CTSB","CTSC","CTSO","CXCL12","CYBRD1","CYFIP1","CYP26A1","CYR61","DCAF6","DDOST","DDR2","DESI2","DKK3","DLC1","DLX1","DLX2","DMD","DNAJC1","DNAJC10","DNAJC3","DNM3OS","DPY19L1","DSE","DUSP14","DUSP5","DUSP6","EDEM1","EDNRA","EFEMP2","EGFR","EGR1","EGR3","EHD2","ELAVL1","ELF1","ELK3","ELK4","EMILIN1","EMP1","ENAH","EPHA3","EPS8","ERBIN","ERLIN1","ERRFI1","ETS1","EVA1A","EXT1","EXTL2","F2R","F2RL2","FAM102B","FAM114A1","FAM120A","FAM129A","FAM3C","FAM43A","FAM46A","FAT1","FBN1","FBN2","FGFR1","FIBIN","FILIP1L","FKBP14","FLNA","FLRT2","FMOD","FN1","FNDC3B","FSTL1","FUCA2","FZD1","FZD2","FZD7","GABRR1","GALNT10","GAS1","GAS2","GDF15","GJA1","GNAI1","GNG12","GNS","GORAB","GPC6","GPR137B","GPX8","GRN","GSN","HES1","HEXB","HIBADH","HIPK3","HIST1H2AC","HIST1H2BK","HLA-A","HLA-B","HLA-C","HLA-F","HLX","HNMT","HOMER1","HS3ST3A1","HSP90B1","HSPA5","HSPB1","HTRA1","HYOU1","ID1","ID3","IFI16","IFITM2","IFITM3","IGF2R","IGFBP5","IGFBP6","IL13RA1","IL6ST","INSIG1","IQGAP2","ITGA10","ITGA4","ITGAV","ITGB1","ITM2B","ITM2C","ITPR1","ITPRIPL2","JAK1","JAM3","KANK2","KCNK2","KCTD12","KDELC2","KDELR2","KDELR3","KDM5B","KIAA1462","KIF13A","KIRREL","KLF10","KLF4","KLF6","L3HYPDH","LAMB1","LAMC1","LAMP1","LAPTM4A","LASP1","LATS2","LEPROT","LGALS1","LHFP","LHX8","LIFR","LIPA","LITAF","LIX1L","LMAN1","LMNA","LOXL2","LPP","LRP10","LRRC17","LRRC8C","LTBP1","LUZP1","MAGT1","MAML2","MAN2A1","MANF","MBD2","MBNL1","MBTPS1","MEOX1","MEOX2","MEST","MGAT2","MGP","MGST1","MICAL2","MMP2","MOB1A","MRC2","MXRA5","MYADM","MYDGF","MYL12A","MYL12B","MYLIP","NANS","NBR1","NEK7","NES","NFIA","NFIC","NID1","NID2","NOTCH2","NOTCH2NL","NPC2","NPTN","NQO1","NR3C1","NRP1","OGFRL1","OLFML2A","OLFML2B","OLFML3","OSTC","P4HA1","PALLD","PAPSS2","PCDH18","PCOLCE2","PCSK5","PDE3A","PDE7B","PDGFC","PDIA3","PDIA4","PDIA6","PDLIM1","PEA15","PEAK1","PHLDA3","PHLDB2","PHTF2","PIAS3","PLAGL1","PLEKHA2","PLEKHH2","PLK2","PLOD2","PLOD3","PLPP1","PLS3","PLSCR1","PLSCR4","PLXDC2","POLR2L","PON2","POSTN","PPIB","PPIC","PPT1","PRCP","PRDM6","PRDX4","PRDX6","PROM1","PRRX1","PTBP1","PTGER4","PTGFRN","PTN","PTPN14","PTPRG","PTPRK","PTRF","PXDC1","PXDN","PYGL","QKI","QSOX1","RAB13","RAB29","RAB31","RAP1A","RAP1B","RBMS1","RCN1","RECK","REST","RGL1","RGS10","RGS3","RHOC","RHOJ","RIN2","RIT1","RNFT1","RNH1","ROBO1","ROR1","RRBP1","S1PR3","SASH1","SCPEP1","SCRG1","SDC2","SDC4","SDCBP","SDF4","SEC14L1","SEL1L3","SEMA3C","SEMA3F","SEPT10","SERPINE2","SERPINH1","SFT2D1","SFT2D2","SGK1","SH3BGRL","SHC1","SHROOM3","SIX1","SIX4","SKIL","SLC16A4","SLC30A1","SLC30A7","SLC35F5","SLC38A2","SLC38A6","SLC39A14","SMAD3","SNAI2","SNAP23","SOSTDC1","SOX9","SPARC","SPARCL1","SPATA20","SPCS3","SPRED1","SPRY1","SPRY4","SPRY4-IT1","SQSTM1","SRPX","SSBP4","SSR1","SSR3","STAT1","STAT3","STEAP1","STK38L","SUCLG2","SURF4","SVIL","SYDE1","SYNJ2","SYPL1","TCF7L2","TFE3","TFPI","TGFB1I1","TGFBR2","THBS1","TIMP1","TJP1")


Hope this helps,

Rui Barradas

?s 14:50 de 13/07/2022, Richard O'Keefe escreveu:
> Breaking up the *line* doesn't mean breaking up the *command*.
> For example,
> x <- c(
>  ??? "FOOBAR", # 1
>  ??? ...
>  ??? "FOOBAR", # 4999
>  ??? "UGGLE")
> works fine, with source(..), with "R -f ...", and other ways.
> Each *line* is short, but it's still one *command*.
> 
> I'd probably put that much data in a file, myself,
> but R doesn't mind.
> 
> 
> On Thu, 14 Jul 2022 at 01:21, Ebert,Timothy Aaron <tebert at ufl.edu 
> <mailto:tebert at ufl.edu>> wrote:
> 
>     The limits to the size of vectors, matrices, data frames, lists, or
>     other data structure does not have a simple answer.
>     1) 2^31 - 1? is the maximum number of rows.
>     https://stackoverflow.com/questions/5233769/practical-limits-of-r-data-frame#:~:text=The%20number%20is%202%5E31,start%20collecting%20several%20of%20them
>     <https://stackoverflow.com/questions/5233769/practical-limits-of-r-data-frame#:~:text=The%20number%20is%202%5E31,start%20collecting%20several%20of%20them>.
>     2) help(Memory) suggests that the default limit for all variables is
>     6 Mb. The help page tells you how to change this.
>     Neither of these two factors have any bearing on this problem except
>     that your vector is not close to these limits.
> 
>     I got the same result you did when I entered your vector into my
>     system (R 4.2 in RStudio, on 64 bit Windows). I shortened it by
>     removing the first entry and it works.
> 
>     I can copy the entire line into Microsoft Word, and count the number
>     of characters (including spaces) and I get 4089. There were seven
>     characters in the first entry including the comma and space. If I
>     add seven spaces between MES and the equal sign I get the original
>     outcome. So the limit is on the number of characters in the line.
>     You can get more entries by shortening each entry, or fewer if each
>     entry was longer.
> 
>     As others have suggested, I would break the line into two pieces and
>     then combine the pieces.
> 
>     Tim
> 
> 
> 
>     -----Original Message-----
>     From: R-help <r-help-bounces at r-project.org
>     <mailto:r-help-bounces at r-project.org>> On Behalf Of Rui Barradas
>     Sent: Wednesday, July 13, 2022 6:36 AM
>     To: core_contingency <ccontingency at gmail.com
>     <mailto:ccontingency at gmail.com>>; r-help at r-project.org
>     <mailto:r-help at r-project.org>
>     Subject: Re: [R] Does the function "c" have a character limit?
> 
>     [External Email]
> 
>     Hello,
> 
>     This is documented behavior.
>      ?From R-intro, last line of section 1.8 [1].
> 
> 
>     Command lines entered at the console are limited4 to about 4095
>     bytes (not characters).
> 
> 
>     The number 4 in limited4 is a footnote link:
> 
> 
>     some of the consoles will not allow you to enter more, and amongst
>     those which do some will silently discard the excess and some will
>     use it as the start of the next line.
> 
> 
> 
>     Prof. Ripley called the r-devel mailing list's attention to this in
>     August 2006 when the limit was 1024 [2], it was then increased to
>     the current 4095. I remember seeing a limit of 2048 (?) but couldn't
>     find where.
> 
> 
>     Try creating a file with your command as only content, then run
> 
> 
>     x <- readLines("rhelp.txt")
>     nchar(x)
>     # [1] 4096
> 
> 
>     You are above the limit by 1 byte.
>     Standard solutions are to break the command line, in your case into
>     at least 2 lines, or to source the command from file, like David
>     proposed.
> 
> 
>     [1]
>     https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dproject.org_doc_manuals_R-2Dintro.html-23R-2Dcommands-5F003b-2Dcase-2Dsensitivity-2Detc&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=g5cbrpfyHaIN9sXycd_-f2iDsOcbuzLe2u3KjvQNm-0&e=
>     <https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dproject.org_doc_manuals_R-2Dintro.html-23R-2Dcommands-5F003b-2Dcase-2Dsensitivity-2Detc&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=g5cbrpfyHaIN9sXycd_-f2iDsOcbuzLe2u3KjvQNm-0&e=>
>     [2]
>     https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_pipermail_r-2Ddevel_2006-2DAugust_038985.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=B1YSca31vHlpy9WJG8o0MBTh7bX4v7M61eWfQ4tmdog&e=
>     <https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_pipermail_r-2Ddevel_2006-2DAugust_038985.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=B1YSca31vHlpy9WJG8o0MBTh7bX4v7M61eWfQ4tmdog&e=>
> 
> 
>     Hope this helps,
> 
>     Rui Barradas
> 
> 
>     ?s 00:36 de 13/07/2022, core_contingency escreveu:
>      > To Whom it May Concern,
>      >
>      > I am creating a vector with the base R function "c", with many
>      > arguments as shown below:
>      >
>      >? ? ? ?$ R
>      >? ? ? ?> MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2",
>     "ACTN1",
>      > "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", "AJUBA",
>      > "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", "ANXA6",
>      > "APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", "ARMCX2",
>      > "ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4",
>     "ATP6V0E1",
>      > "ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", "BOC",
>      > "BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", "CALD1",
>      > "CALU", "CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", "CD44",
>      > "CD59", "CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", "CKAP4",
>      > "CLIC4", "CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", "COL1A1",
>      > "COL27A1", "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2",
>     "COL6A1",
>      > "COL6A2", "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2",
>      > "CREG1", "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", "CTNNA1",
>      > "CTSB", "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", "CYP26A1",
>      > "CYR61", "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", "DLX1",
>      > "DLX2", "DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", "DPY19L1",
>      > "DSE", "DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2",
>     "EGFR",
>      > "EGR1", "EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", "EMILIN1",
>      > "EMP1", "ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1",
>     "ETS1",
>      > "EVA1A", "EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1",
>      > "FAM120A", "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", "FBN1",
>      > "FBN2", "FGFR1", "FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2",
>      > "FMOD", "FN1", "FNDC3B", "FSTL1", "FUCA2", "FZD1", "FZD2", "FZD7",
>      > "GABRR1", "GALNT10", "GAS1", "GAS2", "GDF15", "GJA1", "GNAI1",
>      > "GNG12", "GNS", "GORAB", "GPC6", "GPR137B", "GPX8", "GRN", "GSN",
>      > "HES1", "HEXB", "HIBADH", "HIPK3", "HIST1H2AC", "HIST1H2BK",
>     "HLA-A",
>      > "HLA-B", "HLA-C", "HLA-F", "HLX", "HNMT", "HOMER1", "HS3ST3A1",
>      > "HSP90B1", "HSPA5", "HSPB1", "HTRA1", "HYOU1", "ID1", "ID3",
>     "IFI16",
>      > "IFITM2", "IFITM3", "IGF2R", "IGFBP5", "IGFBP6", "IL13RA1", "IL6ST",
>      > "INSIG1", "IQGAP2", "ITGA10", "ITGA4", "ITGAV", "ITGB1", "ITM2B",
>      > "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", "KANK2", "KCNK2",
>      > "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", "KIAA1462",
>     "KIF13A",
>      > "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", "LAMB1", "LAMC1",
>      > "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", "LGALS1", "LHFP",
>      > "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", "LMNA", "LOXL2",
>      > "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", "MAGT1",
>      > "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1",
>      > "MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", "MOB1A",
>      > "MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP",
>      > "NANS", "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2",
>      > "NOTCH2", "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1",
>      > "OGFRL1", "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", "PALLD",
>      > "PAPSS2", "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", "PDGFC",
>      > "PDIA3", "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", "PHLDA3",
>      > "PHLDB2", "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", "PLK2",
>      > "PLOD2", "PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", "PLXDC2",
>      > "POLR2L", "PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", "PRDM6",
>      > "PRDX4", "PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", "PTGFRN",
>      > "PTN", "PTPN14", "PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", "PYGL",
>      > "QKI", "QSOX1", "RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B",
>     "RBMS1",
>      > "RCN1", "RECK", "REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ",
>      > "RIN2", "RIT1", "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", "S1PR3",
>      > "SASH1", "SCPEP1", "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4",
>      > "SEC14L1", "SEL1L3", "SEMA3C", "SEMA3F", "SEPT10", "SERPINE2",
>      > "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", "SH3BGRL", "SHC1",
>     "SHROOM3",
>      > "SIX1", "SIX4", "SKIL", "SLC16A4", "SLC30A1", "SLC30A7", "SLC35F5",
>      > "SLC38A2", "SLC38A6", "SLC39A14", "SMAD3", "SNAI2", "SNAP23",
>      > "SOSTDC1", "SOX9", "SPARC", "SPARCL1", "SPATA20", "SPCS3", "SPRED1",
>      > "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", "SRPX", "SSBP4", "SSR1",
>      > "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", "SUCLG2", "SURF4",
>     "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", "TFPI",
>     "TGFB1I1", "TGFBR2", "THBS1", "TIMP1", "TJP1")
>      >? ? ? ?+
>      >
>      > For some reason, the R console does not display a ">" symbol,
>      > indicating that it has completed the function, but displays a "+"
>      > symbol instead, which indicates that the function is still
>     waiting for more input.
>      > However, I believe that my syntax is correct. If I shorten my
>     command
>      > by a few characters by removing the last entry, "TJP1":
>      >
>      >? ? ? ?$ R
>      >? ? ? ?> MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2",
>     "ACTN1",
>      > "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", "AJUBA",
>      > "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", "ANXA6",
>      > "APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", "ARMCX2",
>      > "ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4",
>     "ATP6V0E1",
>      > "ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", "BOC",
>      > "BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", "CALD1",
>      > "CALU", "CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", "CD44",
>      > "CD59", "CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", "CKAP4",
>      > "CLIC4", "CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", "COL1A1",
>      > "COL27A1", "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2",
>     "COL6A1",
>      > "COL6A2", "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2",
>      > "CREG1", "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", "CTNNA1",
>      > "CTSB", "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", "CYP26A1",
>      > "CYR61", "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", "DLX1",
>      > "DLX2", "DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", "DPY19L1",
>      > "DSE", "DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2",
>     "EGFR",
>      > "EGR1", "EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", "EMILIN1",
>      > "EMP1", "ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1",
>     "ETS1",
>      > "EVA1A", "EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1",
>      > "FAM120A", "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", "FBN1",
>      > "FBN2", "FGFR1", "FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2",
>      > "FMOD", "FN1", "FNDC3B", "FSTL1", "FUCA2", "FZD1", "FZD2", "FZD7",
>      > "GABRR1", "GALNT10", "GAS1", "GAS2", "GDF15", "GJA1", "GNAI1",
>      > "GNG12", "GNS", "GORAB", "GPC6", "GPR137B", "GPX8", "GRN", "GSN",
>      > "HES1", "HEXB", "HIBADH", "HIPK3", "HIST1H2AC", "HIST1H2BK",
>     "HLA-A",
>      > "HLA-B", "HLA-C", "HLA-F", "HLX", "HNMT", "HOMER1", "HS3ST3A1",
>      > "HSP90B1", "HSPA5", "HSPB1", "HTRA1", "HYOU1", "ID1", "ID3",
>     "IFI16",
>      > "IFITM2", "IFITM3", "IGF2R", "IGFBP5", "IGFBP6", "IL13RA1", "IL6ST",
>      > "INSIG1", "IQGAP2", "ITGA10", "ITGA4", "ITGAV", "ITGB1", "ITM2B",
>      > "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", "KANK2", "KCNK2",
>      > "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", "KIAA1462",
>     "KIF13A",
>      > "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", "LAMB1", "LAMC1",
>      > "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", "LGALS1", "LHFP",
>      > "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", "LMNA", "LOXL2",
>      > "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", "MAGT1",
>      > "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1",
>      > "MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", "MOB1A",
>      > "MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP",
>      > "NANS", "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2",
>      > "NOTCH2", "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1",
>      > "OGFRL1", "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", "PALLD",
>      > "PAPSS2", "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", "PDGFC",
>      > "PDIA3", "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", "PHLDA3",
>      > "PHLDB2", "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", "PLK2",
>      > "PLOD2", "PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", "PLXDC2",
>      > "POLR2L", "PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", "PRDM6",
>      > "PRDX4", "PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", "PTGFRN",
>      > "PTN", "PTPN14", "PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", "PYGL",
>      > "QKI", "QSOX1", "RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B",
>     "RBMS1",
>      > "RCN1", "RECK", "REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ",
>      > "RIN2", "RIT1", "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", "S1PR3",
>      > "SASH1", "SCPEP1", "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4",
>      > "SEC14L1", "SEL1L3", "SEMA3C", "SEMA3F", "SEPT10", "SERPINE2",
>      > "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", "SH3BGRL", "SHC1",
>     "SHROOM3",
>      > "SIX1", "SIX4", "SKIL", "SLC16A4", "SLC30A1", "SLC30A7", "SLC35F5",
>      > "SLC38A2", "SLC38A6", "SLC39A14", "SMAD3", "SNAI2", "SNAP23",
>      > "SOSTDC1", "SOX9", "SPARC", "SPARCL1", "SPATA20", "SPCS3", "SPRED1",
>      > "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", "SRPX", "SSBP4", "SSR1",
>      > "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", "SUCLG2", "SURF4",
>     "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", "TFPI",
>     "TGFB1I1", "TGFBR2", "THBS1", "TIMP1")
>      >? ? ? ?>
>      >
>      > The function now works, and returns a ">" symbol, indicating that
>     the
>      > function completed successfully. The ls() function proves it:
>      >
>      >? ? ? ?$ R
>      >? ? ? ?> ls()
>      >? ? ? ?[1] "MES"
>      >
>      > Is this a bug in the base R "c" function? It seems like the "c"
>      > function can only accept so many characters before it fails.
>      >
>      > Thank you for your time,
>      > core_contingency
>      >
>      >
>      > ______________________________________________
>      > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>     -- To UNSUBSCRIBE and more, see
>      >
>     https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
>     <https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail>
>      >
>     man_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
>      >
>     Rzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD
>      > 2XzMc9x&s=x5SJbPFsoqRiJYh7Y5B0QDKio2Wy4Je38lBBi99AbAE&e=
>      > PLEASE do read the posting guide
>      >
>     https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
>     <https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or>
>      >
>     g_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
>      >
>     sRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0C
>      > D2XzMc9x&s=lCG5D1ItMs8G_wkshvm4nBaVq4Ehy_zyq5mnp2zpt2Y&e=
>      > and provide commented, minimal, self-contained, reproducible code.
> 
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     To UNSUBSCRIBE and more, see
>     https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=x5SJbPFsoqRiJYh7Y5B0QDKio2Wy4Je38lBBi99AbAE&e=
>     <https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=x5SJbPFsoqRiJYh7Y5B0QDKio2Wy4Je38lBBi99AbAE&e=>
>     PLEASE do read the posting guide
>     https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=lCG5D1ItMs8G_wkshvm4nBaVq4Ehy_zyq5mnp2zpt2Y&e=
>     <https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=lCG5D1ItMs8G_wkshvm4nBaVq4Ehy_zyq5mnp2zpt2Y&e=>
>     and provide commented, minimal, self-contained, reproducible code.
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>     and provide commented, minimal, self-contained, reproducible code.
> 

From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed Jul 13 16:48:03 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 13 Jul 2022 15:48:03 +0100
Subject: [R] How to parse a really silly date with lubridate
In-Reply-To: <tami12$11b8$1@ciao.gmane.io>
References: <tami12$11b8$1@ciao.gmane.io>
Message-ID: <c166bf12-124e-0477-8694-4bfa73399447@sapo.pt>

Hello,

With lubridate, note in what sequence your datetime elements occur and 
use the appropriate function.


d <- c('9. Jul 2022 at 11:39', '10. Jul 2022 at 01:58')
lubridate::dmy_hm(d)
#> [1] "2022-07-09 11:39:00 UTC" "2022-07-10 01:58:00 UTC"


Hope this helps,

Rui Barradas

?s 14:40 de 13/07/2022, Dr Eberhard Lisse escreveu:
> 
> Hi,
> 
> I have data file which generated by an otherwise very nice (diabetes
> log) app, but exports dates really silly.
> 
> After reading the enclosed mwe.csv into R like so
> 
>  ???? MWE <- read_delim('mwe.csv', delim = ';') %>%
>  ??????? select(Date) %>%
>  ??????? print()
> 
> 
> this comes out as:
> 
>  ???? # A tibble: 2 ? 1
>  ????Date
>  ????<chr>
>  ???? 1 9. Jul 2022 at 11:39
>  ???? 2 10. Jul 2022 at 01:58
> 
> 
> No matter what I try I am not able to parse this inside R to get at
> proper dates (I have loaded tidyverse and lubridate).
> 
> I can easily do somethig
> 
>  ???? csvq? -d ';' -t '%e. %b %Y at %H:%i' \
>  ??????? 'SELECT Date as oridate,
>  ??????????? DATETIME_FORMAT(Date, "%Y-%m-%d %H:%m") AS date
>  ??????? FROM mwe'
> 
>  ???? +-----------------------+------------------+
>  ???? |??????? oridate??????? |?????? date?????? |
>  ???? +-----------------------+------------------+
>  ???? | 9. Jul 2022 at 11:39? | 2022-07-09 11:07 |
>  ???? | 10. Jul 2022 at 01:58 | 2022-07-10 01:07 |
>  ???? +-----------------------+------------------+
> 
> and hence could easily do something like
> 
>  ???? csvq? -d ';' -t '%e. %b %Y at %H:%i' \
>  ????? 'ALTER mwe
>  ????? SET Date = DATETIME_FORMAT(Date, "%Y-%m-%d %H:%m")'
> 
> but would rather like to be able to do it inside R and would therefor
> appreciate any advice in this regard.
> 
> 
> greetings, el
>


From @vi@e@gross m@iii@g oii gm@ii@com  Wed Jul 13 19:46:49 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Wed, 13 Jul 2022 13:46:49 -0400
Subject: [R] Does the function "c" have a character limit?
In-Reply-To: <BN6PR2201MB155355656C6FD9DC0BD729C1CF899@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <61a4af67-18a8-f363-8068-54fd1bc4d53d@gmail.com>
 <75c7fbeb-8aa1-ab91-b164-11c2a18e176d@sapo.pt>
 <BN6PR2201MB155355656C6FD9DC0BD729C1CF899@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <014b01d896e0$8789def0$969d9cd0$@gmail.com>

If I follow this thread, it looks clear that the problem is superficial and not really about the c() function as it is below sea level.

Is this also a problem if you replace c() with max () or list() as I think it may be? Then it is more about what length the interpreter is able to handle everywhere or on your installation or memory constraints.

There are, of course, lots of ways to work around it and some have been mentioned, as you clearly have data.frames with underlying vectors that are millions of units long including with character data like yours.

I was able to reproduce your problem within RSTUDIO and noted the editor window actually cuts off the text and asks you to click to see more, which may be a hint. Have you tried a paste of this long thing directly to an R interpreter not through RSTUDIO?

I did an experiment and broke up the big monster that failed into multiple short lines and it works fine. It looks like a LINE LENGTH limit, not a statement limit.

So if your data was entered say like this:

MES=c(
  "A2M",
  "ABRACL",
  "ACADVL",
  "ACAP2",
  ...,
  "TIMP1",
  "TJP1"
  )

Then it should work for much larger amounts of data.

And, of course, you can enter multiple smaller units and concatenate them together in the code and remove the originals, as long as each unit was small enough. Reading the data in from a file also should bypass the issue if done right.

There is no reason every programmer should try to make everything a one-liner.


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ebert,Timothy Aaron
Sent: Wednesday, July 13, 2022 9:21 AM
To: Rui Barradas <ruipbarradas at sapo.pt>; core_contingency <ccontingency at gmail.com>; r-help at r-project.org
Subject: Re: [R] Does the function "c" have a character limit?

The limits to the size of vectors, matrices, data frames, lists, or other data structure does not have a simple answer.
1) 2^31 - 1  is the maximum number of rows. https://stackoverflow.com/questions/5233769/practical-limits-of-r-data-frame#:~:text=The%20number%20is%202%5E31,start%20collecting%20several%20of%20them.
2) help(Memory) suggests that the default limit for all variables is 6 Mb. The help page tells you how to change this.
Neither of these two factors have any bearing on this problem except that your vector is not close to these limits.

I got the same result you did when I entered your vector into my system (R 4.2 in RStudio, on 64 bit Windows). I shortened it by removing the first entry and it works.

I can copy the entire line into Microsoft Word, and count the number of characters (including spaces) and I get 4089. There were seven characters in the first entry including the comma and space. If I add seven spaces between MES and the equal sign I get the original outcome. So the limit is on the number of characters in the line. You can get more entries by shortening each entry, or fewer if each entry was longer.

As others have suggested, I would break the line into two pieces and then combine the pieces.

Tim



-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Rui Barradas
Sent: Wednesday, July 13, 2022 6:36 AM
To: core_contingency <ccontingency at gmail.com>; r-help at r-project.org
Subject: Re: [R] Does the function "c" have a character limit?

[External Email]

Hello,

This is documented behavior.
 From R-intro, last line of section 1.8 [1].


Command lines entered at the console are limited4 to about 4095 bytes (not characters).


The number 4 in limited4 is a footnote link:


some of the consoles will not allow you to enter more, and amongst those which do some will silently discard the excess and some will use it as the start of the next line.



Prof. Ripley called the r-devel mailing list's attention to this in August 2006 when the limit was 1024 [2], it was then increased to the current 4095. I remember seeing a limit of 2048 (?) but couldn't find where.


Try creating a file with your command as only content, then run


x <- readLines("rhelp.txt")
nchar(x)
# [1] 4096


You are above the limit by 1 byte.
Standard solutions are to break the command line, in your case into at least 2 lines, or to source the command from file, like David proposed.


[1]
https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dproject.org_doc_manuals_R-2Dintro.html-23R-2Dcommands-5F003b-2Dcase-2Dsensitivity-2Detc&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=g5cbrpfyHaIN9sXycd_-f2iDsOcbuzLe2u3KjvQNm-0&e=
[2] https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_pipermail_r-2Ddevel_2006-2DAugust_038985.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=B1YSca31vHlpy9WJG8o0MBTh7bX4v7M61eWfQ4tmdog&e=


Hope this helps,

Rui Barradas


?s 00:36 de 13/07/2022, core_contingency escreveu:
> To Whom it May Concern,
>
> I am creating a vector with the base R function "c", with many 
> arguments as shown below:
>
>       $ R
>       > MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2", "ACTN1", 
> "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", "AJUBA", 
> "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", "ANXA6", 
> "APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", "ARMCX2", 
> "ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4", "ATP6V0E1", 
> "ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", "BOC", 
> "BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", "CALD1", 
> "CALU", "CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", "CD44", 
> "CD59", "CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", "CKAP4", 
> "CLIC4", "CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", "COL1A1", 
> "COL27A1", "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2", "COL6A1", 
> "COL6A2", "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2", 
> "CREG1", "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", "CTNNA1", 
> "CTSB", "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", "CYP26A1", 
> "CYR61", "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", "DLX1", 
> "DLX2", "DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", "DPY19L1", 
> "DSE", "DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2", "EGFR", 
> "EGR1", "EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", "EMILIN1", 
> "EMP1", "ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1", "ETS1", 
> "EVA1A", "EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1", 
> "FAM120A", "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", "FBN1", 
> "FBN2", "FGFR1", "FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2", 
> "FMOD", "FN1", "FNDC3B", "FSTL1", "FUCA2", "FZD1", "FZD2", "FZD7", 
> "GABRR1", "GALNT10", "GAS1", "GAS2", "GDF15", "GJA1", "GNAI1", 
> "GNG12", "GNS", "GORAB", "GPC6", "GPR137B", "GPX8", "GRN", "GSN", 
> "HES1", "HEXB", "HIBADH", "HIPK3", "HIST1H2AC", "HIST1H2BK", "HLA-A", 
> "HLA-B", "HLA-C", "HLA-F", "HLX", "HNMT", "HOMER1", "HS3ST3A1", 
> "HSP90B1", "HSPA5", "HSPB1", "HTRA1", "HYOU1", "ID1", "ID3", "IFI16", 
> "IFITM2", "IFITM3", "IGF2R", "IGFBP5", "IGFBP6", "IL13RA1", "IL6ST", 
> "INSIG1", "IQGAP2", "ITGA10", "ITGA4", "ITGAV", "ITGB1", "ITM2B", 
> "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", "KANK2", "KCNK2", 
> "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", "KIAA1462", "KIF13A", 
> "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", "LAMB1", "LAMC1", 
> "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", "LGALS1", "LHFP", 
> "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", "LMNA", "LOXL2", 
> "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", "MAGT1", 
> "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1", 
> "MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", "MOB1A", 
> "MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP", 
> "NANS", "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2", 
> "NOTCH2", "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1", 
> "OGFRL1", "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", "PALLD", 
> "PAPSS2", "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", "PDGFC", 
> "PDIA3", "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", "PHLDA3", 
> "PHLDB2", "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", "PLK2", 
> "PLOD2", "PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", "PLXDC2", 
> "POLR2L", "PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", "PRDM6", 
> "PRDX4", "PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", "PTGFRN", 
> "PTN", "PTPN14", "PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", "PYGL", 
> "QKI", "QSOX1", "RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B", "RBMS1", 
> "RCN1", "RECK", "REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ", 
> "RIN2", "RIT1", "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", "S1PR3", 
> "SASH1", "SCPEP1", "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4", 
> "SEC14L1", "SEL1L3", "SEMA3C", "SEMA3F", "SEPT10", "SERPINE2", 
> "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", "SH3BGRL", "SHC1", "SHROOM3", 
> "SIX1", "SIX4", "SKIL", "SLC16A4", "SLC30A1", "SLC30A7", "SLC35F5", 
> "SLC38A2", "SLC38A6", "SLC39A14", "SMAD3", "SNAI2", "SNAP23", 
> "SOSTDC1", "SOX9", "SPARC", "SPARCL1", "SPATA20", "SPCS3", "SPRED1", 
> "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", "SRPX", "SSBP4", "SSR1", 
> "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", "SUCLG2", "SURF4", "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", "TFPI", "TGFB1I1", "TGFBR2", "THBS1", "TIMP1", "TJP1")
>       +
>
> For some reason, the R console does not display a ">" symbol, 
> indicating that it has completed the function, but displays a "+"
> symbol instead, which indicates that the function is still waiting for more input.
> However, I believe that my syntax is correct. If I shorten my command 
> by a few characters by removing the last entry, "TJP1":
>
>       $ R
>       > MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2", "ACTN1", 
> "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", "AJUBA", 
> "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", "ANXA6", 
> "APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", "ARMCX2", 
> "ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4", "ATP6V0E1", 
> "ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", "BOC", 
> "BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", "CALD1", 
> "CALU", "CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", "CD44", 
> "CD59", "CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", "CKAP4", 
> "CLIC4", "CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", "COL1A1", 
> "COL27A1", "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2", "COL6A1", 
> "COL6A2", "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2", 
> "CREG1", "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", "CTNNA1", 
> "CTSB", "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", "CYP26A1", 
> "CYR61", "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", "DLX1", 
> "DLX2", "DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", "DPY19L1", 
> "DSE", "DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2", "EGFR", 
> "EGR1", "EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", "EMILIN1", 
> "EMP1", "ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1", "ETS1", 
> "EVA1A", "EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1", 
> "FAM120A", "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", "FBN1", 
> "FBN2", "FGFR1", "FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2", 
> "FMOD", "FN1", "FNDC3B", "FSTL1", "FUCA2", "FZD1", "FZD2", "FZD7", 
> "GABRR1", "GALNT10", "GAS1", "GAS2", "GDF15", "GJA1", "GNAI1", 
> "GNG12", "GNS", "GORAB", "GPC6", "GPR137B", "GPX8", "GRN", "GSN", 
> "HES1", "HEXB", "HIBADH", "HIPK3", "HIST1H2AC", "HIST1H2BK", "HLA-A", 
> "HLA-B", "HLA-C", "HLA-F", "HLX", "HNMT", "HOMER1", "HS3ST3A1", 
> "HSP90B1", "HSPA5", "HSPB1", "HTRA1", "HYOU1", "ID1", "ID3", "IFI16", 
> "IFITM2", "IFITM3", "IGF2R", "IGFBP5", "IGFBP6", "IL13RA1", "IL6ST", 
> "INSIG1", "IQGAP2", "ITGA10", "ITGA4", "ITGAV", "ITGB1", "ITM2B", 
> "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", "KANK2", "KCNK2", 
> "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", "KIAA1462", "KIF13A", 
> "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", "LAMB1", "LAMC1", 
> "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", "LGALS1", "LHFP", 
> "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", "LMNA", "LOXL2", 
> "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", "MAGT1", 
> "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1", 
> "MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", "MOB1A", 
> "MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP", 
> "NANS", "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2", 
> "NOTCH2", "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1", 
> "OGFRL1", "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", "PALLD", 
> "PAPSS2", "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", "PDGFC", 
> "PDIA3", "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", "PHLDA3", 
> "PHLDB2", "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", "PLK2", 
> "PLOD2", "PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", "PLXDC2", 
> "POLR2L", "PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", "PRDM6", 
> "PRDX4", "PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", "PTGFRN", 
> "PTN", "PTPN14", "PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", "PYGL", 
> "QKI", "QSOX1", "RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B", "RBMS1", 
> "RCN1", "RECK", "REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ", 
> "RIN2", "RIT1", "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", "S1PR3", 
> "SASH1", "SCPEP1", "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4", 
> "SEC14L1", "SEL1L3", "SEMA3C", "SEMA3F", "SEPT10", "SERPINE2", 
> "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", "SH3BGRL", "SHC1", "SHROOM3", 
> "SIX1", "SIX4", "SKIL", "SLC16A4", "SLC30A1", "SLC30A7", "SLC35F5", 
> "SLC38A2", "SLC38A6", "SLC39A14", "SMAD3", "SNAI2", "SNAP23", 
> "SOSTDC1", "SOX9", "SPARC", "SPARCL1", "SPATA20", "SPCS3", "SPRED1", 
> "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", "SRPX", "SSBP4", "SSR1", 
> "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", "SUCLG2", "SURF4", "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", "TFPI", "TGFB1I1", "TGFBR2", "THBS1", "TIMP1")
>       >
>
> The function now works, and returns a ">" symbol, indicating that the 
> function completed successfully. The ls() function proves it:
>
>       $ R
>       > ls()
>       [1] "MES"
>
> Is this a bug in the base R "c" function? It seems like the "c" 
> function can only accept so many characters before it fails.
>
> Thank you for your time,
> core_contingency
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD
> 2XzMc9x&s=x5SJbPFsoqRiJYh7Y5B0QDKio2Wy4Je38lBBi99AbAE&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0C
> D2XzMc9x&s=lCG5D1ItMs8G_wkshvm4nBaVq4Ehy_zyq5mnp2zpt2Y&e=
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=x5SJbPFsoqRiJYh7Y5B0QDKio2Wy4Je38lBBi99AbAE&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=lCG5D1ItMs8G_wkshvm4nBaVq4Ehy_zyq5mnp2zpt2Y&e=
and provide commented, minimal, self-contained, reproducible code.
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From @vi@e@gross m@iii@g oii gm@ii@com  Wed Jul 13 20:36:56 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Wed, 13 Jul 2022 14:36:56 -0400
Subject: [R] How to parse a really silly date with lubridate
In-Reply-To: <c166bf12-124e-0477-8694-4bfa73399447@sapo.pt>
References: <tami12$11b8$1@ciao.gmane.io>
 <c166bf12-124e-0477-8694-4bfa73399447@sapo.pt>
Message-ID: <016e01d896e7$8850ad30$98f20790$@gmail.com>

Eberhard,

Others have supplied ways to do this using various date management functions. But I want to add another option that may make sense if the dates are not all quite as predictable.

You can use your own regular expressions in R as in most languages, that try to match each entry to one or more patterns you supply. When it matches a pattern, you can capture various bits of what you find and recombine them into any valid date format you want or convert them into some date type.

You just showed two pseudo-date-time entries:

	 1 9. Jul 2022 at 11:39
	 2 10. Jul 2022 at 01:58

It looks like the first item in each is a sequence number that can be ignored. But I assume after the ninth, it takes up ever more space as it becomes 10 then 100 and so on. It would thus be of varying width. So, in English, you are looking for a pattern like:

- beginning of the line/entry
- one or more digits in 0-9 followed by what looks like a space, but maybe arbitrary whitespace.
- capture one or two more digits if followed by a period, but not the period.
- capture what looks like a three-letter character string representing a month as a "word" but not what follows.
- whitespace followed by "at" followed by more whitespace can be ignored but must be present.
- capture a two digit number perhaps including the colon and another two-digit number, or two parts without the colon.
- match the end of the entry

If it looks like the above, and you captured the parts needed, you can reconstitute the parts, albeit perhaps first checking to see if the month names are valid (which can be handled by a pattern part that looks like "(Jan|Feb|...|Dec)" and of course making sure the other parts also fit requirements such as days as integers being between 1 and 31 and similar issues about time. My guess is that you don't really want to validate that deeply, let alone do the validation within the regular expression.

But when your data ends up having multiple ways of saying the date that you can enumerate and tell apart, this can be a way to parse them and identify what it matches and so on.

In the above case, full regular expressions may be overkill nd regular string functions might suffice.  There are many ways, including something as simple as this:

dt <- "1 9. Jul 2022 at 11:39"
dt_parts <- strsplit(dt, " ")

Unfortunately, this returns a vector of parts with a decimal point still in place:

> dt_parts
[1] "1"     "9."    "Jul"   "2022"  "at"    "11:39"

You can now ignore  the parts in the first fifth positions as the sequence number and "at" are meaningless and use these:

the_day <- dt_parts[2]
the_month <- dt_parts[3]
the_year <- dt_parts[4]
the_time <- dt_parts[6]

Of course, you now may want further processing using one of many ways to turn something like "9." (note there may be a "12." too) or even make it an integer instead of a character string. You may want to change the dates into numerals with something like this:

month_conversion <- list("Jan"="01", Feb="02", Mar="03", Apr="04", May="05", Jun="06", 
                         Jul="07", Aug="08", Sep="09", Oct="10", Nov="11", Dec="12")
month_numeric_string <- month_conversion[the_month]


My point is that if your format is consistent, you can pick out the parts you want as strings or make them decimals or whatever you need and use string functions, such as paste0() to recombine them into the format you want. 

Of course if you can tell some date-related function how to do it for you, that is obviously simpler and better for you.

And, obviously, it would be even nicer if you could get them to not give dates in this format!

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Rui Barradas
Sent: Wednesday, July 13, 2022 10:48 AM
To: Dr Eberhard Lisse <nospam at lisse.NA>; r-help at r-project.org
Subject: Re: [R] How to parse a really silly date with lubridate

Hello,

With lubridate, note in what sequence your datetime elements occur and use the appropriate function.


d <- c('9. Jul 2022 at 11:39', '10. Jul 2022 at 01:58')
lubridate::dmy_hm(d)
#> [1] "2022-07-09 11:39:00 UTC" "2022-07-10 01:58:00 UTC"


Hope this helps,

Rui Barradas

?s 14:40 de 13/07/2022, Dr Eberhard Lisse escreveu:
> 
> Hi,
> 
> I have data file which generated by an otherwise very nice (diabetes
> log) app, but exports dates really silly.
> 
> After reading the enclosed mwe.csv into R like so
> 
>       MWE <- read_delim('mwe.csv', delim = ';') %>%
>          select(Date) %>%
>          print()
> 
> 
> this comes out as:
> 
>       # A tibble: 2 ? 1
>      Date
>      <chr>
>       1 9. Jul 2022 at 11:39
>       2 10. Jul 2022 at 01:58
> 
> 
> No matter what I try I am not able to parse this inside R to get at
> proper dates (I have loaded tidyverse and lubridate).
> 
> I can easily do somethig
> 
>       csvq  -d ';' -t '%e. %b %Y at %H:%i' \
>          'SELECT Date as oridate,
>              DATETIME_FORMAT(Date, "%Y-%m-%d %H:%m") AS date
>          FROM mwe'
> 
>       +-----------------------+------------------+
>       |        oridate        |       date       |
>       +-----------------------+------------------+
>       | 9. Jul 2022 at 11:39  | 2022-07-09 11:07 |
>       | 10. Jul 2022 at 01:58 | 2022-07-10 01:07 |
>       +-----------------------+------------------+
> 
> and hence could easily do something like
> 
>       csvq  -d ';' -t '%e. %b %Y at %H:%i' \
>        'ALTER mwe
>        SET Date = DATETIME_FORMAT(Date, "%Y-%m-%d %H:%m")'
> 
> but would rather like to be able to do it inside R and would therefor
> appreciate any advice in this regard.
> 
> 
> greetings, el
>

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From @k@h@y_e4 @end|ng |rom hotm@||@com  Wed Jul 13 21:19:58 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Wed, 13 Jul 2022 19:19:58 +0000
Subject: [R] aborting the execution of a function...
Message-ID: <PU4P216MB15682B367ABD8E231ECAA5A3C8899@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear members,
                         I am running a large scraping code in a very powerful AWS ec2 instance:

DATES <- getFirms Dates()

It iterates over 500 stocks from a website. Despite the power of the machine, the execution is very slow.

If I abort the function (by ctrl + C), after, say 150th iteration, the DATES object will still contain the scraped data untill the 150th iteration, right? ( The rest of the 350 entries will be NA's, I suppose).

Many thanks in advance.....

Yours sincerely,
AKSHAY M KULKARNI



	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Jul 13 21:48:40 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 13 Jul 2022 12:48:40 -0700
Subject: [R] aborting the execution of a function...
In-Reply-To: <PU4P216MB15682B367ABD8E231ECAA5A3C8899@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <PU4P216MB15682B367ABD8E231ECAA5A3C8899@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <A5DEF48F-C65C-4A09-9A7B-DB48BCF2C168@dcn.davis.ca.us>

This would be easy for you to test on a small example on your local computer.

But the answer is "no". Nothing is assigned if the function does not return normally... and Ctrl+C is anything but normal.

On July 13, 2022 12:19:58 PM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>Dear members,
>                         I am running a large scraping code in a very powerful AWS ec2 instance:
>
>DATES <- getFirms Dates()
>
>It iterates over 500 stocks from a website. Despite the power of the machine, the execution is very slow.
>
>If I abort the function (by ctrl + C), after, say 150th iteration, the DATES object will still contain the scraped data untill the 150th iteration, right? ( The rest of the 350 entries will be NA's, I suppose).
>
>Many thanks in advance.....
>
>Yours sincerely,
>AKSHAY M KULKARNI
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From w||||@mwdun|@p @end|ng |rom gm@||@com  Wed Jul 13 21:58:45 2022
From: w||||@mwdun|@p @end|ng |rom gm@||@com (Bill Dunlap)
Date: Wed, 13 Jul 2022 12:58:45 -0700
Subject: [R] aborting the execution of a function...
In-Reply-To: <PU4P216MB15682B367ABD8E231ECAA5A3C8899@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <PU4P216MB15682B367ABD8E231ECAA5A3C8899@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <CAHqSRuQoLdy6QtG=WFRYj-SBhhdRN9qu42b5ESnfYJz3EFzK2w@mail.gmail.com>

You could write a function that returns an environment (or list if you
prefer) containing the results collected before the interrupt by using
tryCatch(interrupt=...).  E.g.,

doMany <- function(names) {
    resultEnv <- new.env(parent=emptyenv())
    tryCatch(
        for(name in names) resultEnv[[name]] <- Sys.sleep(1), # replace
Sys.sleep(1) by getStuffFromWeb(name)
        interrupt = function(e) NULL)
    resultEnv
}

Use it as

> system.time(e <- doMany(state.name)) # hit Esc or ^C after a few seconds
^C   user  system elapsed
  0.001   0.000   4.390
> names(e)
[1] "Alabama"  "Alaska"   "Arizona"  "Arkansas"
> eapply(e, identity)
$Alabama
NULL

$Alaska
NULL

$Arizona
NULL

$Arkansas
NULL

-Bill

On Wed, Jul 13, 2022 at 12:20 PM akshay kulkarni <akshay_e4 at hotmail.com>
wrote:

> Dear members,
>                          I am running a large scraping code in a very
> powerful AWS ec2 instance:
>
> DATES <- getFirms Dates()
>
> It iterates over 500 stocks from a website. Despite the power of the
> machine, the execution is very slow.
>
> If I abort the function (by ctrl + C), after, say 150th iteration, the
> DATES object will still contain the scraped data untill the 150th
> iteration, right? ( The rest of the 350 entries will be NA's, I suppose).
>
> Many thanks in advance.....
>
> Yours sincerely,
> AKSHAY M KULKARNI
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ccont|ngency @end|ng |rom gm@||@com  Wed Jul 13 17:49:56 2022
From: ccont|ngency @end|ng |rom gm@||@com (core_contingency)
Date: Wed, 13 Jul 2022 08:49:56 -0700
Subject: [R] Does the function "c" have a character limit?
In-Reply-To: <320a723b-06e5-575b-db12-00b1a8856b29@sapo.pt>
References: <61a4af67-18a8-f363-8068-54fd1bc4d53d@gmail.com>
 <75c7fbeb-8aa1-ab91-b164-11c2a18e176d@sapo.pt>
 <BN6PR2201MB155355656C6FD9DC0BD729C1CF899@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CABcYAdKRhzPSBxa+DgBuYVT78JQFcK3Tm68+tPLcKFZPrfCkwg@mail.gmail.com>
 <320a723b-06e5-575b-db12-00b1a8856b29@sapo.pt>
Message-ID: <ab74e7b0-aa36-05af-18ea-7996b017349b@gmail.com>

Thank you everyone for the help. It is interesting that R sets a 
character limit on each line, but I am glad the solution is simple.

core_contingency

On 7/13/22 07:42, Rui Barradas wrote:
> Hello,
>
> The original command has spaces in it. If they are removed its total 
> length goes down to 3649 bytes (or characters, in this case it's the 
> same). With this length the command is expected to work without 
> further worries. And it does, I have just tried it.
>
>
> MES=c("A2M","ABRACL","ACADVL","ACAP2","ACTA2","ACTN1","ADAM19","ADAM9","ADAMTS5","ADGRE5","ADGRG6","AEBP1","AJUBA","ALDH1A3","AMMECR1","ANTXR1","ANXA1","ANXA2","ANXA5","ANXA6","APOE","APP","ARHGAP1","ARHGEF40","ARL1","ARL4A","ARMCX2","ARPC1B","ASPH","ATP10D","ATP1B1","ATP2B1","ATP2B4","ATP6V0E1","ATP8B2","ATXN1","B2M","BAG3","BGN","BMP5","BNC2","BOC","BTN3A2","C1orf198","C1orf54","C4orf32","C6orf120","CALD1","CALU","CAPN2","CAPN6","CBFB","CBLB","CCDC80","CD164","CD44","CD59","CD63","CDH11","CETN2","CFH","CFI","CILP","CKAP4","CLIC4","CMTM3","CMTM6","CNN3","COL11A1","COL12A1","COL1A1","COL27A1","COL3A1","COL4A1","COL4A2","COL5A1","COL5A2","COL6A1","COL6A2","COL6A3","COPA","CPED1","CPS1","CRABP2","CREB3L2","CREG1","CRELD2","CRISPLD1","CRTAP","CSRP1","CTDSP2","CTNNA1","CTSB","CTSC","CTSO","CXCL12","CYBRD1","CYFIP1","CYP26A1","CYR61","DCAF6","DDOST","DDR2","DESI2","DKK3","DLC1","DLX1","DLX2","DMD","DNAJC1","DNAJC10","DNAJC3","DNM3OS","DPY19L1","DSE","DUSP14","DUSP5","DUSP6","EDEM1","EDNRA","EFEMP2","EGFR","EGR1","EGR3","EHD2","ELAVL1","ELF1","ELK3","ELK4","EMILIN1","EMP1","ENAH","EPHA3","EPS8","ERBIN","ERLIN1","ERRFI1","ETS1","EVA1A","EXT1","EXTL2","F2R","F2RL2","FAM102B","FAM114A1","FAM120A","FAM129A","FAM3C","FAM43A","FAM46A","FAT1","FBN1","FBN2","FGFR1","FIBIN","FILIP1L","FKBP14","FLNA","FLRT2","FMOD","FN1","FNDC3B","FSTL1","FUCA2","FZD1","FZD2","FZD7","GABRR1","GALNT10","GAS1","GAS2","GDF15","GJA1","GNAI1","GNG12","GNS","GORAB","GPC6","GPR137B","GPX8","GRN","GSN","HES1","HEXB","HIBADH","HIPK3","HIST1H2AC","HIST1H2BK","HLA-A","HLA-B","HLA-C","HLA-F","HLX","HNMT","HOMER1","HS3ST3A1","HSP90B1","HSPA5","HSPB1","HTRA1","HYOU1","ID1","ID3","IFI16","IFITM2","IFITM3","IGF2R","IGFBP5","IGFBP6","IL13RA1","IL6ST","INSIG1","IQGAP2","ITGA10","ITGA4","ITGAV","ITGB1","ITM2B","ITM2C","ITPR1","ITPRIPL2","JAK1","JAM3","KANK2","KCNK2","KCTD12","KDELC2","KDELR2","KDELR3","KDM5B","KIAA1462","KIF13A","KIRREL","KLF10","KLF4","KLF6","L3HYPDH","LAMB1","LAMC1","LAMP1","LAPTM4A","LASP1","LATS2","LEPROT","LGALS1","LHFP","LHX8","LIFR","LIPA","LITAF","LIX1L","LMAN1","LMNA","LOXL2","LPP","LRP10","LRRC17","LRRC8C","LTBP1","LUZP1","MAGT1","MAML2","MAN2A1","MANF","MBD2","MBNL1","MBTPS1","MEOX1","MEOX2","MEST","MGAT2","MGP","MGST1","MICAL2","MMP2","MOB1A","MRC2","MXRA5","MYADM","MYDGF","MYL12A","MYL12B","MYLIP","NANS","NBR1","NEK7","NES","NFIA","NFIC","NID1","NID2","NOTCH2","NOTCH2NL","NPC2","NPTN","NQO1","NR3C1","NRP1","OGFRL1","OLFML2A","OLFML2B","OLFML3","OSTC","P4HA1","PALLD","PAPSS2","PCDH18","PCOLCE2","PCSK5","PDE3A","PDE7B","PDGFC","PDIA3","PDIA4","PDIA6","PDLIM1","PEA15","PEAK1","PHLDA3","PHLDB2","PHTF2","PIAS3","PLAGL1","PLEKHA2","PLEKHH2","PLK2","PLOD2","PLOD3","PLPP1","PLS3","PLSCR1","PLSCR4","PLXDC2","POLR2L","PON2","POSTN","PPIB","PPIC","PPT1","PRCP","PRDM6","PRDX4","PRDX6","PROM1","PRRX1","PTBP1","PTGER4","PTGFRN","PTN","PTPN14","PTPRG","PTPRK","PTRF","PXDC1","PXDN","PYGL","QKI","QSOX1","RAB13","RAB29","RAB31","RAP1A","RAP1B","RBMS1","RCN1","RECK","REST","RGL1","RGS10","RGS3","RHOC","RHOJ","RIN2","RIT1","RNFT1","RNH1","ROBO1","ROR1","RRBP1","S1PR3","SASH1","SCPEP1","SCRG1","SDC2","SDC4","SDCBP","SDF4","SEC14L1","SEL1L3","SEMA3C","SEMA3F","SEPT10","SERPINE2","SERPINH1","SFT2D1","SFT2D2","SGK1","SH3BGRL","SHC1","SHROOM3","SIX1","SIX4","SKIL","SLC16A4","SLC30A1","SLC30A7","SLC35F5","SLC38A2","SLC38A6","SLC39A14","SMAD3","SNAI2","SNAP23","SOSTDC1","SOX9","SPARC","SPARCL1","SPATA20","SPCS3","SPRED1","SPRY1","SPRY4","SPRY4-IT1","SQSTM1","SRPX","SSBP4","SSR1","SSR3","STAT1","STAT3","STEAP1","STK38L","SUCLG2","SURF4","SVIL","SYDE1","SYNJ2","SYPL1","TCF7L2","TFE3","TFPI","TGFB1I1","TGFBR2","THBS1","TIMP1","TJP1") 
>
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 14:50 de 13/07/2022, Richard O'Keefe escreveu:
>> Breaking up the *line* doesn't mean breaking up the *command*.
>> For example,
>> x <- c(
>> ???? "FOOBAR", # 1
>> ???? ...
>> ???? "FOOBAR", # 4999
>> ???? "UGGLE")
>> works fine, with source(..), with "R -f ...", and other ways.
>> Each *line* is short, but it's still one *command*.
>>
>> I'd probably put that much data in a file, myself,
>> but R doesn't mind.
>>
>>
>> On Thu, 14 Jul 2022 at 01:21, Ebert,Timothy Aaron <tebert at ufl.edu 
>> <mailto:tebert at ufl.edu>> wrote:
>>
>> ??? The limits to the size of vectors, matrices, data frames, lists, or
>> ??? other data structure does not have a simple answer.
>> ??? 1) 2^31 - 1? is the maximum number of rows.
>> https://stackoverflow.com/questions/5233769/practical-limits-of-r-data-frame#:~:text=The%20number%20is%202%5E31,start%20collecting%20several%20of%20them
>> <https://stackoverflow.com/questions/5233769/practical-limits-of-r-data-frame#:~:text=The%20number%20is%202%5E31,start%20collecting%20several%20of%20them>.
>> ??? 2) help(Memory) suggests that the default limit for all variables is
>> ??? 6 Mb. The help page tells you how to change this.
>> ??? Neither of these two factors have any bearing on this problem except
>> ??? that your vector is not close to these limits.
>>
>> ??? I got the same result you did when I entered your vector into my
>> ??? system (R 4.2 in RStudio, on 64 bit Windows). I shortened it by
>> ??? removing the first entry and it works.
>>
>> ??? I can copy the entire line into Microsoft Word, and count the number
>> ??? of characters (including spaces) and I get 4089. There were seven
>> ??? characters in the first entry including the comma and space. If I
>> ??? add seven spaces between MES and the equal sign I get the original
>> ??? outcome. So the limit is on the number of characters in the line.
>> ??? You can get more entries by shortening each entry, or fewer if each
>> ??? entry was longer.
>>
>> ??? As others have suggested, I would break the line into two pieces and
>> ??? then combine the pieces.
>>
>> ??? Tim
>>
>>
>>
>> ??? -----Original Message-----
>> ??? From: R-help <r-help-bounces at r-project.org
>> ??? <mailto:r-help-bounces at r-project.org>> On Behalf Of Rui Barradas
>> ??? Sent: Wednesday, July 13, 2022 6:36 AM
>> ??? To: core_contingency <ccontingency at gmail.com
>> ??? <mailto:ccontingency at gmail.com>>; r-help at r-project.org
>> ??? <mailto:r-help at r-project.org>
>> ??? Subject: Re: [R] Does the function "c" have a character limit?
>>
>> ??? [External Email]
>>
>> ??? Hello,
>>
>> ??? This is documented behavior.
>> ???? ?From R-intro, last line of section 1.8 [1].
>>
>>
>> ??? Command lines entered at the console are limited4 to about 4095
>> ??? bytes (not characters).
>>
>>
>> ??? The number 4 in limited4 is a footnote link:
>>
>>
>> ??? some of the consoles will not allow you to enter more, and amongst
>> ??? those which do some will silently discard the excess and some will
>> ??? use it as the start of the next line.
>>
>>
>>
>> ??? Prof. Ripley called the r-devel mailing list's attention to this in
>> ??? August 2006 when the limit was 1024 [2], it was then increased to
>> ??? the current 4095. I remember seeing a limit of 2048 (?) but couldn't
>> ??? find where.
>>
>>
>> ??? Try creating a file with your command as only content, then run
>>
>>
>> ??? x <- readLines("rhelp.txt")
>> ??? nchar(x)
>> ??? # [1] 4096
>>
>>
>> ??? You are above the limit by 1 byte.
>> ??? Standard solutions are to break the command line, in your case into
>> ??? at least 2 lines, or to source the command from file, like David
>> ??? proposed.
>>
>>
>> ??? [1]
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dproject.org_doc_manuals_R-2Dintro.html-23R-2Dcommands-5F003b-2Dcase-2Dsensitivity-2Detc&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=g5cbrpfyHaIN9sXycd_-f2iDsOcbuzLe2u3KjvQNm-0&e=
>> <https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dproject.org_doc_manuals_R-2Dintro.html-23R-2Dcommands-5F003b-2Dcase-2Dsensitivity-2Detc&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=g5cbrpfyHaIN9sXycd_-f2iDsOcbuzLe2u3KjvQNm-0&e=>
>> ??? [2]
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_pipermail_r-2Ddevel_2006-2DAugust_038985.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=B1YSca31vHlpy9WJG8o0MBTh7bX4v7M61eWfQ4tmdog&e=
>> <https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_pipermail_r-2Ddevel_2006-2DAugust_038985.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=B1YSca31vHlpy9WJG8o0MBTh7bX4v7M61eWfQ4tmdog&e=>
>>
>>
>> ??? Hope this helps,
>>
>> ??? Rui Barradas
>>
>>
>> ??? ?s 00:36 de 13/07/2022, core_contingency escreveu:
>> ???? > To Whom it May Concern,
>> ???? >
>> ???? > I am creating a vector with the base R function "c", with many
>> ???? > arguments as shown below:
>> ???? >
>> ???? >? ? ? ?$ R
>> ???? >? ? ? ?> MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2",
>> ??? "ACTN1",
>> ???? > "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", 
>> "AJUBA",
>> ???? > "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", 
>> "ANXA6",
>> ???? > "APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", "ARMCX2",
>> ???? > "ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4",
>> ??? "ATP6V0E1",
>> ???? > "ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", "BOC",
>> ???? > "BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", "CALD1",
>> ???? > "CALU", "CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", 
>> "CD44",
>> ???? > "CD59", "CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", "CKAP4",
>> ???? > "CLIC4", "CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", 
>> "COL1A1",
>> ???? > "COL27A1", "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2",
>> ??? "COL6A1",
>> ???? > "COL6A2", "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2",
>> ???? > "CREG1", "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", 
>> "CTNNA1",
>> ???? > "CTSB", "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", "CYP26A1",
>> ???? > "CYR61", "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", 
>> "DLX1",
>> ???? > "DLX2", "DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", 
>> "DPY19L1",
>> ???? > "DSE", "DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2",
>> ??? "EGFR",
>> ???? > "EGR1", "EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", 
>> "EMILIN1",
>> ???? > "EMP1", "ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1",
>> ??? "ETS1",
>> ???? > "EVA1A", "EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1",
>> ???? > "FAM120A", "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", 
>> "FBN1",
>> ???? > "FBN2", "FGFR1", "FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2",
>> ???? > "FMOD", "FN1", "FNDC3B", "FSTL1", "FUCA2", "FZD1", "FZD2", 
>> "FZD7",
>> ???? > "GABRR1", "GALNT10", "GAS1", "GAS2", "GDF15", "GJA1", "GNAI1",
>> ???? > "GNG12", "GNS", "GORAB", "GPC6", "GPR137B", "GPX8", "GRN", "GSN",
>> ???? > "HES1", "HEXB", "HIBADH", "HIPK3", "HIST1H2AC", "HIST1H2BK",
>> ??? "HLA-A",
>> ???? > "HLA-B", "HLA-C", "HLA-F", "HLX", "HNMT", "HOMER1", "HS3ST3A1",
>> ???? > "HSP90B1", "HSPA5", "HSPB1", "HTRA1", "HYOU1", "ID1", "ID3",
>> ??? "IFI16",
>> ???? > "IFITM2", "IFITM3", "IGF2R", "IGFBP5", "IGFBP6", "IL13RA1", 
>> "IL6ST",
>> ???? > "INSIG1", "IQGAP2", "ITGA10", "ITGA4", "ITGAV", "ITGB1", "ITM2B",
>> ???? > "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", "KANK2", "KCNK2",
>> ???? > "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", "KIAA1462",
>> ??? "KIF13A",
>> ???? > "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", "LAMB1", "LAMC1",
>> ???? > "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", "LGALS1", "LHFP",
>> ???? > "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", "LMNA", 
>> "LOXL2",
>> ???? > "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", "MAGT1",
>> ???? > "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1",
>> ???? > "MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", 
>> "MOB1A",
>> ???? > "MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP",
>> ???? > "NANS", "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2",
>> ???? > "NOTCH2", "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1",
>> ???? > "OGFRL1", "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", 
>> "PALLD",
>> ???? > "PAPSS2", "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", 
>> "PDGFC",
>> ???? > "PDIA3", "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", "PHLDA3",
>> ???? > "PHLDB2", "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", 
>> "PLK2",
>> ???? > "PLOD2", "PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", "PLXDC2",
>> ???? > "POLR2L", "PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", 
>> "PRDM6",
>> ???? > "PRDX4", "PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", "PTGFRN",
>> ???? > "PTN", "PTPN14", "PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", 
>> "PYGL",
>> ???? > "QKI", "QSOX1", "RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B",
>> ??? "RBMS1",
>> ???? > "RCN1", "RECK", "REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ",
>> ???? > "RIN2", "RIT1", "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", 
>> "S1PR3",
>> ???? > "SASH1", "SCPEP1", "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4",
>> ???? > "SEC14L1", "SEL1L3", "SEMA3C", "SEMA3F", "SEPT10", "SERPINE2",
>> ???? > "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", "SH3BGRL", "SHC1",
>> ??? "SHROOM3",
>> ???? > "SIX1", "SIX4", "SKIL", "SLC16A4", "SLC30A1", "SLC30A7", 
>> "SLC35F5",
>> ???? > "SLC38A2", "SLC38A6", "SLC39A14", "SMAD3", "SNAI2", "SNAP23",
>> ???? > "SOSTDC1", "SOX9", "SPARC", "SPARCL1", "SPATA20", "SPCS3", 
>> "SPRED1",
>> ???? > "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", "SRPX", "SSBP4", "SSR1",
>> ???? > "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", "SUCLG2", "SURF4",
>> ??? "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", "TFPI",
>> ??? "TGFB1I1", "TGFBR2", "THBS1", "TIMP1", "TJP1")
>> ???? >? ? ? ?+
>> ???? >
>> ???? > For some reason, the R console does not display a ">" symbol,
>> ???? > indicating that it has completed the function, but displays a "+"
>> ???? > symbol instead, which indicates that the function is still
>> ??? waiting for more input.
>> ???? > However, I believe that my syntax is correct. If I shorten my
>> ??? command
>> ???? > by a few characters by removing the last entry, "TJP1":
>> ???? >
>> ???? >? ? ? ?$ R
>> ???? >? ? ? ?> MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2",
>> ??? "ACTN1",
>> ???? > "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", 
>> "AJUBA",
>> ???? > "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", 
>> "ANXA6",
>> ???? > "APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", "ARMCX2",
>> ???? > "ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4",
>> ??? "ATP6V0E1",
>> ???? > "ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", "BOC",
>> ???? > "BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", "CALD1",
>> ???? > "CALU", "CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", 
>> "CD44",
>> ???? > "CD59", "CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", "CKAP4",
>> ???? > "CLIC4", "CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", 
>> "COL1A1",
>> ???? > "COL27A1", "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2",
>> ??? "COL6A1",
>> ???? > "COL6A2", "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", "CREB3L2",
>> ???? > "CREG1", "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", 
>> "CTNNA1",
>> ???? > "CTSB", "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", "CYP26A1",
>> ???? > "CYR61", "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", 
>> "DLX1",
>> ???? > "DLX2", "DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", 
>> "DPY19L1",
>> ???? > "DSE", "DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2",
>> ??? "EGFR",
>> ???? > "EGR1", "EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", 
>> "EMILIN1",
>> ???? > "EMP1", "ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1",
>> ??? "ETS1",
>> ???? > "EVA1A", "EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", "FAM114A1",
>> ???? > "FAM120A", "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", 
>> "FBN1",
>> ???? > "FBN2", "FGFR1", "FIBIN", "FILIP1L", "FKBP14", "FLNA", "FLRT2",
>> ???? > "FMOD", "FN1", "FNDC3B", "FSTL1", "FUCA2", "FZD1", "FZD2", 
>> "FZD7",
>> ???? > "GABRR1", "GALNT10", "GAS1", "GAS2", "GDF15", "GJA1", "GNAI1",
>> ???? > "GNG12", "GNS", "GORAB", "GPC6", "GPR137B", "GPX8", "GRN", "GSN",
>> ???? > "HES1", "HEXB", "HIBADH", "HIPK3", "HIST1H2AC", "HIST1H2BK",
>> ??? "HLA-A",
>> ???? > "HLA-B", "HLA-C", "HLA-F", "HLX", "HNMT", "HOMER1", "HS3ST3A1",
>> ???? > "HSP90B1", "HSPA5", "HSPB1", "HTRA1", "HYOU1", "ID1", "ID3",
>> ??? "IFI16",
>> ???? > "IFITM2", "IFITM3", "IGF2R", "IGFBP5", "IGFBP6", "IL13RA1", 
>> "IL6ST",
>> ???? > "INSIG1", "IQGAP2", "ITGA10", "ITGA4", "ITGAV", "ITGB1", "ITM2B",
>> ???? > "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", "KANK2", "KCNK2",
>> ???? > "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", "KIAA1462",
>> ??? "KIF13A",
>> ???? > "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", "LAMB1", "LAMC1",
>> ???? > "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", "LGALS1", "LHFP",
>> ???? > "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", "LMNA", 
>> "LOXL2",
>> ???? > "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", "MAGT1",
>> ???? > "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1",
>> ???? > "MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", 
>> "MOB1A",
>> ???? > "MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", "MYLIP",
>> ???? > "NANS", "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2",
>> ???? > "NOTCH2", "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1",
>> ???? > "OGFRL1", "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", 
>> "PALLD",
>> ???? > "PAPSS2", "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", 
>> "PDGFC",
>> ???? > "PDIA3", "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", "PHLDA3",
>> ???? > "PHLDB2", "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", 
>> "PLK2",
>> ???? > "PLOD2", "PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", "PLXDC2",
>> ???? > "POLR2L", "PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", 
>> "PRDM6",
>> ???? > "PRDX4", "PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", "PTGFRN",
>> ???? > "PTN", "PTPN14", "PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", 
>> "PYGL",
>> ???? > "QKI", "QSOX1", "RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B",
>> ??? "RBMS1",
>> ???? > "RCN1", "RECK", "REST", "RGL1", "RGS10", "RGS3", "RHOC", "RHOJ",
>> ???? > "RIN2", "RIT1", "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", 
>> "S1PR3",
>> ???? > "SASH1", "SCPEP1", "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4",
>> ???? > "SEC14L1", "SEL1L3", "SEMA3C", "SEMA3F", "SEPT10", "SERPINE2",
>> ???? > "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", "SH3BGRL", "SHC1",
>> ??? "SHROOM3",
>> ???? > "SIX1", "SIX4", "SKIL", "SLC16A4", "SLC30A1", "SLC30A7", 
>> "SLC35F5",
>> ???? > "SLC38A2", "SLC38A6", "SLC39A14", "SMAD3", "SNAI2", "SNAP23",
>> ???? > "SOSTDC1", "SOX9", "SPARC", "SPARCL1", "SPATA20", "SPCS3", 
>> "SPRED1",
>> ???? > "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", "SRPX", "SSBP4", "SSR1",
>> ???? > "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", "SUCLG2", "SURF4",
>> ??? "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", "TFPI",
>> ??? "TGFB1I1", "TGFBR2", "THBS1", "TIMP1")
>> ???? >? ? ? ?>
>> ???? >
>> ???? > The function now works, and returns a ">" symbol, indicating that
>> ??? the
>> ???? > function completed successfully. The ls() function proves it:
>> ???? >
>> ???? >? ? ? ?$ R
>> ???? >? ? ? ?> ls()
>> ???? >? ? ? ?[1] "MES"
>> ???? >
>> ???? > Is this a bug in the base R "c" function? It seems like the "c"
>> ???? > function can only accept so many characters before it fails.
>> ???? >
>> ???? > Thank you for your time,
>> ???? > core_contingency
>> ???? >
>> ???? >
>> ???? > ______________________________________________
>> ???? > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>> ??? -- To UNSUBSCRIBE and more, see
>> ???? >
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
>> <https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail>
>> ???? >
>> man_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
>> ???? >
>> Rzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD
>> ???? > 2XzMc9x&s=x5SJbPFsoqRiJYh7Y5B0QDKio2Wy4Je38lBBi99AbAE&e=
>> ???? > PLEASE do read the posting guide
>> ???? >
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
>> <https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or>
>> ???? >
>> g_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
>> ???? >
>> sRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0C
>> ???? > D2XzMc9x&s=lCG5D1ItMs8G_wkshvm4nBaVq4Ehy_zyq5mnp2zpt2Y&e=
>> ???? > and provide commented, minimal, self-contained, reproducible 
>> code.
>>
>> ??? ______________________________________________
>> ??? R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>> ??? To UNSUBSCRIBE and more, see
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=x5SJbPFsoqRiJYh7Y5B0QDKio2Wy4Je38lBBi99AbAE&e=
>> <https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=x5SJbPFsoqRiJYh7Y5B0QDKio2Wy4Je38lBBi99AbAE&e=>
>> ??? PLEASE do read the posting guide
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=lCG5D1ItMs8G_wkshvm4nBaVq4Ehy_zyq5mnp2zpt2Y&e=
>> <https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=lCG5D1ItMs8G_wkshvm4nBaVq4Ehy_zyq5mnp2zpt2Y&e=>
>> ??? and provide commented, minimal, self-contained, reproducible code.
>> ??? ______________________________________________
>> ??? R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>> ??? To UNSUBSCRIBE and more, see
>> ??? https://stat.ethz.ch/mailman/listinfo/r-help
>> ??? <https://stat.ethz.ch/mailman/listinfo/r-help>
>> ??? PLEASE do read the posting guide
>> ??? http://www.R-project.org/posting-guide.html
>> ??? <http://www.R-project.org/posting-guide.html>
>> ??? and provide commented, minimal, self-contained, reproducible code.
>>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: OpenPGP_signature
Type: application/pgp-signature
Size: 236 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220713/5f055195/attachment.sig>

From no@p@m @end|ng |rom ||@@e@NA  Wed Jul 13 18:14:19 2022
From: no@p@m @end|ng |rom ||@@e@NA (Dr Eberhard W Lisse)
Date: Wed, 13 Jul 2022 18:14:19 +0200
Subject: [R] How to parse a really silly date with lubridate
In-Reply-To: <c166bf12-124e-0477-8694-4bfa73399447@sapo.pt>
References: <tami12$11b8$1@ciao.gmane.io>
 <c166bf12-124e-0477-8694-4bfa73399447@sapo.pt>
Message-ID: <tamr12$14qr$1@ciao.gmane.io>


Hi,

while all of the below work in a character vector, none works in
the tibble.

The following

	DDATA %>%
		add_column(as.tibble(lubridate::dmy_hm(DDATA$Date)),
			.before = "Period") %>%
		rename(NewDate=value) %>%
		select(Date,NewDate) %>%
		filter(between(as.Date(NewDate),as.Date('2022-07-09'),
			as.Date('2022-07-10')))

does work

	# A tibble: 3 ? 2
	  Date                  NewDate
	  <chr>                 <dttm>
	1 9. Jul 2022 at 11:39  2022-07-09 11:39:00
	2 10. Jul 2022 at 01:58 2022-07-10 01:58:00
	3 10. Jul 2022 at 11:26 2022-07-10 11:26:00

but I wonder if that can not be done more elegantly, ie by direct
replacements in the column.

greetings, el

On 2022-07-13 16:48 , Rui Barradas wrote:
[...]
 > d <- c('9. Jul 2022 at 11:39', '10. Jul 2022 at 01:58')
 > lubridate::dmy_hm(d)
 > #> [1] "2022-07-09 11:39:00 UTC" "2022-07-10 01:58:00 UTC"
[...]

On 2022-07-13 16:03 , Ben Tupper wrote:
[...]
 > s = c("9. Jul 2022 at 11:39", "10. Jul 2022 at 01:58")
 > as.POSIXct(s, format = "%d. %b %Y at %H:%M")
 > as.POSIXct(s, format = "%d. %b %Y at %H:%M", tz = "UTC")
[...]

On 2022-07-13 15:52 , Ivan Krylov wrote:
[...]
 > Sys.setlocale('LC_TIME', 'C')
 > strptime(
 >   c('9. Jul 2022 at 11:39', '10. Jul 2022 at 01:58'),
 >   '%d. %b %Y at %H:%M'
 > )
[...]


 > ?s 14:40 de 13/07/2022, Dr Eberhard Lisse escreveu:
 >>
 >> Hi,
 >>
 >> I have data file which generated by an otherwise very nice (diabetes
 >> log) app, but exports dates really silly.
 >>
 >> After reading the enclosed mwe.csv into R like so
 >>
 >>       MWE <- read_delim('mwe.csv', delim = ';') %>%
 >>          select(Date) %>%
 >>          print()
 >>
 >>
 >> this comes out as:
 >>
 >>       # A tibble: 2 ? 1
 >>      Date
 >>      <chr>
 >>       1 9. Jul 2022 at 11:39
 >>       2 10. Jul 2022 at 01:58
 >>
 >>
 >> No matter what I try I am not able to parse this inside R to get at
 >> proper dates (I have loaded tidyverse and lubridate).
 >>
 >> I can easily do somethig
 >>
 >>       csvq  -d ';' -t '%e. %b %Y at %H:%i' \
 >>          'SELECT Date as oridate,
 >>              DATETIME_FORMAT(Date, "%Y-%m-%d %H:%m") AS date
 >>          FROM mwe'
 >>
 >>       +-----------------------+------------------+
 >>       |        oridate        |       date       |
 >>       +-----------------------+------------------+
 >>       | 9. Jul 2022 at 11:39  | 2022-07-09 11:07 |
 >>       | 10. Jul 2022 at 01:58 | 2022-07-10 01:07 |
 >>       +-----------------------+------------------+
 >>
 >> and hence could easily do something like
 >>
 >>       csvq  -d ';' -t '%e. %b %Y at %H:%i' \
 >>        'ALTER mwe
 >>        SET Date = DATETIME_FORMAT(Date, "%Y-%m-%d %H:%m")'
 >>
 >> but would rather like to be able to do it inside R and would therefor
 >> appreciate any advice in this regard.
 >>
 >>
 >> greetings, el
 >>
 >


From @vi@e@gross m@iii@g oii gm@ii@com  Wed Jul 13 22:09:02 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Wed, 13 Jul 2022 16:09:02 -0400
Subject: [R] aborting the execution of a function...
In-Reply-To: <A5DEF48F-C65C-4A09-9A7B-DB48BCF2C168@dcn.davis.ca.us>
References: <PU4P216MB15682B367ABD8E231ECAA5A3C8899@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <A5DEF48F-C65C-4A09-9A7B-DB48BCF2C168@dcn.davis.ca.us>
Message-ID: <01ac01d896f4$66077120$32165360$@gmail.com>

Jeff & Akshay,

What you say is true if you just call a function and do not arrange to
handle various interrupts and errors.

Obviously if you find a way to gracefully handle an error then you can opt
to have your work so far saved. For what is meant to be a fatal error,
though, you are expected to GIVE UP or at least exit rapidly after doing
something graceful. 

I am not clear what machine the user is using and their R setup. It may be
you can find something like a try() or suspendInterrupts method to catch and
handle control-C but may I offer a DIFFERENT solution?

You can identify your R process and send it some other milder signal that
can easily be caught or change your loop to slow it down a bit more.

For example, change your code so it opens a file and writes data to it in
some format like one result per line or a CSV format or whatever makes you
happy. Your loop adds new lines/items to the file and perhaps even closes
the file on interrupt or whatever works. Or it writes the results to the
console where you can copy/paste from if not too long.

And, consider having long-running code periodically check something in the
environment looking for a signal. Say every hundredth iteration it checks
for the existence of a file called "STOP_IT.stupid" and if it sees it,
removes it and exits gracefully while preserving your results somehow or
whatever you need. No interrupt needed, just create an empty file or other
logical marker.

Another variant is to use some form of threading or subprocess that does the
work somewhat in the background but can get commands from the foreground
process as needed including a request to stop. Again, no horrible signals
that kill the program.

And note on some systems, a process can be halted and resumed, if the
problem is that it has run a long time and is using too many resources at a
time they are needed.



-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Jeff Newmiller
Sent: Wednesday, July 13, 2022 3:49 PM
To: r-help at r-project.org; akshay kulkarni <akshay_e4 at hotmail.com>; R help
Mailing list <r-help at r-project.org>
Subject: Re: [R] aborting the execution of a function...

This would be easy for you to test on a small example on your local
computer.

But the answer is "no". Nothing is assigned if the function does not return
normally... and Ctrl+C is anything but normal.

On July 13, 2022 12:19:58 PM PDT, akshay kulkarni <akshay_e4 at hotmail.com>
wrote:
>Dear members,
>                         I am running a large scraping code in a very
powerful AWS ec2 instance:
>
>DATES <- getFirms Dates()
>
>It iterates over 500 stocks from a website. Despite the power of the
machine, the execution is very slow.
>
>If I abort the function (by ctrl + C), after, say 150th iteration, the
DATES object will still contain the scraped data untill the 150th iteration,
right? ( The rest of the 350 entries will be NA's, I suppose).
>
>Many thanks in advance.....
>
>Yours sincerely,
>AKSHAY M KULKARNI
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ce@g|guere @end|ng |rom gm@||@com  Wed Jul 13 22:19:59 2022
From: ce@g|guere @end|ng |rom gm@||@com (=?utf-8?Q?Charles-=C3=89douard_Gigu=C3=A8re?=)
Date: Wed, 13 Jul 2022 16:19:59 -0400
Subject: [R] Bug in packages.
Message-ID: <006b01d896f5$ed7ff6d0$c87fe470$@gmail.com>

Hello everyone,
Is there a mechanism to report a bug in someone package? I plan to email the author, but I was wondering if there is an official way like the issue function in github.
Thank you, 
Charles-?douard

Charles-?douard Gigu?re
Analyste statisticien 
Centre de recherche de l?Institut universitaire en sant? mentale de Montr?al (CR-IUSMM)
514?251-4015, poste 3516


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Jul 13 23:11:51 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 13 Jul 2022 14:11:51 -0700
Subject: [R] Bug in packages.
In-Reply-To: <006b01d896f5$ed7ff6d0$c87fe470$@gmail.com>
References: <006b01d896f5$ed7ff6d0$c87fe470$@gmail.com>
Message-ID: <8C4C6C01-8D0A-4CD3-B4B8-9381DC655F15@dcn.davis.ca.us>

Not all packages are developed on github, and CRAN itself only supports the maintainer function. However, sometimes the package DESCRIPTION file (shown on the CRAN package details page) will point you to a version control website such as GitHub for communicating with the maintainer.

On July 13, 2022 1:19:59 PM PDT, "Charles-?douard Gigu?re" <ce.giguere at gmail.com> wrote:
>Hello everyone,
>Is there a mechanism to report a bug in someone package? I plan to email the author, but I was wondering if there is an official way like the issue function in github.
>Thank you, 
>Charles-?douard
>
>Charles-?douard Gigu?re
>Analyste statisticien 
>Centre de recherche de l?Institut universitaire en sant? mentale de Montr?al (CR-IUSMM)
>514?251-4015, poste 3516
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @vi@e@gross m@iii@g oii gm@ii@com  Wed Jul 13 23:21:02 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Wed, 13 Jul 2022 17:21:02 -0400
Subject: [R] Does the function "c" have a character limit?
In-Reply-To: <ab74e7b0-aa36-05af-18ea-7996b017349b@gmail.com>
References: <61a4af67-18a8-f363-8068-54fd1bc4d53d@gmail.com>
 <75c7fbeb-8aa1-ab91-b164-11c2a18e176d@sapo.pt>
 <BN6PR2201MB155355656C6FD9DC0BD729C1CF899@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CABcYAdKRhzPSBxa+DgBuYVT78JQFcK3Tm68+tPLcKFZPrfCkwg@mail.gmail.com>
 <320a723b-06e5-575b-db12-00b1a8856b29@sapo.pt>
 <ab74e7b0-aa36-05af-18ea-7996b017349b@gmail.com>
Message-ID: <01be01d896fe$74e16de0$5ea449a0$@gmail.com>

To be clear, Everything has limits beyond which it is not expected to have to deal with. Buffers often pick a fixed size and often need complex code to keep grabbing a bigger size and copy and add more, or arrange various methods to link multiple memory areas into a growing whole, such as having an object with a list of smaller parts that are concatenated only when requested.

Many languages likely have similar constraints and one method hackers have used is to arrange to overflow such buffers carefully.

Have you verified if the normal stand-alone R interpreter accepts a longer one-liner and the cause may in RSTUDIO or whatever else you are using?

I will say the current design is flawed in that it does NOT give you an error, but since only some of the line is passed to the functionality then it is waiting for something that closes up the command such as a close parenthesis, or if it stops in middle of a string, also a close quote.

So, as we have said, just don't do that, albeit it is not necessarily easy to avoid when you have no idea the length of your data will be so high.


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of core_contingency
Sent: Wednesday, July 13, 2022 11:50 AM
To: r-help at r-project.org
Subject: Re: [R] Does the function "c" have a character limit?

Thank you everyone for the help. It is interesting that R sets a character limit on each line, but I am glad the solution is simple.

core_contingency

On 7/13/22 07:42, Rui Barradas wrote:
> Hello,
>
> The original command has spaces in it. If they are removed its total 
> length goes down to 3649 bytes (or characters, in this case it's the 
> same). With this length the command is expected to work without 
> further worries. And it does, I have just tried it.
>
>
> MES=c("A2M","ABRACL","ACADVL","ACAP2","ACTA2","ACTN1","ADAM19","ADAM9"
> ,"ADAMTS5","ADGRE5","ADGRG6","AEBP1","AJUBA","ALDH1A3","AMMECR1","ANTX
> R1","ANXA1","ANXA2","ANXA5","ANXA6","APOE","APP","ARHGAP1","ARHGEF40",
> "ARL1","ARL4A","ARMCX2","ARPC1B","ASPH","ATP10D","ATP1B1","ATP2B1","AT
> P2B4","ATP6V0E1","ATP8B2","ATXN1","B2M","BAG3","BGN","BMP5","BNC2","BO
> C","BTN3A2","C1orf198","C1orf54","C4orf32","C6orf120","CALD1","CALU","
> CAPN2","CAPN6","CBFB","CBLB","CCDC80","CD164","CD44","CD59","CD63","CD
> H11","CETN2","CFH","CFI","CILP","CKAP4","CLIC4","CMTM3","CMTM6","CNN3"
> ,"COL11A1","COL12A1","COL1A1","COL27A1","COL3A1","COL4A1","COL4A2","CO
> L5A1","COL5A2","COL6A1","COL6A2","COL6A3","COPA","CPED1","CPS1","CRABP
> 2","CREB3L2","CREG1","CRELD2","CRISPLD1","CRTAP","CSRP1","CTDSP2","CTN
> NA1","CTSB","CTSC","CTSO","CXCL12","CYBRD1","CYFIP1","CYP26A1","CYR61"
> ,"DCAF6","DDOST","DDR2","DESI2","DKK3","DLC1","DLX1","DLX2","DMD","DNA
> JC1","DNAJC10","DNAJC3","DNM3OS","DPY19L1","DSE","DUSP14","DUSP5","DUS
> P6","EDEM1","EDNRA","EFEMP2","EGFR","EGR1","EGR3","EHD2","ELAVL1","ELF
> 1","ELK3","ELK4","EMILIN1","EMP1","ENAH","EPHA3","EPS8","ERBIN","ERLIN
> 1","ERRFI1","ETS1","EVA1A","EXT1","EXTL2","F2R","F2RL2","FAM102B","FAM
> 114A1","FAM120A","FAM129A","FAM3C","FAM43A","FAM46A","FAT1","FBN1","FB
> N2","FGFR1","FIBIN","FILIP1L","FKBP14","FLNA","FLRT2","FMOD","FN1","FN
> DC3B","FSTL1","FUCA2","FZD1","FZD2","FZD7","GABRR1","GALNT10","GAS1","
> GAS2","GDF15","GJA1","GNAI1","GNG12","GNS","GORAB","GPC6","GPR137B","G
> PX8","GRN","GSN","HES1","HEXB","HIBADH","HIPK3","HIST1H2AC","HIST1H2BK
> ","HLA-A","HLA-B","HLA-C","HLA-F","HLX","HNMT","HOMER1","HS3ST3A1","HS
> P90B1","HSPA5","HSPB1","HTRA1","HYOU1","ID1","ID3","IFI16","IFITM2","I
> FITM3","IGF2R","IGFBP5","IGFBP6","IL13RA1","IL6ST","INSIG1","IQGAP2","
> ITGA10","ITGA4","ITGAV","ITGB1","ITM2B","ITM2C","ITPR1","ITPRIPL2","JA
> K1","JAM3","KANK2","KCNK2","KCTD12","KDELC2","KDELR2","KDELR3","KDM5B"
> ,"KIAA1462","KIF13A","KIRREL","KLF10","KLF4","KLF6","L3HYPDH","LAMB1",
> "LAMC1","LAMP1","LAPTM4A","LASP1","LATS2","LEPROT","LGALS1","LHFP","LH
> X8","LIFR","LIPA","LITAF","LIX1L","LMAN1","LMNA","LOXL2","LPP","LRP10"
> ,"LRRC17","LRRC8C","LTBP1","LUZP1","MAGT1","MAML2","MAN2A1","MANF","MB
> D2","MBNL1","MBTPS1","MEOX1","MEOX2","MEST","MGAT2","MGP","MGST1","MIC
> AL2","MMP2","MOB1A","MRC2","MXRA5","MYADM","MYDGF","MYL12A","MYL12B","
> MYLIP","NANS","NBR1","NEK7","NES","NFIA","NFIC","NID1","NID2","NOTCH2"
> ,"NOTCH2NL","NPC2","NPTN","NQO1","NR3C1","NRP1","OGFRL1","OLFML2A","OL
> FML2B","OLFML3","OSTC","P4HA1","PALLD","PAPSS2","PCDH18","PCOLCE2","PC
> SK5","PDE3A","PDE7B","PDGFC","PDIA3","PDIA4","PDIA6","PDLIM1","PEA15",
> "PEAK1","PHLDA3","PHLDB2","PHTF2","PIAS3","PLAGL1","PLEKHA2","PLEKHH2"
> ,"PLK2","PLOD2","PLOD3","PLPP1","PLS3","PLSCR1","PLSCR4","PLXDC2","POL
> R2L","PON2","POSTN","PPIB","PPIC","PPT1","PRCP","PRDM6","PRDX4","PRDX6
> ","PROM1","PRRX1","PTBP1","PTGER4","PTGFRN","PTN","PTPN14","PTPRG","PT
> PRK","PTRF","PXDC1","PXDN","PYGL","QKI","QSOX1","RAB13","RAB29","RAB31
> ","RAP1A","RAP1B","RBMS1","RCN1","RECK","REST","RGL1","RGS10","RGS3","
> RHOC","RHOJ","RIN2","RIT1","RNFT1","RNH1","ROBO1","ROR1","RRBP1","S1PR
> 3","SASH1","SCPEP1","SCRG1","SDC2","SDC4","SDCBP","SDF4","SEC14L1","SE
> L1L3","SEMA3C","SEMA3F","SEPT10","SERPINE2","SERPINH1","SFT2D1","SFT2D
> 2","SGK1","SH3BGRL","SHC1","SHROOM3","SIX1","SIX4","SKIL","SLC16A4","S
> LC30A1","SLC30A7","SLC35F5","SLC38A2","SLC38A6","SLC39A14","SMAD3","SN
> AI2","SNAP23","SOSTDC1","SOX9","SPARC","SPARCL1","SPATA20","SPCS3","SP
> RED1","SPRY1","SPRY4","SPRY4-IT1","SQSTM1","SRPX","SSBP4","SSR1","SSR3
> ","STAT1","STAT3","STEAP1","STK38L","SUCLG2","SURF4","SVIL","SYDE1","S
> YNJ2","SYPL1","TCF7L2","TFE3","TFPI","TGFB1I1","TGFBR2","THBS1","TIMP1
> ","TJP1")
>
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 14:50 de 13/07/2022, Richard O'Keefe escreveu:
>> Breaking up the *line* doesn't mean breaking up the *command*.
>> For example,
>> x <- c(
>>      "FOOBAR", # 1
>>      ...
>>      "FOOBAR", # 4999
>>      "UGGLE")
>> works fine, with source(..), with "R -f ...", and other ways.
>> Each *line* is short, but it's still one *command*.
>>
>> I'd probably put that much data in a file, myself, but R doesn't 
>> mind.
>>
>>
>> On Thu, 14 Jul 2022 at 01:21, Ebert,Timothy Aaron <tebert at ufl.edu 
>> <mailto:tebert at ufl.edu>> wrote:
>>
>>     The limits to the size of vectors, matrices, data frames, lists, 
>> or
>>     other data structure does not have a simple answer.
>>     1) 2^31 - 1  is the maximum number of rows.
>> https://stackoverflow.com/questions/5233769/practical-limits-of-r-dat
>> a-frame#:~:text=The%20number%20is%202%5E31,start%20collecting%20sever
>> al%20of%20them 
>> <https://stackoverflow.com/questions/5233769/practical-limits-of-r-data-frame#:~:text=The%20number%20is%202%5E31,start%20collecting%20several%20of%20them>.
>>     2) help(Memory) suggests that the default limit for all variables 
>> is
>>     6 Mb. The help page tells you how to change this.
>>     Neither of these two factors have any bearing on this problem 
>> except
>>     that your vector is not close to these limits.
>>
>>     I got the same result you did when I entered your vector into my
>>     system (R 4.2 in RStudio, on 64 bit Windows). I shortened it by
>>     removing the first entry and it works.
>>
>>     I can copy the entire line into Microsoft Word, and count the 
>> number
>>     of characters (including spaces) and I get 4089. There were seven
>>     characters in the first entry including the comma and space. If I
>>     add seven spaces between MES and the equal sign I get the 
>> original
>>     outcome. So the limit is on the number of characters in the line.
>>     You can get more entries by shortening each entry, or fewer if 
>> each
>>     entry was longer.
>>
>>     As others have suggested, I would break the line into two pieces 
>> and
>>     then combine the pieces.
>>
>>     Tim
>>
>>
>>
>>     -----Original Message-----
>>     From: R-help <r-help-bounces at r-project.org
>>     <mailto:r-help-bounces at r-project.org>> On Behalf Of Rui Barradas
>>     Sent: Wednesday, July 13, 2022 6:36 AM
>>     To: core_contingency <ccontingency at gmail.com
>>     <mailto:ccontingency at gmail.com>>; r-help at r-project.org
>>     <mailto:r-help at r-project.org>
>>     Subject: Re: [R] Does the function "c" have a character limit?
>>
>>     [External Email]
>>
>>     Hello,
>>
>>     This is documented behavior.
>>       From R-intro, last line of section 1.8 [1].
>>
>>
>>     Command lines entered at the console are limited4 to about 4095
>>     bytes (not characters).
>>
>>
>>     The number 4 in limited4 is a footnote link:
>>
>>
>>     some of the consoles will not allow you to enter more, and 
>> amongst
>>     those which do some will silently discard the excess and some 
>> will
>>     use it as the start of the next line.
>>
>>
>>
>>     Prof. Ripley called the r-devel mailing list's attention to this 
>> in
>>     August 2006 when the limit was 1024 [2], it was then increased to
>>     the current 4095. I remember seeing a limit of 2048 (?) but 
>> couldn't
>>     find where.
>>
>>
>>     Try creating a file with your command as only content, then run
>>
>>
>>     x <- readLines("rhelp.txt")
>>     nchar(x)
>>     # [1] 4096
>>
>>
>>     You are above the limit by 1 byte.
>>     Standard solutions are to break the command line, in your case 
>> into
>>     at least 2 lines, or to source the command from file, like David
>>     proposed.
>>
>>
>>     [1]
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dproject
>> .org_doc_manuals_R-2Dintro.html-23R-2Dcommands-5F003b-2Dcase-2Dsensit
>> ivity-2Detc&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-
>> g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&
>> s=g5cbrpfyHaIN9sXycd_-f2iDsOcbuzLe2u3KjvQNm-0&e=
>> <https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dprojec
>> t.org_doc_manuals_R-2Dintro.html-23R-2Dcommands-5F003b-2Dcase-2Dsensi
>> tivity-2Detc&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP
>> -g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x
>> &s=g5cbrpfyHaIN9sXycd_-f2iDsOcbuzLe2u3KjvQNm-0&e=>
>>     [2]
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_pip
>> ermail_r-2Ddevel_2006-2DAugust_038985.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3E
>> PkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v
>> 91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=B1YSca31vHlpy9WJG8o0MBTh7bX4v7M61eWf
>> Q4tmdog&e= 
>> <https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_pi
>> permail_r-2Ddevel_2006-2DAugust_038985.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3
>> EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0
>> v91xFMGuYQo9NsNdzAp5k0CD2XzMc9x&s=B1YSca31vHlpy9WJG8o0MBTh7bX4v7M61eW
>> fQ4tmdog&e=>
>>
>>
>>     Hope this helps,
>>
>>     Rui Barradas
>>
>>
>>     ?s 00:36 de 13/07/2022, core_contingency escreveu:
>>      > To Whom it May Concern,
>>      >
>>      > I am creating a vector with the base R function "c", with many
>>      > arguments as shown below:
>>      >
>>      >       $ R
>>      >       > MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2",
>>     "ACTN1",
>>      > "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", 
>> "AJUBA",
>>      > "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", 
>> "ANXA6",
>>      > "APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", 
>> "ARMCX2",
>>      > "ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4",
>>     "ATP6V0E1",
>>      > "ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", 
>> "BOC",
>>      > "BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", 
>> "CALD1",
>>      > "CALU", "CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", 
>> "CD44",
>>      > "CD59", "CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", 
>> "CKAP4",
>>      > "CLIC4", "CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", 
>> "COL1A1",
>>      > "COL27A1", "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2",
>>     "COL6A1",
>>      > "COL6A2", "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", 
>> "CREB3L2",
>>      > "CREG1", "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", 
>> "CTNNA1",
>>      > "CTSB", "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", 
>> "CYP26A1",
>>      > "CYR61", "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", 
>> "DLX1",
>>      > "DLX2", "DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", 
>> "DPY19L1",
>>      > "DSE", "DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2",
>>     "EGFR",
>>      > "EGR1", "EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", 
>> "EMILIN1",
>>      > "EMP1", "ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1",
>>     "ETS1",
>>      > "EVA1A", "EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", 
>> "FAM114A1",
>>      > "FAM120A", "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", 
>> "FBN1",
>>      > "FBN2", "FGFR1", "FIBIN", "FILIP1L", "FKBP14", "FLNA", 
>> "FLRT2",
>>      > "FMOD", "FN1", "FNDC3B", "FSTL1", "FUCA2", "FZD1", "FZD2", 
>> "FZD7",
>>      > "GABRR1", "GALNT10", "GAS1", "GAS2", "GDF15", "GJA1", "GNAI1",
>>      > "GNG12", "GNS", "GORAB", "GPC6", "GPR137B", "GPX8", "GRN", 
>> "GSN",
>>      > "HES1", "HEXB", "HIBADH", "HIPK3", "HIST1H2AC", "HIST1H2BK",
>>     "HLA-A",
>>      > "HLA-B", "HLA-C", "HLA-F", "HLX", "HNMT", "HOMER1", 
>> "HS3ST3A1",
>>      > "HSP90B1", "HSPA5", "HSPB1", "HTRA1", "HYOU1", "ID1", "ID3",
>>     "IFI16",
>>      > "IFITM2", "IFITM3", "IGF2R", "IGFBP5", "IGFBP6", "IL13RA1", 
>> "IL6ST",
>>      > "INSIG1", "IQGAP2", "ITGA10", "ITGA4", "ITGAV", "ITGB1", 
>> "ITM2B",
>>      > "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", "KANK2", 
>> "KCNK2",
>>      > "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", "KIAA1462",
>>     "KIF13A",
>>      > "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", "LAMB1", 
>> "LAMC1",
>>      > "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", "LGALS1", 
>> "LHFP",
>>      > "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", "LMNA", 
>> "LOXL2",
>>      > "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", "MAGT1",
>>      > "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1",
>>      > "MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", 
>> "MOB1A",
>>      > "MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", 
>> "MYLIP",
>>      > "NANS", "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2",
>>      > "NOTCH2", "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1",
>>      > "OGFRL1", "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", 
>> "PALLD",
>>      > "PAPSS2", "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", 
>> "PDGFC",
>>      > "PDIA3", "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", 
>> "PHLDA3",
>>      > "PHLDB2", "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", 
>> "PLK2",
>>      > "PLOD2", "PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", 
>> "PLXDC2",
>>      > "POLR2L", "PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", 
>> "PRDM6",
>>      > "PRDX4", "PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", 
>> "PTGFRN",
>>      > "PTN", "PTPN14", "PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", 
>> "PYGL",
>>      > "QKI", "QSOX1", "RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B",
>>     "RBMS1",
>>      > "RCN1", "RECK", "REST", "RGL1", "RGS10", "RGS3", "RHOC", 
>> "RHOJ",
>>      > "RIN2", "RIT1", "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", 
>> "S1PR3",
>>      > "SASH1", "SCPEP1", "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4",
>>      > "SEC14L1", "SEL1L3", "SEMA3C", "SEMA3F", "SEPT10", "SERPINE2",
>>      > "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", "SH3BGRL", "SHC1",
>>     "SHROOM3",
>>      > "SIX1", "SIX4", "SKIL", "SLC16A4", "SLC30A1", "SLC30A7", 
>> "SLC35F5",
>>      > "SLC38A2", "SLC38A6", "SLC39A14", "SMAD3", "SNAI2", "SNAP23",
>>      > "SOSTDC1", "SOX9", "SPARC", "SPARCL1", "SPATA20", "SPCS3", 
>> "SPRED1",
>>      > "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", "SRPX", "SSBP4", 
>> "SSR1",
>>      > "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", "SUCLG2", 
>> "SURF4",
>>     "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", "TFPI",
>>     "TGFB1I1", "TGFBR2", "THBS1", "TIMP1", "TJP1")
>>      >       +
>>      >
>>      > For some reason, the R console does not display a ">" symbol,
>>      > indicating that it has completed the function, but displays a "+"
>>      > symbol instead, which indicates that the function is still
>>     waiting for more input.
>>      > However, I believe that my syntax is correct. If I shorten my
>>     command
>>      > by a few characters by removing the last entry, "TJP1":
>>      >
>>      >       $ R
>>      >       > MES = c("A2M", "ABRACL", "ACADVL", "ACAP2", "ACTA2",
>>     "ACTN1",
>>      > "ADAM19", "ADAM9", "ADAMTS5", "ADGRE5", "ADGRG6", "AEBP1", 
>> "AJUBA",
>>      > "ALDH1A3", "AMMECR1", "ANTXR1", "ANXA1", "ANXA2", "ANXA5", 
>> "ANXA6",
>>      > "APOE", "APP", "ARHGAP1", "ARHGEF40", "ARL1", "ARL4A", 
>> "ARMCX2",
>>      > "ARPC1B", "ASPH", "ATP10D", "ATP1B1", "ATP2B1", "ATP2B4",
>>     "ATP6V0E1",
>>      > "ATP8B2", "ATXN1", "B2M", "BAG3", "BGN", "BMP5", "BNC2", 
>> "BOC",
>>      > "BTN3A2", "C1orf198", "C1orf54", "C4orf32", "C6orf120", 
>> "CALD1",
>>      > "CALU", "CAPN2", "CAPN6", "CBFB", "CBLB", "CCDC80", "CD164", 
>> "CD44",
>>      > "CD59", "CD63", "CDH11", "CETN2", "CFH", "CFI", "CILP", 
>> "CKAP4",
>>      > "CLIC4", "CMTM3", "CMTM6", "CNN3", "COL11A1", "COL12A1", 
>> "COL1A1",
>>      > "COL27A1", "COL3A1", "COL4A1", "COL4A2", "COL5A1", "COL5A2",
>>     "COL6A1",
>>      > "COL6A2", "COL6A3", "COPA", "CPED1", "CPS1", "CRABP2", 
>> "CREB3L2",
>>      > "CREG1", "CRELD2", "CRISPLD1", "CRTAP", "CSRP1", "CTDSP2", 
>> "CTNNA1",
>>      > "CTSB", "CTSC", "CTSO", "CXCL12", "CYBRD1", "CYFIP1", 
>> "CYP26A1",
>>      > "CYR61", "DCAF6", "DDOST", "DDR2", "DESI2", "DKK3", "DLC1", 
>> "DLX1",
>>      > "DLX2", "DMD", "DNAJC1", "DNAJC10", "DNAJC3", "DNM3OS", 
>> "DPY19L1",
>>      > "DSE", "DUSP14", "DUSP5", "DUSP6", "EDEM1", "EDNRA", "EFEMP2",
>>     "EGFR",
>>      > "EGR1", "EGR3", "EHD2", "ELAVL1", "ELF1", "ELK3", "ELK4", 
>> "EMILIN1",
>>      > "EMP1", "ENAH", "EPHA3", "EPS8", "ERBIN", "ERLIN1", "ERRFI1",
>>     "ETS1",
>>      > "EVA1A", "EXT1", "EXTL2", "F2R", "F2RL2", "FAM102B", 
>> "FAM114A1",
>>      > "FAM120A", "FAM129A", "FAM3C", "FAM43A", "FAM46A", "FAT1", 
>> "FBN1",
>>      > "FBN2", "FGFR1", "FIBIN", "FILIP1L", "FKBP14", "FLNA", 
>> "FLRT2",
>>      > "FMOD", "FN1", "FNDC3B", "FSTL1", "FUCA2", "FZD1", "FZD2", 
>> "FZD7",
>>      > "GABRR1", "GALNT10", "GAS1", "GAS2", "GDF15", "GJA1", "GNAI1",
>>      > "GNG12", "GNS", "GORAB", "GPC6", "GPR137B", "GPX8", "GRN", 
>> "GSN",
>>      > "HES1", "HEXB", "HIBADH", "HIPK3", "HIST1H2AC", "HIST1H2BK",
>>     "HLA-A",
>>      > "HLA-B", "HLA-C", "HLA-F", "HLX", "HNMT", "HOMER1", 
>> "HS3ST3A1",
>>      > "HSP90B1", "HSPA5", "HSPB1", "HTRA1", "HYOU1", "ID1", "ID3",
>>     "IFI16",
>>      > "IFITM2", "IFITM3", "IGF2R", "IGFBP5", "IGFBP6", "IL13RA1", 
>> "IL6ST",
>>      > "INSIG1", "IQGAP2", "ITGA10", "ITGA4", "ITGAV", "ITGB1", 
>> "ITM2B",
>>      > "ITM2C", "ITPR1", "ITPRIPL2", "JAK1", "JAM3", "KANK2", 
>> "KCNK2",
>>      > "KCTD12", "KDELC2", "KDELR2", "KDELR3", "KDM5B", "KIAA1462",
>>     "KIF13A",
>>      > "KIRREL", "KLF10", "KLF4", "KLF6", "L3HYPDH", "LAMB1", 
>> "LAMC1",
>>      > "LAMP1", "LAPTM4A", "LASP1", "LATS2", "LEPROT", "LGALS1", 
>> "LHFP",
>>      > "LHX8", "LIFR", "LIPA", "LITAF", "LIX1L", "LMAN1", "LMNA", 
>> "LOXL2",
>>      > "LPP", "LRP10", "LRRC17", "LRRC8C", "LTBP1", "LUZP1", "MAGT1",
>>      > "MAML2", "MAN2A1", "MANF", "MBD2", "MBNL1", "MBTPS1", "MEOX1",
>>      > "MEOX2", "MEST", "MGAT2", "MGP", "MGST1", "MICAL2", "MMP2", 
>> "MOB1A",
>>      > "MRC2", "MXRA5", "MYADM", "MYDGF", "MYL12A", "MYL12B", 
>> "MYLIP",
>>      > "NANS", "NBR1", "NEK7", "NES", "NFIA", "NFIC", "NID1", "NID2",
>>      > "NOTCH2", "NOTCH2NL", "NPC2", "NPTN", "NQO1", "NR3C1", "NRP1",
>>      > "OGFRL1", "OLFML2A", "OLFML2B", "OLFML3", "OSTC", "P4HA1", 
>> "PALLD",
>>      > "PAPSS2", "PCDH18", "PCOLCE2", "PCSK5", "PDE3A", "PDE7B", 
>> "PDGFC",
>>      > "PDIA3", "PDIA4", "PDIA6", "PDLIM1", "PEA15", "PEAK1", 
>> "PHLDA3",
>>      > "PHLDB2", "PHTF2", "PIAS3", "PLAGL1", "PLEKHA2", "PLEKHH2", 
>> "PLK2",
>>      > "PLOD2", "PLOD3", "PLPP1", "PLS3", "PLSCR1", "PLSCR4", 
>> "PLXDC2",
>>      > "POLR2L", "PON2", "POSTN", "PPIB", "PPIC", "PPT1", "PRCP", 
>> "PRDM6",
>>      > "PRDX4", "PRDX6", "PROM1", "PRRX1", "PTBP1", "PTGER4", 
>> "PTGFRN",
>>      > "PTN", "PTPN14", "PTPRG", "PTPRK", "PTRF", "PXDC1", "PXDN", 
>> "PYGL",
>>      > "QKI", "QSOX1", "RAB13", "RAB29", "RAB31", "RAP1A", "RAP1B",
>>     "RBMS1",
>>      > "RCN1", "RECK", "REST", "RGL1", "RGS10", "RGS3", "RHOC", 
>> "RHOJ",
>>      > "RIN2", "RIT1", "RNFT1", "RNH1", "ROBO1", "ROR1", "RRBP1", 
>> "S1PR3",
>>      > "SASH1", "SCPEP1", "SCRG1", "SDC2", "SDC4", "SDCBP", "SDF4",
>>      > "SEC14L1", "SEL1L3", "SEMA3C", "SEMA3F", "SEPT10", "SERPINE2",
>>      > "SERPINH1", "SFT2D1", "SFT2D2", "SGK1", "SH3BGRL", "SHC1",
>>     "SHROOM3",
>>      > "SIX1", "SIX4", "SKIL", "SLC16A4", "SLC30A1", "SLC30A7", 
>> "SLC35F5",
>>      > "SLC38A2", "SLC38A6", "SLC39A14", "SMAD3", "SNAI2", "SNAP23",
>>      > "SOSTDC1", "SOX9", "SPARC", "SPARCL1", "SPATA20", "SPCS3", 
>> "SPRED1",
>>      > "SPRY1", "SPRY4", "SPRY4-IT1", "SQSTM1", "SRPX", "SSBP4", 
>> "SSR1",
>>      > "SSR3", "STAT1", "STAT3", "STEAP1", "STK38L", "SUCLG2", 
>> "SURF4",
>>     "SVIL", "SYDE1", "SYNJ2", "SYPL1", "TCF7L2", "TFE3", "TFPI",
>>     "TGFB1I1", "TGFBR2", "THBS1", "TIMP1")
>>      >       >
>>      >
>>      > The function now works, and returns a ">" symbol, indicating 
>> that
>>     the
>>      > function completed successfully. The ls() function proves it:
>>      >
>>      >       $ R
>>      >       > ls()
>>      >       [1] "MES"
>>      >
>>      > Is this a bug in the base R "c" function? It seems like the "c"
>>      > function can only accept so many characters before it fails.
>>      >
>>      > Thank you for your time,
>>      > core_contingency
>>      >
>>      >
>>      > ______________________________________________
>>      > R-help at r-project.org <mailto:R-help at r-project.org> mailing 
>> list
>>     -- To UNSUBSCRIBE and more, see
>>      >
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
>> l 
>> <https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_ma
>> il>
>>      >
>> man_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
>> s
>>      >
>> Rzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0C
>> D
>>      > 2XzMc9x&s=x5SJbPFsoqRiJYh7Y5B0QDKio2Wy4Je38lBBi99AbAE&e=
>>      > PLEASE do read the posting guide
>>      >
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
>> r 
>> <https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.
>> or>
>>      >
>> g_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVe
>> A
>>      >
>> sRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k0
>> C
>>      > D2XzMc9x&s=lCG5D1ItMs8G_wkshvm4nBaVq4Ehy_zyq5mnp2zpt2Y&e=
>>      > and provide commented, minimal, self-contained, reproducible 
>> code.
>>
>>     ______________________________________________
>>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list 
>> --
>>     To UNSUBSCRIBE and more, see
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
>> lman_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVe
>> AsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5k
>> 0CD2XzMc9x&s=x5SJbPFsoqRiJYh7Y5B0QDKio2Wy4Je38lBBi99AbAE&e=
>> <https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_ma
>> ilman_listinfo_r-2Dhelp&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kV
>> eAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5
>> k0CD2XzMc9x&s=x5SJbPFsoqRiJYh7Y5B0QDKio2Wy4Je38lBBi99AbAE&e=>
>>     PLEASE do read the posting guide
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
>> rg_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kV
>> eAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp5
>> k0CD2XzMc9x&s=lCG5D1ItMs8G_wkshvm4nBaVq4Ehy_zyq5mnp2zpt2Y&e=
>> <https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.
>> org_posting-2Dguide.html&d=DwIDaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2k
>> VeAsRzsn7AkP-g&m=bDsMNmn2OhMbQIvptul8Yl752vW7YkxJ0v91xFMGuYQo9NsNdzAp
>> 5k0CD2XzMc9x&s=lCG5D1ItMs8G_wkshvm4nBaVq4Ehy_zyq5mnp2zpt2Y&e=>
>>     and provide commented, minimal, self-contained, reproducible code.
>>     ______________________________________________
>>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list 
>> --
>>     To UNSUBSCRIBE and more, see
>>     https://stat.ethz.ch/mailman/listinfo/r-help
>>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>>     PLEASE do read the posting guide
>>     http://www.R-project.org/posting-guide.html
>>     <http://www.R-project.org/posting-guide.html>
>>     and provide commented, minimal, self-contained, reproducible code.
>>


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed Jul 13 23:40:54 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 13 Jul 2022 22:40:54 +0100
Subject: [R] How to parse a really silly date with lubridate
In-Reply-To: <tamr12$14qr$1@ciao.gmane.io>
References: <tami12$11b8$1@ciao.gmane.io>
 <c166bf12-124e-0477-8694-4bfa73399447@sapo.pt> <tamr12$14qr$1@ciao.gmane.io>
Message-ID: <f34dae75-c5d0-6574-9cf1-aa43dbf0c2d6@sapo.pt>

Hello,

Are you looking for mutate? In the example below I haven't included the 
filter, since the tibble only has 2 rows. But the date column is coerced 
to an actual datetime class in place, without the need for NewDate.

suppressPackageStartupMessages({
   library(tibble)
   library(dplyr)
})

DDATA <- tibble(Date = c('9. Jul 2022 at 11:39', '10. Jul 2022 at 01:58'))

DDATA %>%
   mutate(Date = lubridate::dmy_hm(Date))
#> # A tibble: 2 ? 1
#>   Date
#>   <dttm>
#> 1 2022-07-09 11:39:00
#> 2 2022-07-10 01:58:00


Hope this helps,

Rui Barradas


?s 17:14 de 13/07/2022, Dr Eberhard W Lisse escreveu:
> 
> Hi,
> 
> while all of the below work in a character vector, none works in
> the tibble.
> 
> The following
> 
>  ????DDATA %>%
>  ??????? add_column(as.tibble(lubridate::dmy_hm(DDATA$Date)),
>  ??????????? .before = "Period") %>%
>  ??????? rename(NewDate=value) %>%
>  ??????? select(Date,NewDate) %>%
>  ??????? filter(between(as.Date(NewDate),as.Date('2022-07-09'),
>  ??????????? as.Date('2022-07-10')))
> 
> does work
> 
>  ????# A tibble: 3 ? 2
>  ????? Date????????????????? NewDate
>  ????? <chr>???????????????? <dttm>
>  ????1 9. Jul 2022 at 11:39? 2022-07-09 11:39:00
>  ????2 10. Jul 2022 at 01:58 2022-07-10 01:58:00
>  ????3 10. Jul 2022 at 11:26 2022-07-10 11:26:00
> 
> but I wonder if that can not be done more elegantly, ie by direct
> replacements in the column.
> 
> greetings, el
> 
> On 2022-07-13 16:48 , Rui Barradas wrote:
> [...]
>  > d <- c('9. Jul 2022 at 11:39', '10. Jul 2022 at 01:58')
>  > lubridate::dmy_hm(d)
>  > #> [1] "2022-07-09 11:39:00 UTC" "2022-07-10 01:58:00 UTC"
> [...]
> 
> On 2022-07-13 16:03 , Ben Tupper wrote:
> [...]
>  > s = c("9. Jul 2022 at 11:39", "10. Jul 2022 at 01:58")
>  > as.POSIXct(s, format = "%d. %b %Y at %H:%M")
>  > as.POSIXct(s, format = "%d. %b %Y at %H:%M", tz = "UTC")
> [...]
> 
> On 2022-07-13 15:52 , Ivan Krylov wrote:
> [...]
>  > Sys.setlocale('LC_TIME', 'C')
>  > strptime(
>  >?? c('9. Jul 2022 at 11:39', '10. Jul 2022 at 01:58'),
>  >?? '%d. %b %Y at %H:%M'
>  > )
> [...]
> 
> 
>  > ?s 14:40 de 13/07/2022, Dr Eberhard Lisse escreveu:
>  >>
>  >> Hi,
>  >>
>  >> I have data file which generated by an otherwise very nice (diabetes
>  >> log) app, but exports dates really silly.
>  >>
>  >> After reading the enclosed mwe.csv into R like so
>  >>
>  >>?????? MWE <- read_delim('mwe.csv', delim = ';') %>%
>  >>????????? select(Date) %>%
>  >>????????? print()
>  >>
>  >>
>  >> this comes out as:
>  >>
>  >>?????? # A tibble: 2 ? 1
>  >>????? Date
>  >>????? <chr>
>  >>?????? 1 9. Jul 2022 at 11:39
>  >>?????? 2 10. Jul 2022 at 01:58
>  >>
>  >>
>  >> No matter what I try I am not able to parse this inside R to get at
>  >> proper dates (I have loaded tidyverse and lubridate).
>  >>
>  >> I can easily do somethig
>  >>
>  >>?????? csvq? -d ';' -t '%e. %b %Y at %H:%i' \
>  >>????????? 'SELECT Date as oridate,
>  >>????????????? DATETIME_FORMAT(Date, "%Y-%m-%d %H:%m") AS date
>  >>????????? FROM mwe'
>  >>
>  >>?????? +-----------------------+------------------+
>  >>?????? |??????? oridate??????? |?????? date?????? |
>  >>?????? +-----------------------+------------------+
>  >>?????? | 9. Jul 2022 at 11:39? | 2022-07-09 11:07 |
>  >>?????? | 10. Jul 2022 at 01:58 | 2022-07-10 01:07 |
>  >>?????? +-----------------------+------------------+
>  >>
>  >> and hence could easily do something like
>  >>
>  >>?????? csvq? -d ';' -t '%e. %b %Y at %H:%i' \
>  >>??????? 'ALTER mwe
>  >>??????? SET Date = DATETIME_FORMAT(Date, "%Y-%m-%d %H:%m")'
>  >>
>  >> but would rather like to be able to do it inside R and would therefor
>  >> appreciate any advice in this regard.
>  >>
>  >>
>  >> greetings, el
>  >>
>  >
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Thu Jul 14 00:07:07 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 13 Jul 2022 15:07:07 -0700
Subject: [R] Bug in packages.
In-Reply-To: <006b01d896f5$ed7ff6d0$c87fe470$@gmail.com>
References: <006b01d896f5$ed7ff6d0$c87fe470$@gmail.com>
Message-ID: <CAGxFJbRmn0SB=BVJeE5aaTBRpDe4pa7gN-FwuqyPThmM=8UVeg@mail.gmail.com>

https://www.r-project.org/bugs.html

for info on bug reporting.

Bert

On Wed, Jul 13, 2022 at 1:49 PM Charles-?douard Gigu?re
<ce.giguere at gmail.com> wrote:
>
> Hello everyone,
> Is there a mechanism to report a bug in someone package? I plan to email the author, but I was wondering if there is an official way like the issue function in github.
> Thank you,
> Charles-?douard
>
> Charles-?douard Gigu?re
> Analyste statisticien
> Centre de recherche de l?Institut universitaire en sant? mentale de Montr?al (CR-IUSMM)
> 514?251-4015, poste 3516
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ggrothend|eck @end|ng |rom gm@||@com  Thu Jul 14 00:18:30 2022
From: ggrothend|eck @end|ng |rom gm@||@com (Gabor Grothendieck)
Date: Wed, 13 Jul 2022 18:18:30 -0400
Subject: [R] Bug in packages.
In-Reply-To: <006b01d896f5$ed7ff6d0$c87fe470$@gmail.com>
References: <006b01d896f5$ed7ff6d0$c87fe470$@gmail.com>
Message-ID: <CAP01uR=z0mEZW_WuiKsE4RNpCaF+5EXDi30vXsqwj+gCfY3zVw@mail.gmail.com>

If this is in reference to a package on CRAN then go to its CRAN home page, for
example,

  https://cran.r-project.org/package=sqldf

and then look for a BugReports line on that page to find where to report bugs.

If there is no such line check the URL line and if it points to a
github URL then
use the issues on github.

Otherwise, send an email to the maintainer listed on the Maintainer line.

On Wed, Jul 13, 2022 at 4:49 PM Charles-?douard Gigu?re
<ce.giguere at gmail.com> wrote:
>
> Hello everyone,
> Is there a mechanism to report a bug in someone package? I plan to email the author, but I was wondering if there is an official way like the issue function in github.
> Thank you,
> Charles-?douard
>
> Charles-?douard Gigu?re
> Analyste statisticien
> Centre de recherche de l?Institut universitaire en sant? mentale de Montr?al (CR-IUSMM)
> 514?251-4015, poste 3516
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From @tyen @end|ng |rom ntu@edu@tw  Thu Jul 14 06:14:35 2022
From: @tyen @end|ng |rom ntu@edu@tw (Steven T. Yen)
Date: Thu, 14 Jul 2022 12:14:35 +0800
Subject: [R] Building binary file failed
Message-ID: <c502ac49-15f0-47b8-c385-292ed8e1e13b@ntu.edu.tw>

Dear

I have? anew laptop and am trying to build R lib / package as usual. Now 
I get the following error message and the .zip file has a size 0. Help?

I just did the following to build the binary file:

File -> Open project -> Build binary package under Build

Thanks.

Steven Yen

==> Rcmd.exe INSTALL --build --preclean yenlib1 * installing to library 
'C:/Users/USER/AppData/Local/R/win-library/4.2' * installing *source* 
package 'yenlib1' ... ** using staged installation ** R ** data *** 
moving datasets to lazyload DB ** byte-compile and prepare package for 
lazy loading ** help *** installing help indices ** building package 
indices ** testing if installed package can be loaded from temporary 
location ** testing if installed package can be loaded from final 
location ** testing if installed package keeps a record of temporary 
installation path * MD5 sums zip error: Zip file structure invalid 
(A:/R/yenlib1_1.1.0.zip) running 'zip' failed * DONE (yenlib1) Binary 
package written to A:/R

	[[alternative HTML version deleted]]


From no@p@m @end|ng |rom ||@@e@NA  Wed Jul 13 23:48:48 2022
From: no@p@m @end|ng |rom ||@@e@NA (Dr Eberhard W Lisse)
Date: Wed, 13 Jul 2022 23:48:48 +0200
Subject: [R] How to parse a really silly date with lubridate
In-Reply-To: <f34dae75-c5d0-6574-9cf1-aa43dbf0c2d6@sapo.pt>
References: <tami12$11b8$1@ciao.gmane.io>
 <c166bf12-124e-0477-8694-4bfa73399447@sapo.pt> <tamr12$14qr$1@ciao.gmane.io>
 <f34dae75-c5d0-6574-9cf1-aa43dbf0c2d6@sapo.pt>
Message-ID: <tanek0$e5e$1@ciao.gmane.io>


Bui,

thanks, this what Avi suggested in an email to me as well and works.

It's so easy if you know it :-)-O

el

On 2022-07-13 23:40 , Rui Barradas wrote:
> Hello,
> 
> Are you looking for mutate? In the example below I haven't included the 
> filter, since the tibble only has 2 rows. But the date column is coerced 
> to an actual datetime class in place, without the need for NewDate.
> 
> suppressPackageStartupMessages({
>  ? library(tibble)
>  ? library(dplyr)
> })
> 
> DDATA <- tibble(Date = c('9. Jul 2022 at 11:39', '10. Jul 2022 at 01:58'))
> 
> DDATA %>%
>  ? mutate(Date = lubridate::dmy_hm(Date))
> #> # A tibble: 2 ? 1
> #>?? Date
> #>?? <dttm>
> #> 1 2022-07-09 11:39:00
> #> 2 2022-07-10 01:58:00
> 
> 
> Hope this helps,
> 
> Rui Barradas
[...]


From kry|ov@r00t @end|ng |rom gm@||@com  Thu Jul 14 07:56:52 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Thu, 14 Jul 2022 08:56:52 +0300
Subject: [R] Bug in packages.
In-Reply-To: <006b01d896f5$ed7ff6d0$c87fe470$@gmail.com>
References: <006b01d896f5$ed7ff6d0$c87fe470$@gmail.com>
Message-ID: <20220714085652.3402cb0d@Tarkus>

On Wed, 13 Jul 2022 16:19:59 -0400
Charles-?douard Gigu?re <ce.giguere at gmail.com> wrote:

> Is there a mechanism to report a bug in someone package? I plan to
> email the author, but I was wondering if there is an official way
> like the issue function in github.

Try running bug.report(package='...'). It should either follow the link
designated by the package maintainer to report bugs, or open your mail
app to contact the maintainer.

-- 
Best regards,
Ivan


From @tyen @end|ng |rom ntu@edu@tw  Thu Jul 14 08:13:32 2022
From: @tyen @end|ng |rom ntu@edu@tw (Steven T. Yen)
Date: Thu, 14 Jul 2022 14:13:32 +0800
Subject: [R] Building binary file failed
In-Reply-To: <c502ac49-15f0-47b8-c385-292ed8e1e13b@ntu.edu.tw>
References: <c502ac49-15f0-47b8-c385-292ed8e1e13b@ntu.edu.tw>
Message-ID: <ba0fd1c6-6ff0-6365-6977-a8feb9e9e32e@ntu.edu.tw>

p.s. I had no problem doing

Build -> Build Source Package

to come up with a .Gz file. This may have to do with the Zip program in 
my new laptop. I removed and re-installed 7-Zip but it did not help. The 
Source package works for me but I'd like to be able to build a binary 
package.

On 7/14/2022 12:14 PM, Steven T. Yen wrote:

> Dear
>
> I have? anew laptop and am trying to build R lib / package as usual. 
> Now I get the following error message and the .zip file has a size 0. 
> Help?
>
> I just did the following to build the binary file:
>
> File -> Open project -> Build binary package under Build
>
> Thanks.
>
> Steven Yen
>
> ==> Rcmd.exe INSTALL --build --preclean yenlib1 * installing to 
> library 'C:/Users/USER/AppData/Local/R/win-library/4.2' * installing 
> *source* package 'yenlib1' ... ** using staged installation ** R ** 
> data *** moving datasets to lazyload DB ** byte-compile and prepare 
> package for lazy loading ** help *** installing help indices ** 
> building package indices ** testing if installed package can be loaded 
> from temporary location ** testing if installed package can be loaded 
> from final location ** testing if installed package keeps a record of 
> temporary installation path * MD5 sums zip error: Zip file structure 
> invalid (A:/R/yenlib1_1.1.0.zip) running 'zip' failed * DONE (yenlib1) 
> Binary package written to A:/R
	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Thu Jul 14 08:38:37 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Thu, 14 Jul 2022 09:38:37 +0300
Subject: [R] Building binary file failed
In-Reply-To: <ba0fd1c6-6ff0-6365-6977-a8feb9e9e32e@ntu.edu.tw>
References: <c502ac49-15f0-47b8-c385-292ed8e1e13b@ntu.edu.tw>
 <ba0fd1c6-6ff0-6365-6977-a8feb9e9e32e@ntu.edu.tw>
Message-ID: <20220714093837.1b6670bf@Tarkus>

On Thu, 14 Jul 2022 14:13:32 +0800
"Steven T. Yen" <styen at ntu.edu.tw> wrote:

> This may have to do with the Zip program in 
> my new laptop. I removed and re-installed 7-Zip but it did not help.

Yes, this is most likely related to the zip program. Adding MD5 sums
involves running zip.exe -r9Xq <filename> <directory>, and the call
fails for some reason.

On the other hand, it can't be 7-zip because it doesn't contain zip.exe.

After R CMD INSTALL --build yenlib1 is done, what is inside the *.zip
file? Does it look like a valid zip file?

If you open a Windows command line and type "where zip", which zip.exe
is returned? Is it the one from Rtools?

> [[alternative HTML version deleted]]

Please compose your messages in plain text. Otherwise, the
automatically translated plain text version is all we see, and it's
severely botched:
https://stat.ethz.ch/pipermail/r-help/2022-July/475284.html

See also: http://www.R-project.org/posting-guide.html

-- 
Best regards,
Ivan


From @tyen @end|ng |rom ntu@edu@tw  Thu Jul 14 09:00:18 2022
From: @tyen @end|ng |rom ntu@edu@tw (Steven T. Yen)
Date: Thu, 14 Jul 2022 15:00:18 +0800
Subject: [R] Building binary file failed
In-Reply-To: <20220714093837.1b6670bf@Tarkus>
References: <c502ac49-15f0-47b8-c385-292ed8e1e13b@ntu.edu.tw>
 <ba0fd1c6-6ff0-6365-6977-a8feb9e9e32e@ntu.edu.tw>
 <20220714093837.1b6670bf@Tarkus>
Message-ID: <3e234bb0-5f6f-a238-2c1b-2030646859dc@ntu.edu.tw>

Ivan

If you are asking me,...

>After R CMD INSTALL --build yenlib1 is done, what is inside the *.zip
>file? Does it look like a valid zip file?

The .zip is zero size. The double-click it and found the folder empty.

>If you open a Windows command line and type "where zip", which zip.exe
>is returned? Is it the one from Rtools?

Running the Windows command line "where zip" there is no response. I have Windows 10.

On 7/14/2022 2:38 PM, Ivan Krylov wrote:
> On Thu, 14 Jul 2022 14:13:32 +0800
> "Steven T. Yen" <styen at ntu.edu.tw> wrote:
>
>> This may have to do with the Zip program in
>> my new laptop. I removed and re-installed 7-Zip but it did not help.
> Yes, this is most likely related to the zip program. Adding MD5 sums
> involves running zip.exe -r9Xq <filename> <directory>, and the call
> fails for some reason.
>
> On the other hand, it can't be 7-zip because it doesn't contain zip.exe.
>
> After R CMD INSTALL --build yenlib1 is done, what is inside the *.zip
> file? Does it look like a valid zip file?
>
> If you open a Windows command line and type "where zip", which zip.exe
> is returned? Is it the one from Rtools?
>
>> [[alternative HTML version deleted]]
> Please compose your messages in plain text. Otherwise, the
> automatically translated plain text version is all we see, and it's
> severely botched:
> https://stat.ethz.ch/pipermail/r-help/2022-July/475284.html
>
> See also: http://www.R-project.org/posting-guide.html
>


From kry|ov@r00t @end|ng |rom gm@||@com  Thu Jul 14 09:10:18 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Thu, 14 Jul 2022 10:10:18 +0300
Subject: [R] Building binary file failed
In-Reply-To: <3e234bb0-5f6f-a238-2c1b-2030646859dc@ntu.edu.tw>
References: <c502ac49-15f0-47b8-c385-292ed8e1e13b@ntu.edu.tw>
 <ba0fd1c6-6ff0-6365-6977-a8feb9e9e32e@ntu.edu.tw>
 <20220714093837.1b6670bf@Tarkus>
 <3e234bb0-5f6f-a238-2c1b-2030646859dc@ntu.edu.tw>
Message-ID: <20220714101018.5272a5e5@Tarkus>

On Thu, 14 Jul 2022 15:00:18 +0800
"Steven T. Yen" <styen at ntu.edu.tw> wrote:

> The .zip is zero size. The double-click it and found the folder empty.

What if you remove the file first? I re-read the source, and now I
think that zip is called only once per R CMD INSTALL --build. If the
file doesn't exist in the first place, it can't have invalid structure,
right?

-- 
Best regards,
Ivan


From @tyen @end|ng |rom ntu@edu@tw  Thu Jul 14 09:20:17 2022
From: @tyen @end|ng |rom ntu@edu@tw (Steven T. Yen)
Date: Thu, 14 Jul 2022 15:20:17 +0800
Subject: [R] Building binary file failed
In-Reply-To: <20220714101018.5272a5e5@Tarkus>
References: <c502ac49-15f0-47b8-c385-292ed8e1e13b@ntu.edu.tw>
 <ba0fd1c6-6ff0-6365-6977-a8feb9e9e32e@ntu.edu.tw>
 <20220714093837.1b6670bf@Tarkus>
 <3e234bb0-5f6f-a238-2c1b-2030646859dc@ntu.edu.tw>
 <20220714101018.5272a5e5@Tarkus>
Message-ID: <25c2a300-e649-8eff-3806-163d28094466@ntu.edu.tw>

Ivan

Thanks. I removed file

"A:\R\yenlib1_1.1.0.zip"

first and then run again. Another .zip file gets created at the same 
location with zero size.

On 7/14/2022 3:10 PM, Ivan Krylov wrote:
> On Thu, 14 Jul 2022 15:00:18 +0800
> "Steven T. Yen" <styen at ntu.edu.tw> wrote:
>
>> The .zip is zero size. The double-click it and found the folder empty.
> What if you remove the file first? I re-read the source, and now I
> think that zip is called only once per R CMD INSTALL --build. If the
> file doesn't exist in the first place, it can't have invalid structure,
> right?
>


From @tyen @end|ng |rom ntu@edu@tw  Thu Jul 14 10:23:56 2022
From: @tyen @end|ng |rom ntu@edu@tw (Steven T. Yen)
Date: Thu, 14 Jul 2022 16:23:56 +0800
Subject: [R] Building binary file failed
In-Reply-To: <3e234bb0-5f6f-a238-2c1b-2030646859dc@ntu.edu.tw>
References: <c502ac49-15f0-47b8-c385-292ed8e1e13b@ntu.edu.tw>
 <ba0fd1c6-6ff0-6365-6977-a8feb9e9e32e@ntu.edu.tw>
 <20220714093837.1b6670bf@Tarkus>
 <3e234bb0-5f6f-a238-2c1b-2030646859dc@ntu.edu.tw>
Message-ID: <81f1b2f9-2b03-9382-3f06-ed2ae9b26681@ntu.edu.tw>

Dear Ivan and all
I had absolutely no idea what I was doing. I read
https://stackoverflow.com/questions/50192038/r-devtools-build-zip-failure
from which I learned to install devtools.
Then, my problem is resolved. Log file is attached below. Thank you all.
If someone can explain what I was doing (cause I still have no clue), 
I'd still be happy to listen.
Steven Yen

====
 > install.packages("devtools")
Installing package into ?C:/Users/USER/AppData/Local/R/win-library/4.2?
(as ?lib? is unspecified)
trying URL 
'https://cran.rstudio.com/bin/windows/contrib/4.2/devtools_2.4.3.zip'
Content type 'application/zip' length 423598 bytes (413 KB)
downloaded 413 KB

package ?devtools? successfully unpacked and MD5 sums checked

The downloaded binary packages are in
C:\Users\USER\AppData\Local\Temp\RtmpUZvAF8\downloaded_packages
 > devtools::find_rtools()
[1] TRUE

 > install.packages("A:/R/yenlib1_1.1.0.zip",repos=NULL) # from local, 
change path
Installing package into ?C:/Users/USER/AppData/Local/R/win-library/4.2?
(as ?lib? is unspecified)
package ?yenlib1? successfully unpacked and MD5 sums checked

On 7/14/2022 3:00 PM, Steven T. Yen wrote:
> Ivan
>
> If you are asking me,...
>
>> After R CMD INSTALL --build yenlib1 is done, what is inside the *.zip
>> file? Does it look like a valid zip file?
>
> The .zip is zero size. The double-click it and found the folder empty.
>
>> If you open a Windows command line and type "where zip", which zip.exe
>> is returned? Is it the one from Rtools?
>
> Running the Windows command line "where zip" there is no response. I 
> have Windows 10.
>
> On 7/14/2022 2:38 PM, Ivan Krylov wrote:
>> On Thu, 14 Jul 2022 14:13:32 +0800
>> "Steven T. Yen" <styen at ntu.edu.tw> wrote:
>>
>>> This may have to do with the Zip program in
>>> my new laptop. I removed and re-installed 7-Zip but it did not help.
>> Yes, this is most likely related to the zip program. Adding MD5 sums
>> involves running zip.exe -r9Xq <filename> <directory>, and the call
>> fails for some reason.
>>
>> On the other hand, it can't be 7-zip because it doesn't contain zip.exe.
>>
>> After R CMD INSTALL --build yenlib1 is done, what is inside the *.zip
>> file? Does it look like a valid zip file?
>>
>> If you open a Windows command line and type "where zip", which zip.exe
>> is returned? Is it the one from Rtools?
>>
>>> [[alternative HTML version deleted]]
>> Please compose your messages in plain text. Otherwise, the
>> automatically translated plain text version is all we see, and it's
>> severely botched:
>> https://stat.ethz.ch/pipermail/r-help/2022-July/475284.html
>>
>> See also: http://www.R-project.org/posting-guide.html
>>


From r@turner @end|ng |rom @uck|@nd@@c@nz  Thu Jul 14 12:20:10 2022
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Thu, 14 Jul 2022 22:20:10 +1200
Subject: [R] Thanks to this list ....
Message-ID: <20220714222010.17fa67a1@rolf-Latitude-E7470>


I set out to appeal to this list for help with disentangling
a bewildering anomaly that was produced by some dynamically loaded
Fortran code.

In composing an email to explain the nature of the anomaly, I *FINALLY*
spotted the loony!  I had an expression in a nested do loop:

    j = npro + (r-1)*nvym1 + s

where r and s were the indices of two of the nested loops (explicitly
declared to be integer at the start of the subroutine in question).

When I copied the foregoing expression into the email I at last noticed
that the variable "nvym1" *should* have been "nyvm1".  See the subtle
difference?  :-)

The variable nvym1 was never initialised, so it took on strange values
plucked out of RAM, I guess.  Whence the anomaly.  Once I corrected that
trivial typo, things were OK.

Thanks everybody!!! :-)

cheers,

Rolf Turner

P.S. What I can't figure out (and won't waste any time trying) is
why the first four values of j were as they should have been, and things
did not go to hell in a handcart until the code got to the 5th value
of j.  I guess the computer gods were just amusing themselves at my
expense.

R. T.

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Jul 14 18:08:31 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 14 Jul 2022 16:08:31 +0000
Subject: [R] aborting the execution of a function...
In-Reply-To: <01ac01d896f4$66077120$32165360$@gmail.com>
References: <PU4P216MB15682B367ABD8E231ECAA5A3C8899@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <A5DEF48F-C65C-4A09-9A7B-DB48BCF2C168@dcn.davis.ca.us>
 <01ac01d896f4$66077120$32165360$@gmail.com>
Message-ID: <PU4P216MB1568921390A9722D85700C7DC8889@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear Avi,
                 THanks a lot...

Yours sincerely,
AKSHAY M KULKARNI
________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of avi.e.gross at gmail.com <avi.e.gross at gmail.com>
Sent: Thursday, July 14, 2022 1:39 AM
To: 'R help Mailing list' <r-help at r-project.org>
Subject: Re: [R] aborting the execution of a function...

Jeff & Akshay,

What you say is true if you just call a function and do not arrange to
handle various interrupts and errors.

Obviously if you find a way to gracefully handle an error then you can opt
to have your work so far saved. For what is meant to be a fatal error,
though, you are expected to GIVE UP or at least exit rapidly after doing
something graceful.

I am not clear what machine the user is using and their R setup. It may be
you can find something like a try() or suspendInterrupts method to catch and
handle control-C but may I offer a DIFFERENT solution?

You can identify your R process and send it some other milder signal that
can easily be caught or change your loop to slow it down a bit more.

For example, change your code so it opens a file and writes data to it in
some format like one result per line or a CSV format or whatever makes you
happy. Your loop adds new lines/items to the file and perhaps even closes
the file on interrupt or whatever works. Or it writes the results to the
console where you can copy/paste from if not too long.

And, consider having long-running code periodically check something in the
environment looking for a signal. Say every hundredth iteration it checks
for the existence of a file called "STOP_IT.stupid" and if it sees it,
removes it and exits gracefully while preserving your results somehow or
whatever you need. No interrupt needed, just create an empty file or other
logical marker.

Another variant is to use some form of threading or subprocess that does the
work somewhat in the background but can get commands from the foreground
process as needed including a request to stop. Again, no horrible signals
that kill the program.

And note on some systems, a process can be halted and resumed, if the
problem is that it has run a long time and is using too many resources at a
time they are needed.



-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Jeff Newmiller
Sent: Wednesday, July 13, 2022 3:49 PM
To: r-help at r-project.org; akshay kulkarni <akshay_e4 at hotmail.com>; R help
Mailing list <r-help at r-project.org>
Subject: Re: [R] aborting the execution of a function...

This would be easy for you to test on a small example on your local
computer.

But the answer is "no". Nothing is assigned if the function does not return
normally... and Ctrl+C is anything but normal.

On July 13, 2022 12:19:58 PM PDT, akshay kulkarni <akshay_e4 at hotmail.com>
wrote:
>Dear members,
>                         I am running a large scraping code in a very
powerful AWS ec2 instance:
>
>DATES <- getFirms Dates()
>
>It iterates over 500 stocks from a website. Despite the power of the
machine, the execution is very slow.
>
>If I abort the function (by ctrl + C), after, say 150th iteration, the
DATES object will still contain the scraped data untill the 150th iteration,
right? ( The rest of the 350 entries will be NA's, I suppose).
>
>Many thanks in advance.....
>
>Yours sincerely,
>AKSHAY M KULKARNI
>
>
>
>       [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Jul 14 18:09:12 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 14 Jul 2022 16:09:12 +0000
Subject: [R] aborting the execution of a function...
In-Reply-To: <A5DEF48F-C65C-4A09-9A7B-DB48BCF2C168@dcn.davis.ca.us>
References: <PU4P216MB15682B367ABD8E231ECAA5A3C8899@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <A5DEF48F-C65C-4A09-9A7B-DB48BCF2C168@dcn.davis.ca.us>
Message-ID: <PU4P216MB1568C696EE4D3EB29FDFC178C8889@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dar Jeff,
              Many thanks...

Yours sincerely,
AKHAY M KULKARNI
________________________________
From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Sent: Thursday, July 14, 2022 1:18 AM
To: r-help at r-project.org <r-help at r-project.org>; akshay kulkarni <akshay_e4 at hotmail.com>; R help Mailing list <r-help at r-project.org>
Subject: Re: [R] aborting the execution of a function...

This would be easy for you to test on a small example on your local computer.

But the answer is "no". Nothing is assigned if the function does not return normally... and Ctrl+C is anything but normal.

On July 13, 2022 12:19:58 PM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>Dear members,
>                         I am running a large scraping code in a very powerful AWS ec2 instance:
>
>DATES <- getFirms Dates()
>
>It iterates over 500 stocks from a website. Despite the power of the machine, the execution is very slow.
>
>If I abort the function (by ctrl + C), after, say 150th iteration, the DATES object will still contain the scraped data untill the 150th iteration, right? ( The rest of the 350 entries will be NA's, I suppose).
>
>Many thanks in advance.....
>
>Yours sincerely,
>AKSHAY M KULKARNI
>
>
>
>       [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Jul 14 18:09:54 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 14 Jul 2022 16:09:54 +0000
Subject: [R] aborting the execution of a function...
In-Reply-To: <CAHqSRuQoLdy6QtG=WFRYj-SBhhdRN9qu42b5ESnfYJz3EFzK2w@mail.gmail.com>
References: <PU4P216MB15682B367ABD8E231ECAA5A3C8899@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <CAHqSRuQoLdy6QtG=WFRYj-SBhhdRN9qu42b5ESnfYJz3EFzK2w@mail.gmail.com>
Message-ID: <PU4P216MB1568BD82A063060179655675C8889@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear Bill,
                Many thanks ......

Yours sincrely,
AKSHAY M KULKARNI
________________________________
From: Bill Dunlap <williamwdunlap at gmail.com>
Sent: Thursday, July 14, 2022 1:28 AM
To: akshay kulkarni <akshay_e4 at hotmail.com>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] aborting the execution of a function...

You could write a function that returns an environment (or list if you prefer) containing the results collected before the interrupt by using tryCatch(interrupt=...).  E.g.,

doMany <- function(names) {
    resultEnv <- new.env(parent=emptyenv())
    tryCatch(
        for(name in names) resultEnv[[name]] <- Sys.sleep(1), # replace Sys.sleep(1) by getStuffFromWeb(name)
        interrupt = function(e) NULL)
    resultEnv
}

Use it as

> system.time(e <- doMany(state.name<http://state.name>)) # hit Esc or ^C after a few seconds
^C   user  system elapsed
  0.001   0.000   4.390
> names(e)
[1] "Alabama"  "Alaska"   "Arizona"  "Arkansas"
> eapply(e, identity)
$Alabama
NULL

$Alaska
NULL

$Arizona
NULL

$Arkansas
NULL

-Bill

On Wed, Jul 13, 2022 at 12:20 PM akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>> wrote:
Dear members,
                         I am running a large scraping code in a very powerful AWS ec2 instance:

DATES <- getFirms Dates()

It iterates over 500 stocks from a website. Despite the power of the machine, the execution is very slow.

If I abort the function (by ctrl + C), after, say 150th iteration, the DATES object will still contain the scraped data untill the 150th iteration, right? ( The rest of the 350 entries will be NA's, I suppose).

Many thanks in advance.....

Yours sincerely,
AKSHAY M KULKARNI



        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From @vi@e@gross m@iii@g oii gm@ii@com  Thu Jul 14 18:43:17 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Thu, 14 Jul 2022 12:43:17 -0400
Subject: [R] How to parse a really silly date with lubridate
In-Reply-To: <tanek0$e5e$1@ciao.gmane.io>
References: <tami12$11b8$1@ciao.gmane.io>
 <c166bf12-124e-0477-8694-4bfa73399447@sapo.pt> <tamr12$14qr$1@ciao.gmane.io>
 <f34dae75-c5d0-6574-9cf1-aa43dbf0c2d6@sapo.pt> <tanek0$e5e$1@ciao.gmane.io>
Message-ID: <007601d897a0$d214a700$763df500$@gmail.com>

To be clear, I take no credit for the rather extraordinary function cll shown below:

mutate(Date = lubridate::dmy_hm(Date))

I would pretty much never have constructed such an interesting and rather unnecessary line of code.

ALL the work is done within the parentheses:

Date = lubridate::dmy_hm(Date))

The above creates a tibble and assigns it to the name of Date. but first it takes a vector called Date containing text and uses a function to parse it and return a form more suitable to use as a date.

So what does mutate() do when it is called with a tibble and asked to do nothing? I mean it sees something more like this:

Mutate(.data=Date)

What is missing are the usual assignment statements to modify existing columns or make new ones. So it does nothing and exists without complaint.

My suggestion was more like the following which uses mutate. For illustration, I will not use the name "Date" repeatedly and make new columns and uses an original_date as the source but note I am not saying the lubridate used will work as I do not see how it handles the initial part of the string containing an index number.

old_tibble <-  tibble(useless=original_date)
new_tibble <- mutate(old_tibble, useful = lubridate::dmy_hm(Date))

The above can be more compact by making the tibble directly in the first argument, and it can be done using old or new pipelines. 

The reason the suggested way worked is because it used the vectorized methods of base-R and I mentioned there was no reason you must use dplyr for this or many other things especially when it is simple.

Now if you wanted to make multiple new columns containing character or integer versions of the month and other parts of the date or even calculate what day of the week that full date was in that year, then mutate can be very useful as you can keep adding requests to make a new column using all old and new columns already specified.

Sometimes when code works, we don't look to see if it works inadvertently, LOL!


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Dr Eberhard W Lisse
Sent: Wednesday, July 13, 2022 5:49 PM
To: r-help at r-project.org
Subject: Re: [R] How to parse a really silly date with lubridate


Bui,

thanks, this what Avi suggested in an email to me as well and works.

It's so easy if you know it :-)-O

el

On 2022-07-13 23:40 , Rui Barradas wrote:
> Hello,
> 
> Are you looking for mutate? In the example below I haven't included 
> the filter, since the tibble only has 2 rows. But the date column is 
> coerced to an actual datetime class in place, without the need for NewDate.
> 
> suppressPackageStartupMessages({
>    library(tibble)
>    library(dplyr)
> })
> 
> DDATA <- tibble(Date = c('9. Jul 2022 at 11:39', '10. Jul 2022 at 
> 01:58'))
> 
> DDATA %>%
>    mutate(Date = lubridate::dmy_hm(Date)) #> # A tibble: 2 ? 1 #>   
> Date #>   <dttm> #> 1 2022-07-09 11:39:00 #> 2 2022-07-10 01:58:00
> 
> 
> Hope this helps,
> 
> Rui Barradas
[...]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From |vmcph@|| @end|ng |rom gm@||@com  Thu Jul 14 19:59:15 2022
From: |vmcph@|| @end|ng |rom gm@||@com (Ian McPhail)
Date: Thu, 14 Jul 2022 13:59:15 -0400
Subject: [R] mice: selecting small subset of variables to impute from
 dataset with many variables (> 2500)
Message-ID: <CAHF845G-5UkXJCQ3kNCO5hVwmpF3v6MKTx2PeE_JCcdzMjGF=w@mail.gmail.com>

Hello,

I am looking for some advice on how to select subsets of variables for
imputing when using the mice package.

>From Van Buuren's original mice paper, I see that selecting variables to be
'skipped' in an imputation can be written as:

ini <- mice(nhanes2, maxit = 0, print = FALSE)
pred <- ini$pred
pred[, "bmi"] <- 0
meth <- ini$meth
meth["bmi"] <- ""

With the last two lines specifying the the "bmi" variable gets skipped over
and not imputed.

And I have come across other examples, but all that I have seen lay out a
method of skipping variables where EVERY variable is named (as "bmi" is
named above). I am wondering if there is a reasonably easy way to select
out approximately 30 variables for imputation from a larger dataset with
around 2500 variables, without having to name all 2450+ other variables.

Thank you,

Ian

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Thu Jul 14 20:09:52 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 14 Jul 2022 11:09:52 -0700
Subject: [R] mice: selecting small subset of variables to impute from
 dataset with many variables (> 2500)
In-Reply-To: <CAHF845G-5UkXJCQ3kNCO5hVwmpF3v6MKTx2PeE_JCcdzMjGF=w@mail.gmail.com>
References: <CAHF845G-5UkXJCQ3kNCO5hVwmpF3v6MKTx2PeE_JCcdzMjGF=w@mail.gmail.com>
Message-ID: <CAGxFJbTc2ukmo4JFw_wF1zgj3qRYrgfAjABU8o0ste8rYedNYw@mail.gmail.com>

If I understand your query correctly, you can use negative indexing to
omit variables. See ?'[' for details.

> dat <- data.frame (a = 1:3, b = letters[1:3], c = 4:6, d = letters[5:7])
> dat
  a b c d
1 1 a 4 e
2 2 b 5 f
3 3 c 6 g
> dat[,-c(2,4)]
  a c
1 1 4
2 2 5
3 3 6

Of course you have to know the numerical index of the columns you wish
to omit, but somethingh of the sort seems unavoidable in any case.

Cheers,
Bert

On Thu, Jul 14, 2022 at 11:00 AM Ian McPhail <ivmcphail at gmail.com> wrote:
>
> Hello,
>
> I am looking for some advice on how to select subsets of variables for
> imputing when using the mice package.
>
> From Van Buuren's original mice paper, I see that selecting variables to be
> 'skipped' in an imputation can be written as:
>
> ini <- mice(nhanes2, maxit = 0, print = FALSE)
> pred <- ini$pred
> pred[, "bmi"] <- 0
> meth <- ini$meth
> meth["bmi"] <- ""
>
> With the last two lines specifying the the "bmi" variable gets skipped over
> and not imputed.
>
> And I have come across other examples, but all that I have seen lay out a
> method of skipping variables where EVERY variable is named (as "bmi" is
> named above). I am wondering if there is a reasonably easy way to select
> out approximately 30 variables for imputation from a larger dataset with
> around 2500 variables, without having to name all 2450+ other variables.
>
> Thank you,
>
> Ian
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tebert @end|ng |rom u||@edu  Thu Jul 14 20:11:50 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Thu, 14 Jul 2022 18:11:50 +0000
Subject: [R] mice: selecting small subset of variables to impute from
 dataset with many variables (> 2500)
In-Reply-To: <CAGxFJbTc2ukmo4JFw_wF1zgj3qRYrgfAjABU8o0ste8rYedNYw@mail.gmail.com>
References: <CAHF845G-5UkXJCQ3kNCO5hVwmpF3v6MKTx2PeE_JCcdzMjGF=w@mail.gmail.com>
 <CAGxFJbTc2ukmo4JFw_wF1zgj3qRYrgfAjABU8o0ste8rYedNYw@mail.gmail.com>
Message-ID: <BN6PR2201MB15534DD9DF008530A1704467CF889@BN6PR2201MB1553.namprd22.prod.outlook.com>

Maybe this is too simple but could you use the select() function from dplyr?
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Bert Gunter
Sent: Thursday, July 14, 2022 2:10 PM
To: Ian McPhail <ivmcphail at gmail.com>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] mice: selecting small subset of variables to impute from dataset with many variables (> 2500)

[External Email]

If I understand your query correctly, you can use negative indexing to omit variables. See ?'[' for details.

> dat <- data.frame (a = 1:3, b = letters[1:3], c = 4:6, d = 
> letters[5:7]) dat
  a b c d
1 1 a 4 e
2 2 b 5 f
3 3 c 6 g
> dat[,-c(2,4)]
  a c
1 1 4
2 2 5
3 3 6

Of course you have to know the numerical index of the columns you wish to omit, but somethingh of the sort seems unavoidable in any case.

Cheers,
Bert

On Thu, Jul 14, 2022 at 11:00 AM Ian McPhail <ivmcphail at gmail.com> wrote:
>
> Hello,
>
> I am looking for some advice on how to select subsets of variables for 
> imputing when using the mice package.
>
> From Van Buuren's original mice paper, I see that selecting variables 
> to be 'skipped' in an imputation can be written as:
>
> ini <- mice(nhanes2, maxit = 0, print = FALSE) pred <- ini$pred pred[, 
> "bmi"] <- 0 meth <- ini$meth meth["bmi"] <- ""
>
> With the last two lines specifying the the "bmi" variable gets skipped 
> over and not imputed.
>
> And I have come across other examples, but all that I have seen lay 
> out a method of skipping variables where EVERY variable is named (as 
> "bmi" is named above). I am wondering if there is a reasonably easy 
> way to select out approximately 30 variables for imputation from a 
> larger dataset with around 2500 variables, without having to name all 2450+ other variables.
>
> Thank you,
>
> Ian
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=UxEz20f8LSF-iyVuq17UnoNVkEe6HoC3E6vHWssLjSBKtqLSrm7qs8v2e
> wcXchwc&s=ABj_L_b515lhH7RIgTmmjylyWxJCbRWvzZDkxUkGw90&e=
> PLEASE do read the posting guide 
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=UxEz20f8LSF-iyVuq17UnoNVkEe6HoC3E6vHWssLjSBKtqLSrm7qs8v2
> ewcXchwc&s=LiocKPLYgq5olAT6tqGjr2xOLwDWw55DRzhuq7gcF5A&e=
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=UxEz20f8LSF-iyVuq17UnoNVkEe6HoC3E6vHWssLjSBKtqLSrm7qs8v2ewcXchwc&s=ABj_L_b515lhH7RIgTmmjylyWxJCbRWvzZDkxUkGw90&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=UxEz20f8LSF-iyVuq17UnoNVkEe6HoC3E6vHWssLjSBKtqLSrm7qs8v2ewcXchwc&s=LiocKPLYgq5olAT6tqGjr2xOLwDWw55DRzhuq7gcF5A&e=
and provide commented, minimal, self-contained, reproducible code.


From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Jul 14 20:32:40 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 14 Jul 2022 18:32:40 +0000
Subject: [R] running a scraping code in parallel...
Message-ID: <PU4P216MB1568F6F3B4A02788130C11DCC8889@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear members,

please feel free to ignore this mail if you feel that it is not about Base R.

                          I have the following web scraping code ( i have 500 stocks to iterate over):
            getFirmsDates <- function() {
             rD <- RsDriver(browser="chrome")
             remDr <- rD$client

             { scrape for stock i }
             }

Will the following code work?

DATES <- mclapply(1:500, getFirmsDates, mc.cores = 48)

Basically, there must be 500 chrome instances and rD and remDr are same for all iterations. If not any suggestions on how to accomplish the task?

I am using RSelenium and rvest packages.

THanking you,
yours sincerely,
AKSHAY M KULKARNI

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Jul 14 20:49:56 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 14 Jul 2022 19:49:56 +0100
Subject: [R] mice: selecting small subset of variables to impute from
 dataset with many variables (> 2500)
In-Reply-To: <CAHF845G-5UkXJCQ3kNCO5hVwmpF3v6MKTx2PeE_JCcdzMjGF=w@mail.gmail.com>
References: <CAHF845G-5UkXJCQ3kNCO5hVwmpF3v6MKTx2PeE_JCcdzMjGF=w@mail.gmail.com>
Message-ID: <6641ef5e-94e3-4625-8108-30c24f179cf7@sapo.pt>

Hello,

You can use mice() argument predictorMatrix to tell mice() which 
variables/blocks are used when imputing which column. If the column 
vector is set to zeros, no column or block will used in its imputation.


library(mice)

predmat <- matrix(1L, ncol(nhanes2), ncol(nhanes2),
                   dimnames = list(names(nhanes2), names(nhanes2)))
diag(predmat) <- 0L
predmat[, "bmi"] <- 0L
predmat
#>     age bmi hyp chl
#> age   0   0   1   1
#> bmi   1   0   1   1
#> hyp   1   0   0   1
#> chl   1   0   1   0




Then use argument where to skip the variables you do not want imputed.
Note that this is not the same as not being imputed according to 
variables shown above as rownames of predmat.

The default of where is the matrix is.na(nhanes2) so make a copy of this 
matrix then set column "bmi" to FALSE. Then call mice().



predmat <- matrix(1L, ncol(nhanes2), ncol(nhanes2),
                   dimnames = list(names(nhanes2), names(nhanes2)))
diag(predmat) <- 0L
predmat[, "bmi"] <- 0L
predmat
#>     age bmi hyp chl
#> age   0   0   1   1
#> bmi   1   0   1   1
#> hyp   1   0   0   1
#> chl   1   0   1   0

not_bmi <- is.na(nhanes2)
not_bmi[, "bmi"] <- FALSE

ini_all <- mice(nhanes2, print = FALSE)
ini_bmi <- mice(nhanes2,
                 predictorMatrix = predmat,
                 where = not_bmi,
                 print = FALSE)


cmpl_all <- complete(ini_all)
head(cmpl_all)
#>     age  bmi hyp chl
#> 1 20-39 28.7  no 187
#> 2 40-59 22.7  no 187
#> 3 20-39 30.1  no 187
#> 4 60-99 27.5 yes 284
#> 5 20-39 20.4  no 113
#> 6 60-99 20.4  no 184
cmpl_bmi <- complete(ini_bmi)
head(cmpl_bmi)
#>     age  bmi hyp chl
#> 1 20-39   NA  no 187
#> 2 40-59 22.7  no 187
#> 3 20-39   NA  no 187
#> 4 60-99   NA yes 206
#> 5 20-39 20.4  no 113
#> 6 60-99   NA yes 184


Hope this helps,

Rui Barradas

?s 18:59 de 14/07/2022, Ian McPhail escreveu:
> Hello,
> 
> I am looking for some advice on how to select subsets of variables for
> imputing when using the mice package.
> 
>  From Van Buuren's original mice paper, I see that selecting variables to be
> 'skipped' in an imputation can be written as:
> 
> ini <- mice(nhanes2, maxit = 0, print = FALSE)
> pred <- ini$pred
> pred[, "bmi"] <- 0
> meth <- ini$meth
> meth["bmi"] <- ""
> 
> With the last two lines specifying the the "bmi" variable gets skipped over
> and not imputed.
> 
> And I have come across other examples, but all that I have seen lay out a
> method of skipping variables where EVERY variable is named (as "bmi" is
> named above). I am wondering if there is a reasonably easy way to select
> out approximately 30 variables for imputation from a larger dataset with
> around 2500 variables, without having to name all 2450+ other variables.
> 
> Thank you,
> 
> Ian
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @vi@e@gross m@iii@g oii gm@ii@com  Thu Jul 14 21:27:34 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Thu, 14 Jul 2022 15:27:34 -0400
Subject: [R] How to parse a really silly date with lubridate
References: <tami12$11b8$1@ciao.gmane.io>
 <c166bf12-124e-0477-8694-4bfa73399447@sapo.pt> <tamr12$14qr$1@ciao.gmane.io>
 <f34dae75-c5d0-6574-9cf1-aa43dbf0c2d6@sapo.pt> <tanek0$e5e$1@ciao.gmane.io> 
Message-ID: <000601d897b7$c53e8d90$4fbba8b0$@gmail.com>

Let me retract and modify my earlier comment based on getting more complete code.

This line of code remains silly if used stand-alone,

mutate(Date = lubridate::dmy_hm(Date))

But it was used in a pipeline as in

Read_delim(...) %>%
mutate(Date = lubridate::dmy_hm(Date)) %>%
...

As such, it makes perfect sense ONCE GIVEN A CONTEXT.

The contents of a file are read into a data.frame which is the implicit first argument to mutate. There is this a second argument that does an appropriate conversion to the column named Date within the data.frame.

Date = lubridate::dmy_hm(Date)

So it is n appropriate use and highlights one reason some do not like pipelines. When using one, you need to KNOW it so you are aware any arguments are after a first argument not shown.



-----Original Message-----
From: avi.e.gross at gmail.com <avi.e.gross at gmail.com> 
Sent: Thursday, July 14, 2022 12:43 PM
To: 'r-help at r-project.org' <r-help at r-project.org>
Subject: RE: [R] How to parse a really silly date with lubridate

To be clear, I take no credit for the rather extraordinary function cll shown below:

mutate(Date = lubridate::dmy_hm(Date))

I would pretty much never have constructed such an interesting and rather unnecessary line of code.

ALL the work is done within the parentheses:

Date = lubridate::dmy_hm(Date))

The above creates a tibble and assigns it to the name of Date. but first it takes a vector called Date containing text and uses a function to parse it and return a form more suitable to use as a date.

So what does mutate() do when it is called with a tibble and asked to do nothing? I mean it sees something more like this:

Mutate(.data=Date)

What is missing are the usual assignment statements to modify existing columns or make new ones. So it does nothing and exists without complaint.

My suggestion was more like the following which uses mutate. For illustration, I will not use the name "Date" repeatedly and make new columns and uses an original_date as the source but note I am not saying the lubridate used will work as I do not see how it handles the initial part of the string containing an index number.

old_tibble <-  tibble(useless=original_date) new_tibble <- mutate(old_tibble, useful = lubridate::dmy_hm(Date))

The above can be more compact by making the tibble directly in the first argument, and it can be done using old or new pipelines. 

The reason the suggested way worked is because it used the vectorized methods of base-R and I mentioned there was no reason you must use dplyr for this or many other things especially when it is simple.

Now if you wanted to make multiple new columns containing character or integer versions of the month and other parts of the date or even calculate what day of the week that full date was in that year, then mutate can be very useful as you can keep adding requests to make a new column using all old and new columns already specified.

Sometimes when code works, we don't look to see if it works inadvertently, LOL!


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Dr Eberhard W Lisse
Sent: Wednesday, July 13, 2022 5:49 PM
To: r-help at r-project.org
Subject: Re: [R] How to parse a really silly date with lubridate


Bui,

thanks, this what Avi suggested in an email to me as well and works.

It's so easy if you know it :-)-O

el

On 2022-07-13 23:40 , Rui Barradas wrote:
> Hello,
> 
> Are you looking for mutate? In the example below I haven't included 
> the filter, since the tibble only has 2 rows. But the date column is 
> coerced to an actual datetime class in place, without the need for NewDate.
> 
> suppressPackageStartupMessages({
>    library(tibble)
>    library(dplyr)
> })
> 
> DDATA <- tibble(Date = c('9. Jul 2022 at 11:39', '10. Jul 2022 at
> 01:58'))
> 
> DDATA %>%
>    mutate(Date = lubridate::dmy_hm(Date)) #> # A tibble: 2 ? 1 #> Date 
> #>   <dttm> #> 1 2022-07-09 11:39:00 #> 2 2022-07-10 01:58:00
> 
> 
> Hope this helps,
> 
> Rui Barradas
[...]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From mzch|@ht| @end|ng |rom eco@q@u@edu@pk  Thu Jul 14 21:39:32 2022
From: mzch|@ht| @end|ng |rom eco@q@u@edu@pk (Muhammad Zubair Chishti)
Date: Fri, 15 Jul 2022 00:39:32 +0500
Subject: [R] Kindly Guide
Message-ID: <CAMfKi3L=eJzXK0Vkbj-iK-YRs76uTLr3MX4w8xm9mw1uR4Di5A@mail.gmail.com>

Dear Experts,
I have obtained the results through applying Time Varying VAR model in R.
Now I want to save my results in a excel sheet. Kindly guide me how to
create a excel or excel table. Although I have tried the following code but
I face error.
Write.xlsx(x, "table name.xlsx")

Kindly guide.

Regards
Chishti

	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Thu Jul 14 21:51:02 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Thu, 14 Jul 2022 19:51:02 +0000
Subject: [R] Kindly Guide
In-Reply-To: <CAMfKi3L=eJzXK0Vkbj-iK-YRs76uTLr3MX4w8xm9mw1uR4Di5A@mail.gmail.com>
References: <CAMfKi3L=eJzXK0Vkbj-iK-YRs76uTLr3MX4w8xm9mw1uR4Di5A@mail.gmail.com>
Message-ID: <BN6PR2201MB1553CE1FE59B2A9ED17D8FE3CF889@BN6PR2201MB1553.namprd22.prod.outlook.com>

I went to google and typed in "save excel in r" and got a few hits:
http://www.sthda.com/english/wiki/writing-data-from-r-to-excel-files-xls-xlsx
This is a good site, but there are others:
https://www.statology.org/r-export-to-excel/
https://www.geeksforgeeks.org/how-to-export-a-dataframe-to-excel-file-in-r/

and many more.


Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Muhammad Zubair Chishti
Sent: Thursday, July 14, 2022 3:40 PM
To: r-help at r-project.org
Subject: [R] Kindly Guide

[External Email]

Dear Experts,
I have obtained the results through applying Time Varying VAR model in R.
Now I want to save my results in a excel sheet. Kindly guide me how to create a excel or excel table. Although I have tried the following code but I face error.
Write.xlsx(x, "table name.xlsx")

Kindly guide.

Regards
Chishti

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=UJkYmZ1qWcJJ0LfLyhLuxeXL_y05ktVLqPyzmV69ar6sOhNRtsVRKTuwDCus8Mlb&s=0nYlal67UZo9DLRtun3VyrocJ90_iskyiDTI19r8CkQ&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=UJkYmZ1qWcJJ0LfLyhLuxeXL_y05ktVLqPyzmV69ar6sOhNRtsVRKTuwDCus8Mlb&s=b9wQQfFdvLd-7HavGZlIhz3vR587ZiZye-3rVJJ9i9I&e=
and provide commented, minimal, self-contained, reproducible code.


From oub @end|ng |rom m@t@ucm@e@  Thu Jul 14 14:58:17 2022
From: oub @end|ng |rom m@t@ucm@e@ (Uwe Brauer)
Date: Thu, 14 Jul 2022 14:58:17 +0200
Subject: [R] the quantile function and problems.
Message-ID: <87wncf6ec6.fsf@mat.ucm.es>


Hi 

I am very acquainted with R. I use it occasionally via the org-babel library of GNU emacs.

I wanted to check the first, second and third quartiles of the scientific science index JCR
https://support.clarivate.com/ScientificandAcademicResearch/s/article/Journal-Citation-Reports-Quartile-rankings-and-other-metrics?language=en_U
S 
Its criterion is 
#+begin_src 
| Quartil | range             |                                       |
| ---------+------------------+---------------------------------------|
| Q1      | 0.0 < Z \leq 0.25 | Highest ranked journals in a category |
| Q2      | 0.25 < Z \leq 0.5 |                                       |
| Q3      | 0.5 < Z \leq 0.75 |                                       |
| Q4      | 0.75 < Z          | Lowest ranked journals in a category  |
#+end_src

Z=(X/Y)

Where X is the journal rank in category and Y is the number of journals in the category.

Now I have a list of 267 journals.

What turns me crazy is that the way R, matlab and the JCR calculate the quartiles gives different results.

Here is a table 
#+begin_matlab :exports both :eval never-export :results output latex
#+RESULTS:
| quartil-limit (last member) |    | floor_Rlang | jcr | jcr_check | floor_check |
|-----------------------------+----+-------------+-----+-----------+-------------|
|                        67.5 | Q1 |          67 |  66 |    0.2472 |      0.2509 |
|                         134 | Q2 |         134 | 133 |    0.4981 |      0.5019 |
|                       200.5 | Q3 |         200 | 200 |    0.7491 |      0.7491 |
|                         267 |    |         267 | 267 |         1 |           1 |
#+TBLFM: $5=$4/267::$6=$3/267
#+end_matlab

I calculated using R (I don't provide the vector from 1 to 267)

#+begin_src R :colnames t :var t1=jcr22
  quantile(t1$Data,c(1/4,1/2,3/4,1))
#+end_src
#+begin_src 
#+RESULTS:
|     x |
|-------|
|  67.5 |
|   134 |
| 200.5 |
|   267 |
#+end_src


So you see the problem with Q1 and Q2.

On top of that matlab gives

#+begin_src matlab :exports results :eval never-export :results output latex
format short
x=1:267;
q1 = quantile(x,1/4);
q2 = quantile(x,1/2);
q3 = quantile(x,3/4);
Q=[q1; q2; q3];
sprintf('|%g|   \n', Q)
#+end_src

#+RESULTS:
#+begin_export latex
|67.25|   
|134|   
|200.75|   
#+end_export

Which is also slightly different from R.

Can anybody enlighten me please?
Thanks and regards 

Uwe Brauer 

-- 
I strongly condemn Putin's war of aggression against the Ukraine.
I support to deliver weapons to Ukraine's military. 
I support the ban of Russia from SWIFT.
I support the EU membership of the Ukraine.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Jul 14 23:50:40 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 14 Jul 2022 22:50:40 +0100
Subject: [R] Kindly Guide
In-Reply-To: <CAMfKi3L=eJzXK0Vkbj-iK-YRs76uTLr3MX4w8xm9mw1uR4Di5A@mail.gmail.com>
References: <CAMfKi3L=eJzXK0Vkbj-iK-YRs76uTLr3MX4w8xm9mw1uR4Di5A@mail.gmail.com>
Message-ID: <915e4412-8e18-f871-43dd-94da2d24a253@sapo.pt>

Hello,

Inline.

?s 20:39 de 14/07/2022, Muhammad Zubair Chishti escreveu:
> Dear Experts,
> I have obtained the results through applying Time Varying VAR model in R.

Can you post a minimal example of the data and code used?
If you use functions from contributed packages please start the example 
with calls to library() in order to load the necessary packages.

Hope this helps,

Rui Barradas

> Now I want to save my results in a excel sheet. Kindly guide me how to
> create a excel or excel table. Although I have tried the following code but
> I face error.
> Write.xlsx(x, "table name.xlsx")
> 
> Kindly guide.
> 
> Regards
> Chishti
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Fri Jul 15 00:22:41 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 14 Jul 2022 15:22:41 -0700
Subject: [R] the quantile function and problems.
In-Reply-To: <87wncf6ec6.fsf@mat.ucm.es>
References: <87wncf6ec6.fsf@mat.ucm.es>
Message-ID: <CAGxFJbTaG0PD_itn_C1zZEL9hF1nToG9f61KxZfKYMjih_OCqA@mail.gmail.com>

Read ?quantile carefully, please (and any references therein that you
may wish to consult).

You are estimating a continuous function by a discrete finite step
function, and as the Help page (and further references) explains,
there are many ways to do this.

Bert


On Thu, Jul 14, 2022 at 2:33 PM Uwe Brauer <oub at mat.ucm.es> wrote:
>
>
> Hi
>
> I am very acquainted with R. I use it occasionally via the org-babel library of GNU emacs.
>
> I wanted to check the first, second and third quartiles of the scientific science index JCR
> https://support.clarivate.com/ScientificandAcademicResearch/s/article/Journal-Citation-Reports-Quartile-rankings-and-other-metrics?language=en_U
> S
> Its criterion is
> #+begin_src
> | Quartil | range             |                                       |
> | ---------+------------------+---------------------------------------|
> | Q1      | 0.0 < Z \leq 0.25 | Highest ranked journals in a category |
> | Q2      | 0.25 < Z \leq 0.5 |                                       |
> | Q3      | 0.5 < Z \leq 0.75 |                                       |
> | Q4      | 0.75 < Z          | Lowest ranked journals in a category  |
> #+end_src
>
> Z=(X/Y)
>
> Where X is the journal rank in category and Y is the number of journals in the category.
>
> Now I have a list of 267 journals.
>
> What turns me crazy is that the way R, matlab and the JCR calculate the quartiles gives different results.
>
> Here is a table
> #+begin_matlab :exports both :eval never-export :results output latex
> #+RESULTS:
> | quartil-limit (last member) |    | floor_Rlang | jcr | jcr_check | floor_check |
> |-----------------------------+----+-------------+-----+-----------+-------------|
> |                        67.5 | Q1 |          67 |  66 |    0.2472 |      0.2509 |
> |                         134 | Q2 |         134 | 133 |    0.4981 |      0.5019 |
> |                       200.5 | Q3 |         200 | 200 |    0.7491 |      0.7491 |
> |                         267 |    |         267 | 267 |         1 |           1 |
> #+TBLFM: $5=$4/267::$6=$3/267
> #+end_matlab
>
> I calculated using R (I don't provide the vector from 1 to 267)
>
> #+begin_src R :colnames t :var t1=jcr22
>   quantile(t1$Data,c(1/4,1/2,3/4,1))
> #+end_src
> #+begin_src
> #+RESULTS:
> |     x |
> |-------|
> |  67.5 |
> |   134 |
> | 200.5 |
> |   267 |
> #+end_src
>
>
> So you see the problem with Q1 and Q2.
>
> On top of that matlab gives
>
> #+begin_src matlab :exports results :eval never-export :results output latex
> format short
> x=1:267;
> q1 = quantile(x,1/4);
> q2 = quantile(x,1/2);
> q3 = quantile(x,3/4);
> Q=[q1; q2; q3];
> sprintf('|%g|   \n', Q)
> #+end_src
>
> #+RESULTS:
> #+begin_export latex
> |67.25|
> |134|
> |200.75|
> #+end_export
>
> Which is also slightly different from R.
>
> Can anybody enlighten me please?
> Thanks and regards
>
> Uwe Brauer
>
> --
> I strongly condemn Putin's war of aggression against the Ukraine.
> I support to deliver weapons to Ukraine's military.
> I support the ban of Russia from SWIFT.
> I support the EU membership of the Ukraine.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @vi@e@gross m@iii@g oii gm@ii@com  Fri Jul 15 03:14:44 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Thu, 14 Jul 2022 21:14:44 -0400
Subject: [R] Kindly Guide
In-Reply-To: <BN6PR2201MB1553CE1FE59B2A9ED17D8FE3CF889@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAMfKi3L=eJzXK0Vkbj-iK-YRs76uTLr3MX4w8xm9mw1uR4Di5A@mail.gmail.com>
 <BN6PR2201MB1553CE1FE59B2A9ED17D8FE3CF889@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <010a01d897e8$45277410$cf765c30$@gmail.com>

We all face  error but the difference is whether we tell us what the error
message says or assume they will know magically.

Sarcasm aside, I hope the upper case "W" was not in the code they claim they
used:

Write.xlsx(x, "table name.xlsx")

Otherwise, I have a series of dumb questions before I can be of help.

- Are they using the "xslx" package or something else?
- Did they ever install it on that machine with install.packages("xlsx")
successfully as well as dependencies.
- Did they run this command in this session before trying it:
library("xlsx")
- Do they have a variable named x with suitable content or did they read the
manual page which mentions an x=arg, file=arg, sheetName=arg
- If they have a variable actually named x, is it of class data.frame
(perhaps also a tibble or data.table) and it is not a data.frame, either
make it into one (as in if it is something like a vector or list or matrix
if those are not automatically handled) or USE SOME OTHER FUNCTION that
handles it.
- Do they have write permission on the current folder or enough empty space
etc.
- Are you trying to append to an existing file or making a new one?
- And the big question. What was the error message and might that mean
revisiting one of the above that has not been checked already?

Once that works, you need to think carefully about whether to accept the
default sheetName or specify another or accept where it puts the data. Some
such functions allow you to specify a starting cell within the .XSLX file.

And, just FYI, in many cases you do not need to do any of this. You can save
a data.frame into a .CSV file and EXCEL can happily open that and display it
and, if you want, save it again as one of many formats including .XSLX ...

The above is not exhaustive but meant to point out that an unspecified error
(or merely the absence of an output file) is not enough to diagnose
anything.  I have had people who copied perfectly good lines of code
precisely then neglected to hit RETURN and asked why it did not work. Hard
to even imagine diagnosing that remotely without asking really silly
questions like do you know how to get the program to know when you are
finished.

I apologize if I am unkindly guiding.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ebert,Timothy Aaron
Sent: Thursday, July 14, 2022 3:51 PM
To: Muhammad Zubair Chishti <mzchishti at eco.qau.edu.pk>; r-help at r-project.org
Subject: Re: [R] Kindly Guide

I went to google and typed in "save excel in r" and got a few hits:
http://www.sthda.com/english/wiki/writing-data-from-r-to-excel-files-xls-xls
x
This is a good site, but there are others:
https://www.statology.org/r-export-to-excel/
https://www.geeksforgeeks.org/how-to-export-a-dataframe-to-excel-file-in-r/

and many more.


Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Muhammad Zubair
Chishti
Sent: Thursday, July 14, 2022 3:40 PM
To: r-help at r-project.org
Subject: [R] Kindly Guide

[External Email]

Dear Experts,
I have obtained the results through applying Time Varying VAR model in R.
Now I want to save my results in a excel sheet. Kindly guide me how to
create a excel or excel table. Although I have tried the following code but
I face error.
Write.xlsx(x, "table name.xlsx")

Kindly guide.

Regards
Chishti

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_li
stinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m
=UJkYmZ1qWcJJ0LfLyhLuxeXL_y05ktVLqPyzmV69ar6sOhNRtsVRKTuwDCus8Mlb&s=0nYlal67
UZo9DLRtun3VyrocJ90_iskyiDTI19r8CkQ&e=
PLEASE do read the posting guide
https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_post
ing-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&
m=UJkYmZ1qWcJJ0LfLyhLuxeXL_y05ktVLqPyzmV69ar6sOhNRtsVRKTuwDCus8Mlb&s=b9wQQfF
dvLd-7HavGZlIhz3vR587ZiZye-3rVJJ9i9I&e=
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From @vi@e@gross m@iii@g oii gm@ii@com  Fri Jul 15 03:38:08 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Thu, 14 Jul 2022 21:38:08 -0400
Subject: [R] mice: selecting small subset of variables to impute from
 dataset with many variables (> 2500)
In-Reply-To: <BN6PR2201MB15534DD9DF008530A1704467CF889@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAHF845G-5UkXJCQ3kNCO5hVwmpF3v6MKTx2PeE_JCcdzMjGF=w@mail.gmail.com>
 <CAGxFJbTc2ukmo4JFw_wF1zgj3qRYrgfAjABU8o0ste8rYedNYw@mail.gmail.com>
 <BN6PR2201MB15534DD9DF008530A1704467CF889@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <011301d897eb$89b85f60$9d291e20$@gmail.com>

Tim,

Your reply is reasonable if you want to read in EVERYTHING and use various
nice features of the select() function in the dplyr package of the tidyverse
that let you exclude a bunch of columns based on names starting or ending or
containing various characters or not being of type integer and so on. 

But another category wants to skip creating some columns in the first place.
Many reader functions that take in data from something like a .CSV file will
allow you to effectively ignore some of the columns of data and thus
hopefully cut down on some overhead.

I assume most of us have no real experience with the package called "mice"
and who is willing to read to page 72 or so in this document: 
https://cran.r-project.org/web/packages/mice/mice.pdf

Anywho, the mice() function this person wants to use has arguments meant to
control what is brought in and stored in whatever internal format as in not
taking some rows. A cursory glance suggests no way to suppress columns other
than not including them before calling the function as it does not read the
data from a file and expects either a data.frame or a matrix.

So your answer is valid. The questioner can use any method they wish to
adjust the initial data.frame and create a partial copy to use. If they want
a small subset of 2500+ columns (and who wouldn't) then it may be easiest to
simply name them in base R or select as in:

 New.df <- Old.df(, c("col36", "col89", "hike"))

On the other hand, if they merely want to exclude lots of columns that have
something in common, yes, select() allows things like:

New.df <- Select(Old.df, -ends_with(c("extra", "comment"))

The tidyverse keeps being rewritten so some new ways may be replacing old,
but there are variants like select_if() that allow arbitrary functions to
decide what columns to include/exclude  such as based on what type they
contain

So the key is to trick before calling the function but leave in everything
needed.

Only the one asking the question knows what all the columns mean and what
rhyme or reasons decides which to keep or exclude. A more specific question
may get a more specific answer.


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ebert,Timothy Aaron
Sent: Thursday, July 14, 2022 2:12 PM
To: Bert Gunter <bgunter.4567 at gmail.com>; Ian McPhail <ivmcphail at gmail.com>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] mice: selecting small subset of variables to impute from
dataset with many variables (> 2500)

Maybe this is too simple but could you use the select() function from dplyr?
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Bert Gunter
Sent: Thursday, July 14, 2022 2:10 PM
To: Ian McPhail <ivmcphail at gmail.com>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] mice: selecting small subset of variables to impute from
dataset with many variables (> 2500)

[External Email]

If I understand your query correctly, you can use negative indexing to omit
variables. See ?'[' for details.

> dat <- data.frame (a = 1:3, b = letters[1:3], c = 4:6, d =
> letters[5:7]) dat
  a b c d
1 1 a 4 e
2 2 b 5 f
3 3 c 6 g
> dat[,-c(2,4)]
  a c
1 1 4
2 2 5
3 3 6

Of course you have to know the numerical index of the columns you wish to
omit, but somethingh of the sort seems unavoidable in any case.

Cheers,
Bert

On Thu, Jul 14, 2022 at 11:00 AM Ian McPhail <ivmcphail at gmail.com> wrote:
>
> Hello,
>
> I am looking for some advice on how to select subsets of variables for 
> imputing when using the mice package.
>
> From Van Buuren's original mice paper, I see that selecting variables 
> to be 'skipped' in an imputation can be written as:
>
> ini <- mice(nhanes2, maxit = 0, print = FALSE) pred <- ini$pred pred[, 
> "bmi"] <- 0 meth <- ini$meth meth["bmi"] <- ""
>
> With the last two lines specifying the the "bmi" variable gets skipped 
> over and not imputed.
>
> And I have come across other examples, but all that I have seen lay 
> out a method of skipping variables where EVERY variable is named (as 
> "bmi" is named above). I am wondering if there is a reasonably easy 
> way to select out approximately 30 variables for imputation from a 
> larger dataset with around 2500 variables, without having to name all
2450+ other variables.
>
> Thank you,
>
> Ian
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=UxEz20f8LSF-iyVuq17UnoNVkEe6HoC3E6vHWssLjSBKtqLSrm7qs8v2e
> wcXchwc&s=ABj_L_b515lhH7RIgTmmjylyWxJCbRWvzZDkxUkGw90&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=UxEz20f8LSF-iyVuq17UnoNVkEe6HoC3E6vHWssLjSBKtqLSrm7qs8v2
> ewcXchwc&s=LiocKPLYgq5olAT6tqGjr2xOLwDWw55DRzhuq7gcF5A&e=
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_li
stinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m
=UxEz20f8LSF-iyVuq17UnoNVkEe6HoC3E6vHWssLjSBKtqLSrm7qs8v2ewcXchwc&s=ABj_L_b5
15lhH7RIgTmmjylyWxJCbRWvzZDkxUkGw90&e=
PLEASE do read the posting guide
https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_post
ing-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&
m=UxEz20f8LSF-iyVuq17UnoNVkEe6HoC3E6vHWssLjSBKtqLSrm7qs8v2ewcXchwc&s=LiocKPL
Ygq5olAT6tqGjr2xOLwDWw55DRzhuq7gcF5A&e=
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From kry|ov@r00t @end|ng |rom gm@||@com  Fri Jul 15 08:20:42 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Fri, 15 Jul 2022 09:20:42 +0300
Subject: [R] the quantile function and problems.
In-Reply-To: <87wncf6ec6.fsf@mat.ucm.es>
References: <87wncf6ec6.fsf@mat.ucm.es>
Message-ID: <20220715092042.4e8189b7@Tarkus>

? Thu, 14 Jul 2022 14:58:17 +0200
Uwe Brauer <oub at mat.ucm.es> ?????:

> What turns me crazy is that the way R, matlab and the JCR calculate
> the quartiles gives different results.

R by itself can give up to 9 slightly different results:

sapply(1:9, function(type) quantile(1:267, 1:3/4, type = type))
#     [,1] [,2] [,3]   [,4]   [,5] [,6]  [,7]      [,8]     [,9]
# 25%   67   67   67  66.75  67.25   67  67.5  67.16667  67.1875
# 50%  134  134  134 133.50 134.00  134 134.0 134.00000 134.0000
# 75%  201  201  200 200.25 200.75  201 200.5 200.83333 200.8125

Choose the ones that fit your ideas of quantile best. See ?quantile for
more info.

-- 
Best regards,
Ivan


From tebert @end|ng |rom u||@edu  Fri Jul 15 14:44:22 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Fri, 15 Jul 2022 12:44:22 +0000
Subject: [R] mice: selecting small subset of variables to impute from
 dataset with many variables (> 2500)
In-Reply-To: <011301d897eb$89b85f60$9d291e20$@gmail.com>
References: <CAHF845G-5UkXJCQ3kNCO5hVwmpF3v6MKTx2PeE_JCcdzMjGF=w@mail.gmail.com>
 <CAGxFJbTc2ukmo4JFw_wF1zgj3qRYrgfAjABU8o0ste8rYedNYw@mail.gmail.com>
 <BN6PR2201MB15534DD9DF008530A1704467CF889@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <011301d897eb$89b85f60$9d291e20$@gmail.com>
Message-ID: <BN6PR2201MB155318FC0162828DCFE64CDACF8B9@BN6PR2201MB1553.namprd22.prod.outlook.com>

Hi Avi,
   Thank you for your reply. I had not considered this possibility.
Tim 

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of avi.e.gross at gmail.com
Sent: Thursday, July 14, 2022 9:38 PM
Cc: 'R-help' <r-help at r-project.org>
Subject: Re: [R] mice: selecting small subset of variables to impute from dataset with many variables (> 2500)

[External Email]

Tim,

Your reply is reasonable if you want to read in EVERYTHING and use various nice features of the select() function in the dplyr package of the tidyverse that let you exclude a bunch of columns based on names starting or ending or containing various characters or not being of type integer and so on.

But another category wants to skip creating some columns in the first place.
Many reader functions that take in data from something like a .CSV file will allow you to effectively ignore some of the columns of data and thus hopefully cut down on some overhead.

I assume most of us have no real experience with the package called "mice"
and who is willing to read to page 72 or so in this document:
https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dproject.org_web_packages_mice_mice.pdf&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=psSXddrLcHW_8Z0VJKPJxvJuaZU8I4WKwySj5LtvA0BlD-G_JUdDzhIpZARfoTfp&s=msOsrEeTuS5_sLcvCR2FvF3kC7_1rYCebqZiKOji5iY&e=

Anywho, the mice() function this person wants to use has arguments meant to control what is brought in and stored in whatever internal format as in not taking some rows. A cursory glance suggests no way to suppress columns other than not including them before calling the function as it does not read the data from a file and expects either a data.frame or a matrix.

So your answer is valid. The questioner can use any method they wish to adjust the initial data.frame and create a partial copy to use. If they want a small subset of 2500+ columns (and who wouldn't) then it may be easiest to simply name them in base R or select as in:

 New.df <- Old.df(, c("col36", "col89", "hike"))

On the other hand, if they merely want to exclude lots of columns that have something in common, yes, select() allows things like:

New.df <- Select(Old.df, -ends_with(c("extra", "comment"))

The tidyverse keeps being rewritten so some new ways may be replacing old, but there are variants like select_if() that allow arbitrary functions to decide what columns to include/exclude  such as based on what type they contain

So the key is to trick before calling the function but leave in everything needed.

Only the one asking the question knows what all the columns mean and what rhyme or reasons decides which to keep or exclude. A more specific question may get a more specific answer.


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ebert,Timothy Aaron
Sent: Thursday, July 14, 2022 2:12 PM
To: Bert Gunter <bgunter.4567 at gmail.com>; Ian McPhail <ivmcphail at gmail.com>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] mice: selecting small subset of variables to impute from dataset with many variables (> 2500)

Maybe this is too simple but could you use the select() function from dplyr?
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Bert Gunter
Sent: Thursday, July 14, 2022 2:10 PM
To: Ian McPhail <ivmcphail at gmail.com>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] mice: selecting small subset of variables to impute from dataset with many variables (> 2500)

[External Email]

If I understand your query correctly, you can use negative indexing to omit variables. See ?'[' for details.

> dat <- data.frame (a = 1:3, b = letters[1:3], c = 4:6, d =
> letters[5:7]) dat
  a b c d
1 1 a 4 e
2 2 b 5 f
3 3 c 6 g
> dat[,-c(2,4)]
  a c
1 1 4
2 2 5
3 3 6

Of course you have to know the numerical index of the columns you wish to omit, but somethingh of the sort seems unavoidable in any case.

Cheers,
Bert

On Thu, Jul 14, 2022 at 11:00 AM Ian McPhail <ivmcphail at gmail.com> wrote:
>
> Hello,
>
> I am looking for some advice on how to select subsets of variables for 
> imputing when using the mice package.
>
> From Van Buuren's original mice paper, I see that selecting variables 
> to be 'skipped' in an imputation can be written as:
>
> ini <- mice(nhanes2, maxit = 0, print = FALSE) pred <- ini$pred pred[, 
> "bmi"] <- 0 meth <- ini$meth meth["bmi"] <- ""
>
> With the last two lines specifying the the "bmi" variable gets skipped 
> over and not imputed.
>
> And I have come across other examples, but all that I have seen lay 
> out a method of skipping variables where EVERY variable is named (as 
> "bmi" is named above). I am wondering if there is a reasonably easy 
> way to select out approximately 30 variables for imputation from a 
> larger dataset with around 2500 variables, without having to name all
2450+ other variables.
>
> Thank you,
>
> Ian
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAs
> Rzsn7AkP-g&m=UxEz20f8LSF-iyVuq17UnoNVkEe6HoC3E6vHWssLjSBKtqLSrm7qs8v2e
> wcXchwc&s=ABj_L_b515lhH7RIgTmmjylyWxJCbRWvzZDkxUkGw90&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeA
> sRzsn7AkP-g&m=UxEz20f8LSF-iyVuq17UnoNVkEe6HoC3E6vHWssLjSBKtqLSrm7qs8v2
> ewcXchwc&s=LiocKPLYgq5olAT6tqGjr2xOLwDWw55DRzhuq7gcF5A&e=
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_li
stinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m
=UxEz20f8LSF-iyVuq17UnoNVkEe6HoC3E6vHWssLjSBKtqLSrm7qs8v2ewcXchwc&s=ABj_L_b5
15lhH7RIgTmmjylyWxJCbRWvzZDkxUkGw90&e=
PLEASE do read the posting guide
https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_post
ing-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&
m=UxEz20f8LSF-iyVuq17UnoNVkEe6HoC3E6vHWssLjSBKtqLSrm7qs8v2ewcXchwc&s=LiocKPL
Ygq5olAT6tqGjr2xOLwDWw55DRzhuq7gcF5A&e=
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=psSXddrLcHW_8Z0VJKPJxvJuaZU8I4WKwySj5LtvA0BlD-G_JUdDzhIpZARfoTfp&s=jVxQsq0xD_WRFP8rsnPCCM2wXEiyQfz2-boRuanxLu0&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=psSXddrLcHW_8Z0VJKPJxvJuaZU8I4WKwySj5LtvA0BlD-G_JUdDzhIpZARfoTfp&s=ZQkPsKDNoHIsIA8nLAK3MaKRcdB08PHd9ECPnrw23Rk&e=
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=psSXddrLcHW_8Z0VJKPJxvJuaZU8I4WKwySj5LtvA0BlD-G_JUdDzhIpZARfoTfp&s=jVxQsq0xD_WRFP8rsnPCCM2wXEiyQfz2-boRuanxLu0&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=psSXddrLcHW_8Z0VJKPJxvJuaZU8I4WKwySj5LtvA0BlD-G_JUdDzhIpZARfoTfp&s=ZQkPsKDNoHIsIA8nLAK3MaKRcdB08PHd9ECPnrw23Rk&e=
and provide commented, minimal, self-contained, reproducible code.


From rb@er @end|ng |rom @t@u@edu  Fri Jul 15 15:34:36 2022
From: rb@er @end|ng |rom @t@u@edu (Robert Baer)
Date: Fri, 15 Jul 2022 08:34:36 -0500
Subject: [R] Thanks to this list ....
In-Reply-To: <20220714222010.17fa67a1@rolf-Latitude-E7470>
References: <20220714222010.17fa67a1@rolf-Latitude-E7470>
Message-ID: <7f0dbe00-0983-0d04-9235-9ae869588244@atsu.edu>

And once again you have shown? that this list has an important side 
effect of helping us to help ourselves ...? :-)


On 7/14/2022 5:20 AM, Rolf Turner wrote:
> I set out to appeal to this list for help with disentangling
> a bewildering anomaly that was produced by some dynamically loaded
> Fortran code.
>
> In composing an email to explain the nature of the anomaly, I *FINALLY*
> spotted the loony!  I had an expression in a nested do loop:
>
>      j = npro + (r-1)*nvym1 + s
>
> where r and s were the indices of two of the nested loops (explicitly
> declared to be integer at the start of the subroutine in question).
>
> When I copied the foregoing expression into the email I at last noticed
> that the variable "nvym1" *should* have been "nyvm1".  See the subtle
> difference?  :-)
>
> The variable nvym1 was never initialised, so it took on strange values
> plucked out of RAM, I guess.  Whence the anomaly.  Once I corrected that
> trivial typo, things were OK.
>
> Thanks everybody!!! :-)
>
> cheers,
>
> Rolf Turner
>
> P.S. What I can't figure out (and won't waste any time trying) is
> why the first four values of j were as they should have been, and things
> did not go to hell in a handcart until the code got to the 5th value
> of j.  I guess the computer gods were just amusing themselves at my
> expense.
>
> R. T.
>
-- 
__________________________________________________
Robert W. Baer, Ph.D.
Professor of Physiolgy
Kirksville College of Osteopathic Medicine
A.T. Still University of Heallth Sciences
800 W. Jefferson St.
Kirksville, MO 63501
P: 660-626-2322


  

              

The ATSU Mission
A.T. Still University of Health Sciences serves as a learning-centered university dedicated to preparing highly competent professionals through innovative academic programs with a commitment to continue its osteopathic heritage and focus on whole person healthcare, scholarship, community health, interprofessional education, diversity, and underserved populations.

Proud recipient of INSIGHT Into Diversity?s 2017-19 Higher Education Excellence in Diversity Awards.


From tebert @end|ng |rom u||@edu  Fri Jul 15 17:54:31 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Fri, 15 Jul 2022 15:54:31 +0000
Subject: [R] the quantile function and problems.
In-Reply-To: <20220715092042.4e8189b7@Tarkus>
References: <87wncf6ec6.fsf@mat.ucm.es> <20220715092042.4e8189b7@Tarkus>
Message-ID: <BN6PR2201MB1553CCCB355489E7721B2BF4CF8B9@BN6PR2201MB1553.namprd22.prod.outlook.com>

Does the choice in how the quantile is calculated influence the validity of a statistical test for differences in the median?
Regards,
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ivan Krylov
Sent: Friday, July 15, 2022 2:21 AM
To: Uwe Brauer <oub at mat.ucm.es>
Cc: r-help at r-project.org
Subject: Re: [R] the quantile function and problems.

[External Email]

? Thu, 14 Jul 2022 14:58:17 +0200
Uwe Brauer <oub at mat.ucm.es> ?????:

> What turns me crazy is that the way R, matlab and the JCR calculate 
> the quartiles gives different results.

R by itself can give up to 9 slightly different results:

sapply(1:9, function(type) quantile(1:267, 1:3/4, type = type))
#     [,1] [,2] [,3]   [,4]   [,5] [,6]  [,7]      [,8]     [,9]
# 25%   67   67   67  66.75  67.25   67  67.5  67.16667  67.1875
# 50%  134  134  134 133.50 134.00  134 134.0 134.00000 134.0000 # 75%  201  201  200 200.25 200.75  201 200.5 200.83333 200.8125

Choose the ones that fit your ideas of quantile best. See ?quantile for more info.

--
Best regards,
Ivan

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=27y7tm3H4Dbasv87iGgaE8-1NMw7x-Skvgftsip0_qqJw5CpboxBiPDg8Hggx-xs&s=dYjPPV9FhTSt-KjJZLi6AXh_PNykZSwIhfrk2E4saXw&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=27y7tm3H4Dbasv87iGgaE8-1NMw7x-Skvgftsip0_qqJw5CpboxBiPDg8Hggx-xs&s=QJ2jSyuRERB_vwvrBw6zUnAUgY6Bals14F_ODjiqPcA&e=
and provide commented, minimal, self-contained, reproducible code.

From bgunter@4567 @end|ng |rom gm@||@com  Fri Jul 15 18:02:59 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 15 Jul 2022 09:02:59 -0700
Subject: [R] the quantile function and problems.
In-Reply-To: <BN6PR2201MB1553CCCB355489E7721B2BF4CF8B9@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <87wncf6ec6.fsf@mat.ucm.es> <20220715092042.4e8189b7@Tarkus>
 <BN6PR2201MB1553CCCB355489E7721B2BF4CF8B9@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <CAGxFJbS_F59sg2AHuym-050QYS1Hw=tETiCP7Ofo6d2KSBG2yw@mail.gmail.com>

This is now wandering from R-Help to statistical issues, which are
slightly off topic.

But the answer should be no, as the tests are calculated from the
underlying data, not quantile estimates.


Cheers,
Bert

On Fri, Jul 15, 2022 at 8:55 AM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>
> Does the choice in how the quantile is calculated influence the validity of a statistical test for differences in the median?
> Regards,
> Tim
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ivan Krylov
> Sent: Friday, July 15, 2022 2:21 AM
> To: Uwe Brauer <oub at mat.ucm.es>
> Cc: r-help at r-project.org
> Subject: Re: [R] the quantile function and problems.
>
> [External Email]
>
> ? Thu, 14 Jul 2022 14:58:17 +0200
> Uwe Brauer <oub at mat.ucm.es> ?????:
>
> > What turns me crazy is that the way R, matlab and the JCR calculate
> > the quartiles gives different results.
>
> R by itself can give up to 9 slightly different results:
>
> sapply(1:9, function(type) quantile(1:267, 1:3/4, type = type))
> #     [,1] [,2] [,3]   [,4]   [,5] [,6]  [,7]      [,8]     [,9]
> # 25%   67   67   67  66.75  67.25   67  67.5  67.16667  67.1875
> # 50%  134  134  134 133.50 134.00  134 134.0 134.00000 134.0000 # 75%  201  201  200 200.25 200.75  201 200.5 200.83333 200.8125
>
> Choose the ones that fit your ideas of quantile best. See ?quantile for more info.
>
> --
> Best regards,
> Ivan
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=27y7tm3H4Dbasv87iGgaE8-1NMw7x-Skvgftsip0_qqJw5CpboxBiPDg8Hggx-xs&s=dYjPPV9FhTSt-KjJZLi6AXh_PNykZSwIhfrk2E4saXw&e=
> PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=27y7tm3H4Dbasv87iGgaE8-1NMw7x-Skvgftsip0_qqJw5CpboxBiPDg8Hggx-xs&s=QJ2jSyuRERB_vwvrBw6zUnAUgY6Bals14F_ODjiqPcA&e=
> and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkr|de@u @end|ng |rom gm@||@com  Fri Jul 15 18:48:36 2022
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Fri, 15 Jul 2022 12:48:36 -0400
Subject: [R] Kindly Guide
In-Reply-To: <CAMfKi3L=eJzXK0Vkbj-iK-YRs76uTLr3MX4w8xm9mw1uR4Di5A@mail.gmail.com>
References: <CAMfKi3L=eJzXK0Vkbj-iK-YRs76uTLr3MX4w8xm9mw1uR4Di5A@mail.gmail.com>
Message-ID: <CAKZQJMAQTE8u_EOFwxHL=SzMX6a7ZWVmqXWDJ3YYxYZCptY7uA@mail.gmail.com>

Typo? Try
write.xlsx(x, "table name.xlsx")

Example
install.packages('openxlsx')
library(openxlsx)

write.xlsx(df, "stations.xlsx")

On Thu, 14 Jul 2022 at 15:40, Muhammad Zubair Chishti
<mzchishti at eco.qau.edu.pk> wrote:
>
> Dear Experts,
> I have obtained the results through applying Time Varying VAR model in R.
> Now I want to save my results in a excel sheet. Kindly guide me how to
> create a excel or excel table. Although I have tried the following code but
> I face error.
> Write.xlsx(x, "table name.xlsx")
>
> Kindly guide.
>
> Regards
> Chishti
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
John Kane
Kingston ON Canada


From oub @end|ng |rom m@t@ucm@e@  Fri Jul 15 07:49:48 2022
From: oub @end|ng |rom m@t@ucm@e@ (Uwe Brauer)
Date: Fri, 15 Jul 2022 07:49:48 +0200
Subject: [R] the quantile function and problems.
References: <87wncf6ec6.fsf@mat.ucm.es>
 <CAGxFJbTaG0PD_itn_C1zZEL9hF1nToG9f61KxZfKYMjih_OCqA@mail.gmail.com>
Message-ID: <875yjzvsar.fsf@mat.ucm.es>

>>> "BG" == Bert Gunter <bgunter.4567 at gmail.com> writes:

> Read ?quantile carefully, please (and any references therein that you
> may wish to consult).

> You are estimating a continuous function by a discrete finite step
> function, and as the Help page (and further references) explains,
> there are many ways to do this.

Ok, thanks

From oub @end|ng |rom m@t@ucm@e@  Fri Jul 15 21:29:19 2022
From: oub @end|ng |rom m@t@ucm@e@ (Uwe Brauer)
Date: Fri, 15 Jul 2022 21:29:19 +0200
Subject: [R] the quantile function and problems.
References: <87wncf6ec6.fsf@mat.ucm.es> <20220715092042.4e8189b7@Tarkus>
Message-ID: <87bktqfa40.fsf@mat.ucm.es>

>>> "IK" == Ivan Krylov <krylov.r00t at gmail.com> writes:

> ? Thu, 14 Jul 2022 14:58:17 +0200
> Uwe Brauer <oub at mat.ucm.es> ?????:

>> What turns me crazy is that the way R, matlab and the JCR calculate
>> the quartiles gives different results.

> R by itself can give up to 9 slightly different results:

> sapply(1:9, function(type) quantile(1:267, 1:3/4, type = type))
> #     [,1] [,2] [,3]   [,4]   [,5] [,6]  [,7]      [,8]     [,9]
> # 25%   67   67   67  66.75  67.25   67  67.5  67.16667  67.1875
> # 50%  134  134  134 133.50 134.00  134 134.0 134.00000 134.0000
> # 75%  201  201  200 200.25 200.75  201 200.5 200.83333 200.8125

Thanks, number 3 is the best for my purpose. 

Very useful



-- 
I strongly condemn Putin's war of aggression against the Ukraine.
I support to deliver weapons to Ukraine's military. 
I support the ban of Russia from SWIFT.
I support the EU membership of the Ukraine. 

From r@turner @end|ng |rom @uck|@nd@@c@nz  Sat Jul 16 04:37:13 2022
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sat, 16 Jul 2022 14:37:13 +1200
Subject: [R] Thanks to this list ....
In-Reply-To: <7f0dbe00-0983-0d04-9235-9ae869588244@atsu.edu>
References: <20220714222010.17fa67a1@rolf-Latitude-E7470>
 <7f0dbe00-0983-0d04-9235-9ae869588244@atsu.edu>
Message-ID: <20220716143713.5a03328c@rolf-Latitude-E7470>

On Fri, 15 Jul 2022 08:34:36 -0500
Robert Baer <rbaer at atsu.edu> wrote:

> And once again you have shown? that this list has an important side 
> effect of helping us to help ourselves ...? :-)

*If* we take the time and make the effort to formulate a clear question!

(A rule more often broken than observed, sad to say.)

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From neh@@bo|ogn@90 @end|ng |rom gm@||@com  Sun Jul 17 21:05:29 2022
From: neh@@bo|ogn@90 @end|ng |rom gm@||@com (Neha gupta)
Date: Sun, 17 Jul 2022 21:05:29 +0200
Subject: [R] How to reduce the (whitespace) gap between boxplots
Message-ID: <CA+nrPnvp1NHs9fJQGxUPz2KrR68rgQGj+gFqj9m7VOiVG4UWJA@mail.gmail.com>

Hi

I have the following code to display boxplots. The problem is it has a wide
space (gap) between them which takes a lot of space on paper. How can I
reduce the size so that it takes less time in a word document.

RF= c(81.8, 81.8, 42.8, 42.8, 100, 53.8)
Ranger= c(66.6, 81.8, 81.8, 33.3, 81.8, 53.8)
SVM= c(81.8, 66.6, 81.8, 53.8, 81.8, 66.6 )
knn= c(17.64, 33.33, 42.8, 25, 53.84, 17.64)
DT= c(33.3 , 66.6, 33.3, 66.6, 33.3, 25)


colors = rep("green",6)
at.x <- seq(1,by=.4, length.out = 10)
boxplot(RF,Ranger, SVM, KNN, DT, range = 0, boxwex = 0.2, col=colors,
names= c("RF","Ranger", "SVM", "KNN", "DT"),main="Degree of consistency
with default and optimized settings")

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun Jul 17 22:24:50 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 17 Jul 2022 13:24:50 -0700
Subject: [R] How to reduce the (whitespace) gap between boxplots
In-Reply-To: <CA+nrPnvp1NHs9fJQGxUPz2KrR68rgQGj+gFqj9m7VOiVG4UWJA@mail.gmail.com>
References: <CA+nrPnvp1NHs9fJQGxUPz2KrR68rgQGj+gFqj9m7VOiVG4UWJA@mail.gmail.com>
Message-ID: <CAGxFJbTDP7K9piAdS=XP_MtDjDeMGFagWXgKSS+MbTzH9Xudww@mail.gmail.com>

1. Typo: should be "KNN = (your data)"  -- not "knn"

You specified at.x, but failed to use it.  You need to specify the
"at" parameter of boxplot() and probably also "xlim" (which is passed
down to bxp(), the function that actually draws the plot).

e.g.

boxplot(RF,Ranger, SVM, KNN, DT, range = 0, boxwex = 0.2, col=colors,
at = seq(1,3,by = .5), xlim = c(.8,3.2),
        names= c("RF","Ranger", "SVM", "KNN", "DT"),main="Degree of consistency
with default and optimized settings")

Modify to your taste.

Cheers,
Bert

On Sun, Jul 17, 2022 at 12:06 PM Neha gupta <neha.bologna90 at gmail.com> wrote:
>
> Hi
>
> I have the following code to display boxplots. The problem is it has a wide
> space (gap) between them which takes a lot of space on paper. How can I
> reduce the size so that it takes less time in a word document.
>
> RF= c(81.8, 81.8, 42.8, 42.8, 100, 53.8)
> Ranger= c(66.6, 81.8, 81.8, 33.3, 81.8, 53.8)
> SVM= c(81.8, 66.6, 81.8, 53.8, 81.8, 66.6 )
> knn= c(17.64, 33.33, 42.8, 25, 53.84, 17.64)
> DT= c(33.3 , 66.6, 33.3, 66.6, 33.3, 25)
>
>
> colors = rep("green",6)
> at.x <- seq(1,by=.4, length.out = 10)
> boxplot(RF,Ranger, SVM, KNN, DT, range = 0, boxwex = 0.2, col=colors,
> names= c("RF","Ranger", "SVM", "KNN", "DT"),main="Degree of consistency
> with default and optimized settings")
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From neh@@bo|ogn@90 @end|ng |rom gm@||@com  Sun Jul 17 22:32:55 2022
From: neh@@bo|ogn@90 @end|ng |rom gm@||@com (Neha gupta)
Date: Sun, 17 Jul 2022 22:32:55 +0200
Subject: [R] How to reduce the (whitespace) gap between boxplots
In-Reply-To: <CAGxFJbTDP7K9piAdS=XP_MtDjDeMGFagWXgKSS+MbTzH9Xudww@mail.gmail.com>
References: <CA+nrPnvp1NHs9fJQGxUPz2KrR68rgQGj+gFqj9m7VOiVG4UWJA@mail.gmail.com>
 <CAGxFJbTDP7K9piAdS=XP_MtDjDeMGFagWXgKSS+MbTzH9Xudww@mail.gmail.com>
Message-ID: <CA+nrPnse8KD-WJ6KT9avTsX3A06SYyLJW_6BCDxV+nzyrGqs4Q@mail.gmail.com>

Thank you so much.

When I modify this, like the following

xlim = c(.2,4.2),

The spaces between boxplots are reduced but the main box (the container)
remains the same which still takes space. Can I reduce the size of the main
box (which contains all the boxplots) to take less space.



On Sun, Jul 17, 2022 at 10:25 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> 1. Typo: should be "KNN = (your data)"  -- not "knn"
>
> You specified at.x, but failed to use it.  You need to specify the
> "at" parameter of boxplot() and probably also "xlim" (which is passed
> down to bxp(), the function that actually draws the plot).
>
> e.g.
>
> boxplot(RF,Ranger, SVM, KNN, DT, range = 0, boxwex = 0.2, col=colors,
> at = seq(1,3,by = .5), xlim = c(.8,3.2),
>         names= c("RF","Ranger", "SVM", "KNN", "DT"),main="Degree of
> consistency
> with default and optimized settings")
>
> Modify to your taste.
>
> Cheers,
> Bert
>
> On Sun, Jul 17, 2022 at 12:06 PM Neha gupta <neha.bologna90 at gmail.com>
> wrote:
> >
> > Hi
> >
> > I have the following code to display boxplots. The problem is it has a
> wide
> > space (gap) between them which takes a lot of space on paper. How can I
> > reduce the size so that it takes less time in a word document.
> >
> > RF= c(81.8, 81.8, 42.8, 42.8, 100, 53.8)
> > Ranger= c(66.6, 81.8, 81.8, 33.3, 81.8, 53.8)
> > SVM= c(81.8, 66.6, 81.8, 53.8, 81.8, 66.6 )
> > knn= c(17.64, 33.33, 42.8, 25, 53.84, 17.64)
> > DT= c(33.3 , 66.6, 33.3, 66.6, 33.3, 25)
> >
> >
> > colors = rep("green",6)
> > at.x <- seq(1,by=.4, length.out = 10)
> > boxplot(RF,Ranger, SVM, KNN, DT, range = 0, boxwex = 0.2, col=colors,
> > names= c("RF","Ranger", "SVM", "KNN", "DT"),main="Degree of consistency
> > with default and optimized settings")
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun Jul 17 22:41:57 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 17 Jul 2022 13:41:57 -0700
Subject: [R] How to reduce the (whitespace) gap between boxplots
In-Reply-To: <CA+nrPnse8KD-WJ6KT9avTsX3A06SYyLJW_6BCDxV+nzyrGqs4Q@mail.gmail.com>
References: <CA+nrPnvp1NHs9fJQGxUPz2KrR68rgQGj+gFqj9m7VOiVG4UWJA@mail.gmail.com>
 <CAGxFJbTDP7K9piAdS=XP_MtDjDeMGFagWXgKSS+MbTzH9Xudww@mail.gmail.com>
 <CA+nrPnse8KD-WJ6KT9avTsX3A06SYyLJW_6BCDxV+nzyrGqs4Q@mail.gmail.com>
Message-ID: <CAGxFJbSFY1NKBxMuoR1vZUhTVochKEqee5mvjBiHmLm1By7_iQ@mail.gmail.com>

Make your xlim values narrower, as I showed you!

Alternatively, you could also us the "boxwex" parameter passed to
bxp() like this:

boxplot(RF,Ranger, SVM, KNN, DT, range = 0,  col=colors,
        at = seq(1:5), boxwex = .5,
        names= c("RF","Ranger", "SVM", "KNN", "DT"),main="Degree of consistency
with default and optimized settings")

-- Bert

On Sun, Jul 17, 2022 at 1:33 PM Neha gupta <neha.bologna90 at gmail.com> wrote:
>
> Thank you so much.
>
> When I modify this, like the following
>
> xlim = c(.2,4.2),
>
> The spaces between boxplots are reduced but the main box (the container) remains the same which still takes space. Can I reduce the size of the main box (which contains all the boxplots) to take less space.
>
>
>
> On Sun, Jul 17, 2022 at 10:25 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>
>> 1. Typo: should be "KNN = (your data)"  -- not "knn"
>>
>> You specified at.x, but failed to use it.  You need to specify the
>> "at" parameter of boxplot() and probably also "xlim" (which is passed
>> down to bxp(), the function that actually draws the plot).
>>
>> e.g.
>>
>> boxplot(RF,Ranger, SVM, KNN, DT, range = 0, boxwex = 0.2, col=colors,
>> at = seq(1,3,by = .5), xlim = c(.8,3.2),
>>         names= c("RF","Ranger", "SVM", "KNN", "DT"),main="Degree of consistency
>> with default and optimized settings")
>>
>> Modify to your taste.
>>
>> Cheers,
>> Bert
>>
>> On Sun, Jul 17, 2022 at 12:06 PM Neha gupta <neha.bologna90 at gmail.com> wrote:
>> >
>> > Hi
>> >
>> > I have the following code to display boxplots. The problem is it has a wide
>> > space (gap) between them which takes a lot of space on paper. How can I
>> > reduce the size so that it takes less time in a word document.
>> >
>> > RF= c(81.8, 81.8, 42.8, 42.8, 100, 53.8)
>> > Ranger= c(66.6, 81.8, 81.8, 33.3, 81.8, 53.8)
>> > SVM= c(81.8, 66.6, 81.8, 53.8, 81.8, 66.6 )
>> > knn= c(17.64, 33.33, 42.8, 25, 53.84, 17.64)
>> > DT= c(33.3 , 66.6, 33.3, 66.6, 33.3, 25)
>> >
>> >
>> > colors = rep("green",6)
>> > at.x <- seq(1,by=.4, length.out = 10)
>> > boxplot(RF,Ranger, SVM, KNN, DT, range = 0, boxwex = 0.2, col=colors,
>> > names= c("RF","Ranger", "SVM", "KNN", "DT"),main="Degree of consistency
>> > with default and optimized settings")
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.


From neh@@bo|ogn@90 @end|ng |rom gm@||@com  Sun Jul 17 22:51:28 2022
From: neh@@bo|ogn@90 @end|ng |rom gm@||@com (Neha gupta)
Date: Sun, 17 Jul 2022 22:51:28 +0200
Subject: [R] How to reduce the (whitespace) gap between boxplots
In-Reply-To: <CAGxFJbSFY1NKBxMuoR1vZUhTVochKEqee5mvjBiHmLm1By7_iQ@mail.gmail.com>
References: <CA+nrPnvp1NHs9fJQGxUPz2KrR68rgQGj+gFqj9m7VOiVG4UWJA@mail.gmail.com>
 <CAGxFJbTDP7K9piAdS=XP_MtDjDeMGFagWXgKSS+MbTzH9Xudww@mail.gmail.com>
 <CA+nrPnse8KD-WJ6KT9avTsX3A06SYyLJW_6BCDxV+nzyrGqs4Q@mail.gmail.com>
 <CAGxFJbSFY1NKBxMuoR1vZUhTVochKEqee5mvjBiHmLm1By7_iQ@mail.gmail.com>
Message-ID: <CA+nrPnusvstH6yLbq9zyWuZDVyGSfYAqq4DxqHvSEHxsOJJp6g@mail.gmail.com>

Ah Ok, thank you so much again for your help.

Best regards

On Sun, Jul 17, 2022 at 10:42 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Make your xlim values narrower, as I showed you!
>
> Alternatively, you could also us the "boxwex" parameter passed to
> bxp() like this:
>
> boxplot(RF,Ranger, SVM, KNN, DT, range = 0,  col=colors,
>         at = seq(1:5), boxwex = .5,
>         names= c("RF","Ranger", "SVM", "KNN", "DT"),main="Degree of
> consistency
> with default and optimized settings")
>
> -- Bert
>
> On Sun, Jul 17, 2022 at 1:33 PM Neha gupta <neha.bologna90 at gmail.com>
> wrote:
> >
> > Thank you so much.
> >
> > When I modify this, like the following
> >
> > xlim = c(.2,4.2),
> >
> > The spaces between boxplots are reduced but the main box (the container)
> remains the same which still takes space. Can I reduce the size of the main
> box (which contains all the boxplots) to take less space.
> >
> >
> >
> > On Sun, Jul 17, 2022 at 10:25 PM Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
> >>
> >> 1. Typo: should be "KNN = (your data)"  -- not "knn"
> >>
> >> You specified at.x, but failed to use it.  You need to specify the
> >> "at" parameter of boxplot() and probably also "xlim" (which is passed
> >> down to bxp(), the function that actually draws the plot).
> >>
> >> e.g.
> >>
> >> boxplot(RF,Ranger, SVM, KNN, DT, range = 0, boxwex = 0.2, col=colors,
> >> at = seq(1,3,by = .5), xlim = c(.8,3.2),
> >>         names= c("RF","Ranger", "SVM", "KNN", "DT"),main="Degree of
> consistency
> >> with default and optimized settings")
> >>
> >> Modify to your taste.
> >>
> >> Cheers,
> >> Bert
> >>
> >> On Sun, Jul 17, 2022 at 12:06 PM Neha gupta <neha.bologna90 at gmail.com>
> wrote:
> >> >
> >> > Hi
> >> >
> >> > I have the following code to display boxplots. The problem is it has
> a wide
> >> > space (gap) between them which takes a lot of space on paper. How can
> I
> >> > reduce the size so that it takes less time in a word document.
> >> >
> >> > RF= c(81.8, 81.8, 42.8, 42.8, 100, 53.8)
> >> > Ranger= c(66.6, 81.8, 81.8, 33.3, 81.8, 53.8)
> >> > SVM= c(81.8, 66.6, 81.8, 53.8, 81.8, 66.6 )
> >> > knn= c(17.64, 33.33, 42.8, 25, 53.84, 17.64)
> >> > DT= c(33.3 , 66.6, 33.3, 66.6, 33.3, 25)
> >> >
> >> >
> >> > colors = rep("green",6)
> >> > at.x <- seq(1,by=.4, length.out = 10)
> >> > boxplot(RF,Ranger, SVM, KNN, DT, range = 0, boxwex = 0.2, col=colors,
> >> > names= c("RF","Ranger", "SVM", "KNN", "DT"),main="Degree of
> consistency
> >> > with default and optimized settings")
> >> >
> >> >         [[alternative HTML version deleted]]
> >> >
> >> > ______________________________________________
> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From gregco@t@ @end|ng |rom me@com  Mon Jul 18 05:47:19 2022
From: gregco@t@ @end|ng |rom me@com (Gregory Coats)
Date: Sun, 17 Jul 2022 23:47:19 -0400
Subject: [R] Date and Time
Message-ID: <FE9F1B27-D8A6-4C77-956D-51D05CA45498@me.com>

For the year from 2022-01-01 to 2022-12-31, I computed the sunrise and sunset times for Reston, Virginia, USA. I am seeking the syntax to direct R to read in these dates and times, and then graphical plot the sunrise and sunset times for each day in the year 2022. How do I tell R that I store the Sunrise Date and Time like this?
YYYY-mm-dd HH:MM:SS
2022-01-01  7:28:10
2022-01-02  7:28:17
Greg Coats
	[[alternative HTML version deleted]]


From ro@||n@ump @end|ng |rom gm@||@com  Mon Jul 18 06:04:00 2022
From: ro@||n@ump @end|ng |rom gm@||@com (roslinazairimah zakaria)
Date: Mon, 18 Jul 2022 12:04:00 +0800
Subject: [R] Extract time and state of charge (Start and End) and Count
Message-ID: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>

Dear all,

I have data of Battery Electric vehicle (BEV). I would like to extract data
from every hour starting from 0.00 to 0.59, 1:00-1:59 for SOC(state of
charge) start to end.

Some examples:
I can extract data from SOC=0 and SOC=12
dt_2014[which(dt_2014$Starting_SoC_of_12==0 &
dt_2014$Ending_SoC_of_12==12),]

I can extract data from SOC=1 and SOC=12
dt_2014[which(dt_2014$Starting_SoC_of_12==1 &
dt_2014$Ending_SoC_of_12==12),]

and I would like to further categorise the data by hour and count how many
cars from 0 state charge to 12 state charge at in that particular hour.

Thank you so much for any help given.

Some data
> dput(dt_2014[1:10,])
structure(list(?..CarID = c("GC10", "GC10", "GC10", "GC10", "GC10",
"GC10", "GC10", "GC10", "GC10", "GC10"), BatteryChargeStartDate =
c("16/2/2014 16:05",
"16/2/2014 18:20", "17/2/2014 8:10", "18/2/2014 7:41", "18/2/2014 15:36",
"18/2/2014 16:36", "18/2/2014 21:26", "19/2/2014 8:57", "19/2/2014 21:08",
"20/2/2014 18:11"), BCStartTime = c("16:05", "18:20", "8:10",
"7:41", "15:36", "16:36", "21:26", "8:57", "21:08", "18:11"),
    Year = c(2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L,
    2014L, 2014L, 2014L), Month = c(2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L), Day = c(16L, 16L, 17L, 18L, 18L, 18L, 18L, 19L,
    19L, 20L), BatteryChargeStopDate = c("16/2/2014 17:05", "16/2/2014
19:00",
    "17/2/2014 15:57", "18/2/2014 9:52", "18/2/2014 15:39", "18/2/2014
17:36",
    "19/2/2014 1:55", "19/2/2014 14:25", "20/2/2014 5:17", "20/2/2014 23:20"
    ), BCStopTime = c("17:05", "19:00", "15:57", "9:52", "15:39",
    "17:36", "1:55", "14:25", "5:17", "23:20"), Year2 = c(2014L,
    2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L
    ), Month2 = c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), Day2 = c(16L,
    16L, 17L, 18L, 18L, 18L, 19L, 19L, 20L, 20L), Starting_SoC_of_12 =
c(1L,
    2L, 4L, 5L, 4L, 2L, 8L, 8L, 4L, 8L), Ending_SoC_of_12 = c(11L,
    11L, 12L, 8L, 4L, 10L, 12L, 12L, 12L, 12L)), row.names = c(NA,
10L), class = "data.frame")


-- 
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinaump at gmail.com <roslinaump at gmail.com>*
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Jul 18 06:34:44 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sun, 17 Jul 2022 21:34:44 -0700
Subject: [R] Date and Time
In-Reply-To: <FE9F1B27-D8A6-4C77-956D-51D05CA45498@me.com>
References: <FE9F1B27-D8A6-4C77-956D-51D05CA45498@me.com>
Message-ID: <6006BB39-8D43-4F1C-BBBF-97D1B6324DF2@dcn.davis.ca.us>

Maybe this [1] will help? Or just read ?strptime...

You can also calculate sunrise/sunset (crepuscule) using maptools, but you need to be careful with timezones.

[1] https://jdnewmil.github.io/time-2018-10/MoreDatetimeHowto.html

On July 17, 2022 8:47:19 PM PDT, Gregory Coats via R-help <r-help at r-project.org> wrote:
>For the year from 2022-01-01 to 2022-12-31, I computed the sunrise and sunset times for Reston, Virginia, USA. I am seeking the syntax to direct R to read in these dates and times, and then graphical plot the sunrise and sunset times for each day in the year 2022. How do I tell R that I store the Sunrise Date and Time like this?
>YYYY-mm-dd HH:MM:SS
>2022-01-01  7:28:10
>2022-01-02  7:28:17
>Greg Coats
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @vi@e@gross m@iii@g oii gm@ii@com  Mon Jul 18 06:38:41 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Mon, 18 Jul 2022 00:38:41 -0400
Subject: [R] Date and Time
In-Reply-To: <FE9F1B27-D8A6-4C77-956D-51D05CA45498@me.com>
References: <FE9F1B27-D8A6-4C77-956D-51D05CA45498@me.com>
Message-ID: <012b01d89a60$41ee8c00$c5cba400$@gmail.com>

Greg,

Strictly speaking, you don't tell R and keep it a secret!

More seriously, are you reading in your data as text of a sort in a file, or
as part of a CSV file or Spreadsheet or what?

What do you plan on graphing the data with? It sounds like you likely need
to make a data.frame or tibble that has at least two columns with one being
a DATE of some kind so you can plot the date, versus another being a kind of
TIME. 

There are many ways to record time in R as dates or date+time or duration
and you need to pick some that will work with the ploting program such as
base R or ggplot. 

You say your dates look like this:

YYYY-mm-dd HH:MM:SS
2022-01-01  7:28:10

It is fairly easy to split that into a date STRING in one variable and a
time string in another.

Then you need to convert a vector of each (perhaps in a data.frame) into
some objects of a type that hold the data properly.  You can use standard R
functions or various packages. If your format is already compatible, so
this:

date_part <- as.Date("2022-01-01")

You can see if it worked by adding 1 day to it:

> date_part+1
[1] "2022-01-02"

If your format is something specific, you can pass templates to guide some
function in parsing it.

Similarly you can find R functions that store a time and of course more
advanced ones that store both in several ways. But for graphing what you
want as a sort of sinusoidal function, you would need to extract the time
and date from it.

If you search online, you can find lots of tutorials and discussions as this
is a very common thing. I don't mean sunsets, but measurements versus dates.


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Gregory Coats via
R-help
Sent: Sunday, July 17, 2022 11:47 PM
To: Gregory Coats via R-help <r-help at r-project.org>
Subject: [R] Date and Time

For the year from 2022-01-01 to 2022-12-31, I computed the sunrise and
sunset times for Reston, Virginia, USA. I am seeking the syntax to direct R
to read in these dates and times, and then graphical plot the sunrise and
sunset times for each day in the year 2022. How do I tell R that I store the
Sunrise Date and Time like this?
YYYY-mm-dd HH:MM:SS
2022-01-01  7:28:10
2022-01-02  7:28:17
Greg Coats


From djnord|und @end|ng |rom gm@||@com  Mon Jul 18 06:50:44 2022
From: djnord|und @end|ng |rom gm@||@com (Daniel Nordlund)
Date: Sun, 17 Jul 2022 21:50:44 -0700
Subject: [R] Date and Time
In-Reply-To: <FE9F1B27-D8A6-4C77-956D-51D05CA45498@me.com>
References: <FE9F1B27-D8A6-4C77-956D-51D05CA45498@me.com>
Message-ID: <4eef21a3-a106-e4bd-1966-345b5793d391@gmail.com>

On 7/17/2022 8:47 PM, Gregory Coats via R-help wrote:
> For the year from 2022-01-01 to 2022-12-31, I computed the sunrise and sunset times for Reston, Virginia, USA. I am seeking the syntax to direct R to read in these dates and times, and then graphical plot the sunrise and sunset times for each day in the year 2022. How do I tell R that I store the Sunrise Date and Time like this?
> YYYY-mm-dd HH:MM:SS
> 2022-01-01  7:28:10
> 2022-01-02  7:28:17
> Greg Coats
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Here is one way using the strptime function,

 > in_dt_str <- '2022-01-01? 7:28:10'
 > dt <- strptime(in_dt_str,'%Y-%m-%d %H:%M:%S')

if the date and time are stored in different variables then paste them 
together before converting

 > in_dt <- '2022-01-01'
 > in_tm <- '7:28:10'
 > dtm <- strptime(paste(in_dt, in_tm, sep=' '),'%Y-%m-%d %H:%M:%S')

Hope this is helpful,

Dan

-- 
Daniel Nordlund
Port Townsend, WA  USA


-- 
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus


From @vi@e@gross m@iii@g oii gm@ii@com  Mon Jul 18 06:59:29 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Mon, 18 Jul 2022 00:59:29 -0400
Subject: [R] Extract time and state of charge (Start and End) and Count
In-Reply-To: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
References: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
Message-ID: <015c01d89a63$29725c30$7c571490$@gmail.com>

It is late here so I will just say I was able to look at your sample data. There may be many approaches here for what I think you want but for simplicity, assume you may want to create a new column or two to use in your analysis.

Your variable names are long and hard to read so I won't use them. I am addressing where you want to categorize the data by the hour. My suggestion is you make new columns where you truncate the hour string to something like 1 or 15 so you can later group the data by, among other things, the hour. 

The next part of your question is less easy to understand as none of the data you supplied had an example of Starting_SoC_of_12 Ending_SoC_of_12 going from 0 to 12. Bottom line is you can make a new column that has some value like TRUE or 1 or whatever if your condition holds that the start was 0 and the end was 12 in that row, otherwise a FALSE or 0 or whatever.

I am not sure what you mean by HOW MANY CARS as your example shows all cars having the same ID. But assuming the rest of your data has what you want, then you want to use whatever means you can to cluster the data. I like the tidyverse/dplyr methods but other ways in base R will do too.

Bottom line is you take your enhanced data.frame and group by various "date" parts as well as the hour column you made and the column about whether it met your condition. Grouping sort of means there is a way to look at each group. In dplyr, once grouped, you can make a sort of report with summarize where it will show each date and hour along with he number of items in that group. In pseudocode:

Filter( keep only rows where the condition variable above is TRUE)
Group_by(date, hour)
Summarize(full_charge=n())

The above and more are normally done in a pipeline but that is ONE of many approaches. Note that temporary variables can often be avoided as in the filter above but may be useful if you do many kinds of analysis.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of roslinazairimah zakaria
Sent: Monday, July 18, 2022 12:04 AM
To: R help Mailing list <r-help at r-project.org>
Subject: [R] Extract time and state of charge (Start and End) and Count

Dear all,

I have data of Battery Electric vehicle (BEV). I would like to extract data from every hour starting from 0.00 to 0.59, 1:00-1:59 for SOC(state of
charge) start to end.

Some examples:
I can extract data from SOC=0 and SOC=12
dt_2014[which(dt_2014$Starting_SoC_of_12==0 & dt_2014$Ending_SoC_of_12==12),]

I can extract data from SOC=1 and SOC=12
dt_2014[which(dt_2014$Starting_SoC_of_12==1 & dt_2014$Ending_SoC_of_12==12),]

and I would like to further categorise the data by hour and count how many cars from 0 state charge to 12 state charge at in that particular hour.

Thank you so much for any help given.

Some data
> dput(dt_2014[1:10,])
structure(list(?..CarID = c("GC10", "GC10", "GC10", "GC10", "GC10", "GC10", "GC10", "GC10", "GC10", "GC10"), BatteryChargeStartDate =
c("16/2/2014 16:05",
"16/2/2014 18:20", "17/2/2014 8:10", "18/2/2014 7:41", "18/2/2014 15:36",
"18/2/2014 16:36", "18/2/2014 21:26", "19/2/2014 8:57", "19/2/2014 21:08",
"20/2/2014 18:11"), BCStartTime = c("16:05", "18:20", "8:10", "7:41", "15:36", "16:36", "21:26", "8:57", "21:08", "18:11"),
    Year = c(2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L,
    2014L, 2014L, 2014L), Month = c(2L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L), Day = c(16L, 16L, 17L, 18L, 18L, 18L, 18L, 19L,
    19L, 20L), BatteryChargeStopDate = c("16/2/2014 17:05", "16/2/2014 19:00",
    "17/2/2014 15:57", "18/2/2014 9:52", "18/2/2014 15:39", "18/2/2014 17:36",
    "19/2/2014 1:55", "19/2/2014 14:25", "20/2/2014 5:17", "20/2/2014 23:20"
    ), BCStopTime = c("17:05", "19:00", "15:57", "9:52", "15:39",
    "17:36", "1:55", "14:25", "5:17", "23:20"), Year2 = c(2014L,
    2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L
    ), Month2 = c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), Day2 = c(16L,
    16L, 17L, 18L, 18L, 18L, 19L, 19L, 20L, 20L), Starting_SoC_of_12 = c(1L,
    2L, 4L, 5L, 4L, 2L, 8L, 8L, 4L, 8L), Ending_SoC_of_12 = c(11L,
    11L, 12L, 8L, 4L, 10L, 12L, 12L, 12L, 12L)), row.names = c(NA, 10L), class = "data.frame")


--
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinaump at gmail.com <roslinaump at gmail.com>* University Malaysia Pahang Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From tebert @end|ng |rom u||@edu  Mon Jul 18 12:23:16 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Mon, 18 Jul 2022 10:23:16 +0000
Subject: [R] Date and Time
In-Reply-To: <6006BB39-8D43-4F1C-BBBF-97D1B6324DF2@dcn.davis.ca.us>
References: <FE9F1B27-D8A6-4C77-956D-51D05CA45498@me.com>
 <6006BB39-8D43-4F1C-BBBF-97D1B6324DF2@dcn.davis.ca.us>
Message-ID: <BN6PR2201MB155353A9A0DBC19788538FEFCF8C9@BN6PR2201MB1553.namprd22.prod.outlook.com>

An alternative is the lubridate() package if you use tidyverse.

Check the data for daylight savings time. Twice per year Virginia changes time where in the Fall clocks are shifted back an hour and in the Spring they jump forwards an hour.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Jeff Newmiller
Sent: Monday, July 18, 2022 12:35 AM
To: Greg Comcast Coats <gregcoats at me.com>; Gregory Coats via R-help <r-help at r-project.org>
Subject: Re: [R] Date and Time

[External Email]

Maybe this [1] will help? Or just read ?strptime...

You can also calculate sunrise/sunset (crepuscule) using maptools, but you need to be careful with timezones.

[1] https://urldefense.proofpoint.com/v2/url?u=https-3A__jdnewmil.github.io_time-2D2018-2D10_MoreDatetimeHowto.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=Eym42IWzdKGREg7m9o0QowA8hP-DgHTrlZwDfW5no2rP6ND_uFJKHbJ_Dj3XGPiP&s=O5VruA8vvqyhQngUZ3fuZtnZiYIm9XEscpzxcGKX9AQ&e=

On July 17, 2022 8:47:19 PM PDT, Gregory Coats via R-help <r-help at r-project.org> wrote:
>For the year from 2022-01-01 to 2022-12-31, I computed the sunrise and sunset times for Reston, Virginia, USA. I am seeking the syntax to direct R to read in these dates and times, and then graphical plot the sunrise and sunset times for each day in the year 2022. How do I tell R that I store the Sunrise Date and Time like this?
>YYYY-mm-dd HH:MM:SS
>2022-01-01  7:28:10
>2022-01-02  7:28:17
>Greg Coats
>       [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailm
>an_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRz
>sn7AkP-g&m=Eym42IWzdKGREg7m9o0QowA8hP-DgHTrlZwDfW5no2rP6ND_uFJKHbJ_Dj3X
>GPiP&s=7XAM9jGAgm-2KbQG5x3oKKxrvNESM5DAvl_O3YxSU3Y&e=
>PLEASE do read the posting guide 
>https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org
>_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsR
>zsn7AkP-g&m=Eym42IWzdKGREg7m9o0QowA8hP-DgHTrlZwDfW5no2rP6ND_uFJKHbJ_Dj3
>XGPiP&s=MwaLmYJXfAwQDsHSzq8Atm-QC2YpJYie_L5oNkAJiVg&e=
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=Eym42IWzdKGREg7m9o0QowA8hP-DgHTrlZwDfW5no2rP6ND_uFJKHbJ_Dj3XGPiP&s=7XAM9jGAgm-2KbQG5x3oKKxrvNESM5DAvl_O3YxSU3Y&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=Eym42IWzdKGREg7m9o0QowA8hP-DgHTrlZwDfW5no2rP6ND_uFJKHbJ_Dj3XGPiP&s=MwaLmYJXfAwQDsHSzq8Atm-QC2YpJYie_L5oNkAJiVg&e=
and provide commented, minimal, self-contained, reproducible code.


From ||z@@|ooten @end|ng |rom ot@go@@c@nz  Mon Jul 18 00:35:23 2022
From: ||z@@|ooten @end|ng |rom ot@go@@c@nz (Elisabeth Slooten)
Date: Sun, 17 Jul 2022 22:35:23 +0000
Subject: [R] Access to cran packages
Message-ID: <94FF6D80-47AC-400D-93C6-3DEDD64D18FC@otago.ac.nz>

I am able to access R packages on my laptop, but not on my desktop computer. Both are Macs, both running OS10.13.6 High Sierra. When I open package installer on the laptop and click"Get list" it provides the usual list of packages. On the desktop computer (Mac Pro Tower, 2012) the following message pops up in the R console window:
"Warning: unable to access index for repository https://cran.stat.auckland.ac.nz/bin/macosx/contrib/4.2:
  cannot open URL 'https://cran.stat.auckland.ac.nz/bin/macosx/contrib/4.2/PACKAGES'

The same happens if I type >install.packages() into the R console window. A list of packages appears on the laptop, but on the desktop computer this happens:

> install.packages()
Warning: unable to access index for repository https://cran.stat.auckland.ac.nz/src/contrib:
  cannot open URL 'https://cran.stat.auckland.ac.nz/src/contrib/PACKAGES'
Error in install.packages() : argument "pkgs" is missing, with no default

> install.packages(sf)
Error in install.packages(sf) : object 'sf' not found

Both computers are online, and on both of them I can see the list of packages here: https://cran.stat.auckland.ac.nz/bin/macosx/contrib/4.2/PACKAGES

Is there a way to directly download the packages?

Many thanks!

Liz Slooten
Professor Emeritus, University of Otago, New Zealand

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Mon Jul 18 12:44:52 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Mon, 18 Jul 2022 20:44:52 +1000
Subject: [R] Access to cran packages
In-Reply-To: <94FF6D80-47AC-400D-93C6-3DEDD64D18FC@otago.ac.nz>
References: <94FF6D80-47AC-400D-93C6-3DEDD64D18FC@otago.ac.nz>
Message-ID: <CA+8X3fXk9_qMUzBf6-wjfjK+vLVC8v_PqLKiE2o5bsK4JC58Hg@mail.gmail.com>

Hi Liz,
Try:

install.packages("sf")

Jim

On Mon, Jul 18, 2022 at 8:37 PM Elisabeth Slooten
<liz.slooten at otago.ac.nz> wrote:
>
> I am able to access R packages on my laptop, but not on my desktop computer. Both are Macs, both running OS10.13.6 High Sierra. When I open package installer on the laptop and click"Get list" it provides the usual list of packages. On the desktop computer (Mac Pro Tower, 2012) the following message pops up in the R console window:
> "Warning: unable to access index for repository https://cran.stat.auckland.ac.nz/bin/macosx/contrib/4.2:
>   cannot open URL 'https://cran.stat.auckland.ac.nz/bin/macosx/contrib/4.2/PACKAGES'
>
> The same happens if I type >install.packages() into the R console window. A list of packages appears on the laptop, but on the desktop computer this happens:
>
> > install.packages()
> Warning: unable to access index for repository https://cran.stat.auckland.ac.nz/src/contrib:
>   cannot open URL 'https://cran.stat.auckland.ac.nz/src/contrib/PACKAGES'
> Error in install.packages() : argument "pkgs" is missing, with no default
>
> > install.packages(sf)
> Error in install.packages(sf) : object 'sf' not found
>
> Both computers are online, and on both of them I can see the list of packages here: https://cran.stat.auckland.ac.nz/bin/macosx/contrib/4.2/PACKAGES
>
> Is there a way to directly download the packages?
>
> Many thanks!
>
> Liz Slooten
> Professor Emeritus, University of Otago, New Zealand
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From kry|ov@r00t @end|ng |rom gm@||@com  Mon Jul 18 13:11:04 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Mon, 18 Jul 2022 14:11:04 +0300
Subject: [R] Access to cran packages
In-Reply-To: <94FF6D80-47AC-400D-93C6-3DEDD64D18FC@otago.ac.nz>
References: <94FF6D80-47AC-400D-93C6-3DEDD64D18FC@otago.ac.nz>
Message-ID: <20220718141104.5341079d@trisector>

On Sun, 17 Jul 2022 22:35:23 +0000
Elisabeth Slooten <liz.slooten at otago.ac.nz> wrote:

> Both computers are online, and on both of them I can see the list of
> packages here:
> https://cran.stat.auckland.ac.nz/bin/macosx/contrib/4.2/PACKAGES

There seems to be something wrong with R's ability to download files
from the Internet on one of your machines. Does
download.file('https://cran.stat.auckland.ac.nz/bin/macosx/contrib/4.2/PACKAGES',
'PACKAGES') provide a useful error message? Is the R version the same
on both computers? Any differences in the sessionInfo() output?
Anything suspect in .Rprofile?

Try asking at r-sig-mac at r-project.org.

> Is there a way to directly download the packages?

You can always visit the package web page (e.g.
<https://CRAN.R-project.org/package=sf>) to get the download link, and
since you're on a macOS machine, you can download the binary.
Unfortunately, it's up to you to resolve the dependencies manually.

To do that semi-manually, you can download
https://cran.r-project.org/web/packages/packages.rds, read it using
readRDS and feed it as the db argument to
tools::package_dependencies('sf', db, recursive = TRUE) in order to
know which packages to download.

-- 
Best regards,
Ivan


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon Jul 18 19:11:05 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 18 Jul 2022 18:11:05 +0100
Subject: [R] Extract time and state of charge (Start and End) and Count
In-Reply-To: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
References: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
Message-ID: <a4a57470-481e-ad36-a577-eb54497a50da@sapo.pt>

Hello,

I'm not sure I understand the problem. Do you want counts of how many 
rows are there per hour?


# these columns need to be fixed
cols <- c("BatteryChargeStartDate", "BatteryChargeStopDate")
dt_2014[cols] <- lapply(dt_2014[cols], \(x) sub("\n", " ", x))
# use package lubridate to coerce to a datetime class
dt_2014[cols] <- lapply(dt_2014[cols], lubridate::dmy_hm)

h <- lubridate::hour(dt_2014[["BatteryChargeStartDate"]])
aggregate(Starting_SoC_of_12 ~ h, dt_2014, length)



It would be better if you post the expected output corresponding to the 
posted data set.

Hope this helps,

Rui Barradas

?s 05:04 de 18/07/2022, roslinazairimah zakaria escreveu:
> Dear all,
> 
> I have data of Battery Electric vehicle (BEV). I would like to extract data
> from every hour starting from 0.00 to 0.59, 1:00-1:59 for SOC(state of
> charge) start to end.
> 
> Some examples:
> I can extract data from SOC=0 and SOC=12
> dt_2014[which(dt_2014$Starting_SoC_of_12==0 &
> dt_2014$Ending_SoC_of_12==12),]
> 
> I can extract data from SOC=1 and SOC=12
> dt_2014[which(dt_2014$Starting_SoC_of_12==1 &
> dt_2014$Ending_SoC_of_12==12),]
> 
> and I would like to further categorise the data by hour and count how many
> cars from 0 state charge to 12 state charge at in that particular hour.
> 
> Thank you so much for any help given.
> 
> Some data
>> dput(dt_2014[1:10,])
> structure(list(?..CarID = c("GC10", "GC10", "GC10", "GC10", "GC10",
> "GC10", "GC10", "GC10", "GC10", "GC10"), BatteryChargeStartDate =
> c("16/2/2014 16:05",
> "16/2/2014 18:20", "17/2/2014 8:10", "18/2/2014 7:41", "18/2/2014 15:36",
> "18/2/2014 16:36", "18/2/2014 21:26", "19/2/2014 8:57", "19/2/2014 21:08",
> "20/2/2014 18:11"), BCStartTime = c("16:05", "18:20", "8:10",
> "7:41", "15:36", "16:36", "21:26", "8:57", "21:08", "18:11"),
>      Year = c(2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L,
>      2014L, 2014L, 2014L), Month = c(2L, 2L, 2L, 2L, 2L, 2L, 2L,
>      2L, 2L, 2L), Day = c(16L, 16L, 17L, 18L, 18L, 18L, 18L, 19L,
>      19L, 20L), BatteryChargeStopDate = c("16/2/2014 17:05", "16/2/2014
> 19:00",
>      "17/2/2014 15:57", "18/2/2014 9:52", "18/2/2014 15:39", "18/2/2014
> 17:36",
>      "19/2/2014 1:55", "19/2/2014 14:25", "20/2/2014 5:17", "20/2/2014 23:20"
>      ), BCStopTime = c("17:05", "19:00", "15:57", "9:52", "15:39",
>      "17:36", "1:55", "14:25", "5:17", "23:20"), Year2 = c(2014L,
>      2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L
>      ), Month2 = c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), Day2 = c(16L,
>      16L, 17L, 18L, 18L, 18L, 19L, 19L, 20L, 20L), Starting_SoC_of_12 =
> c(1L,
>      2L, 4L, 5L, 4L, 2L, 8L, 8L, 4L, 8L), Ending_SoC_of_12 = c(11L,
>      11L, 12L, 8L, 4L, 10L, 12L, 12L, 12L, 12L)), row.names = c(NA,
> 10L), class = "data.frame")
> 
>


From @vi@e@gross m@iii@g oii gm@ii@com  Mon Jul 18 23:56:05 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Mon, 18 Jul 2022 17:56:05 -0400
Subject: [R] Extract time and state of charge (Start and End) and Count
In-Reply-To: <a4a57470-481e-ad36-a577-eb54497a50da@sapo.pt>
References: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
 <a4a57470-481e-ad36-a577-eb54497a50da@sapo.pt>
Message-ID: <003801d89af1$2e605290$8b20f7b0$@gmail.com>

Rui,

The posted data was likely not well chosen as it has no rows that satisfy what I thought was the requirement of going from 0 to 12.

Questions here can often be written more clearly. We can all guess, but my guess was a bit like yours that he/she wanted to count how many rows there are per specific dates/hours (meaning up to 24 per day) that also satisfy a filter requirement. Of course, it is also possible that they do not care about what days, but want to know what happens any day between 9 and 10 Am and other possibilities.

It would be nice if people who asked questions followed up so we stop wasting our time answering what was not asked!



-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Rui Barradas
Sent: Monday, July 18, 2022 1:11 PM
To: roslinazairimah zakaria <roslinaump at gmail.com>; R help Mailing list <r-help at r-project.org>
Subject: Re: [R] Extract time and state of charge (Start and End) and Count

Hello,

I'm not sure I understand the problem. Do you want counts of how many rows are there per hour?


# these columns need to be fixed
cols <- c("BatteryChargeStartDate", "BatteryChargeStopDate")
dt_2014[cols] <- lapply(dt_2014[cols], \(x) sub("\n", " ", x))
# use package lubridate to coerce to a datetime class
dt_2014[cols] <- lapply(dt_2014[cols], lubridate::dmy_hm)

h <- lubridate::hour(dt_2014[["BatteryChargeStartDate"]])
aggregate(Starting_SoC_of_12 ~ h, dt_2014, length)



It would be better if you post the expected output corresponding to the 
posted data set.

Hope this helps,

Rui Barradas

?s 05:04 de 18/07/2022, roslinazairimah zakaria escreveu:
> Dear all,
> 
> I have data of Battery Electric vehicle (BEV). I would like to extract data
> from every hour starting from 0.00 to 0.59, 1:00-1:59 for SOC(state of
> charge) start to end.
> 
> Some examples:
> I can extract data from SOC=0 and SOC=12
> dt_2014[which(dt_2014$Starting_SoC_of_12==0 &
> dt_2014$Ending_SoC_of_12==12),]
> 
> I can extract data from SOC=1 and SOC=12
> dt_2014[which(dt_2014$Starting_SoC_of_12==1 &
> dt_2014$Ending_SoC_of_12==12),]
> 
> and I would like to further categorise the data by hour and count how many
> cars from 0 state charge to 12 state charge at in that particular hour.
> 
> Thank you so much for any help given.
> 
> Some data
>> dput(dt_2014[1:10,])
> structure(list(?..CarID = c("GC10", "GC10", "GC10", "GC10", "GC10",
> "GC10", "GC10", "GC10", "GC10", "GC10"), BatteryChargeStartDate =
> c("16/2/2014 16:05",
> "16/2/2014 18:20", "17/2/2014 8:10", "18/2/2014 7:41", "18/2/2014 15:36",
> "18/2/2014 16:36", "18/2/2014 21:26", "19/2/2014 8:57", "19/2/2014 21:08",
> "20/2/2014 18:11"), BCStartTime = c("16:05", "18:20", "8:10",
> "7:41", "15:36", "16:36", "21:26", "8:57", "21:08", "18:11"),
>      Year = c(2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L,
>      2014L, 2014L, 2014L), Month = c(2L, 2L, 2L, 2L, 2L, 2L, 2L,
>      2L, 2L, 2L), Day = c(16L, 16L, 17L, 18L, 18L, 18L, 18L, 19L,
>      19L, 20L), BatteryChargeStopDate = c("16/2/2014 17:05", "16/2/2014
> 19:00",
>      "17/2/2014 15:57", "18/2/2014 9:52", "18/2/2014 15:39", "18/2/2014
> 17:36",
>      "19/2/2014 1:55", "19/2/2014 14:25", "20/2/2014 5:17", "20/2/2014 23:20"
>      ), BCStopTime = c("17:05", "19:00", "15:57", "9:52", "15:39",
>      "17:36", "1:55", "14:25", "5:17", "23:20"), Year2 = c(2014L,
>      2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L
>      ), Month2 = c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), Day2 = c(16L,
>      16L, 17L, 18L, 18L, 18L, 19L, 19L, 20L, 20L), Starting_SoC_of_12 =
> c(1L,
>      2L, 4L, 5L, 4L, 2L, 8L, 8L, 4L, 8L), Ending_SoC_of_12 = c(11L,
>      11L, 12L, 8L, 4L, 10L, 12L, 12L, 12L, 12L)), row.names = c(NA,
> 10L), class = "data.frame")
> 
>

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Tue Jul 19 00:48:08 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Tue, 19 Jul 2022 08:48:08 +1000
Subject: [R] Fwd: Extract time and state of charge (Start and End) and Count
In-Reply-To: <CA+8X3fXP=iPTO-ZRTRKmnq0wFuFOJHPzyXfiCknX_UQfbOOafQ@mail.gmail.com>
References: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
 <CA+8X3fXP=iPTO-ZRTRKmnq0wFuFOJHPzyXfiCknX_UQfbOOafQ@mail.gmail.com>
Message-ID: <CA+8X3fVkfUd94vsETGKi3Ex1rNehjpHSKkyq8hdwFtuk8s15ow@mail.gmail.com>

---------- Forwarded message ---------
From: Jim Lemon <drjimlemon at gmail.com>
Date: Tue, Jul 19, 2022 at 8:46 AM
Subject: Re: [R] Extract time and state of charge (Start and End) and Count
To: roslinazairimah zakaria <roslinaump at gmail.com>


Hi Roslina,
If I understand your query, you want something like what is plotted in
the attached code and image. The red points mark the beginning of a
charging session, the green points the end. The orange lines represent
 linear approximations of charging and the blue lines that of
discharging. It is a bit messy, but you can obtain numeric values
along these lines using the approx function at each half hour. Given
your data, that is about the best guess you can make.

Jim



On Mon, Jul 18, 2022 at 2:04 PM roslinazairimah zakaria
<roslinaump at gmail.com> wrote:
>
> Dear all,
>
> I have data of Battery Electric vehicle (BEV). I would like to extract data
> from every hour starting from 0.00 to 0.59, 1:00-1:59 for SOC(state of
> charge) start to end.
>
> Some examples:
> I can extract data from SOC=0 and SOC=12
> dt_2014[which(dt_2014$Starting_SoC_of_12==0 &
> dt_2014$Ending_SoC_of_12==12),]
>
> I can extract data from SOC=1 and SOC=12
> dt_2014[which(dt_2014$Starting_SoC_of_12==1 &
> dt_2014$Ending_SoC_of_12==12),]
>
> and I would like to further categorise the data by hour and count how many
> cars from 0 state charge to 12 state charge at in that particular hour.
>
> Thank you so much for any help given.
>
> Some data
> > dput(dt_2014[1:10,])
> structure(list(?..CarID = c("GC10", "GC10", "GC10", "GC10", "GC10",
> "GC10", "GC10", "GC10", "GC10", "GC10"), BatteryChargeStartDate =
> c("16/2/2014 16:05",
> "16/2/2014 18:20", "17/2/2014 8:10", "18/2/2014 7:41", "18/2/2014 15:36",
> "18/2/2014 16:36", "18/2/2014 21:26", "19/2/2014 8:57", "19/2/2014 21:08",
> "20/2/2014 18:11"), BCStartTime = c("16:05", "18:20", "8:10",
> "7:41", "15:36", "16:36", "21:26", "8:57", "21:08", "18:11"),
>     Year = c(2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L,
>     2014L, 2014L, 2014L), Month = c(2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L), Day = c(16L, 16L, 17L, 18L, 18L, 18L, 18L, 19L,
>     19L, 20L), BatteryChargeStopDate = c("16/2/2014 17:05", "16/2/2014
> 19:00",
>     "17/2/2014 15:57", "18/2/2014 9:52", "18/2/2014 15:39", "18/2/2014
> 17:36",
>     "19/2/2014 1:55", "19/2/2014 14:25", "20/2/2014 5:17", "20/2/2014 23:20"
>     ), BCStopTime = c("17:05", "19:00", "15:57", "9:52", "15:39",
>     "17:36", "1:55", "14:25", "5:17", "23:20"), Year2 = c(2014L,
>     2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L
>     ), Month2 = c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), Day2 = c(16L,
>     16L, 17L, 18L, 18L, 18L, 19L, 19L, 20L, 20L), Starting_SoC_of_12 =
> c(1L,
>     2L, 4L, 5L, 4L, 2L, 8L, 8L, 4L, 8L), Ending_SoC_of_12 = c(11L,
>     11L, 12L, 8L, 4L, 10L, 12L, 12L, 12L, 12L)), row.names = c(NA,
> 10L), class = "data.frame")
>
>
> --
> *Roslinazairimah Zakaria*
> *Tel: +609-5492370; Fax. No.+609-5492766*
>
> *Email: roslinaump at gmail.com <roslinaump at gmail.com>*
> University Malaysia Pahang
> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: rz_19_7_2022.png
Type: image/png
Size: 26780 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220719/a62f2fba/attachment.png>

From gregco@t@ @end|ng |rom me@com  Tue Jul 19 01:50:49 2022
From: gregco@t@ @end|ng |rom me@com (Gregory Coats)
Date: Mon, 18 Jul 2022 19:50:49 -0400
Subject: [R] Using R lines() to show sunrise and sunset
Message-ID: <8A6505C0-10DA-4BD8-81C7-25E18B86498F@me.com>

I compiled a program on my Apple MacBook that takes as inputs
	Year and Month and Day
	Latitude and Longitude
And then computes these two outputs
Sunrise Year-Month-Day Hour:Minute:Second
Sunset  Year-Month-Day Hour:Minute:Second
It automatically handles Daylight Savings Time.
A typical input, followed by the automatically computed outputs looks likes this.

./sunrise_05 2022 01 1 38.8586314239524 77.0512533684194
2022-01-01 07:26:45 2022-01-01 16:57:07
./sunrise_05 2022 01 2 38.8586314239524 77.0512533684194
2022-01-02 07:26:52 2022-01-02 16:57:56

I want to use R?s lines() command to show the sunrise and sunset times for the year 2012. How do I tell R that the first computed output is sunrise, and the second computer output is sunset?
Greg Coats
	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Tue Jul 19 01:58:11 2022
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Mon, 18 Jul 2022 18:58:11 -0500
Subject: [R] Using R lines() to show sunrise and sunset
In-Reply-To: <8A6505C0-10DA-4BD8-81C7-25E18B86498F@me.com>
References: <8A6505C0-10DA-4BD8-81C7-25E18B86498F@me.com>
Message-ID: <9037AD40-0F1B-4A58-9BA3-93DE62948634@comcast.net>

The first thought it to do is make sure you have the data in POSIXct format for the date times. 

Sent from my iPhone

> On Jul 18, 2022, at 6:51 PM, Gregory Coats via R-help <r-help at r-project.org> wrote:
> 
> ?I compiled a program on my Apple MacBook that takes as inputs
>    Year and Month and Day
>    Latitude and Longitude
> And then computes these two outputs
> Sunrise Year-Month-Day Hour:Minute:Second
> Sunset  Year-Month-Day Hour:Minute:Second
> It automatically handles Daylight Savings Time.
> A typical input, followed by the automatically computed outputs looks likes this.
> 
> ./sunrise_05 2022 01 1 38.8586314239524 77.0512533684194
> 2022-01-01 07:26:45 2022-01-01 16:57:07
> ./sunrise_05 2022 01 2 38.8586314239524 77.0512533684194
> 2022-01-02 07:26:52 2022-01-02 16:57:56
> 
> I want to use R?s lines() command to show the sunrise and sunset times for the year 2012. How do I tell R that the first computed output is sunrise, and the second computer output is sunset?
> Greg Coats
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From gregco@t@ @end|ng |rom me@com  Tue Jul 19 02:17:51 2022
From: gregco@t@ @end|ng |rom me@com (Gregory Coats)
Date: Mon, 18 Jul 2022 20:17:51 -0400
Subject: [R] Using R lines() to show sunrise and sunset
In-Reply-To: <9037AD40-0F1B-4A58-9BA3-93DE62948634@comcast.net>
References: <8A6505C0-10DA-4BD8-81C7-25E18B86498F@me.com>
 <9037AD40-0F1B-4A58-9BA3-93DE62948634@comcast.net>
Message-ID: <E76BC257-2E0C-4D96-A6F9-580B1C9D9C16@me.com>

Thanks. Yes, I can follow
https://www.stat.berkeley.edu/~s133/dates.html
Dates and Times in R
But my problem is, How to direct R to accept the first computed value as a sunrise, and the second computed values as a sunset?
Greg Coats
571-423-9847

> On Jul 18, 2022, at 7:58 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> The first thought it to do is make sure you have the data in POSIXct format for the date times. 
> 
> Sent from my iPhone
> 
>> On Jul 18, 2022, at 6:51 PM, Gregory Coats via R-help <r-help at r-project.org> wrote:
>> 
>> ?I compiled a program on my Apple MacBook that takes as inputs
>>   Year and Month and Day
>>   Latitude and Longitude
>> And then computes these two outputs
>> Sunrise Year-Month-Day Hour:Minute:Second
>> Sunset  Year-Month-Day Hour:Minute:Second
>> It automatically handles Daylight Savings Time.
>> A typical input, followed by the automatically computed outputs looks likes this.
>> 
>> ./sunrise_05 2022 01 1 38.8586314239524 77.0512533684194
>> 2022-01-01 07:26:45 2022-01-01 16:57:07
>> ./sunrise_05 2022 01 2 38.8586314239524 77.0512533684194
>> 2022-01-02 07:26:52 2022-01-02 16:57:56
>> 
>> I want to use R?s lines() command to show the sunrise and sunset times for the year 2012. How do I tell R that the first computed output is sunrise, and the second computer output is sunset?
>> Greg Coats
>>   [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Tue Jul 19 02:31:53 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Tue, 19 Jul 2022 00:31:53 +0000
Subject: [R] Using R lines() to show sunrise and sunset
In-Reply-To: <E76BC257-2E0C-4D96-A6F9-580B1C9D9C16@me.com>
References: <8A6505C0-10DA-4BD8-81C7-25E18B86498F@me.com>
 <9037AD40-0F1B-4A58-9BA3-93DE62948634@comcast.net>
 <E76BC257-2E0C-4D96-A6F9-580B1C9D9C16@me.com>
Message-ID: <BN6PR2201MB15536FB4499C5D169406451DCF8F9@BN6PR2201MB1553.namprd22.prod.outlook.com>

Make a dummy variable using an if test. Ifelse(as.numeric(hour(time))<11,df$rise=0,df$rise=1)
Then you could use filter() to split the dataset into two parts, or you could use pivot_wider().
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Gregory Coats via R-help
Sent: Monday, July 18, 2022 8:18 PM
To: David Winsemius <dwinsemius at comcast.net>
Cc: Gregory Coats via R-help <r-help at r-project.org>
Subject: Re: [R] Using R lines() to show sunrise and sunset

[External Email]

Thanks. Yes, I can follow
https://urldefense.proofpoint.com/v2/url?u=https-3A__www.stat.berkeley.edu_-7Es133_dates.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=cc9Eq3DGpCl6kcfJbKVLDsJ9i6TKk7SF79PHaRy8-GoQctAlVTXhQIVEHHwO08Q8&s=ym6pjfQa0pw3I4kXT4_gM3iUDXXy52lWTJkwCbXi2NM&e=
Dates and Times in R
But my problem is, How to direct R to accept the first computed value as a sunrise, and the second computed values as a sunset?
Greg Coats
571-423-9847

> On Jul 18, 2022, at 7:58 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>
> The first thought it to do is make sure you have the data in POSIXct format for the date times.
>
> Sent from my iPhone
>
>> On Jul 18, 2022, at 6:51 PM, Gregory Coats via R-help <r-help at r-project.org> wrote:
>>
>> ?I compiled a program on my Apple MacBook that takes as inputs
>>   Year and Month and Day
>>   Latitude and Longitude
>> And then computes these two outputs
>> Sunrise Year-Month-Day Hour:Minute:Second Sunset  Year-Month-Day 
>> Hour:Minute:Second It automatically handles Daylight Savings Time.
>> A typical input, followed by the automatically computed outputs looks likes this.
>>
>> ./sunrise_05 2022 01 1 38.8586314239524 77.0512533684194
>> 2022-01-01 07:26:45 2022-01-01 16:57:07
>> ./sunrise_05 2022 01 2 38.8586314239524 77.0512533684194
>> 2022-01-02 07:26:52 2022-01-02 16:57:56
>>
>> I want to use R?s lines() command to show the sunrise and sunset times for the year 2012. How do I tell R that the first computed output is sunrise, and the second computer output is sunset?
>> Greg Coats
>>   [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
>> lman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVe
>> AsRzsn7AkP-g&m=cc9Eq3DGpCl6kcfJbKVLDsJ9i6TKk7SF79PHaRy8-GoQctAlVTXhQI
>> VEHHwO08Q8&s=7qZYIyeP1-uuz3co5hy-m-ZI7YGOslQI_-_I89eoJ5I&e=
>> PLEASE do read the posting guide 
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
>> rg_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kV
>> eAsRzsn7AkP-g&m=cc9Eq3DGpCl6kcfJbKVLDsJ9i6TKk7SF79PHaRy8-GoQctAlVTXhQ
>> IVEHHwO08Q8&s=EZBJpqiZa4ZdtRdgor6WYiCsrheRBMsp1O31mCpoHPU&e=
>> and provide commented, minimal, self-contained, reproducible code.


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=cc9Eq3DGpCl6kcfJbKVLDsJ9i6TKk7SF79PHaRy8-GoQctAlVTXhQIVEHHwO08Q8&s=7qZYIyeP1-uuz3co5hy-m-ZI7YGOslQI_-_I89eoJ5I&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwIFaQ&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=cc9Eq3DGpCl6kcfJbKVLDsJ9i6TKk7SF79PHaRy8-GoQctAlVTXhQIVEHHwO08Q8&s=EZBJpqiZa4ZdtRdgor6WYiCsrheRBMsp1O31mCpoHPU&e=
and provide commented, minimal, self-contained, reproducible code.

From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Jul 19 03:26:59 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 18 Jul 2022 18:26:59 -0700
Subject: [R] Using R lines() to show sunrise and sunset
In-Reply-To: <E76BC257-2E0C-4D96-A6F9-580B1C9D9C16@me.com>
References: <8A6505C0-10DA-4BD8-81C7-25E18B86498F@me.com>
 <9037AD40-0F1B-4A58-9BA3-93DE62948634@comcast.net>
 <E76BC257-2E0C-4D96-A6F9-580B1C9D9C16@me.com>
Message-ID: <84C8717D-2B11-4470-94E1-1BF669A9A5CB@dcn.davis.ca.us>

a) Wouldn't distinguishing between sunrise and sunset just amount to referring to the relevant column in your data frame? Why is this a problem you need help with?

b) There is no problem with plotting datetimes in R. But you might want to consider plotting hour-of-day instead on the y axis to avoid the monotonocally increasing appearance you would end up with. R does not have a concept of time-of-day separate from date, due to different days having different numbers of hours in them. To compute hour-of-day subtract the beginning of day (use trunc.POSIXt, units = "days") from the actual time and convert the resulting difftime to numeric. 

3) General challenges with formulating a response: I can't tell what to do with your command lines, and your use of formatted email is mushing stuff together. You have been vague about what inputs you have (have you imported the data?) or what kind of plot you expect to get using the lines function. (Also I avoid base R plotting functions most of the time, so I tend to sit out on threads that are looking for help with it.)

On July 18, 2022 5:17:51 PM PDT, Gregory Coats via R-help <r-help at r-project.org> wrote:
>Thanks. Yes, I can follow
>https://www.stat.berkeley.edu/~s133/dates.html
>Dates and Times in R
>But my problem is, How to direct R to accept the first computed value as a sunrise, and the second computed values as a sunset?
>Greg Coats
>571-423-9847
>
>> On Jul 18, 2022, at 7:58 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>> 
>> The first thought it to do is make sure you have the data in POSIXct format for the date times. 
>> 
>> Sent from my iPhone
>> 
>>> On Jul 18, 2022, at 6:51 PM, Gregory Coats via R-help <r-help at r-project.org> wrote:
>>> 
>>> ?I compiled a program on my Apple MacBook that takes as inputs
>>>   Year and Month and Day
>>>   Latitude and Longitude
>>> And then computes these two outputs
>>> Sunrise Year-Month-Day Hour:Minute:Second
>>> Sunset  Year-Month-Day Hour:Minute:Second
>>> It automatically handles Daylight Savings Time.
>>> A typical input, followed by the automatically computed outputs looks likes this.
>>> 
>>> ./sunrise_05 2022 01 1 38.8586314239524 77.0512533684194
>>> 2022-01-01 07:26:45 2022-01-01 16:57:07
>>> ./sunrise_05 2022 01 2 38.8586314239524 77.0512533684194
>>> 2022-01-02 07:26:52 2022-01-02 16:57:56
>>> 
>>> I want to use R?s lines() command to show the sunrise and sunset times for the year 2012. How do I tell R that the first computed output is sunrise, and the second computer output is sunset?
>>> Greg Coats
>>>   [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From bgunter@4567 @end|ng |rom gm@||@com  Tue Jul 19 03:43:21 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 18 Jul 2022 18:43:21 -0700
Subject: [R] Using R lines() to show sunrise and sunset
In-Reply-To: <8A6505C0-10DA-4BD8-81C7-25E18B86498F@me.com>
References: <8A6505C0-10DA-4BD8-81C7-25E18B86498F@me.com>
Message-ID: <CAGxFJbSoHMB_By3bSxgas_x7OPHNnTBgdDUopBFamhA=4sSUWw@mail.gmail.com>

Your description is insufficiently precise for us to do more than guess at
what you have. I shall assume your data consists of text strings as shown.
Presumably, you can remove the inputs and collect the outputs into a data
frame, e.g.

dat <- read.table( text = "
2022-01-01 07:26:45 2022-01-01 16:57:07
2022-01-02 07:26:52 2022-01-02 16:57:56"
)[,-3]
## there are 4 text columns. The [,-3] removes the replicated date.

Then, as Jeff suggested,

names(dat) <- c('Date','Sunrise', 'Sunset')
dat
## gives:
          Date  Sunrise   Sunset
1 2022-01-01 07:26:45 16:57:07
2 2022-01-02 07:26:52 16:57:56

But, again, as you have not described your data structure in sufficient
detail (at least for me), this is just a guess.

Once you have the data in this format, you can extract what you need and
maybe use strptime() to convert to appropriate numeric formats. Or maybe
simply use strplit():

> strsplit(dat$Sunrise, split = ":")
[[1]]
[1] "07" "26" "45"

[[2]]
[1] "07" "26" "52"

Note: as.numeric() converts number strings to numbers:
> as.numeric("45")
[1] 45

Again, just a guess, and therefore perhaps useless.

-- Bert


On Mon, Jul 18, 2022 at 4:51 PM Gregory Coats via R-help <
r-help at r-project.org> wrote:

> I compiled a program on my Apple MacBook that takes as inputs
>         Year and Month and Day
>         Latitude and Longitude
> And then computes these two outputs
> Sunrise Year-Month-Day Hour:Minute:Second
> Sunset  Year-Month-Day Hour:Minute:Second
> It automatically handles Daylight Savings Time.
> A typical input, followed by the automatically computed outputs looks
> likes this.
>
> ./sunrise_05 2022 01 1 38.8586314239524 77.0512533684194
> 2022-01-01 07:26:45 2022-01-01 16:57:07
> ./sunrise_05 2022 01 2 38.8586314239524 77.0512533684194
> 2022-01-02 07:26:52 2022-01-02 16:57:56
>
> I want to use R?s lines() command to show the sunrise and sunset times for
> the year 2012. How do I tell R that the first computed output is sunrise,
> and the second computer output is sunset?
> Greg Coats
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @vi@e@gross m@iii@g oii gm@ii@com  Tue Jul 19 04:09:36 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Mon, 18 Jul 2022 22:09:36 -0400
Subject: [R] Using R lines() to show sunrise and sunset
In-Reply-To: <E76BC257-2E0C-4D96-A6F9-580B1C9D9C16@me.com>
References: <8A6505C0-10DA-4BD8-81C7-25E18B86498F@me.com>
 <9037AD40-0F1B-4A58-9BA3-93DE62948634@comcast.net>
 <E76BC257-2E0C-4D96-A6F9-580B1C9D9C16@me.com>
Message-ID: <008701d89b14$988dca30$c9a95e90$@gmail.com>

Questions like this keep popping up.

Your example shows a single string containing 4 parts: 2022-01-01 07:26:45 2022-01-01 16:57:07

So forget how to ask lines() to do anything. Ask how YOU can make what lines wants.

You need to preprocess what sounds like a vector of such strings into four parts and conveniently you have spaces between them.
2022-01-01
07:26:45
2022-01-01
16:57:07

As it happens, your data is measuring sunrise and sunset on the same day so I suspect you can toss the third part as redundant.

How you extract them is one of many ways. Pick any you like. But use the parts to make three vectors with names like theDate, sunUp and sundown that are character strings.

Use one of many ways to convert the above into a pure Date format and a pure Time Format of your choice as long as your choice of the plot() function can handle them. Using the same names above, you would call plot once with the appropriate arguments to set up your graph and hand it theDate and sunup to place on the axes you want. Then, before the plot is closed, add a call to lines() using theDate and sundown.

The answer as to how to tell R or lines() (neither of which are sentient or will pay any attention) is to ask a different question. What arguments do you provide to lines. The answer is you provide two different vectors with each being of a type that plot() and lines() and other such functions can handle. They do not handle text for plotting. They handle integers, floating point, or things that can in some way be converted to them such as the number of seconds since midnight or the number of seconds since Jan 1, 1970. 

Some would also suggest using ggplot() instead as it allows you to specify multiple sets of lines in a somewhat different way and supplies much more. 

The first step is asking the right question. You may have intended it but I repeat, NOTHING will get plot() or line() to handle your text version directly. What you want is to feed it what it wants.

Some have suggested specifics and there should be enough info for you to do some reading and put the pieces together.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Gregory Coats via R-help
Sent: Monday, July 18, 2022 8:18 PM
To: David Winsemius <dwinsemius at comcast.net>
Cc: Gregory Coats via R-help <r-help at r-project.org>
Subject: Re: [R] Using R lines() to show sunrise and sunset

Thanks. Yes, I can follow
https://www.stat.berkeley.edu/~s133/dates.html
Dates and Times in R
But my problem is, How to direct R to accept the first computed value as a sunrise, and the second computed values as a sunset?
Greg Coats
571-423-9847

> On Jul 18, 2022, at 7:58 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> The first thought it to do is make sure you have the data in POSIXct format for the date times. 
> 
> Sent from my iPhone
> 
>> On Jul 18, 2022, at 6:51 PM, Gregory Coats via R-help <r-help at r-project.org> wrote:
>> 
>> ?I compiled a program on my Apple MacBook that takes as inputs
>>   Year and Month and Day
>>   Latitude and Longitude
>> And then computes these two outputs
>> Sunrise Year-Month-Day Hour:Minute:Second Sunset  Year-Month-Day 
>> Hour:Minute:Second It automatically handles Daylight Savings Time.
>> A typical input, followed by the automatically computed outputs looks likes this.
>> 
>> ./sunrise_05 2022 01 1 38.8586314239524 77.0512533684194
>> 2022-01-01 07:26:45 2022-01-01 16:57:07
>> ./sunrise_05 2022 01 2 38.8586314239524 77.0512533684194
>> 2022-01-02 07:26:52 2022-01-02 16:57:56
>> 
>> I want to use R?s lines() command to show the sunrise and sunset times for the year 2012. How do I tell R that the first computed output is sunrise, and the second computer output is sunset?
>> Greg Coats
>>   [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ro@||n@ump @end|ng |rom gm@||@com  Tue Jul 19 04:46:40 2022
From: ro@||n@ump @end|ng |rom gm@||@com (roslinazairimah zakaria)
Date: Tue, 19 Jul 2022 10:46:40 +0800
Subject: [R] Extract time and state of charge (Start and End) and Count
In-Reply-To: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
References: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
Message-ID: <CANTvJZJETzjMOMzAF5ZMs4dw90WXTr0OvsNqJE=BWHOEyvzsuA@mail.gmail.com>

Thank you so much Avi, Rui and Jim...really appreciate your help.
I am so sorry for not able to make it clearly.

This is the expected output:
  Hour  Starting_SoC_of_12     Ending_SoC_of_12 frequency
   0      0                                    12
    603
   1      1                                    12
    136

dt <- dt3[which(dt3$Year==2014),]
head(dt); tail(dt)
dput(dt[1:5000,])

cols <- c("BatteryChargeStartDate", "BatteryChargeStopDate")
dt[cols] <- lapply(dt[cols], \(x) sub("\n", " ", x))

# use package lubridate to coerce to a datetime class
dt[cols] <- lapply(dt[cols], lubridate::dmy_hm)

h <- lubridate::hour(dt[["BatteryChargeStartDate"]])
aggregate(Starting_SoC_of_12 ~ h, dt, length)

Expected output:
  Hour  Starting_SoC_of_12 Ending_SoC_of_12 frequency
   0      0                     12             603
   1      1                     12             136




On Mon, Jul 18, 2022 at 12:04 PM roslinazairimah zakaria <
roslinaump at gmail.com> wrote:

> Dear all,
>
> I have data of Battery Electric vehicle (BEV). I would like to extract
> data from every hour starting from 0.00 to 0.59, 1:00-1:59 for SOC(state of
> charge) start to end.
>
> Some examples:
> I can extract data from SOC=0 and SOC=12
> dt_2014[which(dt_2014$Starting_SoC_of_12==0 &
> dt_2014$Ending_SoC_of_12==12),]
>
> I can extract data from SOC=1 and SOC=12
> dt_2014[which(dt_2014$Starting_SoC_of_12==1 &
> dt_2014$Ending_SoC_of_12==12),]
>
> and I would like to further categorise the data by hour and count how many
> cars from 0 state charge to 12 state charge at in that particular hour.
>
> Thank you so much for any help given.
>
> Some data
> > dput(dt_2014[1:10,])
> structure(list(?..CarID = c("GC10", "GC10", "GC10", "GC10", "GC10",
> "GC10", "GC10", "GC10", "GC10", "GC10"), BatteryChargeStartDate =
> c("16/2/2014 16:05",
> "16/2/2014 18:20", "17/2/2014 8:10", "18/2/2014 7:41", "18/2/2014 15:36",
> "18/2/2014 16:36", "18/2/2014 21:26", "19/2/2014 8:57", "19/2/2014 21:08",
> "20/2/2014 18:11"), BCStartTime = c("16:05", "18:20", "8:10",
> "7:41", "15:36", "16:36", "21:26", "8:57", "21:08", "18:11"),
>     Year = c(2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L,
>     2014L, 2014L, 2014L), Month = c(2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L), Day = c(16L, 16L, 17L, 18L, 18L, 18L, 18L, 19L,
>     19L, 20L), BatteryChargeStopDate = c("16/2/2014 17:05", "16/2/2014
> 19:00",
>     "17/2/2014 15:57", "18/2/2014 9:52", "18/2/2014 15:39", "18/2/2014
> 17:36",
>     "19/2/2014 1:55", "19/2/2014 14:25", "20/2/2014 5:17", "20/2/2014
> 23:20"
>     ), BCStopTime = c("17:05", "19:00", "15:57", "9:52", "15:39",
>     "17:36", "1:55", "14:25", "5:17", "23:20"), Year2 = c(2014L,
>     2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L
>     ), Month2 = c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), Day2 = c(16L,
>     16L, 17L, 18L, 18L, 18L, 19L, 19L, 20L, 20L), Starting_SoC_of_12 =
> c(1L,
>     2L, 4L, 5L, 4L, 2L, 8L, 8L, 4L, 8L), Ending_SoC_of_12 = c(11L,
>     11L, 12L, 8L, 4L, 10L, 12L, 12L, 12L, 12L)), row.names = c(NA,
> 10L), class = "data.frame")
>
>
> --
> *Roslinazairimah Zakaria*
> *Tel: +609-5492370; Fax. No.+609-5492766*
>
> *Email: roslinaump at gmail.com <roslinaump at gmail.com>*
> University Malaysia Pahang
> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>


-- 
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From ro@||n@ump @end|ng |rom gm@||@com  Tue Jul 19 05:01:29 2022
From: ro@||n@ump @end|ng |rom gm@||@com (roslinazairimah zakaria)
Date: Tue, 19 Jul 2022 11:01:29 +0800
Subject: [R] Extract time and state of charge (Start and End) and Count
In-Reply-To: <CANTvJZJETzjMOMzAF5ZMs4dw90WXTr0OvsNqJE=BWHOEyvzsuA@mail.gmail.com>
References: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
 <CANTvJZJETzjMOMzAF5ZMs4dw90WXTr0OvsNqJE=BWHOEyvzsuA@mail.gmail.com>
Message-ID: <CANTvJZLTq+cDMviguCX_bcDdSE_Qc4Bsz9CMxyJ1kYDrY3yp6g@mail.gmail.com>

Dear all,

This is the desired expected output:
State of Charge (start_end SOC)
Hour 0-12 1-12 2-12 3-12 4-12 5-12 6-12 7-12 8-12 9-12 10-12 11-12 12-12
0
1
2
?
20
21
22
23

On Tue, Jul 19, 2022 at 10:46 AM roslinazairimah zakaria <
roslinaump at gmail.com> wrote:

> Thank you so much Avi, Rui and Jim...really appreciate your help.
> I am so sorry for not able to make it clearly.
>
> This is the expected output:
>   Hour  Starting_SoC_of_12     Ending_SoC_of_12 frequency
>    0      0                                    12
>     603
>    1      1                                    12
>     136
>
> dt <- dt3[which(dt3$Year==2014),]
> head(dt); tail(dt)
> dput(dt[1:5000,])
>
> cols <- c("BatteryChargeStartDate", "BatteryChargeStopDate")
> dt[cols] <- lapply(dt[cols], \(x) sub("\n", " ", x))
>
> # use package lubridate to coerce to a datetime class
> dt[cols] <- lapply(dt[cols], lubridate::dmy_hm)
>
> h <- lubridate::hour(dt[["BatteryChargeStartDate"]])
> aggregate(Starting_SoC_of_12 ~ h, dt, length)
>
> Expected output:
>   Hour  Starting_SoC_of_12 Ending_SoC_of_12 frequency
>    0      0                     12             603
>    1      1                     12             136
>
>
>
>
> On Mon, Jul 18, 2022 at 12:04 PM roslinazairimah zakaria <
> roslinaump at gmail.com> wrote:
>
>> Dear all,
>>
>> I have data of Battery Electric vehicle (BEV). I would like to extract
>> data from every hour starting from 0.00 to 0.59, 1:00-1:59 for SOC(state of
>> charge) start to end.
>>
>> Some examples:
>> I can extract data from SOC=0 and SOC=12
>> dt_2014[which(dt_2014$Starting_SoC_of_12==0 &
>> dt_2014$Ending_SoC_of_12==12),]
>>
>> I can extract data from SOC=1 and SOC=12
>> dt_2014[which(dt_2014$Starting_SoC_of_12==1 &
>> dt_2014$Ending_SoC_of_12==12),]
>>
>> and I would like to further categorise the data by hour and count how
>> many cars from 0 state charge to 12 state charge at in that particular hour.
>>
>> Thank you so much for any help given.
>>
>> Some data
>> > dput(dt_2014[1:10,])
>> structure(list(?..CarID = c("GC10", "GC10", "GC10", "GC10", "GC10",
>> "GC10", "GC10", "GC10", "GC10", "GC10"), BatteryChargeStartDate =
>> c("16/2/2014 16:05",
>> "16/2/2014 18:20", "17/2/2014 8:10", "18/2/2014 7:41", "18/2/2014 15:36",
>> "18/2/2014 16:36", "18/2/2014 21:26", "19/2/2014 8:57", "19/2/2014
>> 21:08",
>> "20/2/2014 18:11"), BCStartTime = c("16:05", "18:20", "8:10",
>> "7:41", "15:36", "16:36", "21:26", "8:57", "21:08", "18:11"),
>>     Year = c(2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L,
>>     2014L, 2014L, 2014L), Month = c(2L, 2L, 2L, 2L, 2L, 2L, 2L,
>>     2L, 2L, 2L), Day = c(16L, 16L, 17L, 18L, 18L, 18L, 18L, 19L,
>>     19L, 20L), BatteryChargeStopDate = c("16/2/2014 17:05", "16/2/2014
>> 19:00",
>>     "17/2/2014 15:57", "18/2/2014 9:52", "18/2/2014 15:39", "18/2/2014
>> 17:36",
>>     "19/2/2014 1:55", "19/2/2014 14:25", "20/2/2014 5:17", "20/2/2014
>> 23:20"
>>     ), BCStopTime = c("17:05", "19:00", "15:57", "9:52", "15:39",
>>     "17:36", "1:55", "14:25", "5:17", "23:20"), Year2 = c(2014L,
>>     2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L
>>     ), Month2 = c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), Day2 = c(16L,
>>     16L, 17L, 18L, 18L, 18L, 19L, 19L, 20L, 20L), Starting_SoC_of_12 =
>> c(1L,
>>     2L, 4L, 5L, 4L, 2L, 8L, 8L, 4L, 8L), Ending_SoC_of_12 = c(11L,
>>     11L, 12L, 8L, 4L, 10L, 12L, 12L, 12L, 12L)), row.names = c(NA,
>> 10L), class = "data.frame")
>>
>>
>> --
>> *Roslinazairimah Zakaria*
>> *Tel: +609-5492370; Fax. No.+609-5492766*
>>
>> *Email: roslinaump at gmail.com <roslinaump at gmail.com>*
>> University Malaysia Pahang
>> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>>
>
>
> --
> *Roslinazairimah Zakaria*
> *Tel: +609-5492370; Fax. No.+609-5492766*
>
> *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
> roslinaump at gmail.com <roslinaump at gmail.com>*
> Faculty of Industrial Sciences & Technology
> University Malaysia Pahang
> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>


-- 
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Tue Jul 19 09:05:45 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Tue, 19 Jul 2022 17:05:45 +1000
Subject: [R] Extract time and state of charge (Start and End) and Count
In-Reply-To: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
References: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
Message-ID: <CA+8X3fVKvf220aOQp-mm1cKtkz+gmyL5Qd6O5WV7ABv3QrP3Rw@mail.gmail.com>

Hi Roslina,
The following gives you the state of charge for the vehicle in your
example data for each hour. This is approximate as your times are not
on even hours.

# get the temporal order of observations
obs_order<-order(c(as.numeric(dt_2014$bc_start),as.numeric(dt_2014$bc_stop)))
numeric_time<-c(as.numeric(dt_2014$bc_start),as.numeric(dt_2014$bc_stop))[obs_order]
nobs<-diff(range(numeric_time))/3600
# find the linear approximation of charge state by hours
hourly_SoC<-approx(numeric_time,
 c(dt_2014$Starting_SoC_of_12,dt_2014$Ending_SoC_of_12)[obs_order],n=nobs)

To get the POSIX times:

hourly_POSIX<-seq(dt_2014$bc_start[1],dt_2014$bc_stop,length.out=nobs)

That will fill part of your table. If you want the state of charge by
hour regardless of day, you'll have to create a new "hour" variable
from, hourly_POSIX, then:

mean_charge_by_hour<-by(hourly_SoC$hour,hourly_SoC$y,mean) #untested
since I don't know whether you want this

Jim

On Mon, Jul 18, 2022 at 2:04 PM roslinazairimah zakaria
<roslinaump at gmail.com> wrote:
>
> Dear all,
>
> I have data of Battery Electric vehicle (BEV). I would like to extract data
> from every hour starting from 0.00 to 0.59, 1:00-1:59 for SOC(state of
> charge) start to end.
>
> Some examples:
> I can extract data from SOC=0 and SOC=12
> dt_2014[which(dt_2014$Starting_SoC_of_12==0 &
> dt_2014$Ending_SoC_of_12==12),]
>
> I can extract data from SOC=1 and SOC=12
> dt_2014[which(dt_2014$Starting_SoC_of_12==1 &
> dt_2014$Ending_SoC_of_12==12),]
>
> and I would like to further categorise the data by hour and count how many
> cars from 0 state charge to 12 state charge at in that particular hour.
>
> Thank you so much for any help given.
>
> Some data
> > dput(dt_2014[1:10,])
> structure(list(?..CarID = c("GC10", "GC10", "GC10", "GC10", "GC10",
> "GC10", "GC10", "GC10", "GC10", "GC10"), BatteryChargeStartDate =
> c("16/2/2014 16:05",
> "16/2/2014 18:20", "17/2/2014 8:10", "18/2/2014 7:41", "18/2/2014 15:36",
> "18/2/2014 16:36", "18/2/2014 21:26", "19/2/2014 8:57", "19/2/2014 21:08",
> "20/2/2014 18:11"), BCStartTime = c("16:05", "18:20", "8:10",
> "7:41", "15:36", "16:36", "21:26", "8:57", "21:08", "18:11"),
>     Year = c(2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L,
>     2014L, 2014L, 2014L), Month = c(2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L), Day = c(16L, 16L, 17L, 18L, 18L, 18L, 18L, 19L,
>     19L, 20L), BatteryChargeStopDate = c("16/2/2014 17:05", "16/2/2014
> 19:00",
>     "17/2/2014 15:57", "18/2/2014 9:52", "18/2/2014 15:39", "18/2/2014
> 17:36",
>     "19/2/2014 1:55", "19/2/2014 14:25", "20/2/2014 5:17", "20/2/2014 23:20"
>     ), BCStopTime = c("17:05", "19:00", "15:57", "9:52", "15:39",
>     "17:36", "1:55", "14:25", "5:17", "23:20"), Year2 = c(2014L,
>     2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L
>     ), Month2 = c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), Day2 = c(16L,
>     16L, 17L, 18L, 18L, 18L, 19L, 19L, 20L, 20L), Starting_SoC_of_12 =
> c(1L,
>     2L, 4L, 5L, 4L, 2L, 8L, 8L, 4L, 8L), Ending_SoC_of_12 = c(11L,
>     11L, 12L, 8L, 4L, 10L, 12L, 12L, 12L, 12L)), row.names = c(NA,
> 10L), class = "data.frame")
>
>
> --
> *Roslinazairimah Zakaria*
> *Tel: +609-5492370; Fax. No.+609-5492766*
>
> *Email: roslinaump at gmail.com <roslinaump at gmail.com>*
> University Malaysia Pahang
> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ro@||n@ump @end|ng |rom gm@||@com  Tue Jul 19 09:22:06 2022
From: ro@||n@ump @end|ng |rom gm@||@com (roslinazairimah zakaria)
Date: Tue, 19 Jul 2022 15:22:06 +0800
Subject: [R] Extract time and state of charge (Start and End) and Count
In-Reply-To: <CA+8X3fVKvf220aOQp-mm1cKtkz+gmyL5Qd6O5WV7ABv3QrP3Rw@mail.gmail.com>
References: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
 <CA+8X3fVKvf220aOQp-mm1cKtkz+gmyL5Qd6O5WV7ABv3QrP3Rw@mail.gmail.com>
Message-ID: <CANTvJZKhpMkN4Bhfy0d4p07HncWtt_C-PyLKQ+LS62E5yfbjNw@mail.gmail.com>

Jim,

Thank you so much!

On Tue, Jul 19, 2022 at 3:05 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Roslina,
> The following gives you the state of charge for the vehicle in your
> example data for each hour. This is approximate as your times are not
> on even hours.
>
> # get the temporal order of observations
>
> obs_order<-order(c(as.numeric(dt_2014$bc_start),as.numeric(dt_2014$bc_stop)))
>
> numeric_time<-c(as.numeric(dt_2014$bc_start),as.numeric(dt_2014$bc_stop))[obs_order]
> nobs<-diff(range(numeric_time))/3600
> # find the linear approximation of charge state by hours
> hourly_SoC<-approx(numeric_time,
>  c(dt_2014$Starting_SoC_of_12,dt_2014$Ending_SoC_of_12)[obs_order],n=nobs)
>
> To get the POSIX times:
>
> hourly_POSIX<-seq(dt_2014$bc_start[1],dt_2014$bc_stop,length.out=nobs)
>
> That will fill part of your table. If you want the state of charge by
> hour regardless of day, you'll have to create a new "hour" variable
> from, hourly_POSIX, then:
>
> mean_charge_by_hour<-by(hourly_SoC$hour,hourly_SoC$y,mean) #untested
> since I don't know whether you want this
>
> Jim
>
> On Mon, Jul 18, 2022 at 2:04 PM roslinazairimah zakaria
> <roslinaump at gmail.com> wrote:
> >
> > Dear all,
> >
> > I have data of Battery Electric vehicle (BEV). I would like to extract
> data
> > from every hour starting from 0.00 to 0.59, 1:00-1:59 for SOC(state of
> > charge) start to end.
> >
> > Some examples:
> > I can extract data from SOC=0 and SOC=12
> > dt_2014[which(dt_2014$Starting_SoC_of_12==0 &
> > dt_2014$Ending_SoC_of_12==12),]
> >
> > I can extract data from SOC=1 and SOC=12
> > dt_2014[which(dt_2014$Starting_SoC_of_12==1 &
> > dt_2014$Ending_SoC_of_12==12),]
> >
> > and I would like to further categorise the data by hour and count how
> many
> > cars from 0 state charge to 12 state charge at in that particular hour.
> >
> > Thank you so much for any help given.
> >
> > Some data
> > > dput(dt_2014[1:10,])
> > structure(list(?..CarID = c("GC10", "GC10", "GC10", "GC10", "GC10",
> > "GC10", "GC10", "GC10", "GC10", "GC10"), BatteryChargeStartDate =
> > c("16/2/2014 16:05",
> > "16/2/2014 18:20", "17/2/2014 8:10", "18/2/2014 7:41", "18/2/2014 15:36",
> > "18/2/2014 16:36", "18/2/2014 21:26", "19/2/2014 8:57", "19/2/2014
> 21:08",
> > "20/2/2014 18:11"), BCStartTime = c("16:05", "18:20", "8:10",
> > "7:41", "15:36", "16:36", "21:26", "8:57", "21:08", "18:11"),
> >     Year = c(2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L,
> >     2014L, 2014L, 2014L), Month = c(2L, 2L, 2L, 2L, 2L, 2L, 2L,
> >     2L, 2L, 2L), Day = c(16L, 16L, 17L, 18L, 18L, 18L, 18L, 19L,
> >     19L, 20L), BatteryChargeStopDate = c("16/2/2014 17:05", "16/2/2014
> > 19:00",
> >     "17/2/2014 15:57", "18/2/2014 9:52", "18/2/2014 15:39", "18/2/2014
> > 17:36",
> >     "19/2/2014 1:55", "19/2/2014 14:25", "20/2/2014 5:17", "20/2/2014
> 23:20"
> >     ), BCStopTime = c("17:05", "19:00", "15:57", "9:52", "15:39",
> >     "17:36", "1:55", "14:25", "5:17", "23:20"), Year2 = c(2014L,
> >     2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L
> >     ), Month2 = c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), Day2 = c(16L,
> >     16L, 17L, 18L, 18L, 18L, 19L, 19L, 20L, 20L), Starting_SoC_of_12 =
> > c(1L,
> >     2L, 4L, 5L, 4L, 2L, 8L, 8L, 4L, 8L), Ending_SoC_of_12 = c(11L,
> >     11L, 12L, 8L, 4L, 10L, 12L, 12L, 12L, 12L)), row.names = c(NA,
> > 10L), class = "data.frame")
> >
> >
> > --
> > *Roslinazairimah Zakaria*
> > *Tel: +609-5492370; Fax. No.+609-5492766*
> >
> > *Email: roslinaump at gmail.com <roslinaump at gmail.com>*
> > University Malaysia Pahang
> > Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>


-- 
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From ro@||n@ump @end|ng |rom gm@||@com  Tue Jul 19 10:53:10 2022
From: ro@||n@ump @end|ng |rom gm@||@com (roslinazairimah zakaria)
Date: Tue, 19 Jul 2022 16:53:10 +0800
Subject: [R] Extract time and state of charge (Start and End) and Count
In-Reply-To: <a4a57470-481e-ad36-a577-eb54497a50da@sapo.pt>
References: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
 <a4a57470-481e-ad36-a577-eb54497a50da@sapo.pt>
Message-ID: <CANTvJZL40HD9ZGpr1SJwVQzd4yd3p8jZxUH6SittGqXuBgDrhA@mail.gmail.com>

Hi Rui,

Yes, I would like to count for each hour, how many in the state of charge
start 0 and SOC 12, then SOC 1 and SOC 12 and so on.

Thank you for your help.

On Tue, Jul 19, 2022 at 1:11 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> I'm not sure I understand the problem. Do you want counts of how many
> rows are there per hour?
>
>
> # these columns need to be fixed
> cols <- c("BatteryChargeStartDate", "BatteryChargeStopDate")
> dt_2014[cols] <- lapply(dt_2014[cols], \(x) sub("\n", " ", x))
> # use package lubridate to coerce to a datetime class
> dt_2014[cols] <- lapply(dt_2014[cols], lubridate::dmy_hm)
>
> h <- lubridate::hour(dt_2014[["BatteryChargeStartDate"]])
> aggregate(Starting_SoC_of_12 ~ h, dt_2014, length)
>
>
>
> It would be better if you post the expected output corresponding to the
> posted data set.
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 05:04 de 18/07/2022, roslinazairimah zakaria escreveu:
> > Dear all,
> >
> > I have data of Battery Electric vehicle (BEV). I would like to extract
> data
> > from every hour starting from 0.00 to 0.59, 1:00-1:59 for SOC(state of
> > charge) start to end.
> >
> > Some examples:
> > I can extract data from SOC=0 and SOC=12
> > dt_2014[which(dt_2014$Starting_SoC_of_12==0 &
> > dt_2014$Ending_SoC_of_12==12),]
> >
> > I can extract data from SOC=1 and SOC=12
> > dt_2014[which(dt_2014$Starting_SoC_of_12==1 &
> > dt_2014$Ending_SoC_of_12==12),]
> >
> > and I would like to further categorise the data by hour and count how
> many
> > cars from 0 state charge to 12 state charge at in that particular hour.
> >
> > Thank you so much for any help given.
> >
> > Some data
> >> dput(dt_2014[1:10,])
> > structure(list(?..CarID = c("GC10", "GC10", "GC10", "GC10", "GC10",
> > "GC10", "GC10", "GC10", "GC10", "GC10"), BatteryChargeStartDate =
> > c("16/2/2014 16:05",
> > "16/2/2014 18:20", "17/2/2014 8:10", "18/2/2014 7:41", "18/2/2014 15:36",
> > "18/2/2014 16:36", "18/2/2014 21:26", "19/2/2014 8:57", "19/2/2014
> 21:08",
> > "20/2/2014 18:11"), BCStartTime = c("16:05", "18:20", "8:10",
> > "7:41", "15:36", "16:36", "21:26", "8:57", "21:08", "18:11"),
> >      Year = c(2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L,
> >      2014L, 2014L, 2014L), Month = c(2L, 2L, 2L, 2L, 2L, 2L, 2L,
> >      2L, 2L, 2L), Day = c(16L, 16L, 17L, 18L, 18L, 18L, 18L, 19L,
> >      19L, 20L), BatteryChargeStopDate = c("16/2/2014 17:05", "16/2/2014
> > 19:00",
> >      "17/2/2014 15:57", "18/2/2014 9:52", "18/2/2014 15:39", "18/2/2014
> > 17:36",
> >      "19/2/2014 1:55", "19/2/2014 14:25", "20/2/2014 5:17", "20/2/2014
> 23:20"
> >      ), BCStopTime = c("17:05", "19:00", "15:57", "9:52", "15:39",
> >      "17:36", "1:55", "14:25", "5:17", "23:20"), Year2 = c(2014L,
> >      2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L
> >      ), Month2 = c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), Day2 = c(16L,
> >      16L, 17L, 18L, 18L, 18L, 19L, 19L, 20L, 20L), Starting_SoC_of_12 =
> > c(1L,
> >      2L, 4L, 5L, 4L, 2L, 8L, 8L, 4L, 8L), Ending_SoC_of_12 = c(11L,
> >      11L, 12L, 8L, 4L, 10L, 12L, 12L, 12L, 12L)), row.names = c(NA,
> > 10L), class = "data.frame")
> >
> >
>


-- 
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From j@de@shod@@ m@iii@g oii googiem@ii@com  Tue Jul 19 17:07:42 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Tue, 19 Jul 2022 16:07:42 +0100
Subject: [R] mgcv: relative risk from GAM with distributed lag
Message-ID: <CANg3_k-18ND8DrwfYJmGAN5hq18QweZFPp4GhCuHwB=XFGAFiA@mail.gmail.com>

Dear list members,

Does anyone know how to obtain a relative risk/ risk ratio from a GAM
with a distributed lag model implemented in mgcv? I have a GAM
predicting daily deaths from time series data consisting of daily
temperature, humidity and rainfall. The GAM includes a distributed lag
model because deaths may occur over several days following a high heat
day.

What I'd like to do is compute (and plot) the relative risk
(accumulated across all lags) for a given temperature vs the
temperature at which the risk is lowest, with corresponding confidence
intervals. I am aware of the predict.gam function but am not sure if
and how it should be used in this case. (Additionally, I'd also like
to plot the relative risk for different lags separately).

I apologise if this seems trivial to some. (Actually, I hope it is,
because that might mean I get a solution!) I've been looking for
examples on how to do this, but found nothing so far. Suggestions
would be very much appreciated!

Below is a reproducible example with the GAM:

library(mgcv)
set.seed(3) # make reproducible example
simdat <- gamSim(1,400) # simulate data
g <- exp(simdat$f/5)
simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
simdat$time <- 1:400  # create time series
names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
"f0", "f1", "f2", "f3", "time")

# lag function based on Simon Wood (book 2017, p.349 and gamair
package documentation p.54
# https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
lagard <- function(x,n.lag=7) {
n <- length(x); X <- matrix(NA,n,n.lag)
for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
X
}

# set up lag, temp, rain and humidity as 7-column matrices
# to create lagged variables - based on Simon Wood's example
dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
deaths=simdat$deaths, time = simdat$time)
dat$temp <- lagard(simdat$temp)
dat$rain <- lagard(simdat$rain)
dat$humidity <- lagard(simdat$humidity)

mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
family = nb, method = 'REML', select = TRUE)

summary(mod)
plot(mod, scheme = 1)
plot(mod, scheme = 2)

Thanks for any suggestions you may have,

Jade


From ro@||n@ump @end|ng |rom gm@||@com  Tue Jul 19 17:10:36 2022
From: ro@||n@ump @end|ng |rom gm@||@com (roslinazairimah zakaria)
Date: Tue, 19 Jul 2022 23:10:36 +0800
Subject: [R] Extract time and state of charge (Start and End) and Count
In-Reply-To: <CANTvJZL40HD9ZGpr1SJwVQzd4yd3p8jZxUH6SittGqXuBgDrhA@mail.gmail.com>
References: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
 <a4a57470-481e-ad36-a577-eb54497a50da@sapo.pt>
 <CANTvJZL40HD9ZGpr1SJwVQzd4yd3p8jZxUH6SittGqXuBgDrhA@mail.gmail.com>
Message-ID: <CANTvJZKR+Q-oBzp2NT+Hdu2Ze9haK-mgyt9h8EKPsKtFfzOYLA@mail.gmail.com>

Hi Rui,
I try to run your code, but all data became NA. Not sure why...
# these columns need to be fixed
cols <- c("BatteryChargeStartDate", "BatteryChargeStopDate")
dt[cols] <- lapply(dt[cols], \(x) sub("\n", " ", x))

# use package lubridate to coerce to a datetime class
library(lubridate)

dt <- lapply(dt, lubridate::dmy_hm)
dt
dt[cols] <- lapply(dt[cols], lubridate::dmy_hm)

h <- lubridate::hour(dt[["BatteryChargeStartDate"]])
aggregate(Starting_SoC_of_12 ~ h, dt, length)


$BCStartTime
   [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
NA NA NA NA NA NA NA NA NA
  [33] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
NA NA NA NA NA NA NA NA NA
  [65] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
NA NA NA NA NA NA NA NA NA
  [97] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
NA NA NA NA NA NA NA NA NA
 [129] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
NA NA NA NA NA NA NA NA NA
 [161] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
NA NA NA NA NA NA NA NA NA
 [193] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
NA NA NA NA NA NA NA NA NA
 [225] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
NA NA NA NA NA NA NA NA NA
 [257] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
NA NA NA NA NA NA NA NA NA
 [289] NA NA NA NA NA NA NA NA

 [929] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
NA NA NA NA NA NA NA NA NA
 [961] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
NA NA NA NA NA NA NA NA NA
 [993] NA NA NA NA NA NA NA NA
 [ reached 'max' / getOption("max.print") -- omitted 34418 entries ]

> dt[cols] <- lapply(dt[cols], lubridate::dmy_hm)
Warning messages:
1: All formats failed to parse. No formats found.
2: All formats failed to parse. No formats found.
>
> h <- lubridate::hour(dt[["BatteryChargeStartDate"]])
> aggregate(Starting_SoC_of_12 ~ h, dt, length)
Error in aggregate.data.frame(lhs, mf[-1L], FUN = FUN, ...) :
  no rows to aggregate

On Tue, Jul 19, 2022 at 4:53 PM roslinazairimah zakaria <
roslinaump at gmail.com> wrote:

> Hi Rui,
>
> Yes, I would like to count for each hour, how many in the state of charge
> start 0 and SOC 12, then SOC 1 and SOC 12 and so on.
>
> Thank you for your help.
>
> On Tue, Jul 19, 2022 at 1:11 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
>> Hello,
>>
>> I'm not sure I understand the problem. Do you want counts of how many
>> rows are there per hour?
>>
>>
>> # these columns need to be fixed
>> cols <- c("BatteryChargeStartDate", "BatteryChargeStopDate")
>> dt_2014[cols] <- lapply(dt_2014[cols], \(x) sub("\n", " ", x))
>> # use package lubridate to coerce to a datetime class
>> dt_2014[cols] <- lapply(dt_2014[cols], lubridate::dmy_hm)
>>
>> h <- lubridate::hour(dt_2014[["BatteryChargeStartDate"]])
>> aggregate(Starting_SoC_of_12 ~ h, dt_2014, length)
>>
>>
>>
>> It would be better if you post the expected output corresponding to the
>> posted data set.
>>
>> Hope this helps,
>>
>> Rui Barradas
>>
>> ?s 05:04 de 18/07/2022, roslinazairimah zakaria escreveu:
>> > Dear all,
>> >
>> > I have data of Battery Electric vehicle (BEV). I would like to extract
>> data
>> > from every hour starting from 0.00 to 0.59, 1:00-1:59 for SOC(state of
>> > charge) start to end.
>> >
>> > Some examples:
>> > I can extract data from SOC=0 and SOC=12
>> > dt_2014[which(dt_2014$Starting_SoC_of_12==0 &
>> > dt_2014$Ending_SoC_of_12==12),]
>> >
>> > I can extract data from SOC=1 and SOC=12
>> > dt_2014[which(dt_2014$Starting_SoC_of_12==1 &
>> > dt_2014$Ending_SoC_of_12==12),]
>> >
>> > and I would like to further categorise the data by hour and count how
>> many
>> > cars from 0 state charge to 12 state charge at in that particular hour.
>> >
>> > Thank you so much for any help given.
>> >
>> > Some data
>> >> dput(dt_2014[1:10,])
>> > structure(list(?..CarID = c("GC10", "GC10", "GC10", "GC10", "GC10",
>> > "GC10", "GC10", "GC10", "GC10", "GC10"), BatteryChargeStartDate =
>> > c("16/2/2014 16:05",
>> > "16/2/2014 18:20", "17/2/2014 8:10", "18/2/2014 7:41", "18/2/2014
>> 15:36",
>> > "18/2/2014 16:36", "18/2/2014 21:26", "19/2/2014 8:57", "19/2/2014
>> 21:08",
>> > "20/2/2014 18:11"), BCStartTime = c("16:05", "18:20", "8:10",
>> > "7:41", "15:36", "16:36", "21:26", "8:57", "21:08", "18:11"),
>> >      Year = c(2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L,
>> >      2014L, 2014L, 2014L), Month = c(2L, 2L, 2L, 2L, 2L, 2L, 2L,
>> >      2L, 2L, 2L), Day = c(16L, 16L, 17L, 18L, 18L, 18L, 18L, 19L,
>> >      19L, 20L), BatteryChargeStopDate = c("16/2/2014 17:05", "16/2/2014
>> > 19:00",
>> >      "17/2/2014 15:57", "18/2/2014 9:52", "18/2/2014 15:39", "18/2/2014
>> > 17:36",
>> >      "19/2/2014 1:55", "19/2/2014 14:25", "20/2/2014 5:17", "20/2/2014
>> 23:20"
>> >      ), BCStopTime = c("17:05", "19:00", "15:57", "9:52", "15:39",
>> >      "17:36", "1:55", "14:25", "5:17", "23:20"), Year2 = c(2014L,
>> >      2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L
>> >      ), Month2 = c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), Day2 =
>> c(16L,
>> >      16L, 17L, 18L, 18L, 18L, 19L, 19L, 20L, 20L), Starting_SoC_of_12 =
>> > c(1L,
>> >      2L, 4L, 5L, 4L, 2L, 8L, 8L, 4L, 8L), Ending_SoC_of_12 = c(11L,
>> >      11L, 12L, 8L, 4L, 10L, 12L, 12L, 12L, 12L)), row.names = c(NA,
>> > 10L), class = "data.frame")
>> >
>> >
>>
>
>
> --
> *Roslinazairimah Zakaria*
> *Tel: +609-5492370; Fax. No.+609-5492766*
>
> *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
> roslinaump at gmail.com <roslinaump at gmail.com>*
> Faculty of Industrial Sciences & Technology
> University Malaysia Pahang
> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>


-- 
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From ro@||n@ump @end|ng |rom gm@||@com  Tue Jul 19 17:18:04 2022
From: ro@||n@ump @end|ng |rom gm@||@com (roslinazairimah zakaria)
Date: Tue, 19 Jul 2022 23:18:04 +0800
Subject: [R] Extract time and state of charge (Start and End) and Count
In-Reply-To: <CANTvJZKhpMkN4Bhfy0d4p07HncWtt_C-PyLKQ+LS62E5yfbjNw@mail.gmail.com>
References: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
 <CA+8X3fVKvf220aOQp-mm1cKtkz+gmyL5Qd6O5WV7ABv3QrP3Rw@mail.gmail.com>
 <CANTvJZKhpMkN4Bhfy0d4p07HncWtt_C-PyLKQ+LS62E5yfbjNw@mail.gmail.com>
Message-ID: <CANTvJZ++YKuq+JsbiujeaRSfkPgkY33g_GuhhHHUUpST3CieLQ@mail.gmail.com>

Hi Jim,

I tried to run your code and got this error.

> # get the temporal order of observations
> obs_order <-
order(c(as.numeric(dt$BCStartTime),as.numeric(dt$BCStopTime)))
Warning messages:
1: In order(c(as.numeric(dt$BCStartTime), as.numeric(dt$BCStopTime))) :
  NAs introduced by coercion
2: In order(c(as.numeric(dt$BCStartTime), as.numeric(dt$BCStopTime))) :
  NAs introduced by coercion
>
numeric_time<-c(as.numeric(dt$BCStartTime),as.numeric(dt$BCStopTime))[obs_order]
Warning messages:
1: NAs introduced by coercion
2: NAs introduced by coercion
> nobs<-diff(range(numeric_time))/3600
> # find the linear approximation of charge state by hours
> hourly_SoC <- approx(numeric_time,
+
 c(dt$Starting_SoC_of_12,dt$Ending_SoC_of_12)[obs_order],n=nobs)
Error in approx(numeric_time, c(dt$Starting_SoC_of_12,
dt$Ending_SoC_of_12)[obs_order],  :
  need at least two non-NA values to interpolate

On Tue, Jul 19, 2022 at 3:22 PM roslinazairimah zakaria <
roslinaump at gmail.com> wrote:

> Jim,
>
> Thank you so much!
>
> On Tue, Jul 19, 2022 at 3:05 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
>> Hi Roslina,
>> The following gives you the state of charge for the vehicle in your
>> example data for each hour. This is approximate as your times are not
>> on even hours.
>>
>> # get the temporal order of observations
>>
>> obs_order<-order(c(as.numeric(dt_2014$bc_start),as.numeric(dt_2014$bc_stop)))
>>
>> numeric_time<-c(as.numeric(dt_2014$bc_start),as.numeric(dt_2014$bc_stop))[obs_order]
>> nobs<-diff(range(numeric_time))/3600
>> # find the linear approximation of charge state by hours
>> hourly_SoC<-approx(numeric_time,
>>  c(dt_2014$Starting_SoC_of_12,dt_2014$Ending_SoC_of_12)[obs_order],n=nobs)
>>
>> To get the POSIX times:
>>
>> hourly_POSIX<-seq(dt_2014$bc_start[1],dt_2014$bc_stop,length.out=nobs)
>>
>> That will fill part of your table. If you want the state of charge by
>> hour regardless of day, you'll have to create a new "hour" variable
>> from, hourly_POSIX, then:
>>
>> mean_charge_by_hour<-by(hourly_SoC$hour,hourly_SoC$y,mean) #untested
>> since I don't know whether you want this
>>
>> Jim
>>
>> On Mon, Jul 18, 2022 at 2:04 PM roslinazairimah zakaria
>> <roslinaump at gmail.com> wrote:
>> >
>> > Dear all,
>> >
>> > I have data of Battery Electric vehicle (BEV). I would like to extract
>> data
>> > from every hour starting from 0.00 to 0.59, 1:00-1:59 for SOC(state of
>> > charge) start to end.
>> >
>> > Some examples:
>> > I can extract data from SOC=0 and SOC=12
>> > dt_2014[which(dt_2014$Starting_SoC_of_12==0 &
>> > dt_2014$Ending_SoC_of_12==12),]
>> >
>> > I can extract data from SOC=1 and SOC=12
>> > dt_2014[which(dt_2014$Starting_SoC_of_12==1 &
>> > dt_2014$Ending_SoC_of_12==12),]
>> >
>> > and I would like to further categorise the data by hour and count how
>> many
>> > cars from 0 state charge to 12 state charge at in that particular hour.
>> >
>> > Thank you so much for any help given.
>> >
>> > Some data
>> > > dput(dt_2014[1:10,])
>> > structure(list(?..CarID = c("GC10", "GC10", "GC10", "GC10", "GC10",
>> > "GC10", "GC10", "GC10", "GC10", "GC10"), BatteryChargeStartDate =
>> > c("16/2/2014 16:05",
>> > "16/2/2014 18:20", "17/2/2014 8:10", "18/2/2014 7:41", "18/2/2014
>> 15:36",
>> > "18/2/2014 16:36", "18/2/2014 21:26", "19/2/2014 8:57", "19/2/2014
>> 21:08",
>> > "20/2/2014 18:11"), BCStartTime = c("16:05", "18:20", "8:10",
>> > "7:41", "15:36", "16:36", "21:26", "8:57", "21:08", "18:11"),
>> >     Year = c(2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L,
>> >     2014L, 2014L, 2014L), Month = c(2L, 2L, 2L, 2L, 2L, 2L, 2L,
>> >     2L, 2L, 2L), Day = c(16L, 16L, 17L, 18L, 18L, 18L, 18L, 19L,
>> >     19L, 20L), BatteryChargeStopDate = c("16/2/2014 17:05", "16/2/2014
>> > 19:00",
>> >     "17/2/2014 15:57", "18/2/2014 9:52", "18/2/2014 15:39", "18/2/2014
>> > 17:36",
>> >     "19/2/2014 1:55", "19/2/2014 14:25", "20/2/2014 5:17", "20/2/2014
>> 23:20"
>> >     ), BCStopTime = c("17:05", "19:00", "15:57", "9:52", "15:39",
>> >     "17:36", "1:55", "14:25", "5:17", "23:20"), Year2 = c(2014L,
>> >     2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L
>> >     ), Month2 = c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), Day2 = c(16L,
>> >     16L, 17L, 18L, 18L, 18L, 19L, 19L, 20L, 20L), Starting_SoC_of_12 =
>> > c(1L,
>> >     2L, 4L, 5L, 4L, 2L, 8L, 8L, 4L, 8L), Ending_SoC_of_12 = c(11L,
>> >     11L, 12L, 8L, 4L, 10L, 12L, 12L, 12L, 12L)), row.names = c(NA,
>> > 10L), class = "data.frame")
>> >
>> >
>> > --
>> > *Roslinazairimah Zakaria*
>> > *Tel: +609-5492370; Fax. No.+609-5492766*
>> >
>> > *Email: roslinaump at gmail.com <roslinaump at gmail.com>*
>> > University Malaysia Pahang
>> > Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>
>
> --
> *Roslinazairimah Zakaria*
> *Tel: +609-5492370; Fax. No.+609-5492766*
>
> *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
> roslinaump at gmail.com <roslinaump at gmail.com>*
> Faculty of Industrial Sciences & Technology
> University Malaysia Pahang
> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>


-- 
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Tue Jul 19 19:03:10 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Tue, 19 Jul 2022 18:03:10 +0100
Subject: [R] Extract time and state of charge (Start and End) and Count
In-Reply-To: <CANTvJZKR+Q-oBzp2NT+Hdu2Ze9haK-mgyt9h8EKPsKtFfzOYLA@mail.gmail.com>
References: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
 <a4a57470-481e-ad36-a577-eb54497a50da@sapo.pt>
 <CANTvJZL40HD9ZGpr1SJwVQzd4yd3p8jZxUH6SittGqXuBgDrhA@mail.gmail.com>
 <CANTvJZKR+Q-oBzp2NT+Hdu2Ze9haK-mgyt9h8EKPsKtFfzOYLA@mail.gmail.com>
Message-ID: <cec03bcc-c407-185a-e626-8e7e05e913df@sapo.pt>

Hello,

There was a bug in the way I copied&pasted your data to my R session, 
hence the NA's.

Here is a tidyverse way of doing what you want. Its output matches the 
expected output in your last post. The column names don't start at zero 
because there was no Starting_SoC_of_12 equal to 0.


library(tidyverse)

result <- dt_2014 %>%
   mutate(Hour = lubridate::hour(BatteryChargeStartDate)) %>%
   group_by(Hour, Starting_SoC_of_12, Ending_SoC_of_12) %>%
   mutate(Count = n()) %>%
   ungroup() %>%
   arrange(Hour, Starting_SoC_of_12, Ending_SoC_of_12) %>%
   pivot_wider(
     id_cols = Hour,
     names_from = c(Starting_SoC_of_12, Ending_SoC_of_12),
     names_sep = "-",
     values_from = Count,
     values_fill = 0L
   )

i <- str_order(names(result)[-1], numeric = TRUE)
result <- cbind(result[1], result[-1][i])
result
#  Hour 1-11 2-10 2-11 4-4 4-12 5-8 8-12
#1    7    0    0    0   0    0   1    0
#2    8    0    0    0   0    1   0    1
#3   15    0    0    0   1    0   0    0
#4   16    1    1    0   0    0   0    0
#5   18    0    0    1   0    0   0    1
#6   21    0    0    0   0    1   0    1


Hope this helps,

Rui Barradas


?s 16:10 de 19/07/2022, roslinazairimah zakaria escreveu:
> Hi Rui,
> I try to run your code, but all data became NA. Not sure why...
> # these columns need to be fixed
> cols <- c("BatteryChargeStartDate", "BatteryChargeStopDate")
> dt[cols] <- lapply(dt[cols], \(x) sub("\n", " ", x))
> 
> # use package lubridate to coerce to a datetime class
> library(lubridate)
> 
> dt <- lapply(dt, lubridate::dmy_hm)
> dt
> dt[cols] <- lapply(dt[cols], lubridate::dmy_hm)
> 
> h <- lubridate::hour(dt[["BatteryChargeStartDate"]])
> aggregate(Starting_SoC_of_12 ~ h, dt, length)
> 
> 
> $BCStartTime
>     [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
> NA NA NA NA NA NA NA NA NA
>    [33] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
> NA NA NA NA NA NA NA NA NA
>    [65] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
> NA NA NA NA NA NA NA NA NA
>    [97] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
> NA NA NA NA NA NA NA NA NA
>   [129] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
> NA NA NA NA NA NA NA NA NA
>   [161] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
> NA NA NA NA NA NA NA NA NA
>   [193] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
> NA NA NA NA NA NA NA NA NA
>   [225] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
> NA NA NA NA NA NA NA NA NA
>   [257] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
> NA NA NA NA NA NA NA NA NA
>   [289] NA NA NA NA NA NA NA NA
> 
>   [929] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
> NA NA NA NA NA NA NA NA NA
>   [961] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
> NA NA NA NA NA NA NA NA NA
>   [993] NA NA NA NA NA NA NA NA
>   [ reached 'max' / getOption("max.print") -- omitted 34418 entries ]
> 
>> dt[cols] <- lapply(dt[cols], lubridate::dmy_hm)
> Warning messages:
> 1: All formats failed to parse. No formats found.
> 2: All formats failed to parse. No formats found.
>>
>> h <- lubridate::hour(dt[["BatteryChargeStartDate"]])
>> aggregate(Starting_SoC_of_12 ~ h, dt, length)
> Error in aggregate.data.frame(lhs, mf[-1L], FUN = FUN, ...) :
>    no rows to aggregate
> 
> On Tue, Jul 19, 2022 at 4:53 PM roslinazairimah zakaria <
> roslinaump at gmail.com> wrote:
> 
>> Hi Rui,
>>
>> Yes, I would like to count for each hour, how many in the state of charge
>> start 0 and SOC 12, then SOC 1 and SOC 12 and so on.
>>
>> Thank you for your help.
>>
>> On Tue, Jul 19, 2022 at 1:11 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>>
>>> Hello,
>>>
>>> I'm not sure I understand the problem. Do you want counts of how many
>>> rows are there per hour?
>>>
>>>
>>> # these columns need to be fixed
>>> cols <- c("BatteryChargeStartDate", "BatteryChargeStopDate")
>>> dt_2014[cols] <- lapply(dt_2014[cols], \(x) sub("\n", " ", x))
>>> # use package lubridate to coerce to a datetime class
>>> dt_2014[cols] <- lapply(dt_2014[cols], lubridate::dmy_hm)
>>>
>>> h <- lubridate::hour(dt_2014[["BatteryChargeStartDate"]])
>>> aggregate(Starting_SoC_of_12 ~ h, dt_2014, length)
>>>
>>>
>>>
>>> It would be better if you post the expected output corresponding to the
>>> posted data set.
>>>
>>> Hope this helps,
>>>
>>> Rui Barradas
>>>
>>> ?s 05:04 de 18/07/2022, roslinazairimah zakaria escreveu:
>>>> Dear all,
>>>>
>>>> I have data of Battery Electric vehicle (BEV). I would like to extract
>>> data
>>>> from every hour starting from 0.00 to 0.59, 1:00-1:59 for SOC(state of
>>>> charge) start to end.
>>>>
>>>> Some examples:
>>>> I can extract data from SOC=0 and SOC=12
>>>> dt_2014[which(dt_2014$Starting_SoC_of_12==0 &
>>>> dt_2014$Ending_SoC_of_12==12),]
>>>>
>>>> I can extract data from SOC=1 and SOC=12
>>>> dt_2014[which(dt_2014$Starting_SoC_of_12==1 &
>>>> dt_2014$Ending_SoC_of_12==12),]
>>>>
>>>> and I would like to further categorise the data by hour and count how
>>> many
>>>> cars from 0 state charge to 12 state charge at in that particular hour.
>>>>
>>>> Thank you so much for any help given.
>>>>
>>>> Some data
>>>>> dput(dt_2014[1:10,])
>>>> structure(list(?..CarID = c("GC10", "GC10", "GC10", "GC10", "GC10",
>>>> "GC10", "GC10", "GC10", "GC10", "GC10"), BatteryChargeStartDate =
>>>> c("16/2/2014 16:05",
>>>> "16/2/2014 18:20", "17/2/2014 8:10", "18/2/2014 7:41", "18/2/2014
>>> 15:36",
>>>> "18/2/2014 16:36", "18/2/2014 21:26", "19/2/2014 8:57", "19/2/2014
>>> 21:08",
>>>> "20/2/2014 18:11"), BCStartTime = c("16:05", "18:20", "8:10",
>>>> "7:41", "15:36", "16:36", "21:26", "8:57", "21:08", "18:11"),
>>>>       Year = c(2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L,
>>>>       2014L, 2014L, 2014L), Month = c(2L, 2L, 2L, 2L, 2L, 2L, 2L,
>>>>       2L, 2L, 2L), Day = c(16L, 16L, 17L, 18L, 18L, 18L, 18L, 19L,
>>>>       19L, 20L), BatteryChargeStopDate = c("16/2/2014 17:05", "16/2/2014
>>>> 19:00",
>>>>       "17/2/2014 15:57", "18/2/2014 9:52", "18/2/2014 15:39", "18/2/2014
>>>> 17:36",
>>>>       "19/2/2014 1:55", "19/2/2014 14:25", "20/2/2014 5:17", "20/2/2014
>>> 23:20"
>>>>       ), BCStopTime = c("17:05", "19:00", "15:57", "9:52", "15:39",
>>>>       "17:36", "1:55", "14:25", "5:17", "23:20"), Year2 = c(2014L,
>>>>       2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L
>>>>       ), Month2 = c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), Day2 =
>>> c(16L,
>>>>       16L, 17L, 18L, 18L, 18L, 19L, 19L, 20L, 20L), Starting_SoC_of_12 =
>>>> c(1L,
>>>>       2L, 4L, 5L, 4L, 2L, 8L, 8L, 4L, 8L), Ending_SoC_of_12 = c(11L,
>>>>       11L, 12L, 8L, 4L, 10L, 12L, 12L, 12L, 12L)), row.names = c(NA,
>>>> 10L), class = "data.frame")
>>>>
>>>>
>>>
>>
>>
>> --
>> *Roslinazairimah Zakaria*
>> *Tel: +609-5492370; Fax. No.+609-5492766*
>>
>> *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
>> roslinaump at gmail.com <roslinaump at gmail.com>*
>> Faculty of Industrial Sciences & Technology
>> University Malaysia Pahang
>> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>>
> 
>


From r@njeetjh@||tkgp @end|ng |rom gm@||@com  Tue Jul 19 15:07:02 2022
From: r@njeetjh@||tkgp @end|ng |rom gm@||@com (Ranjeet Kumar Jha)
Date: Tue, 19 Jul 2022 18:37:02 +0530
Subject: [R] Need panel plots by each state, comprising three columns in R
Message-ID: <CAJfMH3rWJ76dh=z8OWzH1TuoDxyQFS9dDk+pqPYoREW_XXHtZw@mail.gmail.com>

Hello,

I need to draw a panel plot by each importing the attached csv data (sample
panel plot attached as well). Each state should contain three columns-  wt,
avg, and observed, may be represented as legend or x-axis, and y-axis
should show values.
I tried doing it in ggplot in R but couldn't succeed.
Please help plotting this panel plot.

Thanks,
Ranjeet

From tebert @end|ng |rom u||@edu  Tue Jul 19 21:07:56 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Tue, 19 Jul 2022 19:07:56 +0000
Subject: [R] 
 Need panel plots by each state, comprising three columns in R
In-Reply-To: <CAJfMH3rWJ76dh=z8OWzH1TuoDxyQFS9dDk+pqPYoREW_XXHtZw@mail.gmail.com>
References: <CAJfMH3rWJ76dh=z8OWzH1TuoDxyQFS9dDk+pqPYoREW_XXHtZw@mail.gmail.com>
Message-ID: <BN6PR2201MB15533A21C64BC02FE7193893CF8F9@BN6PR2201MB1553.namprd22.prod.outlook.com>

1) Please provide data that is enough similar to your real data that our answer is relevant to you.
2) Please provide any code that you have written that shows an attempt to solve the problem.
3) It sounds like you have a very specific output in mind. Possibly something that you saw and want to get something similar with your data. Please provide that.

Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ranjeet Kumar Jha
Sent: Tuesday, July 19, 2022 9:07 AM
To: R-help <r-help at r-project.org>; R-sig-geo Mailing List <r-sig-geo at r-project.org>
Subject: [R] Need panel plots by each state, comprising three columns in R

[External Email]

Hello,

I need to draw a panel plot by each importing the attached csv data (sample panel plot attached as well). Each state should contain three columns-  wt, avg, and observed, may be represented as legend or x-axis, and y-axis should show values.
I tried doing it in ggplot in R but couldn't succeed.
Please help plotting this panel plot.

Thanks,
Ranjeet
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=w8CWnBvjECi18ouLm7eoGpvGdE7QY5Z64EzBv2mmRj5mak8eDuHQHV6rfNOMqh4f&s=XwTpHoCasKF0UCY0Bv18mWRGjKEqq8TY_X72bey725w&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=w8CWnBvjECi18ouLm7eoGpvGdE7QY5Z64EzBv2mmRj5mak8eDuHQHV6rfNOMqh4f&s=TNtv-h4GV-fuOlbw4zbSr8qI90dDExP3uHxc-Heb6Pc&e=
and provide commented, minimal, self-contained, reproducible code.


From jrkr|de@u @end|ng |rom gm@||@com  Tue Jul 19 22:03:08 2022
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Tue, 19 Jul 2022 16:03:08 -0400
Subject: [R] 
 Need panel plots by each state, comprising three columns in R
In-Reply-To: <CAJfMH3rWJ76dh=z8OWzH1TuoDxyQFS9dDk+pqPYoREW_XXHtZw@mail.gmail.com>
References: <CAJfMH3rWJ76dh=z8OWzH1TuoDxyQFS9dDk+pqPYoREW_XXHtZw@mail.gmail.com>
Message-ID: <CAKZQJMD72HukveL1nzpO4taEyAA_SUQVdfEQj_8mpNnP3uNQVw@mail.gmail.com>

Neither data or plot made it through the spam filtres.  A handy way to
supply some sample data is the dput() function.  In the case of a
large dataset something like dput(head(mydata, 100)) should supply the
data we need. Just copy and paste the output in your e-mail.  For the
plot, .pdf seems most likely to work.

On Tue, 19 Jul 2022 at 14:00, Ranjeet Kumar Jha
<ranjeetjhaiitkgp at gmail.com> wrote:
>
> Hello,
>
> I need to draw a panel plot by each importing the attached csv data (sample
> panel plot attached as well). Each state should contain three columns-  wt,
> avg, and observed, may be represented as legend or x-axis, and y-axis
> should show values.
> I tried doing it in ggplot in R but couldn't succeed.
> Please help plotting this panel plot.
>
> Thanks,
> Ranjeet
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
John Kane
Kingston ON Canada


From drj|m|emon @end|ng |rom gm@||@com  Wed Jul 20 02:49:00 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 20 Jul 2022 10:49:00 +1000
Subject: [R] Extract time and state of charge (Start and End) and Count
In-Reply-To: <CANTvJZ++YKuq+JsbiujeaRSfkPgkY33g_GuhhHHUUpST3CieLQ@mail.gmail.com>
References: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
 <CA+8X3fVKvf220aOQp-mm1cKtkz+gmyL5Qd6O5WV7ABv3QrP3Rw@mail.gmail.com>
 <CANTvJZKhpMkN4Bhfy0d4p07HncWtt_C-PyLKQ+LS62E5yfbjNw@mail.gmail.com>
 <CANTvJZ++YKuq+JsbiujeaRSfkPgkY33g_GuhhHHUUpST3CieLQ@mail.gmail.com>
Message-ID: <CA+8X3fWsD0ECXabR-kQYs+=UvxR5ax7UJQzcwGzvm55AMFhyYA@mail.gmail.com>

Hi Roslina,
I think you have changed the code as "bc_start" in my code is
"BCStartTime" in yours. When I run the attached code, I get a data
frame "hourly_SoC" that looks right, and a matrix "result" (hour by
SoC) that checks against the data frame. I have tried to comment the
code so that you an see what I am doing.

Jim

On Wed, Jul 20, 2022 at 1:18 AM roslinazairimah zakaria
<roslinaump at gmail.com> wrote:
>
> Hi Jim,
>
> I tried to run your code and got this error.
>
> > # get the temporal order of observations
> > obs_order <- order(c(as.numeric(dt$BCStartTime),as.numeric(dt$BCStopTime)))
> Warning messages:
> 1: In order(c(as.numeric(dt$BCStartTime), as.numeric(dt$BCStopTime))) :
>   NAs introduced by coercion
> 2: In order(c(as.numeric(dt$BCStartTime), as.numeric(dt$BCStopTime))) :
>   NAs introduced by coercion
> > numeric_time<-c(as.numeric(dt$BCStartTime),as.numeric(dt$BCStopTime))[obs_order]
> Warning messages:
> 1: NAs introduced by coercion
> 2: NAs introduced by coercion
> > nobs<-diff(range(numeric_time))/3600
> > # find the linear approximation of charge state by hours
> > hourly_SoC <- approx(numeric_time,
> +                    c(dt$Starting_SoC_of_12,dt$Ending_SoC_of_12)[obs_order],n=nobs)
> Error in approx(numeric_time, c(dt$Starting_SoC_of_12, dt$Ending_SoC_of_12)[obs_order],  :
>   need at least two non-NA values to interpolate
>

From ro@||n@ump @end|ng |rom gm@||@com  Wed Jul 20 03:28:26 2022
From: ro@||n@ump @end|ng |rom gm@||@com (roslinazairimah zakaria)
Date: Wed, 20 Jul 2022 09:28:26 +0800
Subject: [R] Extract time and state of charge (Start and End) and Count
In-Reply-To: <CA+8X3fWsD0ECXabR-kQYs+=UvxR5ax7UJQzcwGzvm55AMFhyYA@mail.gmail.com>
References: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
 <CA+8X3fVKvf220aOQp-mm1cKtkz+gmyL5Qd6O5WV7ABv3QrP3Rw@mail.gmail.com>
 <CANTvJZKhpMkN4Bhfy0d4p07HncWtt_C-PyLKQ+LS62E5yfbjNw@mail.gmail.com>
 <CANTvJZ++YKuq+JsbiujeaRSfkPgkY33g_GuhhHHUUpST3CieLQ@mail.gmail.com>
 <CA+8X3fWsD0ECXabR-kQYs+=UvxR5ax7UJQzcwGzvm55AMFhyYA@mail.gmail.com>
Message-ID: <CANTvJZLQUTnjeWpy3Z03Tg45jW+67181FGxe=3_YhyWmHj4bCA@mail.gmail.com>

Hi Jim,
Thanks, I'll take a look.

On Wed, Jul 20, 2022, 08:49 Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Roslina,
> I think you have changed the code as "bc_start" in my code is
> "BCStartTime" in yours. When I run the attached code, I get a data
> frame "hourly_SoC" that looks right, and a matrix "result" (hour by
> SoC) that checks against the data frame. I have tried to comment the
> code so that you an see what I am doing.
>
> Jim
>
> On Wed, Jul 20, 2022 at 1:18 AM roslinazairimah zakaria
> <roslinaump at gmail.com> wrote:
> >
> > Hi Jim,
> >
> > I tried to run your code and got this error.
> >
> > > # get the temporal order of observations
> > > obs_order <-
> order(c(as.numeric(dt$BCStartTime),as.numeric(dt$BCStopTime)))
> > Warning messages:
> > 1: In order(c(as.numeric(dt$BCStartTime), as.numeric(dt$BCStopTime))) :
> >   NAs introduced by coercion
> > 2: In order(c(as.numeric(dt$BCStartTime), as.numeric(dt$BCStopTime))) :
> >   NAs introduced by coercion
> > >
> numeric_time<-c(as.numeric(dt$BCStartTime),as.numeric(dt$BCStopTime))[obs_order]
> > Warning messages:
> > 1: NAs introduced by coercion
> > 2: NAs introduced by coercion
> > > nobs<-diff(range(numeric_time))/3600
> > > # find the linear approximation of charge state by hours
> > > hourly_SoC <- approx(numeric_time,
> > +
> c(dt$Starting_SoC_of_12,dt$Ending_SoC_of_12)[obs_order],n=nobs)
> > Error in approx(numeric_time, c(dt$Starting_SoC_of_12,
> dt$Ending_SoC_of_12)[obs_order],  :
> >   need at least two non-NA values to interpolate
> >
>

	[[alternative HTML version deleted]]


From g@@@uu| @end|ng |rom gm@||@com  Thu Jul 21 02:20:10 2022
From: g@@@uu| @end|ng |rom gm@||@com (ani jaya)
Date: Thu, 21 Jul 2022 09:20:10 +0900
Subject: [R] two colors in one variable in legend
Message-ID: <CAHXS41yi8WSUg5n5vU9ppAMTBFa3rPyr5wYFSs31kStJmr5RRw@mail.gmail.com>

Dear R-Help,

I need to plot two colors in legend in one variable. It is basically
same as this:

https://stackoverflow.com/questions/31004236/r-legend-boxes-with-more-than-one-colour

but I am not confident enough to change the legend script. What I want
to achieve is the color in legend is side by side or half by half, for
example, for oni blue and red and pink and lightblue for dmi.
Here is my script and data.

>dput(head(oni,30))
c(V11 = 0.55, V12 = 0.33, V13 = 0.09, V14 = 0.2, V15 = 0.13,
V16 = 0.37, V17 = 0.02, V18 = -0.23, V19 = -0.12, V110 = -0.11,
V111 = 0.1, V112 = 0.36, V21 = -0.45, V22 = -0.45, V23 = -0.02,
V24 = -0.17, V25 = -0.11, V26 = -0.15, V27 = -0.43, V28 = -0.18,
V29 = -0.07, V210 = -0.03, V211 = -0.27, V212 = 0.07, V31 = -0.04,
V32 = -0.13, V33 = -0.02, V34 = 0.24, V35 = 0.65, V36 = 0.92)


>dput(head(dmi,30))
c(V11 = -0.093, V12 = -0.12, V13 = -0.317, V14 = -0.02, V15 = 0.014,
V16 = -0.268, V17 = -0.564, V18 = -0.71, V19 = -0.639, V110 = -0.498,
V111 = -0.294, V112 = -0.413, V21 = -0.136, V22 = 0.056, V23 = 0.127,
V24 = 0.168, V25 = 0.062, V26 = -0.126, V27 = -0.463, V28 = -0.516,
V29 = -0.651, V210 = -0.447, V211 = -0.204, V212 = 0.05, V31 = 0.209,
V32 = 0.246, V33 = 0.154, V34 = 0.194, V35 = 0.302, V36 = 0.369

#scriptplot(oni[1:30],ylim=c(-3,3),type="l", pch=3, col="white",
     ylab=expression(bold("ONI3.4 & DMI (\u00B0C)")),
     xlab=expression(bold("Year")),lwd=2)
abline(h=0,lwd=2,col="black")
polygon(c(1,1:30,30),c(0,ifelse(oni[1:30]>0,oni,0),0),col="red")
polygon(c(1,1:30,30),c(0,ifelse(oni[1:30]<0,oni,0),0),col="blue")
segments(c(1,1:30,30),0,c(1,1:30,30),c(0,ifelse(dmi[1:30]>0,dmi,0),0),col="pink",
lwd=2)
segments(c(1,1:30,30),0,c(1,1:30,30),c(0,ifelse(dmi[1:30]<0,dmi,0),0),col="lightskyblue",lwd=2)
legend("bottomleft",legend=c("ONI3.4","DMI"), fill=c("red","lightpink"),
       density=c(NA,NA))

Any help is very welcome.
Thank you.

best,
Ani


From drj|m|emon @end|ng |rom gm@||@com  Thu Jul 21 04:12:07 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Thu, 21 Jul 2022 12:12:07 +1000
Subject: [R] two colors in one variable in legend
In-Reply-To: <CAHXS41yi8WSUg5n5vU9ppAMTBFa3rPyr5wYFSs31kStJmr5RRw@mail.gmail.com>
References: <CAHXS41yi8WSUg5n5vU9ppAMTBFa3rPyr5wYFSs31kStJmr5RRw@mail.gmail.com>
Message-ID: <CA+8X3fXVyMU3grCe=Cvez+s2d9wgPo=cQO4atv6aTRG=xpG1pw@mail.gmail.com>

Hi Ani,
Have a look at legendg in the plotrix package.

Jim

On Thu, Jul 21, 2022 at 10:21 AM ani jaya <gaaauul at gmail.com> wrote:
>
> Dear R-Help,
>
> I need to plot two colors in legend in one variable. It is basically
> same as this:
>
> https://stackoverflow.com/questions/31004236/r-legend-boxes-with-more-than-one-colour
>
> but I am not confident enough to change the legend script. What I want
> to achieve is the color in legend is side by side or half by half, for
> example, for oni blue and red and pink and lightblue for dmi.
> Here is my script and data.
>
> >dput(head(oni,30))
> c(V11 = 0.55, V12 = 0.33, V13 = 0.09, V14 = 0.2, V15 = 0.13,
> V16 = 0.37, V17 = 0.02, V18 = -0.23, V19 = -0.12, V110 = -0.11,
> V111 = 0.1, V112 = 0.36, V21 = -0.45, V22 = -0.45, V23 = -0.02,
> V24 = -0.17, V25 = -0.11, V26 = -0.15, V27 = -0.43, V28 = -0.18,
> V29 = -0.07, V210 = -0.03, V211 = -0.27, V212 = 0.07, V31 = -0.04,
> V32 = -0.13, V33 = -0.02, V34 = 0.24, V35 = 0.65, V36 = 0.92)
>
>
> >dput(head(dmi,30))
> c(V11 = -0.093, V12 = -0.12, V13 = -0.317, V14 = -0.02, V15 = 0.014,
> V16 = -0.268, V17 = -0.564, V18 = -0.71, V19 = -0.639, V110 = -0.498,
> V111 = -0.294, V112 = -0.413, V21 = -0.136, V22 = 0.056, V23 = 0.127,
> V24 = 0.168, V25 = 0.062, V26 = -0.126, V27 = -0.463, V28 = -0.516,
> V29 = -0.651, V210 = -0.447, V211 = -0.204, V212 = 0.05, V31 = 0.209,
> V32 = 0.246, V33 = 0.154, V34 = 0.194, V35 = 0.302, V36 = 0.369
>
> #scriptplot(oni[1:30],ylim=c(-3,3),type="l", pch=3, col="white",
>      ylab=expression(bold("ONI3.4 & DMI (\u00B0C)")),
>      xlab=expression(bold("Year")),lwd=2)
> abline(h=0,lwd=2,col="black")
> polygon(c(1,1:30,30),c(0,ifelse(oni[1:30]>0,oni,0),0),col="red")
> polygon(c(1,1:30,30),c(0,ifelse(oni[1:30]<0,oni,0),0),col="blue")
> segments(c(1,1:30,30),0,c(1,1:30,30),c(0,ifelse(dmi[1:30]>0,dmi,0),0),col="pink",
> lwd=2)
> segments(c(1,1:30,30),0,c(1,1:30,30),c(0,ifelse(dmi[1:30]<0,dmi,0),0),col="lightskyblue",lwd=2)
> legend("bottomleft",legend=c("ONI3.4","DMI"), fill=c("red","lightpink"),
>        density=c(NA,NA))
>
> Any help is very welcome.
> Thank you.
>
> best,
> Ani
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From g@@@uu| @end|ng |rom gm@||@com  Thu Jul 21 04:20:45 2022
From: g@@@uu| @end|ng |rom gm@||@com (ani jaya)
Date: Thu, 21 Jul 2022 11:20:45 +0900
Subject: [R] two colors in one variable in legend
In-Reply-To: <CA+8X3fXVyMU3grCe=Cvez+s2d9wgPo=cQO4atv6aTRG=xpG1pw@mail.gmail.com>
References: <CAHXS41yi8WSUg5n5vU9ppAMTBFa3rPyr5wYFSs31kStJmr5RRw@mail.gmail.com>
 <CA+8X3fXVyMU3grCe=Cvez+s2d9wgPo=cQO4atv6aTRG=xpG1pw@mail.gmail.com>
Message-ID: <CAHXS41xdFvsiFVFnbV_oLRCXwCfCBoNeHgWA-hOzZV3KTzEQgg@mail.gmail.com>

Thank you Jim,
That is what I am looking for.

Best,
Ani

On Thu, Jul 21, 2022 at 11:12 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Ani,
> Have a look at legendg in the plotrix package.
>
> Jim
>
> On Thu, Jul 21, 2022 at 10:21 AM ani jaya <gaaauul at gmail.com> wrote:
> >
> > Dear R-Help,
> >
> > I need to plot two colors in legend in one variable. It is basically
> > same as this:
> >
> > https://stackoverflow.com/questions/31004236/r-legend-boxes-with-more-than-one-colour
> >
> > but I am not confident enough to change the legend script. What I want
> > to achieve is the color in legend is side by side or half by half, for
> > example, for oni blue and red and pink and lightblue for dmi.
> > Here is my script and data.
> >
> > >dput(head(oni,30))
> > c(V11 = 0.55, V12 = 0.33, V13 = 0.09, V14 = 0.2, V15 = 0.13,
> > V16 = 0.37, V17 = 0.02, V18 = -0.23, V19 = -0.12, V110 = -0.11,
> > V111 = 0.1, V112 = 0.36, V21 = -0.45, V22 = -0.45, V23 = -0.02,
> > V24 = -0.17, V25 = -0.11, V26 = -0.15, V27 = -0.43, V28 = -0.18,
> > V29 = -0.07, V210 = -0.03, V211 = -0.27, V212 = 0.07, V31 = -0.04,
> > V32 = -0.13, V33 = -0.02, V34 = 0.24, V35 = 0.65, V36 = 0.92)
> >
> >
> > >dput(head(dmi,30))
> > c(V11 = -0.093, V12 = -0.12, V13 = -0.317, V14 = -0.02, V15 = 0.014,
> > V16 = -0.268, V17 = -0.564, V18 = -0.71, V19 = -0.639, V110 = -0.498,
> > V111 = -0.294, V112 = -0.413, V21 = -0.136, V22 = 0.056, V23 = 0.127,
> > V24 = 0.168, V25 = 0.062, V26 = -0.126, V27 = -0.463, V28 = -0.516,
> > V29 = -0.651, V210 = -0.447, V211 = -0.204, V212 = 0.05, V31 = 0.209,
> > V32 = 0.246, V33 = 0.154, V34 = 0.194, V35 = 0.302, V36 = 0.369
> >
> > #scriptplot(oni[1:30],ylim=c(-3,3),type="l", pch=3, col="white",
> >      ylab=expression(bold("ONI3.4 & DMI (\u00B0C)")),
> >      xlab=expression(bold("Year")),lwd=2)
> > abline(h=0,lwd=2,col="black")
> > polygon(c(1,1:30,30),c(0,ifelse(oni[1:30]>0,oni,0),0),col="red")
> > polygon(c(1,1:30,30),c(0,ifelse(oni[1:30]<0,oni,0),0),col="blue")
> > segments(c(1,1:30,30),0,c(1,1:30,30),c(0,ifelse(dmi[1:30]>0,dmi,0),0),col="pink",
> > lwd=2)
> > segments(c(1,1:30,30),0,c(1,1:30,30),c(0,ifelse(dmi[1:30]<0,dmi,0),0),col="lightskyblue",lwd=2)
> > legend("bottomleft",legend=c("ONI3.4","DMI"), fill=c("red","lightpink"),
> >        density=c(NA,NA))
> >
> > Any help is very welcome.
> > Thank you.
> >
> > best,
> > Ani
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From ro@||n@ump @end|ng |rom gm@||@com  Thu Jul 21 07:17:32 2022
From: ro@||n@ump @end|ng |rom gm@||@com (roslinazairimah zakaria)
Date: Thu, 21 Jul 2022 13:17:32 +0800
Subject: [R] Extract time and state of charge (Start and End) and Count
In-Reply-To: <CA+8X3fWsD0ECXabR-kQYs+=UvxR5ax7UJQzcwGzvm55AMFhyYA@mail.gmail.com>
References: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
 <CA+8X3fVKvf220aOQp-mm1cKtkz+gmyL5Qd6O5WV7ABv3QrP3Rw@mail.gmail.com>
 <CANTvJZKhpMkN4Bhfy0d4p07HncWtt_C-PyLKQ+LS62E5yfbjNw@mail.gmail.com>
 <CANTvJZ++YKuq+JsbiujeaRSfkPgkY33g_GuhhHHUUpST3CieLQ@mail.gmail.com>
 <CA+8X3fWsD0ECXabR-kQYs+=UvxR5ax7UJQzcwGzvm55AMFhyYA@mail.gmail.com>
Message-ID: <CANTvJZKcrNkKLgZa6b1GhQXrC_dVi0Ha=e_UFJgq-x19qt1Ceg@mail.gmail.com>

Hi Jim,
I have tried your code and am able to understand most of the code and apply
it to the whole data set.
But I am not sure about this part:

colnames(result) <- paste(0:11,1:12,sep="-")

What is at 0-1, 1, 0? Number of cars at hour 1 at state of charge between 0
and 1?

For 11-12, 1, 268? Number of cars at hour 1 at state of charge between 11
and 12?

> print(result)

   0-1 1-2 2-3 3-4 4-5 5-6 6-7 7-8 8-9 9-10 10-11 11-12

1    0   0   1   0   7   4   2   6  20   19    38   268

2    1   0   5   4   2   9   4  10  17   24    33   255

3    0   0   2   1   8   7   7  16  28   31    49   215

4    0   2   6   3   4   5  15  24  53   38    43   171

5    2   2  13   6  21  19  28  35  43   44    42   109

6    0   4  16  16  29  43  36  38  31   55    48    47

7    0   6  15  23  33  35  56  55  39   47    26    29

8    3   7  15  19  24  46  58  39  37   32    48    36

9    1   6  13  14  28  29  31  52  44   39    46    61

10   3   4   8  12  27  22  33  38  54   66    51    47

11   4   6   7  12  16  33  37  28  53   52    47    69

12   2   6  10  16  17  34  32  51  34   51    56    56

13   0   2  12  12  21  33  30  34  48   48    56    69

14   1   3  17  10  17  28  37  47  46   54    54    50

15   2   5  14  26  21  30  47  53  53   35    44    35

16   3   7  14  24  29  40  39  42  40   50    44    33

17   0   8  28  24  39  36  41  42  38   41    34    34

18   4  11  18  26  25  30  53  40  49   39    36    34

19   4   6  16  19  22  34  47  43  37   48    39    49

20   5   4  10  17  18  30  25  41  42   57    44    72

21   1   4   7  14  19  23  33  22  34   37    53   118

22   0   5   7  19  16  24  24  24  26   36    37   146

23   2   7   6  11  16  15  19  21  16   29    39   183

24   0   0   0   0   0   0   0   0   0    0     0     0



On Wed, Jul 20, 2022 at 8:49 AM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Roslina,
> I think you have changed the code as "bc_start" in my code is
> "BCStartTime" in yours. When I run the attached code, I get a data
> frame "hourly_SoC" that looks right, and a matrix "result" (hour by
> SoC) that checks against the data frame. I have tried to comment the
> code so that you an see what I am doing.
>
> Jim
>
> On Wed, Jul 20, 2022 at 1:18 AM roslinazairimah zakaria
> <roslinaump at gmail.com> wrote:
> >
> > Hi Jim,
> >
> > I tried to run your code and got this error.
> >
> > > # get the temporal order of observations
> > > obs_order <-
> order(c(as.numeric(dt$BCStartTime),as.numeric(dt$BCStopTime)))
> > Warning messages:
> > 1: In order(c(as.numeric(dt$BCStartTime), as.numeric(dt$BCStopTime))) :
> >   NAs introduced by coercion
> > 2: In order(c(as.numeric(dt$BCStartTime), as.numeric(dt$BCStopTime))) :
> >   NAs introduced by coercion
> > >
> numeric_time<-c(as.numeric(dt$BCStartTime),as.numeric(dt$BCStopTime))[obs_order]
> > Warning messages:
> > 1: NAs introduced by coercion
> > 2: NAs introduced by coercion
> > > nobs<-diff(range(numeric_time))/3600
> > > # find the linear approximation of charge state by hours
> > > hourly_SoC <- approx(numeric_time,
> > +
> c(dt$Starting_SoC_of_12,dt$Ending_SoC_of_12)[obs_order],n=nobs)
> > Error in approx(numeric_time, c(dt$Starting_SoC_of_12,
> dt$Ending_SoC_of_12)[obs_order],  :
> >   need at least two non-NA values to interpolate
> >
>


-- 
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Thu Jul 21 14:09:43 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Thu, 21 Jul 2022 22:09:43 +1000
Subject: [R] Extract time and state of charge (Start and End) and Count
In-Reply-To: <CANTvJZKcrNkKLgZa6b1GhQXrC_dVi0Ha=e_UFJgq-x19qt1Ceg@mail.gmail.com>
References: <CANTvJZKmoRdCq18BuzrXD7n4Y1MadfSehJb6rJPMyO5TjjtTpg@mail.gmail.com>
 <CA+8X3fVKvf220aOQp-mm1cKtkz+gmyL5Qd6O5WV7ABv3QrP3Rw@mail.gmail.com>
 <CANTvJZKhpMkN4Bhfy0d4p07HncWtt_C-PyLKQ+LS62E5yfbjNw@mail.gmail.com>
 <CANTvJZ++YKuq+JsbiujeaRSfkPgkY33g_GuhhHHUUpST3CieLQ@mail.gmail.com>
 <CA+8X3fWsD0ECXabR-kQYs+=UvxR5ax7UJQzcwGzvm55AMFhyYA@mail.gmail.com>
 <CANTvJZKcrNkKLgZa6b1GhQXrC_dVi0Ha=e_UFJgq-x19qt1Ceg@mail.gmail.com>
Message-ID: <CA+8X3fU36pdJRPgCK2eJc3aTd0VdFov3skKVwTv=sHWy=7243A@mail.gmail.com>

Hi Roslina,
Yes, the entries in each row refer to the hour of the day and the
state of charge in the columns.

What is at 0-1, 1, 0? Number of cars at hour 1 at state of charge
between 0 and 1? Yes.

For 11-12, 1, 268? Number of cars at hour 1 at state of charge between
11 and 12? Yes.

As I don't have your data set I can't check whether these numbers are
correct, but the pattern looks plausible, with full charges at
nighttime and lower charges during the day when most cars would be in
use.

Jim

On Thu, Jul 21, 2022 at 3:17 PM roslinazairimah zakaria
<roslinaump at gmail.com> wrote:
>
> Hi Jim,
> I have tried your code and am able to understand most of the code and apply it to the whole data set.
> But I am not sure about this part:
>
> colnames(result) <- paste(0:11,1:12,sep="-")
>
> What is at 0-1, 1, 0? Number of cars at hour 1 at state of charge between 0 and 1?
>
> For 11-12, 1, 268? Number of cars at hour 1 at state of charge between 11 and 12?
>
> > print(result)
>
>    0-1 1-2 2-3 3-4 4-5 5-6 6-7 7-8 8-9 9-10 10-11 11-12
>
> 1    0   0   1   0   7   4   2   6  20   19    38   268
>
> 2    1   0   5   4   2   9   4  10  17   24    33   255
>
> 3    0   0   2   1   8   7   7  16  28   31    49   215
>
> 4    0   2   6   3   4   5  15  24  53   38    43   171
>
> 5    2   2  13   6  21  19  28  35  43   44    42   109
>
> 6    0   4  16  16  29  43  36  38  31   55    48    47
>
> 7    0   6  15  23  33  35  56  55  39   47    26    29
>
> 8    3   7  15  19  24  46  58  39  37   32    48    36
>
> 9    1   6  13  14  28  29  31  52  44   39    46    61
>
> 10   3   4   8  12  27  22  33  38  54   66    51    47
>
> 11   4   6   7  12  16  33  37  28  53   52    47    69
>
> 12   2   6  10  16  17  34  32  51  34   51    56    56
>
> 13   0   2  12  12  21  33  30  34  48   48    56    69
>
> 14   1   3  17  10  17  28  37  47  46   54    54    50
>
> 15   2   5  14  26  21  30  47  53  53   35    44    35
>
> 16   3   7  14  24  29  40  39  42  40   50    44    33
>
> 17   0   8  28  24  39  36  41  42  38   41    34    34
>
> 18   4  11  18  26  25  30  53  40  49   39    36    34
>
> 19   4   6  16  19  22  34  47  43  37   48    39    49
>
> 20   5   4  10  17  18  30  25  41  42   57    44    72
>
> 21   1   4   7  14  19  23  33  22  34   37    53   118
>
> 22   0   5   7  19  16  24  24  24  26   36    37   146
>
> 23   2   7   6  11  16  15  19  21  16   29    39   183
>
> 24   0   0   0   0   0   0   0   0   0    0     0     0
>
>
>
> On Wed, Jul 20, 2022 at 8:49 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>>
>> Hi Roslina,
>> I think you have changed the code as "bc_start" in my code is
>> "BCStartTime" in yours. When I run the attached code, I get a data
>> frame "hourly_SoC" that looks right, and a matrix "result" (hour by
>> SoC) that checks against the data frame. I have tried to comment the
>> code so that you an see what I am doing.
>>
>> Jim
>>
>> On Wed, Jul 20, 2022 at 1:18 AM roslinazairimah zakaria
>> <roslinaump at gmail.com> wrote:
>> >
>> > Hi Jim,
>> >
>> > I tried to run your code and got this error.
>> >
>> > > # get the temporal order of observations
>> > > obs_order <- order(c(as.numeric(dt$BCStartTime),as.numeric(dt$BCStopTime)))
>> > Warning messages:
>> > 1: In order(c(as.numeric(dt$BCStartTime), as.numeric(dt$BCStopTime))) :
>> >   NAs introduced by coercion
>> > 2: In order(c(as.numeric(dt$BCStartTime), as.numeric(dt$BCStopTime))) :
>> >   NAs introduced by coercion
>> > > numeric_time<-c(as.numeric(dt$BCStartTime),as.numeric(dt$BCStopTime))[obs_order]
>> > Warning messages:
>> > 1: NAs introduced by coercion
>> > 2: NAs introduced by coercion
>> > > nobs<-diff(range(numeric_time))/3600
>> > > # find the linear approximation of charge state by hours
>> > > hourly_SoC <- approx(numeric_time,
>> > +                    c(dt$Starting_SoC_of_12,dt$Ending_SoC_of_12)[obs_order],n=nobs)
>> > Error in approx(numeric_time, c(dt$Starting_SoC_of_12, dt$Ending_SoC_of_12)[obs_order],  :
>> >   need at least two non-NA values to interpolate
>> >
>
>
>
> --
> Roslinazairimah Zakaria
> Tel: +609-5492370; Fax. No.+609-5492766
> Email: roslinazairimah at ump.edu.my; roslinaump at gmail.com
> Faculty of Industrial Sciences & Technology
> University Malaysia Pahang
> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia


From j@de@shod@@ m@iii@g oii googiem@ii@com  Thu Jul 21 16:19:31 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Thu, 21 Jul 2022 15:19:31 +0100
Subject: [R] mgcv: relative risk from GAM with distributed lag
In-Reply-To: <CANg3_k-18ND8DrwfYJmGAN5hq18QweZFPp4GhCuHwB=XFGAFiA@mail.gmail.com>
References: <CANg3_k-18ND8DrwfYJmGAN5hq18QweZFPp4GhCuHwB=XFGAFiA@mail.gmail.com>
Message-ID: <CANg3_k9=GhwRjPyuLQBYmxD8Q-ksPrM52tS7DW0Cv3cpKn00Zw@mail.gmail.com>

Hello everyone (incl. Simon Wood?),

I'm not sure that my original question (see below, including
reproducible example) was as clear as it could have been. To clarify,
what I would to like to get is:

1) a perspective plot of temperature x lag x relative risk.  I know
how to use plot.gam and vis.gam but don't know how to get plots on the
relative risk scale as opposed to  "response" or "link".

2) a plot of relative risk (accumulated across all lags) vs
temperature, given a reference temperature. An example of such a plot
can be found in figure 2 (bottom) of this paper by Gasparrini et al:
https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3940

I've seen Simon Wood's response to a related issue here:
https://stat.ethz.ch/pipermail/r-help/2012-May/314387.html
However, I'm not sure how to apply this to time series data with
distributed lag, to get the above mentioned figures.

Would be really grateful for suggestions!

Jade

On Tue, 19 Jul 2022 at 16:07, jade.shodan at googlemail.com
<jade.shodan at googlemail.com> wrote:
>
> Dear list members,
>
> Does anyone know how to obtain a relative risk/ risk ratio from a GAM
> with a distributed lag model implemented in mgcv? I have a GAM
> predicting daily deaths from time series data consisting of daily
> temperature, humidity and rainfall. The GAM includes a distributed lag
> model because deaths may occur over several days following a high heat
> day.
>
> What I'd like to do is compute (and plot) the relative risk
> (accumulated across all lags) for a given temperature vs the
> temperature at which the risk is lowest, with corresponding confidence
> intervals. I am aware of the predict.gam function but am not sure if
> and how it should be used in this case. (Additionally, I'd also like
> to plot the relative risk for different lags separately).
>
> I apologise if this seems trivial to some. (Actually, I hope it is,
> because that might mean I get a solution!) I've been looking for
> examples on how to do this, but found nothing so far. Suggestions
> would be very much appreciated!
>
> Below is a reproducible example with the GAM:
>
> library(mgcv)
> set.seed(3) # make reproducible example
> simdat <- gamSim(1,400) # simulate data
> g <- exp(simdat$f/5)
> simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
> simdat$time <- 1:400  # create time series
> names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
> "f0", "f1", "f2", "f3", "time")
>
> # lag function based on Simon Wood (book 2017, p.349 and gamair
> package documentation p.54
> # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
> lagard <- function(x,n.lag=7) {
> n <- length(x); X <- matrix(NA,n,n.lag)
> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> X
> }
>
> # set up lag, temp, rain and humidity as 7-column matrices
> # to create lagged variables - based on Simon Wood's example
> dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
> deaths=simdat$deaths, time = simdat$time)
> dat$temp <- lagard(simdat$temp)
> dat$rain <- lagard(simdat$rain)
> dat$humidity <- lagard(simdat$humidity)
>
> mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
> te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
> family = nb, method = 'REML', select = TRUE)
>
> summary(mod)
> plot(mod, scheme = 1)
> plot(mod, scheme = 2)
>
> Thanks for any suggestions you may have,
>
> Jade


From @|mon@wood @end|ng |rom b@th@edu  Fri Jul 22 10:54:00 2022
From: @|mon@wood @end|ng |rom b@th@edu (Simon Wood)
Date: Fri, 22 Jul 2022 09:54:00 +0100
Subject: [R] mgcv: relative risk from GAM with distributed lag
In-Reply-To: <CANg3_k9=GhwRjPyuLQBYmxD8Q-ksPrM52tS7DW0Cv3cpKn00Zw@mail.gmail.com>
References: <CANg3_k-18ND8DrwfYJmGAN5hq18QweZFPp4GhCuHwB=XFGAFiA@mail.gmail.com>
 <CANg3_k9=GhwRjPyuLQBYmxD8Q-ksPrM52tS7DW0Cv3cpKn00Zw@mail.gmail.com>
Message-ID: <e0f4395b-e1cf-abfa-e4d4-ee3dd519887e@bath.edu>


On 21/07/2022 15:19, jade.shodan--- via R-help wrote:
> Hello everyone (incl. Simon Wood?),
>
> I'm not sure that my original question (see below, including
> reproducible example) was as clear as it could have been. To clarify,
> what I would to like to get is:
>
> 1) a perspective plot of temperature x lag x relative risk.  I know
> how to use plot.gam and vis.gam but don't know how to get plots on the
> relative risk scale as opposed to  "response" or "link".

- You are on the log scale so I think that all you need to do is to use 
'predict.gam', with 'type = "terms"' to? get the predictions for the 
te(temp, lag) term over the required grid of lags and temperatures. 
Suppose the dataframe of prediction data is 'pd'. Now produce pd0, which 
is identical to pd, except that the temperatures are all set to the 
reference temperature. Use predict.gam to predict te(temp,lag) from pd0. 
Now the exponential of the difference between the first and second 
predictions is the required RR, which you can plot using 'persp', 
'contour', 'image' or whatever. If you need credible intervals see pages 
341-343 of my 'GAMs: An intro with R' book (2nd ed).

> 2) a plot of relative risk (accumulated across all lags) vs
> temperature, given a reference temperature. An example of such a plot
> can be found in figure 2 (bottom) of this paper by Gasparrini et al:
> https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3940

- I guess this only makes sense if you have the same temperature at all 
lags. So this time produce a data.frame with each desired temperature 
repeated for each lag: 'pd1'. Again use predict.gam(...,type="terms"). 
Then sum the predictions over lags for each temperature, subtract the 
minimum, and take the exponential. Same as above for CIs.

best,

Simon

> I've seen Simon Wood's response to a related issue here:
> https://stat.ethz.ch/pipermail/r-help/2012-May/314387.html
> However, I'm not sure how to apply this to time series data with
> distributed lag, to get the above mentioned figures.
>
> Would be really grateful for suggestions!
>
> Jade
>
> On Tue, 19 Jul 2022 at 16:07, jade.shodan at googlemail.com
> <jade.shodan at googlemail.com> wrote:
>> Dear list members,
>>
>> Does anyone know how to obtain a relative risk/ risk ratio from a GAM
>> with a distributed lag model implemented in mgcv? I have a GAM
>> predicting daily deaths from time series data consisting of daily
>> temperature, humidity and rainfall. The GAM includes a distributed lag
>> model because deaths may occur over several days following a high heat
>> day.
>>
>> What I'd like to do is compute (and plot) the relative risk
>> (accumulated across all lags) for a given temperature vs the
>> temperature at which the risk is lowest, with corresponding confidence
>> intervals. I am aware of the predict.gam function but am not sure if
>> and how it should be used in this case. (Additionally, I'd also like
>> to plot the relative risk for different lags separately).
>>
>> I apologise if this seems trivial to some. (Actually, I hope it is,
>> because that might mean I get a solution!) I've been looking for
>> examples on how to do this, but found nothing so far. Suggestions
>> would be very much appreciated!
>>
>> Below is a reproducible example with the GAM:
>>
>> library(mgcv)
>> set.seed(3) # make reproducible example
>> simdat <- gamSim(1,400) # simulate data
>> g <- exp(simdat$f/5)
>> simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
>> simdat$time <- 1:400  # create time series
>> names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
>> "f0", "f1", "f2", "f3", "time")
>>
>> # lag function based on Simon Wood (book 2017, p.349 and gamair
>> package documentation p.54
>> # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
>> lagard <- function(x,n.lag=7) {
>> n <- length(x); X <- matrix(NA,n,n.lag)
>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
>> X
>> }
>>
>> # set up lag, temp, rain and humidity as 7-column matrices
>> # to create lagged variables - based on Simon Wood's example
>> dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
>> deaths=simdat$deaths, time = simdat$time)
>> dat$temp <- lagard(simdat$temp)
>> dat$rain <- lagard(simdat$rain)
>> dat$humidity <- lagard(simdat$humidity)
>>
>> mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
>> te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
>> family = nb, method = 'REML', select = TRUE)
>>
>> summary(mod)
>> plot(mod, scheme = 1)
>> plot(mod, scheme = 2)
>>
>> Thanks for any suggestions you may have,
>>
>> Jade
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Simon Wood, School of Mathematics, University of Edinburgh,
https://www.maths.ed.ac.uk/~swood34/


From j@de@shod@@ m@iii@g oii googiem@ii@com  Fri Jul 22 13:47:05 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Fri, 22 Jul 2022 12:47:05 +0100
Subject: [R] mgcv: relative risk from GAM with distributed lag
In-Reply-To: <e0f4395b-e1cf-abfa-e4d4-ee3dd519887e@bath.edu>
References: <CANg3_k-18ND8DrwfYJmGAN5hq18QweZFPp4GhCuHwB=XFGAFiA@mail.gmail.com>
 <CANg3_k9=GhwRjPyuLQBYmxD8Q-ksPrM52tS7DW0Cv3cpKn00Zw@mail.gmail.com>
 <e0f4395b-e1cf-abfa-e4d4-ee3dd519887e@bath.edu>
Message-ID: <CANg3_k9-OKAK1VSirWSVGXQVMWoDeKoG7gN7mUfGfZdX4CnpgQ@mail.gmail.com>

Hi Simon,

Thanks for the pointers! But I'm not sure what to do with the 'time'
variable. (I don't want to make predictions for specific points in
time). I coded as follows (full reproducible example at bottom of
email), but get a warning and error:


N <- 1000     # number of points for smooth to be predicted
# new temperatures and lags for prediction
pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)      ## IS IT CORRECT TO SET
UP LAG LIKE THIS?

# not sure if these covariates are required with type = "terms"
pred_humidity <- rep(median(dat$humidity), N)
pred_rain <- rep(median(dat$rain), N)
pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
pred_humidity, rain = pred_rain)

predictions <- predict(mod, pd, type = "terms")


The predict line creates the following warning and error:

Warning in predict.gam(mod, pd, type = "terms") :
  not all required variables have been supplied in  newdata!

Error in model.frame.default(ff, data = newdata, na.action = na.act) :
  object is not a matrix


For ease of reference, I've (re)included the full reproducible example:

library(mgcv)
set.seed(3) # make reproducible example
simdat <- gamSim(1,400)
g <- exp(simdat$f/5)

simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
simdat$time <- 1:400  # create time series

names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
"f0", "f1", "f2", "f3", "time")

# lag function based on Wood (book 2017, p.349 and gamair package
documentation p.54
# https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
lagard <- function(x,n.lag=7) {
n <- length(x); X <- matrix(NA,n,n.lag)
for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
X
}

# set up lag, temp, rain and humidity as 7-column matrices
# to create lagged variables
dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
deaths=simdat$deaths, time = simdat$time)
dat$temp <- lagard(simdat$temp)
dat$rain <- lagard(simdat$rain)
dat$humidity <- lagard(simdat$humidity)

mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
family = nb, method = 'REML', select = TRUE)

# create prediction data
N <- 1000 # number of points for which to predict the smooths
pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
pred_humidity <- rep(median(dat$humidity), N)
pred_rain <- rep(median(dat$rain), N)
pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
pred_humidity, rain = pred_rain)

# make predictions
predictions <- predict(mod, pd, type = "terms")


On Fri, 22 Jul 2022 at 09:54, Simon Wood <simon.wood at bath.edu> wrote:
>
>
> On 21/07/2022 15:19, jade.shodan--- via R-help wrote:
> > Hello everyone (incl. Simon Wood?),
> >
> > I'm not sure that my original question (see below, including
> > reproducible example) was as clear as it could have been. To clarify,
> > what I would to like to get is:
> >
> > 1) a perspective plot of temperature x lag x relative risk.  I know
> > how to use plot.gam and vis.gam but don't know how to get plots on the
> > relative risk scale as opposed to  "response" or "link".
>
> - You are on the log scale so I think that all you need to do is to use
> 'predict.gam', with 'type = "terms"' to  get the predictions for the
> te(temp, lag) term over the required grid of lags and temperatures.
> Suppose the dataframe of prediction data is 'pd'. Now produce pd0, which
> is identical to pd, except that the temperatures are all set to the
> reference temperature. Use predict.gam to predict te(temp,lag) from pd0.
> Now the exponential of the difference between the first and second
> predictions is the required RR, which you can plot using 'persp',
> 'contour', 'image' or whatever. If you need credible intervals see pages
> 341-343 of my 'GAMs: An intro with R' book (2nd ed).
>
> > 2) a plot of relative risk (accumulated across all lags) vs
> > temperature, given a reference temperature. An example of such a plot
> > can be found in figure 2 (bottom) of this paper by Gasparrini et al:
> > https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3940
>
> - I guess this only makes sense if you have the same temperature at all
> lags. So this time produce a data.frame with each desired temperature
> repeated for each lag: 'pd1'. Again use predict.gam(...,type="terms").
> Then sum the predictions over lags for each temperature, subtract the
> minimum, and take the exponential. Same as above for CIs.
>
> best,
>
> Simon
>
> > I've seen Simon Wood's response to a related issue here:
> > https://stat.ethz.ch/pipermail/r-help/2012-May/314387.html
> > However, I'm not sure how to apply this to time series data with
> > distributed lag, to get the above mentioned figures.
> >
> > Would be really grateful for suggestions!
> >
> > Jade
> >
> > On Tue, 19 Jul 2022 at 16:07, jade.shodan at googlemail.com
> > <jade.shodan at googlemail.com> wrote:
> >> Dear list members,
> >>
> >> Does anyone know how to obtain a relative risk/ risk ratio from a GAM
> >> with a distributed lag model implemented in mgcv? I have a GAM
> >> predicting daily deaths from time series data consisting of daily
> >> temperature, humidity and rainfall. The GAM includes a distributed lag
> >> model because deaths may occur over several days following a high heat
> >> day.
> >>
> >> What I'd like to do is compute (and plot) the relative risk
> >> (accumulated across all lags) for a given temperature vs the
> >> temperature at which the risk is lowest, with corresponding confidence
> >> intervals. I am aware of the predict.gam function but am not sure if
> >> and how it should be used in this case. (Additionally, I'd also like
> >> to plot the relative risk for different lags separately).
> >>
> >> I apologise if this seems trivial to some. (Actually, I hope it is,
> >> because that might mean I get a solution!) I've been looking for
> >> examples on how to do this, but found nothing so far. Suggestions
> >> would be very much appreciated!
> >>
> >> Below is a reproducible example with the GAM:
> >>
> >> library(mgcv)
> >> set.seed(3) # make reproducible example
> >> simdat <- gamSim(1,400) # simulate data
> >> g <- exp(simdat$f/5)
> >> simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
> >> simdat$time <- 1:400  # create time series
> >> names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
> >> "f0", "f1", "f2", "f3", "time")
> >>
> >> # lag function based on Simon Wood (book 2017, p.349 and gamair
> >> package documentation p.54
> >> # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
> >> lagard <- function(x,n.lag=7) {
> >> n <- length(x); X <- matrix(NA,n,n.lag)
> >> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> >> X
> >> }
> >>
> >> # set up lag, temp, rain and humidity as 7-column matrices
> >> # to create lagged variables - based on Simon Wood's example
> >> dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
> >> deaths=simdat$deaths, time = simdat$time)
> >> dat$temp <- lagard(simdat$temp)
> >> dat$rain <- lagard(simdat$rain)
> >> dat$humidity <- lagard(simdat$humidity)
> >>
> >> mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
> >> te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
> >> family = nb, method = 'REML', select = TRUE)
> >>
> >> summary(mod)
> >> plot(mod, scheme = 1)
> >> plot(mod, scheme = 2)
> >>
> >> Thanks for any suggestions you may have,
> >>
> >> Jade
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Simon Wood, School of Mathematics, University of Edinburgh,
> https://www.maths.ed.ac.uk/~swood34/
>


From j@de@shod@@ m@iii@g oii googiem@ii@com  Fri Jul 22 14:29:36 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Fri, 22 Jul 2022 13:29:36 +0100
Subject: [R] mgcv: relative risk from GAM with distributed lag
In-Reply-To: <CANg3_k9-OKAK1VSirWSVGXQVMWoDeKoG7gN7mUfGfZdX4CnpgQ@mail.gmail.com>
References: <CANg3_k-18ND8DrwfYJmGAN5hq18QweZFPp4GhCuHwB=XFGAFiA@mail.gmail.com>
 <CANg3_k9=GhwRjPyuLQBYmxD8Q-ksPrM52tS7DW0Cv3cpKn00Zw@mail.gmail.com>
 <e0f4395b-e1cf-abfa-e4d4-ee3dd519887e@bath.edu>
 <CANg3_k9-OKAK1VSirWSVGXQVMWoDeKoG7gN7mUfGfZdX4CnpgQ@mail.gmail.com>
Message-ID: <CANg3_k9Nkok-Mr4DFOZ3=d1XtA-Pq1seW+E+KtWLttQUe_+XcQ@mail.gmail.com>

I made a small error in the code below (not checking for NAs which are
introduced by the lag function). However, this doesn't solve the issue
I raised).

So here's the problem (with corrected code) again:

I'm not sure what to do with the 'time' variable. (I don't want to
make predictions for specific points in time). I coded as follows
(full reproducible example at bottom of email), but get a warning and
error:


N <- 1000     # number of points for smooth to be predicted
# new temperatures and lags for prediction
pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)      ## IS IT CORRECT TO SET
UP LAG LIKE THIS?

# not sure if these covariates are required with type = "terms"
pred_humidity <- rep(median(dat$humidity, na.rm = T), N)
pred_rain <- rep(median(dat$rain, na.rm = T), N)
pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
pred_humidity, rain = pred_rain)

predictions <- predict(mod, pd, type = "terms")


The predict line creates the following warning and error:

Warning in predict.gam(mod, pd, type = "terms") :
  not all required variables have been supplied in  newdata!

Error in model.frame.default(ff, data = newdata, na.action = na.act) :
  object is not a matrix


For ease of reference, I've (re)included the full reproducible example:

library(mgcv)
set.seed(3) # make reproducible example
simdat <- gamSim(1,400)
g <- exp(simdat$f/5)

simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
simdat$time <- 1:400  # create time series

names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
"f0", "f1", "f2", "f3", "time")

# lag function based on Wood (book 2017, p.349 and gamair package
documentation p.54
# https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
lagard <- function(x,n.lag=7) {
n <- length(x); X <- matrix(NA,n,n.lag)
for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
X
}

# set up lag, temp, rain and humidity as 7-column matrices
# to create lagged variables
dat <- list(lag=matrix(0:6,nrow(simda
t),7,byrow=TRUE),
deaths=simdat$deaths, time = simdat$time)
dat$temp <- lagard(simdat$temp)
dat$rain <- lagard(simdat$rain)
dat$humidity <- lagard(simdat$humidity)

mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
family = nb, method = 'REML', select = TRUE)

# create prediction data
N <- 1000 # number of points for which to predict the smooths
pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
pred_humidity <- rep(median(dat$humidity), N)
pred_rain <- rep(median(dat$rain), N)
pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
pred_humidity, rain = pred_rain)

# make predictions
predictions <- predict(mod, pd, type = "terms")

On Fri, 22 Jul 2022 at 12:47, jade.shodan at googlemail.com
<jade.shodan at googlemail.com> wrote:
>
> Hi Simon,
>
> Thanks for the pointers! But I'm not sure what to do with the 'time'
> variable. (I don't want to make predictions for specific points in
> time). I coded as follows (full reproducible example at bottom of
> email), but get a warning and error:
>
>
> N <- 1000     # number of points for smooth to be predicted
> # new temperatures and lags for prediction
> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
> pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)      ## IS IT CORRECT TO SET
> UP LAG LIKE THIS?
>
> # not sure if these covariates are required with type = "terms"
> pred_humidity <- rep(median(dat$humidity), N)
> pred_rain <- rep(median(dat$rain), N)
> pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> pred_humidity, rain = pred_rain)
>
> predictions <- predict(mod, pd, type = "terms")
>
>
> The predict line creates the following warning and error:
>
> Warning in predict.gam(mod, pd, type = "terms") :
>   not all required variables have been supplied in  newdata!
>
> Error in model.frame.default(ff, data = newdata, na.action = na.act) :
>   object is not a matrix
>
>
> For ease of reference, I've (re)included the full reproducible example:
>
> library(mgcv)
> set.seed(3) # make reproducible example
> simdat <- gamSim(1,400)
> g <- exp(simdat$f/5)
>
> simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
> simdat$time <- 1:400  # create time series
>
> names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
> "f0", "f1", "f2", "f3", "time")
>
> # lag function based on Wood (book 2017, p.349 and gamair package
> documentation p.54
> # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
> lagard <- function(x,n.lag=7) {
> n <- length(x); X <- matrix(NA,n,n.lag)
> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> X
> }
>
> # set up lag, temp, rain and humidity as 7-column matrices
> # to create lagged variables
> dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
> deaths=simdat$deaths, time = simdat$time)
> dat$temp <- lagard(simdat$temp)
> dat$rain <- lagard(simdat$rain)
> dat$humidity <- lagard(simdat$humidity)
>
> mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
> te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
> family = nb, method = 'REML', select = TRUE)
>
> # create prediction data
> N <- 1000 # number of points for which to predict the smooths
> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
> pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
> pred_humidity <- rep(median(dat$humidity), N)
> pred_rain <- rep(median(dat$rain), N)
> pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> pred_humidity, rain = pred_rain)
>
> # make predictions
> predictions <- predict(mod, pd, type = "terms")
>
>
> On Fri, 22 Jul 2022 at 09:54, Simon Wood <simon.wood at bath.edu> wrote:
> >
> >
> > On 21/07/2022 15:19, jade.shodan--- via R-help wrote:
> > > Hello everyone (incl. Simon Wood?),
> > >
> > > I'm not sure that my original question (see below, including
> > > reproducible example) was as clear as it could have been. To clarify,
> > > what I would to like to get is:
> > >
> > > 1) a perspective plot of temperature x lag x relative risk.  I know
> > > how to use plot.gam and vis.gam but don't know how to get plots on the
> > > relative risk scale as opposed to  "response" or "link".
> >
> > - You are on the log scale so I think that all you need to do is to use
> > 'predict.gam', with 'type = "terms"' to  get the predictions for the
> > te(temp, lag) term over the required grid of lags and temperatures.
> > Suppose the dataframe of prediction data is 'pd'. Now produce pd0, which
> > is identical to pd, except that the temperatures are all set to the
> > reference temperature. Use predict.gam to predict te(temp,lag) from pd0.
> > Now the exponential of the difference between the first and second
> > predictions is the required RR, which you can plot using 'persp',
> > 'contour', 'image' or whatever. If you need credible intervals see pages
> > 341-343 of my 'GAMs: An intro with R' book (2nd ed).
> >
> > > 2) a plot of relative risk (accumulated across all lags) vs
> > > temperature, given a reference temperature. An example of such a plot
> > > can be found in figure 2 (bottom) of this paper by Gasparrini et al:
> > > https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3940
> >
> > - I guess this only makes sense if you have the same temperature at all
> > lags. So this time produce a data.frame with each desired temperature
> > repeated for each lag: 'pd1'. Again use predict.gam(...,type="terms").
> > Then sum the predictions over lags for each temperature, subtract the
> > minimum, and take the exponential. Same as above for CIs.
> >
> > best,
> >
> > Simon
> >
> > > I've seen Simon Wood's response to a related issue here:
> > > https://stat.ethz.ch/pipermail/r-help/2012-May/314387.html
> > > However, I'm not sure how to apply this to time series data with
> > > distributed lag, to get the above mentioned figures.
> > >
> > > Would be really grateful for suggestions!
> > >
> > > Jade
> > >
> > > On Tue, 19 Jul 2022 at 16:07, jade.shodan at googlemail.com
> > > <jade.shodan at googlemail.com> wrote:
> > >> Dear list members,
> > >>
> > >> Does anyone know how to obtain a relative risk/ risk ratio from a GAM
> > >> with a distributed lag model implemented in mgcv? I have a GAM
> > >> predicting daily deaths from time series data consisting of daily
> > >> temperature, humidity and rainfall. The GAM includes a distributed lag
> > >> model because deaths may occur over several days following a high heat
> > >> day.
> > >>
> > >> What I'd like to do is compute (and plot) the relative risk
> > >> (accumulated across all lags) for a given temperature vs the
> > >> temperature at which the risk is lowest, with corresponding confidence
> > >> intervals. I am aware of the predict.gam function but am not sure if
> > >> and how it should be used in this case. (Additionally, I'd also like
> > >> to plot the relative risk for different lags separately).
> > >>
> > >> I apologise if this seems trivial to some. (Actually, I hope it is,
> > >> because that might mean I get a solution!) I've been looking for
> > >> examples on how to do this, but found nothing so far. Suggestions
> > >> would be very much appreciated!
> > >>
> > >> Below is a reproducible example with the GAM:
> > >>
> > >> library(mgcv)
> > >> set.seed(3) # make reproducible example
> > >> simdat <- gamSim(1,400) # simulate data
> > >> g <- exp(simdat$f/5)
> > >> simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
> > >> simdat$time <- 1:400  # create time series
> > >> names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
> > >> "f0", "f1", "f2", "f3", "time")
> > >>
> > >> # lag function based on Simon Wood (book 2017, p.349 and gamair
> > >> package documentation p.54
> > >> # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
> > >> lagard <- function(x,n.lag=7) {
> > >> n <- length(x); X <- matrix(NA,n,n.lag)
> > >> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> > >> X
> > >> }
> > >>
> > >> # set up lag, temp, rain and humidity as 7-column matrices
> > >> # to create lagged variables - based on Simon Wood's example
> > >> dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
> > >> deaths=simdat$deaths, time = simdat$time)
> > >> dat$temp <- lagard(simdat$temp)
> > >> dat$rain <- lagard(simdat$rain)
> > >> dat$humidity <- lagard(simdat$humidity)
> > >>
> > >> mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
> > >> te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
> > >> family = nb, method = 'REML', select = TRUE)
> > >>
> > >> summary(mod)
> > >> plot(mod, scheme = 1)
> > >> plot(mod, scheme = 2)
> > >>
> > >> Thanks for any suggestions you may have,
> > >>
> > >> Jade
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > --
> > Simon Wood, School of Mathematics, University of Edinburgh,
> > https://www.maths.ed.ac.uk/~swood34/
> >


From @|dot|@23 @end|ng |rom o@u@edu  Fri Jul 22 15:50:23 2022
From: @|dot|@23 @end|ng |rom o@u@edu (Sidoti, Salvatore)
Date: Fri, 22 Jul 2022 13:50:23 +0000
Subject: [R] Detect QRS Complex & Arrhythmia From Electrocardiogram (ECG)
Message-ID: <DM6PR01MB4314C59E9C3AF9C1CCEFB88BEA909@DM6PR01MB4314.prod.exchangelabs.com>

While I've seen MATLAB and Python programs for this purpose (see subject), do any current R packages offer QRS or arrhythmia detection? Maybe someone has an R algorithm not yet published? I am familiar with the RHRV package, but it assumes that the data already has the heartbeats as a time series. Plus, the RHRV package does not detect arrhythmias.

Cheers,
Salvatore A. Sidoti
Research Associate 2

Wexner Medical Center, College of Medicine
Davis Heart and Lung Research Institute
473 W. 12th Avenue, Suite 110, Columbus, OH, 43210
sidoti.23 at osu.edu


From j@de@shod@@ m@iii@g oii googiem@ii@com  Fri Jul 22 16:10:37 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Fri, 22 Jul 2022 15:10:37 +0100
Subject: [R] 
 Detect QRS Complex & Arrhythmia From Electrocardiogram (ECG)
In-Reply-To: <DM6PR01MB4314C59E9C3AF9C1CCEFB88BEA909@DM6PR01MB4314.prod.exchangelabs.com>
References: <DM6PR01MB4314C59E9C3AF9C1CCEFB88BEA909@DM6PR01MB4314.prod.exchangelabs.com>
Message-ID: <CANg3_k-qzPrd=ttbaXeCGgKk+pjW-cJ=YmRhdwusZT2PuHw0yg@mail.gmail.com>

Hi Salvatore,

This (very recent) paper may be of interest:

Enevoldsen J, Simpson GL, Vistisen ST. Using generalized additive
models to decompose time series and waveforms, and dissect heart?lung
interaction physiology. J Clin Monit Comput [Internet]. 2022 Jun 13
[cited 2022 Jun 13]; Available from:
https://doi.org/10.1007/s10877-022-00873-7

The authors make use of QRS in their GAM models in R.

Not sure if it provides entirely what you are after, but perhaps
references in the paper or contacting Enevoldsen may provide useful
starting points.

Best wishes,

Jade

On Fri, 22 Jul 2022 at 14:50, Sidoti, Salvatore <sidoti.23 at osu.edu> wrote:
>
> While I've seen MATLAB and Python programs for this purpose (see subject), do any current R packages offer QRS or arrhythmia detection? Maybe someone has an R algorithm not yet published? I am familiar with the RHRV package, but it assumes that the data already has the heartbeats as a time series. Plus, the RHRV package does not detect arrhythmias.
>
> Cheers,
> Salvatore A. Sidoti
> Research Associate 2
>
> Wexner Medical Center, College of Medicine
> Davis Heart and Lung Research Institute
> 473 W. 12th Avenue, Suite 110, Columbus, OH, 43210
> sidoti.23 at osu.edu
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dw|n@em|u@ @end|ng |rom comc@@t@net  Fri Jul 22 16:22:52 2022
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Fri, 22 Jul 2022 09:22:52 -0500
Subject: [R] 
 Detect QRS Complex & Arrhythmia From Electrocardiogram (ECG)
In-Reply-To: <CANg3_k-qzPrd=ttbaXeCGgKk+pjW-cJ=YmRhdwusZT2PuHw0yg@mail.gmail.com>
References: <CANg3_k-qzPrd=ttbaXeCGgKk+pjW-cJ=YmRhdwusZT2PuHw0yg@mail.gmail.com>
Message-ID: <6F05D68C-8DD5-407E-A5E0-7889C9E293EC@comcast.net>

The package is documented as accepting data created by the wearables package. It seems to have peak detection so might supply part of your goal. 

? 
David

Sent from my iPhone

> On Jul 22, 2022, at 9:11 AM, jade.shodan--- via R-help <r-help at r-project.org> wrote:
> 
> ?Hi Salvatore,
> 
> This (very recent) paper may be of interest:
> 
> Enevoldsen J, Simpson GL, Vistisen ST. Using generalized additive
> models to decompose time series and waveforms, and dissect heart?lung
> interaction physiology. J Clin Monit Comput [Internet]. 2022 Jun 13
> [cited 2022 Jun 13]; Available from:
> https://doi.org/10.1007/s10877-022-00873-7
> 
> The authors make use of QRS in their GAM models in R.
> 
> Not sure if it provides entirely what you are after, but perhaps
> references in the paper or contacting Enevoldsen may provide useful
> starting points.
> 
> Best wishes,
> 
> Jade
> 
>> On Fri, 22 Jul 2022 at 14:50, Sidoti, Salvatore <sidoti.23 at osu.edu> wrote:
>> 
>> While I've seen MATLAB and Python programs for this purpose (see subject), do any current R packages offer QRS or arrhythmia detection? Maybe someone has an R algorithm not yet published? I am familiar with the RHRV package, but it assumes that the data already has the heartbeats as a time series. Plus, the RHRV package does not detect arrhythmias.
>> 
>> Cheers,
>> Salvatore A. Sidoti
>> Research Associate 2
>> 
>> Wexner Medical Center, College of Medicine
>> Davis Heart and Lung Research Institute
>> 473 W. 12th Avenue, Suite 110, Columbus, OH, 43210
>> sidoti.23 at osu.edu
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j@de@shod@@ m@iii@g oii googiem@ii@com  Sat Jul 23 15:54:16 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Sat, 23 Jul 2022 14:54:16 +0100
Subject: [R] mgcv: relative risk from GAM with distributed lag
In-Reply-To: <CANg3_k9Nkok-Mr4DFOZ3=d1XtA-Pq1seW+E+KtWLttQUe_+XcQ@mail.gmail.com>
References: <CANg3_k-18ND8DrwfYJmGAN5hq18QweZFPp4GhCuHwB=XFGAFiA@mail.gmail.com>
 <CANg3_k9=GhwRjPyuLQBYmxD8Q-ksPrM52tS7DW0Cv3cpKn00Zw@mail.gmail.com>
 <e0f4395b-e1cf-abfa-e4d4-ee3dd519887e@bath.edu>
 <CANg3_k9-OKAK1VSirWSVGXQVMWoDeKoG7gN7mUfGfZdX4CnpgQ@mail.gmail.com>
 <CANg3_k9Nkok-Mr4DFOZ3=d1XtA-Pq1seW+E+KtWLttQUe_+XcQ@mail.gmail.com>
Message-ID: <CANg3_k8C5W1RKL-gsGbr3556hSHq7+-oCSRCZR-ODEsN_7_MAA@mail.gmail.com>

Hi Simon and all,

I've corrected some mistakes in setting up the prediction data frame
(sorry, very stressed and sleep deprived due to closing in deadlines),
but am having problems getting the perspective plot using persp().

I've calculated relative risk (RR) but am having trouble getting the
perspective (or contour) plot. persp() takes x and y vectors in
ascending order, and z as a matrix.  I've seen the outer() function
being used to facilitate perspective plots, but every example I've
seen treats z as a function, to be evaluated at combinations for x and
y. In my case, I already have observations for z. Not sure if I need
to use outer(), and if so, how?

My code for making predictions, getting RR and trying to get the plot
is as follows (full reproducible example including model at the
bottom). Can someone tell me what I'm doing wrong?

########### CODE ########################

# create prediction data
N <- 1000 # number of points for which to predict the smooths
pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T),
length = N)  # prediction data for temperature
pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
pred_rain <- seq(min(dat$rain, na.rm = T), max(dat$rain, na.rm = T), length = N)
pred_humidity <- seq(min(dat$humidity, na.rm = T), max(dat$humidity,
na.rm = T), length = N)
pred_time <- seq(min(dat$time, na.rm = T), max(dat$time, na.rm = T), length = N)

pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
pred_humidity, rain = pred_rain, time = pred_time)

# create prediction data with reference temperature set to median temperature
# identical to pd but now with all temperatures set to median value of
temperature
pred_temp0 <- rep(median(dat$temp, na.rm=T), N)
pd0 <- data.frame(temp = pred_temp0, lag = pred_lag, humidity =
pred_humidity, rain = pred_rain, time = pred_time)

# make predictions
predictions <- predict.gam(mod, pd, type = "terms")
predictions0 <- predict.gam(mod, pd0, type = "terms")

# calculate RR
diff <- predictions[,2] - predictions0[,2]
rr <- as.vector(exp(diff))

# convert rr to matrix required for z argument for persp()
rr_mat <- matrix(rr, nrow = N)
persp(pred_temp, pred_lag, rr_mat)

######### ERROR MESSAGE ##################################

The persp call results in the follow error:

Error in persp.default(pred_temp, pred_lag, rr_mat) :
  increasing 'x' and 'y' values expected

I don't understand this because pred_temp and pred_lag ARE in ascending order.

############################################################
FULL REPRODUCIBLE EXAMPLE
############################################################

library(mgcv)
set.seed(3) # make reproducible example
simdat <- gamSim(1,400)
g <- exp(simdat$f/5)

simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
simdat$time <- 1:400  # create time series

names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
"f0", "f1", "f2", "f3", "time")

# lag function based on Wood (book 2017, p.349 and gamair package
documentation p.54
# https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
lagard <- function(x,n.lag=7) {
n <- length(x); X <- matrix(NA,n,n.lag)
for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
X
}

# set up lag, temp, rain and humidity as 7-column matrices
# to create lagged variables
dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
deaths=simdat$deaths, time = simdat$time)
dat$temp <- lagard(simdat$temp)
dat$rain <- lagard(simdat$rain)
dat$humidity <- lagard(simdat$humidity)

mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
family = nb, method = 'REML', select = TRUE)

summary(mod)
plot(mod, scheme = 1)  # perspective
plot(mod, scheme = 2)  # contour

# create prediction data
N <- 1000 # number of points for which to predict the smooths
pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
pred_rain <- seq(min(dat$rain, na.rm = T), max(dat$rain, na.rm = T), length = N)
pred_humidity <- seq(min(dat$humidity, na.rm = T), max(dat$humidity,
na.rm = T), length = N)
pred_time <- seq(min(dat$time, na.rm = T), max(dat$time, na.rm = T), length = N)

pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
pred_humidity, rain = pred_rain, time = pred_time)

# create prediction data with reference temperature set to median temperature
# identical to pd but now with all temperatures set to median value of
temperature
pred_temp0 <- rep(median(dat$temp, na.rm=T), N)
pd0 <- data.frame(temp = pred_temp0, lag = pred_lag, humidity =
pred_humidity, rain = pred_rain, time = pred_time)

# make predictions
predictions <- predict.gam(mod, pd, type = "terms")
predictions0 <- predict.gam(mod, pd0, type = "terms")

# calculate RR
diff <- predictions[,2] - predictions0[,2]
rr <- as.vector(exp(diff))

# convert rr to matrix required for z argument for persp()
rr_mat <- matrix(rr, nrow = N)
persp(pred_temp, pred_lag, rr_mat)




On Fri, 22 Jul 2022 at 13:29, jade.shodan at googlemail.com
<jade.shodan at googlemail.com> wrote:
>
> I made a small error in the code below (not checking for NAs which are
> introduced by the lag function). However, this doesn't solve the issue
> I raised).
>
> So here's the problem (with corrected code) again:
>
> I'm not sure what to do with the 'time' variable. (I don't want to
> make predictions for specific points in time). I coded as follows
> (full reproducible example at bottom of email), but get a warning and
> error:
>
>
> N <- 1000     # number of points for smooth to be predicted
> # new temperatures and lags for prediction
> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
> pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)      ## IS IT CORRECT TO SET
> UP LAG LIKE THIS?
>
> # not sure if these covariates are required with type = "terms"
> pred_humidity <- rep(median(dat$humidity, na.rm = T), N)
> pred_rain <- rep(median(dat$rain, na.rm = T), N)
> pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> pred_humidity, rain = pred_rain)
>
> predictions <- predict(mod, pd, type = "terms")
>
>
> The predict line creates the following warning and error:
>
> Warning in predict.gam(mod, pd, type = "terms") :
>   not all required variables have been supplied in  newdata!
>
> Error in model.frame.default(ff, data = newdata, na.action = na.act) :
>   object is not a matrix
>
>
> For ease of reference, I've (re)included the full reproducible example:
>
> library(mgcv)
> set.seed(3) # make reproducible example
> simdat <- gamSim(1,400)
> g <- exp(simdat$f/5)
>
> simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
> simdat$time <- 1:400  # create time series
>
> names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
> "f0", "f1", "f2", "f3", "time")
>
> # lag function based on Wood (book 2017, p.349 and gamair package
> documentation p.54
> # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
> lagard <- function(x,n.lag=7) {
> n <- length(x); X <- matrix(NA,n,n.lag)
> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> X
> }
>
> # set up lag, temp, rain and humidity as 7-column matrices
> # to create lagged variables
> dat <- list(lag=matrix(0:6,nrow(simda
> t),7,byrow=TRUE),
> deaths=simdat$deaths, time = simdat$time)
> dat$temp <- lagard(simdat$temp)
> dat$rain <- lagard(simdat$rain)
> dat$humidity <- lagard(simdat$humidity)
>
> mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
> te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
> family = nb, method = 'REML', select = TRUE)
>
> # create prediction data
> N <- 1000 # number of points for which to predict the smooths
> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
> pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
> pred_humidity <- rep(median(dat$humidity), N)
> pred_rain <- rep(median(dat$rain), N)
> pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> pred_humidity, rain = pred_rain)
>
> # make predictions
> predictions <- predict(mod, pd, type = "terms")
>
> On Fri, 22 Jul 2022 at 12:47, jade.shodan at googlemail.com
> <jade.shodan at googlemail.com> wrote:
> >
> > Hi Simon,
> >
> > Thanks for the pointers! But I'm not sure what to do with the 'time'
> > variable. (I don't want to make predictions for specific points in
> > time). I coded as follows (full reproducible example at bottom of
> > email), but get a warning and error:
> >
> >
> > N <- 1000     # number of points for smooth to be predicted
> > # new temperatures and lags for prediction
> > pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
> > pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)      ## IS IT CORRECT TO SET
> > UP LAG LIKE THIS?
> >
> > # not sure if these covariates are required with type = "terms"
> > pred_humidity <- rep(median(dat$humidity), N)
> > pred_rain <- rep(median(dat$rain), N)
> > pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> > pred_humidity, rain = pred_rain)
> >
> > predictions <- predict(mod, pd, type = "terms")
> >
> >
> > The predict line creates the following warning and error:
> >
> > Warning in predict.gam(mod, pd, type = "terms") :
> >   not all required variables have been supplied in  newdata!
> >
> > Error in model.frame.default(ff, data = newdata, na.action = na.act) :
> >   object is not a matrix
> >
> >
> > For ease of reference, I've (re)included the full reproducible example:
> >
> > library(mgcv)
> > set.seed(3) # make reproducible example
> > simdat <- gamSim(1,400)
> > g <- exp(simdat$f/5)
> >
> > simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
> > simdat$time <- 1:400  # create time series
> >
> > names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
> > "f0", "f1", "f2", "f3", "time")
> >
> > # lag function based on Wood (book 2017, p.349 and gamair package
> > documentation p.54
> > # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
> > lagard <- function(x,n.lag=7) {
> > n <- length(x); X <- matrix(NA,n,n.lag)
> > for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> > X
> > }
> >
> > # set up lag, temp, rain and humidity as 7-column matrices
> > # to create lagged variables
> > dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
> > deaths=simdat$deaths, time = simdat$time)
> > dat$temp <- lagard(simdat$temp)
> > dat$rain <- lagard(simdat$rain)
> > dat$humidity <- lagard(simdat$humidity)
> >
> > mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
> > te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
> > family = nb, method = 'REML', select = TRUE)
> >
> > # create prediction data
> > N <- 1000 # number of points for which to predict the smooths
> > pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
> > pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
> > pred_humidity <- rep(median(dat$humidity), N)
> > pred_rain <- rep(median(dat$rain), N)
> > pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> > pred_humidity, rain = pred_rain)
> >
> > # make predictions
> > predictions <- predict(mod, pd, type = "terms")
> >
> >
> > On Fri, 22 Jul 2022 at 09:54, Simon Wood <simon.wood at bath.edu> wrote:
> > >
> > >
> > > On 21/07/2022 15:19, jade.shodan--- via R-help wrote:
> > > > Hello everyone (incl. Simon Wood?),
> > > >
> > > > I'm not sure that my original question (see below, including
> > > > reproducible example) was as clear as it could have been. To clarify,
> > > > what I would to like to get is:
> > > >
> > > > 1) a perspective plot of temperature x lag x relative risk.  I know
> > > > how to use plot.gam and vis.gam but don't know how to get plots on the
> > > > relative risk scale as opposed to  "response" or "link".
> > >
> > > - You are on the log scale so I think that all you need to do is to use
> > > 'predict.gam', with 'type = "terms"' to  get the predictions for the
> > > te(temp, lag) term over the required grid of lags and temperatures.
> > > Suppose the dataframe of prediction data is 'pd'. Now produce pd0, which
> > > is identical to pd, except that the temperatures are all set to the
> > > reference temperature. Use predict.gam to predict te(temp,lag) from pd0.
> > > Now the exponential of the difference between the first and second
> > > predictions is the required RR, which you can plot using 'persp',
> > > 'contour', 'image' or whatever. If you need credible intervals see pages
> > > 341-343 of my 'GAMs: An intro with R' book (2nd ed).
> > >
> > > > 2) a plot of relative risk (accumulated across all lags) vs
> > > > temperature, given a reference temperature. An example of such a plot
> > > > can be found in figure 2 (bottom) of this paper by Gasparrini et al:
> > > > https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3940
> > >
> > > - I guess this only makes sense if you have the same temperature at all
> > > lags. So this time produce a data.frame with each desired temperature
> > > repeated for each lag: 'pd1'. Again use predict.gam(...,type="terms").
> > > Then sum the predictions over lags for each temperature, subtract the
> > > minimum, and take the exponential. Same as above for CIs.
> > >
> > > best,
> > >
> > > Simon
> > >
> > > > I've seen Simon Wood's response to a related issue here:
> > > > https://stat.ethz.ch/pipermail/r-help/2012-May/314387.html
> > > > However, I'm not sure how to apply this to time series data with
> > > > distributed lag, to get the above mentioned figures.
> > > >
> > > > Would be really grateful for suggestions!
> > > >
> > > > Jade
> > > >
> > > > On Tue, 19 Jul 2022 at 16:07, jade.shodan at googlemail.com
> > > > <jade.shodan at googlemail.com> wrote:
> > > >> Dear list members,
> > > >>
> > > >> Does anyone know how to obtain a relative risk/ risk ratio from a GAM
> > > >> with a distributed lag model implemented in mgcv? I have a GAM
> > > >> predicting daily deaths from time series data consisting of daily
> > > >> temperature, humidity and rainfall. The GAM includes a distributed lag
> > > >> model because deaths may occur over several days following a high heat
> > > >> day.
> > > >>
> > > >> What I'd like to do is compute (and plot) the relative risk
> > > >> (accumulated across all lags) for a given temperature vs the
> > > >> temperature at which the risk is lowest, with corresponding confidence
> > > >> intervals. I am aware of the predict.gam function but am not sure if
> > > >> and how it should be used in this case. (Additionally, I'd also like
> > > >> to plot the relative risk for different lags separately).
> > > >>
> > > >> I apologise if this seems trivial to some. (Actually, I hope it is,
> > > >> because that might mean I get a solution!) I've been looking for
> > > >> examples on how to do this, but found nothing so far. Suggestions
> > > >> would be very much appreciated!
> > > >>
> > > >> Below is a reproducible example with the GAM:
> > > >>
> > > >> library(mgcv)
> > > >> set.seed(3) # make reproducible example
> > > >> simdat <- gamSim(1,400) # simulate data
> > > >> g <- exp(simdat$f/5)
> > > >> simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
> > > >> simdat$time <- 1:400  # create time series
> > > >> names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
> > > >> "f0", "f1", "f2", "f3", "time")
> > > >>
> > > >> # lag function based on Simon Wood (book 2017, p.349 and gamair
> > > >> package documentation p.54
> > > >> # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
> > > >> lagard <- function(x,n.lag=7) {
> > > >> n <- length(x); X <- matrix(NA,n,n.lag)
> > > >> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> > > >> X
> > > >> }
> > > >>
> > > >> # set up lag, temp, rain and humidity as 7-column matrices
> > > >> # to create lagged variables - based on Simon Wood's example
> > > >> dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
> > > >> deaths=simdat$deaths, time = simdat$time)
> > > >> dat$temp <- lagard(simdat$temp)
> > > >> dat$rain <- lagard(simdat$rain)
> > > >> dat$humidity <- lagard(simdat$humidity)
> > > >>
> > > >> mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
> > > >> te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
> > > >> family = nb, method = 'REML', select = TRUE)
> > > >>
> > > >> summary(mod)
> > > >> plot(mod, scheme = 1)
> > > >> plot(mod, scheme = 2)
> > > >>
> > > >> Thanks for any suggestions you may have,
> > > >>
> > > >> Jade
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > > --
> > > Simon Wood, School of Mathematics, University of Edinburgh,
> > > https://www.maths.ed.ac.uk/~swood34/
> > >


From @|mon@wood @end|ng |rom b@th@edu  Sat Jul 23 22:26:20 2022
From: @|mon@wood @end|ng |rom b@th@edu (Simon Wood)
Date: Sat, 23 Jul 2022 21:26:20 +0100
Subject: [R] mgcv: relative risk from GAM with distributed lag
In-Reply-To: <CANg3_k8C5W1RKL-gsGbr3556hSHq7+-oCSRCZR-ODEsN_7_MAA@mail.gmail.com>
References: <CANg3_k-18ND8DrwfYJmGAN5hq18QweZFPp4GhCuHwB=XFGAFiA@mail.gmail.com>
 <CANg3_k9=GhwRjPyuLQBYmxD8Q-ksPrM52tS7DW0Cv3cpKn00Zw@mail.gmail.com>
 <e0f4395b-e1cf-abfa-e4d4-ee3dd519887e@bath.edu>
 <CANg3_k9-OKAK1VSirWSVGXQVMWoDeKoG7gN7mUfGfZdX4CnpgQ@mail.gmail.com>
 <CANg3_k9Nkok-Mr4DFOZ3=d1XtA-Pq1seW+E+KtWLttQUe_+XcQ@mail.gmail.com>
 <CANg3_k8C5W1RKL-gsGbr3556hSHq7+-oCSRCZR-ODEsN_7_MAA@mail.gmail.com>
Message-ID: <59ce25f3-7168-2a30-f2fe-8710ed309731@bath.edu>

I doubt you want a 7 by 1000 grid for your persp plot. Here's an example 
of producing a persp plot using predict.gam and a custom grid. Since 
only the effects of x1 and x2 are being plotted it doesn't matter what 
x0 and x3 are set to (the model is additive after all). In your case 
only one smooth term is involved of course.

library(mgcv)
n <- 200
sig <- 2
dat <- gamSim(1,n=n,scale=sig)

b <- gam(y~s(x0)+s(I(x1^2))+s(x2)+offset(x3),data=dat)

m1 <- 20;m2 <- 30; n <- m1*m2
x1 <- seq(.2,.8,length=m1);x2 <- seq(.2,.8,length=m2) ## marginal values 
for evaluation grid
df <- data.frame(x0=rep(.5,n),x1=rep(x1,m2),x2=rep(x2,each=m1),x3=rep(0,n))
pf <- predict(b,newdata=df,type="terms")

persp(x1,x2,matrix(pf[,2]+pf[,3],m1,m2),theta=-130,col="blue",zlab="")

On 23/07/2022 14:54, jade.shodan at googlemail.com wrote:
> Hi Simon and all,
>
> I've corrected some mistakes in setting up the prediction data frame
> (sorry, very stressed and sleep deprived due to closing in deadlines),
> but am having problems getting the perspective plot using persp().
>
> I've calculated relative risk (RR) but am having trouble getting the
> perspective (or contour) plot. persp() takes x and y vectors in
> ascending order, and z as a matrix.  I've seen the outer() function
> being used to facilitate perspective plots, but every example I've
> seen treats z as a function, to be evaluated at combinations for x and
> y. In my case, I already have observations for z. Not sure if I need
> to use outer(), and if so, how?
>
> My code for making predictions, getting RR and trying to get the plot
> is as follows (full reproducible example including model at the
> bottom). Can someone tell me what I'm doing wrong?
>
> ########### CODE ########################
>
> # create prediction data
> N <- 1000 # number of points for which to predict the smooths
> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T),
> length = N)  # prediction data for temperature
> pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
> pred_rain <- seq(min(dat$rain, na.rm = T), max(dat$rain, na.rm = T), length = N)
> pred_humidity <- seq(min(dat$humidity, na.rm = T), max(dat$humidity,
> na.rm = T), length = N)
> pred_time <- seq(min(dat$time, na.rm = T), max(dat$time, na.rm = T), length = N)
>
> pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> pred_humidity, rain = pred_rain, time = pred_time)
>
> # create prediction data with reference temperature set to median temperature
> # identical to pd but now with all temperatures set to median value of
> temperature
> pred_temp0 <- rep(median(dat$temp, na.rm=T), N)
> pd0 <- data.frame(temp = pred_temp0, lag = pred_lag, humidity =
> pred_humidity, rain = pred_rain, time = pred_time)
>
> # make predictions
> predictions <- predict.gam(mod, pd, type = "terms")
> predictions0 <- predict.gam(mod, pd0, type = "terms")
>
> # calculate RR
> diff <- predictions[,2] - predictions0[,2]
> rr <- as.vector(exp(diff))
>
> # convert rr to matrix required for z argument for persp()
> rr_mat <- matrix(rr, nrow = N)
> persp(pred_temp, pred_lag, rr_mat)
>
> ######### ERROR MESSAGE ##################################
>
> The persp call results in the follow error:
>
> Error in persp.default(pred_temp, pred_lag, rr_mat) :
>    increasing 'x' and 'y' values expected
>
> I don't understand this because pred_temp and pred_lag ARE in ascending order.
>
> ############################################################
> FULL REPRODUCIBLE EXAMPLE
> ############################################################
>
> library(mgcv)
> set.seed(3) # make reproducible example
> simdat <- gamSim(1,400)
> g <- exp(simdat$f/5)
>
> simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
> simdat$time <- 1:400  # create time series
>
> names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
> "f0", "f1", "f2", "f3", "time")
>
> # lag function based on Wood (book 2017, p.349 and gamair package
> documentation p.54
> # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
> lagard <- function(x,n.lag=7) {
> n <- length(x); X <- matrix(NA,n,n.lag)
> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> X
> }
>
> # set up lag, temp, rain and humidity as 7-column matrices
> # to create lagged variables
> dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
> deaths=simdat$deaths, time = simdat$time)
> dat$temp <- lagard(simdat$temp)
> dat$rain <- lagard(simdat$rain)
> dat$humidity <- lagard(simdat$humidity)
>
> mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
> te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
> family = nb, method = 'REML', select = TRUE)
>
> summary(mod)
> plot(mod, scheme = 1)  # perspective
> plot(mod, scheme = 2)  # contour
>
> # create prediction data
> N <- 1000 # number of points for which to predict the smooths
> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
> pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
> pred_rain <- seq(min(dat$rain, na.rm = T), max(dat$rain, na.rm = T), length = N)
> pred_humidity <- seq(min(dat$humidity, na.rm = T), max(dat$humidity,
> na.rm = T), length = N)
> pred_time <- seq(min(dat$time, na.rm = T), max(dat$time, na.rm = T), length = N)
>
> pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> pred_humidity, rain = pred_rain, time = pred_time)
>
> # create prediction data with reference temperature set to median temperature
> # identical to pd but now with all temperatures set to median value of
> temperature
> pred_temp0 <- rep(median(dat$temp, na.rm=T), N)
> pd0 <- data.frame(temp = pred_temp0, lag = pred_lag, humidity =
> pred_humidity, rain = pred_rain, time = pred_time)
>
> # make predictions
> predictions <- predict.gam(mod, pd, type = "terms")
> predictions0 <- predict.gam(mod, pd0, type = "terms")
>
> # calculate RR
> diff <- predictions[,2] - predictions0[,2]
> rr <- as.vector(exp(diff))
>
> # convert rr to matrix required for z argument for persp()
> rr_mat <- matrix(rr, nrow = N)
> persp(pred_temp, pred_lag, rr_mat)
>
>
>
>
> On Fri, 22 Jul 2022 at 13:29, jade.shodan at googlemail.com
> <jade.shodan at googlemail.com> wrote:
>> I made a small error in the code below (not checking for NAs which are
>> introduced by the lag function). However, this doesn't solve the issue
>> I raised).
>>
>> So here's the problem (with corrected code) again:
>>
>> I'm not sure what to do with the 'time' variable. (I don't want to
>> make predictions for specific points in time). I coded as follows
>> (full reproducible example at bottom of email), but get a warning and
>> error:
>>
>>
>> N <- 1000     # number of points for smooth to be predicted
>> # new temperatures and lags for prediction
>> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
>> pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)      ## IS IT CORRECT TO SET
>> UP LAG LIKE THIS?
>>
>> # not sure if these covariates are required with type = "terms"
>> pred_humidity <- rep(median(dat$humidity, na.rm = T), N)
>> pred_rain <- rep(median(dat$rain, na.rm = T), N)
>> pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
>> pred_humidity, rain = pred_rain)
>>
>> predictions <- predict(mod, pd, type = "terms")
>>
>>
>> The predict line creates the following warning and error:
>>
>> Warning in predict.gam(mod, pd, type = "terms") :
>>    not all required variables have been supplied in  newdata!
>>
>> Error in model.frame.default(ff, data = newdata, na.action = na.act) :
>>    object is not a matrix
>>
>>
>> For ease of reference, I've (re)included the full reproducible example:
>>
>> library(mgcv)
>> set.seed(3) # make reproducible example
>> simdat <- gamSim(1,400)
>> g <- exp(simdat$f/5)
>>
>> simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
>> simdat$time <- 1:400  # create time series
>>
>> names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
>> "f0", "f1", "f2", "f3", "time")
>>
>> # lag function based on Wood (book 2017, p.349 and gamair package
>> documentation p.54
>> # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
>> lagard <- function(x,n.lag=7) {
>> n <- length(x); X <- matrix(NA,n,n.lag)
>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
>> X
>> }
>>
>> # set up lag, temp, rain and humidity as 7-column matrices
>> # to create lagged variables
>> dat <- list(lag=matrix(0:6,nrow(simda
>> t),7,byrow=TRUE),
>> deaths=simdat$deaths, time = simdat$time)
>> dat$temp <- lagard(simdat$temp)
>> dat$rain <- lagard(simdat$rain)
>> dat$humidity <- lagard(simdat$humidity)
>>
>> mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
>> te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
>> family = nb, method = 'REML', select = TRUE)
>>
>> # create prediction data
>> N <- 1000 # number of points for which to predict the smooths
>> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
>> pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
>> pred_humidity <- rep(median(dat$humidity), N)
>> pred_rain <- rep(median(dat$rain), N)
>> pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
>> pred_humidity, rain = pred_rain)
>>
>> # make predictions
>> predictions <- predict(mod, pd, type = "terms")
>>
>> On Fri, 22 Jul 2022 at 12:47, jade.shodan at googlemail.com
>> <jade.shodan at googlemail.com> wrote:
>>> Hi Simon,
>>>
>>> Thanks for the pointers! But I'm not sure what to do with the 'time'
>>> variable. (I don't want to make predictions for specific points in
>>> time). I coded as follows (full reproducible example at bottom of
>>> email), but get a warning and error:
>>>
>>>
>>> N <- 1000     # number of points for smooth to be predicted
>>> # new temperatures and lags for prediction
>>> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
>>> pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)      ## IS IT CORRECT TO SET
>>> UP LAG LIKE THIS?
>>>
>>> # not sure if these covariates are required with type = "terms"
>>> pred_humidity <- rep(median(dat$humidity), N)
>>> pred_rain <- rep(median(dat$rain), N)
>>> pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
>>> pred_humidity, rain = pred_rain)
>>>
>>> predictions <- predict(mod, pd, type = "terms")
>>>
>>>
>>> The predict line creates the following warning and error:
>>>
>>> Warning in predict.gam(mod, pd, type = "terms") :
>>>    not all required variables have been supplied in  newdata!
>>>
>>> Error in model.frame.default(ff, data = newdata, na.action = na.act) :
>>>    object is not a matrix
>>>
>>>
>>> For ease of reference, I've (re)included the full reproducible example:
>>>
>>> library(mgcv)
>>> set.seed(3) # make reproducible example
>>> simdat <- gamSim(1,400)
>>> g <- exp(simdat$f/5)
>>>
>>> simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
>>> simdat$time <- 1:400  # create time series
>>>
>>> names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
>>> "f0", "f1", "f2", "f3", "time")
>>>
>>> # lag function based on Wood (book 2017, p.349 and gamair package
>>> documentation p.54
>>> # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
>>> lagard <- function(x,n.lag=7) {
>>> n <- length(x); X <- matrix(NA,n,n.lag)
>>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
>>> X
>>> }
>>>
>>> # set up lag, temp, rain and humidity as 7-column matrices
>>> # to create lagged variables
>>> dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
>>> deaths=simdat$deaths, time = simdat$time)
>>> dat$temp <- lagard(simdat$temp)
>>> dat$rain <- lagard(simdat$rain)
>>> dat$humidity <- lagard(simdat$humidity)
>>>
>>> mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
>>> te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
>>> family = nb, method = 'REML', select = TRUE)
>>>
>>> # create prediction data
>>> N <- 1000 # number of points for which to predict the smooths
>>> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
>>> pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
>>> pred_humidity <- rep(median(dat$humidity), N)
>>> pred_rain <- rep(median(dat$rain), N)
>>> pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
>>> pred_humidity, rain = pred_rain)
>>>
>>> # make predictions
>>> predictions <- predict(mod, pd, type = "terms")
>>>
>>>
>>> On Fri, 22 Jul 2022 at 09:54, Simon Wood <simon.wood at bath.edu> wrote:
>>>>
>>>> On 21/07/2022 15:19, jade.shodan--- via R-help wrote:
>>>>> Hello everyone (incl. Simon Wood?),
>>>>>
>>>>> I'm not sure that my original question (see below, including
>>>>> reproducible example) was as clear as it could have been. To clarify,
>>>>> what I would to like to get is:
>>>>>
>>>>> 1) a perspective plot of temperature x lag x relative risk.  I know
>>>>> how to use plot.gam and vis.gam but don't know how to get plots on the
>>>>> relative risk scale as opposed to  "response" or "link".
>>>> - You are on the log scale so I think that all you need to do is to use
>>>> 'predict.gam', with 'type = "terms"' to  get the predictions for the
>>>> te(temp, lag) term over the required grid of lags and temperatures.
>>>> Suppose the dataframe of prediction data is 'pd'. Now produce pd0, which
>>>> is identical to pd, except that the temperatures are all set to the
>>>> reference temperature. Use predict.gam to predict te(temp,lag) from pd0.
>>>> Now the exponential of the difference between the first and second
>>>> predictions is the required RR, which you can plot using 'persp',
>>>> 'contour', 'image' or whatever. If you need credible intervals see pages
>>>> 341-343 of my 'GAMs: An intro with R' book (2nd ed).
>>>>
>>>>> 2) a plot of relative risk (accumulated across all lags) vs
>>>>> temperature, given a reference temperature. An example of such a plot
>>>>> can be found in figure 2 (bottom) of this paper by Gasparrini et al:
>>>>> https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3940
>>>> - I guess this only makes sense if you have the same temperature at all
>>>> lags. So this time produce a data.frame with each desired temperature
>>>> repeated for each lag: 'pd1'. Again use predict.gam(...,type="terms").
>>>> Then sum the predictions over lags for each temperature, subtract the
>>>> minimum, and take the exponential. Same as above for CIs.
>>>>
>>>> best,
>>>>
>>>> Simon
>>>>
>>>>> I've seen Simon Wood's response to a related issue here:
>>>>> https://stat.ethz.ch/pipermail/r-help/2012-May/314387.html
>>>>> However, I'm not sure how to apply this to time series data with
>>>>> distributed lag, to get the above mentioned figures.
>>>>>
>>>>> Would be really grateful for suggestions!
>>>>>
>>>>> Jade
>>>>>
>>>>> On Tue, 19 Jul 2022 at 16:07, jade.shodan at googlemail.com
>>>>> <jade.shodan at googlemail.com> wrote:
>>>>>> Dear list members,
>>>>>>
>>>>>> Does anyone know how to obtain a relative risk/ risk ratio from a GAM
>>>>>> with a distributed lag model implemented in mgcv? I have a GAM
>>>>>> predicting daily deaths from time series data consisting of daily
>>>>>> temperature, humidity and rainfall. The GAM includes a distributed lag
>>>>>> model because deaths may occur over several days following a high heat
>>>>>> day.
>>>>>>
>>>>>> What I'd like to do is compute (and plot) the relative risk
>>>>>> (accumulated across all lags) for a given temperature vs the
>>>>>> temperature at which the risk is lowest, with corresponding confidence
>>>>>> intervals. I am aware of the predict.gam function but am not sure if
>>>>>> and how it should be used in this case. (Additionally, I'd also like
>>>>>> to plot the relative risk for different lags separately).
>>>>>>
>>>>>> I apologise if this seems trivial to some. (Actually, I hope it is,
>>>>>> because that might mean I get a solution!) I've been looking for
>>>>>> examples on how to do this, but found nothing so far. Suggestions
>>>>>> would be very much appreciated!
>>>>>>
>>>>>> Below is a reproducible example with the GAM:
>>>>>>
>>>>>> library(mgcv)
>>>>>> set.seed(3) # make reproducible example
>>>>>> simdat <- gamSim(1,400) # simulate data
>>>>>> g <- exp(simdat$f/5)
>>>>>> simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
>>>>>> simdat$time <- 1:400  # create time series
>>>>>> names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
>>>>>> "f0", "f1", "f2", "f3", "time")
>>>>>>
>>>>>> # lag function based on Simon Wood (book 2017, p.349 and gamair
>>>>>> package documentation p.54
>>>>>> # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
>>>>>> lagard <- function(x,n.lag=7) {
>>>>>> n <- length(x); X <- matrix(NA,n,n.lag)
>>>>>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
>>>>>> X
>>>>>> }
>>>>>>
>>>>>> # set up lag, temp, rain and humidity as 7-column matrices
>>>>>> # to create lagged variables - based on Simon Wood's example
>>>>>> dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
>>>>>> deaths=simdat$deaths, time = simdat$time)
>>>>>> dat$temp <- lagard(simdat$temp)
>>>>>> dat$rain <- lagard(simdat$rain)
>>>>>> dat$humidity <- lagard(simdat$humidity)
>>>>>>
>>>>>> mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
>>>>>> te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
>>>>>> family = nb, method = 'REML', select = TRUE)
>>>>>>
>>>>>> summary(mod)
>>>>>> plot(mod, scheme = 1)
>>>>>> plot(mod, scheme = 2)
>>>>>>
>>>>>> Thanks for any suggestions you may have,
>>>>>>
>>>>>> Jade
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> --
>>>> Simon Wood, School of Mathematics, University of Edinburgh,
>>>> https://www.maths.ed.ac.uk/~swood34/
>>>>
-- 
Simon Wood, School of Mathematics, University of Edinburgh,
https://www.maths.ed.ac.uk/~swood34/


From @@ron @end|ng |rom gene@|@rg@com  Thu Jul 21 21:25:56 2022
From: @@ron @end|ng |rom gene@|@rg@com (Aaron Crowley)
Date: Thu, 21 Jul 2022 15:25:56 -0400
Subject: [R] Error generated by nlme::gnls
Message-ID: <CAMs=EiOaUerb4n0nV0H9SKK+ChiyNDf7WkvVZ_oO7Mue1Cx4eQ@mail.gmail.com>

Approximately 6 months ago, I successfully performed a model-based
network meta-analysis using the gnls function from the nlme package in
R (nlme::gnls). In this analysis, a binary response is captured as a
non-linear non-parametric placebo response model combined with drug
effect as a function of dose and time and is fitted on aggregate data
from 57 trials and 13 treatments.

I am now trying to rerun the analysis but the code no longer runs. An
error message is generated when trying to fit the model using
nlme::gnls. A reproducible example generating the same error message
is below.

library(nlme)

set.seed(548)

df <- Soybean
df$x01 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
df$x02 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
df$x03 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
df$x04 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
df$x05 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
df$x06 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
df$x07 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
df$x08 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
df$x09 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
df$x10 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
df$x11 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
df$x12 <- sample(c(0, 1), size = nrow(df), replace = TRUE)

gnls(
  weight ~ x,
  data = df,
  params = (x ~ -1 + x01 + x02 + x03 + x04 + x05 + x06 + x07 + x08 +
x09 + x10 + x11 + x12),
  start = rep(0, 13)
)
# Error in if (deparse(params[[nm]][[3]]) != "1") { :
#     the condition has length > 1

sessionInfo()
# R version 4.2.1 (2022-06-23 ucrt)
# Platform: x86_64-w64-mingw32/x64 (64-bit)
# Running under: Windows 10 x64 (build 22000)
#
# Matrix products: default
#
# locale:
#   [1] LC_COLLATE=English_United States.utf8  LC_CTYPE=English_United
States.utf8
# [3] LC_MONETARY=English_United States.utf8 LC_NUMERIC=C
# [5] LC_TIME=English_United States.utf8
#
# attached base packages:
#   [1] stats     graphics  grDevices utils     datasets  methods   base
#
# other attached packages:
#   [1] nlme_3.1-157
#
# loaded via a namespace (and not attached):
#   [1] compiler_4.2.1  tools_4.2.1     grid_4.2.1      lattice_0.20-45

Aaron Crowley
Principal Scientist, Biostatistics

e.  aaron at genesisrg.com
w. genesisrg.com


From btupper @end|ng |rom b|ge|ow@org  Sun Jul 24 03:00:25 2022
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Sat, 23 Jul 2022 21:00:25 -0400
Subject: [R] Error generated by nlme::gnls
In-Reply-To: <CAMs=EiOaUerb4n0nV0H9SKK+ChiyNDf7WkvVZ_oO7Mue1Cx4eQ@mail.gmail.com>
References: <CAMs=EiOaUerb4n0nV0H9SKK+ChiyNDf7WkvVZ_oO7Mue1Cx4eQ@mail.gmail.com>
Message-ID: <CALrbzg1BKLoqr9jRpggtT+cZuL+AYt2j-NN6QpHV2J9SUS+VxA@mail.gmail.com>

Could this be related to a new if() behavior introduced in v4.2.0 ?
See the "SIGNIFICANT USER-VISIBLE CHANGES" for v4.2.0 in the NEWS

https://cloud.r-project.org/doc/manuals/r-release/NEWS.html

On Sat, Jul 23, 2022 at 6:26 PM Aaron Crowley <aaron at genesisrg.com> wrote:
>
> Approximately 6 months ago, I successfully performed a model-based
> network meta-analysis using the gnls function from the nlme package in
> R (nlme::gnls). In this analysis, a binary response is captured as a
> non-linear non-parametric placebo response model combined with drug
> effect as a function of dose and time and is fitted on aggregate data
> from 57 trials and 13 treatments.
>
> I am now trying to rerun the analysis but the code no longer runs. An
> error message is generated when trying to fit the model using
> nlme::gnls. A reproducible example generating the same error message
> is below.
>
> library(nlme)
>
> set.seed(548)
>
> df <- Soybean
> df$x01 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
> df$x02 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
> df$x03 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
> df$x04 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
> df$x05 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
> df$x06 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
> df$x07 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
> df$x08 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
> df$x09 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
> df$x10 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
> df$x11 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
> df$x12 <- sample(c(0, 1), size = nrow(df), replace = TRUE)
>
> gnls(
>   weight ~ x,
>   data = df,
>   params = (x ~ -1 + x01 + x02 + x03 + x04 + x05 + x06 + x07 + x08 +
> x09 + x10 + x11 + x12),
>   start = rep(0, 13)
> )
> # Error in if (deparse(params[[nm]][[3]]) != "1") { :
> #     the condition has length > 1
>
> sessionInfo()
> # R version 4.2.1 (2022-06-23 ucrt)
> # Platform: x86_64-w64-mingw32/x64 (64-bit)
> # Running under: Windows 10 x64 (build 22000)
> #
> # Matrix products: default
> #
> # locale:
> #   [1] LC_COLLATE=English_United States.utf8  LC_CTYPE=English_United
> States.utf8
> # [3] LC_MONETARY=English_United States.utf8 LC_NUMERIC=C
> # [5] LC_TIME=English_United States.utf8
> #
> # attached base packages:
> #   [1] stats     graphics  grDevices utils     datasets  methods   base
> #
> # other attached packages:
> #   [1] nlme_3.1-157
> #
> # loaded via a namespace (and not attached):
> #   [1] compiler_4.2.1  tools_4.2.1     grid_4.2.1      lattice_0.20-45
>
> Aaron Crowley
> Principal Scientist, Biostatistics
>
> e.  aaron at genesisrg.com
> w. genesisrg.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Ben Tupper (he/him)
Bigelow Laboratory for Ocean Science
East Boothbay, Maine
http://www.bigelow.org/
https://eco.bigelow.org


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sun Jul 24 06:03:24 2022
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 24 Jul 2022 16:03:24 +1200
Subject: [R] Error generated by nlme::gnls
In-Reply-To: <CALrbzg1BKLoqr9jRpggtT+cZuL+AYt2j-NN6QpHV2J9SUS+VxA@mail.gmail.com>
References: <CAMs=EiOaUerb4n0nV0H9SKK+ChiyNDf7WkvVZ_oO7Mue1Cx4eQ@mail.gmail.com>
 <CALrbzg1BKLoqr9jRpggtT+cZuL+AYt2j-NN6QpHV2J9SUS+VxA@mail.gmail.com>
Message-ID: <20220724160324.018e9531@rolf-Latitude-E7470>


On Sat, 23 Jul 2022 21:00:25 -0400
Ben Tupper <btupper at bigelow.org> wrote:

> Could this be related to a new if() behavior introduced in v4.2.0 ?
> See the "SIGNIFICANT USER-VISIBLE CHANGES" for v4.2.0 in the NEWS
> 
> https://cloud.r-project.org/doc/manuals/r-release/NEWS.html

No.  What's going on is much weirder than that, and looks to me like
a bug has been introduced in formula() or in something related.

My impression is that if the right hand side of a formula gets "too
long", then it gets split into parts --- which messes everything up.

Consider:

lhs1 <- paste(paste0("x",1:12),collapse=" + ")
f1   <- as.formula(paste("x ~ -1 +",lhs1))
print(length(deparse(f1[[3]])))

You get the value 2.  Note that deparse(f1[[3]]) gives

[1] "-1 + x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + "
[2] "    x12"       

If we shorten the right hand side of the formula just a wee bit

lhs2 <- paste(paste0("x",1:11),collapse=" + ")
f2   <- as.formula(paste("x ~ -1 +",lhs2))

then deparse(f2[[3]]) is of length 1, as it should be:

> > deparse(f2[[3]])
> [1] "-1 + x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11"

I could of course be missing something, but it really looks to me as if
something has gone up to Puttee here.

Some input from someone in R-Core would be valuable here.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From kry|ov@r00t @end|ng |rom gm@||@com  Sun Jul 24 06:58:01 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Sun, 24 Jul 2022 07:58:01 +0300
Subject: [R] Error generated by nlme::gnls
In-Reply-To: <20220724160324.018e9531@rolf-Latitude-E7470>
References: <CAMs=EiOaUerb4n0nV0H9SKK+ChiyNDf7WkvVZ_oO7Mue1Cx4eQ@mail.gmail.com>
 <CALrbzg1BKLoqr9jRpggtT+cZuL+AYt2j-NN6QpHV2J9SUS+VxA@mail.gmail.com>
 <20220724160324.018e9531@rolf-Latitude-E7470>
Message-ID: <20220724075801.6414c486@trisector>

On Sun, 24 Jul 2022 16:03:24 +1200
Rolf Turner <r.turner at auckland.ac.nz> wrote:

> My impression is that if the right hand side of a formula gets "too
> long", then it gets split into parts --- which messes everything up.

For new enough R (? 4.0), it's possible to use deparse1() [*], which
guarantees to return a single string. Otherwise, the simplest
workaround would be paste(deparse(...), collapse = ' '), which is
similar to how deparse1 is defined.

-- 
Best regards,
Ivan

[*] https://bugs.r-project.org/show_bug.cgi?id=17671#c1


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sun Jul 24 13:03:02 2022
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 24 Jul 2022 23:03:02 +1200
Subject: [R] Error generated by nlme::gnls
In-Reply-To: <20220724075801.6414c486@trisector>
References: <CAMs=EiOaUerb4n0nV0H9SKK+ChiyNDf7WkvVZ_oO7Mue1Cx4eQ@mail.gmail.com>
 <CALrbzg1BKLoqr9jRpggtT+cZuL+AYt2j-NN6QpHV2J9SUS+VxA@mail.gmail.com>
 <20220724160324.018e9531@rolf-Latitude-E7470>
 <20220724075801.6414c486@trisector>
Message-ID: <20220724230302.4784212d@rolf-Latitude-E7470>

On Sun, 24 Jul 2022 07:58:01 +0300
Ivan Krylov <krylov.r00t at gmail.com> wrote:

> On Sun, 24 Jul 2022 16:03:24 +1200
> Rolf Turner <r.turner at auckland.ac.nz> wrote:
> 
> > My impression is that if the right hand side of a formula gets "too
> > long", then it gets split into parts --- which messes everything up.
> 
> For new enough R (? 4.0), it's possible to use deparse1() [*], which
> guarantees to return a single string. Otherwise, the simplest
> workaround would be paste(deparse(...), collapse = ' '), which is
> similar to how deparse1 is defined.
> 

Yes but .....

I guess my posting did not make things clear.  Sorry 'bout that! My
posting, and Ben Tupper's posting, were in response to an earlier
posting by Aaron Crowley which dealt with an error that now results
from a call to gnls(). Apparently the exact same call had worked
without error in the past.

The call to deparse(), which triggers the error, is in the code
of gnls().  (Line 223 of gnls.R.)

The maintainer of the nlme package (who is, according to maintainer(),
"R-core") could change the code so that it uses invokes deparse1()
rather than deparse, but the user cannot do so, not without in effect
re-creating the package.

Also, the question remains:  why did Aaron Crowley's code work in the
past, whereas now it throws an error?  What changed?

cheers,

Rolf Turner

P.S. Aaron Crowley tells us that he is currently running R 4.2.1.
However he did not tell us what version of R he was running at the time
("approximately six months ago") when the call to gnls() executed
successfully.  Perhaps that version was older than 4.0.  Even so, it's
all a bit mysterious.

R. T.

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From kry|ov@r00t @end|ng |rom gm@||@com  Sun Jul 24 14:57:25 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Sun, 24 Jul 2022 15:57:25 +0300
Subject: [R] Error generated by nlme::gnls
In-Reply-To: <20220724230302.4784212d@rolf-Latitude-E7470>
References: <CAMs=EiOaUerb4n0nV0H9SKK+ChiyNDf7WkvVZ_oO7Mue1Cx4eQ@mail.gmail.com>
 <CALrbzg1BKLoqr9jRpggtT+cZuL+AYt2j-NN6QpHV2J9SUS+VxA@mail.gmail.com>
 <20220724160324.018e9531@rolf-Latitude-E7470>
 <20220724075801.6414c486@trisector>
 <20220724230302.4784212d@rolf-Latitude-E7470>
Message-ID: <20220724155725.24076488@trisector>

Sorry for being too terse in my previous e-mail!

On Sun, 24 Jul 2022 23:03:02 +1200
Rolf Turner <r.turner at auckland.ac.nz> wrote:

> The maintainer of the nlme package (who is, according to maintainer(),
> "R-core") could change the code so that it uses invokes deparse1()
> rather than deparse, but the user cannot do so, not without in effect
> re-creating the package.

You're right. I think there's a buglet in nlme::gnls that nobody
noticed until R 4.2.0 was released *and* Aaron Crowley used the
function with a sufficiently long formula.

> Also, the question remains:  why did Aaron Crowley's code work in the
> past, whereas now it throws an error?  What changed?

gnls() may have been performing the `if (deparse(...) != '1')` test for
a long time, but never crashed before because it wasn't a fatal error
until R 4.2.0. Previously, if() would issue a warning and use the first
element of the boolean vector.

R 4.2.0 was released this April, which was less than 6 months ago. I
think it all fits.

A temporary solution would be to make use of the fact that R is a very
dynamic language and perform surgery on a live function inside a loaded
package:

library(codetools)

nlme <- loadNamespace('nlme')
unlockBinding('gnls', nlme)
nlme$gnls <- `body<-`(fun = nlme$gnls, value = walkCode(
	body(nlme$gnls), makeCodeWalker(
		call = function(e, w)
			as.call(lapply(as.list(e), function(ee)
				if (!missing(ee)) walkCode(ee, w)
			)),
		leaf = function(e, w)
			if (is.symbol(e) && e == 'deparse') {
				as.name('deparse1')
			} else e
	)
))
lockBinding('gnls', nlme)
rm(nlme)

grep('deparse', deparse(nlme::gnls), value = TRUE)
# [1] "                deparse1(pp[[3]]), sep = \"~\"), collapse =
\",\"), " # [2] "        if (deparse1(params[[nm]][[3]]) != \"1\") {"
# [3] "        list(row.names(dataModShrunk), deparse1(form[[2]]))), "

Aaron's example seems to work after this, modulo needing 12 starting
values instead of 13.

-- 
Best regards,
Ivan


From w||||@mwdun|@p @end|ng |rom gm@||@com  Sun Jul 24 17:51:09 2022
From: w||||@mwdun|@p @end|ng |rom gm@||@com (Bill Dunlap)
Date: Sun, 24 Jul 2022 08:51:09 -0700
Subject: [R] Error generated by nlme::gnls
In-Reply-To: <20220724155725.24076488@trisector>
References: <CAMs=EiOaUerb4n0nV0H9SKK+ChiyNDf7WkvVZ_oO7Mue1Cx4eQ@mail.gmail.com>
 <CALrbzg1BKLoqr9jRpggtT+cZuL+AYt2j-NN6QpHV2J9SUS+VxA@mail.gmail.com>
 <20220724160324.018e9531@rolf-Latitude-E7470>
 <20220724075801.6414c486@trisector>
 <20220724230302.4784212d@rolf-Latitude-E7470>
 <20220724155725.24076488@trisector>
Message-ID: <CAHqSRuSKFbxvNQ=YUtZj--K=tHGht_iAfM-Fh=dLTA3xx2UOWw@mail.gmail.com>

I think the intent of this code was to see if the formula had solely a
literal 1 on the right hand side.
Then !identical(pp[[3]], 1) would do it, avoiding the overhead of calling
deparse.  Note that the
1 might be an integer, which the current code would (erroneously) not
catch, so
   !identical(pp[[3]], 1) && !identical(pp[[3]], 1L)
or something equivalent would be better.

-Bill

On Sun, Jul 24, 2022 at 5:58 AM Ivan Krylov <krylov.r00t at gmail.com> wrote:

> Sorry for being too terse in my previous e-mail!
>
> On Sun, 24 Jul 2022 23:03:02 +1200
> Rolf Turner <r.turner at auckland.ac.nz> wrote:
>
> > The maintainer of the nlme package (who is, according to maintainer(),
> > "R-core") could change the code so that it uses invokes deparse1()
> > rather than deparse, but the user cannot do so, not without in effect
> > re-creating the package.
>
> You're right. I think there's a buglet in nlme::gnls that nobody
> noticed until R 4.2.0 was released *and* Aaron Crowley used the
> function with a sufficiently long formula.
>
> > Also, the question remains:  why did Aaron Crowley's code work in the
> > past, whereas now it throws an error?  What changed?
>
> gnls() may have been performing the `if (deparse(...) != '1')` test for
> a long time, but never crashed before because it wasn't a fatal error
> until R 4.2.0. Previously, if() would issue a warning and use the first
> element of the boolean vector.
>
> R 4.2.0 was released this April, which was less than 6 months ago. I
> think it all fits.
>
> A temporary solution would be to make use of the fact that R is a very
> dynamic language and perform surgery on a live function inside a loaded
> package:
>
> library(codetools)
>
> nlme <- loadNamespace('nlme')
> unlockBinding('gnls', nlme)
> nlme$gnls <- `body<-`(fun = nlme$gnls, value = walkCode(
>         body(nlme$gnls), makeCodeWalker(
>                 call = function(e, w)
>                         as.call(lapply(as.list(e), function(ee)
>                                 if (!missing(ee)) walkCode(ee, w)
>                         )),
>                 leaf = function(e, w)
>                         if (is.symbol(e) && e == 'deparse') {
>                                 as.name('deparse1')
>                         } else e
>         )
> ))
> lockBinding('gnls', nlme)
> rm(nlme)
>
> grep('deparse', deparse(nlme::gnls), value = TRUE)
> # [1] "                deparse1(pp[[3]]), sep = \"~\"), collapse =
> \",\"), " # [2] "        if (deparse1(params[[nm]][[3]]) != \"1\") {"
> # [3] "        list(row.names(dataModShrunk), deparse1(form[[2]]))), "
>
> Aaron's example seems to work after this, modulo needing 12 starting
> values instead of 13.
>
> --
> Best regards,
> Ivan
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From Freder|ekM@@rten@Kerckho| @end|ng |rom UGent@be  Thu Jul 21 12:13:48 2022
From: Freder|ekM@@rten@Kerckho| @end|ng |rom UGent@be (Frederiek-Maarten Kerckhof)
Date: Thu, 21 Jul 2022 10:13:48 +0000
Subject: [R] Will R4.2.1 be available for Ubuntu 18.04.6 LTS?
In-Reply-To: <DB8PR09MB421781A9A89FDEB3BD73CAA080919@DB8PR09MB4217.eurprd09.prod.outlook.com>
References: <DB8PR09MB421781A9A89FDEB3BD73CAA080919@DB8PR09MB4217.eurprd09.prod.outlook.com>
Message-ID: <DB8PR09MB42179713CBC6FA855864EE9B80919@DB8PR09MB4217.eurprd09.prod.outlook.com>

Dear,

I have been trying to update our existing R installation on Ubuntu 18.04.6 LTS (amd64) from R 4.2.0 to R 4.2.1. From our CRAN mirror (https://ftp.belnet.be/mirror/CRAN/bin/linux/ubuntu/) I can see that only the latest LTS release remains fully supported (that would be 22.04 at the time of writing). Nevertheless, for focal (20.04 LTS) at https://ftp.belnet.be/mirror/CRAN/bin/linux/ubuntu/focal-cran40/ r-base and r-base-dev are available in 4.2.1 but not for bionic (18.04 LTS) https://ftp.belnet.be/mirror/CRAN/bin/linux/ubuntu/bionic-cran40/. ?

I was wondering if the deb will become available for 18.04? 
I have been able, on another 18.04 system, to install R 4.2.1 using an Rstudio provided deb with gdebi as described on ?https://docs.rstudio.com/resources/install-r/ , but that takes R out of the regular package management with apt. 

Kind regards,

Frederiek-Maarten Kerckhof, Ph.D. - visiting scientist
Center for Microbial Ecology and Technology - Campus Coupure - Ghent University
Coupure Links 653, building A
http://www.cmet.ugent.be/?-?http://www.kytos.be - http://helpdesk.ugent.be/e-maildisclaimer.php


From r@turner @end|ng |rom @uck|@nd@@c@nz  Mon Jul 25 01:40:41 2022
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Mon, 25 Jul 2022 11:40:41 +1200
Subject: [R] Error generated by nlme::gnls
In-Reply-To: <20220724155725.24076488@trisector>
References: <CAMs=EiOaUerb4n0nV0H9SKK+ChiyNDf7WkvVZ_oO7Mue1Cx4eQ@mail.gmail.com>
 <CALrbzg1BKLoqr9jRpggtT+cZuL+AYt2j-NN6QpHV2J9SUS+VxA@mail.gmail.com>
 <20220724160324.018e9531@rolf-Latitude-E7470>
 <20220724075801.6414c486@trisector>
 <20220724230302.4784212d@rolf-Latitude-E7470>
 <20220724155725.24076488@trisector>
Message-ID: <20220725114041.06ec66f0@rolf-Latitude-E7470>


On Sun, 24 Jul 2022 15:57:25 +0300
Ivan Krylov <krylov.r00t at gmail.com> wrote:

> Sorry for being too terse in my previous e-mail!
> 
> On Sun, 24 Jul 2022 23:03:02 +1200
> Rolf Turner <r.turner at auckland.ac.nz> wrote:
> 
> > The maintainer of the nlme package (who is, according to
> > maintainer(), "R-core") could change the code so that it uses
> > invokes deparse1() rather than deparse, but the user cannot do so,
> > not without in effect re-creating the package.
> 
> You're right. I think there's a buglet in nlme::gnls that nobody
> noticed until R 4.2.0 was released *and* Aaron Crowley used the
> function with a sufficiently long formula.

Makes sense. :-)

> > Also, the question remains:  why did Aaron Crowley's code work in
> > the past, whereas now it throws an error?  What changed?
> 
> gnls() may have been performing the `if (deparse(...) != '1')` test
> for a long time, but never crashed before because it wasn't a fatal
> error until R 4.2.0. Previously, if() would issue a warning and use
> the first element of the boolean vector.

N'ya-ha!  Makes sense too.
> 
> R 4.2.0 was released this April, which was less than 6 months ago. I
> think it all fits.

Indeed,  
> 
> A temporary solution would be to make use of the fact that R is a very
> dynamic language and perform surgery on a live function inside a
> loaded package:
> 
> library(codetools)
> 
> nlme <- loadNamespace('nlme')
> unlockBinding('gnls', nlme)
> nlme$gnls <- `body<-`(fun = nlme$gnls, value = walkCode(
> 	body(nlme$gnls), makeCodeWalker(
> 		call = function(e, w)
> 			as.call(lapply(as.list(e), function(ee)
> 				if (!missing(ee)) walkCode(ee, w)
> 			)),
> 		leaf = function(e, w)
> 			if (is.symbol(e) && e == 'deparse') {
> 				as.name('deparse1')
> 			} else e
> 	)
> ))
> lockBinding('gnls', nlme)
> rm(nlme)
> 
> grep('deparse', deparse(nlme::gnls), value = TRUE)
> # [1] "                deparse1(pp[[3]]), sep = \"~\"), collapse =
> \",\"), " # [2] "        if (deparse1(params[[nm]][[3]]) != \"1\") {"
> # [3] "        list(row.names(dataModShrunk), deparse1(form[[2]]))), "
> 
> Aaron's example seems to work after this, modulo needing 12 starting
> values instead of 13.

Such surgery is beyond the capability of us ordinary mortals, but given
your explicit and clear recipe it should be do-able. 

Thanks for all of this.

cheers,

Rolf

P. S.  Ben:  you were correct in your original conjecture, to which I
erroneously said "No".  Lack of insight on my part.

R.

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From wewo|@k| @end|ng |rom gm@||@com  Mon Jul 25 16:43:33 2022
From: wewo|@k| @end|ng |rom gm@||@com (Witold E Wolski)
Date: Mon, 25 Jul 2022 16:43:33 +0200
Subject: [R] Problem installing R 4.2 on Ubuntu 20-04
Message-ID: <CAAjnpdhdtLOTJK3sNwjsPP8FX7JTzZ=ENtW-Wj799bU02Zcm+g@mail.gmail.com>

I am failing to get R 4 installed on Ubuntu. I am following the
instructions given here:
https://cran.r-project.org/bin/linux/ubuntu/

These are the errors I am getting:
<start>
parallels at ubuntu-linux-20-04-desktop:~/Downloads/fragpipe/bin$ sudo
apt install --no-install-recommends r-base
[sudo] password for parallels:
Reading package lists... Done
Building dependency tree
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 r-base : Depends: r-base-core (>= 4.2.1-1.2004.0) but it is not going
to be installed
          Depends: r-recommended (= 4.2.1-1.2004.0) but it is not
going to be installed
E: Unable to correct problems, you have held broken packages.
<end>


This is the tail of the source.lists file:

<start>
deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/
# deb-src https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/
<end>

This is the head of executing: apt-cache policy r-base
<start>
arallels at ubuntu-linux-20-04-desktop:~/Downloads/fragpipe/bin$
apt-cache policy r-base
r-base:
  Installed: (none)
  Candidate: 4.2.1-1.2004.0
  Version table:
     4.2.1-1.2004.0 500
        500 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ Packages
     4.2.0-1.2004.0 500
        500 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ Packages
     4.1.3-1.2004.0 500
        500 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ Packages
     4.1.2-1.2004.0 500

<end>

End this is for executing apt-cache policy r-base-core

<start>
parallels at ubuntu-linux-20-04-desktop:~/Downloads/fragpipe/bin$
apt-cache policy r-base-core
r-base-core:
  Installed: (none)
  Candidate: 3.6.3-2
  Version table:
     3.6.3-2 500
        500 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64 Packages
        100 /var/lib/dpkg/status

<end>

Someone else described the same/similar problem here:

https://askubuntu.com/questions/1373827/problems-installing-latest-version-of-r-in-ubuntu-20-04-lts

What step did I miss executing?

Best regards
Witek


--
Witold Eryk Wolski


From bgunter@4567 @end|ng |rom gm@||@com  Mon Jul 25 16:47:11 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 25 Jul 2022 07:47:11 -0700
Subject: [R] Problem installing R 4.2 on Ubuntu 20-04
In-Reply-To: <CAAjnpdhdtLOTJK3sNwjsPP8FX7JTzZ=ENtW-Wj799bU02Zcm+g@mail.gmail.com>
References: <CAAjnpdhdtLOTJK3sNwjsPP8FX7JTzZ=ENtW-Wj799bU02Zcm+g@mail.gmail.com>
Message-ID: <CAGxFJbQB3fXn2pdN=72tnrtxJWGZycZeREv0aVqhn+tXRorJRQ@mail.gmail.com>

Better posted on R-sig-debian, I think.

Cheers,
Bert

On Mon, Jul 25, 2022 at 7:44 AM Witold E Wolski <wewolski at gmail.com> wrote:

> I am failing to get R 4 installed on Ubuntu. I am following the
> instructions given here:
> https://cran.r-project.org/bin/linux/ubuntu/
>
> These are the errors I am getting:
> <start>
> parallels at ubuntu-linux-20-04-desktop:~/Downloads/fragpipe/bin$ sudo
> apt install --no-install-recommends r-base
> [sudo] password for parallels:
> Reading package lists... Done
> Building dependency tree
> Reading state information... Done
> Some packages could not be installed. This may mean that you have
> requested an impossible situation or if you are using the unstable
> distribution that some required packages have not yet been created
> or been moved out of Incoming.
> The following information may help to resolve the situation:
>
> The following packages have unmet dependencies:
>  r-base : Depends: r-base-core (>= 4.2.1-1.2004.0) but it is not going
> to be installed
>           Depends: r-recommended (= 4.2.1-1.2004.0) but it is not
> going to be installed
> E: Unable to correct problems, you have held broken packages.
> <end>
>
>
> This is the tail of the source.lists file:
>
> <start>
> deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/
> # deb-src https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/
> <end>
>
> This is the head of executing: apt-cache policy r-base
> <start>
> arallels at ubuntu-linux-20-04-desktop:~/Downloads/fragpipe/bin$
> apt-cache policy r-base
> r-base:
>   Installed: (none)
>   Candidate: 4.2.1-1.2004.0
>   Version table:
>      4.2.1-1.2004.0 500
>         500 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/
> Packages
>      4.2.0-1.2004.0 500
>         500 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/
> Packages
>      4.1.3-1.2004.0 500
>         500 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/
> Packages
>      4.1.2-1.2004.0 500
>
> <end>
>
> End this is for executing apt-cache policy r-base-core
>
> <start>
> parallels at ubuntu-linux-20-04-desktop:~/Downloads/fragpipe/bin$
> apt-cache policy r-base-core
> r-base-core:
>   Installed: (none)
>   Candidate: 3.6.3-2
>   Version table:
>      3.6.3-2 500
>         500 http://ports.ubuntu.com/ubuntu-ports focal/universe arm64
> Packages
>         100 /var/lib/dpkg/status
>
> <end>
>
> Someone else described the same/similar problem here:
>
>
> https://askubuntu.com/questions/1373827/problems-installing-latest-version-of-r-in-ubuntu-20-04-lts
>
> What step did I miss executing?
>
> Best regards
> Witek
>
>
> --
> Witold Eryk Wolski
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From wewo|@k| @end|ng |rom gm@||@com  Mon Jul 25 17:56:06 2022
From: wewo|@k| @end|ng |rom gm@||@com (Witold E Wolski)
Date: Mon, 25 Jul 2022 17:56:06 +0200
Subject: [R] Problem installing R 4.2 on Ubuntu 20-04
In-Reply-To: <7b61e20f-d117-e1da-3930-7ce38c29a897@gmail.com>
References: <CAAjnpdhdtLOTJK3sNwjsPP8FX7JTzZ=ENtW-Wj799bU02Zcm+g@mail.gmail.com>
 <7b61e20f-d117-e1da-3930-7ce38c29a897@gmail.com>
Message-ID: <CAAjnpdiwazwK5NY9F15G48rWSKo72Vgq0pbRBQ5oyyCJedeenA@mail.gmail.com>

Is it possible that the reason is that I have an Ubuntu for arm64 installation?
I installed R from source.

Thanks

On Mon, 25 Jul 2022 at 17:45, Micha Silver <tsvibar at gmail.com> wrote:
>
> Just a guess:
>
>
> On 25/07/2022 17:43, Witold E Wolski wrote:
> > I am failing to get R 4 installed on Ubuntu. I am following the
> > instructions given here:
> > https://cran.r-project.org/bin/linux/ubuntu/
> >
> > These are the errors I am getting:
> > <start>
> > parallels at ubuntu-linux-20-04-desktop:~/Downloads/fragpipe/bin$ sudo
> > apt install --no-install-recommends r-base
> >
> --no-install-recommends might be your problem
>
> Try running without that.
>
>
> (And also do sudo apt update first)
>
>
> > The following packages have unmet dependencies:
> And this is why that --no-install-recommends is causing the install to
> fail:
> >   r-base : Depends: r-base-core (>= 4.2.1-1.2004.0) but it is not going
> > to be installed
> >            Depends: r-recommended (= 4.2.1-1.2004.0) but it is not
> > going to be installed
> > E: Unable to correct problems, you have held broken packages.
> > <end>
> >
> >
> > Best regards
> > Witek
> >
> >
> > --
> > Witold Eryk Wolski
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Micha Silver
> Ben Gurion Univ.
> Sde Boker, Remote Sensing Lab
> cell: +972-523-665918
>


-- 
Witold Eryk Wolski


From t@v|b@r @end|ng |rom gm@||@com  Mon Jul 25 17:45:22 2022
From: t@v|b@r @end|ng |rom gm@||@com (Micha Silver)
Date: Mon, 25 Jul 2022 18:45:22 +0300
Subject: [R] Problem installing R 4.2 on Ubuntu 20-04
In-Reply-To: <CAAjnpdhdtLOTJK3sNwjsPP8FX7JTzZ=ENtW-Wj799bU02Zcm+g@mail.gmail.com>
References: <CAAjnpdhdtLOTJK3sNwjsPP8FX7JTzZ=ENtW-Wj799bU02Zcm+g@mail.gmail.com>
Message-ID: <7b61e20f-d117-e1da-3930-7ce38c29a897@gmail.com>

Just a guess:


On 25/07/2022 17:43, Witold E Wolski wrote:
> I am failing to get R 4 installed on Ubuntu. I am following the
> instructions given here:
> https://cran.r-project.org/bin/linux/ubuntu/
>
> These are the errors I am getting:
> <start>
> parallels at ubuntu-linux-20-04-desktop:~/Downloads/fragpipe/bin$ sudo
> apt install --no-install-recommends r-base
>
--no-install-recommends might be your problem

Try running without that.


(And also do sudo apt update first)


> The following packages have unmet dependencies:
And this is why that --no-install-recommends is causing the install to 
fail:
>   r-base : Depends: r-base-core (>= 4.2.1-1.2004.0) but it is not going
> to be installed
>            Depends: r-recommended (= 4.2.1-1.2004.0) but it is not
> going to be installed
> E: Unable to correct problems, you have held broken packages.
> <end>
>
>
> Best regards
> Witek
>
>
> --
> Witold Eryk Wolski
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Micha Silver
Ben Gurion Univ.
Sde Boker, Remote Sensing Lab
cell: +972-523-665918


From kry|ov@r00t @end|ng |rom gm@||@com  Mon Jul 25 18:39:46 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Mon, 25 Jul 2022 19:39:46 +0300
Subject: [R] Problem installing R 4.2 on Ubuntu 20-04
In-Reply-To: <CAAjnpdiwazwK5NY9F15G48rWSKo72Vgq0pbRBQ5oyyCJedeenA@mail.gmail.com>
References: <CAAjnpdhdtLOTJK3sNwjsPP8FX7JTzZ=ENtW-Wj799bU02Zcm+g@mail.gmail.com>
 <7b61e20f-d117-e1da-3930-7ce38c29a897@gmail.com>
 <CAAjnpdiwazwK5NY9F15G48rWSKo72Vgq0pbRBQ5oyyCJedeenA@mail.gmail.com>
Message-ID: <20220725193946.6e42c69d@trisector>

On Mon, 25 Jul 2022 17:56:06 +0200
Witold E Wolski <wewolski at gmail.com> wrote:

> Is it possible that the reason is that I have an Ubuntu for arm64
> installation?

Yes. https://cran.r-project.org/bin/linux/ubuntu/ notes:

>> Focal Fossa (20.04; LTS and amd64 only)

LTS seems to be for amd64 processors only:
https://releases.ubuntu.com/20.04/

-- 
Best regards,
Ivan


From @k@h@y_e4 @end|ng |rom hotm@||@com  Mon Jul 25 20:35:39 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Mon, 25 Jul 2022 18:35:39 +0000
Subject: [R] problems in R in RHEL....
Message-ID: <PU4P216MB1568C281F7F0DC2E3AB426BDC8959@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear members,
                          I am using R in the latest version of RHEL, in an AWS z1d instance. I have the following questions:


  1.  Previously, when I started R by typing R in the bash prompt, a new screen would open with the R banner and the R console. But now the banner and the R console is opening in the same screen as the bash prompt. How do you revert to the previous behaviour?
  2.   When I run fix(Myfun), it is opening a vi editor with the code of Myfun in it. But when I close the editor some of the code is left behind in the R console. How do you close the vi editor opened by fix command without leaving any code behind in the console?

Many thanks in advance....

Yours sincerely
AKSHAY M KULKARNI

	[[alternative HTML version deleted]]


From j@de@shod@@ m@iii@g oii googiem@ii@com  Mon Jul 25 21:17:16 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Mon, 25 Jul 2022 20:17:16 +0100
Subject: [R] mgcv: relative risk from GAM with distributed lag
In-Reply-To: <59ce25f3-7168-2a30-f2fe-8710ed309731@bath.edu>
References: <CANg3_k-18ND8DrwfYJmGAN5hq18QweZFPp4GhCuHwB=XFGAFiA@mail.gmail.com>
 <CANg3_k9=GhwRjPyuLQBYmxD8Q-ksPrM52tS7DW0Cv3cpKn00Zw@mail.gmail.com>
 <e0f4395b-e1cf-abfa-e4d4-ee3dd519887e@bath.edu>
 <CANg3_k9-OKAK1VSirWSVGXQVMWoDeKoG7gN7mUfGfZdX4CnpgQ@mail.gmail.com>
 <CANg3_k9Nkok-Mr4DFOZ3=d1XtA-Pq1seW+E+KtWLttQUe_+XcQ@mail.gmail.com>
 <CANg3_k8C5W1RKL-gsGbr3556hSHq7+-oCSRCZR-ODEsN_7_MAA@mail.gmail.com>
 <59ce25f3-7168-2a30-f2fe-8710ed309731@bath.edu>
Message-ID: <CANg3_k9-p2Vx75BL+EgMKBZ-WkV8QbgdTzzN08NcNQJWaxGWdg@mail.gmail.com>

Hi Simon,

Thanks for that example. It's been very useful!  It turned out that I
created the prediction data for lag incorrectly. I've now managed to
get perspective plots on the relative risk scale.

I've now moved on to trying to get a plot of overall RR (y-axis) for a
range of temperature values (x-axis), where RR for each temperature
value is summed over lag periods 0-6 (see bottom plot page 2230 in
Gasparrini's paper: https://doi.org/10.1002/sim.3940). This now seems
straighforward with predict.gam() and type = "terms" in the way you
suggested, but as acknowledged, that doesn't give me credible
intervals. I've just been told that I MUST present results with
confidence/ credible intervals (which is fair enough).

For the last two days I've tried making the plot of overall RR with
CIs, using the lpmatrix, as you suggested (looked it up in your book,
and also an earlier post of yours with an example:
https://stat.ethz.ch/pipermail/r-help/2012-May/314387.html), but I
don't even understand how the book example can be applied to my
problem, and I can't get the example in the online post to work. I
don't really understand what an lpmatrix is, (other than that it seems
to give me a value for each smooth term for each basis function?) or
what the code in the book example does, so I have no idea what I am
doing wroing, or what good results are supposed to look like. (I'm
sorry, my background is in public health rather than statistics).

I am hoping that I can ask for your (or anyone's)  help one final
time. (Once I have the CI's, my modelling will be finished, and I will
be out of everyone's hair with questions about GAMs... at least for a
while!).

Here's my code with reproducible data to get a plot of overall RR
(y-axis) vs temperature (x-axis) where overall RR for each temperature
value is obtained by summing RR over lag periods 0-6. But I have
absolutely no idea how to use predict.gam with type = lpmatrix to get
from this curve to one with CIs.

library(mgcv)

# simulate data
set.seed(3) # make reproducible example
simdat <- gamSim(1,400)
g <- exp(simdat$f/5)
simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
simdat$time <- 1:400  # create time series
names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
"f0", "f1", "f2", "f3", "time")

# lag function based on  Simon Wood's book (2017, p.349)
lagard <- function(x,n.lag=7) {
n <- length(x); X <- matrix(NA,n,n.lag)
for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
X
}

# create lagged variables as 7-column matrices
dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
deaths=simdat$deaths, time = simdat$time)
dat$temp <- lagard(simdat$temp)
dat$rain <- lagard(simdat$rain)
dat$humidity <- lagard(simdat$humidity)

mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
family = nb, method = 'REML', select = TRUE)

# create prediction data
m1 <- 40 # number of points to predict across the range of each weather variable
m2 <- 7 # number of lag periods
n <- m1*m2 # total number of prediction points

pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T),
length = m1)
pred_lag <- 0:6
pred_rain <- seq(min(dat$rain, na.rm = T), max(dat$rain, na.rm = T),
length = m1)
pred_humidity <- seq(min(dat$humidity, na.rm = T), max(dat$humidity,
na.rm = T), length = m1)
pred_time <- seq(min(dat$time, na.rm = T), max(dat$time, na.rm = T),
length = m1)

pd <- data.frame(temp=rep(pred_temp,m2),lag=rep(pred_lag,each=m1),rain=rep(pred_rain,m2),humidity=rep(pred_humidity,m2),time=rep(pred_time,m2))

#  prediction data with reference temperature set to median
temperature, all else equal to pd
pred_temp0 <- median(dat$temp, na.rm=T)
pd0 <- data.frame(temp=rep(pred_temp0,n),lag=rep(pred_lag,each=m1),rain=rep(pred_rain,m2),humidity=rep(pred_humidity,m2),time=rep(pred_time,m2))

# make predictions
predictions <- predict.gam(mod, newdata=pd, type = "terms")
predictions0 <- predict.gam(mod, newdata=pd0, type = "terms")

# calculate RR (relative to reference temperature set above)
diff <- predictions[,2] - predictions0[,2]
rr <- as.vector(exp(diff))

# convert rr to matrix required for z argument for persp() plot
rr_mat <- matrix(rr, m1, m2)
persp(x=pred_temp,y=pred_lag,z=rr_mat,theta=30,phi=30,ticktype =
"detailed", col="blue",zlab="RR")

# create plots of overall RR (y-axis, summed over lags 0-6) vs
temperature (x-axis)
# convert predictions to matrix to allow summing over lags
pmat <- matrix(predictions[,2],m1,m2)
pmat0 <- matrix(predictions0[,2],m1,m2)

# sum predictions over lags 0-6 for each temperature
psummed <- rowSums(pmat)
psummed0 <-rowSums(pmat0)

# subtract the predictions for the reference temperature from the
predictions for each temperature
pref <- psummed-psummed0

# exponentiating a difference on log-scale gives RR
rr_overall <- exp(pref)
plot(pred_temp, rr_overall, type = "l")

# now create CIs for overall RR plot
Xp <- predict(a1a_high_heat,newdata=pd,type="lpmatrix")

# and how to procede now?????? So close to having results, and yet....



On Sat, 23 Jul 2022 at 21:26, Simon Wood <simon.wood at bath.edu> wrote:
>
> I doubt you want a 7 by 1000 grid for your persp plot. Here's an example
> of producing a persp plot using predict.gam and a custom grid. Since
> only the effects of x1 and x2 are being plotted it doesn't matter what
> x0 and x3 are set to (the model is additive after all). In your case
> only one smooth term is involved of course.
>
> library(mgcv)
> n <- 200
> sig <- 2
> dat <- gamSim(1,n=n,scale=sig)
>
> b <- gam(y~s(x0)+s(I(x1^2))+s(x2)+offset(x3),data=dat)
>
> m1 <- 20;m2 <- 30; n <- m1*m2
> x1 <- seq(.2,.8,length=m1);x2 <- seq(.2,.8,length=m2) ## marginal values
> for evaluation grid
> df <- data.frame(x0=rep(.5,n),x1=rep(x1,m2),x2=rep(x2,each=m1),x3=rep(0,n))
> pf <- predict(b,newdata=df,type="terms")
>
> persp(x1,x2,matrix(pf[,2]+pf[,3],m1,m2),theta=-130,col="blue",zlab="")
>
> On 23/07/2022 14:54, jade.shodan at googlemail.com wrote:
> > Hi Simon and all,
> >
> > I've corrected some mistakes in setting up the prediction data frame
> > (sorry, very stressed and sleep deprived due to closing in deadlines),
> > but am having problems getting the perspective plot using persp().
> >
> > I've calculated relative risk (RR) but am having trouble getting the
> > perspective (or contour) plot. persp() takes x and y vectors in
> > ascending order, and z as a matrix.  I've seen the outer() function
> > being used to facilitate perspective plots, but every example I've
> > seen treats z as a function, to be evaluated at combinations for x and
> > y. In my case, I already have observations for z. Not sure if I need
> > to use outer(), and if so, how?
> >
> > My code for making predictions, getting RR and trying to get the plot
> > is as follows (full reproducible example including model at the
> > bottom). Can someone tell me what I'm doing wrong?
> >
> > ########### CODE ########################
> >
> > # create prediction data
> > N <- 1000 # number of points for which to predict the smooths
> > pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T),
> > length = N)  # prediction data for temperature
> > pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
> > pred_rain <- seq(min(dat$rain, na.rm = T), max(dat$rain, na.rm = T), length = N)
> > pred_humidity <- seq(min(dat$humidity, na.rm = T), max(dat$humidity,
> > na.rm = T), length = N)
> > pred_time <- seq(min(dat$time, na.rm = T), max(dat$time, na.rm = T), length = N)
> >
> > pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> > pred_humidity, rain = pred_rain, time = pred_time)
> >
> > # create prediction data with reference temperature set to median temperature
> > # identical to pd but now with all temperatures set to median value of
> > temperature
> > pred_temp0 <- rep(median(dat$temp, na.rm=T), N)
> > pd0 <- data.frame(temp = pred_temp0, lag = pred_lag, humidity =
> > pred_humidity, rain = pred_rain, time = pred_time)
> >
> > # make predictions
> > predictions <- predict.gam(mod, pd, type = "terms")
> > predictions0 <- predict.gam(mod, pd0, type = "terms")
> >
> > # calculate RR
> > diff <- predictions[,2] - predictions0[,2]
> > rr <- as.vector(exp(diff))
> >
> > # convert rr to matrix required for z argument for persp()
> > rr_mat <- matrix(rr, nrow = N)
> > persp(pred_temp, pred_lag, rr_mat)
> >
> > ######### ERROR MESSAGE ##################################
> >
> > The persp call results in the follow error:
> >
> > Error in persp.default(pred_temp, pred_lag, rr_mat) :
> >    increasing 'x' and 'y' values expected
> >
> > I don't understand this because pred_temp and pred_lag ARE in ascending order.
> >
> > ############################################################
> > FULL REPRODUCIBLE EXAMPLE
> > ############################################################
> >
> > library(mgcv)
> > set.seed(3) # make reproducible example
> > simdat <- gamSim(1,400)
> > g <- exp(simdat$f/5)
> >
> > simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
> > simdat$time <- 1:400  # create time series
> >
> > names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
> > "f0", "f1", "f2", "f3", "time")
> >
> > # lag function based on Wood (book 2017, p.349 and gamair package
> > documentation p.54
> > # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
> > lagard <- function(x,n.lag=7) {
> > n <- length(x); X <- matrix(NA,n,n.lag)
> > for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> > X
> > }
> >
> > # set up lag, temp, rain and humidity as 7-column matrices
> > # to create lagged variables
> > dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
> > deaths=simdat$deaths, time = simdat$time)
> > dat$temp <- lagard(simdat$temp)
> > dat$rain <- lagard(simdat$rain)
> > dat$humidity <- lagard(simdat$humidity)
> >
> > mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
> > te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
> > family = nb, method = 'REML', select = TRUE)
> >
> > summary(mod)
> > plot(mod, scheme = 1)  # perspective
> > plot(mod, scheme = 2)  # contour
> >
> > # create prediction data
> > N <- 1000 # number of points for which to predict the smooths
> > pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
> > pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
> > pred_rain <- seq(min(dat$rain, na.rm = T), max(dat$rain, na.rm = T), length = N)
> > pred_humidity <- seq(min(dat$humidity, na.rm = T), max(dat$humidity,
> > na.rm = T), length = N)
> > pred_time <- seq(min(dat$time, na.rm = T), max(dat$time, na.rm = T), length = N)
> >
> > pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> > pred_humidity, rain = pred_rain, time = pred_time)
> >
> > # create prediction data with reference temperature set to median temperature
> > # identical to pd but now with all temperatures set to median value of
> > temperature
> > pred_temp0 <- rep(median(dat$temp, na.rm=T), N)
> > pd0 <- data.frame(temp = pred_temp0, lag = pred_lag, humidity =
> > pred_humidity, rain = pred_rain, time = pred_time)
> >
> > # make predictions
> > predictions <- predict.gam(mod, pd, type = "terms")
> > predictions0 <- predict.gam(mod, pd0, type = "terms")
> >
> > # calculate RR
> > diff <- predictions[,2] - predictions0[,2]
> > rr <- as.vector(exp(diff))
> >
> > # convert rr to matrix required for z argument for persp()
> > rr_mat <- matrix(rr, nrow = N)
> > persp(pred_temp, pred_lag, rr_mat)
> >
> >
> >
> >
> > On Fri, 22 Jul 2022 at 13:29, jade.shodan at googlemail.com
> > <jade.shodan at googlemail.com> wrote:
> >> I made a small error in the code below (not checking for NAs which are
> >> introduced by the lag function). However, this doesn't solve the issue
> >> I raised).
> >>
> >> So here's the problem (with corrected code) again:
> >>
> >> I'm not sure what to do with the 'time' variable. (I don't want to
> >> make predictions for specific points in time). I coded as follows
> >> (full reproducible example at bottom of email), but get a warning and
> >> error:
> >>
> >>
> >> N <- 1000     # number of points for smooth to be predicted
> >> # new temperatures and lags for prediction
> >> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
> >> pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)      ## IS IT CORRECT TO SET
> >> UP LAG LIKE THIS?
> >>
> >> # not sure if these covariates are required with type = "terms"
> >> pred_humidity <- rep(median(dat$humidity, na.rm = T), N)
> >> pred_rain <- rep(median(dat$rain, na.rm = T), N)
> >> pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> >> pred_humidity, rain = pred_rain)
> >>
> >> predictions <- predict(mod, pd, type = "terms")
> >>
> >>
> >> The predict line creates the following warning and error:
> >>
> >> Warning in predict.gam(mod, pd, type = "terms") :
> >>    not all required variables have been supplied in  newdata!
> >>
> >> Error in model.frame.default(ff, data = newdata, na.action = na.act) :
> >>    object is not a matrix
> >>
> >>
> >> For ease of reference, I've (re)included the full reproducible example:
> >>
> >> library(mgcv)
> >> set.seed(3) # make reproducible example
> >> simdat <- gamSim(1,400)
> >> g <- exp(simdat$f/5)
> >>
> >> simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
> >> simdat$time <- 1:400  # create time series
> >>
> >> names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
> >> "f0", "f1", "f2", "f3", "time")
> >>
> >> # lag function based on Wood (book 2017, p.349 and gamair package
> >> documentation p.54
> >> # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
> >> lagard <- function(x,n.lag=7) {
> >> n <- length(x); X <- matrix(NA,n,n.lag)
> >> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> >> X
> >> }
> >>
> >> # set up lag, temp, rain and humidity as 7-column matrices
> >> # to create lagged variables
> >> dat <- list(lag=matrix(0:6,nrow(simda
> >> t),7,byrow=TRUE),
> >> deaths=simdat$deaths, time = simdat$time)
> >> dat$temp <- lagard(simdat$temp)
> >> dat$rain <- lagard(simdat$rain)
> >> dat$humidity <- lagard(simdat$humidity)
> >>
> >> mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
> >> te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
> >> family = nb, method = 'REML', select = TRUE)
> >>
> >> # create prediction data
> >> N <- 1000 # number of points for which to predict the smooths
> >> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
> >> pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
> >> pred_humidity <- rep(median(dat$humidity), N)
> >> pred_rain <- rep(median(dat$rain), N)
> >> pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> >> pred_humidity, rain = pred_rain)
> >>
> >> # make predictions
> >> predictions <- predict(mod, pd, type = "terms")
> >>
> >> On Fri, 22 Jul 2022 at 12:47, jade.shodan at googlemail.com
> >> <jade.shodan at googlemail.com> wrote:
> >>> Hi Simon,
> >>>
> >>> Thanks for the pointers! But I'm not sure what to do with the 'time'
> >>> variable. (I don't want to make predictions for specific points in
> >>> time). I coded as follows (full reproducible example at bottom of
> >>> email), but get a warning and error:
> >>>
> >>>
> >>> N <- 1000     # number of points for smooth to be predicted
> >>> # new temperatures and lags for prediction
> >>> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
> >>> pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)      ## IS IT CORRECT TO SET
> >>> UP LAG LIKE THIS?
> >>>
> >>> # not sure if these covariates are required with type = "terms"
> >>> pred_humidity <- rep(median(dat$humidity), N)
> >>> pred_rain <- rep(median(dat$rain), N)
> >>> pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> >>> pred_humidity, rain = pred_rain)
> >>>
> >>> predictions <- predict(mod, pd, type = "terms")
> >>>
> >>>
> >>> The predict line creates the following warning and error:
> >>>
> >>> Warning in predict.gam(mod, pd, type = "terms") :
> >>>    not all required variables have been supplied in  newdata!
> >>>
> >>> Error in model.frame.default(ff, data = newdata, na.action = na.act) :
> >>>    object is not a matrix
> >>>
> >>>
> >>> For ease of reference, I've (re)included the full reproducible example:
> >>>
> >>> library(mgcv)
> >>> set.seed(3) # make reproducible example
> >>> simdat <- gamSim(1,400)
> >>> g <- exp(simdat$f/5)
> >>>
> >>> simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
> >>> simdat$time <- 1:400  # create time series
> >>>
> >>> names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
> >>> "f0", "f1", "f2", "f3", "time")
> >>>
> >>> # lag function based on Wood (book 2017, p.349 and gamair package
> >>> documentation p.54
> >>> # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
> >>> lagard <- function(x,n.lag=7) {
> >>> n <- length(x); X <- matrix(NA,n,n.lag)
> >>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> >>> X
> >>> }
> >>>
> >>> # set up lag, temp, rain and humidity as 7-column matrices
> >>> # to create lagged variables
> >>> dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
> >>> deaths=simdat$deaths, time = simdat$time)
> >>> dat$temp <- lagard(simdat$temp)
> >>> dat$rain <- lagard(simdat$rain)
> >>> dat$humidity <- lagard(simdat$humidity)
> >>>
> >>> mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
> >>> te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
> >>> family = nb, method = 'REML', select = TRUE)
> >>>
> >>> # create prediction data
> >>> N <- 1000 # number of points for which to predict the smooths
> >>> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
> >>> pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
> >>> pred_humidity <- rep(median(dat$humidity), N)
> >>> pred_rain <- rep(median(dat$rain), N)
> >>> pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> >>> pred_humidity, rain = pred_rain)
> >>>
> >>> # make predictions
> >>> predictions <- predict(mod, pd, type = "terms")
> >>>
> >>>
> >>> On Fri, 22 Jul 2022 at 09:54, Simon Wood <simon.wood at bath.edu> wrote:
> >>>>
> >>>> On 21/07/2022 15:19, jade.shodan--- via R-help wrote:
> >>>>> Hello everyone (incl. Simon Wood?),
> >>>>>
> >>>>> I'm not sure that my original question (see below, including
> >>>>> reproducible example) was as clear as it could have been. To clarify,
> >>>>> what I would to like to get is:
> >>>>>
> >>>>> 1) a perspective plot of temperature x lag x relative risk.  I know
> >>>>> how to use plot.gam and vis.gam but don't know how to get plots on the
> >>>>> relative risk scale as opposed to  "response" or "link".
> >>>> - You are on the log scale so I think that all you need to do is to use
> >>>> 'predict.gam', with 'type = "terms"' to  get the predictions for the
> >>>> te(temp, lag) term over the required grid of lags and temperatures.
> >>>> Suppose the dataframe of prediction data is 'pd'. Now produce pd0, which
> >>>> is identical to pd, except that the temperatures are all set to the
> >>>> reference temperature. Use predict.gam to predict te(temp,lag) from pd0.
> >>>> Now the exponential of the difference between the first and second
> >>>> predictions is the required RR, which you can plot using 'persp',
> >>>> 'contour', 'image' or whatever. If you need credible intervals see pages
> >>>> 341-343 of my 'GAMs: An intro with R' book (2nd ed).
> >>>>
> >>>>> 2) a plot of relative risk (accumulated across all lags) vs
> >>>>> temperature, given a reference temperature. An example of such a plot
> >>>>> can be found in figure 2 (bottom) of this paper by Gasparrini et al:
> >>>>> https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3940
> >>>> - I guess this only makes sense if you have the same temperature at all
> >>>> lags. So this time produce a data.frame with each desired temperature
> >>>> repeated for each lag: 'pd1'. Again use predict.gam(...,type="terms").
> >>>> Then sum the predictions over lags for each temperature, subtract the
> >>>> minimum, and take the exponential. Same as above for CIs.
> >>>>
> >>>> best,
> >>>>
> >>>> Simon
> >>>>
> >>>>> I've seen Simon Wood's response to a related issue here:
> >>>>> https://stat.ethz.ch/pipermail/r-help/2012-May/314387.html
> >>>>> However, I'm not sure how to apply this to time series data with
> >>>>> distributed lag, to get the above mentioned figures.
> >>>>>
> >>>>> Would be really grateful for suggestions!
> >>>>>
> >>>>> Jade
> >>>>>
> >>>>> On Tue, 19 Jul 2022 at 16:07, jade.shodan at googlemail.com
> >>>>> <jade.shodan at googlemail.com> wrote:
> >>>>>> Dear list members,
> >>>>>>
> >>>>>> Does anyone know how to obtain a relative risk/ risk ratio from a GAM
> >>>>>> with a distributed lag model implemented in mgcv? I have a GAM
> >>>>>> predicting daily deaths from time series data consisting of daily
> >>>>>> temperature, humidity and rainfall. The GAM includes a distributed lag
> >>>>>> model because deaths may occur over several days following a high heat
> >>>>>> day.
> >>>>>>
> >>>>>> What I'd like to do is compute (and plot) the relative risk
> >>>>>> (accumulated across all lags) for a given temperature vs the
> >>>>>> temperature at which the risk is lowest, with corresponding confidence
> >>>>>> intervals. I am aware of the predict.gam function but am not sure if
> >>>>>> and how it should be used in this case. (Additionally, I'd also like
> >>>>>> to plot the relative risk for different lags separately).
> >>>>>>
> >>>>>> I apologise if this seems trivial to some. (Actually, I hope it is,
> >>>>>> because that might mean I get a solution!) I've been looking for
> >>>>>> examples on how to do this, but found nothing so far. Suggestions
> >>>>>> would be very much appreciated!
> >>>>>>
> >>>>>> Below is a reproducible example with the GAM:
> >>>>>>
> >>>>>> library(mgcv)
> >>>>>> set.seed(3) # make reproducible example
> >>>>>> simdat <- gamSim(1,400) # simulate data
> >>>>>> g <- exp(simdat$f/5)
> >>>>>> simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
> >>>>>> simdat$time <- 1:400  # create time series
> >>>>>> names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
> >>>>>> "f0", "f1", "f2", "f3", "time")
> >>>>>>
> >>>>>> # lag function based on Simon Wood (book 2017, p.349 and gamair
> >>>>>> package documentation p.54
> >>>>>> # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
> >>>>>> lagard <- function(x,n.lag=7) {
> >>>>>> n <- length(x); X <- matrix(NA,n,n.lag)
> >>>>>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> >>>>>> X
> >>>>>> }
> >>>>>>
> >>>>>> # set up lag, temp, rain and humidity as 7-column matrices
> >>>>>> # to create lagged variables - based on Simon Wood's example
> >>>>>> dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
> >>>>>> deaths=simdat$deaths, time = simdat$time)
> >>>>>> dat$temp <- lagard(simdat$temp)
> >>>>>> dat$rain <- lagard(simdat$rain)
> >>>>>> dat$humidity <- lagard(simdat$humidity)
> >>>>>>
> >>>>>> mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
> >>>>>> te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
> >>>>>> family = nb, method = 'REML', select = TRUE)
> >>>>>>
> >>>>>> summary(mod)
> >>>>>> plot(mod, scheme = 1)
> >>>>>> plot(mod, scheme = 2)
> >>>>>>
> >>>>>> Thanks for any suggestions you may have,
> >>>>>>
> >>>>>> Jade
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>> --
> >>>> Simon Wood, School of Mathematics, University of Edinburgh,
> >>>> https://www.maths.ed.ac.uk/~swood34/
> >>>>
> --
> Simon Wood, School of Mathematics, University of Edinburgh,
> https://www.maths.ed.ac.uk/~swood34/
>


From @k@h@y_e4 @end|ng |rom hotm@||@com  Mon Jul 25 22:20:24 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Mon, 25 Jul 2022 20:20:24 +0000
Subject: [R] removing dates from xts object..
Message-ID: <PU4P216MB15685F4AFBD050B8EC21858EC8959@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear members,
                         I have:


> head(testOHLC)
           HINDUNILVR.NS.Open HINDUNILVR.NS.High HINDUNILVR.NS.Low HINDUNILVR.NS.Close
2007-01-02             217.80             219.00            215.45              216.40
2007-01-03             217.00             217.65            211.05              212.00
2007-01-04             213.00             214.25            209.65              210.60
2007-01-05             211.40             214.25            209.55              213.35
2007-01-08             213.35             213.35            207.10              210.10
2007-01-09             210.10             214.20            208.70              209.85
           HINDUNILVR.NS.Volume HINDUNILVR.NS.Adjusted
2007-01-02              2626898               155.0396
2007-01-03              4603921               151.8872
2007-01-04              5486460               150.8841
2007-01-05              5706066               152.8545
2007-01-08              3760443               150.5260
2007-01-09              5474633               150.3468

AND:


> head(testOHLC["2007-01-09"])
           HINDUNILVR.NS.Open HINDUNILVR.NS.High HINDUNILVR.NS.Low HINDUNILVR.NS.Close
2007-01-09              210.1              214.2             208.7              209.85
           HINDUNILVR.NS.Volume HINDUNILVR.NS.Adjusted
2007-01-09              5474633               150.3468

BUT:

> head(testOHLC[-"2007-01-09"])
 Error in -"2007-01-09" : invalid argument to unary operator

How do remove rows with certain dates in an xts object? This doesn't work:

x[-dates] (however, this does: x[dates])

Many thanks in advance......

Yours sincreley
AKSHAY M KULKARNI


	[[alternative HTML version deleted]]


From jo@h@m@u|r|ch @end|ng |rom gm@||@com  Mon Jul 25 22:26:22 2022
From: jo@h@m@u|r|ch @end|ng |rom gm@||@com (Joshua Ulrich)
Date: Mon, 25 Jul 2022 15:26:22 -0500
Subject: [R] removing dates from xts object..
In-Reply-To: <PU4P216MB15685F4AFBD050B8EC21858EC8959@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <PU4P216MB15685F4AFBD050B8EC21858EC8959@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <CAPPM_gScmr8nj1EyWEMeqMLiXK4ovihkcJ=rp0WHwLj_oETDJQ@mail.gmail.com>

This is the most straightforward and general way I can think of quickly:

library(xts)
data(sample_matrix)
x <- as.xts(sample_matrix, dateFormat = "Date")
head(x)
##                Open     High      Low    Close
## 2007-01-02 50.03978 50.11778 49.95041 50.11778
## 2007-01-03 50.23050 50.42188 50.23050 50.39767
## 2007-01-04 50.42096 50.42096 50.26414 50.33236
## 2007-01-05 50.37347 50.37347 50.22103 50.33459
## 2007-01-06 50.24433 50.24433 50.11121 50.18112
## 2007-01-07 50.13211 50.21561 49.99185 49.99185

y <- x[!(index(x) %in% as.Date(c("2007-01-04", "2007-01-05")))]
head(y)
##                Open     High      Low    Close
## 2007-01-02 50.03978 50.11778 49.95041 50.11778
## 2007-01-03 50.23050 50.42188 50.23050 50.39767
## 2007-01-06 50.24433 50.24433 50.11121 50.18112
## 2007-01-07 50.13211 50.21561 49.99185 49.99185
## 2007-01-08 50.03555 50.10363 49.96971 49.98806
## 2007-01-09 49.99489 49.99489 49.80454 49.91333

Best,
Josh


On Mon, Jul 25, 2022 at 3:20 PM akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>
> Dear members,
>                          I have:
>
>
> > head(testOHLC)
>            HINDUNILVR.NS.Open HINDUNILVR.NS.High HINDUNILVR.NS.Low HINDUNILVR.NS.Close
> 2007-01-02             217.80             219.00            215.45              216.40
> 2007-01-03             217.00             217.65            211.05              212.00
> 2007-01-04             213.00             214.25            209.65              210.60
> 2007-01-05             211.40             214.25            209.55              213.35
> 2007-01-08             213.35             213.35            207.10              210.10
> 2007-01-09             210.10             214.20            208.70              209.85
>            HINDUNILVR.NS.Volume HINDUNILVR.NS.Adjusted
> 2007-01-02              2626898               155.0396
> 2007-01-03              4603921               151.8872
> 2007-01-04              5486460               150.8841
> 2007-01-05              5706066               152.8545
> 2007-01-08              3760443               150.5260
> 2007-01-09              5474633               150.3468
>
> AND:
>
>
> > head(testOHLC["2007-01-09"])
>            HINDUNILVR.NS.Open HINDUNILVR.NS.High HINDUNILVR.NS.Low HINDUNILVR.NS.Close
> 2007-01-09              210.1              214.2             208.7              209.85
>            HINDUNILVR.NS.Volume HINDUNILVR.NS.Adjusted
> 2007-01-09              5474633               150.3468
>
> BUT:
>
> > head(testOHLC[-"2007-01-09"])
>  Error in -"2007-01-09" : invalid argument to unary operator
>
> How do remove rows with certain dates in an xts object? This doesn't work:
>
> x[-dates] (however, this does: x[dates])
>
> Many thanks in advance......
>
> Yours sincreley
> AKSHAY M KULKARNI
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com


From @k@h@y_e4 @end|ng |rom hotm@||@com  Mon Jul 25 22:34:18 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Mon, 25 Jul 2022 20:34:18 +0000
Subject: [R] removing dates from xts object..
In-Reply-To: <CAPPM_gScmr8nj1EyWEMeqMLiXK4ovihkcJ=rp0WHwLj_oETDJQ@mail.gmail.com>
References: <PU4P216MB15685F4AFBD050B8EC21858EC8959@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <CAPPM_gScmr8nj1EyWEMeqMLiXK4ovihkcJ=rp0WHwLj_oETDJQ@mail.gmail.com>
Message-ID: <PU4P216MB156861C1983669DB653B6B54C8959@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear joshua,
                      THanks for the reply

If I have a list of dates, it should be like this right?

y <- x[!(index(x) %in% dates)]
 where dates is a character vector of dates.

Thanking you,
Yours sincrely,
AKSHAY M KULKARNI



________________________________
From: Joshua Ulrich <josh.m.ulrich at gmail.com>
Sent: Tuesday, July 26, 2022 1:56 AM
To: akshay kulkarni <akshay_e4 at hotmail.com>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] removing dates from xts object..

This is the most straightforward and general way I can think of quickly:

library(xts)
data(sample_matrix)
x <- as.xts(sample_matrix, dateFormat = "Date")
head(x)
##                Open     High      Low    Close
## 2007-01-02 50.03978 50.11778 49.95041 50.11778
## 2007-01-03 50.23050 50.42188 50.23050 50.39767
## 2007-01-04 50.42096 50.42096 50.26414 50.33236
## 2007-01-05 50.37347 50.37347 50.22103 50.33459
## 2007-01-06 50.24433 50.24433 50.11121 50.18112
## 2007-01-07 50.13211 50.21561 49.99185 49.99185

y <- x[!(index(x) %in% as.Date(c("2007-01-04", "2007-01-05")))]
head(y)
##                Open     High      Low    Close
## 2007-01-02 50.03978 50.11778 49.95041 50.11778
## 2007-01-03 50.23050 50.42188 50.23050 50.39767
## 2007-01-06 50.24433 50.24433 50.11121 50.18112
## 2007-01-07 50.13211 50.21561 49.99185 49.99185
## 2007-01-08 50.03555 50.10363 49.96971 49.98806
## 2007-01-09 49.99489 49.99489 49.80454 49.91333

Best,
Josh


On Mon, Jul 25, 2022 at 3:20 PM akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>
> Dear members,
>                          I have:
>
>
> > head(testOHLC)
>            HINDUNILVR.NS.Open HINDUNILVR.NS.High HINDUNILVR.NS.Low HINDUNILVR.NS.Close
> 2007-01-02             217.80             219.00            215.45              216.40
> 2007-01-03             217.00             217.65            211.05              212.00
> 2007-01-04             213.00             214.25            209.65              210.60
> 2007-01-05             211.40             214.25            209.55              213.35
> 2007-01-08             213.35             213.35            207.10              210.10
> 2007-01-09             210.10             214.20            208.70              209.85
>            HINDUNILVR.NS.Volume HINDUNILVR.NS.Adjusted
> 2007-01-02              2626898               155.0396
> 2007-01-03              4603921               151.8872
> 2007-01-04              5486460               150.8841
> 2007-01-05              5706066               152.8545
> 2007-01-08              3760443               150.5260
> 2007-01-09              5474633               150.3468
>
> AND:
>
>
> > head(testOHLC["2007-01-09"])
>            HINDUNILVR.NS.Open HINDUNILVR.NS.High HINDUNILVR.NS.Low HINDUNILVR.NS.Close
> 2007-01-09              210.1              214.2             208.7              209.85
>            HINDUNILVR.NS.Volume HINDUNILVR.NS.Adjusted
> 2007-01-09              5474633               150.3468
>
> BUT:
>
> > head(testOHLC[-"2007-01-09"])
>  Error in -"2007-01-09" : invalid argument to unary operator
>
> How do remove rows with certain dates in an xts object? This doesn't work:
>
> x[-dates] (however, this does: x[dates])
>
> Many thanks in advance......
>
> Yours sincreley
> AKSHAY M KULKARNI
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



--
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com<http://www.fosstrading.com>

	[[alternative HTML version deleted]]


From jo@h@m@u|r|ch @end|ng |rom gm@||@com  Mon Jul 25 22:50:28 2022
From: jo@h@m@u|r|ch @end|ng |rom gm@||@com (Joshua Ulrich)
Date: Mon, 25 Jul 2022 15:50:28 -0500
Subject: [R] removing dates from xts object..
In-Reply-To: <PU4P216MB156861C1983669DB653B6B54C8959@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <PU4P216MB15685F4AFBD050B8EC21858EC8959@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <CAPPM_gScmr8nj1EyWEMeqMLiXK4ovihkcJ=rp0WHwLj_oETDJQ@mail.gmail.com>
 <PU4P216MB156861C1983669DB653B6B54C8959@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <CAPPM_gQwj7FmVt2avWX_UrZwv0d1gzzH6kPFycZphqZvvGtJqA@mail.gmail.com>

On Mon, Jul 25, 2022 at 3:34 PM akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>
> Dear joshua,
>                       THanks for the reply
>
> If I have a list of dates, it should be like this right?
>
> y <- x[!(index(x) %in% dates)]
>  where dates is a character vector of dates.
>

'dates' needs to be a vector of Date-classed values, not character

# character vector
dates_chr <- c("2007-01-04", "2007-01-05")
# Date vector -- use this below
dates_Date <- as.Date(dates_chr)

y <- x[!(index(x) %in% dates_Date)]

> Thanking you,
> Yours sincrely,
> AKSHAY M KULKARNI
>
> ________________________________
> From: Joshua Ulrich <josh.m.ulrich at gmail.com>
> Sent: Tuesday, July 26, 2022 1:56 AM
> To: akshay kulkarni <akshay_e4 at hotmail.com>
> Cc: R help Mailing list <r-help at r-project.org>
> Subject: Re: [R] removing dates from xts object..
>
> This is the most straightforward and general way I can think of quickly:
>
> library(xts)
> data(sample_matrix)
> x <- as.xts(sample_matrix, dateFormat = "Date")
> head(x)
> ##                Open     High      Low    Close
> ## 2007-01-02 50.03978 50.11778 49.95041 50.11778
> ## 2007-01-03 50.23050 50.42188 50.23050 50.39767
> ## 2007-01-04 50.42096 50.42096 50.26414 50.33236
> ## 2007-01-05 50.37347 50.37347 50.22103 50.33459
> ## 2007-01-06 50.24433 50.24433 50.11121 50.18112
> ## 2007-01-07 50.13211 50.21561 49.99185 49.99185
>
> y <- x[!(index(x) %in% as.Date(c("2007-01-04", "2007-01-05")))]
> head(y)
> ##                Open     High      Low    Close
> ## 2007-01-02 50.03978 50.11778 49.95041 50.11778
> ## 2007-01-03 50.23050 50.42188 50.23050 50.39767
> ## 2007-01-06 50.24433 50.24433 50.11121 50.18112
> ## 2007-01-07 50.13211 50.21561 49.99185 49.99185
> ## 2007-01-08 50.03555 50.10363 49.96971 49.98806
> ## 2007-01-09 49.99489 49.99489 49.80454 49.91333
>
> Best,
> Josh
>
>
> On Mon, Jul 25, 2022 at 3:20 PM akshay kulkarni <akshay_e4 at hotmail.com> wrote:
> >
> > Dear members,
> >                          I have:
> >
> >
> > > head(testOHLC)
> >            HINDUNILVR.NS.Open HINDUNILVR.NS.High HINDUNILVR.NS.Low HINDUNILVR.NS.Close
> > 2007-01-02             217.80             219.00            215.45              216.40
> > 2007-01-03             217.00             217.65            211.05              212.00
> > 2007-01-04             213.00             214.25            209.65              210.60
> > 2007-01-05             211.40             214.25            209.55              213.35
> > 2007-01-08             213.35             213.35            207.10              210.10
> > 2007-01-09             210.10             214.20            208.70              209.85
> >            HINDUNILVR.NS.Volume HINDUNILVR.NS.Adjusted
> > 2007-01-02              2626898               155.0396
> > 2007-01-03              4603921               151.8872
> > 2007-01-04              5486460               150.8841
> > 2007-01-05              5706066               152.8545
> > 2007-01-08              3760443               150.5260
> > 2007-01-09              5474633               150.3468
> >
> > AND:
> >
> >
> > > head(testOHLC["2007-01-09"])
> >            HINDUNILVR.NS.Open HINDUNILVR.NS.High HINDUNILVR.NS.Low HINDUNILVR.NS.Close
> > 2007-01-09              210.1              214.2             208.7              209.85
> >            HINDUNILVR.NS.Volume HINDUNILVR.NS.Adjusted
> > 2007-01-09              5474633               150.3468
> >
> > BUT:
> >
> > > head(testOHLC[-"2007-01-09"])
> >  Error in -"2007-01-09" : invalid argument to unary operator
> >
> > How do remove rows with certain dates in an xts object? This doesn't work:
> >
> > x[-dates] (however, this does: x[dates])
> >
> > Many thanks in advance......
> >
> > Yours sincreley
> > AKSHAY M KULKARNI
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Joshua Ulrich  |  about.me/joshuaulrich
> FOSS Trading  |  www.fosstrading.com



-- 
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Jul 25 22:51:29 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 25 Jul 2022 13:51:29 -0700
Subject: [R] problems in R in RHEL....
In-Reply-To: <PU4P216MB1568C281F7F0DC2E3AB426BDC8959@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
References: <PU4P216MB1568C281F7F0DC2E3AB426BDC8959@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
Message-ID: <7084575C-4589-4BF8-9098-71ADC5824A60@dcn.davis.ca.us>

Search for "Redhat" here: https://www.r-project.org/mail.html.

On July 25, 2022 11:35:39 AM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>Dear members,
>                          I am using R in the latest version of RHEL, in an AWS z1d instance. I have the following questions:
>
>
>  1.  Previously, when I started R by typing R in the bash prompt, a new screen would open with the R banner and the R console. But now the banner and the R console is opening in the same screen as the bash prompt. How do you revert to the previous behaviour?
>  2.   When I run fix(Myfun), it is opening a vi editor with the code of Myfun in it. But when I close the editor some of the code is left behind in the R console. How do you close the vi editor opened by fix command without leaving any code behind in the console?
>
>Many thanks in advance....
>
>Yours sincerely
>AKSHAY M KULKARNI
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @k@h@y_e4 @end|ng |rom hotm@||@com  Mon Jul 25 22:56:17 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Mon, 25 Jul 2022 20:56:17 +0000
Subject: [R] removing dates from xts object..
In-Reply-To: <CAPPM_gQwj7FmVt2avWX_UrZwv0d1gzzH6kPFycZphqZvvGtJqA@mail.gmail.com>
References: <PU4P216MB15685F4AFBD050B8EC21858EC8959@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <CAPPM_gScmr8nj1EyWEMeqMLiXK4ovihkcJ=rp0WHwLj_oETDJQ@mail.gmail.com>
 <PU4P216MB156861C1983669DB653B6B54C8959@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <CAPPM_gQwj7FmVt2avWX_UrZwv0d1gzzH6kPFycZphqZvvGtJqA@mail.gmail.com>
Message-ID: <PU4P216MB1568E30ABEE99D166AAD02BDC8959@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear Joshua,
                     Thanks a lot...

Yours sincrely,
AKSHAY M KULKARNI
________________________________
From: Joshua Ulrich <josh.m.ulrich at gmail.com>
Sent: Tuesday, July 26, 2022 2:20 AM
To: akshay kulkarni <akshay_e4 at hotmail.com>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] removing dates from xts object..

On Mon, Jul 25, 2022 at 3:34 PM akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>
> Dear joshua,
>                       THanks for the reply
>
> If I have a list of dates, it should be like this right?
>
> y <- x[!(index(x) %in% dates)]
>  where dates is a character vector of dates.
>

'dates' needs to be a vector of Date-classed values, not character

# character vector
dates_chr <- c("2007-01-04", "2007-01-05")
# Date vector -- use this below
dates_Date <- as.Date(dates_chr)

y <- x[!(index(x) %in% dates_Date)]

> Thanking you,
> Yours sincrely,
> AKSHAY M KULKARNI
>
> ________________________________
> From: Joshua Ulrich <josh.m.ulrich at gmail.com>
> Sent: Tuesday, July 26, 2022 1:56 AM
> To: akshay kulkarni <akshay_e4 at hotmail.com>
> Cc: R help Mailing list <r-help at r-project.org>
> Subject: Re: [R] removing dates from xts object..
>
> This is the most straightforward and general way I can think of quickly:
>
> library(xts)
> data(sample_matrix)
> x <- as.xts(sample_matrix, dateFormat = "Date")
> head(x)
> ##                Open     High      Low    Close
> ## 2007-01-02 50.03978 50.11778 49.95041 50.11778
> ## 2007-01-03 50.23050 50.42188 50.23050 50.39767
> ## 2007-01-04 50.42096 50.42096 50.26414 50.33236
> ## 2007-01-05 50.37347 50.37347 50.22103 50.33459
> ## 2007-01-06 50.24433 50.24433 50.11121 50.18112
> ## 2007-01-07 50.13211 50.21561 49.99185 49.99185
>
> y <- x[!(index(x) %in% as.Date(c("2007-01-04", "2007-01-05")))]
> head(y)
> ##                Open     High      Low    Close
> ## 2007-01-02 50.03978 50.11778 49.95041 50.11778
> ## 2007-01-03 50.23050 50.42188 50.23050 50.39767
> ## 2007-01-06 50.24433 50.24433 50.11121 50.18112
> ## 2007-01-07 50.13211 50.21561 49.99185 49.99185
> ## 2007-01-08 50.03555 50.10363 49.96971 49.98806
> ## 2007-01-09 49.99489 49.99489 49.80454 49.91333
>
> Best,
> Josh
>
>
> On Mon, Jul 25, 2022 at 3:20 PM akshay kulkarni <akshay_e4 at hotmail.com> wrote:
> >
> > Dear members,
> >                          I have:
> >
> >
> > > head(testOHLC)
> >            HINDUNILVR.NS.Open HINDUNILVR.NS.High HINDUNILVR.NS.Low HINDUNILVR.NS.Close
> > 2007-01-02             217.80             219.00            215.45              216.40
> > 2007-01-03             217.00             217.65            211.05              212.00
> > 2007-01-04             213.00             214.25            209.65              210.60
> > 2007-01-05             211.40             214.25            209.55              213.35
> > 2007-01-08             213.35             213.35            207.10              210.10
> > 2007-01-09             210.10             214.20            208.70              209.85
> >            HINDUNILVR.NS.Volume HINDUNILVR.NS.Adjusted
> > 2007-01-02              2626898               155.0396
> > 2007-01-03              4603921               151.8872
> > 2007-01-04              5486460               150.8841
> > 2007-01-05              5706066               152.8545
> > 2007-01-08              3760443               150.5260
> > 2007-01-09              5474633               150.3468
> >
> > AND:
> >
> >
> > > head(testOHLC["2007-01-09"])
> >            HINDUNILVR.NS.Open HINDUNILVR.NS.High HINDUNILVR.NS.Low HINDUNILVR.NS.Close
> > 2007-01-09              210.1              214.2             208.7              209.85
> >            HINDUNILVR.NS.Volume HINDUNILVR.NS.Adjusted
> > 2007-01-09              5474633               150.3468
> >
> > BUT:
> >
> > > head(testOHLC[-"2007-01-09"])
> >  Error in -"2007-01-09" : invalid argument to unary operator
> >
> > How do remove rows with certain dates in an xts object? This doesn't work:
> >
> > x[-dates] (however, this does: x[dates])
> >
> > Many thanks in advance......
> >
> > Yours sincreley
> > AKSHAY M KULKARNI
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Joshua Ulrich  |  about.me/joshuaulrich
> FOSS Trading  |  www.fosstrading.com<http://www.fosstrading.com>



--
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com<http://www.fosstrading.com>

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Mon Jul 25 23:01:06 2022
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Mon, 25 Jul 2022 21:01:06 +0000
Subject: [R] problems in R in RHEL....
In-Reply-To: <7084575C-4589-4BF8-9098-71ADC5824A60@dcn.davis.ca.us>
References: <PU4P216MB1568C281F7F0DC2E3AB426BDC8959@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>
 <7084575C-4589-4BF8-9098-71ADC5824A60@dcn.davis.ca.us>
Message-ID: <PU4P216MB156800F585C5031337D82C07C8959@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear jeff,
                 THanks a lot...

Yours sincerely
AKSHAY M KULKARNI

________________________________
From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Sent: Tuesday, July 26, 2022 2:21 AM
To: r-help at r-project.org <r-help at r-project.org>; akshay kulkarni <akshay_e4 at hotmail.com>; R help Mailing list <r-help at r-project.org>
Subject: Re: [R] problems in R in RHEL....

Search for "Redhat" here: https://www.r-project.org/mail.html.

On July 25, 2022 11:35:39 AM PDT, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>Dear members,
>                          I am using R in the latest version of RHEL, in an AWS z1d instance. I have the following questions:
>
>
>  1.  Previously, when I started R by typing R in the bash prompt, a new screen would open with the R banner and the R console. But now the banner and the R console is opening in the same screen as the bash prompt. How do you revert to the previous behaviour?
>  2.   When I run fix(Myfun), it is opening a vi editor with the code of Myfun in it. But when I close the editor some of the code is left behind in the R console. How do you close the vi editor opened by fix command without leaving any code behind in the console?
>
>Many thanks in advance....
>
>Yours sincerely
>AKSHAY M KULKARNI
>
>       [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

	[[alternative HTML version deleted]]


From j@de@shod@@ m@iii@g oii googiem@ii@com  Tue Jul 26 18:59:14 2022
From: j@de@shod@@ m@iii@g oii googiem@ii@com (j@de@shod@@ m@iii@g oii googiem@ii@com)
Date: Tue, 26 Jul 2022 17:59:14 +0100
Subject: [R] mgcv: relative risk from GAM with distributed lag
In-Reply-To: <CANg3_k9-p2Vx75BL+EgMKBZ-WkV8QbgdTzzN08NcNQJWaxGWdg@mail.gmail.com>
References: <CANg3_k-18ND8DrwfYJmGAN5hq18QweZFPp4GhCuHwB=XFGAFiA@mail.gmail.com>
 <CANg3_k9=GhwRjPyuLQBYmxD8Q-ksPrM52tS7DW0Cv3cpKn00Zw@mail.gmail.com>
 <e0f4395b-e1cf-abfa-e4d4-ee3dd519887e@bath.edu>
 <CANg3_k9-OKAK1VSirWSVGXQVMWoDeKoG7gN7mUfGfZdX4CnpgQ@mail.gmail.com>
 <CANg3_k9Nkok-Mr4DFOZ3=d1XtA-Pq1seW+E+KtWLttQUe_+XcQ@mail.gmail.com>
 <CANg3_k8C5W1RKL-gsGbr3556hSHq7+-oCSRCZR-ODEsN_7_MAA@mail.gmail.com>
 <59ce25f3-7168-2a30-f2fe-8710ed309731@bath.edu>
 <CANg3_k9-p2Vx75BL+EgMKBZ-WkV8QbgdTzzN08NcNQJWaxGWdg@mail.gmail.com>
Message-ID: <CANg3_k-qoOaiA_n1-+Jd8zuOCG=nvcTNAQpzFSWBs7u=SkFNFw@mail.gmail.com>

Dear all,

I am now looking into bootstrapping to get confidence/ credible
intervals on the relative risk estimates from my GAM, as I really
can't work out how  to use the lpmatrix in mgcv to achieve this.

(Unfortunately I have zero experience with bootstrapping -  after
having learned my way around GAMs, time series and distributed lag,
and having such promising results, I fear that getting a 'simple'
confidence interval is the thing that's actually going to stop me from
succesful/ on-time completion of this study/ thesis which is due very
soon. Perhaps the person that sent me an off-list response to an
earlier GAM question telling me I was utterly unqualified to undertake
this study, and that help 'from the internet' was unlikely to be
sufficient, is going to be right after all :(

Anyway, there is no alternative but to struggle on - how diffcult can
it be to get a CI? It's just a bit of shading around a curve  ;-)

My understanding is that I need to use block bootstrapping, because I
have time series data, and also because the GAM has a distributed lag
model. (Deaths may occur over several days following exposure to high
heat). I've come across tsbootstrap from the tseries package which
seems straighforward to use. As a reminder, my GAM in pseudocode is as
follows (and my query is below the code):

log(deaths) = s(time) + te(temperature, lag) + te(rain, lag)

where lagged variables are created as 7-column matrices as follows:

lagard <- function(x,n.lag=7) {
n <- length(x); X <- matrix(NA,n,n.lag)
for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
X
}
dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
deaths=simdat$deaths, time = simdat$time)
dat$temp <- lagard(simdat$temp)
dat$rain <- lagard(simdat$rain)
dat$humidity <- lagard(simdat$humidity)


I am planning to bootstrap the time variable and store, say, 500
replicates and use these replicates to subset the original data frame
that contains all weather variables (as well as time). I will then run
the model on each of these replicates, calculate the relative risk for
a range of temperatures relative to a reference temperature, calculate
the mean RR and its CI for each temperature.

There's a potential complication though because unlike in a GLM, in
order to obtain RR from a GAM, you need to make predictions from the
model, typically over the min-max range of the predictor variable of
interest. When I'm running the GAM 500 times, (I think) the
temperature values for which to predict should be the same for each
run of the model. However, the range of temperatures in each replicate
will differ, so I'll need to find that range of the predictor variable
that all replicates have in common. That I can do, but am worried that
the temperature range for which to predict for each run of the model
may get really narrow, due to the possibility that some replicates may
have a much narrower range for temperature than other replicates.

Am I right in thinking that I should use the same temperatures to
predict from for each run of the model, and that this may lead to a
much narrower range of temperatures to predict from than the
temperature range for which I developed the model?

If so, can anyone think of a solution?

(Alternatively, if anyone could provide an example on how to use the
lpmatrix in mgcv to calculate CIs for the relative risk, I'd be
immensely grateful too!!!)

Best wishes,

Jade

On Mon, 25 Jul 2022 at 20:17, jade.shodan at googlemail.com
<jade.shodan at googlemail.com> wrote:
>
> Hi Simon,
>
> Thanks for that example. It's been very useful!  It turned out that I
> created the prediction data for lag incorrectly. I've now managed to
> get perspective plots on the relative risk scale.
>
> I've now moved on to trying to get a plot of overall RR (y-axis) for a
> range of temperature values (x-axis), where RR for each temperature
> value is summed over lag periods 0-6 (see bottom plot page 2230 in
> Gasparrini's paper: https://doi.org/10.1002/sim.3940). This now seems
> straighforward with predict.gam() and type = "terms" in the way you
> suggested, but as acknowledged, that doesn't give me credible
> intervals. I've just been told that I MUST present results with
> confidence/ credible intervals (which is fair enough).
>
> For the last two days I've tried making the plot of overall RR with
> CIs, using the lpmatrix, as you suggested (looked it up in your book,
> and also an earlier post of yours with an example:
> https://stat.ethz.ch/pipermail/r-help/2012-May/314387.html), but I
> don't even understand how the book example can be applied to my
> problem, and I can't get the example in the online post to work. I
> don't really understand what an lpmatrix is, (other than that it seems
> to give me a value for each smooth term for each basis function?) or
> what the code in the book example does, so I have no idea what I am
> doing wroing, or what good results are supposed to look like. (I'm
> sorry, my background is in public health rather than statistics).
>
> I am hoping that I can ask for your (or anyone's)  help one final
> time. (Once I have the CI's, my modelling will be finished, and I will
> be out of everyone's hair with questions about GAMs... at least for a
> while!).
>
> Here's my code with reproducible data to get a plot of overall RR
> (y-axis) vs temperature (x-axis) where overall RR for each temperature
> value is obtained by summing RR over lag periods 0-6. But I have
> absolutely no idea how to use predict.gam with type = lpmatrix to get
> from this curve to one with CIs.
>
> library(mgcv)
>
> # simulate data
> set.seed(3) # make reproducible example
> simdat <- gamSim(1,400)
> g <- exp(simdat$f/5)
> simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
> simdat$time <- 1:400  # create time series
> names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
> "f0", "f1", "f2", "f3", "time")
>
> # lag function based on  Simon Wood's book (2017, p.349)
> lagard <- function(x,n.lag=7) {
> n <- length(x); X <- matrix(NA,n,n.lag)
> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> X
> }
>
> # create lagged variables as 7-column matrices
> dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
> deaths=simdat$deaths, time = simdat$time)
> dat$temp <- lagard(simdat$temp)
> dat$rain <- lagard(simdat$rain)
> dat$humidity <- lagard(simdat$humidity)
>
> mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
> te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
> family = nb, method = 'REML', select = TRUE)
>
> # create prediction data
> m1 <- 40 # number of points to predict across the range of each weather variable
> m2 <- 7 # number of lag periods
> n <- m1*m2 # total number of prediction points
>
> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T),
> length = m1)
> pred_lag <- 0:6
> pred_rain <- seq(min(dat$rain, na.rm = T), max(dat$rain, na.rm = T),
> length = m1)
> pred_humidity <- seq(min(dat$humidity, na.rm = T), max(dat$humidity,
> na.rm = T), length = m1)
> pred_time <- seq(min(dat$time, na.rm = T), max(dat$time, na.rm = T),
> length = m1)
>
> pd <- data.frame(temp=rep(pred_temp,m2),lag=rep(pred_lag,each=m1),rain=rep(pred_rain,m2),humidity=rep(pred_humidity,m2),time=rep(pred_time,m2))
>
> #  prediction data with reference temperature set to median
> temperature, all else equal to pd
> pred_temp0 <- median(dat$temp, na.rm=T)
> pd0 <- data.frame(temp=rep(pred_temp0,n),lag=rep(pred_lag,each=m1),rain=rep(pred_rain,m2),humidity=rep(pred_humidity,m2),time=rep(pred_time,m2))
>
> # make predictions
> predictions <- predict.gam(mod, newdata=pd, type = "terms")
> predictions0 <- predict.gam(mod, newdata=pd0, type = "terms")
>
> # calculate RR (relative to reference temperature set above)
> diff <- predictions[,2] - predictions0[,2]
> rr <- as.vector(exp(diff))
>
> # convert rr to matrix required for z argument for persp() plot
> rr_mat <- matrix(rr, m1, m2)
> persp(x=pred_temp,y=pred_lag,z=rr_mat,theta=30,phi=30,ticktype =
> "detailed", col="blue",zlab="RR")
>
> # create plots of overall RR (y-axis, summed over lags 0-6) vs
> temperature (x-axis)
> # convert predictions to matrix to allow summing over lags
> pmat <- matrix(predictions[,2],m1,m2)
> pmat0 <- matrix(predictions0[,2],m1,m2)
>
> # sum predictions over lags 0-6 for each temperature
> psummed <- rowSums(pmat)
> psummed0 <-rowSums(pmat0)
>
> # subtract the predictions for the reference temperature from the
> predictions for each temperature
> pref <- psummed-psummed0
>
> # exponentiating a difference on log-scale gives RR
> rr_overall <- exp(pref)
> plot(pred_temp, rr_overall, type = "l")
>
> # now create CIs for overall RR plot
> Xp <- predict(a1a_high_heat,newdata=pd,type="lpmatrix")
>
> # and how to procede now?????? So close to having results, and yet....
>
>
>
> On Sat, 23 Jul 2022 at 21:26, Simon Wood <simon.wood at bath.edu> wrote:
> >
> > I doubt you want a 7 by 1000 grid for your persp plot. Here's an example
> > of producing a persp plot using predict.gam and a custom grid. Since
> > only the effects of x1 and x2 are being plotted it doesn't matter what
> > x0 and x3 are set to (the model is additive after all). In your case
> > only one smooth term is involved of course.
> >
> > library(mgcv)
> > n <- 200
> > sig <- 2
> > dat <- gamSim(1,n=n,scale=sig)
> >
> > b <- gam(y~s(x0)+s(I(x1^2))+s(x2)+offset(x3),data=dat)
> >
> > m1 <- 20;m2 <- 30; n <- m1*m2
> > x1 <- seq(.2,.8,length=m1);x2 <- seq(.2,.8,length=m2) ## marginal values
> > for evaluation grid
> > df <- data.frame(x0=rep(.5,n),x1=rep(x1,m2),x2=rep(x2,each=m1),x3=rep(0,n))
> > pf <- predict(b,newdata=df,type="terms")
> >
> > persp(x1,x2,matrix(pf[,2]+pf[,3],m1,m2),theta=-130,col="blue",zlab="")
> >
> > On 23/07/2022 14:54, jade.shodan at googlemail.com wrote:
> > > Hi Simon and all,
> > >
> > > I've corrected some mistakes in setting up the prediction data frame
> > > (sorry, very stressed and sleep deprived due to closing in deadlines),
> > > but am having problems getting the perspective plot using persp().
> > >
> > > I've calculated relative risk (RR) but am having trouble getting the
> > > perspective (or contour) plot. persp() takes x and y vectors in
> > > ascending order, and z as a matrix.  I've seen the outer() function
> > > being used to facilitate perspective plots, but every example I've
> > > seen treats z as a function, to be evaluated at combinations for x and
> > > y. In my case, I already have observations for z. Not sure if I need
> > > to use outer(), and if so, how?
> > >
> > > My code for making predictions, getting RR and trying to get the plot
> > > is as follows (full reproducible example including model at the
> > > bottom). Can someone tell me what I'm doing wrong?
> > >
> > > ########### CODE ########################
> > >
> > > # create prediction data
> > > N <- 1000 # number of points for which to predict the smooths
> > > pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T),
> > > length = N)  # prediction data for temperature
> > > pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
> > > pred_rain <- seq(min(dat$rain, na.rm = T), max(dat$rain, na.rm = T), length = N)
> > > pred_humidity <- seq(min(dat$humidity, na.rm = T), max(dat$humidity,
> > > na.rm = T), length = N)
> > > pred_time <- seq(min(dat$time, na.rm = T), max(dat$time, na.rm = T), length = N)
> > >
> > > pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> > > pred_humidity, rain = pred_rain, time = pred_time)
> > >
> > > # create prediction data with reference temperature set to median temperature
> > > # identical to pd but now with all temperatures set to median value of
> > > temperature
> > > pred_temp0 <- rep(median(dat$temp, na.rm=T), N)
> > > pd0 <- data.frame(temp = pred_temp0, lag = pred_lag, humidity =
> > > pred_humidity, rain = pred_rain, time = pred_time)
> > >
> > > # make predictions
> > > predictions <- predict.gam(mod, pd, type = "terms")
> > > predictions0 <- predict.gam(mod, pd0, type = "terms")
> > >
> > > # calculate RR
> > > diff <- predictions[,2] - predictions0[,2]
> > > rr <- as.vector(exp(diff))
> > >
> > > # convert rr to matrix required for z argument for persp()
> > > rr_mat <- matrix(rr, nrow = N)
> > > persp(pred_temp, pred_lag, rr_mat)
> > >
> > > ######### ERROR MESSAGE ##################################
> > >
> > > The persp call results in the follow error:
> > >
> > > Error in persp.default(pred_temp, pred_lag, rr_mat) :
> > >    increasing 'x' and 'y' values expected
> > >
> > > I don't understand this because pred_temp and pred_lag ARE in ascending order.
> > >
> > > ############################################################
> > > FULL REPRODUCIBLE EXAMPLE
> > > ############################################################
> > >
> > > library(mgcv)
> > > set.seed(3) # make reproducible example
> > > simdat <- gamSim(1,400)
> > > g <- exp(simdat$f/5)
> > >
> > > simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
> > > simdat$time <- 1:400  # create time series
> > >
> > > names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
> > > "f0", "f1", "f2", "f3", "time")
> > >
> > > # lag function based on Wood (book 2017, p.349 and gamair package
> > > documentation p.54
> > > # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
> > > lagard <- function(x,n.lag=7) {
> > > n <- length(x); X <- matrix(NA,n,n.lag)
> > > for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> > > X
> > > }
> > >
> > > # set up lag, temp, rain and humidity as 7-column matrices
> > > # to create lagged variables
> > > dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
> > > deaths=simdat$deaths, time = simdat$time)
> > > dat$temp <- lagard(simdat$temp)
> > > dat$rain <- lagard(simdat$rain)
> > > dat$humidity <- lagard(simdat$humidity)
> > >
> > > mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
> > > te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
> > > family = nb, method = 'REML', select = TRUE)
> > >
> > > summary(mod)
> > > plot(mod, scheme = 1)  # perspective
> > > plot(mod, scheme = 2)  # contour
> > >
> > > # create prediction data
> > > N <- 1000 # number of points for which to predict the smooths
> > > pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
> > > pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
> > > pred_rain <- seq(min(dat$rain, na.rm = T), max(dat$rain, na.rm = T), length = N)
> > > pred_humidity <- seq(min(dat$humidity, na.rm = T), max(dat$humidity,
> > > na.rm = T), length = N)
> > > pred_time <- seq(min(dat$time, na.rm = T), max(dat$time, na.rm = T), length = N)
> > >
> > > pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> > > pred_humidity, rain = pred_rain, time = pred_time)
> > >
> > > # create prediction data with reference temperature set to median temperature
> > > # identical to pd but now with all temperatures set to median value of
> > > temperature
> > > pred_temp0 <- rep(median(dat$temp, na.rm=T), N)
> > > pd0 <- data.frame(temp = pred_temp0, lag = pred_lag, humidity =
> > > pred_humidity, rain = pred_rain, time = pred_time)
> > >
> > > # make predictions
> > > predictions <- predict.gam(mod, pd, type = "terms")
> > > predictions0 <- predict.gam(mod, pd0, type = "terms")
> > >
> > > # calculate RR
> > > diff <- predictions[,2] - predictions0[,2]
> > > rr <- as.vector(exp(diff))
> > >
> > > # convert rr to matrix required for z argument for persp()
> > > rr_mat <- matrix(rr, nrow = N)
> > > persp(pred_temp, pred_lag, rr_mat)
> > >
> > >
> > >
> > >
> > > On Fri, 22 Jul 2022 at 13:29, jade.shodan at googlemail.com
> > > <jade.shodan at googlemail.com> wrote:
> > >> I made a small error in the code below (not checking for NAs which are
> > >> introduced by the lag function). However, this doesn't solve the issue
> > >> I raised).
> > >>
> > >> So here's the problem (with corrected code) again:
> > >>
> > >> I'm not sure what to do with the 'time' variable. (I don't want to
> > >> make predictions for specific points in time). I coded as follows
> > >> (full reproducible example at bottom of email), but get a warning and
> > >> error:
> > >>
> > >>
> > >> N <- 1000     # number of points for smooth to be predicted
> > >> # new temperatures and lags for prediction
> > >> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
> > >> pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)      ## IS IT CORRECT TO SET
> > >> UP LAG LIKE THIS?
> > >>
> > >> # not sure if these covariates are required with type = "terms"
> > >> pred_humidity <- rep(median(dat$humidity, na.rm = T), N)
> > >> pred_rain <- rep(median(dat$rain, na.rm = T), N)
> > >> pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> > >> pred_humidity, rain = pred_rain)
> > >>
> > >> predictions <- predict(mod, pd, type = "terms")
> > >>
> > >>
> > >> The predict line creates the following warning and error:
> > >>
> > >> Warning in predict.gam(mod, pd, type = "terms") :
> > >>    not all required variables have been supplied in  newdata!
> > >>
> > >> Error in model.frame.default(ff, data = newdata, na.action = na.act) :
> > >>    object is not a matrix
> > >>
> > >>
> > >> For ease of reference, I've (re)included the full reproducible example:
> > >>
> > >> library(mgcv)
> > >> set.seed(3) # make reproducible example
> > >> simdat <- gamSim(1,400)
> > >> g <- exp(simdat$f/5)
> > >>
> > >> simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
> > >> simdat$time <- 1:400  # create time series
> > >>
> > >> names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
> > >> "f0", "f1", "f2", "f3", "time")
> > >>
> > >> # lag function based on Wood (book 2017, p.349 and gamair package
> > >> documentation p.54
> > >> # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
> > >> lagard <- function(x,n.lag=7) {
> > >> n <- length(x); X <- matrix(NA,n,n.lag)
> > >> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> > >> X
> > >> }
> > >>
> > >> # set up lag, temp, rain and humidity as 7-column matrices
> > >> # to create lagged variables
> > >> dat <- list(lag=matrix(0:6,nrow(simda
> > >> t),7,byrow=TRUE),
> > >> deaths=simdat$deaths, time = simdat$time)
> > >> dat$temp <- lagard(simdat$temp)
> > >> dat$rain <- lagard(simdat$rain)
> > >> dat$humidity <- lagard(simdat$humidity)
> > >>
> > >> mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
> > >> te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
> > >> family = nb, method = 'REML', select = TRUE)
> > >>
> > >> # create prediction data
> > >> N <- 1000 # number of points for which to predict the smooths
> > >> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
> > >> pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
> > >> pred_humidity <- rep(median(dat$humidity), N)
> > >> pred_rain <- rep(median(dat$rain), N)
> > >> pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> > >> pred_humidity, rain = pred_rain)
> > >>
> > >> # make predictions
> > >> predictions <- predict(mod, pd, type = "terms")
> > >>
> > >> On Fri, 22 Jul 2022 at 12:47, jade.shodan at googlemail.com
> > >> <jade.shodan at googlemail.com> wrote:
> > >>> Hi Simon,
> > >>>
> > >>> Thanks for the pointers! But I'm not sure what to do with the 'time'
> > >>> variable. (I don't want to make predictions for specific points in
> > >>> time). I coded as follows (full reproducible example at bottom of
> > >>> email), but get a warning and error:
> > >>>
> > >>>
> > >>> N <- 1000     # number of points for smooth to be predicted
> > >>> # new temperatures and lags for prediction
> > >>> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
> > >>> pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)      ## IS IT CORRECT TO SET
> > >>> UP LAG LIKE THIS?
> > >>>
> > >>> # not sure if these covariates are required with type = "terms"
> > >>> pred_humidity <- rep(median(dat$humidity), N)
> > >>> pred_rain <- rep(median(dat$rain), N)
> > >>> pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> > >>> pred_humidity, rain = pred_rain)
> > >>>
> > >>> predictions <- predict(mod, pd, type = "terms")
> > >>>
> > >>>
> > >>> The predict line creates the following warning and error:
> > >>>
> > >>> Warning in predict.gam(mod, pd, type = "terms") :
> > >>>    not all required variables have been supplied in  newdata!
> > >>>
> > >>> Error in model.frame.default(ff, data = newdata, na.action = na.act) :
> > >>>    object is not a matrix
> > >>>
> > >>>
> > >>> For ease of reference, I've (re)included the full reproducible example:
> > >>>
> > >>> library(mgcv)
> > >>> set.seed(3) # make reproducible example
> > >>> simdat <- gamSim(1,400)
> > >>> g <- exp(simdat$f/5)
> > >>>
> > >>> simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
> > >>> simdat$time <- 1:400  # create time series
> > >>>
> > >>> names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
> > >>> "f0", "f1", "f2", "f3", "time")
> > >>>
> > >>> # lag function based on Wood (book 2017, p.349 and gamair package
> > >>> documentation p.54
> > >>> # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
> > >>> lagard <- function(x,n.lag=7) {
> > >>> n <- length(x); X <- matrix(NA,n,n.lag)
> > >>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> > >>> X
> > >>> }
> > >>>
> > >>> # set up lag, temp, rain and humidity as 7-column matrices
> > >>> # to create lagged variables
> > >>> dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
> > >>> deaths=simdat$deaths, time = simdat$time)
> > >>> dat$temp <- lagard(simdat$temp)
> > >>> dat$rain <- lagard(simdat$rain)
> > >>> dat$humidity <- lagard(simdat$humidity)
> > >>>
> > >>> mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
> > >>> te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
> > >>> family = nb, method = 'REML', select = TRUE)
> > >>>
> > >>> # create prediction data
> > >>> N <- 1000 # number of points for which to predict the smooths
> > >>> pred_temp <- seq(min(dat$temp, na.rm = T), max(dat$temp, na.rm = T), length = N)
> > >>> pred_lag <- rep(c(0, 1,2,3,4,5,6),each=N)
> > >>> pred_humidity <- rep(median(dat$humidity), N)
> > >>> pred_rain <- rep(median(dat$rain), N)
> > >>> pd <- data.frame(temp = pred_temp, lag = pred_lag, humidity =
> > >>> pred_humidity, rain = pred_rain)
> > >>>
> > >>> # make predictions
> > >>> predictions <- predict(mod, pd, type = "terms")
> > >>>
> > >>>
> > >>> On Fri, 22 Jul 2022 at 09:54, Simon Wood <simon.wood at bath.edu> wrote:
> > >>>>
> > >>>> On 21/07/2022 15:19, jade.shodan--- via R-help wrote:
> > >>>>> Hello everyone (incl. Simon Wood?),
> > >>>>>
> > >>>>> I'm not sure that my original question (see below, including
> > >>>>> reproducible example) was as clear as it could have been. To clarify,
> > >>>>> what I would to like to get is:
> > >>>>>
> > >>>>> 1) a perspective plot of temperature x lag x relative risk.  I know
> > >>>>> how to use plot.gam and vis.gam but don't know how to get plots on the
> > >>>>> relative risk scale as opposed to  "response" or "link".
> > >>>> - You are on the log scale so I think that all you need to do is to use
> > >>>> 'predict.gam', with 'type = "terms"' to  get the predictions for the
> > >>>> te(temp, lag) term over the required grid of lags and temperatures.
> > >>>> Suppose the dataframe of prediction data is 'pd'. Now produce pd0, which
> > >>>> is identical to pd, except that the temperatures are all set to the
> > >>>> reference temperature. Use predict.gam to predict te(temp,lag) from pd0.
> > >>>> Now the exponential of the difference between the first and second
> > >>>> predictions is the required RR, which you can plot using 'persp',
> > >>>> 'contour', 'image' or whatever. If you need credible intervals see pages
> > >>>> 341-343 of my 'GAMs: An intro with R' book (2nd ed).
> > >>>>
> > >>>>> 2) a plot of relative risk (accumulated across all lags) vs
> > >>>>> temperature, given a reference temperature. An example of such a plot
> > >>>>> can be found in figure 2 (bottom) of this paper by Gasparrini et al:
> > >>>>> https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3940
> > >>>> - I guess this only makes sense if you have the same temperature at all
> > >>>> lags. So this time produce a data.frame with each desired temperature
> > >>>> repeated for each lag: 'pd1'. Again use predict.gam(...,type="terms").
> > >>>> Then sum the predictions over lags for each temperature, subtract the
> > >>>> minimum, and take the exponential. Same as above for CIs.
> > >>>>
> > >>>> best,
> > >>>>
> > >>>> Simon
> > >>>>
> > >>>>> I've seen Simon Wood's response to a related issue here:
> > >>>>> https://stat.ethz.ch/pipermail/r-help/2012-May/314387.html
> > >>>>> However, I'm not sure how to apply this to time series data with
> > >>>>> distributed lag, to get the above mentioned figures.
> > >>>>>
> > >>>>> Would be really grateful for suggestions!
> > >>>>>
> > >>>>> Jade
> > >>>>>
> > >>>>> On Tue, 19 Jul 2022 at 16:07, jade.shodan at googlemail.com
> > >>>>> <jade.shodan at googlemail.com> wrote:
> > >>>>>> Dear list members,
> > >>>>>>
> > >>>>>> Does anyone know how to obtain a relative risk/ risk ratio from a GAM
> > >>>>>> with a distributed lag model implemented in mgcv? I have a GAM
> > >>>>>> predicting daily deaths from time series data consisting of daily
> > >>>>>> temperature, humidity and rainfall. The GAM includes a distributed lag
> > >>>>>> model because deaths may occur over several days following a high heat
> > >>>>>> day.
> > >>>>>>
> > >>>>>> What I'd like to do is compute (and plot) the relative risk
> > >>>>>> (accumulated across all lags) for a given temperature vs the
> > >>>>>> temperature at which the risk is lowest, with corresponding confidence
> > >>>>>> intervals. I am aware of the predict.gam function but am not sure if
> > >>>>>> and how it should be used in this case. (Additionally, I'd also like
> > >>>>>> to plot the relative risk for different lags separately).
> > >>>>>>
> > >>>>>> I apologise if this seems trivial to some. (Actually, I hope it is,
> > >>>>>> because that might mean I get a solution!) I've been looking for
> > >>>>>> examples on how to do this, but found nothing so far. Suggestions
> > >>>>>> would be very much appreciated!
> > >>>>>>
> > >>>>>> Below is a reproducible example with the GAM:
> > >>>>>>
> > >>>>>> library(mgcv)
> > >>>>>> set.seed(3) # make reproducible example
> > >>>>>> simdat <- gamSim(1,400) # simulate data
> > >>>>>> g <- exp(simdat$f/5)
> > >>>>>> simdat$y <- rnbinom(g,size=3,mu=g)  # negative binomial response var
> > >>>>>> simdat$time <- 1:400  # create time series
> > >>>>>> names(simdat) <- c("deaths", "temp", "humidity", "rain", "x3", "f",
> > >>>>>> "f0", "f1", "f2", "f3", "time")
> > >>>>>>
> > >>>>>> # lag function based on Simon Wood (book 2017, p.349 and gamair
> > >>>>>> package documentation p.54
> > >>>>>> # https://cran.rstudio.com/web/packages/gamair/gamair.pdf)
> > >>>>>> lagard <- function(x,n.lag=7) {
> > >>>>>> n <- length(x); X <- matrix(NA,n,n.lag)
> > >>>>>> for (i in 1:n.lag) X[i:n,i] <- x[i:n-i+1]
> > >>>>>> X
> > >>>>>> }
> > >>>>>>
> > >>>>>> # set up lag, temp, rain and humidity as 7-column matrices
> > >>>>>> # to create lagged variables - based on Simon Wood's example
> > >>>>>> dat <- list(lag=matrix(0:6,nrow(simdat),7,byrow=TRUE),
> > >>>>>> deaths=simdat$deaths, time = simdat$time)
> > >>>>>> dat$temp <- lagard(simdat$temp)
> > >>>>>> dat$rain <- lagard(simdat$rain)
> > >>>>>> dat$humidity <- lagard(simdat$humidity)
> > >>>>>>
> > >>>>>> mod <- gam(deaths~s(time, k=70) + te(temp, lag, k=c(12, 4)) +
> > >>>>>> te(humidity, lag, k=c(12, 4)) + te(rain, lag, k=c(12, 4)), data = dat,
> > >>>>>> family = nb, method = 'REML', select = TRUE)
> > >>>>>>
> > >>>>>> summary(mod)
> > >>>>>> plot(mod, scheme = 1)
> > >>>>>> plot(mod, scheme = 2)
> > >>>>>>
> > >>>>>> Thanks for any suggestions you may have,
> > >>>>>>
> > >>>>>> Jade
> > >>>>> ______________________________________________
> > >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >>>>> and provide commented, minimal, self-contained, reproducible code.
> > >>>> --
> > >>>> Simon Wood, School of Mathematics, University of Edinburgh,
> > >>>> https://www.maths.ed.ac.uk/~swood34/
> > >>>>
> > --
> > Simon Wood, School of Mathematics, University of Edinburgh,
> > https://www.maths.ed.ac.uk/~swood34/
> >


From r@njeetjh@||tkgp @end|ng |rom gm@||@com  Mon Jul 25 15:02:33 2022
From: r@njeetjh@||tkgp @end|ng |rom gm@||@com (Ranjeet Kumar Jha)
Date: Mon, 25 Jul 2022 18:32:33 +0530
Subject: [R] Need to insert various rows of data from a data frame after
 particular rows from another dataframe
Message-ID: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>

Hello Everyone,

I have dataset in a particular format in "dacnet_yield_update till
2019.xlsx" file, where I need to insert the data of rows 2018-2019 and
2019-2020 for the districts those data are available in "Kharif crops
yield_18-19.xlsx".  I need to insert these two rows of data belonging to
every district, if data is available in a later excel file, just after the
particular crop group data for the particular district.

I have put the data file in the given link.
https://drive.google.com/drive/u/0/folders/1dNmGTI8_c9PK1QqmfIjnpbyzuiCXgxFC

Please help solving this problem.

Regards and Thanks,
Ranjeet

	[[alternative HTML version deleted]]


From k@ri@iuchs m@iii@g oii sbg@@t  Mon Jul 25 15:20:59 2022
From: k@ri@iuchs m@iii@g oii sbg@@t (k@ri@iuchs m@iii@g oii sbg@@t)
Date: Mon, 25 Jul 2022 15:20:59 +0200
Subject: [R] Request concerning use in book
Message-ID: <008701d8a029$61bd5150$2537f3f0$@sbg.at>

Dear colleagues!

 

We plan to introduce the software R in chapter "subject-specific software in
mathematics" in our book "TEACHING/LEARNING MEDIUM COMPUTER".

 

As we will use screenshots and Plots generated by R we ask if there will be
any fees for using this elements in our book.

 

Yours

Dr. Karl Fuchs, Austria


	[[alternative HTML version deleted]]


From @kw@|mmo @end|ng |rom gm@||@com  Wed Jul 27 04:38:30 2022
From: @kw@|mmo @end|ng |rom gm@||@com (Andrew Simmons)
Date: Tue, 26 Jul 2022 22:38:30 -0400
Subject: [R] Request concerning use in book
In-Reply-To: <008701d8a029$61bd5150$2537f3f0$@sbg.at>
References: <008701d8a029$61bd5150$2537f3f0$@sbg.at>
Message-ID: <CAPcHnpSJ0JTSXUe1z_sny2UgDOEggrfXd9MMHb1koOVh6kMM7A@mail.gmail.com>

There's no fees for using R in publications, but most people will include
citations for the R language and any R packages used.

You can use function citation() (it might be called citations(), I'm not
entirely sure) to generate said citations.

On Tue, Jul 26, 2022, 22:31 <karl.fuchs at sbg.at> wrote:

> Dear colleagues!
>
>
>
> We plan to introduce the software R in chapter "subject-specific software
> in
> mathematics" in our book "TEACHING/LEARNING MEDIUM COMPUTER".
>
>
>
> As we will use screenshots and Plots generated by R we ask if there will be
> any fees for using this elements in our book.
>
>
>
> Yours
>
> Dr. Karl Fuchs, Austria
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From twoo|m@n @end|ng |rom ont@rgettek@com  Wed Jul 27 05:06:29 2022
From: twoo|m@n @end|ng |rom ont@rgettek@com (Tom Woolman)
Date: Tue, 26 Jul 2022 23:06:29 -0400
Subject: [R] Request concerning use in book
In-Reply-To: <CAPcHnpSJ0JTSXUe1z_sny2UgDOEggrfXd9MMHb1koOVh6kMM7A@mail.gmail.com>
References: <008701d8a029$61bd5150$2537f3f0$@sbg.at>
 <CAPcHnpSJ0JTSXUe1z_sny2UgDOEggrfXd9MMHb1koOVh6kMM7A@mail.gmail.com>
Message-ID: <ffc674abd604d2daee9207e5158e242e@ontargettek.com>

I had a paper published about 2 weeks ago that cited R in the references 
section:

Woolman, T. A., & Pickard, J. L. (2022). Gradient Descent Machine 
Learning with Equivalency Testing for Non-Subject Dependent Applications 
in Human Activity Recognition.?EAI Endorsed Transactions on 
Context-aware Systems and Applications,?8, e7-e7.

The citation in the paper that I used for R using APA format was:

R Core Team (2021). R: A language and environment for statistical 
computing. R Foundation for Statistical Computing, Vienna, Austria. 
URL?https://www.R-project.org/.



On 2022-07-26 22:38, Andrew Simmons wrote:
> There's no fees for using R in publications, but most people will 
> include
> citations for the R language and any R packages used.
> 
> You can use function citation() (it might be called citations(), I'm 
> not
> entirely sure) to generate said citations.
> 
> On Tue, Jul 26, 2022, 22:31 <karl.fuchs at sbg.at> wrote:
> 
>> Dear colleagues!
>> 
>> 
>> 
>> We plan to introduce the software R in chapter "subject-specific 
>> software
>> in
>> mathematics" in our book "TEACHING/LEARNING MEDIUM COMPUTER".
>> 
>> 
>> 
>> As we will use screenshots and Plots generated by R we ask if there 
>> will be
>> any fees for using this elements in our book.
>> 
>> 
>> 
>> Yours
>> 
>> Dr. Karl Fuchs, Austria
>> 
>> 
>>         [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Jul 27 05:23:01 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 26 Jul 2022 20:23:01 -0700
Subject: [R] Request concerning use in book
In-Reply-To: <008701d8a029$61bd5150$2537f3f0$@sbg.at>
References: <008701d8a029$61bd5150$2537f3f0$@sbg.at>
Message-ID: <16B37C63-C6D2-439E-87F3-9FEAAC293052@dcn.davis.ca.us>

Please remember that many outputs generated using R are also using contributed packages. Please cite any such packages that you used along with the citation for R itself.

On July 25, 2022 6:20:59 AM PDT, karl.fuchs at sbg.at wrote:
>Dear colleagues!
>
> 
>
>We plan to introduce the software R in chapter "subject-specific software in
>mathematics" in our book "TEACHING/LEARNING MEDIUM COMPUTER".
>
> 
>
>As we will use screenshots and Plots generated by R we ask if there will be
>any fees for using this elements in our book.
>
> 
>
>Yours
>
>Dr. Karl Fuchs, Austria
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From po|c1410 @end|ng |rom gm@||@com  Wed Jul 27 08:22:20 2022
From: po|c1410 @end|ng |rom gm@||@com (CALUM POLWART)
Date: Wed, 27 Jul 2022 07:22:20 +0100
Subject: [R] Need to insert various rows of data from a data frame after
 particular rows from another dataframe
In-Reply-To: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
References: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
Message-ID: <CA+etgPkqeSQ1HULmva_FaQuM1ThOtd_jBw35MoXoVYQuwW0muw@mail.gmail.com>

Not very clear what you are trying to do. But I'd have thought possibly
dplyr left_join might be a solution for you.  The base R equivalent is
merge().

It might be a rbind or cbind can do it too.

On Wed, 27 Jul 2022, 03:30 Ranjeet Kumar Jha, <ranjeetjhaiitkgp at gmail.com>
wrote:

> Hello Everyone,
>
> I have dataset in a particular format in "dacnet_yield_update till
> 2019.xlsx" file, where I need to insert the data of rows 2018-2019 and
> 2019-2020 for the districts those data are available in "Kharif crops
> yield_18-19.xlsx".  I need to insert these two rows of data belonging to
> every district, if data is available in a later excel file, just after the
> particular crop group data for the particular district.
>
> I have put the data file in the given link.
>
> https://drive.google.com/drive/u/0/folders/1dNmGTI8_c9PK1QqmfIjnpbyzuiCXgxFC
>
> Please help solving this problem.
>
> Regards and Thanks,
> Ranjeet
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From petr@p|k@| @end|ng |rom prechez@@cz  Wed Jul 27 08:30:34 2022
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Wed, 27 Jul 2022 06:30:34 +0000
Subject: [R] Need to insert various rows of data from a data frame after
 particular rows from another dataframe
In-Reply-To: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
References: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
Message-ID: <a172da90dd2448779581da58741b913e@SRVEXCHCM1302.precheza.cz>

Hi.

>From what you say, plain "rbind" could be used, if the columns in both sets
are the same and in the same order. After that you can reorder the resulting
data frame as you wish by "order". AFAIK for most functions row order in
data frame does not matter.

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ranjeet Kumar Jha
> Sent: Monday, July 25, 2022 3:03 PM
> To: R-help <r-help at r-project.org>
> Subject: [R] Need to insert various rows of data from a data frame after
> particular rows from another dataframe
> 
> Hello Everyone,
> 
> I have dataset in a particular format in "dacnet_yield_update till
2019.xlsx" file,
> where I need to insert the data of rows 2018-2019 and
> 2019-2020 for the districts those data are available in "Kharif crops
yield_18-
> 19.xlsx".  I need to insert these two rows of data belonging to every
district, if
> data is available in a later excel file, just after the particular crop
group data for
> the particular district.
> 
> I have put the data file in the given link.
> https://drive.google.com/drive/u/0/folders/1dNmGTI8_c9PK1QqmfIjnpbyzuiC
> XgxFC
> 
> Please help solving this problem.
> 
> Regards and Thanks,
> Ranjeet
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From po|c1410 @end|ng |rom gm@||@com  Wed Jul 27 08:37:13 2022
From: po|c1410 @end|ng |rom gm@||@com (CALUM POLWART)
Date: Wed, 27 Jul 2022 07:37:13 +0100
Subject: [R] Need to insert various rows of data from a data frame after
 particular rows from another dataframe
In-Reply-To: <CAJfMH3qj06VHRxky8q4M7WTJqGkw3czF2bKQ6kD_CbMgS7Bz+A@mail.gmail.com>
References: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
 <CA+etgPkqeSQ1HULmva_FaQuM1ThOtd_jBw35MoXoVYQuwW0muw@mail.gmail.com>
 <CAJfMH3qj06VHRxky8q4M7WTJqGkw3czF2bKQ6kD_CbMgS7Bz+A@mail.gmail.com>
Message-ID: <CA+etgPk6T-R1uBVV6Pwz_vStfORjDQ6T4o0PLemS-nEaPScfCQ@mail.gmail.com>

I don't open links to random data files. Sorry.

It sounds like you need to subset or filter your later data to get what you
want and then merge it in.

Good luck finding someone who will open a random excel sheet and then write
your code for you

On Wed, 27 Jul 2022, 07:32 Ranjeet Kumar Jha, <ranjeetjhaiitkgp at gmail.com>
wrote:

> Hi Calum,
>
> Thanks for your reply !
> Actually, I need to bring the 2018-2019 and 2019-2020 yield data
> corresponding to each crop in every district under a state from the "Kharif
> crop yield 18-19.xls" into "dacnet" file, given in the link. These two
> years of data from Kharif file should be brought into dacnet file after
> 2017 or whichever latest previous year yield that crop contains for each
> district in a particular state. Since Kharif file has different format and
> 11-12 crops with large number of districts in each state, I am unable to
> get the data from Kharif to dacnet into same format. So basically, kharif
> file yield data for 2018-2019 needs to be brought into dacnet file in the
> same format as it is in dacnet file.
> I hope it is clear now.
>
> I will really appreciate your help!
>
>
> Regards and Thanks,
>
> On Wed, Jul 27, 2022 at 11:52 AM CALUM POLWART <polc1410 at gmail.com> wrote:
>
>> Not very clear what you are trying to do. But I'd have thought possibly
>> dplyr left_join might be a solution for you.  The base R equivalent is
>> merge().
>>
>> It might be a rbind or cbind can do it too.
>>
>> On Wed, 27 Jul 2022, 03:30 Ranjeet Kumar Jha, <ranjeetjhaiitkgp at gmail.com>
>> wrote:
>>
>>> Hello Everyone,
>>>
>>> I have dataset in a particular format in "dacnet_yield_update till
>>> 2019.xlsx" file, where I need to insert the data of rows 2018-2019 and
>>> 2019-2020 for the districts those data are available in "Kharif crops
>>> yield_18-19.xlsx".  I need to insert these two rows of data belonging to
>>> every district, if data is available in a later excel file, just after
>>> the
>>> particular crop group data for the particular district.
>>>
>>> I have put the data file in the given link.
>>>
>>> https://drive.google.com/drive/u/0/folders/1dNmGTI8_c9PK1QqmfIjnpbyzuiCXgxFC
>>>
>>> Please help solving this problem.
>>>
>>> Regards and Thanks,
>>> Ranjeet
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>
> --
> Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
> https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
> -----------------------------------------------------------
> Email: *ranjeetjhaiitkgp at gmail.com <ranjeetjhaiitkgp at gmail.com>*
>
>
> *"Simple Heart, Humble Attitude and Surrender to Supreme Being make our
> lives beautiful!"*
>
>

	[[alternative HTML version deleted]]


From petr@p|k@| @end|ng |rom prechez@@cz  Wed Jul 27 09:43:41 2022
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Wed, 27 Jul 2022 07:43:41 +0000
Subject: [R] Need to insert various rows of data from a data frame after
 particular rows from another dataframe
In-Reply-To: <CAJfMH3rF9RmJee+G0Y+3Re8U9BsxVkUhoZJhgQyGwYxrUxCSJQ@mail.gmail.com>
References: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
 <a172da90dd2448779581da58741b913e@SRVEXCHCM1302.precheza.cz>
 <CAJfMH3rF9RmJee+G0Y+3Re8U9BsxVkUhoZJhgQyGwYxrUxCSJQ@mail.gmail.com>
Message-ID: <9c4386179ab24616915d2296b70a28d8@SRVEXCHCM1302.precheza.cz>

Hi.

 

?is not working? is extremelly vague.

 

1.

What do you expect this code do?

 

for(cr in seq_along(dacnet_17$district)){

    match(arhar_18$district, dacnet_17$district)

}

 

See ?match and maybe also ??for? and try this

 

x <- letters[1:5]

y <- sample(letters, 100, replace=T)

match(x,y)

[1] 45 16 24 13 71

for(i in 1:3) match(x,y)

 

2.

rbind works as expeceted

 

arhar_18 <- data.frame(a=1:10, b=50, c=letters[1:10])

dacnet_17 <- data.frame(a=11:20, b=100, c=sample(letters,10))

df3=rbind(arhar_18,dacnet_17)

 

if your data has common column order and type.

 

To get more specific answer you need to ask specific question preferably with some data included (most preferably by dput command) and error message.

 

Cheers

Petr

 

 

From: Ranjeet Kumar Jha <ranjeetjhaiitkgp at gmail.com> 
Sent: Wednesday, July 27, 2022 8:35 AM
To: PIKAL Petr <petr.pikal at precheza.cz>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] Need to insert various rows of data from a data frame after particular rows from another dataframe

 

Hi Petr,

 

I used r-bind but it's not working.

Here is the code:

 

arhar_18<-read.csv("D:/Ranjeet/IAMV6/input/yield/kharif_18-19_yield/Kharif_2018/arhar_18.csv")

dacnet_17<-read.csv("D:/Ranjeet/IAMV6/input/yield/dacnet_yield_update till 2019.csv")

 

for(cr in seq_along(dacnet_17$district)){

    match(arhar_18$district, dacnet_17$district)

}

 

df3=rbind(arhar_18,dacnet_17)

df3=df3[order(df3$district,df3$year),]

x<-write.csv(df3,"df3.csv")

view(x)

 

On Wed, Jul 27, 2022 at 12:00 PM PIKAL Petr <petr.pikal at precheza.cz <mailto:petr.pikal at precheza.cz> > wrote:

Hi.

>From what you say, plain "rbind" could be used, if the columns in both sets
are the same and in the same order. After that you can reorder the resulting
data frame as you wish by "order". AFAIK for most functions row order in
data frame does not matter.

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org <mailto:r-help-bounces at r-project.org> > On Behalf Of Ranjeet Kumar Jha
> Sent: Monday, July 25, 2022 3:03 PM
> To: R-help <r-help at r-project.org <mailto:r-help at r-project.org> >
> Subject: [R] Need to insert various rows of data from a data frame after
> particular rows from another dataframe
> 
> Hello Everyone,
> 
> I have dataset in a particular format in "dacnet_yield_update till
2019.xlsx" file,
> where I need to insert the data of rows 2018-2019 and
> 2019-2020 for the districts those data are available in "Kharif crops
yield_18-
> 19.xlsx".  I need to insert these two rows of data belonging to every
district, if
> data is available in a later excel file, just after the particular crop
group data for
> the particular district.
> 
> I have put the data file in the given link.
> https://drive.google.com/drive/u/0/folders/1dNmGTI8_c9PK1QqmfIjnpbyzuiC
> XgxFC
> 
> Please help solving this problem.
> 
> Regards and Thanks,
> Ranjeet
> 
>       [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.




 

-- 

Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)

https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56

-----------------------------------------------------------
Email:  <mailto:ranjeetjhaiitkgp at gmail.com> ranjeetjhaiitkgp at gmail.com





"Simple Heart, Humble Attitude and Surrender to Supreme Being make our lives beautiful!"




From petr@p|k@| @end|ng |rom prechez@@cz  Wed Jul 27 11:48:52 2022
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Wed, 27 Jul 2022 09:48:52 +0000
Subject: [R] Need to insert various rows of data from a data frame after
 particular rows from another dataframe
In-Reply-To: <CAJfMH3oUD720UKT0DiuLWcPr4OvBEdDPeyxPwgpgXZhg3Pz5YA@mail.gmail.com>
References: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
 <a172da90dd2448779581da58741b913e@SRVEXCHCM1302.precheza.cz>
 <CAJfMH3rF9RmJee+G0Y+3Re8U9BsxVkUhoZJhgQyGwYxrUxCSJQ@mail.gmail.com>
 <9c4386179ab24616915d2296b70a28d8@SRVEXCHCM1302.precheza.cz>
 <CAJfMH3oUD720UKT0DiuLWcPr4OvBEdDPeyxPwgpgXZhg3Pz5YA@mail.gmail.com>
Message-ID: <e2f1cc08e8f04cb9bbf4ecb0a7362f40@SRVEXCHCM1302.precheza.cz>

Hallo, 

I do not understand what you really want and you do not help much. 

No error, no data, vague description of your problem, no effort to explain it better. Only you can see your data, only you can see the error message so we are clueless.

Cheers
Petr


From: Ranjeet Kumar Jha <ranjeetjhaiitkgp at gmail.com> 
Sent: Wednesday, July 27, 2022 11:01 AM
To: PIKAL Petr <petr.pikal at precheza.cz>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] Need to insert various rows of data from a data frame after particular rows from another dataframe

yeah rbind works find bur dataframe 2 has different format and then rbind can be used with first dataframe. But here I am struggling to bring the 2nd dataframe in the same format as the 1st one.

On Wed, Jul 27, 2022 at 1:13 PM PIKAL Petr <mailto:petr.pikal at precheza.cz> wrote:
Hi.
 
?is not working? is extremelly vague.
 
1.
What do you expect this code do?
 
for(cr in seq_along(dacnet_17$district)){
    match(arhar_18$district, dacnet_17$district)
}
 
See ?match and maybe also ??for? and try this
 
x <- letters[1:5]
y <- sample(letters, 100, replace=T)
match(x,y)
[1] 45 16 24 13 71
for(i in 1:3) match(x,y)
 
2.
rbind works as expeceted
 
arhar_18 <- data.frame(a=1:10, b=50, c=letters[1:10])
dacnet_17 <- data.frame(a=11:20, b=100, c=sample(letters,10))
df3=rbind(arhar_18,dacnet_17)
 
if your data has common column order and type.
 
To get more specific answer you need to ask specific question preferably with some data included (most preferably by dput command) and error message.
 
Cheers
Petr
 
 
From: Ranjeet Kumar Jha <mailto:ranjeetjhaiitkgp at gmail.com> 
Sent: Wednesday, July 27, 2022 8:35 AM
To: PIKAL Petr <mailto:petr.pikal at precheza.cz>
Cc: R-help <mailto:r-help at r-project.org>
Subject: Re: [R] Need to insert various rows of data from a data frame after particular rows from another dataframe
 
Hi Petr,
 
I used r-bind but it's not working.
Here is the code:
 
arhar_18<-read.csv("D:/Ranjeet/IAMV6/input/yield/kharif_18-19_yield/Kharif_2018/arhar_18.csv")
dacnet_17<-read.csv("D:/Ranjeet/IAMV6/input/yield/dacnet_yield_update till 2019.csv")
 
for(cr in seq_along(dacnet_17$district)){
    match(arhar_18$district, dacnet_17$district)
}
 
df3=rbind(arhar_18,dacnet_17)
df3=df3[order(df3$district,df3$year),]
x<-write.csv(df3,"df3.csv")
view(x)
 
On Wed, Jul 27, 2022 at 12:00 PM PIKAL Petr <mailto:petr.pikal at precheza.cz> wrote:
Hi.

>From what you say, plain "rbind" could be used, if the columns in both sets
are the same and in the same order. After that you can reorder the resulting
data frame as you wish by "order". AFAIK for most functions row order in
data frame does not matter.

Cheers
Petr

> -----Original Message-----
> From: R-help <mailto:r-help-bounces at r-project.org> On Behalf Of Ranjeet Kumar Jha
> Sent: Monday, July 25, 2022 3:03 PM
> To: R-help <mailto:r-help at r-project.org>
> Subject: [R] Need to insert various rows of data from a data frame after
> particular rows from another dataframe
> 
> Hello Everyone,
> 
> I have dataset in a particular format in "dacnet_yield_update till
2019.xlsx" file,
> where I need to insert the data of rows 2018-2019 and
> 2019-2020 for the districts those data are available in "Kharif crops
yield_18-
> 19.xlsx".  I need to insert these two rows of data belonging to every
district, if
> data is available in a later excel file, just after the particular crop
group data for
> the particular district.
> 
> I have put the data file in the given link.
> https://drive.google.com/drive/u/0/folders/1dNmGTI8_c9PK1QqmfIjnpbyzuiC
> XgxFC
> 
> Please help solving this problem.
> 
> Regards and Thanks,
> Ranjeet
> 
>       [[alternative HTML version deleted]]
> 
> ______________________________________________
> mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


 
-- 
Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
-----------------------------------------------------------
Email: mailto:ranjeetjhaiitkgp at gmail.com


"Simple Heart, Humble Attitude and Surrender to Supreme Being make our lives beautiful!"




-- 
Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
-----------------------------------------------------------
Email: mailto:ranjeetjhaiitkgp at gmail.com


"Simple Heart, Humble Attitude and Surrender to Supreme Being make our lives beautiful!"


From r@njeetjh@||tkgp @end|ng |rom gm@||@com  Wed Jul 27 08:32:16 2022
From: r@njeetjh@||tkgp @end|ng |rom gm@||@com (Ranjeet Kumar Jha)
Date: Wed, 27 Jul 2022 12:02:16 +0530
Subject: [R] Need to insert various rows of data from a data frame after
 particular rows from another dataframe
In-Reply-To: <CA+etgPkqeSQ1HULmva_FaQuM1ThOtd_jBw35MoXoVYQuwW0muw@mail.gmail.com>
References: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
 <CA+etgPkqeSQ1HULmva_FaQuM1ThOtd_jBw35MoXoVYQuwW0muw@mail.gmail.com>
Message-ID: <CAJfMH3qj06VHRxky8q4M7WTJqGkw3czF2bKQ6kD_CbMgS7Bz+A@mail.gmail.com>

Hi Calum,

Thanks for your reply !
Actually, I need to bring the 2018-2019 and 2019-2020 yield data
corresponding to each crop in every district under a state from the "Kharif
crop yield 18-19.xls" into "dacnet" file, given in the link. These two
years of data from Kharif file should be brought into dacnet file after
2017 or whichever latest previous year yield that crop contains for each
district in a particular state. Since Kharif file has different format and
11-12 crops with large number of districts in each state, I am unable to
get the data from Kharif to dacnet into same format. So basically, kharif
file yield data for 2018-2019 needs to be brought into dacnet file in the
same format as it is in dacnet file.
I hope it is clear now.

I will really appreciate your help!


Regards and Thanks,

On Wed, Jul 27, 2022 at 11:52 AM CALUM POLWART <polc1410 at gmail.com> wrote:

> Not very clear what you are trying to do. But I'd have thought possibly
> dplyr left_join might be a solution for you.  The base R equivalent is
> merge().
>
> It might be a rbind or cbind can do it too.
>
> On Wed, 27 Jul 2022, 03:30 Ranjeet Kumar Jha, <ranjeetjhaiitkgp at gmail.com>
> wrote:
>
>> Hello Everyone,
>>
>> I have dataset in a particular format in "dacnet_yield_update till
>> 2019.xlsx" file, where I need to insert the data of rows 2018-2019 and
>> 2019-2020 for the districts those data are available in "Kharif crops
>> yield_18-19.xlsx".  I need to insert these two rows of data belonging to
>> every district, if data is available in a later excel file, just after the
>> particular crop group data for the particular district.
>>
>> I have put the data file in the given link.
>>
>> https://drive.google.com/drive/u/0/folders/1dNmGTI8_c9PK1QqmfIjnpbyzuiCXgxFC
>>
>> Please help solving this problem.
>>
>> Regards and Thanks,
>> Ranjeet
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

-- 
Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
-----------------------------------------------------------
Email: *ranjeetjhaiitkgp at gmail.com <ranjeetjhaiitkgp at gmail.com>*


*"Simple Heart, Humble Attitude and Surrender to Supreme Being make our
lives beautiful!"*

	[[alternative HTML version deleted]]


From r@njeetjh@||tkgp @end|ng |rom gm@||@com  Wed Jul 27 08:34:41 2022
From: r@njeetjh@||tkgp @end|ng |rom gm@||@com (Ranjeet Kumar Jha)
Date: Wed, 27 Jul 2022 12:04:41 +0530
Subject: [R] Need to insert various rows of data from a data frame after
 particular rows from another dataframe
In-Reply-To: <a172da90dd2448779581da58741b913e@SRVEXCHCM1302.precheza.cz>
References: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
 <a172da90dd2448779581da58741b913e@SRVEXCHCM1302.precheza.cz>
Message-ID: <CAJfMH3rF9RmJee+G0Y+3Re8U9BsxVkUhoZJhgQyGwYxrUxCSJQ@mail.gmail.com>

Hi Petr,

I used r-bind but it's not working.
Here is the code:

arhar_18<-read.csv("D:/Ranjeet/IAMV6/input/yield/kharif_18-19_yield/Kharif_2018/arhar_18.csv")
dacnet_17<-read.csv("D:/Ranjeet/IAMV6/input/yield/dacnet_yield_update till
2019.csv")

for(cr in seq_along(dacnet_17$district)){
    match(arhar_18$district, dacnet_17$district)
}

df3=rbind(arhar_18,dacnet_17)
df3=df3[order(df3$district,df3$year),]
x<-write.csv(df3,"df3.csv")
view(x)

On Wed, Jul 27, 2022 at 12:00 PM PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hi.
>
> From what you say, plain "rbind" could be used, if the columns in both sets
> are the same and in the same order. After that you can reorder the
> resulting
> data frame as you wish by "order". AFAIK for most functions row order in
> data frame does not matter.
>
> Cheers
> Petr
>
> > -----Original Message-----
> > From: R-help <r-help-bounces at r-project.org> On Behalf Of Ranjeet Kumar
> Jha
> > Sent: Monday, July 25, 2022 3:03 PM
> > To: R-help <r-help at r-project.org>
> > Subject: [R] Need to insert various rows of data from a data frame after
> > particular rows from another dataframe
> >
> > Hello Everyone,
> >
> > I have dataset in a particular format in "dacnet_yield_update till
> 2019.xlsx" file,
> > where I need to insert the data of rows 2018-2019 and
> > 2019-2020 for the districts those data are available in "Kharif crops
> yield_18-
> > 19.xlsx".  I need to insert these two rows of data belonging to every
> district, if
> > data is available in a later excel file, just after the particular crop
> group data for
> > the particular district.
> >
> > I have put the data file in the given link.
> > https://drive.google.com/drive/u/0/folders/1dNmGTI8_c9PK1QqmfIjnpbyzuiC
> > XgxFC
> >
> > Please help solving this problem.
> >
> > Regards and Thanks,
> > Ranjeet
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
-----------------------------------------------------------
Email: *ranjeetjhaiitkgp at gmail.com <ranjeetjhaiitkgp at gmail.com>*


*"Simple Heart, Humble Attitude and Surrender to Supreme Being make our
lives beautiful!"*

	[[alternative HTML version deleted]]


From r@njeetjh@||tkgp @end|ng |rom gm@||@com  Wed Jul 27 11:01:11 2022
From: r@njeetjh@||tkgp @end|ng |rom gm@||@com (Ranjeet Kumar Jha)
Date: Wed, 27 Jul 2022 14:31:11 +0530
Subject: [R] Need to insert various rows of data from a data frame after
 particular rows from another dataframe
In-Reply-To: <9c4386179ab24616915d2296b70a28d8@SRVEXCHCM1302.precheza.cz>
References: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
 <a172da90dd2448779581da58741b913e@SRVEXCHCM1302.precheza.cz>
 <CAJfMH3rF9RmJee+G0Y+3Re8U9BsxVkUhoZJhgQyGwYxrUxCSJQ@mail.gmail.com>
 <9c4386179ab24616915d2296b70a28d8@SRVEXCHCM1302.precheza.cz>
Message-ID: <CAJfMH3oUD720UKT0DiuLWcPr4OvBEdDPeyxPwgpgXZhg3Pz5YA@mail.gmail.com>

yeah rbind works find bur dataframe 2 has different format and then rbind
can be used with first dataframe. But here I am struggling to bring the 2nd
dataframe in the same format as the 1st one.

On Wed, Jul 27, 2022 at 1:13 PM PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hi.
>
>
>
> ?is not working? is extremelly vague.
>
>
>
> 1.
>
> What do you expect this code do?
>
>
>
> for(cr in seq_along(dacnet_17$district)){
>
>     match(arhar_18$district, dacnet_17$district)
>
> }
>
>
>
> See ?match and maybe also ??for? and try this
>
>
>
> x <- letters[1:5]
>
> y <- sample(letters, 100, replace=T)
>
> match(x,y)
>
> [1] 45 16 24 13 71
>
> for(i in 1:3) match(x,y)
>
>
>
> 2.
>
> rbind works as expeceted
>
>
>
> arhar_18 <- data.frame(a=1:10, b=50, c=letters[1:10])
>
> dacnet_17 <- data.frame(a=11:20, b=100, c=sample(letters,10))
>
> df3=rbind(arhar_18,dacnet_17)
>
>
>
> if your data has common column order and type.
>
>
>
> To get more specific answer you need to ask specific question preferably
> with some data included (most preferably by dput command) and error message.
>
>
>
> Cheers
>
> Petr
>
>
>
>
>
> *From:* Ranjeet Kumar Jha <ranjeetjhaiitkgp at gmail.com>
> *Sent:* Wednesday, July 27, 2022 8:35 AM
> *To:* PIKAL Petr <petr.pikal at precheza.cz>
> *Cc:* R-help <r-help at r-project.org>
> *Subject:* Re: [R] Need to insert various rows of data from a data frame
> after particular rows from another dataframe
>
>
>
> Hi Petr,
>
>
>
> I used r-bind but it's not working.
>
> Here is the code:
>
>
>
>
> arhar_18<-read.csv("D:/Ranjeet/IAMV6/input/yield/kharif_18-19_yield/Kharif_2018/arhar_18.csv")
>
> dacnet_17<-read.csv("D:/Ranjeet/IAMV6/input/yield/dacnet_yield_update till
> 2019.csv")
>
>
>
> for(cr in seq_along(dacnet_17$district)){
>
>     match(arhar_18$district, dacnet_17$district)
>
> }
>
>
>
> df3=rbind(arhar_18,dacnet_17)
>
> df3=df3[order(df3$district,df3$year),]
>
> x<-write.csv(df3,"df3.csv")
>
> view(x)
>
>
>
> On Wed, Jul 27, 2022 at 12:00 PM PIKAL Petr <petr.pikal at precheza.cz>
> wrote:
>
> Hi.
>
> From what you say, plain "rbind" could be used, if the columns in both sets
> are the same and in the same order. After that you can reorder the
> resulting
> data frame as you wish by "order". AFAIK for most functions row order in
> data frame does not matter.
>
> Cheers
> Petr
>
> > -----Original Message-----
> > From: R-help <r-help-bounces at r-project.org> On Behalf Of Ranjeet Kumar
> Jha
> > Sent: Monday, July 25, 2022 3:03 PM
> > To: R-help <r-help at r-project.org>
> > Subject: [R] Need to insert various rows of data from a data frame after
> > particular rows from another dataframe
> >
> > Hello Everyone,
> >
> > I have dataset in a particular format in "dacnet_yield_update till
> 2019.xlsx" file,
> > where I need to insert the data of rows 2018-2019 and
> > 2019-2020 for the districts those data are available in "Kharif crops
> yield_18-
> > 19.xlsx".  I need to insert these two rows of data belonging to every
> district, if
> > data is available in a later excel file, just after the particular crop
> group data for
> > the particular district.
> >
> > I have put the data file in the given link.
> > https://drive.google.com/drive/u/0/folders/1dNmGTI8_c9PK1QqmfIjnpbyzuiC
> > XgxFC
> >
> > Please help solving this problem.
> >
> > Regards and Thanks,
> > Ranjeet
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> --
>
> Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
>
> https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
>
> -----------------------------------------------------------
> Email: *ranjeetjhaiitkgp at gmail.com <ranjeetjhaiitkgp at gmail.com>*
>
>
>
>
>
> *"Simple Heart, Humble Attitude and Surrender to Supreme Being make our
> lives beautiful!"*
>
>
>


-- 
Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
-----------------------------------------------------------
Email: *ranjeetjhaiitkgp at gmail.com <ranjeetjhaiitkgp at gmail.com>*


*"Simple Heart, Humble Attitude and Surrender to Supreme Being make our
lives beautiful!"*

	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Wed Jul 27 14:15:16 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Wed, 27 Jul 2022 12:15:16 +0000
Subject: [R] Request concerning use in book
In-Reply-To: <008701d8a029$61bd5150$2537f3f0$@sbg.at>
References: <008701d8a029$61bd5150$2537f3f0$@sbg.at>
Message-ID: <BN6PR2201MB15535EEB6CE948DEF7785D07CF979@BN6PR2201MB1553.namprd22.prod.outlook.com>

My understanding is that the only requirement is to give proper credit. This is necessary anyway as readers would need to know what packages were used and what version of R was used. In R there is the citation() function that returns the approved citation. However, it might be useful to also include the version numbers in the text. 

One problem is how the citation is viewed in the body of the document. I use Endnote and I think Endnote would view this as a name. So in the text I would see (Team 2022) and the bibliography might be Team, R.C. 2022. R: A language ... 
I think that this is solved by putting a comma after Team in the bibliographic entry. Another option would be to use R_Core_Team.


If this is a well-funded project, you might consider a donation. I am sure that R would appreciate any donation you would care to give in support of their work: https://www.r-project.org/foundation/donors.html. They accept annual donations or one-off donations. 


Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of karl.fuchs at sbg.at
Sent: Monday, July 25, 2022 9:21 AM
To: r-help at r-project.org
Subject: [R] Request concerning use in book

[External Email]

Dear colleagues!



We plan to introduce the software R in chapter "subject-specific software in mathematics" in our book "TEACHING/LEARNING MEDIUM COMPUTER".



As we will use screenshots and Plots generated by R we ask if there will be any fees for using this elements in our book.



Yours

Dr. Karl Fuchs, Austria


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=A3HEHpcoQF1M1edkMCL9__nY4ZWvWCW-nbxP99mzSHx6iiEKfLuE840Se5Vgnb73&s=j_53saaRAJYoyNAVUDwyUpFsxg4UTGNuPl3zvxbAV50&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=A3HEHpcoQF1M1edkMCL9__nY4ZWvWCW-nbxP99mzSHx6iiEKfLuE840Se5Vgnb73&s=0VJtv0PNkpCNdziw0p5jF61vrQAM5-OtH_dfP5dxbt8&e=
and provide commented, minimal, self-contained, reproducible code.


From tebert @end|ng |rom u||@edu  Wed Jul 27 14:34:42 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Wed, 27 Jul 2022 12:34:42 +0000
Subject: [R] Need to insert various rows of data from a data frame after
 particular rows from another dataframe
In-Reply-To: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
References: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
Message-ID: <BN6PR2201MB15532960CD3AE91A363043E8CF979@BN6PR2201MB1553.namprd22.prod.outlook.com>

I think what you want is full_join() from the dplyr package.
https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/join

The only requirement is that both data frames must have a column in common wherein the data are entered in the same way. So the column labeled "state" needs to have "ANANTAPUR" in both data frames rather than "ANANTAPUR" in one data frame and "Anantapur" in the other. 

Reformat your excel spreadsheet to remove headers. Your first column should be three columns: State, then Crop, then district rather than headings. The first row can contain variable names. I would make variable names simple (like "Area" and "Production") but some like more information so "Area_Ha" and "Production_TN_per_Ha" would also work. It is best not to use special symbols in variable names and keep variable names as one string of characters (no spaces). Including such will eventually cause problems.
https://www.w3schools.com/r/r_variables.asp#:~:text=Variable%20Names&text=Rules%20for%20R%20variables%20are,be%20followed%20by%20a%20digit.

One exception to the rules in the link is that you can make a variable name T or F. R defaults to interpreting these as TRUE and FALSE. The problem happens when the programmer reassigns these and then tries to use T or F as Boolean in other parts of the program.

Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ranjeet Kumar Jha
Sent: Monday, July 25, 2022 9:03 AM
To: R-help <r-help at r-project.org>
Subject: [R] Need to insert various rows of data from a data frame after particular rows from another dataframe

[External Email]

Hello Everyone,

I have dataset in a particular format in "dacnet_yield_update till 2019.xlsx" file, where I need to insert the data of rows 2018-2019 and
2019-2020 for the districts those data are available in "Kharif crops yield_18-19.xlsx".  I need to insert these two rows of data belonging to every district, if data is available in a later excel file, just after the particular crop group data for the particular district.

I have put the data file in the given link.
https://urldefense.proofpoint.com/v2/url?u=https-3A__drive.google.com_drive_u_0_folders_1dNmGTI8-5Fc9PK1QqmfIjnpbyzuiCXgxFC&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=9B32l682GguXDLEFdPm6j5JZNatveGSlY7lnwLYFVOW2TX1tNLeHbDE49MYxSh_Q&s=4_bhl2_drIA0Pn3LHMcoAd02lX0t6bAx2wSlhVAJelA&e=

Please help solving this problem.

Regards and Thanks,
Ranjeet

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=9B32l682GguXDLEFdPm6j5JZNatveGSlY7lnwLYFVOW2TX1tNLeHbDE49MYxSh_Q&s=MAGsb78RBOWV0usgeNnmHsZcYoQI959dmihJ9Ycs8Lo&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=9B32l682GguXDLEFdPm6j5JZNatveGSlY7lnwLYFVOW2TX1tNLeHbDE49MYxSh_Q&s=PSiyw67xhInkZo69l1HojQKGOqthbxYpGL5Q14cPo8w&e=
and provide commented, minimal, self-contained, reproducible code.


From @vi@e@gross m@iii@g oii gm@ii@com  Wed Jul 27 17:41:56 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Wed, 27 Jul 2022 11:41:56 -0400
Subject: [R] Need to insert various rows of data from a data frame after
 particular rows from another dataframe
In-Reply-To: <CAJfMH3oUD720UKT0DiuLWcPr4OvBEdDPeyxPwgpgXZhg3Pz5YA@mail.gmail.com>
References: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
 <a172da90dd2448779581da58741b913e@SRVEXCHCM1302.precheza.cz>
 <CAJfMH3rF9RmJee+G0Y+3Re8U9BsxVkUhoZJhgQyGwYxrUxCSJQ@mail.gmail.com>
 <9c4386179ab24616915d2296b70a28d8@SRVEXCHCM1302.precheza.cz>
 <CAJfMH3oUD720UKT0DiuLWcPr4OvBEdDPeyxPwgpgXZhg3Pz5YA@mail.gmail.com>
Message-ID: <009e01d8a1cf$678b6340$36a229c0$@gmail.com>

Ranjeet,

As others have said, you have not shown enough to get decent answers.

What you describe sounds quite routine and is the first step many have to do when gathering data from disparate sources that were done by different people without much consideration it has to follow some specific pattern.

Every data frame has umpteen columns with names in some order and containing data in each column that is of some type.

So if you want to combine them vertically using something like rbind() then you must make sure the two align exactly.

You could do a names(df1) and names(df2) to see what you have. If both do not have the same number of columns, you may want to either delete some in one or add to the other. If the names of some columns are not exactly the same, rename them. If the order is not the same, reorder them. This is fairly simple stuff you can do in base R or in something like dplyr and is a standard part of getting data into a form you want.

A tad more subtle is the need for conversions if the above is not working for you. If one file has text where the other has integers, then convert one or the other. If one is all uppercase and the other not, again, you need to make the same if it matters. For rbind() not immediately but for some kind of merge or join, definitely. If your data already is stored as a factor, be careful if the two do not line up with the same factors.

Again, without some details that show how the two are not already aligned, it is up to you to do the work, step by step, until they are.

Of course there are other approaches that can make this less painful. You can simply make a NEW df dynamically like this:

newdf <- data.frame(alpha=c(df1$colx, df2$coly), beta=c(df1$..., df2$...), ...)

This concatenates vectors/columns rather than the entire data.frame and makes some sense if you want to keep just a few columns and the dataframes are otherwise in quite a scrambled order.

And just for correction, when you say your data is in EXCEL, generally that does not mean a .CSV file but a .XSLX file and that would be opened and read another way.


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ranjeet Kumar Jha
Sent: Wednesday, July 27, 2022 5:01 AM
To: PIKAL Petr <petr.pikal at precheza.cz>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] Need to insert various rows of data from a data frame after particular rows from another dataframe

yeah rbind works find bur dataframe 2 has different format and then rbind can be used with first dataframe. But here I am struggling to bring the 2nd dataframe in the same format as the 1st one.


From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Wed Jul 27 22:50:55 2022
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Wed, 27 Jul 2022 15:50:55 -0500
Subject: [R] Parsing XML?
In-Reply-To: <20220727145903.4338c449692f703b@ask.loc.gov>
References: <20220727145903.4338c449692f703b@ask.loc.gov>
Message-ID: <7743581c-0959-1df6-bb31-f140161f83e1@effectivedefense.org>

Hello, All:


	  What would you suggest I do to parse the following XML file into a 
list that I can understand:


XMLfile <- 
"https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/ndnp_Alabama_all-yrs_e_0001_0050.xml" 



	  This is the first of 6666 XML files containing "U.S. Newspaper 
Directory" maintained by the US Library of Congress discussed in the 
thread below.  I've tried various things using the XML and xml2.


XMLdata <- xml2::read_xml(XMLfile)
str(XMLdata)
XMLdat <- XML::xmlParse(XMLdata)
str(XMLdat)
XMLtxt <- xml2::xml_text(XMLdata)
nchar(XMLtxt)
#[1] 29415


	  Someplace there's a schema for this.  I don't know if it's embedded 
in this XML file or in a separate file.  If it's in a separate file, how 
could I describe it to my contacts with the Library of Congress so they 
would understand what I needed and could help me get it.


	  Thanks,
	  Spencer Graves


p.s.  All 29415 characters in XMLtext appear in the thread below.  	  	


-------- Forwarded Message --------
Subject: 	[Newspapers and Current Periodicals] How can I get counts of 
the numbers of newspapers by year in the US, and preferably also 
elsewhere? A search of "U.S. Newspaper Directory,
Date: 	Wed, 27 Jul 2022 14:59:03 +0000
From: 	Kerry Huller <serials at ask.loc.gov>
To: 	Spencer Graves <spencer.graves at effectivedefense.org>
CC: 	twes at loc.gov



--# Type your reply above this line #--

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 27 2022, 10:59am via System

Hello Spencer,

So, when I view the xml, I'm actually looking at it in XML editor 
software, so I can view the tags and it's structured neatly. I've copied 
and pasted the text from the beginning of the file and the first 
newspaper title below from my XML editor:

<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type='text/xsl' 
href='/webservices/catalog/xsl/searchRetrieveResponse.xsl'?>

<searchRetrieveResponse xmlns="http://www.loc.gov/zing/srw/" 
xmlns:oclcterms="http://purl.org/oclc/terms/" 
xmlns:dc="http://purl.org/dc/elements/1.1/" 
xmlns:diag="http://www.loc.gov/zing/srw/diagnostic/" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
<version>1.1</version>
<numberOfRecords>2250</numberOfRecords>
<records>
<record>
<recordSchema>info:srw/schema/1/marcxml</recordSchema>
<recordPacking>xml</recordPacking>
<recordData>
<record xmlns="http://www.loc.gov/MARC21/slim">
  ? ? <leader>00000nas a22000007i 4500</leader>
  ? ? <controlfield tag="001">1030438981</controlfield>
  ? ? <controlfield tag="008">180404c20159999aluwr n ? ? ? 0 ? a0eng 
 ?</controlfield>
  ? ? <datafield ind1=" " ind2=" " tag="010">
  ? ? ? <subfield code="a"> ?2018200464</subfield>
  ? ? </datafield>
  ? ? <datafield ind1=" " ind2=" " tag="040">
  ? ? ? <subfield code="a">DLC</subfield>
  ? ? ? <subfield code="e">rda</subfield>
  ? ? ? <subfield code="c">DLC</subfield>
  ? ? ? <subfield code="b">eng</subfield>
  ? ? </datafield>
  ? ? <datafield ind1=" " ind2=" " tag="012">
  ? ? ? <subfield code="m">1</subfield>
  ? ? </datafield>
  ? ? <datafield ind1="0" ind2=" " tag="022">
  ? ? ? <subfield code="a">2577-5316</subfield>
  ? ? ? <subfield code="2">1</subfield>
  ? ? </datafield>
  ? ? <datafield ind1=" " ind2=" " tag="032">
  ? ? ? <subfield code="a">021110</subfield>
  ? ? ? <subfield code="b">USPS</subfield>
  ? ? </datafield>
  ? ? <datafield ind1=" " ind2=" " tag="037">
  ? ? ? <subfield code="b">711 Alabama Avenue, Selma, AL 36701</subfield>
  ? ? </datafield>
  ? ? <datafield ind1=" " ind2=" " tag="042">
  ? ? ? <subfield code="a">nsdp</subfield>
  ? ? ? <subfield code="a">pcc</subfield>
  ? ? </datafield>
  ? ? <datafield ind1="1" ind2="0" tag="050">
  ? ? ? <subfield code="a">ISSN RECORD</subfield>
  ? ? </datafield>
  ? ? <datafield ind1="1" ind2="0" tag="082">
  ? ? ? <subfield code="a">071</subfield>
  ? ? ? <subfield code="2">15</subfield>
  ? ? </datafield>
  ? ? <datafield ind1=" " ind2="0" tag="222">
  ? ? ? <subfield code="a">Selma sun</subfield>
  ? ? </datafield>
  ? ? <datafield ind1="0" ind2="0" tag="245">
  ? ? ? <subfield code="a">Selma sun.</subfield>
  ? ? </datafield>
  ? ? <datafield ind1=" " ind2="1" tag="264">
  ? ? ? <subfield code="a">Selma, AL :</subfield>
  ? ? ? <subfield code="b">North Shore Press, LLC</subfield>
  ? ? ? <subfield code="c">2016-</subfield>
  ? ? </datafield>
  ? ? <datafield ind1=" " ind2=" " tag="310">
  ? ? ? <subfield code="a">Weekly</subfield>
  ? ? </datafield>
  ? ? <datafield ind1=" " ind2=" " tag="336">
  ? ? ? <subfield code="a">text</subfield>
  ? ? ? <subfield code="b">txt</subfield>
  ? ? ? <subfield code="2">rdacontent</subfield>
  ? ? </datafield>
  ? ? <datafield ind1=" " ind2=" " tag="337">
  ? ? ? <subfield code="a">unmediated</subfield>
  ? ? ? <subfield code="b">n</subfield>
  ? ? ? <subfield code="2">rdamedia</subfield>
  ? ? </datafield>
  ? ? <datafield ind1=" " ind2=" " tag="338">
  ? ? ? <subfield code="a">volume</subfield>
  ? ? ? <subfield code="b">nc</subfield>
  ? ? ? <subfield code="2">rdacarrier</subfield>
  ? ? </datafield>
  ? ? <datafield ind1="1" ind2=" " tag="362">
  ? ? ? <subfield code="a">Began in 2015.</subfield>
  ? ? </datafield>
  ? ? <datafield ind1=" " ind2=" " tag="588">
  ? ? ? <subfield code="a">Description based on: Volume 2, Issue 40 
(October 5, 2017) (surrogate); title from caption.</subfield>
  ? ? </datafield>
  ? ? <datafield ind1=" " ind2=" " tag="588">
  ? ? ? <subfield code="a">Latest issue consulted: Volume 2, Issue 40 
(October 5, 2017).</subfield>
  ? ? </datafield>
  ? ? <datafield ind1=" " ind2=" " tag="752">
  ? ? ? <subfield code="a">United States</subfield>
  ? ? ? <subfield code="b">Alabama</subfield>
  ? ? ? <subfield code="c">Dallas</subfield>
  ? ? ? <subfield code="d">Selma.</subfield>
  ? ? </datafield>
  ? </record>
</recordData>
</record>

When I view the records in the XML editor, these 2 lines below do begin 
each of the records for each individual title, but of course this is 
including the xml tags:

<recordSchema>info:srw/schema/1/marcxml</recordSchema>
<recordPacking>xml</recordPacking>

Hopefully this helps you decide where to break or parse each record.

On another note, I just noticed as well that at the top of this first 
file it lists the total number of records for the Alabama grouping - 
2250. This also appeared to be the case for the Alaska records when I 
took a look at the first one for that state. I imagine that should be 
consistent throughout each "grouping" of records.

Let me know if you have follow-up questions!

Best wishes,

Kerry Huller
Newspaper & Current Periodical Reading Room
Serial & Government Publications Division
Library of Congress

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 27 2022, 10:21am via Email

Hi, Kerry:


Thanks. I understand the chunking in files of at most 50. I've read
the first file "ndnp_Alabama_all-yrs_e_0001_0050.xml" into a string of
29415 characters, copied below. Might you have any suggestions on the
next step in parsing this? Staring at it now, it looks splitting on
"info:srw/schema/1/marcxmlxml" might convert the 29415 characters into
shorter chunks, each of which could then be parsed further.


This is not as bad as reading ancient Egyptian heiroglyphics without
the Rosetta Stone, but I wondered if you might have something that could
make this work easier and more reliable? I guess I could compare with
what I already read as JSON ;-)


Thanks,
Spencer Graves


"1.12250info:srw/schema/1/marcxmlxml00000nas a22000007i
45001030438981180404c20159999aluwr n 0 a0eng
2018200464DLCrdaDLCeng12577-53161021110USPS711 Alabama Avenue, Selma, AL
36701nsdppccISSN RECORD07115Selma sunSelma sun.Selma, AL :North Shore
Press,
LLC2016-WeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan in
2015.Description based on: Volume 2, Issue 40 (October 5, 2017)
(surrogate); title from caption.Latest issue consulted: Volume 2, Issue
40 (October 5, 2017).United
StatesAlabamaDallasSelma.info:srw/schema/1/marcxmlxml00000cas a22000007a
4500502150053100127c20109999aluwr n 0 a0eng
2010200019DLCengDLCDLCOCLCQ112153-18111750USPSB & C Publishing, LLC,
3514 Martin St. S. Ste 104, Cropwell, AL 35054pccnsdpISSN RECORDSt.
Clair County news (Cropwell, Ala.)St. Clair County news(Cropwell,
Ala.)St. Clair County news.Cropwell, AL :B & C Pub.WeeklyBegan in
2010.Description based on: Nov. 4, 2010 (surrogate); title from
caption.info:srw/schema/1/marcxmlxml00000cas a22000007a
4500426491872090720c20099999alumr n 0 a0eng
2009203372DLCengDLCOCLCQ12150-346X2150-346X1AU at 000044489617NZ116076352Devon
Applewhite/Applewhite Publishing Co., 1910 Honeysuckle Rd., #N183,
Dothan, AL 36305mscnsdpISSN RECORD30514Triangle tribune(Dothan,
Ala.)Triangle tribune.Dothan, AL :Applewhite Pub. CoMonthlyBegan with
vol. 1, issue 1 (May 2009).\"Connecting the Tri-State African -American
Community.\"Description based on: Vol. 1, issue 1 (May 2009); title from
masthead.Applewhite, Devon.United StatesAlabama.United
StatesGeorgia.United StatesFlorida.info:srw/schema/1/marcxmlxml00000cas
a22000007a 4500289017315081219c20089999aluwr n | a0eng c
2008213218NSDengNSDOCLCQDLCOCLCQ111945-93191945-93191005270USPSSpringhill Publications,
LLC, P.O. Box 186, Greenville, AL 36037nsdppccISSN RECORD07014Greenville
standardThe Greenville standard.Greenville, AL :Springhill
PublicationsWeeklytexttxtrdacontentunmediatednrdamediaBegan with vol. 1,
issue 1 (Sept. 3, 2008)Description based on surrogate of: Vol. 1, no. 15
(Dec. 18, 2008); title from masthead (publisher's Web site, viewed Dec.
19, 2008).Latest issue consulted: Vol. 1, no. 99 (July 27, 2011)
(surrogate).info:srw/schema/1/marcxmlxml00000cas a22000007a
4500123539969070426c20079999aluwr ne 0 a0eng c
2007212138NSDengNSDNSDOCLCQ101936-95571936-95571The Western Tribune,
1530 Third Ave. N., Bessemer, AL 35020mscnsdpISSN RECORDWestern tribune
(Bessemer, Ala.)The Western tribune(Bessemer, Ala.)The Western
tribune.Bessemer, Ala. :D-Med, Inc.v.WeeklyBegan in 2007.Description
based on: May 23, 2007 (surrogate); title from
caption.AU at 000041575341info:srw/schema/1/marcxmlxml00000cas a22000007a
4500226300653080425c20079999aluwr ne | a0eng
2008212112NSDengNSDNSDOCLCQ11942-20751942-20751nsdppccISSN RECORDThe
corridor messengerThe corridor messenger.Carbon Hill, AL :Corridor
Messenger, Inc.WeeklyBegan with vol. 1, issue (10.03.2007).Description
based on: 1st issue.United StatesAlabamaWalkerCarbon
Hill.http://www.corridormessenger.cominfo:srw/schema/1/marcxmlxml00000cas a22000007a
450077560432070109c20069999aluwr ne 0 a0eng c
2007213400NSDengNSDOCLCQAUBRNOCLCOOCLCFa01935-37901935-37901AU at 000041190283The 

Auburn Villager, P.O. Box 1633, Auburn, AL 36831-1633pccnsdpISSN
RECORDThe Auburn villagerThe Auburn villager.Auburn, AL :Auburn
Villagerv.WeeklyBegan in 2006.Description based on: Vol. 1, no. 4 (July
20, 2006) (surrogate); title from caption.Auburn (Ala.)Newspapers.Lee
County (Ala.)Newspapers.AlabamaAuburn.fast(OCoLC)fst01209634AlabamaLee
County.fast(OCoLC)fst01211930Newspapers.fast(OCoLC)fst01423814United
StatesAlabamaLeeAuburn.info:srw/schema/1/marcxmlxml00000cas a2200000Ii
4500872286785m o d s cr mn|---a||||140311c20069999alucr n o b
s0 a0eng cABCengrdaABCABCOCLCFLD59.13University of Alabama at
Birmingham.The eReporter.[Birmingham, Alabama] :The University of
Alabama at Birmingham,[2006]-[Birmingham, Alabama] :Offices of Public
Relations & Marketing and Information Technology1 online resource2
issues weeklytexttxtrdacontentcomputercrdamediaonline
resourcecrrdacarrierSeptember 19, 2006-\"The eReporter is an official
communication of The University of Alabama at Birmingham, companion to
the UAB Reporter and recommended alternative to mass e-mails.\"Issues
for <March 11, 2014- published and distributed via e-mail subscription
on Tuesdays and Fridays.Description based on: September 19, 2006; title
from title screen (viewed March 12, 2014).University of Alabama at
BirminghamPeriodicals.Periodicals.fast(OCoLC)fst01411641University of
Alabama at Birmingham.fast(OCoLC)fst00645114University of Alabama at
Birmingham.Office of Public Relations and Marketing.University of
Alabama at Birmingham.Information Technology.2006-2012, companion
to:University of Alabama at Birmingham.UAB
reporter.(OCoLC)32435748Archived
issueshttp://hatteras.dpo.uab.edu/cgi-bin/ereporter.cgiinfo:srw/schema/1/marcxmlxml00000cas 

a22000007a 4500166387050070829c20059999aluwr ne | a0eng c
2007215501NSDengNSDOCLCQ11939-68991939-68991The Wilkie Clark Memorial
Foundation, P.O. Box 514, Roanoke, AL 36274$30.00nsdpmscISSN
RECORD305.89614People's voice (Roanoke, Ala.)The people's voice(Roanoke,
Ala.)The people's voice.Roanoke, AL :Wilkie Clark Memorial
Foundationv.WeeklyBegan with vol. 1, no. 1 in 2005.Description based on:
Vol. 2, no. 20 (Apr. 20, 2007); title from caption.Wilkie Clark Memorial
Foundation.United
StatesAlabamaRandolphRoanoke.AU at 000042141390info:srw/schema/1/marcxmlxml00000nas 

a22000007i 45001124677787191021c20uu9999aluwr ne | a0eng
2019202521DLCengrdaDLC12689-3258122730USPSNorth Jackson Press, 42950 Hwy
72, Suite 406, Stevenson, AL 35772nsdppccISSN RECORD071.323North Jackson
pressNorth Jackson press.Stevenson, AL :Caney Creek Publications
LLCWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierDescription
based on surrogate of: Volume 1, number 36 (October 11, 2019); title
from masthead.Latest issue consulted: Volume 1, number 36 (October 11,
2019) (Surrogate).United
StatesAlabamaJacksonStevensoninfo:srw/schema/1/marcxmlxml00000cas
a2200000 a 4500226315099080428d19981998aluwr ne | 0eng c
2008233691GUAengGUAOCLCQOCLCFOCLCO39911644pccn-us-gaThe Dekalb
news.Birmingham, Ala. :Community newspaper holdings Inc.v.WeeklyBegan
with 1st year, no. 1 (Apr. 1, 1998); ceased with 1st year, no. 31 (Oct.
28, 1998).Final issue consulted.Description based on first issue; title
from caption.Decatur (Ga.)Newspapers.DeKalb County
(Ga.)Newspapers.Newspapers.fast(OCoLC)fst01423814GeorgiaDecatur.fast(OCoLC)fst01226234GeorgiaDeKalb 

County.fast(OCoLC)fst01215288United
StatesGeorgiaDeKalbDecatur.Decatur-DeKalb news/era(DLC)sn
89053661(OCoLC)19946163info:srw/schema/1/marcxmlxml00000cas a2200000 i
450050263311m o d cr cn|||||||||020730c19979999alu x neo
0 a0eng c
2015238492AMHengrdapnAMHOCLCQOCLCFOCLCOIULOCLHTMOCLCQCOODLC66460694810970435082687-93791AU at 000050711528OCLCS45109pccnsdpn-us---AP2.B5707023Birmingham 

weekly (Online)Birmingham weekly(Online)Birmingham weekly.Birmingham, AL
:Birmingham Weekly1 online resourceIrregular,Feb. 16-28,
2012-Weekly,Sept. 4-11, 1997-Feb. 9-16,
2012texttxtrdacontentcomputercrdamediaonline resourcecrrdacarrierBegan
with vol. 1, issue 1 (Sept. 4-11, 1997).\"City news, views &
entertainment\"--Cover.Numbering dropped in Mar. 2012.Also issued in
print.Description based on: Publication information from ProQuest; title
from web page (viewed June 18, 2015).Latest issue consulted: Aug. 15-20,
2012.Birmingham (Ala.)Newspapers.Internet resources.Electronic
journals.AlabamaBirmingham.fast(OCoLC)fst01204958Newspapers.fast(OCoLC)fst01423814United 

StatesAlabamaBirmingham.Print version:Birmingham
Weekly(OCoLC)39271050http://apw.softlineweb.com/http://WC2VB5MT8E.search.serialssolutions.com/?sid=sersol&SS_jc=JC_000051895&title=Birmingham+Weeklyinfo:srw/schema/1/marcxmlxml00000cas 

a22000007a 450031471314941116d19941995aluwr ne 0 a0eng csn
94003083
NSDengNSDANEOCLCQOCLCFOCLCOOCLCQ11079-65411079-65411nsdppccn-us-akSoutheast
shopperSoutheast shopper.Juneau, Alaska :Kemper
Communications,1994-volumesWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 

1, no. 1 (Nov. 16, 1994)-Ceased in Feb. 1995.Juneau
(Alaska)Newspapers.AlaskaJuneau.fast(OCoLC)fst01213587Newspapers.fast(OCoLC)fst01423814United 

StatesAlaskaJuneau.AU at 000011356572info:srw/schema/1/marcxmlxml00000cas
a22000008a 450027910515930413c19949999alumr n 0 a0eng dsn
93002581 NSDengNSDOCLCQ11069-06621Birmingham Tribune, 216 Ave. T. Pratt
City, Birmingham, AL 35214nsdpBirmingham tribuneBirmingham
tribune.Birmingham, Ala. :Kervin
Fondren9501volumesMonthlytexttxtrdacontentunmediatednrdamediavolumencrdacarrierPREPUB: 

publication expected Jan.
1995AU at 000025863987info:srw/schema/1/marcxmlxml00000cas a22000007a
450026199931920716d19922013alumr ne 0 a0eng csn 92003357
NSDengNSDOCLOCLCQDLC011064-01341064-01341Black & White, POB 13215,
Birmingham, AL 35202-3215nsdppccBlack & white (Birmingham, Ala.)Black &
white(Birmingham, Ala.)Black & white.Black and whiteBirmingham, Ala.
:Black & White, Inc.v.Biweekly,Oct. 2, 1997-Monthly,May 1, 1992-Sept.
1997Began in May 1992; ceased with Jan. 10, 2013.\"Birmingham's New City
paper.\"Description based on: June 1992.Latest issue consulted: No. 67
(Oct. 16, 1997) (surrogate).info:srw/schema/1/marcxmlxml00000cas
a2200000 a 450032145723950314d19901999alumr ne 0 a0eng csn
95068755
MGNengMGNNSDCLUOCLCQOCLCFOCLCOOCLCA971211082-34841082-34841AU at 000011579542nsdppccn-us-alF335.J5S68The 

Southern shofarThe Southern shofar.Birmingham, AL :L. Brook,-[1999]v.
:ill. ;35 cm.MonthlyBegan in 1990.-v. 9, issue 9 (Aug./Sept. 1999).\"The
monthly newspaper of Alabama's Jewish community.\"Some issues also
available on the Internet via the World Wide Web.Description based on:
Vol. 3, issue 11 (Oct. 1993).Jewish newspapersAlabama.Jewish
newspapers.fast(OCoLC)fst00982872Alabama.fast(OCoLC)fst01204694United
StatesAlabamaJeffersonBirmingham.Deep South Jewish voice(DLC)sn
99018499(OCoLC)42431704CLUhttp://bibpurl.oclc.org/web/719http://www.bham.net/shofar/info:srw/schema/1/marcxmlxml00000cas 

a22000007a 450021265141900326c19909999aluwr ne 0 a0eng csn
90099004 AARengAARCPNNSDOCLCQ11050-08981050-08981005022USPSE.O.N., Inc.,
Main St., Eclectic, AL 36024pccnsdpISSN RECORDThe Eclectic observerThe
Eclectic observer.Eclectic, Ala. :E.O.N., Inc.,1990-v.WeeklyVol. 1, no.
1 (Feb. 22, 1990)-Published by: Price Publications, Inc., <2006->Latest
issue consulted: Vol. 17, no. 1 (Jan. 5, 2006).United
StatesAlabamaElmoreEclectic.AU at 000040212446info:srw/schema/1/marcxmlxml00000cas 

a22000007a 450021214781900314c19909999aluir ne 0 a0eng csn
90002457 AAAengAAANSDOCLCQ111050-20841050-20841931180USPSClanton
Newspapers, 1109 Seventh St., N., PO Box 1379, Clanton, AL
35045nsdppccn-us-alThe Clanton advertiserThe Clanton
advertiser.AdvertiserClanton, Ala. :Clanton Newspapersv. :ill. ;58
cm.Three no. a week,<May 13, 1992->Semiweekly,<Apr. 4, 1990->Began in
Jan. 1990.Description based on: Vol. 19, no. 27 (Wed., Apr. 4,
1990).Latest issue consulted: Vol. 22, no. 58 (May 13, 1992).United
StatesAlabamaChiltonClanton.Independent advertiser (Clanton,
Ala.)(OCoLC)21214732AU at 000025908452info:srw/schema/1/marcxmlxml00000cas
a2200000 a 450021214814900314c19909999aluwr ne 0 a0eng dsn
90099009 AAAengAAACPNNSDOCLCQ11056-32881056-32881505740USPSThe Blount
Countian, 3rd St. at Washington Ave., PO Box 310, Oneonta, AL
35121mscnsdpn-us-alThe Blount countianThe Blount countian.Oneonta, Ala.
:Southern Democrat, Inc.,1990-v. :ill.WeeklyVol. 1, no. 1 (Jan. 3,
1990)-Editor: Molly Howard Ryan, 1990-Latest issue consulted: Vol. 1,
no. 36 (Sept. 5, 1990).Ryan, Molly Howard.United
StatesAlabamaBlountOneonta.Southern Democrat(DLC)sn
85044741(OCoLC)12038577AU at 000025884049info:srw/schema/1/marcxmlxml00000cas
a22000007a 450022413044900920c19909999aluwr ne 0 a0eng dsn
90099011
AARengAARCPNNSDNSTOCLCQ92081707011191053-91231053-91231314240USPSmscnsdpThe
Clay times-journalThe Clay times-journal.Lineville, Ala. :C.L.
Proctor,1990-v.WeeklyVol. 1, no. 1 (Sept. 6, 1990)-United
StatesAlabamaClayLineville.Ashland progress(DLC)sn 85044701Lineville
tribune(DLC)sn 85044702AUinfo:srw/schema/1/marcxmlxml00000cas a22000007a
450021265218900326c19909999aluwr ne 0 0eng dsn 90099005
AARengAARCPNOCLCQmscTrussville news-journal.Trussville, Ala. :Mike
Mitchell,1990-v.BimonthlyVol. 1, no. 1 (Feb. 20, 1990)-United
StatesAlabamaJeffersonTrussville.info:srw/schema/1/marcxmlxml00000cas
a22000007a 450022301035900831c19909999aluwr ne 0 0eng dsn
90099010 AARengAARCPNOCLCQmscWeaver tribune.Oxford, Ala. :Cheaha
Pub.,1990-v.WeeklyVol. 1, no. 1 (July 19, 1990)-United
StatesAlabamaCalhounWeaver.United
StatesAlabamaCalhounOxford.info:srw/schema/1/marcxmlxml00000cas
a22000007a 450015155895870205c19879999aludr ne 0 a0eng csn
87050045
AAAengAAACPNNSDDLCCPNNSDDLCCPNDLCOCLDLCOCLCQOCLCFOCLCQ19261126829944596670892-44570892-44571AU at 000020456714360980USPSThe 

Advertiser, P.O. Box 1000, Montgomery, AL
36192pccnsdpn-us-alNewspaperMontgomery advertiser (Montgomery, Ala. :
1987)The Montgomery advertiser(1987)The Montgomery advertiser.Montgomery
advertiser & the Alabama journalSunday Montgomery advertiserMontgomery,
Ala. :Advertiser Co.,1987-volumes
:illustrationsDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrier160th 

year, no. 1 (Jan. 2, 1987)-On Saturdays, Sundays and holidays a combined
edition is published with the Alabama journal, and called: Montgomery
advertiser and the Alabama journal, Jan. 3, 1987, and: Alabama journal
and Montgomery advertiser, Jan. 4, 1987-Feb. 25, 1990.Issues for Sunday
called: Sunday Montgomery advertiser, Mar. 4, 1990-Issues for Saturday,
Sunday and holidays have their own numbering, Jan. 3, 1987-Feb. 25,
1990.Montgomery
(Ala.)Newspapers.AlabamaMontgomery.fast(OCoLC)fst01202689Newspapers.fast(OCoLC)fst01423814United 

StatesAlabamaMontgomeryMontgomery.Advertiser (Montgomery,
Ala.)0745-3221(DLC)sn 82008412(OCoLC)9049482Alabama journal (Montgomery,
Ala. : 1940)0745-323X(DLC)sn
87062018(OCoLC)2666111info:srw/schema/1/marcxmlxml00000cas a2200000 a
450016942287871105c19879999aludn ne 0 a0eng dsn 88050149
AAAengAAACPNNSDOCLCQy1044-00701044-0070746--32780746-32781565580USPSTroy
Publications, Inc., 113 North Market St., Troy, AL 36081mscnsdpMessenger
(Troy, Ala.)The Messenger(Troy, Ala.)The Messenger.Troy, Ala. :Troy
Pub.,1987-v.Daily (Sunday, Tuesday, Thursday and Friday)Vol. 121, no.
166 (July 1, 1987)-Sunday, Apr. 2, 1989 misprinted as v. 113.Latest
issue consulted: Vol. 113 [sic 123], no. 96 (Sunday, Apr. 2,
1989).United StatesAlabamaPikeTroy.Troy messenger0746-3278(DLC)sn
83009935(OCoLC)9921908info:srw/schema/1/marcxmlxml00000cas a22000007a
450017799786880415c19879999aluir ne 0 a0eng dsn 88050086
AARengAARCPNNSDOCLCQ1p1044-03801044-03800745-75961441520USPSThe
Prattville Progress, 152 W. 3rd St., Prattville, AL
36067mscnsdpPrattville progress (Prattville, Ala. : 1987)The Prattville
progress(Prattville, Ala.)The Prattville progress.Prattville, Ala.
:James C. Seymour,1987-v.Three times a weekVol. 102, no. 8 (Jan. 20,
1987)-Latest issue consulted: Vol. 105, no. 153 (Wednesday, Dec. 26,
1990).United StatesAlabamaAutaugaPrattville.Progress (Prattville,
Ala.)0745-7596(DLC)sn
83007623(OCoLC)9428489info:srw/schema/1/marcxmlxml00000cas a22000007a
450015344667870319c19869999aluwr ne 0 a0eng dsn 87000284
NSDengNSDCPNOCLCQy0893-07670893-07671431800USPSPickens County Herald,
P.O. Drawer E, Carrollton, AL 35447nsdpPickens County heraldPickens
County herald.Pickens County herald and west AlabamianCarrollton, Ala.
:Pickens Newspapers, Inc.,1986-WeeklyVol. 138, no. 40 (Oct. 2,
1986)-United StatesAlabamaPickensCarrollton.Pickens County herald and
west Alabamian0746-0473(DLC)sn
83008141AU at 000040635809info:srw/schema/1/marcxmlxml00000cas a22000007a
450018917586881217c19869999aluwr ne 0 0eng dsn 88050225
CPNengCPNOCLCQmscThe Oxford sun/times.Oxford, Ala.
:[s.n.],1986-v.WeeklyVol. 1, no. 1 (Jan. 16, 1986)-Editor: Andy
Goggans.Numbering is irregular.United StatesAlabamaCalhounOxford.Oxford
sun (Oxford, Ala.)(DLC)sn
85045023AU at 000025803813info:srw/schema/1/marcxmlxml00000cas a22000007a
450013991168860731c19869999aluwr ne 0 0eng dsn 86050322
CPNengCPNOCLCQmscIndependent (Brewton, Ala.)The Independent.Brewton,
Ala. :Jim Thornton,1986-v. :ill. ;58 cm.WeeklyVol. 1, no. 1 (June 19,
1986)-United
StatesAlabamaEscambiaBrewton.info:srw/schema/1/marcxmlxml00000cas
a22000007a 450018957493881231c19859999aluwr ne 0 0eng dsn
88050247 CPNengCPNOCLCQmscPiedmont journal-independent (Piedmont,
Ala.)The Piedmont journal-independent.Journal independentPiedmont, Ala.
:Lane Weatherbee,1985-v.WeeklyVol. 4, no. 52 (Dec. 24, 1985)-Sometimes
published as: Journal independent.United
StatesAlabamaCalhounPiedmont.Journal-independent(DLC)sn
85045014info:srw/schema/1/marcxmlxml00000cas a22000007a
450012715821851024d19841985aluwr ne 0 a0eng dsn 85045014
CPNengCPNNSDCPNOCLCQmscThe Journal-independent.Piedmont, Ala.
:Journal-Independent, Inc.,1984-1985.volumes :illustrations ;58
cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 3,
no. 27 (July 3, 1984)- v. 4, no. 51 (Dec. 18, 1985).Carries the same
vol. numbering as the Piedmont journal-independent.United
StatesAlabamaCalhounPiedmont.Piedmont
journal-independent0890-6017(DLC)sn 85045013Piedmont journal-independent
(Piedmont, Ala.)(DLC)sn 88050247info:srw/schema/1/marcxmlxml00000cas
a22000007a 450012691448851018c19839999aludr ne 0 0eng dsn
85045007 CPNengCPNOCLCQmscTimesDaily.Times dailyFlorence, Ala. :T.S.P.
Newspapers, Inc.,1983-volumes :illustrations ;58
cmDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 114,
no. 226 (Aug. 14, 1983)-United StatesAlabamaLauderdaleFlorence.Florence
times + tri-cities daily(DLC)sn
85044995info:srw/schema/1/marcxmlxml00000cas a22000007a
45009428489830420d19831987aluir ne 0 a0eng dsn 83007623
NSDengNSDCPNNSDNSTOCLCQ89090d0745-75960745-75961The Progress, 152 W. 3rd
St., Prattville, AL 36067nsdpmscProgress (Prattville, Ala.)The
Progress(Prattville, Ala.)The Progress.Prattville, Ala. :The Prattville
Progress,1983-1987.volumes :illustrations ;58 cmThree times a
weektexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 98, no.
32 (Mar. 17, 1983)-v. 102, no. 7 (Jan. 17, 1987).United
StatesAlabamaAutaugaPrattville.Prattville progress(DLC)sn
85044740Prattville progress (Prattville, Ala.)1044-0380(DLC)sn
88050086(OCoLC)12254317AAPinfo:srw/schema/1/marcxmlxml00000cas a2200000
a 45009867255830831c19839999aludr ne 0 a0eng dsn 84008052
AAAengAAANSDOCLOCLCQX0743-15110743-15111617760USPST.S.P. Newspapers,
Inc., 219 W. Tennessee St., Florence, AL 35630nsdpTimesDaily (Shoals
edition)TimesDaily(Shoals ed.)TimesDaily.Times dailyShoals ed.Florence,
Ala. :T.S.P. Newspapersvolumes
:illustrationsDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan 

with: Vol. 114, no. 226 (Aug. 14,
1983).\"Florence/Sheffield/Tuscumbia/Muscle Shoals.\"Shoals ed. and
Regional ed. combined on Sundays.Description based on: Vol. 114, no. 346
(Monday, Dec. 12, 1983).United
StatesAlabamaLauderdaleFlorence.TimesDaily (Regional
edition)0743-152XTimes Tri-cities dailyUnknownDec. 12,
1983info:srw/schema/1/marcxmlxml00000cas a22000007a
450010536023840319c19839999aludr ne 0 a0eng dsn 84008051
NSDengNSDOCLCQ1x0743-152X0743-152X1617760USPST.S.P. Newspapers, Inc.,
219 W. Tennessee St., Florence, AL 35630nsdpTimesDaily (Regional
edition)TimesDaily(Regional ed.)TimesDaily.Times dailyRegional
ed.Florence, Ala. :T.S.P.
NewspapersDailytexttxtrdacontentunmediatednrdamediaBegan with: Vol. 114,
no. 226 (Aug. 14, 1983).Shoals ed. and Regional ed. combined on
Sundays.Description based on: Vol. 114, no. 346 (Monday, Dec. 12,
1983).United StatesAlabamaLauderdaleFlorence.TimesDaily (Shoals
edition)0743-1511Times Tri-cities dailyDec. 12,
1983AU at 000025818125info:srw/schema/1/marcxmlxml00000cas a22000007a
45009049482821213d19821987aludn ne 0 a0eng csn 82008412
AAAengAAANSDNPWCPNDLCCPNNSDDLCNSDDLCCPNNVFDLCOCLCQCRLOCLCFOCLCQ1d0745-32210745-32211nsdppccn-us-alNewspaperAdvertiser 

(Montgomery, Ala.)The Advertiser(Montgomery, Ala.)The advertiser.Alabama
journal and advertiserMontgomery, Ala. :Advertiser Co.,1982-1987.volumes
:illustrationsDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrier155th 

year, no. 232 (Nov. 22, 1982)- ; -v. 14-3, Jan. 1, 1987.On Saturdays,
Sundays and holidays published as: The Alabama journal and advertiser,
Nov. 27, 1982-Jan. 1, 1987.Saturday, Sunday and holiday issues have
their own numbering.Montgomery
(Ala.)Newspapers.AlabamaMontgomery.fast(OCoLC)fst01202689Newspapers.fast(OCoLC)fst01423814United 

StatesAlabamaMontgomeryMontgomery.Montgomery advertiser (Montgomery,
Ala. : Daily)(DLC)sn 84020645(OCoLC)2685433Montgomery advertiser
(Montgomery, Ala. : 1987)0892-4457(DLC)sn
87050045(OCoLC)15155895AU at 000020281746info:srw/schema/1/marcxmlxml00000cas
a2200000 a 45009237931830218c19829999aluwr ne 0 0eng dsn
86050139 AAAengAAACPNOCLOCLCQmscThe Randolph leader.Roanoke, Ala. :David
S. Stevenson,1982-volumes :illustrations ;58
cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 91,
no. 1 (Oct. 6, 1982)-United StatesAlabamaRandolphRoanoke.Roanoke
leader(DLC)sn 86050137Randolph press(DLC)sn
86050138info:srw/schema/1/marcxmlxml00000cas a22000007a
450012715815851024d19821984aluwr ne 0 a0eng dsn 85045013
CPNengCPNNSDCPNOCLCQ110890-60170890-60171432080USPSThe Piedmont
Journal-Independent, 115 N. Center Ave., Piedmont, AL 36272mscnsdpThe
Piedmont journal-independentThe Piedmont journal-independent.Piedmont,
Ala. :Piedmont Journal-Independent, Inc.,1982-1984.volumes
:illustrations ;58
cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 1,
no. 1 (Mar. 31, 1982)-v. 3, no. 26 (June 27, 1984).Latest issue
consulted: Vol. 5, no. 31 (August 20, 1986).United
StatesAlabamaCalhounPiedmont.Piedmont journal(DLC)sn
85045012Journal-independent(DLC)sn
85045014(OCoLC)12715821AU at 000045312916info:srw/schema/1/marcxmlxml00000cas
a22000007a 45009183905830202c19829999aluwr n 0 a0eng dsn
85044580 AAAengAAACPNNSDOCLOCLCQ11098-58671098-58671016409USPSNo. 4,
Rucker Plaza, Enterprise, AL 36331P.O. Box 1536, Enterprise, AL
36331mscnsdpSoutheast sun (Enterprise, Ala.)The southeast
sun(Enterprise, Ala.)The Southeast sun.Enterprise, Ala. :QST
Publicationsvolumes :illustrations ;58
cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan in
1982.Description based on: Vol. 1, no. 25 (Oct. 21, 1982).Latest issue
consulted: Vol. 16, no. 43 (Mar. 4, 1998).United
StatesAlabamaCoffeeEnterprise.AU at 000025827687info:srw/schema/1/marcxmlxml00000cas 

a22000007a 450010487314840305c19819999aluwr ne 0 a0eng dsn
85044906
AAAengAAACPNNSDNSTCPNOCLOCLCQOCLCFOCLCOOCLCAOCLCQ900410885-16620885-16621749310USPSThe 

New Times, 1618 1/2 St. Stephens Rd., Mobile, AL 36603mscnsdpn-us-alNew
times (Mobile, Ala.)The New times(Mobile, Ala.)The new times.Mobile,
Ala. :New Times Groupvolumes
:illustrationsWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan 

in 1981.Vol. 3, no. 49 (Dec. 15-21, 1983) and vol. 3, no. 50 (Dec.
22-28, 1983) are both called vol. 3, no. 49 (Dec. 15-21,
1983).Description based on: Vol. 2, no. 3 (Jan. 28-Feb. 3, 1982).African
AmericansAlabamaNewspapers.African
Americans.fast(OCoLC)fst00799558Alabama.fast(OCoLC)fst01204694Newspapers.fast(OCoLC)fst01423814United 

StatesAlabamaMobileMobile.AAPUnknownAug. 15,
1985AU at 000024686659info:srw/schema/1/marcxmlxml00000cas a22000007a
450018922463881219d19811983alucr ne 0 0eng dsn 88050233
AARengAARCPNNSDOCLCQmscThe Sylacauga daily advance.Advance/Sylacauga
dailySylacauga advanceSunday advanceAdvanceSylacauga, Ala. :Mrs. W.A.
Moody,1981-1893.v.Semiweekly,<Nov. 24, 1982-Feb. 13, 1983>Daily (except
Mon., Tues. & Sat.),<May 26, 1982-Nov. 21, 1982>Daily (except Sat. &
Mon.),<Jan. 1, 1981-May 23, 1982>74th Year, no. 123 (Jan. 1, 1981)-76th
year, no. 83 (Feb. 13, 1983).Days of publication vary.Published as: The
Advance/Sylacauga daily, <Aug. 28, 1981-May 23, 1982>.Published as:
Sylacauga advance, <Nov. 24, 1982-Feb. 13, 1983>.On Sunday, published
as: Sunday advance.United StatesAlabamaTalladegaSylacauga.Childersburg
star(DLC)sn 88050232Coosa press(DLC)sn 86050293Daily
home1059-6461(DLC)sn 88050234info:srw/schema/1/marcxmlxml00000cas
a22000007a 450021026715cr un|||||||||900209c19809999aluwr ne 0
0eng dsn 90099002
AARengAARCPNCUSOCLOCLCQTJCOCLCQOCLCFOCLCOOCLCA926143844AU at 000020585756mscn-us-alSpeakin' 

out news.Speaking out newsDecatur, Ala. :Minority Network,
Inc.v.WeeklyBegan in 1980.Published in Huntsville, Ala., <1987>-Also
issued by subscription via the World Wide Web.Description based on: Vol.
7, no. 8 (Jan. 7-13, 1987).African AmericansAlabamaNewspapers.African
American
newspapersAlabama.AlabamaNewspapers.Newspapers.fast(OCoLC)fst01423814African 

American newspapers.fast(OCoLC)fst00799278African
Americans.fast(OCoLC)fst00799558Alabama.fast(OCoLC)fst01204694United
StatesAlabamaMorganDecatur.United
StatesAlabamaMadisonHuntsville.Speakin' out weekly news(DLC)sn
88050097http://www.softlineweb.com/softlineweb/ethnic.htminfo:srw/schema/1/marcxmlxml00000cas 

a22000007a 450014996511861219c19809999aluwr ne 0 a0eng csn
86050472
AARengAARCPNNSDOCLCQ11080-15021080-15021328110USPSnsdppccWest-Alabama
gazetteWest-Alabama gazette.GazetteMillport, Ala. :Millport Pub.
Co.,1980-volumesWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrier4th 

year, no. 32 (Jan. 3, 1980)-United StatesAlabamaLamarMillport.Gazette
(Millport, Ala.)(DLC)sn 86050471info:srw/schema/1/marcxmlxml00000cas
a2200000 a 450011828156850320c19809999aluwr ne 0 0eng dsn
86050314 AAAengAAACPNOCLOCLCQmscThe Hartford news-herald.Hartford, Ala.
:Geneva Publications,1980-volumes :illustrations ;57-59
cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 80,
no. 20 (Feb. 14, 1980)-United StatesAlabamaGenevaHartford.News-herald
(Hartford, Ala.)(DLC)sn 86050313info:srw/schema/1/marcxmlxml00000cas
a22000007a 450017857788880427d198u198ualusr ne 0 0eng dsn
88050097 AARengAARCPNOCLOCLCQOCLCFOCLCOOCLCAmscn-us-alSpeakin' out
weekly news.Decatur, Ala. :Smothers PublicationsPublished every first
and third Wed. of each monthDescription based on: Vol. 3, no. 13 (May
4-17, 1983).African
AmericansAlabamaNewspapers.Newspapers.fast(OCoLC)fst01423814African
Americans.fast(OCoLC)fst00799558Alabama.fast(OCoLC)fst01204694United
StatesAlabamaMorganDecatur.Weekly news (Huntsville, Ala.)(DLC)sn
87050012Speakin' out news(DLC)sn
90099002info:srw/schema/1/marcxmlxml00000cas a2200000 a
450017807936880418c198u9999aluwr ne 0 a0eng dsn 90099001
AAAengAAACPNOCLOCLCQThe Daleville Sun-Courier, 310 Daleville Ave.,
Daleville, AL 36322mscn-us-alDaleville sun-courier.Daleville, Ala. :QST
Publicationsv. :ill. ;58 cm.WeeklyDescription based on: Vol. 2, no. 28
(Wed., Feb. 17, 1988).United
StatesAlabamaDaleDaleville.AU at 000020585749info:srw/schema/1/marcxmlxml00000cas 

a22000007a 450015580838870423c198u9999aluwr ne 0 0eng dsn
87050128 AARengAARCPNOCLCQmscGreene County independent.Eutaw, Ala.
:Greene County Independent, Inc.v.WeeklyDescription based on: Vol. 2,
no. 10 (Mar. 12, 1987).United
StatesAlabamaGreeneEutaw.info:srw/schema/1/marcxmlxml00000cas a22000007a
450010125135831114d198u198ualucr ne 0 a0eng dsn 83003221
NSDengNSDOCLCQ0d0746-55210746-55211Auburn Bulletin & Lee County Eagle,
PO Box 2111, Auburn, Ala. 36830nsdpThe Auburn bulletin & the Lee County
eagleThe Auburn bulletin & the Lee County eagle.Lee County eagleAuburn
bulletin and the Lee County eagleAuburn, Ala. :[publisher not
identified]Semiweekly,<Sept. 5,
1984->WeeklytexttxtrdacontentunmediatednrdamediaDescription based on:
Oct. 19, 1983.United StatesAlabamaLeeAuburn.Auburn bulletin(DLC)sn
89050006Eagle (Auburn, Ala.)(OCoLC)18435663Sept. 5,
1984info:srw/schema/1/marcxmlxml00000cas a22000007a
450018370324880818c198u9999aluwr ne 0 0eng dsn 88050147
CPNengCPNOCLCQmscTri-city times (Geraldine, Ala.)The Tri-City
times.Geraldine, Ala. :Wanda Nelsonv.WeeklyDescription based on: Vol. 2,
no. 24 (Jan. 6, 1982).United
StatesAlabamaDeKalbGeraldine.info:srw/schema/1/marcxmlxml00000cas
a22000007a 450010199338831208c198u9999aluwr ne 0 a0eng dsn
83005367 NSDengNSDCPNOCLCQ10746-62770746-62771707590USPSSpringville Pub.
Co., 539 Main St., Springville, AL 35146nsdpThe St. Clair clarionThe St.
Clair clarion.Saint Clair clarionSpringville, AL :Gary L.
ShultsWeeklytexttxtrdacontentunmediatednrdamediaDescription based on:
Vol. 2, no. 1 (Jan. 5, 1982).United StatesAlabamaSt.
ClairSpringville.AU at 000025783743info:srw/schema/1/marcxmlxml00000cas
a22000007a 450013787251860627c198u9999aluwr ne 0 a0eng dsn
86001923 NSDengNSDCPNOCLCQ10889-00800889-00801The Westerner Star, P.O.
Box 2060, Bessemer, AL 35021nsdpWestern star (Bessemer, Ala.)The Western
star(Bessemer, Ala.)The western star.Bessemer, Ala. :Hal
HodgensWeeklytexttxtrdacontentunmediatednrdamediaDescription based on:
Vol. 3, no. 15 (Wednesday, June 11, 1986).United
StatesAlabamaJeffersonBessemer.Bessemer advertiser(DLC)sn
87050117AU at 000025805174511.1srw.pc any \"y\" and srw.mt any
\"newspaper\" and srw.cp exact
\"Alabama\"50info:srw/schema/1/marcxmlxml1Date,,0mq1lME887FoIbjulKUV6bx9ImwWQNCv9GqZzGS92IKS31lEbcpRJBNHgcE1l29tFaHP9CHe0Yexk1uWQofffull" 

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 27 2022, 09:22am via System

Hello Spencer,

Thank you for reaching out about the bulk xml files for the US Newspaper 
Directory.

We don't have documentation specific to these bulk xml files, but upon 
further inspection I can say that each of those files don't necessarily 
contain info for 50 newspaper titles. The structure of the titles for 
California and New York for instance are different from say, Alabama.

If you look at California for example, the file naming structure 
indicates the year the title started, and then the number of titles 
included in that xml file. So for instance, the files below include info 
for newspapers that started in 2000, 2001, and 2002 respectively. And 
there is info for 30 titles in the xml file from 2000, and 14 in the 
file for 2001, and so on.

   * ndnp_California_2000_e_0001_0030.xml
   * ndnp_California_2001_e_0001_0014.xml
   * ndnp_California_2002_e_0001_0012.xml

If there's more than 50 titles for a given year, say for California 
starting in 1880, then the next 50 titles will roll into the next xml 
file, and so on. And the last xml file for that year may not include 50 
titles.

Many of the states seem to group all the years together, so each xml 
file contains 50 titles, until possibly the last one for a given state, 
which may contain less.

I hope this information helps explain the total number of records and 
structure a bit better. Let me know if you have any further questions.

Best wishes,

Kerry Huller
Newspaper & Current Periodical Reading Room
Serial & Government Publications Division
Library of Congress

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 25 2022, 02:22pm via Email

Hi, Kerry:


Might there be documentation on the XML files you mentioned?


I've successfully read
'https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/',
extracted the names of 6666 XML files, and read the first one,
"ndnp_Alabama_all-yrs_e_0001_0050.xml". It contains 29415 characters,
beginning, "1.12250info:srw/schema/1/marcxmlxml00000nas a22000007i
45001030438981180404c20159999aluwr n 0 a0eng ". With a bit
more effort, I will likely be able to parse all 6666 of these. The
names suggest that each contains information on 50 newspapers, totaling
333,300. The main page
"https://chroniclingamerica.loc.gov/search/titles/" says there are only
157,521 "Titles currently listed". This suggests that these XML files
include place holders for a little more than double the number of
entries currently in "https://chroniclingamerica.loc.gov/search/titles/".


Thanks for this.


Progress.
------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 07 2022, 08:55am via System

Hi Spencer,

I thought of one more option after I emailed you yesterday that I wanted 
to make you aware of.

I had explained the other day how we pull the records from OCLC into our 
U.S. Newspaper Directory. You can also access all of?the raw MARC 
records found in the directory in xml format from here if you choose: 
https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/ 
<https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/>?These?will 
provide you all of the data from the record fields in MARC format, so 
you'd get all the data you see here for example: 
https://chroniclingamerica.loc.gov/lccn/sn98059792/marc/ 
<https://chroniclingamerica.loc.gov/lccn/sn98059792/marc/>?but in xml. I 
don't know if this might be more data and info than you want to work 
with, but wanted to make sure you were aware of this option as well.

Best wishes,

Kerry Huller
Newspaper & Current Periodical Reading Room
Serial & Government Publications Division
Library of Congress

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 06 2022, 10:55am via System

Hi Spencer,

Thanks for reaching out again. I have been looking at the json view a 
bit closer this morning and your example of "9999."

After talking with a colleague this morning and looking at various 
examples, I see there is some variation in how the titles with either an 
unknown starting/ending date or currently published titles are being 
handled - depending on the view.

As an example, I completed a search in the directory for Alaska and the 
city of Anchorage. There are 80 results, and?on the first page of 
results you'll see # 4. Fort Richardson news, which was published from 
1952-19??. The csv view of this state/city search result will show the 
ending date of 19??. But if I append &format=json to this search result, 
this specific title will show an ending date of 1999. After talking with 
a colleague this morning, I discovered an integer had to be used in 
these cases where dates were "?" so that the search based on year range 
would work. Similarly, if you look at # 12 Alaska digest, which was 
published 1994-current, the "current" becomes "9999" in the json view. 
So, the records you are seeing with "9999" would most likely be titles 
with an ending date of "current."

However, there is an issue with the unknown dates, like "1999" being 
used for "19??" in the example above. The "9" does not get inserted in 
place of "?" when you are looking at the title/LCCN view of a specific 
newspaper. So for instance, if you view the #4 title: Fort Richardson 
news at this url: https://chroniclingamerica.loc.gov/lccn/sn98059792/ 
<https://chroniclingamerica.loc.gov/lccn/sn98059792/>?but append .json 
to the end of the url, after the LCCN, like this: 
https://chroniclingamerica.loc.gov/lccn/sn98059792.json 
<https://chroniclingamerica.loc.gov/lccn/sn98059792.json>?you'll see 
that the end_year is "19??." Viewing the title/LCCN json view for titles 
that are currently published will also show the end_year as "current." 
The Alaska digest example from above can be viewed here: 
https://chroniclingamerica.loc.gov/lccn/sn97060056.json 
<https://chroniclingamerica.loc.gov/lccn/sn97060056.json>

I wasn't aware of the difference between the directory search json view 
and the title/LCCN view. But I think it would be possible to grab 
the?data from?the title/LCCN json url through an additional script 
potentially. The json url is included in the view under the?"url" field.

Of course, there are unknowns with publishing dates, but better to know 
where the question marks are, and what titles are considered to be current.

I hope this clarifies the data a bit more - let me know if any of it 
needs more clarification though. And let me know if you have follow-up 
questions.

Thank you,

Kerry Huller
Newspaper & Current Periodical Reading Room
Serial & Government Publications Division
Library of Congress

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 05 2022, 04:42pm via Email

Hi, Kerry:


What would you suggest I do to get a count of the numbers of
newspapers and publishers operating by year from, say, 1790 to 2021?


I just determined that 20630 (13 percent) of the 157520 records in
the US Newspaper database I downloaded a week ago have end_year = 9999.
I don't think it's feasible to assume that all or even most of those
are still publishing.


Might there be some other database that might have this kind of
information?


I ask, because Robert McChesney (2004) The Problem of the Media
(Monthly Review Pr., esp. pp. 34-35) suggests that in the first half of
the nineteenth century, the US had more newspapers and newspaper
publishers per capita than any other place or time. He suggests that
that diversity of newspapers helped encourage literacy and limit
political corruption, both of which helped propel the young US to its
current dominance of the international political economy. I'm hoping to
get some data to evaluate this claim. Sadly, it looks like there is too
much missing and questionable data in this dataset for me to use this
without a fairly substantive data cleaning effort.
------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 05 2022, 09:05am via System

Hello Spencer,

Thank you for reaching out about your additional questions.

I was looking at the records you mention above, and yes, you are correct 
- those 9 records with the date inconsistencies and the one record?for 
the The New Mexican mining news 
<https://chroniclingamerica.loc.gov/lccn/sn93061507/> containing "Santa 
Fe.\" have typos in them. Thanks for spotting these - it may be possible 
to have the cataloger in our division correct those typos. I will look 
into this further.

The U.S. Newspaper Directory doesn't have a connection with Wikimedia or 
Wikipedia. The Library of Congress?periodically pulls the records for 
the Directory from OCLC Worldcat 
<https://www.oclc.org/en/worldcat.html>. And those?newspaper records in 
OCLC Worldcat have been created by catalogers?at various institutions 
around the U.S. over the span of several years. So, occasionally, you 
will find a typo in the records. Corrections can be?made by OCLC and 
library staff at the various institutions. Every time we complete a new 
pull on the OCLC records, any corrected records will then populate our 
Directory.

Regarding your question on the New-York weekly journal - yes, that is 
also correct that it has two records. There is actually a?record?for 
each format of the newspaper, so this record is for the microfilm format 
<https://chroniclingamerica.loc.gov/lccn/2009252748/> and this one is 
for the original print format 
<https://chroniclingamerica.loc.gov/lccn/sn83030211/>. You can see in 
the heading for the microfilm record where it says [microfilm reel] and 
the print version shows [volume]. You are likely to see this for other 
titles as well because each format has been cataloged with its own LCCN. 
You are also likely to see additional records with [online resource] 
identified as the format as more and more titles are available as 
ePrints or online.

I hope this helps answer your additional questions a bit more. Please 
reach out if you have any other questions.

Thank you,

Kerry Huller
Newspaper & Current Periodical Reading Room
Serial & Government Publications Division
Library of Congress

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 04 2022, 01:47pm via Email

Hi, Kelly:


At the risk of bombing your inbox with more emails than you want,
what is your relationship with Wikipedia and other Wikimedia Foundation
projects like Wikidata?


I ask, because I've logged over 20,000 edits in Wikimedia Foundation
projects since 2010, and I would happily try to answer questions about
Wikidata and other Wikimedia Foundation projects. I have NOT organized
an edit-a-thon, but I've made presentations at conferences with people
who have, and I would happily try to help organize such if you could
find a group of people who want to work to improve this US Newspaper
database. I think it would be good to establish links between this US
Newspaper database and Wikidata, with appropriate procedures so changes
to one could be evaluated for acceptance into the other.


FYI, John Peter Zenger's famous "New-York weekly journal" (1733-1751)
appears TWICE in your database with lccn = 2009252748 and sn83030211 and
ONCE in Wikidata WITHOUT an lccn, even though many other Wikidata items
have an lccn. See:


https://www.wikidata.org/wiki/Q23091960


There's a "WikiProject Newspapers" on Wikipedia and a companion
"WikiProject Periodicals" on Wikidata:


https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Newspapers/Wikidata


https://www.wikidata.org/wiki/Wikidata:WikiProject_Periodicals


I've tried to connect with others on those projects, so far with only
limited success. However, you may know that almost anyone can change
almost anything on Wikipedia and other Wikimedia Foundation projects.
What stays tends to be written from a neutral point of view citing
credible sources. They have problems with vandals, but the problems are
usually easily controlled. This makes Wikipedia and Wikidata very
useful platforms for cleaning up databases like your US Newspaper dataset.


Spencer Graves


##########


Hello, Kelly:


In addition to the invalid JSON, discussed below [NOTE: The "below"
contains a slight addition to the report of the I sent last Friday.], I
found 9 (NINE!) cases where start_year was AFTER end_year. These have
lccn = "sn86071531" "sn95069213" "sn90059096" "sn86058451" "sn90060926"
"sn99065409" "sn89065002" "sn98069857" "sn91059179"


See:


https://chroniclingamerica.loc.gov/lccn/sn86071531/
https://chroniclingamerica.loc.gov/lccn/sn95069213/
https://chroniclingamerica.loc.gov/lccn/sn90059096/
https://chroniclingamerica.loc.gov/lccn/sn86058451/
https://chroniclingamerica.loc.gov/lccn/sn90060926/
https://chroniclingamerica.loc.gov/lccn/sn99065409/
https://chroniclingamerica.loc.gov/lccn/sn89065002/
https://chroniclingamerica.loc.gov/lccn/sn98069857/
https://chroniclingamerica.loc.gov/lccn/sn91059179/


These all have obvious coding errors that can be easily fixed. The
data may not be completely accurate after the fix, but at least they are
not obviously wrong ;-)


##################

I got invalid JSON from:


https://chroniclingamerica.loc.gov/search/titles/results/?rows=500&page=103&format=json


After some experimentation, I was able to replicate the problem with
a request for rows=10:


https://chroniclingamerica.loc.gov/search/titles/results/?rows=10&page=5117&format=json


Duncan Temple Lang <dtemplelang at ucdavis.edu>, Professor of Statistics
and Associate Dean for Graduate Programs at the University of California
- Davis, confirmed that it was a JSON error using:


https://codebeautify.org/jsonvalidator


He is part of the core team developing the R free, open-source
programming language. He said, that starting at offsets 161070 and
161502 in the character string you get from [the R code RCurl::getURL()]
we have:


Santa Fe.\"


and these are in an entry such as


"city": ["Santa Fe.\"]


So the final " is escaped and therefore there is no closing " for the
string. The parser continues to consume characters looking for the end
of that string.


If one "repairs" the text from getURL() with


ftxt= gsub('Santa Fe.\\\\"', 'Santa Fe."', txt)


then the rest of my code worked fine.


You may wish to do something to implement other checks for valid JSON
and repair this problem. I've scanned all the 157520 records that were
in that database a couple of days ago, and this is the only JSON error
identified by the code I used.


NOTE: I was NOT able to replicate this error when downloading records
one at a time. That suggests a problem NOT in the database itself but
in the download algorithm. ???


Thank you for your help. I will almost certainly have other
questions ;-)
------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 03 2022, 10:39pm via Email

Hello, Kelly:


In addition to the invalid JSON, discussed below [NOTE: The "below"
contains a slight addition to the report of the I sent last Friday.], I
found 9 (NINE!) cases where start_year was AFTER end_year. These have
lccn = "sn86071531" "sn95069213" "sn90059096" "sn86058451" "sn90060926"
"sn99065409" "sn89065002" "sn98069857" "sn91059179"


See:


https://chroniclingamerica.loc.gov/lccn/sn86071531/
https://chroniclingamerica.loc.gov/lccn/sn95069213/
https://chroniclingamerica.loc.gov/lccn/sn90059096/
https://chroniclingamerica.loc.gov/lccn/sn86058451/
https://chroniclingamerica.loc.gov/lccn/sn90060926/
https://chroniclingamerica.loc.gov/lccn/sn99065409/
https://chroniclingamerica.loc.gov/lccn/sn89065002/
https://chroniclingamerica.loc.gov/lccn/sn98069857/
https://chroniclingamerica.loc.gov/lccn/sn91059179/


These all have obvious coding errors that can be easily fixed. The
data may not be completely accurate after the fix, but at least they are
not obviously wrong ;-)


##################

I got invalid JSON from:


https://chroniclingamerica.loc.gov/search/titles/results/?rows=500&page=103&format=json


After some experimentation, I was able to replicate the problem with
a request for rows=10:


https://chroniclingamerica.loc.gov/search/titles/results/?rows=10&page=5117&format=json


Duncan Temple Lang <dtemplelang at ucdavis.edu>, Professor of Statistics
and Associate Dean for Graduate Programs at the University of California
- Davis, confirmed that it was a JSON error using:


https://codebeautify.org/jsonvalidator


He is part of the core team developing the R free, open-source
programming language. He said, that starting at offsets 161070 and
161502 in the character string you get from [the R code RCurl::getURL()]
we have:


Santa Fe.\"


and these are in an entry such as


"city": ["Santa Fe.\"]


So the final " is escaped and therefore there is no closing " for the
string. The parser continues to consume characters looking for the end
of that string.


If one "repairs" the text from getURL() with


ftxt= gsub('Santa Fe.\\\\"', 'Santa Fe."', txt)


then the rest of my code worked fine.


You may wish to do something to implement other checks for valid JSON
and repair this problem. I've scanned all the 157520 records that were
in that database a couple of days ago, and this is the only JSON error
identified by the code I used.


NOTE: I was NOT able to replicate this error when downloading records
one at a time. That suggests a problem NOT in the database itself but
in the download algorithm. ???


Thank you for your help. I will almost certainly have other
questions ;-)
------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 01 2022, 11:46am via Email

Hello, Kelly:


I got invalid JSON from:


https://chroniclingamerica.loc.gov/search/titles/results/?rows=500&page=103&format=json


After some experimentation, I was able to replicate the problem with
a request for rows=10:


https://chroniclingamerica.loc.gov/search/titles/results/?rows=10&page=5117&format=json


Duncan Temple Lang <dtemplelang at ucdavis.edu>, Professor of Statistics
and Associate Dean for Graduate Programs at the University of California
- Davis, confirmed that it was a JSON error using:


https://codebeautify.org/jsonvalidator


He is part of the core team developing the R free, open-source
programming language. He said, that starting at offsets 161070 and
161502 in the character string you get from [the R code RCurl::getURL()]
we have:


Santa Fe.\"


and these are in an entry such as


"city": ["Santa Fe.\"]


So the final " is escaped and therefore there is no closing " for the
string. The parser continues to consume characters looking for the end
of that string.


If one "repairs" the text from getURL() with


ftxt= gsub('Santa Fe.\\\\"', 'Santa Fe."', txt)


then the rest of my code worked fine.


You may wish to do something to implement other checks for valid JSON
and repair this problem. I've scanned all the 157520 records that were
in that database a couple of days ago, and this is the only JSON error
identified by the code I used.


Thank you for your help. I will almost certainly have other
questions ;-)
------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jun 28 2022, 02:20pm via System

Hello Spencer,

Thank you for sending along your follow-up questions.

I'm glad to hear the json view?will work for you. It was recommended to 
me that you limit your requests to 500 rows at a time. And a developer 
here at LC suggests the following regarding rate limiting:

?To avoid being blocked by the server, the current rate-limiting rules 
restrict un-cached requests to URLs starting with 
https://chroniclingamerica.loc.gov/search/ 
<https://chroniclingamerica.loc.gov/search/> to 120 requests every 10 
minutes from a single IP address.?

So, I think if you limited each of your requests to 500 rows at a time 
with the proper pauses, then you should be able to access what you need.

As for the csv view, I checked on this as well, and was informed that 
the?csv view was not implemented for all url formats. The csv view was 
only implemented for this view: 
https://chroniclingamerica.loc.gov/newspapers/ 
<https://chroniclingamerica.loc.gov/newspapers/>and urls resulting from 
US Directory search results - for e.g. if you wanted to narrow down your 
search results by state, city, date range, etc. found at this link: 
https://chroniclingamerica.loc.gov/search/titles/ 
<https://chroniclingamerica.loc.gov/search/titles/>. So, if you wanted a 
csv and limited your search by state ( for example: 
https://chroniclingamerica.loc.gov/search/titles/results/?state=Alaska&county=&city=&year1=1690&year2=2022&terms=&frequency=&language=&ethnicity=&labor=&material_type=&lccn=&rows=20&format=csv 
<https://chroniclingamerica.loc.gov/search/titles/results/?state=Alaska&county=&city=&year1=1690&year2=2022&terms=&frequency=&language=&ethnicity=&labor=&material_type=&lccn=&rows=20&format=csv> 
), you could append &format=csv to the search result url and get the csv 
to automatically download. But, if your search results ended up being 
over a couple thousand titles, then the system would probably time out.

I hope this info helps! Let me know if you have any other questions.

Best wishes,

Kerry Huller
Newspaper & Current Periodical Reading Room
Serial & Government Publications Division
Library of Congress

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jun 27 2022, 04:15pm via Email

Hello, Kerry:


Thanks for the reply. Can you please give me some further guidance
on two thing "so that the system is not overwhelmed"?


1. The max size in a small batch?


2. Any limit on the number of small batches in a second or minute?


I've found that I can download small batches under program control
using "RCurl::getURL" in R (programming language) using, e.g.;


https://chroniclingamerica.loc.gov/search/titles/results/?rows=20&page=2&format=json


With this, I can control the batch size with "row=20" vs. "row=50"
vs., e.g., "row=1000". A naive search says there are 157520 "results".
With "row=1000", this would require 158 calls. With "row=20", it
would require 7876 calls. Before I start, I need to decide which fields
I want; I don't need them all.


Thanks,
Spencer Graves


p.s. I tried appending "&format=csv" and got "Error 504 Ray ID:
7220896da85e86e7 ? 2022-06-27 19:19:53 UTC Gateway time-out". I used:


https://chroniclingamerica.loc.gov/search/titles/results/?state=&county=&city=&year1=1690&year2=2022&terms=&frequency=&language=&ethnicity=&labor=&material_type=&lccn=&rows=20&format=csv


I can get what I want using json so do not need csv. However, I
thought you might want to know that I was unable to get csv to work.
------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jun 27 2022, 10:54am via System

Hello Spencer,

Thank you for contacting the Library of Congress about searching the US 
Newspaper Directory. I wanted to follow up with you regarding your 
request to output the data in a machine readable format.

It looks like you were provided the link to the API documentation for 
the website: About the Site and API 
<https://chroniclingamerica.loc.gov/about/api/>. Scroll down to the 
section with the heading, Searching the directory and newspaper pages 
using OpenSearch. This section describes the search functionality and 
structure for the US Newspaper Directory in more detail. It is possible 
to return your directory searches in json format by appending 
&format=json to the end of the url. It is also possible to return search 
results in csv format by appending &format=csv to the end of the url, 
but I would strongly suggest that you do this in small batches by 
putting limits on your search so that the system is not overwhelmed.

So, from the search page for the US Newspaper Directory 
<https://chroniclingamerica.loc.gov/search/titles/>?you could 
potentially limit your search based on state?and city, or date range, 
and/or even frequency. Then once you've completed the search, you can 
add &format=csv to the end of the url to automatically download a csv of 
those records. The resulting csv will contain several fields/headers: 
lccn, title, place of publication, start year, end year, publisher, 
edition, frequency, subject, state, city, country, language, oclc 
number, and holding type. I think these fields include the information 
you were looking for. But, again, I would like to stress that you put 
limits on your search before creating the csv so as not overwhelm the 
system.

Please let me know if you have any other additional questions.

Best wishes,

Kerry Huller
Newspaper & Current Periodical Reading Room
Serial & Government Publications Division
Library of Congress

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jun 23 2022, 01:55pm via System

Mr. Graves,

I'm going to transfer you request to a member of our digital collections 
team who may be of more assistance to you than me.

Mike

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jun 23 2022, 01:51pm via Email

Dear Mr. Queen:


Thanks for the reply. I'm still confused. I downloaded and
installed Docker Desktop and "docker-compose.yml" and ran their "Getting
Started" Tutorial, but I don't see what to do next.


I repeat: I'd like to analyze "U.S. Newspaper Directory,
1690-Present" (https://chroniclingamerica.loc.gov/search/titles/), which
------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jun 22 2022, 07:15pm via System

Mr. Graves,

Programmatic access to the data forChronicling America 
<https://chroniclingamerica.loc.gov/>and possibly the U.S. Newspaper 
Directory <https://chroniclingamerica.loc.gov/search/titles/>can be 
found on theAbout the Site and API 
<https://chroniclingamerica.loc.gov/about/api/>page in various formats. 
Also, please note that Chronicling Americacontains newspapers published 
from 1777-1963, but does not include everyU.S. newspaper published in 
that time period.

Please let me know if I can be of further assistance.


------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jun 22 2022, 06:14pm via Email

Dear Mr. Queen:


Can we simplify this to just giving me the data behind "U.S.
Newspaper Directory, 1690-Present"
(https://chroniclingamerica.loc.gov/search/titles/) in a machine
readable format, e.g., csv or xlsx or a MySQL database?


As I mentioned in my original email, a naive search of that without
restrictions returned 157520 titles in 7876 pages with up to 20 titles
per page giving date ranges in at least some cases. I could probably
write software to scrape those 7876 pages from your web site and combine
them into a data file.


I have a PhD in statistics, I have been using the R programming
language and similar software for decades. This includes publishing
tutorials on how to analyze data like this on Wikiversity.[1] I'd like
to do something similar with this. I could help make your data more
useful to others and discuss with you how we might prioritize
improvements like accessing the other sources you mentioned.


Thanks very much for your reply.


Sincerely,
Spencer Graves, PhD
Founder, EffectiveDefense.org
4550 Warwick Blvd 508
Kansas City, MO 64111
m: 408-655-4567


[1] e.g.:


https://en.wikiversity.org/wiki/US_Gross_Domestic_Product_(GDP)_per_capita
------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jun 22 2022, 05:27pm via System

Mr. Graves

Your request is a little more complex than it first appears and requires 
extensive research. A variety of resources should be consulted to 
determine the circulation statistics of newspapers published prior to 
1851. You will need to check newspaper union lists and newspaper 
histories. Union listspresent lists of newspapers in geographic 
arrangement according to place of publication, and specify which 
libraries or other institutions hold collections of those newspapers and 
the dates of their holdings. These can also be useful for tracking title 
changes throughout a newspaper's history. Newspaper 
historieslikeAmerican Journalism: A History: 1690-1960 
<https://lccn.loc.gov/62007157>(Mott),The Penny Press 
<https://lccn.loc.gov/2004043078>(Thompson), andThe Press and America 
<https://lccn.loc.gov/99044295>(Emery et al.) may not include 
circulation statistics, but they do document the diversity and progress 
of newspaper publishing, including notable newspapers of the era. 
Newspaper histories also cover the history of the printers and printing 
of newspapers in a state, county, or region more generally, and provide 
more condensed histories of the editors, journalists, and evolution of 
the newspapers in a specific area. Newspaper histories and union lists 
should be available at most large public or university libraries. More 
information about union lists, newspaper histories, and researching 
newspapers in general can be found in theU.S. Newspaper Collections at 
the Library of Congress 
<https://guides.loc.gov/united-states-newspapers/introduction>research 
guide (see Reference Sources).

Please let me know if I can be of further assistance.

------------------------------------------------------------------------

Original Question

Jun 20 2022, 02:34pm via System

How can I get counts of the numbers of newspapers by year in the US, and 
preferably also elsewhere? A search of "U.S. Newspaper Directory,
How can I get counts of the numbers of newspapers by year in the US, and 
preferably also elsewhere?

A search of "U.S. Newspaper Directory, 1690-Present" 
(https://chroniclingamerica.loc.gov/search/titles/) returned 157520 
titles in 7876 pages with up to 20 titles per page giving date ranges to 
the extent that it's known. If I can get a data file (e.g., csv or xls), 
I can summarize. I could also use data on circulation and frequency and 
especially parent company for multiple newspapers published by the same 
company, to the extant that such is available.

I'm interested in this, because McChesney quoted Tocqueville in 
suggesting that the US had more newspapers per person (or per million 
population) prior to 1851 than at any other time or place in history. 
I'd like to evaluate that claim with data to the extent that I can. See 
"https://en.wikiversity.org/wiki/Social_construction_of_crime_and_what_we_can_do_about_it#Newspapers_1790_-_present". 


Thanks, Spencer Graves, PhD
m: 408-655-4567

------------------------------------------------------------------------

Thank you for using Newspapers & Current Periodicals Ask a Librarian 
Service!


This email is sent from Ask a Librarian in relationship to ticket #9625195.

Read our privacy policy. <https://springshare.com/privacy.html>


From @vi@e@gross m@iii@g oii gm@ii@com  Thu Jul 28 00:04:37 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Wed, 27 Jul 2022 18:04:37 -0400
Subject: [R] Parsing XML?
In-Reply-To: <7743581c-0959-1df6-bb31-f140161f83e1@effectivedefense.org>
References: <20220727145903.4338c449692f703b@ask.loc.gov>
 <7743581c-0959-1df6-bb31-f140161f83e1@effectivedefense.org>
Message-ID: <002601d8a204$dd3226d0$97967470$@gmail.com>

General XML is not intended to be parsable as a list. But there are lots of tools you can use to extract various patterns out of XML in forms like a list. 

But your data example is huge and I am falling asleep waiting to see if it loads. I looked sideways and it is not that big directly but my browser may be trying to show it as a web page.

How about you copying and pasting a sample of say the first few dozen lines so we see what is in it for the purpose of ...

The schema would be mentioned in an attribute if you know what you are looking for and may be an external file.

So decide what you want, like a list of all titles and use something like xpath().


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Spencer Graves
Sent: Wednesday, July 27, 2022 4:51 PM
To: 'R-help' <r-help at r-project.org>
Subject: [R] Parsing XML?

Hello, All:


	  What would you suggest I do to parse the following XML file into a 
list that I can understand:


XMLfile <- 
"https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/ndnp_Alabama_all-yrs_e_0001_0050.xml" 



	  This is the first of 6666 XML files containing "U.S. Newspaper 
Directory" maintained by the US Library of Congress discussed in the 
thread below.  I've tried various things using the XML and xml2.


XMLdata <- xml2::read_xml(XMLfile)
str(XMLdata)
XMLdat <- XML::xmlParse(XMLdata)
str(XMLdat)
XMLtxt <- xml2::xml_text(XMLdata)
nchar(XMLtxt)
#[1] 29415


	  Someplace there's a schema for this.  I don't know if it's embedded 
in this XML file or in a separate file.  If it's in a separate file, how 
could I describe it to my contacts with the Library of Congress so they 
would understand what I needed and could help me get it.


	  Thanks,
	  Spencer Graves


p.s.  All 29415 characters in XMLtext appear in the thread below.  	  	


-------- Forwarded Message --------
Subject: 	[Newspapers and Current Periodicals] How can I get counts of 
the numbers of newspapers by year in the US, and preferably also 
elsewhere? A search of "U.S. Newspaper Directory,
Date: 	Wed, 27 Jul 2022 14:59:03 +0000
From: 	Kerry Huller <serials at ask.loc.gov>
To: 	Spencer Graves <spencer.graves at effectivedefense.org>
CC: 	twes at loc.gov



--# Type your reply above this line #--

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 27 2022, 10:59am via System

Hello Spencer,

So, when I view the xml, I'm actually looking at it in XML editor 
software, so I can view the tags and it's structured neatly. I've copied 
and pasted the text from the beginning of the file and the first 
newspaper title below from my XML editor:

<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type='text/xsl' 
href='/webservices/catalog/xsl/searchRetrieveResponse.xsl'?>

<searchRetrieveResponse xmlns="http://www.loc.gov/zing/srw/" 
xmlns:oclcterms="http://purl.org/oclc/terms/" 
xmlns:dc="http://purl.org/dc/elements/1.1/" 
xmlns:diag="http://www.loc.gov/zing/srw/diagnostic/" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
<version>1.1</version>
<numberOfRecords>2250</numberOfRecords>
<records>
<record>
<recordSchema>info:srw/schema/1/marcxml</recordSchema>
<recordPacking>xml</recordPacking>
<recordData>
<record xmlns="http://www.loc.gov/MARC21/slim">
      <leader>00000nas a22000007i 4500</leader>
      <controlfield tag="001">1030438981</controlfield>
      <controlfield tag="008">180404c20159999aluwr n       0   a0eng 
  </controlfield>
      <datafield ind1=" " ind2=" " tag="010">
        <subfield code="a">  2018200464</subfield>
      </datafield>
      <datafield ind1=" " ind2=" " tag="040">
        <subfield code="a">DLC</subfield>
        <subfield code="e">rda</subfield>
        <subfield code="c">DLC</subfield>
        <subfield code="b">eng</subfield>
      </datafield>
      <datafield ind1=" " ind2=" " tag="012">
        <subfield code="m">1</subfield>
      </datafield>
      <datafield ind1="0" ind2=" " tag="022">
        <subfield code="a">2577-5316</subfield>
        <subfield code="2">1</subfield>
      </datafield>
      <datafield ind1=" " ind2=" " tag="032">
        <subfield code="a">021110</subfield>
        <subfield code="b">USPS</subfield>
      </datafield>
      <datafield ind1=" " ind2=" " tag="037">
        <subfield code="b">711 Alabama Avenue, Selma, AL 36701</subfield>
      </datafield>
      <datafield ind1=" " ind2=" " tag="042">
        <subfield code="a">nsdp</subfield>
        <subfield code="a">pcc</subfield>
      </datafield>
      <datafield ind1="1" ind2="0" tag="050">
        <subfield code="a">ISSN RECORD</subfield>
      </datafield>
      <datafield ind1="1" ind2="0" tag="082">
        <subfield code="a">071</subfield>
        <subfield code="2">15</subfield>
      </datafield>
      <datafield ind1=" " ind2="0" tag="222">
        <subfield code="a">Selma sun</subfield>
      </datafield>
      <datafield ind1="0" ind2="0" tag="245">
        <subfield code="a">Selma sun.</subfield>
      </datafield>
      <datafield ind1=" " ind2="1" tag="264">
        <subfield code="a">Selma, AL :</subfield>
        <subfield code="b">North Shore Press, LLC</subfield>
        <subfield code="c">2016-</subfield>
      </datafield>
      <datafield ind1=" " ind2=" " tag="310">
        <subfield code="a">Weekly</subfield>
      </datafield>
      <datafield ind1=" " ind2=" " tag="336">
        <subfield code="a">text</subfield>
        <subfield code="b">txt</subfield>
        <subfield code="2">rdacontent</subfield>
      </datafield>
      <datafield ind1=" " ind2=" " tag="337">
        <subfield code="a">unmediated</subfield>
        <subfield code="b">n</subfield>
        <subfield code="2">rdamedia</subfield>
      </datafield>
      <datafield ind1=" " ind2=" " tag="338">
        <subfield code="a">volume</subfield>
        <subfield code="b">nc</subfield>
        <subfield code="2">rdacarrier</subfield>
      </datafield>
      <datafield ind1="1" ind2=" " tag="362">
        <subfield code="a">Began in 2015.</subfield>
      </datafield>
      <datafield ind1=" " ind2=" " tag="588">
        <subfield code="a">Description based on: Volume 2, Issue 40 
(October 5, 2017) (surrogate); title from caption.</subfield>
      </datafield>
      <datafield ind1=" " ind2=" " tag="588">
        <subfield code="a">Latest issue consulted: Volume 2, Issue 40 
(October 5, 2017).</subfield>
      </datafield>
      <datafield ind1=" " ind2=" " tag="752">
        <subfield code="a">United States</subfield>
        <subfield code="b">Alabama</subfield>
        <subfield code="c">Dallas</subfield>
        <subfield code="d">Selma.</subfield>
      </datafield>
    </record>
</recordData>
</record>

When I view the records in the XML editor, these 2 lines below do begin 
each of the records for each individual title, but of course this is 
including the xml tags:

<recordSchema>info:srw/schema/1/marcxml</recordSchema>
<recordPacking>xml</recordPacking>

Hopefully this helps you decide where to break or parse each record.

On another note, I just noticed as well that at the top of this first 
file it lists the total number of records for the Alabama grouping - 
2250. This also appeared to be the case for the Alaska records when I 
took a look at the first one for that state. I imagine that should be 
consistent throughout each "grouping" of records.

Let me know if you have follow-up questions!

Best wishes,

Kerry Huller
Newspaper & Current Periodical Reading Room
Serial & Government Publications Division
Library of Congress

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 27 2022, 10:21am via Email

Hi, Kerry:


Thanks. I understand the chunking in files of at most 50. I've read
the first file "ndnp_Alabama_all-yrs_e_0001_0050.xml" into a string of
29415 characters, copied below. Might you have any suggestions on the
next step in parsing this? Staring at it now, it looks splitting on
"info:srw/schema/1/marcxmlxml" might convert the 29415 characters into
shorter chunks, each of which could then be parsed further.


This is not as bad as reading ancient Egyptian heiroglyphics without
the Rosetta Stone, but I wondered if you might have something that could
make this work easier and more reliable? I guess I could compare with
what I already read as JSON ;-)


Thanks,
Spencer Graves


"1.12250info:srw/schema/1/marcxmlxml00000nas a22000007i
45001030438981180404c20159999aluwr n 0 a0eng
2018200464DLCrdaDLCeng12577-53161021110USPS711 Alabama Avenue, Selma, AL
36701nsdppccISSN RECORD07115Selma sunSelma sun.Selma, AL :North Shore
Press,
LLC2016-WeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan in
2015.Description based on: Volume 2, Issue 40 (October 5, 2017)
(surrogate); title from caption.Latest issue consulted: Volume 2, Issue
40 (October 5, 2017).United
StatesAlabamaDallasSelma.info:srw/schema/1/marcxmlxml00000cas a22000007a
4500502150053100127c20109999aluwr n 0 a0eng
2010200019DLCengDLCDLCOCLCQ112153-18111750USPSB & C Publishing, LLC,
3514 Martin St. S. Ste 104, Cropwell, AL 35054pccnsdpISSN RECORDSt.
Clair County news (Cropwell, Ala.)St. Clair County news(Cropwell,
Ala.)St. Clair County news.Cropwell, AL :B & C Pub.WeeklyBegan in
2010.Description based on: Nov. 4, 2010 (surrogate); title from
caption.info:srw/schema/1/marcxmlxml00000cas a22000007a
4500426491872090720c20099999alumr n 0 a0eng
2009203372DLCengDLCOCLCQ12150-346X2150-346X1AU at 000044489617NZ116076352Devon
Applewhite/Applewhite Publishing Co., 1910 Honeysuckle Rd., #N183,
Dothan, AL 36305mscnsdpISSN RECORD30514Triangle tribune(Dothan,
Ala.)Triangle tribune.Dothan, AL :Applewhite Pub. CoMonthlyBegan with
vol. 1, issue 1 (May 2009).\"Connecting the Tri-State African -American
Community.\"Description based on: Vol. 1, issue 1 (May 2009); title from
masthead.Applewhite, Devon.United StatesAlabama.United
StatesGeorgia.United StatesFlorida.info:srw/schema/1/marcxmlxml00000cas
a22000007a 4500289017315081219c20089999aluwr n | a0eng c
2008213218NSDengNSDOCLCQDLCOCLCQ111945-93191945-93191005270USPSSpringhill Publications,
LLC, P.O. Box 186, Greenville, AL 36037nsdppccISSN RECORD07014Greenville
standardThe Greenville standard.Greenville, AL :Springhill
PublicationsWeeklytexttxtrdacontentunmediatednrdamediaBegan with vol. 1,
issue 1 (Sept. 3, 2008)Description based on surrogate of: Vol. 1, no. 15
(Dec. 18, 2008); title from masthead (publisher's Web site, viewed Dec.
19, 2008).Latest issue consulted: Vol. 1, no. 99 (July 27, 2011)
(surrogate).info:srw/schema/1/marcxmlxml00000cas a22000007a
4500123539969070426c20079999aluwr ne 0 a0eng c
2007212138NSDengNSDNSDOCLCQ101936-95571936-95571The Western Tribune,
1530 Third Ave. N., Bessemer, AL 35020mscnsdpISSN RECORDWestern tribune
(Bessemer, Ala.)The Western tribune(Bessemer, Ala.)The Western
tribune.Bessemer, Ala. :D-Med, Inc.v.WeeklyBegan in 2007.Description
based on: May 23, 2007 (surrogate); title from
caption.AU at 000041575341info:srw/schema/1/marcxmlxml00000cas a22000007a
4500226300653080425c20079999aluwr ne | a0eng
2008212112NSDengNSDNSDOCLCQ11942-20751942-20751nsdppccISSN RECORDThe
corridor messengerThe corridor messenger.Carbon Hill, AL :Corridor
Messenger, Inc.WeeklyBegan with vol. 1, issue (10.03.2007).Description
based on: 1st issue.United StatesAlabamaWalkerCarbon
Hill.http://www.corridormessenger.cominfo:srw/schema/1/marcxmlxml00000cas a22000007a
450077560432070109c20069999aluwr ne 0 a0eng c
2007213400NSDengNSDOCLCQAUBRNOCLCOOCLCFa01935-37901935-37901AU at 000041190283The 

Auburn Villager, P.O. Box 1633, Auburn, AL 36831-1633pccnsdpISSN
RECORDThe Auburn villagerThe Auburn villager.Auburn, AL :Auburn
Villagerv.WeeklyBegan in 2006.Description based on: Vol. 1, no. 4 (July
20, 2006) (surrogate); title from caption.Auburn (Ala.)Newspapers.Lee
County (Ala.)Newspapers.AlabamaAuburn.fast(OCoLC)fst01209634AlabamaLee
County.fast(OCoLC)fst01211930Newspapers.fast(OCoLC)fst01423814United
StatesAlabamaLeeAuburn.info:srw/schema/1/marcxmlxml00000cas a2200000Ii
4500872286785m o d s cr mn|---a||||140311c20069999alucr n o b
s0 a0eng cABCengrdaABCABCOCLCFLD59.13University of Alabama at
Birmingham.The eReporter.[Birmingham, Alabama] :The University of
Alabama at Birmingham,[2006]-[Birmingham, Alabama] :Offices of Public
Relations & Marketing and Information Technology1 online resource2
issues weeklytexttxtrdacontentcomputercrdamediaonline
resourcecrrdacarrierSeptember 19, 2006-\"The eReporter is an official
communication of The University of Alabama at Birmingham, companion to
the UAB Reporter and recommended alternative to mass e-mails.\"Issues
for <March 11, 2014- published and distributed via e-mail subscription
on Tuesdays and Fridays.Description based on: September 19, 2006; title
from title screen (viewed March 12, 2014).University of Alabama at
BirminghamPeriodicals.Periodicals.fast(OCoLC)fst01411641University of
Alabama at Birmingham.fast(OCoLC)fst00645114University of Alabama at
Birmingham.Office of Public Relations and Marketing.University of
Alabama at Birmingham.Information Technology.2006-2012, companion
to:University of Alabama at Birmingham.UAB
reporter.(OCoLC)32435748Archived
issueshttp://hatteras.dpo.uab.edu/cgi-bin/ereporter.cgiinfo:srw/schema/1/marcxmlxml00000cas 

a22000007a 4500166387050070829c20059999aluwr ne | a0eng c
2007215501NSDengNSDOCLCQ11939-68991939-68991The Wilkie Clark Memorial
Foundation, P.O. Box 514, Roanoke, AL 36274$30.00nsdpmscISSN
RECORD305.89614People's voice (Roanoke, Ala.)The people's voice(Roanoke,
Ala.)The people's voice.Roanoke, AL :Wilkie Clark Memorial
Foundationv.WeeklyBegan with vol. 1, no. 1 in 2005.Description based on:
Vol. 2, no. 20 (Apr. 20, 2007); title from caption.Wilkie Clark Memorial
Foundation.United
StatesAlabamaRandolphRoanoke.AU at 000042141390info:srw/schema/1/marcxmlxml00000nas 

a22000007i 45001124677787191021c20uu9999aluwr ne | a0eng
2019202521DLCengrdaDLC12689-3258122730USPSNorth Jackson Press, 42950 Hwy
72, Suite 406, Stevenson, AL 35772nsdppccISSN RECORD071.323North Jackson
pressNorth Jackson press.Stevenson, AL :Caney Creek Publications
LLCWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierDescription
based on surrogate of: Volume 1, number 36 (October 11, 2019); title
from masthead.Latest issue consulted: Volume 1, number 36 (October 11,
2019) (Surrogate).United
StatesAlabamaJacksonStevensoninfo:srw/schema/1/marcxmlxml00000cas
a2200000 a 4500226315099080428d19981998aluwr ne | 0eng c
2008233691GUAengGUAOCLCQOCLCFOCLCO39911644pccn-us-gaThe Dekalb
news.Birmingham, Ala. :Community newspaper holdings Inc.v.WeeklyBegan
with 1st year, no. 1 (Apr. 1, 1998); ceased with 1st year, no. 31 (Oct.
28, 1998).Final issue consulted.Description based on first issue; title
from caption.Decatur (Ga.)Newspapers.DeKalb County
(Ga.)Newspapers.Newspapers.fast(OCoLC)fst01423814GeorgiaDecatur.fast(OCoLC)fst01226234GeorgiaDeKalb 

County.fast(OCoLC)fst01215288United
StatesGeorgiaDeKalbDecatur.Decatur-DeKalb news/era(DLC)sn
89053661(OCoLC)19946163info:srw/schema/1/marcxmlxml00000cas a2200000 i
450050263311m o d cr cn|||||||||020730c19979999alu x neo
0 a0eng c
2015238492AMHengrdapnAMHOCLCQOCLCFOCLCOIULOCLHTMOCLCQCOODLC66460694810970435082687-93791AU at 000050711528OCLCS45109pccnsdpn-us---AP2.B5707023Birmingham 

weekly (Online)Birmingham weekly(Online)Birmingham weekly.Birmingham, AL
:Birmingham Weekly1 online resourceIrregular,Feb. 16-28,
2012-Weekly,Sept. 4-11, 1997-Feb. 9-16,
2012texttxtrdacontentcomputercrdamediaonline resourcecrrdacarrierBegan
with vol. 1, issue 1 (Sept. 4-11, 1997).\"City news, views &
entertainment\"--Cover.Numbering dropped in Mar. 2012.Also issued in
print.Description based on: Publication information from ProQuest; title
from web page (viewed June 18, 2015).Latest issue consulted: Aug. 15-20,
2012.Birmingham (Ala.)Newspapers.Internet resources.Electronic
journals.AlabamaBirmingham.fast(OCoLC)fst01204958Newspapers.fast(OCoLC)fst01423814United 

StatesAlabamaBirmingham.Print version:Birmingham
Weekly(OCoLC)39271050http://apw.softlineweb.com/http://WC2VB5MT8E.search.serialssolutions.com/?sid=sersol&SS_jc=JC_000051895&title=Birmingham+Weeklyinfo:srw/schema/1/marcxmlxml00000cas 

a22000007a 450031471314941116d19941995aluwr ne 0 a0eng csn
94003083
NSDengNSDANEOCLCQOCLCFOCLCOOCLCQ11079-65411079-65411nsdppccn-us-akSoutheast
shopperSoutheast shopper.Juneau, Alaska :Kemper
Communications,1994-volumesWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 

1, no. 1 (Nov. 16, 1994)-Ceased in Feb. 1995.Juneau
(Alaska)Newspapers.AlaskaJuneau.fast(OCoLC)fst01213587Newspapers.fast(OCoLC)fst01423814United 

StatesAlaskaJuneau.AU at 000011356572info:srw/schema/1/marcxmlxml00000cas
a22000008a 450027910515930413c19949999alumr n 0 a0eng dsn
93002581 NSDengNSDOCLCQ11069-06621Birmingham Tribune, 216 Ave. T. Pratt
City, Birmingham, AL 35214nsdpBirmingham tribuneBirmingham
tribune.Birmingham, Ala. :Kervin
Fondren9501volumesMonthlytexttxtrdacontentunmediatednrdamediavolumencrdacarrierPREPUB: 

publication expected Jan.
1995AU at 000025863987info:srw/schema/1/marcxmlxml00000cas a22000007a
450026199931920716d19922013alumr ne 0 a0eng csn 92003357
NSDengNSDOCLOCLCQDLC011064-01341064-01341Black & White, POB 13215,
Birmingham, AL 35202-3215nsdppccBlack & white (Birmingham, Ala.)Black &
white(Birmingham, Ala.)Black & white.Black and whiteBirmingham, Ala.
:Black & White, Inc.v.Biweekly,Oct. 2, 1997-Monthly,May 1, 1992-Sept.
1997Began in May 1992; ceased with Jan. 10, 2013.\"Birmingham's New City
paper.\"Description based on: June 1992.Latest issue consulted: No. 67
(Oct. 16, 1997) (surrogate).info:srw/schema/1/marcxmlxml00000cas
a2200000 a 450032145723950314d19901999alumr ne 0 a0eng csn
95068755
MGNengMGNNSDCLUOCLCQOCLCFOCLCOOCLCA971211082-34841082-34841AU at 000011579542nsdppccn-us-alF335.J5S68The 

Southern shofarThe Southern shofar.Birmingham, AL :L. Brook,-[1999]v.
:ill. ;35 cm.MonthlyBegan in 1990.-v. 9, issue 9 (Aug./Sept. 1999).\"The
monthly newspaper of Alabama's Jewish community.\"Some issues also
available on the Internet via the World Wide Web.Description based on:
Vol. 3, issue 11 (Oct. 1993).Jewish newspapersAlabama.Jewish
newspapers.fast(OCoLC)fst00982872Alabama.fast(OCoLC)fst01204694United
StatesAlabamaJeffersonBirmingham.Deep South Jewish voice(DLC)sn
99018499(OCoLC)42431704CLUhttp://bibpurl.oclc.org/web/719http://www.bham.net/shofar/info:srw/schema/1/marcxmlxml00000cas 

a22000007a 450021265141900326c19909999aluwr ne 0 a0eng csn
90099004 AARengAARCPNNSDOCLCQ11050-08981050-08981005022USPSE.O.N., Inc.,
Main St., Eclectic, AL 36024pccnsdpISSN RECORDThe Eclectic observerThe
Eclectic observer.Eclectic, Ala. :E.O.N., Inc.,1990-v.WeeklyVol. 1, no.
1 (Feb. 22, 1990)-Published by: Price Publications, Inc., <2006->Latest
issue consulted: Vol. 17, no. 1 (Jan. 5, 2006).United
StatesAlabamaElmoreEclectic.AU at 000040212446info:srw/schema/1/marcxmlxml00000cas 

a22000007a 450021214781900314c19909999aluir ne 0 a0eng csn
90002457 AAAengAAANSDOCLCQ111050-20841050-20841931180USPSClanton
Newspapers, 1109 Seventh St., N., PO Box 1379, Clanton, AL
35045nsdppccn-us-alThe Clanton advertiserThe Clanton
advertiser.AdvertiserClanton, Ala. :Clanton Newspapersv. :ill. ;58
cm.Three no. a week,<May 13, 1992->Semiweekly,<Apr. 4, 1990->Began in
Jan. 1990.Description based on: Vol. 19, no. 27 (Wed., Apr. 4,
1990).Latest issue consulted: Vol. 22, no. 58 (May 13, 1992).United
StatesAlabamaChiltonClanton.Independent advertiser (Clanton,
Ala.)(OCoLC)21214732AU at 000025908452info:srw/schema/1/marcxmlxml00000cas
a2200000 a 450021214814900314c19909999aluwr ne 0 a0eng dsn
90099009 AAAengAAACPNNSDOCLCQ11056-32881056-32881505740USPSThe Blount
Countian, 3rd St. at Washington Ave., PO Box 310, Oneonta, AL
35121mscnsdpn-us-alThe Blount countianThe Blount countian.Oneonta, Ala.
:Southern Democrat, Inc.,1990-v. :ill.WeeklyVol. 1, no. 1 (Jan. 3,
1990)-Editor: Molly Howard Ryan, 1990-Latest issue consulted: Vol. 1,
no. 36 (Sept. 5, 1990).Ryan, Molly Howard.United
StatesAlabamaBlountOneonta.Southern Democrat(DLC)sn
85044741(OCoLC)12038577AU at 000025884049info:srw/schema/1/marcxmlxml00000cas
a22000007a 450022413044900920c19909999aluwr ne 0 a0eng dsn
90099011
AARengAARCPNNSDNSTOCLCQ92081707011191053-91231053-91231314240USPSmscnsdpThe
Clay times-journalThe Clay times-journal.Lineville, Ala. :C.L.
Proctor,1990-v.WeeklyVol. 1, no. 1 (Sept. 6, 1990)-United
StatesAlabamaClayLineville.Ashland progress(DLC)sn 85044701Lineville
tribune(DLC)sn 85044702AUinfo:srw/schema/1/marcxmlxml00000cas a22000007a
450021265218900326c19909999aluwr ne 0 0eng dsn 90099005
AARengAARCPNOCLCQmscTrussville news-journal.Trussville, Ala. :Mike
Mitchell,1990-v.BimonthlyVol. 1, no. 1 (Feb. 20, 1990)-United
StatesAlabamaJeffersonTrussville.info:srw/schema/1/marcxmlxml00000cas
a22000007a 450022301035900831c19909999aluwr ne 0 0eng dsn
90099010 AARengAARCPNOCLCQmscWeaver tribune.Oxford, Ala. :Cheaha
Pub.,1990-v.WeeklyVol. 1, no. 1 (July 19, 1990)-United
StatesAlabamaCalhounWeaver.United
StatesAlabamaCalhounOxford.info:srw/schema/1/marcxmlxml00000cas
a22000007a 450015155895870205c19879999aludr ne 0 a0eng csn
87050045
AAAengAAACPNNSDDLCCPNNSDDLCCPNDLCOCLDLCOCLCQOCLCFOCLCQ19261126829944596670892-44570892-44571AU at 000020456714360980USPSThe 

Advertiser, P.O. Box 1000, Montgomery, AL
36192pccnsdpn-us-alNewspaperMontgomery advertiser (Montgomery, Ala. :
1987)The Montgomery advertiser(1987)The Montgomery advertiser.Montgomery
advertiser & the Alabama journalSunday Montgomery advertiserMontgomery,
Ala. :Advertiser Co.,1987-volumes
:illustrationsDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrier160th 

year, no. 1 (Jan. 2, 1987)-On Saturdays, Sundays and holidays a combined
edition is published with the Alabama journal, and called: Montgomery
advertiser and the Alabama journal, Jan. 3, 1987, and: Alabama journal
and Montgomery advertiser, Jan. 4, 1987-Feb. 25, 1990.Issues for Sunday
called: Sunday Montgomery advertiser, Mar. 4, 1990-Issues for Saturday,
Sunday and holidays have their own numbering, Jan. 3, 1987-Feb. 25,
1990.Montgomery
(Ala.)Newspapers.AlabamaMontgomery.fast(OCoLC)fst01202689Newspapers.fast(OCoLC)fst01423814United 

StatesAlabamaMontgomeryMontgomery.Advertiser (Montgomery,
Ala.)0745-3221(DLC)sn 82008412(OCoLC)9049482Alabama journal (Montgomery,
Ala. : 1940)0745-323X(DLC)sn
87062018(OCoLC)2666111info:srw/schema/1/marcxmlxml00000cas a2200000 a
450016942287871105c19879999aludn ne 0 a0eng dsn 88050149
AAAengAAACPNNSDOCLCQy1044-00701044-0070746--32780746-32781565580USPSTroy
Publications, Inc., 113 North Market St., Troy, AL 36081mscnsdpMessenger
(Troy, Ala.)The Messenger(Troy, Ala.)The Messenger.Troy, Ala. :Troy
Pub.,1987-v.Daily (Sunday, Tuesday, Thursday and Friday)Vol. 121, no.
166 (July 1, 1987)-Sunday, Apr. 2, 1989 misprinted as v. 113.Latest
issue consulted: Vol. 113 [sic 123], no. 96 (Sunday, Apr. 2,
1989).United StatesAlabamaPikeTroy.Troy messenger0746-3278(DLC)sn
83009935(OCoLC)9921908info:srw/schema/1/marcxmlxml00000cas a22000007a
450017799786880415c19879999aluir ne 0 a0eng dsn 88050086
AARengAARCPNNSDOCLCQ1p1044-03801044-03800745-75961441520USPSThe
Prattville Progress, 152 W. 3rd St., Prattville, AL
36067mscnsdpPrattville progress (Prattville, Ala. : 1987)The Prattville
progress(Prattville, Ala.)The Prattville progress.Prattville, Ala.
:James C. Seymour,1987-v.Three times a weekVol. 102, no. 8 (Jan. 20,
1987)-Latest issue consulted: Vol. 105, no. 153 (Wednesday, Dec. 26,
1990).United StatesAlabamaAutaugaPrattville.Progress (Prattville,
Ala.)0745-7596(DLC)sn
83007623(OCoLC)9428489info:srw/schema/1/marcxmlxml00000cas a22000007a
450015344667870319c19869999aluwr ne 0 a0eng dsn 87000284
NSDengNSDCPNOCLCQy0893-07670893-07671431800USPSPickens County Herald,
P.O. Drawer E, Carrollton, AL 35447nsdpPickens County heraldPickens
County herald.Pickens County herald and west AlabamianCarrollton, Ala.
:Pickens Newspapers, Inc.,1986-WeeklyVol. 138, no. 40 (Oct. 2,
1986)-United StatesAlabamaPickensCarrollton.Pickens County herald and
west Alabamian0746-0473(DLC)sn
83008141AU at 000040635809info:srw/schema/1/marcxmlxml00000cas a22000007a
450018917586881217c19869999aluwr ne 0 0eng dsn 88050225
CPNengCPNOCLCQmscThe Oxford sun/times.Oxford, Ala.
:[s.n.],1986-v.WeeklyVol. 1, no. 1 (Jan. 16, 1986)-Editor: Andy
Goggans.Numbering is irregular.United StatesAlabamaCalhounOxford.Oxford
sun (Oxford, Ala.)(DLC)sn
85045023AU at 000025803813info:srw/schema/1/marcxmlxml00000cas a22000007a
450013991168860731c19869999aluwr ne 0 0eng dsn 86050322
CPNengCPNOCLCQmscIndependent (Brewton, Ala.)The Independent.Brewton,
Ala. :Jim Thornton,1986-v. :ill. ;58 cm.WeeklyVol. 1, no. 1 (June 19,
1986)-United
StatesAlabamaEscambiaBrewton.info:srw/schema/1/marcxmlxml00000cas
a22000007a 450018957493881231c19859999aluwr ne 0 0eng dsn
88050247 CPNengCPNOCLCQmscPiedmont journal-independent (Piedmont,
Ala.)The Piedmont journal-independent.Journal independentPiedmont, Ala.
:Lane Weatherbee,1985-v.WeeklyVol. 4, no. 52 (Dec. 24, 1985)-Sometimes
published as: Journal independent.United
StatesAlabamaCalhounPiedmont.Journal-independent(DLC)sn
85045014info:srw/schema/1/marcxmlxml00000cas a22000007a
450012715821851024d19841985aluwr ne 0 a0eng dsn 85045014
CPNengCPNNSDCPNOCLCQmscThe Journal-independent.Piedmont, Ala.
:Journal-Independent, Inc.,1984-1985.volumes :illustrations ;58
cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 3,
no. 27 (July 3, 1984)- v. 4, no. 51 (Dec. 18, 1985).Carries the same
vol. numbering as the Piedmont journal-independent.United
StatesAlabamaCalhounPiedmont.Piedmont
journal-independent0890-6017(DLC)sn 85045013Piedmont journal-independent
(Piedmont, Ala.)(DLC)sn 88050247info:srw/schema/1/marcxmlxml00000cas
a22000007a 450012691448851018c19839999aludr ne 0 0eng dsn
85045007 CPNengCPNOCLCQmscTimesDaily.Times dailyFlorence, Ala. :T.S.P.
Newspapers, Inc.,1983-volumes :illustrations ;58
cmDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 114,
no. 226 (Aug. 14, 1983)-United StatesAlabamaLauderdaleFlorence.Florence
times + tri-cities daily(DLC)sn
85044995info:srw/schema/1/marcxmlxml00000cas a22000007a
45009428489830420d19831987aluir ne 0 a0eng dsn 83007623
NSDengNSDCPNNSDNSTOCLCQ89090d0745-75960745-75961The Progress, 152 W. 3rd
St., Prattville, AL 36067nsdpmscProgress (Prattville, Ala.)The
Progress(Prattville, Ala.)The Progress.Prattville, Ala. :The Prattville
Progress,1983-1987.volumes :illustrations ;58 cmThree times a
weektexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 98, no.
32 (Mar. 17, 1983)-v. 102, no. 7 (Jan. 17, 1987).United
StatesAlabamaAutaugaPrattville.Prattville progress(DLC)sn
85044740Prattville progress (Prattville, Ala.)1044-0380(DLC)sn
88050086(OCoLC)12254317AAPinfo:srw/schema/1/marcxmlxml00000cas a2200000
a 45009867255830831c19839999aludr ne 0 a0eng dsn 84008052
AAAengAAANSDOCLOCLCQX0743-15110743-15111617760USPST.S.P. Newspapers,
Inc., 219 W. Tennessee St., Florence, AL 35630nsdpTimesDaily (Shoals
edition)TimesDaily(Shoals ed.)TimesDaily.Times dailyShoals ed.Florence,
Ala. :T.S.P. Newspapersvolumes
:illustrationsDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan 

with: Vol. 114, no. 226 (Aug. 14,
1983).\"Florence/Sheffield/Tuscumbia/Muscle Shoals.\"Shoals ed. and
Regional ed. combined on Sundays.Description based on: Vol. 114, no. 346
(Monday, Dec. 12, 1983).United
StatesAlabamaLauderdaleFlorence.TimesDaily (Regional
edition)0743-152XTimes Tri-cities dailyUnknownDec. 12,
1983info:srw/schema/1/marcxmlxml00000cas a22000007a
450010536023840319c19839999aludr ne 0 a0eng dsn 84008051
NSDengNSDOCLCQ1x0743-152X0743-152X1617760USPST.S.P. Newspapers, Inc.,
219 W. Tennessee St., Florence, AL 35630nsdpTimesDaily (Regional
edition)TimesDaily(Regional ed.)TimesDaily.Times dailyRegional
ed.Florence, Ala. :T.S.P.
NewspapersDailytexttxtrdacontentunmediatednrdamediaBegan with: Vol. 114,
no. 226 (Aug. 14, 1983).Shoals ed. and Regional ed. combined on
Sundays.Description based on: Vol. 114, no. 346 (Monday, Dec. 12,
1983).United StatesAlabamaLauderdaleFlorence.TimesDaily (Shoals
edition)0743-1511Times Tri-cities dailyDec. 12,
1983AU at 000025818125info:srw/schema/1/marcxmlxml00000cas a22000007a
45009049482821213d19821987aludn ne 0 a0eng csn 82008412
AAAengAAANSDNPWCPNDLCCPNNSDDLCNSDDLCCPNNVFDLCOCLCQCRLOCLCFOCLCQ1d0745-32210745-32211nsdppccn-us-alNewspaperAdvertiser 

(Montgomery, Ala.)The Advertiser(Montgomery, Ala.)The advertiser.Alabama
journal and advertiserMontgomery, Ala. :Advertiser Co.,1982-1987.volumes
:illustrationsDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrier155th 

year, no. 232 (Nov. 22, 1982)- ; -v. 14-3, Jan. 1, 1987.On Saturdays,
Sundays and holidays published as: The Alabama journal and advertiser,
Nov. 27, 1982-Jan. 1, 1987.Saturday, Sunday and holiday issues have
their own numbering.Montgomery
(Ala.)Newspapers.AlabamaMontgomery.fast(OCoLC)fst01202689Newspapers.fast(OCoLC)fst01423814United 

StatesAlabamaMontgomeryMontgomery.Montgomery advertiser (Montgomery,
Ala. : Daily)(DLC)sn 84020645(OCoLC)2685433Montgomery advertiser
(Montgomery, Ala. : 1987)0892-4457(DLC)sn
87050045(OCoLC)15155895AU at 000020281746info:srw/schema/1/marcxmlxml00000cas
a2200000 a 45009237931830218c19829999aluwr ne 0 0eng dsn
86050139 AAAengAAACPNOCLOCLCQmscThe Randolph leader.Roanoke, Ala. :David
S. Stevenson,1982-volumes :illustrations ;58
cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 91,
no. 1 (Oct. 6, 1982)-United StatesAlabamaRandolphRoanoke.Roanoke
leader(DLC)sn 86050137Randolph press(DLC)sn
86050138info:srw/schema/1/marcxmlxml00000cas a22000007a
450012715815851024d19821984aluwr ne 0 a0eng dsn 85045013
CPNengCPNNSDCPNOCLCQ110890-60170890-60171432080USPSThe Piedmont
Journal-Independent, 115 N. Center Ave., Piedmont, AL 36272mscnsdpThe
Piedmont journal-independentThe Piedmont journal-independent.Piedmont,
Ala. :Piedmont Journal-Independent, Inc.,1982-1984.volumes
:illustrations ;58
cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 1,
no. 1 (Mar. 31, 1982)-v. 3, no. 26 (June 27, 1984).Latest issue
consulted: Vol. 5, no. 31 (August 20, 1986).United
StatesAlabamaCalhounPiedmont.Piedmont journal(DLC)sn
85045012Journal-independent(DLC)sn
85045014(OCoLC)12715821AU at 000045312916info:srw/schema/1/marcxmlxml00000cas
a22000007a 45009183905830202c19829999aluwr n 0 a0eng dsn
85044580 AAAengAAACPNNSDOCLOCLCQ11098-58671098-58671016409USPSNo. 4,
Rucker Plaza, Enterprise, AL 36331P.O. Box 1536, Enterprise, AL
36331mscnsdpSoutheast sun (Enterprise, Ala.)The southeast
sun(Enterprise, Ala.)The Southeast sun.Enterprise, Ala. :QST
Publicationsvolumes :illustrations ;58
cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan in
1982.Description based on: Vol. 1, no. 25 (Oct. 21, 1982).Latest issue
consulted: Vol. 16, no. 43 (Mar. 4, 1998).United
StatesAlabamaCoffeeEnterprise.AU at 000025827687info:srw/schema/1/marcxmlxml00000cas 

a22000007a 450010487314840305c19819999aluwr ne 0 a0eng dsn
85044906
AAAengAAACPNNSDNSTCPNOCLOCLCQOCLCFOCLCOOCLCAOCLCQ900410885-16620885-16621749310USPSThe 

New Times, 1618 1/2 St. Stephens Rd., Mobile, AL 36603mscnsdpn-us-alNew
times (Mobile, Ala.)The New times(Mobile, Ala.)The new times.Mobile,
Ala. :New Times Groupvolumes
:illustrationsWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan 

in 1981.Vol. 3, no. 49 (Dec. 15-21, 1983) and vol. 3, no. 50 (Dec.
22-28, 1983) are both called vol. 3, no. 49 (Dec. 15-21,
1983).Description based on: Vol. 2, no. 3 (Jan. 28-Feb. 3, 1982).African
AmericansAlabamaNewspapers.African
Americans.fast(OCoLC)fst00799558Alabama.fast(OCoLC)fst01204694Newspapers.fast(OCoLC)fst01423814United 

StatesAlabamaMobileMobile.AAPUnknownAug. 15,
1985AU at 000024686659info:srw/schema/1/marcxmlxml00000cas a22000007a
450018922463881219d19811983alucr ne 0 0eng dsn 88050233
AARengAARCPNNSDOCLCQmscThe Sylacauga daily advance.Advance/Sylacauga
dailySylacauga advanceSunday advanceAdvanceSylacauga, Ala. :Mrs. W.A.
Moody,1981-1893.v.Semiweekly,<Nov. 24, 1982-Feb. 13, 1983>Daily (except
Mon., Tues. & Sat.),<May 26, 1982-Nov. 21, 1982>Daily (except Sat. &
Mon.),<Jan. 1, 1981-May 23, 1982>74th Year, no. 123 (Jan. 1, 1981)-76th
year, no. 83 (Feb. 13, 1983).Days of publication vary.Published as: The
Advance/Sylacauga daily, <Aug. 28, 1981-May 23, 1982>.Published as:
Sylacauga advance, <Nov. 24, 1982-Feb. 13, 1983>.On Sunday, published
as: Sunday advance.United StatesAlabamaTalladegaSylacauga.Childersburg
star(DLC)sn 88050232Coosa press(DLC)sn 86050293Daily
home1059-6461(DLC)sn 88050234info:srw/schema/1/marcxmlxml00000cas
a22000007a 450021026715cr un|||||||||900209c19809999aluwr ne 0
0eng dsn 90099002
AARengAARCPNCUSOCLOCLCQTJCOCLCQOCLCFOCLCOOCLCA926143844AU at 000020585756mscn-us-alSpeakin' 

out news.Speaking out newsDecatur, Ala. :Minority Network,
Inc.v.WeeklyBegan in 1980.Published in Huntsville, Ala., <1987>-Also
issued by subscription via the World Wide Web.Description based on: Vol.
7, no. 8 (Jan. 7-13, 1987).African AmericansAlabamaNewspapers.African
American
newspapersAlabama.AlabamaNewspapers.Newspapers.fast(OCoLC)fst01423814African 

American newspapers.fast(OCoLC)fst00799278African
Americans.fast(OCoLC)fst00799558Alabama.fast(OCoLC)fst01204694United
StatesAlabamaMorganDecatur.United
StatesAlabamaMadisonHuntsville.Speakin' out weekly news(DLC)sn
88050097http://www.softlineweb.com/softlineweb/ethnic.htminfo:srw/schema/1/marcxmlxml00000cas 

a22000007a 450014996511861219c19809999aluwr ne 0 a0eng csn
86050472
AARengAARCPNNSDOCLCQ11080-15021080-15021328110USPSnsdppccWest-Alabama
gazetteWest-Alabama gazette.GazetteMillport, Ala. :Millport Pub.
Co.,1980-volumesWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrier4th 

year, no. 32 (Jan. 3, 1980)-United StatesAlabamaLamarMillport.Gazette
(Millport, Ala.)(DLC)sn 86050471info:srw/schema/1/marcxmlxml00000cas
a2200000 a 450011828156850320c19809999aluwr ne 0 0eng dsn
86050314 AAAengAAACPNOCLOCLCQmscThe Hartford news-herald.Hartford, Ala.
:Geneva Publications,1980-volumes :illustrations ;57-59
cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 80,
no. 20 (Feb. 14, 1980)-United StatesAlabamaGenevaHartford.News-herald
(Hartford, Ala.)(DLC)sn 86050313info:srw/schema/1/marcxmlxml00000cas
a22000007a 450017857788880427d198u198ualusr ne 0 0eng dsn
88050097 AARengAARCPNOCLOCLCQOCLCFOCLCOOCLCAmscn-us-alSpeakin' out
weekly news.Decatur, Ala. :Smothers PublicationsPublished every first
and third Wed. of each monthDescription based on: Vol. 3, no. 13 (May
4-17, 1983).African
AmericansAlabamaNewspapers.Newspapers.fast(OCoLC)fst01423814African
Americans.fast(OCoLC)fst00799558Alabama.fast(OCoLC)fst01204694United
StatesAlabamaMorganDecatur.Weekly news (Huntsville, Ala.)(DLC)sn
87050012Speakin' out news(DLC)sn
90099002info:srw/schema/1/marcxmlxml00000cas a2200000 a
450017807936880418c198u9999aluwr ne 0 a0eng dsn 90099001
AAAengAAACPNOCLOCLCQThe Daleville Sun-Courier, 310 Daleville Ave.,
Daleville, AL 36322mscn-us-alDaleville sun-courier.Daleville, Ala. :QST
Publicationsv. :ill. ;58 cm.WeeklyDescription based on: Vol. 2, no. 28
(Wed., Feb. 17, 1988).United
StatesAlabamaDaleDaleville.AU at 000020585749info:srw/schema/1/marcxmlxml00000cas 

a22000007a 450015580838870423c198u9999aluwr ne 0 0eng dsn
87050128 AARengAARCPNOCLCQmscGreene County independent.Eutaw, Ala.
:Greene County Independent, Inc.v.WeeklyDescription based on: Vol. 2,
no. 10 (Mar. 12, 1987).United
StatesAlabamaGreeneEutaw.info:srw/schema/1/marcxmlxml00000cas a22000007a
450010125135831114d198u198ualucr ne 0 a0eng dsn 83003221
NSDengNSDOCLCQ0d0746-55210746-55211Auburn Bulletin & Lee County Eagle,
PO Box 2111, Auburn, Ala. 36830nsdpThe Auburn bulletin & the Lee County
eagleThe Auburn bulletin & the Lee County eagle.Lee County eagleAuburn
bulletin and the Lee County eagleAuburn, Ala. :[publisher not
identified]Semiweekly,<Sept. 5,
1984->WeeklytexttxtrdacontentunmediatednrdamediaDescription based on:
Oct. 19, 1983.United StatesAlabamaLeeAuburn.Auburn bulletin(DLC)sn
89050006Eagle (Auburn, Ala.)(OCoLC)18435663Sept. 5,
1984info:srw/schema/1/marcxmlxml00000cas a22000007a
450018370324880818c198u9999aluwr ne 0 0eng dsn 88050147
CPNengCPNOCLCQmscTri-city times (Geraldine, Ala.)The Tri-City
times.Geraldine, Ala. :Wanda Nelsonv.WeeklyDescription based on: Vol. 2,
no. 24 (Jan. 6, 1982).United
StatesAlabamaDeKalbGeraldine.info:srw/schema/1/marcxmlxml00000cas
a22000007a 450010199338831208c198u9999aluwr ne 0 a0eng dsn
83005367 NSDengNSDCPNOCLCQ10746-62770746-62771707590USPSSpringville Pub.
Co., 539 Main St., Springville, AL 35146nsdpThe St. Clair clarionThe St.
Clair clarion.Saint Clair clarionSpringville, AL :Gary L.
ShultsWeeklytexttxtrdacontentunmediatednrdamediaDescription based on:
Vol. 2, no. 1 (Jan. 5, 1982).United StatesAlabamaSt.
ClairSpringville.AU at 000025783743info:srw/schema/1/marcxmlxml00000cas
a22000007a 450013787251860627c198u9999aluwr ne 0 a0eng dsn
86001923 NSDengNSDCPNOCLCQ10889-00800889-00801The Westerner Star, P.O.
Box 2060, Bessemer, AL 35021nsdpWestern star (Bessemer, Ala.)The Western
star(Bessemer, Ala.)The western star.Bessemer, Ala. :Hal
HodgensWeeklytexttxtrdacontentunmediatednrdamediaDescription based on:
Vol. 3, no. 15 (Wednesday, June 11, 1986).United
StatesAlabamaJeffersonBessemer.Bessemer advertiser(DLC)sn
87050117AU at 000025805174511.1srw.pc any \"y\" and srw.mt any
\"newspaper\" and srw.cp exact
\"Alabama\"50info:srw/schema/1/marcxmlxml1Date,,0mq1lME887FoIbjulKUV6bx9ImwWQNCv9GqZzGS92IKS31lEbcpRJBNHgcE1l29tFaHP9CHe0Yexk1uWQofffull" 

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 27 2022, 09:22am via System

Hello Spencer,

Thank you for reaching out about the bulk xml files for the US Newspaper 
Directory.

We don't have documentation specific to these bulk xml files, but upon 
further inspection I can say that each of those files don't necessarily 
contain info for 50 newspaper titles. The structure of the titles for 
California and New York for instance are different from say, Alabama.

If you look at California for example, the file naming structure 
indicates the year the title started, and then the number of titles 
included in that xml file. So for instance, the files below include info 
for newspapers that started in 2000, 2001, and 2002 respectively. And 
there is info for 30 titles in the xml file from 2000, and 14 in the 
file for 2001, and so on.

   * ndnp_California_2000_e_0001_0030.xml
   * ndnp_California_2001_e_0001_0014.xml
   * ndnp_California_2002_e_0001_0012.xml

If there's more than 50 titles for a given year, say for California 
starting in 1880, then the next 50 titles will roll into the next xml 
file, and so on. And the last xml file for that year may not include 50 
titles.

Many of the states seem to group all the years together, so each xml 
file contains 50 titles, until possibly the last one for a given state, 
which may contain less.

I hope this information helps explain the total number of records and 
structure a bit better. Let me know if you have any further questions.

Best wishes,

Kerry Huller
Newspaper & Current Periodical Reading Room
Serial & Government Publications Division
Library of Congress

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 25 2022, 02:22pm via Email

Hi, Kerry:


Might there be documentation on the XML files you mentioned?


I've successfully read
'https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/',
extracted the names of 6666 XML files, and read the first one,
"ndnp_Alabama_all-yrs_e_0001_0050.xml". It contains 29415 characters,
beginning, "1.12250info:srw/schema/1/marcxmlxml00000nas a22000007i
45001030438981180404c20159999aluwr n 0 a0eng ". With a bit
more effort, I will likely be able to parse all 6666 of these. The
names suggest that each contains information on 50 newspapers, totaling
333,300. The main page
"https://chroniclingamerica.loc.gov/search/titles/" says there are only
157,521 "Titles currently listed". This suggests that these XML files
include place holders for a little more than double the number of
entries currently in "https://chroniclingamerica.loc.gov/search/titles/".


Thanks for this.


Progress.
------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 07 2022, 08:55am via System

Hi Spencer,

I thought of one more option after I emailed you yesterday that I wanted 
to make you aware of.

I had explained the other day how we pull the records from OCLC into our 
U.S. Newspaper Directory. You can also access all of the raw MARC 
records found in the directory in xml format from here if you choose: 
https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/ 
<https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/> These will 
provide you all of the data from the record fields in MARC format, so 
you'd get all the data you see here for example: 
https://chroniclingamerica.loc.gov/lccn/sn98059792/marc/ 
<https://chroniclingamerica.loc.gov/lccn/sn98059792/marc/> but in xml. I 
don't know if this might be more data and info than you want to work 
with, but wanted to make sure you were aware of this option as well.

Best wishes,

Kerry Huller
Newspaper & Current Periodical Reading Room
Serial & Government Publications Division
Library of Congress

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 06 2022, 10:55am via System

Hi Spencer,

Thanks for reaching out again. I have been looking at the json view a 
bit closer this morning and your example of "9999."

After talking with a colleague this morning and looking at various 
examples, I see there is some variation in how the titles with either an 
unknown starting/ending date or currently published titles are being 
handled - depending on the view.

As an example, I completed a search in the directory for Alaska and the 
city of Anchorage. There are 80 results, and on the first page of 
results you'll see # 4. Fort Richardson news, which was published from 
1952-19??. The csv view of this state/city search result will show the 
ending date of 19??. But if I append &format=json to this search result, 
this specific title will show an ending date of 1999. After talking with 
a colleague this morning, I discovered an integer had to be used in 
these cases where dates were "?" so that the search based on year range 
would work. Similarly, if you look at # 12 Alaska digest, which was 
published 1994-current, the "current" becomes "9999" in the json view. 
So, the records you are seeing with "9999" would most likely be titles 
with an ending date of "current."

However, there is an issue with the unknown dates, like "1999" being 
used for "19??" in the example above. The "9" does not get inserted in 
place of "?" when you are looking at the title/LCCN view of a specific 
newspaper. So for instance, if you view the #4 title: Fort Richardson 
news at this url: https://chroniclingamerica.loc.gov/lccn/sn98059792/ 
<https://chroniclingamerica.loc.gov/lccn/sn98059792/> but append .json 
to the end of the url, after the LCCN, like this: 
https://chroniclingamerica.loc.gov/lccn/sn98059792.json 
<https://chroniclingamerica.loc.gov/lccn/sn98059792.json> you'll see 
that the end_year is "19??." Viewing the title/LCCN json view for titles 
that are currently published will also show the end_year as "current." 
The Alaska digest example from above can be viewed here: 
https://chroniclingamerica.loc.gov/lccn/sn97060056.json 
<https://chroniclingamerica.loc.gov/lccn/sn97060056.json>

I wasn't aware of the difference between the directory search json view 
and the title/LCCN view. But I think it would be possible to grab 
the data from the title/LCCN json url through an additional script 
potentially. The json url is included in the view under the "url" field.

Of course, there are unknowns with publishing dates, but better to know 
where the question marks are, and what titles are considered to be current.

I hope this clarifies the data a bit more - let me know if any of it 
needs more clarification though. And let me know if you have follow-up 
questions.

Thank you,

Kerry Huller
Newspaper & Current Periodical Reading Room
Serial & Government Publications Division
Library of Congress

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 05 2022, 04:42pm via Email

Hi, Kerry:


What would you suggest I do to get a count of the numbers of
newspapers and publishers operating by year from, say, 1790 to 2021?


I just determined that 20630 (13 percent) of the 157520 records in
the US Newspaper database I downloaded a week ago have end_year = 9999.
I don't think it's feasible to assume that all or even most of those
are still publishing.


Might there be some other database that might have this kind of
information?


I ask, because Robert McChesney (2004) The Problem of the Media
(Monthly Review Pr., esp. pp. 34-35) suggests that in the first half of
the nineteenth century, the US had more newspapers and newspaper
publishers per capita than any other place or time. He suggests that
that diversity of newspapers helped encourage literacy and limit
political corruption, both of which helped propel the young US to its
current dominance of the international political economy. I'm hoping to
get some data to evaluate this claim. Sadly, it looks like there is too
much missing and questionable data in this dataset for me to use this
without a fairly substantive data cleaning effort.
------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 05 2022, 09:05am via System

Hello Spencer,

Thank you for reaching out about your additional questions.

I was looking at the records you mention above, and yes, you are correct 
- those 9 records with the date inconsistencies and the one record for 
the The New Mexican mining news 
<https://chroniclingamerica.loc.gov/lccn/sn93061507/> containing "Santa 
Fe.\" have typos in them. Thanks for spotting these - it may be possible 
to have the cataloger in our division correct those typos. I will look 
into this further.

The U.S. Newspaper Directory doesn't have a connection with Wikimedia or 
Wikipedia. The Library of Congress periodically pulls the records for 
the Directory from OCLC Worldcat 
<https://www.oclc.org/en/worldcat.html>. And those newspaper records in 
OCLC Worldcat have been created by catalogers at various institutions 
around the U.S. over the span of several years. So, occasionally, you 
will find a typo in the records. Corrections can be made by OCLC and 
library staff at the various institutions. Every time we complete a new 
pull on the OCLC records, any corrected records will then populate our 
Directory.

Regarding your question on the New-York weekly journal - yes, that is 
also correct that it has two records. There is actually a record for 
each format of the newspaper, so this record is for the microfilm format 
<https://chroniclingamerica.loc.gov/lccn/2009252748/> and this one is 
for the original print format 
<https://chroniclingamerica.loc.gov/lccn/sn83030211/>. You can see in 
the heading for the microfilm record where it says [microfilm reel] and 
the print version shows [volume]. You are likely to see this for other 
titles as well because each format has been cataloged with its own LCCN. 
You are also likely to see additional records with [online resource] 
identified as the format as more and more titles are available as 
ePrints or online.

I hope this helps answer your additional questions a bit more. Please 
reach out if you have any other questions.

Thank you,

Kerry Huller
Newspaper & Current Periodical Reading Room
Serial & Government Publications Division
Library of Congress

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 04 2022, 01:47pm via Email

Hi, Kelly:


At the risk of bombing your inbox with more emails than you want,
what is your relationship with Wikipedia and other Wikimedia Foundation
projects like Wikidata?


I ask, because I've logged over 20,000 edits in Wikimedia Foundation
projects since 2010, and I would happily try to answer questions about
Wikidata and other Wikimedia Foundation projects. I have NOT organized
an edit-a-thon, but I've made presentations at conferences with people
who have, and I would happily try to help organize such if you could
find a group of people who want to work to improve this US Newspaper
database. I think it would be good to establish links between this US
Newspaper database and Wikidata, with appropriate procedures so changes
to one could be evaluated for acceptance into the other.


FYI, John Peter Zenger's famous "New-York weekly journal" (1733-1751)
appears TWICE in your database with lccn = 2009252748 and sn83030211 and
ONCE in Wikidata WITHOUT an lccn, even though many other Wikidata items
have an lccn. See:


https://www.wikidata.org/wiki/Q23091960


There's a "WikiProject Newspapers" on Wikipedia and a companion
"WikiProject Periodicals" on Wikidata:


https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Newspapers/Wikidata


https://www.wikidata.org/wiki/Wikidata:WikiProject_Periodicals


I've tried to connect with others on those projects, so far with only
limited success. However, you may know that almost anyone can change
almost anything on Wikipedia and other Wikimedia Foundation projects.
What stays tends to be written from a neutral point of view citing
credible sources. They have problems with vandals, but the problems are
usually easily controlled. This makes Wikipedia and Wikidata very
useful platforms for cleaning up databases like your US Newspaper dataset.


Spencer Graves


##########


Hello, Kelly:


In addition to the invalid JSON, discussed below [NOTE: The "below"
contains a slight addition to the report of the I sent last Friday.], I
found 9 (NINE!) cases where start_year was AFTER end_year. These have
lccn = "sn86071531" "sn95069213" "sn90059096" "sn86058451" "sn90060926"
"sn99065409" "sn89065002" "sn98069857" "sn91059179"


See:


https://chroniclingamerica.loc.gov/lccn/sn86071531/
https://chroniclingamerica.loc.gov/lccn/sn95069213/
https://chroniclingamerica.loc.gov/lccn/sn90059096/
https://chroniclingamerica.loc.gov/lccn/sn86058451/
https://chroniclingamerica.loc.gov/lccn/sn90060926/
https://chroniclingamerica.loc.gov/lccn/sn99065409/
https://chroniclingamerica.loc.gov/lccn/sn89065002/
https://chroniclingamerica.loc.gov/lccn/sn98069857/
https://chroniclingamerica.loc.gov/lccn/sn91059179/


These all have obvious coding errors that can be easily fixed. The
data may not be completely accurate after the fix, but at least they are
not obviously wrong ;-)


##################

I got invalid JSON from:


https://chroniclingamerica.loc.gov/search/titles/results/?rows=500&page=103&format=json


After some experimentation, I was able to replicate the problem with
a request for rows=10:


https://chroniclingamerica.loc.gov/search/titles/results/?rows=10&page=5117&format=json


Duncan Temple Lang <dtemplelang at ucdavis.edu>, Professor of Statistics
and Associate Dean for Graduate Programs at the University of California
- Davis, confirmed that it was a JSON error using:


https://codebeautify.org/jsonvalidator


He is part of the core team developing the R free, open-source
programming language. He said, that starting at offsets 161070 and
161502 in the character string you get from [the R code RCurl::getURL()]
we have:


Santa Fe.\"


and these are in an entry such as


"city": ["Santa Fe.\"]


So the final " is escaped and therefore there is no closing " for the
string. The parser continues to consume characters looking for the end
of that string.


If one "repairs" the text from getURL() with


ftxt= gsub('Santa Fe.\\\\"', 'Santa Fe."', txt)


then the rest of my code worked fine.


You may wish to do something to implement other checks for valid JSON
and repair this problem. I've scanned all the 157520 records that were
in that database a couple of days ago, and this is the only JSON error
identified by the code I used.


NOTE: I was NOT able to replicate this error when downloading records
one at a time. That suggests a problem NOT in the database itself but
in the download algorithm. ???


Thank you for your help. I will almost certainly have other
questions ;-)
------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 03 2022, 10:39pm via Email

Hello, Kelly:


In addition to the invalid JSON, discussed below [NOTE: The "below"
contains a slight addition to the report of the I sent last Friday.], I
found 9 (NINE!) cases where start_year was AFTER end_year. These have
lccn = "sn86071531" "sn95069213" "sn90059096" "sn86058451" "sn90060926"
"sn99065409" "sn89065002" "sn98069857" "sn91059179"


See:


https://chroniclingamerica.loc.gov/lccn/sn86071531/
https://chroniclingamerica.loc.gov/lccn/sn95069213/
https://chroniclingamerica.loc.gov/lccn/sn90059096/
https://chroniclingamerica.loc.gov/lccn/sn86058451/
https://chroniclingamerica.loc.gov/lccn/sn90060926/
https://chroniclingamerica.loc.gov/lccn/sn99065409/
https://chroniclingamerica.loc.gov/lccn/sn89065002/
https://chroniclingamerica.loc.gov/lccn/sn98069857/
https://chroniclingamerica.loc.gov/lccn/sn91059179/


These all have obvious coding errors that can be easily fixed. The
data may not be completely accurate after the fix, but at least they are
not obviously wrong ;-)


##################

I got invalid JSON from:


https://chroniclingamerica.loc.gov/search/titles/results/?rows=500&page=103&format=json


After some experimentation, I was able to replicate the problem with
a request for rows=10:


https://chroniclingamerica.loc.gov/search/titles/results/?rows=10&page=5117&format=json


Duncan Temple Lang <dtemplelang at ucdavis.edu>, Professor of Statistics
and Associate Dean for Graduate Programs at the University of California
- Davis, confirmed that it was a JSON error using:


https://codebeautify.org/jsonvalidator


He is part of the core team developing the R free, open-source
programming language. He said, that starting at offsets 161070 and
161502 in the character string you get from [the R code RCurl::getURL()]
we have:


Santa Fe.\"


and these are in an entry such as


"city": ["Santa Fe.\"]


So the final " is escaped and therefore there is no closing " for the
string. The parser continues to consume characters looking for the end
of that string.


If one "repairs" the text from getURL() with


ftxt= gsub('Santa Fe.\\\\"', 'Santa Fe."', txt)


then the rest of my code worked fine.


You may wish to do something to implement other checks for valid JSON
and repair this problem. I've scanned all the 157520 records that were
in that database a couple of days ago, and this is the only JSON error
identified by the code I used.


NOTE: I was NOT able to replicate this error when downloading records
one at a time. That suggests a problem NOT in the database itself but
in the download algorithm. ???


Thank you for your help. I will almost certainly have other
questions ;-)
------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jul 01 2022, 11:46am via Email

Hello, Kelly:


I got invalid JSON from:


https://chroniclingamerica.loc.gov/search/titles/results/?rows=500&page=103&format=json


After some experimentation, I was able to replicate the problem with
a request for rows=10:


https://chroniclingamerica.loc.gov/search/titles/results/?rows=10&page=5117&format=json


Duncan Temple Lang <dtemplelang at ucdavis.edu>, Professor of Statistics
and Associate Dean for Graduate Programs at the University of California
- Davis, confirmed that it was a JSON error using:


https://codebeautify.org/jsonvalidator


He is part of the core team developing the R free, open-source
programming language. He said, that starting at offsets 161070 and
161502 in the character string you get from [the R code RCurl::getURL()]
we have:


Santa Fe.\"


and these are in an entry such as


"city": ["Santa Fe.\"]


So the final " is escaped and therefore there is no closing " for the
string. The parser continues to consume characters looking for the end
of that string.


If one "repairs" the text from getURL() with


ftxt= gsub('Santa Fe.\\\\"', 'Santa Fe."', txt)


then the rest of my code worked fine.


You may wish to do something to implement other checks for valid JSON
and repair this problem. I've scanned all the 157520 records that were
in that database a couple of days ago, and this is the only JSON error
identified by the code I used.


Thank you for your help. I will almost certainly have other
questions ;-)
------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jun 28 2022, 02:20pm via System

Hello Spencer,

Thank you for sending along your follow-up questions.

I'm glad to hear the json view will work for you. It was recommended to 
me that you limit your requests to 500 rows at a time. And a developer 
here at LC suggests the following regarding rate limiting:

?To avoid being blocked by the server, the current rate-limiting rules 
restrict un-cached requests to URLs starting with 
https://chroniclingamerica.loc.gov/search/ 
<https://chroniclingamerica.loc.gov/search/> to 120 requests every 10 
minutes from a single IP address.?

So, I think if you limited each of your requests to 500 rows at a time 
with the proper pauses, then you should be able to access what you need.

As for the csv view, I checked on this as well, and was informed that 
the csv view was not implemented for all url formats. The csv view was 
only implemented for this view: 
https://chroniclingamerica.loc.gov/newspapers/ 
<https://chroniclingamerica.loc.gov/newspapers/>and urls resulting from 
US Directory search results - for e.g. if you wanted to narrow down your 
search results by state, city, date range, etc. found at this link: 
https://chroniclingamerica.loc.gov/search/titles/ 
<https://chroniclingamerica.loc.gov/search/titles/>. So, if you wanted a 
csv and limited your search by state ( for example: 
https://chroniclingamerica.loc.gov/search/titles/results/?state=Alaska&county=&city=&year1=1690&year2=2022&terms=&frequency=&language=&ethnicity=&labor=&material_type=&lccn=&rows=20&format=csv 
<https://chroniclingamerica.loc.gov/search/titles/results/?state=Alaska&county=&city=&year1=1690&year2=2022&terms=&frequency=&language=&ethnicity=&labor=&material_type=&lccn=&rows=20&format=csv> 
), you could append &format=csv to the search result url and get the csv 
to automatically download. But, if your search results ended up being 
over a couple thousand titles, then the system would probably time out.

I hope this info helps! Let me know if you have any other questions.

Best wishes,

Kerry Huller
Newspaper & Current Periodical Reading Room
Serial & Government Publications Division
Library of Congress

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jun 27 2022, 04:15pm via Email

Hello, Kerry:


Thanks for the reply. Can you please give me some further guidance
on two thing "so that the system is not overwhelmed"?


1. The max size in a small batch?


2. Any limit on the number of small batches in a second or minute?


I've found that I can download small batches under program control
using "RCurl::getURL" in R (programming language) using, e.g.;


https://chroniclingamerica.loc.gov/search/titles/results/?rows=20&page=2&format=json


With this, I can control the batch size with "row=20" vs. "row=50"
vs., e.g., "row=1000". A naive search says there are 157520 "results".
With "row=1000", this would require 158 calls. With "row=20", it
would require 7876 calls. Before I start, I need to decide which fields
I want; I don't need them all.


Thanks,
Spencer Graves


p.s. I tried appending "&format=csv" and got "Error 504 Ray ID:
7220896da85e86e7 ? 2022-06-27 19:19:53 UTC Gateway time-out". I used:


https://chroniclingamerica.loc.gov/search/titles/results/?state=&county=&city=&year1=1690&year2=2022&terms=&frequency=&language=&ethnicity=&labor=&material_type=&lccn=&rows=20&format=csv


I can get what I want using json so do not need csv. However, I
thought you might want to know that I was unable to get csv to work.
------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jun 27 2022, 10:54am via System

Hello Spencer,

Thank you for contacting the Library of Congress about searching the US 
Newspaper Directory. I wanted to follow up with you regarding your 
request to output the data in a machine readable format.

It looks like you were provided the link to the API documentation for 
the website: About the Site and API 
<https://chroniclingamerica.loc.gov/about/api/>. Scroll down to the 
section with the heading, Searching the directory and newspaper pages 
using OpenSearch. This section describes the search functionality and 
structure for the US Newspaper Directory in more detail. It is possible 
to return your directory searches in json format by appending 
&format=json to the end of the url. It is also possible to return search 
results in csv format by appending &format=csv to the end of the url, 
but I would strongly suggest that you do this in small batches by 
putting limits on your search so that the system is not overwhelmed.

So, from the search page for the US Newspaper Directory 
<https://chroniclingamerica.loc.gov/search/titles/> you could 
potentially limit your search based on state and city, or date range, 
and/or even frequency. Then once you've completed the search, you can 
add &format=csv to the end of the url to automatically download a csv of 
those records. The resulting csv will contain several fields/headers: 
lccn, title, place of publication, start year, end year, publisher, 
edition, frequency, subject, state, city, country, language, oclc 
number, and holding type. I think these fields include the information 
you were looking for. But, again, I would like to stress that you put 
limits on your search before creating the csv so as not overwhelm the 
system.

Please let me know if you have any other additional questions.

Best wishes,

Kerry Huller
Newspaper & Current Periodical Reading Room
Serial & Government Publications Division
Library of Congress

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jun 23 2022, 01:55pm via System

Mr. Graves,

I'm going to transfer you request to a member of our digital collections 
team who may be of more assistance to you than me.

Mike

------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jun 23 2022, 01:51pm via Email

Dear Mr. Queen:


Thanks for the reply. I'm still confused. I downloaded and
installed Docker Desktop and "docker-compose.yml" and ran their "Getting
Started" Tutorial, but I don't see what to do next.


I repeat: I'd like to analyze "U.S. Newspaper Directory,
1690-Present" (https://chroniclingamerica.loc.gov/search/titles/), which
------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jun 22 2022, 07:15pm via System

Mr. Graves,

Programmatic access to the data forChronicling America 
<https://chroniclingamerica.loc.gov/>and possibly the U.S. Newspaper 
Directory <https://chroniclingamerica.loc.gov/search/titles/>can be 
found on theAbout the Site and API 
<https://chroniclingamerica.loc.gov/about/api/>page in various formats. 
Also, please note that Chronicling Americacontains newspapers published 
from 1777-1963, but does not include everyU.S. newspaper published in 
that time period.

Please let me know if I can be of further assistance.


------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jun 22 2022, 06:14pm via Email

Dear Mr. Queen:


Can we simplify this to just giving me the data behind "U.S.
Newspaper Directory, 1690-Present"
(https://chroniclingamerica.loc.gov/search/titles/) in a machine
readable format, e.g., csv or xlsx or a MySQL database?


As I mentioned in my original email, a naive search of that without
restrictions returned 157520 titles in 7876 pages with up to 20 titles
per page giving date ranges in at least some cases. I could probably
write software to scrape those 7876 pages from your web site and combine
them into a data file.


I have a PhD in statistics, I have been using the R programming
language and similar software for decades. This includes publishing
tutorials on how to analyze data like this on Wikiversity.[1] I'd like
to do something similar with this. I could help make your data more
useful to others and discuss with you how we might prioritize
improvements like accessing the other sources you mentioned.


Thanks very much for your reply.


Sincerely,
Spencer Graves, PhD
Founder, EffectiveDefense.org
4550 Warwick Blvd 508
Kansas City, MO 64111
m: 408-655-4567


[1] e.g.:


https://en.wikiversity.org/wiki/US_Gross_Domestic_Product_(GDP)_per_capita
------------------------------------------------------------------------

Newspapers and Current Periodicals Reference Librarian

Jun 22 2022, 05:27pm via System

Mr. Graves

Your request is a little more complex than it first appears and requires 
extensive research. A variety of resources should be consulted to 
determine the circulation statistics of newspapers published prior to 
1851. You will need to check newspaper union lists and newspaper 
histories. Union listspresent lists of newspapers in geographic 
arrangement according to place of publication, and specify which 
libraries or other institutions hold collections of those newspapers and 
the dates of their holdings. These can also be useful for tracking title 
changes throughout a newspaper's history. Newspaper 
historieslikeAmerican Journalism: A History: 1690-1960 
<https://lccn.loc.gov/62007157>(Mott),The Penny Press 
<https://lccn.loc.gov/2004043078>(Thompson), andThe Press and America 
<https://lccn.loc.gov/99044295>(Emery et al.) may not include 
circulation statistics, but they do document the diversity and progress 
of newspaper publishing, including notable newspapers of the era. 
Newspaper histories also cover the history of the printers and printing 
of newspapers in a state, county, or region more generally, and provide 
more condensed histories of the editors, journalists, and evolution of 
the newspapers in a specific area. Newspaper histories and union lists 
should be available at most large public or university libraries. More 
information about union lists, newspaper histories, and researching 
newspapers in general can be found in theU.S. Newspaper Collections at 
the Library of Congress 
<https://guides.loc.gov/united-states-newspapers/introduction>research 
guide (see Reference Sources).

Please let me know if I can be of further assistance.

------------------------------------------------------------------------

Original Question

Jun 20 2022, 02:34pm via System

How can I get counts of the numbers of newspapers by year in the US, and 
preferably also elsewhere? A search of "U.S. Newspaper Directory,
How can I get counts of the numbers of newspapers by year in the US, and 
preferably also elsewhere?

A search of "U.S. Newspaper Directory, 1690-Present" 
(https://chroniclingamerica.loc.gov/search/titles/) returned 157520 
titles in 7876 pages with up to 20 titles per page giving date ranges to 
the extent that it's known. If I can get a data file (e.g., csv or xls), 
I can summarize. I could also use data on circulation and frequency and 
especially parent company for multiple newspapers published by the same 
company, to the extant that such is available.

I'm interested in this, because McChesney quoted Tocqueville in 
suggesting that the US had more newspapers per person (or per million 
population) prior to 1851 than at any other time or place in history. 
I'd like to evaluate that claim with data to the extent that I can. See 
"https://en.wikiversity.org/wiki/Social_construction_of_crime_and_what_we_can_do_about_it#Newspapers_1790_-_present". 


Thanks, Spencer Graves, PhD
m: 408-655-4567

------------------------------------------------------------------------

Thank you for using Newspapers & Current Periodicals Ask a Librarian 
Service!


This email is sent from Ask a Librarian in relationship to ticket #9625195.

Read our privacy policy. <https://springshare.com/privacy.html>

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Thu Jul 28 00:28:16 2022
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Wed, 27 Jul 2022 17:28:16 -0500
Subject: [R] Parsing XML?
In-Reply-To: <002601d8a204$dd3226d0$97967470$@gmail.com>
References: <20220727145903.4338c449692f703b@ask.loc.gov>
 <7743581c-0959-1df6-bb31-f140161f83e1@effectivedefense.org>
 <002601d8a204$dd3226d0$97967470$@gmail.com>
Message-ID: <365816a9-e6f9-e8a7-7eeb-c44cbca31756@effectivedefense.org>



On 7/27/22 5:04 PM, avi.e.gross at gmail.com wrote:
> General XML is not intended to be parsable as a list. But there are lots of tools you can use to extract various patterns out of XML in forms like a list.
> 
> But your data example is huge and I am falling asleep waiting to see if it loads. I looked sideways and it is not that big directly but my browser may be trying to show it as a web page.


	  You have my sympathies.  It loaded with elapsed time of 0.55 seconds 
for me:


XMLfile <-
"https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/ndnp_Alabama_all-yrs_e_0001_0050.xml"
system.time(XMLdata <- xml2::read_xml(XMLfile))
#   user  system elapsed
#  0.048   0.010   0.550
XMLtxt <- xml2::xml_text(XMLdata)
nchar(XMLtxt)
#[1] 29415


	  From staring at those 29415 chracters, I noted that 
'info:srw/schema/1/marcxmlxml' seemed to repeat in places that looked 
like breaks between records.  So I tried the following:


str(XMLt2 <- strsplit(XMLtxt, 'info:srw/schema/1/marcxmlxml')[[1]])
head(XMLt2, 3)

[1] "1.12250" 
 
 
 
 
 

[2] "00000nas a22000007i 45001030438981180404c20159999aluwr n       0 
a0eng    2018200464DLCrdaDLCeng12577-53161021110USPS711 Alabama Avenue, 
Selma, AL 36701nsdppccISSN RECORD07115Selma sunSelma sun.Selma, AL 
:North Shore Press, 
LLC2016-WeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan in 
2015.Description based on: Volume 2, Issue 40 (October 5, 2017) 
(surrogate); title from caption.Latest issue consulted: Volume 2, Issue 
40 (October 5, 2017).United StatesAlabamaDallasSelma."
[3] "00000cas a22000007a 4500502150053100127c20109999aluwr n       0 
a0eng    2010200019DLCengDLCDLCOCLCQ112153-18111750USPSB & C Publishing, 
LLC, 3514 Martin St. S. Ste 104, Cropwell, AL 35054pccnsdpISSN RECORDSt. 
Clair County news (Cropwell, Ala.)St. Clair County news(Cropwell, 
Ala.)St. Clair County news.Cropwell, AL :B & C Pub.WeeklyBegan in 
2010.Description based on: Nov. 4, 2010 (surrogate); title from 
caption."


	  However, I was hoping there were other XML tools that would get me 
more information quicker.


	  Suggestions?
	  Thanks,
	  Spencer


############

 > sessionInfo()
R version 4.2.1 (2022-06-23)
Platform: x86_64-apple-darwin17.0 (64-bit)
Running under: macOS Big Sur 11.6.7

Matrix products: default
LAPACK: 
/Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] sos_2.1-4     brew_1.0-7    XML_3.99-0.10 xml2_1.3.3

loaded via a namespace (and not attached):
  [1] digest_0.6.29   evaluate_0.15   rlang_1.0.4     cli_3.3.0
  [5] curl_4.3.2      rstudioapi_0.13 rmarkdown_2.14  tools_4.2.1
  [9] xfun_0.31       yaml_2.3.5      fastmap_1.1.0   compiler_4.2.1
[13] htmltools_0.5.2 knitr_1.39
 >
> 
> How about you copying and pasting a sample of say the first few dozen lines so we see what is in it for the purpose of ...
> 
> The schema would be mentioned in an attribute if you know what you are looking for and may be an external file.
> 
> So decide what you want, like a list of all titles and use something like xpath().
> 
> 
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Spencer Graves
> Sent: Wednesday, July 27, 2022 4:51 PM
> To: 'R-help' <r-help at r-project.org>
> Subject: [R] Parsing XML?
> 
> Hello, All:
> 
> 
> 	  What would you suggest I do to parse the following XML file into a
> list that I can understand:
> 
> 
> XMLfile <-
> "https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/ndnp_Alabama_all-yrs_e_0001_0050.xml"
> 
> 
> 
> 	  This is the first of 6666 XML files containing "U.S. Newspaper
> Directory" maintained by the US Library of Congress discussed in the
> thread below.  I've tried various things using the XML and xml2.
> 
> 
> XMLdata <- xml2::read_xml(XMLfile)
> str(XMLdata)
> XMLdat <- XML::xmlParse(XMLdata)
> str(XMLdat)
> XMLtxt <- xml2::xml_text(XMLdata)
> nchar(XMLtxt)
> #[1] 29415
> 
> 
> 	  Someplace there's a schema for this.  I don't know if it's embedded
> in this XML file or in a separate file.  If it's in a separate file, how
> could I describe it to my contacts with the Library of Congress so they
> would understand what I needed and could help me get it.
> 
> 
> 	  Thanks,
> 	  Spencer Graves
> 
> 
> p.s.  All 29415 characters in XMLtext appear in the thread below.  	  	
> 
> 
> -------- Forwarded Message --------
> Subject: 	[Newspapers and Current Periodicals] How can I get counts of
> the numbers of newspapers by year in the US, and preferably also
> elsewhere? A search of "U.S. Newspaper Directory,
> Date: 	Wed, 27 Jul 2022 14:59:03 +0000
> From: 	Kerry Huller <serials at ask.loc.gov>
> To: 	Spencer Graves <spencer.graves at effectivedefense.org>
> CC: 	twes at loc.gov
> 
> 
> 
> --# Type your reply above this line #--
> 
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jul 27 2022, 10:59am via System
> 
> Hello Spencer,
> 
> So, when I view the xml, I'm actually looking at it in XML editor
> software, so I can view the tags and it's structured neatly. I've copied
> and pasted the text from the beginning of the file and the first
> newspaper title below from my XML editor:
> 
> <?xml version="1.0" encoding="UTF-8" standalone="no"?>
> <?xml-stylesheet type='text/xsl'
> href='/webservices/catalog/xsl/searchRetrieveResponse.xsl'?>
> 
> <searchRetrieveResponse xmlns="http://www.loc.gov/zing/srw/"
> xmlns:oclcterms="http://purl.org/oclc/terms/"
> xmlns:dc="http://purl.org/dc/elements/1.1/"
> xmlns:diag="http://www.loc.gov/zing/srw/diagnostic/"
> xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
> <version>1.1</version>
> <numberOfRecords>2250</numberOfRecords>
> <records>
> <record>
> <recordSchema>info:srw/schema/1/marcxml</recordSchema>
> <recordPacking>xml</recordPacking>
> <recordData>
> <record xmlns="http://www.loc.gov/MARC21/slim">
>        <leader>00000nas a22000007i 4500</leader>
>        <controlfield tag="001">1030438981</controlfield>
>        <controlfield tag="008">180404c20159999aluwr n       0   a0eng
>    </controlfield>
>        <datafield ind1=" " ind2=" " tag="010">
>          <subfield code="a">  2018200464</subfield>
>        </datafield>
>        <datafield ind1=" " ind2=" " tag="040">
>          <subfield code="a">DLC</subfield>
>          <subfield code="e">rda</subfield>
>          <subfield code="c">DLC</subfield>
>          <subfield code="b">eng</subfield>
>        </datafield>
>        <datafield ind1=" " ind2=" " tag="012">
>          <subfield code="m">1</subfield>
>        </datafield>
>        <datafield ind1="0" ind2=" " tag="022">
>          <subfield code="a">2577-5316</subfield>
>          <subfield code="2">1</subfield>
>        </datafield>
>        <datafield ind1=" " ind2=" " tag="032">
>          <subfield code="a">021110</subfield>
>          <subfield code="b">USPS</subfield>
>        </datafield>
>        <datafield ind1=" " ind2=" " tag="037">
>          <subfield code="b">711 Alabama Avenue, Selma, AL 36701</subfield>
>        </datafield>
>        <datafield ind1=" " ind2=" " tag="042">
>          <subfield code="a">nsdp</subfield>
>          <subfield code="a">pcc</subfield>
>        </datafield>
>        <datafield ind1="1" ind2="0" tag="050">
>          <subfield code="a">ISSN RECORD</subfield>
>        </datafield>
>        <datafield ind1="1" ind2="0" tag="082">
>          <subfield code="a">071</subfield>
>          <subfield code="2">15</subfield>
>        </datafield>
>        <datafield ind1=" " ind2="0" tag="222">
>          <subfield code="a">Selma sun</subfield>
>        </datafield>
>        <datafield ind1="0" ind2="0" tag="245">
>          <subfield code="a">Selma sun.</subfield>
>        </datafield>
>        <datafield ind1=" " ind2="1" tag="264">
>          <subfield code="a">Selma, AL :</subfield>
>          <subfield code="b">North Shore Press, LLC</subfield>
>          <subfield code="c">2016-</subfield>
>        </datafield>
>        <datafield ind1=" " ind2=" " tag="310">
>          <subfield code="a">Weekly</subfield>
>        </datafield>
>        <datafield ind1=" " ind2=" " tag="336">
>          <subfield code="a">text</subfield>
>          <subfield code="b">txt</subfield>
>          <subfield code="2">rdacontent</subfield>
>        </datafield>
>        <datafield ind1=" " ind2=" " tag="337">
>          <subfield code="a">unmediated</subfield>
>          <subfield code="b">n</subfield>
>          <subfield code="2">rdamedia</subfield>
>        </datafield>
>        <datafield ind1=" " ind2=" " tag="338">
>          <subfield code="a">volume</subfield>
>          <subfield code="b">nc</subfield>
>          <subfield code="2">rdacarrier</subfield>
>        </datafield>
>        <datafield ind1="1" ind2=" " tag="362">
>          <subfield code="a">Began in 2015.</subfield>
>        </datafield>
>        <datafield ind1=" " ind2=" " tag="588">
>          <subfield code="a">Description based on: Volume 2, Issue 40
> (October 5, 2017) (surrogate); title from caption.</subfield>
>        </datafield>
>        <datafield ind1=" " ind2=" " tag="588">
>          <subfield code="a">Latest issue consulted: Volume 2, Issue 40
> (October 5, 2017).</subfield>
>        </datafield>
>        <datafield ind1=" " ind2=" " tag="752">
>          <subfield code="a">United States</subfield>
>          <subfield code="b">Alabama</subfield>
>          <subfield code="c">Dallas</subfield>
>          <subfield code="d">Selma.</subfield>
>        </datafield>
>      </record>
> </recordData>
> </record>
> 
> When I view the records in the XML editor, these 2 lines below do begin
> each of the records for each individual title, but of course this is
> including the xml tags:
> 
> <recordSchema>info:srw/schema/1/marcxml</recordSchema>
> <recordPacking>xml</recordPacking>
> 
> Hopefully this helps you decide where to break or parse each record.
> 
> On another note, I just noticed as well that at the top of this first
> file it lists the total number of records for the Alabama grouping -
> 2250. This also appeared to be the case for the Alaska records when I
> took a look at the first one for that state. I imagine that should be
> consistent throughout each "grouping" of records.
> 
> Let me know if you have follow-up questions!
> 
> Best wishes,
> 
> Kerry Huller
> Newspaper & Current Periodical Reading Room
> Serial & Government Publications Division
> Library of Congress
> 
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jul 27 2022, 10:21am via Email
> 
> Hi, Kerry:
> 
> 
> Thanks. I understand the chunking in files of at most 50. I've read
> the first file "ndnp_Alabama_all-yrs_e_0001_0050.xml" into a string of
> 29415 characters, copied below. Might you have any suggestions on the
> next step in parsing this? Staring at it now, it looks splitting on
> "info:srw/schema/1/marcxmlxml" might convert the 29415 characters into
> shorter chunks, each of which could then be parsed further.
> 
> 
> This is not as bad as reading ancient Egyptian heiroglyphics without
> the Rosetta Stone, but I wondered if you might have something that could
> make this work easier and more reliable? I guess I could compare with
> what I already read as JSON ;-)
> 
> 
> Thanks,
> Spencer Graves
> 
> 
> "1.12250info:srw/schema/1/marcxmlxml00000nas a22000007i
> 45001030438981180404c20159999aluwr n 0 a0eng
> 2018200464DLCrdaDLCeng12577-53161021110USPS711 Alabama Avenue, Selma, AL
> 36701nsdppccISSN RECORD07115Selma sunSelma sun.Selma, AL :North Shore
> Press,
> LLC2016-WeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan in
> 2015.Description based on: Volume 2, Issue 40 (October 5, 2017)
> (surrogate); title from caption.Latest issue consulted: Volume 2, Issue
> 40 (October 5, 2017).United
> StatesAlabamaDallasSelma.info:srw/schema/1/marcxmlxml00000cas a22000007a
> 4500502150053100127c20109999aluwr n 0 a0eng
> 2010200019DLCengDLCDLCOCLCQ112153-18111750USPSB & C Publishing, LLC,
> 3514 Martin St. S. Ste 104, Cropwell, AL 35054pccnsdpISSN RECORDSt.
> Clair County news (Cropwell, Ala.)St. Clair County news(Cropwell,
> Ala.)St. Clair County news.Cropwell, AL :B & C Pub.WeeklyBegan in
> 2010.Description based on: Nov. 4, 2010 (surrogate); title from
> caption.info:srw/schema/1/marcxmlxml00000cas a22000007a
> 4500426491872090720c20099999alumr n 0 a0eng
> 2009203372DLCengDLCOCLCQ12150-346X2150-346X1AU at 000044489617NZ116076352Devon
> Applewhite/Applewhite Publishing Co., 1910 Honeysuckle Rd., #N183,
> Dothan, AL 36305mscnsdpISSN RECORD30514Triangle tribune(Dothan,
> Ala.)Triangle tribune.Dothan, AL :Applewhite Pub. CoMonthlyBegan with
> vol. 1, issue 1 (May 2009).\"Connecting the Tri-State African -American
> Community.\"Description based on: Vol. 1, issue 1 (May 2009); title from
> masthead.Applewhite, Devon.United StatesAlabama.United
> StatesGeorgia.United StatesFlorida.info:srw/schema/1/marcxmlxml00000cas
> a22000007a 4500289017315081219c20089999aluwr n | a0eng c
> 2008213218NSDengNSDOCLCQDLCOCLCQ111945-93191945-93191005270USPSSpringhill Publications,
> LLC, P.O. Box 186, Greenville, AL 36037nsdppccISSN RECORD07014Greenville
> standardThe Greenville standard.Greenville, AL :Springhill
> PublicationsWeeklytexttxtrdacontentunmediatednrdamediaBegan with vol. 1,
> issue 1 (Sept. 3, 2008)Description based on surrogate of: Vol. 1, no. 15
> (Dec. 18, 2008); title from masthead (publisher's Web site, viewed Dec.
> 19, 2008).Latest issue consulted: Vol. 1, no. 99 (July 27, 2011)
> (surrogate).info:srw/schema/1/marcxmlxml00000cas a22000007a
> 4500123539969070426c20079999aluwr ne 0 a0eng c
> 2007212138NSDengNSDNSDOCLCQ101936-95571936-95571The Western Tribune,
> 1530 Third Ave. N., Bessemer, AL 35020mscnsdpISSN RECORDWestern tribune
> (Bessemer, Ala.)The Western tribune(Bessemer, Ala.)The Western
> tribune.Bessemer, Ala. :D-Med, Inc.v.WeeklyBegan in 2007.Description
> based on: May 23, 2007 (surrogate); title from
> caption.AU at 000041575341info:srw/schema/1/marcxmlxml00000cas a22000007a
> 4500226300653080425c20079999aluwr ne | a0eng
> 2008212112NSDengNSDNSDOCLCQ11942-20751942-20751nsdppccISSN RECORDThe
> corridor messengerThe corridor messenger.Carbon Hill, AL :Corridor
> Messenger, Inc.WeeklyBegan with vol. 1, issue (10.03.2007).Description
> based on: 1st issue.United StatesAlabamaWalkerCarbon
> Hill.http://www.corridormessenger.cominfo:srw/schema/1/marcxmlxml00000cas a22000007a
> 450077560432070109c20069999aluwr ne 0 a0eng c
> 2007213400NSDengNSDOCLCQAUBRNOCLCOOCLCFa01935-37901935-37901AU at 000041190283The
> 
> Auburn Villager, P.O. Box 1633, Auburn, AL 36831-1633pccnsdpISSN
> RECORDThe Auburn villagerThe Auburn villager.Auburn, AL :Auburn
> Villagerv.WeeklyBegan in 2006.Description based on: Vol. 1, no. 4 (July
> 20, 2006) (surrogate); title from caption.Auburn (Ala.)Newspapers.Lee
> County (Ala.)Newspapers.AlabamaAuburn.fast(OCoLC)fst01209634AlabamaLee
> County.fast(OCoLC)fst01211930Newspapers.fast(OCoLC)fst01423814United
> StatesAlabamaLeeAuburn.info:srw/schema/1/marcxmlxml00000cas a2200000Ii
> 4500872286785m o d s cr mn|---a||||140311c20069999alucr n o b
> s0 a0eng cABCengrdaABCABCOCLCFLD59.13University of Alabama at
> Birmingham.The eReporter.[Birmingham, Alabama] :The University of
> Alabama at Birmingham,[2006]-[Birmingham, Alabama] :Offices of Public
> Relations & Marketing and Information Technology1 online resource2
> issues weeklytexttxtrdacontentcomputercrdamediaonline
> resourcecrrdacarrierSeptember 19, 2006-\"The eReporter is an official
> communication of The University of Alabama at Birmingham, companion to
> the UAB Reporter and recommended alternative to mass e-mails.\"Issues
> for <March 11, 2014- published and distributed via e-mail subscription
> on Tuesdays and Fridays.Description based on: September 19, 2006; title
> from title screen (viewed March 12, 2014).University of Alabama at
> BirminghamPeriodicals.Periodicals.fast(OCoLC)fst01411641University of
> Alabama at Birmingham.fast(OCoLC)fst00645114University of Alabama at
> Birmingham.Office of Public Relations and Marketing.University of
> Alabama at Birmingham.Information Technology.2006-2012, companion
> to:University of Alabama at Birmingham.UAB
> reporter.(OCoLC)32435748Archived
> issueshttp://hatteras.dpo.uab.edu/cgi-bin/ereporter.cgiinfo:srw/schema/1/marcxmlxml00000cas
> 
> a22000007a 4500166387050070829c20059999aluwr ne | a0eng c
> 2007215501NSDengNSDOCLCQ11939-68991939-68991The Wilkie Clark Memorial
> Foundation, P.O. Box 514, Roanoke, AL 36274$30.00nsdpmscISSN
> RECORD305.89614People's voice (Roanoke, Ala.)The people's voice(Roanoke,
> Ala.)The people's voice.Roanoke, AL :Wilkie Clark Memorial
> Foundationv.WeeklyBegan with vol. 1, no. 1 in 2005.Description based on:
> Vol. 2, no. 20 (Apr. 20, 2007); title from caption.Wilkie Clark Memorial
> Foundation.United
> StatesAlabamaRandolphRoanoke.AU at 000042141390info:srw/schema/1/marcxmlxml00000nas
> 
> a22000007i 45001124677787191021c20uu9999aluwr ne | a0eng
> 2019202521DLCengrdaDLC12689-3258122730USPSNorth Jackson Press, 42950 Hwy
> 72, Suite 406, Stevenson, AL 35772nsdppccISSN RECORD071.323North Jackson
> pressNorth Jackson press.Stevenson, AL :Caney Creek Publications
> LLCWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierDescription
> based on surrogate of: Volume 1, number 36 (October 11, 2019); title
> from masthead.Latest issue consulted: Volume 1, number 36 (October 11,
> 2019) (Surrogate).United
> StatesAlabamaJacksonStevensoninfo:srw/schema/1/marcxmlxml00000cas
> a2200000 a 4500226315099080428d19981998aluwr ne | 0eng c
> 2008233691GUAengGUAOCLCQOCLCFOCLCO39911644pccn-us-gaThe Dekalb
> news.Birmingham, Ala. :Community newspaper holdings Inc.v.WeeklyBegan
> with 1st year, no. 1 (Apr. 1, 1998); ceased with 1st year, no. 31 (Oct.
> 28, 1998).Final issue consulted.Description based on first issue; title
> from caption.Decatur (Ga.)Newspapers.DeKalb County
> (Ga.)Newspapers.Newspapers.fast(OCoLC)fst01423814GeorgiaDecatur.fast(OCoLC)fst01226234GeorgiaDeKalb
> 
> County.fast(OCoLC)fst01215288United
> StatesGeorgiaDeKalbDecatur.Decatur-DeKalb news/era(DLC)sn
> 89053661(OCoLC)19946163info:srw/schema/1/marcxmlxml00000cas a2200000 i
> 450050263311m o d cr cn|||||||||020730c19979999alu x neo
> 0 a0eng c
> 2015238492AMHengrdapnAMHOCLCQOCLCFOCLCOIULOCLHTMOCLCQCOODLC66460694810970435082687-93791AU at 000050711528OCLCS45109pccnsdpn-us---AP2.B5707023Birmingham
> 
> weekly (Online)Birmingham weekly(Online)Birmingham weekly.Birmingham, AL
> :Birmingham Weekly1 online resourceIrregular,Feb. 16-28,
> 2012-Weekly,Sept. 4-11, 1997-Feb. 9-16,
> 2012texttxtrdacontentcomputercrdamediaonline resourcecrrdacarrierBegan
> with vol. 1, issue 1 (Sept. 4-11, 1997).\"City news, views &
> entertainment\"--Cover.Numbering dropped in Mar. 2012.Also issued in
> print.Description based on: Publication information from ProQuest; title
> from web page (viewed June 18, 2015).Latest issue consulted: Aug. 15-20,
> 2012.Birmingham (Ala.)Newspapers.Internet resources.Electronic
> journals.AlabamaBirmingham.fast(OCoLC)fst01204958Newspapers.fast(OCoLC)fst01423814United
> 
> StatesAlabamaBirmingham.Print version:Birmingham
> Weekly(OCoLC)39271050http://apw.softlineweb.com/http://WC2VB5MT8E.search.serialssolutions.com/?sid=sersol&SS_jc=JC_000051895&title=Birmingham+Weeklyinfo:srw/schema/1/marcxmlxml00000cas
> 
> a22000007a 450031471314941116d19941995aluwr ne 0 a0eng csn
> 94003083
> NSDengNSDANEOCLCQOCLCFOCLCOOCLCQ11079-65411079-65411nsdppccn-us-akSoutheast
> shopperSoutheast shopper.Juneau, Alaska :Kemper
> Communications,1994-volumesWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol.
> 
> 1, no. 1 (Nov. 16, 1994)-Ceased in Feb. 1995.Juneau
> (Alaska)Newspapers.AlaskaJuneau.fast(OCoLC)fst01213587Newspapers.fast(OCoLC)fst01423814United
> 
> StatesAlaskaJuneau.AU at 000011356572info:srw/schema/1/marcxmlxml00000cas
> a22000008a 450027910515930413c19949999alumr n 0 a0eng dsn
> 93002581 NSDengNSDOCLCQ11069-06621Birmingham Tribune, 216 Ave. T. Pratt
> City, Birmingham, AL 35214nsdpBirmingham tribuneBirmingham
> tribune.Birmingham, Ala. :Kervin
> Fondren9501volumesMonthlytexttxtrdacontentunmediatednrdamediavolumencrdacarrierPREPUB:
> 
> publication expected Jan.
> 1995AU at 000025863987info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450026199931920716d19922013alumr ne 0 a0eng csn 92003357
> NSDengNSDOCLOCLCQDLC011064-01341064-01341Black & White, POB 13215,
> Birmingham, AL 35202-3215nsdppccBlack & white (Birmingham, Ala.)Black &
> white(Birmingham, Ala.)Black & white.Black and whiteBirmingham, Ala.
> :Black & White, Inc.v.Biweekly,Oct. 2, 1997-Monthly,May 1, 1992-Sept.
> 1997Began in May 1992; ceased with Jan. 10, 2013.\"Birmingham's New City
> paper.\"Description based on: June 1992.Latest issue consulted: No. 67
> (Oct. 16, 1997) (surrogate).info:srw/schema/1/marcxmlxml00000cas
> a2200000 a 450032145723950314d19901999alumr ne 0 a0eng csn
> 95068755
> MGNengMGNNSDCLUOCLCQOCLCFOCLCOOCLCA971211082-34841082-34841AU at 000011579542nsdppccn-us-alF335.J5S68The
> 
> Southern shofarThe Southern shofar.Birmingham, AL :L. Brook,-[1999]v.
> :ill. ;35 cm.MonthlyBegan in 1990.-v. 9, issue 9 (Aug./Sept. 1999).\"The
> monthly newspaper of Alabama's Jewish community.\"Some issues also
> available on the Internet via the World Wide Web.Description based on:
> Vol. 3, issue 11 (Oct. 1993).Jewish newspapersAlabama.Jewish
> newspapers.fast(OCoLC)fst00982872Alabama.fast(OCoLC)fst01204694United
> StatesAlabamaJeffersonBirmingham.Deep South Jewish voice(DLC)sn
> 99018499(OCoLC)42431704CLUhttp://bibpurl.oclc.org/web/719http://www.bham.net/shofar/info:srw/schema/1/marcxmlxml00000cas
> 
> a22000007a 450021265141900326c19909999aluwr ne 0 a0eng csn
> 90099004 AARengAARCPNNSDOCLCQ11050-08981050-08981005022USPSE.O.N., Inc.,
> Main St., Eclectic, AL 36024pccnsdpISSN RECORDThe Eclectic observerThe
> Eclectic observer.Eclectic, Ala. :E.O.N., Inc.,1990-v.WeeklyVol. 1, no.
> 1 (Feb. 22, 1990)-Published by: Price Publications, Inc., <2006->Latest
> issue consulted: Vol. 17, no. 1 (Jan. 5, 2006).United
> StatesAlabamaElmoreEclectic.AU at 000040212446info:srw/schema/1/marcxmlxml00000cas
> 
> a22000007a 450021214781900314c19909999aluir ne 0 a0eng csn
> 90002457 AAAengAAANSDOCLCQ111050-20841050-20841931180USPSClanton
> Newspapers, 1109 Seventh St., N., PO Box 1379, Clanton, AL
> 35045nsdppccn-us-alThe Clanton advertiserThe Clanton
> advertiser.AdvertiserClanton, Ala. :Clanton Newspapersv. :ill. ;58
> cm.Three no. a week,<May 13, 1992->Semiweekly,<Apr. 4, 1990->Began in
> Jan. 1990.Description based on: Vol. 19, no. 27 (Wed., Apr. 4,
> 1990).Latest issue consulted: Vol. 22, no. 58 (May 13, 1992).United
> StatesAlabamaChiltonClanton.Independent advertiser (Clanton,
> Ala.)(OCoLC)21214732AU at 000025908452info:srw/schema/1/marcxmlxml00000cas
> a2200000 a 450021214814900314c19909999aluwr ne 0 a0eng dsn
> 90099009 AAAengAAACPNNSDOCLCQ11056-32881056-32881505740USPSThe Blount
> Countian, 3rd St. at Washington Ave., PO Box 310, Oneonta, AL
> 35121mscnsdpn-us-alThe Blount countianThe Blount countian.Oneonta, Ala.
> :Southern Democrat, Inc.,1990-v. :ill.WeeklyVol. 1, no. 1 (Jan. 3,
> 1990)-Editor: Molly Howard Ryan, 1990-Latest issue consulted: Vol. 1,
> no. 36 (Sept. 5, 1990).Ryan, Molly Howard.United
> StatesAlabamaBlountOneonta.Southern Democrat(DLC)sn
> 85044741(OCoLC)12038577AU at 000025884049info:srw/schema/1/marcxmlxml00000cas
> a22000007a 450022413044900920c19909999aluwr ne 0 a0eng dsn
> 90099011
> AARengAARCPNNSDNSTOCLCQ92081707011191053-91231053-91231314240USPSmscnsdpThe
> Clay times-journalThe Clay times-journal.Lineville, Ala. :C.L.
> Proctor,1990-v.WeeklyVol. 1, no. 1 (Sept. 6, 1990)-United
> StatesAlabamaClayLineville.Ashland progress(DLC)sn 85044701Lineville
> tribune(DLC)sn 85044702AUinfo:srw/schema/1/marcxmlxml00000cas a22000007a
> 450021265218900326c19909999aluwr ne 0 0eng dsn 90099005
> AARengAARCPNOCLCQmscTrussville news-journal.Trussville, Ala. :Mike
> Mitchell,1990-v.BimonthlyVol. 1, no. 1 (Feb. 20, 1990)-United
> StatesAlabamaJeffersonTrussville.info:srw/schema/1/marcxmlxml00000cas
> a22000007a 450022301035900831c19909999aluwr ne 0 0eng dsn
> 90099010 AARengAARCPNOCLCQmscWeaver tribune.Oxford, Ala. :Cheaha
> Pub.,1990-v.WeeklyVol. 1, no. 1 (July 19, 1990)-United
> StatesAlabamaCalhounWeaver.United
> StatesAlabamaCalhounOxford.info:srw/schema/1/marcxmlxml00000cas
> a22000007a 450015155895870205c19879999aludr ne 0 a0eng csn
> 87050045
> AAAengAAACPNNSDDLCCPNNSDDLCCPNDLCOCLDLCOCLCQOCLCFOCLCQ19261126829944596670892-44570892-44571AU at 000020456714360980USPSThe
> 
> Advertiser, P.O. Box 1000, Montgomery, AL
> 36192pccnsdpn-us-alNewspaperMontgomery advertiser (Montgomery, Ala. :
> 1987)The Montgomery advertiser(1987)The Montgomery advertiser.Montgomery
> advertiser & the Alabama journalSunday Montgomery advertiserMontgomery,
> Ala. :Advertiser Co.,1987-volumes
> :illustrationsDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrier160th
> 
> year, no. 1 (Jan. 2, 1987)-On Saturdays, Sundays and holidays a combined
> edition is published with the Alabama journal, and called: Montgomery
> advertiser and the Alabama journal, Jan. 3, 1987, and: Alabama journal
> and Montgomery advertiser, Jan. 4, 1987-Feb. 25, 1990.Issues for Sunday
> called: Sunday Montgomery advertiser, Mar. 4, 1990-Issues for Saturday,
> Sunday and holidays have their own numbering, Jan. 3, 1987-Feb. 25,
> 1990.Montgomery
> (Ala.)Newspapers.AlabamaMontgomery.fast(OCoLC)fst01202689Newspapers.fast(OCoLC)fst01423814United
> 
> StatesAlabamaMontgomeryMontgomery.Advertiser (Montgomery,
> Ala.)0745-3221(DLC)sn 82008412(OCoLC)9049482Alabama journal (Montgomery,
> Ala. : 1940)0745-323X(DLC)sn
> 87062018(OCoLC)2666111info:srw/schema/1/marcxmlxml00000cas a2200000 a
> 450016942287871105c19879999aludn ne 0 a0eng dsn 88050149
> AAAengAAACPNNSDOCLCQy1044-00701044-0070746--32780746-32781565580USPSTroy
> Publications, Inc., 113 North Market St., Troy, AL 36081mscnsdpMessenger
> (Troy, Ala.)The Messenger(Troy, Ala.)The Messenger.Troy, Ala. :Troy
> Pub.,1987-v.Daily (Sunday, Tuesday, Thursday and Friday)Vol. 121, no.
> 166 (July 1, 1987)-Sunday, Apr. 2, 1989 misprinted as v. 113.Latest
> issue consulted: Vol. 113 [sic 123], no. 96 (Sunday, Apr. 2,
> 1989).United StatesAlabamaPikeTroy.Troy messenger0746-3278(DLC)sn
> 83009935(OCoLC)9921908info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450017799786880415c19879999aluir ne 0 a0eng dsn 88050086
> AARengAARCPNNSDOCLCQ1p1044-03801044-03800745-75961441520USPSThe
> Prattville Progress, 152 W. 3rd St., Prattville, AL
> 36067mscnsdpPrattville progress (Prattville, Ala. : 1987)The Prattville
> progress(Prattville, Ala.)The Prattville progress.Prattville, Ala.
> :James C. Seymour,1987-v.Three times a weekVol. 102, no. 8 (Jan. 20,
> 1987)-Latest issue consulted: Vol. 105, no. 153 (Wednesday, Dec. 26,
> 1990).United StatesAlabamaAutaugaPrattville.Progress (Prattville,
> Ala.)0745-7596(DLC)sn
> 83007623(OCoLC)9428489info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450015344667870319c19869999aluwr ne 0 a0eng dsn 87000284
> NSDengNSDCPNOCLCQy0893-07670893-07671431800USPSPickens County Herald,
> P.O. Drawer E, Carrollton, AL 35447nsdpPickens County heraldPickens
> County herald.Pickens County herald and west AlabamianCarrollton, Ala.
> :Pickens Newspapers, Inc.,1986-WeeklyVol. 138, no. 40 (Oct. 2,
> 1986)-United StatesAlabamaPickensCarrollton.Pickens County herald and
> west Alabamian0746-0473(DLC)sn
> 83008141AU at 000040635809info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450018917586881217c19869999aluwr ne 0 0eng dsn 88050225
> CPNengCPNOCLCQmscThe Oxford sun/times.Oxford, Ala.
> :[s.n.],1986-v.WeeklyVol. 1, no. 1 (Jan. 16, 1986)-Editor: Andy
> Goggans.Numbering is irregular.United StatesAlabamaCalhounOxford.Oxford
> sun (Oxford, Ala.)(DLC)sn
> 85045023AU at 000025803813info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450013991168860731c19869999aluwr ne 0 0eng dsn 86050322
> CPNengCPNOCLCQmscIndependent (Brewton, Ala.)The Independent.Brewton,
> Ala. :Jim Thornton,1986-v. :ill. ;58 cm.WeeklyVol. 1, no. 1 (June 19,
> 1986)-United
> StatesAlabamaEscambiaBrewton.info:srw/schema/1/marcxmlxml00000cas
> a22000007a 450018957493881231c19859999aluwr ne 0 0eng dsn
> 88050247 CPNengCPNOCLCQmscPiedmont journal-independent (Piedmont,
> Ala.)The Piedmont journal-independent.Journal independentPiedmont, Ala.
> :Lane Weatherbee,1985-v.WeeklyVol. 4, no. 52 (Dec. 24, 1985)-Sometimes
> published as: Journal independent.United
> StatesAlabamaCalhounPiedmont.Journal-independent(DLC)sn
> 85045014info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450012715821851024d19841985aluwr ne 0 a0eng dsn 85045014
> CPNengCPNNSDCPNOCLCQmscThe Journal-independent.Piedmont, Ala.
> :Journal-Independent, Inc.,1984-1985.volumes :illustrations ;58
> cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 3,
> no. 27 (July 3, 1984)- v. 4, no. 51 (Dec. 18, 1985).Carries the same
> vol. numbering as the Piedmont journal-independent.United
> StatesAlabamaCalhounPiedmont.Piedmont
> journal-independent0890-6017(DLC)sn 85045013Piedmont journal-independent
> (Piedmont, Ala.)(DLC)sn 88050247info:srw/schema/1/marcxmlxml00000cas
> a22000007a 450012691448851018c19839999aludr ne 0 0eng dsn
> 85045007 CPNengCPNOCLCQmscTimesDaily.Times dailyFlorence, Ala. :T.S.P.
> Newspapers, Inc.,1983-volumes :illustrations ;58
> cmDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 114,
> no. 226 (Aug. 14, 1983)-United StatesAlabamaLauderdaleFlorence.Florence
> times + tri-cities daily(DLC)sn
> 85044995info:srw/schema/1/marcxmlxml00000cas a22000007a
> 45009428489830420d19831987aluir ne 0 a0eng dsn 83007623
> NSDengNSDCPNNSDNSTOCLCQ89090d0745-75960745-75961The Progress, 152 W. 3rd
> St., Prattville, AL 36067nsdpmscProgress (Prattville, Ala.)The
> Progress(Prattville, Ala.)The Progress.Prattville, Ala. :The Prattville
> Progress,1983-1987.volumes :illustrations ;58 cmThree times a
> weektexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 98, no.
> 32 (Mar. 17, 1983)-v. 102, no. 7 (Jan. 17, 1987).United
> StatesAlabamaAutaugaPrattville.Prattville progress(DLC)sn
> 85044740Prattville progress (Prattville, Ala.)1044-0380(DLC)sn
> 88050086(OCoLC)12254317AAPinfo:srw/schema/1/marcxmlxml00000cas a2200000
> a 45009867255830831c19839999aludr ne 0 a0eng dsn 84008052
> AAAengAAANSDOCLOCLCQX0743-15110743-15111617760USPST.S.P. Newspapers,
> Inc., 219 W. Tennessee St., Florence, AL 35630nsdpTimesDaily (Shoals
> edition)TimesDaily(Shoals ed.)TimesDaily.Times dailyShoals ed.Florence,
> Ala. :T.S.P. Newspapersvolumes
> :illustrationsDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan
> 
> with: Vol. 114, no. 226 (Aug. 14,
> 1983).\"Florence/Sheffield/Tuscumbia/Muscle Shoals.\"Shoals ed. and
> Regional ed. combined on Sundays.Description based on: Vol. 114, no. 346
> (Monday, Dec. 12, 1983).United
> StatesAlabamaLauderdaleFlorence.TimesDaily (Regional
> edition)0743-152XTimes Tri-cities dailyUnknownDec. 12,
> 1983info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450010536023840319c19839999aludr ne 0 a0eng dsn 84008051
> NSDengNSDOCLCQ1x0743-152X0743-152X1617760USPST.S.P. Newspapers, Inc.,
> 219 W. Tennessee St., Florence, AL 35630nsdpTimesDaily (Regional
> edition)TimesDaily(Regional ed.)TimesDaily.Times dailyRegional
> ed.Florence, Ala. :T.S.P.
> NewspapersDailytexttxtrdacontentunmediatednrdamediaBegan with: Vol. 114,
> no. 226 (Aug. 14, 1983).Shoals ed. and Regional ed. combined on
> Sundays.Description based on: Vol. 114, no. 346 (Monday, Dec. 12,
> 1983).United StatesAlabamaLauderdaleFlorence.TimesDaily (Shoals
> edition)0743-1511Times Tri-cities dailyDec. 12,
> 1983AU at 000025818125info:srw/schema/1/marcxmlxml00000cas a22000007a
> 45009049482821213d19821987aludn ne 0 a0eng csn 82008412
> AAAengAAANSDNPWCPNDLCCPNNSDDLCNSDDLCCPNNVFDLCOCLCQCRLOCLCFOCLCQ1d0745-32210745-32211nsdppccn-us-alNewspaperAdvertiser
> 
> (Montgomery, Ala.)The Advertiser(Montgomery, Ala.)The advertiser.Alabama
> journal and advertiserMontgomery, Ala. :Advertiser Co.,1982-1987.volumes
> :illustrationsDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrier155th
> 
> year, no. 232 (Nov. 22, 1982)- ; -v. 14-3, Jan. 1, 1987.On Saturdays,
> Sundays and holidays published as: The Alabama journal and advertiser,
> Nov. 27, 1982-Jan. 1, 1987.Saturday, Sunday and holiday issues have
> their own numbering.Montgomery
> (Ala.)Newspapers.AlabamaMontgomery.fast(OCoLC)fst01202689Newspapers.fast(OCoLC)fst01423814United
> 
> StatesAlabamaMontgomeryMontgomery.Montgomery advertiser (Montgomery,
> Ala. : Daily)(DLC)sn 84020645(OCoLC)2685433Montgomery advertiser
> (Montgomery, Ala. : 1987)0892-4457(DLC)sn
> 87050045(OCoLC)15155895AU at 000020281746info:srw/schema/1/marcxmlxml00000cas
> a2200000 a 45009237931830218c19829999aluwr ne 0 0eng dsn
> 86050139 AAAengAAACPNOCLOCLCQmscThe Randolph leader.Roanoke, Ala. :David
> S. Stevenson,1982-volumes :illustrations ;58
> cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 91,
> no. 1 (Oct. 6, 1982)-United StatesAlabamaRandolphRoanoke.Roanoke
> leader(DLC)sn 86050137Randolph press(DLC)sn
> 86050138info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450012715815851024d19821984aluwr ne 0 a0eng dsn 85045013
> CPNengCPNNSDCPNOCLCQ110890-60170890-60171432080USPSThe Piedmont
> Journal-Independent, 115 N. Center Ave., Piedmont, AL 36272mscnsdpThe
> Piedmont journal-independentThe Piedmont journal-independent.Piedmont,
> Ala. :Piedmont Journal-Independent, Inc.,1982-1984.volumes
> :illustrations ;58
> cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 1,
> no. 1 (Mar. 31, 1982)-v. 3, no. 26 (June 27, 1984).Latest issue
> consulted: Vol. 5, no. 31 (August 20, 1986).United
> StatesAlabamaCalhounPiedmont.Piedmont journal(DLC)sn
> 85045012Journal-independent(DLC)sn
> 85045014(OCoLC)12715821AU at 000045312916info:srw/schema/1/marcxmlxml00000cas
> a22000007a 45009183905830202c19829999aluwr n 0 a0eng dsn
> 85044580 AAAengAAACPNNSDOCLOCLCQ11098-58671098-58671016409USPSNo. 4,
> Rucker Plaza, Enterprise, AL 36331P.O. Box 1536, Enterprise, AL
> 36331mscnsdpSoutheast sun (Enterprise, Ala.)The southeast
> sun(Enterprise, Ala.)The Southeast sun.Enterprise, Ala. :QST
> Publicationsvolumes :illustrations ;58
> cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan in
> 1982.Description based on: Vol. 1, no. 25 (Oct. 21, 1982).Latest issue
> consulted: Vol. 16, no. 43 (Mar. 4, 1998).United
> StatesAlabamaCoffeeEnterprise.AU at 000025827687info:srw/schema/1/marcxmlxml00000cas
> 
> a22000007a 450010487314840305c19819999aluwr ne 0 a0eng dsn
> 85044906
> AAAengAAACPNNSDNSTCPNOCLOCLCQOCLCFOCLCOOCLCAOCLCQ900410885-16620885-16621749310USPSThe
> 
> New Times, 1618 1/2 St. Stephens Rd., Mobile, AL 36603mscnsdpn-us-alNew
> times (Mobile, Ala.)The New times(Mobile, Ala.)The new times.Mobile,
> Ala. :New Times Groupvolumes
> :illustrationsWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan
> 
> in 1981.Vol. 3, no. 49 (Dec. 15-21, 1983) and vol. 3, no. 50 (Dec.
> 22-28, 1983) are both called vol. 3, no. 49 (Dec. 15-21,
> 1983).Description based on: Vol. 2, no. 3 (Jan. 28-Feb. 3, 1982).African
> AmericansAlabamaNewspapers.African
> Americans.fast(OCoLC)fst00799558Alabama.fast(OCoLC)fst01204694Newspapers.fast(OCoLC)fst01423814United
> 
> StatesAlabamaMobileMobile.AAPUnknownAug. 15,
> 1985AU at 000024686659info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450018922463881219d19811983alucr ne 0 0eng dsn 88050233
> AARengAARCPNNSDOCLCQmscThe Sylacauga daily advance.Advance/Sylacauga
> dailySylacauga advanceSunday advanceAdvanceSylacauga, Ala. :Mrs. W.A.
> Moody,1981-1893.v.Semiweekly,<Nov. 24, 1982-Feb. 13, 1983>Daily (except
> Mon., Tues. & Sat.),<May 26, 1982-Nov. 21, 1982>Daily (except Sat. &
> Mon.),<Jan. 1, 1981-May 23, 1982>74th Year, no. 123 (Jan. 1, 1981)-76th
> year, no. 83 (Feb. 13, 1983).Days of publication vary.Published as: The
> Advance/Sylacauga daily, <Aug. 28, 1981-May 23, 1982>.Published as:
> Sylacauga advance, <Nov. 24, 1982-Feb. 13, 1983>.On Sunday, published
> as: Sunday advance.United StatesAlabamaTalladegaSylacauga.Childersburg
> star(DLC)sn 88050232Coosa press(DLC)sn 86050293Daily
> home1059-6461(DLC)sn 88050234info:srw/schema/1/marcxmlxml00000cas
> a22000007a 450021026715cr un|||||||||900209c19809999aluwr ne 0
> 0eng dsn 90099002
> AARengAARCPNCUSOCLOCLCQTJCOCLCQOCLCFOCLCOOCLCA926143844AU at 000020585756mscn-us-alSpeakin'
> 
> out news.Speaking out newsDecatur, Ala. :Minority Network,
> Inc.v.WeeklyBegan in 1980.Published in Huntsville, Ala., <1987>-Also
> issued by subscription via the World Wide Web.Description based on: Vol.
> 7, no. 8 (Jan. 7-13, 1987).African AmericansAlabamaNewspapers.African
> American
> newspapersAlabama.AlabamaNewspapers.Newspapers.fast(OCoLC)fst01423814African
> 
> American newspapers.fast(OCoLC)fst00799278African
> Americans.fast(OCoLC)fst00799558Alabama.fast(OCoLC)fst01204694United
> StatesAlabamaMorganDecatur.United
> StatesAlabamaMadisonHuntsville.Speakin' out weekly news(DLC)sn
> 88050097http://www.softlineweb.com/softlineweb/ethnic.htminfo:srw/schema/1/marcxmlxml00000cas
> 
> a22000007a 450014996511861219c19809999aluwr ne 0 a0eng csn
> 86050472
> AARengAARCPNNSDOCLCQ11080-15021080-15021328110USPSnsdppccWest-Alabama
> gazetteWest-Alabama gazette.GazetteMillport, Ala. :Millport Pub.
> Co.,1980-volumesWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrier4th
> 
> year, no. 32 (Jan. 3, 1980)-United StatesAlabamaLamarMillport.Gazette
> (Millport, Ala.)(DLC)sn 86050471info:srw/schema/1/marcxmlxml00000cas
> a2200000 a 450011828156850320c19809999aluwr ne 0 0eng dsn
> 86050314 AAAengAAACPNOCLOCLCQmscThe Hartford news-herald.Hartford, Ala.
> :Geneva Publications,1980-volumes :illustrations ;57-59
> cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 80,
> no. 20 (Feb. 14, 1980)-United StatesAlabamaGenevaHartford.News-herald
> (Hartford, Ala.)(DLC)sn 86050313info:srw/schema/1/marcxmlxml00000cas
> a22000007a 450017857788880427d198u198ualusr ne 0 0eng dsn
> 88050097 AARengAARCPNOCLOCLCQOCLCFOCLCOOCLCAmscn-us-alSpeakin' out
> weekly news.Decatur, Ala. :Smothers PublicationsPublished every first
> and third Wed. of each monthDescription based on: Vol. 3, no. 13 (May
> 4-17, 1983).African
> AmericansAlabamaNewspapers.Newspapers.fast(OCoLC)fst01423814African
> Americans.fast(OCoLC)fst00799558Alabama.fast(OCoLC)fst01204694United
> StatesAlabamaMorganDecatur.Weekly news (Huntsville, Ala.)(DLC)sn
> 87050012Speakin' out news(DLC)sn
> 90099002info:srw/schema/1/marcxmlxml00000cas a2200000 a
> 450017807936880418c198u9999aluwr ne 0 a0eng dsn 90099001
> AAAengAAACPNOCLOCLCQThe Daleville Sun-Courier, 310 Daleville Ave.,
> Daleville, AL 36322mscn-us-alDaleville sun-courier.Daleville, Ala. :QST
> Publicationsv. :ill. ;58 cm.WeeklyDescription based on: Vol. 2, no. 28
> (Wed., Feb. 17, 1988).United
> StatesAlabamaDaleDaleville.AU at 000020585749info:srw/schema/1/marcxmlxml00000cas
> 
> a22000007a 450015580838870423c198u9999aluwr ne 0 0eng dsn
> 87050128 AARengAARCPNOCLCQmscGreene County independent.Eutaw, Ala.
> :Greene County Independent, Inc.v.WeeklyDescription based on: Vol. 2,
> no. 10 (Mar. 12, 1987).United
> StatesAlabamaGreeneEutaw.info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450010125135831114d198u198ualucr ne 0 a0eng dsn 83003221
> NSDengNSDOCLCQ0d0746-55210746-55211Auburn Bulletin & Lee County Eagle,
> PO Box 2111, Auburn, Ala. 36830nsdpThe Auburn bulletin & the Lee County
> eagleThe Auburn bulletin & the Lee County eagle.Lee County eagleAuburn
> bulletin and the Lee County eagleAuburn, Ala. :[publisher not
> identified]Semiweekly,<Sept. 5,
> 1984->WeeklytexttxtrdacontentunmediatednrdamediaDescription based on:
> Oct. 19, 1983.United StatesAlabamaLeeAuburn.Auburn bulletin(DLC)sn
> 89050006Eagle (Auburn, Ala.)(OCoLC)18435663Sept. 5,
> 1984info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450018370324880818c198u9999aluwr ne 0 0eng dsn 88050147
> CPNengCPNOCLCQmscTri-city times (Geraldine, Ala.)The Tri-City
> times.Geraldine, Ala. :Wanda Nelsonv.WeeklyDescription based on: Vol. 2,
> no. 24 (Jan. 6, 1982).United
> StatesAlabamaDeKalbGeraldine.info:srw/schema/1/marcxmlxml00000cas
> a22000007a 450010199338831208c198u9999aluwr ne 0 a0eng dsn
> 83005367 NSDengNSDCPNOCLCQ10746-62770746-62771707590USPSSpringville Pub.
> Co., 539 Main St., Springville, AL 35146nsdpThe St. Clair clarionThe St.
> Clair clarion.Saint Clair clarionSpringville, AL :Gary L.
> ShultsWeeklytexttxtrdacontentunmediatednrdamediaDescription based on:
> Vol. 2, no. 1 (Jan. 5, 1982).United StatesAlabamaSt.
> ClairSpringville.AU at 000025783743info:srw/schema/1/marcxmlxml00000cas
> a22000007a 450013787251860627c198u9999aluwr ne 0 a0eng dsn
> 86001923 NSDengNSDCPNOCLCQ10889-00800889-00801The Westerner Star, P.O.
> Box 2060, Bessemer, AL 35021nsdpWestern star (Bessemer, Ala.)The Western
> star(Bessemer, Ala.)The western star.Bessemer, Ala. :Hal
> HodgensWeeklytexttxtrdacontentunmediatednrdamediaDescription based on:
> Vol. 3, no. 15 (Wednesday, June 11, 1986).United
> StatesAlabamaJeffersonBessemer.Bessemer advertiser(DLC)sn
> 87050117AU at 000025805174511.1srw.pc any \"y\" and srw.mt any
> \"newspaper\" and srw.cp exact
> \"Alabama\"50info:srw/schema/1/marcxmlxml1Date,,0mq1lME887FoIbjulKUV6bx9ImwWQNCv9GqZzGS92IKS31lEbcpRJBNHgcE1l29tFaHP9CHe0Yexk1uWQofffull"
> 
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jul 27 2022, 09:22am via System
> 
> Hello Spencer,
> 
> Thank you for reaching out about the bulk xml files for the US Newspaper
> Directory.
> 
> We don't have documentation specific to these bulk xml files, but upon
> further inspection I can say that each of those files don't necessarily
> contain info for 50 newspaper titles. The structure of the titles for
> California and New York for instance are different from say, Alabama.
> 
> If you look at California for example, the file naming structure
> indicates the year the title started, and then the number of titles
> included in that xml file. So for instance, the files below include info
> for newspapers that started in 2000, 2001, and 2002 respectively. And
> there is info for 30 titles in the xml file from 2000, and 14 in the
> file for 2001, and so on.
> 
>     * ndnp_California_2000_e_0001_0030.xml
>     * ndnp_California_2001_e_0001_0014.xml
>     * ndnp_California_2002_e_0001_0012.xml
> 
> If there's more than 50 titles for a given year, say for California
> starting in 1880, then the next 50 titles will roll into the next xml
> file, and so on. And the last xml file for that year may not include 50
> titles.
> 
> Many of the states seem to group all the years together, so each xml
> file contains 50 titles, until possibly the last one for a given state,
> which may contain less.
> 
> I hope this information helps explain the total number of records and
> structure a bit better. Let me know if you have any further questions.
> 
> Best wishes,
> 
> Kerry Huller
> Newspaper & Current Periodical Reading Room
> Serial & Government Publications Division
> Library of Congress
> 
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jul 25 2022, 02:22pm via Email
> 
> Hi, Kerry:
> 
> 
> Might there be documentation on the XML files you mentioned?
> 
> 
> I've successfully read
> 'https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/',
> extracted the names of 6666 XML files, and read the first one,
> "ndnp_Alabama_all-yrs_e_0001_0050.xml". It contains 29415 characters,
> beginning, "1.12250info:srw/schema/1/marcxmlxml00000nas a22000007i
> 45001030438981180404c20159999aluwr n 0 a0eng ". With a bit
> more effort, I will likely be able to parse all 6666 of these. The
> names suggest that each contains information on 50 newspapers, totaling
> 333,300. The main page
> "https://chroniclingamerica.loc.gov/search/titles/" says there are only
> 157,521 "Titles currently listed". This suggests that these XML files
> include place holders for a little more than double the number of
> entries currently in "https://chroniclingamerica.loc.gov/search/titles/".
> 
> 
> Thanks for this.
> 
> 
> Progress.
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jul 07 2022, 08:55am via System
> 
> Hi Spencer,
> 
> I thought of one more option after I emailed you yesterday that I wanted
> to make you aware of.
> 
> I had explained the other day how we pull the records from OCLC into our
> U.S. Newspaper Directory. You can also access all of the raw MARC
> records found in the directory in xml format from here if you choose:
> https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/
> <https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/> These will
> provide you all of the data from the record fields in MARC format, so
> you'd get all the data you see here for example:
> https://chroniclingamerica.loc.gov/lccn/sn98059792/marc/
> <https://chroniclingamerica.loc.gov/lccn/sn98059792/marc/> but in xml. I
> don't know if this might be more data and info than you want to work
> with, but wanted to make sure you were aware of this option as well.
> 
> Best wishes,
> 
> Kerry Huller
> Newspaper & Current Periodical Reading Room
> Serial & Government Publications Division
> Library of Congress
> 
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jul 06 2022, 10:55am via System
> 
> Hi Spencer,
> 
> Thanks for reaching out again. I have been looking at the json view a
> bit closer this morning and your example of "9999."
> 
> After talking with a colleague this morning and looking at various
> examples, I see there is some variation in how the titles with either an
> unknown starting/ending date or currently published titles are being
> handled - depending on the view.
> 
> As an example, I completed a search in the directory for Alaska and the
> city of Anchorage. There are 80 results, and on the first page of
> results you'll see # 4. Fort Richardson news, which was published from
> 1952-19??. The csv view of this state/city search result will show the
> ending date of 19??. But if I append &format=json to this search result,
> this specific title will show an ending date of 1999. After talking with
> a colleague this morning, I discovered an integer had to be used in
> these cases where dates were "?" so that the search based on year range
> would work. Similarly, if you look at # 12 Alaska digest, which was
> published 1994-current, the "current" becomes "9999" in the json view.
> So, the records you are seeing with "9999" would most likely be titles
> with an ending date of "current."
> 
> However, there is an issue with the unknown dates, like "1999" being
> used for "19??" in the example above. The "9" does not get inserted in
> place of "?" when you are looking at the title/LCCN view of a specific
> newspaper. So for instance, if you view the #4 title: Fort Richardson
> news at this url: https://chroniclingamerica.loc.gov/lccn/sn98059792/
> <https://chroniclingamerica.loc.gov/lccn/sn98059792/> but append .json
> to the end of the url, after the LCCN, like this:
> https://chroniclingamerica.loc.gov/lccn/sn98059792.json
> <https://chroniclingamerica.loc.gov/lccn/sn98059792.json> you'll see
> that the end_year is "19??." Viewing the title/LCCN json view for titles
> that are currently published will also show the end_year as "current."
> The Alaska digest example from above can be viewed here:
> https://chroniclingamerica.loc.gov/lccn/sn97060056.json
> <https://chroniclingamerica.loc.gov/lccn/sn97060056.json>
> 
> I wasn't aware of the difference between the directory search json view
> and the title/LCCN view. But I think it would be possible to grab
> the data from the title/LCCN json url through an additional script
> potentially. The json url is included in the view under the "url" field.
> 
> Of course, there are unknowns with publishing dates, but better to know
> where the question marks are, and what titles are considered to be current.
> 
> I hope this clarifies the data a bit more - let me know if any of it
> needs more clarification though. And let me know if you have follow-up
> questions.
> 
> Thank you,
> 
> Kerry Huller
> Newspaper & Current Periodical Reading Room
> Serial & Government Publications Division
> Library of Congress
> 
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jul 05 2022, 04:42pm via Email
> 
> Hi, Kerry:
> 
> 
> What would you suggest I do to get a count of the numbers of
> newspapers and publishers operating by year from, say, 1790 to 2021?
> 
> 
> I just determined that 20630 (13 percent) of the 157520 records in
> the US Newspaper database I downloaded a week ago have end_year = 9999.
> I don't think it's feasible to assume that all or even most of those
> are still publishing.
> 
> 
> Might there be some other database that might have this kind of
> information?
> 
> 
> I ask, because Robert McChesney (2004) The Problem of the Media
> (Monthly Review Pr., esp. pp. 34-35) suggests that in the first half of
> the nineteenth century, the US had more newspapers and newspaper
> publishers per capita than any other place or time. He suggests that
> that diversity of newspapers helped encourage literacy and limit
> political corruption, both of which helped propel the young US to its
> current dominance of the international political economy. I'm hoping to
> get some data to evaluate this claim. Sadly, it looks like there is too
> much missing and questionable data in this dataset for me to use this
> without a fairly substantive data cleaning effort.
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jul 05 2022, 09:05am via System
> 
> Hello Spencer,
> 
> Thank you for reaching out about your additional questions.
> 
> I was looking at the records you mention above, and yes, you are correct
> - those 9 records with the date inconsistencies and the one record for
> the The New Mexican mining news
> <https://chroniclingamerica.loc.gov/lccn/sn93061507/> containing "Santa
> Fe.\" have typos in them. Thanks for spotting these - it may be possible
> to have the cataloger in our division correct those typos. I will look
> into this further.
> 
> The U.S. Newspaper Directory doesn't have a connection with Wikimedia or
> Wikipedia. The Library of Congress periodically pulls the records for
> the Directory from OCLC Worldcat
> <https://www.oclc.org/en/worldcat.html>. And those newspaper records in
> OCLC Worldcat have been created by catalogers at various institutions
> around the U.S. over the span of several years. So, occasionally, you
> will find a typo in the records. Corrections can be made by OCLC and
> library staff at the various institutions. Every time we complete a new
> pull on the OCLC records, any corrected records will then populate our
> Directory.
> 
> Regarding your question on the New-York weekly journal - yes, that is
> also correct that it has two records. There is actually a record for
> each format of the newspaper, so this record is for the microfilm format
> <https://chroniclingamerica.loc.gov/lccn/2009252748/> and this one is
> for the original print format
> <https://chroniclingamerica.loc.gov/lccn/sn83030211/>. You can see in
> the heading for the microfilm record where it says [microfilm reel] and
> the print version shows [volume]. You are likely to see this for other
> titles as well because each format has been cataloged with its own LCCN.
> You are also likely to see additional records with [online resource]
> identified as the format as more and more titles are available as
> ePrints or online.
> 
> I hope this helps answer your additional questions a bit more. Please
> reach out if you have any other questions.
> 
> Thank you,
> 
> Kerry Huller
> Newspaper & Current Periodical Reading Room
> Serial & Government Publications Division
> Library of Congress
> 
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jul 04 2022, 01:47pm via Email
> 
> Hi, Kelly:
> 
> 
> At the risk of bombing your inbox with more emails than you want,
> what is your relationship with Wikipedia and other Wikimedia Foundation
> projects like Wikidata?
> 
> 
> I ask, because I've logged over 20,000 edits in Wikimedia Foundation
> projects since 2010, and I would happily try to answer questions about
> Wikidata and other Wikimedia Foundation projects. I have NOT organized
> an edit-a-thon, but I've made presentations at conferences with people
> who have, and I would happily try to help organize such if you could
> find a group of people who want to work to improve this US Newspaper
> database. I think it would be good to establish links between this US
> Newspaper database and Wikidata, with appropriate procedures so changes
> to one could be evaluated for acceptance into the other.
> 
> 
> FYI, John Peter Zenger's famous "New-York weekly journal" (1733-1751)
> appears TWICE in your database with lccn = 2009252748 and sn83030211 and
> ONCE in Wikidata WITHOUT an lccn, even though many other Wikidata items
> have an lccn. See:
> 
> 
> https://www.wikidata.org/wiki/Q23091960
> 
> 
> There's a "WikiProject Newspapers" on Wikipedia and a companion
> "WikiProject Periodicals" on Wikidata:
> 
> 
> https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Newspapers/Wikidata
> 
> 
> https://www.wikidata.org/wiki/Wikidata:WikiProject_Periodicals
> 
> 
> I've tried to connect with others on those projects, so far with only
> limited success. However, you may know that almost anyone can change
> almost anything on Wikipedia and other Wikimedia Foundation projects.
> What stays tends to be written from a neutral point of view citing
> credible sources. They have problems with vandals, but the problems are
> usually easily controlled. This makes Wikipedia and Wikidata very
> useful platforms for cleaning up databases like your US Newspaper dataset.
> 
> 
> Spencer Graves
> 
> 
> ##########
> 
> 
> Hello, Kelly:
> 
> 
> In addition to the invalid JSON, discussed below [NOTE: The "below"
> contains a slight addition to the report of the I sent last Friday.], I
> found 9 (NINE!) cases where start_year was AFTER end_year. These have
> lccn = "sn86071531" "sn95069213" "sn90059096" "sn86058451" "sn90060926"
> "sn99065409" "sn89065002" "sn98069857" "sn91059179"
> 
> 
> See:
> 
> 
> https://chroniclingamerica.loc.gov/lccn/sn86071531/
> https://chroniclingamerica.loc.gov/lccn/sn95069213/
> https://chroniclingamerica.loc.gov/lccn/sn90059096/
> https://chroniclingamerica.loc.gov/lccn/sn86058451/
> https://chroniclingamerica.loc.gov/lccn/sn90060926/
> https://chroniclingamerica.loc.gov/lccn/sn99065409/
> https://chroniclingamerica.loc.gov/lccn/sn89065002/
> https://chroniclingamerica.loc.gov/lccn/sn98069857/
> https://chroniclingamerica.loc.gov/lccn/sn91059179/
> 
> 
> These all have obvious coding errors that can be easily fixed. The
> data may not be completely accurate after the fix, but at least they are
> not obviously wrong ;-)
> 
> 
> ##################
> 
> I got invalid JSON from:
> 
> 
> https://chroniclingamerica.loc.gov/search/titles/results/?rows=500&page=103&format=json
> 
> 
> After some experimentation, I was able to replicate the problem with
> a request for rows=10:
> 
> 
> https://chroniclingamerica.loc.gov/search/titles/results/?rows=10&page=5117&format=json
> 
> 
> Duncan Temple Lang <dtemplelang at ucdavis.edu>, Professor of Statistics
> and Associate Dean for Graduate Programs at the University of California
> - Davis, confirmed that it was a JSON error using:
> 
> 
> https://codebeautify.org/jsonvalidator
> 
> 
> He is part of the core team developing the R free, open-source
> programming language. He said, that starting at offsets 161070 and
> 161502 in the character string you get from [the R code RCurl::getURL()]
> we have:
> 
> 
> Santa Fe.\"
> 
> 
> and these are in an entry such as
> 
> 
> "city": ["Santa Fe.\"]
> 
> 
> So the final " is escaped and therefore there is no closing " for the
> string. The parser continues to consume characters looking for the end
> of that string.
> 
> 
> If one "repairs" the text from getURL() with
> 
> 
> ftxt= gsub('Santa Fe.\\\\"', 'Santa Fe."', txt)
> 
> 
> then the rest of my code worked fine.
> 
> 
> You may wish to do something to implement other checks for valid JSON
> and repair this problem. I've scanned all the 157520 records that were
> in that database a couple of days ago, and this is the only JSON error
> identified by the code I used.
> 
> 
> NOTE: I was NOT able to replicate this error when downloading records
> one at a time. That suggests a problem NOT in the database itself but
> in the download algorithm. ???
> 
> 
> Thank you for your help. I will almost certainly have other
> questions ;-)
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jul 03 2022, 10:39pm via Email
> 
> Hello, Kelly:
> 
> 
> In addition to the invalid JSON, discussed below [NOTE: The "below"
> contains a slight addition to the report of the I sent last Friday.], I
> found 9 (NINE!) cases where start_year was AFTER end_year. These have
> lccn = "sn86071531" "sn95069213" "sn90059096" "sn86058451" "sn90060926"
> "sn99065409" "sn89065002" "sn98069857" "sn91059179"
> 
> 
> See:
> 
> 
> https://chroniclingamerica.loc.gov/lccn/sn86071531/
> https://chroniclingamerica.loc.gov/lccn/sn95069213/
> https://chroniclingamerica.loc.gov/lccn/sn90059096/
> https://chroniclingamerica.loc.gov/lccn/sn86058451/
> https://chroniclingamerica.loc.gov/lccn/sn90060926/
> https://chroniclingamerica.loc.gov/lccn/sn99065409/
> https://chroniclingamerica.loc.gov/lccn/sn89065002/
> https://chroniclingamerica.loc.gov/lccn/sn98069857/
> https://chroniclingamerica.loc.gov/lccn/sn91059179/
> 
> 
> These all have obvious coding errors that can be easily fixed. The
> data may not be completely accurate after the fix, but at least they are
> not obviously wrong ;-)
> 
> 
> ##################
> 
> I got invalid JSON from:
> 
> 
> https://chroniclingamerica.loc.gov/search/titles/results/?rows=500&page=103&format=json
> 
> 
> After some experimentation, I was able to replicate the problem with
> a request for rows=10:
> 
> 
> https://chroniclingamerica.loc.gov/search/titles/results/?rows=10&page=5117&format=json
> 
> 
> Duncan Temple Lang <dtemplelang at ucdavis.edu>, Professor of Statistics
> and Associate Dean for Graduate Programs at the University of California
> - Davis, confirmed that it was a JSON error using:
> 
> 
> https://codebeautify.org/jsonvalidator
> 
> 
> He is part of the core team developing the R free, open-source
> programming language. He said, that starting at offsets 161070 and
> 161502 in the character string you get from [the R code RCurl::getURL()]
> we have:
> 
> 
> Santa Fe.\"
> 
> 
> and these are in an entry such as
> 
> 
> "city": ["Santa Fe.\"]
> 
> 
> So the final " is escaped and therefore there is no closing " for the
> string. The parser continues to consume characters looking for the end
> of that string.
> 
> 
> If one "repairs" the text from getURL() with
> 
> 
> ftxt= gsub('Santa Fe.\\\\"', 'Santa Fe."', txt)
> 
> 
> then the rest of my code worked fine.
> 
> 
> You may wish to do something to implement other checks for valid JSON
> and repair this problem. I've scanned all the 157520 records that were
> in that database a couple of days ago, and this is the only JSON error
> identified by the code I used.
> 
> 
> NOTE: I was NOT able to replicate this error when downloading records
> one at a time. That suggests a problem NOT in the database itself but
> in the download algorithm. ???
> 
> 
> Thank you for your help. I will almost certainly have other
> questions ;-)
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jul 01 2022, 11:46am via Email
> 
> Hello, Kelly:
> 
> 
> I got invalid JSON from:
> 
> 
> https://chroniclingamerica.loc.gov/search/titles/results/?rows=500&page=103&format=json
> 
> 
> After some experimentation, I was able to replicate the problem with
> a request for rows=10:
> 
> 
> https://chroniclingamerica.loc.gov/search/titles/results/?rows=10&page=5117&format=json
> 
> 
> Duncan Temple Lang <dtemplelang at ucdavis.edu>, Professor of Statistics
> and Associate Dean for Graduate Programs at the University of California
> - Davis, confirmed that it was a JSON error using:
> 
> 
> https://codebeautify.org/jsonvalidator
> 
> 
> He is part of the core team developing the R free, open-source
> programming language. He said, that starting at offsets 161070 and
> 161502 in the character string you get from [the R code RCurl::getURL()]
> we have:
> 
> 
> Santa Fe.\"
> 
> 
> and these are in an entry such as
> 
> 
> "city": ["Santa Fe.\"]
> 
> 
> So the final " is escaped and therefore there is no closing " for the
> string. The parser continues to consume characters looking for the end
> of that string.
> 
> 
> If one "repairs" the text from getURL() with
> 
> 
> ftxt= gsub('Santa Fe.\\\\"', 'Santa Fe."', txt)
> 
> 
> then the rest of my code worked fine.
> 
> 
> You may wish to do something to implement other checks for valid JSON
> and repair this problem. I've scanned all the 157520 records that were
> in that database a couple of days ago, and this is the only JSON error
> identified by the code I used.
> 
> 
> Thank you for your help. I will almost certainly have other
> questions ;-)
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jun 28 2022, 02:20pm via System
> 
> Hello Spencer,
> 
> Thank you for sending along your follow-up questions.
> 
> I'm glad to hear the json view will work for you. It was recommended to
> me that you limit your requests to 500 rows at a time. And a developer
> here at LC suggests the following regarding rate limiting:
> 
> ?To avoid being blocked by the server, the current rate-limiting rules
> restrict un-cached requests to URLs starting with
> https://chroniclingamerica.loc.gov/search/
> <https://chroniclingamerica.loc.gov/search/> to 120 requests every 10
> minutes from a single IP address.?
> 
> So, I think if you limited each of your requests to 500 rows at a time
> with the proper pauses, then you should be able to access what you need.
> 
> As for the csv view, I checked on this as well, and was informed that
> the csv view was not implemented for all url formats. The csv view was
> only implemented for this view:
> https://chroniclingamerica.loc.gov/newspapers/
> <https://chroniclingamerica.loc.gov/newspapers/>and urls resulting from
> US Directory search results - for e.g. if you wanted to narrow down your
> search results by state, city, date range, etc. found at this link:
> https://chroniclingamerica.loc.gov/search/titles/
> <https://chroniclingamerica.loc.gov/search/titles/>. So, if you wanted a
> csv and limited your search by state ( for example:
> https://chroniclingamerica.loc.gov/search/titles/results/?state=Alaska&county=&city=&year1=1690&year2=2022&terms=&frequency=&language=&ethnicity=&labor=&material_type=&lccn=&rows=20&format=csv
> <https://chroniclingamerica.loc.gov/search/titles/results/?state=Alaska&county=&city=&year1=1690&year2=2022&terms=&frequency=&language=&ethnicity=&labor=&material_type=&lccn=&rows=20&format=csv>
> ), you could append &format=csv to the search result url and get the csv
> to automatically download. But, if your search results ended up being
> over a couple thousand titles, then the system would probably time out.
> 
> I hope this info helps! Let me know if you have any other questions.
> 
> Best wishes,
> 
> Kerry Huller
> Newspaper & Current Periodical Reading Room
> Serial & Government Publications Division
> Library of Congress
> 
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jun 27 2022, 04:15pm via Email
> 
> Hello, Kerry:
> 
> 
> Thanks for the reply. Can you please give me some further guidance
> on two thing "so that the system is not overwhelmed"?
> 
> 
> 1. The max size in a small batch?
> 
> 
> 2. Any limit on the number of small batches in a second or minute?
> 
> 
> I've found that I can download small batches under program control
> using "RCurl::getURL" in R (programming language) using, e.g.;
> 
> 
> https://chroniclingamerica.loc.gov/search/titles/results/?rows=20&page=2&format=json
> 
> 
> With this, I can control the batch size with "row=20" vs. "row=50"
> vs., e.g., "row=1000". A naive search says there are 157520 "results".
> With "row=1000", this would require 158 calls. With "row=20", it
> would require 7876 calls. Before I start, I need to decide which fields
> I want; I don't need them all.
> 
> 
> Thanks,
> Spencer Graves
> 
> 
> p.s. I tried appending "&format=csv" and got "Error 504 Ray ID:
> 7220896da85e86e7 ? 2022-06-27 19:19:53 UTC Gateway time-out". I used:
> 
> 
> https://chroniclingamerica.loc.gov/search/titles/results/?state=&county=&city=&year1=1690&year2=2022&terms=&frequency=&language=&ethnicity=&labor=&material_type=&lccn=&rows=20&format=csv
> 
> 
> I can get what I want using json so do not need csv. However, I
> thought you might want to know that I was unable to get csv to work.
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jun 27 2022, 10:54am via System
> 
> Hello Spencer,
> 
> Thank you for contacting the Library of Congress about searching the US
> Newspaper Directory. I wanted to follow up with you regarding your
> request to output the data in a machine readable format.
> 
> It looks like you were provided the link to the API documentation for
> the website: About the Site and API
> <https://chroniclingamerica.loc.gov/about/api/>. Scroll down to the
> section with the heading, Searching the directory and newspaper pages
> using OpenSearch. This section describes the search functionality and
> structure for the US Newspaper Directory in more detail. It is possible
> to return your directory searches in json format by appending
> &format=json to the end of the url. It is also possible to return search
> results in csv format by appending &format=csv to the end of the url,
> but I would strongly suggest that you do this in small batches by
> putting limits on your search so that the system is not overwhelmed.
> 
> So, from the search page for the US Newspaper Directory
> <https://chroniclingamerica.loc.gov/search/titles/> you could
> potentially limit your search based on state and city, or date range,
> and/or even frequency. Then once you've completed the search, you can
> add &format=csv to the end of the url to automatically download a csv of
> those records. The resulting csv will contain several fields/headers:
> lccn, title, place of publication, start year, end year, publisher,
> edition, frequency, subject, state, city, country, language, oclc
> number, and holding type. I think these fields include the information
> you were looking for. But, again, I would like to stress that you put
> limits on your search before creating the csv so as not overwhelm the
> system.
> 
> Please let me know if you have any other additional questions.
> 
> Best wishes,
> 
> Kerry Huller
> Newspaper & Current Periodical Reading Room
> Serial & Government Publications Division
> Library of Congress
> 
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jun 23 2022, 01:55pm via System
> 
> Mr. Graves,
> 
> I'm going to transfer you request to a member of our digital collections
> team who may be of more assistance to you than me.
> 
> Mike
> 
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jun 23 2022, 01:51pm via Email
> 
> Dear Mr. Queen:
> 
> 
> Thanks for the reply. I'm still confused. I downloaded and
> installed Docker Desktop and "docker-compose.yml" and ran their "Getting
> Started" Tutorial, but I don't see what to do next.
> 
> 
> I repeat: I'd like to analyze "U.S. Newspaper Directory,
> 1690-Present" (https://chroniclingamerica.loc.gov/search/titles/), which
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jun 22 2022, 07:15pm via System
> 
> Mr. Graves,
> 
> Programmatic access to the data forChronicling America
> <https://chroniclingamerica.loc.gov/>and possibly the U.S. Newspaper
> Directory <https://chroniclingamerica.loc.gov/search/titles/>can be
> found on theAbout the Site and API
> <https://chroniclingamerica.loc.gov/about/api/>page in various formats.
> Also, please note that Chronicling Americacontains newspapers published
> from 1777-1963, but does not include everyU.S. newspaper published in
> that time period.
> 
> Please let me know if I can be of further assistance.
> 
> 
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jun 22 2022, 06:14pm via Email
> 
> Dear Mr. Queen:
> 
> 
> Can we simplify this to just giving me the data behind "U.S.
> Newspaper Directory, 1690-Present"
> (https://chroniclingamerica.loc.gov/search/titles/) in a machine
> readable format, e.g., csv or xlsx or a MySQL database?
> 
> 
> As I mentioned in my original email, a naive search of that without
> restrictions returned 157520 titles in 7876 pages with up to 20 titles
> per page giving date ranges in at least some cases. I could probably
> write software to scrape those 7876 pages from your web site and combine
> them into a data file.
> 
> 
> I have a PhD in statistics, I have been using the R programming
> language and similar software for decades. This includes publishing
> tutorials on how to analyze data like this on Wikiversity.[1] I'd like
> to do something similar with this. I could help make your data more
> useful to others and discuss with you how we might prioritize
> improvements like accessing the other sources you mentioned.
> 
> 
> Thanks very much for your reply.
> 
> 
> Sincerely,
> Spencer Graves, PhD
> Founder, EffectiveDefense.org
> 4550 Warwick Blvd 508
> Kansas City, MO 64111
> m: 408-655-4567
> 
> 
> [1] e.g.:
> 
> 
> https://en.wikiversity.org/wiki/US_Gross_Domestic_Product_(GDP)_per_capita
> ------------------------------------------------------------------------
> 
> Newspapers and Current Periodicals Reference Librarian
> 
> Jun 22 2022, 05:27pm via System
> 
> Mr. Graves
> 
> Your request is a little more complex than it first appears and requires
> extensive research. A variety of resources should be consulted to
> determine the circulation statistics of newspapers published prior to
> 1851. You will need to check newspaper union lists and newspaper
> histories. Union listspresent lists of newspapers in geographic
> arrangement according to place of publication, and specify which
> libraries or other institutions hold collections of those newspapers and
> the dates of their holdings. These can also be useful for tracking title
> changes throughout a newspaper's history. Newspaper
> historieslikeAmerican Journalism: A History: 1690-1960
> <https://lccn.loc.gov/62007157>(Mott),The Penny Press
> <https://lccn.loc.gov/2004043078>(Thompson), andThe Press and America
> <https://lccn.loc.gov/99044295>(Emery et al.) may not include
> circulation statistics, but they do document the diversity and progress
> of newspaper publishing, including notable newspapers of the era.
> Newspaper histories also cover the history of the printers and printing
> of newspapers in a state, county, or region more generally, and provide
> more condensed histories of the editors, journalists, and evolution of
> the newspapers in a specific area. Newspaper histories and union lists
> should be available at most large public or university libraries. More
> information about union lists, newspaper histories, and researching
> newspapers in general can be found in theU.S. Newspaper Collections at
> the Library of Congress
> <https://guides.loc.gov/united-states-newspapers/introduction>research
> guide (see Reference Sources).
> 
> Please let me know if I can be of further assistance.
> 
> ------------------------------------------------------------------------
> 
> Original Question
> 
> Jun 20 2022, 02:34pm via System
> 
> How can I get counts of the numbers of newspapers by year in the US, and
> preferably also elsewhere? A search of "U.S. Newspaper Directory,
> How can I get counts of the numbers of newspapers by year in the US, and
> preferably also elsewhere?
> 
> A search of "U.S. Newspaper Directory, 1690-Present"
> (https://chroniclingamerica.loc.gov/search/titles/) returned 157520
> titles in 7876 pages with up to 20 titles per page giving date ranges to
> the extent that it's known. If I can get a data file (e.g., csv or xls),
> I can summarize. I could also use data on circulation and frequency and
> especially parent company for multiple newspapers published by the same
> company, to the extant that such is available.
> 
> I'm interested in this, because McChesney quoted Tocqueville in
> suggesting that the US had more newspapers per person (or per million
> population) prior to 1851 than at any other time or place in history.
> I'd like to evaluate that claim with data to the extent that I can. See
> "https://en.wikiversity.org/wiki/Social_construction_of_crime_and_what_we_can_do_about_it#Newspapers_1790_-_present".
> 
> 
> Thanks, Spencer Graves, PhD
> m: 408-655-4567
> 
> ------------------------------------------------------------------------
> 
> Thank you for using Newspapers & Current Periodicals Ask a Librarian
> Service!
> 
> 
> This email is sent from Ask a Librarian in relationship to ticket #9625195.
> 
> Read our privacy policy. <https://springshare.com/privacy.html>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@turner @end|ng |rom @uck|@nd@@c@nz  Thu Jul 28 02:26:28 2022
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Thu, 28 Jul 2022 12:26:28 +1200
Subject: [R] Predicted values from glm() when linear predictor is NA.
Message-ID: <20220728122628.4381daa4@rolf-Latitude-E7470>


I have a data frame with a numeric ("TrtTime") and a categorical
("Lifestage") predictor.

Level "L1" of Lifestage occurs only with a single value of TrtTime,
explicitly 12, whence it is not possible to estimate a TrtTime "slope"
when Lifestage is "L1".

Indeed, when I fitted the model

    fit <- glm(cbind(Dead,Alive) ~ TrtTime*Lifestage, family=binomial,
               data=demoDat)

I got:

> as.matrix(coef(fit))
>                                   [,1]
> (Intercept)                -0.91718302
> TrtTime                     0.88846195
> LifestageEgg + L1         -45.36420974
> LifestageL1                14.27570572
> LifestageL1 + L2           -0.30332697
> LifestageL3                -3.58672631
> TrtTime:LifestageEgg + L1   8.10482459
> TrtTime:LifestageL1                 NA
> TrtTime:LifestageL1 + L2    0.05662651
> TrtTime:LifestageL3         1.66743472

That is, TrtTime:LifestageL1 is NA, as expected.

I would have thought that fitted or predicted values corresponding to
Lifestage = "L1" would thereby be NA, but this is not the case:

> predict(fit)[demoDat$Lifestage=="L1"]
>       26       65      131 
> 24.02007 24.02007 24.02007
>
> fitted(fit)[demoDat$Lifestage=="L1"]
>  26  65 131 
>   1   1   1

That is, the predicted values on the scale of the linear predictor are
large and positive, rather than being NA.

What this amounts to, it seems to me, is saying that if the linear
predictor in a Binomial glm is NA, then "success" is a certainty.
This strikes me as being a dubious proposition.  My gut feeling is that
misleading results could be produced.

Can anyone explain to me a rationale for this behaviour pattern?
Is there some justification for it that I am not currently seeing?
Any other comments?  (Please omit comments to the effect of "You are as
thick as two short planks!". :-) )

I have attached the example data set in a file "demoDat.txt", should
anyone want to experiment with it.  The file was created using dput() so
you should access it (if you wish to do so) via something like

    demoDat <- dget("demoDat.txt")

Thanks for any enlightenment.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: demoDat.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20220728/dd771e34/attachment.txt>

From @pro @end|ng |rom un|me|b@edu@@u  Thu Jul 28 02:41:52 2022
From: @pro @end|ng |rom un|me|b@edu@@u (Andrew Robinson)
Date: Thu, 28 Jul 2022 00:41:52 +0000
Subject: [R] 
 [EXT] Predicted values from glm() when linear predictor is NA.
In-Reply-To: <20220728122628.4381daa4@rolf-Latitude-E7470>
References: <20220728122628.4381daa4@rolf-Latitude-E7470>
Message-ID: <da655ff3-6539-4468-a963-60b8a16ae5c7@Spark>

Hi Rolf,

that's an interesting observation.

I agree that it is counter-intuitive that the fitted values / predictions are not NA.

However, I don't agree with your comment that <<if the linear predictor in a Binomial glm is NA, then "success" is a certainty>> - that seems to be a peculiarity of these data - note for these three observations there are thousands dead and zero alive and that you get similar outcomes if you omit TrtTime altogether ...

> predict(glm(cbind(Dead,Alive) ~ Lifestage, family=binomial,data=demoDat))[demoDat$Lifestage=="L1"]
 26 65 131
20.02007 20.02007 20.02007
>

Cheers,

Andrew

--
Andrew Robinson
Chief Executive Officer, CEBRA and Professor of Biosecurity,
School/s of BioSciences and Mathematics & Statistics
University of Melbourne, VIC 3010 Australia
Tel: (+61) 0403 138 955
Email: apro at unimelb.edu.au
Website: https://researchers.ms.unimelb.edu.au/~apro at unimelb/

I acknowledge the Traditional Owners of the land I inhabit, and pay my respects to their Elders.
On 28 Jul 2022, 10:27 AM +1000, Rolf Turner <r.turner at auckland.ac.nz>, wrote:
External email: Please exercise caution


I have a data frame with a numeric ("TrtTime") and a categorical
("Lifestage") predictor.

Level "L1" of Lifestage occurs only with a single value of TrtTime,
explicitly 12, whence it is not possible to estimate a TrtTime "slope"
when Lifestage is "L1".

Indeed, when I fitted the model

fit <- glm(cbind(Dead,Alive) ~ TrtTime*Lifestage, family=binomial,
data=demoDat)

I got:

as.matrix(coef(fit))
[,1]
(Intercept) -0.91718302
TrtTime 0.88846195
LifestageEgg + L1 -45.36420974
LifestageL1 14.27570572
LifestageL1 + L2 -0.30332697
LifestageL3 -3.58672631
TrtTime:LifestageEgg + L1 8.10482459
TrtTime:LifestageL1 NA
TrtTime:LifestageL1 + L2 0.05662651
TrtTime:LifestageL3 1.66743472

That is, TrtTime:LifestageL1 is NA, as expected.

I would have thought that fitted or predicted values corresponding to
Lifestage = "L1" would thereby be NA, but this is not the case:

predict(fit)[demoDat$Lifestage=="L1"]
26 65 131
24.02007 24.02007 24.02007

fitted(fit)[demoDat$Lifestage=="L1"]
26 65 131
1 1 1

That is, the predicted values on the scale of the linear predictor are
large and positive, rather than being NA.

What this amounts to, it seems to me, is saying that if the linear
predictor in a Binomial glm is NA, then "success" is a certainty.
This strikes me as being a dubious proposition. My gut feeling is that
misleading results could be produced.

Can anyone explain to me a rationale for this behaviour pattern?
Is there some justification for it that I am not currently seeing?
Any other comments? (Please omit comments to the effect of "You are as
thick as two short planks!". :-) )

I have attached the example data set in a file "demoDat.txt", should
anyone want to experiment with it. The file was created using dput() so
you should access it (if you wish to do so) via something like

demoDat <- dget("demoDat.txt")

Thanks for any enlightenment.

cheers,

Rolf Turner

--
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Thu Jul 28 02:42:51 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Thu, 28 Jul 2022 00:42:51 +0000
Subject: [R] Predicted values from glm() when linear predictor is NA.
In-Reply-To: <20220728122628.4381daa4@rolf-Latitude-E7470>
References: <20220728122628.4381daa4@rolf-Latitude-E7470>
Message-ID: <BN6PR2201MB15535C8743A9AB024C36EFDDCF969@BN6PR2201MB1553.namprd22.prod.outlook.com>

Time is often used in this sort of problem, but really time is not relevant. A better choice is accumulated thermal units. The insect will molt when X thermal units have been accumulated. This is often expressed as degree days, but could as easily be other units like degree seconds. However, I suspect that fine time units exceeds the accuracy of the measurement system. A growth chamber might maintain 28 C, but the temperature the insect experiences might be somewhat different thereby introducing additional variability in the outcome. No thermal units accumulated, no development, so that is not an issue.
    This approach allows one to predict life stage over a large temperature range. Accuracy can be improved if one knows the lower development threshold. At high temperatures development stops, and a mortality function can be added.

Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Rolf Turner
Sent: Wednesday, July 27, 2022 8:26 PM
To: r-help <r-help at r-project.org>
Subject: [R] Predicted values from glm() when linear predictor is NA.

[External Email]

I have a data frame with a numeric ("TrtTime") and a categorical
("Lifestage") predictor.

Level "L1" of Lifestage occurs only with a single value of TrtTime, explicitly 12, whence it is not possible to estimate a TrtTime "slope"
when Lifestage is "L1".

Indeed, when I fitted the model

    fit <- glm(cbind(Dead,Alive) ~ TrtTime*Lifestage, family=binomial,
               data=demoDat)

I got:

> as.matrix(coef(fit))
>                                   [,1]
> (Intercept)                -0.91718302
> TrtTime                     0.88846195
> LifestageEgg + L1         -45.36420974
> LifestageL1                14.27570572
> LifestageL1 + L2           -0.30332697
> LifestageL3                -3.58672631
> TrtTime:LifestageEgg + L1   8.10482459
> TrtTime:LifestageL1                 NA
> TrtTime:LifestageL1 + L2    0.05662651
> TrtTime:LifestageL3         1.66743472

That is, TrtTime:LifestageL1 is NA, as expected.

I would have thought that fitted or predicted values corresponding to Lifestage = "L1" would thereby be NA, but this is not the case:

> predict(fit)[demoDat$Lifestage=="L1"]
>       26       65      131
> 24.02007 24.02007 24.02007
>
> fitted(fit)[demoDat$Lifestage=="L1"]
>  26  65 131
>   1   1   1

That is, the predicted values on the scale of the linear predictor are large and positive, rather than being NA.

What this amounts to, it seems to me, is saying that if the linear predictor in a Binomial glm is NA, then "success" is a certainty.
This strikes me as being a dubious proposition.  My gut feeling is that misleading results could be produced.

Can anyone explain to me a rationale for this behaviour pattern?
Is there some justification for it that I am not currently seeing?
Any other comments?  (Please omit comments to the effect of "You are as thick as two short planks!". :-) )

I have attached the example data set in a file "demoDat.txt", should anyone want to experiment with it.  The file was created using dput() so you should access it (if you wish to do so) via something like

    demoDat <- dget("demoDat.txt")

Thanks for any enlightenment.

cheers,

Rolf Turner

--
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From dw|n@em|u@ @end|ng |rom comc@@t@net  Thu Jul 28 03:25:23 2022
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Wed, 27 Jul 2022 18:25:23 -0700
Subject: [R] Predicted values from glm() when linear predictor is NA.
In-Reply-To: <20220728122628.4381daa4@rolf-Latitude-E7470>
References: <20220728122628.4381daa4@rolf-Latitude-E7470>
Message-ID: <5371d080-d666-f059-c6f8-e3051a0faea2@comcast.net>


On 7/27/22 17:26, Rolf Turner wrote:
> I have a data frame with a numeric ("TrtTime") and a categorical
> ("Lifestage") predictor.
>
> Level "L1" of Lifestage occurs only with a single value of TrtTime,
> explicitly 12, whence it is not possible to estimate a TrtTime "slope"
> when Lifestage is "L1".
>
> Indeed, when I fitted the model
>
>      fit <- glm(cbind(Dead,Alive) ~ TrtTime*Lifestage, family=binomial,
>                 data=demoDat)
>
> I got:
>
>> as.matrix(coef(fit))
>>                                    [,1]
>> (Intercept)                -0.91718302
>> TrtTime                     0.88846195
>> LifestageEgg + L1         -45.36420974
>> LifestageL1                14.27570572
>> LifestageL1 + L2           -0.30332697
>> LifestageL3                -3.58672631
>> TrtTime:LifestageEgg + L1   8.10482459
>> TrtTime:LifestageL1                 NA
>> TrtTime:LifestageL1 + L2    0.05662651
>> TrtTime:LifestageL3         1.66743472
> That is, TrtTime:LifestageL1 is NA, as expected.
>
> I would have thought that fitted or predicted values corresponding to
> Lifestage = "L1" would thereby be NA, but this is not the case:
>
>> predict(fit)[demoDat$Lifestage=="L1"]
>>        26       65      131
>> 24.02007 24.02007 24.02007
>>
>> fitted(fit)[demoDat$Lifestage=="L1"]
>>   26  65 131
>>    1   1   1
> That is, the predicted values on the scale of the linear predictor are
> large and positive, rather than being NA.
>
> What this amounts to, it seems to me, is saying that if the linear
> predictor in a Binomial glm is NA, then "success" is a certainty.
> This strikes me as being a dubious proposition.  My gut feeling is that
> misleading results could be produced.

The NA is most likely caused by aliasing, so some other combination of 
factors a perfect surrogate for every case with that level of the 
interaction. The `predict.glm` function always requires a complete set 
of values to construct a case. Whether apparent incremental linear 
prediction of that interaction term is large or small will depend on the 
degree of independent contribution of the surrogate levels of other 
variables..


David.

>
> Can anyone explain to me a rationale for this behaviour pattern?
> Is there some justification for it that I am not currently seeing?
> Any other comments?  (Please omit comments to the effect of "You are as
> thick as two short planks!". :-) )
>
> I have attached the example data set in a file "demoDat.txt", should
> anyone want to experiment with it.  The file was created using dput() so
> you should access it (if you wish to do so) via something like
>
>      demoDat <- dget("demoDat.txt")
>
> Thanks for any enlightenment.
>
> cheers,
>
> Rolf Turner
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j|ox @end|ng |rom mcm@@ter@c@  Thu Jul 28 04:04:20 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Wed, 27 Jul 2022 22:04:20 -0400
Subject: [R] Predicted values from glm() when linear predictor is NA.
In-Reply-To: <775_1658968044_26S0RNlm003716_20220728122628.4381daa4@rolf-Latitude-E7470>
References: <775_1658968044_26S0RNlm003716_20220728122628.4381daa4@rolf-Latitude-E7470>
Message-ID: <dcc28626-39f9-3f02-9d83-38d711b16bd2@mcmaster.ca>

Dear Rolf,

The coefficient of TrtTime:LifestageL1 isn't estimable (as you explain) 
and by setting it to NA, glm() effectively removes it from the model. An 
equivalent model is therefore

 > fit2 <- glm(cbind(Dead,Alive) ~ TrtTime + Lifestage +
+               I((Lifestage == "Egg + L1")*TrtTime) +
+               I((Lifestage == "L1 + L2")*TrtTime) +
+               I((Lifestage == "L3")*TrtTime),
+             family=binomial, data=demoDat)
Warning message:
glm.fit: fitted probabilities numerically 0 or 1 occurred

 > cbind(coef(fit, complete=FALSE), coef(fit2))
                                   [,1]         [,2]
(Intercept)                -0.91718302  -0.91718302
TrtTime                     0.88846195   0.88846195
LifestageEgg + L1         -45.36420974 -45.36420974
LifestageL1                14.27570572  14.27570572
LifestageL1 + L2           -0.30332697  -0.30332697
LifestageL3                -3.58672631  -3.58672631
TrtTime:LifestageEgg + L1   8.10482459   8.10482459
TrtTime:LifestageL1 + L2    0.05662651   0.05662651
TrtTime:LifestageL3         1.66743472   1.66743472

There is no problem computing fitted values for the model, specified 
either way. That the fitted values when Lifestage == "L1" all round to 1 
on the probability scale is coincidental -- that is, a consequence of 
the data.

I hope this helps,
  John

On 2022-07-27 8:26 p.m., Rolf Turner wrote:
> 
> I have a data frame with a numeric ("TrtTime") and a categorical
> ("Lifestage") predictor.
> 
> Level "L1" of Lifestage occurs only with a single value of TrtTime,
> explicitly 12, whence it is not possible to estimate a TrtTime "slope"
> when Lifestage is "L1".
> 
> Indeed, when I fitted the model
> 
>      fit <- glm(cbind(Dead,Alive) ~ TrtTime*Lifestage, family=binomial,
>                 data=demoDat)
> 
> I got:
> 
>> as.matrix(coef(fit))
>>                                    [,1]
>> (Intercept)                -0.91718302
>> TrtTime                     0.88846195
>> LifestageEgg + L1         -45.36420974
>> LifestageL1                14.27570572
>> LifestageL1 + L2           -0.30332697
>> LifestageL3                -3.58672631
>> TrtTime:LifestageEgg + L1   8.10482459
>> TrtTime:LifestageL1                 NA
>> TrtTime:LifestageL1 + L2    0.05662651
>> TrtTime:LifestageL3         1.66743472
> 
> That is, TrtTime:LifestageL1 is NA, as expected.
> 
> I would have thought that fitted or predicted values corresponding to
> Lifestage = "L1" would thereby be NA, but this is not the case:
> 
>> predict(fit)[demoDat$Lifestage=="L1"]
>>        26       65      131
>> 24.02007 24.02007 24.02007
>>
>> fitted(fit)[demoDat$Lifestage=="L1"]
>>   26  65 131
>>    1   1   1
> 
> That is, the predicted values on the scale of the linear predictor are
> large and positive, rather than being NA.
> 
> What this amounts to, it seems to me, is saying that if the linear
> predictor in a Binomial glm is NA, then "success" is a certainty.
> This strikes me as being a dubious proposition.  My gut feeling is that
> misleading results could be produced.
> 
> Can anyone explain to me a rationale for this behaviour pattern?
> Is there some justification for it that I am not currently seeing?
> Any other comments?  (Please omit comments to the effect of "You are as
> thick as two short planks!". :-) )
> 
> I have attached the example data set in a file "demoDat.txt", should
> anyone want to experiment with it.  The file was created using dput() so
> you should access it (if you wish to do so) via something like
> 
>      demoDat <- dget("demoDat.txt")
> 
> Thanks for any enlightenment.
> 
> cheers,
> 
> Rolf Turner
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/


From r@turner @end|ng |rom @uck|@nd@@c@nz  Thu Jul 28 04:09:30 2022
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Thu, 28 Jul 2022 14:09:30 +1200
Subject: [R] Predicted values from glm() when linear predictor is NA.
In-Reply-To: <BN6PR2201MB15535C8743A9AB024C36EFDDCF969@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <20220728122628.4381daa4@rolf-Latitude-E7470>
 <BN6PR2201MB15535C8743A9AB024C36EFDDCF969@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <20220728140930.62bf139a@rolf-Latitude-E7470>


On Thu, 28 Jul 2022 00:42:51 +0000
"Ebert,Timothy Aaron" <tebert at ufl.edu> wrote:

> Time is often used in this sort of problem, but really time is not
> relevant. A better choice is accumulated thermal units. The insect
> will molt when X thermal units have been accumulated. This is often
> expressed as degree days, but could as easily be other units like
> degree seconds. However, I suspect that fine time units exceeds the
> accuracy of the measurement system. A growth chamber might maintain
> 28 C, but the temperature the insect experiences might be somewhat
> different thereby introducing additional variability in the outcome.
> No thermal units accumulated, no development, so that is not an
> issue. This approach allows one to predict life stage over a large
> temperature range. Accuracy can be improved if one knows the lower
> development threshold. At high temperatures development stops, and a
> mortality function can be added.

Very cogent comments in respect of dealing with the underlying
practical problem, but I am not so much concerned with the practical
problem at the moment but rather with the workings of the software that
I am using.

cheers,

Rolf

P.S.  I am at several removes from the data set(s) that I am messing
about with.  But if my understanding is correct (always an assumption
of which to be sceptical!) these data were collected with the
temperature being held *constant*, whence time and accumulated thermal
units would be equivalent.  Is it not so?

R.

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r@turner @end|ng |rom @uck|@nd@@c@nz  Thu Jul 28 03:58:44 2022
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Thu, 28 Jul 2022 13:58:44 +1200
Subject: [R] 
 [EXT] Predicted values from glm() when linear predictor is NA.
In-Reply-To: <da655ff3-6539-4468-a963-60b8a16ae5c7@Spark>
References: <20220728122628.4381daa4@rolf-Latitude-E7470>
 <da655ff3-6539-4468-a963-60b8a16ae5c7@Spark>
Message-ID: <20220728135844.5c9cd27e@rolf-Latitude-E7470>

On Thu, 28 Jul 2022 00:41:52 +0000
Andrew Robinson <apro at unimelb.edu.au> wrote:

> Hi Rolf,
> 
> that's an interesting observation.
> 
> I agree that it is counter-intuitive that the fitted values /
> predictions are not NA.

That gives me some comfort!

Does anyone from R-core feel like commenting?

> However, I don't agree with your comment that <<if the linear
> predictor in a Binomial glm is NA, then "success" is a certainty>> -
> that seems to be a peculiarity of these data - note for these three
> observations there are thousands dead and zero alive and that you get
> similar outcomes if you omit TrtTime altogether ...
> 
> > predict(glm(cbind(Dead,Alive) ~ Lifestage,
> > family=binomial,data=demoDat))[demoDat$Lifestage=="L1"]
>  26 65 131
> 20.02007 20.02007 20.02007

Hmm.  Yeah.  I was probably jumping the gun a bit there.  I'll think on
this a bit more.

This issue arose in a more complicated context (which I won't try to go
into) in which I really need some insight into how to deal with missing
values of the linear predictor, and what the resulting fitted values
actually mean.  Psigh!

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r@turner @end|ng |rom @uck|@nd@@c@nz  Thu Jul 28 04:19:49 2022
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Thu, 28 Jul 2022 14:19:49 +1200
Subject: [R] Predicted values from glm() when linear predictor is NA.
In-Reply-To: <5371d080-d666-f059-c6f8-e3051a0faea2@comcast.net>
References: <20220728122628.4381daa4@rolf-Latitude-E7470>
 <5371d080-d666-f059-c6f8-e3051a0faea2@comcast.net>
Message-ID: <20220728141949.53afcb46@rolf-Latitude-E7470>


On Wed, 27 Jul 2022 18:25:23 -0700
David Winsemius <dwinsemius at comcast.net> wrote:

<SNIP>

> The NA is most likely caused by aliasing, so some other combination
> of factors a perfect surrogate for every case with that level of the 
> interaction. 

<SNIP>

No, I think it's much simpler than that.  Essentially it boils down to
the fact that if y is 1:10 and x is rep(1,10) then

    lm(y ~ x)

gives a slope estimate of NA.  As it clearly should.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Jul 28 07:31:14 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 27 Jul 2022 22:31:14 -0700
Subject: [R] Predicted values from glm() when linear predictor is NA.
In-Reply-To: <dcc28626-39f9-3f02-9d83-38d711b16bd2@mcmaster.ca>
References: <775_1658968044_26S0RNlm003716_20220728122628.4381daa4@rolf-Latitude-E7470>
 <dcc28626-39f9-3f02-9d83-38d711b16bd2@mcmaster.ca>
Message-ID: <3A27E228-1C74-4753-BE6F-37074FF54143@dcn.davis.ca.us>

But "disappearing" is not what NA is supposed to do normally. Why is it being treated that way here?

On July 27, 2022 7:04:20 PM PDT, John Fox <jfox at mcmaster.ca> wrote:
>Dear Rolf,
>
>The coefficient of TrtTime:LifestageL1 isn't estimable (as you explain) and by setting it to NA, glm() effectively removes it from the model. An equivalent model is therefore
>
>> fit2 <- glm(cbind(Dead,Alive) ~ TrtTime + Lifestage +
>+               I((Lifestage == "Egg + L1")*TrtTime) +
>+               I((Lifestage == "L1 + L2")*TrtTime) +
>+               I((Lifestage == "L3")*TrtTime),
>+             family=binomial, data=demoDat)
>Warning message:
>glm.fit: fitted probabilities numerically 0 or 1 occurred
>
>> cbind(coef(fit, complete=FALSE), coef(fit2))
>                                  [,1]         [,2]
>(Intercept)                -0.91718302  -0.91718302
>TrtTime                     0.88846195   0.88846195
>LifestageEgg + L1         -45.36420974 -45.36420974
>LifestageL1                14.27570572  14.27570572
>LifestageL1 + L2           -0.30332697  -0.30332697
>LifestageL3                -3.58672631  -3.58672631
>TrtTime:LifestageEgg + L1   8.10482459   8.10482459
>TrtTime:LifestageL1 + L2    0.05662651   0.05662651
>TrtTime:LifestageL3         1.66743472   1.66743472
>
>There is no problem computing fitted values for the model, specified either way. That the fitted values when Lifestage == "L1" all round to 1 on the probability scale is coincidental -- that is, a consequence of the data.
>
>I hope this helps,
> John
>
>On 2022-07-27 8:26 p.m., Rolf Turner wrote:
>> 
>> I have a data frame with a numeric ("TrtTime") and a categorical
>> ("Lifestage") predictor.
>> 
>> Level "L1" of Lifestage occurs only with a single value of TrtTime,
>> explicitly 12, whence it is not possible to estimate a TrtTime "slope"
>> when Lifestage is "L1".
>> 
>> Indeed, when I fitted the model
>> 
>>      fit <- glm(cbind(Dead,Alive) ~ TrtTime*Lifestage, family=binomial,
>>                 data=demoDat)
>> 
>> I got:
>> 
>>> as.matrix(coef(fit))
>>>                                    [,1]
>>> (Intercept)                -0.91718302
>>> TrtTime                     0.88846195
>>> LifestageEgg + L1         -45.36420974
>>> LifestageL1                14.27570572
>>> LifestageL1 + L2           -0.30332697
>>> LifestageL3                -3.58672631
>>> TrtTime:LifestageEgg + L1   8.10482459
>>> TrtTime:LifestageL1                 NA
>>> TrtTime:LifestageL1 + L2    0.05662651
>>> TrtTime:LifestageL3         1.66743472
>> 
>> That is, TrtTime:LifestageL1 is NA, as expected.
>> 
>> I would have thought that fitted or predicted values corresponding to
>> Lifestage = "L1" would thereby be NA, but this is not the case:
>> 
>>> predict(fit)[demoDat$Lifestage=="L1"]
>>>        26       65      131
>>> 24.02007 24.02007 24.02007
>>> 
>>> fitted(fit)[demoDat$Lifestage=="L1"]
>>>   26  65 131
>>>    1   1   1
>> 
>> That is, the predicted values on the scale of the linear predictor are
>> large and positive, rather than being NA.
>> 
>> What this amounts to, it seems to me, is saying that if the linear
>> predictor in a Binomial glm is NA, then "success" is a certainty.
>> This strikes me as being a dubious proposition.  My gut feeling is that
>> misleading results could be produced.
>> 
>> Can anyone explain to me a rationale for this behaviour pattern?
>> Is there some justification for it that I am not currently seeing?
>> Any other comments?  (Please omit comments to the effect of "You are as
>> thick as two short planks!". :-) )
>> 
>> I have attached the example data set in a file "demoDat.txt", should
>> anyone want to experiment with it.  The file was created using dput() so
>> you should access it (if you wish to do so) via something like
>> 
>>      demoDat <- dget("demoDat.txt")
>> 
>> Thanks for any enlightenment.
>> 
>> cheers,
>> 
>> Rolf Turner
>> 
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From r@oknz @end|ng |rom gm@||@com  Thu Jul 28 08:41:34 2022
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Thu, 28 Jul 2022 18:41:34 +1200
Subject: [R] Need to insert various rows of data from a data frame after
 particular rows from another dataframe
In-Reply-To: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
References: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
Message-ID: <CABcYAd+277N-VnvopkbDw9qdgGAK0isdjUKp8VoR-nWy3ptidA@mail.gmail.com>

I'm retired, and I had an hour on my hands while tea cooked and my
granddaughter did her homework, and I just *love* showing off how helpful I
am.

Good news: someone finally looked at your data.
(That would be me.)

Bad news: it's going to be a lot of work to do what you want to, and YOU
SHOULDN'T EVEN TRY because it won't make any sense, as we would all have
known at once had you been clear about the structure of your data in the
beginning.

You have two files.
dacnet_yield_update till 2019.csv
is a straightforward "data" file with structure

  crop factor(arhar,bajra,cotton,gram,maize,moong,mustard,
              potato,rice,soyabean,sugarcane,urad,wheat)
  season factor(kharif,rabi),
  state.id integer(1201..1235),
  state.name factor, # 34 states
  district.id integer(15001..15648),
  district.name factor,
  year integer(1998..2017),
  yield decimal(0.001 .. 314.736, precision=3)

The one problem is that the file name is misleading.  It says "till 2019",
but includes no data for 2019 or 2018.

Ah, but the other file!  That's not a "data" file intended for machine use
at all.  It's a "display" file intended for human beings to look at and go
"wow, gosh, lookit them numbahs".  It's the kind of thing that gets
included as an appendix in an official report which seems as if perversely
designed to impede the development of insight as much as possible.

Amongst other difficulties:
- the same column contains state names, district names, crop names, and
assorted junk;
- state names are not coded the same way in the two files;
- district names are in UPPERCASE in the .xls files and have numbers
prefixed to them for no apparent reason;
- crop names are not coded the same way in the two files;
- yields are not coded the same way in the two files (3 digit precision in
one, 2 digit precision in the other) and I have some doubt as to whether
they are measuring the same thing;
- above all, years appear to be CALENDAR years in the .csv file (e.g.,
2017) but FINANCIAL years in the .xsl file (e.g., 2018-19)

Now I could wrangle the .xls file into something closer to the .csv file
easily enough.  I'd do it by converting the .xsl to .csv, then writing a
script in AWK.  *BUT* my uncertainty that "yield" means the same thing in
the two files and my certainty that "year" does NOT mean the same thing
make it unrewarding to do so.

The .xls file is the end product of some process that derived it from data
better structured for computation.  It seems like a better use of your time
to go and look for the original data.

It also seems like a good use of your time to make certain you know what
the fields of the .csv file actually mean.  ARE those calendar yields, or
just part of a financial year?  Why are only the yields of interest and not
the area planted?  How are the yields computed?





On Wed, 27 Jul 2022 at 14:31, Ranjeet Kumar Jha <ranjeetjhaiitkgp at gmail.com>
wrote:

> Hello Everyone,
>
> I have dataset in a particular format in "dacnet_yield_update till
> 2019.xlsx" file, where I need to insert the data of rows 2018-2019 and
> 2019-2020 for the districts those data are available in "Kharif crops
> yield_18-19.xlsx".  I need to insert these two rows of data belonging to
> every district, if data is available in a later excel file, just after the
> particular crop group data for the particular district.
>
> I have put the data file in the given link.
>
> https://drive.google.com/drive/u/0/folders/1dNmGTI8_c9PK1QqmfIjnpbyzuiCXgxFC
>
> Please help solving this problem.
>
> Regards and Thanks,
> Ranjeet
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r@oknz @end|ng |rom gm@||@com  Thu Jul 28 08:50:47 2022
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Thu, 28 Jul 2022 18:50:47 +1200
Subject: [R] Parsing XML?
In-Reply-To: <7743581c-0959-1df6-bb31-f140161f83e1@effectivedefense.org>
References: <20220727145903.4338c449692f703b@ask.loc.gov>
 <7743581c-0959-1df6-bb31-f140161f83e1@effectivedefense.org>
Message-ID: <CABcYAdLxyWZp5QGH_-3NNZ4zRu3Dp-qF1NK399VD-Rej=Fg-bg@mail.gmail.com>

What do you mean by "a list that I can understand"?
A quick tally of the number of XML elements by identifier:
1 echoedSearchRetrieveRequest
1 frbrGrouping
1 maximumRecords
1 nextRecordPosition
1 numberOfRecords
1 query
1 records
1 resultSetIdleTime
1 searchRetrieveResponse
1 servicelevel
1 sortKeys
1 startRecord
1 wskey
2 version
50 leader
50 recordData
51 recordPacking
51 recordSchema
100 record
105 controlfield
923 datafield
1900 subfield

What of this information do you actually want?
The elements of the list should be what?


On Thu, 28 Jul 2022 at 08:52, Spencer Graves <
spencer.graves at effectivedefense.org> wrote:

> Hello, All:
>
>
>           What would you suggest I do to parse the following XML file into
> a
> list that I can understand:
>
>
> XMLfile <-
> "
> https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/ndnp_Alabama_all-yrs_e_0001_0050.xml"
>
>
>
>
>           This is the first of 6666 XML files containing "U.S. Newspaper
> Directory" maintained by the US Library of Congress discussed in the
> thread below.  I've tried various things using the XML and xml2.
>
>
> XMLdata <- xml2::read_xml(XMLfile)
> str(XMLdata)
> XMLdat <- XML::xmlParse(XMLdata)
> str(XMLdat)
> XMLtxt <- xml2::xml_text(XMLdata)
> nchar(XMLtxt)
> #[1] 29415
>
>
>           Someplace there's a schema for this.  I don't know if it's
> embedded
> in this XML file or in a separate file.  If it's in a separate file, how
> could I describe it to my contacts with the Library of Congress so they
> would understand what I needed and could help me get it.
>
>
>           Thanks,
>           Spencer Graves
>
>
> p.s.  All 29415 characters in XMLtext appear in the thread below.
>
>
>
> -------- Forwarded Message --------
> Subject:        [Newspapers and Current Periodicals] How can I get counts
> of
> the numbers of newspapers by year in the US, and preferably also
> elsewhere? A search of "U.S. Newspaper Directory,
> Date:   Wed, 27 Jul 2022 14:59:03 +0000
> From:   Kerry Huller <serials at ask.loc.gov>
> To:     Spencer Graves <spencer.graves at effectivedefense.org>
> CC:     twes at loc.gov
>
>
>
> --# Type your reply above this line #--
>
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jul 27 2022, 10:59am via System
>
> Hello Spencer,
>
> So, when I view the xml, I'm actually looking at it in XML editor
> software, so I can view the tags and it's structured neatly. I've copied
> and pasted the text from the beginning of the file and the first
> newspaper title below from my XML editor:
>
> <?xml version="1.0" encoding="UTF-8" standalone="no"?>
> <?xml-stylesheet type='text/xsl'
> href='/webservices/catalog/xsl/searchRetrieveResponse.xsl'?>
>
> <searchRetrieveResponse xmlns="http://www.loc.gov/zing/srw/"
> xmlns:oclcterms="http://purl.org/oclc/terms/"
> xmlns:dc="http://purl.org/dc/elements/1.1/"
> xmlns:diag="http://www.loc.gov/zing/srw/diagnostic/"
> xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
> <version>1.1</version>
> <numberOfRecords>2250</numberOfRecords>
> <records>
> <record>
> <recordSchema>info:srw/schema/1/marcxml</recordSchema>
> <recordPacking>xml</recordPacking>
> <recordData>
> <record xmlns="http://www.loc.gov/MARC21/slim">
>       <leader>00000nas a22000007i 4500</leader>
>       <controlfield tag="001">1030438981</controlfield>
>       <controlfield tag="008">180404c20159999aluwr n       0   a0eng
>   </controlfield>
>       <datafield ind1=" " ind2=" " tag="010">
>         <subfield code="a">  2018200464</subfield>
>       </datafield>
>       <datafield ind1=" " ind2=" " tag="040">
>         <subfield code="a">DLC</subfield>
>         <subfield code="e">rda</subfield>
>         <subfield code="c">DLC</subfield>
>         <subfield code="b">eng</subfield>
>       </datafield>
>       <datafield ind1=" " ind2=" " tag="012">
>         <subfield code="m">1</subfield>
>       </datafield>
>       <datafield ind1="0" ind2=" " tag="022">
>         <subfield code="a">2577-5316</subfield>
>         <subfield code="2">1</subfield>
>       </datafield>
>       <datafield ind1=" " ind2=" " tag="032">
>         <subfield code="a">021110</subfield>
>         <subfield code="b">USPS</subfield>
>       </datafield>
>       <datafield ind1=" " ind2=" " tag="037">
>         <subfield code="b">711 Alabama Avenue, Selma, AL 36701</subfield>
>       </datafield>
>       <datafield ind1=" " ind2=" " tag="042">
>         <subfield code="a">nsdp</subfield>
>         <subfield code="a">pcc</subfield>
>       </datafield>
>       <datafield ind1="1" ind2="0" tag="050">
>         <subfield code="a">ISSN RECORD</subfield>
>       </datafield>
>       <datafield ind1="1" ind2="0" tag="082">
>         <subfield code="a">071</subfield>
>         <subfield code="2">15</subfield>
>       </datafield>
>       <datafield ind1=" " ind2="0" tag="222">
>         <subfield code="a">Selma sun</subfield>
>       </datafield>
>       <datafield ind1="0" ind2="0" tag="245">
>         <subfield code="a">Selma sun.</subfield>
>       </datafield>
>       <datafield ind1=" " ind2="1" tag="264">
>         <subfield code="a">Selma, AL :</subfield>
>         <subfield code="b">North Shore Press, LLC</subfield>
>         <subfield code="c">2016-</subfield>
>       </datafield>
>       <datafield ind1=" " ind2=" " tag="310">
>         <subfield code="a">Weekly</subfield>
>       </datafield>
>       <datafield ind1=" " ind2=" " tag="336">
>         <subfield code="a">text</subfield>
>         <subfield code="b">txt</subfield>
>         <subfield code="2">rdacontent</subfield>
>       </datafield>
>       <datafield ind1=" " ind2=" " tag="337">
>         <subfield code="a">unmediated</subfield>
>         <subfield code="b">n</subfield>
>         <subfield code="2">rdamedia</subfield>
>       </datafield>
>       <datafield ind1=" " ind2=" " tag="338">
>         <subfield code="a">volume</subfield>
>         <subfield code="b">nc</subfield>
>         <subfield code="2">rdacarrier</subfield>
>       </datafield>
>       <datafield ind1="1" ind2=" " tag="362">
>         <subfield code="a">Began in 2015.</subfield>
>       </datafield>
>       <datafield ind1=" " ind2=" " tag="588">
>         <subfield code="a">Description based on: Volume 2, Issue 40
> (October 5, 2017) (surrogate); title from caption.</subfield>
>       </datafield>
>       <datafield ind1=" " ind2=" " tag="588">
>         <subfield code="a">Latest issue consulted: Volume 2, Issue 40
> (October 5, 2017).</subfield>
>       </datafield>
>       <datafield ind1=" " ind2=" " tag="752">
>         <subfield code="a">United States</subfield>
>         <subfield code="b">Alabama</subfield>
>         <subfield code="c">Dallas</subfield>
>         <subfield code="d">Selma.</subfield>
>       </datafield>
>     </record>
> </recordData>
> </record>
>
> When I view the records in the XML editor, these 2 lines below do begin
> each of the records for each individual title, but of course this is
> including the xml tags:
>
> <recordSchema>info:srw/schema/1/marcxml</recordSchema>
> <recordPacking>xml</recordPacking>
>
> Hopefully this helps you decide where to break or parse each record.
>
> On another note, I just noticed as well that at the top of this first
> file it lists the total number of records for the Alabama grouping -
> 2250. This also appeared to be the case for the Alaska records when I
> took a look at the first one for that state. I imagine that should be
> consistent throughout each "grouping" of records.
>
> Let me know if you have follow-up questions!
>
> Best wishes,
>
> Kerry Huller
> Newspaper & Current Periodical Reading Room
> Serial & Government Publications Division
> Library of Congress
>
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jul 27 2022, 10:21am via Email
>
> Hi, Kerry:
>
>
> Thanks. I understand the chunking in files of at most 50. I've read
> the first file "ndnp_Alabama_all-yrs_e_0001_0050.xml" into a string of
> 29415 characters, copied below. Might you have any suggestions on the
> next step in parsing this? Staring at it now, it looks splitting on
> "info:srw/schema/1/marcxmlxml" might convert the 29415 characters into
> shorter chunks, each of which could then be parsed further.
>
>
> This is not as bad as reading ancient Egyptian heiroglyphics without
> the Rosetta Stone, but I wondered if you might have something that could
> make this work easier and more reliable? I guess I could compare with
> what I already read as JSON ;-)
>
>
> Thanks,
> Spencer Graves
>
>
> "1.12250info:srw/schema/1/marcxmlxml00000nas a22000007i
> 45001030438981180404c20159999aluwr n 0 a0eng
> 2018200464DLCrdaDLCeng12577-53161021110USPS711 Alabama Avenue, Selma, AL
> 36701nsdppccISSN RECORD07115Selma sunSelma sun.Selma, AL :North Shore
> Press,
> LLC2016-WeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan
> in
> 2015.Description based on: Volume 2, Issue 40 (October 5, 2017)
> (surrogate); title from caption.Latest issue consulted: Volume 2, Issue
> 40 (October 5, 2017).United
> StatesAlabamaDallasSelma.info:srw/schema/1/marcxmlxml00000cas a22000007a
> 4500502150053100127c20109999aluwr n 0 a0eng
> 2010200019DLCengDLCDLCOCLCQ112153-18111750USPSB & C Publishing, LLC,
> 3514 Martin St. S. Ste 104, Cropwell, AL 35054pccnsdpISSN RECORDSt.
> Clair County news (Cropwell, Ala.)St. Clair County news(Cropwell,
> Ala.)St. Clair County news.Cropwell, AL :B & C Pub.WeeklyBegan in
> 2010.Description based on: Nov. 4, 2010 (surrogate); title from
> caption.info:srw/schema/1/marcxmlxml00000cas a22000007a
> 4500426491872090720c20099999alumr n 0 a0eng
> 2009203372DLCengDLCOCLCQ12150-346X2150-346X1AU at 000044489617NZ116076352Devon
> Applewhite/Applewhite Publishing Co., 1910 Honeysuckle Rd., #N183,
> Dothan, AL 36305mscnsdpISSN RECORD30514Triangle tribune(Dothan,
> Ala.)Triangle tribune.Dothan, AL :Applewhite Pub. CoMonthlyBegan with
> vol. 1, issue 1 (May 2009).\"Connecting the Tri-State African -American
> Community.\"Description based on: Vol. 1, issue 1 (May 2009); title from
> masthead.Applewhite, Devon.United StatesAlabama.United
> StatesGeorgia.United StatesFlorida.info:srw/schema/1/marcxmlxml00000cas
> a22000007a 4500289017315081219c20089999aluwr n | a0eng c
> 2008213218NSDengNSDOCLCQDLCOCLCQ111945-93191945-93191005270USPSSpringhill
> Publications,
> LLC, P.O. Box 186, Greenville, AL 36037nsdppccISSN RECORD07014Greenville
> standardThe Greenville standard.Greenville, AL :Springhill
> PublicationsWeeklytexttxtrdacontentunmediatednrdamediaBegan with vol. 1,
> issue 1 (Sept. 3, 2008)Description based on surrogate of: Vol. 1, no. 15
> (Dec. 18, 2008); title from masthead (publisher's Web site, viewed Dec.
> 19, 2008).Latest issue consulted: Vol. 1, no. 99 (July 27, 2011)
> (surrogate).info:srw/schema/1/marcxmlxml00000cas a22000007a
> 4500123539969070426c20079999aluwr ne 0 a0eng c
> 2007212138NSDengNSDNSDOCLCQ101936-95571936-95571The Western Tribune,
> 1530 Third Ave. N., Bessemer, AL 35020mscnsdpISSN RECORDWestern tribune
> (Bessemer, Ala.)The Western tribune(Bessemer, Ala.)The Western
> tribune.Bessemer, Ala. :D-Med, Inc.v.WeeklyBegan in 2007.Description
> based on: May 23, 2007 (surrogate); title from
> caption.AU at 000041575341info:srw/schema/1/marcxmlxml00000cas a22000007a
> 4500226300653080425c20079999aluwr ne | a0eng
> 2008212112NSDengNSDNSDOCLCQ11942-20751942-20751nsdppccISSN RECORDThe
> corridor messengerThe corridor messenger.Carbon Hill, AL :Corridor
> Messenger, Inc.WeeklyBegan with vol. 1, issue (10.03.2007).Description
> based on: 1st issue.United StatesAlabamaWalkerCarbon
> Hill.http://www.corridormessenger.cominfo:srw/schema/1/marcxmlxml00000cas
> a22000007a
> 450077560432070109c20069999aluwr ne 0 a0eng c
>
> 2007213400NSDengNSDOCLCQAUBRNOCLCOOCLCFa01935-37901935-37901AU at 000041190283The
>
> Auburn Villager, P.O. Box 1633, Auburn, AL 36831-1633pccnsdpISSN
> RECORDThe Auburn villagerThe Auburn villager.Auburn, AL :Auburn
> Villagerv.WeeklyBegan in 2006.Description based on: Vol. 1, no. 4 (July
> 20, 2006) (surrogate); title from caption.Auburn (Ala.)Newspapers.Lee
> County (Ala.)Newspapers.AlabamaAuburn.fast(OCoLC)fst01209634AlabamaLee
> County.fast(OCoLC)fst01211930Newspapers.fast(OCoLC)fst01423814United
> StatesAlabamaLeeAuburn.info:srw/schema/1/marcxmlxml00000cas a2200000Ii
> 4500872286785m o d s cr mn|---a||||140311c20069999alucr n o b
> s0 a0eng cABCengrdaABCABCOCLCFLD59.13University of Alabama at
> Birmingham.The eReporter.[Birmingham, Alabama] :The University of
> Alabama at Birmingham,[2006]-[Birmingham, Alabama] :Offices of Public
> Relations & Marketing and Information Technology1 online resource2
> issues weeklytexttxtrdacontentcomputercrdamediaonline
> resourcecrrdacarrierSeptember 19, 2006-\"The eReporter is an official
> communication of The University of Alabama at Birmingham, companion to
> the UAB Reporter and recommended alternative to mass e-mails.\"Issues
> for <March 11, 2014- published and distributed via e-mail subscription
> on Tuesdays and Fridays.Description based on: September 19, 2006; title
> from title screen (viewed March 12, 2014).University of Alabama at
> BirminghamPeriodicals.Periodicals.fast(OCoLC)fst01411641University of
> Alabama at Birmingham.fast(OCoLC)fst00645114University of Alabama at
> Birmingham.Office of Public Relations and Marketing.University of
> Alabama at Birmingham.Information Technology.2006-2012, companion
> to:University of Alabama at Birmingham.UAB
> reporter.(OCoLC)32435748Archived
> issueshttp://
> hatteras.dpo.uab.edu/cgi-bin/ereporter.cgiinfo:srw/schema/1/marcxmlxml00000cas
>
> a22000007a 4500166387050070829c20059999aluwr ne | a0eng c
> 2007215501NSDengNSDOCLCQ11939-68991939-68991The Wilkie Clark Memorial
> Foundation, P.O. Box 514, Roanoke, AL 36274$30.00nsdpmscISSN
> RECORD305.89614People's voice (Roanoke, Ala.)The people's voice(Roanoke,
> Ala.)The people's voice.Roanoke, AL :Wilkie Clark Memorial
> Foundationv.WeeklyBegan with vol. 1, no. 1 in 2005.Description based on:
> Vol. 2, no. 20 (Apr. 20, 2007); title from caption.Wilkie Clark Memorial
> Foundation.United
> StatesAlabamaRandolphRoanoke.AU at 000042141390info:srw/schema/1/marcxmlxml00000nas
>
>
> a22000007i 45001124677787191021c20uu9999aluwr ne | a0eng
> 2019202521DLCengrdaDLC12689-3258122730USPSNorth Jackson Press, 42950 Hwy
> 72, Suite 406, Stevenson, AL 35772nsdppccISSN RECORD071.323North Jackson
> pressNorth Jackson press.Stevenson, AL :Caney Creek Publications
> LLCWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierDescription
> based on surrogate of: Volume 1, number 36 (October 11, 2019); title
> from masthead.Latest issue consulted: Volume 1, number 36 (October 11,
> 2019) (Surrogate).United
> StatesAlabamaJacksonStevensoninfo:srw/schema/1/marcxmlxml00000cas
> a2200000 a 4500226315099080428d19981998aluwr ne | 0eng c
> 2008233691GUAengGUAOCLCQOCLCFOCLCO39911644pccn-us-gaThe Dekalb
> news.Birmingham, Ala. :Community newspaper holdings Inc.v.WeeklyBegan
> with 1st year, no. 1 (Apr. 1, 1998); ceased with 1st year, no. 31 (Oct.
> 28, 1998).Final issue consulted.Description based on first issue; title
> from caption.Decatur (Ga.)Newspapers.DeKalb County
> (Ga.)Newspapers.Newspapers.fast(OCoLC)fst01423814GeorgiaDecatur.fast(OCoLC)fst01226234GeorgiaDeKalb
>
>
> County.fast(OCoLC)fst01215288United
> StatesGeorgiaDeKalbDecatur.Decatur-DeKalb news/era(DLC)sn
> 89053661(OCoLC)19946163info:srw/schema/1/marcxmlxml00000cas a2200000 i
> 450050263311m o d cr cn|||||||||020730c19979999alu x neo
> 0 a0eng c
>
> 2015238492AMHengrdapnAMHOCLCQOCLCFOCLCOIULOCLHTMOCLCQCOODLC66460694810970435082687-93791AU at 000050711528OCLCS45109pccnsdpn-us---AP2.B5707023Birmingham
>
> weekly (Online)Birmingham weekly(Online)Birmingham weekly.Birmingham, AL
> :Birmingham Weekly1 online resourceIrregular,Feb. 16-28,
> 2012-Weekly,Sept. 4-11, 1997-Feb. 9-16,
> 2012texttxtrdacontentcomputercrdamediaonline resourcecrrdacarrierBegan
> with vol. 1, issue 1 (Sept. 4-11, 1997).\"City news, views &
> entertainment\"--Cover.Numbering dropped in Mar. 2012.Also issued in
> print.Description based on: Publication information from ProQuest; title
> from web page (viewed June 18, 2015).Latest issue consulted: Aug. 15-20,
> 2012.Birmingham (Ala.)Newspapers.Internet resources.Electronic
> journals.AlabamaBirmingham.fast(OCoLC)fst01204958Newspapers.fast(OCoLC)fst01423814United
>
>
> StatesAlabamaBirmingham.Print version:Birmingham
> Weekly(OCoLC)39271050
> http://apw.softlineweb.com/http://WC2VB5MT8E.search.serialssolutions.com/?sid=sersol&SS_jc=JC_000051895&title=Birmingham+Weeklyinfo:srw/schema/1/marcxmlxml00000cas
>
> a22000007a 450031471314941116d19941995aluwr ne 0 a0eng csn
> 94003083
> NSDengNSDANEOCLCQOCLCFOCLCOOCLCQ11079-65411079-65411nsdppccn-us-akSoutheast
> shopperSoutheast shopper.Juneau, Alaska :Kemper
> Communications,1994-volumesWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol.
>
>
> 1, no. 1 (Nov. 16, 1994)-Ceased in Feb. 1995.Juneau
> (Alaska)Newspapers.AlaskaJuneau.fast(OCoLC)fst01213587Newspapers.fast(OCoLC)fst01423814United
>
>
> StatesAlaskaJuneau.AU at 000011356572info:srw/schema/1/marcxmlxml00000cas
> a22000008a 450027910515930413c19949999alumr n 0 a0eng dsn
> 93002581 NSDengNSDOCLCQ11069-06621Birmingham Tribune, 216 Ave. T. Pratt
> City, Birmingham, AL 35214nsdpBirmingham tribuneBirmingham
> tribune.Birmingham, Ala. :Kervin
> Fondren9501volumesMonthlytexttxtrdacontentunmediatednrdamediavolumencrdacarrierPREPUB:
>
>
> publication expected Jan.
> 1995AU at 000025863987info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450026199931920716d19922013alumr ne 0 a0eng csn 92003357
> NSDengNSDOCLOCLCQDLC011064-01341064-01341Black & White, POB 13215,
> Birmingham, AL 35202-3215nsdppccBlack & white (Birmingham, Ala.)Black &
> white(Birmingham, Ala.)Black & white.Black and whiteBirmingham, Ala.
> :Black & White, Inc.v.Biweekly,Oct. 2, 1997-Monthly,May 1, 1992-Sept.
> 1997Began in May 1992; ceased with Jan. 10, 2013.\"Birmingham's New City
> paper.\"Description based on: June 1992.Latest issue consulted: No. 67
> (Oct. 16, 1997) (surrogate).info:srw/schema/1/marcxmlxml00000cas
> a2200000 a 450032145723950314d19901999alumr ne 0 a0eng csn
> 95068755
>
> MGNengMGNNSDCLUOCLCQOCLCFOCLCOOCLCA971211082-34841082-34841AU at 000011579542nsdppccn-us-alF335.J5S68The
>
> Southern shofarThe Southern shofar.Birmingham, AL :L. Brook,-[1999]v.
> :ill. ;35 cm.MonthlyBegan in 1990.-v. 9, issue 9 (Aug./Sept. 1999).\"The
> monthly newspaper of Alabama's Jewish community.\"Some issues also
> available on the Internet via the World Wide Web.Description based on:
> Vol. 3, issue 11 (Oct. 1993).Jewish newspapersAlabama.Jewish
> newspapers.fast(OCoLC)fst00982872Alabama.fast(OCoLC)fst01204694United
> StatesAlabamaJeffersonBirmingham.Deep South Jewish voice(DLC)sn
> 99018499(OCoLC)42431704CLUhttp://
> bibpurl.oclc.org/web/719http://www.bham.net/shofar/info:srw/schema/1/marcxmlxml00000cas
>
> a22000007a 450021265141900326c19909999aluwr ne 0 a0eng csn
> 90099004 AARengAARCPNNSDOCLCQ11050-08981050-08981005022USPSE.O.N., Inc.,
> Main St., Eclectic, AL 36024pccnsdpISSN RECORDThe Eclectic observerThe
> Eclectic observer.Eclectic, Ala. :E.O.N., Inc.,1990-v.WeeklyVol. 1, no.
> 1 (Feb. 22, 1990)-Published by: Price Publications, Inc., <2006->Latest
> issue consulted: Vol. 17, no. 1 (Jan. 5, 2006).United
> StatesAlabamaElmoreEclectic.AU at 000040212446info:srw/schema/1/marcxmlxml00000cas
>
>
> a22000007a 450021214781900314c19909999aluir ne 0 a0eng csn
> 90002457 AAAengAAANSDOCLCQ111050-20841050-20841931180USPSClanton
> Newspapers, 1109 Seventh St., N., PO Box 1379, Clanton, AL
> 35045nsdppccn-us-alThe Clanton advertiserThe Clanton
> advertiser.AdvertiserClanton, Ala. :Clanton Newspapersv. :ill. ;58
> cm.Three no. a week,<May 13, 1992->Semiweekly,<Apr. 4, 1990->Began in
> Jan. 1990.Description based on: Vol. 19, no. 27 (Wed., Apr. 4,
> 1990).Latest issue consulted: Vol. 22, no. 58 (May 13, 1992).United
> StatesAlabamaChiltonClanton.Independent advertiser (Clanton,
> Ala.)(OCoLC)21214732AU at 000025908452info:srw/schema/1/marcxmlxml00000cas
> a2200000 a 450021214814900314c19909999aluwr ne 0 a0eng dsn
> 90099009 AAAengAAACPNNSDOCLCQ11056-32881056-32881505740USPSThe Blount
> Countian, 3rd St. at Washington Ave., PO Box 310, Oneonta, AL
> 35121mscnsdpn-us-alThe Blount countianThe Blount countian.Oneonta, Ala.
> :Southern Democrat, Inc.,1990-v. :ill.WeeklyVol. 1, no. 1 (Jan. 3,
> 1990)-Editor: Molly Howard Ryan, 1990-Latest issue consulted: Vol. 1,
> no. 36 (Sept. 5, 1990).Ryan, Molly Howard.United
> StatesAlabamaBlountOneonta.Southern Democrat(DLC)sn
> 85044741(OCoLC)12038577AU at 000025884049info:srw/schema/1/marcxmlxml00000cas
> a22000007a 450022413044900920c19909999aluwr ne 0 a0eng dsn
> 90099011
> AARengAARCPNNSDNSTOCLCQ92081707011191053-91231053-91231314240USPSmscnsdpThe
> Clay times-journalThe Clay times-journal.Lineville, Ala. :C.L.
> Proctor,1990-v.WeeklyVol. 1, no. 1 (Sept. 6, 1990)-United
> StatesAlabamaClayLineville.Ashland progress(DLC)sn 85044701Lineville
> tribune(DLC)sn 85044702AUinfo:srw/schema/1/marcxmlxml00000cas a22000007a
> 450021265218900326c19909999aluwr ne 0 0eng dsn 90099005
> AARengAARCPNOCLCQmscTrussville news-journal.Trussville, Ala. :Mike
> Mitchell,1990-v.BimonthlyVol. 1, no. 1 (Feb. 20, 1990)-United
> StatesAlabamaJeffersonTrussville.info:srw/schema/1/marcxmlxml00000cas
> a22000007a 450022301035900831c19909999aluwr ne 0 0eng dsn
> 90099010 AARengAARCPNOCLCQmscWeaver tribune.Oxford, Ala. :Cheaha
> Pub.,1990-v.WeeklyVol. 1, no. 1 (July 19, 1990)-United
> StatesAlabamaCalhounWeaver.United
> StatesAlabamaCalhounOxford.info:srw/schema/1/marcxmlxml00000cas
> a22000007a 450015155895870205c19879999aludr ne 0 a0eng csn
> 87050045
>
> AAAengAAACPNNSDDLCCPNNSDDLCCPNDLCOCLDLCOCLCQOCLCFOCLCQ19261126829944596670892-44570892-44571AU at 000020456714360980USPSThe
>
> Advertiser, P.O. Box 1000, Montgomery, AL
> 36192pccnsdpn-us-alNewspaperMontgomery advertiser (Montgomery, Ala. :
> 1987)The Montgomery advertiser(1987)The Montgomery advertiser.Montgomery
> advertiser & the Alabama journalSunday Montgomery advertiserMontgomery,
> Ala. :Advertiser Co.,1987-volumes
> :illustrationsDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrier160th
>
>
> year, no. 1 (Jan. 2, 1987)-On Saturdays, Sundays and holidays a combined
> edition is published with the Alabama journal, and called: Montgomery
> advertiser and the Alabama journal, Jan. 3, 1987, and: Alabama journal
> and Montgomery advertiser, Jan. 4, 1987-Feb. 25, 1990.Issues for Sunday
> called: Sunday Montgomery advertiser, Mar. 4, 1990-Issues for Saturday,
> Sunday and holidays have their own numbering, Jan. 3, 1987-Feb. 25,
> 1990.Montgomery
> (Ala.)Newspapers.AlabamaMontgomery.fast(OCoLC)fst01202689Newspapers.fast(OCoLC)fst01423814United
>
>
> StatesAlabamaMontgomeryMontgomery.Advertiser (Montgomery,
> Ala.)0745-3221(DLC)sn 82008412(OCoLC)9049482Alabama journal (Montgomery,
> Ala. : 1940)0745-323X(DLC)sn
> 87062018(OCoLC)2666111info:srw/schema/1/marcxmlxml00000cas a2200000 a
> 450016942287871105c19879999aludn ne 0 a0eng dsn 88050149
> AAAengAAACPNNSDOCLCQy1044-00701044-0070746--32780746-32781565580USPSTroy
> Publications, Inc., 113 North Market St., Troy, AL 36081mscnsdpMessenger
> (Troy, Ala.)The Messenger(Troy, Ala.)The Messenger.Troy, Ala. :Troy
> Pub.,1987-v.Daily (Sunday, Tuesday, Thursday and Friday)Vol. 121, no.
> 166 (July 1, 1987)-Sunday, Apr. 2, 1989 misprinted as v. 113.Latest
> issue consulted: Vol. 113 [sic 123], no. 96 (Sunday, Apr. 2,
> 1989).United StatesAlabamaPikeTroy.Troy messenger0746-3278(DLC)sn
> 83009935(OCoLC)9921908info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450017799786880415c19879999aluir ne 0 a0eng dsn 88050086
> AARengAARCPNNSDOCLCQ1p1044-03801044-03800745-75961441520USPSThe
> Prattville Progress, 152 W. 3rd St., Prattville, AL
> 36067mscnsdpPrattville progress (Prattville, Ala. : 1987)The Prattville
> progress(Prattville, Ala.)The Prattville progress.Prattville, Ala.
> :James C. Seymour,1987-v.Three times a weekVol. 102, no. 8 (Jan. 20,
> 1987)-Latest issue consulted: Vol. 105, no. 153 (Wednesday, Dec. 26,
> 1990).United StatesAlabamaAutaugaPrattville.Progress (Prattville,
> Ala.)0745-7596(DLC)sn
> 83007623(OCoLC)9428489info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450015344667870319c19869999aluwr ne 0 a0eng dsn 87000284
> NSDengNSDCPNOCLCQy0893-07670893-07671431800USPSPickens County Herald,
> P.O. Drawer E, Carrollton, AL 35447nsdpPickens County heraldPickens
> County herald.Pickens County herald and west AlabamianCarrollton, Ala.
> :Pickens Newspapers, Inc.,1986-WeeklyVol. 138, no. 40 (Oct. 2,
> 1986)-United StatesAlabamaPickensCarrollton.Pickens County herald and
> west Alabamian0746-0473(DLC)sn
> 83008141AU at 000040635809info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450018917586881217c19869999aluwr ne 0 0eng dsn 88050225
> CPNengCPNOCLCQmscThe Oxford sun/times.Oxford, Ala.
> :[s.n.],1986-v.WeeklyVol. 1, no. 1 (Jan. 16, 1986)-Editor: Andy
> Goggans.Numbering is irregular.United StatesAlabamaCalhounOxford.Oxford
> sun (Oxford, Ala.)(DLC)sn
> 85045023AU at 000025803813info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450013991168860731c19869999aluwr ne 0 0eng dsn 86050322
> CPNengCPNOCLCQmscIndependent (Brewton, Ala.)The Independent.Brewton,
> Ala. :Jim Thornton,1986-v. :ill. ;58 cm.WeeklyVol. 1, no. 1 (June 19,
> 1986)-United
> StatesAlabamaEscambiaBrewton.info:srw/schema/1/marcxmlxml00000cas
> a22000007a 450018957493881231c19859999aluwr ne 0 0eng dsn
> 88050247 CPNengCPNOCLCQmscPiedmont journal-independent (Piedmont,
> Ala.)The Piedmont journal-independent.Journal independentPiedmont, Ala.
> :Lane Weatherbee,1985-v.WeeklyVol. 4, no. 52 (Dec. 24, 1985)-Sometimes
> published as: Journal independent.United
> StatesAlabamaCalhounPiedmont.Journal-independent(DLC)sn
> 85045014info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450012715821851024d19841985aluwr ne 0 a0eng dsn 85045014
> CPNengCPNNSDCPNOCLCQmscThe Journal-independent.Piedmont, Ala.
> :Journal-Independent, Inc.,1984-1985.volumes :illustrations ;58
> cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 3,
> no. 27 (July 3, 1984)- v. 4, no. 51 (Dec. 18, 1985).Carries the same
> vol. numbering as the Piedmont journal-independent.United
> StatesAlabamaCalhounPiedmont.Piedmont
> journal-independent0890-6017(DLC)sn 85045013Piedmont journal-independent
> (Piedmont, Ala.)(DLC)sn 88050247info:srw/schema/1/marcxmlxml00000cas
> a22000007a 450012691448851018c19839999aludr ne 0 0eng dsn
> 85045007 CPNengCPNOCLCQmscTimesDaily.Times dailyFlorence, Ala. :T.S.P.
> Newspapers, Inc.,1983-volumes :illustrations ;58
> cmDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 114,
> no. 226 (Aug. 14, 1983)-United StatesAlabamaLauderdaleFlorence.Florence
> times + tri-cities daily(DLC)sn
> 85044995info:srw/schema/1/marcxmlxml00000cas a22000007a
> 45009428489830420d19831987aluir ne 0 a0eng dsn 83007623
> NSDengNSDCPNNSDNSTOCLCQ89090d0745-75960745-75961The Progress, 152 W. 3rd
> St., Prattville, AL 36067nsdpmscProgress (Prattville, Ala.)The
> Progress(Prattville, Ala.)The Progress.Prattville, Ala. :The Prattville
> Progress,1983-1987.volumes :illustrations ;58 cmThree times a
> weektexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 98, no.
> 32 (Mar. 17, 1983)-v. 102, no. 7 (Jan. 17, 1987).United
> StatesAlabamaAutaugaPrattville.Prattville progress(DLC)sn
> 85044740Prattville progress (Prattville, Ala.)1044-0380(DLC)sn
> 88050086(OCoLC)12254317AAPinfo:srw/schema/1/marcxmlxml00000cas a2200000
> a 45009867255830831c19839999aludr ne 0 a0eng dsn 84008052
> AAAengAAANSDOCLOCLCQX0743-15110743-15111617760USPST.S.P. Newspapers,
> Inc., 219 W. Tennessee St., Florence, AL 35630nsdpTimesDaily (Shoals
> edition)TimesDaily(Shoals ed.)TimesDaily.Times dailyShoals ed.Florence,
> Ala. :T.S.P. Newspapersvolumes
> :illustrationsDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan
>
>
> with: Vol. 114, no. 226 (Aug. 14,
> 1983).\"Florence/Sheffield/Tuscumbia/Muscle Shoals.\"Shoals ed. and
> Regional ed. combined on Sundays.Description based on: Vol. 114, no. 346
> (Monday, Dec. 12, 1983).United
> StatesAlabamaLauderdaleFlorence.TimesDaily (Regional
> edition)0743-152XTimes Tri-cities dailyUnknownDec. 12,
> 1983info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450010536023840319c19839999aludr ne 0 a0eng dsn 84008051
> NSDengNSDOCLCQ1x0743-152X0743-152X1617760USPST.S.P. Newspapers, Inc.,
> 219 W. Tennessee St., Florence, AL 35630nsdpTimesDaily (Regional
> edition)TimesDaily(Regional ed.)TimesDaily.Times dailyRegional
> ed.Florence, Ala. :T.S.P.
> NewspapersDailytexttxtrdacontentunmediatednrdamediaBegan with: Vol. 114,
> no. 226 (Aug. 14, 1983).Shoals ed. and Regional ed. combined on
> Sundays.Description based on: Vol. 114, no. 346 (Monday, Dec. 12,
> 1983).United StatesAlabamaLauderdaleFlorence.TimesDaily (Shoals
> edition)0743-1511Times Tri-cities dailyDec. 12,
> 1983AU at 000025818125info:srw/schema/1/marcxmlxml00000cas a22000007a
> 45009049482821213d19821987aludn ne 0 a0eng csn 82008412
> AAAengAAANSDNPWCPNDLCCPNNSDDLCNSDDLCCPNNVFDLCOCLCQCRLOCLCFOCLCQ1d0745-32210745-32211nsdppccn-us-alNewspaperAdvertiser
>
>
> (Montgomery, Ala.)The Advertiser(Montgomery, Ala.)The advertiser.Alabama
> journal and advertiserMontgomery, Ala. :Advertiser Co.,1982-1987.volumes
> :illustrationsDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrier155th
>
>
> year, no. 232 (Nov. 22, 1982)- ; -v. 14-3, Jan. 1, 1987.On Saturdays,
> Sundays and holidays published as: The Alabama journal and advertiser,
> Nov. 27, 1982-Jan. 1, 1987.Saturday, Sunday and holiday issues have
> their own numbering.Montgomery
> (Ala.)Newspapers.AlabamaMontgomery.fast(OCoLC)fst01202689Newspapers.fast(OCoLC)fst01423814United
>
>
> StatesAlabamaMontgomeryMontgomery.Montgomery advertiser (Montgomery,
> Ala. : Daily)(DLC)sn 84020645(OCoLC)2685433Montgomery advertiser
> (Montgomery, Ala. : 1987)0892-4457(DLC)sn
> 87050045(OCoLC)15155895AU at 000020281746info:srw/schema/1/marcxmlxml00000cas
> a2200000 a 45009237931830218c19829999aluwr ne 0 0eng dsn
> 86050139 AAAengAAACPNOCLOCLCQmscThe Randolph leader.Roanoke, Ala. :David
> S. Stevenson,1982-volumes :illustrations ;58
> cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 91,
> no. 1 (Oct. 6, 1982)-United StatesAlabamaRandolphRoanoke.Roanoke
> leader(DLC)sn 86050137Randolph press(DLC)sn
> 86050138info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450012715815851024d19821984aluwr ne 0 a0eng dsn 85045013
> CPNengCPNNSDCPNOCLCQ110890-60170890-60171432080USPSThe Piedmont
> Journal-Independent, 115 N. Center Ave., Piedmont, AL 36272mscnsdpThe
> Piedmont journal-independentThe Piedmont journal-independent.Piedmont,
> Ala. :Piedmont Journal-Independent, Inc.,1982-1984.volumes
> :illustrations ;58
> cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 1,
> no. 1 (Mar. 31, 1982)-v. 3, no. 26 (June 27, 1984).Latest issue
> consulted: Vol. 5, no. 31 (August 20, 1986).United
> StatesAlabamaCalhounPiedmont.Piedmont journal(DLC)sn
> 85045012Journal-independent(DLC)sn
> 85045014(OCoLC)12715821AU at 000045312916info:srw/schema/1/marcxmlxml00000cas
> a22000007a 45009183905830202c19829999aluwr n 0 a0eng dsn
> 85044580 AAAengAAACPNNSDOCLOCLCQ11098-58671098-58671016409USPSNo. 4,
> Rucker Plaza, Enterprise, AL 36331P.O. Box 1536, Enterprise, AL
> 36331mscnsdpSoutheast sun (Enterprise, Ala.)The southeast
> sun(Enterprise, Ala.)The Southeast sun.Enterprise, Ala. :QST
> Publicationsvolumes :illustrations ;58
> cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan in
> 1982.Description based on: Vol. 1, no. 25 (Oct. 21, 1982).Latest issue
> consulted: Vol. 16, no. 43 (Mar. 4, 1998).United
> StatesAlabamaCoffeeEnterprise.AU at 000025827687info:srw/schema/1/marcxmlxml00000cas
>
>
> a22000007a 450010487314840305c19819999aluwr ne 0 a0eng dsn
> 85044906
> AAAengAAACPNNSDNSTCPNOCLOCLCQOCLCFOCLCOOCLCAOCLCQ900410885-16620885-16621749310USPSThe
>
>
> New Times, 1618 1/2 St. Stephens Rd., Mobile, AL 36603mscnsdpn-us-alNew
> times (Mobile, Ala.)The New times(Mobile, Ala.)The new times.Mobile,
> Ala. :New Times Groupvolumes
> :illustrationsWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan
>
>
> in 1981.Vol. 3, no. 49 (Dec. 15-21, 1983) and vol. 3, no. 50 (Dec.
> 22-28, 1983) are both called vol. 3, no. 49 (Dec. 15-21,
> 1983).Description based on: Vol. 2, no. 3 (Jan. 28-Feb. 3, 1982).African
> AmericansAlabamaNewspapers.African
> Americans.fast(OCoLC)fst00799558Alabama.fast(OCoLC)fst01204694Newspapers.fast(OCoLC)fst01423814United
>
>
> StatesAlabamaMobileMobile.AAPUnknownAug. 15,
> 1985AU at 000024686659info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450018922463881219d19811983alucr ne 0 0eng dsn 88050233
> AARengAARCPNNSDOCLCQmscThe Sylacauga daily advance.Advance/Sylacauga
> dailySylacauga advanceSunday advanceAdvanceSylacauga, Ala. :Mrs. W.A.
> Moody,1981-1893.v.Semiweekly,<Nov. 24, 1982-Feb. 13, 1983>Daily (except
> Mon., Tues. & Sat.),<May 26, 1982-Nov. 21, 1982>Daily (except Sat. &
> Mon.),<Jan. 1, 1981-May 23, 1982>74th Year, no. 123 (Jan. 1, 1981)-76th
> year, no. 83 (Feb. 13, 1983).Days of publication vary.Published as: The
> Advance/Sylacauga daily, <Aug. 28, 1981-May 23, 1982>.Published as:
> Sylacauga advance, <Nov. 24, 1982-Feb. 13, 1983>.On Sunday, published
> as: Sunday advance.United StatesAlabamaTalladegaSylacauga.Childersburg
> star(DLC)sn 88050232Coosa press(DLC)sn 86050293Daily
> home1059-6461(DLC)sn 88050234info:srw/schema/1/marcxmlxml00000cas
> a22000007a 450021026715cr un|||||||||900209c19809999aluwr ne 0
> 0eng dsn 90099002
>
> AARengAARCPNCUSOCLOCLCQTJCOCLCQOCLCFOCLCOOCLCA926143844AU at 000020585756mscn-us-alSpeakin'
>
>
> out news.Speaking out newsDecatur, Ala. :Minority Network,
> Inc.v.WeeklyBegan in 1980.Published in Huntsville, Ala., <1987>-Also
> issued by subscription via the World Wide Web.Description based on: Vol.
> 7, no. 8 (Jan. 7-13, 1987).African AmericansAlabamaNewspapers.African
> American
> newspapersAlabama.AlabamaNewspapers.Newspapers.fast(OCoLC)fst01423814African
>
>
> American newspapers.fast(OCoLC)fst00799278African
> Americans.fast(OCoLC)fst00799558Alabama.fast(OCoLC)fst01204694United
> StatesAlabamaMorganDecatur.United
> StatesAlabamaMadisonHuntsville.Speakin' out weekly news(DLC)sn
> 88050097
> http://www.softlineweb.com/softlineweb/ethnic.htminfo:srw/schema/1/marcxmlxml00000cas
>
> a22000007a 450014996511861219c19809999aluwr ne 0 a0eng csn
> 86050472
> AARengAARCPNNSDOCLCQ11080-15021080-15021328110USPSnsdppccWest-Alabama
> gazetteWest-Alabama gazette.GazetteMillport, Ala. :Millport Pub.
> Co.,1980-volumesWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrier4th
>
>
> year, no. 32 (Jan. 3, 1980)-United StatesAlabamaLamarMillport.Gazette
> (Millport, Ala.)(DLC)sn 86050471info:srw/schema/1/marcxmlxml00000cas
> a2200000 a 450011828156850320c19809999aluwr ne 0 0eng dsn
> 86050314 AAAengAAACPNOCLOCLCQmscThe Hartford news-herald.Hartford, Ala.
> :Geneva Publications,1980-volumes :illustrations ;57-59
> cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 80,
> no. 20 (Feb. 14, 1980)-United StatesAlabamaGenevaHartford.News-herald
> (Hartford, Ala.)(DLC)sn 86050313info:srw/schema/1/marcxmlxml00000cas
> a22000007a 450017857788880427d198u198ualusr ne 0 0eng dsn
> 88050097 AARengAARCPNOCLOCLCQOCLCFOCLCOOCLCAmscn-us-alSpeakin' out
> weekly news.Decatur, Ala. :Smothers PublicationsPublished every first
> and third Wed. of each monthDescription based on: Vol. 3, no. 13 (May
> 4-17, 1983).African
> AmericansAlabamaNewspapers.Newspapers.fast(OCoLC)fst01423814African
> Americans.fast(OCoLC)fst00799558Alabama.fast(OCoLC)fst01204694United
> StatesAlabamaMorganDecatur.Weekly news (Huntsville, Ala.)(DLC)sn
> 87050012Speakin' out news(DLC)sn
> 90099002info:srw/schema/1/marcxmlxml00000cas a2200000 a
> 450017807936880418c198u9999aluwr ne 0 a0eng dsn 90099001
> AAAengAAACPNOCLOCLCQThe Daleville Sun-Courier, 310 Daleville Ave.,
> Daleville, AL 36322mscn-us-alDaleville sun-courier.Daleville, Ala. :QST
> Publicationsv. :ill. ;58 cm.WeeklyDescription based on: Vol. 2, no. 28
> (Wed., Feb. 17, 1988).United
> StatesAlabamaDaleDaleville.AU at 000020585749info:srw/schema/1/marcxmlxml00000cas
>
>
> a22000007a 450015580838870423c198u9999aluwr ne 0 0eng dsn
> 87050128 AARengAARCPNOCLCQmscGreene County independent.Eutaw, Ala.
> :Greene County Independent, Inc.v.WeeklyDescription based on: Vol. 2,
> no. 10 (Mar. 12, 1987).United
> StatesAlabamaGreeneEutaw.info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450010125135831114d198u198ualucr ne 0 a0eng dsn 83003221
> NSDengNSDOCLCQ0d0746-55210746-55211Auburn Bulletin & Lee County Eagle,
> PO Box 2111, Auburn, Ala. 36830nsdpThe Auburn bulletin & the Lee County
> eagleThe Auburn bulletin & the Lee County eagle.Lee County eagleAuburn
> bulletin and the Lee County eagleAuburn, Ala. :[publisher not
> identified]Semiweekly,<Sept. 5,
> 1984->WeeklytexttxtrdacontentunmediatednrdamediaDescription based on:
> Oct. 19, 1983.United StatesAlabamaLeeAuburn.Auburn bulletin(DLC)sn
> 89050006Eagle (Auburn, Ala.)(OCoLC)18435663Sept. 5,
> 1984info:srw/schema/1/marcxmlxml00000cas a22000007a
> 450018370324880818c198u9999aluwr ne 0 0eng dsn 88050147
> CPNengCPNOCLCQmscTri-city times (Geraldine, Ala.)The Tri-City
> times.Geraldine, Ala. :Wanda Nelsonv.WeeklyDescription based on: Vol. 2,
> no. 24 (Jan. 6, 1982).United
> StatesAlabamaDeKalbGeraldine.info:srw/schema/1/marcxmlxml00000cas
> a22000007a 450010199338831208c198u9999aluwr ne 0 a0eng dsn
> 83005367 NSDengNSDCPNOCLCQ10746-62770746-62771707590USPSSpringville Pub.
> Co., 539 Main St., Springville, AL 35146nsdpThe St. Clair clarionThe St.
> Clair clarion.Saint Clair clarionSpringville, AL :Gary L.
> ShultsWeeklytexttxtrdacontentunmediatednrdamediaDescription based on:
> Vol. 2, no. 1 (Jan. 5, 1982).United StatesAlabamaSt.
> ClairSpringville.AU at 000025783743info:srw/schema/1/marcxmlxml00000cas
> a22000007a 450013787251860627c198u9999aluwr ne 0 a0eng dsn
> 86001923 NSDengNSDCPNOCLCQ10889-00800889-00801The Westerner Star, P.O.
> Box 2060, Bessemer, AL 35021nsdpWestern star (Bessemer, Ala.)The Western
> star(Bessemer, Ala.)The western star.Bessemer, Ala. :Hal
> HodgensWeeklytexttxtrdacontentunmediatednrdamediaDescription based on:
> Vol. 3, no. 15 (Wednesday, June 11, 1986).United
> StatesAlabamaJeffersonBessemer.Bessemer advertiser(DLC)sn
> 87050117AU at 000025805174511.1srw.pc any \"y\" and srw.mt any
> \"newspaper\" and srw.cp exact
> \"Alabama\"50info:srw/schema/1/marcxmlxml1Date,,0mq1lME887FoIbjulKUV6bx9ImwWQNCv9GqZzGS92IKS31lEbcpRJBNHgcE1l29tFaHP9CHe0Yexk1uWQofffull"
>
>
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jul 27 2022, 09:22am via System
>
> Hello Spencer,
>
> Thank you for reaching out about the bulk xml files for the US Newspaper
> Directory.
>
> We don't have documentation specific to these bulk xml files, but upon
> further inspection I can say that each of those files don't necessarily
> contain info for 50 newspaper titles. The structure of the titles for
> California and New York for instance are different from say, Alabama.
>
> If you look at California for example, the file naming structure
> indicates the year the title started, and then the number of titles
> included in that xml file. So for instance, the files below include info
> for newspapers that started in 2000, 2001, and 2002 respectively. And
> there is info for 30 titles in the xml file from 2000, and 14 in the
> file for 2001, and so on.
>
>    * ndnp_California_2000_e_0001_0030.xml
>    * ndnp_California_2001_e_0001_0014.xml
>    * ndnp_California_2002_e_0001_0012.xml
>
> If there's more than 50 titles for a given year, say for California
> starting in 1880, then the next 50 titles will roll into the next xml
> file, and so on. And the last xml file for that year may not include 50
> titles.
>
> Many of the states seem to group all the years together, so each xml
> file contains 50 titles, until possibly the last one for a given state,
> which may contain less.
>
> I hope this information helps explain the total number of records and
> structure a bit better. Let me know if you have any further questions.
>
> Best wishes,
>
> Kerry Huller
> Newspaper & Current Periodical Reading Room
> Serial & Government Publications Division
> Library of Congress
>
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jul 25 2022, 02:22pm via Email
>
> Hi, Kerry:
>
>
> Might there be documentation on the XML files you mentioned?
>
>
> I've successfully read
> 'https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/',
> extracted the names of 6666 XML files, and read the first one,
> "ndnp_Alabama_all-yrs_e_0001_0050.xml". It contains 29415 characters,
> beginning, "1.12250info:srw/schema/1/marcxmlxml00000nas a22000007i
> 45001030438981180404c20159999aluwr n 0 a0eng ". With a bit
> more effort, I will likely be able to parse all 6666 of these. The
> names suggest that each contains information on 50 newspapers, totaling
> 333,300. The main page
> "https://chroniclingamerica.loc.gov/search/titles/" says there are only
> 157,521 "Titles currently listed". This suggests that these XML files
> include place holders for a little more than double the number of
> entries currently in "https://chroniclingamerica.loc.gov/search/titles/".
>
>
> Thanks for this.
>
>
> Progress.
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jul 07 2022, 08:55am via System
>
> Hi Spencer,
>
> I thought of one more option after I emailed you yesterday that I wanted
> to make you aware of.
>
> I had explained the other day how we pull the records from OCLC into our
> U.S. Newspaper Directory. You can also access all of the raw MARC
> records found in the directory in xml format from here if you choose:
> https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/
> <https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/> These will
>
> provide you all of the data from the record fields in MARC format, so
> you'd get all the data you see here for example:
> https://chroniclingamerica.loc.gov/lccn/sn98059792/marc/
> <https://chroniclingamerica.loc.gov/lccn/sn98059792/marc/> but in xml. I
> don't know if this might be more data and info than you want to work
> with, but wanted to make sure you were aware of this option as well.
>
> Best wishes,
>
> Kerry Huller
> Newspaper & Current Periodical Reading Room
> Serial & Government Publications Division
> Library of Congress
>
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jul 06 2022, 10:55am via System
>
> Hi Spencer,
>
> Thanks for reaching out again. I have been looking at the json view a
> bit closer this morning and your example of "9999."
>
> After talking with a colleague this morning and looking at various
> examples, I see there is some variation in how the titles with either an
> unknown starting/ending date or currently published titles are being
> handled - depending on the view.
>
> As an example, I completed a search in the directory for Alaska and the
> city of Anchorage. There are 80 results, and on the first page of
> results you'll see # 4. Fort Richardson news, which was published from
> 1952-19??. The csv view of this state/city search result will show the
> ending date of 19??. But if I append &format=json to this search result,
> this specific title will show an ending date of 1999. After talking with
> a colleague this morning, I discovered an integer had to be used in
> these cases where dates were "?" so that the search based on year range
> would work. Similarly, if you look at # 12 Alaska digest, which was
> published 1994-current, the "current" becomes "9999" in the json view.
> So, the records you are seeing with "9999" would most likely be titles
> with an ending date of "current."
>
> However, there is an issue with the unknown dates, like "1999" being
> used for "19??" in the example above. The "9" does not get inserted in
> place of "?" when you are looking at the title/LCCN view of a specific
> newspaper. So for instance, if you view the #4 title: Fort Richardson
> news at this url: https://chroniclingamerica.loc.gov/lccn/sn98059792/
> <https://chroniclingamerica.loc.gov/lccn/sn98059792/> but append .json
> to the end of the url, after the LCCN, like this:
> https://chroniclingamerica.loc.gov/lccn/sn98059792.json
> <https://chroniclingamerica.loc.gov/lccn/sn98059792.json> you'll see
> that the end_year is "19??." Viewing the title/LCCN json view for titles
> that are currently published will also show the end_year as "current."
> The Alaska digest example from above can be viewed here:
> https://chroniclingamerica.loc.gov/lccn/sn97060056.json
> <https://chroniclingamerica.loc.gov/lccn/sn97060056.json>
>
> I wasn't aware of the difference between the directory search json view
> and the title/LCCN view. But I think it would be possible to grab
> the data from the title/LCCN json url through an additional script
> potentially. The json url is included in the view under the "url" field.
>
> Of course, there are unknowns with publishing dates, but better to know
> where the question marks are, and what titles are considered to be current.
>
> I hope this clarifies the data a bit more - let me know if any of it
> needs more clarification though. And let me know if you have follow-up
> questions.
>
> Thank you,
>
> Kerry Huller
> Newspaper & Current Periodical Reading Room
> Serial & Government Publications Division
> Library of Congress
>
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jul 05 2022, 04:42pm via Email
>
> Hi, Kerry:
>
>
> What would you suggest I do to get a count of the numbers of
> newspapers and publishers operating by year from, say, 1790 to 2021?
>
>
> I just determined that 20630 (13 percent) of the 157520 records in
> the US Newspaper database I downloaded a week ago have end_year = 9999.
> I don't think it's feasible to assume that all or even most of those
> are still publishing.
>
>
> Might there be some other database that might have this kind of
> information?
>
>
> I ask, because Robert McChesney (2004) The Problem of the Media
> (Monthly Review Pr., esp. pp. 34-35) suggests that in the first half of
> the nineteenth century, the US had more newspapers and newspaper
> publishers per capita than any other place or time. He suggests that
> that diversity of newspapers helped encourage literacy and limit
> political corruption, both of which helped propel the young US to its
> current dominance of the international political economy. I'm hoping to
> get some data to evaluate this claim. Sadly, it looks like there is too
> much missing and questionable data in this dataset for me to use this
> without a fairly substantive data cleaning effort.
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jul 05 2022, 09:05am via System
>
> Hello Spencer,
>
> Thank you for reaching out about your additional questions.
>
> I was looking at the records you mention above, and yes, you are correct
> - those 9 records with the date inconsistencies and the one record for
> the The New Mexican mining news
> <https://chroniclingamerica.loc.gov/lccn/sn93061507/> containing "Santa
> Fe.\" have typos in them. Thanks for spotting these - it may be possible
> to have the cataloger in our division correct those typos. I will look
> into this further.
>
> The U.S. Newspaper Directory doesn't have a connection with Wikimedia or
> Wikipedia. The Library of Congress periodically pulls the records for
> the Directory from OCLC Worldcat
> <https://www.oclc.org/en/worldcat.html>. And those newspaper records in
> OCLC Worldcat have been created by catalogers at various institutions
> around the U.S. over the span of several years. So, occasionally, you
> will find a typo in the records. Corrections can be made by OCLC and
> library staff at the various institutions. Every time we complete a new
> pull on the OCLC records, any corrected records will then populate our
> Directory.
>
> Regarding your question on the New-York weekly journal - yes, that is
> also correct that it has two records. There is actually a record for
> each format of the newspaper, so this record is for the microfilm format
> <https://chroniclingamerica.loc.gov/lccn/2009252748/> and this one is
> for the original print format
> <https://chroniclingamerica.loc.gov/lccn/sn83030211/>. You can see in
> the heading for the microfilm record where it says [microfilm reel] and
> the print version shows [volume]. You are likely to see this for other
> titles as well because each format has been cataloged with its own LCCN.
> You are also likely to see additional records with [online resource]
> identified as the format as more and more titles are available as
> ePrints or online.
>
> I hope this helps answer your additional questions a bit more. Please
> reach out if you have any other questions.
>
> Thank you,
>
> Kerry Huller
> Newspaper & Current Periodical Reading Room
> Serial & Government Publications Division
> Library of Congress
>
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jul 04 2022, 01:47pm via Email
>
> Hi, Kelly:
>
>
> At the risk of bombing your inbox with more emails than you want,
> what is your relationship with Wikipedia and other Wikimedia Foundation
> projects like Wikidata?
>
>
> I ask, because I've logged over 20,000 edits in Wikimedia Foundation
> projects since 2010, and I would happily try to answer questions about
> Wikidata and other Wikimedia Foundation projects. I have NOT organized
> an edit-a-thon, but I've made presentations at conferences with people
> who have, and I would happily try to help organize such if you could
> find a group of people who want to work to improve this US Newspaper
> database. I think it would be good to establish links between this US
> Newspaper database and Wikidata, with appropriate procedures so changes
> to one could be evaluated for acceptance into the other.
>
>
> FYI, John Peter Zenger's famous "New-York weekly journal" (1733-1751)
> appears TWICE in your database with lccn = 2009252748 and sn83030211 and
> ONCE in Wikidata WITHOUT an lccn, even though many other Wikidata items
> have an lccn. See:
>
>
> https://www.wikidata.org/wiki/Q23091960
>
>
> There's a "WikiProject Newspapers" on Wikipedia and a companion
> "WikiProject Periodicals" on Wikidata:
>
>
> https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Newspapers/Wikidata
>
>
> https://www.wikidata.org/wiki/Wikidata:WikiProject_Periodicals
>
>
> I've tried to connect with others on those projects, so far with only
> limited success. However, you may know that almost anyone can change
> almost anything on Wikipedia and other Wikimedia Foundation projects.
> What stays tends to be written from a neutral point of view citing
> credible sources. They have problems with vandals, but the problems are
> usually easily controlled. This makes Wikipedia and Wikidata very
> useful platforms for cleaning up databases like your US Newspaper dataset.
>
>
> Spencer Graves
>
>
> ##########
>
>
> Hello, Kelly:
>
>
> In addition to the invalid JSON, discussed below [NOTE: The "below"
> contains a slight addition to the report of the I sent last Friday.], I
> found 9 (NINE!) cases where start_year was AFTER end_year. These have
> lccn = "sn86071531" "sn95069213" "sn90059096" "sn86058451" "sn90060926"
> "sn99065409" "sn89065002" "sn98069857" "sn91059179"
>
>
> See:
>
>
> https://chroniclingamerica.loc.gov/lccn/sn86071531/
> https://chroniclingamerica.loc.gov/lccn/sn95069213/
> https://chroniclingamerica.loc.gov/lccn/sn90059096/
> https://chroniclingamerica.loc.gov/lccn/sn86058451/
> https://chroniclingamerica.loc.gov/lccn/sn90060926/
> https://chroniclingamerica.loc.gov/lccn/sn99065409/
> https://chroniclingamerica.loc.gov/lccn/sn89065002/
> https://chroniclingamerica.loc.gov/lccn/sn98069857/
> https://chroniclingamerica.loc.gov/lccn/sn91059179/
>
>
> These all have obvious coding errors that can be easily fixed. The
> data may not be completely accurate after the fix, but at least they are
> not obviously wrong ;-)
>
>
> ##################
>
> I got invalid JSON from:
>
>
>
> https://chroniclingamerica.loc.gov/search/titles/results/?rows=500&page=103&format=json
>
>
> After some experimentation, I was able to replicate the problem with
> a request for rows=10:
>
>
>
> https://chroniclingamerica.loc.gov/search/titles/results/?rows=10&page=5117&format=json
>
>
> Duncan Temple Lang <dtemplelang at ucdavis.edu>, Professor of Statistics
> and Associate Dean for Graduate Programs at the University of California
> - Davis, confirmed that it was a JSON error using:
>
>
> https://codebeautify.org/jsonvalidator
>
>
> He is part of the core team developing the R free, open-source
> programming language. He said, that starting at offsets 161070 and
> 161502 in the character string you get from [the R code RCurl::getURL()]
> we have:
>
>
> Santa Fe.\"
>
>
> and these are in an entry such as
>
>
> "city": ["Santa Fe.\"]
>
>
> So the final " is escaped and therefore there is no closing " for the
> string. The parser continues to consume characters looking for the end
> of that string.
>
>
> If one "repairs" the text from getURL() with
>
>
> ftxt= gsub('Santa Fe.\\\\"', 'Santa Fe."', txt)
>
>
> then the rest of my code worked fine.
>
>
> You may wish to do something to implement other checks for valid JSON
> and repair this problem. I've scanned all the 157520 records that were
> in that database a couple of days ago, and this is the only JSON error
> identified by the code I used.
>
>
> NOTE: I was NOT able to replicate this error when downloading records
> one at a time. That suggests a problem NOT in the database itself but
> in the download algorithm. ???
>
>
> Thank you for your help. I will almost certainly have other
> questions ;-)
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jul 03 2022, 10:39pm via Email
>
> Hello, Kelly:
>
>
> In addition to the invalid JSON, discussed below [NOTE: The "below"
> contains a slight addition to the report of the I sent last Friday.], I
> found 9 (NINE!) cases where start_year was AFTER end_year. These have
> lccn = "sn86071531" "sn95069213" "sn90059096" "sn86058451" "sn90060926"
> "sn99065409" "sn89065002" "sn98069857" "sn91059179"
>
>
> See:
>
>
> https://chroniclingamerica.loc.gov/lccn/sn86071531/
> https://chroniclingamerica.loc.gov/lccn/sn95069213/
> https://chroniclingamerica.loc.gov/lccn/sn90059096/
> https://chroniclingamerica.loc.gov/lccn/sn86058451/
> https://chroniclingamerica.loc.gov/lccn/sn90060926/
> https://chroniclingamerica.loc.gov/lccn/sn99065409/
> https://chroniclingamerica.loc.gov/lccn/sn89065002/
> https://chroniclingamerica.loc.gov/lccn/sn98069857/
> https://chroniclingamerica.loc.gov/lccn/sn91059179/
>
>
> These all have obvious coding errors that can be easily fixed. The
> data may not be completely accurate after the fix, but at least they are
> not obviously wrong ;-)
>
>
> ##################
>
> I got invalid JSON from:
>
>
>
> https://chroniclingamerica.loc.gov/search/titles/results/?rows=500&page=103&format=json
>
>
> After some experimentation, I was able to replicate the problem with
> a request for rows=10:
>
>
>
> https://chroniclingamerica.loc.gov/search/titles/results/?rows=10&page=5117&format=json
>
>
> Duncan Temple Lang <dtemplelang at ucdavis.edu>, Professor of Statistics
> and Associate Dean for Graduate Programs at the University of California
> - Davis, confirmed that it was a JSON error using:
>
>
> https://codebeautify.org/jsonvalidator
>
>
> He is part of the core team developing the R free, open-source
> programming language. He said, that starting at offsets 161070 and
> 161502 in the character string you get from [the R code RCurl::getURL()]
> we have:
>
>
> Santa Fe.\"
>
>
> and these are in an entry such as
>
>
> "city": ["Santa Fe.\"]
>
>
> So the final " is escaped and therefore there is no closing " for the
> string. The parser continues to consume characters looking for the end
> of that string.
>
>
> If one "repairs" the text from getURL() with
>
>
> ftxt= gsub('Santa Fe.\\\\"', 'Santa Fe."', txt)
>
>
> then the rest of my code worked fine.
>
>
> You may wish to do something to implement other checks for valid JSON
> and repair this problem. I've scanned all the 157520 records that were
> in that database a couple of days ago, and this is the only JSON error
> identified by the code I used.
>
>
> NOTE: I was NOT able to replicate this error when downloading records
> one at a time. That suggests a problem NOT in the database itself but
> in the download algorithm. ???
>
>
> Thank you for your help. I will almost certainly have other
> questions ;-)
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jul 01 2022, 11:46am via Email
>
> Hello, Kelly:
>
>
> I got invalid JSON from:
>
>
>
> https://chroniclingamerica.loc.gov/search/titles/results/?rows=500&page=103&format=json
>
>
> After some experimentation, I was able to replicate the problem with
> a request for rows=10:
>
>
>
> https://chroniclingamerica.loc.gov/search/titles/results/?rows=10&page=5117&format=json
>
>
> Duncan Temple Lang <dtemplelang at ucdavis.edu>, Professor of Statistics
> and Associate Dean for Graduate Programs at the University of California
> - Davis, confirmed that it was a JSON error using:
>
>
> https://codebeautify.org/jsonvalidator
>
>
> He is part of the core team developing the R free, open-source
> programming language. He said, that starting at offsets 161070 and
> 161502 in the character string you get from [the R code RCurl::getURL()]
> we have:
>
>
> Santa Fe.\"
>
>
> and these are in an entry such as
>
>
> "city": ["Santa Fe.\"]
>
>
> So the final " is escaped and therefore there is no closing " for the
> string. The parser continues to consume characters looking for the end
> of that string.
>
>
> If one "repairs" the text from getURL() with
>
>
> ftxt= gsub('Santa Fe.\\\\"', 'Santa Fe."', txt)
>
>
> then the rest of my code worked fine.
>
>
> You may wish to do something to implement other checks for valid JSON
> and repair this problem. I've scanned all the 157520 records that were
> in that database a couple of days ago, and this is the only JSON error
> identified by the code I used.
>
>
> Thank you for your help. I will almost certainly have other
> questions ;-)
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jun 28 2022, 02:20pm via System
>
> Hello Spencer,
>
> Thank you for sending along your follow-up questions.
>
> I'm glad to hear the json view will work for you. It was recommended to
> me that you limit your requests to 500 rows at a time. And a developer
> here at LC suggests the following regarding rate limiting:
>
> ?To avoid being blocked by the server, the current rate-limiting rules
> restrict un-cached requests to URLs starting with
> https://chroniclingamerica.loc.gov/search/
> <https://chroniclingamerica.loc.gov/search/> to 120 requests every 10
> minutes from a single IP address.?
>
> So, I think if you limited each of your requests to 500 rows at a time
> with the proper pauses, then you should be able to access what you need.
>
> As for the csv view, I checked on this as well, and was informed that
> the csv view was not implemented for all url formats. The csv view was
> only implemented for this view:
> https://chroniclingamerica.loc.gov/newspapers/
> <https://chroniclingamerica.loc.gov/newspapers/>and urls resulting from
> US Directory search results - for e.g. if you wanted to narrow down your
> search results by state, city, date range, etc. found at this link:
> https://chroniclingamerica.loc.gov/search/titles/
> <https://chroniclingamerica.loc.gov/search/titles/>. So, if you wanted a
> csv and limited your search by state ( for example:
>
> https://chroniclingamerica.loc.gov/search/titles/results/?state=Alaska&county=&city=&year1=1690&year2=2022&terms=&frequency=&language=&ethnicity=&labor=&material_type=&lccn=&rows=20&format=csv
> <
> https://chroniclingamerica.loc.gov/search/titles/results/?state=Alaska&county=&city=&year1=1690&year2=2022&terms=&frequency=&language=&ethnicity=&labor=&material_type=&lccn=&rows=20&format=csv>
>
> ), you could append &format=csv to the search result url and get the csv
> to automatically download. But, if your search results ended up being
> over a couple thousand titles, then the system would probably time out.
>
> I hope this info helps! Let me know if you have any other questions.
>
> Best wishes,
>
> Kerry Huller
> Newspaper & Current Periodical Reading Room
> Serial & Government Publications Division
> Library of Congress
>
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jun 27 2022, 04:15pm via Email
>
> Hello, Kerry:
>
>
> Thanks for the reply. Can you please give me some further guidance
> on two thing "so that the system is not overwhelmed"?
>
>
> 1. The max size in a small batch?
>
>
> 2. Any limit on the number of small batches in a second or minute?
>
>
> I've found that I can download small batches under program control
> using "RCurl::getURL" in R (programming language) using, e.g.;
>
>
>
> https://chroniclingamerica.loc.gov/search/titles/results/?rows=20&page=2&format=json
>
>
> With this, I can control the batch size with "row=20" vs. "row=50"
> vs., e.g., "row=1000". A naive search says there are 157520 "results".
> With "row=1000", this would require 158 calls. With "row=20", it
> would require 7876 calls. Before I start, I need to decide which fields
> I want; I don't need them all.
>
>
> Thanks,
> Spencer Graves
>
>
> p.s. I tried appending "&format=csv" and got "Error 504 Ray ID:
> 7220896da85e86e7 ? 2022-06-27 19:19:53 UTC Gateway time-out". I used:
>
>
>
> https://chroniclingamerica.loc.gov/search/titles/results/?state=&county=&city=&year1=1690&year2=2022&terms=&frequency=&language=&ethnicity=&labor=&material_type=&lccn=&rows=20&format=csv
>
>
> I can get what I want using json so do not need csv. However, I
> thought you might want to know that I was unable to get csv to work.
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jun 27 2022, 10:54am via System
>
> Hello Spencer,
>
> Thank you for contacting the Library of Congress about searching the US
> Newspaper Directory. I wanted to follow up with you regarding your
> request to output the data in a machine readable format.
>
> It looks like you were provided the link to the API documentation for
> the website: About the Site and API
> <https://chroniclingamerica.loc.gov/about/api/>. Scroll down to the
> section with the heading, Searching the directory and newspaper pages
> using OpenSearch. This section describes the search functionality and
> structure for the US Newspaper Directory in more detail. It is possible
> to return your directory searches in json format by appending
> &format=json to the end of the url. It is also possible to return search
> results in csv format by appending &format=csv to the end of the url,
> but I would strongly suggest that you do this in small batches by
> putting limits on your search so that the system is not overwhelmed.
>
> So, from the search page for the US Newspaper Directory
> <https://chroniclingamerica.loc.gov/search/titles/> you could
> potentially limit your search based on state and city, or date range,
> and/or even frequency. Then once you've completed the search, you can
> add &format=csv to the end of the url to automatically download a csv of
> those records. The resulting csv will contain several fields/headers:
> lccn, title, place of publication, start year, end year, publisher,
> edition, frequency, subject, state, city, country, language, oclc
> number, and holding type. I think these fields include the information
> you were looking for. But, again, I would like to stress that you put
> limits on your search before creating the csv so as not overwhelm the
> system.
>
> Please let me know if you have any other additional questions.
>
> Best wishes,
>
> Kerry Huller
> Newspaper & Current Periodical Reading Room
> Serial & Government Publications Division
> Library of Congress
>
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jun 23 2022, 01:55pm via System
>
> Mr. Graves,
>
> I'm going to transfer you request to a member of our digital collections
> team who may be of more assistance to you than me.
>
> Mike
>
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jun 23 2022, 01:51pm via Email
>
> Dear Mr. Queen:
>
>
> Thanks for the reply. I'm still confused. I downloaded and
> installed Docker Desktop and "docker-compose.yml" and ran their "Getting
> Started" Tutorial, but I don't see what to do next.
>
>
> I repeat: I'd like to analyze "U.S. Newspaper Directory,
> 1690-Present" (https://chroniclingamerica.loc.gov/search/titles/), which
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jun 22 2022, 07:15pm via System
>
> Mr. Graves,
>
> Programmatic access to the data forChronicling America
> <https://chroniclingamerica.loc.gov/>and possibly the U.S. Newspaper
> Directory <https://chroniclingamerica.loc.gov/search/titles/>can be
> found on theAbout the Site and API
> <https://chroniclingamerica.loc.gov/about/api/>page in various formats.
> Also, please note that Chronicling Americacontains newspapers published
> from 1777-1963, but does not include everyU.S. newspaper published in
> that time period.
>
> Please let me know if I can be of further assistance.
>
>
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jun 22 2022, 06:14pm via Email
>
> Dear Mr. Queen:
>
>
> Can we simplify this to just giving me the data behind "U.S.
> Newspaper Directory, 1690-Present"
> (https://chroniclingamerica.loc.gov/search/titles/) in a machine
> readable format, e.g., csv or xlsx or a MySQL database?
>
>
> As I mentioned in my original email, a naive search of that without
> restrictions returned 157520 titles in 7876 pages with up to 20 titles
> per page giving date ranges in at least some cases. I could probably
> write software to scrape those 7876 pages from your web site and combine
> them into a data file.
>
>
> I have a PhD in statistics, I have been using the R programming
> language and similar software for decades. This includes publishing
> tutorials on how to analyze data like this on Wikiversity.[1] I'd like
> to do something similar with this. I could help make your data more
> useful to others and discuss with you how we might prioritize
> improvements like accessing the other sources you mentioned.
>
>
> Thanks very much for your reply.
>
>
> Sincerely,
> Spencer Graves, PhD
> Founder, EffectiveDefense.org
> 4550 Warwick Blvd 508
> Kansas City, MO 64111
> m: 408-655-4567
>
>
> [1] e.g.:
>
>
> https://en.wikiversity.org/wiki/US_Gross_Domestic_Product_(GDP)_per_capita
> ------------------------------------------------------------------------
>
> Newspapers and Current Periodicals Reference Librarian
>
> Jun 22 2022, 05:27pm via System
>
> Mr. Graves
>
> Your request is a little more complex than it first appears and requires
> extensive research. A variety of resources should be consulted to
> determine the circulation statistics of newspapers published prior to
> 1851. You will need to check newspaper union lists and newspaper
> histories. Union listspresent lists of newspapers in geographic
> arrangement according to place of publication, and specify which
> libraries or other institutions hold collections of those newspapers and
> the dates of their holdings. These can also be useful for tracking title
> changes throughout a newspaper's history. Newspaper
> historieslikeAmerican Journalism: A History: 1690-1960
> <https://lccn.loc.gov/62007157>(Mott),The Penny Press
> <https://lccn.loc.gov/2004043078>(Thompson), andThe Press and America
> <https://lccn.loc.gov/99044295>(Emery et al.) may not include
> circulation statistics, but they do document the diversity and progress
> of newspaper publishing, including notable newspapers of the era.
> Newspaper histories also cover the history of the printers and printing
> of newspapers in a state, county, or region more generally, and provide
> more condensed histories of the editors, journalists, and evolution of
> the newspapers in a specific area. Newspaper histories and union lists
> should be available at most large public or university libraries. More
> information about union lists, newspaper histories, and researching
> newspapers in general can be found in theU.S. Newspaper Collections at
> the Library of Congress
> <https://guides.loc.gov/united-states-newspapers/introduction>research
> guide (see Reference Sources).
>
> Please let me know if I can be of further assistance.
>
> ------------------------------------------------------------------------
>
> Original Question
>
> Jun 20 2022, 02:34pm via System
>
> How can I get counts of the numbers of newspapers by year in the US, and
> preferably also elsewhere? A search of "U.S. Newspaper Directory,
> How can I get counts of the numbers of newspapers by year in the US, and
> preferably also elsewhere?
>
> A search of "U.S. Newspaper Directory, 1690-Present"
> (https://chroniclingamerica.loc.gov/search/titles/) returned 157520
> titles in 7876 pages with up to 20 titles per page giving date ranges to
> the extent that it's known. If I can get a data file (e.g., csv or xls),
> I can summarize. I could also use data on circulation and frequency and
> especially parent company for multiple newspapers published by the same
> company, to the extant that such is available.
>
> I'm interested in this, because McChesney quoted Tocqueville in
> suggesting that the US had more newspapers per person (or per million
> population) prior to 1851 than at any other time or place in history.
> I'd like to evaluate that claim with data to the extent that I can. See
> "
> https://en.wikiversity.org/wiki/Social_construction_of_crime_and_what_we_can_do_about_it#Newspapers_1790_-_present".
>
>
>
> Thanks, Spencer Graves, PhD
> m: 408-655-4567
>
> ------------------------------------------------------------------------
>
> Thank you for using Newspapers & Current Periodicals Ask a Librarian
> Service!
>
>
> This email is sent from Ask a Librarian in relationship to ticket #9625195.
>
> Read our privacy policy. <https://springshare.com/privacy.html>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Thu Jul 28 08:49:22 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Thu, 28 Jul 2022 09:49:22 +0300
Subject: [R] Parsing XML?
In-Reply-To: <7743581c-0959-1df6-bb31-f140161f83e1@effectivedefense.org>
References: <20220727145903.4338c449692f703b@ask.loc.gov>
 <7743581c-0959-1df6-bb31-f140161f83e1@effectivedefense.org>
Message-ID: <20220728094922.5f2ab19a@trisector>

On Wed, 27 Jul 2022 15:50:55 -0500
Spencer Graves <spencer.graves at effectivedefense.org> wrote:

> What would you suggest I do to parse the following XML file into a
> list that I can understand:
> 
> XMLfile <-
> "https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/ndnp_Alabama_all-yrs_e_0001_0050.xml" 

> XMLdat <- XML::xmlParse(XMLdata)
> str(XMLdat)

Isn't XMLdat already a tree-like list? For example,
XMLdat[[1]][[1]][[3]][[1]] is the first <record> tag in the file, which
you can further pick apart.

What information do you need from this file and how would you like to
access it? Parsing XML files is typically achieved with XPath
expressions (e.g. 'under every <record> tag, extract the <datafield>
tags containing attribute tag="042"' would look like
'record/datafield[tag="042"]') and/or handlers on specific tags, not by
extracting all text nodes and performing string operations on them.

-- 
Best regards,
Ivan


From r@njeetjh@||tkgp @end|ng |rom gm@||@com  Thu Jul 28 09:33:24 2022
From: r@njeetjh@||tkgp @end|ng |rom gm@||@com (Ranjeet Kumar Jha)
Date: Thu, 28 Jul 2022 13:03:24 +0530
Subject: [R] Need to insert various rows of data from a data frame after
 particular rows from another dataframe
In-Reply-To: <CABcYAd+277N-VnvopkbDw9qdgGAK0isdjUKp8VoR-nWy3ptidA@mail.gmail.com>
References: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
 <CABcYAd+277N-VnvopkbDw9qdgGAK0isdjUKp8VoR-nWy3ptidA@mail.gmail.com>
Message-ID: <CAJfMH3qe8B9FkKWV_OboyeP_EL15n9a0DwBCtfYV-vAtVdYRzQ@mail.gmail.com>

Thank you very much Sir! You please don't do that much hard work now. I
have made life easy for you. I have formatted the 2nd file "kharif_18_19"
in the same format as it is in the dacnet file. Now, both files I want as
combined where in dacnet file 2018 and 2019 yield data for each crop for
every district and state should appear just below the row of previous years
of data.

On Thu, Jul 28, 2022 at 12:11 PM Richard O'Keefe <raoknz at gmail.com> wrote:

> I'm retired, and I had an hour on my hands while tea cooked and my
> granddaughter did her homework, and I just *love* showing off how helpful I
> am.
>
> Good news: someone finally looked at your data.
> (That would be me.)
>
> Bad news: it's going to be a lot of work to do what you want to, and YOU
> SHOULDN'T EVEN TRY because it won't make any sense, as we would all have
> known at once had you been clear about the structure of your data in the
> beginning.
>
> You have two files.
> dacnet_yield_update till 2019.csv
> is a straightforward "data" file with structure
>
>   crop factor(arhar,bajra,cotton,gram,maize,moong,mustard,
>               potato,rice,soyabean,sugarcane,urad,wheat)
>   season factor(kharif,rabi),
>   state.id integer(1201..1235),
>   state.name factor, # 34 states
>   district.id integer(15001..15648),
>   district.name factor,
>   year integer(1998..2017),
>   yield decimal(0.001 .. 314.736, precision=3)
>
> The one problem is that the file name is misleading.  It says "till 2019",
> but includes no data for 2019 or 2018.
>
> Ah, but the other file!  That's not a "data" file intended for machine use
> at all.  It's a "display" file intended for human beings to look at and go
> "wow, gosh, lookit them numbahs".  It's the kind of thing that gets
> included as an appendix in an official report which seems as if perversely
> designed to impede the development of insight as much as possible.
>
> Amongst other difficulties:
> - the same column contains state names, district names, crop names, and
> assorted junk;
> - state names are not coded the same way in the two files;
> - district names are in UPPERCASE in the .xls files and have numbers
> prefixed to them for no apparent reason;
> - crop names are not coded the same way in the two files;
> - yields are not coded the same way in the two files (3 digit precision in
> one, 2 digit precision in the other) and I have some doubt as to whether
> they are measuring the same thing;
> - above all, years appear to be CALENDAR years in the .csv file (e.g.,
> 2017) but FINANCIAL years in the .xsl file (e.g., 2018-19)
>
> Now I could wrangle the .xls file into something closer to the .csv file
> easily enough.  I'd do it by converting the .xsl to .csv, then writing a
> script in AWK.  *BUT* my uncertainty that "yield" means the same thing in
> the two files and my certainty that "year" does NOT mean the same thing
> make it unrewarding to do so.
>
> The .xls file is the end product of some process that derived it from data
> better structured for computation.  It seems like a better use of your time
> to go and look for the original data.
>
> It also seems like a good use of your time to make certain you know what
> the fields of the .csv file actually mean.  ARE those calendar yields, or
> just part of a financial year?  Why are only the yields of interest and not
> the area planted?  How are the yields computed?
>
>
>
>
>
> On Wed, 27 Jul 2022 at 14:31, Ranjeet Kumar Jha <
> ranjeetjhaiitkgp at gmail.com> wrote:
>
>> Hello Everyone,
>>
>> I have dataset in a particular format in "dacnet_yield_update till
>> 2019.xlsx" file, where I need to insert the data of rows 2018-2019 and
>> 2019-2020 for the districts those data are available in "Kharif crops
>> yield_18-19.xlsx".  I need to insert these two rows of data belonging to
>> every district, if data is available in a later excel file, just after the
>> particular crop group data for the particular district.
>>
>> I have put the data file in the given link.
>>
>> https://drive.google.com/drive/u/0/folders/1dNmGTI8_c9PK1QqmfIjnpbyzuiCXgxFC
>>
>> Please help solving this problem.
>>
>> Regards and Thanks,
>> Ranjeet
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

-- 
Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
-----------------------------------------------------------
Email: *ranjeetjhaiitkgp at gmail.com <ranjeetjhaiitkgp at gmail.com>*


*"Simple Heart, Humble Attitude and Surrender to Supreme Being make our
lives beautiful!"*

	[[alternative HTML version deleted]]


From r@njeetjh@||tkgp @end|ng |rom gm@||@com  Thu Jul 28 09:34:57 2022
From: r@njeetjh@||tkgp @end|ng |rom gm@||@com (Ranjeet Kumar Jha)
Date: Thu, 28 Jul 2022 13:04:57 +0530
Subject: [R] Need to insert various rows of data from a data frame after
 particular rows from another dataframe
In-Reply-To: <BN6PR2201MB15532960CD3AE91A363043E8CF979@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
 <BN6PR2201MB15532960CD3AE91A363043E8CF979@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <CAJfMH3oG3b9w9bThCOa6o+6Df8CBfah+Ve6naFUCzSqqxwKfWA@mail.gmail.com>

I tried using the merge, and join function but it is not providing in the
rows below the yield data of previous years for the same crop in the dacnet
file.

On Wed, Jul 27, 2022 at 6:04 PM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:

> I think what you want is full_join() from the dplyr package.
> https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/join
>
> The only requirement is that both data frames must have a column in common
> wherein the data are entered in the same way. So the column labeled "state"
> needs to have "ANANTAPUR" in both data frames rather than "ANANTAPUR" in
> one data frame and "Anantapur" in the other.
>
> Reformat your excel spreadsheet to remove headers. Your first column
> should be three columns: State, then Crop, then district rather than
> headings. The first row can contain variable names. I would make variable
> names simple (like "Area" and "Production") but some like more information
> so "Area_Ha" and "Production_TN_per_Ha" would also work. It is best not to
> use special symbols in variable names and keep variable names as one string
> of characters (no spaces). Including such will eventually cause problems.
>
> https://www.w3schools.com/r/r_variables.asp#:~:text=Variable%20Names&text=Rules%20for%20R%20variables%20are,be%20followed%20by%20a%20digit
> .
>
> One exception to the rules in the link is that you can make a variable
> name T or F. R defaults to interpreting these as TRUE and FALSE. The
> problem happens when the programmer reassigns these and then tries to use T
> or F as Boolean in other parts of the program.
>
> Tim
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ranjeet Kumar Jha
> Sent: Monday, July 25, 2022 9:03 AM
> To: R-help <r-help at r-project.org>
> Subject: [R] Need to insert various rows of data from a data frame after
> particular rows from another dataframe
>
> [External Email]
>
> Hello Everyone,
>
> I have dataset in a particular format in "dacnet_yield_update till
> 2019.xlsx" file, where I need to insert the data of rows 2018-2019 and
> 2019-2020 for the districts those data are available in "Kharif crops
> yield_18-19.xlsx".  I need to insert these two rows of data belonging to
> every district, if data is available in a later excel file, just after the
> particular crop group data for the particular district.
>
> I have put the data file in the given link.
>
> https://urldefense.proofpoint.com/v2/url?u=https-3A__drive.google.com_drive_u_0_folders_1dNmGTI8-5Fc9PK1QqmfIjnpbyzuiCXgxFC&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=9B32l682GguXDLEFdPm6j5JZNatveGSlY7lnwLYFVOW2TX1tNLeHbDE49MYxSh_Q&s=4_bhl2_drIA0Pn3LHMcoAd02lX0t6bAx2wSlhVAJelA&e=
>
> Please help solving this problem.
>
> Regards and Thanks,
> Ranjeet
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=9B32l682GguXDLEFdPm6j5JZNatveGSlY7lnwLYFVOW2TX1tNLeHbDE49MYxSh_Q&s=MAGsb78RBOWV0usgeNnmHsZcYoQI959dmihJ9Ycs8Lo&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=sJ6xIWYx-zLMB3EPkvcnVg&r=9PEhQh2kVeAsRzsn7AkP-g&m=9B32l682GguXDLEFdPm6j5JZNatveGSlY7lnwLYFVOW2TX1tNLeHbDE49MYxSh_Q&s=PSiyw67xhInkZo69l1HojQKGOqthbxYpGL5Q14cPo8w&e=
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
-----------------------------------------------------------
Email: *ranjeetjhaiitkgp at gmail.com <ranjeetjhaiitkgp at gmail.com>*


*"Simple Heart, Humble Attitude and Surrender to Supreme Being make our
lives beautiful!"*

	[[alternative HTML version deleted]]


From r@njeetjh@||tkgp @end|ng |rom gm@||@com  Thu Jul 28 09:42:48 2022
From: r@njeetjh@||tkgp @end|ng |rom gm@||@com (Ranjeet Kumar Jha)
Date: Thu, 28 Jul 2022 13:12:48 +0530
Subject: [R] Need to insert various rows of data from a data frame after
 particular rows from another dataframe
In-Reply-To: <e2f1cc08e8f04cb9bbf4ecb0a7362f40@SRVEXCHCM1302.precheza.cz>
References: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
 <a172da90dd2448779581da58741b913e@SRVEXCHCM1302.precheza.cz>
 <CAJfMH3rF9RmJee+G0Y+3Re8U9BsxVkUhoZJhgQyGwYxrUxCSJQ@mail.gmail.com>
 <9c4386179ab24616915d2296b70a28d8@SRVEXCHCM1302.precheza.cz>
 <CAJfMH3oUD720UKT0DiuLWcPr4OvBEdDPeyxPwgpgXZhg3Pz5YA@mail.gmail.com>
 <e2f1cc08e8f04cb9bbf4ecb0a7362f40@SRVEXCHCM1302.precheza.cz>
Message-ID: <CAJfMH3pxAZ+z5aZQZMhL9rc63Zyvoms3Lw+DGH6_Ac0ZeVFzpQ@mail.gmail.com>

Hi Pikal,

Now I have formatted the kharif data file in the same format as it is in
the dacnet filecode:. However, "district.id" and "state.id " columns have
"N/A" data that are appearing. If I use merge "rbind" function, it still
throws this error:

"Error in rbind(deparse.level, ...) :
  numbers of columns of arguments do not match"

code:
df3=rbind(kharif_18_19,dacnet_17)
df3=df3[order(df3$district,df3$year),]
x<-write.csv(df3,"df3.csv")
view(x)

On Wed, Jul 27, 2022 at 3:18 PM PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hallo,
>
> I do not understand what you really want and you do not help much.
>
> No error, no data, vague description of your problem, no effort to explain
> it better. Only you can see your data, only you can see the error message
> so we are clueless.
>
> Cheers
> Petr
>
>
> From: Ranjeet Kumar Jha <ranjeetjhaiitkgp at gmail.com>
> Sent: Wednesday, July 27, 2022 11:01 AM
> To: PIKAL Petr <petr.pikal at precheza.cz>
> Cc: R-help <r-help at r-project.org>
> Subject: Re: [R] Need to insert various rows of data from a data frame
> after particular rows from another dataframe
>
> yeah rbind works find bur dataframe 2 has different format and then rbind
> can be used with first dataframe. But here I am struggling to bring the 2nd
> dataframe in the same format as the 1st one.
>
> On Wed, Jul 27, 2022 at 1:13 PM PIKAL Petr <mailto:petr.pikal at precheza.cz>
> wrote:
> Hi.
>
> ?is not working? is extremelly vague.
>
> 1.
> What do you expect this code do?
>
> for(cr in seq_along(dacnet_17$district)){
>     match(arhar_18$district, dacnet_17$district)
> }
>
> See ?match and maybe also ??for? and try this
>
> x <- letters[1:5]
> y <- sample(letters, 100, replace=T)
> match(x,y)
> [1] 45 16 24 13 71
> for(i in 1:3) match(x,y)
>
> 2.
> rbind works as expeceted
>
> arhar_18 <- data.frame(a=1:10, b=50, c=letters[1:10])
> dacnet_17 <- data.frame(a=11:20, b=100, c=sample(letters,10))
> df3=rbind(arhar_18,dacnet_17)
>
> if your data has common column order and type.
>
> To get more specific answer you need to ask specific question preferably
> with some data included (most preferably by dput command) and error message.
>
> Cheers
> Petr
>
>
> From: Ranjeet Kumar Jha <mailto:ranjeetjhaiitkgp at gmail.com>
> Sent: Wednesday, July 27, 2022 8:35 AM
> To: PIKAL Petr <mailto:petr.pikal at precheza.cz>
> Cc: R-help <mailto:r-help at r-project.org>
> Subject: Re: [R] Need to insert various rows of data from a data frame
> after particular rows from another dataframe
>
> Hi Petr,
>
> I used r-bind but it's not working.
> Here is the code:
>
>
> arhar_18<-read.csv("D:/Ranjeet/IAMV6/input/yield/kharif_18-19_yield/Kharif_2018/arhar_18.csv")
> dacnet_17<-read.csv("D:/Ranjeet/IAMV6/input/yield/dacnet_yield_update till
> 2019.csv")
>
> for(cr in seq_along(dacnet_17$district)){
>     match(arhar_18$district, dacnet_17$district)
> }
>
> df3=rbind(arhar_18,dacnet_17)
> df3=df3[order(df3$district,df3$year),]
> x<-write.csv(df3,"df3.csv")
> view(x)
>
> On Wed, Jul 27, 2022 at 12:00 PM PIKAL Petr <mailto:petr.pikal at precheza.cz>
> wrote:
> Hi.
>
> From what you say, plain "rbind" could be used, if the columns in both sets
> are the same and in the same order. After that you can reorder the
> resulting
> data frame as you wish by "order". AFAIK for most functions row order in
> data frame does not matter.
>
> Cheers
> Petr
>
> > -----Original Message-----
> > From: R-help <mailto:r-help-bounces at r-project.org> On Behalf Of Ranjeet
> Kumar Jha
> > Sent: Monday, July 25, 2022 3:03 PM
> > To: R-help <mailto:r-help at r-project.org>
> > Subject: [R] Need to insert various rows of data from a data frame after
> > particular rows from another dataframe
> >
> > Hello Everyone,
> >
> > I have dataset in a particular format in "dacnet_yield_update till
> 2019.xlsx" file,
> > where I need to insert the data of rows 2018-2019 and
> > 2019-2020 for the districts those data are available in "Kharif crops
> yield_18-
> > 19.xlsx".  I need to insert these two rows of data belonging to every
> district, if
> > data is available in a later excel file, just after the particular crop
> group data for
> > the particular district.
> >
> > I have put the data file in the given link.
> > https://drive.google.com/drive/u/0/folders/1dNmGTI8_c9PK1QqmfIjnpbyzuiC
> > XgxFC
> >
> > Please help solving this problem.
> >
> > Regards and Thanks,
> > Ranjeet
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
> https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
> -----------------------------------------------------------
> Email: mailto:ranjeetjhaiitkgp at gmail.com
>
>
> "Simple Heart, Humble Attitude and Surrender to Supreme Being make our
> lives beautiful!"
>
>
>
>
> --
> Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
> https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
> -----------------------------------------------------------
> Email: mailto:ranjeetjhaiitkgp at gmail.com
>
>
> "Simple Heart, Humble Attitude and Surrender to Supreme Being make our
> lives beautiful!"
>
>

-- 
Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
-----------------------------------------------------------
Email: *ranjeetjhaiitkgp at gmail.com <ranjeetjhaiitkgp at gmail.com>*


*"Simple Heart, Humble Attitude and Surrender to Supreme Being make our
lives beautiful!"*

	[[alternative HTML version deleted]]


From r@njeetjh@||tkgp @end|ng |rom gm@||@com  Thu Jul 28 09:56:52 2022
From: r@njeetjh@||tkgp @end|ng |rom gm@||@com (Ranjeet Kumar Jha)
Date: Thu, 28 Jul 2022 13:26:52 +0530
Subject: [R] Need to insert various rows of data from a data frame after
 particular rows from another dataframe
In-Reply-To: <CAJfMH3pxAZ+z5aZQZMhL9rc63Zyvoms3Lw+DGH6_Ac0ZeVFzpQ@mail.gmail.com>
References: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
 <a172da90dd2448779581da58741b913e@SRVEXCHCM1302.precheza.cz>
 <CAJfMH3rF9RmJee+G0Y+3Re8U9BsxVkUhoZJhgQyGwYxrUxCSJQ@mail.gmail.com>
 <9c4386179ab24616915d2296b70a28d8@SRVEXCHCM1302.precheza.cz>
 <CAJfMH3oUD720UKT0DiuLWcPr4OvBEdDPeyxPwgpgXZhg3Pz5YA@mail.gmail.com>
 <e2f1cc08e8f04cb9bbf4ecb0a7362f40@SRVEXCHCM1302.precheza.cz>
 <CAJfMH3pxAZ+z5aZQZMhL9rc63Zyvoms3Lw+DGH6_Ac0ZeVFzpQ@mail.gmail.com>
Message-ID: <CAJfMH3oa-fbfap0QYx28bDoyhY4mG6aqZiqVjzCAdL+x3tKa4A@mail.gmail.com>

To check the error in columns when I use "intersection", it returns all the
8 columns. That means no difference in columns.

On Thu, Jul 28, 2022 at 1:12 PM Ranjeet Kumar Jha <
ranjeetjhaiitkgp at gmail.com> wrote:

> Hi Pikal,
>
> Now I have formatted the kharif data file in the same format as it is in
> the dacnet filecode:. However, "district.id" and "state.id " columns have
> "N/A" data that are appearing. If I use merge "rbind" function, it still
> throws this error:
>
> "Error in rbind(deparse.level, ...) :
>   numbers of columns of arguments do not match"
>
> code:
> df3=rbind(kharif_18_19,dacnet_17)
> df3=df3[order(df3$district,df3$year),]
> x<-write.csv(df3,"df3.csv")
> view(x)
>
> On Wed, Jul 27, 2022 at 3:18 PM PIKAL Petr <petr.pikal at precheza.cz> wrote:
>
>> Hallo,
>>
>> I do not understand what you really want and you do not help much.
>>
>> No error, no data, vague description of your problem, no effort to
>> explain it better. Only you can see your data, only you can see the error
>> message so we are clueless.
>>
>> Cheers
>> Petr
>>
>>
>> From: Ranjeet Kumar Jha <ranjeetjhaiitkgp at gmail.com>
>> Sent: Wednesday, July 27, 2022 11:01 AM
>> To: PIKAL Petr <petr.pikal at precheza.cz>
>> Cc: R-help <r-help at r-project.org>
>> Subject: Re: [R] Need to insert various rows of data from a data frame
>> after particular rows from another dataframe
>>
>> yeah rbind works find bur dataframe 2 has different format and then rbind
>> can be used with first dataframe. But here I am struggling to bring the 2nd
>> dataframe in the same format as the 1st one.
>>
>> On Wed, Jul 27, 2022 at 1:13 PM PIKAL Petr <mailto:petr.pikal at precheza.cz>
>> wrote:
>> Hi.
>>
>> ?is not working? is extremelly vague.
>>
>> 1.
>> What do you expect this code do?
>>
>> for(cr in seq_along(dacnet_17$district)){
>>     match(arhar_18$district, dacnet_17$district)
>> }
>>
>> See ?match and maybe also ??for? and try this
>>
>> x <- letters[1:5]
>> y <- sample(letters, 100, replace=T)
>> match(x,y)
>> [1] 45 16 24 13 71
>> for(i in 1:3) match(x,y)
>>
>> 2.
>> rbind works as expeceted
>>
>> arhar_18 <- data.frame(a=1:10, b=50, c=letters[1:10])
>> dacnet_17 <- data.frame(a=11:20, b=100, c=sample(letters,10))
>> df3=rbind(arhar_18,dacnet_17)
>>
>> if your data has common column order and type.
>>
>> To get more specific answer you need to ask specific question preferably
>> with some data included (most preferably by dput command) and error message.
>>
>> Cheers
>> Petr
>>
>>
>> From: Ranjeet Kumar Jha <mailto:ranjeetjhaiitkgp at gmail.com>
>> Sent: Wednesday, July 27, 2022 8:35 AM
>> To: PIKAL Petr <mailto:petr.pikal at precheza.cz>
>> Cc: R-help <mailto:r-help at r-project.org>
>> Subject: Re: [R] Need to insert various rows of data from a data frame
>> after particular rows from another dataframe
>>
>> Hi Petr,
>>
>> I used r-bind but it's not working.
>> Here is the code:
>>
>>
>> arhar_18<-read.csv("D:/Ranjeet/IAMV6/input/yield/kharif_18-19_yield/Kharif_2018/arhar_18.csv")
>> dacnet_17<-read.csv("D:/Ranjeet/IAMV6/input/yield/dacnet_yield_update
>> till 2019.csv")
>>
>> for(cr in seq_along(dacnet_17$district)){
>>     match(arhar_18$district, dacnet_17$district)
>> }
>>
>> df3=rbind(arhar_18,dacnet_17)
>> df3=df3[order(df3$district,df3$year),]
>> x<-write.csv(df3,"df3.csv")
>> view(x)
>>
>> On Wed, Jul 27, 2022 at 12:00 PM PIKAL Petr <mailto:
>> petr.pikal at precheza.cz> wrote:
>> Hi.
>>
>> From what you say, plain "rbind" could be used, if the columns in both
>> sets
>> are the same and in the same order. After that you can reorder the
>> resulting
>> data frame as you wish by "order". AFAIK for most functions row order in
>> data frame does not matter.
>>
>> Cheers
>> Petr
>>
>> > -----Original Message-----
>> > From: R-help <mailto:r-help-bounces at r-project.org> On Behalf Of
>> Ranjeet Kumar Jha
>> > Sent: Monday, July 25, 2022 3:03 PM
>> > To: R-help <mailto:r-help at r-project.org>
>> > Subject: [R] Need to insert various rows of data from a data frame after
>> > particular rows from another dataframe
>> >
>> > Hello Everyone,
>> >
>> > I have dataset in a particular format in "dacnet_yield_update till
>> 2019.xlsx" file,
>> > where I need to insert the data of rows 2018-2019 and
>> > 2019-2020 for the districts those data are available in "Kharif crops
>> yield_18-
>> > 19.xlsx".  I need to insert these two rows of data belonging to every
>> district, if
>> > data is available in a later excel file, just after the particular crop
>> group data for
>> > the particular district.
>> >
>> > I have put the data file in the given link.
>> > https://drive.google.com/drive/u/0/folders/1dNmGTI8_c9PK1QqmfIjnpbyzuiC
>> > XgxFC
>> >
>> > Please help solving this problem.
>> >
>> > Regards and Thanks,
>> > Ranjeet
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>> see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>> --
>> Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
>> https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
>> -----------------------------------------------------------
>> Email: mailto:ranjeetjhaiitkgp at gmail.com
>>
>>
>> "Simple Heart, Humble Attitude and Surrender to Supreme Being make our
>> lives beautiful!"
>>
>>
>>
>>
>> --
>> Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
>> https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
>> -----------------------------------------------------------
>> Email: mailto:ranjeetjhaiitkgp at gmail.com
>>
>>
>> "Simple Heart, Humble Attitude and Surrender to Supreme Being make our
>> lives beautiful!"
>>
>>
>
> --
> Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
> https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
> -----------------------------------------------------------
> Email: *ranjeetjhaiitkgp at gmail.com <ranjeetjhaiitkgp at gmail.com>*
>
>
> *"Simple Heart, Humble Attitude and Surrender to Supreme Being make our
> lives beautiful!"*
>
>

-- 
Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
-----------------------------------------------------------
Email: *ranjeetjhaiitkgp at gmail.com <ranjeetjhaiitkgp at gmail.com>*


*"Simple Heart, Humble Attitude and Surrender to Supreme Being make our
lives beautiful!"*

	[[alternative HTML version deleted]]


From petr@p|k@| @end|ng |rom prechez@@cz  Thu Jul 28 10:31:38 2022
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Thu, 28 Jul 2022 08:31:38 +0000
Subject: [R] Need to insert various rows of data from a data frame after
 particular rows from another dataframe
In-Reply-To: <CAJfMH3oa-fbfap0QYx28bDoyhY4mG6aqZiqVjzCAdL+x3tKa4A@mail.gmail.com>
References: <CAJfMH3rjqeqXkK3ttjNnje2e0rUh+xp2Cjk2XBB535Kt2z3-LQ@mail.gmail.com>
 <a172da90dd2448779581da58741b913e@SRVEXCHCM1302.precheza.cz>
 <CAJfMH3rF9RmJee+G0Y+3Re8U9BsxVkUhoZJhgQyGwYxrUxCSJQ@mail.gmail.com>
 <9c4386179ab24616915d2296b70a28d8@SRVEXCHCM1302.precheza.cz>
 <CAJfMH3oUD720UKT0DiuLWcPr4OvBEdDPeyxPwgpgXZhg3Pz5YA@mail.gmail.com>
 <e2f1cc08e8f04cb9bbf4ecb0a7362f40@SRVEXCHCM1302.precheza.cz>
 <CAJfMH3pxAZ+z5aZQZMhL9rc63Zyvoms3Lw+DGH6_Ac0ZeVFzpQ@mail.gmail.com>
 <CAJfMH3oa-fbfap0QYx28bDoyhY4mG6aqZiqVjzCAdL+x3tKa4A@mail.gmail.com>
Message-ID: <7013dc5d72bb4f7584c444f5ff911ebe@SRVEXCHCM1302.precheza.cz>

Hallo Ranjeet

You got some other answers which revealed that the original data were problematic. The error suggests that you have different number of columns in each data frame.  

df1 <- data.frame(a=letters[1:5], b=1:5)
df2 <- data.frame(a=LETTERS[1:5], b=letters[1:5], c=1:5)
> rbind(df1, df2)
Error in rbind(deparse.level, ...) : 
  numbers of columns of arguments do not match

So for rbind to work, both data frames should have the same number of columns and also the correct column in correct place. If it is not the case, you could use merge, but the missing values in common column, (in this case b) will be filled by NA.

df2 <- data.frame(a=LETTERS[1:10], b=letters[1:10], c=1:10)
merge(df1, df2, by.x="a", by.y="b", all=TRUE)
   a  b a.y  c
1  a  1   A  1
2  b  2   B  2
3  c  3   C  3
4  d  4   D  4
5  e  5   E  5
6  f NA   F  6
7  g NA   G  7
8  h NA   H  8
9  i NA   I  9
10 j NA   J 10
>

If you managed to get both, kharif and dacnet into R it would be worthwhile to show at least the structure of your data by sending result of 

str(kharif_18_19) and str(dacnet_17)
Structure should be the same.

Or better to send a chunk of the data as output from

dput(head(kharif_18_19),20) and dput(head(dacnet_17),20)

so everybody could inspect directly how those objects in your R look like.

Cheers
Petr

And BTW, do not use HTML formating, it could scramble your email as r-help list is plain text only.

From: Ranjeet Kumar Jha <ranjeetjhaiitkgp at gmail.com> 
Sent: Thursday, July 28, 2022 9:57 AM
To: PIKAL Petr <petr.pikal at precheza.cz>; R-help <r-help at r-project.org>
Subject: Re: [R] Need to insert various rows of data from a data frame after particular rows from another dataframe

To check the error in columns when I use "intersection", it returns all the 8 columns. That means no difference in columns.

On Thu, Jul 28, 2022 at 1:12 PM Ranjeet Kumar Jha <mailto:ranjeetjhaiitkgp at gmail.com> wrote:
Hi Pikal,

Now I have formatted the kharif data file in the same format as it is in the dacnet filecode:. However, "http://district.noclick_id" and "http://state.id " columns have "N/A" data that are appearing. If I use merge "rbind" function, it still throws this error:  

"Error in rbind(deparse.level, ...) : 
  numbers of columns of arguments do not match"

code:
df3=rbind(kharif_18_19,dacnet_17)
df3=df3[order(df3$district,df3$year),]
x<-write.csv(df3,"df3.csv")
view(x)

On Wed, Jul 27, 2022 at 3:18 PM PIKAL Petr <mailto:petr.pikal at precheza.cz> wrote:
Hallo, 

I do not understand what you really want and you do not help much. 

No error, no data, vague description of your problem, no effort to explain it better. Only you can see your data, only you can see the error message so we are clueless.

Cheers
Petr


From: Ranjeet Kumar Jha <mailto:ranjeetjhaiitkgp at gmail.com> 
Sent: Wednesday, July 27, 2022 11:01 AM
To: PIKAL Petr <mailto:petr.pikal at precheza.cz>
Cc: R-help <mailto:r-help at r-project.org>
Subject: Re: [R] Need to insert various rows of data from a data frame after particular rows from another dataframe

yeah rbind works find bur dataframe 2 has different format and then rbind can be used with first dataframe. But here I am struggling to bring the 2nd dataframe in the same format as the 1st one.

On Wed, Jul 27, 2022 at 1:13 PM PIKAL Petr <mailto:mailto:petr.pikal at precheza.cz> wrote:
Hi.

?is not working? is extremelly vague.

1.
What do you expect this code do?

for(cr in seq_along(dacnet_17$district)){
    match(arhar_18$district, dacnet_17$district)
}

See ?match and maybe also ??for? and try this

x <- letters[1:5]
y <- sample(letters, 100, replace=T)
match(x,y)
[1] 45 16 24 13 71
for(i in 1:3) match(x,y)

2.
rbind works as expeceted

arhar_18 <- data.frame(a=1:10, b=50, c=letters[1:10])
dacnet_17 <- data.frame(a=11:20, b=100, c=sample(letters,10))
df3=rbind(arhar_18,dacnet_17)

if your data has common column order and type.

To get more specific answer you need to ask specific question preferably with some data included (most preferably by dput command) and error message.

Cheers
Petr


From: Ranjeet Kumar Jha <mailto:mailto:ranjeetjhaiitkgp at gmail.com> 
Sent: Wednesday, July 27, 2022 8:35 AM
To: PIKAL Petr <mailto:mailto:petr.pikal at precheza.cz>
Cc: R-help <mailto:mailto:r-help at r-project.org>
Subject: Re: [R] Need to insert various rows of data from a data frame after particular rows from another dataframe

Hi Petr,

I used r-bind but it's not working.
Here is the code:

arhar_18<-read.csv("D:/Ranjeet/IAMV6/input/yield/kharif_18-19_yield/Kharif_2018/arhar_18.csv")
dacnet_17<-read.csv("D:/Ranjeet/IAMV6/input/yield/dacnet_yield_update till 2019.csv")

for(cr in seq_along(dacnet_17$district)){
    match(arhar_18$district, dacnet_17$district)
}

df3=rbind(arhar_18,dacnet_17)
df3=df3[order(df3$district,df3$year),]
x<-write.csv(df3,"df3.csv")
view(x)

On Wed, Jul 27, 2022 at 12:00 PM PIKAL Petr <mailto:mailto:petr.pikal at precheza.cz> wrote:
Hi.

>From what you say, plain "rbind" could be used, if the columns in both sets
are the same and in the same order. After that you can reorder the resulting
data frame as you wish by "order". AFAIK for most functions row order in
data frame does not matter.

Cheers
Petr

> -----Original Message-----
> From: R-help <mailto:mailto:r-help-bounces at r-project.org> On Behalf Of Ranjeet Kumar Jha
> Sent: Monday, July 25, 2022 3:03 PM
> To: R-help <mailto:mailto:r-help at r-project.org>
> Subject: [R] Need to insert various rows of data from a data frame after
> particular rows from another dataframe
> 
> Hello Everyone,
> 
> I have dataset in a particular format in "dacnet_yield_update till
2019.xlsx" file,
> where I need to insert the data of rows 2018-2019 and
> 2019-2020 for the districts those data are available in "Kharif crops
yield_18-
> 19.xlsx".  I need to insert these two rows of data belonging to every
district, if
> data is available in a later excel file, just after the particular crop
group data for
> the particular district.
> 
> I have put the data file in the given link.
> https://drive.google.com/drive/u/0/folders/1dNmGTI8_c9PK1QqmfIjnpbyzuiC
> XgxFC
> 
> Please help solving this problem.
> 
> Regards and Thanks,
> Ranjeet
> 
>       [[alternative HTML version deleted]]
> 
> ______________________________________________
> mailto:mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
-----------------------------------------------------------
Email: mailto:mailto:ranjeetjhaiitkgp at gmail.com


"Simple Heart, Humble Attitude and Surrender to Supreme Being make our lives beautiful!"




-- 
Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
-----------------------------------------------------------
Email: mailto:mailto:ranjeetjhaiitkgp at gmail.com


"Simple Heart, Humble Attitude and Surrender to Supreme Being make our lives beautiful!"



-- 
Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
-----------------------------------------------------------
Email: mailto:ranjeetjhaiitkgp at gmail.com


"Simple Heart, Humble Attitude and Surrender to Supreme Being make our lives beautiful!"




-- 
Ranjeet  Kumar Jha, M.Tech. (IIT Kharagpur), Ph.D. (USA)
https://www.linkedin.com/in/ranjeet-kumar-jha-ph-d-usa-73a5aa56
-----------------------------------------------------------
Email: mailto:ranjeetjhaiitkgp at gmail.com


"Simple Heart, Humble Attitude and Surrender to Supreme Being make our lives beautiful!"


From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Thu Jul 28 12:52:55 2022
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Thu, 28 Jul 2022 05:52:55 -0500
Subject: [R] Parsing XML?
In-Reply-To: <CABcYAdLxyWZp5QGH_-3NNZ4zRu3Dp-qF1NK399VD-Rej=Fg-bg@mail.gmail.com>
References: <20220727145903.4338c449692f703b@ask.loc.gov>
 <7743581c-0959-1df6-bb31-f140161f83e1@effectivedefense.org>
 <CABcYAdLxyWZp5QGH_-3NNZ4zRu3Dp-qF1NK399VD-Rej=Fg-bg@mail.gmail.com>
Message-ID: <2dc13b7d-17df-21ce-6161-fd593214d9ab@effectivedefense.org>

Hi, Richard et al.:


On 7/28/22 1:50 AM, Richard O'Keefe wrote:
> What do you mean by "a list that I can understand"?
> A quick tally of the number of XML elements by identifier:
> 1 echoedSearchRetrieveRequest
> 1 frbrGrouping
> 1 maximumRecords
> 1 nextRecordPosition
> 1 numberOfRecords
> 1 query
> 1 records
> 1 resultSetIdleTime
> 1 searchRetrieveResponse
> 1 servicelevel
> 1 sortKeys
> 1 startRecord
> 1 wskey
> 2 version
> 50 leader
> 50 recordData
> 51 recordPacking
> 51 recordSchema
> 100 record
> 105 controlfield
> 923 datafield
> 1900 subfield


	  How did you get that?


	  Please forgive me for being so dense.  I've done several web searches 
and tried to work several tutorials, etc., without so far seeing what I 
might do that could be informative.


	  Even this list of "XML elements by identifiers" STILL does not 
include things like the name of the newspaper and publisher plus start 
and end dates.  I believe these fields are there, but I can't see how to 
parse them.  I earlier parsed a JSON version of essentially the same 
dataset.  However, the JSON version seemed not to distinguish between 
newspapers that were still publishing and those for which the end date 
was unknown.  My contact at the Library of Congress then suggested I 
parse the XML version.


	  Thanks,
	  Spencer

> 
> What of this information do you actually want?
> The elements of the list should be what?
> 
> 
> On Thu, 28 Jul 2022 at 08:52, Spencer Graves 
> <spencer.graves at effectivedefense.org 
> <mailto:spencer.graves at effectivedefense.org>> wrote:
> 
>     Hello, All:
> 
> 
>      ? ? ? ? ? What would you suggest I do to parse the following XML
>     file into a
>     list that I can understand:
> 
> 
>     XMLfile <-
>     "https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/ndnp_Alabama_all-yrs_e_0001_0050.xml
>     <https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/ndnp_Alabama_all-yrs_e_0001_0050.xml>"
> 
> 
> 
> 
>      ? ? ? ? ? This is the first of 6666 XML files containing "U.S.
>     Newspaper
>     Directory" maintained by the US Library of Congress discussed in the
>     thread below.? I've tried various things using the XML and xml2.
> 
> 
>     XMLdata <- xml2::read_xml(XMLfile)
>     str(XMLdata)
>     XMLdat <- XML::xmlParse(XMLdata)
>     str(XMLdat)
>     XMLtxt <- xml2::xml_text(XMLdata)
>     nchar(XMLtxt)
>     #[1] 29415
> 
> 
>      ? ? ? ? ? Someplace there's a schema for this.? I don't know if
>     it's embedded
>     in this XML file or in a separate file.? If it's in a separate file,
>     how
>     could I describe it to my contacts with the Library of Congress so they
>     would understand what I needed and could help me get it.
> 
> 
>      ? ? ? ? ? Thanks,
>      ? ? ? ? ? Spencer Graves
> 
> 
>     p.s.? All 29415 characters in XMLtext appear in the thread below.
> 
> 
>     -------- Forwarded Message --------
>     Subject:? ? ? ? [Newspapers and Current Periodicals] How can I get
>     counts of
>     the numbers of newspapers by year in the US, and preferably also
>     elsewhere? A search of "U.S. Newspaper Directory,
>     Date:? ?Wed, 27 Jul 2022 14:59:03 +0000
>     From:? ?Kerry Huller <serials at ask.loc.gov <mailto:serials at ask.loc.gov>>
>     To:? ? ?Spencer Graves <spencer.graves at effectivedefense.org
>     <mailto:spencer.graves at effectivedefense.org>>
>     CC: twes at loc.gov <mailto:twes at loc.gov>
> 
> 
> 
>     --# Type your reply above this line #--
> 
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jul 27 2022, 10:59am via System
> 
>     Hello Spencer,
> 
>     So, when I view the xml, I'm actually looking at it in XML editor
>     software, so I can view the tags and it's structured neatly. I've
>     copied
>     and pasted the text from the beginning of the file and the first
>     newspaper title below from my XML editor:
> 
>     <?xml version="1.0" encoding="UTF-8" standalone="no"?>
>     <?xml-stylesheet type='text/xsl'
>     href='/webservices/catalog/xsl/searchRetrieveResponse.xsl'?>
> 
>     <searchRetrieveResponse xmlns="http://www.loc.gov/zing/srw/
>     <http://www.loc.gov/zing/srw/>"
>     xmlns:oclcterms="http://purl.org/oclc/terms/
>     <http://purl.org/oclc/terms/>"
>     xmlns:dc="http://purl.org/dc/elements/1.1/
>     <http://purl.org/dc/elements/1.1/>"
>     xmlns:diag="http://www.loc.gov/zing/srw/diagnostic/
>     <http://www.loc.gov/zing/srw/diagnostic/>"
>     xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance
>     <http://www.w3.org/2001/XMLSchema-instance>">
>     <version>1.1</version>
>     <numberOfRecords>2250</numberOfRecords>
>     <records>
>     <record>
>     <recordSchema>info:srw/schema/1/marcxml</recordSchema>
>     <recordPacking>xml</recordPacking>
>     <recordData>
>     <record xmlns="http://www.loc.gov/MARC21/slim
>     <http://www.loc.gov/MARC21/slim>">
>      ? ? ? <leader>00000nas a22000007i 4500</leader>
>      ? ? ? <controlfield tag="001">1030438981</controlfield>
>      ? ? ? <controlfield tag="008">180404c20159999aluwr n ? ? ? 0 ? a0eng
>      ??</controlfield>
>      ? ? ? <datafield ind1=" " ind2=" " tag="010">
>      ? ? ? ? <subfield code="a"> ?2018200464</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1=" " ind2=" " tag="040">
>      ? ? ? ? <subfield code="a">DLC</subfield>
>      ? ? ? ? <subfield code="e">rda</subfield>
>      ? ? ? ? <subfield code="c">DLC</subfield>
>      ? ? ? ? <subfield code="b">eng</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1=" " ind2=" " tag="012">
>      ? ? ? ? <subfield code="m">1</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1="0" ind2=" " tag="022">
>      ? ? ? ? <subfield code="a">2577-5316</subfield>
>      ? ? ? ? <subfield code="2">1</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1=" " ind2=" " tag="032">
>      ? ? ? ? <subfield code="a">021110</subfield>
>      ? ? ? ? <subfield code="b">USPS</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1=" " ind2=" " tag="037">
>      ? ? ? ? <subfield code="b">711 Alabama Avenue, Selma, AL
>     36701</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1=" " ind2=" " tag="042">
>      ? ? ? ? <subfield code="a">nsdp</subfield>
>      ? ? ? ? <subfield code="a">pcc</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1="1" ind2="0" tag="050">
>      ? ? ? ? <subfield code="a">ISSN RECORD</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1="1" ind2="0" tag="082">
>      ? ? ? ? <subfield code="a">071</subfield>
>      ? ? ? ? <subfield code="2">15</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1=" " ind2="0" tag="222">
>      ? ? ? ? <subfield code="a">Selma sun</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1="0" ind2="0" tag="245">
>      ? ? ? ? <subfield code="a">Selma sun.</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1=" " ind2="1" tag="264">
>      ? ? ? ? <subfield code="a">Selma, AL :</subfield>
>      ? ? ? ? <subfield code="b">North Shore Press, LLC</subfield>
>      ? ? ? ? <subfield code="c">2016-</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1=" " ind2=" " tag="310">
>      ? ? ? ? <subfield code="a">Weekly</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1=" " ind2=" " tag="336">
>      ? ? ? ? <subfield code="a">text</subfield>
>      ? ? ? ? <subfield code="b">txt</subfield>
>      ? ? ? ? <subfield code="2">rdacontent</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1=" " ind2=" " tag="337">
>      ? ? ? ? <subfield code="a">unmediated</subfield>
>      ? ? ? ? <subfield code="b">n</subfield>
>      ? ? ? ? <subfield code="2">rdamedia</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1=" " ind2=" " tag="338">
>      ? ? ? ? <subfield code="a">volume</subfield>
>      ? ? ? ? <subfield code="b">nc</subfield>
>      ? ? ? ? <subfield code="2">rdacarrier</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1="1" ind2=" " tag="362">
>      ? ? ? ? <subfield code="a">Began in 2015.</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1=" " ind2=" " tag="588">
>      ? ? ? ? <subfield code="a">Description based on: Volume 2, Issue 40
>     (October 5, 2017) (surrogate); title from caption.</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1=" " ind2=" " tag="588">
>      ? ? ? ? <subfield code="a">Latest issue consulted: Volume 2, Issue 40
>     (October 5, 2017).</subfield>
>      ? ? ? </datafield>
>      ? ? ? <datafield ind1=" " ind2=" " tag="752">
>      ? ? ? ? <subfield code="a">United States</subfield>
>      ? ? ? ? <subfield code="b">Alabama</subfield>
>      ? ? ? ? <subfield code="c">Dallas</subfield>
>      ? ? ? ? <subfield code="d">Selma.</subfield>
>      ? ? ? </datafield>
>      ? ? </record>
>     </recordData>
>     </record>
> 
>     When I view the records in the XML editor, these 2 lines below do begin
>     each of the records for each individual title, but of course this is
>     including the xml tags:
> 
>     <recordSchema>info:srw/schema/1/marcxml</recordSchema>
>     <recordPacking>xml</recordPacking>
> 
>     Hopefully this helps you decide where to break or parse each record.
> 
>     On another note, I just noticed as well that at the top of this first
>     file it lists the total number of records for the Alabama grouping -
>     2250. This also appeared to be the case for the Alaska records when I
>     took a look at the first one for that state. I imagine that should be
>     consistent throughout each "grouping" of records.
> 
>     Let me know if you have follow-up questions!
> 
>     Best wishes,
> 
>     Kerry Huller
>     Newspaper & Current Periodical Reading Room
>     Serial & Government Publications Division
>     Library of Congress
> 
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jul 27 2022, 10:21am via Email
> 
>     Hi, Kerry:
> 
> 
>     Thanks. I understand the chunking in files of at most 50. I've read
>     the first file "ndnp_Alabama_all-yrs_e_0001_0050.xml" into a string of
>     29415 characters, copied below. Might you have any suggestions on the
>     next step in parsing this? Staring at it now, it looks splitting on
>     "info:srw/schema/1/marcxmlxml" might convert the 29415 characters into
>     shorter chunks, each of which could then be parsed further.
> 
> 
>     This is not as bad as reading ancient Egyptian heiroglyphics without
>     the Rosetta Stone, but I wondered if you might have something that could
>     make this work easier and more reliable? I guess I could compare with
>     what I already read as JSON ;-)
> 
> 
>     Thanks,
>     Spencer Graves
> 
> 
>     "1.12250info:srw/schema/1/marcxmlxml00000nas a22000007i
>     45001030438981180404c20159999aluwr n 0 a0eng
>     2018200464DLCrdaDLCeng12577-53161021110USPS711 Alabama Avenue, Selma, AL
>     36701nsdppccISSN RECORD07115Selma sunSelma sun.Selma, AL :North Shore
>     Press,
>     LLC2016-WeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan
>     in
>     2015.Description based on: Volume 2, Issue 40 (October 5, 2017)
>     (surrogate); title from caption.Latest issue consulted: Volume 2, Issue
>     40 (October 5, 2017).United
>     StatesAlabamaDallasSelma.info:srw/schema/1/marcxmlxml00000cas a22000007a
>     4500502150053100127c20109999aluwr n 0 a0eng
>     2010200019DLCengDLCDLCOCLCQ112153-18111750USPSB & C Publishing, LLC,
>     3514 Martin St. S. Ste 104, Cropwell, AL 35054pccnsdpISSN RECORDSt.
>     Clair County news (Cropwell, Ala.)St. Clair County news(Cropwell,
>     Ala.)St. Clair County news.Cropwell, AL :B & C Pub.WeeklyBegan in
>     2010.Description based on: Nov. 4, 2010 (surrogate); title from
>     caption.info:srw/schema/1/marcxmlxml00000cas a22000007a
>     4500426491872090720c20099999alumr n 0 a0eng
>     2009203372DLCengDLCOCLCQ12150-346X2150-346X1AU at 000044489617NZ116076352Devon
>     Applewhite/Applewhite Publishing Co., 1910 Honeysuckle Rd., #N183,
>     Dothan, AL 36305mscnsdpISSN RECORD30514Triangle tribune(Dothan,
>     Ala.)Triangle tribune.Dothan, AL :Applewhite Pub. CoMonthlyBegan with
>     vol. 1, issue 1 (May 2009).\"Connecting the Tri-State African -American
>     Community.\"Description based on: Vol. 1, issue 1 (May 2009); title from
>     masthead.Applewhite, Devon.United StatesAlabama.United
>     StatesGeorgia.United StatesFlorida.info:srw/schema/1/marcxmlxml00000cas
>     a22000007a 4500289017315081219c20089999aluwr n | a0eng c
>     2008213218NSDengNSDOCLCQDLCOCLCQ111945-93191945-93191005270USPSSpringhill
>     Publications,
>     LLC, P.O. Box 186, Greenville, AL 36037nsdppccISSN RECORD07014Greenville
>     standardThe Greenville standard.Greenville, AL :Springhill
>     PublicationsWeeklytexttxtrdacontentunmediatednrdamediaBegan with vol. 1,
>     issue 1 (Sept. 3, 2008)Description based on surrogate of: Vol. 1, no. 15
>     (Dec. 18, 2008); title from masthead (publisher's Web site, viewed Dec.
>     19, 2008).Latest issue consulted: Vol. 1, no. 99 (July 27, 2011)
>     (surrogate).info:srw/schema/1/marcxmlxml00000cas a22000007a
>     4500123539969070426c20079999aluwr ne 0 a0eng c
>     2007212138NSDengNSDNSDOCLCQ101936-95571936-95571The Western Tribune,
>     1530 Third Ave. N., Bessemer, AL 35020mscnsdpISSN RECORDWestern tribune
>     (Bessemer, Ala.)The Western tribune(Bessemer, Ala.)The Western
>     tribune.Bessemer, Ala. :D-Med, Inc.v.WeeklyBegan in 2007.Description
>     based on: May 23, 2007 (surrogate); title from
>     caption.AU at 000041575341info:srw/schema/1/marcxmlxml00000cas a22000007a
>     4500226300653080425c20079999aluwr ne | a0eng
>     2008212112NSDengNSDNSDOCLCQ11942-20751942-20751nsdppccISSN RECORDThe
>     corridor messengerThe corridor messenger.Carbon Hill, AL :Corridor
>     Messenger, Inc.WeeklyBegan with vol. 1, issue (10.03.2007).Description
>     based on: 1st issue.United StatesAlabamaWalkerCarbon
>     Hill.http://www
>     <http://www>.corridormessenger.cominfo:srw/schema/1/marcxmlxml00000cas
>     a22000007a
>     450077560432070109c20069999aluwr ne 0 a0eng c
>     2007213400NSDengNSDOCLCQAUBRNOCLCOOCLCFa01935-37901935-37901AU at 000041190283The
> 
> 
>     Auburn Villager, P.O. Box 1633, Auburn, AL 36831-1633pccnsdpISSN
>     RECORDThe Auburn villagerThe Auburn villager.Auburn, AL :Auburn
>     Villagerv.WeeklyBegan in 2006.Description based on: Vol. 1, no. 4 (July
>     20, 2006) (surrogate); title from caption.Auburn (Ala.)Newspapers.Lee
>     County (Ala.)Newspapers.AlabamaAuburn.fast(OCoLC)fst01209634AlabamaLee
>     County.fast(OCoLC)fst01211930Newspapers.fast(OCoLC)fst01423814United
>     StatesAlabamaLeeAuburn.info:srw/schema/1/marcxmlxml00000cas a2200000Ii
>     4500872286785m o d s cr mn|---a||||140311c20069999alucr n o b
>     s0 a0eng cABCengrdaABCABCOCLCFLD59.13University of Alabama at
>     Birmingham.The eReporter.[Birmingham, Alabama] :The University of
>     Alabama at Birmingham,[2006]-[Birmingham, Alabama] :Offices of Public
>     Relations & Marketing and Information Technology1 online resource2
>     issues weeklytexttxtrdacontentcomputercrdamediaonline
>     resourcecrrdacarrierSeptember 19, 2006-\"The eReporter is an official
>     communication of The University of Alabama at Birmingham, companion to
>     the UAB Reporter and recommended alternative to mass e-mails.\"Issues
>     for <March 11, 2014- published and distributed via e-mail subscription
>     on Tuesdays and Fridays.Description based on: September 19, 2006; title
>     from title screen (viewed March 12, 2014).University of Alabama at
>     BirminghamPeriodicals.Periodicals.fast(OCoLC)fst01411641University of
>     Alabama at Birmingham.fast(OCoLC)fst00645114University of Alabama at
>     Birmingham.Office of Public Relations and Marketing.University of
>     Alabama at Birmingham.Information Technology.2006-2012, companion
>     to:University of Alabama at Birmingham.UAB
>     reporter.(OCoLC)32435748Archived
>     issueshttp://hatteras.dpo.uab.edu/cgi-bin/ereporter.cgiinfo:srw/schema/1/marcxmlxml00000cas
>     <http://hatteras.dpo.uab.edu/cgi-bin/ereporter.cgiinfo:srw/schema/1/marcxmlxml00000cas>
> 
> 
>     a22000007a 4500166387050070829c20059999aluwr ne | a0eng c
>     2007215501NSDengNSDOCLCQ11939-68991939-68991The Wilkie Clark Memorial
>     Foundation, P.O. Box 514, Roanoke, AL 36274$30.00nsdpmscISSN
>     RECORD305.89614People's voice (Roanoke, Ala.)The people's voice(Roanoke,
>     Ala.)The people's voice.Roanoke, AL :Wilkie Clark Memorial
>     Foundationv.WeeklyBegan with vol. 1, no. 1 in 2005.Description based on:
>     Vol. 2, no. 20 (Apr. 20, 2007); title from caption.Wilkie Clark Memorial
>     Foundation.United
>     StatesAlabamaRandolphRoanoke.AU at 000042141390info:srw/schema/1/marcxmlxml00000nas
> 
> 
>     a22000007i 45001124677787191021c20uu9999aluwr ne | a0eng
>     2019202521DLCengrdaDLC12689-3258122730USPSNorth Jackson Press, 42950 Hwy
>     72, Suite 406, Stevenson, AL 35772nsdppccISSN RECORD071.323North Jackson
>     pressNorth Jackson press.Stevenson, AL :Caney Creek Publications
>     LLCWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierDescription
>     based on surrogate of: Volume 1, number 36 (October 11, 2019); title
>     from masthead.Latest issue consulted: Volume 1, number 36 (October 11,
>     2019) (Surrogate).United
>     StatesAlabamaJacksonStevensoninfo:srw/schema/1/marcxmlxml00000cas
>     a2200000 a 4500226315099080428d19981998aluwr ne | 0eng c
>     2008233691GUAengGUAOCLCQOCLCFOCLCO39911644pccn-us-gaThe Dekalb
>     news.Birmingham, Ala. :Community newspaper holdings Inc.v.WeeklyBegan
>     with 1st year, no. 1 (Apr. 1, 1998); ceased with 1st year, no. 31 (Oct.
>     28, 1998).Final issue consulted.Description based on first issue; title
>     from caption.Decatur (Ga.)Newspapers.DeKalb County
>     (Ga.)Newspapers.Newspapers.fast(OCoLC)fst01423814GeorgiaDecatur.fast(OCoLC)fst01226234GeorgiaDeKalb
> 
> 
>     County.fast(OCoLC)fst01215288United
>     StatesGeorgiaDeKalbDecatur.Decatur-DeKalb news/era(DLC)sn
>     89053661(OCoLC)19946163info:srw/schema/1/marcxmlxml00000cas a2200000 i
>     450050263311m o d cr cn|||||||||020730c19979999alu x neo
>     0 a0eng c
>     2015238492AMHengrdapnAMHOCLCQOCLCFOCLCOIULOCLHTMOCLCQCOODLC66460694810970435082687-93791AU at 000050711528OCLCS45109pccnsdpn-us---AP2.B5707023Birmingham
> 
> 
>     weekly (Online)Birmingham weekly(Online)Birmingham weekly.Birmingham, AL
>     :Birmingham Weekly1 online resourceIrregular,Feb. 16-28,
>     2012-Weekly,Sept. 4-11, 1997-Feb. 9-16,
>     2012texttxtrdacontentcomputercrdamediaonline resourcecrrdacarrierBegan
>     with vol. 1, issue 1 (Sept. 4-11, 1997).\"City news, views &
>     entertainment\"--Cover.Numbering dropped in Mar. 2012.Also issued in
>     print.Description based on: Publication information from ProQuest; title
>     from web page (viewed June 18, 2015).Latest issue consulted: Aug. 15-20,
>     2012.Birmingham (Ala.)Newspapers.Internet resources.Electronic
>     journals.AlabamaBirmingham.fast(OCoLC)fst01204958Newspapers.fast(OCoLC)fst01423814United
> 
> 
>     StatesAlabamaBirmingham.Print version:Birmingham
>     Weekly(OCoLC)39271050http://apw.softlineweb.com/http://WC2VB5MT8E.search.serialssolutions.com/?sid=sersol&SS_jc=JC_000051895&title=Birmingham+Weeklyinfo:srw/schema/1/marcxmlxml00000cas
>     <http://apw.softlineweb.com/http://WC2VB5MT8E.search.serialssolutions.com/?sid=sersol&SS_jc=JC_000051895&title=Birmingham+Weeklyinfo:srw/schema/1/marcxmlxml00000cas>
> 
> 
>     a22000007a 450031471314941116d19941995aluwr ne 0 a0eng csn
>     94003083
>     NSDengNSDANEOCLCQOCLCFOCLCOOCLCQ11079-65411079-65411nsdppccn-us-akSoutheast
>     shopperSoutheast shopper.Juneau, Alaska :Kemper
>     Communications,1994-volumesWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol.
> 
> 
>     1, no. 1 (Nov. 16, 1994)-Ceased in Feb. 1995.Juneau
>     (Alaska)Newspapers.AlaskaJuneau.fast(OCoLC)fst01213587Newspapers.fast(OCoLC)fst01423814United
> 
> 
>     StatesAlaskaJuneau.AU at 000011356572info:srw/schema/1/marcxmlxml00000cas
>     a22000008a 450027910515930413c19949999alumr n 0 a0eng dsn
>     93002581 NSDengNSDOCLCQ11069-06621Birmingham Tribune, 216 Ave. T. Pratt
>     City, Birmingham, AL 35214nsdpBirmingham tribuneBirmingham
>     tribune.Birmingham, Ala. :Kervin
>     Fondren9501volumesMonthlytexttxtrdacontentunmediatednrdamediavolumencrdacarrierPREPUB:
> 
> 
>     publication expected Jan.
>     1995AU at 000025863987info:srw/schema/1/marcxmlxml00000cas a22000007a
>     450026199931920716d19922013alumr ne 0 a0eng csn 92003357
>     NSDengNSDOCLOCLCQDLC011064-01341064-01341Black & White, POB 13215,
>     Birmingham, AL 35202-3215nsdppccBlack & white (Birmingham, Ala.)Black &
>     white(Birmingham, Ala.)Black & white.Black and whiteBirmingham, Ala.
>     :Black & White, Inc.v.Biweekly,Oct. 2, 1997-Monthly,May 1, 1992-Sept.
>     1997Began in May 1992; ceased with Jan. 10, 2013.\"Birmingham's New City
>     paper.\"Description based on: June 1992.Latest issue consulted: No. 67
>     (Oct. 16, 1997) (surrogate).info:srw/schema/1/marcxmlxml00000cas
>     a2200000 a 450032145723950314d19901999alumr ne 0 a0eng csn
>     95068755
>     MGNengMGNNSDCLUOCLCQOCLCFOCLCOOCLCA971211082-34841082-34841AU at 000011579542nsdppccn-us-alF335.J5S68The
> 
> 
>     Southern shofarThe Southern shofar.Birmingham, AL :L. Brook,-[1999]v.
>     :ill. ;35 cm.MonthlyBegan in 1990.-v. 9, issue 9 (Aug./Sept. 1999).\"The
>     monthly newspaper of Alabama's Jewish community.\"Some issues also
>     available on the Internet via the World Wide Web.Description based on:
>     Vol. 3, issue 11 (Oct. 1993).Jewish newspapersAlabama.Jewish
>     newspapers.fast(OCoLC)fst00982872Alabama.fast(OCoLC)fst01204694United
>     StatesAlabamaJeffersonBirmingham.Deep South Jewish voice(DLC)sn
>     99018499(OCoLC)42431704CLUhttp://bibpurl.oclc.org/web/719http://www.bham.net/shofar/info:srw/schema/1/marcxmlxml00000cas
>     <http://bibpurl.oclc.org/web/719http://www.bham.net/shofar/info:srw/schema/1/marcxmlxml00000cas>
> 
> 
>     a22000007a 450021265141900326c19909999aluwr ne 0 a0eng csn
>     90099004 AARengAARCPNNSDOCLCQ11050-08981050-08981005022USPSE.O.N., Inc.,
>     Main St., Eclectic, AL 36024pccnsdpISSN RECORDThe Eclectic observerThe
>     Eclectic observer.Eclectic, Ala. :E.O.N., Inc.,1990-v.WeeklyVol. 1, no.
>     1 (Feb. 22, 1990)-Published by: Price Publications, Inc., <2006->Latest
>     issue consulted: Vol. 17, no. 1 (Jan. 5, 2006).United
>     StatesAlabamaElmoreEclectic.AU at 000040212446info:srw/schema/1/marcxmlxml00000cas
> 
> 
>     a22000007a 450021214781900314c19909999aluir ne 0 a0eng csn
>     90002457 AAAengAAANSDOCLCQ111050-20841050-20841931180USPSClanton
>     Newspapers, 1109 Seventh St., N., PO Box 1379, Clanton, AL
>     35045nsdppccn-us-alThe Clanton advertiserThe Clanton
>     advertiser.AdvertiserClanton, Ala. :Clanton Newspapersv. :ill. ;58
>     cm.Three no. a week,<May 13, 1992->Semiweekly,<Apr. 4, 1990->Began in
>     Jan. 1990.Description based on: Vol. 19, no. 27 (Wed., Apr. 4,
>     1990).Latest issue consulted: Vol. 22, no. 58 (May 13, 1992).United
>     StatesAlabamaChiltonClanton.Independent advertiser (Clanton,
>     Ala.)(OCoLC)21214732AU at 000025908452info:srw/schema/1/marcxmlxml00000cas
>     a2200000 a 450021214814900314c19909999aluwr ne 0 a0eng dsn
>     90099009 AAAengAAACPNNSDOCLCQ11056-32881056-32881505740USPSThe Blount
>     Countian, 3rd St. at Washington Ave., PO Box 310, Oneonta, AL
>     35121mscnsdpn-us-alThe Blount countianThe Blount countian.Oneonta, Ala.
>     :Southern Democrat, Inc.,1990-v. :ill.WeeklyVol. 1, no. 1 (Jan. 3,
>     1990)-Editor: Molly Howard Ryan, 1990-Latest issue consulted: Vol. 1,
>     no. 36 (Sept. 5, 1990).Ryan, Molly Howard.United
>     StatesAlabamaBlountOneonta.Southern Democrat(DLC)sn
>     85044741(OCoLC)12038577AU at 000025884049info:srw/schema/1/marcxmlxml00000cas
>     a22000007a 450022413044900920c19909999aluwr ne 0 a0eng dsn
>     90099011
>     AARengAARCPNNSDNSTOCLCQ92081707011191053-91231053-91231314240USPSmscnsdpThe
>     Clay times-journalThe Clay times-journal.Lineville, Ala. :C.L.
>     Proctor,1990-v.WeeklyVol. 1, no. 1 (Sept. 6, 1990)-United
>     StatesAlabamaClayLineville.Ashland progress(DLC)sn 85044701Lineville
>     tribune(DLC)sn 85044702AUinfo:srw/schema/1/marcxmlxml00000cas a22000007a
>     450021265218900326c19909999aluwr ne 0 0eng dsn 90099005
>     AARengAARCPNOCLCQmscTrussville news-journal.Trussville, Ala. :Mike
>     Mitchell,1990-v.BimonthlyVol. 1, no. 1 (Feb. 20, 1990)-United
>     StatesAlabamaJeffersonTrussville.info:srw/schema/1/marcxmlxml00000cas
>     a22000007a 450022301035900831c19909999aluwr ne 0 0eng dsn
>     90099010 AARengAARCPNOCLCQmscWeaver tribune.Oxford, Ala. :Cheaha
>     Pub.,1990-v.WeeklyVol. 1, no. 1 (July 19, 1990)-United
>     StatesAlabamaCalhounWeaver.United
>     StatesAlabamaCalhounOxford.info:srw/schema/1/marcxmlxml00000cas
>     a22000007a 450015155895870205c19879999aludr ne 0 a0eng csn
>     87050045
>     AAAengAAACPNNSDDLCCPNNSDDLCCPNDLCOCLDLCOCLCQOCLCFOCLCQ19261126829944596670892-44570892-44571AU at 000020456714360980USPSThe
> 
> 
>     Advertiser, P.O. Box 1000, Montgomery, AL
>     36192pccnsdpn-us-alNewspaperMontgomery advertiser (Montgomery, Ala. :
>     1987)The Montgomery advertiser(1987)The Montgomery advertiser.Montgomery
>     advertiser & the Alabama journalSunday Montgomery advertiserMontgomery,
>     Ala. :Advertiser Co.,1987-volumes
>     :illustrationsDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrier160th
> 
> 
>     year, no. 1 (Jan. 2, 1987)-On Saturdays, Sundays and holidays a combined
>     edition is published with the Alabama journal, and called: Montgomery
>     advertiser and the Alabama journal, Jan. 3, 1987, and: Alabama journal
>     and Montgomery advertiser, Jan. 4, 1987-Feb. 25, 1990.Issues for Sunday
>     called: Sunday Montgomery advertiser, Mar. 4, 1990-Issues for Saturday,
>     Sunday and holidays have their own numbering, Jan. 3, 1987-Feb. 25,
>     1990.Montgomery
>     (Ala.)Newspapers.AlabamaMontgomery.fast(OCoLC)fst01202689Newspapers.fast(OCoLC)fst01423814United
> 
> 
>     StatesAlabamaMontgomeryMontgomery.Advertiser (Montgomery,
>     Ala.)0745-3221(DLC)sn 82008412(OCoLC)9049482Alabama journal (Montgomery,
>     Ala. : 1940)0745-323X(DLC)sn
>     87062018(OCoLC)2666111info:srw/schema/1/marcxmlxml00000cas a2200000 a
>     450016942287871105c19879999aludn ne 0 a0eng dsn 88050149
>     AAAengAAACPNNSDOCLCQy1044-00701044-0070746--32780746-32781565580USPSTroy
>     Publications, Inc., 113 North Market St., Troy, AL 36081mscnsdpMessenger
>     (Troy, Ala.)The Messenger(Troy, Ala.)The Messenger.Troy, Ala. :Troy
>     Pub.,1987-v.Daily (Sunday, Tuesday, Thursday and Friday)Vol. 121, no.
>     166 (July 1, 1987)-Sunday, Apr. 2, 1989 misprinted as v. 113.Latest
>     issue consulted: Vol. 113 [sic 123], no. 96 (Sunday, Apr. 2,
>     1989).United StatesAlabamaPikeTroy.Troy messenger0746-3278(DLC)sn
>     83009935(OCoLC)9921908info:srw/schema/1/marcxmlxml00000cas a22000007a
>     450017799786880415c19879999aluir ne 0 a0eng dsn 88050086
>     AARengAARCPNNSDOCLCQ1p1044-03801044-03800745-75961441520USPSThe
>     Prattville Progress, 152 W. 3rd St., Prattville, AL
>     36067mscnsdpPrattville progress (Prattville, Ala. : 1987)The Prattville
>     progress(Prattville, Ala.)The Prattville progress.Prattville, Ala.
>     :James C. Seymour,1987-v.Three times a weekVol. 102, no. 8 (Jan. 20,
>     1987)-Latest issue consulted: Vol. 105, no. 153 (Wednesday, Dec. 26,
>     1990).United StatesAlabamaAutaugaPrattville.Progress (Prattville,
>     Ala.)0745-7596(DLC)sn
>     83007623(OCoLC)9428489info:srw/schema/1/marcxmlxml00000cas a22000007a
>     450015344667870319c19869999aluwr ne 0 a0eng dsn 87000284
>     NSDengNSDCPNOCLCQy0893-07670893-07671431800USPSPickens County Herald,
>     P.O. Drawer E, Carrollton, AL 35447nsdpPickens County heraldPickens
>     County herald.Pickens County herald and west AlabamianCarrollton, Ala.
>     :Pickens Newspapers, Inc.,1986-WeeklyVol. 138, no. 40 (Oct. 2,
>     1986)-United StatesAlabamaPickensCarrollton.Pickens County herald and
>     west Alabamian0746-0473(DLC)sn
>     83008141AU at 000040635809info:srw/schema/1/marcxmlxml00000cas a22000007a
>     450018917586881217c19869999aluwr ne 0 0eng dsn 88050225
>     CPNengCPNOCLCQmscThe Oxford sun/times.Oxford, Ala.
>     :[s.n.],1986-v.WeeklyVol. 1, no. 1 (Jan. 16, 1986)-Editor: Andy
>     Goggans.Numbering is irregular.United StatesAlabamaCalhounOxford.Oxford
>     sun (Oxford, Ala.)(DLC)sn
>     85045023AU at 000025803813info:srw/schema/1/marcxmlxml00000cas a22000007a
>     450013991168860731c19869999aluwr ne 0 0eng dsn 86050322
>     CPNengCPNOCLCQmscIndependent (Brewton, Ala.)The Independent.Brewton,
>     Ala. :Jim Thornton,1986-v. :ill. ;58 cm.WeeklyVol. 1, no. 1 (June 19,
>     1986)-United
>     StatesAlabamaEscambiaBrewton.info:srw/schema/1/marcxmlxml00000cas
>     a22000007a 450018957493881231c19859999aluwr ne 0 0eng dsn
>     88050247 CPNengCPNOCLCQmscPiedmont journal-independent (Piedmont,
>     Ala.)The Piedmont journal-independent.Journal independentPiedmont, Ala.
>     :Lane Weatherbee,1985-v.WeeklyVol. 4, no. 52 (Dec. 24, 1985)-Sometimes
>     published as: Journal independent.United
>     StatesAlabamaCalhounPiedmont.Journal-independent(DLC)sn
>     85045014info:srw/schema/1/marcxmlxml00000cas a22000007a
>     450012715821851024d19841985aluwr ne 0 a0eng dsn 85045014
>     CPNengCPNNSDCPNOCLCQmscThe Journal-independent.Piedmont, Ala.
>     :Journal-Independent, Inc.,1984-1985.volumes :illustrations ;58
>     cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 3,
>     no. 27 (July 3, 1984)- v. 4, no. 51 (Dec. 18, 1985).Carries the same
>     vol. numbering as the Piedmont journal-independent.United
>     StatesAlabamaCalhounPiedmont.Piedmont
>     journal-independent0890-6017(DLC)sn 85045013Piedmont journal-independent
>     (Piedmont, Ala.)(DLC)sn 88050247info:srw/schema/1/marcxmlxml00000cas
>     a22000007a 450012691448851018c19839999aludr ne 0 0eng dsn
>     85045007 CPNengCPNOCLCQmscTimesDaily.Times dailyFlorence, Ala. :T.S.P.
>     Newspapers, Inc.,1983-volumes :illustrations ;58
>     cmDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 114,
>     no. 226 (Aug. 14, 1983)-United StatesAlabamaLauderdaleFlorence.Florence
>     times + tri-cities daily(DLC)sn
>     85044995info:srw/schema/1/marcxmlxml00000cas a22000007a
>     45009428489830420d19831987aluir ne 0 a0eng dsn 83007623
>     NSDengNSDCPNNSDNSTOCLCQ89090d0745-75960745-75961The Progress, 152 W. 3rd
>     St., Prattville, AL 36067nsdpmscProgress (Prattville, Ala.)The
>     Progress(Prattville, Ala.)The Progress.Prattville, Ala. :The Prattville
>     Progress,1983-1987.volumes :illustrations ;58 cmThree times a
>     weektexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 98, no.
>     32 (Mar. 17, 1983)-v. 102, no. 7 (Jan. 17, 1987).United
>     StatesAlabamaAutaugaPrattville.Prattville progress(DLC)sn
>     85044740Prattville progress (Prattville, Ala.)1044-0380(DLC)sn
>     88050086(OCoLC)12254317AAPinfo:srw/schema/1/marcxmlxml00000cas a2200000
>     a 45009867255830831c19839999aludr ne 0 a0eng dsn 84008052
>     AAAengAAANSDOCLOCLCQX0743-15110743-15111617760USPST.S.P. Newspapers,
>     Inc., 219 W. Tennessee St., Florence, AL 35630nsdpTimesDaily (Shoals
>     edition)TimesDaily(Shoals ed.)TimesDaily.Times dailyShoals ed.Florence,
>     Ala. :T.S.P. Newspapersvolumes
>     :illustrationsDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan
> 
> 
>     with: Vol. 114, no. 226 (Aug. 14,
>     1983).\"Florence/Sheffield/Tuscumbia/Muscle Shoals.\"Shoals ed. and
>     Regional ed. combined on Sundays.Description based on: Vol. 114, no. 346
>     (Monday, Dec. 12, 1983).United
>     StatesAlabamaLauderdaleFlorence.TimesDaily (Regional
>     edition)0743-152XTimes Tri-cities dailyUnknownDec. 12,
>     1983info:srw/schema/1/marcxmlxml00000cas a22000007a
>     450010536023840319c19839999aludr ne 0 a0eng dsn 84008051
>     NSDengNSDOCLCQ1x0743-152X0743-152X1617760USPST.S.P. Newspapers, Inc.,
>     219 W. Tennessee St., Florence, AL 35630nsdpTimesDaily (Regional
>     edition)TimesDaily(Regional ed.)TimesDaily.Times dailyRegional
>     ed.Florence, Ala. :T.S.P.
>     NewspapersDailytexttxtrdacontentunmediatednrdamediaBegan with: Vol. 114,
>     no. 226 (Aug. 14, 1983).Shoals ed. and Regional ed. combined on
>     Sundays.Description based on: Vol. 114, no. 346 (Monday, Dec. 12,
>     1983).United StatesAlabamaLauderdaleFlorence.TimesDaily (Shoals
>     edition)0743-1511Times Tri-cities dailyDec. 12,
>     1983AU at 000025818125info:srw/schema/1/marcxmlxml00000cas a22000007a
>     45009049482821213d19821987aludn ne 0 a0eng csn 82008412
>     AAAengAAANSDNPWCPNDLCCPNNSDDLCNSDDLCCPNNVFDLCOCLCQCRLOCLCFOCLCQ1d0745-32210745-32211nsdppccn-us-alNewspaperAdvertiser
> 
> 
>     (Montgomery, Ala.)The Advertiser(Montgomery, Ala.)The advertiser.Alabama
>     journal and advertiserMontgomery, Ala. :Advertiser Co.,1982-1987.volumes
>     :illustrationsDailytexttxtrdacontentunmediatednrdamediavolumencrdacarrier155th
> 
> 
>     year, no. 232 (Nov. 22, 1982)- ; -v. 14-3, Jan. 1, 1987.On Saturdays,
>     Sundays and holidays published as: The Alabama journal and advertiser,
>     Nov. 27, 1982-Jan. 1, 1987.Saturday, Sunday and holiday issues have
>     their own numbering.Montgomery
>     (Ala.)Newspapers.AlabamaMontgomery.fast(OCoLC)fst01202689Newspapers.fast(OCoLC)fst01423814United
> 
> 
>     StatesAlabamaMontgomeryMontgomery.Montgomery advertiser (Montgomery,
>     Ala. : Daily)(DLC)sn 84020645(OCoLC)2685433Montgomery advertiser
>     (Montgomery, Ala. : 1987)0892-4457(DLC)sn
>     87050045(OCoLC)15155895AU at 000020281746info:srw/schema/1/marcxmlxml00000cas
>     a2200000 a 45009237931830218c19829999aluwr ne 0 0eng dsn
>     86050139 AAAengAAACPNOCLOCLCQmscThe Randolph leader.Roanoke, Ala. :David
>     S. Stevenson,1982-volumes :illustrations ;58
>     cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 91,
>     no. 1 (Oct. 6, 1982)-United StatesAlabamaRandolphRoanoke.Roanoke
>     leader(DLC)sn 86050137Randolph press(DLC)sn
>     86050138info:srw/schema/1/marcxmlxml00000cas a22000007a
>     450012715815851024d19821984aluwr ne 0 a0eng dsn 85045013
>     CPNengCPNNSDCPNOCLCQ110890-60170890-60171432080USPSThe Piedmont
>     Journal-Independent, 115 N. Center Ave., Piedmont, AL 36272mscnsdpThe
>     Piedmont journal-independentThe Piedmont journal-independent.Piedmont,
>     Ala. :Piedmont Journal-Independent, Inc.,1982-1984.volumes
>     :illustrations ;58
>     cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 1,
>     no. 1 (Mar. 31, 1982)-v. 3, no. 26 (June 27, 1984).Latest issue
>     consulted: Vol. 5, no. 31 (August 20, 1986).United
>     StatesAlabamaCalhounPiedmont.Piedmont journal(DLC)sn
>     85045012Journal-independent(DLC)sn
>     85045014(OCoLC)12715821AU at 000045312916info:srw/schema/1/marcxmlxml00000cas
>     a22000007a 45009183905830202c19829999aluwr n 0 a0eng dsn
>     85044580 AAAengAAACPNNSDOCLOCLCQ11098-58671098-58671016409USPSNo. 4,
>     Rucker Plaza, Enterprise, AL 36331P.O. Box 1536, Enterprise, AL
>     36331mscnsdpSoutheast sun (Enterprise, Ala.)The southeast
>     sun(Enterprise, Ala.)The Southeast sun.Enterprise, Ala. :QST
>     Publicationsvolumes :illustrations ;58
>     cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan in
>     1982.Description based on: Vol. 1, no. 25 (Oct. 21, 1982).Latest issue
>     consulted: Vol. 16, no. 43 (Mar. 4, 1998).United
>     StatesAlabamaCoffeeEnterprise.AU at 000025827687info:srw/schema/1/marcxmlxml00000cas
> 
> 
>     a22000007a 450010487314840305c19819999aluwr ne 0 a0eng dsn
>     85044906
>     AAAengAAACPNNSDNSTCPNOCLOCLCQOCLCFOCLCOOCLCAOCLCQ900410885-16620885-16621749310USPSThe
> 
> 
>     New Times, 1618 1/2 St. Stephens Rd., Mobile, AL 36603mscnsdpn-us-alNew
>     times (Mobile, Ala.)The New times(Mobile, Ala.)The new times.Mobile,
>     Ala. :New Times Groupvolumes
>     :illustrationsWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierBegan
> 
> 
>     in 1981.Vol. 3, no. 49 (Dec. 15-21, 1983) and vol. 3, no. 50 (Dec.
>     22-28, 1983) are both called vol. 3, no. 49 (Dec. 15-21,
>     1983).Description based on: Vol. 2, no. 3 (Jan. 28-Feb. 3, 1982).African
>     AmericansAlabamaNewspapers.African
>     Americans.fast(OCoLC)fst00799558Alabama.fast(OCoLC)fst01204694Newspapers.fast(OCoLC)fst01423814United
> 
> 
>     StatesAlabamaMobileMobile.AAPUnknownAug. 15,
>     1985AU at 000024686659info:srw/schema/1/marcxmlxml00000cas a22000007a
>     450018922463881219d19811983alucr ne 0 0eng dsn 88050233
>     AARengAARCPNNSDOCLCQmscThe Sylacauga daily advance.Advance/Sylacauga
>     dailySylacauga advanceSunday advanceAdvanceSylacauga, Ala. :Mrs. W.A.
>     Moody,1981-1893.v.Semiweekly,<Nov. 24, 1982-Feb. 13, 1983>Daily (except
>     Mon., Tues. & Sat.),<May 26, 1982-Nov. 21, 1982>Daily (except Sat. &
>     Mon.),<Jan. 1, 1981-May 23, 1982>74th Year, no. 123 (Jan. 1, 1981)-76th
>     year, no. 83 (Feb. 13, 1983).Days of publication vary.Published as: The
>     Advance/Sylacauga daily, <Aug. 28, 1981-May 23, 1982>.Published as:
>     Sylacauga advance, <Nov. 24, 1982-Feb. 13, 1983>.On Sunday, published
>     as: Sunday advance.United StatesAlabamaTalladegaSylacauga.Childersburg
>     star(DLC)sn 88050232Coosa press(DLC)sn 86050293Daily
>     home1059-6461(DLC)sn 88050234info:srw/schema/1/marcxmlxml00000cas
>     a22000007a 450021026715cr un|||||||||900209c19809999aluwr ne 0
>     0eng dsn 90099002
>     AARengAARCPNCUSOCLOCLCQTJCOCLCQOCLCFOCLCOOCLCA926143844AU at 000020585756mscn-us-alSpeakin'
> 
> 
>     out news.Speaking out newsDecatur, Ala. :Minority Network,
>     Inc.v.WeeklyBegan in 1980.Published in Huntsville, Ala., <1987>-Also
>     issued by subscription via the World Wide Web.Description based on: Vol.
>     7, no. 8 (Jan. 7-13, 1987).African AmericansAlabamaNewspapers.African
>     American
>     newspapersAlabama.AlabamaNewspapers.Newspapers.fast(OCoLC)fst01423814African
> 
> 
>     American newspapers.fast(OCoLC)fst00799278African
>     Americans.fast(OCoLC)fst00799558Alabama.fast(OCoLC)fst01204694United
>     StatesAlabamaMorganDecatur.United
>     StatesAlabamaMadisonHuntsville.Speakin' out weekly news(DLC)sn
>     88050097http://www.softlineweb.com/softlineweb/ethnic.htminfo:srw/schema/1/marcxmlxml00000cas
>     <http://www.softlineweb.com/softlineweb/ethnic.htminfo:srw/schema/1/marcxmlxml00000cas>
> 
> 
>     a22000007a 450014996511861219c19809999aluwr ne 0 a0eng csn
>     86050472
>     AARengAARCPNNSDOCLCQ11080-15021080-15021328110USPSnsdppccWest-Alabama
>     gazetteWest-Alabama gazette.GazetteMillport, Ala. :Millport Pub.
>     Co.,1980-volumesWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrier4th
> 
> 
>     year, no. 32 (Jan. 3, 1980)-United StatesAlabamaLamarMillport.Gazette
>     (Millport, Ala.)(DLC)sn 86050471info:srw/schema/1/marcxmlxml00000cas
>     a2200000 a 450011828156850320c19809999aluwr ne 0 0eng dsn
>     86050314 AAAengAAACPNOCLOCLCQmscThe Hartford news-herald.Hartford, Ala.
>     :Geneva Publications,1980-volumes :illustrations ;57-59
>     cmWeeklytexttxtrdacontentunmediatednrdamediavolumencrdacarrierVol. 80,
>     no. 20 (Feb. 14, 1980)-United StatesAlabamaGenevaHartford.News-herald
>     (Hartford, Ala.)(DLC)sn 86050313info:srw/schema/1/marcxmlxml00000cas
>     a22000007a 450017857788880427d198u198ualusr ne 0 0eng dsn
>     88050097 AARengAARCPNOCLOCLCQOCLCFOCLCOOCLCAmscn-us-alSpeakin' out
>     weekly news.Decatur, Ala. :Smothers PublicationsPublished every first
>     and third Wed. of each monthDescription based on: Vol. 3, no. 13 (May
>     4-17, 1983).African
>     AmericansAlabamaNewspapers.Newspapers.fast(OCoLC)fst01423814African
>     Americans.fast(OCoLC)fst00799558Alabama.fast(OCoLC)fst01204694United
>     StatesAlabamaMorganDecatur.Weekly news (Huntsville, Ala.)(DLC)sn
>     87050012Speakin' out news(DLC)sn
>     90099002info:srw/schema/1/marcxmlxml00000cas a2200000 a
>     450017807936880418c198u9999aluwr ne 0 a0eng dsn 90099001
>     AAAengAAACPNOCLOCLCQThe Daleville Sun-Courier, 310 Daleville Ave.,
>     Daleville, AL 36322mscn-us-alDaleville sun-courier.Daleville, Ala. :QST
>     Publicationsv. :ill. ;58 cm.WeeklyDescription based on: Vol. 2, no. 28
>     (Wed., Feb. 17, 1988).United
>     StatesAlabamaDaleDaleville.AU at 000020585749info:srw/schema/1/marcxmlxml00000cas
> 
> 
>     a22000007a 450015580838870423c198u9999aluwr ne 0 0eng dsn
>     87050128 AARengAARCPNOCLCQmscGreene County independent.Eutaw, Ala.
>     :Greene County Independent, Inc.v.WeeklyDescription based on: Vol. 2,
>     no. 10 (Mar. 12, 1987).United
>     StatesAlabamaGreeneEutaw.info:srw/schema/1/marcxmlxml00000cas a22000007a
>     450010125135831114d198u198ualucr ne 0 a0eng dsn 83003221
>     NSDengNSDOCLCQ0d0746-55210746-55211Auburn Bulletin & Lee County Eagle,
>     PO Box 2111, Auburn, Ala. 36830nsdpThe Auburn bulletin & the Lee County
>     eagleThe Auburn bulletin & the Lee County eagle.Lee County eagleAuburn
>     bulletin and the Lee County eagleAuburn, Ala. :[publisher not
>     identified]Semiweekly,<Sept. 5,
>     1984->WeeklytexttxtrdacontentunmediatednrdamediaDescription based on:
>     Oct. 19, 1983.United StatesAlabamaLeeAuburn.Auburn bulletin(DLC)sn
>     89050006Eagle (Auburn, Ala.)(OCoLC)18435663Sept. 5,
>     1984info:srw/schema/1/marcxmlxml00000cas a22000007a
>     450018370324880818c198u9999aluwr ne 0 0eng dsn 88050147
>     CPNengCPNOCLCQmscTri-city times (Geraldine, Ala.)The Tri-City
>     times.Geraldine, Ala. :Wanda Nelsonv.WeeklyDescription based on: Vol. 2,
>     no. 24 (Jan. 6, 1982).United
>     StatesAlabamaDeKalbGeraldine.info:srw/schema/1/marcxmlxml00000cas
>     a22000007a 450010199338831208c198u9999aluwr ne 0 a0eng dsn
>     83005367 NSDengNSDCPNOCLCQ10746-62770746-62771707590USPSSpringville Pub.
>     Co., 539 Main St., Springville, AL 35146nsdpThe St. Clair clarionThe St.
>     Clair clarion.Saint Clair clarionSpringville, AL :Gary L.
>     ShultsWeeklytexttxtrdacontentunmediatednrdamediaDescription based on:
>     Vol. 2, no. 1 (Jan. 5, 1982).United StatesAlabamaSt.
>     ClairSpringville.AU at 000025783743info:srw/schema/1/marcxmlxml00000cas
>     a22000007a 450013787251860627c198u9999aluwr ne 0 a0eng dsn
>     86001923 NSDengNSDCPNOCLCQ10889-00800889-00801The Westerner Star, P.O.
>     Box 2060, Bessemer, AL 35021nsdpWestern star (Bessemer, Ala.)The Western
>     star(Bessemer, Ala.)The western star.Bessemer, Ala. :Hal
>     HodgensWeeklytexttxtrdacontentunmediatednrdamediaDescription based on:
>     Vol. 3, no. 15 (Wednesday, June 11, 1986).United
>     StatesAlabamaJeffersonBessemer.Bessemer advertiser(DLC)sn
>     87050117AU at 000025805174511.1srw.pc any \"y\" and srw.mt
>     <http://srw.mt> any
>     \"newspaper\" and srw.cp exact
>     \"Alabama\"50info:srw/schema/1/marcxmlxml1Date,,0mq1lME887FoIbjulKUV6bx9ImwWQNCv9GqZzGS92IKS31lEbcpRJBNHgcE1l29tFaHP9CHe0Yexk1uWQofffull"
> 
> 
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jul 27 2022, 09:22am via System
> 
>     Hello Spencer,
> 
>     Thank you for reaching out about the bulk xml files for the US
>     Newspaper
>     Directory.
> 
>     We don't have documentation specific to these bulk xml files, but upon
>     further inspection I can say that each of those files don't necessarily
>     contain info for 50 newspaper titles. The structure of the titles for
>     California and New York for instance are different from say, Alabama.
> 
>     If you look at California for example, the file naming structure
>     indicates the year the title started, and then the number of titles
>     included in that xml file. So for instance, the files below include
>     info
>     for newspapers that started in 2000, 2001, and 2002 respectively. And
>     there is info for 30 titles in the xml file from 2000, and 14 in the
>     file for 2001, and so on.
> 
>      ? ?* ndnp_California_2000_e_0001_0030.xml
>      ? ?* ndnp_California_2001_e_0001_0014.xml
>      ? ?* ndnp_California_2002_e_0001_0012.xml
> 
>     If there's more than 50 titles for a given year, say for California
>     starting in 1880, then the next 50 titles will roll into the next xml
>     file, and so on. And the last xml file for that year may not include 50
>     titles.
> 
>     Many of the states seem to group all the years together, so each xml
>     file contains 50 titles, until possibly the last one for a given state,
>     which may contain less.
> 
>     I hope this information helps explain the total number of records and
>     structure a bit better. Let me know if you have any further questions.
> 
>     Best wishes,
> 
>     Kerry Huller
>     Newspaper & Current Periodical Reading Room
>     Serial & Government Publications Division
>     Library of Congress
> 
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jul 25 2022, 02:22pm via Email
> 
>     Hi, Kerry:
> 
> 
>     Might there be documentation on the XML files you mentioned?
> 
> 
>     I've successfully read
>     'https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/
>     <https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/>',
>     extracted the names of 6666 XML files, and read the first one,
>     "ndnp_Alabama_all-yrs_e_0001_0050.xml". It contains 29415 characters,
>     beginning, "1.12250info:srw/schema/1/marcxmlxml00000nas a22000007i
>     45001030438981180404c20159999aluwr n 0 a0eng ". With a bit
>     more effort, I will likely be able to parse all 6666 of these. The
>     names suggest that each contains information on 50 newspapers, totaling
>     333,300. The main page
>     "https://chroniclingamerica.loc.gov/search/titles/
>     <https://chroniclingamerica.loc.gov/search/titles/>" says there are only
>     157,521 "Titles currently listed". This suggests that these XML files
>     include place holders for a little more than double the number of
>     entries currently in
>     "https://chroniclingamerica.loc.gov/search/titles/
>     <https://chroniclingamerica.loc.gov/search/titles/>".
> 
> 
>     Thanks for this.
> 
> 
>     Progress.
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jul 07 2022, 08:55am via System
> 
>     Hi Spencer,
> 
>     I thought of one more option after I emailed you yesterday that I
>     wanted
>     to make you aware of.
> 
>     I had explained the other day how we pull the records from OCLC into
>     our
>     U.S. Newspaper Directory. You can also access all of?the raw MARC
>     records found in the directory in xml format from here if you choose:
>     https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/
>     <https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/>
>     <https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/
>     <https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/>>?These?will
> 
>     provide you all of the data from the record fields in MARC format, so
>     you'd get all the data you see here for example:
>     https://chroniclingamerica.loc.gov/lccn/sn98059792/marc/
>     <https://chroniclingamerica.loc.gov/lccn/sn98059792/marc/>
>     <https://chroniclingamerica.loc.gov/lccn/sn98059792/marc/
>     <https://chroniclingamerica.loc.gov/lccn/sn98059792/marc/>>?but in
>     xml. I
>     don't know if this might be more data and info than you want to work
>     with, but wanted to make sure you were aware of this option as well.
> 
>     Best wishes,
> 
>     Kerry Huller
>     Newspaper & Current Periodical Reading Room
>     Serial & Government Publications Division
>     Library of Congress
> 
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jul 06 2022, 10:55am via System
> 
>     Hi Spencer,
> 
>     Thanks for reaching out again. I have been looking at the json view a
>     bit closer this morning and your example of "9999."
> 
>     After talking with a colleague this morning and looking at various
>     examples, I see there is some variation in how the titles with
>     either an
>     unknown starting/ending date or currently published titles are being
>     handled - depending on the view.
> 
>     As an example, I completed a search in the directory for Alaska and the
>     city of Anchorage. There are 80 results, and?on the first page of
>     results you'll see # 4. Fort Richardson news, which was published from
>     1952-19??. The csv view of this state/city search result will show the
>     ending date of 19??. But if I append &format=json to this search
>     result,
>     this specific title will show an ending date of 1999. After talking
>     with
>     a colleague this morning, I discovered an integer had to be used in
>     these cases where dates were "?" so that the search based on year range
>     would work. Similarly, if you look at # 12 Alaska digest, which was
>     published 1994-current, the "current" becomes "9999" in the json view.
>     So, the records you are seeing with "9999" would most likely be titles
>     with an ending date of "current."
> 
>     However, there is an issue with the unknown dates, like "1999" being
>     used for "19??" in the example above. The "9" does not get inserted in
>     place of "?" when you are looking at the title/LCCN view of a specific
>     newspaper. So for instance, if you view the #4 title: Fort Richardson
>     news at this url:
>     https://chroniclingamerica.loc.gov/lccn/sn98059792/
>     <https://chroniclingamerica.loc.gov/lccn/sn98059792/>
>     <https://chroniclingamerica.loc.gov/lccn/sn98059792/
>     <https://chroniclingamerica.loc.gov/lccn/sn98059792/>>?but append .json
>     to the end of the url, after the LCCN, like this:
>     https://chroniclingamerica.loc.gov/lccn/sn98059792.json
>     <https://chroniclingamerica.loc.gov/lccn/sn98059792.json>
>     <https://chroniclingamerica.loc.gov/lccn/sn98059792.json
>     <https://chroniclingamerica.loc.gov/lccn/sn98059792.json>>?you'll see
>     that the end_year is "19??." Viewing the title/LCCN json view for
>     titles
>     that are currently published will also show the end_year as "current."
>     The Alaska digest example from above can be viewed here:
>     https://chroniclingamerica.loc.gov/lccn/sn97060056.json
>     <https://chroniclingamerica.loc.gov/lccn/sn97060056.json>
>     <https://chroniclingamerica.loc.gov/lccn/sn97060056.json
>     <https://chroniclingamerica.loc.gov/lccn/sn97060056.json>>
> 
>     I wasn't aware of the difference between the directory search json view
>     and the title/LCCN view. But I think it would be possible to grab
>     the?data from?the title/LCCN json url through an additional script
>     potentially. The json url is included in the view under the?"url" field.
> 
>     Of course, there are unknowns with publishing dates, but better to know
>     where the question marks are, and what titles are considered to be
>     current.
> 
>     I hope this clarifies the data a bit more - let me know if any of it
>     needs more clarification though. And let me know if you have follow-up
>     questions.
> 
>     Thank you,
> 
>     Kerry Huller
>     Newspaper & Current Periodical Reading Room
>     Serial & Government Publications Division
>     Library of Congress
> 
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jul 05 2022, 04:42pm via Email
> 
>     Hi, Kerry:
> 
> 
>     What would you suggest I do to get a count of the numbers of
>     newspapers and publishers operating by year from, say, 1790 to 2021?
> 
> 
>     I just determined that 20630 (13 percent) of the 157520 records in
>     the US Newspaper database I downloaded a week ago have end_year = 9999.
>     I don't think it's feasible to assume that all or even most of those
>     are still publishing.
> 
> 
>     Might there be some other database that might have this kind of
>     information?
> 
> 
>     I ask, because Robert McChesney (2004) The Problem of the Media
>     (Monthly Review Pr., esp. pp. 34-35) suggests that in the first half of
>     the nineteenth century, the US had more newspapers and newspaper
>     publishers per capita than any other place or time. He suggests that
>     that diversity of newspapers helped encourage literacy and limit
>     political corruption, both of which helped propel the young US to its
>     current dominance of the international political economy. I'm hoping to
>     get some data to evaluate this claim. Sadly, it looks like there is too
>     much missing and questionable data in this dataset for me to use this
>     without a fairly substantive data cleaning effort.
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jul 05 2022, 09:05am via System
> 
>     Hello Spencer,
> 
>     Thank you for reaching out about your additional questions.
> 
>     I was looking at the records you mention above, and yes, you are
>     correct
>     - those 9 records with the date inconsistencies and the one record?for
>     the The New Mexican mining news
>     <https://chroniclingamerica.loc.gov/lccn/sn93061507/
>     <https://chroniclingamerica.loc.gov/lccn/sn93061507/>> containing
>     "Santa
>     Fe.\" have typos in them. Thanks for spotting these - it may be
>     possible
>     to have the cataloger in our division correct those typos. I will look
>     into this further.
> 
>     The U.S. Newspaper Directory doesn't have a connection with
>     Wikimedia or
>     Wikipedia. The Library of Congress?periodically pulls the records for
>     the Directory from OCLC Worldcat
>     <https://www.oclc.org/en/worldcat.html
>     <https://www.oclc.org/en/worldcat.html>>. And those?newspaper
>     records in
>     OCLC Worldcat have been created by catalogers?at various institutions
>     around the U.S. over the span of several years. So, occasionally, you
>     will find a typo in the records. Corrections can be?made by OCLC and
>     library staff at the various institutions. Every time we complete a new
>     pull on the OCLC records, any corrected records will then populate our
>     Directory.
> 
>     Regarding your question on the New-York weekly journal - yes, that is
>     also correct that it has two records. There is actually a?record?for
>     each format of the newspaper, so this record is for the microfilm
>     format
>     <https://chroniclingamerica.loc.gov/lccn/2009252748/
>     <https://chroniclingamerica.loc.gov/lccn/2009252748/>> and this one is
>     for the original print format
>     <https://chroniclingamerica.loc.gov/lccn/sn83030211/
>     <https://chroniclingamerica.loc.gov/lccn/sn83030211/>>. You can see in
>     the heading for the microfilm record where it says [microfilm reel] and
>     the print version shows [volume]. You are likely to see this for other
>     titles as well because each format has been cataloged with its own
>     LCCN.
>     You are also likely to see additional records with [online resource]
>     identified as the format as more and more titles are available as
>     ePrints or online.
> 
>     I hope this helps answer your additional questions a bit more. Please
>     reach out if you have any other questions.
> 
>     Thank you,
> 
>     Kerry Huller
>     Newspaper & Current Periodical Reading Room
>     Serial & Government Publications Division
>     Library of Congress
> 
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jul 04 2022, 01:47pm via Email
> 
>     Hi, Kelly:
> 
> 
>     At the risk of bombing your inbox with more emails than you want,
>     what is your relationship with Wikipedia and other Wikimedia Foundation
>     projects like Wikidata?
> 
> 
>     I ask, because I've logged over 20,000 edits in Wikimedia Foundation
>     projects since 2010, and I would happily try to answer questions about
>     Wikidata and other Wikimedia Foundation projects. I have NOT organized
>     an edit-a-thon, but I've made presentations at conferences with people
>     who have, and I would happily try to help organize such if you could
>     find a group of people who want to work to improve this US Newspaper
>     database. I think it would be good to establish links between this US
>     Newspaper database and Wikidata, with appropriate procedures so changes
>     to one could be evaluated for acceptance into the other.
> 
> 
>     FYI, John Peter Zenger's famous "New-York weekly journal" (1733-1751)
>     appears TWICE in your database with lccn = 2009252748 and sn83030211 and
>     ONCE in Wikidata WITHOUT an lccn, even though many other Wikidata items
>     have an lccn. See:
> 
> 
>     https://www.wikidata.org/wiki/Q23091960
>     <https://www.wikidata.org/wiki/Q23091960>
> 
> 
>     There's a "WikiProject Newspapers" on Wikipedia and a companion
>     "WikiProject Periodicals" on Wikidata:
> 
> 
>     https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Newspapers/Wikidata
>     <https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Newspapers/Wikidata>
> 
> 
>     https://www.wikidata.org/wiki/Wikidata:WikiProject_Periodicals
>     <https://www.wikidata.org/wiki/Wikidata:WikiProject_Periodicals>
> 
> 
>     I've tried to connect with others on those projects, so far with only
>     limited success. However, you may know that almost anyone can change
>     almost anything on Wikipedia and other Wikimedia Foundation projects.
>     What stays tends to be written from a neutral point of view citing
>     credible sources. They have problems with vandals, but the problems are
>     usually easily controlled. This makes Wikipedia and Wikidata very
>     useful platforms for cleaning up databases like your US Newspaper
>     dataset.
> 
> 
>     Spencer Graves
> 
> 
>     ##########
> 
> 
>     Hello, Kelly:
> 
> 
>     In addition to the invalid JSON, discussed below [NOTE: The "below"
>     contains a slight addition to the report of the I sent last Friday.], I
>     found 9 (NINE!) cases where start_year was AFTER end_year. These have
>     lccn = "sn86071531" "sn95069213" "sn90059096" "sn86058451" "sn90060926"
>     "sn99065409" "sn89065002" "sn98069857" "sn91059179"
> 
> 
>     See:
> 
> 
>     https://chroniclingamerica.loc.gov/lccn/sn86071531/
>     <https://chroniclingamerica.loc.gov/lccn/sn86071531/>
>     https://chroniclingamerica.loc.gov/lccn/sn95069213/
>     <https://chroniclingamerica.loc.gov/lccn/sn95069213/>
>     https://chroniclingamerica.loc.gov/lccn/sn90059096/
>     <https://chroniclingamerica.loc.gov/lccn/sn90059096/>
>     https://chroniclingamerica.loc.gov/lccn/sn86058451/
>     <https://chroniclingamerica.loc.gov/lccn/sn86058451/>
>     https://chroniclingamerica.loc.gov/lccn/sn90060926/
>     <https://chroniclingamerica.loc.gov/lccn/sn90060926/>
>     https://chroniclingamerica.loc.gov/lccn/sn99065409/
>     <https://chroniclingamerica.loc.gov/lccn/sn99065409/>
>     https://chroniclingamerica.loc.gov/lccn/sn89065002/
>     <https://chroniclingamerica.loc.gov/lccn/sn89065002/>
>     https://chroniclingamerica.loc.gov/lccn/sn98069857/
>     <https://chroniclingamerica.loc.gov/lccn/sn98069857/>
>     https://chroniclingamerica.loc.gov/lccn/sn91059179/
>     <https://chroniclingamerica.loc.gov/lccn/sn91059179/>
> 
> 
>     These all have obvious coding errors that can be easily fixed. The
>     data may not be completely accurate after the fix, but at least they are
>     not obviously wrong ;-)
> 
> 
>     ##################
> 
>     I got invalid JSON from:
> 
> 
>     https://chroniclingamerica.loc.gov/search/titles/results/?rows=500&page=103&format=json
>     <https://chroniclingamerica.loc.gov/search/titles/results/?rows=500&page=103&format=json>
> 
> 
>     After some experimentation, I was able to replicate the problem with
>     a request for rows=10:
> 
> 
>     https://chroniclingamerica.loc.gov/search/titles/results/?rows=10&page=5117&format=json
>     <https://chroniclingamerica.loc.gov/search/titles/results/?rows=10&page=5117&format=json>
> 
> 
>     Duncan Temple Lang <dtemplelang at ucdavis.edu
>     <mailto:dtemplelang at ucdavis.edu>>, Professor of Statistics
>     and Associate Dean for Graduate Programs at the University of California
>     - Davis, confirmed that it was a JSON error using:
> 
> 
>     https://codebeautify.org/jsonvalidator
>     <https://codebeautify.org/jsonvalidator>
> 
> 
>     He is part of the core team developing the R free, open-source
>     programming language. He said, that starting at offsets 161070 and
>     161502 in the character string you get from [the R code RCurl::getURL()]
>     we have:
> 
> 
>     Santa Fe.\"
> 
> 
>     and these are in an entry such as
> 
> 
>     "city": ["Santa Fe.\"]
> 
> 
>     So the final " is escaped and therefore there is no closing " for the
>     string. The parser continues to consume characters looking for the end
>     of that string.
> 
> 
>     If one "repairs" the text from getURL() with
> 
> 
>     ftxt= gsub('Santa Fe.\\\\"', 'Santa Fe."', txt)
> 
> 
>     then the rest of my code worked fine.
> 
> 
>     You may wish to do something to implement other checks for valid JSON
>     and repair this problem. I've scanned all the 157520 records that were
>     in that database a couple of days ago, and this is the only JSON error
>     identified by the code I used.
> 
> 
>     NOTE: I was NOT able to replicate this error when downloading records
>     one at a time. That suggests a problem NOT in the database itself but
>     in the download algorithm. ???
> 
> 
>     Thank you for your help. I will almost certainly have other
>     questions ;-)
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jul 03 2022, 10:39pm via Email
> 
>     Hello, Kelly:
> 
> 
>     In addition to the invalid JSON, discussed below [NOTE: The "below"
>     contains a slight addition to the report of the I sent last Friday.], I
>     found 9 (NINE!) cases where start_year was AFTER end_year. These have
>     lccn = "sn86071531" "sn95069213" "sn90059096" "sn86058451" "sn90060926"
>     "sn99065409" "sn89065002" "sn98069857" "sn91059179"
> 
> 
>     See:
> 
> 
>     https://chroniclingamerica.loc.gov/lccn/sn86071531/
>     <https://chroniclingamerica.loc.gov/lccn/sn86071531/>
>     https://chroniclingamerica.loc.gov/lccn/sn95069213/
>     <https://chroniclingamerica.loc.gov/lccn/sn95069213/>
>     https://chroniclingamerica.loc.gov/lccn/sn90059096/
>     <https://chroniclingamerica.loc.gov/lccn/sn90059096/>
>     https://chroniclingamerica.loc.gov/lccn/sn86058451/
>     <https://chroniclingamerica.loc.gov/lccn/sn86058451/>
>     https://chroniclingamerica.loc.gov/lccn/sn90060926/
>     <https://chroniclingamerica.loc.gov/lccn/sn90060926/>
>     https://chroniclingamerica.loc.gov/lccn/sn99065409/
>     <https://chroniclingamerica.loc.gov/lccn/sn99065409/>
>     https://chroniclingamerica.loc.gov/lccn/sn89065002/
>     <https://chroniclingamerica.loc.gov/lccn/sn89065002/>
>     https://chroniclingamerica.loc.gov/lccn/sn98069857/
>     <https://chroniclingamerica.loc.gov/lccn/sn98069857/>
>     https://chroniclingamerica.loc.gov/lccn/sn91059179/
>     <https://chroniclingamerica.loc.gov/lccn/sn91059179/>
> 
> 
>     These all have obvious coding errors that can be easily fixed. The
>     data may not be completely accurate after the fix, but at least they are
>     not obviously wrong ;-)
> 
> 
>     ##################
> 
>     I got invalid JSON from:
> 
> 
>     https://chroniclingamerica.loc.gov/search/titles/results/?rows=500&page=103&format=json
>     <https://chroniclingamerica.loc.gov/search/titles/results/?rows=500&page=103&format=json>
> 
> 
>     After some experimentation, I was able to replicate the problem with
>     a request for rows=10:
> 
> 
>     https://chroniclingamerica.loc.gov/search/titles/results/?rows=10&page=5117&format=json
>     <https://chroniclingamerica.loc.gov/search/titles/results/?rows=10&page=5117&format=json>
> 
> 
>     Duncan Temple Lang <dtemplelang at ucdavis.edu
>     <mailto:dtemplelang at ucdavis.edu>>, Professor of Statistics
>     and Associate Dean for Graduate Programs at the University of California
>     - Davis, confirmed that it was a JSON error using:
> 
> 
>     https://codebeautify.org/jsonvalidator
>     <https://codebeautify.org/jsonvalidator>
> 
> 
>     He is part of the core team developing the R free, open-source
>     programming language. He said, that starting at offsets 161070 and
>     161502 in the character string you get from [the R code RCurl::getURL()]
>     we have:
> 
> 
>     Santa Fe.\"
> 
> 
>     and these are in an entry such as
> 
> 
>     "city": ["Santa Fe.\"]
> 
> 
>     So the final " is escaped and therefore there is no closing " for the
>     string. The parser continues to consume characters looking for the end
>     of that string.
> 
> 
>     If one "repairs" the text from getURL() with
> 
> 
>     ftxt= gsub('Santa Fe.\\\\"', 'Santa Fe."', txt)
> 
> 
>     then the rest of my code worked fine.
> 
> 
>     You may wish to do something to implement other checks for valid JSON
>     and repair this problem. I've scanned all the 157520 records that were
>     in that database a couple of days ago, and this is the only JSON error
>     identified by the code I used.
> 
> 
>     NOTE: I was NOT able to replicate this error when downloading records
>     one at a time. That suggests a problem NOT in the database itself but
>     in the download algorithm. ???
> 
> 
>     Thank you for your help. I will almost certainly have other
>     questions ;-)
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jul 01 2022, 11:46am via Email
> 
>     Hello, Kelly:
> 
> 
>     I got invalid JSON from:
> 
> 
>     https://chroniclingamerica.loc.gov/search/titles/results/?rows=500&page=103&format=json
>     <https://chroniclingamerica.loc.gov/search/titles/results/?rows=500&page=103&format=json>
> 
> 
>     After some experimentation, I was able to replicate the problem with
>     a request for rows=10:
> 
> 
>     https://chroniclingamerica.loc.gov/search/titles/results/?rows=10&page=5117&format=json
>     <https://chroniclingamerica.loc.gov/search/titles/results/?rows=10&page=5117&format=json>
> 
> 
>     Duncan Temple Lang <dtemplelang at ucdavis.edu
>     <mailto:dtemplelang at ucdavis.edu>>, Professor of Statistics
>     and Associate Dean for Graduate Programs at the University of California
>     - Davis, confirmed that it was a JSON error using:
> 
> 
>     https://codebeautify.org/jsonvalidator
>     <https://codebeautify.org/jsonvalidator>
> 
> 
>     He is part of the core team developing the R free, open-source
>     programming language. He said, that starting at offsets 161070 and
>     161502 in the character string you get from [the R code RCurl::getURL()]
>     we have:
> 
> 
>     Santa Fe.\"
> 
> 
>     and these are in an entry such as
> 
> 
>     "city": ["Santa Fe.\"]
> 
> 
>     So the final " is escaped and therefore there is no closing " for the
>     string. The parser continues to consume characters looking for the end
>     of that string.
> 
> 
>     If one "repairs" the text from getURL() with
> 
> 
>     ftxt= gsub('Santa Fe.\\\\"', 'Santa Fe."', txt)
> 
> 
>     then the rest of my code worked fine.
> 
> 
>     You may wish to do something to implement other checks for valid JSON
>     and repair this problem. I've scanned all the 157520 records that were
>     in that database a couple of days ago, and this is the only JSON error
>     identified by the code I used.
> 
> 
>     Thank you for your help. I will almost certainly have other
>     questions ;-)
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jun 28 2022, 02:20pm via System
> 
>     Hello Spencer,
> 
>     Thank you for sending along your follow-up questions.
> 
>     I'm glad to hear the json view?will work for you. It was recommended to
>     me that you limit your requests to 500 rows at a time. And a developer
>     here at LC suggests the following regarding rate limiting:
> 
>     ?To avoid being blocked by the server, the current rate-limiting rules
>     restrict un-cached requests to URLs starting with
>     https://chroniclingamerica.loc.gov/search/
>     <https://chroniclingamerica.loc.gov/search/>
>     <https://chroniclingamerica.loc.gov/search/
>     <https://chroniclingamerica.loc.gov/search/>> to 120 requests every 10
>     minutes from a single IP address.?
> 
>     So, I think if you limited each of your requests to 500 rows at a time
>     with the proper pauses, then you should be able to access what you need.
> 
>     As for the csv view, I checked on this as well, and was informed that
>     the?csv view was not implemented for all url formats. The csv view was
>     only implemented for this view:
>     https://chroniclingamerica.loc.gov/newspapers/
>     <https://chroniclingamerica.loc.gov/newspapers/>
>     <https://chroniclingamerica.loc.gov/newspapers/
>     <https://chroniclingamerica.loc.gov/newspapers/>>and urls resulting
>     from
>     US Directory search results - for e.g. if you wanted to narrow down
>     your
>     search results by state, city, date range, etc. found at this link:
>     https://chroniclingamerica.loc.gov/search/titles/
>     <https://chroniclingamerica.loc.gov/search/titles/>
>     <https://chroniclingamerica.loc.gov/search/titles/
>     <https://chroniclingamerica.loc.gov/search/titles/>>. So, if you
>     wanted a
>     csv and limited your search by state ( for example:
>     https://chroniclingamerica.loc.gov/search/titles/results/?state=Alaska&county=&city=&year1=1690&year2=2022&terms=&frequency=&language=&ethnicity=&labor=&material_type=&lccn=&rows=20&format=csv
>     <https://chroniclingamerica.loc.gov/search/titles/results/?state=Alaska&county=&city=&year1=1690&year2=2022&terms=&frequency=&language=&ethnicity=&labor=&material_type=&lccn=&rows=20&format=csv>
> 
>     <https://chroniclingamerica.loc.gov/search/titles/results/?state=Alaska&county=&city=&year1=1690&year2=2022&terms=&frequency=&language=&ethnicity=&labor=&material_type=&lccn=&rows=20&format=csv
>     <https://chroniclingamerica.loc.gov/search/titles/results/?state=Alaska&county=&city=&year1=1690&year2=2022&terms=&frequency=&language=&ethnicity=&labor=&material_type=&lccn=&rows=20&format=csv>>
> 
>     ), you could append &format=csv to the search result url and get the
>     csv
>     to automatically download. But, if your search results ended up being
>     over a couple thousand titles, then the system would probably time out.
> 
>     I hope this info helps! Let me know if you have any other questions.
> 
>     Best wishes,
> 
>     Kerry Huller
>     Newspaper & Current Periodical Reading Room
>     Serial & Government Publications Division
>     Library of Congress
> 
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jun 27 2022, 04:15pm via Email
> 
>     Hello, Kerry:
> 
> 
>     Thanks for the reply. Can you please give me some further guidance
>     on two thing "so that the system is not overwhelmed"?
> 
> 
>     1. The max size in a small batch?
> 
> 
>     2. Any limit on the number of small batches in a second or minute?
> 
> 
>     I've found that I can download small batches under program control
>     using "RCurl::getURL" in R (programming language) using, e.g.;
> 
> 
>     https://chroniclingamerica.loc.gov/search/titles/results/?rows=20&page=2&format=json
>     <https://chroniclingamerica.loc.gov/search/titles/results/?rows=20&page=2&format=json>
> 
> 
>     With this, I can control the batch size with "row=20" vs. "row=50"
>     vs., e.g., "row=1000". A naive search says there are 157520 "results".
>     With "row=1000", this would require 158 calls. With "row=20", it
>     would require 7876 calls. Before I start, I need to decide which fields
>     I want; I don't need them all.
> 
> 
>     Thanks,
>     Spencer Graves
> 
> 
>     p.s. I tried appending "&format=csv" and got "Error 504 Ray ID:
>     7220896da85e86e7 ? 2022-06-27 19:19:53 UTC Gateway time-out". I used:
> 
> 
>     https://chroniclingamerica.loc.gov/search/titles/results/?state=&county=&city=&year1=1690&year2=2022&terms=&frequency=&language=&ethnicity=&labor=&material_type=&lccn=&rows=20&format=csv
>     <https://chroniclingamerica.loc.gov/search/titles/results/?state=&county=&city=&year1=1690&year2=2022&terms=&frequency=&language=&ethnicity=&labor=&material_type=&lccn=&rows=20&format=csv>
> 
> 
>     I can get what I want using json so do not need csv. However, I
>     thought you might want to know that I was unable to get csv to work.
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jun 27 2022, 10:54am via System
> 
>     Hello Spencer,
> 
>     Thank you for contacting the Library of Congress about searching the US
>     Newspaper Directory. I wanted to follow up with you regarding your
>     request to output the data in a machine readable format.
> 
>     It looks like you were provided the link to the API documentation for
>     the website: About the Site and API
>     <https://chroniclingamerica.loc.gov/about/api/
>     <https://chroniclingamerica.loc.gov/about/api/>>. Scroll down to the
>     section with the heading, Searching the directory and newspaper pages
>     using OpenSearch. This section describes the search functionality and
>     structure for the US Newspaper Directory in more detail. It is possible
>     to return your directory searches in json format by appending
>     &format=json to the end of the url. It is also possible to return
>     search
>     results in csv format by appending &format=csv to the end of the url,
>     but I would strongly suggest that you do this in small batches by
>     putting limits on your search so that the system is not overwhelmed.
> 
>     So, from the search page for the US Newspaper Directory
>     <https://chroniclingamerica.loc.gov/search/titles/
>     <https://chroniclingamerica.loc.gov/search/titles/>>?you could
>     potentially limit your search based on state?and city, or date range,
>     and/or even frequency. Then once you've completed the search, you can
>     add &format=csv to the end of the url to automatically download a
>     csv of
>     those records. The resulting csv will contain several fields/headers:
>     lccn, title, place of publication, start year, end year, publisher,
>     edition, frequency, subject, state, city, country, language, oclc
>     number, and holding type. I think these fields include the information
>     you were looking for. But, again, I would like to stress that you put
>     limits on your search before creating the csv so as not overwhelm the
>     system.
> 
>     Please let me know if you have any other additional questions.
> 
>     Best wishes,
> 
>     Kerry Huller
>     Newspaper & Current Periodical Reading Room
>     Serial & Government Publications Division
>     Library of Congress
> 
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jun 23 2022, 01:55pm via System
> 
>     Mr. Graves,
> 
>     I'm going to transfer you request to a member of our digital
>     collections
>     team who may be of more assistance to you than me.
> 
>     Mike
> 
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jun 23 2022, 01:51pm via Email
> 
>     Dear Mr. Queen:
> 
> 
>     Thanks for the reply. I'm still confused. I downloaded and
>     installed Docker Desktop and "docker-compose.yml" and ran their "Getting
>     Started" Tutorial, but I don't see what to do next.
> 
> 
>     I repeat: I'd like to analyze "U.S. Newspaper Directory,
>     1690-Present" (https://chroniclingamerica.loc.gov/search/titles/
>     <https://chroniclingamerica.loc.gov/search/titles/>), which
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jun 22 2022, 07:15pm via System
> 
>     Mr. Graves,
> 
>     Programmatic access to the data forChronicling America
>     <https://chroniclingamerica.loc.gov/
>     <https://chroniclingamerica.loc.gov/>>and possibly the U.S. Newspaper
>     Directory <https://chroniclingamerica.loc.gov/search/titles/
>     <https://chroniclingamerica.loc.gov/search/titles/>>can be
>     found on theAbout the Site and API
>     <https://chroniclingamerica.loc.gov/about/api/
>     <https://chroniclingamerica.loc.gov/about/api/>>page in various
>     formats.
>     Also, please note that Chronicling Americacontains newspapers published
>     from 1777-1963, but does not include everyU.S. newspaper published in
>     that time period.
> 
>     Please let me know if I can be of further assistance.
> 
> 
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jun 22 2022, 06:14pm via Email
> 
>     Dear Mr. Queen:
> 
> 
>     Can we simplify this to just giving me the data behind "U.S.
>     Newspaper Directory, 1690-Present"
>     (https://chroniclingamerica.loc.gov/search/titles/
>     <https://chroniclingamerica.loc.gov/search/titles/>) in a machine
>     readable format, e.g., csv or xlsx or a MySQL database?
> 
> 
>     As I mentioned in my original email, a naive search of that without
>     restrictions returned 157520 titles in 7876 pages with up to 20 titles
>     per page giving date ranges in at least some cases. I could probably
>     write software to scrape those 7876 pages from your web site and combine
>     them into a data file.
> 
> 
>     I have a PhD in statistics, I have been using the R programming
>     language and similar software for decades. This includes publishing
>     tutorials on how to analyze data like this on Wikiversity.[1] I'd like
>     to do something similar with this. I could help make your data more
>     useful to others and discuss with you how we might prioritize
>     improvements like accessing the other sources you mentioned.
> 
> 
>     Thanks very much for your reply.
> 
> 
>     Sincerely,
>     Spencer Graves, PhD
>     Founder, EffectiveDefense.org
>     4550 Warwick Blvd 508
>     Kansas City, MO 64111
>     m: 408-655-4567
> 
> 
>     [1] e.g.:
> 
> 
>     https://en.wikiversity.org/wiki/US_Gross_Domestic_Product_(GDP)_per_capita
>     <https://en.wikiversity.org/wiki/US_Gross_Domestic_Product_(GDP)_per_capita>
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jun 22 2022, 05:27pm via System
> 
>     Mr. Graves
> 
>     Your request is a little more complex than it first appears and
>     requires
>     extensive research. A variety of resources should be consulted to
>     determine the circulation statistics of newspapers published prior to
>     1851. You will need to check newspaper union lists and newspaper
>     histories. Union listspresent lists of newspapers in geographic
>     arrangement according to place of publication, and specify which
>     libraries or other institutions hold collections of those newspapers
>     and
>     the dates of their holdings. These can also be useful for tracking
>     title
>     changes throughout a newspaper's history. Newspaper
>     historieslikeAmerican Journalism: A History: 1690-1960
>     <https://lccn.loc.gov/62007157
>     <https://lccn.loc.gov/62007157>>(Mott),The Penny Press
>     <https://lccn.loc.gov/2004043078
>     <https://lccn.loc.gov/2004043078>>(Thompson), andThe Press and America
>     <https://lccn.loc.gov/99044295
>     <https://lccn.loc.gov/99044295>>(Emery et al.) may not include
>     circulation statistics, but they do document the diversity and progress
>     of newspaper publishing, including notable newspapers of the era.
>     Newspaper histories also cover the history of the printers and printing
>     of newspapers in a state, county, or region more generally, and provide
>     more condensed histories of the editors, journalists, and evolution of
>     the newspapers in a specific area. Newspaper histories and union lists
>     should be available at most large public or university libraries. More
>     information about union lists, newspaper histories, and researching
>     newspapers in general can be found in theU.S. Newspaper Collections at
>     the Library of Congress
>     <https://guides.loc.gov/united-states-newspapers/introduction
>     <https://guides.loc.gov/united-states-newspapers/introduction>>research
>     guide (see Reference Sources).
> 
>     Please let me know if I can be of further assistance.
> 
>     ------------------------------------------------------------------------
> 
>     Original Question
> 
>     Jun 20 2022, 02:34pm via System
> 
>     How can I get counts of the numbers of newspapers by year in the US,
>     and
>     preferably also elsewhere? A search of "U.S. Newspaper Directory,
>     How can I get counts of the numbers of newspapers by year in the US,
>     and
>     preferably also elsewhere?
> 
>     A search of "U.S. Newspaper Directory, 1690-Present"
>     (https://chroniclingamerica.loc.gov/search/titles/
>     <https://chroniclingamerica.loc.gov/search/titles/>) returned 157520
>     titles in 7876 pages with up to 20 titles per page giving date
>     ranges to
>     the extent that it's known. If I can get a data file (e.g., csv or
>     xls),
>     I can summarize. I could also use data on circulation and frequency and
>     especially parent company for multiple newspapers published by the same
>     company, to the extant that such is available.
> 
>     I'm interested in this, because McChesney quoted Tocqueville in
>     suggesting that the US had more newspapers per person (or per million
>     population) prior to 1851 than at any other time or place in history.
>     I'd like to evaluate that claim with data to the extent that I can. See
>     "https://en.wikiversity.org/wiki/Social_construction_of_crime_and_what_we_can_do_about_it#Newspapers_1790_-_present
>     <https://en.wikiversity.org/wiki/Social_construction_of_crime_and_what_we_can_do_about_it#Newspapers_1790_-_present>".
> 
> 
> 
>     Thanks, Spencer Graves, PhD
>     m: 408-655-4567
> 
>     ------------------------------------------------------------------------
> 
>     Thank you for using Newspapers & Current Periodicals Ask a Librarian
>     Service!
> 
> 
>     This email is sent from Ask a Librarian in relationship to ticket
>     #9625195.
> 
>     Read our privacy policy. <https://springshare.com/privacy.html
>     <https://springshare.com/privacy.html>>
> 
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     <https://stat.ethz.ch/mailman/listinfo/r-help>
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     <http://www.R-project.org/posting-guide.html>
>     and provide commented, minimal, self-contained, reproducible code.
>


From j|ox @end|ng |rom mcm@@ter@c@  Thu Jul 28 14:45:35 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Thu, 28 Jul 2022 08:45:35 -0400
Subject: [R] Predicted values from glm() when linear predictor is NA.
In-Reply-To: <3A27E228-1C74-4753-BE6F-37074FF54143@dcn.davis.ca.us>
References: <775_1658968044_26S0RNlm003716_20220728122628.4381daa4@rolf-Latitude-E7470>
 <dcc28626-39f9-3f02-9d83-38d711b16bd2@mcmaster.ca>
 <3A27E228-1C74-4753-BE6F-37074FF54143@dcn.davis.ca.us>
Message-ID: <9bae4bf6-0572-d349-d811-1dacbfba5917@mcmaster.ca>

Dear Jeff,

On 2022-07-28 1:31 a.m., Jeff Newmiller wrote:
> But "disappearing" is not what NA is supposed to do normally. Why is it being treated that way here?

NA has a different meaning here than in data.

By default, in glm() the argument singular.ok is TRUE, and so estimates 
are provided even when there are singularities, and even though the 
singularities are resolved arbitrarily.

In this model, the columns of the model matrix labelled LifestageL1 and 
TrtTime:LifestageL1 are perfectly collinear -- the second is 12 times 
the first (both have 0s in the same rows and either 1 or 12 in three of 
the rows) -- and thus both can't be estimated simultaneously, but the 
model can be estimated by eliminating one or the other (effectively 
setting its coefficient to 0), or by taking any linear combination of 
the two regressors (i.e., using any regressor with 0s and some other 
value). The fitted values under the model are invariant with respect to 
this arbitrary choice.

My apologies if I'm stating the obvious and misunderstand your objection.

Best,
  John

> 
> On July 27, 2022 7:04:20 PM PDT, John Fox <jfox at mcmaster.ca> wrote:
>> Dear Rolf,
>>
>> The coefficient of TrtTime:LifestageL1 isn't estimable (as you explain) and by setting it to NA, glm() effectively removes it from the model. An equivalent model is therefore
>>
>>> fit2 <- glm(cbind(Dead,Alive) ~ TrtTime + Lifestage +
>> +               I((Lifestage == "Egg + L1")*TrtTime) +
>> +               I((Lifestage == "L1 + L2")*TrtTime) +
>> +               I((Lifestage == "L3")*TrtTime),
>> +             family=binomial, data=demoDat)
>> Warning message:
>> glm.fit: fitted probabilities numerically 0 or 1 occurred
>>
>>> cbind(coef(fit, complete=FALSE), coef(fit2))
>>                                   [,1]         [,2]
>> (Intercept)                -0.91718302  -0.91718302
>> TrtTime                     0.88846195   0.88846195
>> LifestageEgg + L1         -45.36420974 -45.36420974
>> LifestageL1                14.27570572  14.27570572
>> LifestageL1 + L2           -0.30332697  -0.30332697
>> LifestageL3                -3.58672631  -3.58672631
>> TrtTime:LifestageEgg + L1   8.10482459   8.10482459
>> TrtTime:LifestageL1 + L2    0.05662651   0.05662651
>> TrtTime:LifestageL3         1.66743472   1.66743472
>>
>> There is no problem computing fitted values for the model, specified either way. That the fitted values when Lifestage == "L1" all round to 1 on the probability scale is coincidental -- that is, a consequence of the data.
>>
>> I hope this helps,
>> John
>>
>> On 2022-07-27 8:26 p.m., Rolf Turner wrote:
>>>
>>> I have a data frame with a numeric ("TrtTime") and a categorical
>>> ("Lifestage") predictor.
>>>
>>> Level "L1" of Lifestage occurs only with a single value of TrtTime,
>>> explicitly 12, whence it is not possible to estimate a TrtTime "slope"
>>> when Lifestage is "L1".
>>>
>>> Indeed, when I fitted the model
>>>
>>>       fit <- glm(cbind(Dead,Alive) ~ TrtTime*Lifestage, family=binomial,
>>>                  data=demoDat)
>>>
>>> I got:
>>>
>>>> as.matrix(coef(fit))
>>>>                                     [,1]
>>>> (Intercept)                -0.91718302
>>>> TrtTime                     0.88846195
>>>> LifestageEgg + L1         -45.36420974
>>>> LifestageL1                14.27570572
>>>> LifestageL1 + L2           -0.30332697
>>>> LifestageL3                -3.58672631
>>>> TrtTime:LifestageEgg + L1   8.10482459
>>>> TrtTime:LifestageL1                 NA
>>>> TrtTime:LifestageL1 + L2    0.05662651
>>>> TrtTime:LifestageL3         1.66743472
>>>
>>> That is, TrtTime:LifestageL1 is NA, as expected.
>>>
>>> I would have thought that fitted or predicted values corresponding to
>>> Lifestage = "L1" would thereby be NA, but this is not the case:
>>>
>>>> predict(fit)[demoDat$Lifestage=="L1"]
>>>>         26       65      131
>>>> 24.02007 24.02007 24.02007
>>>>
>>>> fitted(fit)[demoDat$Lifestage=="L1"]
>>>>    26  65 131
>>>>     1   1   1
>>>
>>> That is, the predicted values on the scale of the linear predictor are
>>> large and positive, rather than being NA.
>>>
>>> What this amounts to, it seems to me, is saying that if the linear
>>> predictor in a Binomial glm is NA, then "success" is a certainty.
>>> This strikes me as being a dubious proposition.  My gut feeling is that
>>> misleading results could be produced.
>>>
>>> Can anyone explain to me a rationale for this behaviour pattern?
>>> Is there some justification for it that I am not currently seeing?
>>> Any other comments?  (Please omit comments to the effect of "You are as
>>> thick as two short planks!". :-) )
>>>
>>> I have attached the example data set in a file "demoDat.txt", should
>>> anyone want to experiment with it.  The file was created using dput() so
>>> you should access it (if you wish to do so) via something like
>>>
>>>       demoDat <- dget("demoDat.txt")
>>>
>>> Thanks for any enlightenment.
>>>
>>> cheers,
>>>
>>> Rolf Turner
>>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/


From tebert @end|ng |rom u||@edu  Thu Jul 28 16:19:39 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Thu, 28 Jul 2022 14:19:39 +0000
Subject: [R] Predicted values from glm() when linear predictor is NA.
In-Reply-To: <20220728140930.62bf139a@rolf-Latitude-E7470>
References: <20220728122628.4381daa4@rolf-Latitude-E7470>
 <BN6PR2201MB15535C8743A9AB024C36EFDDCF969@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <20220728140930.62bf139a@rolf-Latitude-E7470>
Message-ID: <BN6PR2201MB15530B0D75E11F419E806E99CF969@BN6PR2201MB1553.namprd22.prod.outlook.com>

Sadly, I see that your practical options are limited. In regards to time and thermal units being equivalent if temperature is constant. The practical answer is yes. The technical answer is no because the growth chamber is not able to keep temperature constant and the insect can moderate the experienced temperature by moving to different parts of the plant (assuming this used plants and not artificial diet or leaf disks). Temperature might be more constant if these are mosquito larvae in vats of water. The water would have thermal mass to even out temperature fluctuations in the growth chamber.
   Often people assume that a growth chamber set to 25C is always 25C. Data show this is not the case even in high end or custom made growth chambers.
   Using time as a surrogate for accumulated thermal units under constant temperature will introduce some error in time due to non-uniform and non-constant temperatures within the growth chamber. Mostly the data are not present to document this let alone include this in the model. So we take the biologists view of statistics and ignore the problem that we cannot solve.

  If I measure time in days, then it makes sense that I can have egg hatch at time 0. However, it is not biologically possible to have anything happen in zero time. If my accuracy in time measurement is "days" then maybe I should consider introducing some small time value for egg hatch. Say eggs hatch at time 0.2 days. That is below the resolution of my data but reconciles the biological impossibility of anything happening in zero time. I note that time is integer in the data set.

  Another odd thing in the data. I assume that the values for "alive" represent the number of living individuals at different time intervals. At the end of the "Alive" data I note that there are a few time intervals where none are alive followed by a living individual. I am not a fan of zombies, or raising the dead (insects). Either the definition of dead is more Monty Python (https://www.youtube.com/watch?v=Jdf5EXo6I68), or I don't quite understand the data.

Tim   

-----Original Message-----
From: Rolf Turner <r.turner at auckland.ac.nz> 
Sent: Wednesday, July 27, 2022 10:10 PM
To: Ebert,Timothy Aaron <tebert at ufl.edu>
Cc: r-help <r-help at r-project.org>
Subject: Re: [R] Predicted values from glm() when linear predictor is NA.

[External Email]

On Thu, 28 Jul 2022 00:42:51 +0000
"Ebert,Timothy Aaron" <tebert at ufl.edu> wrote:

> Time is often used in this sort of problem, but really time is not 
> relevant. A better choice is accumulated thermal units. The insect 
> will molt when X thermal units have been accumulated. This is often 
> expressed as degree days, but could as easily be other units like 
> degree seconds. However, I suspect that fine time units exceeds the 
> accuracy of the measurement system. A growth chamber might maintain
> 28 C, but the temperature the insect experiences might be somewhat 
> different thereby introducing additional variability in the outcome.
> No thermal units accumulated, no development, so that is not an issue. 
> This approach allows one to predict life stage over a large 
> temperature range. Accuracy can be improved if one knows the lower 
> development threshold. At high temperatures development stops, and a 
> mortality function can be added.

Very cogent comments in respect of dealing with the underlying practical problem, but I am not so much concerned with the practical problem at the moment but rather with the workings of the software that I am using.

cheers,

Rolf

P.S.  I am at several removes from the data set(s) that I am messing about with.  But if my understanding is correct (always an assumption of which to be sceptical!) these data were collected with the temperature being held *constant*, whence time and accumulated thermal units would be equivalent.  Is it not so?

R.

--
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Thu Jul 28 16:28:38 2022
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Thu, 28 Jul 2022 16:28:38 +0200
Subject: [R] Error generated by nlme::gnls
In-Reply-To: <CAHqSRuSKFbxvNQ=YUtZj--K=tHGht_iAfM-Fh=dLTA3xx2UOWw@mail.gmail.com>
References: <CAMs=EiOaUerb4n0nV0H9SKK+ChiyNDf7WkvVZ_oO7Mue1Cx4eQ@mail.gmail.com>
 <CALrbzg1BKLoqr9jRpggtT+cZuL+AYt2j-NN6QpHV2J9SUS+VxA@mail.gmail.com>
 <20220724160324.018e9531@rolf-Latitude-E7470>
 <20220724075801.6414c486@trisector>
 <20220724230302.4784212d@rolf-Latitude-E7470>
 <20220724155725.24076488@trisector>
 <CAHqSRuSKFbxvNQ=YUtZj--K=tHGht_iAfM-Fh=dLTA3xx2UOWw@mail.gmail.com>
Message-ID: <25314.40214.756497.289582@stat.math.ethz.ch>

>>>>> Bill Dunlap 
>>>>>     on Sun, 24 Jul 2022 08:51:09 -0700 writes:

    > I think the intent of this code was to see if the formula
    > had solely a literal 1 on the right hand side.  Then
    > !identical(pp[[3]], 1) would do it, avoiding the overhead
    > of calling deparse.  Note that the 1 might be an integer,
    > which the current code would (erroneously) not catch, so
    > !identical(pp[[3]], 1) && !identical(pp[[3]], 1L) or
    > something equivalent would be better.

    > -Bill

Thanks a lot, Bill, Ivan, Ben  et al. !

This is indeed a very old coding bug triggered by the more
strict checks in  R 4.2.x.

I will indeed try Bill's proposal rather than remaining with
deparse by using deparse1().

"Of course", this should hopefully be fixed in the next release
of nlme.

Martin Maechler
ETH Zurich   and  R Core team


    > On Sun, Jul 24, 2022 at 5:58 AM Ivan Krylov
    > <krylov.r00t at gmail.com> wrote:

    >> Sorry for being too terse in my previous e-mail!
    >> 
    >> On Sun, 24 Jul 2022 23:03:02 +1200 Rolf Turner
    >> <r.turner at auckland.ac.nz> wrote:
    >> 
    >> > The maintainer of the nlme package (who is, according
    >> to maintainer(), > "R-core") could change the code so
    >> that it uses invokes deparse1() > rather than deparse,
    >> but the user cannot do so, not without in effect >
    >> re-creating the package.
    >> 
    >> You're right. I think there's a buglet in nlme::gnls that
    >> nobody noticed until R 4.2.0 was released *and* Aaron
    >> Crowley used the function with a sufficiently long
    >> formula.
    >> 
    >> > Also, the question remains: why did Aaron Crowley's
    >> code work in the > past, whereas now it throws an error?
    >> What changed?
    >> 
    >> gnls() may have been performing the `if (deparse(...) !=
    >> '1')` test for a long time, but never crashed before
    >> because it wasn't a fatal error until R
    >> 4.2.0. Previously, if() would issue a warning and use the
    >> first element of the boolean vector.
    >> 
    >> R 4.2.0 was released this April, which was less than 6
    >> months ago. I think it all fits.
    >> 
    >> A temporary solution would be to make use of the fact
    >> that R is a very dynamic language and perform surgery on
    >> a live function inside a loaded package:
    >> 
    >> library(codetools)
    >> 
    >> nlme <- loadNamespace('nlme') unlockBinding('gnls', nlme)
    >> nlme$gnls <- `body<-`(fun = nlme$gnls, value = walkCode(
    >> body(nlme$gnls), makeCodeWalker( call = function(e, w)
    >> as.call(lapply(as.list(e), function(ee) if (!missing(ee))
    >> walkCode(ee, w) )), leaf = function(e, w) if
    >> (is.symbol(e) && e == 'deparse') { as.name('deparse1') }
    >> else e ) )) lockBinding('gnls', nlme) rm(nlme)
    >> 
    >> grep('deparse', deparse(nlme::gnls), value = TRUE) # [1]
    >> " deparse1(pp[[3]]), sep = \"~\"), collapse = \",\"), " #
    >> [2] " if (deparse1(params[[nm]][[3]]) != \"1\") {" # [3]
    >> " list(row.names(dataModShrunk), deparse1(form[[2]]))), "
    >> 
    >> Aaron's example seems to work after this, modulo needing
    >> 12 starting values instead of 13.
    >> 
    >> --
    >> Best regards, Ivan
    >> 
    >> ______________________________________________
    >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    >> more, see https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide
    >> http://www.R-project.org/posting-guide.html and provide
    >> commented, minimal, self-contained, reproducible code.
    >> 

    > 	[[alternative HTML version deleted]]

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    > more, see https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide
    > http://www.R-project.org/posting-guide.html and provide
    > commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Jul 28 17:12:19 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 28 Jul 2022 08:12:19 -0700
Subject: [R] Predicted values from glm() when linear predictor is NA.
In-Reply-To: <9bae4bf6-0572-d349-d811-1dacbfba5917@mcmaster.ca>
References: <775_1658968044_26S0RNlm003716_20220728122628.4381daa4@rolf-Latitude-E7470>
 <dcc28626-39f9-3f02-9d83-38d711b16bd2@mcmaster.ca>
 <3A27E228-1C74-4753-BE6F-37074FF54143@dcn.davis.ca.us>
 <9bae4bf6-0572-d349-d811-1dacbfba5917@mcmaster.ca>
Message-ID: <6F8816F6-8600-4DF9-9B04-00FA53AA8DE1@dcn.davis.ca.us>

No, in this case I think I needed the "obvious" breakdown. Still digesting, though... I would prefer that if an arbitrary selection had been made that it be explicit .. the NA should be replaced with zero if the singular.ok argument is TRUE, rather than making that interpretation in predict.glm.

On July 28, 2022 5:45:35 AM PDT, John Fox <jfox at mcmaster.ca> wrote:
>Dear Jeff,
>
>On 2022-07-28 1:31 a.m., Jeff Newmiller wrote:
>> But "disappearing" is not what NA is supposed to do normally. Why is it being treated that way here?
>
>NA has a different meaning here than in data.
>
>By default, in glm() the argument singular.ok is TRUE, and so estimates are provided even when there are singularities, and even though the singularities are resolved arbitrarily.
>
>In this model, the columns of the model matrix labelled LifestageL1 and TrtTime:LifestageL1 are perfectly collinear -- the second is 12 times the first (both have 0s in the same rows and either 1 or 12 in three of the rows) -- and thus both can't be estimated simultaneously, but the model can be estimated by eliminating one or the other (effectively setting its coefficient to 0), or by taking any linear combination of the two regressors (i.e., using any regressor with 0s and some other value). The fitted values under the model are invariant with respect to this arbitrary choice.
>
>My apologies if I'm stating the obvious and misunderstand your objection.
>
>Best,
> John
>
>> 
>> On July 27, 2022 7:04:20 PM PDT, John Fox <jfox at mcmaster.ca> wrote:
>>> Dear Rolf,
>>> 
>>> The coefficient of TrtTime:LifestageL1 isn't estimable (as you explain) and by setting it to NA, glm() effectively removes it from the model. An equivalent model is therefore
>>> 
>>>> fit2 <- glm(cbind(Dead,Alive) ~ TrtTime + Lifestage +
>>> +               I((Lifestage == "Egg + L1")*TrtTime) +
>>> +               I((Lifestage == "L1 + L2")*TrtTime) +
>>> +               I((Lifestage == "L3")*TrtTime),
>>> +             family=binomial, data=demoDat)
>>> Warning message:
>>> glm.fit: fitted probabilities numerically 0 or 1 occurred
>>> 
>>>> cbind(coef(fit, complete=FALSE), coef(fit2))
>>>                                   [,1]         [,2]
>>> (Intercept)                -0.91718302  -0.91718302
>>> TrtTime                     0.88846195   0.88846195
>>> LifestageEgg + L1         -45.36420974 -45.36420974
>>> LifestageL1                14.27570572  14.27570572
>>> LifestageL1 + L2           -0.30332697  -0.30332697
>>> LifestageL3                -3.58672631  -3.58672631
>>> TrtTime:LifestageEgg + L1   8.10482459   8.10482459
>>> TrtTime:LifestageL1 + L2    0.05662651   0.05662651
>>> TrtTime:LifestageL3         1.66743472   1.66743472
>>> 
>>> There is no problem computing fitted values for the model, specified either way. That the fitted values when Lifestage == "L1" all round to 1 on the probability scale is coincidental -- that is, a consequence of the data.
>>> 
>>> I hope this helps,
>>> John
>>> 
>>> On 2022-07-27 8:26 p.m., Rolf Turner wrote:
>>>> 
>>>> I have a data frame with a numeric ("TrtTime") and a categorical
>>>> ("Lifestage") predictor.
>>>> 
>>>> Level "L1" of Lifestage occurs only with a single value of TrtTime,
>>>> explicitly 12, whence it is not possible to estimate a TrtTime "slope"
>>>> when Lifestage is "L1".
>>>> 
>>>> Indeed, when I fitted the model
>>>> 
>>>>       fit <- glm(cbind(Dead,Alive) ~ TrtTime*Lifestage, family=binomial,
>>>>                  data=demoDat)
>>>> 
>>>> I got:
>>>> 
>>>>> as.matrix(coef(fit))
>>>>>                                     [,1]
>>>>> (Intercept)                -0.91718302
>>>>> TrtTime                     0.88846195
>>>>> LifestageEgg + L1         -45.36420974
>>>>> LifestageL1                14.27570572
>>>>> LifestageL1 + L2           -0.30332697
>>>>> LifestageL3                -3.58672631
>>>>> TrtTime:LifestageEgg + L1   8.10482459
>>>>> TrtTime:LifestageL1                 NA
>>>>> TrtTime:LifestageL1 + L2    0.05662651
>>>>> TrtTime:LifestageL3         1.66743472
>>>> 
>>>> That is, TrtTime:LifestageL1 is NA, as expected.
>>>> 
>>>> I would have thought that fitted or predicted values corresponding to
>>>> Lifestage = "L1" would thereby be NA, but this is not the case:
>>>> 
>>>>> predict(fit)[demoDat$Lifestage=="L1"]
>>>>>         26       65      131
>>>>> 24.02007 24.02007 24.02007
>>>>> 
>>>>> fitted(fit)[demoDat$Lifestage=="L1"]
>>>>>    26  65 131
>>>>>     1   1   1
>>>> 
>>>> That is, the predicted values on the scale of the linear predictor are
>>>> large and positive, rather than being NA.
>>>> 
>>>> What this amounts to, it seems to me, is saying that if the linear
>>>> predictor in a Binomial glm is NA, then "success" is a certainty.
>>>> This strikes me as being a dubious proposition.  My gut feeling is that
>>>> misleading results could be produced.
>>>> 
>>>> Can anyone explain to me a rationale for this behaviour pattern?
>>>> Is there some justification for it that I am not currently seeing?
>>>> Any other comments?  (Please omit comments to the effect of "You are as
>>>> thick as two short planks!". :-) )
>>>> 
>>>> I have attached the example data set in a file "demoDat.txt", should
>>>> anyone want to experiment with it.  The file was created using dput() so
>>>> you should access it (if you wish to do so) via something like
>>>> 
>>>>       demoDat <- dget("demoDat.txt")
>>>> 
>>>> Thanks for any enlightenment.
>>>> 
>>>> cheers,
>>>> 
>>>> Rolf Turner
>>>> 
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>> 

-- 
Sent from my phone. Please excuse my brevity.


From j|ox @end|ng |rom mcm@@ter@c@  Thu Jul 28 17:46:13 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Thu, 28 Jul 2022 11:46:13 -0400
Subject: [R] Predicted values from glm() when linear predictor is NA.
In-Reply-To: <6F8816F6-8600-4DF9-9B04-00FA53AA8DE1@dcn.davis.ca.us>
References: <775_1658968044_26S0RNlm003716_20220728122628.4381daa4@rolf-Latitude-E7470>
 <dcc28626-39f9-3f02-9d83-38d711b16bd2@mcmaster.ca>
 <3A27E228-1C74-4753-BE6F-37074FF54143@dcn.davis.ca.us>
 <9bae4bf6-0572-d349-d811-1dacbfba5917@mcmaster.ca>
 <6F8816F6-8600-4DF9-9B04-00FA53AA8DE1@dcn.davis.ca.us>
Message-ID: <6dcb13ed-8c8c-56a0-1651-dbbf756d9916@mcmaster.ca>

Dear Jeff,

On 2022-07-28 11:12 a.m., Jeff Newmiller wrote:
> No, in this case I think I needed the "obvious" breakdown. Still digesting, though... I would prefer that if an arbitrary selection had been made that it be explicit .. the NA should be replaced with zero if the singular.ok argument is TRUE, rather than making that interpretation in predict.glm.

That's one way to think about, but another is that the model matrix X 
has 10 columns but is of rank 9. Thus 9 basis vectors are needed to span 
the column space of X, and a simple way to provide a basis is to 
eliminate a redundant column, hence the NA. The fitted values y-hat in a 
linear model are the orthogonal projection of y onto the space spanned 
by the columns of X, and are thus independent of the basis chosen. A GLM 
is a little more complicated, but it's still the column space of X 
that's important.

Best,
  John

> 
> On July 28, 2022 5:45:35 AM PDT, John Fox <jfox at mcmaster.ca> wrote:
>> Dear Jeff,
>>
>> On 2022-07-28 1:31 a.m., Jeff Newmiller wrote:
>>> But "disappearing" is not what NA is supposed to do normally. Why is it being treated that way here?
>>
>> NA has a different meaning here than in data.
>>
>> By default, in glm() the argument singular.ok is TRUE, and so estimates are provided even when there are singularities, and even though the singularities are resolved arbitrarily.
>>
>> In this model, the columns of the model matrix labelled LifestageL1 and TrtTime:LifestageL1 are perfectly collinear -- the second is 12 times the first (both have 0s in the same rows and either 1 or 12 in three of the rows) -- and thus both can't be estimated simultaneously, but the model can be estimated by eliminating one or the other (effectively setting its coefficient to 0), or by taking any linear combination of the two regressors (i.e., using any regressor with 0s and some other value). The fitted values under the model are invariant with respect to this arbitrary choice.
>>
>> My apologies if I'm stating the obvious and misunderstand your objection.
>>
>> Best,
>> John
>>
>>>
>>> On July 27, 2022 7:04:20 PM PDT, John Fox <jfox at mcmaster.ca> wrote:
>>>> Dear Rolf,
>>>>
>>>> The coefficient of TrtTime:LifestageL1 isn't estimable (as you explain) and by setting it to NA, glm() effectively removes it from the model. An equivalent model is therefore
>>>>
>>>>> fit2 <- glm(cbind(Dead,Alive) ~ TrtTime + Lifestage +
>>>> +               I((Lifestage == "Egg + L1")*TrtTime) +
>>>> +               I((Lifestage == "L1 + L2")*TrtTime) +
>>>> +               I((Lifestage == "L3")*TrtTime),
>>>> +             family=binomial, data=demoDat)
>>>> Warning message:
>>>> glm.fit: fitted probabilities numerically 0 or 1 occurred
>>>>
>>>>> cbind(coef(fit, complete=FALSE), coef(fit2))
>>>>                                    [,1]         [,2]
>>>> (Intercept)                -0.91718302  -0.91718302
>>>> TrtTime                     0.88846195   0.88846195
>>>> LifestageEgg + L1         -45.36420974 -45.36420974
>>>> LifestageL1                14.27570572  14.27570572
>>>> LifestageL1 + L2           -0.30332697  -0.30332697
>>>> LifestageL3                -3.58672631  -3.58672631
>>>> TrtTime:LifestageEgg + L1   8.10482459   8.10482459
>>>> TrtTime:LifestageL1 + L2    0.05662651   0.05662651
>>>> TrtTime:LifestageL3         1.66743472   1.66743472
>>>>
>>>> There is no problem computing fitted values for the model, specified either way. That the fitted values when Lifestage == "L1" all round to 1 on the probability scale is coincidental -- that is, a consequence of the data.
>>>>
>>>> I hope this helps,
>>>> John
>>>>
>>>> On 2022-07-27 8:26 p.m., Rolf Turner wrote:
>>>>>
>>>>> I have a data frame with a numeric ("TrtTime") and a categorical
>>>>> ("Lifestage") predictor.
>>>>>
>>>>> Level "L1" of Lifestage occurs only with a single value of TrtTime,
>>>>> explicitly 12, whence it is not possible to estimate a TrtTime "slope"
>>>>> when Lifestage is "L1".
>>>>>
>>>>> Indeed, when I fitted the model
>>>>>
>>>>>        fit <- glm(cbind(Dead,Alive) ~ TrtTime*Lifestage, family=binomial,
>>>>>                   data=demoDat)
>>>>>
>>>>> I got:
>>>>>
>>>>>> as.matrix(coef(fit))
>>>>>>                                      [,1]
>>>>>> (Intercept)                -0.91718302
>>>>>> TrtTime                     0.88846195
>>>>>> LifestageEgg + L1         -45.36420974
>>>>>> LifestageL1                14.27570572
>>>>>> LifestageL1 + L2           -0.30332697
>>>>>> LifestageL3                -3.58672631
>>>>>> TrtTime:LifestageEgg + L1   8.10482459
>>>>>> TrtTime:LifestageL1                 NA
>>>>>> TrtTime:LifestageL1 + L2    0.05662651
>>>>>> TrtTime:LifestageL3         1.66743472
>>>>>
>>>>> That is, TrtTime:LifestageL1 is NA, as expected.
>>>>>
>>>>> I would have thought that fitted or predicted values corresponding to
>>>>> Lifestage = "L1" would thereby be NA, but this is not the case:
>>>>>
>>>>>> predict(fit)[demoDat$Lifestage=="L1"]
>>>>>>          26       65      131
>>>>>> 24.02007 24.02007 24.02007
>>>>>>
>>>>>> fitted(fit)[demoDat$Lifestage=="L1"]
>>>>>>     26  65 131
>>>>>>      1   1   1
>>>>>
>>>>> That is, the predicted values on the scale of the linear predictor are
>>>>> large and positive, rather than being NA.
>>>>>
>>>>> What this amounts to, it seems to me, is saying that if the linear
>>>>> predictor in a Binomial glm is NA, then "success" is a certainty.
>>>>> This strikes me as being a dubious proposition.  My gut feeling is that
>>>>> misleading results could be produced.
>>>>>
>>>>> Can anyone explain to me a rationale for this behaviour pattern?
>>>>> Is there some justification for it that I am not currently seeing?
>>>>> Any other comments?  (Please omit comments to the effect of "You are as
>>>>> thick as two short planks!". :-) )
>>>>>
>>>>> I have attached the example data set in a file "demoDat.txt", should
>>>>> anyone want to experiment with it.  The file was created using dput() so
>>>>> you should access it (if you wish to do so) via something like
>>>>>
>>>>>        demoDat <- dget("demoDat.txt")
>>>>>
>>>>> Thanks for any enlightenment.
>>>>>
>>>>> cheers,
>>>>>
>>>>> Rolf Turner
>>>>>
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
> 
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Jul 28 18:50:02 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 28 Jul 2022 09:50:02 -0700
Subject: [R] Predicted values from glm() when linear predictor is NA.
In-Reply-To: <6dcb13ed-8c8c-56a0-1651-dbbf756d9916@mcmaster.ca>
References: <775_1658968044_26S0RNlm003716_20220728122628.4381daa4@rolf-Latitude-E7470>
 <dcc28626-39f9-3f02-9d83-38d711b16bd2@mcmaster.ca>
 <3A27E228-1C74-4753-BE6F-37074FF54143@dcn.davis.ca.us>
 <9bae4bf6-0572-d349-d811-1dacbfba5917@mcmaster.ca>
 <6F8816F6-8600-4DF9-9B04-00FA53AA8DE1@dcn.davis.ca.us>
 <6dcb13ed-8c8c-56a0-1651-dbbf756d9916@mcmaster.ca>
Message-ID: <D918BB71-61D1-49AB-8350-07FCF3B5D2C2@dcn.davis.ca.us>

Yes, I am familiar with linear algebra. I nod along with you until you get to "hence" and then you make a leap. The user made the decision (or accepted the default) that the rank problem be ignored... they have the option to reduce their model, but they wanted this coefficient (communicated via the model and the parameter) to behave as if it were zero. The issue is that that decision is getting hidden behind this additional implementation decision to exclude certain columns in the model matrix _even after the model has been solved_. If they try to use the coefficients themselves then they have to apply this odd interpretation of NA that is embedded in predict.glm but not apparent in their model instead of using a simple newdata %*% coefs.

It may be easier to implement summary.glm with an NA there as a hint to the rank deficiency, but at the cost of mathematical inconsistency in the reporting of coefficients. IMO either the NA should always be presented to the user as if it were a zero or the rank deficiency should be recorded separately.

On July 28, 2022 8:46:13 AM PDT, John Fox <jfox at mcmaster.ca> wrote:
>Dear Jeff,
>
>On 2022-07-28 11:12 a.m., Jeff Newmiller wrote:
>> No, in this case I think I needed the "obvious" breakdown. Still digesting, though... I would prefer that if an arbitrary selection had been made that it be explicit .. the NA should be replaced with zero if the singular.ok argument is TRUE, rather than making that interpretation in predict.glm.
>
>That's one way to think about, but another is that the model matrix X has 10 columns but is of rank 9. Thus 9 basis vectors are needed to span the column space of X, and a simple way to provide a basis is to eliminate a redundant column, hence the NA. The fitted values y-hat in a linear model are the orthogonal projection of y onto the space spanned by the columns of X, and are thus independent of the basis chosen. A GLM is a little more complicated, but it's still the column space of X that's important.
>
>Best,
> John
>
>> 
>> On July 28, 2022 5:45:35 AM PDT, John Fox <jfox at mcmaster.ca> wrote:
>>> Dear Jeff,
>>> 
>>> On 2022-07-28 1:31 a.m., Jeff Newmiller wrote:
>>>> But "disappearing" is not what NA is supposed to do normally. Why is it being treated that way here?
>>> 
>>> NA has a different meaning here than in data.
>>> 
>>> By default, in glm() the argument singular.ok is TRUE, and so estimates are provided even when there are singularities, and even though the singularities are resolved arbitrarily.
>>> 
>>> In this model, the columns of the model matrix labelled LifestageL1 and TrtTime:LifestageL1 are perfectly collinear -- the second is 12 times the first (both have 0s in the same rows and either 1 or 12 in three of the rows) -- and thus both can't be estimated simultaneously, but the model can be estimated by eliminating one or the other (effectively setting its coefficient to 0), or by taking any linear combination of the two regressors (i.e., using any regressor with 0s and some other value). The fitted values under the model are invariant with respect to this arbitrary choice.
>>> 
>>> My apologies if I'm stating the obvious and misunderstand your objection.
>>> 
>>> Best,
>>> John
>>> 
>>>> 
>>>> On July 27, 2022 7:04:20 PM PDT, John Fox <jfox at mcmaster.ca> wrote:
>>>>> Dear Rolf,
>>>>> 
>>>>> The coefficient of TrtTime:LifestageL1 isn't estimable (as you explain) and by setting it to NA, glm() effectively removes it from the model. An equivalent model is therefore
>>>>> 
>>>>>> fit2 <- glm(cbind(Dead,Alive) ~ TrtTime + Lifestage +
>>>>> +               I((Lifestage == "Egg + L1")*TrtTime) +
>>>>> +               I((Lifestage == "L1 + L2")*TrtTime) +
>>>>> +               I((Lifestage == "L3")*TrtTime),
>>>>> +             family=binomial, data=demoDat)
>>>>> Warning message:
>>>>> glm.fit: fitted probabilities numerically 0 or 1 occurred
>>>>> 
>>>>>> cbind(coef(fit, complete=FALSE), coef(fit2))
>>>>>                                    [,1]         [,2]
>>>>> (Intercept)                -0.91718302  -0.91718302
>>>>> TrtTime                     0.88846195   0.88846195
>>>>> LifestageEgg + L1         -45.36420974 -45.36420974
>>>>> LifestageL1                14.27570572  14.27570572
>>>>> LifestageL1 + L2           -0.30332697  -0.30332697
>>>>> LifestageL3                -3.58672631  -3.58672631
>>>>> TrtTime:LifestageEgg + L1   8.10482459   8.10482459
>>>>> TrtTime:LifestageL1 + L2    0.05662651   0.05662651
>>>>> TrtTime:LifestageL3         1.66743472   1.66743472
>>>>> 
>>>>> There is no problem computing fitted values for the model, specified either way. That the fitted values when Lifestage == "L1" all round to 1 on the probability scale is coincidental -- that is, a consequence of the data.
>>>>> 
>>>>> I hope this helps,
>>>>> John
>>>>> 
>>>>> On 2022-07-27 8:26 p.m., Rolf Turner wrote:
>>>>>> 
>>>>>> I have a data frame with a numeric ("TrtTime") and a categorical
>>>>>> ("Lifestage") predictor.
>>>>>> 
>>>>>> Level "L1" of Lifestage occurs only with a single value of TrtTime,
>>>>>> explicitly 12, whence it is not possible to estimate a TrtTime "slope"
>>>>>> when Lifestage is "L1".
>>>>>> 
>>>>>> Indeed, when I fitted the model
>>>>>> 
>>>>>>        fit <- glm(cbind(Dead,Alive) ~ TrtTime*Lifestage, family=binomial,
>>>>>>                   data=demoDat)
>>>>>> 
>>>>>> I got:
>>>>>> 
>>>>>>> as.matrix(coef(fit))
>>>>>>>                                      [,1]
>>>>>>> (Intercept)                -0.91718302
>>>>>>> TrtTime                     0.88846195
>>>>>>> LifestageEgg + L1         -45.36420974
>>>>>>> LifestageL1                14.27570572
>>>>>>> LifestageL1 + L2           -0.30332697
>>>>>>> LifestageL3                -3.58672631
>>>>>>> TrtTime:LifestageEgg + L1   8.10482459
>>>>>>> TrtTime:LifestageL1                 NA
>>>>>>> TrtTime:LifestageL1 + L2    0.05662651
>>>>>>> TrtTime:LifestageL3         1.66743472
>>>>>> 
>>>>>> That is, TrtTime:LifestageL1 is NA, as expected.
>>>>>> 
>>>>>> I would have thought that fitted or predicted values corresponding to
>>>>>> Lifestage = "L1" would thereby be NA, but this is not the case:
>>>>>> 
>>>>>>> predict(fit)[demoDat$Lifestage=="L1"]
>>>>>>>          26       65      131
>>>>>>> 24.02007 24.02007 24.02007
>>>>>>> 
>>>>>>> fitted(fit)[demoDat$Lifestage=="L1"]
>>>>>>>     26  65 131
>>>>>>>      1   1   1
>>>>>> 
>>>>>> That is, the predicted values on the scale of the linear predictor are
>>>>>> large and positive, rather than being NA.
>>>>>> 
>>>>>> What this amounts to, it seems to me, is saying that if the linear
>>>>>> predictor in a Binomial glm is NA, then "success" is a certainty.
>>>>>> This strikes me as being a dubious proposition.  My gut feeling is that
>>>>>> misleading results could be produced.
>>>>>> 
>>>>>> Can anyone explain to me a rationale for this behaviour pattern?
>>>>>> Is there some justification for it that I am not currently seeing?
>>>>>> Any other comments?  (Please omit comments to the effect of "You are as
>>>>>> thick as two short planks!". :-) )
>>>>>> 
>>>>>> I have attached the example data set in a file "demoDat.txt", should
>>>>>> anyone want to experiment with it.  The file was created using dput() so
>>>>>> you should access it (if you wish to do so) via something like
>>>>>> 
>>>>>>        demoDat <- dget("demoDat.txt")
>>>>>> 
>>>>>> Thanks for any enlightenment.
>>>>>> 
>>>>>> cheers,
>>>>>> 
>>>>>> Rolf Turner
>>>>>> 
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>> 

-- 
Sent from my phone. Please excuse my brevity.


From @vi@e@gross m@iii@g oii gm@ii@com  Thu Jul 28 19:27:38 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Thu, 28 Jul 2022 13:27:38 -0400
Subject: [R] Parsing XML?
In-Reply-To: <2dc13b7d-17df-21ce-6161-fd593214d9ab@effectivedefense.org>
References: <20220727145903.4338c449692f703b@ask.loc.gov>
 <7743581c-0959-1df6-bb31-f140161f83e1@effectivedefense.org>
 <CABcYAdLxyWZp5QGH_-3NNZ4zRu3Dp-qF1NK399VD-Rej=Fg-bg@mail.gmail.com>
 <2dc13b7d-17df-21ce-6161-fd593214d9ab@effectivedefense.org>
Message-ID: <005301d8a2a7$5645c200$02d14600$@gmail.com>

Spencer,

You have lots of learning to do it you want to be able to properly play around inside XML. The list you see of main nodesis an example of many things you can do BUT is very incomplete. This is an R forum, not an XML forum, so the focus you need is what R packages not only let you read in XML properly but let you navigate it make queries using an underlying xpath mechanism or anything else.  You must find which package meets some needs AND also be able to in some logical way specify what you want it to find.

But if the XML data is not set up for you to find, you are wasting your time and ours.

I looked at your raw XML file and searched for Dates by looking for things like "Nov" and my suspicion is they are NOT set up with fields specifying things like First Date, Last Date or anything like that. There may be other dates formatted in ways that may not be obvious, like number of seconds since 1970. Do you have access to a description of what the file should look like?

The dates I found seem to often be parts of comments.

If you were using xpath, you might look for something like "//recordData/record/datafield/subfield" carefully as I found dates such as:

    <datafield ind1=" " ind2=" " tag="588">
      <subfield code="a">Description based on: Nov. 4, 2010 (surrogate); title from caption.</subfield>
    </datafield>


    <datafield ind1="0" ind2=" " tag="362">
      <subfield code="a">Vol. 1, no. 1 (Nov. 16, 1994)-</subfield>
    </datafield>

    <datafield ind1="1" ind2=" " tag="362">
      <subfield code="a">Began in May 1992; ceased with Jan. 10, 2013.</subfield>
    </datafield>

Note this entry tells when it ceased. The next when something began;

<datafield ind1="1" ind2=" " tag="362">
      <subfield code="a">Began in Jan. 1990.</subfield>
    </datafield>

BUT looking for patterns, they are NOT all found with code="a" as in this case:

<datafield ind1=" " ind2=" " tag="321">
      <subfield code="a">Semiweekly,</subfield>
      <subfield code="b">&lt;Apr. 4, 1990-&gt;</subfield>
    </datafield>

My guess is this XML pretty much has the same info from your perspective as the JSON version, albeit a challenge to search. It likely does NOT have the info you need for your speculation about when a newpaper or magazine STOPPED publishing except maybe in a note here and there that needs a human.

You can write R software that after getting the XML will perform searches and let you print something carefully sich as searching for subfield containing the attribute code="t" is clearly a title of a periodical.

Again, well-designed XML might have had the data you want. If this was info about people and contained fields identifiable as a date of birth and a date of death, you could play games. But it does not seem to be and is more like notes about what issues something is in than when a periodical stopped publishing.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Spencer Graves
Sent: Thursday, July 28, 2022 6:53 AM
To: Richard O'Keefe <raoknz at gmail.com>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] Parsing XML?

Hi, Richard et al.:


On 7/28/22 1:50 AM, Richard O'Keefe wrote:
> What do you mean by "a list that I can understand"?
> A quick tally of the number of XML elements by identifier:
> 1 echoedSearchRetrieveRequest
> 1 frbrGrouping
> 1 maximumRecords
> 1 nextRecordPosition
> 1 numberOfRecords
> 1 query
> 1 records
> 1 resultSetIdleTime
> 1 searchRetrieveResponse
> 1 servicelevel
> 1 sortKeys
> 1 startRecord
> 1 wskey
> 2 version
> 50 leader
> 50 recordData
> 51 recordPacking
> 51 recordSchema
> 100 record
> 105 controlfield
> 923 datafield
> 1900 subfield


	  How did you get that?


	  Please forgive me for being so dense.  I've done several web searches 
and tried to work several tutorials, etc., without so far seeing what I 
might do that could be informative.


	  Even this list of "XML elements by identifiers" STILL does not 
include things like the name of the newspaper and publisher plus start 
and end dates.  I believe these fields are there, but I can't see how to 
parse them.  I earlier parsed a JSON version of essentially the same 
dataset.  However, the JSON version seemed not to distinguish between 
newspapers that were still publishing and those for which the end date 
was unknown.  My contact at the Library of Congress then suggested I 
parse the XML version.


	  Thanks,
	  Spencer

> 
> What of this information do you actually want?
> The elements of the list should be what?
> 
> 
> On Thu, 28 Jul 2022 at 08:52, Spencer Graves 
> <spencer.graves at effectivedefense.org 
> <mailto:spencer.graves at effectivedefense.org>> wrote:
> 
>     Hello, All:
> 
> 
>                What would you suggest I do to parse the following XML
>     file into a
>     list that I can understand:
> 
> 
>     XMLfile <-
>     "https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/ndnp_Alabama_all-yrs_e_0001_0050.xml
>     <https://chroniclingamerica.loc.gov/data/bib/worldcat_titles/bulk5/ndnp_Alabama_all-yrs_e_0001_0050.xml>"
> 
> 
> 
> 
>                This is the first of 6666 XML files containing "U.S.
>     Newspaper
>     Directory" maintained by the US Library of Congress discussed in the
>     thread below.  I've tried various things using the XML and xml2.
> 
> 
>     XMLdata <- xml2::read_xml(XMLfile)
>     str(XMLdata)
>     XMLdat <- XML::xmlParse(XMLdata)
>     str(XMLdat)
>     XMLtxt <- xml2::xml_text(XMLdata)
>     nchar(XMLtxt)
>     #[1] 29415
> 
> 
>                Someplace there's a schema for this.  I don't know if
>     it's embedded
>     in this XML file or in a separate file.  If it's in a separate file,
>     how
>     could I describe it to my contacts with the Library of Congress so they
>     would understand what I needed and could help me get it.
> 
> 
>                Thanks,
>                Spencer Graves
> 
> 
>     p.s.  All 29415 characters in XMLtext appear in the thread below.
> 
> 
>     -------- Forwarded Message --------
>     Subject:        [Newspapers and Current Periodicals] How can I get
>     counts of
>     the numbers of newspapers by year in the US, and preferably also
>     elsewhere? A search of "U.S. Newspaper Directory,
>     Date:   Wed, 27 Jul 2022 14:59:03 +0000
>     From:   Kerry Huller <serials at ask.loc.gov <mailto:serials at ask.loc.gov>>
>     To:     Spencer Graves <spencer.graves at effectivedefense.org
>     <mailto:spencer.graves at effectivedefense.org>>
>     CC: twes at loc.gov <mailto:twes at loc.gov>
> 
> 
> 
>     --# Type your reply above this line #--
> 
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jul 27 2022, 10:59am via System
> 
>     Hello Spencer,
> 
>     So, when I view the xml, I'm actually looking at it in XML editor
>     software, so I can view the tags and it's structured neatly. I've
>     copied
>     and pasted the text from the beginning of the file and the first
>     newspaper title below from my XML editor:
> 
>     <?xml version="1.0" encoding="UTF-8" standalone="no"?>
>     <?xml-stylesheet type='text/xsl'
>     href='/webservices/catalog/xsl/searchRetrieveResponse.xsl'?>
> 
>     <searchRetrieveResponse xmlns="http://www.loc.gov/zing/srw/
>     <http://www.loc.gov/zing/srw/>"
>     xmlns:oclcterms="http://purl.org/oclc/terms/
>     <http://purl.org/oclc/terms/>"
>     xmlns:dc="http://purl.org/dc/elements/1.1/
>     <http://purl.org/dc/elements/1.1/>"
>     xmlns:diag="http://www.loc.gov/zing/srw/diagnostic/
>     <http://www.loc.gov/zing/srw/diagnostic/>"
>     xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance
>     <http://www.w3.org/2001/XMLSchema-instance>">
>     <version>1.1</version>
>     <numberOfRecords>2250</numberOfRecords>
>     <records>
>     <record>
>     <recordSchema>info:srw/schema/1/marcxml</recordSchema>
>     <recordPacking>xml</recordPacking>
>     <recordData>
>     <record xmlns="http://www.loc.gov/MARC21/slim
>     <http://www.loc.gov/MARC21/slim>">
>            <leader>00000nas a22000007i 4500</leader>
>            <controlfield tag="001">1030438981</controlfield>
>            <controlfield tag="008">180404c20159999aluwr n       0   a0eng
>        </controlfield>
>            <datafield ind1=" " ind2=" " tag="010">
>              <subfield code="a">  2018200464</subfield>
>            </datafield>
>            <datafield ind1=" " ind2=" " tag="040">
>              <subfield code="a">DLC</subfield>
>              <subfield code="e">rda</subfield>
>              <subfield code="c">DLC</subfield>
>              <subfield code="b">eng</subfield>
>            </datafield>
>            <datafield ind1=" " ind2=" " tag="012">
>              <subfield code="m">1</subfield>
>            </datafield>
>            <datafield ind1="0" ind2=" " tag="022">
>              <subfield code="a">2577-5316</subfield>
>              <subfield code="2">1</subfield>
>            </datafield>
>            <datafield ind1=" " ind2=" " tag="032">
>              <subfield code="a">021110</subfield>
>              <subfield code="b">USPS</subfield>
>            </datafield>
>            <datafield ind1=" " ind2=" " tag="037">
>              <subfield code="b">711 Alabama Avenue, Selma, AL
>     36701</subfield>
>            </datafield>
>            <datafield ind1=" " ind2=" " tag="042">
>              <subfield code="a">nsdp</subfield>
>              <subfield code="a">pcc</subfield>
>            </datafield>
>            <datafield ind1="1" ind2="0" tag="050">
>              <subfield code="a">ISSN RECORD</subfield>
>            </datafield>
>            <datafield ind1="1" ind2="0" tag="082">
>              <subfield code="a">071</subfield>
>              <subfield code="2">15</subfield>
>            </datafield>
>            <datafield ind1=" " ind2="0" tag="222">
>              <subfield code="a">Selma sun</subfield>
>            </datafield>
>            <datafield ind1="0" ind2="0" tag="245">
>              <subfield code="a">Selma sun.</subfield>
>            </datafield>
>            <datafield ind1=" " ind2="1" tag="264">
>              <subfield code="a">Selma, AL :</subfield>
>              <subfield code="b">North Shore Press, LLC</subfield>
>              <subfield code="c">2016-</subfield>
>            </datafield>
>            <datafield ind1=" " ind2=" " tag="310">
>              <subfield code="a">Weekly</subfield>
>            </datafield>
>            <datafield ind1=" " ind2=" " tag="336">
>              <subfield code="a">text</subfield>
>              <subfield code="b">txt</subfield>
>              <subfield code="2">rdacontent</subfield>
>            </datafield>
>            <datafield ind1=" " ind2=" " tag="337">
>              <subfield code="a">unmediated</subfield>
>              <subfield code="b">n</subfield>
>              <subfield code="2">rdamedia</subfield>
>            </datafield>
>            <datafield ind1=" " ind2=" " tag="338">
>              <subfield code="a">volume</subfield>
>              <subfield code="b">nc</subfield>
>              <subfield code="2">rdacarrier</subfield>
>            </datafield>
>            <datafield ind1="1" ind2=" " tag="362">
>              <subfield code="a">Began in 2015.</subfield>
>            </datafield>
>            <datafield ind1=" " ind2=" " tag="588">
>              <subfield code="a">Description based on: Volume 2, Issue 40
>     (October 5, 2017) (surrogate); title from caption.</subfield>
>            </datafield>
>            <datafield ind1=" " ind2=" " tag="588">
>              <subfield code="a">Latest issue consulted: Volume 2, Issue 40
>     (October 5, 2017).</subfield>
>            </datafield>
>            <datafield ind1=" " ind2=" " tag="752">
>              <subfield code="a">United States</subfield>
>              <subfield code="b">Alabama</subfield>
>              <subfield code="c">Dallas</subfield>
>              <subfield code="d">Selma.</subfield>
>            </datafield>
>          </record>
>     </recordData>
>     </record>
> 
>     When I view the records in the XML editor, these 2 lines below do begin
>     each of the records for each individual title, but of course this is
>     including the xml tags:
> 
>     <recordSchema>info:srw/schema/1/marcxml</recordSchema>
>     <recordPacking>xml</recordPacking>
> 
>     Hopefully this helps you decide where to break or parse each record.
> 
>     On another note, I just noticed as well that at the top of this first
>     file it lists the total number of records for the Alabama grouping -
>     2250. This also appeared to be the case for the Alaska records when I
>     took a look at the first one for that state. I imagine that should be
>     consistent throughout each "grouping" of records.
> 
>     Let me know if you have follow-up questions!
> 
>     Best wishes,
> 
>     Kerry Huller
>     Newspaper & Current Periodical Reading Room
>     Serial & Government Publications Division
>     Library of Congress
> 
>     ------------------------------------------------------------------------
> 
>     Newspapers and Current Periodicals Reference Librarian
> 
>     Jul 27 2022, 10:21am via Email
> 
>     Hi, Kerry:
> 
> 
>     Thanks. I understand the chunking in files of at most 50. I've read
>     the first file "ndnp_Alabama_all-yrs_e_0001_0050.xml" into a string of
>     29415 characters, copied below. Might you have any suggestions on the
>     next step in parsing this? Staring at it now, it looks splitting on
>     "info:srw/schema/1/marcxmlxml" might convert the 29415 characters into
>     shorter chunks, each of which could then be parsed further.
> 
> 
>     This is not as bad as reading ancient Egyptian heiroglyphics without
>     the Rosetta Stone, but I wondered if you might have something that could
>     make this work easier and more reliable? I guess I could compare with
>     what I already read as JSON ;-)
> 
> 
>     Thanks,
>     Spencer Graves
> 


From j|ox @end|ng |rom mcm@@ter@c@  Thu Jul 28 19:47:07 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Thu, 28 Jul 2022 13:47:07 -0400
Subject: [R] Predicted values from glm() when linear predictor is NA.
In-Reply-To: <22757_1659027030_26SGoUMm030525_D918BB71-61D1-49AB-8350-07FCF3B5D2C2@dcn.davis.ca.us>
References: <775_1658968044_26S0RNlm003716_20220728122628.4381daa4@rolf-Latitude-E7470>
 <dcc28626-39f9-3f02-9d83-38d711b16bd2@mcmaster.ca>
 <3A27E228-1C74-4753-BE6F-37074FF54143@dcn.davis.ca.us>
 <9bae4bf6-0572-d349-d811-1dacbfba5917@mcmaster.ca>
 <6F8816F6-8600-4DF9-9B04-00FA53AA8DE1@dcn.davis.ca.us>
 <6dcb13ed-8c8c-56a0-1651-dbbf756d9916@mcmaster.ca>
 <22757_1659027030_26SGoUMm030525_D918BB71-61D1-49AB-8350-07FCF3B5D2C2@dcn.davis.ca.us>
Message-ID: <8ae36f19-2413-dbc6-37a9-4d4e5e4921d4@mcmaster.ca>

Dear Jeff,

I'm not sure that pursuing this further is sensible, since I don't think 
that we disagree about anything of consequence, but I'll take one more 
crack at explaining why I think lm()'s behaviour is reasonable, if not 
the only approach:

On 2022-07-28 12:50 p.m., Jeff Newmiller wrote:
> Yes, I am familiar with linear algebra. I nod along with you until you get to "hence" and then you make a leap. The user made the decision (or accepted the default) that the rank problem be ignored... they have the option to reduce their model, but they wanted this coefficient (communicated via the model and the parameter) to behave as if it were zero. The issue is that that decision is getting hidden behind this additional implementation decision to exclude certain columns in the model matrix _even after the model has been solved_. 

But the model is "solved" by reducing the columns of the model matrix to 
a linearly independent set, providing a basis for the column space of 
the original rank-deficient model matrix.

> If they try to use the coefficients themselves then they have to apply this odd interpretation of NA that is embedded in predict.glm but not apparent in their model instead of using a simple newdata %*% coefs.
> 

One could do newdata %*% coef(model, complete=FALSE).

> It may be easier to implement summary.glm with an NA there as a hint to the rank deficiency, but at the cost of mathematical inconsistency in the reporting of coefficients. IMO either the NA should always be presented to the user as if it were a zero or the rank deficiency should be recorded separately.

There are advantages and disadvantages to reducing the model matrix to 
full column rank. The strategy you advocate -- to retain the 
deficient-rank model matrix but constrain the coefficients by setting 
one arbitrarily to 0 -- is coherent, and is similar, if I remember 
right, to what's implemented in the linear model procedure in SAS. This 
is really six of one and a half-dozen of the other.

Best,
John

> 
> On July 28, 2022 8:46:13 AM PDT, John Fox <jfox at mcmaster.ca> wrote:
>> Dear Jeff,
>>
>> On 2022-07-28 11:12 a.m., Jeff Newmiller wrote:
>>> No, in this case I think I needed the "obvious" breakdown. Still digesting, though... I would prefer that if an arbitrary selection had been made that it be explicit .. the NA should be replaced with zero if the singular.ok argument is TRUE, rather than making that interpretation in predict.glm.
>>
>> That's one way to think about, but another is that the model matrix X has 10 columns but is of rank 9. Thus 9 basis vectors are needed to span the column space of X, and a simple way to provide a basis is to eliminate a redundant column, hence the NA. The fitted values y-hat in a linear model are the orthogonal projection of y onto the space spanned by the columns of X, and are thus independent of the basis chosen. A GLM is a little more complicated, but it's still the column space of X that's important.
>>
>> Best,
>> John
>>
>>>
>>> On July 28, 2022 5:45:35 AM PDT, John Fox <jfox at mcmaster.ca> wrote:
>>>> Dear Jeff,
>>>>
>>>> On 2022-07-28 1:31 a.m., Jeff Newmiller wrote:
>>>>> But "disappearing" is not what NA is supposed to do normally. Why is it being treated that way here?
>>>>
>>>> NA has a different meaning here than in data.
>>>>
>>>> By default, in glm() the argument singular.ok is TRUE, and so estimates are provided even when there are singularities, and even though the singularities are resolved arbitrarily.
>>>>
>>>> In this model, the columns of the model matrix labelled LifestageL1 and TrtTime:LifestageL1 are perfectly collinear -- the second is 12 times the first (both have 0s in the same rows and either 1 or 12 in three of the rows) -- and thus both can't be estimated simultaneously, but the model can be estimated by eliminating one or the other (effectively setting its coefficient to 0), or by taking any linear combination of the two regressors (i.e., using any regressor with 0s and some other value). The fitted values under the model are invariant with respect to this arbitrary choice.
>>>>
>>>> My apologies if I'm stating the obvious and misunderstand your objection.
>>>>
>>>> Best,
>>>> John
>>>>
>>>>>
>>>>> On July 27, 2022 7:04:20 PM PDT, John Fox <jfox at mcmaster.ca> wrote:
>>>>>> Dear Rolf,
>>>>>>
>>>>>> The coefficient of TrtTime:LifestageL1 isn't estimable (as you explain) and by setting it to NA, glm() effectively removes it from the model. An equivalent model is therefore
>>>>>>
>>>>>>> fit2 <- glm(cbind(Dead,Alive) ~ TrtTime + Lifestage +
>>>>>> +               I((Lifestage == "Egg + L1")*TrtTime) +
>>>>>> +               I((Lifestage == "L1 + L2")*TrtTime) +
>>>>>> +               I((Lifestage == "L3")*TrtTime),
>>>>>> +             family=binomial, data=demoDat)
>>>>>> Warning message:
>>>>>> glm.fit: fitted probabilities numerically 0 or 1 occurred
>>>>>>
>>>>>>> cbind(coef(fit, complete=FALSE), coef(fit2))
>>>>>>                                     [,1]         [,2]
>>>>>> (Intercept)                -0.91718302  -0.91718302
>>>>>> TrtTime                     0.88846195   0.88846195
>>>>>> LifestageEgg + L1         -45.36420974 -45.36420974
>>>>>> LifestageL1                14.27570572  14.27570572
>>>>>> LifestageL1 + L2           -0.30332697  -0.30332697
>>>>>> LifestageL3                -3.58672631  -3.58672631
>>>>>> TrtTime:LifestageEgg + L1   8.10482459   8.10482459
>>>>>> TrtTime:LifestageL1 + L2    0.05662651   0.05662651
>>>>>> TrtTime:LifestageL3         1.66743472   1.66743472
>>>>>>
>>>>>> There is no problem computing fitted values for the model, specified either way. That the fitted values when Lifestage == "L1" all round to 1 on the probability scale is coincidental -- that is, a consequence of the data.
>>>>>>
>>>>>> I hope this helps,
>>>>>> John
>>>>>>
>>>>>> On 2022-07-27 8:26 p.m., Rolf Turner wrote:
>>>>>>>
>>>>>>> I have a data frame with a numeric ("TrtTime") and a categorical
>>>>>>> ("Lifestage") predictor.
>>>>>>>
>>>>>>> Level "L1" of Lifestage occurs only with a single value of TrtTime,
>>>>>>> explicitly 12, whence it is not possible to estimate a TrtTime "slope"
>>>>>>> when Lifestage is "L1".
>>>>>>>
>>>>>>> Indeed, when I fitted the model
>>>>>>>
>>>>>>>         fit <- glm(cbind(Dead,Alive) ~ TrtTime*Lifestage, family=binomial,
>>>>>>>                    data=demoDat)
>>>>>>>
>>>>>>> I got:
>>>>>>>
>>>>>>>> as.matrix(coef(fit))
>>>>>>>>                                       [,1]
>>>>>>>> (Intercept)                -0.91718302
>>>>>>>> TrtTime                     0.88846195
>>>>>>>> LifestageEgg + L1         -45.36420974
>>>>>>>> LifestageL1                14.27570572
>>>>>>>> LifestageL1 + L2           -0.30332697
>>>>>>>> LifestageL3                -3.58672631
>>>>>>>> TrtTime:LifestageEgg + L1   8.10482459
>>>>>>>> TrtTime:LifestageL1                 NA
>>>>>>>> TrtTime:LifestageL1 + L2    0.05662651
>>>>>>>> TrtTime:LifestageL3         1.66743472
>>>>>>>
>>>>>>> That is, TrtTime:LifestageL1 is NA, as expected.
>>>>>>>
>>>>>>> I would have thought that fitted or predicted values corresponding to
>>>>>>> Lifestage = "L1" would thereby be NA, but this is not the case:
>>>>>>>
>>>>>>>> predict(fit)[demoDat$Lifestage=="L1"]
>>>>>>>>           26       65      131
>>>>>>>> 24.02007 24.02007 24.02007
>>>>>>>>
>>>>>>>> fitted(fit)[demoDat$Lifestage=="L1"]
>>>>>>>>      26  65 131
>>>>>>>>       1   1   1
>>>>>>>
>>>>>>> That is, the predicted values on the scale of the linear predictor are
>>>>>>> large and positive, rather than being NA.
>>>>>>>
>>>>>>> What this amounts to, it seems to me, is saying that if the linear
>>>>>>> predictor in a Binomial glm is NA, then "success" is a certainty.
>>>>>>> This strikes me as being a dubious proposition.  My gut feeling is that
>>>>>>> misleading results could be produced.
>>>>>>>
>>>>>>> Can anyone explain to me a rationale for this behaviour pattern?
>>>>>>> Is there some justification for it that I am not currently seeing?
>>>>>>> Any other comments?  (Please omit comments to the effect of "You are as
>>>>>>> thick as two short planks!". :-) )
>>>>>>>
>>>>>>> I have attached the example data set in a file "demoDat.txt", should
>>>>>>> anyone want to experiment with it.  The file was created using dput() so
>>>>>>> you should access it (if you wish to do so) via something like
>>>>>>>
>>>>>>>         demoDat <- dget("demoDat.txt")
>>>>>>>
>>>>>>> Thanks for any enlightenment.
>>>>>>>
>>>>>>> cheers,
>>>>>>>
>>>>>>> Rolf Turner
>>>>>>>
>>>>>>>
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>
> 
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/


From w||||@mwdun|@p @end|ng |rom gm@||@com  Thu Jul 28 20:00:16 2022
From: w||||@mwdun|@p @end|ng |rom gm@||@com (Bill Dunlap)
Date: Thu, 28 Jul 2022 11:00:16 -0700
Subject: [R] Predicted values from glm() when linear predictor is NA.
In-Reply-To: <D918BB71-61D1-49AB-8350-07FCF3B5D2C2@dcn.davis.ca.us>
References: <775_1658968044_26S0RNlm003716_20220728122628.4381daa4@rolf-Latitude-E7470>
 <dcc28626-39f9-3f02-9d83-38d711b16bd2@mcmaster.ca>
 <3A27E228-1C74-4753-BE6F-37074FF54143@dcn.davis.ca.us>
 <9bae4bf6-0572-d349-d811-1dacbfba5917@mcmaster.ca>
 <6F8816F6-8600-4DF9-9B04-00FA53AA8DE1@dcn.davis.ca.us>
 <6dcb13ed-8c8c-56a0-1651-dbbf756d9916@mcmaster.ca>
 <D918BB71-61D1-49AB-8350-07FCF3B5D2C2@dcn.davis.ca.us>
Message-ID: <CAHqSRuQsf18jamtJH0y=4-cvfLcSwqxwWwJ5AUj7njPzyAL=Tg@mail.gmail.com>

In this example, x has an unknown influence on y (since x doesn't vary)
   > coef(lm(y ~ x, data=data.frame(y=101:105,x=rep(7,5))))
   (Intercept)           x
           103          NA
while in this example x has no influence on y, a very different conclusion
   > coef(lm(y ~ x, data=data.frame(y=rep(103,5),x=1:5)))
    (Intercept)           x
            103           0

Are you arguing that lm's (or glm's) output should have another component
telling us whether a column was included in the model or not (i.e., if a
coefficient was well defined or not)?  Then coef() would have to copy this
information so it could show the user that (e.g., by printing '-' instead
of NA).  That might be reasonable.  Using NA for an unknown coefficient
value also seems reasonable and a lot of code currently depends upon it.
Does it need to be documented better?

-Bill


On Thu, Jul 28, 2022 at 9:50 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Yes, I am familiar with linear algebra. I nod along with you until you get
> to "hence" and then you make a leap. The user made the decision (or
> accepted the default) that the rank problem be ignored... they have the
> option to reduce their model, but they wanted this coefficient
> (communicated via the model and the parameter) to behave as if it were
> zero. The issue is that that decision is getting hidden behind this
> additional implementation decision to exclude certain columns in the model
> matrix _even after the model has been solved_. If they try to use the
> coefficients themselves then they have to apply this odd interpretation of
> NA that is embedded in predict.glm but not apparent in their model instead
> of using a simple newdata %*% coefs.
>
> It may be easier to implement summary.glm with an NA there as a hint to
> the rank deficiency, but at the cost of mathematical inconsistency in the
> reporting of coefficients. IMO either the NA should always be presented to
> the user as if it were a zero or the rank deficiency should be recorded
> separately.
>
> On July 28, 2022 8:46:13 AM PDT, John Fox <jfox at mcmaster.ca> wrote:
> >Dear Jeff,
> >
> >On 2022-07-28 11:12 a.m., Jeff Newmiller wrote:
> >> No, in this case I think I needed the "obvious" breakdown. Still
> digesting, though... I would prefer that if an arbitrary selection had been
> made that it be explicit .. the NA should be replaced with zero if the
> singular.ok argument is TRUE, rather than making that interpretation in
> predict.glm.
> >
> >That's one way to think about, but another is that the model matrix X has
> 10 columns but is of rank 9. Thus 9 basis vectors are needed to span the
> column space of X, and a simple way to provide a basis is to eliminate a
> redundant column, hence the NA. The fitted values y-hat in a linear model
> are the orthogonal projection of y onto the space spanned by the columns of
> X, and are thus independent of the basis chosen. A GLM is a little more
> complicated, but it's still the column space of X that's important.
> >
> >Best,
> > John
> >
> >>
> >> On July 28, 2022 5:45:35 AM PDT, John Fox <jfox at mcmaster.ca> wrote:
> >>> Dear Jeff,
> >>>
> >>> On 2022-07-28 1:31 a.m., Jeff Newmiller wrote:
> >>>> But "disappearing" is not what NA is supposed to do normally. Why is
> it being treated that way here?
> >>>
> >>> NA has a different meaning here than in data.
> >>>
> >>> By default, in glm() the argument singular.ok is TRUE, and so
> estimates are provided even when there are singularities, and even though
> the singularities are resolved arbitrarily.
> >>>
> >>> In this model, the columns of the model matrix labelled LifestageL1
> and TrtTime:LifestageL1 are perfectly collinear -- the second is 12 times
> the first (both have 0s in the same rows and either 1 or 12 in three of the
> rows) -- and thus both can't be estimated simultaneously, but the model can
> be estimated by eliminating one or the other (effectively setting its
> coefficient to 0), or by taking any linear combination of the two
> regressors (i.e., using any regressor with 0s and some other value). The
> fitted values under the model are invariant with respect to this arbitrary
> choice.
> >>>
> >>> My apologies if I'm stating the obvious and misunderstand your
> objection.
> >>>
> >>> Best,
> >>> John
> >>>
> >>>>
> >>>> On July 27, 2022 7:04:20 PM PDT, John Fox <jfox at mcmaster.ca> wrote:
> >>>>> Dear Rolf,
> >>>>>
> >>>>> The coefficient of TrtTime:LifestageL1 isn't estimable (as you
> explain) and by setting it to NA, glm() effectively removes it from the
> model. An equivalent model is therefore
> >>>>>
> >>>>>> fit2 <- glm(cbind(Dead,Alive) ~ TrtTime + Lifestage +
> >>>>> +               I((Lifestage == "Egg + L1")*TrtTime) +
> >>>>> +               I((Lifestage == "L1 + L2")*TrtTime) +
> >>>>> +               I((Lifestage == "L3")*TrtTime),
> >>>>> +             family=binomial, data=demoDat)
> >>>>> Warning message:
> >>>>> glm.fit: fitted probabilities numerically 0 or 1 occurred
> >>>>>
> >>>>>> cbind(coef(fit, complete=FALSE), coef(fit2))
> >>>>>                                    [,1]         [,2]
> >>>>> (Intercept)                -0.91718302  -0.91718302
> >>>>> TrtTime                     0.88846195   0.88846195
> >>>>> LifestageEgg + L1         -45.36420974 -45.36420974
> >>>>> LifestageL1                14.27570572  14.27570572
> >>>>> LifestageL1 + L2           -0.30332697  -0.30332697
> >>>>> LifestageL3                -3.58672631  -3.58672631
> >>>>> TrtTime:LifestageEgg + L1   8.10482459   8.10482459
> >>>>> TrtTime:LifestageL1 + L2    0.05662651   0.05662651
> >>>>> TrtTime:LifestageL3         1.66743472   1.66743472
> >>>>>
> >>>>> There is no problem computing fitted values for the model, specified
> either way. That the fitted values when Lifestage == "L1" all round to 1 on
> the probability scale is coincidental -- that is, a consequence of the data.
> >>>>>
> >>>>> I hope this helps,
> >>>>> John
> >>>>>
> >>>>> On 2022-07-27 8:26 p.m., Rolf Turner wrote:
> >>>>>>
> >>>>>> I have a data frame with a numeric ("TrtTime") and a categorical
> >>>>>> ("Lifestage") predictor.
> >>>>>>
> >>>>>> Level "L1" of Lifestage occurs only with a single value of TrtTime,
> >>>>>> explicitly 12, whence it is not possible to estimate a TrtTime
> "slope"
> >>>>>> when Lifestage is "L1".
> >>>>>>
> >>>>>> Indeed, when I fitted the model
> >>>>>>
> >>>>>>        fit <- glm(cbind(Dead,Alive) ~ TrtTime*Lifestage,
> family=binomial,
> >>>>>>                   data=demoDat)
> >>>>>>
> >>>>>> I got:
> >>>>>>
> >>>>>>> as.matrix(coef(fit))
> >>>>>>>                                      [,1]
> >>>>>>> (Intercept)                -0.91718302
> >>>>>>> TrtTime                     0.88846195
> >>>>>>> LifestageEgg + L1         -45.36420974
> >>>>>>> LifestageL1                14.27570572
> >>>>>>> LifestageL1 + L2           -0.30332697
> >>>>>>> LifestageL3                -3.58672631
> >>>>>>> TrtTime:LifestageEgg + L1   8.10482459
> >>>>>>> TrtTime:LifestageL1                 NA
> >>>>>>> TrtTime:LifestageL1 + L2    0.05662651
> >>>>>>> TrtTime:LifestageL3         1.66743472
> >>>>>>
> >>>>>> That is, TrtTime:LifestageL1 is NA, as expected.
> >>>>>>
> >>>>>> I would have thought that fitted or predicted values corresponding
> to
> >>>>>> Lifestage = "L1" would thereby be NA, but this is not the case:
> >>>>>>
> >>>>>>> predict(fit)[demoDat$Lifestage=="L1"]
> >>>>>>>          26       65      131
> >>>>>>> 24.02007 24.02007 24.02007
> >>>>>>>
> >>>>>>> fitted(fit)[demoDat$Lifestage=="L1"]
> >>>>>>>     26  65 131
> >>>>>>>      1   1   1
> >>>>>>
> >>>>>> That is, the predicted values on the scale of the linear predictor
> are
> >>>>>> large and positive, rather than being NA.
> >>>>>>
> >>>>>> What this amounts to, it seems to me, is saying that if the linear
> >>>>>> predictor in a Binomial glm is NA, then "success" is a certainty.
> >>>>>> This strikes me as being a dubious proposition.  My gut feeling is
> that
> >>>>>> misleading results could be produced.
> >>>>>>
> >>>>>> Can anyone explain to me a rationale for this behaviour pattern?
> >>>>>> Is there some justification for it that I am not currently seeing?
> >>>>>> Any other comments?  (Please omit comments to the effect of "You
> are as
> >>>>>> thick as two short planks!". :-) )
> >>>>>>
> >>>>>> I have attached the example data set in a file "demoDat.txt", should
> >>>>>> anyone want to experiment with it.  The file was created using
> dput() so
> >>>>>> you should access it (if you wish to do so) via something like
> >>>>>>
> >>>>>>        demoDat <- dget("demoDat.txt")
> >>>>>>
> >>>>>> Thanks for any enlightenment.
> >>>>>>
> >>>>>> cheers,
> >>>>>>
> >>>>>> Rolf Turner
> >>>>>>
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Jul 28 22:58:51 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 28 Jul 2022 13:58:51 -0700
Subject: [R] Predicted values from glm() when linear predictor is NA.
In-Reply-To: <CAHqSRuQsf18jamtJH0y=4-cvfLcSwqxwWwJ5AUj7njPzyAL=Tg@mail.gmail.com>
References: <775_1658968044_26S0RNlm003716_20220728122628.4381daa4@rolf-Latitude-E7470>
 <dcc28626-39f9-3f02-9d83-38d711b16bd2@mcmaster.ca>
 <3A27E228-1C74-4753-BE6F-37074FF54143@dcn.davis.ca.us>
 <9bae4bf6-0572-d349-d811-1dacbfba5917@mcmaster.ca>
 <6F8816F6-8600-4DF9-9B04-00FA53AA8DE1@dcn.davis.ca.us>
 <6dcb13ed-8c8c-56a0-1651-dbbf756d9916@mcmaster.ca>
 <D918BB71-61D1-49AB-8350-07FCF3B5D2C2@dcn.davis.ca.us>
 <CAHqSRuQsf18jamtJH0y=4-cvfLcSwqxwWwJ5AUj7njPzyAL=Tg@mail.gmail.com>
Message-ID: <E9C34440-A942-4A22-99C8-6CC0FD43D2CD@dcn.davis.ca.us>

Thank you for this simplified example, Bill. I think John hit the nail on the head when he asserts that the deficient model can be simulated as a reduced model or using zeros, but I disagree that these are interchangeable. In the former case the resulting model does not match what was specified... while in the latter case the model conforms to the original specification but needs to have zeroes as coefficients in order to obtain the same results as the reduced model does. IMO the decision to re-write the model is distinctly different and clearly surprising for Rolf (and myself now). I would prefer to make the default for singular.ok be FALSE if the behavior for TRUE is to return a different model as it does now.

dta <- data.frame(y=101:105,x=rep(7,5))
deficient1 <- lm( y ~ x
                , data=dta
                , singular.ok = TRUE # indeterminate = 0
               )
summary( deficient1 )
newdata <- data.frame( x = 5:6 )
cat( "predict deficient\n" )
predict( deficient1, newdata = newdata ) # coef = 0
cat( "coef deficient complete=FALSE\n" )
coef( deficient1, complete = FALSE )
cat( "coef deficient complete=TRUE\n" )
coef( deficient1, complete = TRUE )
cat( "multiply newdata matrix by coef complete=TRUE\n" )
model.matrix( ~x, data = newdata ) %*% coef( deficient1, complete = TRUE ) # expect all NA
# cat( "multiply newdata matrix by coef complete=FALSE\n" )
# model.matrix( ~x, data = newdata ) %*% coef( deficient1, complete = FALSE ) # dimensions mismatch
cat( "multiply newdata matrix by zeroed coef for complete=TRUE\n" )
effective_coefs <- coef( deficient1, complete = TRUE )
effective_coefs[ is.na( effective_coefs ) ] <- 0
effective_coefs
model.matrix( ~x, data = newdata ) %*% effective_coefs
#deficient2 <- lm( y ~ x
#                , data=dta
#                , singular.ok = FALSE # error
#                )

On July 28, 2022 11:00:16 AM PDT, Bill Dunlap <williamwdunlap at gmail.com> wrote:
>In this example, x has an unknown influence on y (since x doesn't vary)
>   > coef(lm(y ~ x, data=data.frame(y=101:105,x=rep(7,5))))
>   (Intercept)           x
>           103          NA
>while in this example x has no influence on y, a very different conclusion
>   > coef(lm(y ~ x, data=data.frame(y=rep(103,5),x=1:5)))
>    (Intercept)           x
>            103           0
>
>Are you arguing that lm's (or glm's) output should have another component
>telling us whether a column was included in the model or not (i.e., if a
>coefficient was well defined or not)?  Then coef() would have to copy this
>information so it could show the user that (e.g., by printing '-' instead
>of NA).  That might be reasonable.  Using NA for an unknown coefficient
>value also seems reasonable and a lot of code currently depends upon it.
>Does it need to be documented better?
>
>-Bill
>
>
>On Thu, Jul 28, 2022 at 9:50 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>wrote:
>
>> Yes, I am familiar with linear algebra. I nod along with you until you get
>> to "hence" and then you make a leap. The user made the decision (or
>> accepted the default) that the rank problem be ignored... they have the
>> option to reduce their model, but they wanted this coefficient
>> (communicated via the model and the parameter) to behave as if it were
>> zero. The issue is that that decision is getting hidden behind this
>> additional implementation decision to exclude certain columns in the model
>> matrix _even after the model has been solved_. If they try to use the
>> coefficients themselves then they have to apply this odd interpretation of
>> NA that is embedded in predict.glm but not apparent in their model instead
>> of using a simple newdata %*% coefs.
>>
>> It may be easier to implement summary.glm with an NA there as a hint to
>> the rank deficiency, but at the cost of mathematical inconsistency in the
>> reporting of coefficients. IMO either the NA should always be presented to
>> the user as if it were a zero or the rank deficiency should be recorded
>> separately.
>>
>> On July 28, 2022 8:46:13 AM PDT, John Fox <jfox at mcmaster.ca> wrote:
>> >Dear Jeff,
>> >
>> >On 2022-07-28 11:12 a.m., Jeff Newmiller wrote:
>> >> No, in this case I think I needed the "obvious" breakdown. Still
>> digesting, though... I would prefer that if an arbitrary selection had been
>> made that it be explicit .. the NA should be replaced with zero if the
>> singular.ok argument is TRUE, rather than making that interpretation in
>> predict.glm.
>> >
>> >That's one way to think about, but another is that the model matrix X has
>> 10 columns but is of rank 9. Thus 9 basis vectors are needed to span the
>> column space of X, and a simple way to provide a basis is to eliminate a
>> redundant column, hence the NA. The fitted values y-hat in a linear model
>> are the orthogonal projection of y onto the space spanned by the columns of
>> X, and are thus independent of the basis chosen. A GLM is a little more
>> complicated, but it's still the column space of X that's important.
>> >
>> >Best,
>> > John
>> >
>> >>
>> >> On July 28, 2022 5:45:35 AM PDT, John Fox <jfox at mcmaster.ca> wrote:
>> >>> Dear Jeff,
>> >>>
>> >>> On 2022-07-28 1:31 a.m., Jeff Newmiller wrote:
>> >>>> But "disappearing" is not what NA is supposed to do normally. Why is
>> it being treated that way here?
>> >>>
>> >>> NA has a different meaning here than in data.
>> >>>
>> >>> By default, in glm() the argument singular.ok is TRUE, and so
>> estimates are provided even when there are singularities, and even though
>> the singularities are resolved arbitrarily.
>> >>>
>> >>> In this model, the columns of the model matrix labelled LifestageL1
>> and TrtTime:LifestageL1 are perfectly collinear -- the second is 12 times
>> the first (both have 0s in the same rows and either 1 or 12 in three of the
>> rows) -- and thus both can't be estimated simultaneously, but the model can
>> be estimated by eliminating one or the other (effectively setting its
>> coefficient to 0), or by taking any linear combination of the two
>> regressors (i.e., using any regressor with 0s and some other value). The
>> fitted values under the model are invariant with respect to this arbitrary
>> choice.
>> >>>
>> >>> My apologies if I'm stating the obvious and misunderstand your
>> objection.
>> >>>
>> >>> Best,
>> >>> John
>> >>>
>> >>>>
>> >>>> On July 27, 2022 7:04:20 PM PDT, John Fox <jfox at mcmaster.ca> wrote:
>> >>>>> Dear Rolf,
>> >>>>>
>> >>>>> The coefficient of TrtTime:LifestageL1 isn't estimable (as you
>> explain) and by setting it to NA, glm() effectively removes it from the
>> model. An equivalent model is therefore
>> >>>>>
>> >>>>>> fit2 <- glm(cbind(Dead,Alive) ~ TrtTime + Lifestage +
>> >>>>> +               I((Lifestage == "Egg + L1")*TrtTime) +
>> >>>>> +               I((Lifestage == "L1 + L2")*TrtTime) +
>> >>>>> +               I((Lifestage == "L3")*TrtTime),
>> >>>>> +             family=binomial, data=demoDat)
>> >>>>> Warning message:
>> >>>>> glm.fit: fitted probabilities numerically 0 or 1 occurred
>> >>>>>
>> >>>>>> cbind(coef(fit, complete=FALSE), coef(fit2))
>> >>>>>                                    [,1]         [,2]
>> >>>>> (Intercept)                -0.91718302  -0.91718302
>> >>>>> TrtTime                     0.88846195   0.88846195
>> >>>>> LifestageEgg + L1         -45.36420974 -45.36420974
>> >>>>> LifestageL1                14.27570572  14.27570572
>> >>>>> LifestageL1 + L2           -0.30332697  -0.30332697
>> >>>>> LifestageL3                -3.58672631  -3.58672631
>> >>>>> TrtTime:LifestageEgg + L1   8.10482459   8.10482459
>> >>>>> TrtTime:LifestageL1 + L2    0.05662651   0.05662651
>> >>>>> TrtTime:LifestageL3         1.66743472   1.66743472
>> >>>>>
>> >>>>> There is no problem computing fitted values for the model, specified
>> either way. That the fitted values when Lifestage == "L1" all round to 1 on
>> the probability scale is coincidental -- that is, a consequence of the data.
>> >>>>>
>> >>>>> I hope this helps,
>> >>>>> John
>> >>>>>
>> >>>>> On 2022-07-27 8:26 p.m., Rolf Turner wrote:
>> >>>>>>
>> >>>>>> I have a data frame with a numeric ("TrtTime") and a categorical
>> >>>>>> ("Lifestage") predictor.
>> >>>>>>
>> >>>>>> Level "L1" of Lifestage occurs only with a single value of TrtTime,
>> >>>>>> explicitly 12, whence it is not possible to estimate a TrtTime
>> "slope"
>> >>>>>> when Lifestage is "L1".
>> >>>>>>
>> >>>>>> Indeed, when I fitted the model
>> >>>>>>
>> >>>>>>        fit <- glm(cbind(Dead,Alive) ~ TrtTime*Lifestage,
>> family=binomial,
>> >>>>>>                   data=demoDat)
>> >>>>>>
>> >>>>>> I got:
>> >>>>>>
>> >>>>>>> as.matrix(coef(fit))
>> >>>>>>>                                      [,1]
>> >>>>>>> (Intercept)                -0.91718302
>> >>>>>>> TrtTime                     0.88846195
>> >>>>>>> LifestageEgg + L1         -45.36420974
>> >>>>>>> LifestageL1                14.27570572
>> >>>>>>> LifestageL1 + L2           -0.30332697
>> >>>>>>> LifestageL3                -3.58672631
>> >>>>>>> TrtTime:LifestageEgg + L1   8.10482459
>> >>>>>>> TrtTime:LifestageL1                 NA
>> >>>>>>> TrtTime:LifestageL1 + L2    0.05662651
>> >>>>>>> TrtTime:LifestageL3         1.66743472
>> >>>>>>
>> >>>>>> That is, TrtTime:LifestageL1 is NA, as expected.
>> >>>>>>
>> >>>>>> I would have thought that fitted or predicted values corresponding
>> to
>> >>>>>> Lifestage = "L1" would thereby be NA, but this is not the case:
>> >>>>>>
>> >>>>>>> predict(fit)[demoDat$Lifestage=="L1"]
>> >>>>>>>          26       65      131
>> >>>>>>> 24.02007 24.02007 24.02007
>> >>>>>>>
>> >>>>>>> fitted(fit)[demoDat$Lifestage=="L1"]
>> >>>>>>>     26  65 131
>> >>>>>>>      1   1   1
>> >>>>>>
>> >>>>>> That is, the predicted values on the scale of the linear predictor
>> are
>> >>>>>> large and positive, rather than being NA.
>> >>>>>>
>> >>>>>> What this amounts to, it seems to me, is saying that if the linear
>> >>>>>> predictor in a Binomial glm is NA, then "success" is a certainty.
>> >>>>>> This strikes me as being a dubious proposition.  My gut feeling is
>> that
>> >>>>>> misleading results could be produced.
>> >>>>>>
>> >>>>>> Can anyone explain to me a rationale for this behaviour pattern?
>> >>>>>> Is there some justification for it that I am not currently seeing?
>> >>>>>> Any other comments?  (Please omit comments to the effect of "You
>> are as
>> >>>>>> thick as two short planks!". :-) )
>> >>>>>>
>> >>>>>> I have attached the example data set in a file "demoDat.txt", should
>> >>>>>> anyone want to experiment with it.  The file was created using
>> dput() so
>> >>>>>> you should access it (if you wish to do so) via something like
>> >>>>>>
>> >>>>>>        demoDat <- dget("demoDat.txt")
>> >>>>>>
>> >>>>>> Thanks for any enlightenment.
>> >>>>>>
>> >>>>>> cheers,
>> >>>>>>
>> >>>>>> Rolf Turner
>> >>>>>>
>> >>>>>>
>> >>>>>> ______________________________________________
>> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> >>>>>> and provide commented, minimal, self-contained, reproducible code.
>> >>>>
>> >>
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>

-- 
Sent from my phone. Please excuse my brevity.


