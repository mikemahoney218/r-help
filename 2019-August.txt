From |orenzo@|@e||@ @end|ng |rom gm@||@com  Thu Aug  1 12:27:05 2019
From: |orenzo@|@e||@ @end|ng |rom gm@||@com (Lorenzo Isella)
Date: Thu, 1 Aug 2019 12:27:05 +0200
Subject: [R] Integer Sample with Mean Dependent on Size
Message-ID: <20190801102705.nukp4evw3cesfzbm@chicca2>

Dear All,
I cannot unfortunately provide any R code, otherwise I would not need to post this in the first place.
I need to generate a sample of N positive non-zero integers such that their mean is *exactly* 2(N-1)/N, i.e. the mean depends on the length of the sample.
For a start, we can assume that every integer in my sample can assume only the values 1, 2 and 3.
Any suggestion is appreciated.
Cheers

Lorenzo


From gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de  Thu Aug  1 12:37:47 2019
From: gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de (Gerrit Eichner)
Date: Thu, 1 Aug 2019 12:37:47 +0200
Subject: [R] Integer Sample with Mean Dependent on Size
In-Reply-To: <20190801102705.nukp4evw3cesfzbm@chicca2>
References: <20190801102705.nukp4evw3cesfzbm@chicca2>
Message-ID: <888e6842-adaa-99c4-382c-136f65e064c1@math.uni-giessen.de>

n_1 + ... + n_N = 2(N-1) is requested for integers n_i >= 1.

What about c(rep(2, N-2), 1, 1))?

but I'm afraid that this was not what you really wanted. ;-)
However, you didn't say if your sample should be random. :-)

  Best regards  --  Gerrit

---------------------------------------------------------------------
Dr. Gerrit Eichner                   Mathematical Institute, Room 212
gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
http://www.uni-giessen.de/eichner
---------------------------------------------------------------------

Am 01.08.2019 um 12:27 schrieb Lorenzo Isella:
> Dear All,
> I cannot unfortunately provide any R code, otherwise I would not need to 
> post this in the first place.
> I need to generate a sample of N positive non-zero integers such that 
> their mean is *exactly* 2(N-1)/N, i.e. the mean depends on the length of 
> the sample.
> For a start, we can assume that every integer in my sample can assume 
> only the values 1, 2 and 3.
> Any suggestion is appreciated.
> Cheers
> 
> Lorenzo
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From |orenzo@|@e||@ @end|ng |rom gm@||@com  Thu Aug  1 12:47:13 2019
From: |orenzo@|@e||@ @end|ng |rom gm@||@com (Lorenzo Isella)
Date: Thu, 1 Aug 2019 12:47:13 +0200
Subject: [R] Integer Sample with Mean Dependent on Size
In-Reply-To: <888e6842-adaa-99c4-382c-136f65e064c1@math.uni-giessen.de>
References: <20190801102705.nukp4evw3cesfzbm@chicca2>
 <888e6842-adaa-99c4-382c-136f65e064c1@math.uni-giessen.de>
Message-ID: <20190801104713.zsnjxvwmvp7lad3k@chicca2>

Yes, you are right (and yours is one of the possible cases).
I think this works (I resorted to pen and paper for once)



generate_k <- function(N, n3){

    n1 <- 2+n3
    n2 <- N-n1-n3


    out <- c(rep(1, n1), rep(2, n2), rep(3, n3))

    return(out)
}

where N is the length of the sample, n3 is the number of times I have the number 3 in the sample (n1 and n2 are the same for 1 and 2, respectively).
So far it holds!

L.

On Thu, Aug 01, 2019 at 12:37:47PM +0200, Gerrit Eichner wrote:
>n_1 + ... + n_N = 2(N-1) is requested for integers n_i >= 1.
>
>What about c(rep(2, N-2), 1, 1))?
>
>but I'm afraid that this was not what you really wanted. ;-)
>However, you didn't say if your sample should be random. :-)
>
> Best regards  --  Gerrit
>
>---------------------------------------------------------------------
>Dr. Gerrit Eichner                   Mathematical Institute, Room 212
>gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
>Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
>http://www.uni-giessen.de/eichner
>---------------------------------------------------------------------
>
>Am 01.08.2019 um 12:27 schrieb Lorenzo Isella:
>>Dear All,
>>I cannot unfortunately provide any R code, otherwise I 
>>would not need to post this in the first place.
>>I need to generate a sample of N positive non-zero 
>>integers such that their mean is *exactly* 2(N-1)/N, 
>>i.e. the mean depends on the length of the sample.
>>For a start, we can assume that every integer in my 
>>sample can assume only the values 1, 2 and 3.
>>Any suggestion is appreciated.
>>Cheers
>>
>>Lorenzo
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.


From r@oknz @end|ng |rom gm@||@com  Thu Aug  1 13:17:30 2019
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Thu, 1 Aug 2019 23:17:30 +1200
Subject: [R] Integer Sample with Mean Dependent on Size
In-Reply-To: <20190801102705.nukp4evw3cesfzbm@chicca2>
References: <20190801102705.nukp4evw3cesfzbm@chicca2>
Message-ID: <CABcYAd+Pf4gCtpwN2GUC4wncLX1LOqW-rh9xzQ4K=bNNVShayw@mail.gmail.com>

2(N-1)/N = 2 - 2/N.
So one way to get exactly that mean is to make all the numbers
2 except for two of them which are 1.

N < 2 : can't be done.
N = 2 : only [1,1] does the job.
N = 3 : the sum of the three numbers must be 4, so none of them
        can be 3, so [1,1,2] [1,2,1] [2,1,1] are the only
        possibilities.
N = 4 : [1,1,1,3] [1,1,2,2] [1,1,3,1] [1,2,1,2] [1,2,2,1] [1,3,1,1]
[2,1,1,2] [2,1,2,1] [2,2,1,1] [3,1,1,1]

Is there a pattern here?
Yes.  There must be an integer k such that 0 <= 2k <= N-2
and then you have a rearrangement of (k+2) 1s, (N-2-2k) 2s, and k 3s.

	[[alternative HTML version deleted]]


From r@oknz @end|ng |rom gm@||@com  Thu Aug  1 13:22:54 2019
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Thu, 1 Aug 2019 23:22:54 +1200
Subject: [R] Integer Sample with Mean Dependent on Size
In-Reply-To: <20190801104713.zsnjxvwmvp7lad3k@chicca2>
References: <20190801102705.nukp4evw3cesfzbm@chicca2>
 <888e6842-adaa-99c4-382c-136f65e064c1@math.uni-giessen.de>
 <20190801104713.zsnjxvwmvp7lad3k@chicca2>
Message-ID: <CABcYAd+gS=o-tZw0ENn2oSFWGOWBng85YMYB_W289Ud1vW9hog@mail.gmail.com>

I don't know why I thought you wanted a *random* sequence..
The 'rep' function can do more than you realise.

generate_k <- function (N, n3) rep(1:3, c(n3+2, N-2-n3*2, n3))



On Thu, 1 Aug 2019 at 22:48, Lorenzo Isella <lorenzo.isella at gmail.com>
wrote:

> Yes, you are right (and yours is one of the possible cases).
> I think this works (I resorted to pen and paper for once)
>
>
>
> generate_k <- function(N, n3){
>
>     n1 <- 2+n3
>     n2 <- N-n1-n3
>
>
>     out <- c(rep(1, n1), rep(2, n2), rep(3, n3))
>
>     return(out)
> }
>
> where N is the length of the sample, n3 is the number of times I have the
> number 3 in the sample (n1 and n2 are the same for 1 and 2, respectively).
> So far it holds!
>
> L.
>
> On Thu, Aug 01, 2019 at 12:37:47PM +0200, Gerrit Eichner wrote:
> >n_1 + ... + n_N = 2(N-1) is requested for integers n_i >= 1.
> >
> >What about c(rep(2, N-2), 1, 1))?
> >
> >but I'm afraid that this was not what you really wanted. ;-)
> >However, you didn't say if your sample should be random. :-)
> >
> > Best regards  --  Gerrit
> >
> >---------------------------------------------------------------------
> >Dr. Gerrit Eichner                   Mathematical Institute, Room 212
> >gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
> >Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> >http://www.uni-giessen.de/eichner
> >---------------------------------------------------------------------
> >
> >Am 01.08.2019 um 12:27 schrieb Lorenzo Isella:
> >>Dear All,
> >>I cannot unfortunately provide any R code, otherwise I
> >>would not need to post this in the first place.
> >>I need to generate a sample of N positive non-zero
> >>integers such that their mean is *exactly* 2(N-1)/N,
> >>i.e. the mean depends on the length of the sample.
> >>For a start, we can assume that every integer in my
> >>sample can assume only the values 1, 2 and 3.
> >>Any suggestion is appreciated.
> >>Cheers
> >>
> >>Lorenzo
> >>
> >>______________________________________________
> >>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide
> >>http://www.R-project.org/posting-guide.html
> >>and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From |orenzo@|@e||@ @end|ng |rom gm@||@com  Thu Aug  1 13:28:02 2019
From: |orenzo@|@e||@ @end|ng |rom gm@||@com (Lorenzo Isella)
Date: Thu, 1 Aug 2019 13:28:02 +0200
Subject: [R] Integer Sample with Mean Dependent on Size
In-Reply-To: <CABcYAd+Pf4gCtpwN2GUC4wncLX1LOqW-rh9xzQ4K=bNNVShayw@mail.gmail.com>
References: <20190801102705.nukp4evw3cesfzbm@chicca2>
 <CABcYAd+Pf4gCtpwN2GUC4wncLX1LOqW-rh9xzQ4K=bNNVShayw@mail.gmail.com>
Message-ID: <20190801112802.65dspvvninuxkgtd@chicca2>

Hello,

On Thu, Aug 01, 2019 at 11:17:30PM +1200, Richard O'Keefe wrote:
>2(N-1)/N = 2 - 2/N.
>So one way to get exactly that mean is to make all the numbers
>2 except for two of them which are 1.
>
>N < 2 : can't be done.
>N = 2 : only [1,1] does the job.
>N = 3 : the sum of the three numbers must be 4, so none of them
>        can be 3, so [1,1,2] [1,2,1] [2,1,1] are the only
>        possibilities.
>N = 4 : [1,1,1,3] [1,1,2,2] [1,1,3,1] [1,2,1,2] [1,2,2,1] [1,3,1,1]
>[2,1,1,2] [2,1,2,1] [2,2,1,1] [3,1,1,1]
>
>Is there a pattern here?
>Yes.  There must be an integer k such that 0 <= 2k <= N-2
>and then you have a rearrangement of (k+2) 1s, (N-2-2k) 2s, and k 3s.


This is what I did by solving a set of equations

n1+n2+n3=N (i.e. the number of 1, 2 and 3 equals N, the length of my sample) and
n2+2*n2+3*n3=2*(N-1)

without delving into the details, you are composing (fractal) structures of N spheres in 3D, with an algorithm which establishes that every sphere has on average (2*N-1)/N neighbours. One solution is a simple chain, without bifurcations, where every sphere has two neighbours apart from the two terminal spheres, each of which has just one neighbour. Whenever a bifurcation arises, one sphere will have three neighbors.

L.


From joh@nne@@r@nke @end|ng |rom jrwb@de  Thu Aug  1 08:49:58 2019
From: joh@nne@@r@nke @end|ng |rom jrwb@de (Johannes Ranke)
Date: Thu, 01 Aug 2019 08:49:58 +0200
Subject: [R] [R-sig-Debian] 3.6 on debian stretch
In-Reply-To: <CACwCsY7PggYpoPNLvFtbw0ciFErGZuuq664_XAGTGr1rE-GV_w@mail.gmail.com>
References: <CACwCsY7PggYpoPNLvFtbw0ciFErGZuuq664_XAGTGr1rE-GV_w@mail.gmail.com>
Message-ID: <1633338.f7jCTC8NRC@ryz>

Dear Larry,

yes, because of various factors the r-cran-* packages from the stable and 
especially the oldstable distribution are often not compatible with the 
backported r-base packages provided on CRAN. At current, buster is not 
affected, but your case shows that using stretch gives this problem.

In principle it would be nice to have a repo containing all the r-cran-* and 
r-bioc-* packages recompiled for these backports. Such a repository does exist 
for Ubuntu as far as I know (I don't use it). For Debian, such a repository 
currently does not exist.

In your situation, I think the easiest solution is to simply install caret and 
ggplot2 from within R.

Personally, I like the install.r and update.r scripts available from Dirks 
littler package. They make it possible to conveniently install and update R 
packages from source using the command line, if you have the scripts in your 
PATH.

Kind regards,

Johannes

Am Mittwoch, 31. Juli 2019, 21:41:14 CEST schrieb Larry Martell:
> I need to run 3.6 on debian stretch - I followed the instructions here:
> 
> https://cran.r-project.org/bin/linux/debian/
> 
> and I was able to install it. But 2 packages I need,  r-cran-caret and
> r-cran-ggplot2 will not install:
> 
> # apt-get install r-cran-ggplot2
> Reading package lists... Done
> Building dependency tree
> Reading state information... Done
> Some packages could not be installed. This may mean that you have
> requested an impossible situation or if you are using the unstable
> distribution that some required packages have not yet been created
> or been moved out of Incoming.
> The following information may help to resolve the situation:
> 
> The following packages have unmet dependencies:
>  r-cran-ggplot2 : Depends: r-api-3
>                   Depends: r-cran-digest but it is not going to be installed
> Depends: r-cran-gtable (>= 0.1.1) but it is not
> going to be installed
>                   Depends: r-cran-plyr (>= 1.7.1) but it is not going
> to be installed
>                   Depends: r-cran-reshape2 but it is not going to be
> installed Depends: r-cran-scales (>= 0.4.1) but it is not
> going to be installed
>                   Depends: r-cran-tibble but it is not going to be installed
> Depends: r-cran-lazyeval but it is not going to be installed E: Unable to
> correct problems, you have held broken packages.
> 
> Is there a way to get these 2 packages for my environment?
> 
> Thanks!
> 
> _______________________________________________
> R-SIG-Debian mailing list
> R-SIG-Debian at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-debian


From M@C@Gomez-C@no @end|ng |rom exeter@@c@uk  Thu Aug  1 12:25:31 2019
From: M@C@Gomez-C@no @end|ng |rom exeter@@c@uk (Gomez Cano, Mayam)
Date: Thu, 1 Aug 2019 10:25:31 +0000
Subject: [R] fa function in psych package and missing values
In-Reply-To: <1564654991886-0.post@n4.nabble.com>
References: <1564654991886-0.post@n4.nabble.com>
Message-ID: <CWLP265MB158766BF4275D2C2D2333B1AA7DE0@CWLP265MB1587.GBRP265.PROD.OUTLOOK.COM>


Does fa function in psych package consider only complete cases when missing=FALSE?
Why do I get slightly different results when I restrict my data to complete cases compared to the whole data with the default missing=FALSE?





--
Sent from: http://r.789695.n4.nabble.com/R-help-f789696.html


From @pbr@ckett20 @end|ng |rom @@|ntjo@ephh@@com  Fri Aug  2 00:13:08 2019
From: @pbr@ckett20 @end|ng |rom @@|ntjo@ephh@@com (Spencer Brackett)
Date: Thu, 1 Aug 2019 18:13:08 -0400
Subject: [R] Loading large tar.gz XenaHub Data into R
Message-ID: <CAPQaxLN_=rPqDf1Cs+ySd0zh3x_1tmoU7mEweyUK6OxZV-+QNg@mail.gmail.com>

Good evening,

I am attempting to load the following Xena dataset
https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz

I am trying to unpack the dataset and read it into R as a table, but due to
the size of the file, I am having some trouble. The following are the
commands I have tried thus far.

HumanMethylation450 <- fread("
https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
")

readLines("
https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
")

                 ###These two above attempts failed with warning messages
from R###

Methyl <-read.delim("
https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
")

               ##This attempt is still processing, but has been doing so
for quite some time##

Any ideas as to what else I could try?

Best,

Spencer

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Fri Aug  2 00:37:17 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 1 Aug 2019 15:37:17 -0700
Subject: [R] Loading large tar.gz XenaHub Data into R
In-Reply-To: <CAPQaxLN_=rPqDf1Cs+ySd0zh3x_1tmoU7mEweyUK6OxZV-+QNg@mail.gmail.com>
References: <CAPQaxLN_=rPqDf1Cs+ySd0zh3x_1tmoU7mEweyUK6OxZV-+QNg@mail.gmail.com>
Message-ID: <CAGxFJbRz+-uiNhb3k6XOvJ=Fg3y7sUuFVP2Yt-6-Nu+672TMOw@mail.gmail.com>

These are gzipped files, I assume. So see ?gzfile and associated info
for how to open a gzip connection and read from it. You may also
prefer to search (e.g. at rseek.org) on "read a gzipped file" or
similar for possible alternatives.

Of course, if they're not gzipped files, then ignore the above. If
they are, your current approach is hopeless.


Cheers,
Bert

On Thu, Aug 1, 2019 at 3:13 PM Spencer Brackett
<spbrackett20 at saintjosephhs.com> wrote:
>
> Good evening,
>
> I am attempting to load the following Xena dataset
> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
>
> I am trying to unpack the dataset and read it into R as a table, but due to
> the size of the file, I am having some trouble. The following are the
> commands I have tried thus far.
>
> HumanMethylation450 <- fread("
> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
> ")
>
> readLines("
> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
> ")
>
>                  ###These two above attempts failed with warning messages
> from R###
>
> Methyl <-read.delim("
> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
> ")
>
>                ##This attempt is still processing, but has been doing so
> for quite some time##
>
> Any ideas as to what else I could try?
>
> Best,
>
> Spencer
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wdun|@p @end|ng |rom t|bco@com  Fri Aug  2 00:46:50 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Thu, 1 Aug 2019 15:46:50 -0700
Subject: [R] Loading large tar.gz XenaHub Data into R
In-Reply-To: <CAGxFJbRz+-uiNhb3k6XOvJ=Fg3y7sUuFVP2Yt-6-Nu+672TMOw@mail.gmail.com>
References: <CAPQaxLN_=rPqDf1Cs+ySd0zh3x_1tmoU7mEweyUK6OxZV-+QNg@mail.gmail.com>
 <CAGxFJbRz+-uiNhb3k6XOvJ=Fg3y7sUuFVP2Yt-6-Nu+672TMOw@mail.gmail.com>
Message-ID: <CAF8bMcZyzAhnwRCfM-FhsjnFmc1S9APUhSjhKnQpYtL5vVAH1w@mail.gmail.com>

By the way, instead of saying only that there were warnings, it would be
nice to show some of them.  E.g.,
> z <- readLines("
https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
")
[ Hit control-C or Esc to interrupt, or wait a long time ]
There were 50 or more warnings (use warnings() to see the first 50)
> warnings()
Warning messages:
1: In readLines("
https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz")
:
  line 1 appears to contain an embedded nul
2: In readLines("
https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz")
:
  line 4 appears to contain an embedded nul
3: In readLines("
https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz")
:
  line 7 appears to contain an embedded nul

Burt's guess looks right, as the following gives 10 long lines of
reasonable-looking data.  Remove the 'n=10' to get all of it.

z <- readLines(gzcon(url("
https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz")),
n=10)

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Thu, Aug 1, 2019 at 3:37 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> These are gzipped files, I assume. So see ?gzfile and associated info
> for how to open a gzip connection and read from it. You may also
> prefer to search (e.g. at rseek.org) on "read a gzipped file" or
> similar for possible alternatives.
>
> Of course, if they're not gzipped files, then ignore the above. If
> they are, your current approach is hopeless.
>
>
> Cheers,
> Bert
>
> On Thu, Aug 1, 2019 at 3:13 PM Spencer Brackett
> <spbrackett20 at saintjosephhs.com> wrote:
> >
> > Good evening,
> >
> > I am attempting to load the following Xena dataset
> >
> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
> >
> > I am trying to unpack the dataset and read it into R as a table, but due
> to
> > the size of the file, I am having some trouble. The following are the
> > commands I have tried thus far.
> >
> > HumanMethylation450 <- fread("
> >
> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
> > ")
> >
> > readLines("
> >
> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
> > ")
> >
> >                  ###These two above attempts failed with warning messages
> > from R###
> >
> > Methyl <-read.delim("
> >
> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
> > ")
> >
> >                ##This attempt is still processing, but has been doing so
> > for quite some time##
> >
> > Any ideas as to what else I could try?
> >
> > Best,
> >
> > Spencer
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @pbr@ckett20 @end|ng |rom @@|ntjo@ephh@@com  Fri Aug  2 02:06:44 2019
From: @pbr@ckett20 @end|ng |rom @@|ntjo@ephh@@com (Spencer Brackett)
Date: Thu, 1 Aug 2019 20:06:44 -0400
Subject: [R] Loading large tar.gz XenaHub Data into R
In-Reply-To: <CAF8bMcZyzAhnwRCfM-FhsjnFmc1S9APUhSjhKnQpYtL5vVAH1w@mail.gmail.com>
References: <CAPQaxLN_=rPqDf1Cs+ySd0zh3x_1tmoU7mEweyUK6OxZV-+QNg@mail.gmail.com>
 <CAGxFJbRz+-uiNhb3k6XOvJ=Fg3y7sUuFVP2Yt-6-Nu+672TMOw@mail.gmail.com>
 <CAF8bMcZyzAhnwRCfM-FhsjnFmc1S9APUhSjhKnQpYtL5vVAH1w@mail.gmail.com>
Message-ID: <CAPQaxLPaBgQRtGOYB4Dz+K7UzduETmk2hJokXLsfd=op_DQAmw@mail.gmail.com>

Thank you both for your advice! The z <- readLines(gzcon(url("
https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz")),
) command worked out nicely

On Thu, Aug 1, 2019 at 6:47 PM William Dunlap <wdunlap at tibco.com> wrote:

> By the way, instead of saying only that there were warnings, it would be
> nice to show some of them.  E.g.,
> > z <- readLines("
> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
> ")
> [ Hit control-C or Esc to interrupt, or wait a long time ]
> There were 50 or more warnings (use warnings() to see the first 50)
> > warnings()
> Warning messages:
> 1: In readLines("
> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz")
> :
>   line 1 appears to contain an embedded nul
> 2: In readLines("
> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz")
> :
>   line 4 appears to contain an embedded nul
> 3: In readLines("
> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz")
> :
>   line 7 appears to contain an embedded nul
>
> Burt's guess looks right, as the following gives 10 long lines of
> reasonable-looking data.  Remove the 'n=10' to get all of it.
>
> z <- readLines(gzcon(url("
> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz")),
> n=10)
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Thu, Aug 1, 2019 at 3:37 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
>> These are gzipped files, I assume. So see ?gzfile and associated info
>> for how to open a gzip connection and read from it. You may also
>> prefer to search (e.g. at rseek.org) on "read a gzipped file" or
>> similar for possible alternatives.
>>
>> Of course, if they're not gzipped files, then ignore the above. If
>> they are, your current approach is hopeless.
>>
>>
>> Cheers,
>> Bert
>>
>> On Thu, Aug 1, 2019 at 3:13 PM Spencer Brackett
>> <spbrackett20 at saintjosephhs.com> wrote:
>> >
>> > Good evening,
>> >
>> > I am attempting to load the following Xena dataset
>> >
>> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
>> >
>> > I am trying to unpack the dataset and read it into R as a table, but
>> due to
>> > the size of the file, I am having some trouble. The following are the
>> > commands I have tried thus far.
>> >
>> > HumanMethylation450 <- fread("
>> >
>> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
>> > ")
>> >
>> > readLines("
>> >
>> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
>> > ")
>> >
>> >                  ###These two above attempts failed with warning
>> messages
>> > from R###
>> >
>> > Methyl <-read.delim("
>> >
>> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
>> > ")
>> >
>> >                ##This attempt is still processing, but has been doing so
>> > for quite some time##
>> >
>> > Any ideas as to what else I could try?
>> >
>> > Best,
>> >
>> > Spencer
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From @pbr@ckett20 @end|ng |rom @@|ntjo@ephh@@com  Sat Aug  3 05:05:21 2019
From: @pbr@ckett20 @end|ng |rom @@|ntjo@ephh@@com (Spencer Brackett)
Date: Fri, 2 Aug 2019 23:05:21 -0400
Subject: [R] Determining survival correlation among probes
Message-ID: <CAPQaxLO4Fdmr1omyPRO9s-K=7Hv09NJope3dunC-8V9La0R8SQ@mail.gmail.com>

Good evening,

 I have a few data objects that are the result of an analysis producing
regression models associated with methylation for TCGA GBM and LGG
subjects. I am trying to figure out how I can use R to find out which
probes among these data objects correlate with patient survival? With this
information, I hope to establish if the gene MGMT is one of the genes whose
methylation show an association with survival. Is there a general procedure
that I might look to as a guide for this? I believe the result of such a
query into probeset data would result in a heatmap visualizing the
association with survival.

Best,

Spencer

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sat Aug  3 05:51:12 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 2 Aug 2019 20:51:12 -0700
Subject: [R] Determining survival correlation among probes
In-Reply-To: <CAPQaxLO4Fdmr1omyPRO9s-K=7Hv09NJope3dunC-8V9La0R8SQ@mail.gmail.com>
References: <CAPQaxLO4Fdmr1omyPRO9s-K=7Hv09NJope3dunC-8V9La0R8SQ@mail.gmail.com>
Message-ID: <CAGxFJbTF-iommOt42GOu0Dhib7QPpGciPU++fsjMUiqC_tG7_g@mail.gmail.com>

Spencer:

Sorry, but I'll be blunt. IMO, you are misusing this list (see the
posting guide). You clearly don't know what you're doing statistically
and need to consult with your advisors. This list cannot and is not
meant to serve that purpose -- it is for for R programming issues. If
your advisors can't or won't help you, you might try
stats.stackexchange.com, which does deal with statistical issues. But
I could not guarantee that they would act as a statistical consulting
service for you, which seems to be what you seek.

Cheers,
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Fri, Aug 2, 2019 at 8:05 PM Spencer Brackett
<spbrackett20 at saintjosephhs.com> wrote:
>
> Good evening,
>
>  I have a few data objects that are the result of an analysis producing
> regression models associated with methylation for TCGA GBM and LGG
> subjects. I am trying to figure out how I can use R to find out which
> probes among these data objects correlate with patient survival? With this
> information, I hope to establish if the gene MGMT is one of the genes whose
> methylation show an association with survival. Is there a general procedure
> that I might look to as a guide for this? I believe the result of such a
> query into probeset data would result in a heatmap visualizing the
> association with survival.
>
> Best,
>
> Spencer
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sat Aug  3 07:34:28 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sat, 3 Aug 2019 17:34:28 +1200
Subject: [R] [FORGED] Re:  Determining survival correlation among probes
In-Reply-To: <CAGxFJbTF-iommOt42GOu0Dhib7QPpGciPU++fsjMUiqC_tG7_g@mail.gmail.com>
References: <CAPQaxLO4Fdmr1omyPRO9s-K=7Hv09NJope3dunC-8V9La0R8SQ@mail.gmail.com>
 <CAGxFJbTF-iommOt42GOu0Dhib7QPpGciPU++fsjMUiqC_tG7_g@mail.gmail.com>
Message-ID: <2a76ceef-678c-2a5a-9c1b-475a2a87269b@auckland.ac.nz>


On 3/08/19 3:51 PM, Bert Gunter wrote:

> Spencer:
> 
> Sorry, but I'll be blunt. IMO, you are misusing this list (see the
> posting guide). You clearly don't know what you're doing statistically
> and need to consult with your advisors. This list cannot and is not
> meant to serve that purpose -- it is for for R programming issues. If
> your advisors can't or won't help you, you might try
> stats.stackexchange.com, which does deal with statistical issues. But
> I could not guarantee that they would act as a statistical consulting
> service for you, which seems to be what you seek.

I very firmly agree with Bert's post.  It seems to me that Spencer's 
postings demonstrate fundamental misunderstanding both of R and of 
statistical science.  He gives the impression that he *thinks* he 
understands a great deal more than he actually does, with the result 
that the questions he asks are generally ill-conceived and ill-posed. 
He appears to be attempting advanced statistical analysis without having 
mastered the elementary basics.

Spencer should focus on remedying his misconceptions and filling in the 
lacunae in his knowledge.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From m@015k3113 @end|ng |rom b|ueyonder@co@uk  Mon Aug  5 13:03:00 2019
From: m@015k3113 @end|ng |rom b|ueyonder@co@uk (e-mail ma015k3113)
Date: Mon, 5 Aug 2019 12:03:00 +0100 (BST)
Subject: [R] Connect to Oracle database via ODBC
In-Reply-To: <C02592F3-6C54-41E5-BD38-38F147E106E6@dcn.davis.ca.us>
References: <VI1PR07MB6381072984D70F108BC927D8E2CB0@VI1PR07MB6381.eurprd07.prod.outlook.com>,
 <CAGxFJbS77URSG2MK+-T6Lg4SJf-SXvDzBoYN5k-oxdVN4JohAg@mail.gmail.com>
 <VI1PR07MB63815B5DD62811E213469B3BE2CB0@VI1PR07MB6381.eurprd07.prod.outlook.com>
 <C02592F3-6C54-41E5-BD38-38F147E106E6@dcn.davis.ca.us>
Message-ID: <345608099.59546.1565002980507@mail2.virginmedia.com>

Dear All, can anyone point me towards information for connecting to a Oracle instance via DSN. I have already established a ODBC connection.


Sorry if this is very elementary-I am just getting started with R after using SAS for almost 2 decades.


Kind regards

Ahson


From m@rc_@chw@rtz @end|ng |rom me@com  Mon Aug  5 13:54:20 2019
From: m@rc_@chw@rtz @end|ng |rom me@com (Marc Schwartz)
Date: Mon, 5 Aug 2019 07:54:20 -0400
Subject: [R] Connect to Oracle database via ODBC
In-Reply-To: <345608099.59546.1565002980507@mail2.virginmedia.com>
References: <VI1PR07MB6381072984D70F108BC927D8E2CB0@VI1PR07MB6381.eurprd07.prod.outlook.com>
 <CAGxFJbS77URSG2MK+-T6Lg4SJf-SXvDzBoYN5k-oxdVN4JohAg@mail.gmail.com>
 <VI1PR07MB63815B5DD62811E213469B3BE2CB0@VI1PR07MB6381.eurprd07.prod.outlook.com>
 <C02592F3-6C54-41E5-BD38-38F147E106E6@dcn.davis.ca.us>
 <345608099.59546.1565002980507@mail2.virginmedia.com>
Message-ID: <93B48BC4-EBED-45F5-BC2A-891290BA3D0F@me.com>


> On Aug 5, 2019, at 7:03 AM, e-mail ma015k3113 via R-help <r-help at r-project.org> wrote:
> 
> Dear All, can anyone point me towards information for connecting to a Oracle instance via DSN. I have already established a ODBC connection.
> 
> Sorry if this is very elementary-I am just getting started with R after using SAS for almost 2 decades.
> 
> Kind regards
> 
> Ahson


Hi,

Look at the RODBC package on CRAN by Prof. Ripley:

  https://cran.r-project.org/web/packages/RODBC/index.html

and be sure to read the package vignette for additional information:

  https://cran.r-project.org/web/packages/RODBC/vignettes/RODBC.pdf

There is also the r-sig-db e-mail list, which is focused in this domain, and where follow up questions should be posted:

  https://stat.ethz.ch/mailman/listinfo/r-sig-db

Regards,

Marc Schwartz


From er|cjberger @end|ng |rom gm@||@com  Mon Aug  5 14:19:08 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Mon, 5 Aug 2019 15:19:08 +0300
Subject: [R] Connect to Oracle database via ODBC
In-Reply-To: <93B48BC4-EBED-45F5-BC2A-891290BA3D0F@me.com>
References: <VI1PR07MB6381072984D70F108BC927D8E2CB0@VI1PR07MB6381.eurprd07.prod.outlook.com>
 <CAGxFJbS77URSG2MK+-T6Lg4SJf-SXvDzBoYN5k-oxdVN4JohAg@mail.gmail.com>
 <VI1PR07MB63815B5DD62811E213469B3BE2CB0@VI1PR07MB6381.eurprd07.prod.outlook.com>
 <C02592F3-6C54-41E5-BD38-38F147E106E6@dcn.davis.ca.us>
 <345608099.59546.1565002980507@mail2.virginmedia.com>
 <93B48BC4-EBED-45F5-BC2A-891290BA3D0F@me.com>
Message-ID: <CAGgJW744v1FasFVx0JVROnbAYHY8FJL_gN=7cSvHR5Sp43-sMw@mail.gmail.com>

Hi Ahson,
Many people use R via RStudio, an IDE that has both free and non-free
versions.
RStudio has invested a lot of effort into making it easier to establish
connections to databases.
If this sounds of interest to you, take a look at
https://db.rstudio.com/

HTH,
Eric


On Mon, Aug 5, 2019 at 2:54 PM Marc Schwartz via R-help <
r-help at r-project.org> wrote:

>
> > On Aug 5, 2019, at 7:03 AM, e-mail ma015k3113 via R-help <
> r-help at r-project.org> wrote:
> >
> > Dear All, can anyone point me towards information for connecting to a
> Oracle instance via DSN. I have already established a ODBC connection.
> >
> > Sorry if this is very elementary-I am just getting started with R after
> using SAS for almost 2 decades.
> >
> > Kind regards
> >
> > Ahson
>
>
> Hi,
>
> Look at the RODBC package on CRAN by Prof. Ripley:
>
>   https://cran.r-project.org/web/packages/RODBC/index.html
>
> and be sure to read the package vignette for additional information:
>
>   https://cran.r-project.org/web/packages/RODBC/vignettes/RODBC.pdf
>
> There is also the r-sig-db e-mail list, which is focused in this domain,
> and where follow up questions should be posted:
>
>   https://stat.ethz.ch/mailman/listinfo/r-sig-db
>
> Regards,
>
> Marc Schwartz
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From rv15| @end|ng |rom y@hoo@@e  Mon Aug  5 15:18:14 2019
From: rv15| @end|ng |rom y@hoo@@e (ravi)
Date: Mon, 5 Aug 2019 13:18:14 +0000 (UTC)
Subject: [R] Accessing C source files
References: <776437535.1675595.1565011094777.ref@mail.yahoo.com>
Message-ID: <776437535.1675595.1565011094777@mail.yahoo.com>

Hi all,On looking at the source code for the integrate function, I find that it has the following call:
wk <- .External(C_call_dqagi, ff, rho = environment(),?? ? ? ? ? ? as.double(bound), inf, as.double(abs.tol), as.double(rel.tol),?? ? ? ? ? ? limit = limit)How do I access the source code for C_call_dqagi? From other references ("Writing R-extensions section 6.9), I find a reference to Rdqagi when discussing the integrate function. I would like to know if?C_call_dqagi and Rdqagi are the same.How do I access the source code for either of these functions? From the article titled "R help desk Acessing the sources" by Uwe Ligges in Rnews_2006-4, I gather that the C files can be found in the folder?$R_HOME/src/main/. But when I look at the equivalent folder in R 3.6.1,?I find that the main sub-folder is missing in src (the only sub-folder found there is library). I do not find any C file in the src subfolder?
Where do I find the C source files? How do I get to look at them? I have a lot of follow-up questions on interfacing with the C files, but I would like to first know where I can find them. At least on how I can access them.Thanks,Ravi Sutradhara


	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Aug  5 15:31:54 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 05 Aug 2019 06:31:54 -0700
Subject: [R] Accessing C source files
In-Reply-To: <776437535.1675595.1565011094777@mail.yahoo.com>
References: <776437535.1675595.1565011094777.ref@mail.yahoo.com>
 <776437535.1675595.1565011094777@mail.yahoo.com>
Message-ID: <5A74F16F-0BAD-4DDA-880A-B52E6A41159A@dcn.davis.ca.us>

Seems like you are looking in the wrong place. Did you download the source code? [1]

[1] https://cran.r-project.org/

On August 5, 2019 6:18:14 AM PDT, ravi via R-help <r-help at r-project.org> wrote:
>Hi all,On looking at the source code for the integrate function, I find
>that it has the following call:
>wk <- .External(C_call_dqagi, ff, rho = environment(),?? ? ? ? ? ?
>as.double(bound), inf, as.double(abs.tol), as.double(rel.tol),?? ? ? ?
>? ? limit = limit)How do I access the source code for C_call_dqagi?
>From other references ("Writing R-extensions section 6.9), I find a
>reference to Rdqagi when discussing the integrate function. I would
>like to know if?C_call_dqagi and Rdqagi are the same.How do I access
>the source code for either of these functions? From the article titled
>"R help desk Acessing the sources" by Uwe Ligges in Rnews_2006-4, I
>gather that the C files can be found in the folder?$R_HOME/src/main/.
>But when I look at the equivalent folder in R 3.6.1,?I find that the
>main sub-folder is missing in src (the only sub-folder found there is
>library). I do not find any C file in the src subfolder?
>Where do I find the C source files? How do I get to look at them? I
>have a lot of follow-up questions on interfacing with the C files, but
>I would like to first know where I can find them. At least on how I can
>access them.Thanks,Ravi Sutradhara
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From rv15| @end|ng |rom y@hoo@@e  Mon Aug  5 15:43:59 2019
From: rv15| @end|ng |rom y@hoo@@e (ravi)
Date: Mon, 5 Aug 2019 13:43:59 +0000 (UTC)
Subject: [R] Accessing C source files
In-Reply-To: <5A74F16F-0BAD-4DDA-880A-B52E6A41159A@dcn.davis.ca.us>
References: <776437535.1675595.1565011094777.ref@mail.yahoo.com>
 <776437535.1675595.1565011094777@mail.yahoo.com>
 <5A74F16F-0BAD-4DDA-880A-B52E6A41159A@dcn.davis.ca.us>
Message-ID: <119705198.1698522.1565012639387@mail.yahoo.com>

 Hi Jeff,Thanks for your quick answer. But I don't understand. I have installed R from the installer (I think that it is called the binary). Do you mean that I will have to do it via the source? Will I see the source code only then? Are there any other methods? In any case, it would be helpful if you can explain what you mean by "downloading the source code".Thanks,Ravi
    On Monday, 5 August 2019, 15:32:00 CEST, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:  
 
 Seems like you are looking in the wrong place. Did you download the source code? [1]

[1] https://cran.r-project.org/

On August 5, 2019 6:18:14 AM PDT, ravi via R-help <r-help at r-project.org> wrote:
>Hi all,On looking at the source code for the integrate function, I find
>that it has the following call:
>wk <- .External(C_call_dqagi, ff, rho = environment(),?? ? ? ? ? ?
>as.double(bound), inf, as.double(abs.tol), as.double(rel.tol),?? ? ? ?
>? ? limit = limit)How do I access the source code for C_call_dqagi?
>From other references ("Writing R-extensions section 6.9), I find a
>reference to Rdqagi when discussing the integrate function. I would
>like to know if?C_call_dqagi and Rdqagi are the same.How do I access
>the source code for either of these functions? From the article titled
>"R help desk Acessing the sources" by Uwe Ligges in Rnews_2006-4, I
>gather that the C files can be found in the folder?$R_HOME/src/main/.
>But when I look at the equivalent folder in R 3.6.1,?I find that the
>main sub-folder is missing in src (the only sub-folder found there is
>library). I do not find any C file in the src subfolder?
>Where do I find the C source files? How do I get to look at them? I
>have a lot of follow-up questions on interfacing with the C files, but
>I would like to first know where I can find them. At least on how I can
>access them.Thanks,Ravi Sutradhara
>
>
>??? [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.  
	[[alternative HTML version deleted]]


From rv15| @end|ng |rom y@hoo@@e  Mon Aug  5 15:48:25 2019
From: rv15| @end|ng |rom y@hoo@@e (ravi)
Date: Mon, 5 Aug 2019 13:48:25 +0000 (UTC)
Subject: [R] Accessing C source files
In-Reply-To: <5A74F16F-0BAD-4DDA-880A-B52E6A41159A@dcn.davis.ca.us>
References: <776437535.1675595.1565011094777.ref@mail.yahoo.com>
 <776437535.1675595.1565011094777@mail.yahoo.com>
 <5A74F16F-0BAD-4DDA-880A-B52E6A41159A@dcn.davis.ca.us>
Message-ID: <2086156269.1700333.1565012905032@mail.yahoo.com>

 Hi Jeff,Thanks for your quick answer. But I don't understand. I have installed R from the installer (I think that it is called the binary). Do you mean that I will have to do it via the source? Will I see the source code only then? Are there any other methods? In any case, it would be helpful if you can explain what you mean by "downloading the source code".Thanks,Ravi
    On Monday, 5 August 2019, 15:32:00 CEST, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:  
 
 Seems like you are looking in the wrong place. Did you download the source code? [1]

[1] https://cran.r-project.org/

On August 5, 2019 6:18:14 AM PDT, ravi via R-help <r-help at r-project.org> wrote:
>Hi all,On looking at the source code for the integrate function, I find
>that it has the following call:
>wk <- .External(C_call_dqagi, ff, rho = environment(),?? ? ? ? ? ?
>as.double(bound), inf, as.double(abs.tol), as.double(rel.tol),?? ? ? ?
>? ? limit = limit)How do I access the source code for C_call_dqagi?
>From other references ("Writing R-extensions section 6.9), I find a
>reference to Rdqagi when discussing the integrate function. I would
>like to know if?C_call_dqagi and Rdqagi are the same.How do I access
>the source code for either of these functions? From the article titled
>"R help desk Acessing the sources" by Uwe Ligges in Rnews_2006-4, I
>gather that the C files can be found in the folder?$R_HOME/src/main/.
>But when I look at the equivalent folder in R 3.6.1,?I find that the
>main sub-folder is missing in src (the only sub-folder found there is
>library). I do not find any C file in the src subfolder?
>Where do I find the C source files? How do I get to look at them? I
>have a lot of follow-up questions on interfacing with the C files, but
>I would like to first know where I can find them. At least on how I can
>access them.Thanks,Ravi Sutradhara
>
>
>??? [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.  
	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Mon Aug  5 15:49:46 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Mon, 5 Aug 2019 09:49:46 -0400
Subject: [R] Accessing C source files
In-Reply-To: <119705198.1698522.1565012639387@mail.yahoo.com>
References: <776437535.1675595.1565011094777.ref@mail.yahoo.com>
 <776437535.1675595.1565011094777@mail.yahoo.com>
 <5A74F16F-0BAD-4DDA-880A-B52E6A41159A@dcn.davis.ca.us>
 <119705198.1698522.1565012639387@mail.yahoo.com>
Message-ID: <ed00fea9-16a0-8d8c-c1ed-1d5a9b235366@gmail.com>

On 05/08/2019 9:43 a.m., ravi via R-help wrote:
>   Hi Jeff,Thanks for your quick answer. But I don't understand. I have installed R from the installer (I think that it is called the binary). Do you mean that I will have to do it via the source? Will I see the source code only then? Are there any other methods? In any case, it would be helpful if you can explain what you mean by "downloading the source code".Thanks,Ravi

You can see each file online if you don't want to download it all.  The 
current development code is in https://svn.r-project.org/R/trunk, 
individual versions are in subdirectories of 
https://svn.r-project.org/R/tags .  Pick one of these, and then follow 
links to src etc. within it.

Duncan Murdoch

>      On Monday, 5 August 2019, 15:32:00 CEST, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>   
>   Seems like you are looking in the wrong place. Did you download the source code? [1]
> 
> [1] https://cran.r-project.org/
> 
> On August 5, 2019 6:18:14 AM PDT, ravi via R-help <r-help at r-project.org> wrote:
>> Hi all,On looking at the source code for the integrate function, I find
>> that it has the following call:
>> wk <- .External(C_call_dqagi, ff, rho = environment(),
>> as.double(bound), inf, as.double(abs.tol), as.double(rel.tol),
>>  ? ? limit = limit)How do I access the source code for C_call_dqagi?
>>From other references ("Writing R-extensions section 6.9), I find a
>> reference to Rdqagi when discussing the integrate function. I would
>> like to know if?C_call_dqagi and Rdqagi are the same.How do I access
>> the source code for either of these functions? From the article titled
>> "R help desk Acessing the sources" by Uwe Ligges in Rnews_2006-4, I
>> gather that the C files can be found in the folder?$R_HOME/src/main/.
>> But when I look at the equivalent folder in R 3.6.1,?I find that the
>> main sub-folder is missing in src (the only sub-folder found there is
>> library). I do not find any C file in the src subfolder
>> Where do I find the C source files? How do I get to look at them? I
>> have a lot of follow-up questions on interfacing with the C files, but
>> I would like to first know where I can find them. At least on how I can
>> access them.Thanks,Ravi Sutradhara
>>
>>
>>  ??? [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From pd@|gd @end|ng |rom gm@||@com  Mon Aug  5 15:51:12 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Mon, 5 Aug 2019 15:51:12 +0200
Subject: [R] Accessing C source files
In-Reply-To: <5A74F16F-0BAD-4DDA-880A-B52E6A41159A@dcn.davis.ca.us>
References: <776437535.1675595.1565011094777.ref@mail.yahoo.com>
 <776437535.1675595.1565011094777@mail.yahoo.com>
 <5A74F16F-0BAD-4DDA-880A-B52E6A41159A@dcn.davis.ca.us>
Message-ID: <616BE97B-07DB-4D5F-AF10-F9FF0F6F73AA@gmail.com>

Also, Uwe's "Accessing the sources": https://cran.r-project.org/doc/Rnews/Rnews_2006-4.pdf, p.43.

It's a bit old, but I don't think massively out of date, except possibly with respect to name mangling like thee C_ prefix.

-pd

> On 5 Aug 2019, at 15:31 , Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> Seems like you are looking in the wrong place. Did you download the source code? [1]
> 
> [1] https://cran.r-project.org/
> 
> On August 5, 2019 6:18:14 AM PDT, ravi via R-help <r-help at r-project.org> wrote:
>> Hi all,On looking at the source code for the integrate function, I find
>> that it has the following call:
>> wk <- .External(C_call_dqagi, ff, rho = environment(),            
>> as.double(bound), inf, as.double(abs.tol), as.double(rel.tol),        
>>     limit = limit)How do I access the source code for C_call_dqagi?
>> From other references ("Writing R-extensions section 6.9), I find a
>> reference to Rdqagi when discussing the integrate function. I would
>> like to know if C_call_dqagi and Rdqagi are the same.How do I access
>> the source code for either of these functions? From the article titled
>> "R help desk Acessing the sources" by Uwe Ligges in Rnews_2006-4, I
>> gather that the C files can be found in the folder $R_HOME/src/main/.
>> But when I look at the equivalent folder in R 3.6.1, I find that the
>> main sub-folder is missing in src (the only sub-folder found there is
>> library). I do not find any C file in the src subfolder 
>> Where do I find the C source files? How do I get to look at them? I
>> have a lot of follow-up questions on interfacing with the C files, but
>> I would like to first know where I can find them. At least on how I can
>> access them.Thanks,Ravi Sutradhara
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> Sent from my phone. Please excuse my brevity.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Aug  5 15:56:08 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 05 Aug 2019 06:56:08 -0700
Subject: [R] Accessing C source files
In-Reply-To: <119705198.1698522.1565012639387@mail.yahoo.com>
References: <776437535.1675595.1565011094777.ref@mail.yahoo.com>
 <776437535.1675595.1565011094777@mail.yahoo.com>
 <5A74F16F-0BAD-4DDA-880A-B52E6A41159A@dcn.davis.ca.us>
 <119705198.1698522.1565012639387@mail.yahoo.com>
Message-ID: <3785A774-80FD-4303-B0A6-18408290BB01@dcn.davis.ca.us>

You cannot see C source code unless you download the source code tar.gz file and extract files from it. I gave you the URL. However, you are going to have to do some self-study to make sense of it... this is not the right place to learn/teach about how to download files or how C software works.

On August 5, 2019 6:43:59 AM PDT, ravi <rv15i at yahoo.se> wrote:
>Hi Jeff,Thanks for your quick answer. But I don't understand. I have
>installed R from the installer (I think that it is called the binary).
>Do you mean that I will have to do it via the source? Will I see the
>source code only then? Are there any other methods? In any case, it
>would be helpful if you can explain what you mean by "downloading the
>source code".Thanks,Ravi
>On Monday, 5 August 2019, 15:32:00 CEST, Jeff Newmiller
><jdnewmil at dcn.davis.ca.us> wrote:  
> 
>Seems like you are looking in the wrong place. Did you download the
>source code? [1]
>
>[1] https://cran.r-project.org/
>
>On August 5, 2019 6:18:14 AM PDT, ravi via R-help
><r-help at r-project.org> wrote:
>>Hi all,On looking at the source code for the integrate function, I
>find
>>that it has the following call:
>>wk <- .External(C_call_dqagi, ff, rho = environment(),?? ? ? ? ? ?
>>as.double(bound), inf, as.double(abs.tol), as.double(rel.tol),?? ? ? ?
>>? ? limit = limit)How do I access the source code for C_call_dqagi?
>>From other references ("Writing R-extensions section 6.9), I find a
>>reference to Rdqagi when discussing the integrate function. I would
>>like to know if?C_call_dqagi and Rdqagi are the same.How do I access
>>the source code for either of these functions? From the article titled
>>"R help desk Acessing the sources" by Uwe Ligges in Rnews_2006-4, I
>>gather that the C files can be found in the folder?$R_HOME/src/main/.
>>But when I look at the equivalent folder in R 3.6.1,?I find that the
>>main sub-folder is missing in src (the only sub-folder found there is
>>library). I do not find any C file in the src subfolder?
>>Where do I find the C source files? How do I get to look at them? I
>>have a lot of follow-up questions on interfacing with the C files, but
>>I would like to first know where I can find them. At least on how I
>can
>>access them.Thanks,Ravi Sutradhara
>>
>>
>>??? [[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From m@015k3113 @end|ng |rom b|ueyonder@co@uk  Mon Aug  5 18:32:42 2019
From: m@015k3113 @end|ng |rom b|ueyonder@co@uk (e-mail ma015k3113)
Date: Mon, 5 Aug 2019 17:32:42 +0100 (BST)
Subject: [R] Connect to Oracle database via ODBC
In-Reply-To: <93B48BC4-EBED-45F5-BC2A-891290BA3D0F@me.com>
References: <VI1PR07MB6381072984D70F108BC927D8E2CB0@VI1PR07MB6381.eurprd07.prod.outlook.com>
 <CAGxFJbS77URSG2MK+-T6Lg4SJf-SXvDzBoYN5k-oxdVN4JohAg@mail.gmail.com>
 <VI1PR07MB63815B5DD62811E213469B3BE2CB0@VI1PR07MB6381.eurprd07.prod.outlook.com>
 <C02592F3-6C54-41E5-BD38-38F147E106E6@dcn.davis.ca.us>
 <345608099.59546.1565002980507@mail2.virginmedia.com>
 <93B48BC4-EBED-45F5-BC2A-891290BA3D0F@me.com>
Message-ID: <1662559453.70988.1565022762446@mail2.virginmedia.com>

Dear Marc,
Thanks-much appreciated

Kind regards

Ahson
> On 05 August 2019 at 12:54 Marc Schwartz <marc_schwartz at me.com> wrote:
> 
> 
> 
> > On Aug 5, 2019, at 7:03 AM, e-mail ma015k3113 via R-help <r-help at r-project.org> wrote:
> > 
> > Dear All, can anyone point me towards information for connecting to a Oracle instance via DSN. I have already established a ODBC connection.
> > 
> > Sorry if this is very elementary-I am just getting started with R after using SAS for almost 2 decades.
> > 
> > Kind regards
> > 
> > Ahson
> 
> 
> Hi,
> 
> Look at the RODBC package on CRAN by Prof. Ripley:
> 
>   https://cran.r-project.org/web/packages/RODBC/index.html
> 
> and be sure to read the package vignette for additional information:
> 
>   https://cran.r-project.org/web/packages/RODBC/vignettes/RODBC.pdf
> 
> There is also the r-sig-db e-mail list, which is focused in this domain, and where follow up questions should be posted:
> 
>   https://stat.ethz.ch/mailman/listinfo/r-sig-db
> 
> Regards,
> 
> Marc Schwartz
>


From m@015k3113 @end|ng |rom b|ueyonder@co@uk  Mon Aug  5 18:33:54 2019
From: m@015k3113 @end|ng |rom b|ueyonder@co@uk (e-mail ma015k3113)
Date: Mon, 5 Aug 2019 17:33:54 +0100 (BST)
Subject: [R] Connect to Oracle database via ODBC
In-Reply-To: <CAGgJW744v1FasFVx0JVROnbAYHY8FJL_gN=7cSvHR5Sp43-sMw@mail.gmail.com>
References: <VI1PR07MB6381072984D70F108BC927D8E2CB0@VI1PR07MB6381.eurprd07.prod.outlook.com>
 <CAGxFJbS77URSG2MK+-T6Lg4SJf-SXvDzBoYN5k-oxdVN4JohAg@mail.gmail.com>
 <VI1PR07MB63815B5DD62811E213469B3BE2CB0@VI1PR07MB6381.eurprd07.prod.outlook.com>
 <C02592F3-6C54-41E5-BD38-38F147E106E6@dcn.davis.ca.us>
 <345608099.59546.1565002980507@mail2.virginmedia.com>
 <93B48BC4-EBED-45F5-BC2A-891290BA3D0F@me.com>
 <CAGgJW744v1FasFVx0JVROnbAYHY8FJL_gN=7cSvHR5Sp43-sMw@mail.gmail.com>
Message-ID: <761150405.71033.1565022834198@mail2.virginmedia.com>

Dear Eric, thanks-much appreciated.


Kind regards


Ahson

> On 05 August 2019 at 13:19 Eric Berger <ericjberger at gmail.com> wrote:
> 
>     Hi Ahson,
>     Many people use R via RStudio, an IDE that has both free and non-free versions.
>     RStudio has invested a lot of effort into making it easier to establish connections to databases.
>     If this sounds of interest to you, take a look at
>     https://db.rstudio.com/ 
> 
>     HTH,
>     Eric
>      
> 
>     On Mon, Aug 5, 2019 at 2:54 PM Marc Schwartz via R-help < r-help at r-project.org mailto:r-help at r-project.org > wrote:
> 
>         > >         > On Aug 5, 2019, at 7:03 AM, e-mail ma015k3113 via R-help < r-help at r-project.org mailto:r-help at r-project.org > wrote:
> >         >
> >         > Dear All, can anyone point me towards information for connecting to a Oracle instance via DSN. I have already established a ODBC connection.
> >         >
> >         > Sorry if this is very elementary-I am just getting started with R after using SAS for almost 2 decades.
> >         >
> >         > Kind regards
> >         >
> >         > Ahson
> > 
> > 
> >         Hi,
> > 
> >         Look at the RODBC package on CRAN by Prof. Ripley:
> > 
> >           https://cran.r-project.org/web/packages/RODBC/index.html
> > 
> >         and be sure to read the package vignette for additional information:
> > 
> >           https://cran.r-project.org/web/packages/RODBC/vignettes/RODBC.pdf
> > 
> >         There is also the r-sig-db e-mail list, which is focused in this domain, and where follow up questions should be posted:
> > 
> >           https://stat.ethz.ch/mailman/listinfo/r-sig-db
> > 
> >         Regards,
> > 
> >         Marc Schwartz
> > 
> >         ______________________________________________
> >         R-help at r-project.org mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >         https://stat.ethz.ch/mailman/listinfo/r-help
> >         PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >         and provide commented, minimal, self-contained, reproducible code.
> > 
> >     > 

	[[alternative HTML version deleted]]


From p@u|bern@|07 @end|ng |rom gm@||@com  Mon Aug  5 18:53:32 2019
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Mon, 5 Aug 2019 11:53:32 -0500
Subject: [R] accuracy function from forecast package
Message-ID: <CAMOcQfPmdE_LStWvOYqfsG1mhRoxQkWuDdkeCqrzCFDkMEgQKQ@mail.gmail.com>

Dear friends,

Hope you are all doing great. Does R?s accuracy function from the forecast
package  performs cross-validation? Or can I say that the accuracy function
does cross-validation?

Best regards,

Paul

	[[alternative HTML version deleted]]


From reichm@@j m@iii@g oii sbcgiob@i@@et  Tue Aug  6 04:25:48 2019
From: reichm@@j m@iii@g oii sbcgiob@i@@et (reichm@@j m@iii@g oii sbcgiob@i@@et)
Date: Mon, 5 Aug 2019 21:25:48 -0500
Subject: [R] Plotting hclust()  results
Message-ID: <000d01d54bfe$43178950$c9469bf0$@sbcglobal.net>

R Help Forum

I have output from hierarchal clustering and want to plot the results using
the ggplot2 function. Where I have a scatter plot with 5 lines representing
the 5 clusters. I assuming I need to transform the data into three columns
(like) cluster, variable, and value. Not an issue but was wondering if I
could plot the data as is.  

  Cluster     A     B     C     D     E     F
    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
1       1  0.67  0.56  0.54  0.03 -0.97 -1.09
2       2  0.52  0.43  0.51  0.05 -0.86 -0.96
3       3  0.2   0.3   0.19  0.08  0.02  0.09
4       4 -1.3  -1.13 -1.78 -0.05 -0.23 -0.23
5       5 -1.6  -1.3  -1.34 -0.23 -0.45 -0.45

Something like

Cluster Var	Value
1	A	0.67
1	B	0.56 etc

 Then group by cluster??

Jeff


From g3h@5 @end|ng |rom comc@@t@net  Sun Aug  4 02:18:42 2019
From: g3h@5 @end|ng |rom comc@@t@net (Barnet Wagman)
Date: Sat, 3 Aug 2019 17:18:42 -0700
Subject: [R] Can't install R6 in new installation of R 3.5.2
Message-ID: <a003333f-7006-5091-2462-fdd23bfb1eaf@comcast.net>

I've just installed R 3.5.2 (on Debian 10, Buster) and am unable to 
install R6.

    install.packages("R6");

yields

    Installing package into ?/home/xxx/R/x86_64-pc-linux-gnu-library/3.5?
    (as ?lib? is unspecified)
    trying URL 'https://cloud.r-project.org/src/contrib/R6_2.4.0.tar.gz'
    Content type 'application/x-gzip' length 31545 bytes (30 KB)
    ==================================================
    downloaded 30 KB

    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted

    The downloaded source packages are in
     ??? ?/tmp/RtmpMQfLun/downloaded_packages?
    Warning message:
    In install.packages("R6") :
     ? installation of package ?R6? had non-zero exit status

Adding the 'dependencies=TRUE option' yields a whole slew of

    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted

errors.? (The full output is below.)

I'd appreciate any thoughts on this.? I've installed R /many/ times and 
haven't run into this problem before.

Thanks.


------------------------------------------------------------------------

Full output of 'install.packages("R6",dependencies=TRUE)'

     > install.packages("R6",dependencies=TRUE)
    Installing package into ?/home/xxx/R/x86_64-pc-linux-gnu-library/3.5?
    (as ?lib? is unspecified)
    also installing the dependencies ?backports?, ?ellipsis?, ?zeallot?,
    ?utf8?, ?vctrs?, ?mime?, ?glue?, ?stringi?, ?assertthat?, ?plyr?,
    ?fansi?, ?pillar?, ?pkgconfig?, ?colorspace?, ?evaluate?, ?highr?,
    ?markdown?, ?stringr?, ?yaml?, ?xfun?, ?Rcpp?, ?cli?, ?crayon?,
    ?digest?, ?magrittr?, ?praise?, ?rlang?, ?withr?, ?gtable?,
    ?lazyeval?, ?reshape2?, ?tibble?, ?viridisLite?, ?labeling?,
    ?munsell?, ?RColorBrewer?, ?knitr?, ?microbenchmark?, ?pryr?,
    ?testthat?, ?ggplot2?, ?scales?

    trying URL
    'https://cloud.r-project.org/src/contrib/backports_1.1.4.tar.gz'
    Content type 'application/x-gzip' length 13859 bytes (13 KB)
    ==================================================
    downloaded 13 KB

    trying URL
    'https://cloud.r-project.org/src/contrib/ellipsis_0.2.0.1.tar.gz'
    Content type 'application/x-gzip' length 7045 bytes
    ==================================================
    downloaded 7045 bytes

    trying URL
    'https://cloud.r-project.org/src/contrib/zeallot_0.1.0.tar.gz'
    Content type 'application/x-gzip' length 32549 bytes (31 KB)
    ==================================================
    downloaded 31 KB

    trying URL 'https://cloud.r-project.org/src/contrib/utf8_1.1.4.tar.gz'
    Content type 'application/x-gzip' length 218882 bytes (213 KB)
    ==================================================
    downloaded 213 KB

    trying URL 'https://cloud.r-project.org/src/contrib/vctrs_0.2.0.tar.gz'
    Content type 'application/x-gzip' length 668258 bytes (652 KB)
    ==================================================
    downloaded 652 KB

    trying URL 'https://cloud.r-project.org/src/contrib/mime_0.7.tar.gz'
    Content type 'application/x-gzip' length 13130 bytes (12 KB)
    ==================================================
    downloaded 12 KB

    trying URL 'https://cloud.r-project.org/src/contrib/glue_1.3.1.tar.gz'
    Content type 'application/x-gzip' length 122950 bytes (120 KB)
    ==================================================
    downloaded 120 KB

    trying URL
    'https://cloud.r-project.org/src/contrib/stringi_1.4.3.tar.gz'
    Content type 'application/x-gzip' length 7290890 bytes (7.0 MB)
    ==================================================
    downloaded 7.0 MB

    trying URL
    'https://cloud.r-project.org/src/contrib/assertthat_0.2.1.tar.gz'
    Content type 'application/x-gzip' length 12742 bytes (12 KB)
    ==================================================
    downloaded 12 KB

    trying URL 'https://cloud.r-project.org/src/contrib/plyr_1.8.4.tar.gz'
    Content type 'application/x-gzip' length 392451 bytes (383 KB)
    ==================================================
    downloaded 383 KB

    trying URL 'https://cloud.r-project.org/src/contrib/fansi_0.4.0.tar.gz'
    Content type 'application/x-gzip' length 266123 bytes (259 KB)
    ==================================================
    downloaded 259 KB

    trying URL 'https://cloud.r-project.org/src/contrib/pillar_1.4.2.tar.gz'
    Content type 'application/x-gzip' length 228815 bytes (223 KB)
    ==================================================
    downloaded 223 KB

    trying URL
    'https://cloud.r-project.org/src/contrib/pkgconfig_2.0.2.tar.gz'
    Content type 'application/x-gzip' length 6024 bytes
    ==================================================
    downloaded 6024 bytes

    trying URL
    'https://cloud.r-project.org/src/contrib/colorspace_1.4-1.tar.gz'
    Content type 'application/x-gzip' length 2152594 bytes (2.1 MB)
    ==================================================
    downloaded 2.1 MB

    trying URL
    'https://cloud.r-project.org/src/contrib/evaluate_0.14.tar.gz'
    Content type 'application/x-gzip' length 24206 bytes (23 KB)
    ==================================================
    downloaded 23 KB

    trying URL 'https://cloud.r-project.org/src/contrib/highr_0.8.tar.gz'
    Content type 'application/x-gzip' length 17445 bytes (17 KB)
    ==================================================
    downloaded 17 KB

    trying URL 'https://cloud.r-project.org/src/contrib/markdown_1.0.tar.gz'
    Content type 'application/x-gzip' length 80843 bytes (78 KB)
    ==================================================
    downloaded 78 KB

    trying URL
    'https://cloud.r-project.org/src/contrib/stringr_1.4.0.tar.gz'
    Content type 'application/x-gzip' length 135777 bytes (132 KB)
    ==================================================
    downloaded 132 KB

    trying URL 'https://cloud.r-project.org/src/contrib/yaml_2.2.0.tar.gz'
    Content type 'application/x-gzip' length 89447 bytes (87 KB)
    ==================================================
    downloaded 87 KB

    trying URL 'https://cloud.r-project.org/src/contrib/xfun_0.8.tar.gz'
    Content type 'application/x-gzip' length 54885 bytes (53 KB)
    ==================================================
    downloaded 53 KB

    trying URL 'https://cloud.r-project.org/src/contrib/Rcpp_1.0.2.tar.gz'
    Content type 'application/x-gzip' length 3699570 bytes (3.5 MB)
    ==================================================
    downloaded 3.5 MB

    trying URL 'https://cloud.r-project.org/src/contrib/cli_1.1.0.tar.gz'
    Content type 'application/x-gzip' length 40232 bytes (39 KB)
    ==================================================
    downloaded 39 KB

    trying URL 'https://cloud.r-project.org/src/contrib/crayon_1.3.4.tar.gz'
    Content type 'application/x-gzip' length 658694 bytes (643 KB)
    ==================================================
    downloaded 643 KB

    trying URL
    'https://cloud.r-project.org/src/contrib/digest_0.6.20.tar.gz'
    Content type 'application/x-gzip' length 135997 bytes (132 KB)
    ==================================================
    downloaded 132 KB

    trying URL 'https://cloud.r-project.org/src/contrib/magrittr_1.5.tar.gz'
    Content type 'application/x-gzip' length 200504 bytes (195 KB)
    ==================================================
    downloaded 195 KB

    trying URL 'https://cloud.r-project.org/src/contrib/praise_1.0.0.tar.gz'
    Content type 'application/x-gzip' length 6100 bytes
    ==================================================
    downloaded 6100 bytes

    trying URL 'https://cloud.r-project.org/src/contrib/rlang_0.4.0.tar.gz'
    Content type 'application/x-gzip' length 859737 bytes (839 KB)
    ==================================================
    downloaded 839 KB

    trying URL 'https://cloud.r-project.org/src/contrib/withr_2.1.2.tar.gz'
    Content type 'application/x-gzip' length 53578 bytes (52 KB)
    ==================================================
    downloaded 52 KB

    trying URL 'https://cloud.r-project.org/src/contrib/gtable_0.3.0.tar.gz'
    Content type 'application/x-gzip' length 368081 bytes (359 KB)
    ==================================================
    downloaded 359 KB

    trying URL
    'https://cloud.r-project.org/src/contrib/lazyeval_0.2.2.tar.gz'
    Content type 'application/x-gzip' length 83482 bytes (81 KB)
    ==================================================
    downloaded 81 KB

    trying URL
    'https://cloud.r-project.org/src/contrib/reshape2_1.4.3.tar.gz'
    Content type 'application/x-gzip' length 36405 bytes (35 KB)
    ==================================================
    downloaded 35 KB

    trying URL 'https://cloud.r-project.org/src/contrib/tibble_2.1.3.tar.gz'
    Content type 'application/x-gzip' length 310774 bytes (303 KB)
    ==================================================
    downloaded 303 KB

    trying URL
    'https://cloud.r-project.org/src/contrib/viridisLite_0.3.0.tar.gz'
    Content type 'application/x-gzip' length 44019 bytes (42 KB)
    ==================================================
    downloaded 42 KB

    trying URL 'https://cloud.r-project.org/src/contrib/labeling_0.3.tar.gz'
    Content type 'application/x-gzip' length 10722 bytes (10 KB)
    ==================================================
    downloaded 10 KB

    trying URL
    'https://cloud.r-project.org/src/contrib/munsell_0.5.0.tar.gz'
    Content type 'application/x-gzip' length 182653 bytes (178 KB)
    ==================================================
    downloaded 178 KB

    trying URL
    'https://cloud.r-project.org/src/contrib/RColorBrewer_1.1-2.tar.gz'
    Content type 'application/x-gzip' length 11532 bytes (11 KB)
    ==================================================
    downloaded 11 KB

    trying URL 'https://cloud.r-project.org/src/contrib/knitr_1.23.tar.gz'
    Content type 'application/x-gzip' length 880134 bytes (859 KB)
    ==================================================
    downloaded 859 KB

    trying URL
    'https://cloud.r-project.org/src/contrib/microbenchmark_1.4-6.tar.gz'
    Content type 'application/x-gzip' length 57682 bytes (56 KB)
    ==================================================
    downloaded 56 KB

    trying URL 'https://cloud.r-project.org/src/contrib/pryr_0.1.4.tar.gz'
    Content type 'application/x-gzip' length 43041 bytes (42 KB)
    ==================================================
    downloaded 42 KB

    trying URL
    'https://cloud.r-project.org/src/contrib/testthat_2.2.1.tar.gz'
    Content type 'application/x-gzip' length 235874 bytes (230 KB)
    ==================================================
    downloaded 230 KB

    trying URL
    'https://cloud.r-project.org/src/contrib/ggplot2_3.2.0.tar.gz'
    Content type 'application/x-gzip' length 3193995 bytes (3.0 MB)
    ==================================================
    downloaded 3.0 MB

    trying URL 'https://cloud.r-project.org/src/contrib/scales_1.0.0.tar.gz'
    Content type 'application/x-gzip' length 299262 bytes (292 KB)
    ==================================================
    downloaded 292 KB

    trying URL 'https://cloud.r-project.org/src/contrib/R6_2.4.0.tar.gz'
    Content type 'application/x-gzip' length 31545 bytes (30 KB)
    ==================================================
    downloaded 30 KB

    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted
    Error in library(R6) : there is no package called ?R6?
    Calls: source -> withVisible -> eval -> eval -> library
    Execution halted

    The downloaded source packages are in
     ??? ?/tmp/RtmpMQfLun/downloaded_packages?
    There were 43 warnings (use warnings() to see them)


From bdwgen @end|ng |rom gm@||@com  Sun Aug  4 02:21:05 2019
From: bdwgen @end|ng |rom gm@||@com (Barnet Wagman)
Date: Sat, 3 Aug 2019 17:21:05 -0700
Subject: [R] Can't install R6 in new installation of R 3.5.2
Message-ID: <b349f491-9b5c-3f82-0b0b-3e4e26b0fc26@gmail.com>

I've just installed R 3.5.2 (on Debian 10, Buster) and am unable to 
install R6.

install.packages("R6");

yields

Installing package into ?/home/xxx/R/x86_64-pc-linux-gnu-library/3.5?
(as ?lib? is unspecified)
trying URL 'https://cloud.r-project.org/src/contrib/R6_2.4.0.tar.gz'
Content type 'application/x-gzip' length 31545 bytes (30 KB)
==================================================
downloaded 30 KB

Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted

The downloaded source packages are in
 ??? ?/tmp/RtmpMQfLun/downloaded_packages?
Warning message:
In install.packages("R6") :
 ? installation of package ?R6? had non-zero exit status

Adding the 'dependencies=TRUE option' yields a whole slew of

Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted

errors.? (The full output is below.)

I'd appreciate any thoughts on this.? I've installed R /many/ times and 
haven't run into this problem before.

Thanks.


------------------------------------------------------------------------

Full output of 'install.packages("R6",dependencies=TRUE)'

 > install.packages("R6",dependencies=TRUE)
Installing package into ?/home/xxx/R/x86_64-pc-linux-gnu-library/3.5?
(as ?lib? is unspecified)
also installing the dependencies ?backports?, ?ellipsis?, ?zeallot?,
?utf8?, ?vctrs?, ?mime?, ?glue?, ?stringi?, ?assertthat?, ?plyr?,
?fansi?, ?pillar?, ?pkgconfig?, ?colorspace?, ?evaluate?, ?highr?,
?markdown?, ?stringr?, ?yaml?, ?xfun?, ?Rcpp?, ?cli?, ?crayon?,
?digest?, ?magrittr?, ?praise?, ?rlang?, ?withr?, ?gtable?,
?lazyeval?, ?reshape2?, ?tibble?, ?viridisLite?, ?labeling?,
?munsell?, ?RColorBrewer?, ?knitr?, ?microbenchmark?, ?pryr?,
?testthat?, ?ggplot2?, ?scales?

trying URL
'https://cloud.r-project.org/src/contrib/backports_1.1.4.tar.gz'
Content type 'application/x-gzip' length 13859 bytes (13 KB)
==================================================
downloaded 13 KB

trying URL
'https://cloud.r-project.org/src/contrib/ellipsis_0.2.0.1.tar.gz'
Content type 'application/x-gzip' length 7045 bytes
==================================================
downloaded 7045 bytes

trying URL
'https://cloud.r-project.org/src/contrib/zeallot_0.1.0.tar.gz'
Content type 'application/x-gzip' length 32549 bytes (31 KB)
==================================================
downloaded 31 KB

trying URL 'https://cloud.r-project.org/src/contrib/utf8_1.1.4.tar.gz'
Content type 'application/x-gzip' length 218882 bytes (213 KB)
==================================================
downloaded 213 KB

trying URL 'https://cloud.r-project.org/src/contrib/vctrs_0.2.0.tar.gz'
Content type 'application/x-gzip' length 668258 bytes (652 KB)
==================================================
downloaded 652 KB

trying URL 'https://cloud.r-project.org/src/contrib/mime_0.7.tar.gz'
Content type 'application/x-gzip' length 13130 bytes (12 KB)
==================================================
downloaded 12 KB

trying URL 'https://cloud.r-project.org/src/contrib/glue_1.3.1.tar.gz'
Content type 'application/x-gzip' length 122950 bytes (120 KB)
==================================================
downloaded 120 KB

trying URL
'https://cloud.r-project.org/src/contrib/stringi_1.4.3.tar.gz'
Content type 'application/x-gzip' length 7290890 bytes (7.0 MB)
==================================================
downloaded 7.0 MB

trying URL
'https://cloud.r-project.org/src/contrib/assertthat_0.2.1.tar.gz'
Content type 'application/x-gzip' length 12742 bytes (12 KB)
==================================================
downloaded 12 KB

trying URL 'https://cloud.r-project.org/src/contrib/plyr_1.8.4.tar.gz'
Content type 'application/x-gzip' length 392451 bytes (383 KB)
==================================================
downloaded 383 KB

trying URL 'https://cloud.r-project.org/src/contrib/fansi_0.4.0.tar.gz'
Content type 'application/x-gzip' length 266123 bytes (259 KB)
==================================================
downloaded 259 KB

trying URL 'https://cloud.r-project.org/src/contrib/pillar_1.4.2.tar.gz'
Content type 'application/x-gzip' length 228815 bytes (223 KB)
==================================================
downloaded 223 KB

trying URL
'https://cloud.r-project.org/src/contrib/pkgconfig_2.0.2.tar.gz'
Content type 'application/x-gzip' length 6024 bytes
==================================================
downloaded 6024 bytes

trying URL
'https://cloud.r-project.org/src/contrib/colorspace_1.4-1.tar.gz'
Content type 'application/x-gzip' length 2152594 bytes (2.1 MB)
==================================================
downloaded 2.1 MB

trying URL
'https://cloud.r-project.org/src/contrib/evaluate_0.14.tar.gz'
Content type 'application/x-gzip' length 24206 bytes (23 KB)
==================================================
downloaded 23 KB

trying URL 'https://cloud.r-project.org/src/contrib/highr_0.8.tar.gz'
Content type 'application/x-gzip' length 17445 bytes (17 KB)
==================================================
downloaded 17 KB

trying URL 'https://cloud.r-project.org/src/contrib/markdown_1.0.tar.gz'
Content type 'application/x-gzip' length 80843 bytes (78 KB)
==================================================
downloaded 78 KB

trying URL
'https://cloud.r-project.org/src/contrib/stringr_1.4.0.tar.gz'
Content type 'application/x-gzip' length 135777 bytes (132 KB)
==================================================
downloaded 132 KB

trying URL 'https://cloud.r-project.org/src/contrib/yaml_2.2.0.tar.gz'
Content type 'application/x-gzip' length 89447 bytes (87 KB)
==================================================
downloaded 87 KB

trying URL 'https://cloud.r-project.org/src/contrib/xfun_0.8.tar.gz'
Content type 'application/x-gzip' length 54885 bytes (53 KB)
==================================================
downloaded 53 KB

trying URL 'https://cloud.r-project.org/src/contrib/Rcpp_1.0.2.tar.gz'
Content type 'application/x-gzip' length 3699570 bytes (3.5 MB)
==================================================
downloaded 3.5 MB

trying URL 'https://cloud.r-project.org/src/contrib/cli_1.1.0.tar.gz'
Content type 'application/x-gzip' length 40232 bytes (39 KB)
==================================================
downloaded 39 KB

trying URL 'https://cloud.r-project.org/src/contrib/crayon_1.3.4.tar.gz'
Content type 'application/x-gzip' length 658694 bytes (643 KB)
==================================================
downloaded 643 KB

trying URL
'https://cloud.r-project.org/src/contrib/digest_0.6.20.tar.gz'
Content type 'application/x-gzip' length 135997 bytes (132 KB)
==================================================
downloaded 132 KB

trying URL 'https://cloud.r-project.org/src/contrib/magrittr_1.5.tar.gz'
Content type 'application/x-gzip' length 200504 bytes (195 KB)
==================================================
downloaded 195 KB

trying URL 'https://cloud.r-project.org/src/contrib/praise_1.0.0.tar.gz'
Content type 'application/x-gzip' length 6100 bytes
==================================================
downloaded 6100 bytes

trying URL 'https://cloud.r-project.org/src/contrib/rlang_0.4.0.tar.gz'
Content type 'application/x-gzip' length 859737 bytes (839 KB)
==================================================
downloaded 839 KB

trying URL 'https://cloud.r-project.org/src/contrib/withr_2.1.2.tar.gz'
Content type 'application/x-gzip' length 53578 bytes (52 KB)
==================================================
downloaded 52 KB

trying URL 'https://cloud.r-project.org/src/contrib/gtable_0.3.0.tar.gz'
Content type 'application/x-gzip' length 368081 bytes (359 KB)
==================================================
downloaded 359 KB

trying URL
'https://cloud.r-project.org/src/contrib/lazyeval_0.2.2.tar.gz'
Content type 'application/x-gzip' length 83482 bytes (81 KB)
==================================================
downloaded 81 KB

trying URL
'https://cloud.r-project.org/src/contrib/reshape2_1.4.3.tar.gz'
Content type 'application/x-gzip' length 36405 bytes (35 KB)
==================================================
downloaded 35 KB

trying URL 'https://cloud.r-project.org/src/contrib/tibble_2.1.3.tar.gz'
Content type 'application/x-gzip' length 310774 bytes (303 KB)
==================================================
downloaded 303 KB

trying URL
'https://cloud.r-project.org/src/contrib/viridisLite_0.3.0.tar.gz'
Content type 'application/x-gzip' length 44019 bytes (42 KB)
==================================================
downloaded 42 KB

trying URL 'https://cloud.r-project.org/src/contrib/labeling_0.3.tar.gz'
Content type 'application/x-gzip' length 10722 bytes (10 KB)
==================================================
downloaded 10 KB

trying URL
'https://cloud.r-project.org/src/contrib/munsell_0.5.0.tar.gz'
Content type 'application/x-gzip' length 182653 bytes (178 KB)
==================================================
downloaded 178 KB

trying URL
'https://cloud.r-project.org/src/contrib/RColorBrewer_1.1-2.tar.gz'
Content type 'application/x-gzip' length 11532 bytes (11 KB)
==================================================
downloaded 11 KB

trying URL 'https://cloud.r-project.org/src/contrib/knitr_1.23.tar.gz'
Content type 'application/x-gzip' length 880134 bytes (859 KB)
==================================================
downloaded 859 KB

trying URL
'https://cloud.r-project.org/src/contrib/microbenchmark_1.4-6.tar.gz'
Content type 'application/x-gzip' length 57682 bytes (56 KB)
==================================================
downloaded 56 KB

trying URL 'https://cloud.r-project.org/src/contrib/pryr_0.1.4.tar.gz'
Content type 'application/x-gzip' length 43041 bytes (42 KB)
==================================================
downloaded 42 KB

trying URL
'https://cloud.r-project.org/src/contrib/testthat_2.2.1.tar.gz'
Content type 'application/x-gzip' length 235874 bytes (230 KB)
==================================================
downloaded 230 KB

trying URL
'https://cloud.r-project.org/src/contrib/ggplot2_3.2.0.tar.gz'
Content type 'application/x-gzip' length 3193995 bytes (3.0 MB)
==================================================
downloaded 3.0 MB

trying URL 'https://cloud.r-project.org/src/contrib/scales_1.0.0.tar.gz'
Content type 'application/x-gzip' length 299262 bytes (292 KB)
==================================================
downloaded 292 KB

trying URL 'https://cloud.r-project.org/src/contrib/R6_2.4.0.tar.gz'
Content type 'application/x-gzip' length 31545 bytes (30 KB)
==================================================
downloaded 30 KB

Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted
Error in library(R6) : there is no package called ?R6?
Calls: source -> withVisible -> eval -> eval -> library
Execution halted

The downloaded source packages are in
 ??? ?/tmp/RtmpMQfLun/downloaded_packages?
There were 43 warnings (use warnings() to see them)


From bdwgen @end|ng |rom gm@||@com  Sun Aug  4 03:11:15 2019
From: bdwgen @end|ng |rom gm@||@com (Barnet Wagman)
Date: Sat, 3 Aug 2019 18:11:15 -0700
Subject: [R] Solution: Re: Can't install R6 in new installation of R 3.5.2
In-Reply-To: <b349f491-9b5c-3f82-0b0b-3e4e26b0fc26@gmail.com>
References: <b349f491-9b5c-3f82-0b0b-3e4e26b0fc26@gmail.com>
Message-ID: <adc660f1-d909-8c43-2aa0-608119750b95@gmail.com>

The problem was not specific to R6 (and probably not to R 3.5.2).

In my .RProfile,? I invoke source() on several of my *.R files that load 
R6.? Apparently the failed attempt to load a package prevents it from 
being installed. Commenting out the offending lines in .RProfile solved 
the problem.

FYI is this true attempting to do an install both from within an R 
session and at the command line with 'R CMD INSTALL ...'

Regards

On 8/3/19 5:21 PM, Barnet Wagman wrote:
> I've just installed R 3.5.2 (on Debian 10, Buster) and am unable to 
> install R6.
>
> install.packages("R6");
>
> yields
>
> Installing package into ?/home/xxx/R/x86_64-pc-linux-gnu-library/3.5?
> (as ?lib? is unspecified)
> trying URL 'https://cloud.r-project.org/src/contrib/R6_2.4.0.tar.gz'
> Content type 'application/x-gzip' length 31545 bytes (30 KB)
> ==================================================
> downloaded 30 KB
>
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
>
> The downloaded source packages are in
> ??? ?/tmp/RtmpMQfLun/downloaded_packages?
> Warning message:
> In install.packages("R6") :
> ? installation of package ?R6? had non-zero exit status
>
> Adding the 'dependencies=TRUE option' yields a whole slew of
>
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
>
> errors.? (The full output is below.)
>
> I'd appreciate any thoughts on this.? I've installed R /many/ times 
> and haven't run into this problem before.
>
> Thanks.
>
>
> ------------------------------------------------------------------------
>
> Full output of 'install.packages("R6",dependencies=TRUE)'
>
> > install.packages("R6",dependencies=TRUE)
> Installing package into ?/home/xxx/R/x86_64-pc-linux-gnu-library/3.5?
> (as ?lib? is unspecified)
> also installing the dependencies ?backports?, ?ellipsis?, ?zeallot?,
> ?utf8?, ?vctrs?, ?mime?, ?glue?, ?stringi?, ?assertthat?, ?plyr?,
> ?fansi?, ?pillar?, ?pkgconfig?, ?colorspace?, ?evaluate?, ?highr?,
> ?markdown?, ?stringr?, ?yaml?, ?xfun?, ?Rcpp?, ?cli?, ?crayon?,
> ?digest?, ?magrittr?, ?praise?, ?rlang?, ?withr?, ?gtable?,
> ?lazyeval?, ?reshape2?, ?tibble?, ?viridisLite?, ?labeling?,
> ?munsell?, ?RColorBrewer?, ?knitr?, ?microbenchmark?, ?pryr?,
> ?testthat?, ?ggplot2?, ?scales?
>
> trying URL
> 'https://cloud.r-project.org/src/contrib/backports_1.1.4.tar.gz'
> Content type 'application/x-gzip' length 13859 bytes (13 KB)
> ==================================================
> downloaded 13 KB
>
> trying URL
> 'https://cloud.r-project.org/src/contrib/ellipsis_0.2.0.1.tar.gz'
> Content type 'application/x-gzip' length 7045 bytes
> ==================================================
> downloaded 7045 bytes
>
> trying URL
> 'https://cloud.r-project.org/src/contrib/zeallot_0.1.0.tar.gz'
> Content type 'application/x-gzip' length 32549 bytes (31 KB)
> ==================================================
> downloaded 31 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/utf8_1.1.4.tar.gz'
> Content type 'application/x-gzip' length 218882 bytes (213 KB)
> ==================================================
> downloaded 213 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/vctrs_0.2.0.tar.gz'
> Content type 'application/x-gzip' length 668258 bytes (652 KB)
> ==================================================
> downloaded 652 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/mime_0.7.tar.gz'
> Content type 'application/x-gzip' length 13130 bytes (12 KB)
> ==================================================
> downloaded 12 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/glue_1.3.1.tar.gz'
> Content type 'application/x-gzip' length 122950 bytes (120 KB)
> ==================================================
> downloaded 120 KB
>
> trying URL
> 'https://cloud.r-project.org/src/contrib/stringi_1.4.3.tar.gz'
> Content type 'application/x-gzip' length 7290890 bytes (7.0 MB)
> ==================================================
> downloaded 7.0 MB
>
> trying URL
> 'https://cloud.r-project.org/src/contrib/assertthat_0.2.1.tar.gz'
> Content type 'application/x-gzip' length 12742 bytes (12 KB)
> ==================================================
> downloaded 12 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/plyr_1.8.4.tar.gz'
> Content type 'application/x-gzip' length 392451 bytes (383 KB)
> ==================================================
> downloaded 383 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/fansi_0.4.0.tar.gz'
> Content type 'application/x-gzip' length 266123 bytes (259 KB)
> ==================================================
> downloaded 259 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/pillar_1.4.2.tar.gz'
> Content type 'application/x-gzip' length 228815 bytes (223 KB)
> ==================================================
> downloaded 223 KB
>
> trying URL
> 'https://cloud.r-project.org/src/contrib/pkgconfig_2.0.2.tar.gz'
> Content type 'application/x-gzip' length 6024 bytes
> ==================================================
> downloaded 6024 bytes
>
> trying URL
> 'https://cloud.r-project.org/src/contrib/colorspace_1.4-1.tar.gz'
> Content type 'application/x-gzip' length 2152594 bytes (2.1 MB)
> ==================================================
> downloaded 2.1 MB
>
> trying URL
> 'https://cloud.r-project.org/src/contrib/evaluate_0.14.tar.gz'
> Content type 'application/x-gzip' length 24206 bytes (23 KB)
> ==================================================
> downloaded 23 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/highr_0.8.tar.gz'
> Content type 'application/x-gzip' length 17445 bytes (17 KB)
> ==================================================
> downloaded 17 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/markdown_1.0.tar.gz'
> Content type 'application/x-gzip' length 80843 bytes (78 KB)
> ==================================================
> downloaded 78 KB
>
> trying URL
> 'https://cloud.r-project.org/src/contrib/stringr_1.4.0.tar.gz'
> Content type 'application/x-gzip' length 135777 bytes (132 KB)
> ==================================================
> downloaded 132 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/yaml_2.2.0.tar.gz'
> Content type 'application/x-gzip' length 89447 bytes (87 KB)
> ==================================================
> downloaded 87 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/xfun_0.8.tar.gz'
> Content type 'application/x-gzip' length 54885 bytes (53 KB)
> ==================================================
> downloaded 53 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/Rcpp_1.0.2.tar.gz'
> Content type 'application/x-gzip' length 3699570 bytes (3.5 MB)
> ==================================================
> downloaded 3.5 MB
>
> trying URL 'https://cloud.r-project.org/src/contrib/cli_1.1.0.tar.gz'
> Content type 'application/x-gzip' length 40232 bytes (39 KB)
> ==================================================
> downloaded 39 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/crayon_1.3.4.tar.gz'
> Content type 'application/x-gzip' length 658694 bytes (643 KB)
> ==================================================
> downloaded 643 KB
>
> trying URL
> 'https://cloud.r-project.org/src/contrib/digest_0.6.20.tar.gz'
> Content type 'application/x-gzip' length 135997 bytes (132 KB)
> ==================================================
> downloaded 132 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/magrittr_1.5.tar.gz'
> Content type 'application/x-gzip' length 200504 bytes (195 KB)
> ==================================================
> downloaded 195 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/praise_1.0.0.tar.gz'
> Content type 'application/x-gzip' length 6100 bytes
> ==================================================
> downloaded 6100 bytes
>
> trying URL 'https://cloud.r-project.org/src/contrib/rlang_0.4.0.tar.gz'
> Content type 'application/x-gzip' length 859737 bytes (839 KB)
> ==================================================
> downloaded 839 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/withr_2.1.2.tar.gz'
> Content type 'application/x-gzip' length 53578 bytes (52 KB)
> ==================================================
> downloaded 52 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/gtable_0.3.0.tar.gz'
> Content type 'application/x-gzip' length 368081 bytes (359 KB)
> ==================================================
> downloaded 359 KB
>
> trying URL
> 'https://cloud.r-project.org/src/contrib/lazyeval_0.2.2.tar.gz'
> Content type 'application/x-gzip' length 83482 bytes (81 KB)
> ==================================================
> downloaded 81 KB
>
> trying URL
> 'https://cloud.r-project.org/src/contrib/reshape2_1.4.3.tar.gz'
> Content type 'application/x-gzip' length 36405 bytes (35 KB)
> ==================================================
> downloaded 35 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/tibble_2.1.3.tar.gz'
> Content type 'application/x-gzip' length 310774 bytes (303 KB)
> ==================================================
> downloaded 303 KB
>
> trying URL
> 'https://cloud.r-project.org/src/contrib/viridisLite_0.3.0.tar.gz'
> Content type 'application/x-gzip' length 44019 bytes (42 KB)
> ==================================================
> downloaded 42 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/labeling_0.3.tar.gz'
> Content type 'application/x-gzip' length 10722 bytes (10 KB)
> ==================================================
> downloaded 10 KB
>
> trying URL
> 'https://cloud.r-project.org/src/contrib/munsell_0.5.0.tar.gz'
> Content type 'application/x-gzip' length 182653 bytes (178 KB)
> ==================================================
> downloaded 178 KB
>
> trying URL
> 'https://cloud.r-project.org/src/contrib/RColorBrewer_1.1-2.tar.gz'
> Content type 'application/x-gzip' length 11532 bytes (11 KB)
> ==================================================
> downloaded 11 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/knitr_1.23.tar.gz'
> Content type 'application/x-gzip' length 880134 bytes (859 KB)
> ==================================================
> downloaded 859 KB
>
> trying URL
> 'https://cloud.r-project.org/src/contrib/microbenchmark_1.4-6.tar.gz'
> Content type 'application/x-gzip' length 57682 bytes (56 KB)
> ==================================================
> downloaded 56 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/pryr_0.1.4.tar.gz'
> Content type 'application/x-gzip' length 43041 bytes (42 KB)
> ==================================================
> downloaded 42 KB
>
> trying URL
> 'https://cloud.r-project.org/src/contrib/testthat_2.2.1.tar.gz'
> Content type 'application/x-gzip' length 235874 bytes (230 KB)
> ==================================================
> downloaded 230 KB
>
> trying URL
> 'https://cloud.r-project.org/src/contrib/ggplot2_3.2.0.tar.gz'
> Content type 'application/x-gzip' length 3193995 bytes (3.0 MB)
> ==================================================
> downloaded 3.0 MB
>
> trying URL 'https://cloud.r-project.org/src/contrib/scales_1.0.0.tar.gz'
> Content type 'application/x-gzip' length 299262 bytes (292 KB)
> ==================================================
> downloaded 292 KB
>
> trying URL 'https://cloud.r-project.org/src/contrib/R6_2.4.0.tar.gz'
> Content type 'application/x-gzip' length 31545 bytes (30 KB)
> ==================================================
> downloaded 30 KB
>
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
> Error in library(R6) : there is no package called ?R6?
> Calls: source -> withVisible -> eval -> eval -> library
> Execution halted
>
> The downloaded source packages are in
> ??? ?/tmp/RtmpMQfLun/downloaded_packages?
> There were 43 warnings (use warnings() to see them)
>
>
>
>
>


From m@ng|nduvho @end|ng |rom gm@||@com  Tue Aug  6 14:07:23 2019
From: m@ng|nduvho @end|ng |rom gm@||@com (Mangalani Makananisa)
Date: Tue, 6 Aug 2019 05:07:23 -0700
Subject: [R] (no subject)
Message-ID: <CADCVYB0QDOPEEmP14+Bcp+iEgQx85wawkcdOt=KFxVRsrCwAqA@mail.gmail.com>

Dear all,

I have been reading some documentations including Latent variable Modeling
using R and I am confronted with some challenges.  Could you please
direction/guidance me with the following problem?

If I am given continuous time series ?quarterly data? with three indicators
(Y1, Y2 and Y3) and 5 causal variables (X1, X2, . . ., X5) and assuming
one latent variable (F1). The observed variables are stationary at first
difference I(1).

How do i specify the model in lavaan() given that this is a time series
 quarterly data with  continuous variables only. I would like to see a
draft model specification in lavaan.

 Looking forward to hearing from you soon and I will give feedback.

Kind regards,

Peter

	[[alternative HTML version deleted]]


From m@ng|nduvho @end|ng |rom gm@||@com  Tue Aug  6 14:54:12 2019
From: m@ng|nduvho @end|ng |rom gm@||@com (Mangalani Makananisa)
Date: Tue, 6 Aug 2019 05:54:12 -0700
Subject: [R] Time Series Latent Variable modeling in lavaan
Message-ID: <CADCVYB3Vx+tR6nhRybwP0yrNJ0cULZxsdGeODAaC+H=6ApvyFg@mail.gmail.com>

Dear all,

I have been reading some documentations including Latent variable Modeling
using R and I am confronted with some challenges.  Could you please
direction/guidance me with the following problem?

If I am given continuous time series ?quarterly data? with three indicators
(Y1, Y2 and Y3) and 5 causal variables (X1, X2, . . ., X5) and assuming
one latent variable (F1). The observed variables are stationary at first
difference I(1).

How do i specify the model in lavaan() given that this is a time series
 quarterly data with  continuous variables only. I would like to see a
draft model specification in lavaan.

 Looking forward to hearing from you soon and I will give feedback.

Kind regards,

Peter

	[[alternative HTML version deleted]]


From @|gbert @end|ng |rom w|w|@hu-ber||n@de  Tue Aug  6 09:52:20 2019
From: @|gbert @end|ng |rom w|w|@hu-ber||n@de (Sigbert Klinke)
Date: Tue, 6 Aug 2019 09:52:20 +0200
Subject: [R] shinyWidgets::sliderTextInput
Message-ID: <1f85aa25-9914-954b-7695-6c68edcbfd85@wiwi.hu-berlin.de>

Hi,

I'am using

sliderTextInput('myinput', choices=list("choice1"=1, "choice2"=2, 
"choice3"=3))

But in the shiny app the UI element shows '1', '2', instead of 
'choice1', 'choice2' etc.

I think the reason is that in shinyTextInput is done

     if (!is.character(choices)) {
         choices <- as.character(choices)
     }

Any idea if I could achieve my wished behaviour?

Best Sigbert

-- 
https://hu.berlin/sk
https://hu.berlin/mmstat3


From to|u|ope@de@gbo @end|ng |rom gm@||@com  Tue Aug  6 10:53:09 2019
From: to|u|ope@de@gbo @end|ng |rom gm@||@com (Tolulope Adeagbo)
Date: Tue, 6 Aug 2019 09:53:09 +0100
Subject: [R] Loop Repetition
Message-ID: <CAL+C-=6+kOKTykXPF9egBozv9=+-yw7jBu+c2kpAxBZqOsq9ag@mail.gmail.com>

Hey guys,

I'm trying to write a loop that will repeat an action for a stipulated
number of times. I have written some code but i think i'm missing something.

for (x in 5) {

  repeat{

    x = runif(1:4, min = 0, max = 1)


    print(x)


    if (x== var_1[5]){

      print("done")
    }

    print(x)}
}

The goal is to generate the random number 5 times.

Please help....

	[[alternative HTML version deleted]]


From rn|@boh @end|ng |rom gm@||@com  Tue Aug  6 11:05:53 2019
From: rn|@boh @end|ng |rom gm@||@com (Bob O'Hara)
Date: Tue, 6 Aug 2019 11:05:53 +0200
Subject: [R] Loop Repetition
In-Reply-To: <CAL+C-=6+kOKTykXPF9egBozv9=+-yw7jBu+c2kpAxBZqOsq9ag@mail.gmail.com>
References: <CAL+C-=6+kOKTykXPF9egBozv9=+-yw7jBu+c2kpAxBZqOsq9ag@mail.gmail.com>
Message-ID: <CAN-Z0xVorPooUX9P_=DGij=bJ2B202JCvvTATzpZLEu+NZbk2A@mail.gmail.com>

Is there anything wrong with just doing this?

x <- runif(5, min = 0, max = 1)

Also note that you use x to be at last 2 things: in

for (x in 5) {

you set it to 5, and then in the loop you

x = runif(1:4, min = 0, max = 1)

you make it a vector of length 4.

You also fail to use break to stop the repeat (something I never knew
existed in R!).

Bob

On Tue, 6 Aug 2019 at 10:54, Tolulope Adeagbo <tolulopeadeagbo at gmail.com> wrote:
>
> Hey guys,
>
> I'm trying to write a loop that will repeat an action for a stipulated
> number of times. I have written some code but i think i'm missing something.
>
> for (x in 5) {
>
>   repeat{
>
>     x = runif(1:4, min = 0, max = 1)
>
>
>     print(x)
>
>
>     if (x== var_1[5]){
>
>       print("done")
>     }
>
>     print(x)}
> }
>
> The goal is to generate the random number 5 times.
>
> Please help....
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Bob O'Hara
Institutt for matematiske fag
NTNU
7491 Trondheim
Norway

Mobile: +47 915 54 416
Journal of Negative Results - EEB: www.jnr-eeb.org


From drj|m|emon @end|ng |rom gm@||@com  Tue Aug  6 11:08:51 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Tue, 6 Aug 2019 19:08:51 +1000
Subject: [R] Loop Repetition
In-Reply-To: <CAL+C-=6+kOKTykXPF9egBozv9=+-yw7jBu+c2kpAxBZqOsq9ag@mail.gmail.com>
References: <CAL+C-=6+kOKTykXPF9egBozv9=+-yw7jBu+c2kpAxBZqOsq9ag@mail.gmail.com>
Message-ID: <CA+8X3fXqo3nQn0td_0sHOX+vDk1h3_QgvDQh5uNe-bEo9Tn-nw@mail.gmail.com>

Hi Tolulope,
The "in" operator steps through each element of the vector on the
right. You only have one element. Therefore you probably want:

for(x in 1:5)
...

Jim

Jim

On Tue, Aug 6, 2019 at 6:54 PM Tolulope Adeagbo
<tolulopeadeagbo at gmail.com> wrote:
>
> Hey guys,
>
> I'm trying to write a loop that will repeat an action for a stipulated
> number of times. I have written some code but i think i'm missing something.
>
> for (x in 5) {
>
>   repeat{
>
>     x = runif(1:4, min = 0, max = 1)
>
>
>     print(x)
>
>
>     if (x== var_1[5]){
>
>       print("done")
>     }
>
>     print(x)}
> }
>
> The goal is to generate the random number 5 times.
>
> Please help....
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From to|u|ope@de@gbo @end|ng |rom gm@||@com  Tue Aug  6 11:21:36 2019
From: to|u|ope@de@gbo @end|ng |rom gm@||@com (Tolulope Adeagbo)
Date: Tue, 6 Aug 2019 10:21:36 +0100
Subject: [R] Loop Repetition
In-Reply-To: <CA+8X3fXqo3nQn0td_0sHOX+vDk1h3_QgvDQh5uNe-bEo9Tn-nw@mail.gmail.com>
References: <CAL+C-=6+kOKTykXPF9egBozv9=+-yw7jBu+c2kpAxBZqOsq9ag@mail.gmail.com>
 <CA+8X3fXqo3nQn0td_0sHOX+vDk1h3_QgvDQh5uNe-bEo9Tn-nw@mail.gmail.com>
Message-ID: <CAL+C-=5REVYcNx+7oyvREWPKxj8+ir4aqZpdbn3Rd=D4+UQ1ew@mail.gmail.com>

Thanks guys, I've tried all you're suggesting,  both for (x in 1:5) and
break, but I cant seem to ascertain when the loop has generated a vector of
4 random numbers  5 times.

On Tue, 6 Aug 2019, 10:09 Jim Lemon, <drjimlemon at gmail.com> wrote:

> Hi Tolulope,
> The "in" operator steps through each element of the vector on the
> right. You only have one element. Therefore you probably want:
>
> for(x in 1:5)
> ...
>
> Jim
>
> Jim
>
> On Tue, Aug 6, 2019 at 6:54 PM Tolulope Adeagbo
> <tolulopeadeagbo at gmail.com> wrote:
> >
> > Hey guys,
> >
> > I'm trying to write a loop that will repeat an action for a stipulated
> > number of times. I have written some code but i think i'm missing
> something.
> >
> > for (x in 5) {
> >
> >   repeat{
> >
> >     x = runif(1:4, min = 0, max = 1)
> >
> >
> >     print(x)
> >
> >
> >     if (x== var_1[5]){
> >
> >       print("done")
> >     }
> >
> >     print(x)}
> > }
> >
> > The goal is to generate the random number 5 times.
> >
> > Please help....
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From rn|@boh @end|ng |rom gm@||@com  Tue Aug  6 11:45:47 2019
From: rn|@boh @end|ng |rom gm@||@com (Bob O'Hara)
Date: Tue, 6 Aug 2019 11:45:47 +0200
Subject: [R] Loop Repetition
In-Reply-To: <CAL+C-=5REVYcNx+7oyvREWPKxj8+ir4aqZpdbn3Rd=D4+UQ1ew@mail.gmail.com>
References: <CAL+C-=6+kOKTykXPF9egBozv9=+-yw7jBu+c2kpAxBZqOsq9ag@mail.gmail.com>
 <CA+8X3fXqo3nQn0td_0sHOX+vDk1h3_QgvDQh5uNe-bEo9Tn-nw@mail.gmail.com>
 <CAL+C-=5REVYcNx+7oyvREWPKxj8+ir4aqZpdbn3Rd=D4+UQ1ew@mail.gmail.com>
Message-ID: <CAN-Z0xVKEf8g0Koru_Zx3WpeFN4tK8fmQQ7-bz3R-FUu0Go2bQ@mail.gmail.com>

For a start, try this:

for(i in 1:5) {
  x <- runif(4,0,1)
}

Which will do what you want, but will over-write x each time (so isn't
very good). Better (if you want to use the random numbers outside the
loop) is this:

x <- matrix(NA, nrow=5, ncol=4)
for(i in 1:5) {
  x[i,] <- runif(4,0,1)
}

But better than this is not to use a loop at all, but to use R's vectorisation:

x <- matrix(runif(20,0,1), nrow=5, ncol=4)

i.e. generate a vector of random numbers (the runif()), and then put
that into a matrix (the matrix()).

Oh, and you could also do this:
replicate(5, runif(4, 0,1))
which is slightly odd here, but if you want to use the random numbers
to do something, you can do all of it in a function, e.g.

CalcMean <- function(n=4) {
  x <- runif(n, 0, 1)
  mean(x)
}
replicate(5, CalcMean(n=4))

Using a function makes code writing a lot easier, as you can write and
debug the function on its own, and then use replicate() to run the
loop (there are also functions like vapply() and apply() if you need
to pass different arguments into the function for different
iterations).

Bob




On Tue, 6 Aug 2019 at 11:28, Tolulope Adeagbo <tolulopeadeagbo at gmail.com> wrote:
>
> Thanks guys, I've tried all you're suggesting,  both for (x in 1:5) and
> break, but I cant seem to ascertain when the loop has generated a vector of
> 4 random numbers  5 times.
>
> On Tue, 6 Aug 2019, 10:09 Jim Lemon, <drjimlemon at gmail.com> wrote:
>
> > Hi Tolulope,
> > The "in" operator steps through each element of the vector on the
> > right. You only have one element. Therefore you probably want:
> >
> > for(x in 1:5)
> > ...
> >
> > Jim
> >
> > Jim
> >
> > On Tue, Aug 6, 2019 at 6:54 PM Tolulope Adeagbo
> > <tolulopeadeagbo at gmail.com> wrote:
> > >
> > > Hey guys,
> > >
> > > I'm trying to write a loop that will repeat an action for a stipulated
> > > number of times. I have written some code but i think i'm missing
> > something.
> > >
> > > for (x in 5) {
> > >
> > >   repeat{
> > >
> > >     x = runif(1:4, min = 0, max = 1)
> > >
> > >
> > >     print(x)
> > >
> > >
> > >     if (x== var_1[5]){
> > >
> > >       print("done")
> > >     }
> > >
> > >     print(x)}
> > > }
> > >
> > > The goal is to generate the random number 5 times.
> > >
> > > Please help....
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Bob O'Hara
Institutt for matematiske fag
NTNU
7491 Trondheim
Norway

Mobile: +47 915 54 416
Journal of Negative Results - EEB: www.jnr-eeb.org


From to|u|ope@de@gbo @end|ng |rom gm@||@com  Tue Aug  6 11:50:20 2019
From: to|u|ope@de@gbo @end|ng |rom gm@||@com (Tolulope Adeagbo)
Date: Tue, 6 Aug 2019 10:50:20 +0100
Subject: [R] Loop Repetition
In-Reply-To: <CAN-Z0xVKEf8g0Koru_Zx3WpeFN4tK8fmQQ7-bz3R-FUu0Go2bQ@mail.gmail.com>
References: <CAL+C-=6+kOKTykXPF9egBozv9=+-yw7jBu+c2kpAxBZqOsq9ag@mail.gmail.com>
 <CA+8X3fXqo3nQn0td_0sHOX+vDk1h3_QgvDQh5uNe-bEo9Tn-nw@mail.gmail.com>
 <CAL+C-=5REVYcNx+7oyvREWPKxj8+ir4aqZpdbn3Rd=D4+UQ1ew@mail.gmail.com>
 <CAN-Z0xVKEf8g0Koru_Zx3WpeFN4tK8fmQQ7-bz3R-FUu0Go2bQ@mail.gmail.com>
Message-ID: <CAL+C-=6Aq2XW6G2-0EEMwrDm4wpcEZ0YSOWGtkBiRR2mVVmLhQ@mail.gmail.com>

Wow...Great one BOB...Gracias, Merci.

On Tue, 6 Aug 2019, 10:46 Bob O'Hara, <rni.boh at gmail.com> wrote:

> For a start, try this:
>
> for(i in 1:5) {
>   x <- runif(4,0,1)
> }
>
> Which will do what you want, but will over-write x each time (so isn't
> very good). Better (if you want to use the random numbers outside the
> loop) is this:
>
> x <- matrix(NA, nrow=5, ncol=4)
> for(i in 1:5) {
>   x[i,] <- runif(4,0,1)
> }
>
> But better than this is not to use a loop at all, but to use R's
> vectorisation:
>
> x <- matrix(runif(20,0,1), nrow=5, ncol=4)
>
> i.e. generate a vector of random numbers (the runif()), and then put
> that into a matrix (the matrix()).
>
> Oh, and you could also do this:
> replicate(5, runif(4, 0,1))
> which is slightly odd here, but if you want to use the random numbers
> to do something, you can do all of it in a function, e.g.
>
> CalcMean <- function(n=4) {
>   x <- runif(n, 0, 1)
>   mean(x)
> }
> replicate(5, CalcMean(n=4))
>
> Using a function makes code writing a lot easier, as you can write and
> debug the function on its own, and then use replicate() to run the
> loop (there are also functions like vapply() and apply() if you need
> to pass different arguments into the function for different
> iterations).
>
> Bob
>
>
>
>
> On Tue, 6 Aug 2019 at 11:28, Tolulope Adeagbo <tolulopeadeagbo at gmail.com>
> wrote:
> >
> > Thanks guys, I've tried all you're suggesting,  both for (x in 1:5) and
> > break, but I cant seem to ascertain when the loop has generated a vector
> of
> > 4 random numbers  5 times.
> >
> > On Tue, 6 Aug 2019, 10:09 Jim Lemon, <drjimlemon at gmail.com> wrote:
> >
> > > Hi Tolulope,
> > > The "in" operator steps through each element of the vector on the
> > > right. You only have one element. Therefore you probably want:
> > >
> > > for(x in 1:5)
> > > ...
> > >
> > > Jim
> > >
> > > Jim
> > >
> > > On Tue, Aug 6, 2019 at 6:54 PM Tolulope Adeagbo
> > > <tolulopeadeagbo at gmail.com> wrote:
> > > >
> > > > Hey guys,
> > > >
> > > > I'm trying to write a loop that will repeat an action for a
> stipulated
> > > > number of times. I have written some code but i think i'm missing
> > > something.
> > > >
> > > > for (x in 5) {
> > > >
> > > >   repeat{
> > > >
> > > >     x = runif(1:4, min = 0, max = 1)
> > > >
> > > >
> > > >     print(x)
> > > >
> > > >
> > > >     if (x== var_1[5]){
> > > >
> > > >       print("done")
> > > >     }
> > > >
> > > >     print(x)}
> > > > }
> > > >
> > > > The goal is to generate the random number 5 times.
> > > >
> > > > Please help....
> > > >
> > > >         [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Bob O'Hara
> Institutt for matematiske fag
> NTNU
> 7491 Trondheim
> Norway
>
> Mobile: +47 915 54 416
> Journal of Negative Results - EEB: www.jnr-eeb.org
>

	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Tue Aug  6 11:53:22 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Tue, 6 Aug 2019 12:53:22 +0300
Subject: [R] shinyWidgets::sliderTextInput
In-Reply-To: <1f85aa25-9914-954b-7695-6c68edcbfd85@wiwi.hu-berlin.de>
References: <1f85aa25-9914-954b-7695-6c68edcbfd85@wiwi.hu-berlin.de>
Message-ID: <CAGgJW7665y2NNzpf1UhZonTf+d9QgApD4_1BE+LpZvgo7LiJJw@mail.gmail.com>

Hi Sigbert,

Just leave out the '=...', as in

sliderTextInput('myinput', choices=list("choice1", "choice2", "choice3") )

HTH,
Eric



On Tue, Aug 6, 2019 at 10:52 AM Sigbert Klinke <sigbert at wiwi.hu-berlin.de>
wrote:

> Hi,
>
> I'am using
>
> sliderTextInput('myinput', choices=list("choice1"=1, "choice2"=2,
> "choice3"=3))
>
> But in the shiny app the UI element shows '1', '2', instead of
> 'choice1', 'choice2' etc.
>
> I think the reason is that in shinyTextInput is done
>
>      if (!is.character(choices)) {
>          choices <- as.character(choices)
>      }
>
> Any idea if I could achieve my wished behaviour?
>
> Best Sigbert
>
> --
> https://hu.berlin/sk
> https://hu.berlin/mmstat3
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Tue Aug  6 13:12:09 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Tue, 6 Aug 2019 12:12:09 +0100
Subject: [R] Plotting hclust() results
In-Reply-To: <000d01d54bfe$43178950$c9469bf0$@sbcglobal.net>
References: <000d01d54bfe$43178950$c9469bf0$@sbcglobal.net>
Message-ID: <dccb1aac-e26b-133f-c4d0-a42c6e087007@sapo.pt>

Hello,

I don't know if you want something like this:

hc_long <- reshape::melt(hc, id.vars = "Cluster") # convert to long format

library(ggplot2)

ggplot(hc_long, aes(x = Cluster, y = value, colour = variable)) +
   geom_point() +
   geom_line() +
   coord_flip() +
   facet_wrap(~ variable)

#Data

hc <- read.table(text = "
   Cluster     A     B     C     D     E     F
                  1       1  0.67  0.56  0.54  0.03 -0.97 -1.09
                  2       2  0.52  0.43  0.51  0.05 -0.86 -0.96
                  3       3  0.2   0.3   0.19  0.08  0.02  0.09
                  4       4 -1.3  -1.13 -1.78 -0.05 -0.23 -0.23
                  5       5 -1.6  -1.3  -1.34 -0.23 -0.45 -0.45

", header = TRUE)


Hope this helps,

Rui Barradas

?s 03:25 de 06/08/19, reichmanj at sbcglobal.net escreveu:
> R Help Forum
> 
> I have output from hierarchal clustering and want to plot the results using
> the ggplot2 function. Where I have a scatter plot with 5 lines representing
> the 5 clusters. I assuming I need to transform the data into three columns
> (like) cluster, variable, and value. Not an issue but was wondering if I
> could plot the data as is.
> 
>    Cluster     A     B     C     D     E     F
>      <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
> 1       1  0.67  0.56  0.54  0.03 -0.97 -1.09
> 2       2  0.52  0.43  0.51  0.05 -0.86 -0.96
> 3       3  0.2   0.3   0.19  0.08  0.02  0.09
> 4       4 -1.3  -1.13 -1.78 -0.05 -0.23 -0.23
> 5       5 -1.6  -1.3  -1.34 -0.23 -0.45 -0.45
> 
> Something like
> 
> Cluster Var	Value
> 1	A	0.67
> 1	B	0.56 etc
> 
>   Then group by cluster??
> 
> Jeff
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From SW@y @end|ng |rom meco@com  Tue Aug  6 14:33:50 2019
From: SW@y @end|ng |rom meco@com (Shawn Way)
Date: Tue, 6 Aug 2019 12:33:50 +0000
Subject: [R] Using Partial Column Matching for Mutate
Message-ID: <a880376694ed48c585bed823d4acadd9@CTC-HOU-EXMB-02.ctcloud.local>

I have a tibble that has a large number of variables and because I'm partial lazy (and I really want to know how to do this), I would like find out if it possible to partial column matching with the mutate function in the tidyverse.

I have a tibble with the following

> gross_test  <- gross_df %>%
+     select(Job,Mfg_Labor_Hrs_Planned,Mfg_Labor_Hrs_Actual,Eng_Labor_Hrs_Planned,Eng_Labor_Hrs_Actual)
> gross_test
# A tibble: 6 x 5
    Job Mfg_Labor_Hrs_Pla~ Mfg_Labor_Hrs_Ac~ Eng_Labor_Hrs_Pl~ Eng_Labor_Hrs_Ac~
* <dbl>              <dbl>             <dbl>             <dbl>             <dbl>
1  9892               950.             1082.               133              302.
2  9893               950.             1082.               133              302.
3  9652               950.             1082.               133              302.
4  9894               950.             1082.               133              302.
5  9652               950.             1082.               133              302.
6  9894               950.             1082.               133              302.
>

The column names follow a pattern of (.+)_Planned and (.+)_Actual and there are a large number of them.  What I would like to do is the following:

mutate(Mfg_Labor_Hrs_Diff= Mfg_Labor_Hrs_Actual- Mfg_Labor_Hrs_Planned)

using a regex.  Something like the following

mutate( $1_Diff = $1_Actual - $1_Planned)

Where $1 was Mfg_Labor_Hrs.  This would iterate over the entire set of columns doing Mfg_Labor_Hrs and then Eng_Labor_Hrs, etc.

Is this even possible or will I need to explicitly write out the mutate for each combination?

Thanks for looking at this.

Shawn Way, P.E.

???


From er|cjberger @end|ng |rom gm@||@com  Tue Aug  6 15:29:48 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Tue, 6 Aug 2019 16:29:48 +0300
Subject: [R] Using Partial Column Matching for Mutate
In-Reply-To: <a880376694ed48c585bed823d4acadd9@CTC-HOU-EXMB-02.ctcloud.local>
References: <a880376694ed48c585bed823d4acadd9@CTC-HOU-EXMB-02.ctcloud.local>
Message-ID: <CAGgJW74p-PqDVtSUdNqgBrt+8-uyxQjgBaOzeocTRD-61KjFyg@mail.gmail.com>

Hi Shawn,
Here is a solution using base R (no dplyr). The only regex appears in the
statement to get the common prefixes.

colsPrefixes <- sub("_Planned$","",colnames(gross_test)[
grep("_Planned$",colnames(gross_test))])
f <- function(s) {
  gross_test[,paste(s,"Diff",sep="_")] <<-
gross_test[,paste(s,"Actual",sep="_")] -
gross_test[,paste(s,"Planned",sep="_")]
}
tmpOut <- sapply(colsPrefixes,f)

HTH,
Eric


On Tue, Aug 6, 2019 at 3:34 PM Shawn Way <SWay at meco.com> wrote:

> I have a tibble that has a large number of variables and because I'm
> partial lazy (and I really want to know how to do this), I would like find
> out if it possible to partial column matching with the mutate function in
> the tidyverse.
>
> I have a tibble with the following
>
> > gross_test  <- gross_df %>%
> +
>  select(Job,Mfg_Labor_Hrs_Planned,Mfg_Labor_Hrs_Actual,Eng_Labor_Hrs_Planned,Eng_Labor_Hrs_Actual)
> > gross_test
> # A tibble: 6 x 5
>     Job Mfg_Labor_Hrs_Pla~ Mfg_Labor_Hrs_Ac~ Eng_Labor_Hrs_Pl~
> Eng_Labor_Hrs_Ac~
> * <dbl>              <dbl>             <dbl>             <dbl>
>  <dbl>
> 1  9892               950.             1082.               133
>   302.
> 2  9893               950.             1082.               133
>   302.
> 3  9652               950.             1082.               133
>   302.
> 4  9894               950.             1082.               133
>   302.
> 5  9652               950.             1082.               133
>   302.
> 6  9894               950.             1082.               133
>   302.
> >
>
> The column names follow a pattern of (.+)_Planned and (.+)_Actual and
> there are a large number of them.  What I would like to do is the following:
>
> mutate(Mfg_Labor_Hrs_Diff= Mfg_Labor_Hrs_Actual- Mfg_Labor_Hrs_Planned)
>
> using a regex.  Something like the following
>
> mutate( $1_Diff = $1_Actual - $1_Planned)
>
> Where $1 was Mfg_Labor_Hrs.  This would iterate over the entire set of
> columns doing Mfg_Labor_Hrs and then Eng_Labor_Hrs, etc.
>
> Is this even possible or will I need to explicitly write out the mutate
> for each combination?
>
> Thanks for looking at this.
>
> Shawn Way, P.E.
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From SW@y @end|ng |rom meco@com  Tue Aug  6 16:17:01 2019
From: SW@y @end|ng |rom meco@com (Shawn Way)
Date: Tue, 6 Aug 2019 14:17:01 +0000
Subject: [R] Using Partial Column Matching for Mutate
In-Reply-To: <CAGgJW74p-PqDVtSUdNqgBrt+8-uyxQjgBaOzeocTRD-61KjFyg@mail.gmail.com>
References: <a880376694ed48c585bed823d4acadd9@CTC-HOU-EXMB-02.ctcloud.local>
 <CAGgJW74p-PqDVtSUdNqgBrt+8-uyxQjgBaOzeocTRD-61KjFyg@mail.gmail.com>
Message-ID: <288c26b56a974e26a61670ed5340bde0@CTC-HOU-EXMB-02.ctcloud.local>

Frankly, this is great.  I don?t really care if it base or tidy, I just need it to work.

Thank you kindly!

Shawn Way, PE

From: Eric Berger <ericjberger at gmail.com>
Sent: Tuesday, August 06, 2019 8:30 AM
To: Shawn Way <SWay at meco.com>
Cc: r-help at r-project.org
Subject: Re: [R] Using Partial Column Matching for Mutate

** External Email **: This email originated from outside of the organization. Do not click links or open attachments unless you recognize the sender and know the content is safe.

Hi Shawn,
Here is a solution using base R (no dplyr). The only regex appears in the statement to get the common prefixes.

colsPrefixes <- sub("_Planned$","",colnames(gross_test)[ grep("_Planned$",colnames(gross_test))])
f <- function(s) {
  gross_test[,paste(s,"Diff",sep="_")] <<- gross_test[,paste(s,"Actual",sep="_")] - gross_test[,paste(s,"Planned",sep="_")]
}
tmpOut <- sapply(colsPrefixes,f)

HTH,
Eric


On Tue, Aug 6, 2019 at 3:34 PM Shawn Way <SWay at meco.com<mailto:SWay at meco.com>> wrote:
I have a tibble that has a large number of variables and because I'm partial lazy (and I really want to know how to do this), I would like find out if it possible to partial column matching with the mutate function in the tidyverse.

I have a tibble with the following

> gross_test  <- gross_df %>%
+     select(Job,Mfg_Labor_Hrs_Planned,Mfg_Labor_Hrs_Actual,Eng_Labor_Hrs_Planned,Eng_Labor_Hrs_Actual)
> gross_test
# A tibble: 6 x 5
    Job Mfg_Labor_Hrs_Pla~ Mfg_Labor_Hrs_Ac~ Eng_Labor_Hrs_Pl~ Eng_Labor_Hrs_Ac~
* <dbl>              <dbl>             <dbl>             <dbl>             <dbl>
1  9892               950.             1082.               133              302.
2  9893               950.             1082.               133              302.
3  9652               950.             1082.               133              302.
4  9894               950.             1082.               133              302.
5  9652               950.             1082.               133              302.
6  9894               950.             1082.               133              302.
>

The column names follow a pattern of (.+)_Planned and (.+)_Actual and there are a large number of them.  What I would like to do is the following:

mutate(Mfg_Labor_Hrs_Diff= Mfg_Labor_Hrs_Actual- Mfg_Labor_Hrs_Planned)

using a regex.  Something like the following

mutate( $1_Diff = $1_Actual - $1_Planned)

Where $1 was Mfg_Labor_Hrs.  This would iterate over the entire set of columns doing Mfg_Labor_Hrs and then Eng_Labor_Hrs, etc.

Is this even possible or will I need to explicitly write out the mutate for each combination?

Thanks for looking at this.

Shawn Way, P.E.



______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From er|nm@hodge@@ @end|ng |rom gm@||@com  Tue Aug  6 16:57:43 2019
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Tue, 6 Aug 2019 08:57:43 -0600
Subject: [R] shinyWidgets::sliderTextInput
In-Reply-To: <1f85aa25-9914-954b-7695-6c68edcbfd85@wiwi.hu-berlin.de>
References: <1f85aa25-9914-954b-7695-6c68edcbfd85@wiwi.hu-berlin.de>
Message-ID: <CACxE24kFV_bi2f0u9QyPo3sLCTM+qE2K8x7JaqHGNdWfk7YoCw@mail.gmail.com>

Hi Sigbert:

Is there a label argument, please? I think that might be it.

Thanks,
Erin

On Tue, Aug 6, 2019 at 1:52 AM Sigbert Klinke <sigbert at wiwi.hu-berlin.de>
wrote:

> Hi,
>
> I'am using
>
> sliderTextInput('myinput', choices=list("choice1"=1, "choice2"=2,
> "choice3"=3))
>
> But in the shiny app the UI element shows '1', '2', instead of
> 'choice1', 'choice2' etc.
>
> I think the reason is that in shinyTextInput is done
>
>      if (!is.character(choices)) {
>          choices <- as.character(choices)
>      }
>
> Any idea if I could achieve my wished behaviour?
>
> Best Sigbert
>
> --
> https://hu.berlin/sk
> https://hu.berlin/mmstat3
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
-- 
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Tue Aug  6 20:19:33 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Tue, 6 Aug 2019 13:19:33 -0500
Subject: [R] filter and add a column
Message-ID: <CAF9-5jPQee34Ads+Y2hfyKOAyJ8vvvjtcVMoB35XXD-kMm7gqA@mail.gmail.com>

Hello,

I am filtering my data frame "tot" via:

controls=tot %>% filter_all(any_vars(. %in% c("E109", "E119","E149"))) %>%
filter_all(any_vars(. %in% c("Caucasian"))) %>% filter_all(any_vars(. %in%
c("No kinship found","Ten or more third-degree relatives identified")))

> dim(controls)
[1] 15381  1093
> dim(tot)
[1] 502536   1093

how do I add in my data frame "tot" a new column called "controls" where
every of those filtered 15381 rows would have the value 1 and the rest
which can be found in tot have the value -9?

Thanks
Ana

	[[alternative HTML version deleted]]


From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Tue Aug  6 20:33:34 2019
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Tue, 6 Aug 2019 14:33:34 -0400
Subject: [R] filter and add a column
In-Reply-To: <CAF9-5jPQee34Ads+Y2hfyKOAyJ8vvvjtcVMoB35XXD-kMm7gqA@mail.gmail.com>
References: <CAF9-5jPQee34Ads+Y2hfyKOAyJ8vvvjtcVMoB35XXD-kMm7gqA@mail.gmail.com>
Message-ID: <CAJc=yOESAdib1y740Pv4GqpJhqSfO3-GwMbv6P5dO5_puu8=aQ@mail.gmail.com>

Do you have some kind of ID variable? If so, it should be
straightforward with the appropriate joining function.

What have you tried?

Also, please post in plain text.


On Tue, Aug 6, 2019 at 2:16 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Hello,
>
> I am filtering my data frame "tot" via:
>
> controls=tot %>% filter_all(any_vars(. %in% c("E109", "E119","E149"))) %>%
> filter_all(any_vars(. %in% c("Caucasian"))) %>% filter_all(any_vars(. %in%
> c("No kinship found","Ten or more third-degree relatives identified")))
>
> > dim(controls)
> [1] 15381  1093
> > dim(tot)
> [1] 502536   1093
>
> how do I add in my data frame "tot" a new column called "controls" where
> every of those filtered 15381 rows would have the value 1 and the rest
> which can be found in tot have the value -9?
>
> Thanks
> Ana
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Tue Aug  6 20:42:29 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Tue, 6 Aug 2019 13:42:29 -0500
Subject: [R] filter and add a column
In-Reply-To: <CAJc=yOESAdib1y740Pv4GqpJhqSfO3-GwMbv6P5dO5_puu8=aQ@mail.gmail.com>
References: <CAF9-5jPQee34Ads+Y2hfyKOAyJ8vvvjtcVMoB35XXD-kMm7gqA@mail.gmail.com>
 <CAJc=yOESAdib1y740Pv4GqpJhqSfO3-GwMbv6P5dO5_puu8=aQ@mail.gmail.com>
Message-ID: <CAF9-5jOjN5z87hsy898Bu9uJ=+sUnM+xx3Mb7EQEp3WGjTNd0g@mail.gmail.com>

Hi Patrick,

yes both controls and tot have "eid" column, please see attached

Can you please tell em what means to post in "plain text" ?

Thanks
Ana

On Tue, Aug 6, 2019 at 1:33 PM Patrick (Malone Quantitative) <
malone at malonequantitative.com> wrote:

> Do you have some kind of ID variable? If so, it should be
> straightforward with the appropriate joining function.
>
> What have you tried?
>
> Also, please post in plain text.
>
>
> On Tue, Aug 6, 2019 at 2:16 PM Ana Marija <sokovic.anamarija at gmail.com>
> wrote:
> >
> > Hello,
> >
> > I am filtering my data frame "tot" via:
> >
> > controls=tot %>% filter_all(any_vars(. %in% c("E109", "E119","E149")))
> %>%
> > filter_all(any_vars(. %in% c("Caucasian"))) %>% filter_all(any_vars(.
> %in%
> > c("No kinship found","Ten or more third-degree relatives identified")))
> >
> > > dim(controls)
> > [1] 15381  1093
> > > dim(tot)
> > [1] 502536   1093
> >
> > how do I add in my data frame "tot" a new column called "controls" where
> > every of those filtered 15381 rows would have the value 1 and the rest
> > which can be found in tot have the value -9?
> >
> > Thanks
> > Ana
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Tue Aug  6 20:42:04 2019
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Tue, 6 Aug 2019 14:42:04 -0400
Subject: [R] filter and add a column
In-Reply-To: <CAF9-5jOjN5z87hsy898Bu9uJ=+sUnM+xx3Mb7EQEp3WGjTNd0g@mail.gmail.com>
References: <CAF9-5jPQee34Ads+Y2hfyKOAyJ8vvvjtcVMoB35XXD-kMm7gqA@mail.gmail.com>
 <CAJc=yOESAdib1y740Pv4GqpJhqSfO3-GwMbv6P5dO5_puu8=aQ@mail.gmail.com>
 <CAF9-5jOjN5z87hsy898Bu9uJ=+sUnM+xx3Mb7EQEp3WGjTNd0g@mail.gmail.com>
Message-ID: <CAJc=yOG0hJUHLBfuZoJGmGC0pqgHTCmq0k6sW2JejJ09y4Oe-A@mail.gmail.com>

See the posting guide for this list about plain text.

Again, what have you tried? This is also mentioned in the posting guide.


On Tue, Aug 6, 2019 at 2:38 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Hi Patrick,
>
> yes both controls and tot have "eid" column, please see attached
>
> Can you please tell em what means to post in "plain text" ?
>
> Thanks
> Ana
>
> On Tue, Aug 6, 2019 at 1:33 PM Patrick (Malone Quantitative) <malone at malonequantitative.com> wrote:
>>
>> Do you have some kind of ID variable? If so, it should be
>> straightforward with the appropriate joining function.
>>
>> What have you tried?
>>
>> Also, please post in plain text.
>>
>>
>> On Tue, Aug 6, 2019 at 2:16 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>> >
>> > Hello,
>> >
>> > I am filtering my data frame "tot" via:
>> >
>> > controls=tot %>% filter_all(any_vars(. %in% c("E109", "E119","E149"))) %>%
>> > filter_all(any_vars(. %in% c("Caucasian"))) %>% filter_all(any_vars(. %in%
>> > c("No kinship found","Ten or more third-degree relatives identified")))
>> >
>> > > dim(controls)
>> > [1] 15381  1093
>> > > dim(tot)
>> > [1] 502536   1093
>> >
>> > how do I add in my data frame "tot" a new column called "controls" where
>> > every of those filtered 15381 rows would have the value 1 and the rest
>> > which can be found in tot have the value -9?
>> >
>> > Thanks
>> > Ana
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Tue Aug  6 21:05:19 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Tue, 6 Aug 2019 14:05:19 -0500
Subject: [R] filter and add a column
In-Reply-To: <CAJc=yOG0hJUHLBfuZoJGmGC0pqgHTCmq0k6sW2JejJ09y4Oe-A@mail.gmail.com>
References: <CAF9-5jPQee34Ads+Y2hfyKOAyJ8vvvjtcVMoB35XXD-kMm7gqA@mail.gmail.com>
 <CAJc=yOESAdib1y740Pv4GqpJhqSfO3-GwMbv6P5dO5_puu8=aQ@mail.gmail.com>
 <CAF9-5jOjN5z87hsy898Bu9uJ=+sUnM+xx3Mb7EQEp3WGjTNd0g@mail.gmail.com>
 <CAJc=yOG0hJUHLBfuZoJGmGC0pqgHTCmq0k6sW2JejJ09y4Oe-A@mail.gmail.com>
Message-ID: <CAF9-5jPVmfGN8uJBgV-XKDrDV_ci5+8JfOx7VpzmES+3gHsjiQ@mail.gmail.com>

I really don't know how I would implement this

On Tue, Aug 6, 2019 at 1:42 PM Patrick (Malone Quantitative) <
malone at malonequantitative.com> wrote:

> See the posting guide for this list about plain text.
>
> Again, what have you tried? This is also mentioned in the posting guide.
>
>
> On Tue, Aug 6, 2019 at 2:38 PM Ana Marija <sokovic.anamarija at gmail.com>
> wrote:
> >
> > Hi Patrick,
> >
> > yes both controls and tot have "eid" column, please see attached
> >
> > Can you please tell em what means to post in "plain text" ?
> >
> > Thanks
> > Ana
> >
> > On Tue, Aug 6, 2019 at 1:33 PM Patrick (Malone Quantitative) <
> malone at malonequantitative.com> wrote:
> >>
> >> Do you have some kind of ID variable? If so, it should be
> >> straightforward with the appropriate joining function.
> >>
> >> What have you tried?
> >>
> >> Also, please post in plain text.
> >>
> >>
> >> On Tue, Aug 6, 2019 at 2:16 PM Ana Marija <sokovic.anamarija at gmail.com>
> wrote:
> >> >
> >> > Hello,
> >> >
> >> > I am filtering my data frame "tot" via:
> >> >
> >> > controls=tot %>% filter_all(any_vars(. %in% c("E109",
> "E119","E149"))) %>%
> >> > filter_all(any_vars(. %in% c("Caucasian"))) %>% filter_all(any_vars(.
> %in%
> >> > c("No kinship found","Ten or more third-degree relatives
> identified")))
> >> >
> >> > > dim(controls)
> >> > [1] 15381  1093
> >> > > dim(tot)
> >> > [1] 502536   1093
> >> >
> >> > how do I add in my data frame "tot" a new column called "controls"
> where
> >> > every of those filtered 15381 rows would have the value 1 and the rest
> >> > which can be found in tot have the value -9?
> >> >
> >> > Thanks
> >> > Ana
> >> >
> >> >         [[alternative HTML version deleted]]
> >> >
> >> > ______________________________________________
> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Tue Aug  6 21:14:46 2019
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Tue, 6 Aug 2019 15:14:46 -0400
Subject: [R] filter and add a column
In-Reply-To: <CAF9-5jPVmfGN8uJBgV-XKDrDV_ci5+8JfOx7VpzmES+3gHsjiQ@mail.gmail.com>
References: <CAF9-5jPQee34Ads+Y2hfyKOAyJ8vvvjtcVMoB35XXD-kMm7gqA@mail.gmail.com>
 <CAJc=yOESAdib1y740Pv4GqpJhqSfO3-GwMbv6P5dO5_puu8=aQ@mail.gmail.com>
 <CAF9-5jOjN5z87hsy898Bu9uJ=+sUnM+xx3Mb7EQEp3WGjTNd0g@mail.gmail.com>
 <CAJc=yOG0hJUHLBfuZoJGmGC0pqgHTCmq0k6sW2JejJ09y4Oe-A@mail.gmail.com>
 <CAF9-5jPVmfGN8uJBgV-XKDrDV_ci5+8JfOx7VpzmES+3gHsjiQ@mail.gmail.com>
Message-ID: <CAJc=yOE093G5oS9H39mOYw5z1_y9SBP-5UmuKx8FnD_NFzq9ag@mail.gmail.com>

This should get you going:

Add a vector of 1s to controls before joining the datasets.

On Tue, Aug 6, 2019 at 3:01 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> I really don't know how I would implement this
>
> On Tue, Aug 6, 2019 at 1:42 PM Patrick (Malone Quantitative) <malone at malonequantitative.com> wrote:
>>
>> See the posting guide for this list about plain text.
>>
>> Again, what have you tried? This is also mentioned in the posting guide.
>>
>>
>> On Tue, Aug 6, 2019 at 2:38 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>> >
>> > Hi Patrick,
>> >
>> > yes both controls and tot have "eid" column, please see attached
>> >
>> > Can you please tell em what means to post in "plain text" ?
>> >
>> > Thanks
>> > Ana
>> >
>> > On Tue, Aug 6, 2019 at 1:33 PM Patrick (Malone Quantitative) <malone at malonequantitative.com> wrote:
>> >>
>> >> Do you have some kind of ID variable? If so, it should be
>> >> straightforward with the appropriate joining function.
>> >>
>> >> What have you tried?
>> >>
>> >> Also, please post in plain text.
>> >>
>> >>
>> >> On Tue, Aug 6, 2019 at 2:16 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>> >> >
>> >> > Hello,
>> >> >
>> >> > I am filtering my data frame "tot" via:
>> >> >
>> >> > controls=tot %>% filter_all(any_vars(. %in% c("E109", "E119","E149"))) %>%
>> >> > filter_all(any_vars(. %in% c("Caucasian"))) %>% filter_all(any_vars(. %in%
>> >> > c("No kinship found","Ten or more third-degree relatives identified")))
>> >> >
>> >> > > dim(controls)
>> >> > [1] 15381  1093
>> >> > > dim(tot)
>> >> > [1] 502536   1093
>> >> >
>> >> > how do I add in my data frame "tot" a new column called "controls" where
>> >> > every of those filtered 15381 rows would have the value 1 and the rest
>> >> > which can be found in tot have the value -9?
>> >> >
>> >> > Thanks
>> >> > Ana
>> >> >
>> >> >         [[alternative HTML version deleted]]
>> >> >
>> >> > ______________________________________________
>> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> > https://stat.ethz.ch/mailman/listinfo/r-help
>> >> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> >> > and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Tue Aug  6 21:39:27 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Tue, 6 Aug 2019 14:39:27 -0500
Subject: [R] filter and add a column
In-Reply-To: <CAJc=yOE093G5oS9H39mOYw5z1_y9SBP-5UmuKx8FnD_NFzq9ag@mail.gmail.com>
References: <CAF9-5jPQee34Ads+Y2hfyKOAyJ8vvvjtcVMoB35XXD-kMm7gqA@mail.gmail.com>
 <CAJc=yOESAdib1y740Pv4GqpJhqSfO3-GwMbv6P5dO5_puu8=aQ@mail.gmail.com>
 <CAF9-5jOjN5z87hsy898Bu9uJ=+sUnM+xx3Mb7EQEp3WGjTNd0g@mail.gmail.com>
 <CAJc=yOG0hJUHLBfuZoJGmGC0pqgHTCmq0k6sW2JejJ09y4Oe-A@mail.gmail.com>
 <CAF9-5jPVmfGN8uJBgV-XKDrDV_ci5+8JfOx7VpzmES+3gHsjiQ@mail.gmail.com>
 <CAJc=yOE093G5oS9H39mOYw5z1_y9SBP-5UmuKx8FnD_NFzq9ag@mail.gmail.com>
Message-ID: <CAF9-5jOjT6wJ-XZbcXCR6rgj7WT7OUNqxg06OsdaBbMu21VbHQ@mail.gmail.com>

I guess this is what you meant:

    controls$control <- rep(1,nrow(controls))
    t1=merge(tot,controls, by="eid", all = T)

On Tue, Aug 6, 2019 at 2:15 PM Patrick (Malone Quantitative) <
malone at malonequantitative.com> wrote:

> This should get you going:
>
> Add a vector of 1s to controls before joining the datasets.
>
> On Tue, Aug 6, 2019 at 3:01 PM Ana Marija <sokovic.anamarija at gmail.com>
> wrote:
> >
> > I really don't know how I would implement this
> >
> > On Tue, Aug 6, 2019 at 1:42 PM Patrick (Malone Quantitative) <
> malone at malonequantitative.com> wrote:
> >>
> >> See the posting guide for this list about plain text.
> >>
> >> Again, what have you tried? This is also mentioned in the posting guide.
> >>
> >>
> >> On Tue, Aug 6, 2019 at 2:38 PM Ana Marija <sokovic.anamarija at gmail.com>
> wrote:
> >> >
> >> > Hi Patrick,
> >> >
> >> > yes both controls and tot have "eid" column, please see attached
> >> >
> >> > Can you please tell em what means to post in "plain text" ?
> >> >
> >> > Thanks
> >> > Ana
> >> >
> >> > On Tue, Aug 6, 2019 at 1:33 PM Patrick (Malone Quantitative) <
> malone at malonequantitative.com> wrote:
> >> >>
> >> >> Do you have some kind of ID variable? If so, it should be
> >> >> straightforward with the appropriate joining function.
> >> >>
> >> >> What have you tried?
> >> >>
> >> >> Also, please post in plain text.
> >> >>
> >> >>
> >> >> On Tue, Aug 6, 2019 at 2:16 PM Ana Marija <
> sokovic.anamarija at gmail.com> wrote:
> >> >> >
> >> >> > Hello,
> >> >> >
> >> >> > I am filtering my data frame "tot" via:
> >> >> >
> >> >> > controls=tot %>% filter_all(any_vars(. %in% c("E109",
> "E119","E149"))) %>%
> >> >> > filter_all(any_vars(. %in% c("Caucasian"))) %>%
> filter_all(any_vars(. %in%
> >> >> > c("No kinship found","Ten or more third-degree relatives
> identified")))
> >> >> >
> >> >> > > dim(controls)
> >> >> > [1] 15381  1093
> >> >> > > dim(tot)
> >> >> > [1] 502536   1093
> >> >> >
> >> >> > how do I add in my data frame "tot" a new column called "controls"
> where
> >> >> > every of those filtered 15381 rows would have the value 1 and the
> rest
> >> >> > which can be found in tot have the value -9?
> >> >> >
> >> >> > Thanks
> >> >> > Ana
> >> >> >
> >> >> >         [[alternative HTML version deleted]]
> >> >> >
> >> >> > ______________________________________________
> >> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> >> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> >> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From kz @end|ng |rom bre@d||n@nce@com  Tue Aug  6 19:52:05 2019
From: kz @end|ng |rom bre@d||n@nce@com (KZ Win)
Date: Tue, 6 Aug 2019 13:52:05 -0400
Subject: [R] broken installation on ubuntu 16.04 for 3.4.4-1xenial0
Message-ID: <CANk4Xz9M60hvjbwUKh_13KDGc6POrkU4yB2xTJypx9uD6b3ECg@mail.gmail.com>

We have a test  system for boostrapping a production machine running R
code.  It spins up a new machine and tries to install this version whenever
there is a new commit to our infrastructure code repo.  This version has
been in place since Mar 2018 but a few weeks ago this test fails because it
can no longer install this package even though we have not made any changes
to this installation process.

The failure comes with this message

```
WARNING: The following packages cannot be authenticated!
r-base-core r-cran-boot r-cran-cluster r-cran-foreign r-cran-mass
r-cran-kernsmooth r-cran-lattice r-cran-nlme r-cran-matrix r-cran-mgcv
r-cran-rpart r-cran-survival r-cran-class r-cran-nnet r-cran-spatial
r-cran-codetools r-recommended r-base r-base-dev r-base-html r-doc-html
STDERR: E: There were unauthenticated packages and -y was used without
--allow-unauthenticated```

Any idea on whether there is a way to install `r-base` without using
`--allow-unauthenticated`

I can provide more information if necessary.

Thank you

	[[alternative HTML version deleted]]


From t@m@@@|erenc| @end|ng |rom med@t@t@hu  Tue Aug  6 21:49:48 2019
From: t@m@@@|erenc| @end|ng |rom med@t@t@hu (Ferenci Tamas)
Date: Tue, 6 Aug 2019 21:49:48 +0200
Subject: [R] conflicting results for a time-varying coefficient in a Cox
 model
Message-ID: <433649795.20190806214948@medstat.hu>

Dear All,

I was thinking of two possible ways to plot a time-varying coefficient
in a Cox model.

One is simply to use survival::plot.cox.zph which directly produces a
beta(t) vs t diagram.

The other is to transform the dataset to counting process format and
manually include an interaction with time, expanded with spline (to be
similar to plot.cox.zph). Plotting the coefficient produces the needed
beta(t) vs t diagram.

I understand that they're slightly different approaches, so I don't
expect totally identical results, but nevertheless, they approximate
the very same thing, so I do expect that the results are more or less
similar.


However:

library( survival )
library( splines )

data( veteran )

zp <- cox.zph( coxph(Surv(time, status) ~ trt + prior + karno,
                     data = veteran ), transform = "identity" )[ 3 ]

veteran3 <- survSplit( Surv(time, status) ~ trt + prior + karno,
                       data = veteran, cut = 1:max(veteran$time) )

fit <- coxph(Surv(tstart,time, status) ~ trt + prior + karno +
               karno:ns( time, df = 4 ), data = veteran3 )
cf <- coef( fit )
nsvet <- ns( veteran3$time, df = 4 )

plot( zp )
lines( 0:1000, ns( 0:1000, df = 4, knots = attr( nsvet, "knots" ),
                   Boundary.knots = attr( nsvet, "Boundary.knots" ) )%*%cf[
                     grep( "karno:ns", names( cf ) ) ] + cf["karno"],
       type = "l", col = "red" )

Where is the mistake? Something must be going on here, because the
plots are vastly different...

Thank you in advance,
Tamas


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Aug  7 03:13:46 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 06 Aug 2019 18:13:46 -0700
Subject: [R] broken installation on ubuntu 16.04 for 3.4.4-1xenial0
In-Reply-To: <CANk4Xz9M60hvjbwUKh_13KDGc6POrkU4yB2xTJypx9uD6b3ECg@mail.gmail.com>
References: <CANk4Xz9M60hvjbwUKh_13KDGc6POrkU4yB2xTJypx9uD6b3ECg@mail.gmail.com>
Message-ID: <70FE0468-DD2B-4DFC-A2A1-CA9CF0F5674D@dcn.davis.ca.us>

As the Posting Guide says... this is a plain text mailing list. You will help yourself if you pay attention when posting.

The standard instructions for Ubuntu [1] describe how to set up authentication.

[1] https://cran.r-project.org/bin/linux/ubuntu/README.html

On August 6, 2019 10:52:05 AM PDT, KZ Win <kz at breadfinance.com> wrote:
>We have a test  system for boostrapping a production machine running R
>code.  It spins up a new machine and tries to install this version
>whenever
>there is a new commit to our infrastructure code repo.  This version
>has
>been in place since Mar 2018 but a few weeks ago this test fails
>because it
>can no longer install this package even though we have not made any
>changes
>to this installation process.
>
>The failure comes with this message
>
>```
>WARNING: The following packages cannot be authenticated!
>r-base-core r-cran-boot r-cran-cluster r-cran-foreign r-cran-mass
>r-cran-kernsmooth r-cran-lattice r-cran-nlme r-cran-matrix r-cran-mgcv
>r-cran-rpart r-cran-survival r-cran-class r-cran-nnet r-cran-spatial
>r-cran-codetools r-recommended r-base r-base-dev r-base-html r-doc-html
>STDERR: E: There were unauthenticated packages and -y was used without
>--allow-unauthenticated```
>
>Any idea on whether there is a way to install `r-base` without using
>`--allow-unauthenticated`
>
>I can provide more information if necessary.
>
>Thank you
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From er|cjberger @end|ng |rom gm@||@com  Wed Aug  7 11:49:17 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Wed, 7 Aug 2019 12:49:17 +0300
Subject: [R] filter and add a column
In-Reply-To: <CAF9-5jOjT6wJ-XZbcXCR6rgj7WT7OUNqxg06OsdaBbMu21VbHQ@mail.gmail.com>
References: <CAF9-5jPQee34Ads+Y2hfyKOAyJ8vvvjtcVMoB35XXD-kMm7gqA@mail.gmail.com>
 <CAJc=yOESAdib1y740Pv4GqpJhqSfO3-GwMbv6P5dO5_puu8=aQ@mail.gmail.com>
 <CAF9-5jOjN5z87hsy898Bu9uJ=+sUnM+xx3Mb7EQEp3WGjTNd0g@mail.gmail.com>
 <CAJc=yOG0hJUHLBfuZoJGmGC0pqgHTCmq0k6sW2JejJ09y4Oe-A@mail.gmail.com>
 <CAF9-5jPVmfGN8uJBgV-XKDrDV_ci5+8JfOx7VpzmES+3gHsjiQ@mail.gmail.com>
 <CAJc=yOE093G5oS9H39mOYw5z1_y9SBP-5UmuKx8FnD_NFzq9ag@mail.gmail.com>
 <CAF9-5jOjT6wJ-XZbcXCR6rgj7WT7OUNqxg06OsdaBbMu21VbHQ@mail.gmail.com>
Message-ID: <CAGgJW75dDDRO_pWZJuKhdgEN35ySopPn-cSoVkU+MwCpsaBDxw@mail.gmail.com>

Here is a different approach that does not involve dplyr or the creation of
the intermediate data frame 'controls'

f <- function(v) {
  any(v %in% c("E109","E119","E149")) & any(v %in% c("Caucasian")) &
    any(v %in% c("No kinship found","Ten or more third-degree relatives
identified"))
}

tot$controls <- ifelse( apply( tot,1,f), 1, -9 )

HTH,
Eric



On Tue, Aug 6, 2019 at 10:36 PM Ana Marija <sokovic.anamarija at gmail.com>
wrote:

> I guess this is what you meant:
>
>     controls$control <- rep(1,nrow(controls))
>     t1=merge(tot,controls, by="eid", all = T)
>
> On Tue, Aug 6, 2019 at 2:15 PM Patrick (Malone Quantitative) <
> malone at malonequantitative.com> wrote:
>
> > This should get you going:
> >
> > Add a vector of 1s to controls before joining the datasets.
> >
> > On Tue, Aug 6, 2019 at 3:01 PM Ana Marija <sokovic.anamarija at gmail.com>
> > wrote:
> > >
> > > I really don't know how I would implement this
> > >
> > > On Tue, Aug 6, 2019 at 1:42 PM Patrick (Malone Quantitative) <
> > malone at malonequantitative.com> wrote:
> > >>
> > >> See the posting guide for this list about plain text.
> > >>
> > >> Again, what have you tried? This is also mentioned in the posting
> guide.
> > >>
> > >>
> > >> On Tue, Aug 6, 2019 at 2:38 PM Ana Marija <
> sokovic.anamarija at gmail.com>
> > wrote:
> > >> >
> > >> > Hi Patrick,
> > >> >
> > >> > yes both controls and tot have "eid" column, please see attached
> > >> >
> > >> > Can you please tell em what means to post in "plain text" ?
> > >> >
> > >> > Thanks
> > >> > Ana
> > >> >
> > >> > On Tue, Aug 6, 2019 at 1:33 PM Patrick (Malone Quantitative) <
> > malone at malonequantitative.com> wrote:
> > >> >>
> > >> >> Do you have some kind of ID variable? If so, it should be
> > >> >> straightforward with the appropriate joining function.
> > >> >>
> > >> >> What have you tried?
> > >> >>
> > >> >> Also, please post in plain text.
> > >> >>
> > >> >>
> > >> >> On Tue, Aug 6, 2019 at 2:16 PM Ana Marija <
> > sokovic.anamarija at gmail.com> wrote:
> > >> >> >
> > >> >> > Hello,
> > >> >> >
> > >> >> > I am filtering my data frame "tot" via:
> > >> >> >
> > >> >> > controls=tot %>% filter_all(any_vars(. %in% c("E109",
> > "E119","E149"))) %>%
> > >> >> > filter_all(any_vars(. %in% c("Caucasian"))) %>%
> > filter_all(any_vars(. %in%
> > >> >> > c("No kinship found","Ten or more third-degree relatives
> > identified")))
> > >> >> >
> > >> >> > > dim(controls)
> > >> >> > [1] 15381  1093
> > >> >> > > dim(tot)
> > >> >> > [1] 502536   1093
> > >> >> >
> > >> >> > how do I add in my data frame "tot" a new column called
> "controls"
> > where
> > >> >> > every of those filtered 15381 rows would have the value 1 and the
> > rest
> > >> >> > which can be found in tot have the value -9?
> > >> >> >
> > >> >> > Thanks
> > >> >> > Ana
> > >> >> >
> > >> >> >         [[alternative HTML version deleted]]
> > >> >> >
> > >> >> > ______________________________________________
> > >> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> see
> > >> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> > >> >> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > >> >> > and provide commented, minimal, self-contained, reproducible
> code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @|gbert @end|ng |rom w|w|@hu-ber||n@de  Wed Aug  7 14:31:50 2019
From: @|gbert @end|ng |rom w|w|@hu-ber||n@de (Sigbert Klinke)
Date: Wed, 7 Aug 2019 14:31:50 +0200
Subject: [R] shinyWidgets::sliderTextInput
In-Reply-To: <CACxE24kFV_bi2f0u9QyPo3sLCTM+qE2K8x7JaqHGNdWfk7YoCw@mail.gmail.com>
References: <1f85aa25-9914-954b-7695-6c68edcbfd85@wiwi.hu-berlin.de>
 <CACxE24kFV_bi2f0u9QyPo3sLCTM+qE2K8x7JaqHGNdWfk7YoCw@mail.gmail.com>
Message-ID: <93bafa63-667c-29ff-8472-096c23b60ca1@wiwi.hu-berlin.de>

Hi,

> Is there a label argument, please? I think that might be it.

In my example I had a label, but this not the problem since

 > choices=list("choice1"=1, "choice2"=2,"choice3"=3)
 > as.character(choices)
[1] "1" "2" "3"

delivers the result as it is used in sliderTextInput. I was hoping there 
was a way to bypass that behaviour without changing the sliderTextInput 
function ;)
The default action of selectInput using a list for choices is to show in 
the UI element the list names, but to deliver to the shiny app the 
number if one is selected.

Thanks Sigbert

-- 
https://hu.berlin/sk
https://hu.berlin/mmstat3


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Wed Aug  7 15:09:02 2019
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Wed, 7 Aug 2019 13:09:02 +0000 (UTC)
Subject: [R] Simulations of GAM and MARS models : sample size ;
 Y-outliers and missing X-data
References: <1436794675.3113973.1565183342017.ref@mail.yahoo.com>
Message-ID: <1436794675.3113973.1565183342017@mail.yahoo.com>

Dear Experts,

I have fitted MARS and GAM models on a real dataset. My goal is prediction. I have run crossvalidation?many times to get an idea of the out-of-bag accuracy value. I use the Mean Squared Error (MSE) as an error evaluation criterion. I have published my paper and the reviewers ask me to do simulations.
So, my goal is now to do simulations as simulation studies may be a better alternative for objectively comparing the performances of these 2 algorithms. My goal is to figure out which method (GAM or MARS) performs better (minimizing MSE) in what circumstances.
I want to consider 3 different factors : n (sample size) ; the presence of Y-outliers and the presence of missing data (X-data).
I want to know the influence of the sample size, the influence of the percentage of Y-outliers and the influence of the percentage of X?missing?data.

Sample size : n=50 ; n=100 ; n=200; n=300 and n=500
Y-outliers : 10% of Y-outliers ; 20% of Y-outliers ; 30% of Y-outliers ; 40% of Y-outliers and 50% of Y-outliers
Missing data : 10% of X missing data ; 20% of X missing data ; 30% of X missing data ; 40% of X missing data and 50% of X missing data

Here below are the reproducible R codes for GAM and MARS I use to calculate the MSE running cross-validation many times.?
How can I modify my R codes to simulate the sample size, the presence of Y-outliers and the presence of missing data ?

###MSE CROSSVALIDATION GAM (gam1)
install.packages("ISLR")
library(ISLR)
install.packages("mgcv")
library(mgcv)
?
set.seed(123)
# Create a list to store the results
lst<-list()
?
# This statement does the repetitions (looping)
for(i in 1?:1000){
?
n=dim(Wage)[1]
?
p=0.667
?
sam=sample(1?:n,floor(p*n),replace=FALSE)
?
Training =Wage [sam,]
Testing = Wage [-sam,]
?
GAM1<-gam(wage ~education+s(age,bs="ps")+year,data=Wage)
?
ypred=predict(GAM1,newdata=Testing)
y=Testing$wage

MSE = mean((y-ypred)^2)
MSE
lst[i]<-MSE
}
mean(unlist(lst))
########

#####MSE CROSSVALIDATION MARS (Mars1)
install.packages("ISLR")
library(ISLR)
install.packages("earth")
library(earth)

set.seed(123)
# Create a list to store the results
lst<-list()
?
# This statement does the repetitions (looping)
for(i in 1?:1000){
?
n=dim(Wage)[1]
?
p=0.667
?
sam=sample(1?:n,floor(p*n),replace=FALSE)
?
Training =Wage [sam,]
Testing = Wage [-sam,]
?
mars1 <- earth(wage~age+as.factor(education)+year, data=Wage)
?
ypred=predict(mars1,newdata=Testing)
y=Testing$wage

MSE = mean((y-ypred)^2)
MSE
lst[i]<-MSE
}
mean(unlist(lst))
#########


From wh|tney@cory @end|ng |rom gm@||@com  Tue Aug  6 16:31:38 2019
From: wh|tney@cory @end|ng |rom gm@||@com (Cory Whitney)
Date: Tue, 6 Aug 2019 16:31:38 +0200
Subject: [R] [R-pkgs] New functions in ethnobotanyR 0.1.5
Message-ID: <9787B662-33B4-4696-B337-68EBB589FF99@gmail.com>

ethnobotanyR 0.1.5 is a patch. The package now includes four new functions:

Updated UVs() following Tardio and Pardo-de-Santayana (2008)

simple_UVs() now calculates a simple UVs cf. Albuquerque et al. (2006).

Fidelity level per species FLs() from Friedman et al. (1986) is added.

Now includes the CVe() from Reyes-Garcia et al. (2006).

Modeling options and more common quantitative ethnobotany and anthropology tools will be added in the next version.

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From tom|err|93 @end|ng |rom gm@||@com  Wed Aug  7 12:23:44 2019
From: tom|err|93 @end|ng |rom gm@||@com (Tommaso Ferrari)
Date: Wed, 7 Aug 2019 12:23:44 +0200
Subject: [R] Understanding parameters of ugarch and DCC
Message-ID: <CAKPkyQQ5b_p3FRr7P9jxnGBixGzAb=fV-gj=TAsXYs4oqbg0OA@mail.gmail.com>

Dear all,
I'm using *rugarch* and *rmgarch *to implement a DCC model over 30
different assets.

While using *ugarchspec *for defining input parameters of univariate GARCH
using a t-Student distribution, I have noticed that there is a parameter
called *fixed.pars*. Using the example given in GitHub, this parameter is
set as following:

fixed.pars = list(shape = 5)

in case of a t-Student distribution. If I change this setting from 5 to 3
(for example) results are very different. Can someone tell me the
importante and the usage of this parameter?

Thanks to all
Tommaso

	[[alternative HTML version deleted]]


From er|nm@hodge@@ @end|ng |rom gm@||@com  Wed Aug  7 18:01:38 2019
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Wed, 7 Aug 2019 10:01:38 -0600
Subject: [R] shinyWidgets::sliderTextInput
In-Reply-To: <93bafa63-667c-29ff-8472-096c23b60ca1@wiwi.hu-berlin.de>
References: <1f85aa25-9914-954b-7695-6c68edcbfd85@wiwi.hu-berlin.de>
 <CACxE24kFV_bi2f0u9QyPo3sLCTM+qE2K8x7JaqHGNdWfk7YoCw@mail.gmail.com>
 <93bafa63-667c-29ff-8472-096c23b60ca1@wiwi.hu-berlin.de>
Message-ID: <CACxE24mz7bAaSkoPw5HG2gvh2RusqQ1gnC3PFCRAbjL-4Eqeug@mail.gmail.com>

Hi Sigbert


Here is something that has both the sliderTextInput and Radio Buttons,
which may be helpful:

library("shiny")

library("shinyWidgets")


ui <- fluidPage(

  br(),

  sliderTextInput(

    inputId = "mySliderText",

    label = "My House",

    choices = list("choice 1" = "choice 1","choice 2" ="choice 2",

    "choice 3" ="choice 3"),

    selected = "choice 1"

  ),


          radioButtons(inputId="slope", label="Radio Buttons",

               choices=list("Choice 1" = "Choice 1","Choice 2" = "Choice 2",

                 "Choice 3" = "Choice 3")),



  verbatimTextOutput(outputId = "result")

)


server <- function(input, output, session) {

  output$result <- renderPrint({

  str(input$mySliderText)

  str(input$slope)

}

)


}


shinyApp(ui = ui, server = server)



Hope this helps!


Sincerely,

Erin



Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com


On Wed, Aug 7, 2019 at 6:31 AM Sigbert Klinke <sigbert at wiwi.hu-berlin.de>
wrote:

> Hi,
>
> > Is there a label argument, please? I think that might be it.
>
> In my example I had a label, but this not the problem since
>
>  > choices=list("choice1"=1, "choice2"=2,"choice3"=3)
>  > as.character(choices)
> [1] "1" "2" "3"
>
> delivers the result as it is used in sliderTextInput. I was hoping there
> was a way to bypass that behaviour without changing the sliderTextInput
> function ;)
> The default action of selectInput using a list for choices is to show in
> the UI element the list names, but to deliver to the shiny app the
> number if one is selected.
>
> Thanks Sigbert
>
> --
> https://hu.berlin/sk
> https://hu.berlin/mmstat3
>

	[[alternative HTML version deleted]]


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Wed Aug  7 20:40:42 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 7 Aug 2019 13:40:42 -0500
Subject: [R] Help with if else statement
Message-ID: <CAF9-5jOYU26Mua0Ggmi7TTHpZK6RCVSb5wm3o4uty1SUU3j2+g@mail.gmail.com>

Hello,

I have a data frame which looks like this:

> head(pt)
     eidQ phenoQ phenoH
1 1000017     -9     -9
2 1000025     -9     -9
3 1000038     -9      1
4 1000042     -9     -9
5 1000056     -9     -9
6 1000074     -9     -9
7   1000038     -9      1
8  1000127      2      1
9  1000690      2     -9
10  1000711      2     -9
11 1001431      2      1
12 1001710     -9      1

I would like to create the 3rd column called "pheno" which would have
these values:
-9,-9,1,-9,-9,-9,1,2,2,2,2,1

so in other words:
-if -9 appears in both phenoQ and phenoH I will have -9 in pheno
-if 2 appears in any of phenoQ or phenoH I will have 2 in pheno
-if I have -9 and 1 or 1 and -9 in those columns I will have 1 in pheno
-if I have -9 and 2 or 2 and -9 in those columns I will have 2 in pheno

Can you please tell me how my if else statement would look like or any
other way how to do that in R if my data frame name is "pt"

Thanks
Ana
Ana


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Wed Aug  7 20:51:41 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 7 Aug 2019 13:51:41 -0500
Subject: [R] Help with if else statement
In-Reply-To: <CAF9-5jOYU26Mua0Ggmi7TTHpZK6RCVSb5wm3o4uty1SUU3j2+g@mail.gmail.com>
References: <CAF9-5jOYU26Mua0Ggmi7TTHpZK6RCVSb5wm3o4uty1SUU3j2+g@mail.gmail.com>
Message-ID: <CAF9-5jNsmnPvy_g-P=h1WH6LZOVoyLET_bTYXj16WCOKP6GQFA@mail.gmail.com>

does this look ok:

pt$pheno=ifelse(pt$phenoQ==-9 & pt$phenoH==-9,-9,ifelse(pt$phenoH==2 |
pt$phenoQ==2,2,1))

On Wed, Aug 7, 2019 at 1:40 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Hello,
>
> I have a data frame which looks like this:
>
> > head(pt)
>      eidQ phenoQ phenoH
> 1 1000017     -9     -9
> 2 1000025     -9     -9
> 3 1000038     -9      1
> 4 1000042     -9     -9
> 5 1000056     -9     -9
> 6 1000074     -9     -9
> 7   1000038     -9      1
> 8  1000127      2      1
> 9  1000690      2     -9
> 10  1000711      2     -9
> 11 1001431      2      1
> 12 1001710     -9      1
>
> I would like to create the 3rd column called "pheno" which would have
> these values:
> -9,-9,1,-9,-9,-9,1,2,2,2,2,1
>
> so in other words:
> -if -9 appears in both phenoQ and phenoH I will have -9 in pheno
> -if 2 appears in any of phenoQ or phenoH I will have 2 in pheno
> -if I have -9 and 1 or 1 and -9 in those columns I will have 1 in pheno
> -if I have -9 and 2 or 2 and -9 in those columns I will have 2 in pheno
>
> Can you please tell me how my if else statement would look like or any
> other way how to do that in R if my data frame name is "pt"
>
> Thanks
> Ana
> Ana


From @pro @end|ng |rom un|me|b@edu@@u  Wed Aug  7 21:12:20 2019
From: @pro @end|ng |rom un|me|b@edu@@u (Andrew Robinson)
Date: Wed, 7 Aug 2019 19:12:20 +0000
Subject: [R] Help with if else statement
In-Reply-To: <CAF9-5jOYU26Mua0Ggmi7TTHpZK6RCVSb5wm3o4uty1SUU3j2+g@mail.gmail.com>
References: <CAF9-5jOYU26Mua0Ggmi7TTHpZK6RCVSb5wm3o4uty1SUU3j2+g@mail.gmail.com>
Message-ID: <ME1PR01MB0995ABD435C4637D0CC67092E2D40@ME1PR01MB0995.ausprd01.prod.outlook.com>

pmax() should work in this instance, as in any case you want the larger value.

Andrew

--
Andrew Robinson
Director, CEBRA, School of BioSciences
Reader & Associate Professor in Applied Statistics Tel: (+61) 0403 138 955
School of Mathematics and Statistics Fax: (+61) 03 8344 4599
University of Melbourne, VIC 3010 Australia
Email: apro at unimelb.edu.au
Website: http://cebra.unimelb.edu.au/
________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of Ana Marija <sokovic.anamarija at gmail.com>
Sent: Thursday, August 8, 2019 4:37 am
To: r-help
Subject: [R] Help with if else statement

Hello,

I have a data frame which looks like this:

> head(pt)
     eidQ phenoQ phenoH
1 1000017     -9     -9
2 1000025     -9     -9
3 1000038     -9      1
4 1000042     -9     -9
5 1000056     -9     -9
6 1000074     -9     -9
7   1000038     -9      1
8  1000127      2      1
9  1000690      2     -9
10  1000711      2     -9
11 1001431      2      1
12 1001710     -9      1

I would like to create the 3rd column called "pheno" which would have
these values:
-9,-9,1,-9,-9,-9,1,2,2,2,2,1

so in other words:
-if -9 appears in both phenoQ and phenoH I will have -9 in pheno
-if 2 appears in any of phenoQ or phenoH I will have 2 in pheno
-if I have -9 and 1 or 1 and -9 in those columns I will have 1 in pheno
-if I have -9 and 2 or 2 and -9 in those columns I will have 2 in pheno

Can you please tell me how my if else statement would look like or any
other way how to do that in R if my data frame name is "pt"

Thanks
Ana
Ana

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Wed Aug  7 21:12:53 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 7 Aug 2019 12:12:53 -0700
Subject: [R] Help with if else statement
In-Reply-To: <CAF9-5jNsmnPvy_g-P=h1WH6LZOVoyLET_bTYXj16WCOKP6GQFA@mail.gmail.com>
References: <CAF9-5jOYU26Mua0Ggmi7TTHpZK6RCVSb5wm3o4uty1SUU3j2+g@mail.gmail.com>
 <CAF9-5jNsmnPvy_g-P=h1WH6LZOVoyLET_bTYXj16WCOKP6GQFA@mail.gmail.com>
Message-ID: <CAGxFJbT5B6Rq_TM7LZxzWrb40EqqpTVtTHHqhpMMushFL=7WFg@mail.gmail.com>

The ifelse() construction is fast, but after a couple of nested iterations,
it gets messy and error-prone; so I believe to be avoided.

In your case, there is a much better alternative, ?pmax . Ergo, something
like:

pt$pheno <- do.call(pmax, pt[, -1])

?do.call is necessary to pass the list of columns pt[, -1] to pmax.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Aug 7, 2019 at 11:55 AM Ana Marija <sokovic.anamarija at gmail.com>
wrote:

> does this look ok:
>
> pt$pheno=ifelse(pt$phenoQ==-9 & pt$phenoH==-9,-9,ifelse(pt$phenoH==2 |
> pt$phenoQ==2,2,1))
>
> On Wed, Aug 7, 2019 at 1:40 PM Ana Marija <sokovic.anamarija at gmail.com>
> wrote:
> >
> > Hello,
> >
> > I have a data frame which looks like this:
> >
> > > head(pt)
> >      eidQ phenoQ phenoH
> > 1 1000017     -9     -9
> > 2 1000025     -9     -9
> > 3 1000038     -9      1
> > 4 1000042     -9     -9
> > 5 1000056     -9     -9
> > 6 1000074     -9     -9
> > 7   1000038     -9      1
> > 8  1000127      2      1
> > 9  1000690      2     -9
> > 10  1000711      2     -9
> > 11 1001431      2      1
> > 12 1001710     -9      1
> >
> > I would like to create the 3rd column called "pheno" which would have
> > these values:
> > -9,-9,1,-9,-9,-9,1,2,2,2,2,1
> >
> > so in other words:
> > -if -9 appears in both phenoQ and phenoH I will have -9 in pheno
> > -if 2 appears in any of phenoQ or phenoH I will have 2 in pheno
> > -if I have -9 and 1 or 1 and -9 in those columns I will have 1 in pheno
> > -if I have -9 and 2 or 2 and -9 in those columns I will have 2 in pheno
> >
> > Can you please tell me how my if else statement would look like or any
> > other way how to do that in R if my data frame name is "pt"
> >
> > Thanks
> > Ana
> > Ana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Thu Aug  8 00:15:10 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Thu, 8 Aug 2019 08:15:10 +1000
Subject: [R] Help with if else statement
In-Reply-To: <CAF9-5jOYU26Mua0Ggmi7TTHpZK6RCVSb5wm3o4uty1SUU3j2+g@mail.gmail.com>
References: <CAF9-5jOYU26Mua0Ggmi7TTHpZK6RCVSb5wm3o4uty1SUU3j2+g@mail.gmail.com>
Message-ID: <CA+8X3fVKQTw7GWoi_sM7qT1rLc8nNRTLR1U1WgYZkvK4NcQwUg@mail.gmail.com>

Hi Ana,
Or just for a bit of fun:

pt<-read.table(text="eidQ phenoQ phenoH
 1000017     -9     -9
 1000025     -9     -9
 1000038     -9      1
 1000042     -9     -9
 1000056     -9     -9
 1000074     -9     -9
 1000038     -9      1
 1000127      2      1
 1000690      2     -9
 1000711      2     -9
 1001431      2      1
 1001710     -9      1",
header=TRUE)
pt$pheno<-apply(pt[,2:3],1,FUN=max)

Jim

On Thu, Aug 8, 2019 at 4:37 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Hello,
>
> I have a data frame which looks like this:
>
> > head(pt)
>      eidQ phenoQ phenoH
> 1 1000017     -9     -9
> 2 1000025     -9     -9
> 3 1000038     -9      1
> 4 1000042     -9     -9
> 5 1000056     -9     -9
> 6 1000074     -9     -9
> 7   1000038     -9      1
> 8  1000127      2      1
> 9  1000690      2     -9
> 10  1000711      2     -9
> 11 1001431      2      1
> 12 1001710     -9      1
>
> I would like to create the 3rd column called "pheno" which would have
> these values:
> -9,-9,1,-9,-9,-9,1,2,2,2,2,1
>
> so in other words:
> -if -9 appears in both phenoQ and phenoH I will have -9 in pheno
> -if 2 appears in any of phenoQ or phenoH I will have 2 in pheno
> -if I have -9 and 1 or 1 and -9 in those columns I will have 1 in pheno
> -if I have -9 and 2 or 2 and -9 in those columns I will have 2 in pheno
>
> Can you please tell me how my if else statement would look like or any
> other way how to do that in R if my data frame name is "pt"
>
> Thanks
> Ana
> Ana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Thu Aug  8 05:29:43 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Thu, 8 Aug 2019 15:29:43 +1200
Subject: [R] Simulations of GAM and MARS models : sample size ;
 Y-outliers and missing X-data
In-Reply-To: <1436794675.3113973.1565183342017@mail.yahoo.com>
References: <1436794675.3113973.1565183342017.ref@mail.yahoo.com>
 <1436794675.3113973.1565183342017@mail.yahoo.com>
Message-ID: <CAB8pepw9AWsJRs2_MExGG05MsJ838LRLdQ3Xn_ZhjVHvpo6KrA@mail.gmail.com>

> How can I modify my R codes to simulate the sample size, the presence of
Y-outliers and the presence of missing data ?

I don't know what it means for data to have 50% Y-outliers.
That's new to me...

As for the rest of your question.
Modify your code so that a single function, say sim.test() computes your
simulated statistics, for n sample size and m missing values, and returns
the results, say as a two-element list.

Then write a top level script (or function), something like:

+ ns = c (50, 100, 200, 300, 500)
+ ms = (1:5) * 0.1

+ n = rep (ns, each=5)
+ m = rep (ms, times=5)
+ GAM.stat = MARS.stat = numeric (25)

+ for (i in 1:25)
+ {   results = sim.test (n [i], m [i], ...other.args...)
+     GAM.stat [i] = results$GAM.stat
+     MARS.stat [i] = results$MARS.stat
+ }

+ cbind (n, m, GAM.stat, MARS.stat)

Note that from past experience, what you are doing may produce misleading
results.
Because your results are dependent on your simulated data.
(Different simulated data will produce different results, and different end
conclusions).

I haven't checked how the functions, you've used to fit models, handle
missing values.
But assuming that missing values are NAs, this should be easy to do.

Do you want *each* x variable to have m% missing values, or *all* the x
variables (collectively), to have m% missing values?

	[[alternative HTML version deleted]]


From g@teo|t|me9 @end|ng |rom gm@||@com  Wed Aug  7 19:09:10 2019
From: g@teo|t|me9 @end|ng |rom gm@||@com (Jay Waldron)
Date: Wed, 7 Aug 2019 11:09:10 -0600
Subject: [R] Stats Package Fix
Message-ID: <CAE7w3nK-Z5p1W_8yC6+0TaJw267cBev9=MwGLRZ67R=YSiA3sg@mail.gmail.com>

 Good afternoon,

Using this file, with the tab for Growth_Value (I have pasted some code
below), if I add the argument *exact = F *it produces the Wilcoxon signed
rank test with continuity correction. The description documentation leads
me to believe that's the wrong argument (should be the "correct" not the
"exact" argument. I am using R Studio Version 1.2.1335 running R 3.6.1

This was posed in our Business Statistics Class, taught at Utah Valley
University, of which I am a senior undergraduate.

A response by tomorrow would be ideal, but I would still like an answer
even if that timeline is too aggressive. Thank you for your consideration.
My contact information is:

Jay Spencer Waldron
Personal Email (This is the one I am subscribed to R-Help with):
GateofTime9 at gmail.com
(385) 335-7879

#---Code paste begins---
> library(readxl)
> Growth_Value <- read_excel("2019
SUMMER/ECON-3340-003/Ch20/Chapter20.xlsx",
+     sheet = "Growth_Value")
> View(Growth_Value)
> wilcox.test(Growth_Value$'Growth', mu=5, alternative="greater")

Wilcoxon signed rank test

data:  Growth_Value$Growth
V = 40, p-value = 0.1162
alternative hypothesis: true location is greater than 5

>
>
> wilcox.test(Growth_Value$'Growth', Growth_Value$'Value',
alternative="two.sided", paired=TRUE)

Wilcoxon signed rank test

data:  Growth_Value$Growth and Growth_Value$Value
V = 40, p-value = 0.2324
alternative hypothesis: true location shift is not equal to 0

> wilcox.test(Growth_Value$'Growth', mu=5, alternative="greater", exact=F)

Wilcoxon signed rank test with continuity correction

data:  Growth_Value$Growth
V = 40, p-value = 0.1106
alternative hypothesis: true location is greater than 5


#---Code Paste Ends---

*Documentation referenced*




*exact a logical indicating whether an exact p-value should be
computed.correct a logical indicating whether to apply continuity
correction in the normal approximation for the p-value.*


Thanks you for your time,

Jay
Educational Email:
10809669 at my.uvu.edu

	[[alternative HTML version deleted]]


From c@|kev|n555 @end|ng |rom gm@||@com  Wed Aug  7 18:57:09 2019
From: c@|kev|n555 @end|ng |rom gm@||@com (Kevin Cai)
Date: Wed, 7 Aug 2019 12:57:09 -0400
Subject: [R] Help with saving model created using caret's train function
 into a readable format
Message-ID: <CAETGggrf3gc_eKZkLmErDSFCXjOpFVBxBxn78ELp9pv8e-Z6Uw@mail.gmail.com>

Hello all,

I am trying to port some R code over to Python for someone else. In
particular, I want to save a model/object created using the train function
from the caret package:

model <- train(
    x, y,
    trControl = trcontrol_1,
    tuneGrid = xgbGrid_1,
    method = "xgbTree"
)

'model' is later used as input for the predict function on some test data.
I've tried doing save(model, ascii=TRUE, file="model_ascii_dump") so far,
but the output isn't really readable/understandable.

Is there any way to save 'model' into a readable format or some format that
can be used to make predictions in Python?

Sorry if I'm not explaining myself well or if the question is too vague;
this is my first time dealing with R.

Thanks in advance for any help you can give.

	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Thu Aug  8 07:21:15 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Thu, 8 Aug 2019 17:21:15 +1200
Subject: [R] Simulations of GAM and MARS models : sample size ;
 Y-outliers and missing X-data
In-Reply-To: <CAB8pepw9AWsJRs2_MExGG05MsJ838LRLdQ3Xn_ZhjVHvpo6KrA@mail.gmail.com>
References: <1436794675.3113973.1565183342017.ref@mail.yahoo.com>
 <1436794675.3113973.1565183342017@mail.yahoo.com>
 <CAB8pepw9AWsJRs2_MExGG05MsJ838LRLdQ3Xn_ZhjVHvpo6KrA@mail.gmail.com>
Message-ID: <CAB8pepws1Jr+z7WpfOKx_gqKUBWTFGS31Lf1eBz1E3hA-E4c+Q@mail.gmail.com>

Sorry.
Some more comments.

(1) If you want an arbitrary sample size, you may need to write a function
to produce a simulated data set, for that given sample size.
Alternatively, you can use a random sample of size n, of an initial data
set, assuming the initial data set is relatively large.
(2) For each combination of n sample size and m% missing values, you may
need to compute your statistic, many (i.e. thousands of) times.
Then compute the mean and variance of your statistic.
(3) I have assumed that you want to start with a data.frame with no missing
values, and then set random subset(s) to missing values.
(4) Note that GAMs can have interaction terms.

	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Thu Aug  8 07:44:08 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Thu, 8 Aug 2019 17:44:08 +1200
Subject: [R] Stats Package Fix
In-Reply-To: <CAE7w3nK-Z5p1W_8yC6+0TaJw267cBev9=MwGLRZ67R=YSiA3sg@mail.gmail.com>
References: <CAE7w3nK-Z5p1W_8yC6+0TaJw267cBev9=MwGLRZ67R=YSiA3sg@mail.gmail.com>
Message-ID: <CAB8pepwMnH2vk3vkoO7Lt7Q81q2EAbnAbbcSC4RC3ZGR9i-WrQ@mail.gmail.com>

(excerpts only)
> Using this file, with the tab for Growth_Value (I have pasted some code
> below), if I add the argument *exact = F *it produces the Wilcoxon signed
> rank test with continuity correction. The description documentation leads
> me to believe that's the wrong argument (should be the "correct" not the
> "exact" argument.
> A response by tomorrow would be ideal, but I would still like an answer

I do *not* speak for R Core, and I'm not expert on this test.
However, it appears it's doing what it's supposed to be doing.

There are separate arguments for exact and correct (which defaults to true).
And according to the documentation:

"By default (if exact is not specified), an exact p-value is computed if
the samples contain less than 50 finite values and there are no ties.
Otherwise, a normal approximation is used."

When you set exact to false (FALSE is preferable to F), the normal
approximation is used, with the continuity correction.

	[[alternative HTML version deleted]]


From ||nu@@|@chen @end|ng |rom gm@||@com  Thu Aug  8 10:46:27 2019
From: ||nu@@|@chen @end|ng |rom gm@||@com (Linus Chen)
Date: Thu, 8 Aug 2019 10:46:27 +0200
Subject: [R] Help with if else statement
In-Reply-To: <CAGxFJbT5B6Rq_TM7LZxzWrb40EqqpTVtTHHqhpMMushFL=7WFg@mail.gmail.com>
References: <CAF9-5jOYU26Mua0Ggmi7TTHpZK6RCVSb5wm3o4uty1SUU3j2+g@mail.gmail.com>
 <CAF9-5jNsmnPvy_g-P=h1WH6LZOVoyLET_bTYXj16WCOKP6GQFA@mail.gmail.com>
 <CAGxFJbT5B6Rq_TM7LZxzWrb40EqqpTVtTHHqhpMMushFL=7WFg@mail.gmail.com>
Message-ID: <CAPm+3sAOmCm+FrXhnm_zf-oOXH70LZ4Ow5rJx=s00XM-xj4Ypw@mail.gmail.com>

Bert's answer is great, but since there is only two columns to be
used, why not simply
pt$pheno <- pmax( p$phenoQ ,p$phenoH )

Cheers,
Lei

On Wed, 7 Aug 2019 at 21:23, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
> The ifelse() construction is fast, but after a couple of nested iterations,
> it gets messy and error-prone; so I believe to be avoided.
>
> In your case, there is a much better alternative, ?pmax . Ergo, something
> like:
>
> pt$pheno <- do.call(pmax, pt[, -1])
>
> ?do.call is necessary to pass the list of columns pt[, -1] to pmax.
>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Wed, Aug 7, 2019 at 11:55 AM Ana Marija <sokovic.anamarija at gmail.com>
> wrote:
>
> > does this look ok:
> >
> > pt$pheno=ifelse(pt$phenoQ==-9 & pt$phenoH==-9,-9,ifelse(pt$phenoH==2 |
> > pt$phenoQ==2,2,1))
> >
> > On Wed, Aug 7, 2019 at 1:40 PM Ana Marija <sokovic.anamarija at gmail.com>
> > wrote:
> > >
> > > Hello,
> > >
> > > I have a data frame which looks like this:
> > >
> > > > head(pt)
> > >      eidQ phenoQ phenoH
> > > 1 1000017     -9     -9
> > > 2 1000025     -9     -9
> > > 3 1000038     -9      1
> > > 4 1000042     -9     -9
> > > 5 1000056     -9     -9
> > > 6 1000074     -9     -9
> > > 7   1000038     -9      1
> > > 8  1000127      2      1
> > > 9  1000690      2     -9
> > > 10  1000711      2     -9
> > > 11 1001431      2      1
> > > 12 1001710     -9      1
> > >
> > > I would like to create the 3rd column called "pheno" which would have
> > > these values:
> > > -9,-9,1,-9,-9,-9,1,2,2,2,2,1
> > >
> > > so in other words:
> > > -if -9 appears in both phenoQ and phenoH I will have -9 in pheno
> > > -if 2 appears in any of phenoQ or phenoH I will have 2 in pheno
> > > -if I have -9 and 1 or 1 and -9 in those columns I will have 1 in pheno
> > > -if I have -9 and 2 or 2 and -9 in those columns I will have 2 in pheno
> > >
> > > Can you please tell me how my if else statement would look like or any
> > > other way how to do that in R if my data frame name is "pt"
> > >
> > > Thanks
> > > Ana
> > > Ana
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Thu Aug  8 14:11:05 2019
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Thu, 8 Aug 2019 12:11:05 +0000 (UTC)
Subject: [R] Simulations of GAM and MARS models : sample size ;
 Y-outliers and missing X-data
In-Reply-To: <CAB8pepw9AWsJRs2_MExGG05MsJ838LRLdQ3Xn_ZhjVHvpo6KrA@mail.gmail.com>
References: <1436794675.3113973.1565183342017.ref@mail.yahoo.com>
 <1436794675.3113973.1565183342017@mail.yahoo.com>
 <CAB8pepw9AWsJRs2_MExGG05MsJ838LRLdQ3Xn_ZhjVHvpo6KrA@mail.gmail.com>
Message-ID: <1983628866.3770816.1565266265855@mail.yahoo.com>

Dear Abby,

Many thanks for your response.

To answer your question. For me better all the x variables (collectively), to have m% missing values.

When you tell me : "Modify your code so that a single function say sim.test() computes your simulated statistics, for n sample size and m missing values, and returns the results, say as a two-element list".
I trust you and guess it is a really good idea, but don't know how to do that... :=(







Le jeudi 8 ao?t 2019 ? 05:29:55 UTC+2, Abby Spurdle <spurdle.a at gmail.com> a ?crit : 





> How can I modify my R codes to simulate the sample size, the presence of Y-outliers and the presence of missing data ?


I don't know what it means for data to have 50% Y-outliers.
That's new to me...

As for the rest of your question.
Modify your code so that a single function, say sim.test() computes your simulated statistics, for n sample size and m missing values, and returns the results, say as a two-element list.

Then write a top level script (or function), something like:

+ ns = c (50, 100, 200, 300, 500)
+ ms = (1:5) * 0.1

+ n = rep (ns, each=5)
+ m = rep (ms, times=5)
+ GAM.stat = MARS.stat = numeric (25)

+ for (i in 1:25)
+?{? ?results = sim.test (n [i], m [i], ...other.args...)
+? ? ?GAM.stat [i] = results$GAM.stat
+? ? ?MARS.stat [i] = results$MARS.stat
+?}

+ cbind (n, m, GAM.stat, MARS.stat)

Note that from past experience, what you are doing may produce misleading results.
Because your results are dependent on your simulated data.
(Different simulated data will produce different results, and different end conclusions).

I haven't checked how the functions, you've used to fit models, handle missing values.
But assuming that missing values are NAs, this should be easy to do.

Do you want *each* x variable to have m% missing values, or *all* the x variables (collectively), to have m% missing values?


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Thu Aug  8 16:08:27 2019
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Thu, 8 Aug 2019 16:08:27 +0200
Subject: [R] Error exporting dataframe from Julia to R with Feather
Message-ID: <CAMk+s2RVUKu0Z-oNnwbsiij5v7tKzt25HbonNO0rYuq=Okez=g@mail.gmail.com>

Hello,

since I am encountering a lot of problems exporting dataframes from
julia to R (there is always something wrong with the formatting,
probably a missing quote) so I am trying to use Feather to do the job.

I have installed Feather in Julia with `pkg.add("Feather")` and
imported it with `using Feather`. I created a dataframe and saved it
with `Feather.write("/dir/dataframe.feather", df)` and it worked. I
can even open it back with `df =
Feather.read("/dir/dataframe.feather")` and get: `julia> nrow(df)
128544`.
The problem is with R. I am using Rstudio to test each step.
I installed the package, imported it with `library(feather)` and used it as:
```
df = read_feather("/dir/dataframe.feather")
```
and then Rstudio simply crashes.
any idea why?
Thank you


From tom@bo|ger @end|ng |rom ucd@|e  Thu Aug  8 16:11:50 2019
From: tom@bo|ger @end|ng |rom ucd@|e (Thomas Bolger)
Date: Thu, 8 Aug 2019 15:11:50 +0100
Subject: [R] gen.geneaslogy
Message-ID: <CANVQ5Q2F0WJLvkb79E32vV+4Dntrd8C7qxFc4AOui-TOz1s4zA@mail.gmail.com>

Hi

I have just started to do some analysis of genealogies and seem to be doing
something wrong when using gen.genealogy. The following is the script and
output that I used. Any help greatfully apprciated

library(GENLIB)
> library(ggenealogy)
> library(igraph)
> #
> #
> ###Data input as data frame
>
ind<-c(501,502,601,603,605,608,701,702,703,704,705,706,707,708,709,710,801,802)
>
father<-c(401,411,501,501,501,501,601,601,601,603,603,603,605,605,608,608,701,701)
>
mother<-c(402,412,502,502,502,502,602,602,602,604,604,604,606,607,609,609,711,711)
> sex<-c(1,2,1,1,1,1,2,2,1,2,1,2,1,1,1,1,1,2)
> gen.df<-data.frame(ind, father, mother, sex)
> #print data to check
> print (gen.df)
   ind father mother sex
1  501    401    402   1
2  502    411    412   2
3  601    501    502   1
4  603    501    502   1
5  605    501    502   1
6  608    501    502   1
7  701    601    602   2
8  702    601    602   2
9  703    601    602   1
10 704    603    604   2
11 705    603    604   1
12 706    603    604   2
13 707    605    606   1
14 708    605    607   1
15 709    608    609   1
16 710    608    609   1
17 801    701    711   1
18 802    701    711   2
> #
> #
> genex<-gen.genealogy(gen.df)
Error in gen.genealogy(gen.df) :
  Invalid 'gen' parameter: identical individual number for both 'father'
and 'mother'
> #
>

-- 
Thomas Bolger
Emeritus Full Professor of Zoology


UCD School of Biology and Environmental Science
Belfield
Dublin 4
Ireland

Telephone : +353-1-7162330

	[[alternative HTML version deleted]]


From den|@e@je@ne@b @end|ng |rom gm@||@com  Thu Aug  8 16:23:29 2019
From: den|@e@je@ne@b @end|ng |rom gm@||@com (Denise b)
Date: Thu, 8 Aug 2019 10:23:29 -0400
Subject: [R] CoxPH multivariate frailty model with coxph (survival)
In-Reply-To: <5704e2cf2d3d4672a1cb88d9a8045998@med.umich.edu>
References: <CAJfThnQjEWXndTCYroqg39OjdE6vpUdRNR-3SrHE0_-=afD00A@mail.gmail.com>
 <40f13e2d-7165-a29c-b05c-d4661e22b8e9@comcast.net>
 <5704e2cf2d3d4672a1cb88d9a8045998@med.umich.edu>
Message-ID: <CAJfThnSyE5hsmkYTB-GhS5gW1t3PVXjCrFsh5wkJ_4_e3A8amg@mail.gmail.com>

 Chris,

Thanks a lot for the help and your investigation by simulations.

> Interacting a fixed effect with a strata variable is not uncommon.  No
> main effect of the strata variable is >  possible but the interaction term
> allows the fixed effect variable to have different values in different
>  strata.  I've more often seen it coded as a direct interaction:
> coxph( Surv(Time, Status) ~ treatment*strata(Event_type) + frailty(ID),
> data=example)
> (e.g., page 47 of Therneau and Grambsch)

This formulation works but doesn't provide the beta estimate of Tx on the
second event type (we need to sum the two beta). An equivalent formulation
that provides beta estimates on the two types of events is to create
intermediate variables as proposed page 177 of Therneau and Grambsch and
attached here.

<https://r.789695.n4.nabble.com/file/t388337/example.png>

> I don't see anything inherently wrong with your interpretation of the
> model.  While it seems that the
> assumption of no effect of one event on the other is very strong, I don't
> know the context of your
> analysis. I visualized it as a 4-state model.  Everyone starts in 0
> ("neither event").

To clarify the context of this analysis, we are interested to estimate
(&test) the effect of a genetic variant on two time-to-complication traits
(non-competitive based on expert's knowledge), while accounting for
potential unexplained dependency between the traits. Multi-state models can
effectively be an interesting
alternative model formulation that I need to think a bit more in this
context.

> I did not observe much gain from the frailty term (unmeasured covariate)
> with only 2 events in the short > simulation I tried (except when the
> effect was very strong and I got convergence warnings).

I modified your simulation script to plot the beta estimates from r=100 data
replicates obtained from the following model comparisons:
modf  : Cox PH frailty model (model of interest)
modc  : Cox PH including the simulated gamma variable (for comparison with
modf )
modcf : Cox PH frailty model  + gamma (for comparisons with modf & modc; the
introduction of the simulated gamma variable should reduced the variance of
the frailty term; )
modsep: separate Cox PH model for each time-to-event trait (no frailty, no
gamma)

It Overall, I got some expected results:

-modf reduces the bias of Tx effect on event 1 and event2 compared to modsep
(see boxplots)
-modsep show an increased bias, while not for modf when I increase the
variance of the simulated gamma term (see boxplots)
-when the gamma simulated covariate is introduced as a covariate in the
model (modc), the variance of the frailty term is reduced (modcf versus modf
results; see for the last replicates).

So, it validates some conclusions I had before, but :
1) modf returns an estimate of the variance of the frailty term that tends
to be lower than the one specified for the data simulation
2) To be sure the specified model fits exactly what I want, I need to check
the likelihood function especially because they use an equivalence with a
penalized likelihood for the gamma frailty (see help of frailty()).

Regarding the model I specified for the two time-to-event traits, and the
conditional assumption I assume (the two time-to-event traits are assumed
independent given the frailty term),
the contribution of each subject i to the likelihood of the model of
interest should have the form:
 Li = Li_ev1(beta1|ui) * Li_ev2(beta2|ui) * p(ui|zz),
with ui = subject's frailty term and ui~gamma(shape,scale) & p the gamma
distribution
Li_ev1 and Li_ev2 are the contribution of i to the likelihood for event1 &
event2

There is a description of the frailty model in Therneau's  et al paper:
 https://www.mayo.edu/research/documents/frailtypdf/doc-10027273 starting
page 15, detailed page 38.
The formulation is described in a different context, with N individuals
(1<=i<=N) in q independent clusters j (1<=j<=q) with a frailty term that
accounts for the dependence between individuals within each cluster.
In my case, each cluster corresponds to an individual i and dependencies
between the two observations for each subject are accounted by the frailty
term. Using their notations, j= individual and i= type of event.
I still need to clarify if their likelihood covers the likelihood of my
model formulation.

Any comments, suggestions or experiences are welcome. :)


If some have other suggestions of packages to fit this type of model,
please let me know (other potential R packages: parfm, frailtySurv, cph from
rms, mets, MST,..?).

Denise,


I attached below the extension of Chris's code.

#######################
# Modified Chris's code
#######################
library(flexsurv)
library(survival)

set.seed(20190729)

res <- NULL
for(r in 1:100) {
# Multiple non-competing outcomes, connected only by frailty (unmeasured
covariate)
nn <- 1000
kk <- 2

# frailty, 1 per individual
# variance of random effect
#tau <- 0.01^2
#tau <- 2^2
#tau <- 0.4^2
tau <- 0.8^2
#tau <- 1^2
#tau <- 0.6^2
gamma <- rgamma(nn, shape=1/tau, scale=tau) # mean is 1
hist(gamma)
sd(gamma) # ~ tau

# covariate
tx <- sample(0:1, nn, replace = TRUE)
# covariate effect on log hazard
beta <- 1
# might want to allow different treatment effects for different events
beta <- seq(kk)/kk

short <- data.frame(id=seq(nn), tx=tx, gamma=gamma)

# survival times, kk per individual
# tt <- rweibullPH(kk*nn, shape=2, scale=exp(beta*tx)*gamma)
# hist(tt)
# might want to allow different shapes or scales for different events
for (k in seq(kk)) {
        short[[paste0("time", k)]] <- rweibullPH(nn, shape=2,
scale=exp(beta[k]*tx)*gamma)
}
# might want to allow censoring

long <- reshape(short, direction="long", varying = paste0("time", seq(kk)),
v.names="times", timevar="event", idvar="id", times=seq(kk))
long <- long[order(long$id, long$event),]
mod <- coxph(Surv(times) ~ tx * strata(event) + frailty(id), data=long)
summary(mod)

mod0 <- coxph(Surv(times) ~ tx * strata(event), data=long)
coef(mod0)

mod1 <- coxph(Surv(times) ~ tx, data=long, subset=event==1)
coef(mod1)

mod2 <- coxph(Surv(times) ~ tx, data=long, subset=event==2)
coef(mod2)

# Transform long data & create contrasts terms
# create tx_ev1 & tx_ev2 to estimate effects of TX on event 1 & on event 2
long2 <- long
long2$event <- long$event-1
long2$tx_ev1 <- long2$tx*(1-long2$event)
long2$tx_ev2 <- long2$tx*(long2$event)

# mod1 corresponds to separate analysis of each time-to-event trait
modsep <- coxph(Surv(times)~tx_ev1 + tx_ev2 + strata(event), data=long2)
coef(modsep) # should match exactly Chris's results for mod1 & mod2
coef(mod1) # for verification
coef(mod2) # for verification

# Frailty model for time-to-event1 & time-to-event2 (frailty term captures
unexplained dependency)
modf <- coxph(Surv(times)~ tx_ev1 + tx_ev2  + strata(event) + frailty(id,
distribution="gamma"), data=long2)
coef(modf) # should match results of coef(mod)

#### for comparisons
# Corresponds to full model (includes simulated gamma as covariate, for
comparisons)
modc <- coxph(Surv(times)~tx_ev1 + tx_ev2 + gamma + strata(event) ,
data=long2) # corresponds to "true" model
coef(modc)

# Includes frailty term + gamma as a covariate (for comparisons)
modcf <- coxph(Surv(times)~tx_ev1 + tx_ev2 + gamma + strata(event) +
frailty(id, distribution="gamma") , data=long2) # corresponds to "true"
model
coef(modcf)

res <- rbind(res,list(coef(modc), coef(modf),coef(modcf),coef(modsep)))
}


pdf(paste("Boxplot.beta.event1.event2.tau",tau,".pdf",sep=""))
par(mfrow=c(2,4))
#boxplot for beta of Tx on event1
beta.tx.ev1.TM <- unlist(lapply(res[,1],function(x) {x[1] }))
boxplot(beta.tx.ev1.TM, main="Complete Model",ylab="beta of TX on event
1",col="grey")
abline(h=beta[1],col="red")
mean(beta.tx.ev1.TM)- beta[1] #bias

beta.tx.ev1.FM <-unlist(lapply(res[,2],function(x) {x[1] }))
boxplot(beta.tx.ev1.FM, main="Frailty Model",ylab="beta of TX on event
1",col="grey")
abline(h=beta[1],col="red")
mean(beta.tx.ev1.FM)- beta[1] #bias

beta.tx.ev1.FGM <-unlist(lapply(res[,3],function(x) {x[1] }))
boxplot(beta.tx.ev1.FGM, main="Frailty Model with sim. gamma
covariate",ylab="beta of TX on event 1",col="grey")
abline(h=beta[1],col="red")
mean(beta.tx.ev1.FGM)- beta[1] #bias

beta.tx.ev1.SM <- unlist(lapply(res[,4],function(x) {x[1] }))
boxplot(beta.tx.ev1.SM,main="Separate Models",ylab="beta of TX on event
1",col="grey")
abline(h=beta[1],col="red")
mean(beta.tx.ev1.SM)- beta[1] #bias

#beta of TX on event 2
beta.tx.ev2.TM <- unlist(lapply(res[,1],function(x) {x[2] }))
boxplot(beta.tx.ev2.TM,main="Complete Model",ylab="beta of TX on event
2",col="grey")
abline(h=beta[2],col="red")
mean(beta.tx.ev2.TM)- beta[2] #bias

beta.tx.ev2.FM <-unlist(lapply(res[,2],function(x) {x[2] }))
boxplot(beta.tx.ev2.FM,main="Frailty Model",ylab="beta of TX on event
2",col="grey")
abline(h=beta[2],col="red")
mean(beta.tx.ev2.FM)- beta[2] #bias

beta.tx.ev2.FGM <-unlist(lapply(res[,3],function(x) {x[2] }))
boxplot(beta.tx.ev2.FGM,main="Frailty Model with sim. gamma
covariate",ylab="beta of TX on event 2",col="grey")
abline(h=beta[2],col="red")
mean(beta.tx.ev2.FGM)- beta[2] #bias

beta.tx.ev2.SM <- unlist(lapply(res[,3],function(x) {x[2] }))
boxplot(beta.tx.ev2.SM,main="Separate Models",ylab="beta of TX on event
2",col="grey")
abline(h=beta[2],col="red")
mean(beta.tx.ev2.SM)- beta[2] #bias
dev.off()

# Print summary for the last data replicate
summary(modc)
summary(modf)
summary(modcf)
summary(modsep)







Le lun. 29 juil. 2019 ? 12:00, Andrews, Chris <chrisaa at med.umich.edu> a
?crit :

> Denise,
>
> Interacting a fixed effect with a strata variable is not uncommon.  No
> main effect of the strata variable is possible but the interaction term
> allows the fixed effect variable to have different values in different
> strata.  I've more often seen it coded as a direct interaction:
>
> coxph( Surv(Time, Status) ~ treatment*strata(Event_type) + frailty(ID),
> data=example)
>
> (e.g., page 47 of Therneau and Grambsch)
>
> I don't see anything inherently wrong with your interpretation of the
> model.  While it seems that the assumption of no effect of one event on the
> other is very strong, I don't know the context of your analysis. I
> visualized it as a 4-state model.  Everyone starts in 0 ("neither event").
> There are hazards of moving from state 0 to state 1 (lambda01(t)) and from
> state 0 to state 2 (lambda02(t)).  The last state is "both events".  There
> are hazards of moving from state 1 to state 3 (lambda13(t), which you are
> assuming is identical to lambda02(t)) and from state 2 to state 3
> (lambda23(t), which you are assuming is identical to lambda01(t)).
>
> I did not observe much gain from the frailty term (unmeasured covariate)
> with only 2 events in the short simulation I tried (except when the effect
> was very strong and I got convergence warnings).
>
> Chris
>
>
> library("flexsurv")
>
> set.seed(20190729)
>
> # Multiple non-competing outcomes, connected only by frailty (unmeasured
> covariate)
>
> nn <- 1000
> kk <- 2
>
> # frailty, 1 per individual
> # variance of random effect
> #tau <- 0.01^2
> tau <- 0.1^2
> #tau <- 0.4^2
> #tau <- 1^2
> gamma <- rgamma(nn, shape=1/tau, scale=tau) # mean is 1
> hist(gamma)
> sd(gamma) # ~ tau
>
> # covariate
> tx <- sample(0:1, nn, replace = TRUE)
> # covariate effect on log hazard
> beta <- 1
> # might want to allow different treatment effects for different events
> beta <- seq(kk)/kk
>
> short <- data.frame(id=seq(nn), tx=tx, gamma=gamma)
>
> # survival times, kk per individual
> # tt <- rweibullPH(kk*nn, shape=2, scale=exp(beta*tx)*gamma)
> # hist(tt)
> # might want to allow different shapes or scales for different events
> for (k in seq(kk)) {
>         short[[paste0("time", k)]] <- rweibullPH(nn, shape=2,
> scale=exp(beta[k]*tx)*gamma)
> }
> # might want to allow censoring
>
> long <- reshape(short, direction="long", varying = paste0("time",
> seq(kk)), v.names="times", timevar="event", idvar="id", times=seq(kk))
> long <- long[order(long$id, long$event),]
>
> mod <- coxph(Surv(times) ~ tx * strata(event) + frailty(id), data=long)
> #summary(mod)
>
> mod0 <- coxph(Surv(times) ~ tx * strata(event), data=long)
> #summary(mod0)
>
> mod1 <- coxph(Surv(times) ~ tx, data=long, subset=event==1)
> #summary(mod1)
>
> mod2 <- coxph(Surv(times) ~ tx, data=long, subset=event==2)
> #summary(mod2)
>
> coef(mod)
> coef(mod0)
> coef(mod1)
> coef(mod2) - coef(mod1)
>
> coef(summary(mod))
> coef(summary(mod0))
> coef(summary(mod1))
>
>
>
> -----Original Message-----
> From: David Winsemius [mailto:dwinsemius at comcast.net]
> Sent: Sunday, July 28, 2019 2:03 AM
> To: r-help at r-project.org
> Subject: Re: [R] CoxPH multivariate frailty model with coxph (survival)
>
>
> On 7/19/19 10:19 AM, Denise b wrote:
> >   Dear R users,
> >
> > I am interested in estimating the effects of a treatment on two
> > time-to-event traits (on simulated data), accounting for the dependency
> > between the two time-to-event outcomes.
> >
> > I precise that the events are NOT recurrent, NOT competitive, NOT
> ordered.
> > The individuals are NOT related and can have 0, 1 or the 2 events (in any
> > ordered).
> >
> > So, I specified a time-to-event model with one Cox PH hazard function for
> > each outcome.
> > The two hazard functions are linked by a common subject-specific frailty
> > term (gamma distribution) to account for the dependency. The likelihood
> > function of that model would include two risks sets (1 for eachoutcome)
> > connected via the shared frailty term.
> >
> > To fit that model, I used the coxph function (survival R package, T.
> > Thernaud):
> > coxph( Surv(Time, Status) ~ treatment*Event_type + strata(Event_type) +
> > frailty(ID), data=example)
> If you have Event_type as a strata variable, it seems problematic that
> it be also interacting as a fixed effect.
> > - where example is my dataset, with 2*N individuals (2 rows for each
> > individual, corresponding to each time-to-event outcome)
> > - Time = c(Time_outcome1, Time_outcome2)
> How exactly do you expect the Surv-function to handle such a vector?
> > - Status=c(Event_outcome1, Event_outcome2)
>
> The help page for Surv() says:  "For multiple endpoint data the event
> variable will be a factor, whose first level is treated as censoring."
>
> So it, too, would not be "expecting" a two-item vector. Seems that the
> multi-state formulation would make more sense.
>
> > - Event_type=c(rep(0,length(Time_outcome1)),rep(1,length(Time_outcome2))
>
>
> I'm reasonably sure 0 would be interpreted as a censored observation.
>
> > - ID=c(ID,ID)
> >
> > So, the model implies different baseline hazard function for each outcome
> > (argument strata) and estimate a treatment effect for each event type(
> > treatment*Event_type).
>
>
> I doubt it. I'm pretty sure there would be a conflation of effects and
> stratification.
>
> >
> > Although the model returns some estimates close to what I expected
> > (simulation study), the problem is that most of the examples I found with
> > this type of formulation are for competitive time-to-event models (as
> > presented in Therneau's and Grambush's book "Modeling survival
> > data"  (pages 240-260 and 175-185) and also in some info I found on
>
>
> I cannot lay my hands on my copy of that text, but I'm fairly sure it
> never mixed strata(.) and fixed effects for a variable in the same model.
>
>
> I suppose this is where I should admit  that I'm trained as an
> epidemiologist and not as a statistician, but I've done a fair amount of
> work with these models. Seems to me that your code are sufficiently out
> of the mainstream practice that you would at least want to create a
> simulated data-set and see if this results in agreement with assumptions.
>
>
> --
>
> David.
>
> > internet).
> >
> > So, I am wondering if some of you have the same or different
> interpretation
> > of mime.
> > Otherwise do you have other functions to recommend me to fit the desired
> > model, such as coxme...? I also checked the package " mets" which also
> > describes several examples for competitive time-to-event traits, but not
> > for NON competitive events.
> >
> > Thanks in advance for your help,
> > Denise
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
> **********************************************************
> Electronic Mail is not secure, may not be read every day, and should not
> be used for urgent or sensitive issues
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Thu Aug  8 16:34:39 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 8 Aug 2019 07:34:39 -0700
Subject: [R] Error exporting dataframe from Julia to R with Feather
In-Reply-To: <CAMk+s2RVUKu0Z-oNnwbsiij5v7tKzt25HbonNO0rYuq=Okez=g@mail.gmail.com>
References: <CAMk+s2RVUKu0Z-oNnwbsiij5v7tKzt25HbonNO0rYuq=Okez=g@mail.gmail.com>
Message-ID: <CAGxFJbTuxAvj8Pjp6zxZ0ap6z=+h695_TMOd=vt+Gn5gLEwFdw@mail.gmail.com>

You may have to contact RStudio about this. RStudio is a separate IDE
independent of R -- i.e. developed and maintained by a separate
organization from the R project.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Aug 8, 2019 at 7:08 AM Luigi Marongiu <marongiu.luigi at gmail.com>
wrote:

> Hello,
>
> since I am encountering a lot of problems exporting dataframes from
> julia to R (there is always something wrong with the formatting,
> probably a missing quote) so I am trying to use Feather to do the job.
>
> I have installed Feather in Julia with `pkg.add("Feather")` and
> imported it with `using Feather`. I created a dataframe and saved it
> with `Feather.write("/dir/dataframe.feather", df)` and it worked. I
> can even open it back with `df =
> Feather.read("/dir/dataframe.feather")` and get: `julia> nrow(df)
> 128544`.
> The problem is with R. I am using Rstudio to test each step.
> I installed the package, imported it with `library(feather)` and used it
> as:
> ```
> df = read_feather("/dir/dataframe.feather")
> ```
> and then Rstudio simply crashes.
> any idea why?
> Thank you
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Aug  8 16:57:13 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 08 Aug 2019 07:57:13 -0700
Subject: [R] gen.geneaslogy
In-Reply-To: <CANVQ5Q2F0WJLvkb79E32vV+4Dntrd8C7qxFc4AOui-TOz1s4zA@mail.gmail.com>
References: <CANVQ5Q2F0WJLvkb79E32vV+4Dntrd8C7qxFc4AOui-TOz1s4zA@mail.gmail.com>
Message-ID: <F6CDC10F-749F-4482-89F2-D8E27E46FBE6@dcn.davis.ca.us>

This is a rather specialized issue... you probably should be cc'ing the package maintainer as added here.

I have never used this package.. but perhaps the fact that 701 is female yet listed as a father of 801 and 802 could be causing problems. (If so, this may raise issues of family structure flexibility, though it could be appropriate for genetics studies.)

On August 8, 2019 7:11:50 AM PDT, Thomas Bolger <tom.bolger at ucd.ie> wrote:
>Hi
>
>I have just started to do some analysis of genealogies and seem to be
>doing
>something wrong when using gen.genealogy. The following is the script
>and
>output that I used. Any help greatfully apprciated
>
>library(GENLIB)
>> library(ggenealogy)
>> library(igraph)
>> #
>> #
>> ###Data input as data frame
>>
>ind<-c(501,502,601,603,605,608,701,702,703,704,705,706,707,708,709,710,801,802)
>>
>father<-c(401,411,501,501,501,501,601,601,601,603,603,603,605,605,608,608,701,701)
>>
>mother<-c(402,412,502,502,502,502,602,602,602,604,604,604,606,607,609,609,711,711)
>> sex<-c(1,2,1,1,1,1,2,2,1,2,1,2,1,1,1,1,1,2)
>> gen.df<-data.frame(ind, father, mother, sex)
>> #print data to check
>> print (gen.df)
>   ind father mother sex
>1  501    401    402   1
>2  502    411    412   2
>3  601    501    502   1
>4  603    501    502   1
>5  605    501    502   1
>6  608    501    502   1
>7  701    601    602   2
>8  702    601    602   2
>9  703    601    602   1
>10 704    603    604   2
>11 705    603    604   1
>12 706    603    604   2
>13 707    605    606   1
>14 708    605    607   1
>15 709    608    609   1
>16 710    608    609   1
>17 801    701    711   1
>18 802    701    711   2
>> #
>> #
>> genex<-gen.genealogy(gen.df)
>Error in gen.genealogy(gen.df) :
> Invalid 'gen' parameter: identical individual number for both 'father'
>and 'mother'
>> #
>>

-- 
Sent from my phone. Please excuse my brevity.


From pd@|gd @end|ng |rom gm@||@com  Thu Aug  8 17:29:54 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Thu, 8 Aug 2019 17:29:54 +0200
Subject: [R] Error exporting dataframe from Julia to R with Feather
In-Reply-To: <CAGxFJbTuxAvj8Pjp6zxZ0ap6z=+h695_TMOd=vt+Gn5gLEwFdw@mail.gmail.com>
References: <CAMk+s2RVUKu0Z-oNnwbsiij5v7tKzt25HbonNO0rYuq=Okez=g@mail.gmail.com>
 <CAGxFJbTuxAvj8Pjp6zxZ0ap6z=+h695_TMOd=vt+Gn5gLEwFdw@mail.gmail.com>
Message-ID: <3149BCC3-ECE7-4D3C-B799-FF2074292D3E@gmail.com>

Alternatively, try running your example from plain R (in a terminal, R.app, or Rgui, depending on your platform), and see if the problem occurs without RStudio in the equation. If it does, then the feather package probably owns the problem.

-pd

> On 8 Aug 2019, at 16:34 , Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> You may have to contact RStudio about this. RStudio is a separate IDE
> independent of R -- i.e. developed and maintained by a separate
> organization from the R project.
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
> On Thu, Aug 8, 2019 at 7:08 AM Luigi Marongiu <marongiu.luigi at gmail.com>
> wrote:
> 
>> Hello,
>> 
>> since I am encountering a lot of problems exporting dataframes from
>> julia to R (there is always something wrong with the formatting,
>> probably a missing quote) so I am trying to use Feather to do the job.
>> 
>> I have installed Feather in Julia with `pkg.add("Feather")` and
>> imported it with `using Feather`. I created a dataframe and saved it
>> with `Feather.write("/dir/dataframe.feather", df)` and it worked. I
>> can even open it back with `df =
>> Feather.read("/dir/dataframe.feather")` and get: `julia> nrow(df)
>> 128544`.
>> The problem is with R. I am using Rstudio to test each step.
>> I installed the package, imported it with `library(feather)` and used it
>> as:
>> ```
>> df = read_feather("/dir/dataframe.feather")
>> ```
>> and then Rstudio simply crashes.
>> any idea why?
>> Thank you
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From bgunter@4567 @end|ng |rom gm@||@com  Thu Aug  8 17:35:49 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 8 Aug 2019 08:35:49 -0700
Subject: [R] Error exporting dataframe from Julia to R with Feather
In-Reply-To: <3149BCC3-ECE7-4D3C-B799-FF2074292D3E@gmail.com>
References: <CAMk+s2RVUKu0Z-oNnwbsiij5v7tKzt25HbonNO0rYuq=Okez=g@mail.gmail.com>
 <CAGxFJbTuxAvj8Pjp6zxZ0ap6z=+h695_TMOd=vt+Gn5gLEwFdw@mail.gmail.com>
 <3149BCC3-ECE7-4D3C-B799-FF2074292D3E@gmail.com>
Message-ID: <CAGxFJbSLOmDCq1z8faUdOPK80mPFrAXuDuBs-Lh5RwUWrmNMLg@mail.gmail.com>

That is a better path, I agree.

However, I suspect that RStudio would still like to know about the issue,
even *if* feather/R is what crashes. They probably do not want RStudio to
crash even so.

-- Bert

On Thu, Aug 8, 2019 at 8:29 AM peter dalgaard <pdalgd at gmail.com> wrote:

> Alternatively, try running your example from plain R (in a terminal,
> R.app, or Rgui, depending on your platform), and see if the problem occurs
> without RStudio in the equation. If it does, then the feather package
> probably owns the problem.
>
> -pd
>
> > On 8 Aug 2019, at 16:34 , Bert Gunter <bgunter.4567 at gmail.com> wrote:
> >
> > You may have to contact RStudio about this. RStudio is a separate IDE
> > independent of R -- i.e. developed and maintained by a separate
> > organization from the R project.
> >
> > Bert Gunter
> >
> > "The trouble with having an open mind is that people keep coming along
> and
> > sticking things into it."
> > -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >
> >
> > On Thu, Aug 8, 2019 at 7:08 AM Luigi Marongiu <marongiu.luigi at gmail.com>
> > wrote:
> >
> >> Hello,
> >>
> >> since I am encountering a lot of problems exporting dataframes from
> >> julia to R (there is always something wrong with the formatting,
> >> probably a missing quote) so I am trying to use Feather to do the job.
> >>
> >> I have installed Feather in Julia with `pkg.add("Feather")` and
> >> imported it with `using Feather`. I created a dataframe and saved it
> >> with `Feather.write("/dir/dataframe.feather", df)` and it worked. I
> >> can even open it back with `df =
> >> Feather.read("/dir/dataframe.feather")` and get: `julia> nrow(df)
> >> 128544`.
> >> The problem is with R. I am using Rstudio to test each step.
> >> I installed the package, imported it with `library(feather)` and used it
> >> as:
> >> ```
> >> df = read_feather("/dir/dataframe.feather")
> >> ```
> >> and then Rstudio simply crashes.
> >> any idea why?
> >> Thank you
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From bog@@o@chr|@to|er @end|ng |rom gm@||@com  Thu Aug  8 17:43:38 2019
From: bog@@o@chr|@to|er @end|ng |rom gm@||@com (Christofer Bogaso)
Date: Thu, 8 Aug 2019 21:13:38 +0530
Subject: [R] Extract row as NA with no matching name
Message-ID: <CA+dpOJ=XXddrrGjGs58WdaEJnUAYADm-ZoLyCojwFWcnTmA51Q@mail.gmail.com>

Hi,

Let say I have below matrix

mdat <- matrix(c(1,2,3, 11,12,13), nrow = 2, ncol = 3, byrow = TRUE,
               dimnames = list(c("row1", "row2"),
                               c("C.1", "C.2", "C.3")))


Now I can extract a raw by rowname as

> mdat['row1', ]

C.1 C.2 C.3

  1   2   3


However I am also looking for was to extract values as NA when a
rowname is supplied which is not existing rownames

I should get

> mdat['new_raw', ]

C.1 C.2 C.3

  NA   NA   NA


Current it throws error as default functionality. Is there any way to
force R to provide values as NA instead of showing any errore?

	[[alternative HTML version deleted]]


From therne@u @end|ng |rom m@yo@edu  Thu Aug  8 18:17:38 2019
From: therne@u @end|ng |rom m@yo@edu (Therneau, Terry M., Ph.D.)
Date: Thu, 08 Aug 2019 11:17:38 -0500
Subject: [R] conflicting results for a time-varying coefficient in a Cox
 model
In-Reply-To: <1697509011.20190808160746@medstat.hu>
References: <1697509011.20190808160746@medstat.hu>
Message-ID: <771925$c6ifvb@ironport10.mayo.edu>

This is an excellent question.
The answer, in this particular case, mostly has to do with the outlier time values.? (I've 
never been convinced that the death at time 999 isn't really a misplaced code for 
"missing", actually).??? If you change the knots used by the spline you can get quite 
different values.
For instance, using a smaller data set:

fit1 <- coxph(Surv(tstart, time, status) ~ trt + prior + karno, veteran)
zph1 <- cox.zph(fit1, transform='identity')
plot(zph1[3])

dtime <- unique(veteran$time[veteran$status ==1])??? # all of the death times
veteran2 <- survSplit( Surv(time, status) ~ ., data=veteran, cut=dtime)
fit2 <- coxph(Surv(tstart, time, status) ~ trt + prior + karno +
 ??????? karno:ns(time, df=4),? data=veteran2)
tx <- 0:100 * 10??? # x positions for plot
ncall <- attr(terms(fit2), "predvars")[[6]]
ty <- eval(ncall, data.frame(time = tx)) %*% coef(fit2)[4:7] + coef(fit2)[3]
lines(tx, ty, col=2)

-------------

Now it looks even worse!? The only difference is that the ns() function has chosen a 
different set of knots.

The test used by the cox.zph function is based on a score test and is solid.? The plot 
that it produces uses a smoothed approximation to the variance matrix and is approximate.? 
So the diagnostic plot will never exactly match an actual fit.?? In this data set the 
outliers exacerbate the issue.? To see this try a different time scale.

------------
zph2 <- cox.zph(fit1, transform= sqrt)
plot(zph2[3])
veteran2$stime <- sqrt(veteran2$time)
fit3 <- coxph(Surv(tstart, time, status) ~ trt + prior + karno +
 ?????????? karno:ns(stime, df=4),? data=veteran2)

ncall3 <-attr(terms(fit3), "predvars")[[6]]
ty3 <- eval(ncall3, data.frame(stime= sqrt(tx))) %*% coef(fit3)[4:7] + coef(fit3)[3]
lines(sqrt(tx), ty3, col=2)

----------------

The right tail is now better behaved.?? Eliminating the points >900 makes things even 
better behaved.

Terry T.




On 8/8/19 9:07 AM, Ferenci Tamas wrote:
> I was thinking of two possible ways to
> plot a time-varying coefficient in a Cox model.
>
> One is simply to use survival::plot.cox.zph which directly produces a
> beta(t) vs t diagram.
>
> The other is to transform the dataset to counting process format and
> manually include an interaction with time, expanded with spline (to be
> similar to plot.cox.zph). Plotting the coefficient produces the needed
> beta(t) vs t diagram.
>
> I understand that they're slightly different approaches, so I don't
> expect totally identical results, but nevertheless, they approximate
> the very same thing, so I do expect that the results are more or less
> similar.
>
> However:
>
> library( survival )
> library( splines )
>
> data( veteran )
>
> zp <- cox.zph( coxph(Surv(time, status) ~ trt + prior + karno,
>                       data = veteran ), transform = "identity" )[ 3 ]
>
> veteran3 <- survSplit( Surv(time, status) ~ trt + prior + karno,
>                         data = veteran, cut = 1:max(veteran$time) )
>
> fit <- coxph(Surv(tstart,time, status) ~ trt + prior + karno +
>                 karno:ns( time, df = 4 ), data = veteran3 )
> cf <- coef( fit )
> nsvet <- ns( veteran3$time, df = 4 )
>
> plot( zp )
> lines( 0:1000, ns( 0:1000, df = 4, knots = attr( nsvet, "knots" ),
>                     Boundary.knots = attr( nsvet, "Boundary.knots" ) )%*%cf[
>                       grep( "karno:ns", names( cf ) ) ] + cf["karno"],
>         type = "l", col = "red" )
>
> Where is the mistake? Something must be going on here, because the
> plots are vastly different...
>
> Thank you very much in advance,
> Tamas


	[[alternative HTML version deleted]]


From tom@bo|ger @end|ng |rom ucd@|e  Thu Aug  8 18:29:04 2019
From: tom@bo|ger @end|ng |rom ucd@|e (Thomas Bolger)
Date: Thu, 8 Aug 2019 17:29:04 +0100
Subject: [R] gen.geneaslogy
In-Reply-To: <F6CDC10F-749F-4482-89F2-D8E27E46FBE6@dcn.davis.ca.us>
References: <CANVQ5Q2F0WJLvkb79E32vV+4Dntrd8C7qxFc4AOui-TOz1s4zA@mail.gmail.com>
 <F6CDC10F-749F-4482-89F2-D8E27E46FBE6@dcn.davis.ca.us>
Message-ID: <CANVQ5Q3cPM49Q2S02jK4b28zY3tKwLng2wG7iCUcj66i2qcFYA@mail.gmail.com>

Hi

Many thanks for your very quick response. I should have spotted the error
in the data. I have corrected it but the error remains. Hopefully. the
package maintainer will be able to help.

Thank you very much for your reply.

Tom

On Thu, 8 Aug 2019 at 15:57, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> This is a rather specialized issue... you probably should be cc'ing the
> package maintainer as added here.
>
> I have never used this package.. but perhaps the fact that 701 is female
> yet listed as a father of 801 and 802 could be causing problems. (If so,
> this may raise issues of family structure flexibility, though it could be
> appropriate for genetics studies.)
>
> On August 8, 2019 7:11:50 AM PDT, Thomas Bolger <tom.bolger at ucd.ie> wrote:
> >Hi
> >
> >I have just started to do some analysis of genealogies and seem to be
> >doing
> >something wrong when using gen.genealogy. The following is the script
> >and
> >output that I used. Any help greatfully apprciated
> >
> >library(GENLIB)
> >> library(ggenealogy)
> >> library(igraph)
> >> #
> >> #
> >> ###Data input as data frame
> >>
>
> >ind<-c(501,502,601,603,605,608,701,702,703,704,705,706,707,708,709,710,801,802)
> >>
>
> >father<-c(401,411,501,501,501,501,601,601,601,603,603,603,605,605,608,608,701,701)
> >>
>
> >mother<-c(402,412,502,502,502,502,602,602,602,604,604,604,606,607,609,609,711,711)
> >> sex<-c(1,2,1,1,1,1,2,2,1,2,1,2,1,1,1,1,1,2)
> >> gen.df<-data.frame(ind, father, mother, sex)
> >> #print data to check
> >> print (gen.df)
> >   ind father mother sex
> >1  501    401    402   1
> >2  502    411    412   2
> >3  601    501    502   1
> >4  603    501    502   1
> >5  605    501    502   1
> >6  608    501    502   1
> >7  701    601    602   2
> >8  702    601    602   2
> >9  703    601    602   1
> >10 704    603    604   2
> >11 705    603    604   1
> >12 706    603    604   2
> >13 707    605    606   1
> >14 708    605    607   1
> >15 709    608    609   1
> >16 710    608    609   1
> >17 801    701    711   1
> >18 802    701    711   2
> >> #
> >> #
> >> genex<-gen.genealogy(gen.df)
> >Error in gen.genealogy(gen.df) :
> > Invalid 'gen' parameter: identical individual number for both 'father'
> >and 'mother'
> >> #
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>


-- 
Thomas Bolger
Emeritus Full Professor of Zoology


UCD School of Biology and Environmental Science
Belfield
Dublin 4
Ireland

Telephone : +353-1-7162330

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Aug  8 19:28:07 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 08 Aug 2019 10:28:07 -0700
Subject: [R] Error exporting dataframe from Julia to R with Feather
In-Reply-To: <CAGxFJbSLOmDCq1z8faUdOPK80mPFrAXuDuBs-Lh5RwUWrmNMLg@mail.gmail.com>
References: <CAMk+s2RVUKu0Z-oNnwbsiij5v7tKzt25HbonNO0rYuq=Okez=g@mail.gmail.com>
 <CAGxFJbTuxAvj8Pjp6zxZ0ap6z=+h695_TMOd=vt+Gn5gLEwFdw@mail.gmail.com>
 <3149BCC3-ECE7-4D3C-B799-FF2074292D3E@gmail.com>
 <CAGxFJbSLOmDCq1z8faUdOPK80mPFrAXuDuBs-Lh5RwUWrmNMLg@mail.gmail.com>
Message-ID: <48941177-2DD7-4B75-B81D-C9407249B380@dcn.davis.ca.us>

If R crashes, RStudio typically also crashes... so not necessarily news to them.

I will say that reproducible examples are nearly always necessary in cases like this to obtain meaningful answers from anyone, and the output of sessionInfo() is also usually needed.

On August 8, 2019 8:35:49 AM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>That is a better path, I agree.
>
>However, I suspect that RStudio would still like to know about the
>issue,
>even *if* feather/R is what crashes. They probably do not want RStudio
>to
>crash even so.
>
>-- Bert
>
>On Thu, Aug 8, 2019 at 8:29 AM peter dalgaard <pdalgd at gmail.com> wrote:
>
>> Alternatively, try running your example from plain R (in a terminal,
>> R.app, or Rgui, depending on your platform), and see if the problem
>occurs
>> without RStudio in the equation. If it does, then the feather package
>> probably owns the problem.
>>
>> -pd
>>
>> > On 8 Aug 2019, at 16:34 , Bert Gunter <bgunter.4567 at gmail.com>
>wrote:
>> >
>> > You may have to contact RStudio about this. RStudio is a separate
>IDE
>> > independent of R -- i.e. developed and maintained by a separate
>> > organization from the R project.
>> >
>> > Bert Gunter
>> >
>> > "The trouble with having an open mind is that people keep coming
>along
>> and
>> > sticking things into it."
>> > -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>> >
>> >
>> > On Thu, Aug 8, 2019 at 7:08 AM Luigi Marongiu
><marongiu.luigi at gmail.com>
>> > wrote:
>> >
>> >> Hello,
>> >>
>> >> since I am encountering a lot of problems exporting dataframes
>from
>> >> julia to R (there is always something wrong with the formatting,
>> >> probably a missing quote) so I am trying to use Feather to do the
>job.
>> >>
>> >> I have installed Feather in Julia with `pkg.add("Feather")` and
>> >> imported it with `using Feather`. I created a dataframe and saved
>it
>> >> with `Feather.write("/dir/dataframe.feather", df)` and it worked.
>I
>> >> can even open it back with `df =
>> >> Feather.read("/dir/dataframe.feather")` and get: `julia> nrow(df)
>> >> 128544`.
>> >> The problem is with R. I am using Rstudio to test each step.
>> >> I installed the package, imported it with `library(feather)` and
>used it
>> >> as:
>> >> ```
>> >> df = read_feather("/dir/dataframe.feather")
>> >> ```
>> >> and then Rstudio simply crashes.
>> >> any idea why?
>> >> Thank you
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From bgunter@4567 @end|ng |rom gm@||@com  Thu Aug  8 19:41:49 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 8 Aug 2019 10:41:49 -0700
Subject: [R] Error exporting dataframe from Julia to R with Feather
In-Reply-To: <48941177-2DD7-4B75-B81D-C9407249B380@dcn.davis.ca.us>
References: <CAMk+s2RVUKu0Z-oNnwbsiij5v7tKzt25HbonNO0rYuq=Okez=g@mail.gmail.com>
 <CAGxFJbTuxAvj8Pjp6zxZ0ap6z=+h695_TMOd=vt+Gn5gLEwFdw@mail.gmail.com>
 <3149BCC3-ECE7-4D3C-B799-FF2074292D3E@gmail.com>
 <CAGxFJbSLOmDCq1z8faUdOPK80mPFrAXuDuBs-Lh5RwUWrmNMLg@mail.gmail.com>
 <48941177-2DD7-4B75-B81D-C9407249B380@dcn.davis.ca.us>
Message-ID: <CAGxFJbQRhzHhNJ5ejs658_XdrMbGmSUhrw9Gh_ueCBvf0MbQMQ@mail.gmail.com>

Jeff:
Fair enough. As I have no data (no experience with crashes), I am happy to
defer to those who do.

Cheers,
Bert

On Thu, Aug 8, 2019 at 10:28 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> If R crashes, RStudio typically also crashes... so not necessarily news to
> them.
>
> I will say that reproducible examples are nearly always necessary in cases
> like this to obtain meaningful answers from anyone, and the output of
> sessionInfo() is also usually needed.
>
> On August 8, 2019 8:35:49 AM PDT, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
> >That is a better path, I agree.
> >
> >However, I suspect that RStudio would still like to know about the
> >issue,
> >even *if* feather/R is what crashes. They probably do not want RStudio
> >to
> >crash even so.
> >
> >-- Bert
> >
> >On Thu, Aug 8, 2019 at 8:29 AM peter dalgaard <pdalgd at gmail.com> wrote:
> >
> >> Alternatively, try running your example from plain R (in a terminal,
> >> R.app, or Rgui, depending on your platform), and see if the problem
> >occurs
> >> without RStudio in the equation. If it does, then the feather package
> >> probably owns the problem.
> >>
> >> -pd
> >>
> >> > On 8 Aug 2019, at 16:34 , Bert Gunter <bgunter.4567 at gmail.com>
> >wrote:
> >> >
> >> > You may have to contact RStudio about this. RStudio is a separate
> >IDE
> >> > independent of R -- i.e. developed and maintained by a separate
> >> > organization from the R project.
> >> >
> >> > Bert Gunter
> >> >
> >> > "The trouble with having an open mind is that people keep coming
> >along
> >> and
> >> > sticking things into it."
> >> > -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >> >
> >> >
> >> > On Thu, Aug 8, 2019 at 7:08 AM Luigi Marongiu
> ><marongiu.luigi at gmail.com>
> >> > wrote:
> >> >
> >> >> Hello,
> >> >>
> >> >> since I am encountering a lot of problems exporting dataframes
> >from
> >> >> julia to R (there is always something wrong with the formatting,
> >> >> probably a missing quote) so I am trying to use Feather to do the
> >job.
> >> >>
> >> >> I have installed Feather in Julia with `pkg.add("Feather")` and
> >> >> imported it with `using Feather`. I created a dataframe and saved
> >it
> >> >> with `Feather.write("/dir/dataframe.feather", df)` and it worked.
> >I
> >> >> can even open it back with `df =
> >> >> Feather.read("/dir/dataframe.feather")` and get: `julia> nrow(df)
> >> >> 128544`.
> >> >> The problem is with R. I am using Rstudio to test each step.
> >> >> I installed the package, imported it with `library(feather)` and
> >used it
> >> >> as:
> >> >> ```
> >> >> df = read_feather("/dir/dataframe.feather")
> >> >> ```
> >> >> and then Rstudio simply crashes.
> >> >> any idea why?
> >> >> Thank you
> >> >>
> >> >> ______________________________________________
> >> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> >> PLEASE do read the posting guide
> >> >> http://www.R-project.org/posting-guide.html
> >> >> and provide commented, minimal, self-contained, reproducible code.
> >> >>
> >> >
> >> >       [[alternative HTML version deleted]]
> >> >
> >> > ______________________________________________
> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible code.
> >>
> >> --
> >> Peter Dalgaard, Professor,
> >> Center for Statistics, Copenhagen Business School
> >> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> >> Phone: (+45)38153501
> >> Office: A 4.23
> >> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Thu Aug  8 19:49:41 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Thu, 8 Aug 2019 12:49:41 -0500
Subject: [R] how to reverse colors on boxplot
Message-ID: <CAF9-5jMq39De88Xa=BYD=kzSgrKYnmWL-TqJ5fNbO5AH-a2m6Q@mail.gmail.com>

Hello,

I made plot in attach using:

boxplot(flcn_M~subject,data=dx,col =
c("royalblue1","palevioletred1"),xlab="subjects",ylab="Expression
estimate in delta (log2)",boxwex = 0.2,frame.plot = FALSE)
stripchart(flcn_M~subject, vertical = TRUE, data = dx,method =
"jitter", add = TRUE,pch = 20, col=rgb(0,0,0,.5),jitter = 0.001)

How do I reverse colors so that PDR is shown in this pink and nDR in blue?

Thanks
Ana


From chr|@ho|d @end|ng |rom p@yctc@org  Thu Aug  8 19:54:41 2019
From: chr|@ho|d @end|ng |rom p@yctc@org (Chris Evans)
Date: Thu, 8 Aug 2019 18:54:41 +0100 (BST)
Subject: [R] Creating a web site checker using R
Message-ID: <513562968.2534827.1565286881081.JavaMail.zimbra@psyctc.org>

I use R a great deal but the huge web crawling power of it isn't an area I've used. I don't want to reinvent a cyberwheel and I suspect someone has done what I want.  That is a program that would run once a day (easy for me to set up as a cron task) and would crawl a single root of a web site (mine) and get the file size and a CRC or some similar check value for each page as pulled off the site (and, obviously, I'd want it not to follow off site links). The other key thing would be for it to store the values and URLs and be capable of being run in "create/update database" mode or in "check pages" mode and for the change mode run to Email me a warning if a page changes.  The reason I want this is that two of my sites have recently had content "disappear": neither I nor the ISP can see what's happened and we are lacking the very useful diagnostic of the date when the change happened which might have mapped it some component of WordPress, plugins or themes having updated.

I am failing to find anything such and all the services that offer site checking of this sort are prohibitively expensive for me (my sites are zero income and either personal or offering free utilities and information).

If anyone has done this, or something similar, I'd love to hear if you were willing to share it.  Failing that, I think I will have to create this but I know it will take me days as this isn't my area of R expertise and as, to be brutally honest, I'm a pretty poor programmer.  If I go that way, I'm sure people may be able to point me to things I may be (legitimately) able to recycle in parts to help construct this.

Thanks in advance,

Chris

-- 
Chris Evans <chris at psyctc.org> Skype: chris-psyctc
Visiting Professor, University of Sheffield <chris.evans at sheffield.ac.uk>
I do some consultation work for the University of Roehampton <chris.evans at roehampton.ac.uk> and other places but this <chris at psyctc.org> remains my main Email address.
I have "semigrated" to France, see: https://www.psyctc.org/pelerinage2016/semigrating-to-france/ if you want to book to talk, I am trying to keep that to Thursdays and my diary is now available at: https://www.psyctc.org/pelerinage2016/ecwd_calendar/calendar/
Beware: French time, generally an hour ahead of UK.  That page will also take you to my blog which started with earlier joys in France and Spain!


From dw|n@em|u@ @end|ng |rom comc@@t@net  Thu Aug  8 21:10:37 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Thu, 8 Aug 2019 12:10:37 -0700
Subject: [R] Extract row as NA with no matching name
In-Reply-To: <CA+dpOJ=XXddrrGjGs58WdaEJnUAYADm-ZoLyCojwFWcnTmA51Q@mail.gmail.com>
References: <CA+dpOJ=XXddrrGjGs58WdaEJnUAYADm-ZoLyCojwFWcnTmA51Q@mail.gmail.com>
Message-ID: <21559b34-765e-c4d2-5630-6b811c54b462@comcast.net>

I don't know a clean way of delivering that result but if you use 
logical indexing you can get an empty matrix with three columns:


str( mdat["nope" %in% rownames(mdat), ] )
 ?num[0 , 1:3]
 ?- attr(*, "dimnames")=List of 2
 ? ..$ : NULL
 ? ..$ : chr [1:3] "C.1" "C.2" "C.3"

# it prints thus to the console

 ?mdat[FALSE,? ]
#???? C.1 C.2 C.3


If you have a vector, test_vec of possible matches you could use:


mdat[ rownames(mdat) %in% test_vec,? ]


**Yet again I am advising you to post in plain text. It's very easy to 
post in plain text from gmail. Please do so.**


-- 

David.

On 8/8/19 8:43 AM, Christofer Bogaso wrote:
> Hi,
>
> Let say I have below matrix
>
> mdat <- matrix(c(1,2,3, 11,12,13), nrow = 2, ncol = 3, byrow = TRUE,
>                 dimnames = list(c("row1", "row2"),
>                                 c("C.1", "C.2", "C.3")))
>
>
> Now I can extract a raw by rowname as
>
>> mdat['row1', ]
> C.1 C.2 C.3
>
>    1   2   3
>
>
> However I am also looking for was to extract values as NA when a
> rowname is supplied which is not existing rownames
>
> I should get
>
>> mdat['new_raw', ]
> C.1 C.2 C.3
>
>    NA   NA   NA
>
>
> Current it throws error as default functionality. Is there any way to
> force R to provide values as NA instead of showing any errore?
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Aug  8 21:14:59 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 8 Aug 2019 20:14:59 +0100
Subject: [R] how to reverse colors on boxplot
In-Reply-To: <CAF9-5jMq39De88Xa=BYD=kzSgrKYnmWL-TqJ5fNbO5AH-a2m6Q@mail.gmail.com>
References: <CAF9-5jMq39De88Xa=BYD=kzSgrKYnmWL-TqJ5fNbO5AH-a2m6Q@mail.gmail.com>
Message-ID: <c6007636-9c9f-c95f-f677-ca7adba9b22a@sapo.pt>

Hello,

Maybe I don't understand the question but isn't all you have to do is 
to, well, reverse the colors

col = c("palevioletred1", "royalblue1")

in the boxplot call?

boxplot(flcn_M ~ subject, data = dx,
         col = c("palevioletred1", "royalblue1"),
         xlab="subjects",
         ylab="Expression estimate in delta (log2)",
         boxwex = 0.2,
         frame.plot = FALSE)
stripchart(flcn_M ~ subject, vertical = TRUE,
            data = dx,
            method = "jitter",
            add = TRUE,
            pch = 20,
            col = rgb(0, 0, 0, 0.5),
            jitter = 0.001)


Hope this helps,

Rui Barradas

?s 18:49 de 08/08/19, Ana Marija escreveu:
> Hello,
> 
> I made plot in attach using:
> 
> boxplot(flcn_M~subject,data=dx,col =
> c("royalblue1","palevioletred1"),xlab="subjects",ylab="Expression
> estimate in delta (log2)",boxwex = 0.2,frame.plot = FALSE)
> stripchart(flcn_M~subject, vertical = TRUE, data = dx,method =
> "jitter", add = TRUE,pch = 20, col=rgb(0,0,0,.5),jitter = 0.001)
> 
> How do I reverse colors so that PDR is shown in this pink and nDR in blue?
> 
> Thanks
> Ana
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From kz m@iii@g oii bre@dii@@@ce@com  Thu Aug  8 21:52:42 2019
From: kz m@iii@g oii bre@dii@@@ce@com (kz m@iii@g oii bre@dii@@@ce@com)
Date: Thu, 8 Aug 2019 15:52:42 -0400
Subject: [R] broken installation on ubuntu 16.04 for 3.4.4-1xenial0
In-Reply-To: <70FE0468-DD2B-4DFC-A2A1-CA9CF0F5674D@dcn.davis.ca.us>
References: <CANk4Xz9M60hvjbwUKh_13KDGc6POrkU4yB2xTJypx9uD6b3ECg@mail.gmail.com>
 <70FE0468-DD2B-4DFC-A2A1-CA9CF0F5674D@dcn.davis.ca.us>
Message-ID: <CAF3BF0D-757A-42D8-9C51-2A1A8B5DFDBF@getbread.com>

Sorry for unintentionally sending a rich text email.

I have confirmed that installation follows ubuntu guide lines in your link and I reproduce the output of git blame of the installation recipe showing that it has always been using the right key.  The change before we get these errors are on March 2019.  The unauthenticated errors start late july or so.  This recipe runs at least once every week for a year or more and only a few weeks ago the installation fails.


71637a7e7 (MK 2018-01-08 16:03:26 -0500  40) apt_repository 'rproject' do
71637a7e7 (MK 2018-01-08 16:03:26 -0500  41)   uri 'http://cran.r-project.org/bin/linux/ubuntu'
5b7928b56 (MK 2018-01-05 13:16:07 -0500  42)   keyserver 'keyserver.ubuntu.com'
5b7928b56 (MK 2018-01-05 13:16:07 -0500  43)   key 'E084DAB9'
71637a7e7 (MK 2018-01-08 16:03:26 -0500  44)   components ['']
71637a7e7 (MK 2018-01-08 16:03:26 -0500  45)   distribution node['lsb']['codename'] + '/'
71637a7e7 (MK 2018-01-08 16:03:26 -0500  46)   action :add
5b7928b56 (MK 2018-01-05 13:16:07 -0500  47) end
5b7928b56 (MK 2018-01-05 13:16:07 -0500  48)
5b7928b56 (MK 2018-01-05 13:16:07 -0500  49) package 'r-base' do
5a1e57649 (k z  2018-03-19 19:21:02 -0400  50)   version '3.4.4-1xenial0'
86242e5cf (k z   2019-08-07 08:27:42 -0400  51)   options '--allow-unauthenticated'
5b7928b56 (MK 2018-01-05 13:16:07 -0500  52) end



> On Aug 6, 2019, at 9:13 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> As the Posting Guide says... this is a plain text mailing list. You will help yourself if you pay attention when posting.
> 
> The standard instructions for Ubuntu [1] describe how to set up authentication.
> 
> [1] https://cran.r-project.org/bin/linux/ubuntu/README.html
> 
> On August 6, 2019 10:52:05 AM PDT, KZ Win <kz at breadfinance.com> wrote:
>> We have a test  system for boostrapping a production machine running R
>> code.  It spins up a new machine and tries to install this version
>> whenever
>> there is a new commit to our infrastructure code repo.  This version
>> has
>> been in place since Mar 2018 but a few weeks ago this test fails
>> because it
>> can no longer install this package even though we have not made any
>> changes
>> to this installation process.
>> 
>> The failure comes with this message
>> 
>> ```
>> WARNING: The following packages cannot be authenticated!
>> r-base-core r-cran-boot r-cran-cluster r-cran-foreign r-cran-mass
>> r-cran-kernsmooth r-cran-lattice r-cran-nlme r-cran-matrix r-cran-mgcv
>> r-cran-rpart r-cran-survival r-cran-class r-cran-nnet r-cran-spatial
>> r-cran-codetools r-recommended r-base r-base-dev r-base-html r-doc-html
>> STDERR: E: There were unauthenticated packages and -y was used without
>> --allow-unauthenticated```
>> 
>> Any idea on whether there is a way to install `r-base` without using
>> `--allow-unauthenticated`
>> 
>> I can provide more information if necessary.
>> 
>> Thank you
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> Sent from my phone. Please excuse my brevity.


From dw|n@em|u@ @end|ng |rom comc@@t@net  Thu Aug  8 22:26:42 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Thu, 8 Aug 2019 13:26:42 -0700
Subject: [R] broken installation on ubuntu 16.04 for 3.4.4-1xenial0
In-Reply-To: <CAF3BF0D-757A-42D8-9C51-2A1A8B5DFDBF@getbread.com>
References: <CANk4Xz9M60hvjbwUKh_13KDGc6POrkU4yB2xTJypx9uD6b3ECg@mail.gmail.com>
 <70FE0468-DD2B-4DFC-A2A1-CA9CF0F5674D@dcn.davis.ca.us>
 <CAF3BF0D-757A-42D8-9C51-2A1A8B5DFDBF@getbread.com>
Message-ID: <081489bf-8be9-2f6e-a568-507e1fbff2c3@comcast.net>

The proper place for Ubuntu/Debian questions is r-sig-debian at r-project.org


-- 

David

On 8/8/19 12:52 PM, kz at breadfinance.com wrote:
> Sorry for unintentionally sending a rich text email.
>
> I have confirmed that installation follows ubuntu guide lines in your link and I reproduce the output of git blame of the installation recipe showing that it has always been using the right key.  The change before we get these errors are on March 2019.  The unauthenticated errors start late july or so.  This recipe runs at least once every week for a year or more and only a few weeks ago the installation fails.
>
>
> 71637a7e7 (MK 2018-01-08 16:03:26 -0500  40) apt_repository 'rproject' do
> 71637a7e7 (MK 2018-01-08 16:03:26 -0500  41)   uri 'http://cran.r-project.org/bin/linux/ubuntu'
> 5b7928b56 (MK 2018-01-05 13:16:07 -0500  42)   keyserver 'keyserver.ubuntu.com'
> 5b7928b56 (MK 2018-01-05 13:16:07 -0500  43)   key 'E084DAB9'
> 71637a7e7 (MK 2018-01-08 16:03:26 -0500  44)   components ['']
> 71637a7e7 (MK 2018-01-08 16:03:26 -0500  45)   distribution node['lsb']['codename'] + '/'
> 71637a7e7 (MK 2018-01-08 16:03:26 -0500  46)   action :add
> 5b7928b56 (MK 2018-01-05 13:16:07 -0500  47) end
> 5b7928b56 (MK 2018-01-05 13:16:07 -0500  48)
> 5b7928b56 (MK 2018-01-05 13:16:07 -0500  49) package 'r-base' do
> 5a1e57649 (k z  2018-03-19 19:21:02 -0400  50)   version '3.4.4-1xenial0'
> 86242e5cf (k z   2019-08-07 08:27:42 -0400  51)   options '--allow-unauthenticated'
> 5b7928b56 (MK 2018-01-05 13:16:07 -0500  52) end
>
>
>
>> On Aug 6, 2019, at 9:13 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>
>> As the Posting Guide says... this is a plain text mailing list. You will help yourself if you pay attention when posting.
>>
>> The standard instructions for Ubuntu [1] describe how to set up authentication.
>>
>> [1] https://cran.r-project.org/bin/linux/ubuntu/README.html
>>
>> On August 6, 2019 10:52:05 AM PDT, KZ Win <kz at breadfinance.com> wrote:
>>> We have a test  system for boostrapping a production machine running R
>>> code.  It spins up a new machine and tries to install this version
>>> whenever
>>> there is a new commit to our infrastructure code repo.  This version
>>> has
>>> been in place since Mar 2018 but a few weeks ago this test fails
>>> because it
>>> can no longer install this package even though we have not made any
>>> changes
>>> to this installation process.
>>>
>>> The failure comes with this message
>>>
>>> ```
>>> WARNING: The following packages cannot be authenticated!
>>> r-base-core r-cran-boot r-cran-cluster r-cran-foreign r-cran-mass
>>> r-cran-kernsmooth r-cran-lattice r-cran-nlme r-cran-matrix r-cran-mgcv
>>> r-cran-rpart r-cran-survival r-cran-class r-cran-nnet r-cran-spatial
>>> r-cran-codetools r-recommended r-base r-base-dev r-base-html r-doc-html
>>> STDERR: E: There were unauthenticated packages and -y was used without
>>> --allow-unauthenticated```
>>>
>>> Any idea on whether there is a way to install `r-base` without using
>>> `--allow-unauthenticated`
>>>
>>> I can provide more information if necessary.
>>>
>>> Thank you
>>>
>>> 	[[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> -- 
>> Sent from my phone. Please excuse my brevity.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From v@|kremk @end|ng |rom gm@||@com  Fri Aug  9 02:11:05 2019
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Thu, 8 Aug 2019 19:11:05 -0500
Subject: [R] read
Message-ID: <CAJOiR6Zzw6FAAo=9_+-VftGJc+y-35aZq7o5QGjjeQ76ThTNdA@mail.gmail.com>

Hi all,

I am trying to red data where single and double quotes are embedded
in some of the fields and prevented to read the data.   As an example
please see below.

vld<-read.table(text="name prof
  A      '4.5
  B       "3.2
  C       5.5 ",header=TRUE)

Error in read.table(text = "name prof \n  A      '4.5    \n  B
3.2     \n  C       5.5 ",  :
  incomplete final line found by readTableHeader on 'text'

Is there a way how to  read this data and gt the following output
  name prof
1    A  4.5
2    B  3.2
3    C  5.5

Thank you inadvertence


From M@Thev@r@j@ @end|ng |rom m@@@ey@@c@nz  Fri Aug  9 02:18:54 2019
From: M@Thev@r@j@ @end|ng |rom m@@@ey@@c@nz (Thevaraja, Mayooran)
Date: Fri, 9 Aug 2019 00:18:54 +0000
Subject: [R] read
In-Reply-To: <CAJOiR6Zzw6FAAo=9_+-VftGJc+y-35aZq7o5QGjjeQ76ThTNdA@mail.gmail.com>
References: <CAJOiR6Zzw6FAAo=9_+-VftGJc+y-35aZq7o5QGjjeQ76ThTNdA@mail.gmail.com>
Message-ID: <MEAPR01MB359298C076E6F134D28F3E40C9D60@MEAPR01MB3592.ausprd01.prod.outlook.com>

Hi you can save your data file in txt or csv file. Then you can use function " vld <-read.table("C:/Users/........ .txt",header=T)".

Regards
Mayooran

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
Sent: Friday, 9 August 2019 12:11 PM
To: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
Subject: [R] read

Hi all,

I am trying to red data where single and double quotes are embedded
in some of the fields and prevented to read the data.   As an example
please see below.

vld<-read.table(text="name prof
  A      '4.5
  B       "3.2
  C       5.5 ",header=TRUE)

Error in read.table(text = "name prof \n  A      '4.5    \n  B
3.2     \n  C       5.5 ",  :
  incomplete final line found by readTableHeader on 'text'

Is there a way how to  read this data and gt the following output
  name prof
1    A  4.5
2    B  3.2
3    C  5.5

Thank you inadvertence

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Fri Aug  9 02:22:13 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 8 Aug 2019 17:22:13 -0700
Subject: [R] read
In-Reply-To: <CAJOiR6Zzw6FAAo=9_+-VftGJc+y-35aZq7o5QGjjeQ76ThTNdA@mail.gmail.com>
References: <CAJOiR6Zzw6FAAo=9_+-VftGJc+y-35aZq7o5QGjjeQ76ThTNdA@mail.gmail.com>
Message-ID: <CAGxFJbQcMMCjaauc==SgWmJGVQgH+z20WRdY89fcPW=bnMoucg@mail.gmail.com>

read.table() does not have a "text" argument, so maybe you need to go back
and go through a tutorial or two to learn R basics (e.g. about function
calls and function arguments ?)
See ?read.table  (of course)

Cheers,

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Aug 8, 2019 at 5:11 PM Val <valkremk at gmail.com> wrote:

> Hi all,
>
> I am trying to red data where single and double quotes are embedded
> in some of the fields and prevented to read the data.   As an example
> please see below.
>
> vld<-read.table(text="name prof
>   A      '4.5
>   B       "3.2
>   C       5.5 ",header=TRUE)
>
> Error in read.table(text = "name prof \n  A      '4.5    \n  B
> 3.2     \n  C       5.5 ",  :
>   incomplete final line found by readTableHeader on 'text'
>
> Is there a way how to  read this data and gt the following output
>   name prof
> 1    A  4.5
> 2    B  3.2
> 3    C  5.5
>
> Thank you inadvertence
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @n@@nth@np|||@| @end|ng |rom gm@||@com  Fri Aug  9 02:24:03 2019
From: @n@@nth@np|||@| @end|ng |rom gm@||@com (Anaanthan Pillai)
Date: Fri, 9 Aug 2019 08:24:03 +0800
Subject: [R] read
In-Reply-To: <CAJOiR6Zzw6FAAo=9_+-VftGJc+y-35aZq7o5QGjjeQ76ThTNdA@mail.gmail.com>
References: <CAJOiR6Zzw6FAAo=9_+-VftGJc+y-35aZq7o5QGjjeQ76ThTNdA@mail.gmail.com>
Message-ID: <D654B767-F7BF-4E39-A2CD-AB5CAF1D3C82@gmail.com>

data <- read.table(header=TRUE, text='
 name prof 
  A  4.5
  B  3.2
  C  5.5   
 ')
> On 9 Aug 2019, at 8:11 AM, Val <valkremk at gmail.com> wrote:
> 
> Hi all,
> 
> I am trying to red data where single and double quotes are embedded
> in some of the fields and prevented to read the data.   As an example
> please see below.
> 
> vld<-read.table(text="name prof
>  A      '4.5
>  B       "3.2
>  C       5.5 ",header=TRUE)
> 
> Error in read.table(text = "name prof \n  A      '4.5    \n  B
> 3.2     \n  C       5.5 ",  :
>  incomplete final line found by readTableHeader on 'text'
> 
> Is there a way how to  read this data and gt the following output
>  name prof
> 1    A  4.5
> 2    B  3.2
> 3    C  5.5
> 
> Thank you inadvertence
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From v@|kremk @end|ng |rom gm@||@com  Fri Aug  9 02:40:08 2019
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Thu, 8 Aug 2019 19:40:08 -0500
Subject: [R] read
In-Reply-To: <D654B767-F7BF-4E39-A2CD-AB5CAF1D3C82@gmail.com>
References: <CAJOiR6Zzw6FAAo=9_+-VftGJc+y-35aZq7o5QGjjeQ76ThTNdA@mail.gmail.com>
 <D654B767-F7BF-4E39-A2CD-AB5CAF1D3C82@gmail.com>
Message-ID: <CAJOiR6bGkBtYtSHQF8eRU2BwR3gYR7x3ac93yfg2VeqtTrZgnw@mail.gmail.com>

Thank you  all, I can read the text file but the problem was there is
a single quote embedded  in  the first row of second column. This
quote causes the problem

vld<-read.table(text="name prof
  A      '4.5
  B       "3.2
  C       5.5 ",header=TRUE)

On Thu, Aug 8, 2019 at 7:24 PM Anaanthan Pillai
<anaanthanpillai at gmail.com> wrote:
>
> data <- read.table(header=TRUE, text='
>  name prof
>   A  4.5
>   B  3.2
>   C  5.5
>  ')
> > On 9 Aug 2019, at 8:11 AM, Val <valkremk at gmail.com> wrote:
> >
> > Hi all,
> >
> > I am trying to red data where single and double quotes are embedded
> > in some of the fields and prevented to read the data.   As an example
> > please see below.
> >
> > vld<-read.table(text="name prof
> >  A      '4.5
> >  B       "3.2
> >  C       5.5 ",header=TRUE)
> >
> > Error in read.table(text = "name prof \n  A      '4.5    \n  B
> > 3.2     \n  C       5.5 ",  :
> >  incomplete final line found by readTableHeader on 'text'
> >
> > Is there a way how to  read this data and gt the following output
> >  name prof
> > 1    A  4.5
> > 2    B  3.2
> > 3    C  5.5
> >
> > Thank you inadvertence
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>


From peter@|@ng|e|der @end|ng |rom gm@||@com  Fri Aug  9 03:01:00 2019
From: peter@|@ng|e|der @end|ng |rom gm@||@com (Peter Langfelder)
Date: Thu, 8 Aug 2019 18:01:00 -0700
Subject: [R] read
In-Reply-To: <CAJOiR6bGkBtYtSHQF8eRU2BwR3gYR7x3ac93yfg2VeqtTrZgnw@mail.gmail.com>
References: <CAJOiR6Zzw6FAAo=9_+-VftGJc+y-35aZq7o5QGjjeQ76ThTNdA@mail.gmail.com>
 <D654B767-F7BF-4E39-A2CD-AB5CAF1D3C82@gmail.com>
 <CAJOiR6bGkBtYtSHQF8eRU2BwR3gYR7x3ac93yfg2VeqtTrZgnw@mail.gmail.com>
Message-ID: <CA+hbrhVcuFXC+7d1392xGGE8avfxtgCwKSRfRGYVXvSQmn1uZQ@mail.gmail.com>

I would remove the quotes using sub, something like

# Read the file as text lines
text = readLines(con = file(yourFileName))
# Remove the offending quotes
text = gsub("'|\"", "", text)
# Concatenate and turn into a data frame
concat = paste(text, collapse = "\n")
df = read.table(text = concat, ...) # Change arguments as needed

HTH,

Peter

On Thu, Aug 8, 2019 at 5:41 PM Val <valkremk at gmail.com> wrote:
>
> Thank you  all, I can read the text file but the problem was there is
> a single quote embedded  in  the first row of second column. This
> quote causes the problem
>
> vld<-read.table(text="name prof
>   A      '4.5
>   B       "3.2
>   C       5.5 ",header=TRUE)
>
> On Thu, Aug 8, 2019 at 7:24 PM Anaanthan Pillai
> <anaanthanpillai at gmail.com> wrote:
> >
> > data <- read.table(header=TRUE, text='
> >  name prof
> >   A  4.5
> >   B  3.2
> >   C  5.5
> >  ')
> > > On 9 Aug 2019, at 8:11 AM, Val <valkremk at gmail.com> wrote:
> > >
> > > Hi all,
> > >
> > > I am trying to red data where single and double quotes are embedded
> > > in some of the fields and prevented to read the data.   As an example
> > > please see below.
> > >
> > > vld<-read.table(text="name prof
> > >  A      '4.5
> > >  B       "3.2
> > >  C       5.5 ",header=TRUE)
> > >
> > > Error in read.table(text = "name prof \n  A      '4.5    \n  B
> > > 3.2     \n  C       5.5 ",  :
> > >  incomplete final line found by readTableHeader on 'text'
> > >
> > > Is there a way how to  read this data and gt the following output
> > >  name prof
> > > 1    A  4.5
> > > 2    B  3.2
> > > 3    C  5.5
> > >
> > > Thank you inadvertence
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri Aug  9 04:11:29 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 08 Aug 2019 19:11:29 -0700
Subject: [R] read
In-Reply-To: <CAGxFJbQcMMCjaauc==SgWmJGVQgH+z20WRdY89fcPW=bnMoucg@mail.gmail.com>
References: <CAJOiR6Zzw6FAAo=9_+-VftGJc+y-35aZq7o5QGjjeQ76ThTNdA@mail.gmail.com>
 <CAGxFJbQcMMCjaauc==SgWmJGVQgH+z20WRdY89fcPW=bnMoucg@mail.gmail.com>
Message-ID: <0EC88BF1-580C-4047-9B19-7AB5BCACDA0B@dcn.davis.ca.us>

Val 1
Bert 0

On August 8, 2019 5:22:13 PM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>read.table() does not have a "text" argument, so maybe you need to go
>back
>and go through a tutorial or two to learn R basics (e.g. about function
>calls and function arguments ?)
>See ?read.table  (of course)
>
>Cheers,
>
>Bert Gunter
>
>"The trouble with having an open mind is that people keep coming along
>and
>sticking things into it."
>-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
>On Thu, Aug 8, 2019 at 5:11 PM Val <valkremk at gmail.com> wrote:
>
>> Hi all,
>>
>> I am trying to red data where single and double quotes are embedded
>> in some of the fields and prevented to read the data.   As an example
>> please see below.
>>
>> vld<-read.table(text="name prof
>>   A      '4.5
>>   B       "3.2
>>   C       5.5 ",header=TRUE)
>>
>> Error in read.table(text = "name prof \n  A      '4.5    \n  B
>> 3.2     \n  C       5.5 ",  :
>>   incomplete final line found by readTableHeader on 'text'
>>
>> Is there a way how to  read this data and gt the following output
>>   name prof
>> 1    A  4.5
>> 2    B  3.2
>> 3    C  5.5
>>
>> Thank you inadvertence
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri Aug  9 04:51:49 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 08 Aug 2019 19:51:49 -0700
Subject: [R] read
In-Reply-To: <CAJOiR6bGkBtYtSHQF8eRU2BwR3gYR7x3ac93yfg2VeqtTrZgnw@mail.gmail.com>
References: <CAJOiR6Zzw6FAAo=9_+-VftGJc+y-35aZq7o5QGjjeQ76ThTNdA@mail.gmail.com>
 <D654B767-F7BF-4E39-A2CD-AB5CAF1D3C82@gmail.com>
 <CAJOiR6bGkBtYtSHQF8eRU2BwR3gYR7x3ac93yfg2VeqtTrZgnw@mail.gmail.com>
Message-ID: <2ED459DA-7624-4802-880C-FC24FFE4B10D@dcn.davis.ca.us>

Assuming your actual case is a file containing those characters, your example R string has to quote them. However, it seems like you want to disable interpreting quotes while you read this file.

vld<-read.table(text=
"name prof
  A      '4.5
  B       \"3.2
  C       5.5 "
,header=TRUE,quote="")

(The escape character isn't really there.)

On August 8, 2019 5:40:08 PM PDT, Val <valkremk at gmail.com> wrote:
>Thank you  all, I can read the text file but the problem was there is
>a single quote embedded  in  the first row of second column. This
>quote causes the problem
>
>vld<-read.table(text="name prof
>  A      '4.5
>  B       "3.2
>  C       5.5 ",header=TRUE)
>
>On Thu, Aug 8, 2019 at 7:24 PM Anaanthan Pillai
><anaanthanpillai at gmail.com> wrote:
>>
>> data <- read.table(header=TRUE, text='
>>  name prof
>>   A  4.5
>>   B  3.2
>>   C  5.5
>>  ')
>> > On 9 Aug 2019, at 8:11 AM, Val <valkremk at gmail.com> wrote:
>> >
>> > Hi all,
>> >
>> > I am trying to red data where single and double quotes are embedded
>> > in some of the fields and prevented to read the data.   As an
>example
>> > please see below.
>> >
>> > vld<-read.table(text="name prof
>> >  A      '4.5
>> >  B       "3.2
>> >  C       5.5 ",header=TRUE)
>> >
>> > Error in read.table(text = "name prof \n  A      '4.5    \n  B
>> > 3.2     \n  C       5.5 ",  :
>> >  incomplete final line found by readTableHeader on 'text'
>> >
>> > Is there a way how to  read this data and gt the following output
>> >  name prof
>> > 1    A  4.5
>> > 2    B  3.2
>> > 3    C  5.5
>> >
>> > Thank you inadvertence
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From bgunter@4567 @end|ng |rom gm@||@com  Fri Aug  9 06:05:48 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 8 Aug 2019 21:05:48 -0700
Subject: [R] read
In-Reply-To: <0EC88BF1-580C-4047-9B19-7AB5BCACDA0B@dcn.davis.ca.us>
References: <CAJOiR6Zzw6FAAo=9_+-VftGJc+y-35aZq7o5QGjjeQ76ThTNdA@mail.gmail.com>
 <CAGxFJbQcMMCjaauc==SgWmJGVQgH+z20WRdY89fcPW=bnMoucg@mail.gmail.com>
 <0EC88BF1-580C-4047-9B19-7AB5BCACDA0B@dcn.davis.ca.us>
Message-ID: <CAGxFJbTQ-CJCGCbefNgZEe_6FxnMqdkBa_J3TgBQp3F_zEC9dw@mail.gmail.com>

I stand corrected!

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Aug 8, 2019 at 7:11 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Val 1
> Bert 0
>
> On August 8, 2019 5:22:13 PM PDT, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
> >read.table() does not have a "text" argument, so maybe you need to go
> >back
> >and go through a tutorial or two to learn R basics (e.g. about function
> >calls and function arguments ?)
> >See ?read.table  (of course)
> >
> >Cheers,
> >
> >Bert Gunter
> >
> >"The trouble with having an open mind is that people keep coming along
> >and
> >sticking things into it."
> >-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >
> >
> >On Thu, Aug 8, 2019 at 5:11 PM Val <valkremk at gmail.com> wrote:
> >
> >> Hi all,
> >>
> >> I am trying to red data where single and double quotes are embedded
> >> in some of the fields and prevented to read the data.   As an example
> >> please see below.
> >>
> >> vld<-read.table(text="name prof
> >>   A      '4.5
> >>   B       "3.2
> >>   C       5.5 ",header=TRUE)
> >>
> >> Error in read.table(text = "name prof \n  A      '4.5    \n  B
> >> 3.2     \n  C       5.5 ",  :
> >>   incomplete final line found by readTableHeader on 'text'
> >>
> >> Is there a way how to  read this data and gt the following output
> >>   name prof
> >> 1    A  4.5
> >> 2    B  3.2
> >> 3    C  5.5
> >>
> >> Thank you inadvertence
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From petr@p|k@| @end|ng |rom prechez@@cz  Fri Aug  9 08:12:06 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Fri, 9 Aug 2019 06:12:06 +0000
Subject: [R] Extract row as NA with no matching name
In-Reply-To: <CA+dpOJ=XXddrrGjGs58WdaEJnUAYADm-ZoLyCojwFWcnTmA51Q@mail.gmail.com>
References: <CA+dpOJ=XXddrrGjGs58WdaEJnUAYADm-ZoLyCojwFWcnTmA51Q@mail.gmail.com>
Message-ID: <e0dd37a76bf541beababc472bce231cd@SRVEXCHCM1302.precheza.cz>

Hi

Do you insist to use matrix?

If you change matrix to data frame it returns NA as required.

mdat
     C.1 C.2 C.3
row1   1   2   3
row2  11  12  13
> mdat["some",]
Error in mdat["some", ] : subscript out of bounds
> mdatf<-as.data.frame(mdat)
> mdatf["some",]
   C.1 C.2 C.3
NA  NA  NA  NA

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Christofer Bogaso
> Sent: Thursday, August 8, 2019 5:44 PM
> To: r-help <r-help at r-project.org>
> Subject: [R] Extract row as NA with no matching name
>
> Hi,
>
> Let say I have below matrix
>
> mdat <- matrix(c(1,2,3, 11,12,13), nrow = 2, ncol = 3, byrow = TRUE,
>                dimnames = list(c("row1", "row2"),
>                                c("C.1", "C.2", "C.3")))
>
>
> Now I can extract a raw by rowname as
>
> > mdat['row1', ]
>
> C.1 C.2 C.3
>
>   1   2   3
>
>
> However I am also looking for was to extract values as NA when a rowname is
> supplied which is not existing rownames
>
> I should get
>
> > mdat['new_raw', ]
>
> C.1 C.2 C.3
>
>   NA   NA   NA
>
>
> Current it throws error as default functionality. Is there any way to force R to
> provide values as NA instead of showing any errore?
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From @|gbert @end|ng |rom w|w|@hu-ber||n@de  Fri Aug  9 08:25:36 2019
From: @|gbert @end|ng |rom w|w|@hu-ber||n@de (Sigbert Klinke)
Date: Fri, 9 Aug 2019 08:25:36 +0200
Subject: [R] shinyWidgets::sliderTextInput
In-Reply-To: <CACxE24mz7bAaSkoPw5HG2gvh2RusqQ1gnC3PFCRAbjL-4Eqeug@mail.gmail.com>
References: <1f85aa25-9914-954b-7695-6c68edcbfd85@wiwi.hu-berlin.de>
 <CACxE24kFV_bi2f0u9QyPo3sLCTM+qE2K8x7JaqHGNdWfk7YoCw@mail.gmail.com>
 <93bafa63-667c-29ff-8472-096c23b60ca1@wiwi.hu-berlin.de>
 <CACxE24mz7bAaSkoPw5HG2gvh2RusqQ1gnC3PFCRAbjL-4Eqeug@mail.gmail.com>
Message-ID: <88aad165-974b-0b49-b474-588ed7d8b27c@wiwi.hu-berlin.de>

Hi Erin,

do you see the difference between sliderTextInput and selectInput? I 
would like to have the selectInput behaviour (list element name in 
display, but result as value of list element) in sliderTextInput as well.

Best Sigbert

---

library("shiny")
library("shinyWidgets")

ui <- fluidPage(
   br(),
   sliderTextInput(
     inputId = "mySliderText",
     label = "My House",
     choices = list("choice 1" = 1,"choice 2" =2,
                    "choice 3" = 3)
   ),
   selectInput(inputId="slope", label="selectInput",
                choices=list("Choice 1" = "A","Choice 2" = "B",
                             "Choice 3" = "C")),
   verbatimTextOutput(outputId = "result")
)


server <- function(input, output, session) {

   output$result <- renderPrint({
     str(input$mySliderText)
     str(input$slope)
   })
}

shinyApp(ui = ui, server = server)


-- 
https://hu.berlin/sk
https://hu.berlin/mmstat3


From e@ @end|ng |rom enr|co@chum@nn@net  Fri Aug  9 08:36:02 2019
From: e@ @end|ng |rom enr|co@chum@nn@net (Enrico Schumann)
Date: Fri, 09 Aug 2019 08:36:02 +0200
Subject: [R] Creating a web site checker using R
In-Reply-To: <513562968.2534827.1565286881081.JavaMail.zimbra@psyctc.org>
 (Chris Evans's message of "Thu, 8 Aug 2019 18:54:41 +0100 (BST)")
References: <513562968.2534827.1565286881081.JavaMail.zimbra@psyctc.org>
Message-ID: <87h86qrgkt.fsf@enricoschumann.net>

>>>>> "Chris" == Chris Evans <chrishold at psyctc.org> writes:

    Chris> I use R a great deal but the huge web crawling power of
    Chris> it isn't an area I've used. I don't want to reinvent a
    Chris> cyberwheel and I suspect someone has done what I want.
    Chris> That is a program that would run once a day (easy for
    Chris> me to set up as a cron task) and would crawl a single
    Chris> root of a web site (mine) and get the file size and a
    Chris> CRC or some similar check value for each page as pulled
    Chris> off the site (and, obviously, I'd want it not to follow
    Chris> off site links). The other key thing would be for it to
    Chris> store the values and URLs and be capable of being run
    Chris> in "create/update database" mode or in "check pages"
    Chris> mode and for the change mode run to Email me a warning
    Chris> if a page changes.  The reason I want this is that two
    Chris> of my sites have recently had content "disappear":
    Chris> neither I nor the ISP can see what's happened and we
    Chris> are lacking the very useful diagnostic of the date when
    Chris> the change happened which might have mapped it some
    Chris> component of WordPress, plugins or themes having
    Chris> updated.

    Chris> I am failing to find anything such and all the services
    Chris> that offer site checking of this sort are prohibitively
    Chris> expensive for me (my sites are zero income and either
    Chris> personal or offering free utilities and information).

    Chris> If anyone has done this, or something similar, I'd love
    Chris> to hear if you were willing to share it.  Failing that,
    Chris> I think I will have to create this but I know it will
    Chris> take me days as this isn't my area of R expertise and
    Chris> as, to be brutally honest, I'm a pretty poor
    Chris> programmer.  If I go that way, I'm sure people may be
    Chris> able to point me to things I may be (legitimately) able
    Chris> to recycle in parts to help construct this.

    Chris> Thanks in advance,

    Chris> Chris

    Chris> -- 
    Chris> Chris Evans <chris at psyctc.org> Skype: chris-psyctc
    Chris> Visiting Professor, University of Sheffield <chris.evans at sheffield.ac.uk>
    Chris> I do some consultation work for the University of Roehampton <chris.evans at roehampton.ac.uk> and other places but this <chris at psyctc.org> remains my main Email address.
    Chris> I have "semigrated" to France, see: https://www.psyctc.org/pelerinage2016/semigrating-to-france/ if you want to book to talk, I am trying to keep that to Thursdays and my diary is now available at: https://www.psyctc.org/pelerinage2016/ecwd_calendar/calendar/
    Chris> Beware: French time, generally an hour ahead of UK.  That page will also take you to my blog which started with earlier joys in France and Spain!

Not an answer, but perhaps two pointers/ideas:

1) Since you know cron, I suppose you work on a
   Unix-like system, and you likely have a programme
   called 'wget' either installed or can easily install
   it. 'wget' has an option 'mirror', which allows you
   to mirror a website.

2) There is tools::md5sum for computing checksums. You
   could store those to a file and check changes in the
   files content (e.g. via 'diff').


regards
        Enrico
-- 
Enrico Schumann
Lucerne, Switzerland
http://enricoschumann.net


From m@rc_grt @end|ng |rom y@hoo@|r  Fri Aug  9 14:05:22 2019
From: m@rc_grt @end|ng |rom y@hoo@|r (Marc Girondot)
Date: Fri, 9 Aug 2019 14:05:22 +0200
Subject: [R] knitr error for small text in pdf (bug)
Message-ID: <a0bb6f5e-4c8f-00b0-498a-dc1957185ed3@yahoo.fr>

Let try this minimal Rmarkdown file

---
title: "cex in Rmarkdown"
output: word_document
---

```{r}
knitr::opts_chunk$set(dev='pdf')
```


```{r}
plot((0:160)/4, 0:160, type="n")
text(x=20, y=70, labels =expression(alpha), cex=1e-7)
```

When knitr-red from Rstudio (with r 3.6.1 on MacosX with knitr 1.24), it 
produced an error with this message ("Information of metric non 
available for this family") [translation from French because of my 
system configuration]

However,

text(x=20, y=70, labels =expression(alpha), cex=0)

and

text(x=20, y=70, labels =expression(alpha), cex=0.1)

work.

Also text(x=20, y=70, labels ="a", cex=1e-7) works.

The error does not occur when dev='png' is used but it occurs also for 
dev='postscript'

I don't know where I should report the bug.

Thanks


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Fri Aug  9 17:53:43 2019
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Fri, 9 Aug 2019 17:53:43 +0200
Subject: [R] Error exporting dataframe from Julia to R with Feather
In-Reply-To: <3149BCC3-ECE7-4D3C-B799-FF2074292D3E@gmail.com>
References: <CAMk+s2RVUKu0Z-oNnwbsiij5v7tKzt25HbonNO0rYuq=Okez=g@mail.gmail.com>
 <CAGxFJbTuxAvj8Pjp6zxZ0ap6z=+h695_TMOd=vt+Gn5gLEwFdw@mail.gmail.com>
 <3149BCC3-ECE7-4D3C-B799-FF2074292D3E@gmail.com>
Message-ID: <CAMk+s2QV0Ft7pDqcfBaKQ4KO2Jph94gMGs27_36og8zVH4c6jw@mail.gmail.com>

Hi, I ran from terminal:
```
$ Rscript try.R
[1] "library feather loaded"
[1] "open file"

 *** caught segfault ***
address (nil), cause 'memory not mapped'

Traceback:
 1: .Call(`_feather_openFeather`, path)
 2: openFeather(path)
 3: feather(path)
 4: read_feather("Hits_dedupl.feather")
An irrecoverable exception occurred. R is aborting now ...
Segmentation fault (core dumped)
```

Perhaps is the problem with the original database rather than feather.
When I save the dataframe made with Julia with CSV.write and reload in
R with read.table I get the error
```
Warning message:
In scan(file = file, what = what, sep = sep, quote = quote, dec = dec,
: EOF within quoted string > str(hit) 'data.frame': 81365 obs. of 1
variable: $ Group.Sample.Start.Match_len.Read_len.Hit_len.Hit.ID: chr
"Normal\tA4\t6064657\t27\t101\t13669\tNC_038931.1 Thermoproteus tenax
virus 1 (TTV1) genome\tNC_038931"
"Normal\tA0\t6064658\t29\t101\t13669\tNC_038931.1 Thermoproteus tenax
virus 1 (TTV1) genome\tNC_038931"
"Normal\tA0\t6064659\t28\t101\t13669\tNC_038931.1 Thermoproteus tenax
virus 1 (TTV1) genome\tNC_038931"
"Normal\tA0\t6064660\t27\t101\t13669\tNC_038931.1 Thermoproteus tenax
virus 1 (TTV1) genome\tNC_038931" ...
```


On Thu, Aug 8, 2019 at 5:29 PM peter dalgaard <pdalgd at gmail.com> wrote:
>
> Alternatively, try running your example from plain R (in a terminal, R.app, or Rgui, depending on your platform), and see if the problem occurs without RStudio in the equation. If it does, then the feather package probably owns the problem.
>
> -pd
>
> > On 8 Aug 2019, at 16:34 , Bert Gunter <bgunter.4567 at gmail.com> wrote:
> >
> > You may have to contact RStudio about this. RStudio is a separate IDE
> > independent of R -- i.e. developed and maintained by a separate
> > organization from the R project.
> >
> > Bert Gunter
> >
> > "The trouble with having an open mind is that people keep coming along and
> > sticking things into it."
> > -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >
> >
> > On Thu, Aug 8, 2019 at 7:08 AM Luigi Marongiu <marongiu.luigi at gmail.com>
> > wrote:
> >
> >> Hello,
> >>
> >> since I am encountering a lot of problems exporting dataframes from
> >> julia to R (there is always something wrong with the formatting,
> >> probably a missing quote) so I am trying to use Feather to do the job.
> >>
> >> I have installed Feather in Julia with `pkg.add("Feather")` and
> >> imported it with `using Feather`. I created a dataframe and saved it
> >> with `Feather.write("/dir/dataframe.feather", df)` and it worked. I
> >> can even open it back with `df =
> >> Feather.read("/dir/dataframe.feather")` and get: `julia> nrow(df)
> >> 128544`.
> >> The problem is with R. I am using Rstudio to test each step.
> >> I installed the package, imported it with `library(feather)` and used it
> >> as:
> >> ```
> >> df = read_feather("/dir/dataframe.feather")
> >> ```
> >> and then Rstudio simply crashes.
> >> any idea why?
> >> Thank you
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>


--
Best regards,
Luigi


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Fri Aug  9 18:15:49 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Fri, 9 Aug 2019 09:15:49 -0700 (PDT)
Subject: [R] Installing multiple packages fails
Message-ID: <alpine.LNX.2.20.1908090907380.9134@salmo.appl-ecosys.com>

Running 3.6.1 here and migrating from my old 32-bit server/workstation to a
new 64-bit server/workstation (both running fully patched Slackware-14.2).

On the old host .libPaths() returns "/usr/lib/R/library"; on the new host it
returned nothing so I ran .libPaths("/usr/lib64/R/library") to create the
system-wide library.

First question is how I was able to print a dataframe list of installed
libraries on the new host if the path was not defined?

Second question is why I get an error on the new host after defining the
library and running the install.packages() function:

> install.packages("Blossom","INLA","RTisean","RcppProgress","STRbook",
"askpass","classInt","ellipsis","generics","lpSolve","odesolve","ranger","sf",
"sys","units")

Warning in install.packages("Blossom", "INLA", "RTisean", "RcppProgress", :
   'lib = "INLA"' is not writable

What have I missed or done incorrectly?

TIA,

Rich


From jho|tm@n @end|ng |rom gm@||@com  Fri Aug  9 18:34:23 2019
From: jho|tm@n @end|ng |rom gm@||@com (jim holtman)
Date: Fri, 9 Aug 2019 09:34:23 -0700
Subject: [R] Installing multiple packages fails
In-Reply-To: <alpine.LNX.2.20.1908090907380.9134@salmo.appl-ecosys.com>
References: <alpine.LNX.2.20.1908090907380.9134@salmo.appl-ecosys.com>
Message-ID: <CAAxdm-7uw3Ta9rNrvDZHM-_V+tA061TBn=x6un9oTcHP_Z80aQ@mail.gmail.com>

The first parameter needs to be a character vector:

 install.packages(c("Blossom","INLA","RTisean","RcppProgress","STRbook",
"askpass","classInt","ellipsis","generics","lpSolve","
odesolve","ranger","sf",
"sys","units") )

Jim Holtman
*Data Munger Guru*


*What is the problem that you are trying to solve?Tell me what you want to
do, not how you want to do it.*


On Fri, Aug 9, 2019 at 9:16 AM Rich Shepard <rshepard at appl-ecosys.com>
wrote:

> Running 3.6.1 here and migrating from my old 32-bit server/workstation to a
> new 64-bit server/workstation (both running fully patched Slackware-14.2).
>
> On the old host .libPaths() returns "/usr/lib/R/library"; on the new host
> it
> returned nothing so I ran .libPaths("/usr/lib64/R/library") to create the
> system-wide library.
>
> First question is how I was able to print a dataframe list of installed
> libraries on the new host if the path was not defined?
>
> Second question is why I get an error on the new host after defining the
> library and running the install.packages() function:
>
> > install.packages("Blossom","INLA","RTisean","RcppProgress","STRbook",
>
> "askpass","classInt","ellipsis","generics","lpSolve","odesolve","ranger","sf",
> "sys","units")
>
> Warning in install.packages("Blossom", "INLA", "RTisean", "RcppProgress", :
>    'lib = "INLA"' is not writable
>
> What have I missed or done incorrectly?
>
> TIA,
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Fri Aug  9 18:51:41 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Fri, 9 Aug 2019 09:51:41 -0700 (PDT)
Subject: [R] Installing multiple packages fails
In-Reply-To: <CAAxdm-7uw3Ta9rNrvDZHM-_V+tA061TBn=x6un9oTcHP_Z80aQ@mail.gmail.com>
References: <alpine.LNX.2.20.1908090907380.9134@salmo.appl-ecosys.com>
 <CAAxdm-7uw3Ta9rNrvDZHM-_V+tA061TBn=x6un9oTcHP_Z80aQ@mail.gmail.com>
Message-ID: <alpine.LNX.2.20.1908090950570.9134@salmo.appl-ecosys.com>

On Fri, 9 Aug 2019, jim holtman wrote:

> The first parameter needs to be a character vector:
>
> install.packages(c("Blossom","INLA","RTisean","RcppProgress","STRbook",
> "askpass","classInt","ellipsis","generics","lpSolve","
> odesolve","ranger","sf",
> "sys","units") )

Jim,

Of course! I'm so used to installing single packages that I overlooked that.

Thanks very much,

Rich


From v@|kremk @end|ng |rom gm@||@com  Fri Aug  9 22:43:36 2019
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Fri, 9 Aug 2019 15:43:36 -0500
Subject: [R] read
In-Reply-To: <CAGxFJbTQ-CJCGCbefNgZEe_6FxnMqdkBa_J3TgBQp3F_zEC9dw@mail.gmail.com>
References: <CAJOiR6Zzw6FAAo=9_+-VftGJc+y-35aZq7o5QGjjeQ76ThTNdA@mail.gmail.com>
 <CAGxFJbQcMMCjaauc==SgWmJGVQgH+z20WRdY89fcPW=bnMoucg@mail.gmail.com>
 <0EC88BF1-580C-4047-9B19-7AB5BCACDA0B@dcn.davis.ca.us>
 <CAGxFJbTQ-CJCGCbefNgZEe_6FxnMqdkBa_J3TgBQp3F_zEC9dw@mail.gmail.com>
Message-ID: <CAJOiR6aizz6GBvR70ySHa4L_9P_4RZyoDwu5McL6EE9ZrBBLGA@mail.gmail.com>

Thank you Jeff! That was so easy command.

On Thu, Aug 8, 2019 at 11:06 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
> I stand corrected!
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Thu, Aug 8, 2019 at 7:11 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>
>> Val 1
>> Bert 0
>>
>> On August 8, 2019 5:22:13 PM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>> >read.table() does not have a "text" argument, so maybe you need to go
>> >back
>> >and go through a tutorial or two to learn R basics (e.g. about function
>> >calls and function arguments ?)
>> >See ?read.table  (of course)
>> >
>> >Cheers,
>> >
>> >Bert Gunter
>> >
>> >"The trouble with having an open mind is that people keep coming along
>> >and
>> >sticking things into it."
>> >-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>> >
>> >
>> >On Thu, Aug 8, 2019 at 5:11 PM Val <valkremk at gmail.com> wrote:
>> >
>> >> Hi all,
>> >>
>> >> I am trying to red data where single and double quotes are embedded
>> >> in some of the fields and prevented to read the data.   As an example
>> >> please see below.
>> >>
>> >> vld<-read.table(text="name prof
>> >>   A      '4.5
>> >>   B       "3.2
>> >>   C       5.5 ",header=TRUE)
>> >>
>> >> Error in read.table(text = "name prof \n  A      '4.5    \n  B
>> >> 3.2     \n  C       5.5 ",  :
>> >>   incomplete final line found by readTableHeader on 'text'
>> >>
>> >> Is there a way how to  read this data and gt the following output
>> >>   name prof
>> >> 1    A  4.5
>> >> 2    B  3.2
>> >> 3    C  5.5
>> >>
>> >> Thank you inadvertence
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> >______________________________________________
>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Sent from my phone. Please excuse my brevity.


From @purd|e@@ @end|ng |rom gm@||@com  Sat Aug 10 01:12:09 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sat, 10 Aug 2019 11:12:09 +1200
Subject: [R] Simulations of GAM and MARS models : sample size ;
 Y-outliers and missing X-data
In-Reply-To: <1983628866.3770816.1565266265855@mail.yahoo.com>
References: <1436794675.3113973.1565183342017.ref@mail.yahoo.com>
 <1436794675.3113973.1565183342017@mail.yahoo.com>
 <CAB8pepw9AWsJRs2_MExGG05MsJ838LRLdQ3Xn_ZhjVHvpo6KrA@mail.gmail.com>
 <1983628866.3770816.1565266265855@mail.yahoo.com>
Message-ID: <CAB8pepzZvUntzADpZ8FOh6mxgaydB5sWLt1ujYJAiGXod4ud_g@mail.gmail.com>

> For me better all the x variables (collectively), to have m% missing
values.

I checked the mgcv documentation.
Observations with (any) missing values are ignored.
(i.e. The entire row, from your input data).

"If there are missing values in the reponse or covariates of a GAM then the
default is simply to use only the ?complete cases?."

I haven't checked the ISLR package.

	[[alternative HTML version deleted]]


From rv15| @end|ng |rom y@hoo@@e  Sat Aug 10 19:20:13 2019
From: rv15| @end|ng |rom y@hoo@@e (ravi)
Date: Sat, 10 Aug 2019 17:20:13 +0000 (UTC)
Subject: [R] vectorizing the integrate function
References: <1395066570.5017281.1565457613467.ref@mail.yahoo.com>
Message-ID: <1395066570.5017281.1565457613467@mail.yahoo.com>

Hi all,I am having some difficulties in vectorizing the integrate function. Let me explain with an example.
a <- 10; b <- 3; c <- 4
f <- function(x) {exp(-a*x^3-b*x^2-c*x)}
integrate(f,0,Inf) # works fine

My difficulties start when I want to vectorize.

# attempts to vectorize fail
a <- seq(from=0,to=1,by=0.5)
b <- seq(from=5,to=10,by=1)
m <- seq(from=10,to=20,by=5)
f2 <- function(x,a,b,c) {exp(-a*x^3-b*x^2-m*x)}
fv <- Vectorize(integrate(f2,0,Inf),vectorize.args=c("a","b","m"),SIMPLIFY=TRUE)

I want the result as a 3-d array with dimensions of the lengths of a, b and c. I have tried several variants but am not having much luck. Will appreciate any help that I can get.
Thanks,Ravi Sutradhara




	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sat Aug 10 20:03:08 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 10 Aug 2019 11:03:08 -0700
Subject: [R] vectorizing the integrate function
In-Reply-To: <1395066570.5017281.1565457613467@mail.yahoo.com>
References: <1395066570.5017281.1565457613467.ref@mail.yahoo.com>
 <1395066570.5017281.1565457613467@mail.yahoo.com>
Message-ID: <CAGxFJbTtfu_Uo1chv7fJUS7RHfz9C-F4nYUHisOC8H=tXs1SpQ@mail.gmail.com>

Ravi:

First of all, you're calling Vectorize incorrectly. The first argument must
be a function *name*, not a function call. Here's what you need to do (and
thanks for the reprex -- wouldn't have been able to help without it!):

f2 <- function(a,b,c)integrate(function(x){exp(-a*x^3-b*x^2-c*x)},lower
=0,upper = Inf)
fv <- Vectorize(f2,vectorize.args=c("a","b","c"),SIMPLIFY=TRUE)

Second of all, as you may realize, the vectorization uses mapply() and so
vectorizes the c(a,b,c) triplet "in parallel," which will "recycle" shorter
arguments to the length of longer. Hence you will get 6 results from your
example:

> fv(a,b,m)
             [,1]         [,2]         [,3]        [,4]         [,5]
  [,6]
value        0.09207851   0.0635289    0.04837997  0.08856628   0.06224138
  0.04777941
abs.error    3.365173e-08 3.108388e-06 1.00652e-09 3.284876e-09
1.796619e-08 6.348142e-09
subdivisions 2            1            2           2            2
 2
message      "OK"         "OK"         "OK"        "OK"         "OK"
  "OK"
call         Expression   Expression   Expression  Expression   Expression
  Expression

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Aug 10, 2019 at 10:20 AM ravi via R-help <r-help at r-project.org>
wrote:

> Hi all,I am having some difficulties in vectorizing the integrate
> function. Let me explain with an example.
> a <- 10; b <- 3; c <- 4
> f <- function(x) {exp(-a*x^3-b*x^2-c*x)}
> integrate(f,0,Inf) # works fine
>
> My difficulties start when I want to vectorize.
>
> # attempts to vectorize fail
> a <- seq(from=0,to=1,by=0.5)
> b <- seq(from=5,to=10,by=1)
> m <- seq(from=10,to=20,by=5)
> f2 <- function(x,a,b,c) {exp(-a*x^3-b*x^2-m*x)}
> fv <-
> Vectorize(integrate(f2,0,Inf),vectorize.args=c("a","b","m"),SIMPLIFY=TRUE)
>
> I want the result as a 3-d array with dimensions of the lengths of a, b
> and c. I have tried several variants but am not having much luck. Will
> appreciate any help that I can get.
> Thanks,Ravi Sutradhara
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @pbr@ckett20 @end|ng |rom @@|ntjo@ephh@@com  Sat Aug 10 21:29:28 2019
From: @pbr@ckett20 @end|ng |rom @@|ntjo@ephh@@com (Spencer Brackett)
Date: Sat, 10 Aug 2019 15:29:28 -0400
Subject: [R] Using read.table for importing gz file
Message-ID: <CAPQaxLPG9YHBaZOpQhZCfwCjCQsrnRDK9mYZ53kYgkJ9ZC66oA@mail.gmail.com>

Hello,

I am trying to read the following Xena dataset into R for data analysis:
https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz

I tried to run the following read.table(gzfile("HumanMethylation450.gz")),
but R ended up crashing as a result.

Is there perhaps a way to use read.table with fread in some way to do this?

Many thanks,

Spencer

	[[alternative HTML version deleted]]


From rv15| @end|ng |rom y@hoo@@e  Sat Aug 10 22:14:28 2019
From: rv15| @end|ng |rom y@hoo@@e (ravi)
Date: Sat, 10 Aug 2019 20:14:28 +0000 (UTC)
Subject: [R] vectorizing the integrate function
In-Reply-To: <CAGxFJbTtfu_Uo1chv7fJUS7RHfz9C-F4nYUHisOC8H=tXs1SpQ@mail.gmail.com>
References: <1395066570.5017281.1565457613467.ref@mail.yahoo.com>
 <1395066570.5017281.1565457613467@mail.yahoo.com>
 <CAGxFJbTtfu_Uo1chv7fJUS7RHfz9C-F4nYUHisOC8H=tXs1SpQ@mail.gmail.com>
Message-ID: <752875199.5065164.1565468068845@mail.yahoo.com>

 Bert,Thanks a lot for your help. I have a few follow-up questions (shown in the comment lines).? Numerical values and error for a (i) vector and (ii) array.

a <- seq(from=0,to=1,by=0.5)
b <- seq(from=5,to=10,by=1)
m <- seq(from=10,to=20,by=5)

f2 <- function(a,b,m)integrate(function(x){exp(-a*x^3-b*x^2-m*x)},lower=0,upper = Inf)
fv <- Vectorize(f2,vectorize.args=c("a","b","m"),SIMPLIFY=TRUE)

r1 <- fv(a,b,m)
# How to get just the numerical results as an array
# for scalar values of a,b and m, I can get it with 

#s <- integrate(f,0,Inf)$value
# How do I get this for a vector

# Finally, if I want an array for all the values of a, b and m, how should I proceed?
r <- array(0,c(3,6,3))
r.error <- array(0,c(3,6,3))
# I want the results of the vectorized integrate function in the above arrays. How do I extract these values?

Thanks,Ravi


    On Saturday, 10 August 2019, 20:03:22 CEST, Bert Gunter <bgunter.4567 at gmail.com> wrote:  
 
 Ravi:
First of all, you're calling Vectorize incorrectly. The first argument must be a function *name*, not a function call. Here's what you need to do (and thanks for the reprex -- wouldn't have been able to help without it!):
f2 <- function(a,b,c)integrate(function(x){exp(-a*x^3-b*x^2-c*x)},lower =0,upper = Inf)
fv <- Vectorize(f2,vectorize.args=c("a","b","c"),SIMPLIFY=TRUE)
Second of all, as you may realize, the vectorization uses mapply() and so vectorizes the c(a,b,c) triplet "in parallel," which will "recycle" shorter arguments to the length of longer. Hence you will get 6 results from your example:
> fv(a,b,m)
? ? ? ? ? ? ?[,1] ? ? ? ? [,2] ? ? ? ? [,3] ? ? ? ?[,4] ? ? ? ? [,5] ? ? ? ? [,6] ? ? ? ?
value ? ? ? ?0.09207851 ? 0.0635289 ? ?0.04837997 ?0.08856628 ? 0.06224138 ? 0.04777941 ?
abs.error ? ?3.365173e-08 3.108388e-06 1.00652e-09 3.284876e-09 1.796619e-08 6.348142e-09
subdivisions 2 ? ? ? ? ? ?1 ? ? ? ? ? ?2 ? ? ? ? ? 2 ? ? ? ? ? ?2 ? ? ? ? ? ?2 ? ? ? ? ? 
message ? ? ?"OK" ? ? ? ? "OK" ? ? ? ? "OK" ? ? ? ?"OK" ? ? ? ? "OK" ? ? ? ? "OK" ? ? ? ?
call ? ? ? ? Expression ? Expression ? Expression ?Expression ? Expression ? Expression? 

Cheers,Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Aug 10, 2019 at 10:20 AM ravi via R-help <r-help at r-project.org> wrote:

Hi all,I am having some difficulties in vectorizing the integrate function. Let me explain with an example.
a <- 10; b <- 3; c <- 4
f <- function(x) {exp(-a*x^3-b*x^2-c*x)}
integrate(f,0,Inf) # works fine

My difficulties start when I want to vectorize.

# attempts to vectorize fail
a <- seq(from=0,to=1,by=0.5)
b <- seq(from=5,to=10,by=1)
m <- seq(from=10,to=20,by=5)
f2 <- function(x,a,b,c) {exp(-a*x^3-b*x^2-m*x)}
fv <- Vectorize(integrate(f2,0,Inf),vectorize.args=c("a","b","m"),SIMPLIFY=TRUE)

I want the result as a 3-d array with dimensions of the lengths of a, b and c. I have tried several variants but am not having much luck. Will appreciate any help that I can get.
Thanks,Ravi Sutradhara




? ? ? ? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

  
	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Sat Aug 10 23:51:54 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Sat, 10 Aug 2019 14:51:54 -0700
Subject: [R] Using read.table for importing gz file
In-Reply-To: <CAPQaxLPG9YHBaZOpQhZCfwCjCQsrnRDK9mYZ53kYgkJ9ZC66oA@mail.gmail.com>
References: <CAPQaxLPG9YHBaZOpQhZCfwCjCQsrnRDK9mYZ53kYgkJ9ZC66oA@mail.gmail.com>
Message-ID: <36ee7a45-d045-444e-ecb1-aa5330aa5139@comcast.net>

Have you tried using readLines in the manner illustrated on the ?gzfile 
help page?


David.

On 8/10/19 12:29 PM, Spencer Brackett wrote:
> Hello,
>
> I am trying to read the following Xena dataset into R for data analysis:
> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
>
> I tried to run the following read.table(gzfile("HumanMethylation450.gz")),
> but R ended up crashing as a result.
>
> Is there perhaps a way to use read.table with fread in some way to do this?
>
> Many thanks,
>
> Spencer
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Aug 11 00:03:22 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 10 Aug 2019 23:03:22 +0100
Subject: [R] vectorizing the integrate function
In-Reply-To: <752875199.5065164.1565468068845@mail.yahoo.com>
References: <1395066570.5017281.1565457613467.ref@mail.yahoo.com>
 <1395066570.5017281.1565457613467@mail.yahoo.com>
 <CAGxFJbTtfu_Uo1chv7fJUS7RHfz9C-F4nYUHisOC8H=tXs1SpQ@mail.gmail.com>
 <752875199.5065164.1565468068845@mail.yahoo.com>
Message-ID: <604cab3b-b6f5-189e-6c61-404233675cb3@sapo.pt>

Hello,

The following code might not be the best way of computing the integrals 
for all combinations of the arguments but it gets the job done and I 
believe it's readable code.


f2 <- function(a, b, c){
   integrate(function(x) {exp(-a*x^3 - b*x^2 - c*x)},
             lower = 0, upper = Inf)
}

f2_all_args <- function(a, b, c){
   lapply(a, function(.a)
     lapply(b, function(.b)
       lapply(c, function(.c)
         f2(.a, .b, .c)[1:2]
       )
     )
   )
}

a <- seq(from = 0, to = 1, by = 0.5)
b <- seq(from = 5, to = 10, by = 1)
m <- seq(from = 10, to = 20, by = 5)

res <- f2_all_args(a, b, m)

r <- array(0, c(3, 6, 3))
r.error <- array(0, c(3, 6, 3))

for(i in seq_along(a)){
   res_i <- res[[i]]
   for(j in seq_along(b)){
     res_i_j <- res_i[[j]]
     for(k in seq_along(m)){
       r[i, j, k] <- res_i_j[[k]]$value
       r.error[i, j, k] <- res_i_j[[k]]$abs.error
     }
   }
}


Hope this helps,

Rui Barradas


?s 21:14 de 10/08/19, ravi via R-help escreveu:
>   Bert,Thanks a lot for your help. I have a few follow-up questions (shown in the comment lines).? Numerical values and error for a (i) vector and (ii) array.
> 
> a <- seq(from=0,to=1,by=0.5)
> b <- seq(from=5,to=10,by=1)
> m <- seq(from=10,to=20,by=5)
> 
> f2 <- function(a,b,m)integrate(function(x){exp(-a*x^3-b*x^2-m*x)},lower=0,upper = Inf)
> fv <- Vectorize(f2,vectorize.args=c("a","b","m"),SIMPLIFY=TRUE)
> 
> r1 <- fv(a,b,m)
> # How to get just the numerical results as an array
> # for scalar values of a,b and m, I can get it with
> 
> #s <- integrate(f,0,Inf)$value
> # How do I get this for a vector
> 
> # Finally, if I want an array for all the values of a, b and m, how should I proceed?
> r <- array(0,c(3,6,3))
> r.error <- array(0,c(3,6,3))
> # I want the results of the vectorized integrate function in the above arrays. How do I extract these values?
> 
> Thanks,Ravi
> 
> 
>      On Saturday, 10 August 2019, 20:03:22 CEST, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>   
>   Ravi:
> First of all, you're calling Vectorize incorrectly. The first argument must be a function *name*, not a function call. Here's what you need to do (and thanks for the reprex -- wouldn't have been able to help without it!):
> f2 <- function(a,b,c)integrate(function(x){exp(-a*x^3-b*x^2-c*x)},lower =0,upper = Inf)
> fv <- Vectorize(f2,vectorize.args=c("a","b","c"),SIMPLIFY=TRUE)
> Second of all, as you may realize, the vectorization uses mapply() and so vectorizes the c(a,b,c) triplet "in parallel," which will "recycle" shorter arguments to the length of longer. Hence you will get 6 results from your example:
>> fv(a,b,m)
>  ? ? ? ? ? ? ?[,1] ? ? ? ? [,2] ? ? ? ? [,3] ? ? ? ?[,4] ? ? ? ? [,5] ? ? ? ? [,6]
> value ? ? ? ?0.09207851 ? 0.0635289 ? ?0.04837997 ?0.08856628 ? 0.06224138 ? 0.04777941
> abs.error ? ?3.365173e-08 3.108388e-06 1.00652e-09 3.284876e-09 1.796619e-08 6.348142e-09
> subdivisions 2 ? ? ? ? ? ?1 ? ? ? ? ? ?2 ? ? ? ? ? 2 ? ? ? ? ? ?2 ? ? ? ? ? ?2
> message ? ? ?"OK" ? ? ? ? "OK" ? ? ? ? "OK" ? ? ? ?"OK" ? ? ? ? "OK" ? ? ? ? "OK"
> call ? ? ? ? Expression ? Expression ? Expression ?Expression ? Expression ? Expression
> 
> Cheers,Bert
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
> On Sat, Aug 10, 2019 at 10:20 AM ravi via R-help <r-help at r-project.org> wrote:
> 
> Hi all,I am having some difficulties in vectorizing the integrate function. Let me explain with an example.
> a <- 10; b <- 3; c <- 4
> f <- function(x) {exp(-a*x^3-b*x^2-c*x)}
> integrate(f,0,Inf) # works fine
> 
> My difficulties start when I want to vectorize.
> 
> # attempts to vectorize fail
> a <- seq(from=0,to=1,by=0.5)
> b <- seq(from=5,to=10,by=1)
> m <- seq(from=10,to=20,by=5)
> f2 <- function(x,a,b,c) {exp(-a*x^3-b*x^2-m*x)}
> fv <- Vectorize(integrate(f2,0,Inf),vectorize.args=c("a","b","m"),SIMPLIFY=TRUE)
> 
> I want the result as a 3-d array with dimensions of the lengths of a, b and c. I have tried several variants but am not having much luck. Will appreciate any help that I can get.
> Thanks,Ravi Sutradhara
> 
> 
> 
> 
>  ? ? ? ? [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>    
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bgunter@4567 @end|ng |rom gm@||@com  Sun Aug 11 00:51:49 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 10 Aug 2019 15:51:49 -0700
Subject: [R] vectorizing the integrate function
In-Reply-To: <604cab3b-b6f5-189e-6c61-404233675cb3@sapo.pt>
References: <1395066570.5017281.1565457613467.ref@mail.yahoo.com>
 <1395066570.5017281.1565457613467@mail.yahoo.com>
 <CAGxFJbTtfu_Uo1chv7fJUS7RHfz9C-F4nYUHisOC8H=tXs1SpQ@mail.gmail.com>
 <752875199.5065164.1565468068845@mail.yahoo.com>
 <604cab3b-b6f5-189e-6c61-404233675cb3@sapo.pt>
Message-ID: <CAGxFJbRJVe2Uf_Rg1gpOUoVmrUCt7soVheANVmLS9++qwFo0Tg@mail.gmail.com>

Cleaner, I think, is to use ?expand.grid and ?do.call, noting that any
n-column data frame is also an n-component list.

Using your example as before:

> f2 <- function(a,b,m)integrate(function(x){exp(-a*x^3-b*x^2-m*x)},lower
=0,upper = Inf)
> fv <- Vectorize(f2,vectorize.args=c("a","b","m"),SIMPLIFY=TRUE)

## Now use expand.grid to get all combinations of a,b, and m in a data frame

> allvals <- expand.grid(a=a, b=b, m=m)
## Here's what it looks like (note the **named** columns to avoid problems)
> head(allvals)
    a b  m
1 0.0 5 10
2 0.5 5 10
3 1.0 5 10
4 0.0 6 10
5 0.5 6 10
6 1.0 6 10
## etc.  3 x 6 x 3 = 54 rows in all

## now use do.call to do fv() on these three column vectors (considered as
a list of 3 components)

> results <- do.call(fv, allvals)

## So what does results look like?
> class(results)
[1] "matrix"
> dim(results)
[1]  5 54
> dimnames(results)[[1]]
[1] "value"        "abs.error"    "subdivisions" "message"      "call"

## So the first row of the matrix contains the values. If you extract them
by results[1, ],
## you'll find, however, that it's a list of 54 components, each of which
is a vector with
## one value. So unlist it first to avoid this:

> vals <- unlist(results[1,])
> vals
 [1] 0.09207851 0.09193111 0.09178672 0.09083412 0.09070066 0.09056960
0.08966646 0.08954474
 [9] 0.08942499 0.08856628 0.08845459 0.08834454 0.08752598 0.08742295
0.08732131 0.08653926
[17] 0.08644379 0.08634949 0.06403081 0.06398995 0.06394947 0.06356740
0.06352890 0.06349075
[25] 0.06312099 0.06308463 0.06304856 0.06269028 0.06265583 0.06262164
0.06227409 0.06224138
[33] 0.06220891 0.06187141 0.06184029 0.06180937 0.04883372 0.04881874
0.04880384 0.04861813
[41] 0.04860372 0.04858939 0.04840766 0.04839378 0.04837997 0.04820203
0.04818865 0.04817532
[49] 0.04800099 0.04798808 0.04797521 0.04780433 0.04779185 0.04777941

You can either convert this into an array or just add it as a column to
allvals. The results are listed in
the order given there.

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Aug 10, 2019 at 3:03 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> The following code might not be the best way of computing the integrals
> for all combinations of the arguments but it gets the job done and I
> believe it's readable code.
>
>
> f2 <- function(a, b, c){
>    integrate(function(x) {exp(-a*x^3 - b*x^2 - c*x)},
>              lower = 0, upper = Inf)
> }
>
> f2_all_args <- function(a, b, c){
>    lapply(a, function(.a)
>      lapply(b, function(.b)
>        lapply(c, function(.c)
>          f2(.a, .b, .c)[1:2]
>        )
>      )
>    )
> }
>
> a <- seq(from = 0, to = 1, by = 0.5)
> b <- seq(from = 5, to = 10, by = 1)
> m <- seq(from = 10, to = 20, by = 5)
>
> res <- f2_all_args(a, b, m)
>
> r <- array(0, c(3, 6, 3))
> r.error <- array(0, c(3, 6, 3))
>
> for(i in seq_along(a)){
>    res_i <- res[[i]]
>    for(j in seq_along(b)){
>      res_i_j <- res_i[[j]]
>      for(k in seq_along(m)){
>        r[i, j, k] <- res_i_j[[k]]$value
>        r.error[i, j, k] <- res_i_j[[k]]$abs.error
>      }
>    }
> }
>
>
> Hope this helps,
>
> Rui Barradas
>
>
> ?s 21:14 de 10/08/19, ravi via R-help escreveu:
> >   Bert,Thanks a lot for your help. I have a few follow-up questions
> (shown in the comment lines).  Numerical values and error for a (i) vector
> and (ii) array.
> >
> > a <- seq(from=0,to=1,by=0.5)
> > b <- seq(from=5,to=10,by=1)
> > m <- seq(from=10,to=20,by=5)
> >
> > f2 <-
> function(a,b,m)integrate(function(x){exp(-a*x^3-b*x^2-m*x)},lower=0,upper =
> Inf)
> > fv <- Vectorize(f2,vectorize.args=c("a","b","m"),SIMPLIFY=TRUE)
> >
> > r1 <- fv(a,b,m)
> > # How to get just the numerical results as an array
> > # for scalar values of a,b and m, I can get it with
> >
> > #s <- integrate(f,0,Inf)$value
> > # How do I get this for a vector
> >
> > # Finally, if I want an array for all the values of a, b and m, how
> should I proceed?
> > r <- array(0,c(3,6,3))
> > r.error <- array(0,c(3,6,3))
> > # I want the results of the vectorized integrate function in the above
> arrays. How do I extract these values?
> >
> > Thanks,Ravi
> >
> >
> >      On Saturday, 10 August 2019, 20:03:22 CEST, Bert Gunter <
> bgunter.4567 at gmail.com> wrote:
> >
> >   Ravi:
> > First of all, you're calling Vectorize incorrectly. The first argument
> must be a function *name*, not a function call. Here's what you need to do
> (and thanks for the reprex -- wouldn't have been able to help without it!):
> > f2 <- function(a,b,c)integrate(function(x){exp(-a*x^3-b*x^2-c*x)},lower
> =0,upper = Inf)
> > fv <- Vectorize(f2,vectorize.args=c("a","b","c"),SIMPLIFY=TRUE)
> > Second of all, as you may realize, the vectorization uses mapply() and
> so vectorizes the c(a,b,c) triplet "in parallel," which will "recycle"
> shorter arguments to the length of longer. Hence you will get 6 results
> from your example:
> >> fv(a,b,m)
> >               [,1]         [,2]         [,3]        [,4]         [,5]
>       [,6]
> > value        0.09207851   0.0635289    0.04837997  0.08856628
> 0.06224138   0.04777941
> > abs.error    3.365173e-08 3.108388e-06 1.00652e-09 3.284876e-09
> 1.796619e-08 6.348142e-09
> > subdivisions 2            1            2           2            2
>      2
> > message      "OK"         "OK"         "OK"        "OK"         "OK"
>     "OK"
> > call         Expression   Expression   Expression  Expression
> Expression   Expression
> >
> > Cheers,Bert
> > Bert Gunter
> >
> > "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> > -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >
> >
> > On Sat, Aug 10, 2019 at 10:20 AM ravi via R-help <r-help at r-project.org>
> wrote:
> >
> > Hi all,I am having some difficulties in vectorizing the integrate
> function. Let me explain with an example.
> > a <- 10; b <- 3; c <- 4
> > f <- function(x) {exp(-a*x^3-b*x^2-c*x)}
> > integrate(f,0,Inf) # works fine
> >
> > My difficulties start when I want to vectorize.
> >
> > # attempts to vectorize fail
> > a <- seq(from=0,to=1,by=0.5)
> > b <- seq(from=5,to=10,by=1)
> > m <- seq(from=10,to=20,by=5)
> > f2 <- function(x,a,b,c) {exp(-a*x^3-b*x^2-m*x)}
> > fv <-
> Vectorize(integrate(f2,0,Inf),vectorize.args=c("a","b","m"),SIMPLIFY=TRUE)
> >
> > I want the result as a 3-d array with dimensions of the lengths of a, b
> and c. I have tried several variants but am not having much luck. Will
> appreciate any help that I can get.
> > Thanks,Ravi Sutradhara
> >
> >
> >
> >
> >          [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>

	[[alternative HTML version deleted]]


From rv15| @end|ng |rom y@hoo@@e  Sun Aug 11 00:56:38 2019
From: rv15| @end|ng |rom y@hoo@@e (ravi)
Date: Sat, 10 Aug 2019 22:56:38 +0000 (UTC)
Subject: [R] vectorizing the integrate function
In-Reply-To: <CAGxFJbTtfu_Uo1chv7fJUS7RHfz9C-F4nYUHisOC8H=tXs1SpQ@mail.gmail.com>
References: <1395066570.5017281.1565457613467.ref@mail.yahoo.com>
 <1395066570.5017281.1565457613467@mail.yahoo.com>
 <CAGxFJbTtfu_Uo1chv7fJUS7RHfz9C-F4nYUHisOC8H=tXs1SpQ@mail.gmail.com>
Message-ID: <672135830.5097072.1565477798138@mail.yahoo.com>

 Rui,Thanks for your help in getting the result with for loops. That is useful, but I have a complicated integral and I am interested in improving the performance with the benefit of vectorization.
In the solution from Bert, I would like to know how I can extract the numerical vector from the function fv. That is, extract the fv$value as a vector. I can then perhaps try to improvise and extend it to arrays.Thanks,Ravi

    On Saturday, 10 August 2019, 20:03:22 CEST, Bert Gunter <bgunter.4567 at gmail.com> wrote:  
 
 Ravi:
First of all, you're calling Vectorize incorrectly. The first argument must be a function *name*, not a function call. Here's what you need to do (and thanks for the reprex -- wouldn't have been able to help without it!):
f2 <- function(a,b,c)integrate(function(x){exp(-a*x^3-b*x^2-c*x)},lower =0,upper = Inf)
fv <- Vectorize(f2,vectorize.args=c("a","b","c"),SIMPLIFY=TRUE)
Second of all, as you may realize, the vectorization uses mapply() and so vectorizes the c(a,b,c) triplet "in parallel," which will "recycle" shorter arguments to the length of longer. Hence you will get 6 results from your example:
> fv(a,b,m)
? ? ? ? ? ? ?[,1] ? ? ? ? [,2] ? ? ? ? [,3] ? ? ? ?[,4] ? ? ? ? [,5] ? ? ? ? [,6] ? ? ? ?
value ? ? ? ?0.09207851 ? 0.0635289 ? ?0.04837997 ?0.08856628 ? 0.06224138 ? 0.04777941 ?
abs.error ? ?3.365173e-08 3.108388e-06 1.00652e-09 3.284876e-09 1.796619e-08 6.348142e-09
subdivisions 2 ? ? ? ? ? ?1 ? ? ? ? ? ?2 ? ? ? ? ? 2 ? ? ? ? ? ?2 ? ? ? ? ? ?2 ? ? ? ? ? 
message ? ? ?"OK" ? ? ? ? "OK" ? ? ? ? "OK" ? ? ? ?"OK" ? ? ? ? "OK" ? ? ? ? "OK" ? ? ? ?
call ? ? ? ? Expression ? Expression ? Expression ?Expression ? Expression ? Expression? 

Cheers,Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Aug 10, 2019 at 10:20 AM ravi via R-help <r-help at r-project.org> wrote:

Hi all,I am having some difficulties in vectorizing the integrate function. Let me explain with an example.
a <- 10; b <- 3; c <- 4
f <- function(x) {exp(-a*x^3-b*x^2-c*x)}
integrate(f,0,Inf) # works fine

My difficulties start when I want to vectorize.

# attempts to vectorize fail
a <- seq(from=0,to=1,by=0.5)
b <- seq(from=5,to=10,by=1)
m <- seq(from=10,to=20,by=5)
f2 <- function(x,a,b,c) {exp(-a*x^3-b*x^2-m*x)}
fv <- Vectorize(integrate(f2,0,Inf),vectorize.args=c("a","b","m"),SIMPLIFY=TRUE)

I want the result as a 3-d array with dimensions of the lengths of a, b and c. I have tried several variants but am not having much luck. Will appreciate any help that I can get.
Thanks,Ravi Sutradhara




? ? ? ? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

  
	[[alternative HTML version deleted]]


From rv15| @end|ng |rom y@hoo@@e  Sun Aug 11 01:01:01 2019
From: rv15| @end|ng |rom y@hoo@@e (ravi)
Date: Sat, 10 Aug 2019 23:01:01 +0000 (UTC)
Subject: [R] vectorizing the integrate function
In-Reply-To: <CAGxFJbRJVe2Uf_Rg1gpOUoVmrUCt7soVheANVmLS9++qwFo0Tg@mail.gmail.com>
References: <1395066570.5017281.1565457613467.ref@mail.yahoo.com>
 <1395066570.5017281.1565457613467@mail.yahoo.com>
 <CAGxFJbTtfu_Uo1chv7fJUS7RHfz9C-F4nYUHisOC8H=tXs1SpQ@mail.gmail.com>
 <752875199.5065164.1565468068845@mail.yahoo.com>
 <604cab3b-b6f5-189e-6c61-404233675cb3@sapo.pt>
 <CAGxFJbRJVe2Uf_Rg1gpOUoVmrUCt7soVheANVmLS9++qwFo0Tg@mail.gmail.com>
Message-ID: <754561512.5094665.1565478061441@mail.yahoo.com>

 Bert,Thanks a lot. This meets all my expectations for the moment. Thanks.Ravi

    On Sunday, 11 August 2019, 00:52:02 CEST, Bert Gunter <bgunter.4567 at gmail.com> wrote:  
 
 Cleaner, I think, is to use ?expand.grid and ?do.call, noting that any n-column data frame is also an n-component list.
Using your example as before:
> f2 <- function(a,b,m)integrate(function(x){exp(-a*x^3-b*x^2-m*x)},lower =0,upper = Inf)
> fv <- Vectorize(f2,vectorize.args=c("a","b","m"),SIMPLIFY=TRUE)
?## Now use expand.grid to get all combinations of a,b, and m in a data frame
> allvals <- expand.grid(a=a, b=b, m=m)## Here's what it looks like (note the **named** columns to avoid problems)> head(allvals)
? ? a b ?m
1 0.0 5 10
2 0.5 5 10
3 1.0 5 10
4 0.0 6 10
5 0.5 6 10
6 1.0 6 10## etc.? 3 x 6 x 3 = 54 rows in all
## now use do.call to do fv() on these three column vectors (considered as a list of 3 components)
> results <- do.call(fv, allvals)
## So what does results look like?
> class(results)
[1] "matrix"
> dim(results)
[1] ?5 54
> dimnames(results)[[1]]
[1] "value" ? ? ? ?"abs.error" ? ?"subdivisions" "message" ? ? ?"call" ? ?? 

## So the first row of the matrix contains the values. If you extract them by results[1, ], 
## you'll find, however, that it's a list of 54 components, each of which is a vector with
## one value. So unlist it first to avoid this:
> vals <- unlist(results[1,])
> vals
?[1] 0.09207851 0.09193111 0.09178672 0.09083412 0.09070066 0.09056960 0.08966646 0.08954474
?[9] 0.08942499 0.08856628 0.08845459 0.08834454 0.08752598 0.08742295 0.08732131 0.08653926
[17] 0.08644379 0.08634949 0.06403081 0.06398995 0.06394947 0.06356740 0.06352890 0.06349075
[25] 0.06312099 0.06308463 0.06304856 0.06269028 0.06265583 0.06262164 0.06227409 0.06224138
[33] 0.06220891 0.06187141 0.06184029 0.06180937 0.04883372 0.04881874 0.04880384 0.04861813
[41] 0.04860372 0.04858939 0.04840766 0.04839378 0.04837997 0.04820203 0.04818865 0.04817532
[49] 0.04800099 0.04798808 0.04797521 0.04780433 0.04779185 0.04777941
You can either convert this into an array or just add it as a column to allvals. The results are listed in 
the order given there.
Cheers,BertBert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Aug 10, 2019 at 3:03 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:

Hello,

The following code might not be the best way of computing the integrals 
for all combinations of the arguments but it gets the job done and I 
believe it's readable code.


f2 <- function(a, b, c){
? ?integrate(function(x) {exp(-a*x^3 - b*x^2 - c*x)},
? ? ? ? ? ? ?lower = 0, upper = Inf)
}

f2_all_args <- function(a, b, c){
? ?lapply(a, function(.a)
? ? ?lapply(b, function(.b)
? ? ? ?lapply(c, function(.c)
? ? ? ? ?f2(.a, .b, .c)[1:2]
? ? ? ?)
? ? ?)
? ?)
}

a <- seq(from = 0, to = 1, by = 0.5)
b <- seq(from = 5, to = 10, by = 1)
m <- seq(from = 10, to = 20, by = 5)

res <- f2_all_args(a, b, m)

r <- array(0, c(3, 6, 3))
r.error <- array(0, c(3, 6, 3))

for(i in seq_along(a)){
? ?res_i <- res[[i]]
? ?for(j in seq_along(b)){
? ? ?res_i_j <- res_i[[j]]
? ? ?for(k in seq_along(m)){
? ? ? ?r[i, j, k] <- res_i_j[[k]]$value
? ? ? ?r.error[i, j, k] <- res_i_j[[k]]$abs.error
? ? ?}
? ?}
}


Hope this helps,

Rui Barradas


?s 21:14 de 10/08/19, ravi via R-help escreveu:
>? ?Bert,Thanks a lot for your help. I have a few follow-up questions (shown in the comment lines).? Numerical values and error for a (i) vector and (ii) array.
> 
> a <- seq(from=0,to=1,by=0.5)
> b <- seq(from=5,to=10,by=1)
> m <- seq(from=10,to=20,by=5)
> 
> f2 <- function(a,b,m)integrate(function(x){exp(-a*x^3-b*x^2-m*x)},lower=0,upper = Inf)
> fv <- Vectorize(f2,vectorize.args=c("a","b","m"),SIMPLIFY=TRUE)
> 
> r1 <- fv(a,b,m)
> # How to get just the numerical results as an array
> # for scalar values of a,b and m, I can get it with
> 
> #s <- integrate(f,0,Inf)$value
> # How do I get this for a vector
> 
> # Finally, if I want an array for all the values of a, b and m, how should I proceed?
> r <- array(0,c(3,6,3))
> r.error <- array(0,c(3,6,3))
> # I want the results of the vectorized integrate function in the above arrays. How do I extract these values?
> 
> Thanks,Ravi
> 
> 
>? ? ? On Saturday, 10 August 2019, 20:03:22 CEST, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>? ?
>? ?Ravi:
> First of all, you're calling Vectorize incorrectly. The first argument must be a function *name*, not a function call. Here's what you need to do (and thanks for the reprex -- wouldn't have been able to help without it!):
> f2 <- function(a,b,c)integrate(function(x){exp(-a*x^3-b*x^2-c*x)},lower =0,upper = Inf)
> fv <- Vectorize(f2,vectorize.args=c("a","b","c"),SIMPLIFY=TRUE)
> Second of all, as you may realize, the vectorization uses mapply() and so vectorizes the c(a,b,c) triplet "in parallel," which will "recycle" shorter arguments to the length of longer. Hence you will get 6 results from your example:
>> fv(a,b,m)
>? ? ? ? ? ? ? ?[,1] ? ? ? ? [,2] ? ? ? ? [,3] ? ? ? ?[,4] ? ? ? ? [,5] ? ? ? ? [,6]
> value ? ? ? ?0.09207851 ? 0.0635289 ? ?0.04837997 ?0.08856628 ? 0.06224138 ? 0.04777941
> abs.error ? ?3.365173e-08 3.108388e-06 1.00652e-09 3.284876e-09 1.796619e-08 6.348142e-09
> subdivisions 2 ? ? ? ? ? ?1 ? ? ? ? ? ?2 ? ? ? ? ? 2 ? ? ? ? ? ?2 ? ? ? ? ? ?2
> message ? ? ?"OK" ? ? ? ? "OK" ? ? ? ? "OK" ? ? ? ?"OK" ? ? ? ? "OK" ? ? ? ? "OK"
> call ? ? ? ? Expression ? Expression ? Expression ?Expression ? Expression ? Expression
> 
> Cheers,Bert
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
> On Sat, Aug 10, 2019 at 10:20 AM ravi via R-help <r-help at r-project.org> wrote:
> 
> Hi all,I am having some difficulties in vectorizing the integrate function. Let me explain with an example.
> a <- 10; b <- 3; c <- 4
> f <- function(x) {exp(-a*x^3-b*x^2-c*x)}
> integrate(f,0,Inf) # works fine
> 
> My difficulties start when I want to vectorize.
> 
> # attempts to vectorize fail
> a <- seq(from=0,to=1,by=0.5)
> b <- seq(from=5,to=10,by=1)
> m <- seq(from=10,to=20,by=5)
> f2 <- function(x,a,b,c) {exp(-a*x^3-b*x^2-m*x)}
> fv <- Vectorize(integrate(f2,0,Inf),vectorize.args=c("a","b","m"),SIMPLIFY=TRUE)
> 
> I want the result as a 3-d array with dimensions of the lengths of a, b and c. I have tried several variants but am not having much luck. Will appreciate any help that I can get.
> Thanks,Ravi Sutradhara
> 
> 
> 
> 
>? ? ? ? ? [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>? ? 
>? ? ? ?[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

  
	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun Aug 11 01:40:35 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 10 Aug 2019 16:40:35 -0700
Subject: [R] vectorizing the integrate function
In-Reply-To: <672135830.5097072.1565477798138@mail.yahoo.com>
References: <1395066570.5017281.1565457613467.ref@mail.yahoo.com>
 <1395066570.5017281.1565457613467@mail.yahoo.com>
 <CAGxFJbTtfu_Uo1chv7fJUS7RHfz9C-F4nYUHisOC8H=tXs1SpQ@mail.gmail.com>
 <672135830.5097072.1565477798138@mail.yahoo.com>
Message-ID: <CAGxFJbQwuaAsSxKrqE55njxsypVRJ5bia-LuXivS=fOD+T=p7A@mail.gmail.com>

Ravi:

I believe you are under a misconception. The Vectorize() function
vectorizes the function call, **but it does not vectorize the computation
by moving loops down to the C level**, which is typically what is meant
when it is recommended that R users use inbuilt vectorized functions when
possible to move looping from the interpreted (R) level to the compiled (C
level) code. That is, mapply() -- and thus Vectorize, which is just a
wrapper for mapply -- is still looping at the interpreted level and
therefore is essentially the same as explicit iterative or *apply loops. It
may be a bit faster or slower, depending, but not the orders of magnitude
faster that one can get with true vectorized (at the C level) code.

Finally, if you want to just add the integration results to the allvals
list, here's a
slightly different one-liner to do it:

> allvals$vals <- unlist(with(allvals,fv(a,b,m))[1,])
> head(allvals)
    a b  m       vals
1 0.0 5 10 0.09207851
2 0.5 5 10 0.09193111
3 1.0 5 10 0.09178672
4 0.0 6 10 0.09083412
5 0.5 6 10 0.09070066
6 1.0 6 10 0.09056960

or the one liner using the previous do.call() construction that just
extracts the vector of values:

> vals <- unlist(do.call(fv,allvals)[1,])
Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Aug 10, 2019 at 3:56 PM ravi <rv15i at yahoo.se> wrote:

> Rui,
> Thanks for your help in getting the result with for loops. That is useful,
> but I have a complicated integral and I am interested in improving the
> performance with the benefit of vectorization.
>
> In the solution from Bert, I would like to know how I can extract the
> numerical vector from the function fv. That is, extract the fv$value as a
> vector. I can then perhaps try to improvise and extend it to arrays.
> Thanks,
> Ravi
>
> On Saturday, 10 August 2019, 20:03:22 CEST, Bert Gunter <
> bgunter.4567 at gmail.com> wrote:
>
>
> Ravi:
>
> First of all, you're calling Vectorize incorrectly. The first argument
> must be a function *name*, not a function call. Here's what you need to do
> (and thanks for the reprex -- wouldn't have been able to help without it!):
>
> f2 <- function(a,b,c)integrate(function(x){exp(-a*x^3-b*x^2-c*x)},lower
> =0,upper = Inf)
> fv <- Vectorize(f2,vectorize.args=c("a","b","c"),SIMPLIFY=TRUE)
>
> Second of all, as you may realize, the vectorization uses mapply() and so
> vectorizes the c(a,b,c) triplet "in parallel," which will "recycle" shorter
> arguments to the length of longer. Hence you will get 6 results from your
> example:
>
> > fv(a,b,m)
>              [,1]         [,2]         [,3]        [,4]         [,5]
>   [,6]
> value        0.09207851   0.0635289    0.04837997  0.08856628   0.06224138
>   0.04777941
> abs.error    3.365173e-08 3.108388e-06 1.00652e-09 3.284876e-09
> 1.796619e-08 6.348142e-09
> subdivisions 2            1            2           2            2
>    2
> message      "OK"         "OK"         "OK"        "OK"         "OK"
>   "OK"
> call         Expression   Expression   Expression  Expression   Expression
>   Expression
>
> Cheers,
> Bert
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Sat, Aug 10, 2019 at 10:20 AM ravi via R-help <r-help at r-project.org>
> wrote:
>
> Hi all,I am having some difficulties in vectorizing the integrate
> function. Let me explain with an example.
> a <- 10; b <- 3; c <- 4
> f <- function(x) {exp(-a*x^3-b*x^2-c*x)}
> integrate(f,0,Inf) # works fine
>
> My difficulties start when I want to vectorize.
>
> # attempts to vectorize fail
> a <- seq(from=0,to=1,by=0.5)
> b <- seq(from=5,to=10,by=1)
> m <- seq(from=10,to=20,by=5)
> f2 <- function(x,a,b,c) {exp(-a*x^3-b*x^2-m*x)}
> fv <-
> Vectorize(integrate(f2,0,Inf),vectorize.args=c("a","b","m"),SIMPLIFY=TRUE)
>
> I want the result as a 3-d array with dimensions of the lengths of a, b
> and c. I have tried several variants but am not having much luck. Will
> appreciate any help that I can get.
> Thanks,Ravi Sutradhara
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Sun Aug 11 02:32:22 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Sat, 10 Aug 2019 17:32:22 -0700
Subject: [R] Using read.table for importing gz file
In-Reply-To: <CAPQaxLPZQ9QB+sROHNbVpzwaSq2bDz=bbUxTv0VFedprKt=b5g@mail.gmail.com>
References: <CAPQaxLPG9YHBaZOpQhZCfwCjCQsrnRDK9mYZ53kYgkJ9ZC66oA@mail.gmail.com>
 <36ee7a45-d045-444e-ecb1-aa5330aa5139@comcast.net>
 <CAPQaxLPZQ9QB+sROHNbVpzwaSq2bDz=bbUxTv0VFedprKt=b5g@mail.gmail.com>
Message-ID: <8825c89c-022e-3372-d5bc-e18fb1fc5792@comcast.net>

Well, let's see about "rules"? ... you posted in HTML when this is a 
plain text mailing list and then you replied to only me when you are 
supposed reply to the list (so I'm putting back the list address in my 
reply:


When I copied your code and then attempted to do a bit of debugging I get:


 > z <- 
readLines(gzcon(url(?https://TCGA.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz?)), 
n = 100)
Error: unexpected input in "z <- readLines(gzcon(url(?"

# that was because you had "smart-quotes" rather than ASCII quotes:


 > z <- readLines(gzcon(url( 
'https://TCGA.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz' 
)), n = 100)
 > z[1:10]
 ?[1] 
"sample\tTCGA-E1-5319-01\tTCGA-HT-7693-01\tTCGA-CS-6665-01\tTCGA-S9-A7J2-01\tTCGA-FG-A6J3-01\tTCGA-FG-6688-01\tTCGA-S9-A6TX-01\tTCGA-VM-A8C8-01\tTCGA-74-6577-01\tTCGA-06-AABW-11\tTCGA-06-0125-02\tTCGA-HT-A74L-01\tTCGA-26-A7UX-01\tTCGA-DU-A5TS-01\tTCGA-06-6388-01\tTCGA-DB-A4XA-01\tTCGA-06-A7TL-01\tTCGA-HT-A4DV-01\tTCGA-TQ-A7RP-01\tTCGA-E1-5311-01\tTCGA-28-5213-01\tTCGA-E1-A7YI-01\tTCGA-E1-5305-01\tTCGA-F6-A8O4-01\tTCGA-HT-8113-01\tTCGA-DH-A66G-01\tTCGA-76-4932-01\t

Snipped hundreds of lines. So this seems to indicate that this is a tab 
separated file. Don't you have some documentation to refer to?


This seems possibly useful:


 > z <- read.table( 
text=readLines(gzcon(url('https://TCGA.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz')), 
n = 100), header=TRUE, sep="\t")
 > str(z)
'data.frame':??? 99 obs. of? 686 variables:
 ?$ sample???????? : Factor w/ 99 levels "cg00036732","cg00651829",..: 
53 2 60 41 16 13 37 20 70 21 ...
 ?$ TCGA.E1.5319.01: num? 0.4019 0.0215 0.053 0.0453 0.515 ...
 ?$ TCGA.HT.7693.01: num? 0.9364 0.0216 0.0547 0.0819 0.6129 ...
 ?$ TCGA.CS.6665.01: num? 0.0345 0.0164 0.0719 0.0497 0.6648 ...
 ?$ TCGA.S9.A7J2.01: num? 0.0295 0.0168 0.0421 0.0867 0.1657 ...
 ?$ TCGA.FG.A6J3.01: num? 0.0248 0.0161 0.0556 0.0902 0.5042 ...
 ?$ TCGA.FG.6688.01: num? 0.0203 0.0179 0.0321 0.0513 0.1075 ...
 ?$ TCGA.S9.A6TX.01: num? 0.0378 0.0199 0.0623 0.0992 0.7662 ...
 ?$ TCGA.VM.A8C8.01: num? 0.0271 0.0172 0.0466 0.0564 0.3478 ...
 ?$ TCGA.74.6577.01: num? 0.0237 0.0193 0.0196 0.0961 0.1242 ...
 ?$ TCGA.06.AABW.11: num? 0.0323 0.0156 0.0395 0.0708 0.1136 ...
 ?$ TCGA.06.0125.02: num? 0.0238 0.0181 0.039 0.068 0.0796 ...
 ?$ TCGA.HT.A74L.01: num? 0.7409 0.0221 0.0596 0.0765 0.8157 ...

#snipped the output

# there seemed to be 686 columns


-- 

David.



On 8/10/19 3:07 PM, Spencer Brackett wrote:
> I?ve tried z <- 
> readLines(gzcon(url(?https://TCGA.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz?)), 
> n = 100)
>
> Which prints out the indicated 10 rows, but I can not seem to run the 
> same code excluding the n = 100 without R stalling and me being forced 
> to close the program. All I am trying to do is ensure that the whole 
> file is imported into R so that I can proceed with a survival analysis.
>
> Also, what particular rule of the mailing list did I break? I 
> apologize in advance, as I thought that code specific queries like the 
> one I asked were acceptable.
>
> Many thanks,
>
> Spencer
>
> On Sat, Aug 10, 2019 at 5:51 PM David Winsemius 
> <dwinsemius at comcast.net <mailto:dwinsemius at comcast.net>> wrote:
>
>     Have you tried using readLines in the manner illustrated on the
>     ?gzfile
>     help page?
>
>
>     David.
>
>     On 8/10/19 12:29 PM, Spencer Brackett wrote:
>     > Hello,
>     >
>     > I am trying to read the following Xena dataset into R for data
>     analysis:
>     >
>     https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
>     >
>     > I tried to run the following
>     read.table(gzfile("HumanMethylation450.gz")),
>     > but R ended up crashing as a result.
>     >
>     > Is there perhaps a way to use read.table with fread in some way
>     to do this?
>     >
>     > Many thanks,
>     >
>     > Spencer
>     >
>     >? ? ? ?[[alternative HTML version deleted]]
>     >
>     > ______________________________________________
>     > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>     -- To UNSUBSCRIBE and more, see
>     > https://stat.ethz.ch/mailman/listinfo/r-help
>     > PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     > and provide commented, minimal, self-contained, reproducible code.
>


From dw|n@em|u@ @end|ng |rom comc@@t@net  Sun Aug 11 02:47:11 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Sat, 10 Aug 2019 17:47:11 -0700
Subject: [R] Using read.table for importing gz file
In-Reply-To: <8825c89c-022e-3372-d5bc-e18fb1fc5792@comcast.net>
References: <CAPQaxLPG9YHBaZOpQhZCfwCjCQsrnRDK9mYZ53kYgkJ9ZC66oA@mail.gmail.com>
 <36ee7a45-d045-444e-ecb1-aa5330aa5139@comcast.net>
 <CAPQaxLPZQ9QB+sROHNbVpzwaSq2bDz=bbUxTv0VFedprKt=b5g@mail.gmail.com>
 <8825c89c-022e-3372-d5bc-e18fb1fc5792@comcast.net>
Message-ID: <fac6eb2f-a8b6-016d-4e4a-4fb09a09bece@comcast.net>

Further note:


After three minutes of waiting? ... not a particularly long wait in my 
opinion, I get this:


 > z <- read.table( 
text=readLines(gzcon(url('https://TCGA.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz')) 
), header=TRUE, sep="\t")
 > dim(z)
[1] 485577??? 686

So almost half a million lines of data in a rather wide dataset for an 
incompletely described file.


I'd say R seems to be "working" properly.

data.table::fread was more informative about the process but acheived 
basically the same result in 1/6th the time:


 ??fread
system.time( z <- 
fread('https://TCGA.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz', 
sep="\t")? )

#-----------

[100%] Downloaded 597770433 bytes...
 ?? user? system elapsed
 ?20.682?? 3.322? 29.292

 > dim(z)
[1] 485577??? 686

-- 

David.

On 8/10/19 5:32 PM, David Winsemius wrote:
> Well, let's see about "rules"? ... you posted in HTML when this is a 
> plain text mailing list and then you replied to only me when you are 
> supposed reply to the list (so I'm putting back the list address in my 
> reply:
>
>
> When I copied your code and then attempted to do a bit of debugging I 
> get:
>
>
> > z <- 
> readLines(gzcon(url(?https://TCGA.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz?)), 
> n = 100)
> Error: unexpected input in "z <- readLines(gzcon(url(?"
>
> # that was because you had "smart-quotes" rather than ASCII quotes:
>
>
> > z <- readLines(gzcon(url( 
> 'https://TCGA.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz' 
> )), n = 100)
> > z[1:10]
> ?[1] 
> "sample\tTCGA-E1-5319-01\tTCGA-HT-7693-01\tTCGA-CS-6665-01\tTCGA-S9-A7J2-01\tTCGA-FG-A6J3-01\tTCGA-FG-6688-01\tTCGA-S9-A6TX-01\tTCGA-VM-A8C8-01\tTCGA-74-6577-01\tTCGA-06-AABW-11\tTCGA-06-0125-02\tTCGA-HT-A74L-01\tTCGA-26-A7UX-01\tTCGA-DU-A5TS-01\tTCGA-06-6388-01\tTCGA-DB-A4XA-01\tTCGA-06-A7TL-01\tTCGA-HT-A4DV-01\tTCGA-TQ-A7RP-01\tTCGA-E1-5311-01\tTCGA-28-5213-01\tTCGA-E1-A7YI-01\tTCGA-E1-5305-01\tTCGA-F6-A8O4-01\tTCGA-HT-8113-01\tTCGA-DH-A66G-01\tTCGA-76-4932-01\t
>
> Snipped hundreds of lines. So this seems to indicate that this is a 
> tab separated file. Don't you have some documentation to refer to?
>
>
> This seems possibly useful:
>
>
> > z <- read.table( 
> text=readLines(gzcon(url('https://TCGA.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz')), 
> n = 100), header=TRUE, sep="\t")
> > str(z)
> 'data.frame':??? 99 obs. of? 686 variables:
> ?$ sample???????? : Factor w/ 99 levels "cg00036732","cg00651829",..: 
> 53 2 60 41 16 13 37 20 70 21 ...
> ?$ TCGA.E1.5319.01: num? 0.4019 0.0215 0.053 0.0453 0.515 ...
> ?$ TCGA.HT.7693.01: num? 0.9364 0.0216 0.0547 0.0819 0.6129 ...
> ?$ TCGA.CS.6665.01: num? 0.0345 0.0164 0.0719 0.0497 0.6648 ...
> ?$ TCGA.S9.A7J2.01: num? 0.0295 0.0168 0.0421 0.0867 0.1657 ...
> ?$ TCGA.FG.A6J3.01: num? 0.0248 0.0161 0.0556 0.0902 0.5042 ...
> ?$ TCGA.FG.6688.01: num? 0.0203 0.0179 0.0321 0.0513 0.1075 ...
> ?$ TCGA.S9.A6TX.01: num? 0.0378 0.0199 0.0623 0.0992 0.7662 ...
> ?$ TCGA.VM.A8C8.01: num? 0.0271 0.0172 0.0466 0.0564 0.3478 ...
> ?$ TCGA.74.6577.01: num? 0.0237 0.0193 0.0196 0.0961 0.1242 ...
> ?$ TCGA.06.AABW.11: num? 0.0323 0.0156 0.0395 0.0708 0.1136 ...
> ?$ TCGA.06.0125.02: num? 0.0238 0.0181 0.039 0.068 0.0796 ...
> ?$ TCGA.HT.A74L.01: num? 0.7409 0.0221 0.0596 0.0765 0.8157 ...
>
> #snipped the output
>
> # there seemed to be 686 columns
>
>


From @@m|r@r@ch|d@z@|m @end|ng |rom gm@||@com  Sat Aug 10 20:36:45 2019
From: @@m|r@r@ch|d@z@|m @end|ng |rom gm@||@com (Samir Rachid Zaim)
Date: Sat, 10 Aug 2019 11:36:45 -0700
Subject: [R] Question on extracting subsampled features from node in Random
 forest Package
Message-ID: <CAPs3B=Ow7=gEF6M9MYfToPayEJpYq5qRYdq-cHPquicfbqCUoA@mail.gmail.com>

Hi all,

------------------------------------------------------------------------------------------------------------
*Question:*
*Is there a way to see what variables are subsampled in a node in a tree in
a random forest?*
------------------------------------------------------------------------------------------------------------
I posted this question on cross-validated *(see here)
<https://stats.stackexchange.com/questions/419872/is-there-a-way-to-see-what-variables-are-subsampled-in-a-node-in-a-tree-in-a-ran>*,
but they closed it as "off-topic", so I'll rewrite and hopefully get some
feedback there. I'd figure I'd also try here.

In a random forest, the *mtry* parameter determines what
percentage*/*proportion
of features gets subsampled at each node in a tree in a *randomForest*
classifier. At the moment, if I search through the randomForest object, I
can get a tree map that shows you what feature was chosen, and what was the
splitting value for that node.

My question, is, is there a way to also see what features were subsampled
for that node?
If not, is than an option for a future release?

The example below shows what's currently available when you scan through
the forest in the rf.object, but it seems to only include the final
variable and splitting value, rather than the entire set of features at
each node.
------------------------------------------------------------------------------------------------------------
*Example:*

X = matrix(rnorm(1000), ncol=10)

beta= rep(1,10)

z = X %*% beta

y = factor(rbinom(100, size=1, prob= 1/(1 + exp(-z))))

rf.object <- randomForest::randomForest(X,y, keep.forest=T)

str(rf.object$forest)
------------------------------------------------------------------------------------------------------------

I appreciate any and all help. Thanks!!

-- 
Samir Rachid Zaim,
PhD Student in Statistics and Data Science,
Data Science Ambassador, College of Medicine
University of Arizona Bioscience Research Lab
1230 N Cherry Ave,
Tucson, AZ 85719

email: samirrachidzaim at email.arizona.edu
website: https://samirrachidzaim.github.io/

	[[alternative HTML version deleted]]


From j@me@hum@humphrey@ @end|ng |rom gm@||@com  Sat Aug 10 21:35:29 2019
From: j@me@hum@humphrey@ @end|ng |rom gm@||@com (J H)
Date: Sat, 10 Aug 2019 20:35:29 +0100
Subject: [R] Using read.table for importing gz file
In-Reply-To: <CAPQaxLPG9YHBaZOpQhZCfwCjCQsrnRDK9mYZ53kYgkJ9ZC66oA@mail.gmail.com>
References: <CAPQaxLPG9YHBaZOpQhZCfwCjCQsrnRDK9mYZ53kYgkJ9ZC66oA@mail.gmail.com>
Message-ID: <CAONxt0M9qgH4iOnFNduVgbdf_muXWecaNGz6jSjL8_jxWOYELQ@mail.gmail.com>

Unsubscribe

On Sat, 10 Aug 2019 at 20:30, Spencer Brackett <
spbrackett20 at saintjosephhs.com> wrote:

> Hello,
>
> I am trying to read the following Xena dataset into R for data analysis:
>
> https://tcga.xenahubs.net/download/TCGA.GBMLGG.sampleMap/HumanMethylation450.gz
>
> I tried to run the following read.table(gzfile("HumanMethylation450.gz")),
> but R ended up crashing as a result.
>
> Is there perhaps a way to use read.table with fread in some way to do this?
>
> Many thanks,
>
> Spencer
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ||nu@@|@chen @end|ng |rom gm@||@com  Sun Aug 11 12:24:04 2019
From: ||nu@@|@chen @end|ng |rom gm@||@com (Linus Chen)
Date: Sun, 11 Aug 2019 12:24:04 +0200
Subject: [R] vectorizing the integrate function
In-Reply-To: <1395066570.5017281.1565457613467@mail.yahoo.com>
References: <1395066570.5017281.1565457613467.ref@mail.yahoo.com>
 <1395066570.5017281.1565457613467@mail.yahoo.com>
Message-ID: <CAPm+3sAiaw-uAmgcAgPU+qQ2njN9-SR=1h01VjHx87Tr_pvs8w@mail.gmail.com>

Dear ravi,

In your example, the function "f" is linear to a, b, and c.
Is this the general case in your task?

If it is, you can save  *lots* of computation taking advantage of the linearity.

Lei


On Sat, 10 Aug 2019 at 19:20, ravi via R-help <r-help at r-project.org> wrote:
>
> Hi all,I am having some difficulties in vectorizing the integrate function. Let me explain with an example.
> a <- 10; b <- 3; c <- 4
> f <- function(x) {exp(-a*x^3-b*x^2-c*x)}
> integrate(f,0,Inf) # works fine
>
> My difficulties start when I want to vectorize.
>
> # attempts to vectorize fail
> a <- seq(from=0,to=1,by=0.5)
> b <- seq(from=5,to=10,by=1)
> m <- seq(from=10,to=20,by=5)
> f2 <- function(x,a,b,c) {exp(-a*x^3-b*x^2-m*x)}
> fv <- Vectorize(integrate(f2,0,Inf),vectorize.args=c("a","b","m"),SIMPLIFY=TRUE)
>
> I want the result as a 3-d array with dimensions of the lengths of a, b and c. I have tried several variants but am not having much luck. Will appreciate any help that I can get.
> Thanks,Ravi Sutradhara
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tr|ng @end|ng |rom gvdnet@dk  Sun Aug 11 18:13:37 2019
From: tr|ng @end|ng |rom gvdnet@dk (Troels Ring)
Date: Sun, 11 Aug 2019 18:13:37 +0200
Subject: [R] ggplot2 fill problem in colour shading
Message-ID: <4e67b01d5505f$bbe43cb0$33acb610$@gvdnet.dk>

Dear friends - I have 2  problems with ggplot2

Here is the code for illustration

 

x <- seq(1,10,length=10000)

y <- x^2

fill <- rep(0,length(x))

fill[(5<x)&(x<7.5)] <- "red"

ddf <- data.frame(x,y,fill)

ggplot(data=ddf,aes(x=x,y=y)) + geom_area(aes(fill=fill)) + geom_line()

 

First the embarrassing one: I would really like no fill except from x from 5
to 7.5 or transparency so the background is still visible. Have tried NULL
and NA with no effect.

The other thing is that it appears that above the blue area some of the
other color spills true - reddish on my system.

 

Windows 10 

R version 3.6.1 

 

All best wishes

Troels Ring

Aalborg, Denmark


	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Aug 11 18:57:22 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 11 Aug 2019 17:57:22 +0100
Subject: [R] ggplot2 fill problem in colour shading
In-Reply-To: <4e67b01d5505f$bbe43cb0$33acb610$@gvdnet.dk>
References: <4e67b01d5505f$bbe43cb0$33acb610$@gvdnet.dk>
Message-ID: <d27c16d4-bfa5-27f2-d9b7-ed737bebd6e4@sapo.pt>

Hello,

You must put the NA in scale_fill_manual. Like this:

ggplot(data = ddf, aes(x = x, y = y)) +
   geom_area(aes(fill = fill)) +
   geom_line() +
   scale_fill_manual(values = c(NA, "red"))


Note also that 'fill' is a factor, were it numeric you would need fill = 
factor(fill).


ddf2 <- data.frame(x,y,fill = as.integer(fill == "red"))

ggplot(data = ddf2, aes(x = x, y = y)) +
   geom_area(aes(fill = fill)) +
   geom_line() +
   scale_fill_manual(values = c(NA, "red"))
#Error: Continuous value supplied to discrete scale


Just change to fill = factor(fill) and the graph is exactly the same.
Which leads me to propose an easier way of creating the vector 'fill', 
in the first place.

fill <- as.integer((5 < x) & (x < 7.5))

Will give the same result, coerce to factor if you like.


Hope this helps,

Rui Barradas

?s 17:13 de 11/08/19, Troels Ring escreveu:
> Dear friends - I have 2  problems with ggplot2
> 
> Here is the code for illustration
> 
>   
> 
> x <- seq(1,10,length=10000)
> 
> y <- x^2
> 
> fill <- rep(0,length(x))
> 
> fill[(5<x)&(x<7.5)] <- "red"
> 
> ddf <- data.frame(x,y,fill)
> 
> ggplot(data=ddf,aes(x=x,y=y)) + geom_area(aes(fill=fill)) + geom_line()
> 
>   
> 
> First the embarrassing one: I would really like no fill except from x from 5
> to 7.5 or transparency so the background is still visible. Have tried NULL
> and NA with no effect.
> 
> The other thing is that it appears that above the blue area some of the
> other color spills true - reddish on my system.
> 
>   
> 
> Windows 10
> 
> R version 3.6.1
> 
>   
> 
> All best wishes
> 
> Troels Ring
> 
> Aalborg, Denmark
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dw|n@em|u@ @end|ng |rom comc@@t@net  Sun Aug 11 19:02:56 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Sun, 11 Aug 2019 10:02:56 -0700
Subject: [R] ggplot2 fill problem in colour shading
In-Reply-To: <4e67b01d5505f$bbe43cb0$33acb610$@gvdnet.dk>
References: <4e67b01d5505f$bbe43cb0$33acb610$@gvdnet.dk>
Message-ID: <E68B7EA3-5D6C-42B1-A348-820CE8D1C6C4@comcast.net>


> On Aug 11, 2019, at 9:13 AM, Troels Ring <tring at gvdnet.dk> wrote:
> 
> Dear friends - I have 2  problems with ggplot2
> 
> Here is the code for illustration
> 
> 
> 
> x <- seq(1,10,length=10000)
> 
> y <- x^2
> 
> fill <- rep(0,length(x))
> 
> fill[(5<x)&(x<7.5)] <- "red"
> 
> ddf <- data.frame(x,y,fill)
> 
> ggplot(data=ddf,aes(x=x,y=y)) + geom_area(aes(fill=fill)) + geom_line()
> 
> 
> 
> First the embarrassing one: I would really like no fill except from x from 5
> to 7.5 or transparency so the background is still visible. Have tried NULL
> and NA with no effect.
> 
> The other thing is that it appears that above the blue area some of the
> other color spills true - reddish on my system.
> 

I'm not exactly clear on the request, but perhaps this will help clarify. Need to extract the desired portion of the ddf object to give to the geom_area function:

ggplot(data=ddf,aes(x=x,y=y)) + geom_line(color="blue") + geom_area(data=ddf[(5<ddf$x)&(ddf$x<7.5),], inherit.aes=FALSE, aes(x=x,y=y,fill=fill) )

-- 
David.
> 
> 
> Windows 10 
> 
> R version 3.6.1 
> 
> 
> 
> All best wishes
> 
> Troels Ring
> 
> Aalborg, Denmark
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From t@@yi_cm2000 m@iii@g oii y@hoo@com  Sun Aug 11 03:01:30 2019
From: t@@yi_cm2000 m@iii@g oii y@hoo@com (t@@yi_cm2000 m@iii@g oii y@hoo@com)
Date: Sun, 11 Aug 2019 01:01:30 +0000 (UTC)
Subject: [R] Help
References: <1391069614.2781747.1565485290024.ref@mail.yahoo.com>
Message-ID: <1391069614.2781747.1565485290024@mail.yahoo.com>

Hi,my name is; William, a graduate student in the Department of EconomicsState University of New York(SUNY),at Albany.I'm using R-3.6,1 and Rstudio on Window, however, i've problems installing packageslike:gEcon,BMR on R. I've used:install.packages("gEcon") and? install.packages(type=gEcon,"source"),it didn't help.
Can any one help?ThanksWilliam

	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Sun Aug 11 20:02:26 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Sun, 11 Aug 2019 11:02:26 -0700
Subject: [R] Help
In-Reply-To: <1391069614.2781747.1565485290024@mail.yahoo.com>
References: <1391069614.2781747.1565485290024.ref@mail.yahoo.com>
 <1391069614.2781747.1565485290024@mail.yahoo.com>
Message-ID: <386455B4-D905-45AA-B12A-1FB4A67D8741@comcast.net>


> On Aug 10, 2019, at 6:01 PM, tanyi_cm2000--- via R-help <r-help at r-project.org> wrote:
> 
> Hi,my name is; William, a graduate student in the Department of EconomicsState University of New York(SUNY),at Albany.I'm using R-3.6,1 and Rstudio on Window, however, i've problems installing packageslike:gEcon,BMR on R. I've used:install.packages("gEcon") and  install.packages(type=gEcon,"source"),it didn't help.

You are not telling us what actually happened. Did you get an error? If so, you should cut-and-paste it into you email message.

That said, I'm not seeing that package in the current list of CRAN packages:

> "gEcon" %in% available.packages()[,"Package"] 
[1] FALSE

Unlike a package I know to be available:

> "survival" %in% available.packages()[,"Package"]
[1] TRUE

The full list is here:
https://cran.r-project.org/web/packages/available_packages_by_name.html


> Can any one help?ThanksWilliam
> 
> 	[[alternative HTML version deleted]]

This is a plain-text mailing list.

> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

'Any technology distinguishable from magic is insufficiently advanced.'   -Gehm's Corollary to Clarke's Third Law


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Aug 11 20:49:36 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 11 Aug 2019 19:49:36 +0100
Subject: [R] Help
In-Reply-To: <1391069614.2781747.1565485290024@mail.yahoo.com>
References: <1391069614.2781747.1565485290024.ref@mail.yahoo.com>
 <1391069614.2781747.1565485290024@mail.yahoo.com>
Message-ID: <5d645b07-f4e4-48bc-308f-2865fc13c7d6@sapo.pt>

Hello,

This seems to be the site you want, not CRAN.


http://gecon.r-forge.r-project.org/


And please no HTML, post in plain text.


Hope this helps,

Rui Barradas

?s 02:01 de 11/08/19, tanyi_cm2000--- via R-help escreveu:
> Hi,my name is; William, a graduate student in the Department of EconomicsState University of New York(SUNY),at Albany.I'm using R-3.6,1 and Rstudio on Window, however, i've problems installing packageslike:gEcon,BMR on R. I've used:install.packages("gEcon") and? install.packages(type=gEcon,"source"),it didn't help.
> Can any one help?ThanksWilliam
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rv15| @end|ng |rom y@hoo@@e  Mon Aug 12 13:19:22 2019
From: rv15| @end|ng |rom y@hoo@@e (ravi)
Date: Mon, 12 Aug 2019 11:19:22 +0000 (UTC)
Subject: [R] vectorizing the integrate function
In-Reply-To: <CAPm+3sAiaw-uAmgcAgPU+qQ2njN9-SR=1h01VjHx87Tr_pvs8w@mail.gmail.com>
References: <1395066570.5017281.1565457613467.ref@mail.yahoo.com>
 <1395066570.5017281.1565457613467@mail.yahoo.com>
 <CAPm+3sAiaw-uAmgcAgPU+qQ2njN9-SR=1h01VjHx87Tr_pvs8w@mail.gmail.com>
Message-ID: <1870571262.5730598.1565608762727@mail.yahoo.com>

 Hi all,I would like to thank every one who has responded to my question. I have received valuable help.Linus, the function that I gave here is just a toy function. I think that is possible even to find an analytical solution. I chose that just to include many constant parameters. I wanted to find out how I can vectorize. I have obtained nice answers from Bert and I appreciate the help. I knew even previously that the vectorizing primarily improves the readability of the code. Thanks Bert for making it crystal clear that the looping is still at the interpreted level of R.Thanks,Ravi
    On Sunday, 11 August 2019, 12:24:43 CEST, Linus Chen <linus.l.chen at gmail.com> wrote:  
 
 Dear ravi,

In your example, the function "f" is linear to a, b, and c.
Is this the general case in your task?

If it is, you can save? *lots* of computation taking advantage of the linearity.

Lei


On Sat, 10 Aug 2019 at 19:20, ravi via R-help <r-help at r-project.org> wrote:
>
> Hi all,I am having some difficulties in vectorizing the integrate function. Let me explain with an example.
> a <- 10; b <- 3; c <- 4
> f <- function(x) {exp(-a*x^3-b*x^2-c*x)}
> integrate(f,0,Inf) # works fine
>
> My difficulties start when I want to vectorize.
>
> # attempts to vectorize fail
> a <- seq(from=0,to=1,by=0.5)
> b <- seq(from=5,to=10,by=1)
> m <- seq(from=10,to=20,by=5)
> f2 <- function(x,a,b,c) {exp(-a*x^3-b*x^2-m*x)}
> fv <- Vectorize(integrate(f2,0,Inf),vectorize.args=c("a","b","m"),SIMPLIFY=TRUE)
>
> I want the result as a 3-d array with dimensions of the lengths of a, b and c. I have tried several variants but am not having much luck. Will appreciate any help that I can get.
> Thanks,Ravi Sutradhara
>
>
>
>
>? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
  
	[[alternative HTML version deleted]]


From reichm@@j m@iii@g oii sbcgiob@i@@et  Tue Aug 13 01:22:36 2019
From: reichm@@j m@iii@g oii sbcgiob@i@@et (reichm@@j m@iii@g oii sbcgiob@i@@et)
Date: Mon, 12 Aug 2019 18:22:36 -0500
Subject: [R] separate and gather functions
Message-ID: <000201d55164$d4165330$7c42f990$@sbcglobal.net>

R-Help Forum

 

I have a data set from which I have extracted two columns Column 1 is a
listing of Federal agencies and Column 2 lists functions like this

 

Col1                       Col2

Agency A             Function1, Function2, Function3, Function4

Agency B              Function2, Function4

Agency C              Function1, Function3, Function4

 

What I need is a long list like this

 

Col1                       Col2

Agency A             Function1

Agency A             Function2

Agency 4              Function3 etc

 

Is there a more elegant /efficient way other than what I did which was to
separate Col2 into separate columns and then gather the data back up

 

myDat3 <- separate(data= myDat2, col = type, into = c("x1", "x2", "x3",
"x4", "x5"), sep = ",")  

myDat4 <- gather(data=myDat3, key="type", value = "Value",
"x1","x2","x3","x4","x5", na.rm = TRUE)

 

while it works, looking for a more elegant solution, if there is one

 

Jeff


	[[alternative HTML version deleted]]


From wdun|@p @end|ng |rom t|bco@com  Tue Aug 13 01:45:55 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Mon, 12 Aug 2019 16:45:55 -0700
Subject: [R] separate and gather functions
In-Reply-To: <000201d55164$d4165330$7c42f990$@sbcglobal.net>
References: <000201d55164$d4165330$7c42f990$@sbcglobal.net>
Message-ID: <CAF8bMcbaz2ULv1LvwcwVWiF=ZtUb_5iC=oLPamuQTcbk8z1MyQ@mail.gmail.com>

This one uses only core R functions.  Does that count toward "elegance"?

> # your data, I assume, in a form one can copy and paste into R
> d <- data.frame(stringsAsFactors = FALSE,
    Col1 = c("Agency A", "Agency B", "Agency C"),
    Col2 = c("Function1, Function2, Function3, Function4",
        "Function2, Function4", "Function1, Function3, Function4"))
> # split Col2 by comma following by any number of spaces
> tmp <- strsplit(d$Col2, split=", *")
> data.frame(Col1 = rep(d$Col1, lengths(tmp)), Col2 = unlist(tmp),
stringsAsFactors=FALSE)
      Col1      Col2
1 Agency A Function1
2 Agency A Function2
3 Agency A Function3
4 Agency A Function4
5 Agency B Function2
6 Agency B Function4
7 Agency C Function1
8 Agency C Function3
9 Agency C Function4

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Mon, Aug 12, 2019 at 4:23 PM <reichmanj at sbcglobal.net> wrote:

> R-Help Forum
>
>
>
> I have a data set from which I have extracted two columns Column 1 is a
> listing of Federal agencies and Column 2 lists functions like this
>
>
>
> Col1                       Col2
>
> Agency A             Function1, Function2, Function3, Function4
>
> Agency B              Function2, Function4
>
> Agency C              Function1, Function3, Function4
>
>
>
> What I need is a long list like this
>
>
>
> Col1                       Col2
>
> Agency A             Function1
>
> Agency A             Function2
>
> Agency 4              Function3 etc
>
>
>
> Is there a more elegant /efficient way other than what I did which was to
> separate Col2 into separate columns and then gather the data back up
>
>
>
> myDat3 <- separate(data= myDat2, col = type, into = c("x1", "x2", "x3",
> "x4", "x5"), sep = ",")
>
> myDat4 <- gather(data=myDat3, key="type", value = "Value",
> "x1","x2","x3","x4","x5", na.rm = TRUE)
>
>
>
> while it works, looking for a more elegant solution, if there is one
>
>
>
> Jeff
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Tue Aug 13 04:20:04 2019
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Tue, 13 Aug 2019 02:20:04 +0000
Subject: [R] Trying to understand how to sort a DF on two columns
Message-ID: <BN7PR03MB3730E1AE2F924B882A7EF1D7E2D20@BN7PR03MB3730.namprd03.prod.outlook.com>

I want to sort a DF, temp, on two columns, patid and time. I have searched the internet and found code that I was able to modify to get my data sorted. Unfortunately I don't understand how the code works. I would appreciate it if someone could explain to me how the code works. Among other questions, despite reading, I don't understand how with() works, nor what it does in the current setting.

code:
data4xsort<-temp[
  with( temp, order(temp[,"patid"], temp[,"time"])),
]

Thank you,
John





John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)


	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Tue Aug 13 04:35:43 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 12 Aug 2019 19:35:43 -0700
Subject: [R] Trying to understand how to sort a DF on two columns
In-Reply-To: <BN7PR03MB3730E1AE2F924B882A7EF1D7E2D20@BN7PR03MB3730.namprd03.prod.outlook.com>
References: <BN7PR03MB3730E1AE2F924B882A7EF1D7E2D20@BN7PR03MB3730.namprd03.prod.outlook.com>
Message-ID: <CAGxFJbR-h=iBvQU879xgErTNtmV+bd3-cm-QLGfe+N4_DVaHrQ@mail.gmail.com>

https://stackoverflow.com/questions/2315601/understanding-the-order-function

Do a web search on "How does order() work R" or similar for more.

I can't explain with() any better than the docs: saying that it evaluates
the expression argument in the data argument environment -- a data frame
for the data frame method -- probably won't help you.

-- Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Aug 12, 2019 at 7:20 PM Sorkin, John <jsorkin at som.umaryland.edu>
wrote:

> I want to sort a DF, temp, on two columns, patid and time. I have searched
> the internet and found code that I was able to modify to get my data
> sorted. Unfortunately I don't understand how the code works. I would
> appreciate it if someone could explain to me how the code works. Among
> other questions, despite reading, I don't understand how with() works, nor
> what it does in the current setting.
>
> code:
> data4xsort<-temp[
>   with( temp, order(temp[,"patid"], temp[,"time"])),
> ]
>
> Thank you,
> John
>
>
>
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and
> Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Tue Aug 13 04:43:42 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Mon, 12 Aug 2019 19:43:42 -0700
Subject: [R] Trying to understand how to sort a DF on two columns
In-Reply-To: <BN7PR03MB3730E1AE2F924B882A7EF1D7E2D20@BN7PR03MB3730.namprd03.prod.outlook.com>
References: <BN7PR03MB3730E1AE2F924B882A7EF1D7E2D20@BN7PR03MB3730.namprd03.prod.outlook.com>
Message-ID: <7dd838dc-e01f-9c1d-63d3-f95f24ccc075@comcast.net>

On 8/12/19 7:20 PM, Sorkin, John wrote:
> I want to sort a DF, temp, on two columns, patid and time. I have searched the internet and found code that I was able to modify to get my data sorted. Unfortunately I don't understand how the code works. I would appreciate it if someone could explain to me how the code works. Among other questions, despite reading, I don't understand how with() works, nor what it does in the current setting.
>
> code:
> data4xsort<-temp[
>    with( temp, order(temp[,"patid"], temp[,"time"])),
> ]
>
> Thank you,
> John


The `order`-function returns a numeric vector which is the length of its 
inputs (and there is recycling when the inputs are of different length). 
The numbers are the order in which the items would be if they were 
sorted smallest to largest. There are arguments that let you control the 
behavior in the case of ties. So when used in an indexing application as 
seen here, it results in the dataframe returned with its rows in 
ascending order based primarily on its first argument, patid,? and in 
case of ties on the second argument, time. If you put a minus sign in 
from of the argument it the ordering is largest to smallest.


If that is code you are getting from elsewhere, you should realize that 
it is somewhat redundant and you should question the level of R skills 
of its author.? In this code it is doing absolutely nothing. The `with( 
...) is not needed because the arguments already have an unambiguous 
place to get the column names.? A more compact expression if you were 
going to use `with` would be:

data4xsort<-temp[ with( temp, order(patid, time)), ]

But using `with` carries risks because there are sometimes confusion about which environment it will find the named objects or columns.

Safer would be to not using it in this situation.

Your headers suggest you are using Outlook. Surely there must be a way to specify a plain text format for outgoing emails. This is a plain text mailing list.

David.

>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Tue Aug 13 04:44:29 2019
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Tue, 13 Aug 2019 02:44:29 +0000
Subject: [R] Trying to understand how to sort a DF on two columns
In-Reply-To: <CAGxFJbR-h=iBvQU879xgErTNtmV+bd3-cm-QLGfe+N4_DVaHrQ@mail.gmail.com>
References: <BN7PR03MB3730E1AE2F924B882A7EF1D7E2D20@BN7PR03MB3730.namprd03.prod.outlook.com>
 <CAGxFJbR-h=iBvQU879xgErTNtmV+bd3-cm-QLGfe+N4_DVaHrQ@mail.gmail.com>
Message-ID: <BN7PR03MB37309D29269E836C9FA9F1A6E2D20@BN7PR03MB3730.namprd03.prod.outlook.com>

Bert,

Thank you for your reply (and the many other questions to the list that you answer).

I understand how order works when ordering based on a single column. What I don?t understand is how the code I included with my email works. I believe my problem is a lack of understanding of what with does. I have read about the with function, but I must be missing something.

Thank you,
John

From: Bert Gunter <bgunter.4567 at gmail.com>
Sent: Monday, August 12, 2019 10:36 PM
To: Sorkin, John <jsorkin at som.umaryland.edu>
Cc: r-help at r-project.org (r-help at r-project.org) <r-help at r-project.org>
Subject: Re: [R] Trying to understand how to sort a DF on two columns

https://stackoverflow.com/questions/2315601/understanding-the-order-function

Do a web search on "How does order() work R" or similar for more.

I can't explain with() any better than the docs: saying that it evaluates the expression argument in the data argument environment -- a data frame for the data frame method -- probably won't help you.

-- Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Aug 12, 2019 at 7:20 PM Sorkin, John <jsorkin at som.umaryland.edu<mailto:jsorkin at som.umaryland.edu>> wrote:
I want to sort a DF, temp, on two columns, patid and time. I have searched the internet and found code that I was able to modify to get my data sorted. Unfortunately I don't understand how the code works. I would appreciate it if someone could explain to me how the code works. Among other questions, despite reading, I don't understand how with() works, nor what it does in the current setting.

code:
data4xsort<-temp[
  with( temp, order(temp[,"patid"], temp[,"time"])),
]

Thank you,
John





John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From reichm@@j m@iii@g oii sbcgiob@i@@et  Tue Aug 13 04:46:10 2019
From: reichm@@j m@iii@g oii sbcgiob@i@@et (reichm@@j m@iii@g oii sbcgiob@i@@et)
Date: Mon, 12 Aug 2019 21:46:10 -0500
Subject: [R] separate and gather functions
In-Reply-To: <CAF8bMcbaz2ULv1LvwcwVWiF=ZtUb_5iC=oLPamuQTcbk8z1MyQ@mail.gmail.com>
References: <000201d55164$d4165330$7c42f990$@sbcglobal.net>
 <CAF8bMcbaz2ULv1LvwcwVWiF=ZtUb_5iC=oLPamuQTcbk8z1MyQ@mail.gmail.com>
Message-ID: <000d01d55181$443654a0$cca2fde0$@sbcglobal.net>

William

 

Yes that works a little better as I don?t have to create notional variables. Thank you

 

Jeff

 

From: William Dunlap <wdunlap at tibco.com> 
Sent: Monday, August 12, 2019 6:46 PM
To: Jeff Reichman <reichmanj at sbcglobal.net>
Cc: r-help at r-project.org
Subject: Re: [R] separate and gather functions

 

This one uses only core R functions.  Does that count toward "elegance"?

 

> # your data, I assume, in a form one can copy and paste into R

> d <- data.frame(stringsAsFactors = FALSE,
    Col1 = c("Agency A", "Agency B", "Agency C"),
    Col2 = c("Function1, Function2, Function3, Function4", 
        "Function2, Function4", "Function1, Function3, Function4"))

> # split Col2 by comma following by any number of spaces

> tmp <- strsplit(d$Col2, split=", *")
> data.frame(Col1 = rep(d$Col1, lengths(tmp)), Col2 = unlist(tmp), stringsAsFactors=FALSE)
      Col1      Col2
1 Agency A Function1
2 Agency A Function2
3 Agency A Function3
4 Agency A Function4
5 Agency B Function2
6 Agency B Function4
7 Agency C Function1
8 Agency C Function3
9 Agency C Function4

 

Bill Dunlap
TIBCO Software
wdunlap tibco.com <http://tibco.com> 

 

 

On Mon, Aug 12, 2019 at 4:23 PM <reichmanj at sbcglobal.net <mailto:reichmanj at sbcglobal.net> > wrote:

R-Help Forum



I have a data set from which I have extracted two columns Column 1 is a
listing of Federal agencies and Column 2 lists functions like this



Col1                       Col2

Agency A             Function1, Function2, Function3, Function4

Agency B              Function2, Function4

Agency C              Function1, Function3, Function4



What I need is a long list like this



Col1                       Col2

Agency A             Function1

Agency A             Function2

Agency 4              Function3 etc



Is there a more elegant /efficient way other than what I did which was to
separate Col2 into separate columns and then gather the data back up



myDat3 <- separate(data= myDat2, col = type, into = c("x1", "x2", "x3",
"x4", "x5"), sep = ",")  

myDat4 <- gather(data=myDat3, key="type", value = "Value",
"x1","x2","x3","x4","x5", na.rm = TRUE)



while it works, looking for a more elegant solution, if there is one



Jeff


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Aug 13 06:51:15 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 12 Aug 2019 21:51:15 -0700
Subject: [R] Trying to understand how to sort a DF on two columns
In-Reply-To: <BN7PR03MB37309D29269E836C9FA9F1A6E2D20@BN7PR03MB3730.namprd03.prod.outlook.com>
References: <BN7PR03MB3730E1AE2F924B882A7EF1D7E2D20@BN7PR03MB3730.namprd03.prod.outlook.com>
 <CAGxFJbR-h=iBvQU879xgErTNtmV+bd3-cm-QLGfe+N4_DVaHrQ@mail.gmail.com>
 <BN7PR03MB37309D29269E836C9FA9F1A6E2D20@BN7PR03MB3730.namprd03.prod.outlook.com>
Message-ID: <DAEA3303-773B-45E1-9B20-731BC000B8A4@dcn.davis.ca.us>

With this as an example, no wonder you don't understand it. This is horrible.

`with` is very much like the `subset` function... it alleviates the need to re-type the containing object name repeatedly.

data4xsort <- temp[ with( temp, order( patid, time ) ), ]

is the same as

data4xsort <- temp[ order( temp$patid, temp$time ), ]

The example you gave makes no use of the `with` function.

On August 12, 2019 7:44:29 PM PDT, "Sorkin, John" <jsorkin at som.umaryland.edu> wrote:
>Bert,
>
>Thank you for your reply (and the many other questions to the list that
>you answer).
>
>I understand how order works when ordering based on a single column.
>What I don?t understand is how the code I included with my email works.
>I believe my problem is a lack of understanding of what with does. I
>have read about the with function, but I must be missing something.
>
>Thank you,
>John
>
>From: Bert Gunter <bgunter.4567 at gmail.com>
>Sent: Monday, August 12, 2019 10:36 PM
>To: Sorkin, John <jsorkin at som.umaryland.edu>
>Cc: r-help at r-project.org (r-help at r-project.org) <r-help at r-project.org>
>Subject: Re: [R] Trying to understand how to sort a DF on two columns
>
>https://stackoverflow.com/questions/2315601/understanding-the-order-function
>
>Do a web search on "How does order() work R" or similar for more.
>
>I can't explain with() any better than the docs: saying that it
>evaluates the expression argument in the data argument environment -- a
>data frame for the data frame method -- probably won't help you.
>
>-- Bert
>
>Bert Gunter
>
>"The trouble with having an open mind is that people keep coming along
>and sticking things into it."
>-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
>On Mon, Aug 12, 2019 at 7:20 PM Sorkin, John
><jsorkin at som.umaryland.edu<mailto:jsorkin at som.umaryland.edu>> wrote:
>I want to sort a DF, temp, on two columns, patid and time. I have
>searched the internet and found code that I was able to modify to get
>my data sorted. Unfortunately I don't understand how the code works. I
>would appreciate it if someone could explain to me how the code works.
>Among other questions, despite reading, I don't understand how with()
>works, nor what it does in the current setting.
>
>code:
>data4xsort<-temp[
>  with( temp, order(temp[,"patid"], temp[,"time"])),
>]
>
>Thank you,
>John
>
>
>
>
>
>John David Sorkin M.D., Ph.D.
>Professor of Medicine
>Chief, Biostatistics and Informatics
>University of Maryland School of Medicine Division of Gerontology and
>Geriatric Medicine
>Baltimore VA Medical Center
>10 North Greene Street
>GRECC (BT/18/GR)
>Baltimore, MD 21201-1524
>(Phone) 410-605-7119
>(Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
>        [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
>UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Tue Aug 13 10:11:15 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Tue, 13 Aug 2019 09:11:15 +0100
Subject: [R] Trying to understand how to sort a DF on two columns
In-Reply-To: <BN7PR03MB3730E1AE2F924B882A7EF1D7E2D20@BN7PR03MB3730.namprd03.prod.outlook.com>
References: <BN7PR03MB3730E1AE2F924B882A7EF1D7E2D20@BN7PR03MB3730.namprd03.prod.outlook.com>
Message-ID: <bff25fae-2f59-1aa3-d79e-ee0d646555ad@sapo.pt>

Hello,

Though good answers were already given, I would like to say something.

1.
If you are lazy (typing), use with, if you prefer to play safe, don't.
I am lazy many times, but in interactive mode only.

2.
I find it better in the long run *not* to take advantage of R's 
one-liners, they tend to be less readable. Instead of putting everything 
in the same instruction why not

i <- order( temp$patid, temp$time )
data4xsort <- temp[ i, ]


This has the disadvantage of creating an extra variable but are you 
really having memory problems? If not, use the clearer code. Besides, if 
this goes into a function all temporary variables will be gone and the 
memory released, in which case there will be no problem.

(The with equivalent is i <- with(temp, order(patid, time)), btw.)

Hope this helps,

Rui Barradas

?s 03:20 de 13/08/19, Sorkin, John escreveu:
> I want to sort a DF, temp, on two columns, patid and time. I have searched the internet and found code that I was able to modify to get my data sorted. Unfortunately I don't understand how the code works. I would appreciate it if someone could explain to me how the code works. Among other questions, despite reading, I don't understand how with() works, nor what it does in the current setting.
> 
> code:
> data4xsort<-temp[
>    with( temp, order(temp[,"patid"], temp[,"time"])),
> ]
> 
> Thank you,
> John
> 
> 
> 
> 
> 
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From neotrop|c@|@b@t@ @end|ng |rom gm@||@com  Tue Aug 13 14:59:20 2019
From: neotrop|c@|@b@t@ @end|ng |rom gm@||@com (Neotropical bat risk assessments)
Date: Tue, 13 Aug 2019 08:59:20 -0400
Subject: [R] Need help with an iteration
In-Reply-To: <mailman.355776.1.1565690401.51337.r-help@r-project.org>
References: <mailman.355776.1.1565690401.51337.r-help@r-project.org>
Message-ID: <3163be94-b070-ee35-1c51-1444c665798e@gmail.com>

Hi all,

I am having an issue with trying to run iterations for the same analysis 
with a list of data sets.? I am assuming I am missing a basic step here.

Code I am trying is below.? Not certain if a NEXT call needs to be 
included somewhere or not.
This is run under the Deducer GUI.? I would also be content to simply 
run in a basic R console if that works.

# First step, was to create a folder with the simple data files for the 
analysis

# Set the WD for that directory

setwd("C://Embalonurids")

# Call all the names of the files in that folder

file.names <- dir(pattern = "*.txt")

#Make a LOOP for the analysis

for(i in 1:length(file.names)){file <- 
read.table(file.names[i],header=TRUE) # read the text files one by one

 ? # Deducer code used

 ? file[c("TBC")] <- recode.variables(file[c("TBC")] , "0 -> NA;")

 ? # Create new BW variable and data set then change data file name for 
the analysis

 ? BW<-within(file,BW<-Fmax-Fmin)

 ? BW<-within(BW,FcH1<-Fc*.5)

 ? BW<-within(BW,FcH3<-FcH1*3)

 ? BatStats<-BW
# In the Deducer GUI this is the code that runs manually if I run these 
one by one
BatStats<-Deducer::descriptive.table (vars = d 
(Dur,TBC,Fmin,Fmax,BW,Fmean,Fk,FcH1,Fc,FcH3,Sc,Pmc),data= 
BatStats,func.names =c("Valid N","Minimum","Maximum","Mean","St. 
Deviation"),func.additional= list(p75=function(x) quantile(x, c(0.75), 
na.rm=TRUE),p85=function(x) quantile(x, c(0.85), 
na.rm=TRUE),p90=function(x) quantile(x, 
c(0.90),na.rm=TRUE),p99=function(x) quantile(x, c(0.99),na.rm=TRUE)))

 ?## Then the results need to be written to an outputdirectory

 ?name <- as.character(paste(file.names[i], ".csv", sep="")) # this 
creates an element that will have the name of the file that is used in 
each run

# I have tried both setting a new WD or simply defined where to writ the 
results

setwd("C://Embalonurids/Results")

write.csv(BatStats, name)??? #this should assign the names of the files 
as it iterates one by one })

#also tried directing where to write the outputas I do when manually 
running this

#write.csv(BatStats,name,file="C:\\Emballonurids\Results"))


I would like to run this as an iteration as I have > 200 data files to 
process and would love to have it run in background while I am working 
on other things.

An assistance welcomed.

Bruce

-- 
Bruce W. Miller, PhD.
Neotropical bat risk assessments
Conservation Fellow - Wildlife Conservation Society

If we lose the bats, we may lose much of the tropical vegetation and the lungs of the planet

Using acoustic sampling to identify and map species distributions
and pioneering acoustic tools for ecology and conservation of bats for >25 years.

Key projects include providing free interactive identification keys and call fact sheets for the vocal signatures of New World Bats


From |@t@z@hn @end|ng |rom gm@||@com  Tue Aug 13 15:21:40 2019
From: |@t@z@hn @end|ng |rom gm@||@com (Ista Zahn)
Date: Tue, 13 Aug 2019 09:21:40 -0400
Subject: [R] separate and gather functions
In-Reply-To: <000d01d55181$443654a0$cca2fde0$@sbcglobal.net>
References: <000201d55164$d4165330$7c42f990$@sbcglobal.net>
 <CAF8bMcbaz2ULv1LvwcwVWiF=ZtUb_5iC=oLPamuQTcbk8z1MyQ@mail.gmail.com>
 <000d01d55181$443654a0$cca2fde0$@sbcglobal.net>
Message-ID: <CA+vqiLH8bfvE-eArE+OY4GSZB5kMUjneCcAF+nX35nqTCxGB_w@mail.gmail.com>

How about

> library(tidyr)
> separate_rows(d, Col2)
      Col1      Col2
1 Agency A Function1
2 Agency A Function2
3 Agency A Function3
4 Agency A Function4
5 Agency B Function2
6 Agency B Function4
7 Agency C Function1
8 Agency C Function3
9 Agency C Function4


On Mon, Aug 12, 2019 at 11:06 PM <reichmanj at sbcglobal.net> wrote:
>
> William
>
>
>
> Yes that works a little better as I don?t have to create notional variables. Thank you
>
>
>
> Jeff
>
>
>
> From: William Dunlap <wdunlap at tibco.com>
> Sent: Monday, August 12, 2019 6:46 PM
> To: Jeff Reichman <reichmanj at sbcglobal.net>
> Cc: r-help at r-project.org
> Subject: Re: [R] separate and gather functions
>
>
>
> This one uses only core R functions.  Does that count toward "elegance"?
>
>
>
> > # your data, I assume, in a form one can copy and paste into R
>
> > d <- data.frame(stringsAsFactors = FALSE,
>     Col1 = c("Agency A", "Agency B", "Agency C"),
>     Col2 = c("Function1, Function2, Function3, Function4",
>         "Function2, Function4", "Function1, Function3, Function4"))
>
> > # split Col2 by comma following by any number of spaces
>
> > tmp <- strsplit(d$Col2, split=", *")
> > data.frame(Col1 = rep(d$Col1, lengths(tmp)), Col2 = unlist(tmp), stringsAsFactors=FALSE)
>       Col1      Col2
> 1 Agency A Function1
> 2 Agency A Function2
> 3 Agency A Function3
> 4 Agency A Function4
> 5 Agency B Function2
> 6 Agency B Function4
> 7 Agency C Function1
> 8 Agency C Function3
> 9 Agency C Function4
>
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com <http://tibco.com>
>
>
>
>
>
> On Mon, Aug 12, 2019 at 4:23 PM <reichmanj at sbcglobal.net <mailto:reichmanj at sbcglobal.net> > wrote:
>
> R-Help Forum
>
>
>
> I have a data set from which I have extracted two columns Column 1 is a
> listing of Federal agencies and Column 2 lists functions like this
>
>
>
> Col1                       Col2
>
> Agency A             Function1, Function2, Function3, Function4
>
> Agency B              Function2, Function4
>
> Agency C              Function1, Function3, Function4
>
>
>
> What I need is a long list like this
>
>
>
> Col1                       Col2
>
> Agency A             Function1
>
> Agency A             Function2
>
> Agency 4              Function3 etc
>
>
>
> Is there a more elegant /efficient way other than what I did which was to
> separate Col2 into separate columns and then gather the data back up
>
>
>
> myDat3 <- separate(data= myDat2, col = type, into = c("x1", "x2", "x3",
> "x4", "x5"), sep = ",")
>
> myDat4 <- gather(data=myDat3, key="type", value = "Value",
> "x1","x2","x3","x4","x5", na.rm = TRUE)
>
>
>
> while it works, looking for a more elegant solution, if there is one
>
>
>
> Jeff
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr@p|k@| @end|ng |rom prechez@@cz  Tue Aug 13 15:26:43 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Tue, 13 Aug 2019 13:26:43 +0000
Subject: [R] Need help with an iteration
In-Reply-To: <3163be94-b070-ee35-1c51-1444c665798e@gmail.com>
References: <mailman.355776.1.1565690401.51337.r-help@r-project.org>
 <3163be94-b070-ee35-1c51-1444c665798e@gmail.com>
Message-ID: <1215cbdaffb04d07a8a60be2b8d0f903@SRVEXCHCM1302.precheza.cz>

Hi

I does not use Deducer so I do not know if it has some features for such task

In plain R instead
file.names <- dir(pattern = "*.txt")

 use
file.names <- list.files(pattern = "*.txt")

for(i in 1:length(file.names)){file <-
> read.table(file.names[i],header=TRUE) #
Here you want to change zeros to NA in TBC column

>    file[c("TBC")] <- recode.variables(file[c("TBC")] , "0 -> NA;")

is.na(file[, "TBC"]) <- (file[,"TBC"]==0)

This is cryptic for me

# Create new BW variable and data set then change data file name for
the analysis

but if I understand correctly, you want to have some summary statistics on each file

For this you could try
?apply, ?summary, ?fivenum or use your own function - something like

myf <-function(x) c(mean(x), min(x), max(x), sd(x))

Store result in one row
result[i,] <- apply(file, 2,myf)

Output result after cycle ends.
}
write.csv(result, "somename.csv")

Data frame "result" could be predefined based on number of your files and output columns.

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Neotropical bat
> risk assessments
> Sent: Tuesday, August 13, 2019 2:59 PM
> To: r-help at r-project.org
> Subject: [R] Need help with an iteration
>
> Hi all,
>
> I am having an issue with trying to run iterations for the same analysis with a
> list of data sets.  I am assuming I am missing a basic step here.
>
> Code I am trying is below.  Not certain if a NEXT call needs to be included
> somewhere or not.
> This is run under the Deducer GUI.  I would also be content to simply run in a
> basic R console if that works.
>
> # First step, was to create a folder with the simple data files for the analysis
>
> # Set the WD for that directory
>
> setwd("C://Embalonurids")
>
> # Call all the names of the files in that folder
>
> file.names <- dir(pattern = "*.txt")
>
> #Make a LOOP for the analysis
>
> for(i in 1:length(file.names)){file <-
> read.table(file.names[i],header=TRUE) # read the text files one by one
>
>    # Deducer code used
>
>    file[c("TBC")] <- recode.variables(file[c("TBC")] , "0 -> NA;")


>
>    # Create new BW variable and data set then change data file name for the
> analysis
>
>    BW<-within(file,BW<-Fmax-Fmin)
>
>    BW<-within(BW,FcH1<-Fc*.5)
>
>    BW<-within(BW,FcH3<-FcH1*3)
>
>    BatStats<-BW
> # In the Deducer GUI this is the code that runs manually if I run these one by
> one BatStats<-Deducer::descriptive.table (vars = d
> (Dur,TBC,Fmin,Fmax,BW,Fmean,Fk,FcH1,Fc,FcH3,Sc,Pmc),data=
> BatStats,func.names =c("Valid N","Minimum","Maximum","Mean","St.
> Deviation"),func.additional= list(p75=function(x) quantile(x, c(0.75),
> na.rm=TRUE),p85=function(x) quantile(x, c(0.85),
> na.rm=TRUE),p90=function(x) quantile(x,
> c(0.90),na.rm=TRUE),p99=function(x) quantile(x, c(0.99),na.rm=TRUE)))
>
>   ## Then the results need to be written to an outputdirectory
>
>   name <- as.character(paste(file.names[i], ".csv", sep="")) # this creates an
> element that will have the name of the file that is used in each run
>
> # I have tried both setting a new WD or simply defined where to writ the
> results
>
> setwd("C://Embalonurids/Results")
>
> write.csv(BatStats, name)    #this should assign the names of the files as it
> iterates one by one })
>
> #also tried directing where to write the outputas I do when manually running
> this
>
> #write.csv(BatStats,name,file="C:\\Emballonurids\Results"))
>
>
> I would like to run this as an iteration as I have > 200 data files to process and
> would love to have it run in background while I am working on other things.
>
> An assistance welcomed.
>
> Bruce
>
> --
> Bruce W. Miller, PhD.
> Neotropical bat risk assessments
> Conservation Fellow - Wildlife Conservation Society
>
> If we lose the bats, we may lose much of the tropical vegetation and the lungs
> of the planet
>
> Using acoustic sampling to identify and map species distributions and
> pioneering acoustic tools for ecology and conservation of bats for >25 years.
>
> Key projects include providing free interactive identification keys and call fact
> sheets for the vocal signatures of New World Bats
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From er|cjberger @end|ng |rom gm@||@com  Tue Aug 13 15:29:21 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Tue, 13 Aug 2019 16:29:21 +0300
Subject: [R] separate and gather functions
In-Reply-To: <CA+vqiLH8bfvE-eArE+OY4GSZB5kMUjneCcAF+nX35nqTCxGB_w@mail.gmail.com>
References: <000201d55164$d4165330$7c42f990$@sbcglobal.net>
 <CAF8bMcbaz2ULv1LvwcwVWiF=ZtUb_5iC=oLPamuQTcbk8z1MyQ@mail.gmail.com>
 <000d01d55181$443654a0$cca2fde0$@sbcglobal.net>
 <CA+vqiLH8bfvE-eArE+OY4GSZB5kMUjneCcAF+nX35nqTCxGB_w@mail.gmail.com>
Message-ID: <CAGgJW77+hMfGW3H-Tt64h53_UkJJ-61b6buiSntJdyMq_p8NrA@mail.gmail.com>

Chapeau Ista :-)

On Tue, Aug 13, 2019 at 4:22 PM Ista Zahn <istazahn at gmail.com> wrote:

> How about
>
> > library(tidyr)
> > separate_rows(d, Col2)
>       Col1      Col2
> 1 Agency A Function1
> 2 Agency A Function2
> 3 Agency A Function3
> 4 Agency A Function4
> 5 Agency B Function2
> 6 Agency B Function4
> 7 Agency C Function1
> 8 Agency C Function3
> 9 Agency C Function4
>
>
> On Mon, Aug 12, 2019 at 11:06 PM <reichmanj at sbcglobal.net> wrote:
> >
> > William
> >
> >
> >
> > Yes that works a little better as I don?t have to create notional
> variables. Thank you
> >
> >
> >
> > Jeff
> >
> >
> >
> > From: William Dunlap <wdunlap at tibco.com>
> > Sent: Monday, August 12, 2019 6:46 PM
> > To: Jeff Reichman <reichmanj at sbcglobal.net>
> > Cc: r-help at r-project.org
> > Subject: Re: [R] separate and gather functions
> >
> >
> >
> > This one uses only core R functions.  Does that count toward "elegance"?
> >
> >
> >
> > > # your data, I assume, in a form one can copy and paste into R
> >
> > > d <- data.frame(stringsAsFactors = FALSE,
> >     Col1 = c("Agency A", "Agency B", "Agency C"),
> >     Col2 = c("Function1, Function2, Function3, Function4",
> >         "Function2, Function4", "Function1, Function3, Function4"))
> >
> > > # split Col2 by comma following by any number of spaces
> >
> > > tmp <- strsplit(d$Col2, split=", *")
> > > data.frame(Col1 = rep(d$Col1, lengths(tmp)), Col2 = unlist(tmp),
> stringsAsFactors=FALSE)
> >       Col1      Col2
> > 1 Agency A Function1
> > 2 Agency A Function2
> > 3 Agency A Function3
> > 4 Agency A Function4
> > 5 Agency B Function2
> > 6 Agency B Function4
> > 7 Agency C Function1
> > 8 Agency C Function3
> > 9 Agency C Function4
> >
> >
> >
> > Bill Dunlap
> > TIBCO Software
> > wdunlap tibco.com <http://tibco.com>
> >
> >
> >
> >
> >
> > On Mon, Aug 12, 2019 at 4:23 PM <reichmanj at sbcglobal.net <mailto:
> reichmanj at sbcglobal.net> > wrote:
> >
> > R-Help Forum
> >
> >
> >
> > I have a data set from which I have extracted two columns Column 1 is a
> > listing of Federal agencies and Column 2 lists functions like this
> >
> >
> >
> > Col1                       Col2
> >
> > Agency A             Function1, Function2, Function3, Function4
> >
> > Agency B              Function2, Function4
> >
> > Agency C              Function1, Function3, Function4
> >
> >
> >
> > What I need is a long list like this
> >
> >
> >
> > Col1                       Col2
> >
> > Agency A             Function1
> >
> > Agency A             Function2
> >
> > Agency 4              Function3 etc
> >
> >
> >
> > Is there a more elegant /efficient way other than what I did which was to
> > separate Col2 into separate columns and then gather the data back up
> >
> >
> >
> > myDat3 <- separate(data= myDat2, col = type, into = c("x1", "x2", "x3",
> > "x4", "x5"), sep = ",")
> >
> > myDat4 <- gather(data=myDat3, key="type", value = "Value",
> > "x1","x2","x3","x4","x5", na.rm = TRUE)
> >
> >
> >
> > while it works, looking for a more elegant solution, if there is one
> >
> >
> >
> > Jeff
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To
> UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From cry@n @end|ng |rom b|ngh@mton@edu  Tue Aug 13 19:59:56 2019
From: cry@n @end|ng |rom b|ngh@mton@edu (Christopher W Ryan)
Date: Tue, 13 Aug 2019 13:59:56 -0400
Subject: [R] reading in csv files,
 some of which have column names and some of which don't
Message-ID: <CAM+rpYnbTSdbYNthzO5rNUO4=+ueqGA+t41WXwnsBD6ZWkakew@mail.gmail.com>

Alas, we spend so much time and energy on data wrangling . . . .

I'm given a collection of csv files to work with---"found data". They arose
via saving Excel files to csv format. They all have the same column
structure, except that some were saved with column names and some were not.

I have a code snippet that I've used before to traverse a directory and
read into R all the csv files of a certain filename pattern within it, and
combine them all into a single dataframe:

library(dplyr)
## specify the csv files that I will want to access
files.to.read <- list.files(path = "H:/EH", pattern =
"WICLeadLabOrdersDone.+", all.files = FALSE, full.names = TRUE, recursive =
FALSE, ignore.case = FALSE, include.dirs = FALSE, no.. = FALSE)

## function to read csv files back in
read.csv.files <- function(filename) {
    bb <- read.csv(filename, colClasses = "character", header = TRUE)
    bb
}

## now read the csv files, as all character
b <- lapply(files.to.read, read.csv.files)

ddd <- bind_rows(b)

But this assumes that all files have column names in their first row. In
this case, some don't. Any advice how to handle it so that those with
column names and those without are read in and combined properly? The only
thing I've come up with so far is:

## function to read csv files back in
## Unfortunately, some of the csv files are saved with column headers, and
some are saved without them.
## This presents a problem when defining the function to read them: header
= TRUE or header = FALSE?
## The best solution I can think of as of 13 August 2019 is to use header =
FALSE and skip the
## first row of every file. This will sacrifice one record from each csv of
about 80 files
read.csv.files <- function(filename) {
    bb <- read.csv(filename, colClasses = "character", header = FALSE, skip
= 1)
    bb
}

This sacrifices about 80 out of about 1600 records. For my purposes in this
instance, this may be acceptable, but of course I'd rather not.

Thanks.

--Chris Ryan

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Tue Aug 13 20:32:37 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 13 Aug 2019 11:32:37 -0700
Subject: [R] reading in csv files,
 some of which have column names and some of which don't
In-Reply-To: <CAM+rpYnbTSdbYNthzO5rNUO4=+ueqGA+t41WXwnsBD6ZWkakew@mail.gmail.com>
References: <CAM+rpYnbTSdbYNthzO5rNUO4=+ueqGA+t41WXwnsBD6ZWkakew@mail.gmail.com>
Message-ID: <CAGxFJbR26n0TUcxp3OyQqe9cmU+0RMWRDOw2hoYy=o5hwWmAoA@mail.gmail.com>

Are these files of numerics? In other words, how would one know whether the
first line of a file of alpha data are headers or not?  read.table's Help
file contains some info that may or may not be relevant for your files also.

Assuming a criterion for distinction, one could simply read the first line
of a file, check the criterion, and then read it with or without headers as
appropriate. R can create default column names.  One could also use
readLines with connections, but I don't think this is necessary, though
maybe it's more elegant or faster.

Without knowing how to tell whether the first line is a header or not, I
have no clue. Maybe the filename/ suffix might tell you something.

-- Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Aug 13, 2019 at 11:00 AM Christopher W Ryan <cryan at binghamton.edu>
wrote:

> Alas, we spend so much time and energy on data wrangling . . . .
>
> I'm given a collection of csv files to work with---"found data". They arose
> via saving Excel files to csv format. They all have the same column
> structure, except that some were saved with column names and some were not.
>
> I have a code snippet that I've used before to traverse a directory and
> read into R all the csv files of a certain filename pattern within it, and
> combine them all into a single dataframe:
>
> library(dplyr)
> ## specify the csv files that I will want to access
> files.to.read <- list.files(path = "H:/EH", pattern =
> "WICLeadLabOrdersDone.+", all.files = FALSE, full.names = TRUE, recursive =
> FALSE, ignore.case = FALSE, include.dirs = FALSE, no.. = FALSE)
>
> ## function to read csv files back in
> read.csv.files <- function(filename) {
>     bb <- read.csv(filename, colClasses = "character", header = TRUE)
>     bb
> }
>
> ## now read the csv files, as all character
> b <- lapply(files.to.read, read.csv.files)
>
> ddd <- bind_rows(b)
>
> But this assumes that all files have column names in their first row. In
> this case, some don't. Any advice how to handle it so that those with
> column names and those without are read in and combined properly? The only
> thing I've come up with so far is:
>
> ## function to read csv files back in
> ## Unfortunately, some of the csv files are saved with column headers, and
> some are saved without them.
> ## This presents a problem when defining the function to read them: header
> = TRUE or header = FALSE?
> ## The best solution I can think of as of 13 August 2019 is to use header =
> FALSE and skip the
> ## first row of every file. This will sacrifice one record from each csv of
> about 80 files
> read.csv.files <- function(filename) {
>     bb <- read.csv(filename, colClasses = "character", header = FALSE, skip
> = 1)
>     bb
> }
>
> This sacrifices about 80 out of about 1600 records. For my purposes in this
> instance, this may be acceptable, but of course I'd rather not.
>
> Thanks.
>
> --Chris Ryan
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From peter@|@ng|e|der @end|ng |rom gm@||@com  Tue Aug 13 20:32:41 2019
From: peter@|@ng|e|der @end|ng |rom gm@||@com (Peter Langfelder)
Date: Tue, 13 Aug 2019 11:32:41 -0700
Subject: [R] reading in csv files,
 some of which have column names and some of which don't
In-Reply-To: <CAM+rpYnbTSdbYNthzO5rNUO4=+ueqGA+t41WXwnsBD6ZWkakew@mail.gmail.com>
References: <CAM+rpYnbTSdbYNthzO5rNUO4=+ueqGA+t41WXwnsBD6ZWkakew@mail.gmail.com>
Message-ID: <CA+hbrhU8AK-sgM2JdZxB9RUo1x2B+jegeu44XVh4sQvC3Af5ow@mail.gmail.com>

If the data are numeric (or at least some columns are numeric), a
brute force solution is to read a file once with header = FALSE, check
the relevant column(s) for being numeric, and if they are not numeric,
re-read with header = TRUE. Alternatively, if you know the column
names (headers) beforehand, read with header = FALSE and check the
first row for being equal to the known column names; if it contains
the column names, re-read with header = TRUE.

With a total of 1600 records, reading each file (at most) twice should
not be a problem.

Peter

On Tue, Aug 13, 2019 at 11:00 AM Christopher W Ryan
<cryan at binghamton.edu> wrote:
>
> Alas, we spend so much time and energy on data wrangling . . . .
>
> I'm given a collection of csv files to work with---"found data". They arose
> via saving Excel files to csv format. They all have the same column
> structure, except that some were saved with column names and some were not.
>
> I have a code snippet that I've used before to traverse a directory and
> read into R all the csv files of a certain filename pattern within it, and
> combine them all into a single dataframe:
>
> library(dplyr)
> ## specify the csv files that I will want to access
> files.to.read <- list.files(path = "H:/EH", pattern =
> "WICLeadLabOrdersDone.+", all.files = FALSE, full.names = TRUE, recursive =
> FALSE, ignore.case = FALSE, include.dirs = FALSE, no.. = FALSE)
>
> ## function to read csv files back in
> read.csv.files <- function(filename) {
>     bb <- read.csv(filename, colClasses = "character", header = TRUE)
>     bb
> }
>
> ## now read the csv files, as all character
> b <- lapply(files.to.read, read.csv.files)
>
> ddd <- bind_rows(b)
>
> But this assumes that all files have column names in their first row. In
> this case, some don't. Any advice how to handle it so that those with
> column names and those without are read in and combined properly? The only
> thing I've come up with so far is:
>
> ## function to read csv files back in
> ## Unfortunately, some of the csv files are saved with column headers, and
> some are saved without them.
> ## This presents a problem when defining the function to read them: header
> = TRUE or header = FALSE?
> ## The best solution I can think of as of 13 August 2019 is to use header =
> FALSE and skip the
> ## first row of every file. This will sacrifice one record from each csv of
> about 80 files
> read.csv.files <- function(filename) {
>     bb <- read.csv(filename, colClasses = "character", header = FALSE, skip
> = 1)
>     bb
> }
>
> This sacrifices about 80 out of about 1600 records. For my purposes in this
> instance, this may be acceptable, but of course I'd rather not.
>
> Thanks.
>
> --Chris Ryan
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @@r@h@go@|ee @end|ng |rom gm@||@com  Tue Aug 13 20:39:43 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Tue, 13 Aug 2019 14:39:43 -0400
Subject: [R] reading in csv files,
 some of which have column names and some of which don't
In-Reply-To: <CAM+rpYnbTSdbYNthzO5rNUO4=+ueqGA+t41WXwnsBD6ZWkakew@mail.gmail.com>
References: <CAM+rpYnbTSdbYNthzO5rNUO4=+ueqGA+t41WXwnsBD6ZWkakew@mail.gmail.com>
Message-ID: <CAM_vjun7rxa_RDUZdSGY6Nq4NLJzjpodHu9vNQOAyGjDzJvjgg@mail.gmail.com>

Like Bert, I can't see an easy approach for datasets that have
character rather than numeric data. But here's a simple approach for
distinguishing files that have possible character headers but numeric
data.



readheader <- function(filename) {

possibleheader <- read.table(filename, nrows=1, sep=",", header=FALSE)

if(all(is.numeric(possibleheader[,1]))) {
# no header
infile <- read.table(filename, sep=",", header=FALSE)
} else {
# has header
infile <- read.table(filename, sep=",", header=TRUE)
}

infile
}



#### file noheader.csv ####

1,1,1
2,2,2
3,3,3


#### file hasheader.csv ####

a,b,c
1,1,1
2,2,2
3,3,3

########################

> readheader("noheader.csv")
  V1 V2 V3
1  1  1  1
2  2  2  2
3  3  3  3
> readheader("hasheader.csv")
  a b c
1 1 1 1
2 2 2 2
3 3 3 3

Sarah

On Tue, Aug 13, 2019 at 2:00 PM Christopher W Ryan <cryan at binghamton.edu> wrote:
>
> Alas, we spend so much time and energy on data wrangling . . . .
>
> I'm given a collection of csv files to work with---"found data". They arose
> via saving Excel files to csv format. They all have the same column
> structure, except that some were saved with column names and some were not.
>
> I have a code snippet that I've used before to traverse a directory and
> read into R all the csv files of a certain filename pattern within it, and
> combine them all into a single dataframe:
>
> library(dplyr)
> ## specify the csv files that I will want to access
> files.to.read <- list.files(path = "H:/EH", pattern =
> "WICLeadLabOrdersDone.+", all.files = FALSE, full.names = TRUE, recursive =
> FALSE, ignore.case = FALSE, include.dirs = FALSE, no.. = FALSE)
>
> ## function to read csv files back in
> read.csv.files <- function(filename) {
>     bb <- read.csv(filename, colClasses = "character", header = TRUE)
>     bb
> }
>
> ## now read the csv files, as all character
> b <- lapply(files.to.read, read.csv.files)
>
> ddd <- bind_rows(b)
>
> But this assumes that all files have column names in their first row. In
> this case, some don't. Any advice how to handle it so that those with
> column names and those without are read in and combined properly? The only
> thing I've come up with so far is:
>
> ## function to read csv files back in
> ## Unfortunately, some of the csv files are saved with column headers, and
> some are saved without them.
> ## This presents a problem when defining the function to read them: header
> = TRUE or header = FALSE?
> ## The best solution I can think of as of 13 August 2019 is to use header =
> FALSE and skip the
> ## first row of every file. This will sacrifice one record from each csv of
> about 80 files
> read.csv.files <- function(filename) {
>     bb <- read.csv(filename, colClasses = "character", header = FALSE, skip
> = 1)
>     bb
> }
>
> This sacrifices about 80 out of about 1600 records. For my purposes in this
> instance, this may be acceptable, but of course I'd rather not.
>
> Thanks.
>
> --Chris Ryan
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From pd@|gd @end|ng |rom gm@||@com  Tue Aug 13 23:27:45 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Tue, 13 Aug 2019 23:27:45 +0200
Subject: [R] reading in csv files,
 some of which have column names and some of which don't
In-Reply-To: <CAM_vjun7rxa_RDUZdSGY6Nq4NLJzjpodHu9vNQOAyGjDzJvjgg@mail.gmail.com>
References: <CAM+rpYnbTSdbYNthzO5rNUO4=+ueqGA+t41WXwnsBD6ZWkakew@mail.gmail.com>
 <CAM_vjun7rxa_RDUZdSGY6Nq4NLJzjpodHu9vNQOAyGjDzJvjgg@mail.gmail.com>
Message-ID: <85C75692-16C8-486E-B2F4-27F6CC29F60D@gmail.com>

Yes. Also, the original poster said that the files had the same column structure, so there may be stronger heuristics to see whether the first line is a header line. E.g., assuming that the first column is called "ID" (and doesn't have ID as a possible value) use 

first <- readLines(file, 1)
if (grepl("^ID", first) 
...
else
...

-pd
 

> On 13 Aug 2019, at 20:39 , Sarah Goslee <sarah.goslee at gmail.com> wrote:
> 
> Like Bert, I can't see an easy approach for datasets that have
> character rather than numeric data. But here's a simple approach for
> distinguishing files that have possible character headers but numeric
> data.
> 
> 
> 
> readheader <- function(filename) {
> 
> possibleheader <- read.table(filename, nrows=1, sep=",", header=FALSE)
> 
> if(all(is.numeric(possibleheader[,1]))) {
> # no header
> infile <- read.table(filename, sep=",", header=FALSE)
> } else {
> # has header
> infile <- read.table(filename, sep=",", header=TRUE)
> }
> 
> infile
> }
> 
> 
> 
> #### file noheader.csv ####
> 
> 1,1,1
> 2,2,2
> 3,3,3
> 
> 
> #### file hasheader.csv ####
> 
> a,b,c
> 1,1,1
> 2,2,2
> 3,3,3
> 
> ########################
> 
>> readheader("noheader.csv")
>  V1 V2 V3
> 1  1  1  1
> 2  2  2  2
> 3  3  3  3
>> readheader("hasheader.csv")
>  a b c
> 1 1 1 1
> 2 2 2 2
> 3 3 3 3
> 
> Sarah
> 
> On Tue, Aug 13, 2019 at 2:00 PM Christopher W Ryan <cryan at binghamton.edu> wrote:
>> 
>> Alas, we spend so much time and energy on data wrangling . . . .
>> 
>> I'm given a collection of csv files to work with---"found data". They arose
>> via saving Excel files to csv format. They all have the same column
>> structure, except that some were saved with column names and some were not.
>> 
>> I have a code snippet that I've used before to traverse a directory and
>> read into R all the csv files of a certain filename pattern within it, and
>> combine them all into a single dataframe:
>> 
>> library(dplyr)
>> ## specify the csv files that I will want to access
>> files.to.read <- list.files(path = "H:/EH", pattern =
>> "WICLeadLabOrdersDone.+", all.files = FALSE, full.names = TRUE, recursive =
>> FALSE, ignore.case = FALSE, include.dirs = FALSE, no.. = FALSE)
>> 
>> ## function to read csv files back in
>> read.csv.files <- function(filename) {
>>    bb <- read.csv(filename, colClasses = "character", header = TRUE)
>>    bb
>> }
>> 
>> ## now read the csv files, as all character
>> b <- lapply(files.to.read, read.csv.files)
>> 
>> ddd <- bind_rows(b)
>> 
>> But this assumes that all files have column names in their first row. In
>> this case, some don't. Any advice how to handle it so that those with
>> column names and those without are read in and combined properly? The only
>> thing I've come up with so far is:
>> 
>> ## function to read csv files back in
>> ## Unfortunately, some of the csv files are saved with column headers, and
>> some are saved without them.
>> ## This presents a problem when defining the function to read them: header
>> = TRUE or header = FALSE?
>> ## The best solution I can think of as of 13 August 2019 is to use header =
>> FALSE and skip the
>> ## first row of every file. This will sacrifice one record from each csv of
>> about 80 files
>> read.csv.files <- function(filename) {
>>    bb <- read.csv(filename, colClasses = "character", header = FALSE, skip
>> = 1)
>>    bb
>> }
>> 
>> This sacrifices about 80 out of about 1600 records. For my purposes in this
>> instance, this may be acceptable, but of course I'd rather not.
>> 
>> Thanks.
>> 
>> --Chris Ryan
>> 
>>        [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
> -- 
> Sarah Goslee (she/her)
> http://www.numberwright.com
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From beno|t@v@|||@nt @end|ng |rom no-|og@org  Wed Aug 14 07:09:43 2019
From: beno|t@v@|||@nt @end|ng |rom no-|og@org (Benoit Vaillant)
Date: Wed, 14 Aug 2019 07:09:43 +0200
Subject: [R] reading in csv files,
 some of which have column names and some of which don't
In-Reply-To: <CAM+rpYnbTSdbYNthzO5rNUO4=+ueqGA+t41WXwnsBD6ZWkakew@mail.gmail.com>
References: <CAM+rpYnbTSdbYNthzO5rNUO4=+ueqGA+t41WXwnsBD6ZWkakew@mail.gmail.com>
Message-ID: <20190814050943.qo2li52unrqh37vo@auroras.fr>

Hello,

On Tue, Aug 13, 2019 at 01:59:56PM -0400, Christopher W Ryan wrote:
> But this assumes that all files have column names in their first row. In
> this case, some don't. Any advice how to handle it so that those with
> column names and those without are read in and combined properly?

It obvously depends on the data, but here is an other approach (which
I hope has not been suggested yet):

1. For each file, read only the first row (and keep track of
   file => first row),
2. Make counts of dinstinct first rows. If data is sufficely not
   identical on first rows, the highest count will indicate that its a
   header, so mark this as the header,
3. Reread files, since there is some form of mapping kept in step 1,
   one knows if header should be TRUE or FALSE, and fix headers after
   reading with headers set to FALSE.
   
This can of course miserably fail if the set of saved files do not
have enough ones with headers included.

HTH in your case, but it's definitely not generic.

-- 
Beno?t

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 866 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190814/b6e32028/attachment.sig>

From @@h|mk@poor @end|ng |rom gm@||@com  Wed Aug 14 11:09:39 2019
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Wed, 14 Aug 2019 14:39:39 +0530
Subject: [R] Correct way to use a lemmatizer in package tm
Message-ID: <CAC8=1erjVGL-Z6+d5282yxKpve3oEyWTp4CMX-8g-nDRSUCB-w@mail.gmail.com>

Dear All,

I want to do lemmatization using the tm package and textstem package.

The following is how I am doing it currently :-

library("tm")
library("wordcloud")
library("RColorBrewer")

filePath = < Path to any text file >
text <- readLines(filePath)
docs <- Corpus(VectorSource(text))

# Convert the text to lower case
docs <- tm_map(docs,content_transformer(tolower))

# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text Lemmatization
library(textstem)
docs <- tm_map(docs, content_transformer(lemmatize_words))

My query : Is the above line the correct way to do lemmatization ? Can
someone please confirm?

For the sake of giving a complete example I am giving the following code as
well.

dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "Dark2"))
Thank you,
Ashim

	[[alternative HTML version deleted]]


From joh@n|@@@en @end|ng |rom gm@||@com  Wed Aug 14 11:23:26 2019
From: joh@n|@@@en @end|ng |rom gm@||@com (Johan Lassen)
Date: Wed, 14 Aug 2019 11:23:26 +0200
Subject: [R] Isolation forest using "solitude" package: help to predict
Message-ID: <CAAqXfsda+UmUQHZBhEGvfc_M4YsstfAV7q7+_V43GD6F+0YGJg@mail.gmail.com>

Dear community,

I would like to know if someone can help clarifying how to predict anomaly
scores on new data sets using the "solitude" package. A simple model can be
trained using:

library(solitude)
# Training the model:
iris_train <- iris[1:100, ]
model <- isolation_forest(iris_train[, 1:4], seed =
100,num.trees=100,importance="none")

# The anomaly scores of a new test data set can be calculated by
iris_test <- iris[100:150, ]
predicted_anomalies <- predict(mo, iris_test[, 1:4],type="anomaly_score")

#The challenge is how to predict the anomaly scores for a data set with
less observations than the #number of observations in the training data
set.
# Example: using a subset of just 11 observations as compared to the 51
observations results in anomaly scores that are smaller:

iris_test <- iris[100:110, ]
predicted_anomalies <- predict(mo, iris_test[, 1:4],type="anomaly_score")

Anyone knows how to predict "normalised (with respect to sample size)"
anomaly scores using the solitude package for R?

Thanks in advance!
Johan


-- 
Johan Lassen

"In the cities people live in time -
in the mountains people live in space" (Budistisk munk).

	[[alternative HTML version deleted]]


From reichm@@j m@iii@g oii sbcgiob@i@@et  Wed Aug 14 15:06:28 2019
From: reichm@@j m@iii@g oii sbcgiob@i@@et (reichm@@j m@iii@g oii sbcgiob@i@@et)
Date: Wed, 14 Aug 2019 08:06:28 -0500
Subject: [R] separate and gather functions
In-Reply-To: <CA+vqiLH8bfvE-eArE+OY4GSZB5kMUjneCcAF+nX35nqTCxGB_w@mail.gmail.com>
References: <000201d55164$d4165330$7c42f990$@sbcglobal.net>
 <CAF8bMcbaz2ULv1LvwcwVWiF=ZtUb_5iC=oLPamuQTcbk8z1MyQ@mail.gmail.com>
 <000d01d55181$443654a0$cca2fde0$@sbcglobal.net>
 <CA+vqiLH8bfvE-eArE+OY4GSZB5kMUjneCcAF+nX35nqTCxGB_w@mail.gmail.com>
Message-ID: <001c01d552a1$161f4ce0$425de6a0$@sbcglobal.net>

That's even easier

-----Original Message-----
From: Ista Zahn <istazahn at gmail.com> 
Sent: Tuesday, August 13, 2019 8:22 AM
To: reichmanj at sbcglobal.net
Cc: William Dunlap <wdunlap at tibco.com>; r-help at r-project.org
Subject: Re: [R] separate and gather functions

How about

> library(tidyr)
> separate_rows(d, Col2)
      Col1      Col2
1 Agency A Function1
2 Agency A Function2
3 Agency A Function3
4 Agency A Function4
5 Agency B Function2
6 Agency B Function4
7 Agency C Function1
8 Agency C Function3
9 Agency C Function4


On Mon, Aug 12, 2019 at 11:06 PM <reichmanj at sbcglobal.net> wrote:
>
> William
>
>
>
> Yes that works a little better as I don?t have to create notional 
> variables. Thank you
>
>
>
> Jeff
>
>
>
> From: William Dunlap <wdunlap at tibco.com>
> Sent: Monday, August 12, 2019 6:46 PM
> To: Jeff Reichman <reichmanj at sbcglobal.net>
> Cc: r-help at r-project.org
> Subject: Re: [R] separate and gather functions
>
>
>
> This one uses only core R functions.  Does that count toward "elegance"?
>
>
>
> > # your data, I assume, in a form one can copy and paste into R
>
> > d <- data.frame(stringsAsFactors = FALSE,
>     Col1 = c("Agency A", "Agency B", "Agency C"),
>     Col2 = c("Function1, Function2, Function3, Function4",
>         "Function2, Function4", "Function1, Function3, Function4"))
>
> > # split Col2 by comma following by any number of spaces
>
> > tmp <- strsplit(d$Col2, split=", *")
> > data.frame(Col1 = rep(d$Col1, lengths(tmp)), Col2 = unlist(tmp), 
> > stringsAsFactors=FALSE)
>       Col1      Col2
> 1 Agency A Function1
> 2 Agency A Function2
> 3 Agency A Function3
> 4 Agency A Function4
> 5 Agency B Function2
> 6 Agency B Function4
> 7 Agency C Function1
> 8 Agency C Function3
> 9 Agency C Function4
>
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com <http://tibco.com>
>
>
>
>
>
> On Mon, Aug 12, 2019 at 4:23 PM <reichmanj at sbcglobal.net <mailto:reichmanj at sbcglobal.net> > wrote:
>
> R-Help Forum
>
>
>
> I have a data set from which I have extracted two columns Column 1 is 
> a listing of Federal agencies and Column 2 lists functions like this
>
>
>
> Col1                       Col2
>
> Agency A             Function1, Function2, Function3, Function4
>
> Agency B              Function2, Function4
>
> Agency C              Function1, Function3, Function4
>
>
>
> What I need is a long list like this
>
>
>
> Col1                       Col2
>
> Agency A             Function1
>
> Agency A             Function2
>
> Agency 4              Function3 etc
>
>
>
> Is there a more elegant /efficient way other than what I did which was 
> to separate Col2 into separate columns and then gather the data back 
> up
>
>
>
> myDat3 <- separate(data= myDat2, col = type, into = c("x1", "x2", 
> "x3", "x4", "x5"), sep = ",")
>
> myDat4 <- gather(data=myDat3, key="type", value = "Value", 
> "x1","x2","x3","x4","x5", na.rm = TRUE)
>
>
>
> while it works, looking for a more elegant solution, if there is one
>
>
>
> Jeff
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To 
> UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From S@E|||@on @end|ng |rom LGCGroup@com  Wed Aug 14 16:10:32 2019
From: S@E|||@on @end|ng |rom LGCGroup@com (Stephen Ellison)
Date: Wed, 14 Aug 2019 14:10:32 +0000
Subject: [R] Trying to understand how to sort a DF on two columns
In-Reply-To: <BN7PR03MB37309D29269E836C9FA9F1A6E2D20@BN7PR03MB3730.namprd03.prod.outlook.com>
References: <BN7PR03MB3730E1AE2F924B882A7EF1D7E2D20@BN7PR03MB3730.namprd03.prod.outlook.com>
 <CAGxFJbR-h=iBvQU879xgErTNtmV+bd3-cm-QLGfe+N4_DVaHrQ@mail.gmail.com>
 <BN7PR03MB37309D29269E836C9FA9F1A6E2D20@BN7PR03MB3730.namprd03.prod.outlook.com>
Message-ID: <127aa6dd8a31440ea0670c4bfe391b32@GBDCVPEXC08.corp.lgc-group.com>


> I want to sort a DF, temp, on two columns, patid and time. I have searched
> the internet and found code that I was able to modify to get my data sorted.
> Unfortunately I don't understand how the code works. I would appreciate it
> if someone could explain to me how the code works. Among other
> questions, despite reading, I don't understand how with() works, nor what it
> does in the current setting.
> 
> code:
 data4xsort<-temp[
   with( temp, order(temp[,"patid"], temp[,"time"])),
 ]

With apologies for brevity-induced brusqueness:

1) You don't need 'with' in the code. You could say
data4xsort<- temp[order(temp[,"patid"], temp[,"time"]), ]
or
data4xsort<- temp[order(temp$patid, temp$time), ]

2) If you _did_ use 'with', you could say
data4xsort<- temp[with(temp, order(patid,time)), ]

Basically, 'with(x, ...)' says 'look in x first for anything in '...'. 

3. order. order is a bit of a mindbender. It gives you the numeric indices you need to convert an unsorted object into a sorted obbject.
If we said
a <- c(2,3,1)  
order(a)
by default, we get back
# [1] 3 1 2

These are indexes into a that put the elements of a in ascending order. a[3] is 1, a[1] is 2 and so on. 
So if we say
oo <- order(a) 
a[oo]

we get
[1] 1 2 3
... which is a, in ascending order. And to do that, we used oo as indexes in a.

4. For a data frame, you generally want to sort rows into a particular order. So let's say we have a data frame like
d <- data.frame(a=c(2,3,1,3,1,2), b=c(1,2,2,1,1,2))
d
  a b
1 2 1
2 3 2
3 1 2
4 3 1
5 1 1
6 2 2

We can say
oo.d <- with(d, order(a, b)) #which says 'look in 'd' to find 'a' and 'b' 
	#We could also have said oo.d <- order(d$a, d$b)

This gives us the row numbers of d, arranged to give us the row ordering we asked 'order' to generate. 
Now, if we say 
d[oo.d, ]     #where we need the empty second index so that the first is treated as a row index
# we get d, with rows sorted by a first and then b:
  a b
5 1 1
3 1 2
1 2 1
6 2 2
4 3 1
2 3 2

#You might notice that the default row numbers from d - the left hand colum above - are now identical to oo.d; 
# this is particular to default row numbers, though.

5. If you want to pack that into one line without assigning the ordering to oo.d, it goes (for example)
d[ with(d, order(a, b)), ]

... which is pretty much what your code is doing.

The only thing I've missed is that when you wrap something like 
order(temp[,"patid"], temp[,"time"]) 
in 'with', 'with' is not doing anything useful for you. 
temp[,"patid"] has already told R where to look for patid, 
so R doesn?t need to look anywhere else. 


Does that help?

Steve Ellison


*******************************************************************
This email and any attachments are confidential. Any use, copying or
disclosure other than by the intended recipient is unauthorised. If 
you have received this message in error, please notify the sender 
immediately via +44(0)20 8943 7000 or notify postmaster at lgcgroup.com 
and delete this message and any copies from your computer and network. 
LGC Limited. Registered in England 2991879. 
Registered office: Queens Road, Teddington, Middlesex, TW11 0LY, UK

From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Tue Aug 13 21:47:31 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Tue, 13 Aug 2019 14:47:31 -0500
Subject: [R] QQ plot
Message-ID: <CAF9-5jMWDvmjmT7ORV66PP-dOfct1n6Z2qxaKn9a9dqB6Dhf3A@mail.gmail.com>

Hello,

I was plotting QQ plot (in attach) via:

qqunif = function(p, BH=T, MAIN = " ", SUB=" ")
{
  nn = length(p)
  xx =  -log10((1:nn)/(nn+1))
  plot( xx,  -sort(log10(p)),
        main = MAIN, sub= SUB, cex.sub=1.3,
        xlab=expression(Expected~~-log[10](italic(p))),
        ylab=expression(Observed~~-log[10](italic(p))),
        cex.lab=1.0,mgp=c(2,1,0))
  abline(0,1,col='red')
  if(BH) ## BH = include Benjamini Hochberg FDR
  {

    abline(-log10(0.05),1, col='black',lty=1)
    text(0.5,1.9 , "FDR=0.05", col = "gray60",srt=45, cex=1)
    abline(-log10(0.10),1, col='black',lty=1)
    text(0.5, 1.6, "FDR=0.10", col = "gray60",srt=45, cex=1)
    abline(-log10(0.25),1, col='black',lty=1)
    text(0.5, 1.2, "FDR=0.25", col = "gray60",srt=45, cex=1)
    #legend('topleft', c("FDR = 0.05","FDR = 0.10","FDR = 0.25"),
           #col=c('black','black','black'),lty=c(1,1,1), cex=0.8)
    if (BF)
    {
      abline(h=-log10(0.05/nn), col='black') ## bonferroni
    }
  }
}

db=fread("/Users/ams/Desktop/GWAS_all.txt", header=T)
dd=db[db$P<1e-3,]
qqunif(dd$P)

what do I need to change in my code so that FDR lines look like on the
2nd attachment?

-------------- next part --------------
A non-text attachment was scrubbed...
Name: first.png
Type: image/png
Size: 150022 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190813/5790ad69/attachment.png>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: second.png
Type: image/png
Size: 214214 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190813/5790ad69/attachment-0001.png>

From @ky|erb@@|eeby@n @end|ng |rom gm@||@com  Wed Aug 14 05:41:25 2019
From: @ky|erb@@|eeby@n @end|ng |rom gm@||@com (Skyler Saleebyan)
Date: Tue, 13 Aug 2019 22:41:25 -0500
Subject: [R] Rsession memory leak freezes Ubuntu 16.04
Message-ID: <CA+gY42Ck7SJ1RXuriHF=D0TDygWJQNsQ_7qrD-KCTYiOua7NMw@mail.gmail.com>

Hi,

Earlier R crashed on my system, after repeated attempts rebooting, running
it under gdb, and clearing the ~/.rstudio-desktop, I have not been to run
rstudio without rsession growing until it completely takes up my systems 16
GB of RAM and freezes it.

I know that rsession is where the leak is occurring because I killed the
process once before the leak froze the whole system and I got this for
memory usage  about 10 minutes after killing rstudio.

skyler at skyler-G5-5587:~$ ps -A --sort -rss -o comm,pmem | head -n 11
COMMAND         %MEM
rsession        74.6
compiz           1.5
Discord          1.5
Discord          0.7
Discord          0.7
gdb              0.7
gnome-software   0.4
Xorg             0.4
evolution-calen  0.3
gnome-system-mo  0.3

before killing gdb to avoid a full freeze the trace through this error:
[Thread 0x7fffdce3a700 (LWP 2481) exited]


How do I scrub the current rsession and restart rstudio?

Thanks,
Skyler

	[[alternative HTML version deleted]]


From |@b|@n@@gordon @end|ng |rom br|@to|@@c@uk  Wed Aug 14 11:26:20 2019
From: |@b|@n@@gordon @end|ng |rom br|@to|@@c@uk (Fabiana Gordon)
Date: Wed, 14 Aug 2019 09:26:20 +0000
Subject: [R] Interval censored data with left truncation
Message-ID: <AM0PR06MB44672BFDC080F274D947E35CC8AD0@AM0PR06MB4467.eurprd06.prod.outlook.com>

Hello,


I have interval censored data with some late-entry subjects. Does any R package for survival analysis with  interval censored data allows for left truncation and if so how?

Thanks and Best Wishes,

Fabiana

	[[alternative HTML version deleted]]


From |@b|@n@@gordon @end|ng |rom br|@to|@@c@uk  Wed Aug 14 14:42:33 2019
From: |@b|@n@@gordon @end|ng |rom br|@to|@@c@uk (Fabiana Gordon)
Date: Wed, 14 Aug 2019 12:42:33 +0000
Subject: [R] Interval censored with left truncation
Message-ID: <AM0PR06MB4467062550E0934718F88110C8AD0@AM0PR06MB4467.eurprd06.prod.outlook.com>

Hello,

I have interval censored data with some late-entry subjects. Does any R package for survival analysis with  interval censored data allows for left truncation and if so how?

Thanks and Best Wishes,

Fabiana


	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Wed Aug 14 19:39:52 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 14 Aug 2019 10:39:52 -0700
Subject: [R] Interval censored data with left truncation
In-Reply-To: <AM0PR06MB44672BFDC080F274D947E35CC8AD0@AM0PR06MB4467.eurprd06.prod.outlook.com>
References: <AM0PR06MB44672BFDC080F274D947E35CC8AD0@AM0PR06MB4467.eurprd06.prod.outlook.com>
Message-ID: <CAGxFJbRL4hb_PQdZHeiXbt2rzPNhmK4G2wQAf5LLRveMK7i4YQ@mail.gmail.com>

Search on "interval censored data with left truncation" at rseek.org. There
appeared to be relevant hits there.


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Aug 14, 2019 at 10:28 AM Fabiana Gordon <
fabiana.gordon at bristol.ac.uk> wrote:

> Hello,
>
>
> I have interval censored data with some late-entry subjects. Does any R
> package for survival analysis with  interval censored data allows for left
> truncation and if so how?
>
> Thanks and Best Wishes,
>
> Fabiana
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Wed Aug 14 19:41:29 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 14 Aug 2019 10:41:29 -0700
Subject: [R] Interval censored data with left truncation
In-Reply-To: <CAGxFJbRL4hb_PQdZHeiXbt2rzPNhmK4G2wQAf5LLRveMK7i4YQ@mail.gmail.com>
References: <AM0PR06MB44672BFDC080F274D947E35CC8AD0@AM0PR06MB4467.eurprd06.prod.outlook.com>
 <CAGxFJbRL4hb_PQdZHeiXbt2rzPNhmK4G2wQAf5LLRveMK7i4YQ@mail.gmail.com>
Message-ID: <CAGxFJbQzZxAz0rQNoY6kUir_u3+xOuobG_kCUBgZN=Yb0cFzuQ@mail.gmail.com>

Also, check out the CRAN task view for survival.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Aug 14, 2019 at 10:39 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Search on "interval censored data with left truncation" at rseek.org.
> There appeared to be relevant hits there.
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Wed, Aug 14, 2019 at 10:28 AM Fabiana Gordon <
> fabiana.gordon at bristol.ac.uk> wrote:
>
>> Hello,
>>
>>
>> I have interval censored data with some late-entry subjects. Does any R
>> package for survival analysis with  interval censored data allows for left
>> truncation and if so how?
>>
>> Thanks and Best Wishes,
>>
>> Fabiana
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Aug 14 20:07:53 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 14 Aug 2019 11:07:53 -0700
Subject: [R] Rsession memory leak freezes Ubuntu 16.04
In-Reply-To: <CA+gY42Ck7SJ1RXuriHF=D0TDygWJQNsQ_7qrD-KCTYiOua7NMw@mail.gmail.com>
References: <CA+gY42Ck7SJ1RXuriHF=D0TDygWJQNsQ_7qrD-KCTYiOua7NMw@mail.gmail.com>
Message-ID: <DBEAA39E-20F5-4C0B-8F16-C8FB38C5FA8B@dcn.davis.ca.us>

Sorry, this is not the RStudio support area. If you have a problem running R at the command line, let us know, along with the output of sessionInfo().

https://support.rstudio.com

On August 13, 2019 8:41:25 PM PDT, Skyler Saleebyan <skylerbsaleebyan at gmail.com> wrote:
>Hi,
>
>Earlier R crashed on my system, after repeated attempts rebooting,
>running
>it under gdb, and clearing the ~/.rstudio-desktop, I have not been to
>run
>rstudio without rsession growing until it completely takes up my
>systems 16
>GB of RAM and freezes it.
>
>I know that rsession is where the leak is occurring because I killed
>the
>process once before the leak froze the whole system and I got this for
>memory usage  about 10 minutes after killing rstudio.
>
>skyler at skyler-G5-5587:~$ ps -A --sort -rss -o comm,pmem | head -n 11
>COMMAND         %MEM
>rsession        74.6
>compiz           1.5
>Discord          1.5
>Discord          0.7
>Discord          0.7
>gdb              0.7
>gnome-software   0.4
>Xorg             0.4
>evolution-calen  0.3
>gnome-system-mo  0.3
>
>before killing gdb to avoid a full freeze the trace through this error:
>[Thread 0x7fffdce3a700 (LWP 2481) exited]
>
>
>How do I scrub the current rsession and restart rstudio?
>
>Thanks,
>Skyler
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From drj|m|emon @end|ng |rom gm@||@com  Wed Aug 14 23:20:55 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Thu, 15 Aug 2019 07:20:55 +1000
Subject: [R] QQ plot
In-Reply-To: <CAF9-5jMWDvmjmT7ORV66PP-dOfct1n6Z2qxaKn9a9dqB6Dhf3A@mail.gmail.com>
References: <CAF9-5jMWDvmjmT7ORV66PP-dOfct1n6Z2qxaKn9a9dqB6Dhf3A@mail.gmail.com>
Message-ID: <CA+8X3fX+A+Ad2svXE0iEOZwaTee2W6Egyfka1+wJmSwmhrO6GA@mail.gmail.com>

Hi Ana Marija,
It would help if we had the data, but I suspect that the data that we
don't have is not uniformly distributed:

library(plotrix)
ams_norm<-rnorm(100)
png("ams_norm.png")
qqunif(rescale(ams_norm,c(0,1)))
dev.off()
ams_unif<-runif(100)
png("ams_unif.png")
qqunif(ams_unif)
dev.off()

Jim

On Thu, Aug 15, 2019 at 3:27 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Hello,
>
> I was plotting QQ plot (in attach) via:
>
> qqunif = function(p, BH=T, MAIN = " ", SUB=" ")
> {
>   nn = length(p)
>   xx =  -log10((1:nn)/(nn+1))
>   plot( xx,  -sort(log10(p)),
>         main = MAIN, sub= SUB, cex.sub=1.3,
>         xlab=expression(Expected~~-log[10](italic(p))),
>         ylab=expression(Observed~~-log[10](italic(p))),
>         cex.lab=1.0,mgp=c(2,1,0))
>   abline(0,1,col='red')
>   if(BH) ## BH = include Benjamini Hochberg FDR
>   {
>
>     abline(-log10(0.05),1, col='black',lty=1)
>     text(0.5,1.9 , "FDR=0.05", col = "gray60",srt=45, cex=1)
>     abline(-log10(0.10),1, col='black',lty=1)
>     text(0.5, 1.6, "FDR=0.10", col = "gray60",srt=45, cex=1)
>     abline(-log10(0.25),1, col='black',lty=1)
>     text(0.5, 1.2, "FDR=0.25", col = "gray60",srt=45, cex=1)
>     #legend('topleft', c("FDR = 0.05","FDR = 0.10","FDR = 0.25"),
>            #col=c('black','black','black'),lty=c(1,1,1), cex=0.8)
>     if (BF)
>     {
>       abline(h=-log10(0.05/nn), col='black') ## bonferroni
>     }
>   }
> }
>
> db=fread("/Users/ams/Desktop/GWAS_all.txt", header=T)
> dd=db[db$P<1e-3,]
> qqunif(dd$P)
>
> what do I need to change in my code so that FDR lines look like on the
> 2nd attachment?
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: ams_norm.png
Type: image/png
Size: 2715 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190815/66f11eb4/attachment.png>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: ams_unif.png
Type: image/png
Size: 2607 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190815/66f11eb4/attachment-0001.png>

From den|@e@je@ne@b @end|ng |rom gm@||@com  Fri Aug 16 16:18:38 2019
From: den|@e@je@ne@b @end|ng |rom gm@||@com (Denise b)
Date: Fri, 16 Aug 2019 10:18:38 -0400
Subject: [R] Score test for a subset of parameters estimated with coxph
In-Reply-To: <CAJfThnRV7Ryq7Uu4q6ZWsO6nDM27Hgqvu6eUqukd=rqUCRJMBQ@mail.gmail.com>
References: <mailman.355798.3521.1565906984.8808.r-help@r-project.org>
 <CAJfThnRV7Ryq7Uu4q6ZWsO6nDM27Hgqvu6eUqukd=rqUCRJMBQ@mail.gmail.com>
Message-ID: <CAJfThnTtbQqsXWxmByieSwdhXega_RGj1m5vuZGR=TfquK=V8g@mail.gmail.com>

>
> Dear R help list,
>
> I would like to perform a score test for a subset of the parameters
> estimated with coxph using the frailty() option.
> As illustrated in the following reproducible example, I am able to perform
> the score test with the standard coxph() but not in presence of frailty()
> argument. See Examples 1 and 2
>
> library(survival)
>
> # Example 1: score test using standard coxph()
> # score test for ph.karno & pat.karno coefficients
> fit1<-coxph(Surv(time,status)~sex+ ph.karno + pat.karno , data=lung)
> fit0<-coxph(Surv(time,status)~pat.karno , data=lung) # fit under null
> scorestat <- coxph(Surv(time,status)~sex+ ph.karno + pat.karno,
> init=c(0,0,coefficients(fit0)) ,iter=0, data=lung)
> scorestat$score
> [1] 10.50409
>
> # Example 2: coxph with frailty term
> # score test for ph.karno & pat.karno coefficients
> frailtyfit1<-coxph(Surv(time,status)~sex+ ph.karno + pat.karno
> +frailty(inst), data=lung)
> frailtyfit0<-coxph(Surv(time,status)~sex+ frailty(inst), data=lung)
> scorefrailtyfit1<-coxph(Surv(time,status)~sex+ ph.karno + pat.karno
> +frailty(inst), data=lung,
> init=c(0,0,coefficients(frailtyfit0)) ,iter=0)
>
> I get the following error message:
> Error in coxph(Surv(time, status) ~ sex + ph.karno + pat.karno +
> frailty(inst),  :
>   wrong length for init argument
>
>
> When I checked in the code I found
> if (length(init) != ncol(X))
>             stop("wrong length for init argument")
> Which seems that the length of the init argument doesnt match the number of
> predictors in the model.
>
> It seems that the code is expecting that I specify another parameter for
> frailty term.
> So, I tried to specify 2 as an inital value for frailty but got another
> error message later:
>
> scorefrailtyfit1<-coxph(Surv(time,status)~sex+ ph.karno + pat.karno
> +frailty(inst), data=lung,
> + init=c(0,0,coefficients(frailtyfit0),4.5),iter=0)
> Error in coxpenal.fit(X, Y, strats, offset, init = init, control, weights =
> weights,  :
>   Wrong length for inital values
>
> I have also checked coxme() but it seems that there is no option
> implemented
> for the score test.
> Do you have an idea of what could be the problem?
>
> Thanks,
> Denise
>
>
>

	[[alternative HTML version deleted]]


From m@vz@@hmed @end|ng |rom utoronto@c@  Fri Aug 16 17:38:02 2019
From: m@vz@@hmed @end|ng |rom utoronto@c@ (Mavra Ahmed)
Date: Fri, 16 Aug 2019 15:38:02 +0000
Subject: [R] Survey Data - comparing multiple groups
In-Reply-To: <CAB8pepyJwL1NFuP7-M8RdYFp-7qodpwBTpnfAeAhtXqJFpAYNw@mail.gmail.com>
References: <YTOPR0101MB2217A024D3A14B8B6ED7A5649BC00@YTOPR0101MB2217.CANPRD01.PROD.OUTLOOK.COM>
 <CAB8pepy=kPjp8LVKhn+s=LmDP0OXfoz7N2--c31dokVqPwsKZA@mail.gmail.com>
 <YTOPR0101MB2217A3348B2709185698D96E9BC00@YTOPR0101MB2217.CANPRD01.PROD.OUTLOOK.COM>,
 <CAB8pepyJwL1NFuP7-M8RdYFp-7qodpwBTpnfAeAhtXqJFpAYNw@mail.gmail.com>
Message-ID: <YTOPR0101MB221798F2F3FC2A8B58665CCC9BAF0@YTOPR0101MB2217.CANPRD01.PROD.OUTLOOK.COM>

Thank you so much for your response and apologies for the delay in responding. This has been helpful.


I have been looking into several other options over the last few weeks and been talking with statisticians as well and SAS seems to have some options that work for survey type of data. At this point, I am still exploring R and if I am able to provide additional feedback on this issue, I will post. I really appreciate your help. Thank you.


________________________________
From: Abby Spurdle <spurdle.a at gmail.com>
Sent: July 27, 2019 9:39:49 PM
To: Mavra Ahmed <mavz.ahmed at utoronto.ca>
Cc: r-help at r-project.org <r-help at r-project.org>
Subject: Re: [R] Survey Data - comparing multiple groups

> I would like to be able to get p-values between the groups and be able to adjust for multiple comparisons and would appreciate if someone can guide me.

> I did convert my df into a svrepdesign object as follows:
> > df<-svrepdesign (data=df1, scale=1, repweights = df1[, 496:995], type="BRR", combined.weights=TRUE, weight=~WTS_P, na.rm=TRUE)
>  therefore, I have called "df" as a svrepdesign however, from the survey package, the svyranktest is shown  with svydesign only and not with svrepdesign. I could consider using other tests such as svyglm but am also having issues with it.

This is outside my area.
However, as no one else as responded, I'll offer some more comments.

Firstly, I'm doubtful the Kruskal Wallis test will give you pairwise p-values.
There are other tests for this.
It's also possible to create a matrix of p-values, using an arbitrary test, and then use a correction method.
However, I recommend caution with this.

Secondly, I don't know if the svyranktest() function will work with the svrepdesign object or not.
However, if not, I would assume that other functions in the package would work.

Either way, I'm wondering if you've created the svrepdesign object correctly.
In particular, "weight=~WTS_P" would assign a formula to weights, probably, which doesn't sound right.
And "repweights = df1[, 496:995]" looks questionable, because one, is has hundreds of columns, and two, you've already given that data in the first argument.



	[[alternative HTML version deleted]]


From tr@xp|@yer @end|ng |rom gm@||@com  Sat Aug 17 01:10:24 2019
From: tr@xp|@yer @end|ng |rom gm@||@com (=?UTF-8?Q?Martin_M=C3=B8ller_Skarbiniks_Pedersen?=)
Date: Sat, 17 Aug 2019 01:10:24 +0200
Subject: [R] gmp coredump - where to report?
Message-ID: <CAGAA5beN68CRhYcxmjwhvZQMK5eGUUq28gKnzk6Chg8wX90LsQ@mail.gmail.com>

Hi,

  I has trying to convert some raw values into a big number with the
library gmp.
However the library makes R crash. Two questions:

1. Should I report the problem and if yes, where can I report the problem?
2. Is the source code for the R version of GMP somewhere on eg. github, so
I can
post an issue?

Regards
Martin

Here is the code that generate the core-dump:

$ head  -3 ~/R/x86_64-pc-linux-gnu-library/3.6/gmp/DESCRIPTION
Package: gmp
Version: 0.5-13.5
Date: 2019-02-21

$ R --version | head -3
R version 3.6.1 (2019-07-05) -- "Action of the Toes"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

$ R --vanilla

> library(gmp)
> as.bigz(charToRaw("a"))

 *** caught segfault ***
address 0x560e78e1dd4c, cause 'memory not mapped'

	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Sat Aug 17 01:34:31 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sat, 17 Aug 2019 11:34:31 +1200
Subject: [R] gmp coredump - where to report?
In-Reply-To: <CAGAA5beN68CRhYcxmjwhvZQMK5eGUUq28gKnzk6Chg8wX90LsQ@mail.gmail.com>
References: <CAGAA5beN68CRhYcxmjwhvZQMK5eGUUq28gKnzk6Chg8wX90LsQ@mail.gmail.com>
Message-ID: <CAB8pepxWB1ANcmagmd-GNhpPHFWbWMcLtbweDwa+wUSk9chk+Q@mail.gmail.com>

(excerpts only)
> I has trying to convert some raw values into a big number with the
> library gmp.
> However the library makes R crash. Two questions:
> 1. Should I report the problem and if yes, where can I report the problem?
> 2. Is the source code for the R version of GMP somewhere on eg. github, so
> I can
> post an issue?

> > library(gmp)
> > as.bigz(charToRaw("a"))

>  *** caught segfault ***
> address 0x560e78e1dd4c, cause 'memory not mapped'

(1)
No, you should not.
According to the gmp package documentation for the function as.bigz():
"either integer, numeric (i.e., double) or character vector"

So, your *input is unsuitable*.

In principle, it should give a better error message.
However, the version number is < 1.
So, I would wait for the authors to finish the package.
However, it's possible they will read your post.

(2)
It's not an R version of GMP, as such.
It's an R library, with the same name, which calls the C library.
The source code is on CRAN, as per usual.

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat Aug 17 03:05:01 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 16 Aug 2019 18:05:01 -0700
Subject: [R] gmp coredump - where to report?
In-Reply-To: <CAB8pepxWB1ANcmagmd-GNhpPHFWbWMcLtbweDwa+wUSk9chk+Q@mail.gmail.com>
References: <CAGAA5beN68CRhYcxmjwhvZQMK5eGUUq28gKnzk6Chg8wX90LsQ@mail.gmail.com>
 <CAB8pepxWB1ANcmagmd-GNhpPHFWbWMcLtbweDwa+wUSk9chk+Q@mail.gmail.com>
Message-ID: <C51C98C2-5721-4AE3-85F3-D7C4A806C1CD@dcn.davis.ca.us>

And for a different opinion...

Packages that allow the user to trigger core dumps (regardless of how) should be reported and fixed. It is the responsibility of the R package author to put in input validation to avoid such errors. You can read the description file via the maintainer function or on the CRAN website (cc'ed).

And not so much opinion as a quibble... in many compiled languages the term for a group of functions is a library... but in R a group of functions is a package and a group of installed packages in a specific directory layout is a library. This is clearly stated in the R Administration and Installation manual at the beginning of Section 6. Anyway, the lingo is that the package maintainer makes sure the C library is called with valid parameters, while the R user of that package installs the package in their user library and does not normally directly interact with the C library.


On August 16, 2019 4:34:31 PM PDT, Abby Spurdle <spurdle.a at gmail.com> wrote:
>(excerpts only)
>> I has trying to convert some raw values into a big number with the
>> library gmp.
>> However the library makes R crash. Two questions:
>> 1. Should I report the problem and if yes, where can I report the
>problem?
>> 2. Is the source code for the R version of GMP somewhere on eg.
>github, so
>> I can
>> post an issue?
>
>> > library(gmp)
>> > as.bigz(charToRaw("a"))
>
>>  *** caught segfault ***
>> address 0x560e78e1dd4c, cause 'memory not mapped'
>
>(1)
>No, you should not.
>According to the gmp package documentation for the function as.bigz():
>"either integer, numeric (i.e., double) or character vector"
>
>So, your *input is unsuitable*.
>
>In principle, it should give a better error message.
>However, the version number is < 1.
>So, I would wait for the authors to finish the package.
>However, it's possible they will read your post.
>
>(2)
>It's not an R version of GMP, as such.
>It's an R library, with the same name, which calls the C library.
>The source code is on CRAN, as per usual.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From tr@xp|@yer @end|ng |rom gm@||@com  Sat Aug 17 11:03:49 2019
From: tr@xp|@yer @end|ng |rom gm@||@com (=?UTF-8?Q?Martin_M=C3=B8ller_Skarbiniks_Pedersen?=)
Date: Sat, 17 Aug 2019 11:03:49 +0200
Subject: [R] GMP help - converting rosetta RSA-code to R
Message-ID: <CAGAA5bdeaw0uXGy-ctcb+XAiSeOjUH_2eqmN19=5PXH4YHwMgA@mail.gmail.com>

Hi,
    I am trying to make a R version a RosettaCode task involving big number.
    More precise, I am trying to convert the c-solution
http://rosettacode.org/wiki/RSA_code#C
to R.

   These two lines in C gives me problems:
const char *plaintext = "Rossetta Code";
mpz_import(pt, strlen(plaintext), 1, 1, 0, 0, plaintext);

I have tried:
library(gmp)
plaintext <- "Rossetta Code"
as.bigz(charToRaw(plaintext))

and
library(gmp)
plaintext <- "Rossetta Code
as.big(split(plaintext,""))

Thanks for any help/suggestions

Regards
Martin M. S. Pedersen

	[[alternative HTML version deleted]]


From wdun|@p @end|ng |rom t|bco@com  Sat Aug 17 18:17:11 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Sat, 17 Aug 2019 09:17:11 -0700
Subject: [R] gmp coredump - where to report?
In-Reply-To: <CAGAA5beN68CRhYcxmjwhvZQMK5eGUUq28gKnzk6Chg8wX90LsQ@mail.gmail.com>
References: <CAGAA5beN68CRhYcxmjwhvZQMK5eGUUq28gKnzk6Chg8wX90LsQ@mail.gmail.com>
Message-ID: <CAF8bMcZhb-293MBs0WnusyrPA_neT=p7Je0dtKJCmJtqQW1K4g@mail.gmail.com>

  I has trying to convert some raw values into a big number with the
library gmp.
However the library makes R crash. Two questions:
1. Should I report the problem and if yes, where can I report the problem?

You can report the problem by calling bug.report(package="gmp") and filling
in the details.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Fri, Aug 16, 2019 at 4:10 PM Martin M?ller Skarbiniks Pedersen <
traxplayer at gmail.com> wrote:

> Hi,
>
>   I has trying to convert some raw values into a big number with the
> library gmp.
> However the library makes R crash. Two questions:
>
> 1. Should I report the problem and if yes, where can I report the problem?
> 2. Is the source code for the R version of GMP somewhere on eg. github, so
> I can
> post an issue?
>
> Regards
> Martin
>
> Here is the code that generate the core-dump:
>
> $ head  -3 ~/R/x86_64-pc-linux-gnu-library/3.6/gmp/DESCRIPTION
> Package: gmp
> Version: 0.5-13.5
> Date: 2019-02-21
>
> $ R --version | head -3
> R version 3.6.1 (2019-07-05) -- "Action of the Toes"
> Copyright (C) 2019 The R Foundation for Statistical Computing
> Platform: x86_64-pc-linux-gnu (64-bit)
>
> $ R --vanilla
>
> > library(gmp)
> > as.bigz(charToRaw("a"))
>
>  *** caught segfault ***
> address 0x560e78e1dd4c, cause 'memory not mapped'
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat Aug 17 18:47:00 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 17 Aug 2019 09:47:00 -0700 (PDT)
Subject: [R] GMP help - converting rosetta RSA-code to R
In-Reply-To: <CAGAA5bdeaw0uXGy-ctcb+XAiSeOjUH_2eqmN19=5PXH4YHwMgA@mail.gmail.com>
References: <CAGAA5bdeaw0uXGy-ctcb+XAiSeOjUH_2eqmN19=5PXH4YHwMgA@mail.gmail.com>
Message-ID: <alpine.BSF.2.00.1908170940140.33041@pedal.dcn.davis.ca.us>

Please post in plain text. You assume significant risk that we will 
receive a corrupt version of your code if you fail to do this.

As far as I can see, the gmp package does not currently support a raw 
interface to create bigz values. Here is my workaround:

####
plaintext <- "Rossetta Code"
stringtoasciihex <- function( s ) {
   vs <- strsplit( s, "", fixed = TRUE )[[ 1 ]]
   asc <- sapply( vs, function( C ) as.character( charToRaw( C ) ) )
   paste( c( "0x", asc ), collapse="" )
}
gmp::as.bigz( stringtoasciihex( plaintext ) )
###

On Sat, 17 Aug 2019, Martin M?ller Skarbiniks Pedersen wrote:

> Hi,
>    I am trying to make a R version a RosettaCode task involving big number.
>    More precise, I am trying to convert the c-solution
> http://rosettacode.org/wiki/RSA_code#C
> to R.
>
>   These two lines in C gives me problems:
> const char *plaintext = "Rossetta Code";
> mpz_import(pt, strlen(plaintext), 1, 1, 0, 0, plaintext);
>
> I have tried:
> library(gmp)
> plaintext <- "Rossetta Code"
> as.bigz(charToRaw(plaintext))
>
> and
> library(gmp)
> plaintext <- "Rossetta Code
> as.big(split(plaintext,""))
>
> Thanks for any help/suggestions
>
> Regards
> Martin M. S. Pedersen
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From tte@|@yg @end|ng |rom gm@||@com  Sat Aug 17 16:56:35 2019
From: tte@|@yg @end|ng |rom gm@||@com (Tewelde Tesfaye)
Date: Sat, 17 Aug 2019 17:56:35 +0300
Subject: [R] Unable to load GRanges
Message-ID: <CAOGW4qFCY7yz-UaNFR_Tdoecbq0B_oVzdQBZVNVJ-B8p+zNq5g@mail.gmail.com>

Dear Sir/Madam
Greetings
Error message [
install.packages("BiocManager")
Installing package into
?C:/Users/default.DESKTOP-DE6RR7L/Documents/R/win-library/3.6?
(as ?lib? is unspecified)
--- Please select a CRAN mirror for use in this session ---
trying URL '
https://cloud.r-project.org/bin/windows/contrib/3.6/BiocManager_1.30.4.zip'
Content type 'application/zip' length 292998 bytes (286 KB)
downloaded 286 KB

package ?BiocManager? successfully unpacked and MD5 sums checked

The downloaded binary packages are in

C:\Users\default.DESKTOP-DE6RR7L\AppData\Local\Temp\RtmpKSLUUH\downloaded_packages
> BiocManager::install("GenomicRanges")
Bioconductor version 3.9 (BiocManager 1.30.4), R 3.6.1 (2019-07-05)
Installing package(s) 'GenomicRanges'
trying URL '
https://bioconductor.org/packages/3.9/bioc/bin/windows/contrib/3.6/GenomicRanges_1.36.0.zip
'
Content type 'application/zip' length 2135144 bytes (2.0 MB)
downloaded 2.0 MB

package ?GenomicRanges? successfully unpacked and MD5 sums checked

The downloaded binary packages are in

C:\Users\default.DESKTOP-DE6RR7L\AppData\Local\Temp\RtmpKSLUUH\downloaded_packages
installation path not writeable, unable to update packages: boot, foreign,
nlme ]

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun Aug 18 00:24:27 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 17 Aug 2019 15:24:27 -0700
Subject: [R] Unable to load GRanges
In-Reply-To: <CAOGW4qFCY7yz-UaNFR_Tdoecbq0B_oVzdQBZVNVJ-B8p+zNq5g@mail.gmail.com>
References: <CAOGW4qFCY7yz-UaNFR_Tdoecbq0B_oVzdQBZVNVJ-B8p+zNq5g@mail.gmail.com>
Message-ID: <50D93A8B-06DF-4B78-923F-A9C023BBD11E@dcn.davis.ca.us>

This mailing list is the wrong place to ask a BioC question. I do recommend asking a question when you finally do arrive at the correct forum ... the below information seems to suggest that something you did or something odd about your account privileges lead to that output but you haven't indicated what you did.

https://www.bioconductor.org/help/support/
https://bioconductor.org/packages/release/bioc/html/GenomicRanges.html

On August 17, 2019 7:56:35 AM PDT, Tewelde Tesfaye <ttesfayg at gmail.com> wrote:
>Dear Sir/Madam
>Greetings
>Error message [
>install.packages("BiocManager")
>Installing package into
>?C:/Users/default.DESKTOP-DE6RR7L/Documents/R/win-library/3.6?
>(as ?lib? is unspecified)
>--- Please select a CRAN mirror for use in this session ---
>trying URL '
>https://cloud.r-project.org/bin/windows/contrib/3.6/BiocManager_1.30.4.zip'
>Content type 'application/zip' length 292998 bytes (286 KB)
>downloaded 286 KB
>
>package ?BiocManager? successfully unpacked and MD5 sums checked
>
>The downloaded binary packages are in
>
>C:\Users\default.DESKTOP-DE6RR7L\AppData\Local\Temp\RtmpKSLUUH\downloaded_packages
>> BiocManager::install("GenomicRanges")
>Bioconductor version 3.9 (BiocManager 1.30.4), R 3.6.1 (2019-07-05)
>Installing package(s) 'GenomicRanges'
>trying URL '
>https://bioconductor.org/packages/3.9/bioc/bin/windows/contrib/3.6/GenomicRanges_1.36.0.zip
>'
>Content type 'application/zip' length 2135144 bytes (2.0 MB)
>downloaded 2.0 MB
>
>package ?GenomicRanges? successfully unpacked and MD5 sums checked
>
>The downloaded binary packages are in
>
>C:\Users\default.DESKTOP-DE6RR7L\AppData\Local\Temp\RtmpKSLUUH\downloaded_packages
>installation path not writeable, unable to update packages: boot,
>foreign,
>nlme ]
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From dw|n@em|u@ @end|ng |rom comc@@t@net  Sun Aug 18 00:53:57 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Sat, 17 Aug 2019 15:53:57 -0700
Subject: [R] Unable to load GRanges
In-Reply-To: <CAOGW4qFCY7yz-UaNFR_Tdoecbq0B_oVzdQBZVNVJ-B8p+zNq5g@mail.gmail.com>
References: <CAOGW4qFCY7yz-UaNFR_Tdoecbq0B_oVzdQBZVNVJ-B8p+zNq5g@mail.gmail.com>
Message-ID: <193912cd-8a07-d76d-5df9-80ca239f77b7@comcast.net>

Looks like you do not have permissions to write to that directory, but 
it's not clear that this is a major problem. The updating of boot and 
foreign is not something that regular non BioC users would have usually 
needed.? Did you recently update your R installation? Library locations 
sometimes get changed when you do that.? If so, what happens when you 
execute:

update.packages(checkBuilt=TRUE)


-- 

David.

On 8/17/19 7:56 AM, Tewelde Tesfaye wrote:
> Dear Sir/Madam
> Greetings
> Error message [
> install.packages("BiocManager")
> Installing package into
> ?C:/Users/default.DESKTOP-DE6RR7L/Documents/R/win-library/3.6?
> (as ?lib? is unspecified)
> --- Please select a CRAN mirror for use in this session ---
> trying URL '
> https://cloud.r-project.org/bin/windows/contrib/3.6/BiocManager_1.30.4.zip'
> Content type 'application/zip' length 292998 bytes (286 KB)
> downloaded 286 KB
>
> package ?BiocManager? successfully unpacked and MD5 sums checked
>
> The downloaded binary packages are in
>
> C:\Users\default.DESKTOP-DE6RR7L\AppData\Local\Temp\RtmpKSLUUH\downloaded_packages
>> BiocManager::install("GenomicRanges")
> Bioconductor version 3.9 (BiocManager 1.30.4), R 3.6.1 (2019-07-05)
> Installing package(s) 'GenomicRanges'
> trying URL '
> https://bioconductor.org/packages/3.9/bioc/bin/windows/contrib/3.6/GenomicRanges_1.36.0.zip
> '
> Content type 'application/zip' length 2135144 bytes (2.0 MB)
> downloaded 2.0 MB
>
> package ?GenomicRanges? successfully unpacked and MD5 sums checked
>
> The downloaded binary packages are in
>
> C:\Users\default.DESKTOP-DE6RR7L\AppData\Local\Temp\RtmpKSLUUH\downloaded_packages
> installation path not writeable, unable to update packages: boot, foreign,
> nlme ]
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From t@m@@@|erenc| @end|ng |rom med@t@t@hu  Sun Aug 18 19:07:07 2019
From: t@m@@@|erenc| @end|ng |rom med@t@t@hu (Ferenci Tamas)
Date: Sun, 18 Aug 2019 19:07:07 +0200
Subject: [R] conflicting results for a time-varying coefficient in a Cox
 model
In-Reply-To: <771925$c6ifva@ironport10.mayo.edu>
References: <1697509011.20190808160746@medstat.hu>
 <771925$c6ifva@ironport10.mayo.edu>
Message-ID: <2310514431.20190818190707@medstat.hu>

Dear Dr. Therneau,

Thank you very much for your exhaustive answer!

I now see the issue. Perhaps even more important was your confirmation that my approach with karno:ns(time, df=4) is theoretically correct. (I knew that plot.cox.zph is sound, so I was afraid that the difference can be attributed to some fundamental mistake in my approach.)

Thank you again,
Tamas


2019. augusztus 8., 18:17:38, ?rtad:


This is an excellent question.  
The answer, in this particular case, mostly has to do with the outlier time values.  (I've never been convinced that the death at time 999 isn't really a misplaced code for "missing", actually).    If you change the knots used by the spline you can get quite different values.
For instance, using a smaller data set:

fit1 <-  coxph(Surv(tstart, time, status) ~ trt + prior + karno, veteran)
zph1 <- cox.zph(fit1, transform='identity')
plot(zph1[3])

dtime <- unique(veteran$time[veteran$status ==1])    # all of the death times
veteran2 <- survSplit( Surv(time, status) ~ ., data=veteran, cut=dtime)
fit2 <- coxph(Surv(tstart, time, status) ~ trt + prior + karno +
        karno:ns(time, df=4),  data=veteran2)
tx <- 0:100 * 10    # x positions for plot
ncall <- attr(terms(fit2), "predvars")[[6]]
ty <- eval(ncall, data.frame(time = tx)) %*% coef(fit2)[4:7] + coef(fit2)[3]
lines(tx, ty, col=2)

-------------

Now it looks even worse!  The only difference is that the ns() function has chosen a different set of knots.   

The test used by the cox.zph function is based on a score test and is solid.  The plot that it produces uses a smoothed approximation to the variance matrix and is approximate.  So the diagnostic plot will never exactly match an actual fit.   In this data set the outliers exacerbate the issue.  To see this try a different time scale.

------------
zph2 <- cox.zph(fit1, transform= sqrt)
plot(zph2[3])
veteran2$stime <- sqrt(veteran2$time)
fit3 <- coxph(Surv(tstart, time, status) ~ trt + prior + karno +
           karno:ns(stime, df=4),  data=veteran2)

ncall3 <- attr(terms(fit3), "predvars")[[6]] 
ty3 <- eval(ncall3, data.frame(stime= sqrt(tx))) %*% coef(fit3)[4:7] + coef(fit3)[3]
lines(sqrt(tx), ty3, col=2)

----------------

The right tail is now better behaved.   Eliminating the points >900 makes things even better behaved.

Terry T.




On 8/8/19 9:07 AM, Ferenci Tamas wrote:

I was thinking of two possible ways to
plot a time-varying coefficient in a Cox model.

One is simply to use survival::plot.cox.zph which directly produces a
beta(t) vs t diagram.

The other is to transform the dataset to counting process format and
manually include an interaction with time, expanded with spline (to be
similar to plot.cox.zph). Plotting the coefficient produces the needed
beta(t) vs t diagram.

I understand that they're slightly different approaches, so I don't
expect totally identical results, but nevertheless, they approximate
the very same thing, so I do expect that the results are more or less
similar.

However:

library( survival )
library( splines )

data( veteran )

zp <- cox.zph( coxph(Surv(time, status) ~ trt + prior + karno,
                     data = veteran ), transform = "identity" )[ 3 ]

veteran3 <- survSplit( Surv(time, status) ~ trt + prior + karno,
                       data = veteran, cut = 1:max(veteran$time) )

fit <- coxph(Surv(tstart,time, status) ~ trt + prior + karno +
               karno:ns( time, df = 4 ), data = veteran3 )
cf <- coef( fit )
nsvet <- ns( veteran3$time, df = 4 )

plot( zp )
lines( 0:1000, ns( 0:1000, df = 4, knots = attr( nsvet, "knots" ),
                   Boundary.knots = attr( nsvet, "Boundary.knots" ) )%*%cf[
                     grep( "karno:ns", names( cf ) ) ] + cf["karno"],
       type = "l", col = "red" )

Where is the mistake? Something must be going on here, because the
plots are vastly different...

Thank you very much in advance,
Tamas


From t@m@@@|erenc| @end|ng |rom med@t@t@hu  Sun Aug 18 19:10:12 2019
From: t@m@@@|erenc| @end|ng |rom med@t@t@hu (Ferenci Tamas)
Date: Sun, 18 Aug 2019 19:10:12 +0200
Subject: [R] results of a survival analysis change when converting the data
 to counting process format
Message-ID: <1846389355.20190818191012@medstat.hu>

Dear All,

Consider the following simple example:

library( survival )
data( veteran )

coef( coxph(Surv(time, status) ~ trt + prior + karno, data = veteran) )
         trt        prior        karno 
 0.180197194 -0.005550919 -0.033771018

Note that we have neither time-dependent covariates, nor time-varying
coefficients, so the results should be the same if we change to
counting process format, no matter where we cut the times.

That's true if we cut at event times:

veteran2 <- survSplit( Surv(time, status) ~ trt + prior + karno,
                       data = veteran, cut = unique( veteran$time ) )

coef( coxph(Surv(tstart,time, status) ~ trt + prior + karno, data = veteran2 ) )
         trt        prior        karno 
 0.180197194 -0.005550919 -0.033771018 

But quite interestingly not true, if we cut at every day:

veteran3 <- survSplit( Surv(time, status) ~ trt + prior + karno,
                       data = veteran, cut = 1:max(veteran$time) )

coef( coxph(Surv(tstart,time, status) ~ trt + prior + karno, data = veteran3 ) )
         trt        prior        karno 
 0.180197215 -0.005550913 -0.033771016 

The difference is not large, but definitely more than just a rounding
error, or something like that.

What's going on? How can the results get wrong, especially by
including more cutpoints?

Thank you in advance,
Tamas


From bog@@o@chr|@to|er @end|ng |rom gm@||@com  Sun Aug 18 20:01:35 2019
From: bog@@o@chr|@to|er @end|ng |rom gm@||@com (Christofer Bogaso)
Date: Sun, 18 Aug 2019 23:31:35 +0530
Subject: [R] Partition a vector into select groups with fixed length
Message-ID: <CA+dpOJmjnuBAzMjtTEphUYwU8EF1WvQBOdAb_XFL8SkEumTBCQ@mail.gmail.com>

Hi,

Let say I have a vector as below

Vec = LETTERS

Now I want to break this vector into groups of the same length of 5.

So,
1st group consists - "A" "B" "C" "D" "E"
2nd group - "F" "G" "H" "I" "J"

and so on..
last group will consist only the leftover elements

I have a very large initial vector, so looking for some efficient way
to achieve the same. Any pointer will be highly appreciated.

Thanks for your time.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Aug 18 20:37:26 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 18 Aug 2019 19:37:26 +0100
Subject: [R] Partition a vector into select groups with fixed length
In-Reply-To: <CA+dpOJmjnuBAzMjtTEphUYwU8EF1WvQBOdAb_XFL8SkEumTBCQ@mail.gmail.com>
References: <CA+dpOJmjnuBAzMjtTEphUYwU8EF1WvQBOdAb_XFL8SkEumTBCQ@mail.gmail.com>
Message-ID: <e337e01f-d9f5-44de-0026-2b3d29d87d25@sapo.pt>

Hello,

The following function will do it.
It uses a standard cumsum trick to create a break vector f.
And predicts special cases (n = 0 or n = 1).

breakVec <- function(x, n = 5){
   if(n < 1) stop(paste("Illegal value n:", n))
   if(n == 1){
     f = ""
   }else{
     f <- c(1, rep(0, n - 1))
     f <- rep(f, length.out = length(x))
     f <- cumsum(f)
   }
   split(x, f)
}

breakVec(LETTERS)
breakVec(LETTERS, 4)
breakVec(LETTERS, 1)
breakVec(LETTERS, -1)


Hope this helps,

Rui Barradas

?s 19:01 de 18/08/19, Christofer Bogaso escreveu:
> Hi,
> 
> Let say I have a vector as below
> 
> Vec = LETTERS
> 
> Now I want to break this vector into groups of the same length of 5.
> 
> So,
> 1st group consists - "A" "B" "C" "D" "E"
> 2nd group - "F" "G" "H" "I" "J"
> 
> and so on..
> last group will consist only the leftover elements
> 
> I have a very large initial vector, so looking for some efficient way
> to achieve the same. Any pointer will be highly appreciated.
> 
> Thanks for your time.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de  Sun Aug 18 20:41:45 2019
From: gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de (Gerrit Eichner)
Date: Sun, 18 Aug 2019 20:41:45 +0200
Subject: [R] Partition a vector into select groups with fixed length
In-Reply-To: <CA+dpOJmjnuBAzMjtTEphUYwU8EF1WvQBOdAb_XFL8SkEumTBCQ@mail.gmail.com>
References: <CA+dpOJmjnuBAzMjtTEphUYwU8EF1WvQBOdAb_XFL8SkEumTBCQ@mail.gmail.com>
Message-ID: <2fb2af0b-e0c0-c2b5-09fd-90579eb14db6@math.uni-giessen.de>

Hi, Christofer,

try something along

len <- 5
split(Vec, rep(seq(ceiling(length(Vec)/len)), each = len))

  Hth  --  Gerrit

---------------------------------------------------------------------
Dr. Gerrit Eichner                   Mathematical Institute, Room 212
gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
Fax: +49-(0)641-99-32109            http://www.uni-giessen.de/eichner
---------------------------------------------------------------------

Am 18.08.2019 um 20:01 schrieb Christofer Bogaso:
> Hi,
> 
> Let say I have a vector as below
> 
> Vec = LETTERS
> 
> Now I want to break this vector into groups of the same length of 5.
> 
> So,
> 1st group consists - "A" "B" "C" "D" "E"
> 2nd group - "F" "G" "H" "I" "J"
> 
> and so on..
> last group will consist only the leftover elements
> 
> I have a very large initial vector, so looking for some efficient way
> to achieve the same. Any pointer will be highly appreciated.
> 
> Thanks for your time.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From g@e@sth@m@giibert m@iii@g oii gm@ii@com  Sun Aug 18 20:47:20 2019
From: g@e@sth@m@giibert m@iii@g oii gm@ii@com (g@e@sth@m@giibert m@iii@g oii gm@ii@com)
Date: Sun, 18 Aug 2019 14:47:20 -0400
Subject: [R] Creating data using multiple for loops
Message-ID: <CAFycp74spy+kQCBO9UdFopzDeEpyojgoq55wRo=12Fohm0f66Q@mail.gmail.com>

I would like to create pseudo identification numbers in the format of last
four of a social security number (0000 to 9999), month of birth (01 to 12),
and day of birth (01-28). The IDs can be character.

I have gotten this far:

for (ssn in 0:9){
     for (month in 1:3){
          for (day in 1:5){
                      }
                      id <-paste(ssn, month, day, sep="")
            }
}

limiting each value above for demonstration purposes. I cannot figure out
how to store the created IDs. I know I have to create a container, but I
don't know, among other things, how to index the container.  Any help is
appreciated. TIA

-Greg

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun Aug 18 21:08:03 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 18 Aug 2019 12:08:03 -0700
Subject: [R] Partition a vector into select groups with fixed length
In-Reply-To: <2fb2af0b-e0c0-c2b5-09fd-90579eb14db6@math.uni-giessen.de>
References: <CA+dpOJmjnuBAzMjtTEphUYwU8EF1WvQBOdAb_XFL8SkEumTBCQ@mail.gmail.com>
 <2fb2af0b-e0c0-c2b5-09fd-90579eb14db6@math.uni-giessen.de>
Message-ID: <CAGxFJbSEaCF9FxwXLZJD0O_-0BP8Hm-dWH9DQgeF5O88HJc5YQ@mail.gmail.com>

Perhaps simpler:
Hint:  (seq_along(LETTERS) -1) %/% 5
## modular arithmetic can be useful for this sort of thing


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Aug 18, 2019 at 11:47 AM Gerrit Eichner <
gerrit.eichner at math.uni-giessen.de> wrote:

> Hi, Christofer,
>
> try something along
>
> len <- 5
> split(Vec, rep(seq(ceiling(length(Vec)/len)), each = len))
>
>   Hth  --  Gerrit
>
> ---------------------------------------------------------------------
> Dr. Gerrit Eichner                   Mathematical Institute, Room 212
> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> Fax: +49-(0)641-99-32109            http://www.uni-giessen.de/eichner
> ---------------------------------------------------------------------
>
> Am 18.08.2019 um 20:01 schrieb Christofer Bogaso:
> > Hi,
> >
> > Let say I have a vector as below
> >
> > Vec = LETTERS
> >
> > Now I want to break this vector into groups of the same length of 5.
> >
> > So,
> > 1st group consists - "A" "B" "C" "D" "E"
> > 2nd group - "F" "G" "H" "I" "J"
> >
> > and so on..
> > last group will consist only the leftover elements
> >
> > I have a very large initial vector, so looking for some efficient way
> > to achieve the same. Any pointer will be highly appreciated.
> >
> > Thanks for your time.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun Aug 18 21:17:28 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 18 Aug 2019 12:17:28 -0700
Subject: [R] Creating data using multiple for loops
In-Reply-To: <CAFycp74spy+kQCBO9UdFopzDeEpyojgoq55wRo=12Fohm0f66Q@mail.gmail.com>
References: <CAFycp74spy+kQCBO9UdFopzDeEpyojgoq55wRo=12Fohm0f66Q@mail.gmail.com>
Message-ID: <CAGxFJbS4cORBWiSpmiya=EFZQycSqVxt9Xsp+Z1RTd-3FLiwCw@mail.gmail.com>

id <- do.call(paste0,expand.grid(0:9, 1:3, 1:5))

Comment: If you use R much, you'll do much better using R language
constructs than trying to apply those from other languages (Java perhaps?).
I realize this can be difficult, especially if you are experienced in the
another language (or languages), but it's worth the effort.


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Aug 18, 2019 at 11:58 AM <g.eastham.gilbert at gmail.com> wrote:

> I would like to create pseudo identification numbers in the format of last
> four of a social security number (0000 to 9999), month of birth (01 to 12),
> and day of birth (01-28). The IDs can be character.
>
> I have gotten this far:
>
> for (ssn in 0:9){
>      for (month in 1:3){
>           for (day in 1:5){
>                       }
>                       id <-paste(ssn, month, day, sep="")
>             }
> }
>
> limiting each value above for demonstration purposes. I cannot figure out
> how to store the created IDs. I know I have to create a container, but I
> don't know, among other things, how to index the container.  Any help is
> appreciated. TIA
>
> -Greg
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Aug 18 21:22:41 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 18 Aug 2019 20:22:41 +0100
Subject: [R] Partition a vector into select groups with fixed length
In-Reply-To: <CAGxFJbSEaCF9FxwXLZJD0O_-0BP8Hm-dWH9DQgeF5O88HJc5YQ@mail.gmail.com>
References: <CA+dpOJmjnuBAzMjtTEphUYwU8EF1WvQBOdAb_XFL8SkEumTBCQ@mail.gmail.com>
 <2fb2af0b-e0c0-c2b5-09fd-90579eb14db6@math.uni-giessen.de>
 <CAGxFJbSEaCF9FxwXLZJD0O_-0BP8Hm-dWH9DQgeF5O88HJc5YQ@mail.gmail.com>
Message-ID: <efcff278-9ced-764e-24ad-6434d2a039a7@sapo.pt>

Hello,
Bert's modular arithmetic way simplifies my code a lot.

breakVec <- function(x, n = 5){
   if(n < 1) stop(paste("Illegal value n:", n))
   f <- if(n == 1) "" else (seq_along(x) - 1) %/% n
   split(x, f)
}



Hope this helps,

Rui Barradas

?s 20:08 de 18/08/19, Bert Gunter escreveu:
> Perhaps simpler:
> Hint:  (seq_along(LETTERS) -1) %/% 5
> ## modular arithmetic can be useful for this sort of thing
> 
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
> On Sun, Aug 18, 2019 at 11:47 AM Gerrit Eichner <
> gerrit.eichner at math.uni-giessen.de> wrote:
> 
>> Hi, Christofer,
>>
>> try something along
>>
>> len <- 5
>> split(Vec, rep(seq(ceiling(length(Vec)/len)), each = len))
>>
>>    Hth  --  Gerrit
>>
>> ---------------------------------------------------------------------
>> Dr. Gerrit Eichner                   Mathematical Institute, Room 212
>> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
>> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
>> Fax: +49-(0)641-99-32109            http://www.uni-giessen.de/eichner
>> ---------------------------------------------------------------------
>>
>> Am 18.08.2019 um 20:01 schrieb Christofer Bogaso:
>>> Hi,
>>>
>>> Let say I have a vector as below
>>>
>>> Vec = LETTERS
>>>
>>> Now I want to break this vector into groups of the same length of 5.
>>>
>>> So,
>>> 1st group consists - "A" "B" "C" "D" "E"
>>> 2nd group - "F" "G" "H" "I" "J"
>>>
>>> and so on..
>>> last group will consist only the leftover elements
>>>
>>> I have a very large initial vector, so looking for some efficient way
>>> to achieve the same. Any pointer will be highly appreciated.
>>>
>>> Thanks for your time.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From drj|m|emon @end|ng |rom gm@||@com  Sun Aug 18 23:46:15 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Mon, 19 Aug 2019 07:46:15 +1000
Subject: [R] Creating data using multiple for loops
In-Reply-To: <CAFycp74spy+kQCBO9UdFopzDeEpyojgoq55wRo=12Fohm0f66Q@mail.gmail.com>
References: <CAFycp74spy+kQCBO9UdFopzDeEpyojgoq55wRo=12Fohm0f66Q@mail.gmail.com>
Message-ID: <CA+8X3fVBkdvLHYMSJvA=onYOTr-OhHUXQWiTpoLvo4AjKDgoCw@mail.gmail.com>

Hi Greg,
One problem is that you have misplaced the closing brace in the third
loop. It should follow the assignment statement. Because you used
loops rather than Bert's suggestion, perhaps you are trying to order
the values assigned. In your example, the ordering will be ssn, then
month of birth, then day of birth. Occasionally people resort to an
explicit calculation for the index:

id<-vector("character",10*3*5)
for (ssn in 0:9){
     for (month in 1:3){
          for (day in 1:5){
                      id[day+(month-1)*5+ssn*15] <-paste0(ssn, month, day)
          }
     }
}

This would order the values in the opposite precedence. Also, you may
not want to create well over 3 million values as in your initial
specification, in which case a different strategy using "sample" would
be appropriate.

Jim

On Mon, Aug 19, 2019 at 4:58 AM <g.eastham.gilbert at gmail.com> wrote:
>
> I would like to create pseudo identification numbers in the format of last
> four of a social security number (0000 to 9999), month of birth (01 to 12),
> and day of birth (01-28). The IDs can be character.
>
> I have gotten this far:
>
> for (ssn in 0:9){
>      for (month in 1:3){
>           for (day in 1:5){
>                       }
>                       id <-paste(ssn, month, day, sep="")
>             }
> }
>
> limiting each value above for demonstration purposes. I cannot figure out
> how to store the created IDs. I know I have to create a container, but I
> don't know, among other things, how to index the container.  Any help is
> appreciated. TIA
>
> -Greg
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @r|@chocho @end|ng |rom gm@||@com  Mon Aug 19 08:36:26 2019
From: @r|@chocho @end|ng |rom gm@||@com (Sri Priya)
Date: Mon, 19 Aug 2019 12:06:26 +0530
Subject: [R] Simulate I x J contingency tables using correlation coefficient
 using bivariate normal distribution
Message-ID: <CANt2G=QkvEPeFEg3fe4_0Atf3fQrfma3vP4NMezCnqm7unMv5g@mail.gmail.com>

Dear R Users,

I am interested in generating contingency tables from bivariate normal
distribution using different correlation coefficient values.

I am experimenting numerous ways of generating contingency tables, and one
possible way is to generate from multinomial distribution.

I wonder how to generate the count variables from a continuous distribution
using correlation structure. Generating bivariate normal variables can be
easily done using mvrnorm() in R. I am struggling to write R code for
generating count from continuous variables.

Any suggestions is very much appreciated.

Thanks.
Sripriya.

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Mon Aug 19 11:22:59 2019
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Mon, 19 Aug 2019 11:22:59 +0200
Subject: [R] 
 Simulate I x J contingency tables using correlation coefficient
 using bivariate normal distribution
In-Reply-To: <CANt2G=QkvEPeFEg3fe4_0Atf3fQrfma3vP4NMezCnqm7unMv5g@mail.gmail.com>
References: <CANt2G=QkvEPeFEg3fe4_0Atf3fQrfma3vP4NMezCnqm7unMv5g@mail.gmail.com>
Message-ID: <CAJuCY5yLtWVAsLwDBoyRQZXTL0f_SEXwRHgQt2qzNm7wgohSZg@mail.gmail.com>

Dear Sripriya,

Step 1. generate random values from a multivariate normal distribution
Step 2. convert the random values into probabilities
Step 3. convert the probabilities into values from the target distribution

library(mvtnorm)
n <- 1e3
correl <- 0.9
lambda <- c(10, 50)
sigma <- matrix(correl, ncol = length(lambda), nrow = length(lambda))
diag(sigma) <- 1
binorm <- rmvnorm(n, sigma = sigma)
bip <- apply(binorm, 2, pnorm)
bipois <- sapply(
  seq_along(lambda),
  function(i) {
    qpois(bip[, i], lambda = lambda[i])
  }
)
plot(bipois)
table(data.frame(bipois))

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op ma 19 aug. 2019 om 10:29 schreef Sri Priya <sri.chocho at gmail.com>:

> Dear R Users,
>
> I am interested in generating contingency tables from bivariate normal
> distribution using different correlation coefficient values.
>
> I am experimenting numerous ways of generating contingency tables, and one
> possible way is to generate from multinomial distribution.
>
> I wonder how to generate the count variables from a continuous distribution
> using correlation structure. Generating bivariate normal variables can be
> easily done using mvrnorm() in R. I am struggling to write R code for
> generating count from continuous variables.
>
> Any suggestions is very much appreciated.
>
> Thanks.
> Sripriya.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From he|mut@@chuetz @end|ng |rom beb@c@@t  Mon Aug 19 12:28:41 2019
From: he|mut@@chuetz @end|ng |rom beb@c@@t (=?UTF-8?Q?Helmut_Sch=c3=bctz?=)
Date: Mon, 19 Aug 2019 12:28:41 +0200
Subject: [R] linear mixed model required for the U.S. FDA
Message-ID: <418f92dc-6a0c-04b5-dee7-0eb38cf4191b@bebac.at>

Dear all,

I?m struggling to set up a model required for the FDA (haha, and the 
Chinese agency). The closest I could get given at the end (which matches 
the one preferred by other regulatory agencies worldwide). The FDA is 
happy with R but "close" is not close /enough/.

Don't hit me. I'm well aware of the community's attitudes towards SAS. 
I'm not a SASian myself (software agnostic) but that's not related to 
SAS; one could set up this model in other (commercial...) software as well.

The FDA?s model allows different subject effects for each treatment 
(i.e., a subject-by-treatment interaction), and therefore, has 5 
variance terms:
 ? 1. within subject for T
 ? 2. within subject for R
 ? 3. between subject for T
 ? 4. between subject for R
 ? 5. covariance for between subject Test and Reference
The last three are combined to give the subject ? formulation 
interaction variance component.

The code provides a lot of significant digits only for comparison.

# FDA 2001 (APPENDIX E)
# https://www.fda.gov/media/70958/download
# FDA 2011 (p. 8)
# 
https://www.accessdata.fda.gov/drugsatfda_docs/psg/Progesterone_caps_19781_RC02-11.pdf
###############################################
# PROC MIXED;???????????????????????????????? #
# CLASSES SEQ SUBJ PER TRT;?????????????????? #
# MODEL Y = SEQ PER TRT/ DDFM = SATTERTH;???? #
# RANDOM TRT/TYPE = FA0(2) SUB = SUBJ G;????? #
# REPEATED/GRP=TRT SUB = SUBJ;??????????????? #
# ESTIMATE 'T vs. R' TRT 1 -1/CL ALPHA = 0.1; #
###############################################
# Example data set (EMA)
# 
https://www.ema.europa.eu/en/documents/other/31-annex-ii-statistical-analysis-bioequivalence-study-example-data-set_en.pdf
library(RCurl)
library(lme4)
library(emmeans)
url? <- getURL("https://bebac.at/downloads/ds01.csv")
data <- read.csv(text = url, colClasses=c(rep("factor", 4), "numeric"))
mod? <- lmer(log(PK) ~ period + sequence + treatment + (1|subject),
 ?????????????????????? data = data)
res1 <- test(pairs(emmeans(mod, ~ treatment, mode = "satterth"),
 ?????????????????? reverse = TRUE), delta = log(0.8))
res2 <- confint(emmeans(mod, pairwise ~ treatment, mode = "satterth"),
 ??????????????? level = 0.9)
# Workaround at the end because of lexical order
#?? I tried relevel(data$treatment, ref = "R") /before/ the model
#?? However, is not observed by confint(...)
cat(paste0("\nEMA Example data set 1",
 ?????????? "\nAnalysis of log-transformed data",
 ?????????? "\nSatterthwaite\u2019s degrees of freedom, 90% CI",
 ?????????? "\n\n? SAS 9.4, Phoenix/WinNonlin 8.1",
 ?????????? "\n?????????????????? mean???????? SE?????? df p.value",
 ?????????? "\n??? R????? :? 7.6704296 0.10396421? 74.762420",
 ?????????? "\n??? T????? :? 7.8158939 0.09860609? 74.926384",
 ?????????? "\n??? T vs. R:? 0.1454643 0.04650124 207.734958 0.00201129",
 ?????????? "\n???????????????????? PE? lower.CL? upper.CL",
 ?????????? "\n??? antilog:? 1.1565764 1.0710440 1.2489393",
 ?????????? "\n\n? lmer (lme 1.1-21), emmeans 1.4",
 ?????????? "\n?????????????????? mean???????? SE?????? df p.value",
 ?????????? "\n??? R????? :? ", sprintf("%.7f %.8f? %3.6f",
 ?????????????????????????????????????? res2$emmeans$emmean[1],
 ?????????????????????????????????????? res2$emmeans$SE[1],
 ?????????????????????????????????????? res2$emmeans$df[1]),
 ?????????? "\n??? T????? :? ", sprintf("%.7f %.8f? %3.6f",
 ?????????????????????????????????????? res2$emmeans$emmean[2],
 ?????????????????????????????????????? res2$emmeans$SE[2],
 ?????????????????????????????????????? res2$emmeans$df[2]),
 ?????????? "\n??? T vs. R:? ", sprintf("%.7f %.8f %3.6f %.8f",
 ?????????????????????????????????????? res1$estimate, res1$SE, res1$df,
 ?????????????????????????????????????? res1$p.value),
 ?????????? "\n???????????????????? PE? lower.CL? upper.CL",
 ?????????? "\n??? antilog:? ", sprintf("%.7f %.7f %.7f",
exp(-res2$contrasts$estimate),
exp(-res2$contrasts$upper.CL),
exp(-res2$contrasts$lower.CL)), "\n"))

Cheers,
Helmut

-- 
Ing. Helmut Sch?tz
BEBAC?? Consultancy Services for
W https://bebac.at/
C https://bebac.at/Contact.htm
F https://forum.bebac.at/


From th|erry@onke||nx @end|ng |rom |nbo@be  Mon Aug 19 13:00:46 2019
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Mon, 19 Aug 2019 13:00:46 +0200
Subject: [R] linear mixed model required for the U.S. FDA
In-Reply-To: <418f92dc-6a0c-04b5-dee7-0eb38cf4191b@bebac.at>
References: <418f92dc-6a0c-04b5-dee7-0eb38cf4191b@bebac.at>
Message-ID: <CAJuCY5y_vQmH74+=LbbKWp4VqYFLriA98fpintZ_PAayDdBpoQ@mail.gmail.com>

Dear Helmut,

The mixed models list is more suitable for this kind of question. I'm
forwarding it to that list. Please send any follow-up to that list instead
of the general R help list.

If I understand correctly, you'll need a different variance term for both
treatments (the within subject for T and R). I don't think you can do that
with lmer(). However, you can with nlme::lme() by using the weights
argument. The model does not converge on my machine.

library(nlme)
model2 <- lme(log(PK) ~ period + sequence + treatment , random = ~
treatment | subject, data = data, weights = varIdent(~treatment))

Another option is to go Bayesian with the INLA package (r-inla.org). Note
that the data needs some preparing. And the summary returns the precision
(1/var).

data$lPK_T <- ifelse(data$treatment == "T", log(data$PK), NA)
data$lPK_R <- ifelse(data$treatment == "R", log(data$PK), NA)
data$subject_T <- as.integer(data$subject)
n_subject <- max(data$subject_T)
data$subject_R <- ifelse(data$treatment == "R", data$subject_T + n_subject,
NA)
data$subject_T[data$treatment == "R"] <- NA

library(INLA)
model3 <- inla(
  cbind(lPK_T, lPK_R) ~ period + sequence + treatment +
    f(subject_T, model = "iid2d", n = 2 * n_subject) +
    f(subject_R, copy = "subject_T"),
  data = data,
  family = c("gaussian", "gaussian")
)
summary(model3)

Fixed effects:
                mean     sd 0.025quant 0.5quant 0.975quant    mode kld
(Intercept)   7.6501 0.1529     7.3492   7.6501     7.9507  7.6501   0
period2       0.0423 0.0729    -0.1011   0.0423     0.1854  0.0423   0
period3       0.0057 0.0613    -0.1148   0.0057     0.1262  0.0057   0
period4       0.0718 0.0731    -0.0718   0.0718     0.2153  0.0718   0
sequenceTRTR -0.0218 0.1960    -0.4076  -0.0218     0.3636 -0.0217   0
treatmentT    0.1462 0.0597     0.0288   0.1462     0.2636  0.1462   0

Random effects:
Name  Model
 subject_T   IID2D model
subject_R   Copy

Model hyperparameters:
                                             mean     sd 0.025quant
0.5quant 0.975quant   mode
Precision for the Gaussian observations    9.4943 1.4716     6.8915
9.3972    12.6699 9.2192
Precision for the Gaussian observations[2] 5.7145 0.8390     4.2257
5.6602     7.5228 5.5594
Precision for subject_T (component 1)      1.4670 0.2541     1.0265
1.4471     2.0243 1.4092
Precision for subject_T (component 2)      1.3545 0.2436     0.9350
1.3345     1.8913 1.2962
Rho1:2 for subject_T                       0.9176 0.0236     0.8631
0.9205     0.9551 0.9261

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op ma 19 aug. 2019 om 12:29 schreef Helmut Sch?tz <helmut.schuetz at bebac.at>:

> Dear all,
>
> I?m struggling to set up a model required for the FDA (haha, and the
> Chinese agency). The closest I could get given at the end (which matches
> the one preferred by other regulatory agencies worldwide). The FDA is
> happy with R but "close" is not close /enough/.
>
> Don't hit me. I'm well aware of the community's attitudes towards SAS.
> I'm not a SASian myself (software agnostic) but that's not related to
> SAS; one could set up this model in other (commercial...) software as well.
>
> The FDA?s model allows different subject effects for each treatment
> (i.e., a subject-by-treatment interaction), and therefore, has 5
> variance terms:
>    1. within subject for T
>    2. within subject for R
>    3. between subject for T
>    4. between subject for R
>    5. covariance for between subject Test and Reference
> The last three are combined to give the subject ? formulation
> interaction variance component.
>
> The code provides a lot of significant digits only for comparison.
>
> # FDA 2001 (APPENDIX E)
> # https://www.fda.gov/media/70958/download
> # FDA 2011 (p. 8)
> #
>
> https://www.accessdata.fda.gov/drugsatfda_docs/psg/Progesterone_caps_19781_RC02-11.pdf
> ###############################################
> # PROC MIXED;                                 #
> # CLASSES SEQ SUBJ PER TRT;                   #
> # MODEL Y = SEQ PER TRT/ DDFM = SATTERTH;     #
> # RANDOM TRT/TYPE = FA0(2) SUB = SUBJ G;      #
> # REPEATED/GRP=TRT SUB = SUBJ;                #
> # ESTIMATE 'T vs. R' TRT 1 -1/CL ALPHA = 0.1; #
> ###############################################
> # Example data set (EMA)
> #
>
> https://www.ema.europa.eu/en/documents/other/31-annex-ii-statistical-analysis-bioequivalence-study-example-data-set_en.pdf
> library(RCurl)
> library(lme4)
> library(emmeans)
> url  <- getURL("https://bebac.at/downloads/ds01.csv")
> data <- read.csv(text = url, colClasses=c(rep("factor", 4), "numeric"))
> mod  <- lmer(log(PK) ~ period + sequence + treatment + (1|subject),
>                         data = data)
> res1 <- test(pairs(emmeans(mod, ~ treatment, mode = "satterth"),
>                     reverse = TRUE), delta = log(0.8))
> res2 <- confint(emmeans(mod, pairwise ~ treatment, mode = "satterth"),
>                  level = 0.9)
> # Workaround at the end because of lexical order
> #   I tried relevel(data$treatment, ref = "R") /before/ the model
> #   However, is not observed by confint(...)
> cat(paste0("\nEMA Example data set 1",
>             "\nAnalysis of log-transformed data",
>             "\nSatterthwaite\u2019s degrees of freedom, 90% CI",
>             "\n\n  SAS 9.4, Phoenix/WinNonlin 8.1",
>             "\n                   mean         SE       df p.value",
>             "\n    R      :  7.6704296 0.10396421  74.762420",
>             "\n    T      :  7.8158939 0.09860609  74.926384",
>             "\n    T vs. R:  0.1454643 0.04650124 207.734958 0.00201129",
>             "\n                     PE  lower.CL  upper.CL",
>             "\n    antilog:  1.1565764 1.0710440 1.2489393",
>             "\n\n  lmer (lme 1.1-21), emmeans 1.4",
>             "\n                   mean         SE       df p.value",
>             "\n    R      :  ", sprintf("%.7f %.8f  %3.6f",
>                                         res2$emmeans$emmean[1],
>                                         res2$emmeans$SE[1],
>                                         res2$emmeans$df[1]),
>             "\n    T      :  ", sprintf("%.7f %.8f  %3.6f",
>                                         res2$emmeans$emmean[2],
>                                         res2$emmeans$SE[2],
>                                         res2$emmeans$df[2]),
>             "\n    T vs. R:  ", sprintf("%.7f %.8f %3.6f %.8f",
>                                         res1$estimate, res1$SE, res1$df,
>                                         res1$p.value),
>             "\n                     PE  lower.CL  upper.CL",
>             "\n    antilog:  ", sprintf("%.7f %.7f %.7f",
> exp(-res2$contrasts$estimate),
> exp(-res2$contrasts$upper.CL),
> exp(-res2$contrasts$lower.CL)), "\n"))
>
> Cheers,
> Helmut
>
> --
> Ing. Helmut Sch?tz
> BEBAC ? Consultancy Services for
> W https://bebac.at/
> C https://bebac.at/Contact.htm
> F https://forum.bebac.at/
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From wdun|@p @end|ng |rom t|bco@com  Mon Aug 19 20:29:36 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Mon, 19 Aug 2019 11:29:36 -0700
Subject: [R] Creating data using multiple for loops
In-Reply-To: <CAGxFJbS4cORBWiSpmiya=EFZQycSqVxt9Xsp+Z1RTd-3FLiwCw@mail.gmail.com>
References: <CAFycp74spy+kQCBO9UdFopzDeEpyojgoq55wRo=12Fohm0f66Q@mail.gmail.com>
 <CAGxFJbS4cORBWiSpmiya=EFZQycSqVxt9Xsp+Z1RTd-3FLiwCw@mail.gmail.com>
Message-ID: <CAF8bMcb5FZN5nw-gd_NG7GRup5JSPiiiRf59o9uDUOzOHC+3rA@mail.gmail.com>

do.call(paste0,expand.grid(0:1000, 1:12, 1:30)) takes care of storing all
the values, but note that paste() doesn't put leading zeroes in front of
small numbers so this maps lots of  ssn/month/day combos to the the same
id.  sprintf() can take care of that:
id <- with(expand.grid(ssn=0:1000, month=1:12, day=1:30),
sprintf("%04d%02d%02d", ssn, month, day))

You probably should define a function to map vectors of ssn, month,  and
day to a vector of ids (it can also check for inappropriate inputs), check
that it works, and use it instead of repeating the sprintf() or paste0()
code.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Sun, Aug 18, 2019 at 12:18 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> id <- do.call(paste0,expand.grid(0:9, 1:3, 1:5))
>
> Comment: If you use R much, you'll do much better using R language
> constructs than trying to apply those from other languages (Java perhaps?).
> I realize this can be difficult, especially if you are experienced in the
> another language (or languages), but it's worth the effort.
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Sun, Aug 18, 2019 at 11:58 AM <g.eastham.gilbert at gmail.com> wrote:
>
> > I would like to create pseudo identification numbers in the format of
> last
> > four of a social security number (0000 to 9999), month of birth (01 to
> 12),
> > and day of birth (01-28). The IDs can be character.
> >
> > I have gotten this far:
> >
> > for (ssn in 0:9){
> >      for (month in 1:3){
> >           for (day in 1:5){
> >                       }
> >                       id <-paste(ssn, month, day, sep="")
> >             }
> > }
> >
> > limiting each value above for demonstration purposes. I cannot figure out
> > how to store the created IDs. I know I have to create a container, but I
> > don't know, among other things, how to index the container.  Any help is
> > appreciated. TIA
> >
> > -Greg
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From cbg@rc|@r @end|ng |rom un@|@edu@co  Mon Aug 19 14:11:11 2019
From: cbg@rc|@r @end|ng |rom un@|@edu@co (Camilo Bernardo Garcia Ramirez)
Date: Mon, 19 Aug 2019 07:11:11 -0500
Subject: [R] Cannot load vegan and vcd, "object vI not found"
Message-ID: <CAMWOOZ8uf0AU9Lb8VDUd0=k46n5Q-9nOpFOX4GwCTwnRdG6fUg@mail.gmail.com>

Hi: Could you help me with the following issue?:

I downloaded the packages vegan and vcd with R3.6.1. When I try to load
them I got this error messages:

 > library(vegan)
Loading required package: permute
Loading required package: lattice
Error: package or namespace load failed for ?vegan? in loadNamespace(j <-
i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 objeto 'vI' no encontrado (translation: object vI not found - my R is in
Spanish)

> library(vcd)
Loading required package: grid
Error: package or namespace load failed for ?vcd? in loadNamespace(j <-
i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 objeto 'vI' no encontrado (translation: object vI not found - my R is in
Spanish)

Sofar no one knows what that means. I am not an R expert! Could you help
me?.
Regards,
Camilo B. Garcia

-- 
Camilo B. Garc?a, Dr. rer. nat
Profesor Asociado, Departamento de Biolog?a
Universidad Nacional de Colombia - Sede Bogot?

	[[alternative HTML version deleted]]


From @r|@chocho @end|ng |rom gm@||@com  Mon Aug 19 18:24:26 2019
From: @r|@chocho @end|ng |rom gm@||@com (Sri Priya)
Date: Mon, 19 Aug 2019 21:54:26 +0530
Subject: [R] 
 Simulate I x J contingency tables using correlation coefficient
 using bivariate normal distribution
In-Reply-To: <CAJuCY5yLtWVAsLwDBoyRQZXTL0f_SEXwRHgQt2qzNm7wgohSZg@mail.gmail.com>
References: <CANt2G=QkvEPeFEg3fe4_0Atf3fQrfma3vP4NMezCnqm7unMv5g@mail.gmail.com>
 <CAJuCY5yLtWVAsLwDBoyRQZXTL0f_SEXwRHgQt2qzNm7wgohSZg@mail.gmail.com>
Message-ID: <CANt2G=SGM7euy8WiSdWBKT32nA8Zu5wkJ4+e+em+bM0zU93wPw@mail.gmail.com>

Dear Thierry,

Thank you very much for your answer.  I got some idea about generating
tables from bivariate normal from your code. But still there will be an
argument to choose the lambda value. So, I am thinking whether to generate
count data using margial CDF? Because one may wish to test the independence
from any contingency table. I am planning to test the same for this kind of
table versus multinomial tables.

Once again thank you for your timely response.

Regards
Sripriya

On Mon, Aug 19, 2019 at 2:53 PM Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Dear Sripriya,
>
> Step 1. generate random values from a multivariate normal distribution
> Step 2. convert the random values into probabilities
> Step 3. convert the probabilities into values from the target distribution
>
> library(mvtnorm)
> n <- 1e3
> correl <- 0.9
> lambda <- c(10, 50)
> sigma <- matrix(correl, ncol = length(lambda), nrow = length(lambda))
> diag(sigma) <- 1
> binorm <- rmvnorm(n, sigma = sigma)
> bip <- apply(binorm, 2, pnorm)
> bipois <- sapply(
>   seq_along(lambda),
>   function(i) {
>     qpois(bip[, i], lambda = lambda[i])
>   }
> )
> plot(bipois)
> table(data.frame(bipois))
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op ma 19 aug. 2019 om 10:29 schreef Sri Priya <sri.chocho at gmail.com>:
>
>> Dear R Users,
>>
>> I am interested in generating contingency tables from bivariate normal
>> distribution using different correlation coefficient values.
>>
>> I am experimenting numerous ways of generating contingency tables, and one
>> possible way is to generate from multinomial distribution.
>>
>> I wonder how to generate the count variables from a continuous
>> distribution
>> using correlation structure. Generating bivariate normal variables can be
>> easily done using mvrnorm() in R. I am struggling to write R code for
>> generating count from continuous variables.
>>
>> Any suggestions is very much appreciated.
>>
>> Thanks.
>> Sripriya.
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Mon Aug 19 23:09:59 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Tue, 20 Aug 2019 07:09:59 +1000
Subject: [R] Creating data using multiple for loops
In-Reply-To: <CAFycp77BYEtdPDhEfVHC-WrtcYGMU_qMgz+fY_69eK8cHnRV0Q@mail.gmail.com>
References: <CAFycp74spy+kQCBO9UdFopzDeEpyojgoq55wRo=12Fohm0f66Q@mail.gmail.com>
 <CA+8X3fVBkdvLHYMSJvA=onYOTr-OhHUXQWiTpoLvo4AjKDgoCw@mail.gmail.com>
 <CAFycp77BYEtdPDhEfVHC-WrtcYGMU_qMgz+fY_69eK8cHnRV0Q@mail.gmail.com>
Message-ID: <CA+8X3fUy+CX+M4h7vwriWtdpF_GYdHv=vrbG-+ytCGP7adq4Kg@mail.gmail.com>

Hi Greg,
I replied because I thought the name of the "expand.grid" function can
be puzzling. While "expand.grid" is a very elegant and useful
function, it is much easier to see what is happening with explicit
loops rather than loops buried deep inside "expand.grid". Also note
Bill's comment about producing repeats by converting numeric values to
character without the leading zeros. You can also use "formatC" to
deal with that problem.

Jim

On Tue, Aug 20, 2019 at 12:05 AM <g.eastham.gilbert at gmail.com> wrote:
>
> Jim,
>
> Thank you very much for your help. I have "unpacked" the code and have a rudimentary understanding of what you did. Thanks again. However, I have no idea to what Bert is referring. Could you help me understand his suggestion? Thanks.
>
> -Greg


From bgunter@4567 @end|ng |rom gm@||@com  Tue Aug 20 00:01:01 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 19 Aug 2019 15:01:01 -0700
Subject: [R] Creating data using multiple for loops
In-Reply-To: <CA+8X3fUy+CX+M4h7vwriWtdpF_GYdHv=vrbG-+ytCGP7adq4Kg@mail.gmail.com>
References: <CAFycp74spy+kQCBO9UdFopzDeEpyojgoq55wRo=12Fohm0f66Q@mail.gmail.com>
 <CA+8X3fVBkdvLHYMSJvA=onYOTr-OhHUXQWiTpoLvo4AjKDgoCw@mail.gmail.com>
 <CAFycp77BYEtdPDhEfVHC-WrtcYGMU_qMgz+fY_69eK8cHnRV0Q@mail.gmail.com>
 <CA+8X3fUy+CX+M4h7vwriWtdpF_GYdHv=vrbG-+ytCGP7adq4Kg@mail.gmail.com>
Message-ID: <CAGxFJbRNLfiiNnbJg25nnk8W5-7ivUOZLvwLfmVK4fVwA-z8zg@mail.gmail.com>

From section 9.2.2 (on looping) in "An Introduction to R":

"*Warning*: for() loops are used in R code much less often than in compiled
languages. Code that takes a ?whole object? view is likely to be both
clearer and faster in R."

Web searching on "for loops in R" and similar will give you further
comments and perspectives.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Aug 19, 2019 at 2:12 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Greg,
> I replied because I thought the name of the "expand.grid" function can
> be puzzling. While "expand.grid" is a very elegant and useful
> function, it is much easier to see what is happening with explicit
> loops rather than loops buried deep inside "expand.grid". Also note
> Bill's comment about producing repeats by converting numeric values to
> character without the leading zeros. You can also use "formatC" to
> deal with that problem.
>
> Jim
>
> On Tue, Aug 20, 2019 at 12:05 AM <g.eastham.gilbert at gmail.com> wrote:
> >
> > Jim,
> >
> > Thank you very much for your help. I have "unpacked" the code and have a
> rudimentary understanding of what you did. Thanks again. However, I have no
> idea to what Bert is referring. Could you help me understand his
> suggestion? Thanks.
> >
> > -Greg
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Aug 20 00:02:36 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 19 Aug 2019 15:02:36 -0700
Subject: [R] Creating data using multiple for loops
In-Reply-To: <CA+8X3fUy+CX+M4h7vwriWtdpF_GYdHv=vrbG-+ytCGP7adq4Kg@mail.gmail.com>
References: <CAFycp74spy+kQCBO9UdFopzDeEpyojgoq55wRo=12Fohm0f66Q@mail.gmail.com>
 <CA+8X3fVBkdvLHYMSJvA=onYOTr-OhHUXQWiTpoLvo4AjKDgoCw@mail.gmail.com>
 <CAFycp77BYEtdPDhEfVHC-WrtcYGMU_qMgz+fY_69eK8cHnRV0Q@mail.gmail.com>
 <CA+8X3fUy+CX+M4h7vwriWtdpF_GYdHv=vrbG-+ytCGP7adq4Kg@mail.gmail.com>
Message-ID: <7B9BDEA7-6147-4872-8C67-29191847419E@dcn.davis.ca.us>

Perhaps different people find different concepts the most challenging, but I find looking at the output of expand.grid quite informative... do try it out.

The do.call function seems to be the more obscure function here, but Bert's code

id <- do.call( paste0, expand.grid(0:9,1:3,1:5) )

is equivalent to

all_comb <- expand.grid( 0:9, 1:3, 1:5 )
all_comb # look at it for learning, remove once you understand
paste0( all_comb[[1]], all_comb[[2]], all_comb[[3]] )

because all_comb is a data frame, which is a list of column vectors all the same length. The do.call function expects the first argument to be a function symbol, while the second argument to do.call should be a single object that is a list of arguments you want that function to be given as separate arguments. The paste0 function puts the three vectors together into one character vector, element by element.

Read the help pages for each function:
?expand.grid
?paste0
?do.call

On the other hand, nested for loops seem to become spaghetti quickly in my mind... essentially just write-only code because I never want to look at it again.

On August 19, 2019 2:09:59 PM PDT, Jim Lemon <drjimlemon at gmail.com> wrote:
>Hi Greg,
>I replied because I thought the name of the "expand.grid" function can
>be puzzling. While "expand.grid" is a very elegant and useful
>function, it is much easier to see what is happening with explicit
>loops rather than loops buried deep inside "expand.grid". Also note
>Bill's comment about producing repeats by converting numeric values to
>character without the leading zeros. You can also use "formatC" to
>deal with that problem.
>
>Jim
>
>On Tue, Aug 20, 2019 at 12:05 AM <g.eastham.gilbert at gmail.com> wrote:
>>
>> Jim,
>>
>> Thank you very much for your help. I have "unpacked" the code and
>have a rudimentary understanding of what you did. Thanks again.
>However, I have no idea to what Bert is referring. Could you help me
>understand his suggestion? Thanks.
>>
>> -Greg
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @purd|e@@ @end|ng |rom gm@||@com  Tue Aug 20 00:21:12 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Tue, 20 Aug 2019 10:21:12 +1200
Subject: [R] 
 Simulate I x J contingency tables using correlation coefficient
 using bivariate normal distribution
In-Reply-To: <CAJuCY5yLtWVAsLwDBoyRQZXTL0f_SEXwRHgQt2qzNm7wgohSZg@mail.gmail.com>
References: <CANt2G=QkvEPeFEg3fe4_0Atf3fQrfma3vP4NMezCnqm7unMv5g@mail.gmail.com>
 <CAJuCY5yLtWVAsLwDBoyRQZXTL0f_SEXwRHgQt2qzNm7wgohSZg@mail.gmail.com>
Message-ID: <CAB8pepz3cBSjKrmYu+AQpveTd1_Dd+WEFt3De-jZqpMXim6G9A@mail.gmail.com>

> Step 1. generate random values from a multivariate normal distribution
> Step 2. convert the random values into probabilities
> Step 3. convert the probabilities into values from the target distribution

This requires a cautionary note.
The method above is simple in the univariate case (where people would
usually start with a uniform distribution), however, generalizing it
to the multivariate case, is not as simple as Thierry as suggested.

That doesn't necessarily mean that his solution is wrong.
(Because there's more than one way of defining a bivariate Poisson
distribution).


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Aug 20 02:38:19 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 19 Aug 2019 17:38:19 -0700
Subject: [R] Cannot load vegan and vcd, "object vI not found"
In-Reply-To: <CAMWOOZ8uf0AU9Lb8VDUd0=k46n5Q-9nOpFOX4GwCTwnRdG6fUg@mail.gmail.com>
References: <CAMWOOZ8uf0AU9Lb8VDUd0=k46n5Q-9nOpFOX4GwCTwnRdG6fUg@mail.gmail.com>
Message-ID: <B78A1F9C-E590-4F19-A540-2587BE6C8AEB@dcn.davis.ca.us>

Try closing all instances of R except for one and then run the update.packages() function. If that doesn't work then post the output of sessionInfo().

On August 19, 2019 5:11:11 AM PDT, Camilo Bernardo Garcia Ramirez <cbgarciar at unal.edu.co> wrote:
>Hi: Could you help me with the following issue?:
>
>I downloaded the packages vegan and vcd with R3.6.1. When I try to load
>them I got this error messages:
>
> > library(vegan)
>Loading required package: permute
>Loading required package: lattice
>Error: package or namespace load failed for ?vegan? in loadNamespace(j
><-
>i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
>objeto 'vI' no encontrado (translation: object vI not found - my R is
>in
>Spanish)
>
>> library(vcd)
>Loading required package: grid
>Error: package or namespace load failed for ?vcd? in loadNamespace(j <-
>i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
>objeto 'vI' no encontrado (translation: object vI not found - my R is
>in
>Spanish)
>
>Sofar no one knows what that means. I am not an R expert! Could you
>help
>me?.
>Regards,
>Camilo B. Garcia

-- 
Sent from my phone. Please excuse my brevity.


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Tue Aug 20 12:31:23 2019
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Tue, 20 Aug 2019 12:31:23 +0200
Subject: [R] Cannot load vegan and vcd, "object vI not found"
In-Reply-To: <B78A1F9C-E590-4F19-A540-2587BE6C8AEB@dcn.davis.ca.us>
References: <CAMWOOZ8uf0AU9Lb8VDUd0=k46n5Q-9nOpFOX4GwCTwnRdG6fUg@mail.gmail.com>
 <B78A1F9C-E590-4F19-A540-2587BE6C8AEB@dcn.davis.ca.us>
Message-ID: <23899.52219.150924.749641@stat.math.ethz.ch>

>>>>> Jeff Newmiller 
>>>>>     on Mon, 19 Aug 2019 17:38:19 -0700 writes:

    > Try closing all instances of R except for one and then run the update.packages() function. If that doesn't work then post the output of sessionInfo().

I also strongly suspect that some of the packages concerned were
not correctly installed.

    > On August 19, 2019 5:11:11 AM PDT, Camilo Bernardo Garcia Ramirez <cbgarciar at unal.edu.co> wrote:
    >> Hi: Could you help me with the following issue?:
    >> 
    >> I downloaded the packages vegan and vcd with R3.6.1. When I try to load
    >> them I got this error messages:
    >> 
    >> > library(vegan)
    >> Loading required package: permute
    >> Loading required package: lattice
    >> Error: package or namespace load failed for ?vegan? in loadNamespace(j
    >> <-
    >> i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
    >> objeto 'vI' no encontrado (translation: object vI not found - my R is
    >> in
    >> Spanish)

The error message above comes from base R's function
loadNamespace()  [which calls itself recursively, when importing
from other packages].

I think the error message is not really useful, and indeed I think
should *not* happen at this time:  'vI' is not defined here,
because earlier the 'pkgInfo' result was not really valid.

So I think you may have touched on a "buglet" in R in the sense
that for such semi-validly / invalidly installed packages, you
should get a better error message.

Martin Maechler
ETH Zurich  and  R Core team.


    >>> library(vcd)
    >> Loading required package: grid
    >> Error: package or namespace load failed for ?vcd? in loadNamespace(j <-
    >> i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
    >> objeto 'vI' no encontrado (translation: object vI not found - my R is
    >> in
    >> Spanish)
    >> 
    >> Sofar no one knows what that means. I am not an R expert! Could you
    >> help
    >> me?.
    >> Regards,
    >> Camilo B. Garcia

    > -- 
    > Sent from my phone. Please excuse my brevity.

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


From m|ch@e|@z@mo @end|ng |rom meteo@|r  Tue Aug 20 15:07:47 2019
From: m|ch@e|@z@mo @end|ng |rom meteo@|r (ZAMO =?utf-8?Q?Micha=C3=ABl?=)
Date: Tue, 20 Aug 2019 15:07:47 +0200 (CEST)
Subject: [R] How to only include some covariates in a vglm model depending
 on the linear predictor?
Message-ID: <1710088132.19405943.1566306467156.JavaMail.zimbra@meteo.fr>



Dear all, 





I am modelling a positive random variable with VGAM R-package with a truncated normal distribution. I want to include some explanatory variables to model the mean and other explanatory variables to model the standard deviation. 




I try to do this with the "constraints" argument of vglm function. 




Here is an example with the "iris" dataset (it is not the actual data I will use). I want to model the Sepal.Length's mean with the Sepal.Width, and the Sepal.Length's standard deviation with the Petal.Length and the Petal.Width. 

Here is my code, with the list of constraints as I understood it should be written. 
library ( VGAM ) iris = iris
constraints = list ( "(Intercept)" = diag ( 2 ), Sepal.Width = rbind ( 1 , 0 ), Petal.Length = rbind ( 0 , 1 ), Petal.Width = rbind ( 0 , 1 )) formula = Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width
out = vglm ( formula , posnormal ( zero = NULL ), data = iris , constraints = constraints ) 


Here are my constraint list: 
$ `(Intercept)` [, 1 ] [, 2 ] [ 1 ,] 1 0 [ 2 ,] 0 1 $ Sepal.Width [, 1 ] [ 1 ,] 1 [ 2 ,] 0 $ Petal.Length [, 1 ] [ 1 ,] 0 [ 2 ,] 1 $ Petal.Width [, 1 ] [ 1 ,] 0 [ 2 ,] 1 


I get the following error message from function process.constraints: 
Error in process.constraints ( constraints , x = x , M = M , specialCM = specialCM , : constraint matrix has too many columns " 


This function takes as argument a list of constraint matrices, which appears to be : 
$ `(Intercept)` [, 1 ] [, 2 ] [, 3 ] [, 4 ] [ 1 ,] 1 1 0 0 [ 2 ,] 0 0 1 1 $ Sepal.Width [, 1 ] [, 2 ] [ 1 ,] 1 1 [ 2 ,] 0 0 $ Petal.Length [, 1 ] [, 2 ] [ 1 ,] 0 0 [ 2 ,] 1 1 $ Petal.Width [, 1 ] [, 2 ] [ 1 ,] 0 0 [ 2 ,] 1 1 


This is not what I entered, because there has been some postprocessing of my initial constraint list, which adds some columns in the constraint matrix before getting to the process.constraints function. 

I don't get if this is a bug or if I did not initially write the right constraints for what I want to do. Or maybe should I proceed differently? 

I already posted this question on StackOverflow, but don't get any answer. 

Thanks for you help, 


Micha?l Zamo 
Ing?nieur d'?tudes et de d?veloppement 

M?t?o-France 
DirOP/COMPAS/DOP 
bureau B315 
42 avenue Coriolis 
31057 Toulouse cedex 05 
France 

T?l. : +33 5 61 07 86 36 
Courriel : michael.zamo at meteo.fr 


	[[alternative HTML version deleted]]


From @oph|e@b|eke01 @end|ng |rom gm@||@com  Tue Aug 20 14:23:40 2019
From: @oph|e@b|eke01 @end|ng |rom gm@||@com (Sophie Bleke)
Date: Tue, 20 Aug 2019 14:23:40 +0200
Subject: [R] ski.mack test in R
Message-ID: <CALffwBOY4ZghV9ToB6U5C80KhHkg1LzJAxPoJoQNWSWEs6AF_g@mail.gmail.com>

Dear R-Help List,

For part of my data, which I analyse with Friedman.test, there is data
missing, which is a normal result of the experimental setup. If I am
correct, I need to use the Ski.Mack test in R for it.

Normally, I read my data in for Friedman or post hoc Wilcoxon like below:

a <- read.table(file.choose(), header=T)

attach(a)
a<-as.matrix(a)

friedman.test(a)

I tried to replace the friedman line with your ski.mack command, but it is
not getting me where I need to get.

Ski.Mack(a, simulate.p.value = TRUE, B = 1000)

or

Ski.Mack(a)

Both don't work.

Is there as simple command way, as above, to read out the ski.mack test
from my txt file, without manually typing all the data points as rows and
colums in the R console, which is recommended in the ski.mack author's
online PDF? It's a lot of data and manually inputting rows and colums and
data points won't be doable in this case.

I would be very grateful to hear of an elegant short solution, similar to
the friedman command / read-in lines above.

Sophie

	[[alternative HTML version deleted]]


From @@r@h@go@|ee @end|ng |rom gm@||@com  Tue Aug 20 19:18:21 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Tue, 20 Aug 2019 13:18:21 -0400
Subject: [R] ski.mack test in R
In-Reply-To: <CALffwBOY4ZghV9ToB6U5C80KhHkg1LzJAxPoJoQNWSWEs6AF_g@mail.gmail.com>
References: <CALffwBOY4ZghV9ToB6U5C80KhHkg1LzJAxPoJoQNWSWEs6AF_g@mail.gmail.com>
Message-ID: <CAM_vjumgm+z=BU=25utvPE4wgPUp3qK3mQvheEgZT0BcOzFY+Q@mail.gmail.com>

Hi Sophie,

We don't have enough information to help you.

You DON'T need to type in the data for your test - that's how the
package authors are creating a reproducible example for you.

But you do need to have your data in the correct format, and we have
no idea what your a looks like.


a <- read.table(file.choose(), header=T) # good
# attach(a) # don't do this
a<-as.matrix(a)

library(Skillings.Mack) # you forgot to tell us where you got the
function you're using

a needs to be "Either a numeric vector of data values, or a data
matrix. If a matrix is used, columns and rows are correspondent to
blocks and treatments (groups), respectively."

Since your a is a matrix, then it should be the latter. Is that what you have?

You say that

Ski.Mack(a)

"doesn't work." That isn't enough for us to help. What error message
do you get? We need specifics about what "doesn't work" means.

We also need a reproducible example, possibly with toy data like the
package authors used
[.https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example]
The output of

str(a)

would also be very helpful.

Sarah

On Tue, Aug 20, 2019 at 12:02 PM Sophie Bleke <sophie.bleke01 at gmail.com> wrote:
>
> Dear R-Help List,
>
> For part of my data, which I analyse with Friedman.test, there is data
> missing, which is a normal result of the experimental setup. If I am
> correct, I need to use the Ski.Mack test in R for it.
>
> Normally, I read my data in for Friedman or post hoc Wilcoxon like below:
>
> a <- read.table(file.choose(), header=T)
>
> attach(a)
> a<-as.matrix(a)
>
> friedman.test(a)
>
> I tried to replace the friedman line with your ski.mack command, but it is
> not getting me where I need to get.
>
> Ski.Mack(a, simulate.p.value = TRUE, B = 1000)
>
> or
>
> Ski.Mack(a)
>
> Both don't work.
>
> Is there as simple command way, as above, to read out the ski.mack test
> from my txt file, without manually typing all the data points as rows and
> colums in the R console, which is recommended in the ski.mack author's
> online PDF? It's a lot of data and manually inputting rows and colums and
> data points won't be doable in this case.
>
> I would be very grateful to hear of an elegant short solution, similar to
> the friedman command / read-in lines above.
>
> Sophie
>


-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From cbg@rc|@r @end|ng |rom un@|@edu@co  Wed Aug 21 00:03:22 2019
From: cbg@rc|@r @end|ng |rom un@|@edu@co (Camilo Bernardo Garcia Ramirez)
Date: Tue, 20 Aug 2019 17:03:22 -0500
Subject: [R] Cannot load vegan and vcd, "object vI not found"
In-Reply-To: <23899.52219.150924.749641@stat.math.ethz.ch>
References: <CAMWOOZ8uf0AU9Lb8VDUd0=k46n5Q-9nOpFOX4GwCTwnRdG6fUg@mail.gmail.com>
 <B78A1F9C-E590-4F19-A540-2587BE6C8AEB@dcn.davis.ca.us>
 <23899.52219.150924.749641@stat.math.ethz.ch>
Message-ID: <CAMWOOZ_JDsNokR0jjez4KRVBt+v5GGvoQZ0tQyX3ALtH-9_t9Q@mail.gmail.com>

Hi again: First at all, thanks for your answer. My problem is still there.
Here the session. I did remove and (re)install vegan and then called for
update but no effect in the error. I included the sessionInfo() at the end.
Camilo

> remove.packages("vegan")
Removing package from ?C:/Users/Camilo/Documents/R/win-library/3.6?
(as ?lib? is unspecified)
> install.packages("vegan", dependencies=TRUE)
Installing package into ?C:/Users/Camilo/Documents/R/win-library/3.6?
(as ?lib? is unspecified)
--- Please select a CRAN mirror for use in this session ---
probando la URL '
https://cran.wu.ac.at/bin/windows/contrib/3.6/vegan_2.5-5.zip'
Content type 'application/zip' length 3862370 bytes (3.7 MB)
downloaded 3.7 MB

package ?vegan? successfully unpacked and MD5 sums checked

The downloaded binary packages are in
        C:\Users\Camilo\AppData\Local\Temp\Rtmpu0yOZa\downloaded_packages
> update.packages("vegan")
> library(vegan)
Loading required package: permute
Loading required package: lattice
Error: package or namespace load failed for ?vegan? in loadNamespace(j <-
i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 objeto 'vI' no encontrado

> sessionInfo()
R version 3.6.1 (2019-07-05)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 17134)

Matrix products: default

locale:
[1] LC_COLLATE=Spanish_Spain.1252  LC_CTYPE=Spanish_Spain.1252
[3] LC_MONETARY=Spanish_Spain.1252 LC_NUMERIC=C
[5] LC_TIME=Spanish_Spain.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] lattice_0.20-38 permute_0.9-5

loaded via a namespace (and not attached):
[1] compiler_3.6.1 parallel_3.6.1 tools_3.6.1    grid_3.6.1
>

El mar., 20 ago. 2019 a las 5:31, Martin Maechler (<
maechler at stat.math.ethz.ch>) escribi?:

> >>>>> Jeff Newmiller
> >>>>>     on Mon, 19 Aug 2019 17:38:19 -0700 writes:
>
>     > Try closing all instances of R except for one and then run the
> update.packages() function. If that doesn't work then post the output of
> sessionInfo().
>
> I also strongly suspect that some of the packages concerned were
> not correctly installed.
>
>     > On August 19, 2019 5:11:11 AM PDT, Camilo Bernardo Garcia Ramirez <
> cbgarciar at unal.edu.co> wrote:
>     >> Hi: Could you help me with the following issue?:
>     >>
>     >> I downloaded the packages vegan and vcd with R3.6.1. When I try to
> load
>     >> them I got this error messages:
>     >>
>     >> > library(vegan)
>     >> Loading required package: permute
>     >> Loading required package: lattice
>     >> Error: package or namespace load failed for ?vegan? in
> loadNamespace(j
>     >> <-
>     >> i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
>     >> objeto 'vI' no encontrado (translation: object vI not found - my R
> is
>     >> in
>     >> Spanish)
>
> The error message above comes from base R's function
> loadNamespace()  [which calls itself recursively, when importing
> from other packages].
>
> I think the error message is not really useful, and indeed I think
> should *not* happen at this time:  'vI' is not defined here,
> because earlier the 'pkgInfo' result was not really valid.
>
> So I think you may have touched on a "buglet" in R in the sense
> that for such semi-validly / invalidly installed packages, you
> should get a better error message.
>
> Martin Maechler
> ETH Zurich  and  R Core team.
>
>
>     >>> library(vcd)
>     >> Loading required package: grid
>     >> Error: package or namespace load failed for ?vcd? in
> loadNamespace(j <-
>     >> i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
>     >> objeto 'vI' no encontrado (translation: object vI not found - my R
> is
>     >> in
>     >> Spanish)
>     >>
>     >> Sofar no one knows what that means. I am not an R expert! Could you
>     >> help
>     >> me?.
>     >> Regards,
>     >> Camilo B. Garcia
>
>     > --
>     > Sent from my phone. Please excuse my brevity.
>
>     > ______________________________________________
>     > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>     > https://stat.ethz.ch/mailman/listinfo/r-help
>     > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
>     > and provide commented, minimal, self-contained, reproducible code.
>


-- 
Camilo B. Garc?a, Dr. rer. nat
Profesor Asociado, Departamento de Biolog?a
Universidad Nacional de Colombia - Sede Bogot?

	[[alternative HTML version deleted]]


From tobby @end|ng |rom htu@@t  Wed Aug 21 12:00:59 2019
From: tobby @end|ng |rom htu@@t (Tobias Fellinger)
Date: Wed, 21 Aug 2019 12:00:59 +0200
Subject: [R] subsetting/slicing xml2 nodesets
Message-ID: <4fabeec558924980b2d95fd17ea96a14@htu.at>

Dear R-help members,

I'm working with the xml2 package to parse an xml document, and I don't 
understand how subsetting / slicing of xml_nodesets works. I'd expect 
xml_find_all to only return children of the nodes I selected with [ or 
[[ but it returns all nodes found in the whole document. I did not find 
any documentation on the [ and [[ operators for xml_nodeset. Below is a 
small example and the sessionInfo.

thanks in advance, Tobias Fellinger



# load package
require(xml2)

# test document as text
test_chr <- "
<html>
<body>
<p>paragraph 1</p>
<p>paragraph 2</p>
</body>
</html>
"

# parse test document
test_doc <- read_xml(test_chr)

# extract nodeset
test_nodeset <- xml_find_all(test_doc, "//p")

# subset nodeset (working as expected)
test_nodeset[1]
# {xml_nodeset (1)}
# [1] <p>paragraph 1</p>
test_nodeset[[1]]
# {xml_node}
# <p>

# extract from subset (not working as expected)
xml_find_all(test_nodeset[1], "//p")
# {xml_nodeset (2)}
# [1] <p>paragraph 1</p>
# [2] <p>paragraph 2</p>
xml_find_all(test_nodeset[[1]], "//p")
# {xml_nodeset (2)}
# [1] <p>paragraph 1</p>
# [2] <p>paragraph 2</p>

sessionInfo()
# R version 3.6.0 (2019-04-26)
# Platform: x86_64-w64-mingw32/x64 (64-bit)
# Running under: Windows 7 x64 (build 7601) Service Pack 1
#
# Matrix products: default
#
# locale:
#   [1] LC_COLLATE=German_Austria.1252  LC_CTYPE=German_Austria.1252    
LC_MONETARY=German_Austria.1252 LC_NUMERIC=C                    
LC_TIME=German_Austria.1252
#
# attached base packages:
#   [1] stats     graphics  grDevices utils     datasets  methods   
base
#
# other attached packages:
#   [1] xml2_1.2.2
#
# loaded via a namespace (and not attached):
#   [1] compiler_3.6.0 tools_3.6.0    Rcpp_1.0.2     packrat_0.5.0


From @@r@h@go@|ee @end|ng |rom gm@||@com  Wed Aug 21 19:26:35 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Wed, 21 Aug 2019 13:26:35 -0400
Subject: [R] Cannot load vegan and vcd, "object vI not found"
In-Reply-To: <CAMWOOZ_JDsNokR0jjez4KRVBt+v5GGvoQZ0tQyX3ALtH-9_t9Q@mail.gmail.com>
References: <CAMWOOZ8uf0AU9Lb8VDUd0=k46n5Q-9nOpFOX4GwCTwnRdG6fUg@mail.gmail.com>
 <B78A1F9C-E590-4F19-A540-2587BE6C8AEB@dcn.davis.ca.us>
 <23899.52219.150924.749641@stat.math.ethz.ch>
 <CAMWOOZ_JDsNokR0jjez4KRVBt+v5GGvoQZ0tQyX3ALtH-9_t9Q@mail.gmail.com>
Message-ID: <CAM_vjumknAnw6vP+H-UNOcHx8V9drpx1mxqF7u6mn+76DzjRGQ@mail.gmail.com>

Hi,

You misunderstood the suggestion, which was to run update.packages()
to update all packages, not just vegan (which doesn't need updated
because you just reinstalled it).
To make your own life easier, I'd run it as:

update.packages(ask=FALSE, checkBuilt = TRUE)

You could also try reinstalling the packages that vegan requires,
which seems to be where the error actually is. (lattice, perhaps, but
I'd reinstall

Sarah

On Tue, Aug 20, 2019 at 6:27 PM Camilo Bernardo Garcia Ramirez
<cbgarciar at unal.edu.co> wrote:
>
> Hi again: First at all, thanks for your answer. My problem is still there.
> Here the session. I did remove and (re)install vegan and then called for
> update but no effect in the error. I included the sessionInfo() at the end.
> Camilo
>
> > remove.packages("vegan")
> Removing package from ?C:/Users/Camilo/Documents/R/win-library/3.6?
> (as ?lib? is unspecified)
> > install.packages("vegan", dependencies=TRUE)
> Installing package into ?C:/Users/Camilo/Documents/R/win-library/3.6?
> (as ?lib? is unspecified)
> --- Please select a CRAN mirror for use in this session ---
> probando la URL '
> https://cran.wu.ac.at/bin/windows/contrib/3.6/vegan_2.5-5.zip'
> Content type 'application/zip' length 3862370 bytes (3.7 MB)
> downloaded 3.7 MB
>
> package ?vegan? successfully unpacked and MD5 sums checked
>
> The downloaded binary packages are in
>         C:\Users\Camilo\AppData\Local\Temp\Rtmpu0yOZa\downloaded_packages
> > update.packages("vegan")
> > library(vegan)
> Loading required package: permute
> Loading required package: lattice
> Error: package or namespace load failed for ?vegan? in loadNamespace(j <-
> i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
>  objeto 'vI' no encontrado
>
> > sessionInfo()
> R version 3.6.1 (2019-07-05)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 10 x64 (build 17134)
>
> Matrix products: default
>
> locale:
> [1] LC_COLLATE=Spanish_Spain.1252  LC_CTYPE=Spanish_Spain.1252
> [3] LC_MONETARY=Spanish_Spain.1252 LC_NUMERIC=C
> [5] LC_TIME=Spanish_Spain.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] lattice_0.20-38 permute_0.9-5
>
> loaded via a namespace (and not attached):
> [1] compiler_3.6.1 parallel_3.6.1 tools_3.6.1    grid_3.6.1
> >
>
> El mar., 20 ago. 2019 a las 5:31, Martin Maechler (<
> maechler at stat.math.ethz.ch>) escribi?:
>
> > >>>>> Jeff Newmiller
> > >>>>>     on Mon, 19 Aug 2019 17:38:19 -0700 writes:
> >
> >     > Try closing all instances of R except for one and then run the
> > update.packages() function. If that doesn't work then post the output of
> > sessionInfo().
> >
> > I also strongly suspect that some of the packages concerned were
> > not correctly installed.
> >
> >     > On August 19, 2019 5:11:11 AM PDT, Camilo Bernardo Garcia Ramirez <
> > cbgarciar at unal.edu.co> wrote:
> >     >> Hi: Could you help me with the following issue?:
> >     >>
> >     >> I downloaded the packages vegan and vcd with R3.6.1. When I try to
> > load
> >     >> them I got this error messages:
> >     >>
> >     >> > library(vegan)
> >     >> Loading required package: permute
> >     >> Loading required package: lattice
> >     >> Error: package or namespace load failed for ?vegan? in
> > loadNamespace(j
> >     >> <-
> >     >> i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
> >     >> objeto 'vI' no encontrado (translation: object vI not found - my R
> > is
> >     >> in
> >     >> Spanish)
> >
> > The error message above comes from base R's function
> > loadNamespace()  [which calls itself recursively, when importing
> > from other packages].
> >
> > I think the error message is not really useful, and indeed I think
> > should *not* happen at this time:  'vI' is not defined here,
> > because earlier the 'pkgInfo' result was not really valid.
> >
> > So I think you may have touched on a "buglet" in R in the sense
> > that for such semi-validly / invalidly installed packages, you
> > should get a better error message.
> >
> > Martin Maechler
> > ETH Zurich  and  R Core team.
> >
> >
> >     >>> library(vcd)
> >     >> Loading required package: grid
> >     >> Error: package or namespace load failed for ?vcd? in
> > loadNamespace(j <-
> >     >> i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
> >     >> objeto 'vI' no encontrado (translation: object vI not found - my R
> > is
> >     >> in
> >     >> Spanish)
> >     >>
> >     >> Sofar no one knows what that means. I am not an R expert! Could you
> >     >> help
> >     >> me?.
> >     >> Regards,
> >     >> Camilo B. Garcia
> >
> >     > --


From den|@e@je@ne@b @end|ng |rom gm@||@com  Thu Aug 22 01:15:11 2019
From: den|@e@je@ne@b @end|ng |rom gm@||@com (Denise b)
Date: Wed, 21 Aug 2019 19:15:11 -0400
Subject: [R] Score test for a subset of parameters estimated with coxph
In-Reply-To: <CAJfThnRV7Ryq7Uu4q6ZWsO6nDM27Hgqvu6eUqukd=rqUCRJMBQ@mail.gmail.com>
References: <mailman.355798.3521.1565906984.8808.r-help@r-project.org>
 <CAJfThnRV7Ryq7Uu4q6ZWsO6nDM27Hgqvu6eUqukd=rqUCRJMBQ@mail.gmail.com>
Message-ID: <CAJfThnSAX_umW+CfhObexwKciQrxmPeu6PeoTyaahdCFjwXRcg@mail.gmail.com>

An update regarding my question to compute the score test for a subset of
parameters with coxph,
when the option frailty is used:

After further explorations, based on my previous examples, I realized that
in presence of frailty term,
coxph returns only the results from the likelihood ratio test, whereas when
there is no frailty term in the model specified, it returns the likelihood
ratio, wald and score test as illustrated in the following example.

It could be due to the penalized-likelihood approach used by coxph for
frailty model in coxph.
Is there any other potential explanation?

# R code
library(survival)

fit1<-coxph(Surv(time,status)~sex+ ph.karno + pat.karno , data=lung)
summary(fit1)
Call:
coxph(formula = Surv(time, status) ~ sex + ph.karno + pat.karno,
    data = lung)

  n= 224, number of events= 161
   (4 observations deleted due to missingness)

               coef exp(coef)  se(coef)      z Pr(>|z|)
sex       -0.511878  0.599369  0.169275 -3.024  0.00249 **
ph.karno  -0.006155  0.993864  0.006822 -0.902  0.36695
pat.karno -0.017020  0.983124  0.006535 -2.604  0.00920 **
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

          exp(coef) exp(-coef) lower .95 upper .95
sex          0.5994      1.668    0.4301    0.8352
ph.karno     0.9939      1.006    0.9807    1.0072
pat.karno    0.9831      1.017    0.9706    0.9958

Concordance= 0.646  (se = 0.026 )
Rsquare= 0.097   (max possible= 0.999 )
Likelihood ratio test= 22.91  on 3 df,   p=4.208e-05
Wald test            = 22.64  on 3 df,   p=4.789e-05
Score (logrank) test = 23.2  on 3 df,   p=3.673e-05

frailtyfit1<-coxph(Surv(time,status)~sex+ ph.karno + pat.karno
+frailty(inst), data=lung)
summary(frailtyfit1)
Call:
coxph(formula = Surv(time, status) ~ sex + ph.karno + pat.karno +
    frailty(inst), data = lung)

  n= 223, number of events= 160
   (5 observations deleted due to missingness)

              coef     se(coef) se2      Chisq DF   p
sex           -0.50984 0.169534 0.169498 9.04  1.00 0.0026
ph.karno      -0.00620 0.006850 0.006842 0.82  1.00 0.3700
pat.karno     -0.01703 0.006544 0.006541 6.77  1.00 0.0093
frailty(inst)                            0.25  0.21 0.3600

          exp(coef) exp(-coef) lower .95 upper .95
sex          0.6006      1.665    0.4308    0.8373
ph.karno     0.9938      1.006    0.9806    1.0073
pat.karno    0.9831      1.017    0.9706    0.9958

Iterations: 5 outer, 21 Newton-Raphson
     Variance of random effect= 0.001494078   I-likelihood = -711.9
Degrees of freedom for terms= 1.0 1.0 1.0 0.2
Concordance= 0.649  (se = 0.026 )
Likelihood ratio test= 23.22  on 3.21 df,   p=4.703e-05

Le jeu. 15 ao?t 2019 ? 18:11, Denise b <denise.jeane.b at gmail.com> a ?crit :

> Dear R help list,
>
> I would like to perform a score test for a subset of the parameters
> estimated with coxph using the frailty() option.
> As illustrated in the following reproducible example, I am able to perform
> the score test with the standard coxph() but not in presence of frailty()
> argument. See Examples 1 and 2
>
> library(survival)
>
> # Example 1: score test using standard coxph()
> # score test for ph.karno & pat.karno coefficients
> fit1<-coxph(Surv(time,status)~sex+ ph.karno + pat.karno , data=lung)
> fit0<-coxph(Surv(time,status)~pat.karno , data=lung) # fit under null
> scorestat <- coxph(Surv(time,status)~sex+ ph.karno + pat.karno,
> init=c(0,0,coefficients(fit0)) ,iter=0, data=lung)
> scorestat$score
> [1] 10.50409
>
> # Example 2: coxph with frailty term
> # score test for ph.karno & pat.karno coefficients
> frailtyfit1<-coxph(Surv(time,status)~sex+ ph.karno + pat.karno
> +frailty(inst), data=lung)
> frailtyfit0<-coxph(Surv(time,status)~sex+ frailty(inst), data=lung)
> scorefrailtyfit1<-coxph(Surv(time,status)~sex+ ph.karno + pat.karno
> +frailty(inst), data=lung,
> init=c(0,0,coefficients(frailtyfit0)) ,iter=0)
>
> I get the following error message:
> Error in coxph(Surv(time, status) ~ sex + ph.karno + pat.karno +
> frailty(inst),  :
>   wrong length for init argument
>
>
> When I checked in the code I found
> if (length(init) != ncol(X))
>             stop("wrong length for init argument")
> Which seems that the length of the init argument doesnt match the number of
> predictors in the model.
>
> It seems that the code is expecting that I specify another parameter for
> frailty term.
> So, I tried to specify 2 as an inital value for frailty but got another
> error message later:
>
> scorefrailtyfit1<-coxph(Surv(time,status)~sex+ ph.karno + pat.karno
> +frailty(inst), data=lung,
> + init=c(0,0,coefficients(frailtyfit0),4.5),iter=0)
> Error in coxpenal.fit(X, Y, strats, offset, init = init, control, weights =
> weights,  :
>   Wrong length for inital values
>
> I have also checked coxme() but it seems that there is no option
> implemented
> for the score test.
> Do you have an idea of what could be the problem?
>
> Thanks,
> Denise
>
>
>

	[[alternative HTML version deleted]]


From tom|err|93 @end|ng |rom gm@||@com  Thu Aug 22 14:55:43 2019
From: tom|err|93 @end|ng |rom gm@||@com (Tommaso Ferrari)
Date: Thu, 22 Aug 2019 14:55:43 +0200
Subject: [R] DCC model estimation with t-Student distribution rmgarch
Message-ID: <CAKPkyQSNMK7YtDeZkd8xj2W2E5mwWB2pCrQq7+W0tAoRn05JzA@mail.gmail.com>

Dear all,
I was analyzing and implementing the DCC model (Dynamic Conditional
Correlation) for the one-day forecast calculation of the
variance-covariance matrix of a system consisting of, approximately,
30 stocks. For each title I consider a historical series of logarithmic
daily returns of 250 samples.
In particular, I was interested in the simulation of this model using the
t-Student distribution.
In this regard I was using the "rugarch" and "rmgarch" packages.
According to the examples found in the literature, I run my analysis in the
following way:

1) specification of the univariate garch model for each stock passing the
number of degrees of freedom of the t-Student distribution as input
(parameter mshape)

2) multifitting of the univariate garch models

3) specification of DCC model with a multivariate t-Student distribution
(degrees of freedom are not passed as input, in this case)

4) fitting of the DCC model

5) variance-covariance matrix forecasting

I give an example of the code I'm running (qxts is the time series data,
mshape is the number of degrees of freedom):

# GARCH(1,1) specification
garch11.spec = ugarchspec(mean.model = list(armaOrder = c(0, 0)),
variance.model = list(garchOrder = c(1, 1), model = "sGARCH"),
distribution.model = "std", fixed.pars = list(shape = mshape))
# replicate Garch(1,1) spec for both time series
uspec = multispec(replicate(ncol(qxts), garch11.spec))
# Fit Garch models: hybrid -> in case of non convergence, all solvers are
used
multf = multifit(uspec, qxts, solver = "hybrid", fit.control = list(scale =
1))
# Dcc model spec
dcc.garch11.spec = dccspec(uspec = uspec, dccOrder = c(1, 1), distribution
= "mvt", model = "DCC")
# Fitting parameters of DCC
dcc.fit = dccfit(dcc.garch11.spec, qxts, fit = multf, fit.control =
list(scale = TRUE))
# dcc.fit = dccfit(dcc.garch11.spec, qxts, fit = multf, fit.control =
list(scale = 1))
# Forecast
dcc.fcst = dccforecast(dcc.fit, n.ahead = 1)
varmat = rcov(dcc.fcst)[[dt_cov]][,, 1]

However, using a number of degrees of freedom of 2.5, I get the following
error:

Error in solve.default(A) : system is computationally singular: reciprocal
condition number = 1.19994e-18

If, for example, I change the degrees of freedom from 2.5 to 2.6, the error
no longer appears.
I would like to know if there is a way to allow the calculation to be
performed even using a number of degrees of freedom equals to 2.5, as I
cannot find any reference in the literature that addresses this problem.

I also tried the following idea.
I don't pass the number of degrees of freedom as input, but I make sure
that, for each title, the number of degrees of freedom is calculated
internally by the function multifit.
Obviously, I get different degrees of freedom depending on the stock
considered.
In this case, however, passing the calculated parameters of the multifit to
the dccfunction, leads to the following error:
'data' must be of a vector type, was 'NULL'
even if the data sample I pass as input contains no null value.

I would like to know if these problems are due solely to the numerical
values of the data passed as input or if I am conceptually wrong in the
implementation of the method.

Thanks to all

	[[alternative HTML version deleted]]


From gor@n@bro@trom @end|ng |rom umu@@e  Thu Aug 22 21:48:51 2019
From: gor@n@bro@trom @end|ng |rom umu@@e (=?UTF-8?Q?G=c3=b6ran_Brostr=c3=b6m?=)
Date: Thu, 22 Aug 2019 21:48:51 +0200
Subject: [R] 
 results of a survival analysis change when converting the data
 to counting process format
In-Reply-To: <1846389355.20190818191012@medstat.hu>
References: <1846389355.20190818191012@medstat.hu>
Message-ID: <103d4f04-e156-5078-1229-6ccae1ea70dc@umu.se>



On 2019-08-18 19:10, Ferenci Tamas wrote:
> Dear All,
> 
> Consider the following simple example:
> 
> library( survival )
> data( veteran )
> 
> coef( coxph(Surv(time, status) ~ trt + prior + karno, data = veteran) )
>           trt        prior        karno
>   0.180197194 -0.005550919 -0.033771018
> 
> Note that we have neither time-dependent covariates, nor time-varying
> coefficients, so the results should be the same if we change to
> counting process format, no matter where we cut the times.
> 
> That's true if we cut at event times:
> 
> veteran2 <- survSplit( Surv(time, status) ~ trt + prior + karno,
>                         data = veteran, cut = unique( veteran$time ) )
> 
> coef( coxph(Surv(tstart,time, status) ~ trt + prior + karno, data = veteran2 ) )
>           trt        prior        karno
>   0.180197194 -0.005550919 -0.033771018
> 
> But quite interestingly not true, if we cut at every day:
> 
> veteran3 <- survSplit( Surv(time, status) ~ trt + prior + karno,
>                         data = veteran, cut = 1:max(veteran$time) )
> 
> coef( coxph(Surv(tstart,time, status) ~ trt + prior + karno, data = veteran3 ) )
>           trt        prior        karno
>   0.180197215 -0.005550913 -0.033771016
> 
> The difference is not large, but definitely more than just a rounding
> error, or something like that.
> 
> What's going on? How can the results get wrong, especially by
> including more cutpoints?

All results are wrong, but they are useful (paraphrasing George EP Box).

G?ran

> 
> Thank you in advance,
> Tamas
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri Aug 23 04:06:31 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 22 Aug 2019 19:06:31 -0700
Subject: [R] DCC model estimation with t-Student distribution rmgarch
In-Reply-To: <CAKPkyQSNMK7YtDeZkd8xj2W2E5mwWB2pCrQq7+W0tAoRn05JzA@mail.gmail.com>
References: <CAKPkyQSNMK7YtDeZkd8xj2W2E5mwWB2pCrQq7+W0tAoRn05JzA@mail.gmail.com>
Message-ID: <600AFEBA-6B02-4E0D-B726-8003F347B06C@dcn.davis.ca.us>

This seems squarely in the "asking for statistical expertise" area, rather than the "how do I make R implement this theory I understand" area, so it is off topic on this mailing list. You might be in better company on Stack Exchange.

On August 22, 2019 5:55:43 AM PDT, Tommaso Ferrari <tomferri93 at gmail.com> wrote:
>Dear all,
>I was analyzing and implementing the DCC model (Dynamic Conditional
>Correlation) for the one-day forecast calculation of the
>variance-covariance matrix of a system consisting of, approximately,
>30 stocks. For each title I consider a historical series of logarithmic
>daily returns of 250 samples.
>In particular, I was interested in the simulation of this model using
>the
>t-Student distribution.
>In this regard I was using the "rugarch" and "rmgarch" packages.
>According to the examples found in the literature, I run my analysis in
>the
>following way:
>
>1) specification of the univariate garch model for each stock passing
>the
>number of degrees of freedom of the t-Student distribution as input
>(parameter mshape)
>
>2) multifitting of the univariate garch models
>
>3) specification of DCC model with a multivariate t-Student
>distribution
>(degrees of freedom are not passed as input, in this case)
>
>4) fitting of the DCC model
>
>5) variance-covariance matrix forecasting
>
>I give an example of the code I'm running (qxts is the time series
>data,
>mshape is the number of degrees of freedom):
>
># GARCH(1,1) specification
>garch11.spec = ugarchspec(mean.model = list(armaOrder = c(0, 0)),
>variance.model = list(garchOrder = c(1, 1), model = "sGARCH"),
>distribution.model = "std", fixed.pars = list(shape = mshape))
># replicate Garch(1,1) spec for both time series
>uspec = multispec(replicate(ncol(qxts), garch11.spec))
># Fit Garch models: hybrid -> in case of non convergence, all solvers
>are
>used
>multf = multifit(uspec, qxts, solver = "hybrid", fit.control =
>list(scale =
>1))
># Dcc model spec
>dcc.garch11.spec = dccspec(uspec = uspec, dccOrder = c(1, 1),
>distribution
>= "mvt", model = "DCC")
># Fitting parameters of DCC
>dcc.fit = dccfit(dcc.garch11.spec, qxts, fit = multf, fit.control =
>list(scale = TRUE))
># dcc.fit = dccfit(dcc.garch11.spec, qxts, fit = multf, fit.control =
>list(scale = 1))
># Forecast
>dcc.fcst = dccforecast(dcc.fit, n.ahead = 1)
>varmat = rcov(dcc.fcst)[[dt_cov]][,, 1]
>
>However, using a number of degrees of freedom of 2.5, I get the
>following
>error:
>
>Error in solve.default(A) : system is computationally singular:
>reciprocal
>condition number = 1.19994e-18
>
>If, for example, I change the degrees of freedom from 2.5 to 2.6, the
>error
>no longer appears.
>I would like to know if there is a way to allow the calculation to be
>performed even using a number of degrees of freedom equals to 2.5, as I
>cannot find any reference in the literature that addresses this
>problem.
>
>I also tried the following idea.
>I don't pass the number of degrees of freedom as input, but I make sure
>that, for each title, the number of degrees of freedom is calculated
>internally by the function multifit.
>Obviously, I get different degrees of freedom depending on the stock
>considered.
>In this case, however, passing the calculated parameters of the
>multifit to
>the dccfunction, leads to the following error:
>'data' must be of a vector type, was 'NULL'
>even if the data sample I pass as input contains no null value.
>
>I would like to know if these problems are due solely to the numerical
>values of the data passed as input or if I am conceptually wrong in the
>implementation of the method.
>
>Thanks to all
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From cbg@rc|@r @end|ng |rom un@|@edu@co  Thu Aug 22 22:58:37 2019
From: cbg@rc|@r @end|ng |rom un@|@edu@co (Camilo Bernardo Garcia Ramirez)
Date: Thu, 22 Aug 2019 15:58:37 -0500
Subject: [R] Cannot load vegan and vcd, "object vI not found"
In-Reply-To: <CAM_vjumknAnw6vP+H-UNOcHx8V9drpx1mxqF7u6mn+76DzjRGQ@mail.gmail.com>
References: <CAMWOOZ8uf0AU9Lb8VDUd0=k46n5Q-9nOpFOX4GwCTwnRdG6fUg@mail.gmail.com>
 <B78A1F9C-E590-4F19-A540-2587BE6C8AEB@dcn.davis.ca.us>
 <23899.52219.150924.749641@stat.math.ethz.ch>
 <CAMWOOZ_JDsNokR0jjez4KRVBt+v5GGvoQZ0tQyX3ALtH-9_t9Q@mail.gmail.com>
 <CAM_vjumknAnw6vP+H-UNOcHx8V9drpx1mxqF7u6mn+76DzjRGQ@mail.gmail.com>
Message-ID: <CAMWOOZ_pa72WOSx6i+9Vgn5EMWaz9tMU-u4VnJB0Kijx0o+7Yg@mail.gmail.com>

Dear All: Problem solved. Package MASS was not properly installed. Thanks a
lot.
Camilo B. Garcia

El mi?., 21 ago. 2019 a las 12:26, Sarah Goslee (<sarah.goslee at gmail.com>)
escribi?:

> Hi,
>
> You misunderstood the suggestion, which was to run update.packages()
> to update all packages, not just vegan (which doesn't need updated
> because you just reinstalled it).
> To make your own life easier, I'd run it as:
>
> update.packages(ask=FALSE, checkBuilt = TRUE)
>
> You could also try reinstalling the packages that vegan requires,
> which seems to be where the error actually is. (lattice, perhaps, but
> I'd reinstall
>
> Sarah
>
> On Tue, Aug 20, 2019 at 6:27 PM Camilo Bernardo Garcia Ramirez
> <cbgarciar at unal.edu.co> wrote:
> >
> > Hi again: First at all, thanks for your answer. My problem is still
> there.
> > Here the session. I did remove and (re)install vegan and then called for
> > update but no effect in the error. I included the sessionInfo() at the
> end.
> > Camilo
> >
> > > remove.packages("vegan")
> > Removing package from ?C:/Users/Camilo/Documents/R/win-library/3.6?
> > (as ?lib? is unspecified)
> > > install.packages("vegan", dependencies=TRUE)
> > Installing package into ?C:/Users/Camilo/Documents/R/win-library/3.6?
> > (as ?lib? is unspecified)
> > --- Please select a CRAN mirror for use in this session ---
> > probando la URL '
> > https://cran.wu.ac.at/bin/windows/contrib/3.6/vegan_2.5-5.zip'
> > Content type 'application/zip' length 3862370 bytes (3.7 MB)
> > downloaded 3.7 MB
> >
> > package ?vegan? successfully unpacked and MD5 sums checked
> >
> > The downloaded binary packages are in
> >         C:\Users\Camilo\AppData\Local\Temp\Rtmpu0yOZa\downloaded_packages
> > > update.packages("vegan")
> > > library(vegan)
> > Loading required package: permute
> > Loading required package: lattice
> > Error: package or namespace load failed for ?vegan? in loadNamespace(j <-
> > i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
> >  objeto 'vI' no encontrado
> >
> > > sessionInfo()
> > R version 3.6.1 (2019-07-05)
> > Platform: x86_64-w64-mingw32/x64 (64-bit)
> > Running under: Windows 10 x64 (build 17134)
> >
> > Matrix products: default
> >
> > locale:
> > [1] LC_COLLATE=Spanish_Spain.1252  LC_CTYPE=Spanish_Spain.1252
> > [3] LC_MONETARY=Spanish_Spain.1252 LC_NUMERIC=C
> > [5] LC_TIME=Spanish_Spain.1252
> >
> > attached base packages:
> > [1] stats     graphics  grDevices utils     datasets  methods   base
> >
> > other attached packages:
> > [1] lattice_0.20-38 permute_0.9-5
> >
> > loaded via a namespace (and not attached):
> > [1] compiler_3.6.1 parallel_3.6.1 tools_3.6.1    grid_3.6.1
> > >
> >
> > El mar., 20 ago. 2019 a las 5:31, Martin Maechler (<
> > maechler at stat.math.ethz.ch>) escribi?:
> >
> > > >>>>> Jeff Newmiller
> > > >>>>>     on Mon, 19 Aug 2019 17:38:19 -0700 writes:
> > >
> > >     > Try closing all instances of R except for one and then run the
> > > update.packages() function. If that doesn't work then post the output
> of
> > > sessionInfo().
> > >
> > > I also strongly suspect that some of the packages concerned were
> > > not correctly installed.
> > >
> > >     > On August 19, 2019 5:11:11 AM PDT, Camilo Bernardo Garcia
> Ramirez <
> > > cbgarciar at unal.edu.co> wrote:
> > >     >> Hi: Could you help me with the following issue?:
> > >     >>
> > >     >> I downloaded the packages vegan and vcd with R3.6.1. When I try
> to
> > > load
> > >     >> them I got this error messages:
> > >     >>
> > >     >> > library(vegan)
> > >     >> Loading required package: permute
> > >     >> Loading required package: lattice
> > >     >> Error: package or namespace load failed for ?vegan? in
> > > loadNamespace(j
> > >     >> <-
> > >     >> i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
> > >     >> objeto 'vI' no encontrado (translation: object vI not found -
> my R
> > > is
> > >     >> in
> > >     >> Spanish)
> > >
> > > The error message above comes from base R's function
> > > loadNamespace()  [which calls itself recursively, when importing
> > > from other packages].
> > >
> > > I think the error message is not really useful, and indeed I think
> > > should *not* happen at this time:  'vI' is not defined here,
> > > because earlier the 'pkgInfo' result was not really valid.
> > >
> > > So I think you may have touched on a "buglet" in R in the sense
> > > that for such semi-validly / invalidly installed packages, you
> > > should get a better error message.
> > >
> > > Martin Maechler
> > > ETH Zurich  and  R Core team.
> > >
> > >
> > >     >>> library(vcd)
> > >     >> Loading required package: grid
> > >     >> Error: package or namespace load failed for ?vcd? in
> > > loadNamespace(j <-
> > >     >> i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
> > >     >> objeto 'vI' no encontrado (translation: object vI not found -
> my R
> > > is
> > >     >> in
> > >     >> Spanish)
> > >     >>
> > >     >> Sofar no one knows what that means. I am not an R expert! Could
> you
> > >     >> help
> > >     >> me?.
> > >     >> Regards,
> > >     >> Camilo B. Garcia
> > >
> > >     > --
>


-- 
Camilo B. Garc?a, Dr. rer. nat
Profesor Asociado, Departamento de Biolog?a
Universidad Nacional de Colombia - Sede Bogot?

	[[alternative HTML version deleted]]


From gor@n@bro@trom @end|ng |rom umu@@e  Fri Aug 23 11:12:36 2019
From: gor@n@bro@trom @end|ng |rom umu@@e (=?UTF-8?Q?G=c3=b6ran_Brostr=c3=b6m?=)
Date: Fri, 23 Aug 2019 11:12:36 +0200
Subject: [R] 
 results of a survival analysis change when converting the data
 to counting process format
In-Reply-To: <103d4f04-e156-5078-1229-6ccae1ea70dc@umu.se>
References: <1846389355.20190818191012@medstat.hu>
 <103d4f04-e156-5078-1229-6ccae1ea70dc@umu.se>
Message-ID: <7071d643-952c-e20d-5730-ab11fe1593d3@umu.se>



Den 2019-08-22 kl. 21:48, skrev G?ran Brostr?m:
> 
> 
> On 2019-08-18 19:10, Ferenci Tamas wrote:
>> Dear All,
>>
>> Consider the following simple example:
>>
>> library( survival )
>> data( veteran )
>>
>> coef( coxph(Surv(time, status) ~ trt + prior + karno, data = veteran) )
>> ????????? trt??????? prior??????? karno
>> ? 0.180197194 -0.005550919 -0.033771018
>>
>> Note that we have neither time-dependent covariates, nor time-varying
>> coefficients, so the results should be the same if we change to
>> counting process format, no matter where we cut the times.
>>
>> That's true if we cut at event times:
>>
>> veteran2 <- survSplit( Surv(time, status) ~ trt + prior + karno,
>> ??????????????????????? data = veteran, cut = unique( veteran$time ) )
>>
>> coef( coxph(Surv(tstart,time, status) ~ trt + prior + karno, data = 
>> veteran2 ) )
>> ????????? trt??????? prior??????? karno
>> ? 0.180197194 -0.005550919 -0.033771018
>>
>> But quite interestingly not true, if we cut at every day:
>>
>> veteran3 <- survSplit( Surv(time, status) ~ trt + prior + karno,
>> ??????????????????????? data = veteran, cut = 1:max(veteran$time) )
>>
>> coef( coxph(Surv(tstart,time, status) ~ trt + prior + karno, data = 
>> veteran3 ) )
>> ????????? trt??????? prior??????? karno
>> ? 0.180197215 -0.005550913 -0.033771016
>>
>> The difference is not large, but definitely more than just a rounding
>> error, or something like that.
>>
>> What's going on? How can the results get wrong, especially by
>> including more cutpoints?
> 
> All results are wrong, but they are useful (paraphrasing George EP Box).

That said, it is a little surprising: The generated risk sets are 
(should be) identical in all cases, and one would expect rounding errors 
to be the same. But data get stored differently, and ... who knows?

I tried your examples on my computer and got exactly the same results as 
you. Which surprised me.

G,

> 
> G?ran
> 
>>
>> Thank you in advance,
>> Tamas
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pd@|gd @end|ng |rom gm@||@com  Fri Aug 23 13:38:19 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Fri, 23 Aug 2019 13:38:19 +0200
Subject: [R] 
 results of a survival analysis change when converting the data
 to counting process format
In-Reply-To: <7071d643-952c-e20d-5730-ab11fe1593d3@umu.se>
References: <1846389355.20190818191012@medstat.hu>
 <103d4f04-e156-5078-1229-6ccae1ea70dc@umu.se>
 <7071d643-952c-e20d-5730-ab11fe1593d3@umu.se>
Message-ID: <C1C1D9B7-6A19-4DE1-8CB2-B0BFB62DD47E@gmail.com>

I think this is a case of (a - x) + x != a in floating arithmetic. When updating the risk set, you subtract sums of covariates at the end of a time interval, then add them back at the beginning of the next interval. Something like that, anyway. As in

> x <- rnorm(1000)
> sum(c(x,-x))
[1] -1.387779e-17

Or (worse, since sum() is smart and doesn't operate sequentially)

> tt <- 0
> for (i in x) tt <- tt+i
> for (i in x) tt <- tt-i
> tt
[1] 2.553513e-14

-pd

> On 23 Aug 2019, at 11:12 , G?ran Brostr?m <goran.brostrom at umu.se> wrote:
> 
> 
> 
> Den 2019-08-22 kl. 21:48, skrev G?ran Brostr?m:
>> On 2019-08-18 19:10, Ferenci Tamas wrote:
>>> Dear All,
>>> 
>>> Consider the following simple example:
>>> 
>>> library( survival )
>>> data( veteran )
>>> 
>>> coef( coxph(Surv(time, status) ~ trt + prior + karno, data = veteran) )
>>>           trt        prior        karno
>>>   0.180197194 -0.005550919 -0.033771018
>>> 
>>> Note that we have neither time-dependent covariates, nor time-varying
>>> coefficients, so the results should be the same if we change to
>>> counting process format, no matter where we cut the times.
>>> 
>>> That's true if we cut at event times:
>>> 
>>> veteran2 <- survSplit( Surv(time, status) ~ trt + prior + karno,
>>>                         data = veteran, cut = unique( veteran$time ) )
>>> 
>>> coef( coxph(Surv(tstart,time, status) ~ trt + prior + karno, data = veteran2 ) )
>>>           trt        prior        karno
>>>   0.180197194 -0.005550919 -0.033771018
>>> 
>>> But quite interestingly not true, if we cut at every day:
>>> 
>>> veteran3 <- survSplit( Surv(time, status) ~ trt + prior + karno,
>>>                         data = veteran, cut = 1:max(veteran$time) )
>>> 
>>> coef( coxph(Surv(tstart,time, status) ~ trt + prior + karno, data = veteran3 ) )
>>>           trt        prior        karno
>>>   0.180197215 -0.005550913 -0.033771016
>>> 
>>> The difference is not large, but definitely more than just a rounding
>>> error, or something like that.
>>> 
>>> What's going on? How can the results get wrong, especially by
>>> including more cutpoints?
>> All results are wrong, but they are useful (paraphrasing George EP Box).
> 
> That said, it is a little surprising: The generated risk sets are (should be) identical in all cases, and one would expect rounding errors to be the same. But data get stored differently, and ... who knows?
> 
> I tried your examples on my computer and got exactly the same results as you. Which surprised me.
> 
> G,
> 
>> G?ran
>>> 
>>> Thank you in advance,
>>> Tamas
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From chr|@@@ @end|ng |rom med@um|ch@edu  Fri Aug 23 15:18:19 2019
From: chr|@@@ @end|ng |rom med@um|ch@edu (Andrews, Chris)
Date: Fri, 23 Aug 2019 13:18:19 +0000
Subject: [R] 
 results of a survival analysis change when converting the data
 to counting process format
In-Reply-To: <7071d643-952c-e20d-5730-ab11fe1593d3@umu.se>
References: <1846389355.20190818191012@medstat.hu>
 <103d4f04-e156-5078-1229-6ccae1ea70dc@umu.se>
 <7071d643-952c-e20d-5730-ab11fe1593d3@umu.se>
Message-ID: <53ef808ad93547449cbb110f432c9338@med.umich.edu>


# For what it is worth, even the second fit (cuts at observation times) does not give identical coefficient estimates as using the original data structure.

answer <- coef( coxph(Surv(time, status) ~ trt + prior + karno, data = veteran) )
veteran2 <- survSplit( Surv(time, status) ~ trt + prior + karno, data = veteran, cut = unique( veteran$time ) )
answer2 <- coef( coxph(Surv(tstart,time, status) ~ trt + prior + karno, data = veteran2 ) )
answer2 - answer
 #          trt         prior         karno 
 # 2.775558e-16 -1.127570e-17 -6.938894e-18

# If you cut daily, but not all the way to 999, you get a few different fits

ff <- function(m) {
	veteran3 <- survSplit( Surv(time, status) ~ trt + prior + karno, data = veteran, cut = seq(m))
	answer3 <- coef( coxph(Surv(tstart,time, status) ~ trt + prior + karno, data = veteran3 ) )
	return(answer3)
}

answers3 <- sapply(100:999, ff)
plot(100:999, answers3[1,] - answer[1])

# But certainly all these differences are of no practical importance.



-----Original Message-----
From: G?ran Brostr?m [mailto:goran.brostrom at umu.se] 
Sent: Friday, August 23, 2019 5:13 AM
To: r-help at r-project.org; tamas.ferenci at medstat.hu
Subject: Re: [R] results of a survival analysis change when converting the data to counting process format



Den 2019-08-22 kl. 21:48, skrev G?ran Brostr?m:
> 
> 
> On 2019-08-18 19:10, Ferenci Tamas wrote:
>> Dear All,
>>
>> Consider the following simple example:
>>
>> library( survival )
>> data( veteran )
>>
>> coef( coxph(Surv(time, status) ~ trt + prior + karno, data = veteran) )
>> ????????? trt??????? prior??????? karno
>> ? 0.180197194 -0.005550919 -0.033771018
>>
>> Note that we have neither time-dependent covariates, nor time-varying
>> coefficients, so the results should be the same if we change to
>> counting process format, no matter where we cut the times.
>>
>> That's true if we cut at event times:
>>
>> veteran2 <- survSplit( Surv(time, status) ~ trt + prior + karno,
>> ??????????????????????? data = veteran, cut = unique( veteran$time ) )
>>
>> coef( coxph(Surv(tstart,time, status) ~ trt + prior + karno, data = 
>> veteran2 ) )
>> ????????? trt??????? prior??????? karno
>> ? 0.180197194 -0.005550919 -0.033771018
>>
>> But quite interestingly not true, if we cut at every day:
>>
>> veteran3 <- survSplit( Surv(time, status) ~ trt + prior + karno,
>> ??????????????????????? data = veteran, cut = 1:max(veteran$time) )
>>
>> coef( coxph(Surv(tstart,time, status) ~ trt + prior + karno, data = 
>> veteran3 ) )
>> ????????? trt??????? prior??????? karno
>> ? 0.180197215 -0.005550913 -0.033771016
>>
>> The difference is not large, but definitely more than just a rounding
>> error, or something like that.
>>
>> What's going on? How can the results get wrong, especially by
>> including more cutpoints?
> 
> All results are wrong, but they are useful (paraphrasing George EP Box).

That said, it is a little surprising: The generated risk sets are 
(should be) identical in all cases, and one would expect rounding errors 
to be the same. But data get stored differently, and ... who knows?

I tried your examples on my computer and got exactly the same results as 
you. Which surprised me.

G,

> 
> G?ran
> 
>>
>> Thank you in advance,
>> Tamas
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues 

From @@r@h@go@|ee @end|ng |rom gm@||@com  Fri Aug 23 15:46:02 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Fri, 23 Aug 2019 09:46:02 -0400
Subject: [R] ski.mack test in R
In-Reply-To: <CALffwBOMh81RKH+skBVAiteJ4Vv9t82zdSV87vHPoy9upFzbZg@mail.gmail.com>
References: <CALffwBOY4ZghV9ToB6U5C80KhHkg1LzJAxPoJoQNWSWEs6AF_g@mail.gmail.com>
 <CAM_vjumgm+z=BU=25utvPE4wgPUp3qK3mQvheEgZT0BcOzFY+Q@mail.gmail.com>
 <CALffwBOMh81RKH+skBVAiteJ4Vv9t82zdSV87vHPoy9upFzbZg@mail.gmail.com>
Message-ID: <CAM_vju=mk+df-b569YmciQ+rk_sW6=CqgLJnajcHe9kp0LZPpA@mail.gmail.com>

Hi Sophie,

It's good form to reply to the list, not directly to me - I'm not able
to help people individually. It's also good form to put the necessary
info directly into your email, rather than expecting people to open
attachments (which the mailing list might not even allow, depending on
format).

However, your problem is very simple:

You need to load a package before using functions from it.

Ski.Mack() is not found because you first need to do

library(Skillings.Mack)

to make it available to your R session.

(Assuming you've installed the package; if not

install.packages("Skillings.Mack")

will do that for you.

I strongly, strongly suggest that you read one of the good
introductions to R out there, possibly even the one that comes with R,
and is also available here:

https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf

That you've had this problem shows that you don't understand the very
basics of how to use R, so you should familiarize yourself with it
first.

Best,
Sarah

On Fri, Aug 23, 2019 at 6:24 AM Sophie Bleke <sophie.bleke01 at gmail.com> wrote:
>
> Dear Sarah,
>
> Thank you for you kind and quick response.
>
> I attach the PDF in which I found the ski.mack instructions. This file is available online.
> I also attach some toy data, on which I am trying to perform the ski.mack test.
> Please also find attached a txt. file in which you can find the output of str(a) and error message of ski.mack(a) etc.
>
> I would be very grateful to hear back from you or the list.
>
> With all my best,
> Sophie
>
> Am Di., 20. Aug. 2019 um 19:18 Uhr schrieb Sarah Goslee <sarah.goslee at gmail.com>:
>>
>> Hi Sophie,
>>
>> We don't have enough information to help you.
>>
>> You DON'T need to type in the data for your test - that's how the
>> package authors are creating a reproducible example for you.
>>
>> But you do need to have your data in the correct format, and we have
>> no idea what your a looks like.
>>
>>
>> a <- read.table(file.choose(), header=T) # good
>> # attach(a) # don't do this
>> a<-as.matrix(a)
>>
>> library(Skillings.Mack) # you forgot to tell us where you got the
>> function you're using
>>
>> a needs to be "Either a numeric vector of data values, or a data
>> matrix. If a matrix is used, columns and rows are correspondent to
>> blocks and treatments (groups), respectively."
>>
>> Since your a is a matrix, then it should be the latter. Is that what you have?
>>
>> You say that
>>
>> Ski.Mack(a)
>>
>> "doesn't work." That isn't enough for us to help. What error message
>> do you get? We need specifics about what "doesn't work" means.
>>
>> We also need a reproducible example, possibly with toy data like the
>> package authors used
>> [.https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example]
>> The output of
>>
>> str(a)
>>
>> would also be very helpful.
>>
>> Sarah
>>
>> On Tue, Aug 20, 2019 at 12:02 PM Sophie Bleke <sophie.bleke01 at gmail.com> wrote:
>> >
>> > Dear R-Help List,
>> >
>> > For part of my data, which I analyse with Friedman.test, there is data
>> > missing, which is a normal result of the experimental setup. If I am
>> > correct, I need to use the Ski.Mack test in R for it.
>> >
>> > Normally, I read my data in for Friedman or post hoc Wilcoxon like below:
>> >
>> > a <- read.table(file.choose(), header=T)
>> >
>> > attach(a)
>> > a<-as.matrix(a)
>> >
>> > friedman.test(a)
>> >
>> > I tried to replace the friedman line with your ski.mack command, but it is
>> > not getting me where I need to get.
>> >
>> > Ski.Mack(a, simulate.p.value = TRUE, B = 1000)
>> >
>> > or
>> >
>> > Ski.Mack(a)
>> >
>> > Both don't work.
>> >
>> > Is there as simple command way, as above, to read out the ski.mack test
>> > from my txt file, without manually typing all the data points as rows and
>> > colums in the R console, which is recommended in the ski.mack author's
>> > online PDF? It's a lot of data and manually inputting rows and colums and
>> > data points won't be doable in this case.
>> >
>> > I would be very grateful to hear of an elegant short solution, similar to
>> > the friedman command / read-in lines above.
>> >
>> > Sophie
>> >
>>
>>
>> --
>> Sarah Goslee (she/her)
>> http://www.numberwright.com


From chr|@@@ @end|ng |rom med@um|ch@edu  Fri Aug 23 15:50:05 2019
From: chr|@@@ @end|ng |rom med@um|ch@edu (Andrews, Chris)
Date: Fri, 23 Aug 2019 13:50:05 +0000
Subject: [R] 
 results of a survival analysis change when converting the data
 to counting process format
References: <1846389355.20190818191012@medstat.hu>
 <103d4f04-e156-5078-1229-6ccae1ea70dc@umu.se>
 <7071d643-952c-e20d-5730-ab11fe1593d3@umu.se> 
Message-ID: <1c7f3f65f6814cf6b4ff844daaa3a8bd@med.umich.edu>


On the other hand, SAS gets the same answer all three ways.  And the answer SAS gets is closest to the one that R gets when using the daily cutting.

Obs _TIES_ _TYPE_ _STATUS_ _NAME_ trt prior karno _LNLIKE_ 
1 EFRON PARMS 0 Converged time 0.1801972156 -0.005550913 -0.033771016 -483.9277463 
2 EFRON PARMS 0 Converged time 0.1801972156 -0.005550913 -0.033771016 -483.9277463 
3 EFRON PARMS 0 Converged time 0.1801972156 -0.005550913 -0.033771016 -483.9277463 

proc phreg data=veteran1 outest = vet1;
	model time * status(0) = trt prior karno / ties = efron;
proc phreg data=veteran2 outest = vet2;
	model time * status(0) = trt prior karno / ties = efron entry=tstart;
proc phreg data=veteran3 outest = vet3;
	model time * status(0) = trt prior karno / ties = efron entry=tstart;

data vets; set vet1 vet2 vet3;

proc print data=vets width=FULL;

run;


-----Original Message-----
From: Andrews, Chris 
Sent: Friday, August 23, 2019 9:18 AM
To: 'G?ran Brostr?m'; r-help at r-project.org; tamas.ferenci at medstat.hu
Subject: RE: [R] results of a survival analysis change when converting the data to counting process format


# For what it is worth, even the second fit (cuts at observation times) does not give identical coefficient estimates as using the original data structure.

answer <- coef( coxph(Surv(time, status) ~ trt + prior + karno, data = veteran) )
veteran2 <- survSplit( Surv(time, status) ~ trt + prior + karno, data = veteran, cut = unique( veteran$time ) )
answer2 <- coef( coxph(Surv(tstart,time, status) ~ trt + prior + karno, data = veteran2 ) )
answer2 - answer
 #          trt         prior         karno 
 # 2.775558e-16 -1.127570e-17 -6.938894e-18

# If you cut daily, but not all the way to 999, you get a few different fits

ff <- function(m) {
	veteran3 <- survSplit( Surv(time, status) ~ trt + prior + karno, data = veteran, cut = seq(m))
	answer3 <- coef( coxph(Surv(tstart,time, status) ~ trt + prior + karno, data = veteran3 ) )
	return(answer3)
}

answers3 <- sapply(100:999, ff)
plot(100:999, answers3[1,] - answer[1])

# But certainly all these differences are of no practical importance.



-----Original Message-----
From: G?ran Brostr?m [mailto:goran.brostrom at umu.se] 
Sent: Friday, August 23, 2019 5:13 AM
To: r-help at r-project.org; tamas.ferenci at medstat.hu
Subject: Re: [R] results of a survival analysis change when converting the data to counting process format



Den 2019-08-22 kl. 21:48, skrev G?ran Brostr?m:
> 
> 
> On 2019-08-18 19:10, Ferenci Tamas wrote:
>> Dear All,
>>
>> Consider the following simple example:
>>
>> library( survival )
>> data( veteran )
>>
>> coef( coxph(Surv(time, status) ~ trt + prior + karno, data = veteran) )
>> ????????? trt??????? prior??????? karno
>> ? 0.180197194 -0.005550919 -0.033771018
>>
>> Note that we have neither time-dependent covariates, nor time-varying
>> coefficients, so the results should be the same if we change to
>> counting process format, no matter where we cut the times.
>>
>> That's true if we cut at event times:
>>
>> veteran2 <- survSplit( Surv(time, status) ~ trt + prior + karno,
>> ??????????????????????? data = veteran, cut = unique( veteran$time ) )
>>
>> coef( coxph(Surv(tstart,time, status) ~ trt + prior + karno, data = 
>> veteran2 ) )
>> ????????? trt??????? prior??????? karno
>> ? 0.180197194 -0.005550919 -0.033771018
>>
>> But quite interestingly not true, if we cut at every day:
>>
>> veteran3 <- survSplit( Surv(time, status) ~ trt + prior + karno,
>> ??????????????????????? data = veteran, cut = 1:max(veteran$time) )
>>
>> coef( coxph(Surv(tstart,time, status) ~ trt + prior + karno, data = 
>> veteran3 ) )
>> ????????? trt??????? prior??????? karno
>> ? 0.180197215 -0.005550913 -0.033771016
>>
>> The difference is not large, but definitely more than just a rounding
>> error, or something like that.
>>
>> What's going on? How can the results get wrong, especially by
>> including more cutpoints?
> 
> All results are wrong, but they are useful (paraphrasing George EP Box).

That said, it is a little surprising: The generated risk sets are 
(should be) identical in all cases, and one would expect rounding errors 
to be the same. But data get stored differently, and ... who knows?

I tried your examples on my computer and got exactly the same results as 
you. Which surprised me.

G,

> 
> G?ran
> 
>>
>> Thank you in advance,
>> Tamas
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues 

From t@m@@@|erenc| @end|ng |rom med@t@t@hu  Fri Aug 23 16:39:49 2019
From: t@m@@@|erenc| @end|ng |rom med@t@t@hu (Ferenci Tamas)
Date: Fri, 23 Aug 2019 16:39:49 +0200
Subject: [R] 
 results of a survival analysis change when converting the data
 to counting process format
In-Reply-To: <771925$c9arbn@ironport10.mayo.edu>
References: <mailman.355844.1.1566554402.25347.r-help@r-project.org>
 <771925$c9arbn@ironport10.mayo.edu>
Message-ID: <638689157.20190823163949@medstat.hu>

Thanks for all those comments, and thanks Terry for the repair!

Tamas


2019. augusztus 23., 14:40:05, ?rtad:


I've spent some time chasing this down, and another error of similar type.   It's repaired in version 3.0-9  (soon to go to CRAN, or at least that is the plan  --- I'm working through one last fail in the checks.) 

For those who want to know more---

When there is start, stop data such as this:  (0, 101]  (101, 123]   (123, 400], ....  the algorithm used by coxph is to work from largest to smallest time.  Every time it hits the start of an interval (400, 123, 101) that observation is added into the risk set, and whenever it hits the start of an interval the observation is removed (123, 101, 0).    "Add" and "remove" also means "add to the current estimate of the running mean and variance" of the covariates.   At each event time that current mean and variance are are used to update the loglik and its derivatives.

When a subject is diced into little peices of (1,2] (2,3] (3,4], ....  like the last fit below, the sheer number of updates to the running mean/variance can lead to accumulated round off error.   The code works hard to avoid this, and some of that is subtle --- round off error is a tricky business --- and has gone through several refinements.    Nevertheless, the result for this data set should not have been that far off.  Another user example from about the same time was worse so I've been focusing on that one: one of the two splits led to an exp() overflow and one didn't, giving results that were completely different.  This led to a more careful review and some changes that addressed the example below as well.

Terry T.

On 8/23/19 5:00 AM, r-help-request at r-project.org wrote:

On 2019-08-18 19:10, Ferenci Tamas wrote:

Dear All,



Consider the following simple example:



library( survival )

data( veteran )



coef( coxph(Surv(time, status) ~ trt + prior + karno, data = veteran) )

          trt        prior        karno

  0.180197194 -0.005550919 -0.033771018



Note that we have neither time-dependent covariates, nor time-varying

coefficients, so the results should be the same if we change to

counting process format, no matter where we cut the times.



That's true if we cut at event times:



veteran2 <- survSplit( Surv(time, status) ~ trt + prior + karno,

                        data = veteran, cut = unique( veteran$time ) )



coef( coxph(Surv(tstart,time, status) ~ trt + prior + karno, data = veteran2 ) )

          trt        prior        karno

  0.180197194 -0.005550919 -0.033771018



But quite interestingly not true, if we cut at every day:



veteran3 <- survSplit( Surv(time, status) ~ trt + prior + karno,

                        data = veteran, cut = 1:max(veteran$time) )



coef( coxph(Surv(tstart,time, status) ~ trt + prior + karno, data = veteran3 ) )

          trt        prior        karno

  0.180197215 -0.005550913 -0.033771016



The difference is not large, but definitely more than just a rounding

error, or something like that.



What's going on? How can the results get wrong, especially by

including more cutpoints?


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Fri Aug 23 21:45:52 2019
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Fri, 23 Aug 2019 19:45:52 +0000
Subject: [R] Code that works when run as straight code,
 but that fails when run as a function
Message-ID: <BN7PR03MB373007AD86ED82DD38173FD1E2A40@BN7PR03MB3730.namprd03.prod.outlook.com>

I have code that works perfectly when run as in-line code, but that fails when the code is put into a function with the following message:
Error in contest1D.lmerModLmerTest(model, ll, rhs = rhs, ddf = ddf, confint = confint, :
length(L) == length(model at beta) is not TRUE

Why does moving the code to a function lead to an error? What can I do to avoid the error?
Thank you,
John

In-line code:
fit2aCort <- lmer(SBP~age+jFStage+AHIgt5+jFStage*AHIgt5+(1|patid),data=alldata)

c1 <-c(1,  5.99,  1, 0,   0,  1, 0 )
c2 <-c(1,  5.99,  0, 1,   0,  0, 1 )

model <- fit2aCort
s1a <- contest(model,L=c1,joint=FALSE)
s1as1E <- s1a$Estimate

s2a <- contest(model,L=c2,joint=FALSE)
s2E <- s2a$Estimate

Time <- c(1,2)
SBP  <- c(s1E,s2E)
plot(Time,SBP)
title("Predicted Sys SBP, age 5.99 yr (SS 524 obs with BP 190mmHg removed)")
plotCI(Time,SBP,uiw=1.96*c(s1a$"Std. Error",s2a$"Std. Error"))


Code put into a function that fails:
plotit <- function(c1,c2,model)
{
s1a <- contest(model,L=c1,joint=FALSE)
s1as1E <- s1a$Estimate

s2a <- contest(model,L=c2,joint=FALSE)
s2E <- s2a$Estimate

Time <- c(1,2)
SBP  <- c(s1E,s2E)
plot(Time,SBP)
title("Predicted Sys SBP, age 5.99 yr (SS 524 obs with BP 190mmHg removed)")
plotCI(Time,SBP,uiw=1.96*c(s1a$"Std. Error",s2a$"Std. Error"))
}

fit2aCort <- lmer(SBP~age+jFStage+AHIgt5+jFStage*AHIgt5+(1|patid),data=alldata)

c1 <-c(1,  5.99,  1, 0,   0,  1, 0 )
c2 <-c(1,  5.99,  0, 1,   0,  0, 1 )
plotit(c1,c2,fit2aCort)

	[[alternative HTML version deleted]]


From @@r@h@go@|ee @end|ng |rom gm@||@com  Fri Aug 23 22:11:57 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Fri, 23 Aug 2019 16:11:57 -0400
Subject: [R] Code that works when run as straight code,
 but that fails when run as a function
In-Reply-To: <BN7PR03MB373007AD86ED82DD38173FD1E2A40@BN7PR03MB3730.namprd03.prod.outlook.com>
References: <BN7PR03MB373007AD86ED82DD38173FD1E2A40@BN7PR03MB3730.namprd03.prod.outlook.com>
Message-ID: <CAM_vju=RZp9fQgFkjUth6u-8SzS-nhWVFUzgr=_MABGoqDJVZw@mail.gmail.com>

Since this isn't reproducible, my first guess would be that alldata,
which is required for your model, is not visible within the
environment of the function.

Or something similar: that kind of problem is almost always a scoping issue.

Sarah

On Fri, Aug 23, 2019 at 3:46 PM Sorkin, John <jsorkin at som.umaryland.edu> wrote:
>
> I have code that works perfectly when run as in-line code, but that fails when the code is put into a function with the following message:
> Error in contest1D.lmerModLmerTest(model, ll, rhs = rhs, ddf = ddf, confint = confint, :
> length(L) == length(model at beta) is not TRUE
>
> Why does moving the code to a function lead to an error? What can I do to avoid the error?
> Thank you,
> John
>
> In-line code:
> fit2aCort <- lmer(SBP~age+jFStage+AHIgt5+jFStage*AHIgt5+(1|patid),data=alldata)
>
> c1 <-c(1,  5.99,  1, 0,   0,  1, 0 )
> c2 <-c(1,  5.99,  0, 1,   0,  0, 1 )
>
> model <- fit2aCort
> s1a <- contest(model,L=c1,joint=FALSE)
> s1as1E <- s1a$Estimate
>
> s2a <- contest(model,L=c2,joint=FALSE)
> s2E <- s2a$Estimate
>
> Time <- c(1,2)
> SBP  <- c(s1E,s2E)
> plot(Time,SBP)
> title("Predicted Sys SBP, age 5.99 yr (SS 524 obs with BP 190mmHg removed)")
> plotCI(Time,SBP,uiw=1.96*c(s1a$"Std. Error",s2a$"Std. Error"))
>
>
> Code put into a function that fails:
> plotit <- function(c1,c2,model)
> {
> s1a <- contest(model,L=c1,joint=FALSE)
> s1as1E <- s1a$Estimate
>
> s2a <- contest(model,L=c2,joint=FALSE)
> s2E <- s2a$Estimate
>
> Time <- c(1,2)
> SBP  <- c(s1E,s2E)
> plot(Time,SBP)
> title("Predicted Sys SBP, age 5.99 yr (SS 524 obs with BP 190mmHg removed)")
> plotCI(Time,SBP,uiw=1.96*c(s1a$"Std. Error",s2a$"Std. Error"))
> }
>
> fit2aCort <- lmer(SBP~age+jFStage+AHIgt5+jFStage*AHIgt5+(1|patid),data=alldata)
>
> c1 <-c(1,  5.99,  1, 0,   0,  1, 0 )
> c2 <-c(1,  5.99,  0, 1,   0,  0, 1 )
> plotit(c1,c2,fit2aCort)
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From bgunter@4567 @end|ng |rom gm@||@com  Fri Aug 23 23:08:50 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 23 Aug 2019 14:08:50 -0700
Subject: [R] Fwd:  Code that works when run as straight code,
 but that fails when run as a function
In-Reply-To: <CAGxFJbQEcdJWh6rKJtL01SOTZXbsmHG=KrJwhAVpF14N5SV4MA@mail.gmail.com>
References: <BN7PR03MB373007AD86ED82DD38173FD1E2A40@BN7PR03MB3730.namprd03.prod.outlook.com>
 <CAGxFJbQEcdJWh6rKJtL01SOTZXbsmHG=KrJwhAVpF14N5SV4MA@mail.gmail.com>
Message-ID: <CAGxFJbQHnyC01rhiouu5PqBG8L_wSBWU0iCZtZheMNL0muquLA@mail.gmail.com>

Sorry -- neglected to cc the list.
Bert

---------- Forwarded message ---------
From: Bert Gunter <bgunter.4567 at gmail.com>
Date: Fri, Aug 23, 2019 at 2:06 PM
Subject: Re: [R] Code that works when run as straight code, but that fails
when run as a function
To: Sorkin, John <jsorkin at som.umaryland.edu>


s1E is not defined in your function in:
SBP  <- c(s1E,s2E)

R will then look for it in the caller (the Global environment presumably),
but maybe it isn't there. Presumably it was when you ran the code
interactively.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Aug 23, 2019 at 12:46 PM Sorkin, John <jsorkin at som.umaryland.edu>
wrote:

> I have code that works perfectly when run as in-line code, but that fails
> when the code is put into a function with the following message:
> Error in contest1D.lmerModLmerTest(model, ll, rhs = rhs, ddf = ddf,
> confint = confint, :
> length(L) == length(model at beta) is not TRUE
>
> Why does moving the code to a function lead to an error? What can I do to
> avoid the error?
> Thank you,
> John
>
> In-line code:
> fit2aCort <-
> lmer(SBP~age+jFStage+AHIgt5+jFStage*AHIgt5+(1|patid),data=alldata)
>
> c1 <-c(1,  5.99,  1, 0,   0,  1, 0 )
> c2 <-c(1,  5.99,  0, 1,   0,  0, 1 )
>
> model <- fit2aCort
> s1a <- contest(model,L=c1,joint=FALSE)
> s1as1E <- s1a$Estimate
>
> s2a <- contest(model,L=c2,joint=FALSE)
> s2E <- s2a$Estimate
>
> Time <- c(1,2)
> SBP  <- c(s1E,s2E)
> plot(Time,SBP)
> title("Predicted Sys SBP, age 5.99 yr (SS 524 obs with BP 190mmHg
> removed)")
> plotCI(Time,SBP,uiw=1.96*c(s1a$"Std. Error",s2a$"Std. Error"))
>
>
> Code put into a function that fails:
> plotit <- function(c1,c2,model)
> {
> s1a <- contest(model,L=c1,joint=FALSE)
> s1as1E <- s1a$Estimate
>
> s2a <- contest(model,L=c2,joint=FALSE)
> s2E <- s2a$Estimate
>
> Time <- c(1,2)
> SBP  <- c(s1E,s2E)
> plot(Time,SBP)
> title("Predicted Sys SBP, age 5.99 yr (SS 524 obs with BP 190mmHg
> removed)")
> plotCI(Time,SBP,uiw=1.96*c(s1a$"Std. Error",s2a$"Std. Error"))
> }
>
> fit2aCort <-
> lmer(SBP~age+jFStage+AHIgt5+jFStage*AHIgt5+(1|patid),data=alldata)
>
> c1 <-c(1,  5.99,  1, 0,   0,  1, 0 )
> c2 <-c(1,  5.99,  0, 1,   0,  0, 1 )
> plotit(c1,c2,fit2aCort)
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri Aug 23 23:24:18 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 23 Aug 2019 22:24:18 +0100
Subject: [R] Code that works when run as straight code,
 but that fails when run as a function
In-Reply-To: <BN7PR03MB373007AD86ED82DD38173FD1E2A40@BN7PR03MB3730.namprd03.prod.outlook.com>
References: <BN7PR03MB373007AD86ED82DD38173FD1E2A40@BN7PR03MB3730.namprd03.prod.outlook.com>
Message-ID: <0cfecd8b-673e-3f15-ae79-b45751646479@sapo.pt>

Hello,

Inline.

?s 20:45 de 23/08/19, Sorkin, John escreveu:
> I have code that works perfectly when run as in-line code, but that fails when the code is put into a function with the following message:
> Error in contest1D.lmerModLmerTest(model, ll, rhs = rhs, ddf = ddf, confint = confint, :
> length(L) == length(model at beta) is not TRUE
> 
> Why does moving the code to a function lead to an error? What can I do to avoid the error?
> Thank you,
> John
> 
> In-line code:
> fit2aCort <- lmer(SBP~age+jFStage+AHIgt5+jFStage*AHIgt5+(1|patid),data=alldata)
> 
> c1 <-c(1,  5.99,  1, 0,   0,  1, 0 )
> c2 <-c(1,  5.99,  0, 1,   0,  0, 1 )
> 
> model <- fit2aCort
> s1a <- contest(model,L=c1,joint=FALSE)
> s1as1E <- s1a$Estimate
> 
> s2a <- contest(model,L=c2,joint=FALSE)
> s2E <- s2a$Estimate
> 
> Time <- c(1,2)
> SBP  <- c(s1E,s2E)
> plot(Time,SBP)
> title("Predicted Sys SBP, age 5.99 yr (SS 524 obs with BP 190mmHg removed)")
> plotCI(Time,SBP,uiw=1.96*c(s1a$"Std. Error",s2a$"Std. Error"))
> 
> 
> Code put into a function that fails:
> plotit <- function(c1,c2,model)
> {
> s1a <- contest(model,L=c1,joint=FALSE)
> s1as1E <- s1a$Estimate
> 
> s2a <- contest(model,L=c2,joint=FALSE)
> s2E <- s2a$Estimate

Is s1as1E a typo? The way you define and use s2E suggests that it should be

s1E <- s1a$Estimate


Hope this helps,

Rui Barradas

> 
> Time <- c(1,2)
> SBP  <- c(s1E,s2E)
> plot(Time,SBP)
> title("Predicted Sys SBP, age 5.99 yr (SS 524 obs with BP 190mmHg removed)")
> plotCI(Time,SBP,uiw=1.96*c(s1a$"Std. Error",s2a$"Std. Error"))
> }
> 
> fit2aCort <- lmer(SBP~age+jFStage+AHIgt5+jFStage*AHIgt5+(1|patid),data=alldata)
> 
> c1 <-c(1,  5.99,  1, 0,   0,  1, 0 )
> c2 <-c(1,  5.99,  0, 1,   0,  0, 1 )
> plotit(c1,c2,fit2aCort)
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @eren@de@te|@n| @end|ng |rom gm@||@com  Fri Aug 23 23:52:55 2019
From: @eren@de@te|@n| @end|ng |rom gm@||@com (Serena De Stefani)
Date: Fri, 23 Aug 2019 17:52:55 -0400
Subject: [R] A goodness of fit test for two discrete distributions with
 unequal variance?
Message-ID: <CAPk26q9Dm9LFGVM91omcA+vR2JFjr84KfYz23Bjtojo5z_c93w@mail.gmail.com>

I have a computer simulation in which a virtual agent end up in different
areas of a layout based on several factors. There are 18 conditions in
total.
If I collapse the datapoint into bins, where each bin is one of the areas,
the data would look like this:

    x0 <- c(3,3,5,5,2) # computer simulation

Now I would like to validate this model having human subjects going trough
the same conditions, but I run into two sets of issues:

 1. the first issue is due to the fact that the dataset is discrete and
small (there may be less than 5 counts in a bin, and that's a problem for a
Chi-Square Goodness of Fit test), also there may be ties. After some online
digging I found two options:
- a permutation test
- a Cramer-von Mises test of goodness-of-fit (see this paper
<https://journal.r-project.org/archive/2011/RJ-2011-016/RJ-2011-016.pdf>
 https://journal.r-project.org/archive/2011/RJ-2011-016/RJ-2011-016.pdf)

I thought the Cramer-von Mises test of goodness-of-fit test could work, so
I ran it with made-up data for *one human subject* and I get the following
result:

    x0 <- c(3,3,5,5,2) # computer simulation
    x1 <- c(4,2,5,4,3) # subject 1

    library(goftest)

    cvm.test(x0, ecdf(x1))

    >Cramer-von Mises test of goodness-of-fit
>Null hypothesis: distribution ?ecdf(x1)?

    >data:  x0
    >omega2 = 0.14667, p-value = 0.4106

So far so good. But now let?s say I would like to have more than one human
subject, let?s say four of them. These are the results from the additional
subjects:

    x2 <- c(3,3,5,2,5) # subject 2
    x3 <- c(2,2,5,6,3) # subject 3
    x4 <- c(3,2,5,6,2) # subject 4

Now I run in the second set of issues:

2. on the one side I have a single computer simulation, on the other side I
have data from four subjects. Should I take the mean of the results for the
human subjects? Then would my data still be ?discrete?? Or should I run my
simulation four times? But I would get always the same results, so the
variance between the two datasets would be different.

Any ideas? Maybe I should change the design and have more levels for my
factors, so that I have more trials and the bins get bigger?

	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Sat Aug 24 00:03:11 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Fri, 23 Aug 2019 15:03:11 -0700
Subject: [R] A goodness of fit test for two discrete distributions with
 unequal variance?
In-Reply-To: <CAPk26q9Dm9LFGVM91omcA+vR2JFjr84KfYz23Bjtojo5z_c93w@mail.gmail.com>
References: <CAPk26q9Dm9LFGVM91omcA+vR2JFjr84KfYz23Bjtojo5z_c93w@mail.gmail.com>
Message-ID: <ac1cdbf8-62ab-5c75-9983-7c1d8155c0cc@comcast.net>


On 8/23/19 2:52 PM, Serena De Stefani wrote:
> I have a computer simulation in which a virtual agent end up in different
> areas of a layout based on several factors. There are 18 conditions in
> total.
> If I collapse the datapoint into bins, where each bin is one of the areas,
> the data would look like this:
>
>      x0 <- c(3,3,5,5,2) # computer simulation
>
> Now I would like to validate this model having human subjects going trough
> the same conditions, but I run into two sets of issues:
>
>   1. the first issue is due to the fact that the dataset is discrete and
> small (there may be less than 5 counts in a bin, and that's a problem for a
> Chi-Square Goodness of Fit test), also there may be ties. After some online
> digging I found two options:
> - a permutation test
> - a Cramer-von Mises test of goodness-of-fit (see this paper
> <https://journal.r-project.org/archive/2011/RJ-2011-016/RJ-2011-016.pdf>
>   https://journal.r-project.org/archive/2011/RJ-2011-016/RJ-2011-016.pdf)
>
> I thought the Cramer-von Mises test of goodness-of-fit test could work, so
> I ran it with made-up data for *one human subject* and I get the following
> result:
>
>      x0 <- c(3,3,5,5,2) # computer simulation
>      x1 <- c(4,2,5,4,3) # subject 1
>
>      library(goftest)
>
>      cvm.test(x0, ecdf(x1))
>
>      >Cramer-von Mises test of goodness-of-fit
>> Null hypothesis: distribution ?ecdf(x1)?
>      >data:  x0
>      >omega2 = 0.14667, p-value = 0.4106
>
> So far so good. But now let?s say I would like to have more than one human
> subject, let?s say four of them. These are the results from the additional
> subjects:
>
>      x2 <- c(3,3,5,2,5) # subject 2
>      x3 <- c(2,2,5,6,3) # subject 3
>      x4 <- c(3,2,5,6,2) # subject 4
>
> Now I run in the second set of issues:
>
> 2. on the one side I have a single computer simulation, on the other side I
> have data from four subjects. Should I take the mean of the results for the
> human subjects? Then would my data still be ?discrete?? Or should I run my
> simulation four times? But I would get always the same results, so the
> variance between the two datasets would be different.
>
> Any ideas? Maybe I should change the design and have more levels for my
> factors, so that I have more trials and the bins get bigger?
>
> 	[[alternative HTML version deleted]]


Statistics questions, especially those from people who have failed to 
heed the advice of the Posting Guide to post in plain text, are 
off-topic on rhelp and should be posted to a forum where statistics 
questions are welcomed. (My suspicion is that this question will be 
greeted with further requests for clarification of goals, since asking 
what you "should" do requires an careful explanation of what your 
standards of evidence are and what you are attempting to demonstrate.


-- 

David.

>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From phii m@iii@g oii phiiipsmith@c@  Sun Aug 25 04:25:38 2019
From: phii m@iii@g oii phiiipsmith@c@ (phii m@iii@g oii phiiipsmith@c@)
Date: Sat, 24 Aug 2019 22:25:38 -0400
Subject: [R] Colouring selected columns in a facetted column chart
Message-ID: <ecbd4d10a93deb6d99755ee76c995a14@philipsmith.ca>

I am having difficulty with a chart using ggplot. It is a facetted 
column chart showing GDP growth rates by country. The columns are 
coloured navyblue, except that I want to colour the most recent columns, 
for 2019-Q1 and 2019-Q2, red. For some countries data are available up 
to 2019-Q2 while for others data are only available up to 2019-Q1. My 
code and data frame are shown below and it almost works, but not quite. 
For some reason the red bars for Germany, Korea, Norway, Sweden and 
United Kingdom are slightly off. Any help will be much appreciated.

Here is my reprex:

library(tidyverse)
t1 <- read.table("t1.txt",header=TRUE,sep="\t")
col <- rep("navyblue",nrow(t1))
for (i in 1:nrow(t1)) {
   if((t1$TIME[i]=="2019-Q1" | t1$TIME[i]=="2019-Q2")) {
     col[i] <- "red"}
}
ggplot(t1) +
   geom_col(aes(x=TIME,y=GDPgr),fill=col) +
   facet_wrap(~Country,ncol=3)

Here is my data frame, called "t1.txt":

"TIME"	"LOCATION"	"Country"	"Value"	"GDPgr"
"2016-Q4"	"AUS"	"Australia"	440518	1
"2017-Q1"	"AUS"	"Australia"	442141	0.4
"2017-Q2"	"AUS"	"Australia"	445739	0.8
"2017-Q3"	"AUS"	"Australia"	448672	0.7
"2017-Q4"	"AUS"	"Australia"	451302	0.6
"2018-Q1"	"AUS"	"Australia"	455680	1
"2018-Q2"	"AUS"	"Australia"	459697	0.9
"2018-Q3"	"AUS"	"Australia"	461024	0.3
"2018-Q4"	"AUS"	"Australia"	462032	0.2
"2019-Q1"	"AUS"	"Australia"	463907	0.4
"2016-Q4"	"BEL"	"Belgium"	106675	0.3
"2017-Q1"	"BEL"	"Belgium"	107394	0.7
"2017-Q2"	"BEL"	"Belgium"	107828	0.4
"2017-Q3"	"BEL"	"Belgium"	108003	0.2
"2017-Q4"	"BEL"	"Belgium"	108744	0.7
"2018-Q1"	"BEL"	"Belgium"	109037	0.3
"2018-Q2"	"BEL"	"Belgium"	109386	0.3
"2018-Q3"	"BEL"	"Belgium"	109676	0.3
"2018-Q4"	"BEL"	"Belgium"	110081	0.4
"2019-Q1"	"BEL"	"Belgium"	110459	0.3
"2019-Q2"	"BEL"	"Belgium"	110680	0.2
"2016-Q4"	"CAN"	"Canada"	493742	0.6
"2017-Q1"	"CAN"	"Canada"	498719	1
"2017-Q2"	"CAN"	"Canada"	504100.5	1.1
"2017-Q3"	"CAN"	"Canada"	505745	0.3
"2017-Q4"	"CAN"	"Canada"	507883	0.4
"2018-Q1"	"CAN"	"Canada"	509758.75	0.4
"2018-Q2"	"CAN"	"Canada"	512958	0.6
"2018-Q3"	"CAN"	"Canada"	515639.25	0.5
"2018-Q4"	"CAN"	"Canada"	515971.75	0.1
"2019-Q1"	"CAN"	"Canada"	516489.5	0.1
"2016-Q4"	"DNK"	"Denmark"	499945	0.9
"2017-Q1"	"DNK"	"Denmark"	511319	2.3
"2017-Q2"	"DNK"	"Denmark"	505254	-1.2
"2017-Q3"	"DNK"	"Denmark"	500363	-1
"2017-Q4"	"DNK"	"Denmark"	504837	0.9
"2018-Q1"	"DNK"	"Denmark"	508633	0.8
"2018-Q2"	"DNK"	"Denmark"	511901	0.6
"2018-Q3"	"DNK"	"Denmark"	513630	0.3
"2018-Q4"	"DNK"	"Denmark"	517726	0.8
"2019-Q1"	"DNK"	"Denmark"	518368	0.1
"2016-Q4"	"EU28"	"European Union (28 countries)"	3301202.652555	0.8
"2017-Q1"	"EU28"	"European Union (28 countries)"	3323886.876398	0.7
"2017-Q2"	"EU28"	"European Union (28 countries)"	3345038.332666	0.6
"2017-Q3"	"EU28"	"European Union (28 countries)"	3367136.027609	0.7
"2017-Q4"	"EU28"	"European Union (28 countries)"	3390431.080785	0.7
"2018-Q1"	"EU28"	"European Union (28 countries)"	3404554.778774	0.4
"2018-Q2"	"EU28"	"European Union (28 countries)"	3419358.570571	0.4
"2018-Q3"	"EU28"	"European Union (28 countries)"	3430321.169276	0.3
"2018-Q4"	"EU28"	"European Union (28 countries)"	3440915.89772	0.3
"2019-Q1"	"EU28"	"European Union (28 countries)"	3458087.265837	0.5
"2019-Q2"	"EU28"	"European Union (28 countries)"	3465003.441	0.2
"2016-Q4"	"FIN"	"Finland"	48525	0.2
"2017-Q1"	"FIN"	"Finland"	49368	1.7
"2017-Q2"	"FIN"	"Finland"	49430	0.1
"2017-Q3"	"FIN"	"Finland"	49596	0.3
"2017-Q4"	"FIN"	"Finland"	50153	1.1
"2018-Q1"	"FIN"	"Finland"	50352	0.4
"2018-Q2"	"FIN"	"Finland"	50449	0.2
"2018-Q3"	"FIN"	"Finland"	50507	0.1
"2018-Q4"	"FIN"	"Finland"	50530	0
"2019-Q1"	"FIN"	"Finland"	50822	0.6
"2016-Q4"	"FRA"	"France"	551760	0.6
"2017-Q1"	"FRA"	"France"	556305	0.8
"2017-Q2"	"FRA"	"France"	560160	0.7
"2017-Q3"	"FRA"	"France"	563998	0.7
"2017-Q4"	"FRA"	"France"	568125	0.7
"2018-Q1"	"FRA"	"France"	569542	0.2
"2018-Q2"	"FRA"	"France"	570670	0.2
"2018-Q3"	"FRA"	"France"	572387	0.3
"2018-Q4"	"FRA"	"France"	574640	0.4
"2019-Q1"	"FRA"	"France"	576494	0.3
"2019-Q2"	"FRA"	"France"	577905	0.2
"2016-Q4"	"DEU"	"Germany"	716743.4074	0.4
"2017-Q1"	"DEU"	"Germany"	725268.5864	1.2
"2017-Q2"	"DEU"	"Germany"	729321.5731	0.6
"2017-Q3"	"DEU"	"Germany"	735610.6375	0.9
"2017-Q4"	"DEU"	"Germany"	740991.229	0.7
"2018-Q1"	"DEU"	"Germany"	741969.5787	0.1
"2018-Q2"	"DEU"	"Germany"	744834.6127	0.4
"2018-Q3"	"DEU"	"Germany"	744065.912	-0.1
"2018-Q4"	"DEU"	"Germany"	745603.2305	0.2
"2019-Q1"	"DEU"	"Germany"	748468.2276	0.4
"2019-Q2"	"DEU"	"Germany"	747909.2496	-0.1
"2016-Q4"	"ISR"	"Israel"	307789.55	0.9
"2017-Q1"	"ISR"	"Israel"	308323.023	0.2
"2017-Q2"	"ISR"	"Israel"	311759.624	1.1
"2017-Q3"	"ISR"	"Israel"	315651.46	1.2
"2017-Q4"	"ISR"	"Israel"	319056.442	1.1
"2018-Q1"	"ISR"	"Israel"	322272.592	1
"2018-Q2"	"ISR"	"Israel"	323422.356	0.4
"2018-Q3"	"ISR"	"Israel"	325702.534	0.7
"2018-Q4"	"ISR"	"Israel"	329052.641	1
"2019-Q1"	"ISR"	"Israel"	332851.725	1.2
"2019-Q2"	"ISR"	"Israel"	333686.876	0.3
"2016-Q4"	"ITA"	"Italy"	396162.2	0.5
"2017-Q1"	"ITA"	"Italy"	398379	0.6
"2017-Q2"	"ITA"	"Italy"	399893	0.4
"2017-Q3"	"ITA"	"Italy"	401534	0.4
"2017-Q4"	"ITA"	"Italy"	403053.4	0.4
"2018-Q1"	"ITA"	"Italy"	403937.8	0.2
"2018-Q2"	"ITA"	"Italy"	403977.3	0
"2018-Q3"	"ITA"	"Italy"	403434.2	-0.1
"2018-Q4"	"ITA"	"Italy"	403190.7	-0.1
"2019-Q1"	"ITA"	"Italy"	403697.9	0.1
"2019-Q2"	"ITA"	"Italy"	403794.7	0
"2016-Q4"	"JPN"	"Japan"	130406025	0.2
"2017-Q1"	"JPN"	"Japan"	131558850	0.9
"2017-Q2"	"JPN"	"Japan"	132121450	0.4
"2017-Q3"	"JPN"	"Japan"	133064400	0.7
"2017-Q4"	"JPN"	"Japan"	133475100	0.3
"2018-Q1"	"JPN"	"Japan"	133386850	-0.1
"2018-Q2"	"JPN"	"Japan"	133931825	0.4
"2018-Q3"	"JPN"	"Japan"	133289800	-0.5
"2018-Q4"	"JPN"	"Japan"	133836225	0.4
"2019-Q1"	"JPN"	"Japan"	134777725	0.7
"2019-Q2"	"JPN"	"Japan"	135369050	0.4
"2016-Q4"	"KOR"	"Korea"	431473400	0.8
"2017-Q1"	"KOR"	"Korea"	435435200	0.9
"2017-Q2"	"KOR"	"Korea"	437712100	0.5
"2017-Q3"	"KOR"	"Korea"	444064400	1.5
"2017-Q4"	"KOR"	"Korea"	443599800	-0.1
"2018-Q1"	"KOR"	"Korea"	447909300	1
"2018-Q2"	"KOR"	"Korea"	450495800	0.6
"2018-Q3"	"KOR"	"Korea"	452561100	0.5
"2018-Q4"	"KOR"	"Korea"	456769700	0.9
"2019-Q1"	"KOR"	"Korea"	455081000	-0.4
"2019-Q2"	"KOR"	"Korea"	459958000	1.1
"2016-Q4"	"NLD"	"Netherlands"	178453.593134	0.9
"2017-Q1"	"NLD"	"Netherlands"	179367.793134	0.5
"2017-Q2"	"NLD"	"Netherlands"	180964.533134	0.9
"2017-Q3"	"NLD"	"Netherlands"	182189.893134	0.7
"2017-Q4"	"NLD"	"Netherlands"	183625.193134	0.8
"2018-Q1"	"NLD"	"Netherlands"	184793.473134	0.6
"2018-Q2"	"NLD"	"Netherlands"	185981.973134	0.6
"2018-Q3"	"NLD"	"Netherlands"	186425.153134	0.2
"2018-Q4"	"NLD"	"Netherlands"	187434.343134	0.5
"2019-Q1"	"NLD"	"Netherlands"	188324.263134	0.5
"2019-Q2"	"NLD"	"Netherlands"	189297.773134	0.5
"2016-Q4"	"NZL"	"New Zealand"	59062	0.5
"2017-Q1"	"NZL"	"New Zealand"	59348	0.5
"2017-Q2"	"NZL"	"New Zealand"	59743	0.7
"2017-Q3"	"NZL"	"New Zealand"	60320	1
"2017-Q4"	"NZL"	"New Zealand"	60737	0.7
"2018-Q1"	"NZL"	"New Zealand"	61031	0.5
"2018-Q2"	"NZL"	"New Zealand"	61655	1
"2018-Q3"	"NZL"	"New Zealand"	61927	0.4
"2018-Q4"	"NZL"	"New Zealand"	62282	0.6
"2019-Q1"	"NZL"	"New Zealand"	62800	0.8
"2016-Q4"	"NOR"	"Norway"	784704	2
"2017-Q1"	"NOR"	"Norway"	788709	0.5
"2017-Q2"	"NOR"	"Norway"	794220	0.7
"2017-Q3"	"NOR"	"Norway"	798283	0.5
"2017-Q4"	"NOR"	"Norway"	800232	0.2
"2018-Q1"	"NOR"	"Norway"	803756	0.4
"2018-Q2"	"NOR"	"Norway"	807187	0.4
"2018-Q3"	"NOR"	"Norway"	810942	0.5
"2018-Q4"	"NOR"	"Norway"	815921	0.6
"2019-Q1"	"NOR"	"Norway"	815323	-0.1
"2016-Q4"	"PRT"	"Portugal"	44303.821	0.8
"2017-Q1"	"PRT"	"Portugal"	44632.068	0.7
"2017-Q2"	"PRT"	"Portugal"	44803.721	0.4
"2017-Q3"	"PRT"	"Portugal"	45062.631	0.6
"2017-Q4"	"PRT"	"Portugal"	45426.14	0.8
"2018-Q1"	"PRT"	"Portugal"	45641.37	0.5
"2018-Q2"	"PRT"	"Portugal"	45910.934	0.6
"2018-Q3"	"PRT"	"Portugal"	46027.362	0.3
"2018-Q4"	"PRT"	"Portugal"	46203.578	0.4
"2019-Q1"	"PRT"	"Portugal"	46453.392	0.5
"2019-Q2"	"PRT"	"Portugal"	46685.65896	0.5
"2016-Q4"	"ESP"	"Spain"	279431	0.6
"2017-Q1"	"ESP"	"Spain"	281707	0.8
"2017-Q2"	"ESP"	"Spain"	284169	0.9
"2017-Q3"	"ESP"	"Spain"	285986	0.6
"2017-Q4"	"ESP"	"Spain"	288064	0.7
"2018-Q1"	"ESP"	"Spain"	289861	0.6
"2018-Q2"	"ESP"	"Spain"	291583	0.6
"2018-Q3"	"ESP"	"Spain"	293145	0.5
"2018-Q4"	"ESP"	"Spain"	294768	0.6
"2019-Q1"	"ESP"	"Spain"	296732	0.7
"2019-Q2"	"ESP"	"Spain"	298147	0.5
"2016-Q4"	"SWE"	"Sweden"	1150761	0.4
"2017-Q1"	"SWE"	"Sweden"	1151977	0.1
"2017-Q2"	"SWE"	"Sweden"	1169243	1.5
"2017-Q3"	"SWE"	"Sweden"	1177835	0.7
"2017-Q4"	"SWE"	"Sweden"	1181734	0.3
"2018-Q1"	"SWE"	"Sweden"	1192111	0.9
"2018-Q2"	"SWE"	"Sweden"	1197931	0.5
"2018-Q3"	"SWE"	"Sweden"	1196262	-0.1
"2018-Q4"	"SWE"	"Sweden"	1209430	1.1
"2019-Q1"	"SWE"	"Sweden"	1215583	0.5
"2019-Q2"	"SWE"	"Sweden"	1214691	-0.1
"2016-Q4"	"CHE"	"Switzerland"	168268.356822	-0.1
"2017-Q1"	"CHE"	"Switzerland"	168865.076317	0.4
"2017-Q2"	"CHE"	"Switzerland"	170078.764694	0.7
"2017-Q3"	"CHE"	"Switzerland"	171405.16327	0.8
"2017-Q4"	"CHE"	"Switzerland"	172777.427869	0.8
"2018-Q1"	"CHE"	"Switzerland"	174168.535837	0.8
"2018-Q2"	"CHE"	"Switzerland"	175400.870886	0.7
"2018-Q3"	"CHE"	"Switzerland"	175089.0314	-0.2
"2018-Q4"	"CHE"	"Switzerland"	175664.228343	0.3
"2019-Q1"	"CHE"	"Switzerland"	176651.744992	0.6
"2016-Q4"	"GBR"	"United Kingdom"	496470	0.7
"2017-Q1"	"GBR"	"United Kingdom"	498582	0.4
"2017-Q2"	"GBR"	"United Kingdom"	499885	0.3
"2017-Q3"	"GBR"	"United Kingdom"	502473	0.5
"2017-Q4"	"GBR"	"United Kingdom"	504487	0.4
"2018-Q1"	"GBR"	"United Kingdom"	504785	0.1
"2018-Q2"	"GBR"	"United Kingdom"	506842	0.4
"2018-Q3"	"GBR"	"United Kingdom"	510346	0.7
"2018-Q4"	"GBR"	"United Kingdom"	511482	0.2
"2019-Q1"	"GBR"	"United Kingdom"	514019	0.5
"2019-Q2"	"GBR"	"United Kingdom"	513029	-0.2
"2016-Q4"	"USA"	"United States"	4456057.75	0.5
"2017-Q1"	"USA"	"United States"	4481314	0.6
"2017-Q2"	"USA"	"United States"	4505262	0.5
"2017-Q3"	"USA"	"United States"	4540889.5	0.8
"2017-Q4"	"USA"	"United States"	4580616	0.9
"2018-Q1"	"USA"	"United States"	4609563.5	0.6
"2018-Q2"	"USA"	"United States"	4649533.75	0.9
"2018-Q3"	"USA"	"United States"	4683180	0.7
"2018-Q4"	"USA"	"United States"	4695887	0.3
"2019-Q1"	"USA"	"United States"	4731820.25	0.8
"2019-Q2"	"USA"	"United States"	4755955	0.5


From er|cjberger @end|ng |rom gm@||@com  Sun Aug 25 04:39:21 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Sun, 25 Aug 2019 05:39:21 +0300
Subject: [R] Colouring selected columns in a facetted column chart
In-Reply-To: <ecbd4d10a93deb6d99755ee76c995a14@philipsmith.ca>
References: <ecbd4d10a93deb6d99755ee76c995a14@philipsmith.ca>
Message-ID: <CAGgJW77ByTO7CrHyrjd17AiOw2m9Qs5K91TM4J6Df7EX4pvynQ@mail.gmail.com>

Hi Phil,
Please resubmit your question with the data frame contents shown as the
output from the command
dput(t1.txt). This will make it easier for people to run your reprex and
respond to your question.

Best,
Eric



On Sun, Aug 25, 2019 at 5:26 AM <phil at philipsmith.ca> wrote:

> I am having difficulty with a chart using ggplot. It is a facetted
> column chart showing GDP growth rates by country. The columns are
> coloured navyblue, except that I want to colour the most recent columns,
> for 2019-Q1 and 2019-Q2, red. For some countries data are available up
> to 2019-Q2 while for others data are only available up to 2019-Q1. My
> code and data frame are shown below and it almost works, but not quite.
> For some reason the red bars for Germany, Korea, Norway, Sweden and
> United Kingdom are slightly off. Any help will be much appreciated.
>
> Here is my reprex:
>
> library(tidyverse)
> t1 <- read.table("t1.txt",header=TRUE,sep="\t")
> col <- rep("navyblue",nrow(t1))
> for (i in 1:nrow(t1)) {
>    if((t1$TIME[i]=="2019-Q1" | t1$TIME[i]=="2019-Q2")) {
>      col[i] <- "red"}
> }
> ggplot(t1) +
>    geom_col(aes(x=TIME,y=GDPgr),fill=col) +
>    facet_wrap(~Country,ncol=3)
>
> Here is my data frame, called "t1.txt":
>
> "TIME"  "LOCATION"      "Country"       "Value" "GDPgr"
> "2016-Q4"       "AUS"   "Australia"     440518  1
> "2017-Q1"       "AUS"   "Australia"     442141  0.4
> "2017-Q2"       "AUS"   "Australia"     445739  0.8
> "2017-Q3"       "AUS"   "Australia"     448672  0.7
> "2017-Q4"       "AUS"   "Australia"     451302  0.6
> "2018-Q1"       "AUS"   "Australia"     455680  1
> "2018-Q2"       "AUS"   "Australia"     459697  0.9
> "2018-Q3"       "AUS"   "Australia"     461024  0.3
> "2018-Q4"       "AUS"   "Australia"     462032  0.2
> "2019-Q1"       "AUS"   "Australia"     463907  0.4
> "2016-Q4"       "BEL"   "Belgium"       106675  0.3
> "2017-Q1"       "BEL"   "Belgium"       107394  0.7
> "2017-Q2"       "BEL"   "Belgium"       107828  0.4
> "2017-Q3"       "BEL"   "Belgium"       108003  0.2
> "2017-Q4"       "BEL"   "Belgium"       108744  0.7
> "2018-Q1"       "BEL"   "Belgium"       109037  0.3
> "2018-Q2"       "BEL"   "Belgium"       109386  0.3
> "2018-Q3"       "BEL"   "Belgium"       109676  0.3
> "2018-Q4"       "BEL"   "Belgium"       110081  0.4
> "2019-Q1"       "BEL"   "Belgium"       110459  0.3
> "2019-Q2"       "BEL"   "Belgium"       110680  0.2
> "2016-Q4"       "CAN"   "Canada"        493742  0.6
> "2017-Q1"       "CAN"   "Canada"        498719  1
> "2017-Q2"       "CAN"   "Canada"        504100.5        1.1
> "2017-Q3"       "CAN"   "Canada"        505745  0.3
> "2017-Q4"       "CAN"   "Canada"        507883  0.4
> "2018-Q1"       "CAN"   "Canada"        509758.75       0.4
> "2018-Q2"       "CAN"   "Canada"        512958  0.6
> "2018-Q3"       "CAN"   "Canada"        515639.25       0.5
> "2018-Q4"       "CAN"   "Canada"        515971.75       0.1
> "2019-Q1"       "CAN"   "Canada"        516489.5        0.1
> "2016-Q4"       "DNK"   "Denmark"       499945  0.9
> "2017-Q1"       "DNK"   "Denmark"       511319  2.3
> "2017-Q2"       "DNK"   "Denmark"       505254  -1.2
> "2017-Q3"       "DNK"   "Denmark"       500363  -1
> "2017-Q4"       "DNK"   "Denmark"       504837  0.9
> "2018-Q1"       "DNK"   "Denmark"       508633  0.8
> "2018-Q2"       "DNK"   "Denmark"       511901  0.6
> "2018-Q3"       "DNK"   "Denmark"       513630  0.3
> "2018-Q4"       "DNK"   "Denmark"       517726  0.8
> "2019-Q1"       "DNK"   "Denmark"       518368  0.1
> "2016-Q4"       "EU28"  "European Union (28 countries)" 3301202.652555  0.8
> "2017-Q1"       "EU28"  "European Union (28 countries)" 3323886.876398  0.7
> "2017-Q2"       "EU28"  "European Union (28 countries)" 3345038.332666  0.6
> "2017-Q3"       "EU28"  "European Union (28 countries)" 3367136.027609  0.7
> "2017-Q4"       "EU28"  "European Union (28 countries)" 3390431.080785  0.7
> "2018-Q1"       "EU28"  "European Union (28 countries)" 3404554.778774  0.4
> "2018-Q2"       "EU28"  "European Union (28 countries)" 3419358.570571  0.4
> "2018-Q3"       "EU28"  "European Union (28 countries)" 3430321.169276  0.3
> "2018-Q4"       "EU28"  "European Union (28 countries)" 3440915.89772   0.3
> "2019-Q1"       "EU28"  "European Union (28 countries)" 3458087.265837  0.5
> "2019-Q2"       "EU28"  "European Union (28 countries)" 3465003.441     0.2
> "2016-Q4"       "FIN"   "Finland"       48525   0.2
> "2017-Q1"       "FIN"   "Finland"       49368   1.7
> "2017-Q2"       "FIN"   "Finland"       49430   0.1
> "2017-Q3"       "FIN"   "Finland"       49596   0.3
> "2017-Q4"       "FIN"   "Finland"       50153   1.1
> "2018-Q1"       "FIN"   "Finland"       50352   0.4
> "2018-Q2"       "FIN"   "Finland"       50449   0.2
> "2018-Q3"       "FIN"   "Finland"       50507   0.1
> "2018-Q4"       "FIN"   "Finland"       50530   0
> "2019-Q1"       "FIN"   "Finland"       50822   0.6
> "2016-Q4"       "FRA"   "France"        551760  0.6
> "2017-Q1"       "FRA"   "France"        556305  0.8
> "2017-Q2"       "FRA"   "France"        560160  0.7
> "2017-Q3"       "FRA"   "France"        563998  0.7
> "2017-Q4"       "FRA"   "France"        568125  0.7
> "2018-Q1"       "FRA"   "France"        569542  0.2
> "2018-Q2"       "FRA"   "France"        570670  0.2
> "2018-Q3"       "FRA"   "France"        572387  0.3
> "2018-Q4"       "FRA"   "France"        574640  0.4
> "2019-Q1"       "FRA"   "France"        576494  0.3
> "2019-Q2"       "FRA"   "France"        577905  0.2
> "2016-Q4"       "DEU"   "Germany"       716743.4074     0.4
> "2017-Q1"       "DEU"   "Germany"       725268.5864     1.2
> "2017-Q2"       "DEU"   "Germany"       729321.5731     0.6
> "2017-Q3"       "DEU"   "Germany"       735610.6375     0.9
> "2017-Q4"       "DEU"   "Germany"       740991.229      0.7
> "2018-Q1"       "DEU"   "Germany"       741969.5787     0.1
> "2018-Q2"       "DEU"   "Germany"       744834.6127     0.4
> "2018-Q3"       "DEU"   "Germany"       744065.912      -0.1
> "2018-Q4"       "DEU"   "Germany"       745603.2305     0.2
> "2019-Q1"       "DEU"   "Germany"       748468.2276     0.4
> "2019-Q2"       "DEU"   "Germany"       747909.2496     -0.1
> "2016-Q4"       "ISR"   "Israel"        307789.55       0.9
> "2017-Q1"       "ISR"   "Israel"        308323.023      0.2
> "2017-Q2"       "ISR"   "Israel"        311759.624      1.1
> "2017-Q3"       "ISR"   "Israel"        315651.46       1.2
> "2017-Q4"       "ISR"   "Israel"        319056.442      1.1
> "2018-Q1"       "ISR"   "Israel"        322272.592      1
> "2018-Q2"       "ISR"   "Israel"        323422.356      0.4
> "2018-Q3"       "ISR"   "Israel"        325702.534      0.7
> "2018-Q4"       "ISR"   "Israel"        329052.641      1
> "2019-Q1"       "ISR"   "Israel"        332851.725      1.2
> "2019-Q2"       "ISR"   "Israel"        333686.876      0.3
> "2016-Q4"       "ITA"   "Italy" 396162.2        0.5
> "2017-Q1"       "ITA"   "Italy" 398379  0.6
> "2017-Q2"       "ITA"   "Italy" 399893  0.4
> "2017-Q3"       "ITA"   "Italy" 401534  0.4
> "2017-Q4"       "ITA"   "Italy" 403053.4        0.4
> "2018-Q1"       "ITA"   "Italy" 403937.8        0.2
> "2018-Q2"       "ITA"   "Italy" 403977.3        0
> "2018-Q3"       "ITA"   "Italy" 403434.2        -0.1
> "2018-Q4"       "ITA"   "Italy" 403190.7        -0.1
> "2019-Q1"       "ITA"   "Italy" 403697.9        0.1
> "2019-Q2"       "ITA"   "Italy" 403794.7        0
> "2016-Q4"       "JPN"   "Japan" 130406025       0.2
> "2017-Q1"       "JPN"   "Japan" 131558850       0.9
> "2017-Q2"       "JPN"   "Japan" 132121450       0.4
> "2017-Q3"       "JPN"   "Japan" 133064400       0.7
> "2017-Q4"       "JPN"   "Japan" 133475100       0.3
> "2018-Q1"       "JPN"   "Japan" 133386850       -0.1
> "2018-Q2"       "JPN"   "Japan" 133931825       0.4
> "2018-Q3"       "JPN"   "Japan" 133289800       -0.5
> "2018-Q4"       "JPN"   "Japan" 133836225       0.4
> "2019-Q1"       "JPN"   "Japan" 134777725       0.7
> "2019-Q2"       "JPN"   "Japan" 135369050       0.4
> "2016-Q4"       "KOR"   "Korea" 431473400       0.8
> "2017-Q1"       "KOR"   "Korea" 435435200       0.9
> "2017-Q2"       "KOR"   "Korea" 437712100       0.5
> "2017-Q3"       "KOR"   "Korea" 444064400       1.5
> "2017-Q4"       "KOR"   "Korea" 443599800       -0.1
> "2018-Q1"       "KOR"   "Korea" 447909300       1
> "2018-Q2"       "KOR"   "Korea" 450495800       0.6
> "2018-Q3"       "KOR"   "Korea" 452561100       0.5
> "2018-Q4"       "KOR"   "Korea" 456769700       0.9
> "2019-Q1"       "KOR"   "Korea" 455081000       -0.4
> "2019-Q2"       "KOR"   "Korea" 459958000       1.1
> "2016-Q4"       "NLD"   "Netherlands"   178453.593134   0.9
> "2017-Q1"       "NLD"   "Netherlands"   179367.793134   0.5
> "2017-Q2"       "NLD"   "Netherlands"   180964.533134   0.9
> "2017-Q3"       "NLD"   "Netherlands"   182189.893134   0.7
> "2017-Q4"       "NLD"   "Netherlands"   183625.193134   0.8
> "2018-Q1"       "NLD"   "Netherlands"   184793.473134   0.6
> "2018-Q2"       "NLD"   "Netherlands"   185981.973134   0.6
> "2018-Q3"       "NLD"   "Netherlands"   186425.153134   0.2
> "2018-Q4"       "NLD"   "Netherlands"   187434.343134   0.5
> "2019-Q1"       "NLD"   "Netherlands"   188324.263134   0.5
> "2019-Q2"       "NLD"   "Netherlands"   189297.773134   0.5
> "2016-Q4"       "NZL"   "New Zealand"   59062   0.5
> "2017-Q1"       "NZL"   "New Zealand"   59348   0.5
> "2017-Q2"       "NZL"   "New Zealand"   59743   0.7
> "2017-Q3"       "NZL"   "New Zealand"   60320   1
> "2017-Q4"       "NZL"   "New Zealand"   60737   0.7
> "2018-Q1"       "NZL"   "New Zealand"   61031   0.5
> "2018-Q2"       "NZL"   "New Zealand"   61655   1
> "2018-Q3"       "NZL"   "New Zealand"   61927   0.4
> "2018-Q4"       "NZL"   "New Zealand"   62282   0.6
> "2019-Q1"       "NZL"   "New Zealand"   62800   0.8
> "2016-Q4"       "NOR"   "Norway"        784704  2
> "2017-Q1"       "NOR"   "Norway"        788709  0.5
> "2017-Q2"       "NOR"   "Norway"        794220  0.7
> "2017-Q3"       "NOR"   "Norway"        798283  0.5
> "2017-Q4"       "NOR"   "Norway"        800232  0.2
> "2018-Q1"       "NOR"   "Norway"        803756  0.4
> "2018-Q2"       "NOR"   "Norway"        807187  0.4
> "2018-Q3"       "NOR"   "Norway"        810942  0.5
> "2018-Q4"       "NOR"   "Norway"        815921  0.6
> "2019-Q1"       "NOR"   "Norway"        815323  -0.1
> "2016-Q4"       "PRT"   "Portugal"      44303.821       0.8
> "2017-Q1"       "PRT"   "Portugal"      44632.068       0.7
> "2017-Q2"       "PRT"   "Portugal"      44803.721       0.4
> "2017-Q3"       "PRT"   "Portugal"      45062.631       0.6
> "2017-Q4"       "PRT"   "Portugal"      45426.14        0.8
> "2018-Q1"       "PRT"   "Portugal"      45641.37        0.5
> "2018-Q2"       "PRT"   "Portugal"      45910.934       0.6
> "2018-Q3"       "PRT"   "Portugal"      46027.362       0.3
> "2018-Q4"       "PRT"   "Portugal"      46203.578       0.4
> "2019-Q1"       "PRT"   "Portugal"      46453.392       0.5
> "2019-Q2"       "PRT"   "Portugal"      46685.65896     0.5
> "2016-Q4"       "ESP"   "Spain" 279431  0.6
> "2017-Q1"       "ESP"   "Spain" 281707  0.8
> "2017-Q2"       "ESP"   "Spain" 284169  0.9
> "2017-Q3"       "ESP"   "Spain" 285986  0.6
> "2017-Q4"       "ESP"   "Spain" 288064  0.7
> "2018-Q1"       "ESP"   "Spain" 289861  0.6
> "2018-Q2"       "ESP"   "Spain" 291583  0.6
> "2018-Q3"       "ESP"   "Spain" 293145  0.5
> "2018-Q4"       "ESP"   "Spain" 294768  0.6
> "2019-Q1"       "ESP"   "Spain" 296732  0.7
> "2019-Q2"       "ESP"   "Spain" 298147  0.5
> "2016-Q4"       "SWE"   "Sweden"        1150761 0.4
> "2017-Q1"       "SWE"   "Sweden"        1151977 0.1
> "2017-Q2"       "SWE"   "Sweden"        1169243 1.5
> "2017-Q3"       "SWE"   "Sweden"        1177835 0.7
> "2017-Q4"       "SWE"   "Sweden"        1181734 0.3
> "2018-Q1"       "SWE"   "Sweden"        1192111 0.9
> "2018-Q2"       "SWE"   "Sweden"        1197931 0.5
> "2018-Q3"       "SWE"   "Sweden"        1196262 -0.1
> "2018-Q4"       "SWE"   "Sweden"        1209430 1.1
> "2019-Q1"       "SWE"   "Sweden"        1215583 0.5
> "2019-Q2"       "SWE"   "Sweden"        1214691 -0.1
> "2016-Q4"       "CHE"   "Switzerland"   168268.356822   -0.1
> "2017-Q1"       "CHE"   "Switzerland"   168865.076317   0.4
> "2017-Q2"       "CHE"   "Switzerland"   170078.764694   0.7
> "2017-Q3"       "CHE"   "Switzerland"   171405.16327    0.8
> "2017-Q4"       "CHE"   "Switzerland"   172777.427869   0.8
> "2018-Q1"       "CHE"   "Switzerland"   174168.535837   0.8
> "2018-Q2"       "CHE"   "Switzerland"   175400.870886   0.7
> "2018-Q3"       "CHE"   "Switzerland"   175089.0314     -0.2
> "2018-Q4"       "CHE"   "Switzerland"   175664.228343   0.3
> "2019-Q1"       "CHE"   "Switzerland"   176651.744992   0.6
> "2016-Q4"       "GBR"   "United Kingdom"        496470  0.7
> "2017-Q1"       "GBR"   "United Kingdom"        498582  0.4
> "2017-Q2"       "GBR"   "United Kingdom"        499885  0.3
> "2017-Q3"       "GBR"   "United Kingdom"        502473  0.5
> "2017-Q4"       "GBR"   "United Kingdom"        504487  0.4
> "2018-Q1"       "GBR"   "United Kingdom"        504785  0.1
> "2018-Q2"       "GBR"   "United Kingdom"        506842  0.4
> "2018-Q3"       "GBR"   "United Kingdom"        510346  0.7
> "2018-Q4"       "GBR"   "United Kingdom"        511482  0.2
> "2019-Q1"       "GBR"   "United Kingdom"        514019  0.5
> "2019-Q2"       "GBR"   "United Kingdom"        513029  -0.2
> "2016-Q4"       "USA"   "United States" 4456057.75      0.5
> "2017-Q1"       "USA"   "United States" 4481314 0.6
> "2017-Q2"       "USA"   "United States" 4505262 0.5
> "2017-Q3"       "USA"   "United States" 4540889.5       0.8
> "2017-Q4"       "USA"   "United States" 4580616 0.9
> "2018-Q1"       "USA"   "United States" 4609563.5       0.6
> "2018-Q2"       "USA"   "United States" 4649533.75      0.9
> "2018-Q3"       "USA"   "United States" 4683180 0.7
> "2018-Q4"       "USA"   "United States" 4695887 0.3
> "2019-Q1"       "USA"   "United States" 4731820.25      0.8
> "2019-Q2"       "USA"   "United States" 4755955 0.5
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From phii m@iii@g oii phiiipsmith@c@  Sun Aug 25 04:45:32 2019
From: phii m@iii@g oii phiiipsmith@c@ (phii m@iii@g oii phiiipsmith@c@)
Date: Sat, 24 Aug 2019 22:45:32 -0400
Subject: [R] Colouring selected columns in a facetted column chart
In-Reply-To: <CAGgJW77ByTO7CrHyrjd17AiOw2m9Qs5K91TM4J6Df7EX4pvynQ@mail.gmail.com>
References: <ecbd4d10a93deb6d99755ee76c995a14@philipsmith.ca>
 <CAGgJW77ByTO7CrHyrjd17AiOw2m9Qs5K91TM4J6Df7EX4pvynQ@mail.gmail.com>
Message-ID: <e91780dbe3a95b73673e71a0431fd5e5@philipsmith.ca>

Resubmitted as recommended:

I am having difficulty with a chart using ggplot. It is a facetted 
column chart showing GDP growth rates by country. The columns are 
coloured navyblue, except that I want to colour the most recent columns, 
for 2019-Q1 and 2019-Q2, red. For some countries data are available up 
to 2019-Q2 while for others data are only available up to 2019-Q1. My 
code and data frame are shown below and it almost works, but not quite. 
For some reason the red bars for Germany, Korea, Norway, Sweden and 
United Kingdom are slightly off. Any help will be much appreciated.

Here is my reprex:

library(tidyverse)
t1 <- read.table("t1.txt",header=TRUE,sep="\t")
col <- rep("navyblue",nrow(t1))
for (i in 1:nrow(t1)) {
   if((t1$TIME[i]=="2019-Q1" | t1$TIME[i]=="2019-Q2")) {
     col[i] <- "red"}
}
ggplot(t1) +
   geom_col(aes(x=TIME,y=GDPgr),fill=col) +
   facet_wrap(~Country,ncol=3)

Here is my data frame, called "t1.txt", output by dput():

structure(list(TIME = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L,
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L,
7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L,
6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L,
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L,
7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L,
4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L,
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L,
7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L,
5L, 6L, 7L, 8L, 9L, 10L, 11L), .Label = c("2016-Q4", "2017-Q1",
"2017-Q2", "2017-Q3", "2017-Q4", "2018-Q1", "2018-Q2", "2018-Q3",
"2018-Q4", "2019-Q1", "2019-Q2"), class = "factor"), LOCATION = 
structure(c(1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 10L, 10L,
10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 12L, 12L, 12L, 12L, 12L, 12L, 12L,
12L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L,
13L, 13L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L,
15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 16L, 16L,
16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 18L, 18L, 18L, 18L,
18L, 18L, 18L, 18L, 18L, 18L, 17L, 17L, 17L, 17L, 17L, 17L, 17L,
17L, 17L, 17L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L,
19L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 20L, 20L, 20L,
20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 4L, 4L, 4L, 4L, 4L, 4L,
4L, 4L, 4L, 4L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L,
11L, 11L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L
), .Label = c("AUS", "BEL", "CAN", "CHE", "DEU", "DNK", "ESP",
"EU28", "FIN", "FRA", "GBR", "ISR", "ITA", "JPN", "KOR", "NLD",
"NOR", "NZL", "PRT", "SWE", "USA"), class = "factor"), Country = 
structure(c(1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L,
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 10L,
10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 11L, 11L, 11L,
11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 12L, 12L, 12L, 12L, 12L,
12L, 12L, 12L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 13L, 13L, 13L,
13L, 13L, 13L, 13L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L,
14L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 16L, 16L,
16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 17L, 17L, 17L, 17L,
17L, 17L, 17L, 17L, 17L, 17L, 17L, 18L, 18L, 18L, 18L, 18L, 18L,
18L, 18L, 18L, 18L, 18L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L,
19L, 19L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L,
21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L), .Label = 
c("Australia",
"Belgium", "Canada", "Denmark", "European Union (28 countries)",
"Finland", "France", "Germany", "Israel", "Italy", "Japan", "Korea",
"Netherlands", "New Zealand", "Norway", "Portugal", "Spain",
"Sweden", "Switzerland", "United Kingdom", "United States"), class = 
"factor"),
     Value = c(440518, 442141, 445739, 448672, 451302, 455680,
     459697, 461024, 462032, 463907, 106675, 107394, 107828, 108003,
     108744, 109037, 109386, 109676, 110081, 110459, 110680, 493742,
     498719, 504100.5, 505745, 507883, 509758.75, 512958, 515639.25,
     515971.75, 516489.5, 499945, 511319, 505254, 500363, 504837,
     508633, 511901, 513630, 517726, 518368, 3301202.652555, 
3323886.876398,
     3345038.332666, 3367136.027609, 3390431.080785, 3404554.778774,
     3419358.570571, 3430321.169276, 3440915.89772, 3458087.265837,
     3465003.441, 48525, 49368, 49430, 49596, 50153, 50352, 50449,
     50507, 50530, 50822, 551760, 556305, 560160, 563998, 568125,
     569542, 570670, 572387, 574640, 576494, 577905, 716743.4074,
     725268.5864, 729321.5731, 735610.6375, 740991.229, 741969.5787,
     744834.6127, 744065.912, 745603.2305, 748468.2276, 747909.2496,
     307789.55, 308323.023, 311759.624, 315651.46, 319056.442,
     322272.592, 323422.356, 325702.534, 329052.641, 332851.725,
     333686.876, 396162.2, 398379, 399893, 401534, 403053.4, 403937.8,
     403977.3, 403434.2, 403190.7, 403697.9, 403794.7, 130406025,
     131558850, 132121450, 133064400, 133475100, 133386850, 133931825,
     133289800, 133836225, 134777725, 135369050, 431473400, 435435200,
     437712100, 444064400, 443599800, 447909300, 450495800, 452561100,
     456769700, 455081000, 459958000, 178453.593134, 179367.793134,
     180964.533134, 182189.893134, 183625.193134, 184793.473134,
     185981.973134, 186425.153134, 187434.343134, 188324.263134,
     189297.773134, 59062, 59348, 59743, 60320, 60737, 61031,
     61655, 61927, 62282, 62800, 784704, 788709, 794220, 798283,
     800232, 803756, 807187, 810942, 815921, 815323, 44303.821,
     44632.068, 44803.721, 45062.631, 45426.14, 45641.37, 45910.934,
     46027.362, 46203.578, 46453.392, 46685.65896, 279431, 281707,
     284169, 285986, 288064, 289861, 291583, 293145, 294768, 296732,
     298147, 1150761, 1151977, 1169243, 1177835, 1181734, 1192111,
     1197931, 1196262, 1209430, 1215583, 1214691, 168268.356822,
     168865.076317, 170078.764694, 171405.16327, 172777.427869,
     174168.535837, 175400.870886, 175089.0314, 175664.228343,
     176651.744992, 496470, 498582, 499885, 502473, 504487, 504785,
     506842, 510346, 511482, 514019, 513029, 4456057.75, 4481314,
     4505262, 4540889.5, 4580616, 4609563.5, 4649533.75, 4683180,
     4695887, 4731820.25, 4755955), GDPgr = c(1, 0.4, 0.8, 0.7,
     0.6, 1, 0.9, 0.3, 0.2, 0.4, 0.3, 0.7, 0.4, 0.2, 0.7, 0.3,
     0.3, 0.3, 0.4, 0.3, 0.2, 0.6, 1, 1.1, 0.3, 0.4, 0.4, 0.6,
     0.5, 0.1, 0.1, 0.9, 2.3, -1.2, -1, 0.9, 0.8, 0.6, 0.3, 0.8,
     0.1, 0.8, 0.7, 0.6, 0.7, 0.7, 0.4, 0.4, 0.3, 0.3, 0.5, 0.2,
     0.2, 1.7, 0.1, 0.3, 1.1, 0.4, 0.2, 0.1, 0, 0.6, 0.6, 0.8,
     0.7, 0.7, 0.7, 0.2, 0.2, 0.3, 0.4, 0.3, 0.2, 0.4, 1.2, 0.6,
     0.9, 0.7, 0.1, 0.4, -0.1, 0.2, 0.4, -0.1, 0.9, 0.2, 1.1,
     1.2, 1.1, 1, 0.4, 0.7, 1, 1.2, 0.3, 0.5, 0.6, 0.4, 0.4, 0.4,
     0.2, 0, -0.1, -0.1, 0.1, 0, 0.2, 0.9, 0.4, 0.7, 0.3, -0.1,
     0.4, -0.5, 0.4, 0.7, 0.4, 0.8, 0.9, 0.5, 1.5, -0.1, 1, 0.6,
     0.5, 0.9, -0.4, 1.1, 0.9, 0.5, 0.9, 0.7, 0.8, 0.6, 0.6, 0.2,
     0.5, 0.5, 0.5, 0.5, 0.5, 0.7, 1, 0.7, 0.5, 1, 0.4, 0.6, 0.8,
     2, 0.5, 0.7, 0.5, 0.2, 0.4, 0.4, 0.5, 0.6, -0.1, 0.8, 0.7,
     0.4, 0.6, 0.8, 0.5, 0.6, 0.3, 0.4, 0.5, 0.5, 0.6, 0.8, 0.9,
     0.6, 0.7, 0.6, 0.6, 0.5, 0.6, 0.7, 0.5, 0.4, 0.1, 1.5, 0.7,
     0.3, 0.9, 0.5, -0.1, 1.1, 0.5, -0.1, -0.1, 0.4, 0.7, 0.8,
     0.8, 0.8, 0.7, -0.2, 0.3, 0.6, 0.7, 0.4, 0.3, 0.5, 0.4, 0.1,
     0.4, 0.7, 0.2, 0.5, -0.2, 0.5, 0.6, 0.5, 0.8, 0.9, 0.6, 0.9,
     0.7, 0.3, 0.8, 0.5)), class = "data.frame", row.names = c(NA,
-224L))



On 2019-08-24 22:39, Eric Berger wrote:
> Hi Phil,
> Please resubmit your question with the data frame contents shown as
> the output from the command
> dput(t1.txt). This will make it easier for people to run your reprex
> and respond to your question.
> 
> Best,
> Eric
> 
> On Sun, Aug 25, 2019 at 5:26 AM <phil at philipsmith.ca> wrote:
> 
>> I am having difficulty with a chart using ggplot. It is a facetted
>> column chart showing GDP growth rates by country. The columns are
>> coloured navyblue, except that I want to colour the most recent
>> columns,
>> for 2019-Q1 and 2019-Q2, red. For some countries data are available
>> up
>> to 2019-Q2 while for others data are only available up to 2019-Q1.
>> My
>> code and data frame are shown below and it almost works, but not
>> quite.
>> For some reason the red bars for Germany, Korea, Norway, Sweden and
>> United Kingdom are slightly off. Any help will be much appreciated.
>> 
>> Here is my reprex:
>> 
>> library(tidyverse)
>> t1 <- read.table("t1.txt",header=TRUE,sep="\t")
>> col <- rep("navyblue",nrow(t1))
>> for (i in 1:nrow(t1)) {
>> if((t1$TIME[i]=="2019-Q1" | t1$TIME[i]=="2019-Q2")) {
>> col[i] <- "red"}
>> }
>> ggplot(t1) +
>> geom_col(aes(x=TIME,y=GDPgr),fill=col) +
>> facet_wrap(~Country,ncol=3)
>> 
>> Here is my data frame, called "t1.txt":
>> 
>> "TIME"  "LOCATION"      "Country"       "Value" "GDPgr"
>> "2016-Q4"       "AUS"   "Australia"     440518  1
>> "2017-Q1"       "AUS"   "Australia"     442141  0.4
>> "2017-Q2"       "AUS"   "Australia"     445739  0.8
>> "2017-Q3"       "AUS"   "Australia"     448672  0.7
>> "2017-Q4"       "AUS"   "Australia"     451302  0.6
>> "2018-Q1"       "AUS"   "Australia"     455680  1
>> "2018-Q2"       "AUS"   "Australia"     459697  0.9
>> "2018-Q3"       "AUS"   "Australia"     461024  0.3
>> "2018-Q4"       "AUS"   "Australia"     462032  0.2
>> "2019-Q1"       "AUS"   "Australia"     463907  0.4
>> "2016-Q4"       "BEL"   "Belgium"       106675  0.3
>> "2017-Q1"       "BEL"   "Belgium"       107394  0.7
>> "2017-Q2"       "BEL"   "Belgium"       107828  0.4
>> "2017-Q3"       "BEL"   "Belgium"       108003  0.2
>> "2017-Q4"       "BEL"   "Belgium"       108744  0.7
>> "2018-Q1"       "BEL"   "Belgium"       109037  0.3
>> "2018-Q2"       "BEL"   "Belgium"       109386  0.3
>> "2018-Q3"       "BEL"   "Belgium"       109676  0.3
>> "2018-Q4"       "BEL"   "Belgium"       110081  0.4
>> "2019-Q1"       "BEL"   "Belgium"       110459  0.3
>> "2019-Q2"       "BEL"   "Belgium"       110680  0.2
>> "2016-Q4"       "CAN"   "Canada"        493742  0.6
>> "2017-Q1"       "CAN"   "Canada"        498719  1
>> "2017-Q2"       "CAN"   "Canada"        504100.5        1.1
>> "2017-Q3"       "CAN"   "Canada"        505745  0.3
>> "2017-Q4"       "CAN"   "Canada"        507883  0.4
>> "2018-Q1"       "CAN"   "Canada"        509758.75       0.4
>> "2018-Q2"       "CAN"   "Canada"        512958  0.6
>> "2018-Q3"       "CAN"   "Canada"        515639.25       0.5
>> "2018-Q4"       "CAN"   "Canada"        515971.75       0.1
>> "2019-Q1"       "CAN"   "Canada"        516489.5        0.1
>> "2016-Q4"       "DNK"   "Denmark"       499945  0.9
>> "2017-Q1"       "DNK"   "Denmark"       511319  2.3
>> "2017-Q2"       "DNK"   "Denmark"       505254  -1.2
>> "2017-Q3"       "DNK"   "Denmark"       500363  -1
>> "2017-Q4"       "DNK"   "Denmark"       504837  0.9
>> "2018-Q1"       "DNK"   "Denmark"       508633  0.8
>> "2018-Q2"       "DNK"   "Denmark"       511901  0.6
>> "2018-Q3"       "DNK"   "Denmark"       513630  0.3
>> "2018-Q4"       "DNK"   "Denmark"       517726  0.8
>> "2019-Q1"       "DNK"   "Denmark"       518368  0.1
>> "2016-Q4"       "EU28"  "European Union (28 countries)"
>> 3301202.652555  0.8
>> "2017-Q1"       "EU28"  "European Union (28 countries)"
>> 3323886.876398  0.7
>> "2017-Q2"       "EU28"  "European Union (28 countries)"
>> 3345038.332666  0.6
>> "2017-Q3"       "EU28"  "European Union (28 countries)"
>> 3367136.027609  0.7
>> "2017-Q4"       "EU28"  "European Union (28 countries)"
>> 3390431.080785  0.7
>> "2018-Q1"       "EU28"  "European Union (28 countries)"
>> 3404554.778774  0.4
>> "2018-Q2"       "EU28"  "European Union (28 countries)"
>> 3419358.570571  0.4
>> "2018-Q3"       "EU28"  "European Union (28 countries)"
>> 3430321.169276  0.3
>> "2018-Q4"       "EU28"  "European Union (28 countries)"
>> 3440915.89772   0.3
>> "2019-Q1"       "EU28"  "European Union (28 countries)"
>> 3458087.265837  0.5
>> "2019-Q2"       "EU28"  "European Union (28 countries)" 3465003.441
>> 0.2
>> "2016-Q4"       "FIN"   "Finland"       48525   0.2
>> "2017-Q1"       "FIN"   "Finland"       49368   1.7
>> "2017-Q2"       "FIN"   "Finland"       49430   0.1
>> "2017-Q3"       "FIN"   "Finland"       49596   0.3
>> "2017-Q4"       "FIN"   "Finland"       50153   1.1
>> "2018-Q1"       "FIN"   "Finland"       50352   0.4
>> "2018-Q2"       "FIN"   "Finland"       50449   0.2
>> "2018-Q3"       "FIN"   "Finland"       50507   0.1
>> "2018-Q4"       "FIN"   "Finland"       50530   0
>> "2019-Q1"       "FIN"   "Finland"       50822   0.6
>> "2016-Q4"       "FRA"   "France"        551760  0.6
>> "2017-Q1"       "FRA"   "France"        556305  0.8
>> "2017-Q2"       "FRA"   "France"        560160  0.7
>> "2017-Q3"       "FRA"   "France"        563998  0.7
>> "2017-Q4"       "FRA"   "France"        568125  0.7
>> "2018-Q1"       "FRA"   "France"        569542  0.2
>> "2018-Q2"       "FRA"   "France"        570670  0.2
>> "2018-Q3"       "FRA"   "France"        572387  0.3
>> "2018-Q4"       "FRA"   "France"        574640  0.4
>> "2019-Q1"       "FRA"   "France"        576494  0.3
>> "2019-Q2"       "FRA"   "France"        577905  0.2
>> "2016-Q4"       "DEU"   "Germany"       716743.4074     0.4
>> "2017-Q1"       "DEU"   "Germany"       725268.5864     1.2
>> "2017-Q2"       "DEU"   "Germany"       729321.5731     0.6
>> "2017-Q3"       "DEU"   "Germany"       735610.6375     0.9
>> "2017-Q4"       "DEU"   "Germany"       740991.229      0.7
>> "2018-Q1"       "DEU"   "Germany"       741969.5787     0.1
>> "2018-Q2"       "DEU"   "Germany"       744834.6127     0.4
>> "2018-Q3"       "DEU"   "Germany"       744065.912      -0.1
>> "2018-Q4"       "DEU"   "Germany"       745603.2305     0.2
>> "2019-Q1"       "DEU"   "Germany"       748468.2276     0.4
>> "2019-Q2"       "DEU"   "Germany"       747909.2496     -0.1
>> "2016-Q4"       "ISR"   "Israel"        307789.55       0.9
>> "2017-Q1"       "ISR"   "Israel"        308323.023      0.2
>> "2017-Q2"       "ISR"   "Israel"        311759.624      1.1
>> "2017-Q3"       "ISR"   "Israel"        315651.46       1.2
>> "2017-Q4"       "ISR"   "Israel"        319056.442      1.1
>> "2018-Q1"       "ISR"   "Israel"        322272.592      1
>> "2018-Q2"       "ISR"   "Israel"        323422.356      0.4
>> "2018-Q3"       "ISR"   "Israel"        325702.534      0.7
>> "2018-Q4"       "ISR"   "Israel"        329052.641      1
>> "2019-Q1"       "ISR"   "Israel"        332851.725      1.2
>> "2019-Q2"       "ISR"   "Israel"        333686.876      0.3
>> "2016-Q4"       "ITA"   "Italy" 396162.2        0.5
>> "2017-Q1"       "ITA"   "Italy" 398379  0.6
>> "2017-Q2"       "ITA"   "Italy" 399893  0.4
>> "2017-Q3"       "ITA"   "Italy" 401534  0.4
>> "2017-Q4"       "ITA"   "Italy" 403053.4        0.4
>> "2018-Q1"       "ITA"   "Italy" 403937.8        0.2
>> "2018-Q2"       "ITA"   "Italy" 403977.3        0
>> "2018-Q3"       "ITA"   "Italy" 403434.2        -0.1
>> "2018-Q4"       "ITA"   "Italy" 403190.7        -0.1
>> "2019-Q1"       "ITA"   "Italy" 403697.9        0.1
>> "2019-Q2"       "ITA"   "Italy" 403794.7        0
>> "2016-Q4"       "JPN"   "Japan" 130406025       0.2
>> "2017-Q1"       "JPN"   "Japan" 131558850       0.9
>> "2017-Q2"       "JPN"   "Japan" 132121450       0.4
>> "2017-Q3"       "JPN"   "Japan" 133064400       0.7
>> "2017-Q4"       "JPN"   "Japan" 133475100       0.3
>> "2018-Q1"       "JPN"   "Japan" 133386850       -0.1
>> "2018-Q2"       "JPN"   "Japan" 133931825       0.4
>> "2018-Q3"       "JPN"   "Japan" 133289800       -0.5
>> "2018-Q4"       "JPN"   "Japan" 133836225       0.4
>> "2019-Q1"       "JPN"   "Japan" 134777725       0.7
>> "2019-Q2"       "JPN"   "Japan" 135369050       0.4
>> "2016-Q4"       "KOR"   "Korea" 431473400       0.8
>> "2017-Q1"       "KOR"   "Korea" 435435200       0.9
>> "2017-Q2"       "KOR"   "Korea" 437712100       0.5
>> "2017-Q3"       "KOR"   "Korea" 444064400       1.5
>> "2017-Q4"       "KOR"   "Korea" 443599800       -0.1
>> "2018-Q1"       "KOR"   "Korea" 447909300       1
>> "2018-Q2"       "KOR"   "Korea" 450495800       0.6
>> "2018-Q3"       "KOR"   "Korea" 452561100       0.5
>> "2018-Q4"       "KOR"   "Korea" 456769700       0.9
>> "2019-Q1"       "KOR"   "Korea" 455081000       -0.4
>> "2019-Q2"       "KOR"   "Korea" 459958000       1.1
>> "2016-Q4"       "NLD"   "Netherlands"   178453.593134   0.9
>> "2017-Q1"       "NLD"   "Netherlands"   179367.793134   0.5
>> "2017-Q2"       "NLD"   "Netherlands"   180964.533134   0.9
>> "2017-Q3"       "NLD"   "Netherlands"   182189.893134   0.7
>> "2017-Q4"       "NLD"   "Netherlands"   183625.193134   0.8
>> "2018-Q1"       "NLD"   "Netherlands"   184793.473134   0.6
>> "2018-Q2"       "NLD"   "Netherlands"   185981.973134   0.6
>> "2018-Q3"       "NLD"   "Netherlands"   186425.153134   0.2
>> "2018-Q4"       "NLD"   "Netherlands"   187434.343134   0.5
>> "2019-Q1"       "NLD"   "Netherlands"   188324.263134   0.5
>> "2019-Q2"       "NLD"   "Netherlands"   189297.773134   0.5
>> "2016-Q4"       "NZL"   "New Zealand"   59062   0.5
>> "2017-Q1"       "NZL"   "New Zealand"   59348   0.5
>> "2017-Q2"       "NZL"   "New Zealand"   59743   0.7
>> "2017-Q3"       "NZL"   "New Zealand"   60320   1
>> "2017-Q4"       "NZL"   "New Zealand"   60737   0.7
>> "2018-Q1"       "NZL"   "New Zealand"   61031   0.5
>> "2018-Q2"       "NZL"   "New Zealand"   61655   1
>> "2018-Q3"       "NZL"   "New Zealand"   61927   0.4
>> "2018-Q4"       "NZL"   "New Zealand"   62282   0.6
>> "2019-Q1"       "NZL"   "New Zealand"   62800   0.8
>> "2016-Q4"       "NOR"   "Norway"        784704  2
>> "2017-Q1"       "NOR"   "Norway"        788709  0.5
>> "2017-Q2"       "NOR"   "Norway"        794220  0.7
>> "2017-Q3"       "NOR"   "Norway"        798283  0.5
>> "2017-Q4"       "NOR"   "Norway"        800232  0.2
>> "2018-Q1"       "NOR"   "Norway"        803756  0.4
>> "2018-Q2"       "NOR"   "Norway"        807187  0.4
>> "2018-Q3"       "NOR"   "Norway"        810942  0.5
>> "2018-Q4"       "NOR"   "Norway"        815921  0.6
>> "2019-Q1"       "NOR"   "Norway"        815323  -0.1
>> "2016-Q4"       "PRT"   "Portugal"      44303.821       0.8
>> "2017-Q1"       "PRT"   "Portugal"      44632.068       0.7
>> "2017-Q2"       "PRT"   "Portugal"      44803.721       0.4
>> "2017-Q3"       "PRT"   "Portugal"      45062.631       0.6
>> "2017-Q4"       "PRT"   "Portugal"      45426.14        0.8
>> "2018-Q1"       "PRT"   "Portugal"      45641.37        0.5
>> "2018-Q2"       "PRT"   "Portugal"      45910.934       0.6
>> "2018-Q3"       "PRT"   "Portugal"      46027.362       0.3
>> "2018-Q4"       "PRT"   "Portugal"      46203.578       0.4
>> "2019-Q1"       "PRT"   "Portugal"      46453.392       0.5
>> "2019-Q2"       "PRT"   "Portugal"      46685.65896     0.5
>> "2016-Q4"       "ESP"   "Spain" 279431  0.6
>> "2017-Q1"       "ESP"   "Spain" 281707  0.8
>> "2017-Q2"       "ESP"   "Spain" 284169  0.9
>> "2017-Q3"       "ESP"   "Spain" 285986  0.6
>> "2017-Q4"       "ESP"   "Spain" 288064  0.7
>> "2018-Q1"       "ESP"   "Spain" 289861  0.6
>> "2018-Q2"       "ESP"   "Spain" 291583  0.6
>> "2018-Q3"       "ESP"   "Spain" 293145  0.5
>> "2018-Q4"       "ESP"   "Spain" 294768  0.6
>> "2019-Q1"       "ESP"   "Spain" 296732  0.7
>> "2019-Q2"       "ESP"   "Spain" 298147  0.5
>> "2016-Q4"       "SWE"   "Sweden"        1150761 0.4
>> "2017-Q1"       "SWE"   "Sweden"        1151977 0.1
>> "2017-Q2"       "SWE"   "Sweden"        1169243 1.5
>> "2017-Q3"       "SWE"   "Sweden"        1177835 0.7
>> "2017-Q4"       "SWE"   "Sweden"        1181734 0.3
>> "2018-Q1"       "SWE"   "Sweden"        1192111 0.9
>> "2018-Q2"       "SWE"   "Sweden"        1197931 0.5
>> "2018-Q3"       "SWE"   "Sweden"        1196262 -0.1
>> "2018-Q4"       "SWE"   "Sweden"        1209430 1.1
>> "2019-Q1"       "SWE"   "Sweden"        1215583 0.5
>> "2019-Q2"       "SWE"   "Sweden"        1214691 -0.1
>> "2016-Q4"       "CHE"   "Switzerland"   168268.356822   -0.1
>> "2017-Q1"       "CHE"   "Switzerland"   168865.076317   0.4
>> "2017-Q2"       "CHE"   "Switzerland"   170078.764694   0.7
>> "2017-Q3"       "CHE"   "Switzerland"   171405.16327    0.8
>> "2017-Q4"       "CHE"   "Switzerland"   172777.427869   0.8
>> "2018-Q1"       "CHE"   "Switzerland"   174168.535837   0.8
>> "2018-Q2"       "CHE"   "Switzerland"   175400.870886   0.7
>> "2018-Q3"       "CHE"   "Switzerland"   175089.0314     -0.2
>> "2018-Q4"       "CHE"   "Switzerland"   175664.228343   0.3
>> "2019-Q1"       "CHE"   "Switzerland"   176651.744992   0.6
>> "2016-Q4"       "GBR"   "United Kingdom"        496470  0.7
>> "2017-Q1"       "GBR"   "United Kingdom"        498582  0.4
>> "2017-Q2"       "GBR"   "United Kingdom"        499885  0.3
>> "2017-Q3"       "GBR"   "United Kingdom"        502473  0.5
>> "2017-Q4"       "GBR"   "United Kingdom"        504487  0.4
>> "2018-Q1"       "GBR"   "United Kingdom"        504785  0.1
>> "2018-Q2"       "GBR"   "United Kingdom"        506842  0.4
>> "2018-Q3"       "GBR"   "United Kingdom"        510346  0.7
>> "2018-Q4"       "GBR"   "United Kingdom"        511482  0.2
>> "2019-Q1"       "GBR"   "United Kingdom"        514019  0.5
>> "2019-Q2"       "GBR"   "United Kingdom"        513029  -0.2
>> "2016-Q4"       "USA"   "United States" 4456057.75      0.5
>> "2017-Q1"       "USA"   "United States" 4481314 0.6
>> "2017-Q2"       "USA"   "United States" 4505262 0.5
>> "2017-Q3"       "USA"   "United States" 4540889.5       0.8
>> "2017-Q4"       "USA"   "United States" 4580616 0.9
>> "2018-Q1"       "USA"   "United States" 4609563.5       0.6
>> "2018-Q2"       "USA"   "United States" 4649533.75      0.9
>> "2018-Q3"       "USA"   "United States" 4683180 0.7
>> "2018-Q4"       "USA"   "United States" 4695887 0.3
>> "2019-Q1"       "USA"   "United States" 4731820.25      0.8
>> "2019-Q2"       "USA"   "United States" 4755955 0.5
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From er|cjberger @end|ng |rom gm@||@com  Sun Aug 25 05:21:42 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Sun, 25 Aug 2019 06:21:42 +0300
Subject: [R] Colouring selected columns in a facetted column chart
In-Reply-To: <e91780dbe3a95b73673e71a0431fd5e5@philipsmith.ca>
References: <ecbd4d10a93deb6d99755ee76c995a14@philipsmith.ca>
 <CAGgJW77ByTO7CrHyrjd17AiOw2m9Qs5K91TM4J6Df7EX4pvynQ@mail.gmail.com>
 <e91780dbe3a95b73673e71a0431fd5e5@philipsmith.ca>
Message-ID: <CAGgJW75BGt=1oGh8yt0fu3DkhN+vZdww60W1THQ8B-uKdf0S6g@mail.gmail.com>

This seems to work
ggplot(t1) +
  geom_col(aes(x=TIME,y=GDPgr,fill=col),show.legend=FALSE) +
     scale_fill_manual(values=c("navyblue","red")) +
         facet_wrap(~Country,ncol=3)

HTH,
Eric


On Sun, Aug 25, 2019 at 5:45 AM <phil at philipsmith.ca> wrote:

> Resubmitted as recommended:
>
> I am having difficulty with a chart using ggplot. It is a facetted
> column chart showing GDP growth rates by country. The columns are
> coloured navyblue, except that I want to colour the most recent columns,
> for 2019-Q1 and 2019-Q2, red. For some countries data are available up
> to 2019-Q2 while for others data are only available up to 2019-Q1. My
> code and data frame are shown below and it almost works, but not quite.
> For some reason the red bars for Germany, Korea, Norway, Sweden and
> United Kingdom are slightly off. Any help will be much appreciated.
>
> Here is my reprex:
>
> library(tidyverse)
> t1 <- read.table("t1.txt",header=TRUE,sep="\t")
> col <- rep("navyblue",nrow(t1))
> for (i in 1:nrow(t1)) {
>    if((t1$TIME[i]=="2019-Q1" | t1$TIME[i]=="2019-Q2")) {
>      col[i] <- "red"}
> }
> ggplot(t1) +
>    geom_col(aes(x=TIME,y=GDPgr),fill=col) +
>    facet_wrap(~Country,ncol=3)
>
> Here is my data frame, called "t1.txt", output by dput():
>
> structure(list(TIME = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L,
> 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L,
> 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L,
> 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
> 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L,
> 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
> 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L,
> 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L,
> 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
> 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L,
> 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
> 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L,
> 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L,
> 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
> 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L,
> 5L, 6L, 7L, 8L, 9L, 10L, 11L), .Label = c("2016-Q4", "2017-Q1",
> "2017-Q2", "2017-Q3", "2017-Q4", "2018-Q1", "2018-Q2", "2018-Q3",
> "2018-Q4", "2019-Q1", "2019-Q2"), class = "factor"), LOCATION =
> structure(c(1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 6L, 6L,
> 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
> 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 10L, 10L,
> 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 5L, 5L, 5L, 5L,
> 5L, 5L, 5L, 5L, 5L, 5L, 5L, 12L, 12L, 12L, 12L, 12L, 12L, 12L,
> 12L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L,
> 13L, 13L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L,
> 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 16L, 16L,
> 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 18L, 18L, 18L, 18L,
> 18L, 18L, 18L, 18L, 18L, 18L, 17L, 17L, 17L, 17L, 17L, 17L, 17L,
> 17L, 17L, 17L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L,
> 19L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 20L, 20L, 20L,
> 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 4L, 4L, 4L, 4L, 4L, 4L,
> 4L, 4L, 4L, 4L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L,
> 11L, 11L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L
> ), .Label = c("AUS", "BEL", "CAN", "CHE", "DEU", "DNK", "ESP",
> "EU28", "FIN", "FRA", "GBR", "ISR", "ITA", "JPN", "KOR", "NLD",
> "NOR", "NZL", "PRT", "SWE", "USA"), class = "factor"), Country =
> structure(c(1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L,
> 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
> 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L,
> 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
> 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 10L,
> 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 11L, 11L, 11L,
> 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 12L, 12L, 12L, 12L, 12L,
> 12L, 12L, 12L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 13L, 13L, 13L,
> 13L, 13L, 13L, 13L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L,
> 14L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 16L, 16L,
> 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 17L, 17L, 17L, 17L,
> 17L, 17L, 17L, 17L, 17L, 17L, 17L, 18L, 18L, 18L, 18L, 18L, 18L,
> 18L, 18L, 18L, 18L, 18L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L,
> 19L, 19L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L,
> 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L), .Label =
> c("Australia",
> "Belgium", "Canada", "Denmark", "European Union (28 countries)",
> "Finland", "France", "Germany", "Israel", "Italy", "Japan", "Korea",
> "Netherlands", "New Zealand", "Norway", "Portugal", "Spain",
> "Sweden", "Switzerland", "United Kingdom", "United States"), class =
> "factor"),
>      Value = c(440518, 442141, 445739, 448672, 451302, 455680,
>      459697, 461024, 462032, 463907, 106675, 107394, 107828, 108003,
>      108744, 109037, 109386, 109676, 110081, 110459, 110680, 493742,
>      498719, 504100.5, 505745, 507883, 509758.75, 512958, 515639.25,
>      515971.75, 516489.5, 499945, 511319, 505254, 500363, 504837,
>      508633, 511901, 513630, 517726, 518368, 3301202.652555,
> 3323886.876398,
>      3345038.332666, 3367136.027609, 3390431.080785, 3404554.778774,
>      3419358.570571, 3430321.169276, 3440915.89772, 3458087.265837,
>      3465003.441, 48525, 49368, 49430, 49596, 50153, 50352, 50449,
>      50507, 50530, 50822, 551760, 556305, 560160, 563998, 568125,
>      569542, 570670, 572387, 574640, 576494, 577905, 716743.4074,
>      725268.5864, 729321.5731, 735610.6375, 740991.229, 741969.5787,
>      744834.6127, 744065.912, 745603.2305, 748468.2276, 747909.2496,
>      307789.55, 308323.023, 311759.624, 315651.46, 319056.442,
>      322272.592, 323422.356, 325702.534, 329052.641, 332851.725,
>      333686.876, 396162.2, 398379, 399893, 401534, 403053.4, 403937.8,
>      403977.3, 403434.2, 403190.7, 403697.9, 403794.7, 130406025,
>      131558850, 132121450, 133064400, 133475100, 133386850, 133931825,
>      133289800, 133836225, 134777725, 135369050, 431473400, 435435200,
>      437712100, 444064400, 443599800, 447909300, 450495800, 452561100,
>      456769700, 455081000, 459958000, 178453.593134, 179367.793134,
>      180964.533134, 182189.893134, 183625.193134, 184793.473134,
>      185981.973134, 186425.153134, 187434.343134, 188324.263134,
>      189297.773134, 59062, 59348, 59743, 60320, 60737, 61031,
>      61655, 61927, 62282, 62800, 784704, 788709, 794220, 798283,
>      800232, 803756, 807187, 810942, 815921, 815323, 44303.821,
>      44632.068, 44803.721, 45062.631, 45426.14, 45641.37, 45910.934,
>      46027.362, 46203.578, 46453.392, 46685.65896, 279431, 281707,
>      284169, 285986, 288064, 289861, 291583, 293145, 294768, 296732,
>      298147, 1150761, 1151977, 1169243, 1177835, 1181734, 1192111,
>      1197931, 1196262, 1209430, 1215583, 1214691, 168268.356822,
>      168865.076317, 170078.764694, 171405.16327, 172777.427869,
>      174168.535837, 175400.870886, 175089.0314, 175664.228343,
>      176651.744992, 496470, 498582, 499885, 502473, 504487, 504785,
>      506842, 510346, 511482, 514019, 513029, 4456057.75, 4481314,
>      4505262, 4540889.5, 4580616, 4609563.5, 4649533.75, 4683180,
>      4695887, 4731820.25, 4755955), GDPgr = c(1, 0.4, 0.8, 0.7,
>      0.6, 1, 0.9, 0.3, 0.2, 0.4, 0.3, 0.7, 0.4, 0.2, 0.7, 0.3,
>      0.3, 0.3, 0.4, 0.3, 0.2, 0.6, 1, 1.1, 0.3, 0.4, 0.4, 0.6,
>      0.5, 0.1, 0.1, 0.9, 2.3, -1.2, -1, 0.9, 0.8, 0.6, 0.3, 0.8,
>      0.1, 0.8, 0.7, 0.6, 0.7, 0.7, 0.4, 0.4, 0.3, 0.3, 0.5, 0.2,
>      0.2, 1.7, 0.1, 0.3, 1.1, 0.4, 0.2, 0.1, 0, 0.6, 0.6, 0.8,
>      0.7, 0.7, 0.7, 0.2, 0.2, 0.3, 0.4, 0.3, 0.2, 0.4, 1.2, 0.6,
>      0.9, 0.7, 0.1, 0.4, -0.1, 0.2, 0.4, -0.1, 0.9, 0.2, 1.1,
>      1.2, 1.1, 1, 0.4, 0.7, 1, 1.2, 0.3, 0.5, 0.6, 0.4, 0.4, 0.4,
>      0.2, 0, -0.1, -0.1, 0.1, 0, 0.2, 0.9, 0.4, 0.7, 0.3, -0.1,
>      0.4, -0.5, 0.4, 0.7, 0.4, 0.8, 0.9, 0.5, 1.5, -0.1, 1, 0.6,
>      0.5, 0.9, -0.4, 1.1, 0.9, 0.5, 0.9, 0.7, 0.8, 0.6, 0.6, 0.2,
>      0.5, 0.5, 0.5, 0.5, 0.5, 0.7, 1, 0.7, 0.5, 1, 0.4, 0.6, 0.8,
>      2, 0.5, 0.7, 0.5, 0.2, 0.4, 0.4, 0.5, 0.6, -0.1, 0.8, 0.7,
>      0.4, 0.6, 0.8, 0.5, 0.6, 0.3, 0.4, 0.5, 0.5, 0.6, 0.8, 0.9,
>      0.6, 0.7, 0.6, 0.6, 0.5, 0.6, 0.7, 0.5, 0.4, 0.1, 1.5, 0.7,
>      0.3, 0.9, 0.5, -0.1, 1.1, 0.5, -0.1, -0.1, 0.4, 0.7, 0.8,
>      0.8, 0.8, 0.7, -0.2, 0.3, 0.6, 0.7, 0.4, 0.3, 0.5, 0.4, 0.1,
>      0.4, 0.7, 0.2, 0.5, -0.2, 0.5, 0.6, 0.5, 0.8, 0.9, 0.6, 0.9,
>      0.7, 0.3, 0.8, 0.5)), class = "data.frame", row.names = c(NA,
> -224L))
>
>
>
> On 2019-08-24 22:39, Eric Berger wrote:
> > Hi Phil,
> > Please resubmit your question with the data frame contents shown as
> > the output from the command
> > dput(t1.txt). This will make it easier for people to run your reprex
> > and respond to your question.
> >
> > Best,
> > Eric
> >
> > On Sun, Aug 25, 2019 at 5:26 AM <phil at philipsmith.ca> wrote:
> >
> >> I am having difficulty with a chart using ggplot. It is a facetted
> >> column chart showing GDP growth rates by country. The columns are
> >> coloured navyblue, except that I want to colour the most recent
> >> columns,
> >> for 2019-Q1 and 2019-Q2, red. For some countries data are available
> >> up
> >> to 2019-Q2 while for others data are only available up to 2019-Q1.
> >> My
> >> code and data frame are shown below and it almost works, but not
> >> quite.
> >> For some reason the red bars for Germany, Korea, Norway, Sweden and
> >> United Kingdom are slightly off. Any help will be much appreciated.
> >>
> >> Here is my reprex:
> >>
> >> library(tidyverse)
> >> t1 <- read.table("t1.txt",header=TRUE,sep="\t")
> >> col <- rep("navyblue",nrow(t1))
> >> for (i in 1:nrow(t1)) {
> >> if((t1$TIME[i]=="2019-Q1" | t1$TIME[i]=="2019-Q2")) {
> >> col[i] <- "red"}
> >> }
> >> ggplot(t1) +
> >> geom_col(aes(x=TIME,y=GDPgr),fill=col) +
> >> facet_wrap(~Country,ncol=3)
> >>
> >> Here is my data frame, called "t1.txt":
> >>
> >> "TIME"  "LOCATION"      "Country"       "Value" "GDPgr"
> >> "2016-Q4"       "AUS"   "Australia"     440518  1
> >> "2017-Q1"       "AUS"   "Australia"     442141  0.4
> >> "2017-Q2"       "AUS"   "Australia"     445739  0.8
> >> "2017-Q3"       "AUS"   "Australia"     448672  0.7
> >> "2017-Q4"       "AUS"   "Australia"     451302  0.6
> >> "2018-Q1"       "AUS"   "Australia"     455680  1
> >> "2018-Q2"       "AUS"   "Australia"     459697  0.9
> >> "2018-Q3"       "AUS"   "Australia"     461024  0.3
> >> "2018-Q4"       "AUS"   "Australia"     462032  0.2
> >> "2019-Q1"       "AUS"   "Australia"     463907  0.4
> >> "2016-Q4"       "BEL"   "Belgium"       106675  0.3
> >> "2017-Q1"       "BEL"   "Belgium"       107394  0.7
> >> "2017-Q2"       "BEL"   "Belgium"       107828  0.4
> >> "2017-Q3"       "BEL"   "Belgium"       108003  0.2
> >> "2017-Q4"       "BEL"   "Belgium"       108744  0.7
> >> "2018-Q1"       "BEL"   "Belgium"       109037  0.3
> >> "2018-Q2"       "BEL"   "Belgium"       109386  0.3
> >> "2018-Q3"       "BEL"   "Belgium"       109676  0.3
> >> "2018-Q4"       "BEL"   "Belgium"       110081  0.4
> >> "2019-Q1"       "BEL"   "Belgium"       110459  0.3
> >> "2019-Q2"       "BEL"   "Belgium"       110680  0.2
> >> "2016-Q4"       "CAN"   "Canada"        493742  0.6
> >> "2017-Q1"       "CAN"   "Canada"        498719  1
> >> "2017-Q2"       "CAN"   "Canada"        504100.5        1.1
> >> "2017-Q3"       "CAN"   "Canada"        505745  0.3
> >> "2017-Q4"       "CAN"   "Canada"        507883  0.4
> >> "2018-Q1"       "CAN"   "Canada"        509758.75       0.4
> >> "2018-Q2"       "CAN"   "Canada"        512958  0.6
> >> "2018-Q3"       "CAN"   "Canada"        515639.25       0.5
> >> "2018-Q4"       "CAN"   "Canada"        515971.75       0.1
> >> "2019-Q1"       "CAN"   "Canada"        516489.5        0.1
> >> "2016-Q4"       "DNK"   "Denmark"       499945  0.9
> >> "2017-Q1"       "DNK"   "Denmark"       511319  2.3
> >> "2017-Q2"       "DNK"   "Denmark"       505254  -1.2
> >> "2017-Q3"       "DNK"   "Denmark"       500363  -1
> >> "2017-Q4"       "DNK"   "Denmark"       504837  0.9
> >> "2018-Q1"       "DNK"   "Denmark"       508633  0.8
> >> "2018-Q2"       "DNK"   "Denmark"       511901  0.6
> >> "2018-Q3"       "DNK"   "Denmark"       513630  0.3
> >> "2018-Q4"       "DNK"   "Denmark"       517726  0.8
> >> "2019-Q1"       "DNK"   "Denmark"       518368  0.1
> >> "2016-Q4"       "EU28"  "European Union (28 countries)"
> >> 3301202.652555  0.8
> >> "2017-Q1"       "EU28"  "European Union (28 countries)"
> >> 3323886.876398  0.7
> >> "2017-Q2"       "EU28"  "European Union (28 countries)"
> >> 3345038.332666  0.6
> >> "2017-Q3"       "EU28"  "European Union (28 countries)"
> >> 3367136.027609  0.7
> >> "2017-Q4"       "EU28"  "European Union (28 countries)"
> >> 3390431.080785  0.7
> >> "2018-Q1"       "EU28"  "European Union (28 countries)"
> >> 3404554.778774  0.4
> >> "2018-Q2"       "EU28"  "European Union (28 countries)"
> >> 3419358.570571  0.4
> >> "2018-Q3"       "EU28"  "European Union (28 countries)"
> >> 3430321.169276  0.3
> >> "2018-Q4"       "EU28"  "European Union (28 countries)"
> >> 3440915.89772   0.3
> >> "2019-Q1"       "EU28"  "European Union (28 countries)"
> >> 3458087.265837  0.5
> >> "2019-Q2"       "EU28"  "European Union (28 countries)" 3465003.441
> >> 0.2
> >> "2016-Q4"       "FIN"   "Finland"       48525   0.2
> >> "2017-Q1"       "FIN"   "Finland"       49368   1.7
> >> "2017-Q2"       "FIN"   "Finland"       49430   0.1
> >> "2017-Q3"       "FIN"   "Finland"       49596   0.3
> >> "2017-Q4"       "FIN"   "Finland"       50153   1.1
> >> "2018-Q1"       "FIN"   "Finland"       50352   0.4
> >> "2018-Q2"       "FIN"   "Finland"       50449   0.2
> >> "2018-Q3"       "FIN"   "Finland"       50507   0.1
> >> "2018-Q4"       "FIN"   "Finland"       50530   0
> >> "2019-Q1"       "FIN"   "Finland"       50822   0.6
> >> "2016-Q4"       "FRA"   "France"        551760  0.6
> >> "2017-Q1"       "FRA"   "France"        556305  0.8
> >> "2017-Q2"       "FRA"   "France"        560160  0.7
> >> "2017-Q3"       "FRA"   "France"        563998  0.7
> >> "2017-Q4"       "FRA"   "France"        568125  0.7
> >> "2018-Q1"       "FRA"   "France"        569542  0.2
> >> "2018-Q2"       "FRA"   "France"        570670  0.2
> >> "2018-Q3"       "FRA"   "France"        572387  0.3
> >> "2018-Q4"       "FRA"   "France"        574640  0.4
> >> "2019-Q1"       "FRA"   "France"        576494  0.3
> >> "2019-Q2"       "FRA"   "France"        577905  0.2
> >> "2016-Q4"       "DEU"   "Germany"       716743.4074     0.4
> >> "2017-Q1"       "DEU"   "Germany"       725268.5864     1.2
> >> "2017-Q2"       "DEU"   "Germany"       729321.5731     0.6
> >> "2017-Q3"       "DEU"   "Germany"       735610.6375     0.9
> >> "2017-Q4"       "DEU"   "Germany"       740991.229      0.7
> >> "2018-Q1"       "DEU"   "Germany"       741969.5787     0.1
> >> "2018-Q2"       "DEU"   "Germany"       744834.6127     0.4
> >> "2018-Q3"       "DEU"   "Germany"       744065.912      -0.1
> >> "2018-Q4"       "DEU"   "Germany"       745603.2305     0.2
> >> "2019-Q1"       "DEU"   "Germany"       748468.2276     0.4
> >> "2019-Q2"       "DEU"   "Germany"       747909.2496     -0.1
> >> "2016-Q4"       "ISR"   "Israel"        307789.55       0.9
> >> "2017-Q1"       "ISR"   "Israel"        308323.023      0.2
> >> "2017-Q2"       "ISR"   "Israel"        311759.624      1.1
> >> "2017-Q3"       "ISR"   "Israel"        315651.46       1.2
> >> "2017-Q4"       "ISR"   "Israel"        319056.442      1.1
> >> "2018-Q1"       "ISR"   "Israel"        322272.592      1
> >> "2018-Q2"       "ISR"   "Israel"        323422.356      0.4
> >> "2018-Q3"       "ISR"   "Israel"        325702.534      0.7
> >> "2018-Q4"       "ISR"   "Israel"        329052.641      1
> >> "2019-Q1"       "ISR"   "Israel"        332851.725      1.2
> >> "2019-Q2"       "ISR"   "Israel"        333686.876      0.3
> >> "2016-Q4"       "ITA"   "Italy" 396162.2        0.5
> >> "2017-Q1"       "ITA"   "Italy" 398379  0.6
> >> "2017-Q2"       "ITA"   "Italy" 399893  0.4
> >> "2017-Q3"       "ITA"   "Italy" 401534  0.4
> >> "2017-Q4"       "ITA"   "Italy" 403053.4        0.4
> >> "2018-Q1"       "ITA"   "Italy" 403937.8        0.2
> >> "2018-Q2"       "ITA"   "Italy" 403977.3        0
> >> "2018-Q3"       "ITA"   "Italy" 403434.2        -0.1
> >> "2018-Q4"       "ITA"   "Italy" 403190.7        -0.1
> >> "2019-Q1"       "ITA"   "Italy" 403697.9        0.1
> >> "2019-Q2"       "ITA"   "Italy" 403794.7        0
> >> "2016-Q4"       "JPN"   "Japan" 130406025       0.2
> >> "2017-Q1"       "JPN"   "Japan" 131558850       0.9
> >> "2017-Q2"       "JPN"   "Japan" 132121450       0.4
> >> "2017-Q3"       "JPN"   "Japan" 133064400       0.7
> >> "2017-Q4"       "JPN"   "Japan" 133475100       0.3
> >> "2018-Q1"       "JPN"   "Japan" 133386850       -0.1
> >> "2018-Q2"       "JPN"   "Japan" 133931825       0.4
> >> "2018-Q3"       "JPN"   "Japan" 133289800       -0.5
> >> "2018-Q4"       "JPN"   "Japan" 133836225       0.4
> >> "2019-Q1"       "JPN"   "Japan" 134777725       0.7
> >> "2019-Q2"       "JPN"   "Japan" 135369050       0.4
> >> "2016-Q4"       "KOR"   "Korea" 431473400       0.8
> >> "2017-Q1"       "KOR"   "Korea" 435435200       0.9
> >> "2017-Q2"       "KOR"   "Korea" 437712100       0.5
> >> "2017-Q3"       "KOR"   "Korea" 444064400       1.5
> >> "2017-Q4"       "KOR"   "Korea" 443599800       -0.1
> >> "2018-Q1"       "KOR"   "Korea" 447909300       1
> >> "2018-Q2"       "KOR"   "Korea" 450495800       0.6
> >> "2018-Q3"       "KOR"   "Korea" 452561100       0.5
> >> "2018-Q4"       "KOR"   "Korea" 456769700       0.9
> >> "2019-Q1"       "KOR"   "Korea" 455081000       -0.4
> >> "2019-Q2"       "KOR"   "Korea" 459958000       1.1
> >> "2016-Q4"       "NLD"   "Netherlands"   178453.593134   0.9
> >> "2017-Q1"       "NLD"   "Netherlands"   179367.793134   0.5
> >> "2017-Q2"       "NLD"   "Netherlands"   180964.533134   0.9
> >> "2017-Q3"       "NLD"   "Netherlands"   182189.893134   0.7
> >> "2017-Q4"       "NLD"   "Netherlands"   183625.193134   0.8
> >> "2018-Q1"       "NLD"   "Netherlands"   184793.473134   0.6
> >> "2018-Q2"       "NLD"   "Netherlands"   185981.973134   0.6
> >> "2018-Q3"       "NLD"   "Netherlands"   186425.153134   0.2
> >> "2018-Q4"       "NLD"   "Netherlands"   187434.343134   0.5
> >> "2019-Q1"       "NLD"   "Netherlands"   188324.263134   0.5
> >> "2019-Q2"       "NLD"   "Netherlands"   189297.773134   0.5
> >> "2016-Q4"       "NZL"   "New Zealand"   59062   0.5
> >> "2017-Q1"       "NZL"   "New Zealand"   59348   0.5
> >> "2017-Q2"       "NZL"   "New Zealand"   59743   0.7
> >> "2017-Q3"       "NZL"   "New Zealand"   60320   1
> >> "2017-Q4"       "NZL"   "New Zealand"   60737   0.7
> >> "2018-Q1"       "NZL"   "New Zealand"   61031   0.5
> >> "2018-Q2"       "NZL"   "New Zealand"   61655   1
> >> "2018-Q3"       "NZL"   "New Zealand"   61927   0.4
> >> "2018-Q4"       "NZL"   "New Zealand"   62282   0.6
> >> "2019-Q1"       "NZL"   "New Zealand"   62800   0.8
> >> "2016-Q4"       "NOR"   "Norway"        784704  2
> >> "2017-Q1"       "NOR"   "Norway"        788709  0.5
> >> "2017-Q2"       "NOR"   "Norway"        794220  0.7
> >> "2017-Q3"       "NOR"   "Norway"        798283  0.5
> >> "2017-Q4"       "NOR"   "Norway"        800232  0.2
> >> "2018-Q1"       "NOR"   "Norway"        803756  0.4
> >> "2018-Q2"       "NOR"   "Norway"        807187  0.4
> >> "2018-Q3"       "NOR"   "Norway"        810942  0.5
> >> "2018-Q4"       "NOR"   "Norway"        815921  0.6
> >> "2019-Q1"       "NOR"   "Norway"        815323  -0.1
> >> "2016-Q4"       "PRT"   "Portugal"      44303.821       0.8
> >> "2017-Q1"       "PRT"   "Portugal"      44632.068       0.7
> >> "2017-Q2"       "PRT"   "Portugal"      44803.721       0.4
> >> "2017-Q3"       "PRT"   "Portugal"      45062.631       0.6
> >> "2017-Q4"       "PRT"   "Portugal"      45426.14        0.8
> >> "2018-Q1"       "PRT"   "Portugal"      45641.37        0.5
> >> "2018-Q2"       "PRT"   "Portugal"      45910.934       0.6
> >> "2018-Q3"       "PRT"   "Portugal"      46027.362       0.3
> >> "2018-Q4"       "PRT"   "Portugal"      46203.578       0.4
> >> "2019-Q1"       "PRT"   "Portugal"      46453.392       0.5
> >> "2019-Q2"       "PRT"   "Portugal"      46685.65896     0.5
> >> "2016-Q4"       "ESP"   "Spain" 279431  0.6
> >> "2017-Q1"       "ESP"   "Spain" 281707  0.8
> >> "2017-Q2"       "ESP"   "Spain" 284169  0.9
> >> "2017-Q3"       "ESP"   "Spain" 285986  0.6
> >> "2017-Q4"       "ESP"   "Spain" 288064  0.7
> >> "2018-Q1"       "ESP"   "Spain" 289861  0.6
> >> "2018-Q2"       "ESP"   "Spain" 291583  0.6
> >> "2018-Q3"       "ESP"   "Spain" 293145  0.5
> >> "2018-Q4"       "ESP"   "Spain" 294768  0.6
> >> "2019-Q1"       "ESP"   "Spain" 296732  0.7
> >> "2019-Q2"       "ESP"   "Spain" 298147  0.5
> >> "2016-Q4"       "SWE"   "Sweden"        1150761 0.4
> >> "2017-Q1"       "SWE"   "Sweden"        1151977 0.1
> >> "2017-Q2"       "SWE"   "Sweden"        1169243 1.5
> >> "2017-Q3"       "SWE"   "Sweden"        1177835 0.7
> >> "2017-Q4"       "SWE"   "Sweden"        1181734 0.3
> >> "2018-Q1"       "SWE"   "Sweden"        1192111 0.9
> >> "2018-Q2"       "SWE"   "Sweden"        1197931 0.5
> >> "2018-Q3"       "SWE"   "Sweden"        1196262 -0.1
> >> "2018-Q4"       "SWE"   "Sweden"        1209430 1.1
> >> "2019-Q1"       "SWE"   "Sweden"        1215583 0.5
> >> "2019-Q2"       "SWE"   "Sweden"        1214691 -0.1
> >> "2016-Q4"       "CHE"   "Switzerland"   168268.356822   -0.1
> >> "2017-Q1"       "CHE"   "Switzerland"   168865.076317   0.4
> >> "2017-Q2"       "CHE"   "Switzerland"   170078.764694   0.7
> >> "2017-Q3"       "CHE"   "Switzerland"   171405.16327    0.8
> >> "2017-Q4"       "CHE"   "Switzerland"   172777.427869   0.8
> >> "2018-Q1"       "CHE"   "Switzerland"   174168.535837   0.8
> >> "2018-Q2"       "CHE"   "Switzerland"   175400.870886   0.7
> >> "2018-Q3"       "CHE"   "Switzerland"   175089.0314     -0.2
> >> "2018-Q4"       "CHE"   "Switzerland"   175664.228343   0.3
> >> "2019-Q1"       "CHE"   "Switzerland"   176651.744992   0.6
> >> "2016-Q4"       "GBR"   "United Kingdom"        496470  0.7
> >> "2017-Q1"       "GBR"   "United Kingdom"        498582  0.4
> >> "2017-Q2"       "GBR"   "United Kingdom"        499885  0.3
> >> "2017-Q3"       "GBR"   "United Kingdom"        502473  0.5
> >> "2017-Q4"       "GBR"   "United Kingdom"        504487  0.4
> >> "2018-Q1"       "GBR"   "United Kingdom"        504785  0.1
> >> "2018-Q2"       "GBR"   "United Kingdom"        506842  0.4
> >> "2018-Q3"       "GBR"   "United Kingdom"        510346  0.7
> >> "2018-Q4"       "GBR"   "United Kingdom"        511482  0.2
> >> "2019-Q1"       "GBR"   "United Kingdom"        514019  0.5
> >> "2019-Q2"       "GBR"   "United Kingdom"        513029  -0.2
> >> "2016-Q4"       "USA"   "United States" 4456057.75      0.5
> >> "2017-Q1"       "USA"   "United States" 4481314 0.6
> >> "2017-Q2"       "USA"   "United States" 4505262 0.5
> >> "2017-Q3"       "USA"   "United States" 4540889.5       0.8
> >> "2017-Q4"       "USA"   "United States" 4580616 0.9
> >> "2018-Q1"       "USA"   "United States" 4609563.5       0.6
> >> "2018-Q2"       "USA"   "United States" 4649533.75      0.9
> >> "2018-Q3"       "USA"   "United States" 4683180 0.7
> >> "2018-Q4"       "USA"   "United States" 4695887 0.3
> >> "2019-Q1"       "USA"   "United States" 4731820.25      0.8
> >> "2019-Q2"       "USA"   "United States" 4755955 0.5
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From g||ted|||e2014 @end|ng |rom gm@||@com  Sun Aug 25 06:48:35 2019
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Sun, 25 Aug 2019 05:48:35 +0100
Subject: [R] Labeling Dates of different length on two in axis plot
Message-ID: <CAC8ss33xjKkx+HVPCxAVGF+ttEd2W=YjDLOcM=8=nxJ9Ks-b6Q@mail.gmail.com>

Dear Contributors,
I have two dataset of different lengths, each containing year, month,
day and counts.
After converting the date using as.Date function, I plotted the two
dateset on one graph. That was fine.

I am, however, having problems with the axis 1 where I am trying to
put the dates.
Since the two dates are not exactly the same, I stored the first date
as x1 and the second x2. x1 runs from 1953-01-02 to 2006-11-15 while
the range of x2 is between 1957-07-26 and 1994-07-17.
I tired a number of approaches but the one that seems to come close to
what I am looking for is: axis.Date(1, at=seq(min(x1), max(x1), by="50
mon"), format="%Y").

Unfortunately, it only labeled the axis up to 1994 even when I tried
to replace x1 with x2 in the code. Since one of the dates is up to
2006, I I wish to display the minimum (1953) and maximum  (2006) dates
and some possible intermediary dates.
I am always indebted to you.

Best wishes
Ogbos


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Aug 25 09:54:49 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 25 Aug 2019 08:54:49 +0100
Subject: [R] Colouring selected columns in a facetted column chart
In-Reply-To: <CAGgJW75BGt=1oGh8yt0fu3DkhN+vZdww60W1THQ8B-uKdf0S6g@mail.gmail.com>
References: <ecbd4d10a93deb6d99755ee76c995a14@philipsmith.ca>
 <CAGgJW77ByTO7CrHyrjd17AiOw2m9Qs5K91TM4J6Df7EX4pvynQ@mail.gmail.com>
 <e91780dbe3a95b73673e71a0431fd5e5@philipsmith.ca>
 <CAGgJW75BGt=1oGh8yt0fu3DkhN+vZdww60W1THQ8B-uKdf0S6g@mail.gmail.com>
Message-ID: <ac31ba4d-588d-fcd8-6b06-29876f54cb10@sapo.pt>

Hello,

The code in Eric's answer works, but maybe it's better to redo the 'col' 
code.
It's much simpler to create a factor with appropriate labels. Then, the 
values argument in scale_fill_manual can be set more naturally, it can 
depend on col.

(I have also added a theme to make the axis labels more readable, they 
were over each other. Remove it if not needed.)


col <- (t1$TIME %in% c("2019-Q1", "2019-Q2")) + 1L
col <- factor(col, labels = c("navyblue", "red"))


ggplot(t1) +
   geom_col(aes(x = TIME, y = GDPgr, fill = col), show.legend = FALSE) +
   scale_fill_manual(values = levels(col)) +
   facet_wrap(~ Country, ncol = 3) +
   theme(axis.text.x = element_text(angle = 50, hjust = 1))


Hope this helps,

Rui Barradas

?s 04:21 de 25/08/19, Eric Berger escreveu:
> This seems to work
> ggplot(t1) +
>    geom_col(aes(x=TIME,y=GDPgr,fill=col),show.legend=FALSE) +
>       scale_fill_manual(values=c("navyblue","red")) +
>           facet_wrap(~Country,ncol=3)
> 
> HTH,
> Eric
> 
> 
> On Sun, Aug 25, 2019 at 5:45 AM <phil at philipsmith.ca> wrote:
> 
>> Resubmitted as recommended:
>>
>> I am having difficulty with a chart using ggplot. It is a facetted
>> column chart showing GDP growth rates by country. The columns are
>> coloured navyblue, except that I want to colour the most recent columns,
>> for 2019-Q1 and 2019-Q2, red. For some countries data are available up
>> to 2019-Q2 while for others data are only available up to 2019-Q1. My
>> code and data frame are shown below and it almost works, but not quite.
>> For some reason the red bars for Germany, Korea, Norway, Sweden and
>> United Kingdom are slightly off. Any help will be much appreciated.
>>
>> Here is my reprex:
>>
>> library(tidyverse)
>> t1 <- read.table("t1.txt",header=TRUE,sep="\t")
>> col <- rep("navyblue",nrow(t1))
>> for (i in 1:nrow(t1)) {
>>     if((t1$TIME[i]=="2019-Q1" | t1$TIME[i]=="2019-Q2")) {
>>       col[i] <- "red"}
>> }
>> ggplot(t1) +
>>     geom_col(aes(x=TIME,y=GDPgr),fill=col) +
>>     facet_wrap(~Country,ncol=3)
>>
>> Here is my data frame, called "t1.txt", output by dput():
>>
>> structure(list(TIME = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L,
>> 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L,
>> 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L,
>> 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
>> 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L,
>> 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
>> 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L,
>> 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L,
>> 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
>> 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L,
>> 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
>> 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L,
>> 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L,
>> 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
>> 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L,
>> 5L, 6L, 7L, 8L, 9L, 10L, 11L), .Label = c("2016-Q4", "2017-Q1",
>> "2017-Q2", "2017-Q3", "2017-Q4", "2018-Q1", "2018-Q2", "2018-Q3",
>> "2018-Q4", "2019-Q1", "2019-Q2"), class = "factor"), LOCATION =
>> structure(c(1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>> 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 6L, 6L,
>> 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>> 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 10L, 10L,
>> 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 5L, 5L, 5L, 5L,
>> 5L, 5L, 5L, 5L, 5L, 5L, 5L, 12L, 12L, 12L, 12L, 12L, 12L, 12L,
>> 12L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L,
>> 13L, 13L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L,
>> 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 16L, 16L,
>> 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 18L, 18L, 18L, 18L,
>> 18L, 18L, 18L, 18L, 18L, 18L, 17L, 17L, 17L, 17L, 17L, 17L, 17L,
>> 17L, 17L, 17L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L,
>> 19L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 20L, 20L, 20L,
>> 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 4L, 4L, 4L, 4L, 4L, 4L,
>> 4L, 4L, 4L, 4L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L,
>> 11L, 11L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L
>> ), .Label = c("AUS", "BEL", "CAN", "CHE", "DEU", "DNK", "ESP",
>> "EU28", "FIN", "FRA", "GBR", "ISR", "ITA", "JPN", "KOR", "NLD",
>> "NOR", "NZL", "PRT", "SWE", "USA"), class = "factor"), Country =
>> structure(c(1L,
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>> 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L,
>> 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>> 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L,
>> 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>> 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 10L,
>> 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 11L, 11L, 11L,
>> 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 12L, 12L, 12L, 12L, 12L,
>> 12L, 12L, 12L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 13L, 13L, 13L,
>> 13L, 13L, 13L, 13L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L,
>> 14L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 16L, 16L,
>> 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 17L, 17L, 17L, 17L,
>> 17L, 17L, 17L, 17L, 17L, 17L, 17L, 18L, 18L, 18L, 18L, 18L, 18L,
>> 18L, 18L, 18L, 18L, 18L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L,
>> 19L, 19L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L,
>> 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L), .Label =
>> c("Australia",
>> "Belgium", "Canada", "Denmark", "European Union (28 countries)",
>> "Finland", "France", "Germany", "Israel", "Italy", "Japan", "Korea",
>> "Netherlands", "New Zealand", "Norway", "Portugal", "Spain",
>> "Sweden", "Switzerland", "United Kingdom", "United States"), class =
>> "factor"),
>>       Value = c(440518, 442141, 445739, 448672, 451302, 455680,
>>       459697, 461024, 462032, 463907, 106675, 107394, 107828, 108003,
>>       108744, 109037, 109386, 109676, 110081, 110459, 110680, 493742,
>>       498719, 504100.5, 505745, 507883, 509758.75, 512958, 515639.25,
>>       515971.75, 516489.5, 499945, 511319, 505254, 500363, 504837,
>>       508633, 511901, 513630, 517726, 518368, 3301202.652555,
>> 3323886.876398,
>>       3345038.332666, 3367136.027609, 3390431.080785, 3404554.778774,
>>       3419358.570571, 3430321.169276, 3440915.89772, 3458087.265837,
>>       3465003.441, 48525, 49368, 49430, 49596, 50153, 50352, 50449,
>>       50507, 50530, 50822, 551760, 556305, 560160, 563998, 568125,
>>       569542, 570670, 572387, 574640, 576494, 577905, 716743.4074,
>>       725268.5864, 729321.5731, 735610.6375, 740991.229, 741969.5787,
>>       744834.6127, 744065.912, 745603.2305, 748468.2276, 747909.2496,
>>       307789.55, 308323.023, 311759.624, 315651.46, 319056.442,
>>       322272.592, 323422.356, 325702.534, 329052.641, 332851.725,
>>       333686.876, 396162.2, 398379, 399893, 401534, 403053.4, 403937.8,
>>       403977.3, 403434.2, 403190.7, 403697.9, 403794.7, 130406025,
>>       131558850, 132121450, 133064400, 133475100, 133386850, 133931825,
>>       133289800, 133836225, 134777725, 135369050, 431473400, 435435200,
>>       437712100, 444064400, 443599800, 447909300, 450495800, 452561100,
>>       456769700, 455081000, 459958000, 178453.593134, 179367.793134,
>>       180964.533134, 182189.893134, 183625.193134, 184793.473134,
>>       185981.973134, 186425.153134, 187434.343134, 188324.263134,
>>       189297.773134, 59062, 59348, 59743, 60320, 60737, 61031,
>>       61655, 61927, 62282, 62800, 784704, 788709, 794220, 798283,
>>       800232, 803756, 807187, 810942, 815921, 815323, 44303.821,
>>       44632.068, 44803.721, 45062.631, 45426.14, 45641.37, 45910.934,
>>       46027.362, 46203.578, 46453.392, 46685.65896, 279431, 281707,
>>       284169, 285986, 288064, 289861, 291583, 293145, 294768, 296732,
>>       298147, 1150761, 1151977, 1169243, 1177835, 1181734, 1192111,
>>       1197931, 1196262, 1209430, 1215583, 1214691, 168268.356822,
>>       168865.076317, 170078.764694, 171405.16327, 172777.427869,
>>       174168.535837, 175400.870886, 175089.0314, 175664.228343,
>>       176651.744992, 496470, 498582, 499885, 502473, 504487, 504785,
>>       506842, 510346, 511482, 514019, 513029, 4456057.75, 4481314,
>>       4505262, 4540889.5, 4580616, 4609563.5, 4649533.75, 4683180,
>>       4695887, 4731820.25, 4755955), GDPgr = c(1, 0.4, 0.8, 0.7,
>>       0.6, 1, 0.9, 0.3, 0.2, 0.4, 0.3, 0.7, 0.4, 0.2, 0.7, 0.3,
>>       0.3, 0.3, 0.4, 0.3, 0.2, 0.6, 1, 1.1, 0.3, 0.4, 0.4, 0.6,
>>       0.5, 0.1, 0.1, 0.9, 2.3, -1.2, -1, 0.9, 0.8, 0.6, 0.3, 0.8,
>>       0.1, 0.8, 0.7, 0.6, 0.7, 0.7, 0.4, 0.4, 0.3, 0.3, 0.5, 0.2,
>>       0.2, 1.7, 0.1, 0.3, 1.1, 0.4, 0.2, 0.1, 0, 0.6, 0.6, 0.8,
>>       0.7, 0.7, 0.7, 0.2, 0.2, 0.3, 0.4, 0.3, 0.2, 0.4, 1.2, 0.6,
>>       0.9, 0.7, 0.1, 0.4, -0.1, 0.2, 0.4, -0.1, 0.9, 0.2, 1.1,
>>       1.2, 1.1, 1, 0.4, 0.7, 1, 1.2, 0.3, 0.5, 0.6, 0.4, 0.4, 0.4,
>>       0.2, 0, -0.1, -0.1, 0.1, 0, 0.2, 0.9, 0.4, 0.7, 0.3, -0.1,
>>       0.4, -0.5, 0.4, 0.7, 0.4, 0.8, 0.9, 0.5, 1.5, -0.1, 1, 0.6,
>>       0.5, 0.9, -0.4, 1.1, 0.9, 0.5, 0.9, 0.7, 0.8, 0.6, 0.6, 0.2,
>>       0.5, 0.5, 0.5, 0.5, 0.5, 0.7, 1, 0.7, 0.5, 1, 0.4, 0.6, 0.8,
>>       2, 0.5, 0.7, 0.5, 0.2, 0.4, 0.4, 0.5, 0.6, -0.1, 0.8, 0.7,
>>       0.4, 0.6, 0.8, 0.5, 0.6, 0.3, 0.4, 0.5, 0.5, 0.6, 0.8, 0.9,
>>       0.6, 0.7, 0.6, 0.6, 0.5, 0.6, 0.7, 0.5, 0.4, 0.1, 1.5, 0.7,
>>       0.3, 0.9, 0.5, -0.1, 1.1, 0.5, -0.1, -0.1, 0.4, 0.7, 0.8,
>>       0.8, 0.8, 0.7, -0.2, 0.3, 0.6, 0.7, 0.4, 0.3, 0.5, 0.4, 0.1,
>>       0.4, 0.7, 0.2, 0.5, -0.2, 0.5, 0.6, 0.5, 0.8, 0.9, 0.6, 0.9,
>>       0.7, 0.3, 0.8, 0.5)), class = "data.frame", row.names = c(NA,
>> -224L))
>>
>>
>>
>> On 2019-08-24 22:39, Eric Berger wrote:
>>> Hi Phil,
>>> Please resubmit your question with the data frame contents shown as
>>> the output from the command
>>> dput(t1.txt). This will make it easier for people to run your reprex
>>> and respond to your question.
>>>
>>> Best,
>>> Eric
>>>
>>> On Sun, Aug 25, 2019 at 5:26 AM <phil at philipsmith.ca> wrote:
>>>
>>>> I am having difficulty with a chart using ggplot. It is a facetted
>>>> column chart showing GDP growth rates by country. The columns are
>>>> coloured navyblue, except that I want to colour the most recent
>>>> columns,
>>>> for 2019-Q1 and 2019-Q2, red. For some countries data are available
>>>> up
>>>> to 2019-Q2 while for others data are only available up to 2019-Q1.
>>>> My
>>>> code and data frame are shown below and it almost works, but not
>>>> quite.
>>>> For some reason the red bars for Germany, Korea, Norway, Sweden and
>>>> United Kingdom are slightly off. Any help will be much appreciated.
>>>>
>>>> Here is my reprex:
>>>>
>>>> library(tidyverse)
>>>> t1 <- read.table("t1.txt",header=TRUE,sep="\t")
>>>> col <- rep("navyblue",nrow(t1))
>>>> for (i in 1:nrow(t1)) {
>>>> if((t1$TIME[i]=="2019-Q1" | t1$TIME[i]=="2019-Q2")) {
>>>> col[i] <- "red"}
>>>> }
>>>> ggplot(t1) +
>>>> geom_col(aes(x=TIME,y=GDPgr),fill=col) +
>>>> facet_wrap(~Country,ncol=3)
>>>>
>>>> Here is my data frame, called "t1.txt":
>>>>
>>>> "TIME"  "LOCATION"      "Country"       "Value" "GDPgr"
>>>> "2016-Q4"       "AUS"   "Australia"     440518  1
>>>> "2017-Q1"       "AUS"   "Australia"     442141  0.4
>>>> "2017-Q2"       "AUS"   "Australia"     445739  0.8
>>>> "2017-Q3"       "AUS"   "Australia"     448672  0.7
>>>> "2017-Q4"       "AUS"   "Australia"     451302  0.6
>>>> "2018-Q1"       "AUS"   "Australia"     455680  1
>>>> "2018-Q2"       "AUS"   "Australia"     459697  0.9
>>>> "2018-Q3"       "AUS"   "Australia"     461024  0.3
>>>> "2018-Q4"       "AUS"   "Australia"     462032  0.2
>>>> "2019-Q1"       "AUS"   "Australia"     463907  0.4
>>>> "2016-Q4"       "BEL"   "Belgium"       106675  0.3
>>>> "2017-Q1"       "BEL"   "Belgium"       107394  0.7
>>>> "2017-Q2"       "BEL"   "Belgium"       107828  0.4
>>>> "2017-Q3"       "BEL"   "Belgium"       108003  0.2
>>>> "2017-Q4"       "BEL"   "Belgium"       108744  0.7
>>>> "2018-Q1"       "BEL"   "Belgium"       109037  0.3
>>>> "2018-Q2"       "BEL"   "Belgium"       109386  0.3
>>>> "2018-Q3"       "BEL"   "Belgium"       109676  0.3
>>>> "2018-Q4"       "BEL"   "Belgium"       110081  0.4
>>>> "2019-Q1"       "BEL"   "Belgium"       110459  0.3
>>>> "2019-Q2"       "BEL"   "Belgium"       110680  0.2
>>>> "2016-Q4"       "CAN"   "Canada"        493742  0.6
>>>> "2017-Q1"       "CAN"   "Canada"        498719  1
>>>> "2017-Q2"       "CAN"   "Canada"        504100.5        1.1
>>>> "2017-Q3"       "CAN"   "Canada"        505745  0.3
>>>> "2017-Q4"       "CAN"   "Canada"        507883  0.4
>>>> "2018-Q1"       "CAN"   "Canada"        509758.75       0.4
>>>> "2018-Q2"       "CAN"   "Canada"        512958  0.6
>>>> "2018-Q3"       "CAN"   "Canada"        515639.25       0.5
>>>> "2018-Q4"       "CAN"   "Canada"        515971.75       0.1
>>>> "2019-Q1"       "CAN"   "Canada"        516489.5        0.1
>>>> "2016-Q4"       "DNK"   "Denmark"       499945  0.9
>>>> "2017-Q1"       "DNK"   "Denmark"       511319  2.3
>>>> "2017-Q2"       "DNK"   "Denmark"       505254  -1.2
>>>> "2017-Q3"       "DNK"   "Denmark"       500363  -1
>>>> "2017-Q4"       "DNK"   "Denmark"       504837  0.9
>>>> "2018-Q1"       "DNK"   "Denmark"       508633  0.8
>>>> "2018-Q2"       "DNK"   "Denmark"       511901  0.6
>>>> "2018-Q3"       "DNK"   "Denmark"       513630  0.3
>>>> "2018-Q4"       "DNK"   "Denmark"       517726  0.8
>>>> "2019-Q1"       "DNK"   "Denmark"       518368  0.1
>>>> "2016-Q4"       "EU28"  "European Union (28 countries)"
>>>> 3301202.652555  0.8
>>>> "2017-Q1"       "EU28"  "European Union (28 countries)"
>>>> 3323886.876398  0.7
>>>> "2017-Q2"       "EU28"  "European Union (28 countries)"
>>>> 3345038.332666  0.6
>>>> "2017-Q3"       "EU28"  "European Union (28 countries)"
>>>> 3367136.027609  0.7
>>>> "2017-Q4"       "EU28"  "European Union (28 countries)"
>>>> 3390431.080785  0.7
>>>> "2018-Q1"       "EU28"  "European Union (28 countries)"
>>>> 3404554.778774  0.4
>>>> "2018-Q2"       "EU28"  "European Union (28 countries)"
>>>> 3419358.570571  0.4
>>>> "2018-Q3"       "EU28"  "European Union (28 countries)"
>>>> 3430321.169276  0.3
>>>> "2018-Q4"       "EU28"  "European Union (28 countries)"
>>>> 3440915.89772   0.3
>>>> "2019-Q1"       "EU28"  "European Union (28 countries)"
>>>> 3458087.265837  0.5
>>>> "2019-Q2"       "EU28"  "European Union (28 countries)" 3465003.441
>>>> 0.2
>>>> "2016-Q4"       "FIN"   "Finland"       48525   0.2
>>>> "2017-Q1"       "FIN"   "Finland"       49368   1.7
>>>> "2017-Q2"       "FIN"   "Finland"       49430   0.1
>>>> "2017-Q3"       "FIN"   "Finland"       49596   0.3
>>>> "2017-Q4"       "FIN"   "Finland"       50153   1.1
>>>> "2018-Q1"       "FIN"   "Finland"       50352   0.4
>>>> "2018-Q2"       "FIN"   "Finland"       50449   0.2
>>>> "2018-Q3"       "FIN"   "Finland"       50507   0.1
>>>> "2018-Q4"       "FIN"   "Finland"       50530   0
>>>> "2019-Q1"       "FIN"   "Finland"       50822   0.6
>>>> "2016-Q4"       "FRA"   "France"        551760  0.6
>>>> "2017-Q1"       "FRA"   "France"        556305  0.8
>>>> "2017-Q2"       "FRA"   "France"        560160  0.7
>>>> "2017-Q3"       "FRA"   "France"        563998  0.7
>>>> "2017-Q4"       "FRA"   "France"        568125  0.7
>>>> "2018-Q1"       "FRA"   "France"        569542  0.2
>>>> "2018-Q2"       "FRA"   "France"        570670  0.2
>>>> "2018-Q3"       "FRA"   "France"        572387  0.3
>>>> "2018-Q4"       "FRA"   "France"        574640  0.4
>>>> "2019-Q1"       "FRA"   "France"        576494  0.3
>>>> "2019-Q2"       "FRA"   "France"        577905  0.2
>>>> "2016-Q4"       "DEU"   "Germany"       716743.4074     0.4
>>>> "2017-Q1"       "DEU"   "Germany"       725268.5864     1.2
>>>> "2017-Q2"       "DEU"   "Germany"       729321.5731     0.6
>>>> "2017-Q3"       "DEU"   "Germany"       735610.6375     0.9
>>>> "2017-Q4"       "DEU"   "Germany"       740991.229      0.7
>>>> "2018-Q1"       "DEU"   "Germany"       741969.5787     0.1
>>>> "2018-Q2"       "DEU"   "Germany"       744834.6127     0.4
>>>> "2018-Q3"       "DEU"   "Germany"       744065.912      -0.1
>>>> "2018-Q4"       "DEU"   "Germany"       745603.2305     0.2
>>>> "2019-Q1"       "DEU"   "Germany"       748468.2276     0.4
>>>> "2019-Q2"       "DEU"   "Germany"       747909.2496     -0.1
>>>> "2016-Q4"       "ISR"   "Israel"        307789.55       0.9
>>>> "2017-Q1"       "ISR"   "Israel"        308323.023      0.2
>>>> "2017-Q2"       "ISR"   "Israel"        311759.624      1.1
>>>> "2017-Q3"       "ISR"   "Israel"        315651.46       1.2
>>>> "2017-Q4"       "ISR"   "Israel"        319056.442      1.1
>>>> "2018-Q1"       "ISR"   "Israel"        322272.592      1
>>>> "2018-Q2"       "ISR"   "Israel"        323422.356      0.4
>>>> "2018-Q3"       "ISR"   "Israel"        325702.534      0.7
>>>> "2018-Q4"       "ISR"   "Israel"        329052.641      1
>>>> "2019-Q1"       "ISR"   "Israel"        332851.725      1.2
>>>> "2019-Q2"       "ISR"   "Israel"        333686.876      0.3
>>>> "2016-Q4"       "ITA"   "Italy" 396162.2        0.5
>>>> "2017-Q1"       "ITA"   "Italy" 398379  0.6
>>>> "2017-Q2"       "ITA"   "Italy" 399893  0.4
>>>> "2017-Q3"       "ITA"   "Italy" 401534  0.4
>>>> "2017-Q4"       "ITA"   "Italy" 403053.4        0.4
>>>> "2018-Q1"       "ITA"   "Italy" 403937.8        0.2
>>>> "2018-Q2"       "ITA"   "Italy" 403977.3        0
>>>> "2018-Q3"       "ITA"   "Italy" 403434.2        -0.1
>>>> "2018-Q4"       "ITA"   "Italy" 403190.7        -0.1
>>>> "2019-Q1"       "ITA"   "Italy" 403697.9        0.1
>>>> "2019-Q2"       "ITA"   "Italy" 403794.7        0
>>>> "2016-Q4"       "JPN"   "Japan" 130406025       0.2
>>>> "2017-Q1"       "JPN"   "Japan" 131558850       0.9
>>>> "2017-Q2"       "JPN"   "Japan" 132121450       0.4
>>>> "2017-Q3"       "JPN"   "Japan" 133064400       0.7
>>>> "2017-Q4"       "JPN"   "Japan" 133475100       0.3
>>>> "2018-Q1"       "JPN"   "Japan" 133386850       -0.1
>>>> "2018-Q2"       "JPN"   "Japan" 133931825       0.4
>>>> "2018-Q3"       "JPN"   "Japan" 133289800       -0.5
>>>> "2018-Q4"       "JPN"   "Japan" 133836225       0.4
>>>> "2019-Q1"       "JPN"   "Japan" 134777725       0.7
>>>> "2019-Q2"       "JPN"   "Japan" 135369050       0.4
>>>> "2016-Q4"       "KOR"   "Korea" 431473400       0.8
>>>> "2017-Q1"       "KOR"   "Korea" 435435200       0.9
>>>> "2017-Q2"       "KOR"   "Korea" 437712100       0.5
>>>> "2017-Q3"       "KOR"   "Korea" 444064400       1.5
>>>> "2017-Q4"       "KOR"   "Korea" 443599800       -0.1
>>>> "2018-Q1"       "KOR"   "Korea" 447909300       1
>>>> "2018-Q2"       "KOR"   "Korea" 450495800       0.6
>>>> "2018-Q3"       "KOR"   "Korea" 452561100       0.5
>>>> "2018-Q4"       "KOR"   "Korea" 456769700       0.9
>>>> "2019-Q1"       "KOR"   "Korea" 455081000       -0.4
>>>> "2019-Q2"       "KOR"   "Korea" 459958000       1.1
>>>> "2016-Q4"       "NLD"   "Netherlands"   178453.593134   0.9
>>>> "2017-Q1"       "NLD"   "Netherlands"   179367.793134   0.5
>>>> "2017-Q2"       "NLD"   "Netherlands"   180964.533134   0.9
>>>> "2017-Q3"       "NLD"   "Netherlands"   182189.893134   0.7
>>>> "2017-Q4"       "NLD"   "Netherlands"   183625.193134   0.8
>>>> "2018-Q1"       "NLD"   "Netherlands"   184793.473134   0.6
>>>> "2018-Q2"       "NLD"   "Netherlands"   185981.973134   0.6
>>>> "2018-Q3"       "NLD"   "Netherlands"   186425.153134   0.2
>>>> "2018-Q4"       "NLD"   "Netherlands"   187434.343134   0.5
>>>> "2019-Q1"       "NLD"   "Netherlands"   188324.263134   0.5
>>>> "2019-Q2"       "NLD"   "Netherlands"   189297.773134   0.5
>>>> "2016-Q4"       "NZL"   "New Zealand"   59062   0.5
>>>> "2017-Q1"       "NZL"   "New Zealand"   59348   0.5
>>>> "2017-Q2"       "NZL"   "New Zealand"   59743   0.7
>>>> "2017-Q3"       "NZL"   "New Zealand"   60320   1
>>>> "2017-Q4"       "NZL"   "New Zealand"   60737   0.7
>>>> "2018-Q1"       "NZL"   "New Zealand"   61031   0.5
>>>> "2018-Q2"       "NZL"   "New Zealand"   61655   1
>>>> "2018-Q3"       "NZL"   "New Zealand"   61927   0.4
>>>> "2018-Q4"       "NZL"   "New Zealand"   62282   0.6
>>>> "2019-Q1"       "NZL"   "New Zealand"   62800   0.8
>>>> "2016-Q4"       "NOR"   "Norway"        784704  2
>>>> "2017-Q1"       "NOR"   "Norway"        788709  0.5
>>>> "2017-Q2"       "NOR"   "Norway"        794220  0.7
>>>> "2017-Q3"       "NOR"   "Norway"        798283  0.5
>>>> "2017-Q4"       "NOR"   "Norway"        800232  0.2
>>>> "2018-Q1"       "NOR"   "Norway"        803756  0.4
>>>> "2018-Q2"       "NOR"   "Norway"        807187  0.4
>>>> "2018-Q3"       "NOR"   "Norway"        810942  0.5
>>>> "2018-Q4"       "NOR"   "Norway"        815921  0.6
>>>> "2019-Q1"       "NOR"   "Norway"        815323  -0.1
>>>> "2016-Q4"       "PRT"   "Portugal"      44303.821       0.8
>>>> "2017-Q1"       "PRT"   "Portugal"      44632.068       0.7
>>>> "2017-Q2"       "PRT"   "Portugal"      44803.721       0.4
>>>> "2017-Q3"       "PRT"   "Portugal"      45062.631       0.6
>>>> "2017-Q4"       "PRT"   "Portugal"      45426.14        0.8
>>>> "2018-Q1"       "PRT"   "Portugal"      45641.37        0.5
>>>> "2018-Q2"       "PRT"   "Portugal"      45910.934       0.6
>>>> "2018-Q3"       "PRT"   "Portugal"      46027.362       0.3
>>>> "2018-Q4"       "PRT"   "Portugal"      46203.578       0.4
>>>> "2019-Q1"       "PRT"   "Portugal"      46453.392       0.5
>>>> "2019-Q2"       "PRT"   "Portugal"      46685.65896     0.5
>>>> "2016-Q4"       "ESP"   "Spain" 279431  0.6
>>>> "2017-Q1"       "ESP"   "Spain" 281707  0.8
>>>> "2017-Q2"       "ESP"   "Spain" 284169  0.9
>>>> "2017-Q3"       "ESP"   "Spain" 285986  0.6
>>>> "2017-Q4"       "ESP"   "Spain" 288064  0.7
>>>> "2018-Q1"       "ESP"   "Spain" 289861  0.6
>>>> "2018-Q2"       "ESP"   "Spain" 291583  0.6
>>>> "2018-Q3"       "ESP"   "Spain" 293145  0.5
>>>> "2018-Q4"       "ESP"   "Spain" 294768  0.6
>>>> "2019-Q1"       "ESP"   "Spain" 296732  0.7
>>>> "2019-Q2"       "ESP"   "Spain" 298147  0.5
>>>> "2016-Q4"       "SWE"   "Sweden"        1150761 0.4
>>>> "2017-Q1"       "SWE"   "Sweden"        1151977 0.1
>>>> "2017-Q2"       "SWE"   "Sweden"        1169243 1.5
>>>> "2017-Q3"       "SWE"   "Sweden"        1177835 0.7
>>>> "2017-Q4"       "SWE"   "Sweden"        1181734 0.3
>>>> "2018-Q1"       "SWE"   "Sweden"        1192111 0.9
>>>> "2018-Q2"       "SWE"   "Sweden"        1197931 0.5
>>>> "2018-Q3"       "SWE"   "Sweden"        1196262 -0.1
>>>> "2018-Q4"       "SWE"   "Sweden"        1209430 1.1
>>>> "2019-Q1"       "SWE"   "Sweden"        1215583 0.5
>>>> "2019-Q2"       "SWE"   "Sweden"        1214691 -0.1
>>>> "2016-Q4"       "CHE"   "Switzerland"   168268.356822   -0.1
>>>> "2017-Q1"       "CHE"   "Switzerland"   168865.076317   0.4
>>>> "2017-Q2"       "CHE"   "Switzerland"   170078.764694   0.7
>>>> "2017-Q3"       "CHE"   "Switzerland"   171405.16327    0.8
>>>> "2017-Q4"       "CHE"   "Switzerland"   172777.427869   0.8
>>>> "2018-Q1"       "CHE"   "Switzerland"   174168.535837   0.8
>>>> "2018-Q2"       "CHE"   "Switzerland"   175400.870886   0.7
>>>> "2018-Q3"       "CHE"   "Switzerland"   175089.0314     -0.2
>>>> "2018-Q4"       "CHE"   "Switzerland"   175664.228343   0.3
>>>> "2019-Q1"       "CHE"   "Switzerland"   176651.744992   0.6
>>>> "2016-Q4"       "GBR"   "United Kingdom"        496470  0.7
>>>> "2017-Q1"       "GBR"   "United Kingdom"        498582  0.4
>>>> "2017-Q2"       "GBR"   "United Kingdom"        499885  0.3
>>>> "2017-Q3"       "GBR"   "United Kingdom"        502473  0.5
>>>> "2017-Q4"       "GBR"   "United Kingdom"        504487  0.4
>>>> "2018-Q1"       "GBR"   "United Kingdom"        504785  0.1
>>>> "2018-Q2"       "GBR"   "United Kingdom"        506842  0.4
>>>> "2018-Q3"       "GBR"   "United Kingdom"        510346  0.7
>>>> "2018-Q4"       "GBR"   "United Kingdom"        511482  0.2
>>>> "2019-Q1"       "GBR"   "United Kingdom"        514019  0.5
>>>> "2019-Q2"       "GBR"   "United Kingdom"        513029  -0.2
>>>> "2016-Q4"       "USA"   "United States" 4456057.75      0.5
>>>> "2017-Q1"       "USA"   "United States" 4481314 0.6
>>>> "2017-Q2"       "USA"   "United States" 4505262 0.5
>>>> "2017-Q3"       "USA"   "United States" 4540889.5       0.8
>>>> "2017-Q4"       "USA"   "United States" 4580616 0.9
>>>> "2018-Q1"       "USA"   "United States" 4609563.5       0.6
>>>> "2018-Q2"       "USA"   "United States" 4649533.75      0.9
>>>> "2018-Q3"       "USA"   "United States" 4683180 0.7
>>>> "2018-Q4"       "USA"   "United States" 4695887 0.3
>>>> "2019-Q1"       "USA"   "United States" 4731820.25      0.8
>>>> "2019-Q2"       "USA"   "United States" 4755955 0.5
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From drj|m|emon @end|ng |rom gm@||@com  Sun Aug 25 10:33:17 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sun, 25 Aug 2019 18:33:17 +1000
Subject: [R] Labeling Dates of different length on two in axis plot
In-Reply-To: <CAC8ss33xjKkx+HVPCxAVGF+ttEd2W=YjDLOcM=8=nxJ9Ks-b6Q@mail.gmail.com>
References: <CAC8ss33xjKkx+HVPCxAVGF+ttEd2W=YjDLOcM=8=nxJ9Ks-b6Q@mail.gmail.com>
Message-ID: <CA+8X3fX8SkEH2eGOFPk3DHnFVAL8rXvAZXwg8BO9n+aZAXcaDw@mail.gmail.com>

Hi Ogbos,
Hope things are going well for you. Perhaps this is what you want:

date_x1<-seq(as.Date("1953-01-02"),as.Date("2006-11-15"),length.out=8)
value_x1<-sample(1000:5000,8)
date_x2<-seq(as.Date("1957-07-26"),as.Date("1994-07-17"),length.out=6)
value_x2<-sample(0:1000,6)
plot(date_x1,value_x1,type="b",col="red",xlab="Year",ylab="x1",
 ylim=c(0,max(value_x1)))
points(date_x2,value_x2,type="b",col="blue")

Jim

On Sun, Aug 25, 2019 at 2:49 PM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>
> Dear Contributors,
> I have two dataset of different lengths, each containing year, month,
> day and counts.
> After converting the date using as.Date function, I plotted the two
> dateset on one graph. That was fine.
>
> I am, however, having problems with the axis 1 where I am trying to
> put the dates.
> Since the two dates are not exactly the same, I stored the first date
> as x1 and the second x2. x1 runs from 1953-01-02 to 2006-11-15 while
> the range of x2 is between 1957-07-26 and 1994-07-17.
> I tired a number of approaches but the one that seems to come close to
> what I am looking for is: axis.Date(1, at=seq(min(x1), max(x1), by="50
> mon"), format="%Y").
>
> Unfortunately, it only labeled the axis up to 1994 even when I tried
> to replace x1 with x2 in the code. Since one of the dates is up to
> 2006, I I wish to display the minimum (1953) and maximum  (2006) dates
> and some possible intermediary dates.
> I am always indebted to you.
>
> Best wishes
> Ogbos
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From phii m@iii@g oii phiiipsmith@c@  Sun Aug 25 14:26:11 2019
From: phii m@iii@g oii phiiipsmith@c@ (phii m@iii@g oii phiiipsmith@c@)
Date: Sun, 25 Aug 2019 08:26:11 -0400
Subject: [R] Colouring selected columns in a facetted column chart
In-Reply-To: <ac31ba4d-588d-fcd8-6b06-29876f54cb10@sapo.pt>
References: <ecbd4d10a93deb6d99755ee76c995a14@philipsmith.ca>
 <CAGgJW77ByTO7CrHyrjd17AiOw2m9Qs5K91TM4J6Df7EX4pvynQ@mail.gmail.com>
 <e91780dbe3a95b73673e71a0431fd5e5@philipsmith.ca>
 <CAGgJW75BGt=1oGh8yt0fu3DkhN+vZdww60W1THQ8B-uKdf0S6g@mail.gmail.com>
 <ac31ba4d-588d-fcd8-6b06-29876f54cb10@sapo.pt>
Message-ID: <6e10ebe3879328aecc41705117bd16cf@philipsmith.ca>

Thank you so much for your help.

Philip

On 2019-08-25 03:54, Rui Barradas wrote:
> Hello,
> 
> The code in Eric's answer works, but maybe it's better to redo the 
> 'col' code.
> It's much simpler to create a factor with appropriate labels. Then,
> the values argument in scale_fill_manual can be set more naturally, it
> can depend on col.
> 
> (I have also added a theme to make the axis labels more readable, they
> were over each other. Remove it if not needed.)
> 
> 
> col <- (t1$TIME %in% c("2019-Q1", "2019-Q2")) + 1L
> col <- factor(col, labels = c("navyblue", "red"))
> 
> 
> ggplot(t1) +
>   geom_col(aes(x = TIME, y = GDPgr, fill = col), show.legend = FALSE) +
>   scale_fill_manual(values = levels(col)) +
>   facet_wrap(~ Country, ncol = 3) +
>   theme(axis.text.x = element_text(angle = 50, hjust = 1))
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 04:21 de 25/08/19, Eric Berger escreveu:
>> This seems to work
>> ggplot(t1) +
>>    geom_col(aes(x=TIME,y=GDPgr,fill=col),show.legend=FALSE) +
>>       scale_fill_manual(values=c("navyblue","red")) +
>>           facet_wrap(~Country,ncol=3)
>> 
>> HTH,
>> Eric
>> 
>> 
>> On Sun, Aug 25, 2019 at 5:45 AM <phil at philipsmith.ca> wrote:
>> 
>>> Resubmitted as recommended:
>>> 
>>> I am having difficulty with a chart using ggplot. It is a facetted
>>> column chart showing GDP growth rates by country. The columns are
>>> coloured navyblue, except that I want to colour the most recent 
>>> columns,
>>> for 2019-Q1 and 2019-Q2, red. For some countries data are available 
>>> up
>>> to 2019-Q2 while for others data are only available up to 2019-Q1. My
>>> code and data frame are shown below and it almost works, but not 
>>> quite.
>>> For some reason the red bars for Germany, Korea, Norway, Sweden and
>>> United Kingdom are slightly off. Any help will be much appreciated.
>>> 
>>> Here is my reprex:
>>> 
>>> library(tidyverse)
>>> t1 <- read.table("t1.txt",header=TRUE,sep="\t")
>>> col <- rep("navyblue",nrow(t1))
>>> for (i in 1:nrow(t1)) {
>>>     if((t1$TIME[i]=="2019-Q1" | t1$TIME[i]=="2019-Q2")) {
>>>       col[i] <- "red"}
>>> }
>>> ggplot(t1) +
>>>     geom_col(aes(x=TIME,y=GDPgr),fill=col) +
>>>     facet_wrap(~Country,ncol=3)
>>> 
>>> Here is my data frame, called "t1.txt", output by dput():
>>> 
>>> structure(list(TIME = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L,
>>> 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L,
>>> 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L,
>>> 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
>>> 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L,
>>> 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
>>> 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L,
>>> 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L,
>>> 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
>>> 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L,
>>> 4L, 5L, 6L, 7L, 8L, 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
>>> 9L, 10L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L,
>>> 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L,
>>> 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
>>> 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 1L, 2L, 3L, 4L,
>>> 5L, 6L, 7L, 8L, 9L, 10L, 11L), .Label = c("2016-Q4", "2017-Q1",
>>> "2017-Q2", "2017-Q3", "2017-Q4", "2018-Q1", "2018-Q2", "2018-Q3",
>>> "2018-Q4", "2019-Q1", "2019-Q2"), class = "factor"), LOCATION =
>>> structure(c(1L,
>>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>>> 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 6L, 6L,
>>> 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>>> 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 10L, 10L,
>>> 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 5L, 5L, 5L, 5L,
>>> 5L, 5L, 5L, 5L, 5L, 5L, 5L, 12L, 12L, 12L, 12L, 12L, 12L, 12L,
>>> 12L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L,
>>> 13L, 13L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L,
>>> 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 16L, 16L,
>>> 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 18L, 18L, 18L, 18L,
>>> 18L, 18L, 18L, 18L, 18L, 18L, 17L, 17L, 17L, 17L, 17L, 17L, 17L,
>>> 17L, 17L, 17L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L,
>>> 19L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 20L, 20L, 20L,
>>> 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 4L, 4L, 4L, 4L, 4L, 4L,
>>> 4L, 4L, 4L, 4L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L,
>>> 11L, 11L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L
>>> ), .Label = c("AUS", "BEL", "CAN", "CHE", "DEU", "DNK", "ESP",
>>> "EU28", "FIN", "FRA", "GBR", "ISR", "ITA", "JPN", "KOR", "NLD",
>>> "NOR", "NZL", "PRT", "SWE", "USA"), class = "factor"), Country =
>>> structure(c(1L,
>>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>>> 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L,
>>> 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
>>> 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L,
>>> 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
>>> 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 10L,
>>> 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 11L, 11L, 11L,
>>> 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 12L, 12L, 12L, 12L, 12L,
>>> 12L, 12L, 12L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 13L, 13L, 13L,
>>> 13L, 13L, 13L, 13L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L,
>>> 14L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 16L, 16L,
>>> 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 17L, 17L, 17L, 17L,
>>> 17L, 17L, 17L, 17L, 17L, 17L, 17L, 18L, 18L, 18L, 18L, 18L, 18L,
>>> 18L, 18L, 18L, 18L, 18L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L,
>>> 19L, 19L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L,
>>> 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L), .Label =
>>> c("Australia",
>>> "Belgium", "Canada", "Denmark", "European Union (28 countries)",
>>> "Finland", "France", "Germany", "Israel", "Italy", "Japan", "Korea",
>>> "Netherlands", "New Zealand", "Norway", "Portugal", "Spain",
>>> "Sweden", "Switzerland", "United Kingdom", "United States"), class =
>>> "factor"),
>>>       Value = c(440518, 442141, 445739, 448672, 451302, 455680,
>>>       459697, 461024, 462032, 463907, 106675, 107394, 107828, 108003,
>>>       108744, 109037, 109386, 109676, 110081, 110459, 110680, 493742,
>>>       498719, 504100.5, 505745, 507883, 509758.75, 512958, 515639.25,
>>>       515971.75, 516489.5, 499945, 511319, 505254, 500363, 504837,
>>>       508633, 511901, 513630, 517726, 518368, 3301202.652555,
>>> 3323886.876398,
>>>       3345038.332666, 3367136.027609, 3390431.080785, 3404554.778774,
>>>       3419358.570571, 3430321.169276, 3440915.89772, 3458087.265837,
>>>       3465003.441, 48525, 49368, 49430, 49596, 50153, 50352, 50449,
>>>       50507, 50530, 50822, 551760, 556305, 560160, 563998, 568125,
>>>       569542, 570670, 572387, 574640, 576494, 577905, 716743.4074,
>>>       725268.5864, 729321.5731, 735610.6375, 740991.229, 741969.5787,
>>>       744834.6127, 744065.912, 745603.2305, 748468.2276, 747909.2496,
>>>       307789.55, 308323.023, 311759.624, 315651.46, 319056.442,
>>>       322272.592, 323422.356, 325702.534, 329052.641, 332851.725,
>>>       333686.876, 396162.2, 398379, 399893, 401534, 403053.4, 
>>> 403937.8,
>>>       403977.3, 403434.2, 403190.7, 403697.9, 403794.7, 130406025,
>>>       131558850, 132121450, 133064400, 133475100, 133386850, 
>>> 133931825,
>>>       133289800, 133836225, 134777725, 135369050, 431473400, 
>>> 435435200,
>>>       437712100, 444064400, 443599800, 447909300, 450495800, 
>>> 452561100,
>>>       456769700, 455081000, 459958000, 178453.593134, 179367.793134,
>>>       180964.533134, 182189.893134, 183625.193134, 184793.473134,
>>>       185981.973134, 186425.153134, 187434.343134, 188324.263134,
>>>       189297.773134, 59062, 59348, 59743, 60320, 60737, 61031,
>>>       61655, 61927, 62282, 62800, 784704, 788709, 794220, 798283,
>>>       800232, 803756, 807187, 810942, 815921, 815323, 44303.821,
>>>       44632.068, 44803.721, 45062.631, 45426.14, 45641.37, 45910.934,
>>>       46027.362, 46203.578, 46453.392, 46685.65896, 279431, 281707,
>>>       284169, 285986, 288064, 289861, 291583, 293145, 294768, 296732,
>>>       298147, 1150761, 1151977, 1169243, 1177835, 1181734, 1192111,
>>>       1197931, 1196262, 1209430, 1215583, 1214691, 168268.356822,
>>>       168865.076317, 170078.764694, 171405.16327, 172777.427869,
>>>       174168.535837, 175400.870886, 175089.0314, 175664.228343,
>>>       176651.744992, 496470, 498582, 499885, 502473, 504487, 504785,
>>>       506842, 510346, 511482, 514019, 513029, 4456057.75, 4481314,
>>>       4505262, 4540889.5, 4580616, 4609563.5, 4649533.75, 4683180,
>>>       4695887, 4731820.25, 4755955), GDPgr = c(1, 0.4, 0.8, 0.7,
>>>       0.6, 1, 0.9, 0.3, 0.2, 0.4, 0.3, 0.7, 0.4, 0.2, 0.7, 0.3,
>>>       0.3, 0.3, 0.4, 0.3, 0.2, 0.6, 1, 1.1, 0.3, 0.4, 0.4, 0.6,
>>>       0.5, 0.1, 0.1, 0.9, 2.3, -1.2, -1, 0.9, 0.8, 0.6, 0.3, 0.8,
>>>       0.1, 0.8, 0.7, 0.6, 0.7, 0.7, 0.4, 0.4, 0.3, 0.3, 0.5, 0.2,
>>>       0.2, 1.7, 0.1, 0.3, 1.1, 0.4, 0.2, 0.1, 0, 0.6, 0.6, 0.8,
>>>       0.7, 0.7, 0.7, 0.2, 0.2, 0.3, 0.4, 0.3, 0.2, 0.4, 1.2, 0.6,
>>>       0.9, 0.7, 0.1, 0.4, -0.1, 0.2, 0.4, -0.1, 0.9, 0.2, 1.1,
>>>       1.2, 1.1, 1, 0.4, 0.7, 1, 1.2, 0.3, 0.5, 0.6, 0.4, 0.4, 0.4,
>>>       0.2, 0, -0.1, -0.1, 0.1, 0, 0.2, 0.9, 0.4, 0.7, 0.3, -0.1,
>>>       0.4, -0.5, 0.4, 0.7, 0.4, 0.8, 0.9, 0.5, 1.5, -0.1, 1, 0.6,
>>>       0.5, 0.9, -0.4, 1.1, 0.9, 0.5, 0.9, 0.7, 0.8, 0.6, 0.6, 0.2,
>>>       0.5, 0.5, 0.5, 0.5, 0.5, 0.7, 1, 0.7, 0.5, 1, 0.4, 0.6, 0.8,
>>>       2, 0.5, 0.7, 0.5, 0.2, 0.4, 0.4, 0.5, 0.6, -0.1, 0.8, 0.7,
>>>       0.4, 0.6, 0.8, 0.5, 0.6, 0.3, 0.4, 0.5, 0.5, 0.6, 0.8, 0.9,
>>>       0.6, 0.7, 0.6, 0.6, 0.5, 0.6, 0.7, 0.5, 0.4, 0.1, 1.5, 0.7,
>>>       0.3, 0.9, 0.5, -0.1, 1.1, 0.5, -0.1, -0.1, 0.4, 0.7, 0.8,
>>>       0.8, 0.8, 0.7, -0.2, 0.3, 0.6, 0.7, 0.4, 0.3, 0.5, 0.4, 0.1,
>>>       0.4, 0.7, 0.2, 0.5, -0.2, 0.5, 0.6, 0.5, 0.8, 0.9, 0.6, 0.9,
>>>       0.7, 0.3, 0.8, 0.5)), class = "data.frame", row.names = c(NA,
>>> -224L))
>>> 
>>> 
>>> 
>>> On 2019-08-24 22:39, Eric Berger wrote:
>>>> Hi Phil,
>>>> Please resubmit your question with the data frame contents shown as
>>>> the output from the command
>>>> dput(t1.txt). This will make it easier for people to run your reprex
>>>> and respond to your question.
>>>> 
>>>> Best,
>>>> Eric
>>>> 
>>>> On Sun, Aug 25, 2019 at 5:26 AM <phil at philipsmith.ca> wrote:
>>>> 
>>>>> I am having difficulty with a chart using ggplot. It is a facetted
>>>>> column chart showing GDP growth rates by country. The columns are
>>>>> coloured navyblue, except that I want to colour the most recent
>>>>> columns,
>>>>> for 2019-Q1 and 2019-Q2, red. For some countries data are available
>>>>> up
>>>>> to 2019-Q2 while for others data are only available up to 2019-Q1.
>>>>> My
>>>>> code and data frame are shown below and it almost works, but not
>>>>> quite.
>>>>> For some reason the red bars for Germany, Korea, Norway, Sweden and
>>>>> United Kingdom are slightly off. Any help will be much appreciated.
>>>>> 
>>>>> Here is my reprex:
>>>>> 
>>>>> library(tidyverse)
>>>>> t1 <- read.table("t1.txt",header=TRUE,sep="\t")
>>>>> col <- rep("navyblue",nrow(t1))
>>>>> for (i in 1:nrow(t1)) {
>>>>> if((t1$TIME[i]=="2019-Q1" | t1$TIME[i]=="2019-Q2")) {
>>>>> col[i] <- "red"}
>>>>> }
>>>>> ggplot(t1) +
>>>>> geom_col(aes(x=TIME,y=GDPgr),fill=col) +
>>>>> facet_wrap(~Country,ncol=3)
>>>>> 
>>>>> Here is my data frame, called "t1.txt":
>>>>> 
>>>>> "TIME"  "LOCATION"      "Country"       "Value" "GDPgr"
>>>>> "2016-Q4"       "AUS"   "Australia"     440518  1
>>>>> "2017-Q1"       "AUS"   "Australia"     442141  0.4
>>>>> "2017-Q2"       "AUS"   "Australia"     445739  0.8
>>>>> "2017-Q3"       "AUS"   "Australia"     448672  0.7
>>>>> "2017-Q4"       "AUS"   "Australia"     451302  0.6
>>>>> "2018-Q1"       "AUS"   "Australia"     455680  1
>>>>> "2018-Q2"       "AUS"   "Australia"     459697  0.9
>>>>> "2018-Q3"       "AUS"   "Australia"     461024  0.3
>>>>> "2018-Q4"       "AUS"   "Australia"     462032  0.2
>>>>> "2019-Q1"       "AUS"   "Australia"     463907  0.4
>>>>> "2016-Q4"       "BEL"   "Belgium"       106675  0.3
>>>>> "2017-Q1"       "BEL"   "Belgium"       107394  0.7
>>>>> "2017-Q2"       "BEL"   "Belgium"       107828  0.4
>>>>> "2017-Q3"       "BEL"   "Belgium"       108003  0.2
>>>>> "2017-Q4"       "BEL"   "Belgium"       108744  0.7
>>>>> "2018-Q1"       "BEL"   "Belgium"       109037  0.3
>>>>> "2018-Q2"       "BEL"   "Belgium"       109386  0.3
>>>>> "2018-Q3"       "BEL"   "Belgium"       109676  0.3
>>>>> "2018-Q4"       "BEL"   "Belgium"       110081  0.4
>>>>> "2019-Q1"       "BEL"   "Belgium"       110459  0.3
>>>>> "2019-Q2"       "BEL"   "Belgium"       110680  0.2
>>>>> "2016-Q4"       "CAN"   "Canada"        493742  0.6
>>>>> "2017-Q1"       "CAN"   "Canada"        498719  1
>>>>> "2017-Q2"       "CAN"   "Canada"        504100.5        1.1
>>>>> "2017-Q3"       "CAN"   "Canada"        505745  0.3
>>>>> "2017-Q4"       "CAN"   "Canada"        507883  0.4
>>>>> "2018-Q1"       "CAN"   "Canada"        509758.75       0.4
>>>>> "2018-Q2"       "CAN"   "Canada"        512958  0.6
>>>>> "2018-Q3"       "CAN"   "Canada"        515639.25       0.5
>>>>> "2018-Q4"       "CAN"   "Canada"        515971.75       0.1
>>>>> "2019-Q1"       "CAN"   "Canada"        516489.5        0.1
>>>>> "2016-Q4"       "DNK"   "Denmark"       499945  0.9
>>>>> "2017-Q1"       "DNK"   "Denmark"       511319  2.3
>>>>> "2017-Q2"       "DNK"   "Denmark"       505254  -1.2
>>>>> "2017-Q3"       "DNK"   "Denmark"       500363  -1
>>>>> "2017-Q4"       "DNK"   "Denmark"       504837  0.9
>>>>> "2018-Q1"       "DNK"   "Denmark"       508633  0.8
>>>>> "2018-Q2"       "DNK"   "Denmark"       511901  0.6
>>>>> "2018-Q3"       "DNK"   "Denmark"       513630  0.3
>>>>> "2018-Q4"       "DNK"   "Denmark"       517726  0.8
>>>>> "2019-Q1"       "DNK"   "Denmark"       518368  0.1
>>>>> "2016-Q4"       "EU28"  "European Union (28 countries)"
>>>>> 3301202.652555  0.8
>>>>> "2017-Q1"       "EU28"  "European Union (28 countries)"
>>>>> 3323886.876398  0.7
>>>>> "2017-Q2"       "EU28"  "European Union (28 countries)"
>>>>> 3345038.332666  0.6
>>>>> "2017-Q3"       "EU28"  "European Union (28 countries)"
>>>>> 3367136.027609  0.7
>>>>> "2017-Q4"       "EU28"  "European Union (28 countries)"
>>>>> 3390431.080785  0.7
>>>>> "2018-Q1"       "EU28"  "European Union (28 countries)"
>>>>> 3404554.778774  0.4
>>>>> "2018-Q2"       "EU28"  "European Union (28 countries)"
>>>>> 3419358.570571  0.4
>>>>> "2018-Q3"       "EU28"  "European Union (28 countries)"
>>>>> 3430321.169276  0.3
>>>>> "2018-Q4"       "EU28"  "European Union (28 countries)"
>>>>> 3440915.89772   0.3
>>>>> "2019-Q1"       "EU28"  "European Union (28 countries)"
>>>>> 3458087.265837  0.5
>>>>> "2019-Q2"       "EU28"  "European Union (28 countries)" 3465003.441
>>>>> 0.2
>>>>> "2016-Q4"       "FIN"   "Finland"       48525   0.2
>>>>> "2017-Q1"       "FIN"   "Finland"       49368   1.7
>>>>> "2017-Q2"       "FIN"   "Finland"       49430   0.1
>>>>> "2017-Q3"       "FIN"   "Finland"       49596   0.3
>>>>> "2017-Q4"       "FIN"   "Finland"       50153   1.1
>>>>> "2018-Q1"       "FIN"   "Finland"       50352   0.4
>>>>> "2018-Q2"       "FIN"   "Finland"       50449   0.2
>>>>> "2018-Q3"       "FIN"   "Finland"       50507   0.1
>>>>> "2018-Q4"       "FIN"   "Finland"       50530   0
>>>>> "2019-Q1"       "FIN"   "Finland"       50822   0.6
>>>>> "2016-Q4"       "FRA"   "France"        551760  0.6
>>>>> "2017-Q1"       "FRA"   "France"        556305  0.8
>>>>> "2017-Q2"       "FRA"   "France"        560160  0.7
>>>>> "2017-Q3"       "FRA"   "France"        563998  0.7
>>>>> "2017-Q4"       "FRA"   "France"        568125  0.7
>>>>> "2018-Q1"       "FRA"   "France"        569542  0.2
>>>>> "2018-Q2"       "FRA"   "France"        570670  0.2
>>>>> "2018-Q3"       "FRA"   "France"        572387  0.3
>>>>> "2018-Q4"       "FRA"   "France"        574640  0.4
>>>>> "2019-Q1"       "FRA"   "France"        576494  0.3
>>>>> "2019-Q2"       "FRA"   "France"        577905  0.2
>>>>> "2016-Q4"       "DEU"   "Germany"       716743.4074     0.4
>>>>> "2017-Q1"       "DEU"   "Germany"       725268.5864     1.2
>>>>> "2017-Q2"       "DEU"   "Germany"       729321.5731     0.6
>>>>> "2017-Q3"       "DEU"   "Germany"       735610.6375     0.9
>>>>> "2017-Q4"       "DEU"   "Germany"       740991.229      0.7
>>>>> "2018-Q1"       "DEU"   "Germany"       741969.5787     0.1
>>>>> "2018-Q2"       "DEU"   "Germany"       744834.6127     0.4
>>>>> "2018-Q3"       "DEU"   "Germany"       744065.912      -0.1
>>>>> "2018-Q4"       "DEU"   "Germany"       745603.2305     0.2
>>>>> "2019-Q1"       "DEU"   "Germany"       748468.2276     0.4
>>>>> "2019-Q2"       "DEU"   "Germany"       747909.2496     -0.1
>>>>> "2016-Q4"       "ISR"   "Israel"        307789.55       0.9
>>>>> "2017-Q1"       "ISR"   "Israel"        308323.023      0.2
>>>>> "2017-Q2"       "ISR"   "Israel"        311759.624      1.1
>>>>> "2017-Q3"       "ISR"   "Israel"        315651.46       1.2
>>>>> "2017-Q4"       "ISR"   "Israel"        319056.442      1.1
>>>>> "2018-Q1"       "ISR"   "Israel"        322272.592      1
>>>>> "2018-Q2"       "ISR"   "Israel"        323422.356      0.4
>>>>> "2018-Q3"       "ISR"   "Israel"        325702.534      0.7
>>>>> "2018-Q4"       "ISR"   "Israel"        329052.641      1
>>>>> "2019-Q1"       "ISR"   "Israel"        332851.725      1.2
>>>>> "2019-Q2"       "ISR"   "Israel"        333686.876      0.3
>>>>> "2016-Q4"       "ITA"   "Italy" 396162.2        0.5
>>>>> "2017-Q1"       "ITA"   "Italy" 398379  0.6
>>>>> "2017-Q2"       "ITA"   "Italy" 399893  0.4
>>>>> "2017-Q3"       "ITA"   "Italy" 401534  0.4
>>>>> "2017-Q4"       "ITA"   "Italy" 403053.4        0.4
>>>>> "2018-Q1"       "ITA"   "Italy" 403937.8        0.2
>>>>> "2018-Q2"       "ITA"   "Italy" 403977.3        0
>>>>> "2018-Q3"       "ITA"   "Italy" 403434.2        -0.1
>>>>> "2018-Q4"       "ITA"   "Italy" 403190.7        -0.1
>>>>> "2019-Q1"       "ITA"   "Italy" 403697.9        0.1
>>>>> "2019-Q2"       "ITA"   "Italy" 403794.7        0
>>>>> "2016-Q4"       "JPN"   "Japan" 130406025       0.2
>>>>> "2017-Q1"       "JPN"   "Japan" 131558850       0.9
>>>>> "2017-Q2"       "JPN"   "Japan" 132121450       0.4
>>>>> "2017-Q3"       "JPN"   "Japan" 133064400       0.7
>>>>> "2017-Q4"       "JPN"   "Japan" 133475100       0.3
>>>>> "2018-Q1"       "JPN"   "Japan" 133386850       -0.1
>>>>> "2018-Q2"       "JPN"   "Japan" 133931825       0.4
>>>>> "2018-Q3"       "JPN"   "Japan" 133289800       -0.5
>>>>> "2018-Q4"       "JPN"   "Japan" 133836225       0.4
>>>>> "2019-Q1"       "JPN"   "Japan" 134777725       0.7
>>>>> "2019-Q2"       "JPN"   "Japan" 135369050       0.4
>>>>> "2016-Q4"       "KOR"   "Korea" 431473400       0.8
>>>>> "2017-Q1"       "KOR"   "Korea" 435435200       0.9
>>>>> "2017-Q2"       "KOR"   "Korea" 437712100       0.5
>>>>> "2017-Q3"       "KOR"   "Korea" 444064400       1.5
>>>>> "2017-Q4"       "KOR"   "Korea" 443599800       -0.1
>>>>> "2018-Q1"       "KOR"   "Korea" 447909300       1
>>>>> "2018-Q2"       "KOR"   "Korea" 450495800       0.6
>>>>> "2018-Q3"       "KOR"   "Korea" 452561100       0.5
>>>>> "2018-Q4"       "KOR"   "Korea" 456769700       0.9
>>>>> "2019-Q1"       "KOR"   "Korea" 455081000       -0.4
>>>>> "2019-Q2"       "KOR"   "Korea" 459958000       1.1
>>>>> "2016-Q4"       "NLD"   "Netherlands"   178453.593134   0.9
>>>>> "2017-Q1"       "NLD"   "Netherlands"   179367.793134   0.5
>>>>> "2017-Q2"       "NLD"   "Netherlands"   180964.533134   0.9
>>>>> "2017-Q3"       "NLD"   "Netherlands"   182189.893134   0.7
>>>>> "2017-Q4"       "NLD"   "Netherlands"   183625.193134   0.8
>>>>> "2018-Q1"       "NLD"   "Netherlands"   184793.473134   0.6
>>>>> "2018-Q2"       "NLD"   "Netherlands"   185981.973134   0.6
>>>>> "2018-Q3"       "NLD"   "Netherlands"   186425.153134   0.2
>>>>> "2018-Q4"       "NLD"   "Netherlands"   187434.343134   0.5
>>>>> "2019-Q1"       "NLD"   "Netherlands"   188324.263134   0.5
>>>>> "2019-Q2"       "NLD"   "Netherlands"   189297.773134   0.5
>>>>> "2016-Q4"       "NZL"   "New Zealand"   59062   0.5
>>>>> "2017-Q1"       "NZL"   "New Zealand"   59348   0.5
>>>>> "2017-Q2"       "NZL"   "New Zealand"   59743   0.7
>>>>> "2017-Q3"       "NZL"   "New Zealand"   60320   1
>>>>> "2017-Q4"       "NZL"   "New Zealand"   60737   0.7
>>>>> "2018-Q1"       "NZL"   "New Zealand"   61031   0.5
>>>>> "2018-Q2"       "NZL"   "New Zealand"   61655   1
>>>>> "2018-Q3"       "NZL"   "New Zealand"   61927   0.4
>>>>> "2018-Q4"       "NZL"   "New Zealand"   62282   0.6
>>>>> "2019-Q1"       "NZL"   "New Zealand"   62800   0.8
>>>>> "2016-Q4"       "NOR"   "Norway"        784704  2
>>>>> "2017-Q1"       "NOR"   "Norway"        788709  0.5
>>>>> "2017-Q2"       "NOR"   "Norway"        794220  0.7
>>>>> "2017-Q3"       "NOR"   "Norway"        798283  0.5
>>>>> "2017-Q4"       "NOR"   "Norway"        800232  0.2
>>>>> "2018-Q1"       "NOR"   "Norway"        803756  0.4
>>>>> "2018-Q2"       "NOR"   "Norway"        807187  0.4
>>>>> "2018-Q3"       "NOR"   "Norway"        810942  0.5
>>>>> "2018-Q4"       "NOR"   "Norway"        815921  0.6
>>>>> "2019-Q1"       "NOR"   "Norway"        815323  -0.1
>>>>> "2016-Q4"       "PRT"   "Portugal"      44303.821       0.8
>>>>> "2017-Q1"       "PRT"   "Portugal"      44632.068       0.7
>>>>> "2017-Q2"       "PRT"   "Portugal"      44803.721       0.4
>>>>> "2017-Q3"       "PRT"   "Portugal"      45062.631       0.6
>>>>> "2017-Q4"       "PRT"   "Portugal"      45426.14        0.8
>>>>> "2018-Q1"       "PRT"   "Portugal"      45641.37        0.5
>>>>> "2018-Q2"       "PRT"   "Portugal"      45910.934       0.6
>>>>> "2018-Q3"       "PRT"   "Portugal"      46027.362       0.3
>>>>> "2018-Q4"       "PRT"   "Portugal"      46203.578       0.4
>>>>> "2019-Q1"       "PRT"   "Portugal"      46453.392       0.5
>>>>> "2019-Q2"       "PRT"   "Portugal"      46685.65896     0.5
>>>>> "2016-Q4"       "ESP"   "Spain" 279431  0.6
>>>>> "2017-Q1"       "ESP"   "Spain" 281707  0.8
>>>>> "2017-Q2"       "ESP"   "Spain" 284169  0.9
>>>>> "2017-Q3"       "ESP"   "Spain" 285986  0.6
>>>>> "2017-Q4"       "ESP"   "Spain" 288064  0.7
>>>>> "2018-Q1"       "ESP"   "Spain" 289861  0.6
>>>>> "2018-Q2"       "ESP"   "Spain" 291583  0.6
>>>>> "2018-Q3"       "ESP"   "Spain" 293145  0.5
>>>>> "2018-Q4"       "ESP"   "Spain" 294768  0.6
>>>>> "2019-Q1"       "ESP"   "Spain" 296732  0.7
>>>>> "2019-Q2"       "ESP"   "Spain" 298147  0.5
>>>>> "2016-Q4"       "SWE"   "Sweden"        1150761 0.4
>>>>> "2017-Q1"       "SWE"   "Sweden"        1151977 0.1
>>>>> "2017-Q2"       "SWE"   "Sweden"        1169243 1.5
>>>>> "2017-Q3"       "SWE"   "Sweden"        1177835 0.7
>>>>> "2017-Q4"       "SWE"   "Sweden"        1181734 0.3
>>>>> "2018-Q1"       "SWE"   "Sweden"        1192111 0.9
>>>>> "2018-Q2"       "SWE"   "Sweden"        1197931 0.5
>>>>> "2018-Q3"       "SWE"   "Sweden"        1196262 -0.1
>>>>> "2018-Q4"       "SWE"   "Sweden"        1209430 1.1
>>>>> "2019-Q1"       "SWE"   "Sweden"        1215583 0.5
>>>>> "2019-Q2"       "SWE"   "Sweden"        1214691 -0.1
>>>>> "2016-Q4"       "CHE"   "Switzerland"   168268.356822   -0.1
>>>>> "2017-Q1"       "CHE"   "Switzerland"   168865.076317   0.4
>>>>> "2017-Q2"       "CHE"   "Switzerland"   170078.764694   0.7
>>>>> "2017-Q3"       "CHE"   "Switzerland"   171405.16327    0.8
>>>>> "2017-Q4"       "CHE"   "Switzerland"   172777.427869   0.8
>>>>> "2018-Q1"       "CHE"   "Switzerland"   174168.535837   0.8
>>>>> "2018-Q2"       "CHE"   "Switzerland"   175400.870886   0.7
>>>>> "2018-Q3"       "CHE"   "Switzerland"   175089.0314     -0.2
>>>>> "2018-Q4"       "CHE"   "Switzerland"   175664.228343   0.3
>>>>> "2019-Q1"       "CHE"   "Switzerland"   176651.744992   0.6
>>>>> "2016-Q4"       "GBR"   "United Kingdom"        496470  0.7
>>>>> "2017-Q1"       "GBR"   "United Kingdom"        498582  0.4
>>>>> "2017-Q2"       "GBR"   "United Kingdom"        499885  0.3
>>>>> "2017-Q3"       "GBR"   "United Kingdom"        502473  0.5
>>>>> "2017-Q4"       "GBR"   "United Kingdom"        504487  0.4
>>>>> "2018-Q1"       "GBR"   "United Kingdom"        504785  0.1
>>>>> "2018-Q2"       "GBR"   "United Kingdom"        506842  0.4
>>>>> "2018-Q3"       "GBR"   "United Kingdom"        510346  0.7
>>>>> "2018-Q4"       "GBR"   "United Kingdom"        511482  0.2
>>>>> "2019-Q1"       "GBR"   "United Kingdom"        514019  0.5
>>>>> "2019-Q2"       "GBR"   "United Kingdom"        513029  -0.2
>>>>> "2016-Q4"       "USA"   "United States" 4456057.75      0.5
>>>>> "2017-Q1"       "USA"   "United States" 4481314 0.6
>>>>> "2017-Q2"       "USA"   "United States" 4505262 0.5
>>>>> "2017-Q3"       "USA"   "United States" 4540889.5       0.8
>>>>> "2017-Q4"       "USA"   "United States" 4580616 0.9
>>>>> "2018-Q1"       "USA"   "United States" 4609563.5       0.6
>>>>> "2018-Q2"       "USA"   "United States" 4649533.75      0.9
>>>>> "2018-Q3"       "USA"   "United States" 4683180 0.7
>>>>> "2018-Q4"       "USA"   "United States" 4695887 0.3
>>>>> "2019-Q1"       "USA"   "United States" 4731820.25      0.8
>>>>> "2019-Q2"       "USA"   "United States" 4755955 0.5
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


From bgunter@4567 @end|ng |rom gm@||@com  Sun Aug 25 16:57:46 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 25 Aug 2019 07:57:46 -0700
Subject: [R] Labeling Dates of different length on two in axis plot
In-Reply-To: <CAC8ss33xjKkx+HVPCxAVGF+ttEd2W=YjDLOcM=8=nxJ9Ks-b6Q@mail.gmail.com>
References: <CAC8ss33xjKkx+HVPCxAVGF+ttEd2W=YjDLOcM=8=nxJ9Ks-b6Q@mail.gmail.com>
Message-ID: <CAGxFJbTt-NSFjVF_gyHhPNAjLKLCopAz_h-YDgCD4C3TfMLQoQ@mail.gmail.com>

I think you will need to show us your code to get useful help (see the
posting guide). A small subset of your data and a reprex may also be
needed. R has several different graphics systems, and the answer depends on
which you are using. In base graphics, the axis() function (see its Help
page) may be what you need, but that's just a guess. Others may be able to
be more specific.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Aug 24, 2019 at 9:49 PM Ogbos Okike <giftedlife2014 at gmail.com>
wrote:

> Dear Contributors,
> I have two dataset of different lengths, each containing year, month,
> day and counts.
> After converting the date using as.Date function, I plotted the two
> dateset on one graph. That was fine.
>
> I am, however, having problems with the axis 1 where I am trying to
> put the dates.
> Since the two dates are not exactly the same, I stored the first date
> as x1 and the second x2. x1 runs from 1953-01-02 to 2006-11-15 while
> the range of x2 is between 1957-07-26 and 1994-07-17.
> I tired a number of approaches but the one that seems to come close to
> what I am looking for is: axis.Date(1, at=seq(min(x1), max(x1), by="50
> mon"), format="%Y").
>
> Unfortunately, it only labeled the axis up to 1994 even when I tried
> to replace x1 with x2 in the code. Since one of the dates is up to
> 2006, I I wish to display the minimum (1953) and maximum  (2006) dates
> and some possible intermediary dates.
> I am always indebted to you.
>
> Best wishes
> Ogbos
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From g||ted|||e2014 @end|ng |rom gm@||@com  Sun Aug 25 17:52:21 2019
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Sun, 25 Aug 2019 16:52:21 +0100
Subject: [R] Labeling Dates of different length on two in axis plot
In-Reply-To: <CAGxFJbTt-NSFjVF_gyHhPNAjLKLCopAz_h-YDgCD4C3TfMLQoQ@mail.gmail.com>
References: <CAC8ss33xjKkx+HVPCxAVGF+ttEd2W=YjDLOcM=8=nxJ9Ks-b6Q@mail.gmail.com>
 <CAGxFJbTt-NSFjVF_gyHhPNAjLKLCopAz_h-YDgCD4C3TfMLQoQ@mail.gmail.com>
Message-ID: <CAC8ss32dwkq2o0pZTdo69xM+XoHCF=iTsgo897oP3+zT+GNXdw@mail.gmail.com>

Dear Jim and Bert,
Thank your for looking into this for me. I will try to reproduce a
part of my data and the plot I have.

Data 1:
53 01 02 -1.28792560381641
53 01 06 -1.1854773963453
53 01 08 -1.55920165458006
53 01 15 -1.29196482429683
53 01 20 -1.06082194329819
53 01 22 -1.15430411152234
53 01 24 -1.14775155345262
53 01 26 -1.19116423468105
53 01 28 -0.924542121164923
53 01 31 -0.869543637175866
53 02 03 -0.764466677206437
53 02 05 -0.707705051599105
53 02 07 -0.750908466521414
53 02 16 -1.6848903649715
53 02 20 -0.760876361940755
53 02 22 -0.803816591115474
53 02 26 -0.818160891300922
53 02 28 -0.831013020625678
53 03 04 -1.06661138281899
till the year 2006. The length is 1575

Data B:
57 07 26 -3.62351759422703
57 07 29 -4.51297753038541
57 08 02 -3.29214621128099
57 08 05 -6.44144118698436
57 08 16 -3.09828319252649
57 08 22 -1.58619965424759
57 08 28 -4.3638763217976
57 08 30 -9.93305062749234
57 09 03 -9.29132498735799
57 09 05 -8.50042591710719
57 09 13 -5.58659819576993
57 09 19 -2.4834966827445
57 09 23 -5.69132573226493
57 09 26 -3.6296458026372
to  the year 1994. The length is 1203.

The plot is attached. How to label the x-axis is the problem.
I will send the code in another email so that the moderator will let it go.
Thank again
Best
Ogbos

On Sun, Aug 25, 2019 at 3:58 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
> I think you will need to show us your code to get useful help (see the posting guide). A small subset of your data and a reprex may also be needed. R has several different graphics systems, and the answer depends on which you are using. In base graphics, the axis() function (see its Help page) may be what you need, but that's just a guess. Others may be able to be more specific.
>
> Cheers,
> Bert
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Sat, Aug 24, 2019 at 9:49 PM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>>
>> Dear Contributors,
>> I have two dataset of different lengths, each containing year, month,
>> day and counts.
>> After converting the date using as.Date function, I plotted the two
>> dateset on one graph. That was fine.
>>
>> I am, however, having problems with the axis 1 where I am trying to
>> put the dates.
>> Since the two dates are not exactly the same, I stored the first date
>> as x1 and the second x2. x1 runs from 1953-01-02 to 2006-11-15 while
>> the range of x2 is between 1957-07-26 and 1994-07-17.
>> I tired a number of approaches but the one that seems to come close to
>> what I am looking for is: axis.Date(1, at=seq(min(x1), max(x1), by="50
>> mon"), format="%Y").
>>
>> Unfortunately, it only labeled the axis up to 1994 even when I tried
>> to replace x1 with x2 in the code. Since one of the dates is up to
>> 2006, I I wish to display the minimum (1953) and maximum  (2006) dates
>> and some possible intermediary dates.
>> I am always indebted to you.
>>
>> Best wishes
>> Ogbos
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Ogbos.pdf
Type: application/pdf
Size: 39312 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190825/d6c0f233/attachment.pdf>

From g||ted|||e2014 @end|ng |rom gm@||@com  Sun Aug 25 19:06:07 2019
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Sun, 25 Aug 2019 18:06:07 +0100
Subject: [R] Labeling Dates of different length on two in axis plot
In-Reply-To: <CAC8ss32dwkq2o0pZTdo69xM+XoHCF=iTsgo897oP3+zT+GNXdw@mail.gmail.com>
References: <CAC8ss33xjKkx+HVPCxAVGF+ttEd2W=YjDLOcM=8=nxJ9Ks-b6Q@mail.gmail.com>
 <CAGxFJbTt-NSFjVF_gyHhPNAjLKLCopAz_h-YDgCD4C3TfMLQoQ@mail.gmail.com>
 <CAC8ss32dwkq2o0pZTdo69xM+XoHCF=iTsgo897oP3+zT+GNXdw@mail.gmail.com>
Message-ID: <CAC8ss33fNYZHZUy9eztS8=vaKZMp8okULinpWj0mY7gWxZz_Qw@mail.gmail.com>

Dear Bert and Jim,
Here is the code:
data <- read.table("CLMX.txt", col.names = c("year", "month", "day", "CLMX"))

new.century <- data$year < 50

data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)

data$date <- as.Date(ISOdate(data$year, data$month, data$day))
x1 = data$date


 Li<-data$CLMX
###########
data <- read.table("DPRV.txt", col.names = c("year", "month", "day", "DPRV"))

new.century <- data$year < 50

data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)

data$date <- as.Date(ISOdate(data$year, data$month, data$day))
x2 = data$date


CR<-data$DPRV



 ###################
  date_x1<-seq(as.Date("1953-01-02"),as.Date("2006-11-15"),length.out=1575)
date_x2<-seq(as.Date("1957-07-26"),as.Date("1994-07-17"),length.out=1203)


pdf("CLMXandDPRV.pdf")
 par(mar = c(5, 4, 4, 4) + 2)
library(plotrix)
plot(date_x1,Li,pch=16,axes=FALSE,xlab="",ylab="",type="l",col="black",ylim=c(-0.5001663,-10.27402))
points(date_x1,Li,col="black")
axis(2, ylim=c(-0.5001663,-10.27402),col="black",las=1)
mtext("FD (%)", side=2, line=4)
text(as.Date("1992-02-21"),3523,"FD1")

text(as.Date("1972-02-28"),-8,"Total FDs = 1203",col="red")
text(as.Date("1972-03-02"),-9,"Total FDs = 1575")
#box()

par(new=TRUE)
plot(date_x2,CR, pch=16,
xlab="",ylab="",ylim=c(-0.5030138,-14.39884),axes=FALSE,type="l",col="red")
points(date_x2,CR,col="red")
mtext("FD (%)",side=4,col="red",line=3)
axis(4, ylim=c(-0.5030138,-14.39884), col="red",col.axis="red",las=1)




mtext("Date",side=1,col="black",line=2.5)

legend("topleft",col=c("red","black"),lty=1,legend=c("DPRV","CLMX"))

dev.off()

Thanks again.
Best wishes
Ogbos


On Sun, Aug 25, 2019 at 4:52 PM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>
> Dear Jim and Bert,
> Thank your for looking into this for me. I will try to reproduce a
> part of my data and the plot I have.
>
> Data 1:
> 53 01 02 -1.28792560381641
> 53 01 06 -1.1854773963453
> 53 01 08 -1.55920165458006
> 53 01 15 -1.29196482429683
> 53 01 20 -1.06082194329819
> 53 01 22 -1.15430411152234
> 53 01 24 -1.14775155345262
> 53 01 26 -1.19116423468105
> 53 01 28 -0.924542121164923
> 53 01 31 -0.869543637175866
> 53 02 03 -0.764466677206437
> 53 02 05 -0.707705051599105
> 53 02 07 -0.750908466521414
> 53 02 16 -1.6848903649715
> 53 02 20 -0.760876361940755
> 53 02 22 -0.803816591115474
> 53 02 26 -0.818160891300922
> 53 02 28 -0.831013020625678
> 53 03 04 -1.06661138281899
> till the year 2006. The length is 1575
>
> Data B:
> 57 07 26 -3.62351759422703
> 57 07 29 -4.51297753038541
> 57 08 02 -3.29214621128099
> 57 08 05 -6.44144118698436
> 57 08 16 -3.09828319252649
> 57 08 22 -1.58619965424759
> 57 08 28 -4.3638763217976
> 57 08 30 -9.93305062749234
> 57 09 03 -9.29132498735799
> 57 09 05 -8.50042591710719
> 57 09 13 -5.58659819576993
> 57 09 19 -2.4834966827445
> 57 09 23 -5.69132573226493
> 57 09 26 -3.6296458026372
> to  the year 1994. The length is 1203.
>
> The plot is attached. How to label the x-axis is the problem.
> I will send the code in another email so that the moderator will let it go.
> Thank again
> Best
> Ogbos
>
> On Sun, Aug 25, 2019 at 3:58 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:
> >
> > I think you will need to show us your code to get useful help (see the posting guide). A small subset of your data and a reprex may also be needed. R has several different graphics systems, and the answer depends on which you are using. In base graphics, the axis() function (see its Help page) may be what you need, but that's just a guess. Others may be able to be more specific.
> >
> > Cheers,
> > Bert
> >
> > Bert Gunter
> >
> > "The trouble with having an open mind is that people keep coming along and sticking things into it."
> > -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >
> >
> > On Sat, Aug 24, 2019 at 9:49 PM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
> >>
> >> Dear Contributors,
> >> I have two dataset of different lengths, each containing year, month,
> >> day and counts.
> >> After converting the date using as.Date function, I plotted the two
> >> dateset on one graph. That was fine.
> >>
> >> I am, however, having problems with the axis 1 where I am trying to
> >> put the dates.
> >> Since the two dates are not exactly the same, I stored the first date
> >> as x1 and the second x2. x1 runs from 1953-01-02 to 2006-11-15 while
> >> the range of x2 is between 1957-07-26 and 1994-07-17.
> >> I tired a number of approaches but the one that seems to come close to
> >> what I am looking for is: axis.Date(1, at=seq(min(x1), max(x1), by="50
> >> mon"), format="%Y").
> >>
> >> Unfortunately, it only labeled the axis up to 1994 even when I tried
> >> to replace x1 with x2 in the code. Since one of the dates is up to
> >> 2006, I I wish to display the minimum (1953) and maximum  (2006) dates
> >> and some possible intermediary dates.
> >> I am always indebted to you.
> >>
> >> Best wishes
> >> Ogbos
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.


From Anne@Ch@tton @end|ng |rom hcuge@ch  Mon Aug 26 12:24:24 2019
From: Anne@Ch@tton @end|ng |rom hcuge@ch (CHATTON Anne)
Date: Mon, 26 Aug 2019 10:24:24 +0000
Subject: [R] Code modification for post-hoc power
Message-ID: <931267eff584419995fcce370ae6058a@wexchprod08.huge.ad.hcuge.ch>

Hello everybody,

I am trying to accommodate the R codes provided by Donohue for sample size calculation in the package "longpower" with lmmpower function to estimate the post-hoc power (asked by a reviewer) of a binary GEE model with a three-way interaction (time x condition x continuous predictor) given a fixed sample size. In other words instead of the sample size I would like to estimate the power of my study. 

Could anyone please help me to modify these codes as to obtain the power I'm looking for. 

I would really appreciate receiving any feedback on this subject.

Yours sincerely,
 
Anne


From m@rc_@chw@rtz @end|ng |rom me@com  Mon Aug 26 14:42:33 2019
From: m@rc_@chw@rtz @end|ng |rom me@com (Marc Schwartz)
Date: Mon, 26 Aug 2019 08:42:33 -0400
Subject: [R] Code modification for post-hoc power
In-Reply-To: <931267eff584419995fcce370ae6058a@wexchprod08.huge.ad.hcuge.ch>
References: <931267eff584419995fcce370ae6058a@wexchprod08.huge.ad.hcuge.ch>
Message-ID: <FC6DDE74-7C37-47BB-8B24-FE1787B1D183@me.com>


> On Aug 26, 2019, at 6:24 AM, CHATTON Anne via R-help <r-help at r-project.org> wrote:
> 
> Hello everybody,
> 
> I am trying to accommodate the R codes provided by Donohue for sample size calculation in the package "longpower" with lmmpower function to estimate the post-hoc power (asked by a reviewer) of a binary GEE model with a three-way interaction (time x condition x continuous predictor) given a fixed sample size. In other words instead of the sample size I would like to estimate the power of my study. 
> 
> Could anyone please help me to modify these codes as to obtain the power I'm looking for. 
> 
> I would really appreciate receiving any feedback on this subject.
> 
> Yours sincerely,
> 
> Anne


Hi,

Three comments:

1. Don't calculate post hoc power. Do a Google search and you will find a plethora of papers and discussions on why not, including these:

  The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis
  The American Statistician, February 2001, Vol. 55, No. 1
  https://www.vims.edu/people/hoenig_jm/pubs/hoenig2.pdf

  Post Hoc Power: Tables and Commentary
  https://stat.uiowa.edu/sites/stat.uiowa.edu/files/techrep/tr378.pdf

  Observed power, and what to do if your editor asks for post-hoc power analyses
  http://daniellakens.blogspot.com/2014/12/observed-power-and-what-to-do-if-your.html

  Retraction Watch:
  Statisticians clamor for retraction of paper by Harvard researchers they say uses a ?nonsense statistic?
  https://retractionwatch.com/2019/06/19/statisticians-clamor-for-retraction-of-paper-by-harvard-researchers-they-say-uses-a-nonsense-statistic/

  PubPeer Comments on the paper cited in the above RW post:
  https://pubpeer.com/publications/4399282A80691D9421B497E8316CF6

  A discussion on Frank's Data Methods forum also related to the same paper cited above:
  "Observed Power" and other "Power" Issues
  https://discourse.datamethods.org/t/observed-power-and-other-power-issues/731/30


2. If you are still compelled (voluntarily or involuntarily), you may want to review the vignette for the longpower package which may have some insights, and/or contact the package maintainer for additional guidance on how to structure the code. See the vignette here:

  https://cran.r-project.org/web/packages/longpower/vignettes/longpower.pdf


3. Don't calculate post hoc power.


Regards,

Marc Schwartz


From he|mut@@chuetz @end|ng |rom beb@c@@t  Mon Aug 26 16:31:47 2019
From: he|mut@@chuetz @end|ng |rom beb@c@@t (=?UTF-8?Q?Helmut_Sch=c3=bctz?=)
Date: Mon, 26 Aug 2019 16:31:47 +0200
Subject: [R] TOC in vignette (knitr::rmarkdown)
Message-ID: <6319ea48-aeac-4c2a-8285-7ac5b73ddd0c@bebac.at>

Dear all,

I can't figure out how to include a table of contents in a vignette.

What I have now:
---
title: "myPackage"
subtitle: mySubtitle"
author: "Me"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
 ? %\VignetteIndexEntry{replicateBE}
 ? %\VignetteEngine{knitr::rmarkdown}
 ? %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
 ? collapse = TRUE,
 ? comment = "#>"
)
```

This one works fine in my README.Rdm:
---
title: "myPackage"
output:
 ? github_document:
 ??? toc: true
 ??? toc_depth: 4
---

Cheers,
Helmut

-- 
Ing. Helmut Sch?tz
BEBAC?? Consultancy Services for
Bioequivalence and Bioavailability Studies
E helmut.schuetz at bebac.at
W https://bebac.at/
C https://bebac.at/Contact.htm
F https://forum.bebac.at/


From bgunter@4567 @end|ng |rom gm@||@com  Mon Aug 26 16:52:52 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 26 Aug 2019 07:52:52 -0700
Subject: [R] TOC in vignette (knitr::rmarkdown)
In-Reply-To: <6319ea48-aeac-4c2a-8285-7ac5b73ddd0c@bebac.at>
References: <6319ea48-aeac-4c2a-8285-7ac5b73ddd0c@bebac.at>
Message-ID: <CAGxFJbSx+CLjhPi4J5HdZN2=7Hyp+HRePr39AV=E7TtaCDGxjg@mail.gmail.com>

Almost surely better posted on the r-package-devel mailing list.

Cheers,

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Aug 26, 2019 at 7:32 AM Helmut Sch?tz <helmut.schuetz at bebac.at>
wrote:

> Dear all,
>
> I can't figure out how to include a table of contents in a vignette.
>
> What I have now:
> ---
> title: "myPackage"
> subtitle: mySubtitle"
> author: "Me"
> date: "`r Sys.Date()`"
> output: rmarkdown::html_vignette
> vignette: >
>    %\VignetteIndexEntry{replicateBE}
>    %\VignetteEngine{knitr::rmarkdown}
>    %\VignetteEncoding{UTF-8}
> ---
>
> ```{r, include = FALSE}
> knitr::opts_chunk$set(
>    collapse = TRUE,
>    comment = "#>"
> )
> ```
>
> This one works fine in my README.Rdm:
> ---
> title: "myPackage"
> output:
>    github_document:
>      toc: true
>      toc_depth: 4
> ---
>
> Cheers,
> Helmut
>
> --
> Ing. Helmut Sch?tz
> BEBAC ? Consultancy Services for
> Bioequivalence and Bioavailability Studies
> E helmut.schuetz at bebac.at
> W https://bebac.at/
> C https://bebac.at/Contact.htm
> F https://forum.bebac.at/
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From x|e @end|ng |rom y|hu|@n@me  Mon Aug 26 16:59:16 2019
From: x|e @end|ng |rom y|hu|@n@me (Yihui Xie)
Date: Mon, 26 Aug 2019 09:59:16 -0500
Subject: [R] TOC in vignette (knitr::rmarkdown)
In-Reply-To: <6319ea48-aeac-4c2a-8285-7ac5b73ddd0c@bebac.at>
References: <6319ea48-aeac-4c2a-8285-7ac5b73ddd0c@bebac.at>
Message-ID: <CANROs4dUozhd2HjkDYJtsQ6YLkNUmCDnNRKnmzpKH2cCUnVYmA@mail.gmail.com>

output:
  rmarkdown::html_vignette:
    toc: true

The syntax is the same for all R Markdown output formats:
https://bookdown.org/yihui/rmarkdown/html-document.html#table-of-contents

Regards,
Yihui
--
https://yihui.name

On Mon, Aug 26, 2019 at 9:32 AM Helmut Sch?tz <helmut.schuetz at bebac.at> wrote:
>
> Dear all,
>
> I can't figure out how to include a table of contents in a vignette.
>
> What I have now:
> ---
> title: "myPackage"
> subtitle: mySubtitle"
> author: "Me"
> date: "`r Sys.Date()`"
> output: rmarkdown::html_vignette
> vignette: >
>    %\VignetteIndexEntry{replicateBE}
>    %\VignetteEngine{knitr::rmarkdown}
>    %\VignetteEncoding{UTF-8}
> ---
>
> ```{r, include = FALSE}
> knitr::opts_chunk$set(
>    collapse = TRUE,
>    comment = "#>"
> )
> ```
>
> This one works fine in my README.Rdm:
> ---
> title: "myPackage"
> output:
>    github_document:
>      toc: true
>      toc_depth: 4
> ---
>
> Cheers,
> Helmut


From he|mut@@chuetz @end|ng |rom beb@c@@t  Mon Aug 26 17:05:47 2019
From: he|mut@@chuetz @end|ng |rom beb@c@@t (=?UTF-8?Q?Helmut_Sch=c3=bctz?=)
Date: Mon, 26 Aug 2019 17:05:47 +0200
Subject: [R] TOC in vignette (knitr::rmarkdown)
In-Reply-To: <CANROs4dUozhd2HjkDYJtsQ6YLkNUmCDnNRKnmzpKH2cCUnVYmA@mail.gmail.com>
References: <6319ea48-aeac-4c2a-8285-7ac5b73ddd0c@bebac.at>
 <CANROs4dUozhd2HjkDYJtsQ6YLkNUmCDnNRKnmzpKH2cCUnVYmA@mail.gmail.com>
Message-ID: <47f4347d-4136-d98b-2fb0-f700b86d0359@bebac.at>

Hi Yihui,

THX; you made my day!

Cheers,
Helmut

Yihui Xie wrote on 2019-08-26 16:59:
> output:
>    rmarkdown::html_vignette:
>      toc: true
>
> The syntax is the same for all R Markdown output formats:
> https://bookdown.org/yihui/rmarkdown/html-document.html#table-of-contents
>
> Regards,
> Yihui

-- 
Ing. Helmut Sch?tz
BEBAC?? Consultancy Services for
Bioequivalence and Bioavailability Studies
Neubaugasse 36/11
1070 Vienna, Austria
T +43 1 2311746
M +43 699 10792458
E helmut.schuetz at bebac.at
W https://bebac.at/
C https://bebac.at/Contact.htm
F https://forum.bebac.at/
GIS 24799386, VAT?ATU61115625, DUNS?300370568, EORI?ATEOS1000096209
GDPR https://bebac.at/Data-Protection.htm
This e-mail is confidential and may also be legally privileged. If you
are not the intended recipient please reply to sender, do not disclose
its contents to any person and delete the e-mail. Any unauthorized
review, use, disclosure, copying or distribution is strictly prohibited.


From he|mut@@chuetz @end|ng |rom beb@c@@t  Mon Aug 26 17:09:57 2019
From: he|mut@@chuetz @end|ng |rom beb@c@@t (=?UTF-8?Q?Helmut_Sch=c3=bctz?=)
Date: Mon, 26 Aug 2019 17:09:57 +0200
Subject: [R] TOC in vignette (knitr::rmarkdown)
In-Reply-To: <CAGxFJbSx+CLjhPi4J5HdZN2=7Hyp+HRePr39AV=E7TtaCDGxjg@mail.gmail.com>
References: <6319ea48-aeac-4c2a-8285-7ac5b73ddd0c@bebac.at>
 <CAGxFJbSx+CLjhPi4J5HdZN2=7Hyp+HRePr39AV=E7TtaCDGxjg@mail.gmail.com>
Message-ID: <b6f08848-e0fc-bc27-56a2-f58732c68654@bebac.at>

Hi Bert,

Bert Gunter wrote on 2019-08-26 16:52:
> Almost surely better posted on the r-package-devel mailing list.

Do you have a macro for those ?answers? of yours? SCNR.
*Which* kind of questions are -- in your opinion -- suitable to be 
posted *here*?

Cheers,
Helmut

-- 
Ing. Helmut Sch?tz
BEBAC?? Consultancy Services for
Bioequivalence and Bioavailability Studies
W https://bebac.at/
C https://bebac.at/Contact.htm
F https://forum.bebac.at/


	[[alternative HTML version deleted]]


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Mon Aug 26 18:29:10 2019
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Mon, 26 Aug 2019 17:29:10 +0100
Subject: [R] Code modification for post-hoc power
In-Reply-To: <FC6DDE74-7C37-47BB-8B24-FE1787B1D183@me.com>
References: <931267eff584419995fcce370ae6058a@wexchprod08.huge.ad.hcuge.ch>
 <FC6DDE74-7C37-47BB-8B24-FE1787B1D183@me.com>
Message-ID: <87608904-6058-9f52-3017-356b395a20ef@dewey.myzen.co.uk>

Dear Anne

In addition to Marc's comments if you are forced to do this then, 
assuming your package computes sample size from power then just feed it 
a range of powers and find the one for which it calculates the sample 
size you had. There is a more elegant way to do this using uniroot but 
brute force should work.

Michael

On 26/08/2019 13:42, Marc Schwartz via R-help wrote:
> 
>> On Aug 26, 2019, at 6:24 AM, CHATTON Anne via R-help <r-help at r-project.org> wrote:
>>
>> Hello everybody,
>>
>> I am trying to accommodate the R codes provided by Donohue for sample size calculation in the package "longpower" with lmmpower function to estimate the post-hoc power (asked by a reviewer) of a binary GEE model with a three-way interaction (time x condition x continuous predictor) given a fixed sample size. In other words instead of the sample size I would like to estimate the power of my study.
>>
>> Could anyone please help me to modify these codes as to obtain the power I'm looking for.
>>
>> I would really appreciate receiving any feedback on this subject.
>>
>> Yours sincerely,
>>
>> Anne
> 
> 
> Hi,
> 
> Three comments:
> 
> 1. Don't calculate post hoc power. Do a Google search and you will find a plethora of papers and discussions on why not, including these:
> 
>    The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis
>    The American Statistician, February 2001, Vol. 55, No. 1
>    https://www.vims.edu/people/hoenig_jm/pubs/hoenig2.pdf
> 
>    Post Hoc Power: Tables and Commentary
>    https://stat.uiowa.edu/sites/stat.uiowa.edu/files/techrep/tr378.pdf
> 
>    Observed power, and what to do if your editor asks for post-hoc power analyses
>    http://daniellakens.blogspot.com/2014/12/observed-power-and-what-to-do-if-your.html
> 
>    Retraction Watch:
>    Statisticians clamor for retraction of paper by Harvard researchers they say uses a ?nonsense statistic?
>    https://retractionwatch.com/2019/06/19/statisticians-clamor-for-retraction-of-paper-by-harvard-researchers-they-say-uses-a-nonsense-statistic/
> 
>    PubPeer Comments on the paper cited in the above RW post:
>    https://pubpeer.com/publications/4399282A80691D9421B497E8316CF6
> 
>    A discussion on Frank's Data Methods forum also related to the same paper cited above:
>    "Observed Power" and other "Power" Issues
>    https://discourse.datamethods.org/t/observed-power-and-other-power-issues/731/30
> 
> 
> 2. If you are still compelled (voluntarily or involuntarily), you may want to review the vignette for the longpower package which may have some insights, and/or contact the package maintainer for additional guidance on how to structure the code. See the vignette here:
> 
>    https://cran.r-project.org/web/packages/longpower/vignettes/longpower.pdf
> 
> 
> 3. Don't calculate post hoc power.
> 
> 
> Regards,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ---
> This email has been checked for viruses by AVG.
> https://www.avg.com
> 
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From pd@|gd @end|ng |rom gm@||@com  Mon Aug 26 19:18:21 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Mon, 26 Aug 2019 19:18:21 +0200
Subject: [R] Code modification for post-hoc power
In-Reply-To: <87608904-6058-9f52-3017-356b395a20ef@dewey.myzen.co.uk>
References: <931267eff584419995fcce370ae6058a@wexchprod08.huge.ad.hcuge.ch>
 <FC6DDE74-7C37-47BB-8B24-FE1787B1D183@me.com>
 <87608904-6058-9f52-3017-356b395a20ef@dewey.myzen.co.uk>
Message-ID: <D74D56F3-079E-4F54-9920-B93FB5344269@gmail.com>

That doesn't work. In caricature, post-hoc power is

- I observe a difference of nearly zero 
- However, to find a significant difference of that size I'd need 200000 observations
- I only used 100 observations
- Therefore my study is useless and can be discarded

(or: I calculate the probability of Type II error if the true difference is the observed and get 0.999... Therefore, etc.)

Best way out is a confidence interval. Second best (but in principle wrong) is to redo the pre-study power calculation and say that the study was designed to find a difference of delta, which it clearly didn't, so the true difference is probably less than delta.

-pd

> On 26 Aug 2019, at 18:29 , Michael Dewey <lists at dewey.myzen.co.uk> wrote:
> 
> Dear Anne
> 
> In addition to Marc's comments if you are forced to do this then, assuming your package computes sample size from power then just feed it a range of powers and find the one for which it calculates the sample size you had. There is a more elegant way to do this using uniroot but brute force should work.
> 
> Michael
> 
> On 26/08/2019 13:42, Marc Schwartz via R-help wrote:
>>> On Aug 26, 2019, at 6:24 AM, CHATTON Anne via R-help <r-help at r-project.org> wrote:
>>> 
>>> Hello everybody,
>>> 
>>> I am trying to accommodate the R codes provided by Donohue for sample size calculation in the package "longpower" with lmmpower function to estimate the post-hoc power (asked by a reviewer) of a binary GEE model with a three-way interaction (time x condition x continuous predictor) given a fixed sample size. In other words instead of the sample size I would like to estimate the power of my study.
>>> 
>>> Could anyone please help me to modify these codes as to obtain the power I'm looking for.
>>> 
>>> I would really appreciate receiving any feedback on this subject.
>>> 
>>> Yours sincerely,
>>> 
>>> Anne
>> Hi,
>> Three comments:
>> 1. Don't calculate post hoc power. Do a Google search and you will find a plethora of papers and discussions on why not, including these:
>>   The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis
>>   The American Statistician, February 2001, Vol. 55, No. 1
>>   https://www.vims.edu/people/hoenig_jm/pubs/hoenig2.pdf
>>   Post Hoc Power: Tables and Commentary
>>   https://stat.uiowa.edu/sites/stat.uiowa.edu/files/techrep/tr378.pdf
>>   Observed power, and what to do if your editor asks for post-hoc power analyses
>>   http://daniellakens.blogspot.com/2014/12/observed-power-and-what-to-do-if-your.html
>>   Retraction Watch:
>>   Statisticians clamor for retraction of paper by Harvard researchers they say uses a ?nonsense statistic?
>>   https://retractionwatch.com/2019/06/19/statisticians-clamor-for-retraction-of-paper-by-harvard-researchers-they-say-uses-a-nonsense-statistic/
>>   PubPeer Comments on the paper cited in the above RW post:
>>   https://pubpeer.com/publications/4399282A80691D9421B497E8316CF6
>>   A discussion on Frank's Data Methods forum also related to the same paper cited above:
>>   "Observed Power" and other "Power" Issues
>>   https://discourse.datamethods.org/t/observed-power-and-other-power-issues/731/30
>> 2. If you are still compelled (voluntarily or involuntarily), you may want to review the vignette for the longpower package which may have some insights, and/or contact the package maintainer for additional guidance on how to structure the code. See the vignette here:
>>   https://cran.r-project.org/web/packages/longpower/vignettes/longpower.pdf
>> 3. Don't calculate post hoc power.
>> Regards,
>> Marc Schwartz
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> ---
>> This email has been checked for viruses by AVG.
>> https://www.avg.com
> 
> -- 
> Michael
> http://www.dewey.myzen.co.uk/home.html
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From jb@rth1235 @end|ng |rom gm@||@com  Mon Aug 26 13:06:56 2019
From: jb@rth1235 @end|ng |rom gm@||@com (Josh B)
Date: Mon, 26 Aug 2019 07:06:56 -0400
Subject: [R] Graduated Symbol and Color Scale Coding Question
Message-ID: <CAGi70XV1ix66HRx8PXCHRbKDrmw_u74xyAMe0nimGaa9tqJKmg@mail.gmail.com>

I am mapping radio telemetry detections of shorebirds using ggmap. I want
the map to depict a specific point (lat/lon) where the bird was detected. I
want this depicted date of detection to be represented by a color scale.
However, multiple birds are detected at the same telemetry tower (lat/lon)
but at different dates. To avoid masking detections due to multiple birds
being detected at the same point at different dates I would like to include
a graduated point that will change size based upon the number of
individuals detected. The goal is to show what date a bird was detected at
a telemetry tower and how many birds were detected at that telemetry, using
a color scale and graduated symbol.

My thought is to have the graduated color symbol stack on itself... larger
symbol on the bottom and the smallest on the top.

p<-ggmap(gmap)
p+geom_point(data=df.tmp,aes(recvLon,recvLat,color=as.Date(ts),size=2)+scale_fill_gradient2(low="yellow",mid="orange",high="red")

I expected the map to portray the color gradient I called, but the color
scale is a range of blue (dark to light)? I get no errors and the map shows
a color scale of the time stamp like I want, but not the actual colors I
want.

Also, I do not know how to combine a graduated symbol with the color scale?

*Joshua N. Barth*

	[[alternative HTML version deleted]]


From @@mo@n|er @end|ng |rom gm@||@com  Mon Aug 26 16:51:01 2019
From: @@mo@n|er @end|ng |rom gm@||@com (Arnaud Mosnier)
Date: Mon, 26 Aug 2019 10:51:01 -0400
Subject: [R] Data frame organization
Message-ID: <CANkFkEcc9SHOyCKyufb1718j41HajV+p7wYBV4z_82DLEr7eoA@mail.gmail.com>

Hi,

I have a really simple question.
I need to convert a data.frame with the following format

A   10
B   5
C   9
A   5
B   15
C   20

in this format

A   10   5
B   5    15
C   9    20

Thanks !!!

	[[alternative HTML version deleted]]


From @@mch@ry@ @end|ng |rom y@hoo@com@@u  Mon Aug 26 21:19:54 2019
From: @@mch@ry@ @end|ng |rom y@hoo@com@@u (Sam Charya)
Date: Mon, 26 Aug 2019 19:19:54 +0000 (UTC)
Subject: [R] Data frame organization
In-Reply-To: <CANkFkEcc9SHOyCKyufb1718j41HajV+p7wYBV4z_82DLEr7eoA@mail.gmail.com>
References: <CANkFkEcc9SHOyCKyufb1718j41HajV+p7wYBV4z_82DLEr7eoA@mail.gmail.com>
Message-ID: <689227777.3072039.1566847194270@mail.yahoo.com>

 Dear Arnaud,
I just played around with your data a bit and found this to be useful. But kindly note that I am NO expert like the other people in the group. My answer to you is purely for help purposes. My knowledge in R too is limited. I used the reshape function and arrived at?something. I am?sure others will arrive at a better and more crisp answer that I have. Again please note: I am only a novice.
x <- c('A', 'B', 'C', 'A', 'B', 'C')y <- c(10, 5, 9, 5, 15, 20)df <- data.frame(x,y)dff <- reshape(df, v.names = "y", idvar = "x", timevar = "y", direction = "wide")
RESULT:
> f? x y.10 y.5 y.9 y.15 y.201 A? ?10? ?5? NA? ?NA? ?NA2 B? ?NA? ?5? NA? ?15? ?NA3 C? ?NA? NA? ?9? ?NA? ?20

Hope this is of any use.?
Kind Regards,
s.?

    On Monday, 26 August 2019, 11:37:13 pm GMT+5:30, Arnaud Mosnier <a.mosnier at gmail.com> wrote:  
 
 Hi,

I have a really simple question.
I need to convert a data.frame with the following format

A? 10
B? 5
C? 9
A? 5
B? 15
C? 20

in this format

A? 10? 5
B? 5? ? 15
C? 9? ? 20

Thanks !!!

??? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
  
	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Aug 26 21:37:31 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 26 Aug 2019 12:37:31 -0700
Subject: [R] TOC in vignette (knitr::rmarkdown)
In-Reply-To: <b6f08848-e0fc-bc27-56a2-f58732c68654@bebac.at>
References: <6319ea48-aeac-4c2a-8285-7ac5b73ddd0c@bebac.at>
 <CAGxFJbSx+CLjhPi4J5HdZN2=7Hyp+HRePr39AV=E7TtaCDGxjg@mail.gmail.com>
 <b6f08848-e0fc-bc27-56a2-f58732c68654@bebac.at>
Message-ID: <022D80F9-D517-4576-814C-A7E8701AFDD5@dcn.davis.ca.us>

The footer of every message on this list has a link to the Posting Guide, which discusses several mailing lists. Among the issues mentioned there that seem particularly relevant to your emails are:

- R-help is for questions about R (which is a very general vehicle for implementing solution methods for a wide range of domains). "Questions likely to prompt discussion unintelligible to non-programmers should rather go to R-devel than R-help."

- R-package-devel is specifically about making packages for R. Clearly, that is a subset of topics related to the R language, but it is relatively specialized so it has a dedicated mailing list to avoid cluttering the mailboxes of people not interested in those details.

- There is a discussion regarding "Questions about statistics", which frequently triggers the "go elsewhere" response you may have seen.

- There is a warning that not all packages are on-topic in R-help:
"For questions about functions in standard packages distributed with R (see the FAQ?Add-On Packages in R), ask questions on R-help. If the question relates to a?contributed package?, e.g., one downloaded from CRAN, try contacting the package maintainer first. You can also use?find("functionname")?and?packageDescription("packagename")?to find this information.?Only?send such questions to R-help or R-devel if you get no reply or need further assistance. This applies to both requests for help and to bug reports."

- There is a warning that the mailing list is a text-only list. It is difficult to cover all the "whys and hows" for this issue even in a Posting Guide, but when you fail to set your originating email program to send using plain text format then you run a significant risk that the people you want answers from will see a corrupted version of what you intended for them to see, making it less likely that they will respond.

Just think "is this about the R programming language and is there no more specific place to ask it" before you post to avoid most off-topic diversions.


On August 26, 2019 8:09:57 AM PDT, "Helmut Sch?tz" <helmut.schuetz at bebac.at> wrote:
>Hi Bert,
>
>Bert Gunter wrote on 2019-08-26 16:52:
>> Almost surely better posted on the r-package-devel mailing list.
>
>Do you have a macro for those ?answers? of yours? SCNR.
>*Which* kind of questions are -- in your opinion -- suitable to be 
>posted *here*?
>
>Cheers,
>Helmut

-- 
Sent from my phone. Please excuse my brevity.


From @@mch@ry@ @end|ng |rom y@hoo@com@@u  Mon Aug 26 21:48:39 2019
From: @@mch@ry@ @end|ng |rom y@hoo@com@@u (Sam Charya)
Date: Mon, 26 Aug 2019 19:48:39 +0000 (UTC)
Subject: [R] Data Frame Organization
In-Reply-To: <992673152.3091071.1566848668184@mail.yahoo.com>
References: <992673152.3091071.1566848668184.ref@mail.yahoo.com>
 <992673152.3091071.1566848668184@mail.yahoo.com>
Message-ID: <291991037.3081119.1566848919657@mail.yahoo.com>

 There is?some issue with the plain text vs. HTML - please find the answer again. If illegible kindly?see the attached pic.
Best Wishes.
s.

x <- c('A', 'B', 'C', 'A', 'B', 'C')
y <- c(10, 5, 9, 5, 15, 20)
df <- data.frame(x,y)
df
f <- reshape(df, v.names = "y", idvar = "x", timevar = "y", direction = "wide")
RESULT:
> f
? x y.10 y.5 y.9 y.15 y.201 A? ?10? ?5? NA? ?NA? ?NA2 B? ?NA? ?5? NA? ?15? ?NA3 C? ?NA? NA? ?9? ?NA? ?20
  

From drj|m|emon @end|ng |rom gm@||@com  Tue Aug 27 00:28:28 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Tue, 27 Aug 2019 08:28:28 +1000
Subject: [R] Data frame organization
In-Reply-To: <CANkFkEcc9SHOyCKyufb1718j41HajV+p7wYBV4z_82DLEr7eoA@mail.gmail.com>
References: <CANkFkEcc9SHOyCKyufb1718j41HajV+p7wYBV4z_82DLEr7eoA@mail.gmail.com>
Message-ID: <CA+8X3fVf4uLb9LdducxdmgAW3cjzAfVgaDoP8vQh1bWif9wJYQ@mail.gmail.com>

Hi Arnaud,
The reason I wrote the following function is that it always takes me
half a dozen tries with "reshape" before I get the syntax right:

amdf<-read.table(text="A   10
B   5
C   9
A   5
B   15
C   20")
library(prettyR)
stretch_df(amdf,"V1","V2")
 V1 V2_1 V2_2
1  A   10    5
2  B    5   15
3  C    9   20

Jim

On Tue, Aug 27, 2019 at 4:06 AM Arnaud Mosnier <a.mosnier at gmail.com> wrote:
>
> Hi,
>
> I have a really simple question.
> I need to convert a data.frame with the following format
>
> A   10
> B   5
> C   9
> A   5
> B   15
> C   20
>
> in this format
>
> A   10   5
> B   5    15
> C   9    20
>
> Thanks !!!
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From k|ebyn @end|ng |rom y@hoo@com@br  Tue Aug 27 01:47:36 2019
From: k|ebyn @end|ng |rom y@hoo@com@br (Cleber N.Borges)
Date: Mon, 26 Aug 2019 20:47:36 -0300
Subject: [R] Gtk3 into R
Message-ID: <eda96d86-7476-8911-55a1-ff38f32813ab@yahoo.com.br>

hello all,

Is there currently any way to use Gtk3 to build GUI on R?
Or some more up-to-date graphical toolkit than Gtk2 (via RGtk) or Tcl / 
Tk (via tcltk) ...

best regards

Cleber Borges


	[[alternative HTML version deleted]]


From @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com  Tue Aug 27 07:19:27 2019
From: @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com (Subhamitra Patra)
Date: Tue, 27 Aug 2019 10:49:27 +0530
Subject: [R] [r] Applying rollapply in VR test
Message-ID: <CAOFE=kPP3m1OjzrEJU-V_8kJk1XAzcFn3OaAWf-29W2u4zB0QQ@mail.gmail.com>

Dear R users,

I want to use rollapply function from the zoo package with the VR test by
using the following code.

library(vrtest)
library(zoo)
x <- Data
y <- zoo(x)
z <- rollapply(y,50, function(x) AutoBoot.test(x,nboot=30,
wild="Normal")$AutoBoot.test)


But, I am getting some error message, and unable to do that. Is there any
mistake in my code?

Please help me with this.

Thank you.

-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
08/27/19,
10:48:23 AM

	[[alternative HTML version deleted]]


From Anne@Ch@tton @end|ng |rom hcuge@ch  Tue Aug 27 09:02:15 2019
From: Anne@Ch@tton @end|ng |rom hcuge@ch (CHATTON Anne)
Date: Tue, 27 Aug 2019 07:02:15 +0000
Subject: [R] Code modification for post-hoc power
In-Reply-To: <87608904-6058-9f52-3017-356b395a20ef@dewey.myzen.co.uk>
References: <931267eff584419995fcce370ae6058a@wexchprod08.huge.ad.hcuge.ch>
 <FC6DDE74-7C37-47BB-8B24-FE1787B1D183@me.com>
 <87608904-6058-9f52-3017-356b395a20ef@dewey.myzen.co.uk>
Message-ID: <b9e7d52c6a7d4558aa8576cd2bd690a6@wexchprod08.huge.ad.hcuge.ch>

Dear Michael,

Thanks a lot for your suggestion. This is what I am trying to do with R (longpower and gee packages). But I am getting stuck with a confusing error message sent earlier I don't understand.

Best,

Anne

-----Message d'origine-----
De?: Michael Dewey [mailto:lists at dewey.myzen.co.uk] 
Envoy??: lundi, 26 ao?t 2019 18:29
??: Marc Schwartz <marc_schwartz at me.com>; CHATTON Anne <Anne.Chatton at hcuge.ch>
Cc?: R-help <r-help at r-project.org>
Objet?: Re: [R] Code modification for post-hoc power

Dear Anne

In addition to Marc's comments if you are forced to do this then, assuming your package computes sample size from power then just feed it a range of powers and find the one for which it calculates the sample size you had. There is a more elegant way to do this using uniroot but brute force should work.

Michael

On 26/08/2019 13:42, Marc Schwartz via R-help wrote:
> 
>> On Aug 26, 2019, at 6:24 AM, CHATTON Anne via R-help <r-help at r-project.org> wrote:
>>
>> Hello everybody,
>>
>> I am trying to accommodate the R codes provided by Donohue for sample size calculation in the package "longpower" with lmmpower function to estimate the post-hoc power (asked by a reviewer) of a binary GEE model with a three-way interaction (time x condition x continuous predictor) given a fixed sample size. In other words instead of the sample size I would like to estimate the power of my study.
>>
>> Could anyone please help me to modify these codes as to obtain the power I'm looking for.
>>
>> I would really appreciate receiving any feedback on this subject.
>>
>> Yours sincerely,
>>
>> Anne
> 
> 
> Hi,
> 
> Three comments:
> 
> 1. Don't calculate post hoc power. Do a Google search and you will find a plethora of papers and discussions on why not, including these:
> 
>    The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis
>    The American Statistician, February 2001, Vol. 55, No. 1
>    https://www.vims.edu/people/hoenig_jm/pubs/hoenig2.pdf
> 
>    Post Hoc Power: Tables and Commentary
>    https://stat.uiowa.edu/sites/stat.uiowa.edu/files/techrep/tr378.pdf
> 
>    Observed power, and what to do if your editor asks for post-hoc power analyses
>    
> http://daniellakens.blogspot.com/2014/12/observed-power-and-what-to-do
> -if-your.html
> 
>    Retraction Watch:
>    Statisticians clamor for retraction of paper by Harvard researchers they say uses a ?nonsense statistic?
>    
> https://retractionwatch.com/2019/06/19/statisticians-clamor-for-retrac
> tion-of-paper-by-harvard-researchers-they-say-uses-a-nonsense-statisti
> c/
> 
>    PubPeer Comments on the paper cited in the above RW post:
>    https://pubpeer.com/publications/4399282A80691D9421B497E8316CF6
> 
>    A discussion on Frank's Data Methods forum also related to the same paper cited above:
>    "Observed Power" and other "Power" Issues
>    
> https://discourse.datamethods.org/t/observed-power-and-other-power-iss
> ues/731/30
> 
> 
> 2. If you are still compelled (voluntarily or involuntarily), you may want to review the vignette for the longpower package which may have some insights, and/or contact the package maintainer for additional guidance on how to structure the code. See the vignette here:
> 
>    
> https://cran.r-project.org/web/packages/longpower/vignettes/longpower.
> pdf
> 
> 
> 3. Don't calculate post hoc power.
> 
> 
> Regards,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ---
> This email has been checked for viruses by AVG.
> https://www.avg.com
> 
> 

--
Michael
http://www.dewey.myzen.co.uk/home.html

From HDor@n @end|ng |rom @|r@org  Tue Aug 27 12:10:19 2019
From: HDor@n @end|ng |rom @|r@org (Doran, Harold)
Date: Tue, 27 Aug 2019 10:10:19 +0000
Subject: [R] Gtk3 into R
In-Reply-To: <eda96d86-7476-8911-55a1-ff38f32813ab@yahoo.com.br>
References: <eda96d86-7476-8911-55a1-ff38f32813ab@yahoo.com.br>
Message-ID: <BL0PR05MB481841094FF0CCE8E26838E2CAA00@BL0PR05MB4818.namprd05.prod.outlook.com>

Yes, see Shiny by R studio. Here is an example of my site that is built on Shiny and R. It also combines CSS, SQL, javascript, and some HTML.

https://shiny.airast.org/METRICS/

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Cleber N.Borges
Sent: Monday, August 26, 2019 7:48 PM
To: r-help at r-project.org
Subject: [R] Gtk3 into R

hello all,

Is there currently any way to use Gtk3 to build GUI on R?
Or some more up-to-date graphical toolkit than Gtk2 (via RGtk) or Tcl / Tk (via tcltk) ...

best regards

Cleber Borges


	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Tue Aug 27 13:25:33 2019
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Tue, 27 Aug 2019 12:25:33 +0100
Subject: [R] Code modification for post-hoc power
In-Reply-To: <b9e7d52c6a7d4558aa8576cd2bd690a6@wexchprod08.huge.ad.hcuge.ch>
References: <931267eff584419995fcce370ae6058a@wexchprod08.huge.ad.hcuge.ch>
 <FC6DDE74-7C37-47BB-8B24-FE1787B1D183@me.com>
 <87608904-6058-9f52-3017-356b395a20ef@dewey.myzen.co.uk>
 <b9e7d52c6a7d4558aa8576cd2bd690a6@wexchprod08.huge.ad.hcuge.ch>
Message-ID: <65257ee7-903c-003a-3967-4ac10146440a@dewey.myzen.co.uk>

Dear Anne

Can you resend the eror message which you accidentally sent only to me 
please?

Michael

On 27/08/2019 08:02, CHATTON Anne wrote:
> Dear Michael,
> 
> Thanks a lot for your suggestion. This is what I am trying to do with R (longpower and gee packages). But I am getting stuck with a confusing error message sent earlier I don't understand.
> 
> Best,
> 
> Anne
> 
> -----Message d'origine-----
> De?: Michael Dewey [mailto:lists at dewey.myzen.co.uk]
> Envoy??: lundi, 26 ao?t 2019 18:29
> ??: Marc Schwartz <marc_schwartz at me.com>; CHATTON Anne <Anne.Chatton at hcuge.ch>
> Cc?: R-help <r-help at r-project.org>
> Objet?: Re: [R] Code modification for post-hoc power
> 
> Dear Anne
> 
> In addition to Marc's comments if you are forced to do this then, assuming your package computes sample size from power then just feed it a range of powers and find the one for which it calculates the sample size you had. There is a more elegant way to do this using uniroot but brute force should work.
> 
> Michael
> 
> On 26/08/2019 13:42, Marc Schwartz via R-help wrote:
>>
>>> On Aug 26, 2019, at 6:24 AM, CHATTON Anne via R-help <r-help at r-project.org> wrote:
>>>
>>> Hello everybody,
>>>
>>> I am trying to accommodate the R codes provided by Donohue for sample size calculation in the package "longpower" with lmmpower function to estimate the post-hoc power (asked by a reviewer) of a binary GEE model with a three-way interaction (time x condition x continuous predictor) given a fixed sample size. In other words instead of the sample size I would like to estimate the power of my study.
>>>
>>> Could anyone please help me to modify these codes as to obtain the power I'm looking for.
>>>
>>> I would really appreciate receiving any feedback on this subject.
>>>
>>> Yours sincerely,
>>>
>>> Anne
>>
>>
>> Hi,
>>
>> Three comments:
>>
>> 1. Don't calculate post hoc power. Do a Google search and you will find a plethora of papers and discussions on why not, including these:
>>
>>     The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis
>>     The American Statistician, February 2001, Vol. 55, No. 1
>>     https://www.vims.edu/people/hoenig_jm/pubs/hoenig2.pdf
>>
>>     Post Hoc Power: Tables and Commentary
>>     https://stat.uiowa.edu/sites/stat.uiowa.edu/files/techrep/tr378.pdf
>>
>>     Observed power, and what to do if your editor asks for post-hoc power analyses
>>     
>> http://daniellakens.blogspot.com/2014/12/observed-power-and-what-to-do
>> -if-your.html
>>
>>     Retraction Watch:
>>     Statisticians clamor for retraction of paper by Harvard researchers they say uses a ?nonsense statistic?
>>     
>> https://retractionwatch.com/2019/06/19/statisticians-clamor-for-retrac
>> tion-of-paper-by-harvard-researchers-they-say-uses-a-nonsense-statisti
>> c/
>>
>>     PubPeer Comments on the paper cited in the above RW post:
>>     https://pubpeer.com/publications/4399282A80691D9421B497E8316CF6
>>
>>     A discussion on Frank's Data Methods forum also related to the same paper cited above:
>>     "Observed Power" and other "Power" Issues
>>     
>> https://discourse.datamethods.org/t/observed-power-and-other-power-iss
>> ues/731/30
>>
>>
>> 2. If you are still compelled (voluntarily or involuntarily), you may want to review the vignette for the longpower package which may have some insights, and/or contact the package maintainer for additional guidance on how to structure the code. See the vignette here:
>>
>>     
>> https://cran.r-project.org/web/packages/longpower/vignettes/longpower.
>> pdf
>>
>>
>> 3. Don't calculate post hoc power.
>>
>>
>> Regards,
>>
>> Marc Schwartz
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ---
>> This email has been checked for viruses by AVG.
>> https://www.avg.com
>>
>>
> 
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From Anne@Ch@tton @end|ng |rom hcuge@ch  Tue Aug 27 14:14:12 2019
From: Anne@Ch@tton @end|ng |rom hcuge@ch (CHATTON Anne)
Date: Tue, 27 Aug 2019 12:14:12 +0000
Subject: [R] Change of language interface in R
Message-ID: <43f860cd650a4d22b34c828714ce2fe3@wexchprod08.huge.ad.hcuge.ch>

Can anyone please tell me how to change the language interface so as to receive error messages in English. Currently it's a mixed jargon of French an English.

Thank you for any help.

Anne


	[[alternative HTML version deleted]]


From petr@p|k@| @end|ng |rom prechez@@cz  Tue Aug 27 14:24:03 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Tue, 27 Aug 2019 12:24:03 +0000
Subject: [R] Change of language interface in R
In-Reply-To: <43f860cd650a4d22b34c828714ce2fe3@wexchprod08.huge.ad.hcuge.ch>
References: <43f860cd650a4d22b34c828714ce2fe3@wexchprod08.huge.ad.hcuge.ch>
Message-ID: <2082641ec04848fd90a495979e7f92ec@SRVEXCHCM1302.precheza.cz>

Hi

Google tells it to you in seconds

https://stackoverflow.com/questions/13575180/how-to-change-language-settings-in-r

BTW this was the first hit from phrase
R how to change the language interface

Cheers
Petr


> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of CHATTON Anne
> via R-help
> Sent: Tuesday, August 27, 2019 2:14 PM
> To: R-help at r-project.org
> Subject: [R] Change of language interface in R
>
> Can anyone please tell me how to change the language interface so as to
> receive error messages in English. Currently it's a mixed jargon of French an
> English.
>
> Thank you for any help.
>
> Anne
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From p@u|bern@|07 @end|ng |rom gm@||@com  Tue Aug 27 14:44:32 2019
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Tue, 27 Aug 2019 07:44:32 -0500
Subject: [R] Confidence Intervals with mlp forecasts
Message-ID: <CAMOcQfOL58z+mt=ADE5bn2d=P+vsGCw2NtiEouMU2F+GO6cB6Q@mail.gmail.com>

Dear friends,

Hope you are all doing well. I am currently using function mlp (to fit
multiple layer percentron model) to generate forecasts using package nnfor.

I would like to know if the mlp function provides, or is there a way to
construct confidence intervals for the forecasts generated by this mlp
function.

Any help and/or  guidance would be much appreciated,

Best regards,

Paul

	[[alternative HTML version deleted]]


From ggrothend|eck @end|ng |rom gm@||@com  Tue Aug 27 14:59:11 2019
From: ggrothend|eck @end|ng |rom gm@||@com (Gabor Grothendieck)
Date: Tue, 27 Aug 2019 08:59:11 -0400
Subject: [R] [r] Applying rollapply in VR test
In-Reply-To: <CAOFE=kPP3m1OjzrEJU-V_8kJk1XAzcFn3OaAWf-29W2u4zB0QQ@mail.gmail.com>
References: <CAOFE=kPP3m1OjzrEJU-V_8kJk1XAzcFn3OaAWf-29W2u4zB0QQ@mail.gmail.com>
Message-ID: <CAP01uRkn-BQ3hWHSWwbX5edyniREV62Divsan2n3N1-nsvC+mg@mail.gmail.com>

If I run this (which is identical to your code except I supplied some
random input since your
post did not include any) I get no error:

  library(vrtest)
  library(zoo)
  set.seed(123)
  x <- rnorm(100)
  y <- zoo(x)
  z <- rollapply(y,50, function(x) AutoBoot.test(x,nboot=30,
    wild="Normal")$AutoBoot.test)


On Tue, Aug 27, 2019 at 1:20 AM Subhamitra Patra
<subhamitra.patra at gmail.com> wrote:
>
> Dear R users,
>
> I want to use rollapply function from the zoo package with the VR test by
> using the following code.
>
> library(vrtest)
> library(zoo)
> x <- Data
> y <- zoo(x)
> z <- rollapply(y,50, function(x) AutoBoot.test(x,nboot=30,
> wild="Normal")$AutoBoot.test)
>
>
> But, I am getting some error message, and unable to do that. Is there any
> mistake in my code?
>
> Please help me with this.
>
> Thank you.
>
> --
> *Best Regards,*
> *Subhamitra Patra*
> *Phd. Research Scholar*
> *Department of Humanities and Social Sciences*
> *Indian Institute of Technology, Kharagpur*
> *INDIA*
>
> [image: Mailtrack]
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
> Sender
> notified by
> Mailtrack
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
> 08/27/19,
> 10:48:23 AM
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com  Tue Aug 27 15:20:24 2019
From: @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com (Subhamitra Patra)
Date: Tue, 27 Aug 2019 18:50:24 +0530
Subject: [R] [r] Applying rollapply in VR test
In-Reply-To: <CAP01uRkn-BQ3hWHSWwbX5edyniREV62Divsan2n3N1-nsvC+mg@mail.gmail.com>
References: <CAOFE=kPP3m1OjzrEJU-V_8kJk1XAzcFn3OaAWf-29W2u4zB0QQ@mail.gmail.com>
 <CAP01uRkn-BQ3hWHSWwbX5edyniREV62Divsan2n3N1-nsvC+mg@mail.gmail.com>
Message-ID: <CAOFE=kOKbmhg8eaiB9LgbtKf4dJadLASPnueXr3OSStgED54kQ@mail.gmail.com>

Actually, after running this code, z object is creating which has no
observation, means there are no outputs in the z object. When I am giving
the command to plot or view z object, it is showing an error.

I thought there is some error in the code, so I posted.

Please help me regarding this.

Thanks for your response.
[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
08/27/19,
06:46:12 PM

On Tue, Aug 27, 2019 at 6:29 PM Gabor Grothendieck <ggrothendieck at gmail.com>
wrote:

> If I run this (which is identical to your code except I supplied some
> random input since your
> post did not include any) I get no error:
>
>   library(vrtest)
>   library(zoo)
>   set.seed(123)
>   x <- rnorm(100)
>   y <- zoo(x)
>   z <- rollapply(y,50, function(x) AutoBoot.test(x,nboot=30,
>     wild="Normal")$AutoBoot.test)
>
>
> On Tue, Aug 27, 2019 at 1:20 AM Subhamitra Patra
> <subhamitra.patra at gmail.com> wrote:
> >
> > Dear R users,
> >
> > I want to use rollapply function from the zoo package with the VR test by
> > using the following code.
> >
> > library(vrtest)
> > library(zoo)
> > x <- Data
> > y <- zoo(x)
> > z <- rollapply(y,50, function(x) AutoBoot.test(x,nboot=30,
> > wild="Normal")$AutoBoot.test)
> >
> >
> > But, I am getting some error message, and unable to do that. Is there any
> > mistake in my code?
> >
> > Please help me with this.
> >
> > Thank you.
> >
> > --
> > *Best Regards,*
> > *Subhamitra Patra*
> > *Phd. Research Scholar*
> > *Department of Humanities and Social Sciences*
> > *Indian Institute of Technology, Kharagpur*
> > *INDIA*
> >
> > [image: Mailtrack]
> > <
> https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&
> >
> > Sender
> > notified by
> > Mailtrack
> > <
> https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&
> >
> > 08/27/19,
> > 10:48:23 AM
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com
>


-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

	[[alternative HTML version deleted]]


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Tue Aug 27 18:15:38 2019
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Tue, 27 Aug 2019 17:15:38 +0100
Subject: [R] Code modification for post-hoc power
In-Reply-To: <65257ee7-903c-003a-3967-4ac10146440a@dewey.myzen.co.uk>
References: <931267eff584419995fcce370ae6058a@wexchprod08.huge.ad.hcuge.ch>
 <FC6DDE74-7C37-47BB-8B24-FE1787B1D183@me.com>
 <87608904-6058-9f52-3017-356b395a20ef@dewey.myzen.co.uk>
 <b9e7d52c6a7d4558aa8576cd2bd690a6@wexchprod08.huge.ad.hcuge.ch>
 <65257ee7-903c-003a-3967-4ac10146440a@dewey.myzen.co.uk>
Message-ID: <b913a864-370e-e6a6-4635-b9f98196cf6a@dewey.myzen.co.uk>

Anne sent me off-line the error message.
============ error message starts here ==========
This error message:
"Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27 Error in 
`[.default`(xj, i) : type 'closure' d'indice incorrect"

appears after the following R codes:

library(gee)
attach(geefile) # stored in some spss directory
geereg <- gee(outcome ~ trial + group + trial*group + trial*group*beckce 
+ trial*group*beckref, id = subject, data = geefile, na.omit, tol = 
0.001, maxiter = 25, family = binomial, corstr = "exchangeable", silent 
= TRUE)

The following command would have allowed me to modify the power so as to 
obtain the sample size I have:
lmmpower(geereg, pct.change = 0.10, t = seq(0,6,3), power = 0.80)

================ end ===============

I am not an expert on R programming but in my experience that error has 
meant that I was including as a parameter something which was not of the 
type which the program expected. I would suggest as a first step not 
using attach() at all and instead using data=geefile in the call to gee 
or investigating the with() command if it does not allow a data parameter.

On 27/08/2019 12:25, Michael Dewey wrote:
> Dear Anne
> 
> Can you resend the eror message which you accidentally sent only to me 
> please?
> 
> Michael
> 
> On 27/08/2019 08:02, CHATTON Anne wrote:
>> Dear Michael,
>>
>> Thanks a lot for your suggestion. This is what I am trying to do with 
>> R (longpower and gee packages). But I am getting stuck with a 
>> confusing error message sent earlier I don't understand.
>>
>> Best,
>>
>> Anne
>>
>> -----Message d'origine-----
>> De?: Michael Dewey [mailto:lists at dewey.myzen.co.uk]
>> Envoy??: lundi, 26 ao?t 2019 18:29
>> ??: Marc Schwartz <marc_schwartz at me.com>; CHATTON Anne 
>> <Anne.Chatton at hcuge.ch>
>> Cc?: R-help <r-help at r-project.org>
>> Objet?: Re: [R] Code modification for post-hoc power
>>
>> Dear Anne
>>
>> In addition to Marc's comments if you are forced to do this then, 
>> assuming your package computes sample size from power then just feed 
>> it a range of powers and find the one for which it calculates the 
>> sample size you had. There is a more elegant way to do this using 
>> uniroot but brute force should work.
>>
>> Michael
>>
>> On 26/08/2019 13:42, Marc Schwartz via R-help wrote:
>>>
>>>> On Aug 26, 2019, at 6:24 AM, CHATTON Anne via R-help 
>>>> <r-help at r-project.org> wrote:
>>>>
>>>> Hello everybody,
>>>>
>>>> I am trying to accommodate the R codes provided by Donohue for 
>>>> sample size calculation in the package "longpower" with lmmpower 
>>>> function to estimate the post-hoc power (asked by a reviewer) of a 
>>>> binary GEE model with a three-way interaction (time x condition x 
>>>> continuous predictor) given a fixed sample size. In other words 
>>>> instead of the sample size I would like to estimate the power of my 
>>>> study.
>>>>
>>>> Could anyone please help me to modify these codes as to obtain the 
>>>> power I'm looking for.
>>>>
>>>> I would really appreciate receiving any feedback on this subject.
>>>>
>>>> Yours sincerely,
>>>>
>>>> Anne
>>>
>>>
>>> Hi,
>>>
>>> Three comments:
>>>
>>> 1. Don't calculate post hoc power. Do a Google search and you will 
>>> find a plethora of papers and discussions on why not, including these:
>>>
>>> ??? The Abuse of Power: The Pervasive Fallacy of Power Calculations 
>>> for Data Analysis
>>> ??? The American Statistician, February 2001, Vol. 55, No. 1
>>> ??? https://www.vims.edu/people/hoenig_jm/pubs/hoenig2.pdf
>>>
>>> ??? Post Hoc Power: Tables and Commentary
>>> ??? https://stat.uiowa.edu/sites/stat.uiowa.edu/files/techrep/tr378.pdf
>>>
>>> ??? Observed power, and what to do if your editor asks for post-hoc 
>>> power analyses
>>> http://daniellakens.blogspot.com/2014/12/observed-power-and-what-to-do
>>> -if-your.html
>>>
>>> ??? Retraction Watch:
>>> ??? Statisticians clamor for retraction of paper by Harvard 
>>> researchers they say uses a ?nonsense statistic?
>>> https://retractionwatch.com/2019/06/19/statisticians-clamor-for-retrac
>>> tion-of-paper-by-harvard-researchers-they-say-uses-a-nonsense-statisti
>>> c/
>>>
>>> ??? PubPeer Comments on the paper cited in the above RW post:
>>> ??? https://pubpeer.com/publications/4399282A80691D9421B497E8316CF6
>>>
>>> ??? A discussion on Frank's Data Methods forum also related to the 
>>> same paper cited above:
>>> ??? "Observed Power" and other "Power" Issues
>>> https://discourse.datamethods.org/t/observed-power-and-other-power-iss
>>> ues/731/30
>>>
>>>
>>> 2. If you are still compelled (voluntarily or involuntarily), you may 
>>> want to review the vignette for the longpower package which may have 
>>> some insights, and/or contact the package maintainer for additional 
>>> guidance on how to structure the code. See the vignette here:
>>>
>>> https://cran.r-project.org/web/packages/longpower/vignettes/longpower.
>>> pdf
>>>
>>>
>>> 3. Don't calculate post hoc power.
>>>
>>>
>>> Regards,
>>>
>>> Marc Schwartz
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ---
>>> This email has been checked for viruses by AVG.
>>> https://www.avg.com
>>>
>>>
>>
>> -- 
>> Michael
>> http://www.dewey.myzen.co.uk/home.html
>>
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From @@mo@n|er @end|ng |rom gm@||@com  Tue Aug 27 00:41:50 2019
From: @@mo@n|er @end|ng |rom gm@||@com (Arnaud Mosnier)
Date: Mon, 26 Aug 2019 18:41:50 -0400
Subject: [R] Data frame organization
In-Reply-To: <CA+8X3fVf4uLb9LdducxdmgAW3cjzAfVgaDoP8vQh1bWif9wJYQ@mail.gmail.com>
References: <CANkFkEcc9SHOyCKyufb1718j41HajV+p7wYBV4z_82DLEr7eoA@mail.gmail.com>
 <CA+8X3fVf4uLb9LdducxdmgAW3cjzAfVgaDoP8vQh1bWif9wJYQ@mail.gmail.com>
Message-ID: <CANkFkEcvS8FM4wS3AsZkULibTsMRGhWOrfb4+15EV1LdQjpYJg@mail.gmail.com>

Aaaahhhhh finally !!! Thanks a lot !!!

Arnaud

Le lun. 26 ao?t 2019 18 h 28, Jim Lemon <drjimlemon at gmail.com> a ?crit :

> Hi Arnaud,
> The reason I wrote the following function is that it always takes me
> half a dozen tries with "reshape" before I get the syntax right:
>
> amdf<-read.table(text="A   10
> B   5
> C   9
> A   5
> B   15
> C   20")
> library(prettyR)
> stretch_df(amdf,"V1","V2")
>  V1 V2_1 V2_2
> 1  A   10    5
> 2  B    5   15
> 3  C    9   20
>
> Jim
>
> On Tue, Aug 27, 2019 at 4:06 AM Arnaud Mosnier <a.mosnier at gmail.com>
> wrote:
> >
> > Hi,
> >
> > I have a really simple question.
> > I need to convert a data.frame with the following format
> >
> > A   10
> > B   5
> > C   9
> > A   5
> > B   15
> > C   20
> >
> > in this format
> >
> > A   10   5
> > B   5    15
> > C   9    20
> >
> > Thanks !!!
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @pr||ett|ngton @end|ng |rom gm@||@com  Tue Aug 27 07:56:24 2019
From: @pr||ett|ngton @end|ng |rom gm@||@com (April Ettington)
Date: Tue, 27 Aug 2019 17:56:24 +1200
Subject: [R] parsing files with "\" character
Message-ID: <CAE9tUWfdzAioD9N9wXp5i6AykCr=wt0iA4gYc15cLmFruKK_Mw@mail.gmail.com>

 Is there any way to parse files that include the \ character in a string?

When I try to use grep to extract strings with a pattern that includes "\"
it fails.

If there is no way to do it with R, is it possible with python or a bash
script?

Thank you,

April

	[[alternative HTML version deleted]]


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Tue Aug 27 19:46:06 2019
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Tue, 27 Aug 2019 18:46:06 +0100
Subject: [R] parsing files with "\" character
In-Reply-To: <CAE9tUWfdzAioD9N9wXp5i6AykCr=wt0iA4gYc15cLmFruKK_Mw@mail.gmail.com>
References: <CAE9tUWfdzAioD9N9wXp5i6AykCr=wt0iA4gYc15cLmFruKK_Mw@mail.gmail.com>
Message-ID: <3c4e36fe-1650-a84f-2d90-c51b6ec41279@dewey.myzen.co.uk>

Dear April

Can you show us an example of what you are trying to do and how it 
fails? There are rules about backspaces but I find that if one backspace 
does not work try two, three, four until it works. It would be better to 
understand the rules but life is short.

Michael

On 27/08/2019 06:56, April Ettington wrote:
>   Is there any way to parse files that include the \ character in a string?
> 
> When I try to use grep to extract strings with a pattern that includes "\"
> it fails.
> 
> If there is no way to do it with R, is it possible with python or a bash
> script?
> 
> Thank you,
> 
> April
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ---
> This email has been checked for viruses by AVG.
> https://www.avg.com
> 
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From m@dhupu@u|ur| @end|ng |rom gm@||@com  Tue Aug 27 07:03:14 2019
From: m@dhupu@u|ur| @end|ng |rom gm@||@com (pusuluri madhu)
Date: Tue, 27 Aug 2019 10:33:14 +0530
Subject: [R] File corrupted in R/magic number
Message-ID: <CAALKAO_m2d3-_SqY406muKBj5-nTK+3BN=a9v5K==95CFmcPew@mail.gmail.com>

Please help me to get rid out of this situation. While loading the csv file
error msg like
Error: bad restore file manager (file may be corrupted)-- no data loaded.
In addition warning message: file "S_D.csv" has magic number GeneN. Use of
save versions more than 2 is deprecated.

Please kindly suggest

-- 

*Madhu Pusuluri, Ph D*
*Visiting Scientist.*
*Genomics & Trait discovery- Genetic Gains*

*International Crops Research Institute for the Semi-Arid Tropics (ICRISAT)*

*Patancheru, Hyderabad,  Telangana 502324, INDIA.*

*E-mail: **m.pusuluri at cgiar.org <m.pusuluri at cgiar.org>**;
madhupusuluri at gmail.com
<https://outlook.office.com/owa/madhupusuluri at gmail.com>*

*Tel: +91 9912241982.*

	[[alternative HTML version deleted]]


From n|ko|@o@ @end|ng |rom kourentze@@com  Tue Aug 27 16:37:41 2019
From: n|ko|@o@ @end|ng |rom kourentze@@com (Nikolaos Kourentzes)
Date: Tue, 27 Aug 2019 14:37:41 +0000 (UTC)
Subject: [R] Confidence Intervals with mlp forecasts
In-Reply-To: <CAMOcQfOL58z+mt=ADE5bn2d=P+vsGCw2NtiEouMU2F+GO6cB6Q@mail.gmail.com>
References: <CAMOcQfOL58z+mt=ADE5bn2d=P+vsGCw2NtiEouMU2F+GO6cB6Q@mail.gmail.com>
Message-ID: <280604333.526101.1566916661840@mail.yahoo.com>

Hi Paul,
Currently it does not provide prediction intervals, as it is not assuming a generative model or a particular error distribution.
I think the best way forward, with nnfor, is to construct empirical ones. 
Have a look at this paper for some relatively straightforward approaches that work quite well under a variety of conditions. 
The paper looks at safety stocks, but the same approach can be used for generating prediction intervals.https://kourentzes.com/forecasting/2018/06/20/empirical-safety-stock-estimation-based-on-kernel-and-garch-models/
Best,Nikos


Professor, Dept. Management Science


Lancaster University Management School, UK
 
Centre for Marketing Analytics & Forecasting


blog: http://nikolaos.kourentzes.com
 
twitter:?@nkourentz

Book: Ord, Fildes & Kourentzes (2017) Principles of Business Forecasting (2nd ed.), Wessex.
 

    On Tuesday, August 27, 2019, 3:44:45 PM GMT+3, Paul Bernal <paulbernal07 at gmail.com> wrote:  
 
 Dear friends,

Hope you are all doing well. I am currently using function mlp (to fit multiple layer percentron model) to generate forecasts using package nnfor.
I would like to know if the mlp function provides, or is there a way to construct confidence intervals for the forecasts generated by this mlp function.
Any help and/or? guidance would be much appreciated,
Best regards,
Paul  
	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Aug 27 20:43:56 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 27 Aug 2019 11:43:56 -0700
Subject: [R] parsing files with "\" character
In-Reply-To: <CAE9tUWfdzAioD9N9wXp5i6AykCr=wt0iA4gYc15cLmFruKK_Mw@mail.gmail.com>
References: <CAE9tUWfdzAioD9N9wXp5i6AykCr=wt0iA4gYc15cLmFruKK_Mw@mail.gmail.com>
Message-ID: <150F3D33-9782-479C-A939-1392B5AD8C0E@dcn.davis.ca.us>

The principles of regex are basically the same between R and those other languages, so I don't see why you would switch... but if you did, asking here would be inappropriate.

I think the short answer is yes, but can't be specific without a reproducible example. ([1] is recommended but not required.)

Keep in mind that R has a syntax for strings that uses \ for escaping the following character. Thus, "\\" is a single-character string with a backslash in it. However, grep also uses backslash for escaping, so if you want to search for a single backslash in a target string then grep needs two backslashes, but each of those has to be escaped in an R string so you would use four backslashes in R source code strings to match one in the target.

[1] package reprex

On August 26, 2019 10:56:24 PM PDT, April Ettington <aprilettington at gmail.com> wrote:
>Is there any way to parse files that include the \ character in a
>string?
>
>When I try to use grep to extract strings with a pattern that includes
>"\"
>it fails.
>
>If there is no way to do it with R, is it possible with python or a
>bash
>script?
>
>Thank you,
>
>April
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From jrkr|de@u @end|ng |rom gm@||@com  Tue Aug 27 22:38:19 2019
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Tue, 27 Aug 2019 16:38:19 -0400
Subject: [R] File corrupted in R/magic number
In-Reply-To: <CAALKAO_m2d3-_SqY406muKBj5-nTK+3BN=a9v5K==95CFmcPew@mail.gmail.com>
References: <CAALKAO_m2d3-_SqY406muKBj5-nTK+3BN=a9v5K==95CFmcPew@mail.gmail.com>
Message-ID: <CAKZQJMBc_ZzjbnGT5wpVH9At1mtxd1iLCgHz0tFSTLXKMzSziQ@mail.gmail.com>

To be honest , we would probably have to see a sample of the data.
Can you supply a location for small data sample. Other than that  we
are guessing.

Other than that , can you open it in a text editor?  That will help
assure it is really not a corrupt file.

After using R for over 15 years, the worst thing is importing data files !

On Tue, 27 Aug 2019 at 14:00, pusuluri madhu <madhupusuluri at gmail.com> wrote:
>
> Please help me to get rid out of this situation. While loading the csv file
> error msg like
> Error: bad restore file manager (file may be corrupted)-- no data loaded.
> In addition warning message: file "S_D.csv" has magic number GeneN. Use of
> save versions more than 2 is deprecated.
>
> Please kindly suggest
>
> --
>
> *Madhu Pusuluri, Ph D*
> *Visiting Scientist.*
> *Genomics & Trait discovery- Genetic Gains*
>
> *International Crops Research Institute for the Semi-Arid Tropics (ICRISAT)*
>
> *Patancheru, Hyderabad,  Telangana 502324, INDIA.*
>
> *E-mail: **m.pusuluri at cgiar.org <m.pusuluri at cgiar.org>**;
> madhupusuluri at gmail.com
> <https://outlook.office.com/owa/madhupusuluri at gmail.com>*
>
> *Tel: +91 9912241982.*
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
John Kane
Kingston ON Canada


From @@r@h@go@|ee @end|ng |rom gm@||@com  Tue Aug 27 22:49:39 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Tue, 27 Aug 2019 16:49:39 -0400
Subject: [R] File corrupted in R/magic number
In-Reply-To: <CAALKAO_m2d3-_SqY406muKBj5-nTK+3BN=a9v5K==95CFmcPew@mail.gmail.com>
References: <CAALKAO_m2d3-_SqY406muKBj5-nTK+3BN=a9v5K==95CFmcPew@mail.gmail.com>
Message-ID: <CAM_vjuk2SwbJAMwip3i=ew9Oa8MCy87grjeYh2+JFtEjX0hrXQ@mail.gmail.com>

Summoning my telepathy, I suspect that you tried to use load() to get
your CSV file into R, instead of read.csv()

Here's the first link that came up when I searched for "read csv file into R"

http://rprogramming.net/read-csv-in-r/

On Tue, Aug 27, 2019 at 2:00 PM pusuluri madhu <madhupusuluri at gmail.com> wrote:
>
> Please help me to get rid out of this situation. While loading the csv file
> error msg like
> Error: bad restore file manager (file may be corrupted)-- no data loaded.
> In addition warning message: file "S_D.csv" has magic number GeneN. Use of
> save versions more than 2 is deprecated.
>
> Please kindly suggest
>
> --
>
> *Madhu Pusuluri, Ph D*
> *Visiting Scientist.*
> *Genomics & Trait discovery- Genetic Gains*
>
> *International Crops Research Institute for the Semi-Arid Tropics (ICRISAT)*
>
> *Patancheru, Hyderabad,  Telangana 502324, INDIA.*
>
> *E-mail: **m.pusuluri at cgiar.org <m.pusuluri at cgiar.org>**;
> madhupusuluri at gmail.com
> <https://outlook.office.com/owa/madhupusuluri at gmail.com>*
>
> *Tel: +91 9912241982.*
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From @topro888 @end|ng |rom gm@||@com  Tue Aug 27 23:10:16 2019
From: @topro888 @end|ng |rom gm@||@com (Alex Naverniak)
Date: Tue, 27 Aug 2019 17:10:16 -0400
Subject: [R] Structuring Inventory in R
Message-ID: <CAC3qRfxuDb+-1j3RchB8ASsqcLhe4qZ-QSAhJV9yxn4Wv+pTtw@mail.gmail.com>

Hi,
I am trying to create Inventory structure with item SKU and 3 subitems like
"Price"; "Qty"; "ID". I tried list(SKU,list("Item1","Item2","Item3")). It
seem not to work. Please help with ideas. Thanks.
Alex

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Wed Aug 28 06:49:28 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 28 Aug 2019 14:49:28 +1000
Subject: [R] Structuring Inventory in R
In-Reply-To: <CAC3qRfxuDb+-1j3RchB8ASsqcLhe4qZ-QSAhJV9yxn4Wv+pTtw@mail.gmail.com>
References: <CAC3qRfxuDb+-1j3RchB8ASsqcLhe4qZ-QSAhJV9yxn4Wv+pTtw@mail.gmail.com>
Message-ID: <CA+8X3fXi_v-yw=qfpx93-co62S4gzDUOtgqyYuQJX5hZo=r_9Q@mail.gmail.com>

Hi Alex,
At a guess you may want something like this:

data.frame(item="SKU",price=10,qty=2,ID="00001")

This produces a data frame with one row. You will probably want many more rows.

Jim

On Wed, Aug 28, 2019 at 2:21 PM Alex Naverniak <stopro888 at gmail.com> wrote:
>
> Hi,
> I am trying to create Inventory structure with item SKU and 3 subitems like
> "Price"; "Qty"; "ID". I tried list(SKU,list("Item1","Item2","Item3")). It
> seem not to work. Please help with ideas. Thanks.
> Alex
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Aug 28 08:18:03 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 27 Aug 2019 23:18:03 -0700
Subject: [R] Structuring Inventory in R
In-Reply-To: <CA+8X3fXi_v-yw=qfpx93-co62S4gzDUOtgqyYuQJX5hZo=r_9Q@mail.gmail.com>
References: <CAC3qRfxuDb+-1j3RchB8ASsqcLhe4qZ-QSAhJV9yxn4Wv+pTtw@mail.gmail.com>
 <CA+8X3fXi_v-yw=qfpx93-co62S4gzDUOtgqyYuQJX5hZo=r_9Q@mail.gmail.com>
Message-ID: <3DB074ED-747E-4449-BFC9-EF2581D2464A@dcn.davis.ca.us>

Perhaps

dta <- data.frame( SKU=c("4950","8488","1159"), Price=c(10,15,3),Qty=c(24,144,16),ID=c("10208473","38447769","43759115") )

Lists are not as easy to optimize performance with as tables are... unless you have a specific reason to use them I would minimize your use of single-element items in lists.

On August 27, 2019 9:49:28 PM PDT, Jim Lemon <drjimlemon at gmail.com> wrote:
>Hi Alex,
>At a guess you may want something like this:
>
>data.frame(item="SKU",price=10,qty=2,ID="00001")
>
>This produces a data frame with one row. You will probably want many
>more rows.
>
>Jim
>
>On Wed, Aug 28, 2019 at 2:21 PM Alex Naverniak <stopro888 at gmail.com>
>wrote:
>>
>> Hi,
>> I am trying to create Inventory structure with item SKU and 3
>subitems like
>> "Price"; "Qty"; "ID". I tried
>list(SKU,list("Item1","Item2","Item3")). It
>> seem not to work. Please help with ideas. Thanks.
>> Alex
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From pt|t_b|eu @end|ng |rom y@hoo@|r  Wed Aug 28 08:20:38 2019
From: pt|t_b|eu @end|ng |rom y@hoo@|r (lionel sicot)
Date: Wed, 28 Aug 2019 06:20:38 +0000 (UTC)
Subject: [R] Use gather with a various number of columns
References: <250390410.3266953.1566973238795.ref@mail.yahoo.com>
Message-ID: <250390410.3266953.1566973238795@mail.yahoo.com>

Hello,
I'm using the gather function from the tidyr package to reshape data.frames.
For example, for the the following dataframe, I apply the command below.df_initial<-data.frame(col1name=c(1,2,3,4,5,6,7), col2name=c(4,5,6,7,8,9,1), col3name=c(9,8,7,6,5,4,3))df_reshaped<-gather(df_initial, "col1name", "col2name", "col3name", key="colall", value="value")

In this example, there are 3 columns because there were 3 devices. But the number of devices can change and I would like to avoid adding new columns by hand in the 'gather' command.
Is there a way to adapt the command so that it automatically detect the number of columns of df_initial :

df_initial<-data.frame(col1name=c(1,2,3,4,5,6,7), col2name=c(4,5,6,7,8,9,1), col3name=c(9,8,7,6,5,4,3))
df_initial<-data.frame(col1name=c(1,2,3,4,5,6,7), col2name=c(4,5,6,7,8,9,1))
df_initial<-data.frame(col1name=c(1,2,3,4,5,6,7), col2name=c(4,5,6,7,8,9,1), col3name=c(9,8,7,6,5,4,3), col4name=c(5,6,8,9,7,4,1))
df_reshaped_auto<-gather(df_initial, ?????????, key="colall", value="value")

Thanks in advance to help me replace??????????,Ptit Bleu.

	[[alternative HTML version deleted]]


From ruipb@rr@d@s m@iii@g oii s@po@pt  Wed Aug 28 09:03:20 2019
From: ruipb@rr@d@s m@iii@g oii s@po@pt (ruipb@rr@d@s m@iii@g oii s@po@pt)
Date: Wed, 28 Aug 2019 08:03:20 +0100
Subject: [R] Use gather with a various number of columns
In-Reply-To: <250390410.3266953.1566973238795@mail.yahoo.com>
References: <250390410.3266953.1566973238795.ref@mail.yahoo.com>
 <250390410.3266953.1566973238795@mail.yahoo.com>
Message-ID: <20190828080320.Horde.78yCyzfmhlPypt7BD8Yf708@mail.sapo.pt>

Hello,

Just don't include the names of those columns.
Is this what you want?


library(tidyr)

df_initial1 <- data.frame(col1name=c(1,2,3,4,5,6,7),  
col2name=c(4,5,6,7,8,9,1), col3name=c(9,8,7,6,5,4,3))
df_initial2 <- data.frame(col1name=c(1,2,3,4,5,6,7),  
col2name=c(4,5,6,7,8,9,1))
df_initial3 <- data.frame(col1name=c(1,2,3,4,5,6,7),  
col2name=c(4,5,6,7,8,9,1), col3name=c(9,8,7,6,5,4,3),  
col4name=c(5,6,8,9,7,4,1))

gather(df_initial1, key="colall", value="value")
gather(df_initial2, key="colall", value="value")
gather(df_initial3, key="colall", value="value")


Hope this helps,

Rui Barradas









Citando lionel sicot via R-help <r-help at r-project.org>:

> Hello,
> I'm using the gather function from the tidyr package to reshape data.frames.
> For example, for the the following dataframe, I apply the command  
> below.df_initial<-data.frame(col1name=c(1,2,3,4,5,6,7),  
> col2name=c(4,5,6,7,8,9,1),  
> col3name=c(9,8,7,6,5,4,3))df_reshaped<-gather(df_initial,  
> "col1name", "col2name", "col3name", key="colall", value="value")
>
> In this example, there are 3 columns because there were 3 devices.  
> But the number of devices can change and I would like to avoid  
> adding new columns by hand in the 'gather' command.
> Is there a way to adapt the command so that it automatically detect  
> the number of columns of df_initial :
>
> df_initial<-data.frame(col1name=c(1,2,3,4,5,6,7),  
> col2name=c(4,5,6,7,8,9,1), col3name=c(9,8,7,6,5,4,3))
> df_initial<-data.frame(col1name=c(1,2,3,4,5,6,7), col2name=c(4,5,6,7,8,9,1))
> df_initial<-data.frame(col1name=c(1,2,3,4,5,6,7),  
> col2name=c(4,5,6,7,8,9,1), col3name=c(9,8,7,6,5,4,3),  
> col4name=c(5,6,8,9,7,4,1))
> df_reshaped_auto<-gather(df_initial, ?????????, key="colall", value="value")
>
> Thanks in advance to help me replace??????????,Ptit Bleu.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From beno|t@pe|e @end|ng |rom @co@@@|r  Wed Aug 28 12:34:46 2019
From: beno|t@pe|e @end|ng |rom @co@@@|r (=?iso-8859-1?Q?PELE_Beno=EEt_=28Acoss=29?=)
Date: Wed, 28 Aug 2019 10:34:46 +0000
Subject: [R] SVM - calculating values problem
Message-ID: <PR2P264MB0285AB0EAEB5DC79B860622984A30@PR2P264MB0285.FRAP264.PROD.OUTLOOK.COM>

Hello everybody,

That is the first time that I am working on a SVM modeling and I would like to calculate by myself the result values from the SVM for each line of my database (named x_appr_svm).

First I tested a linear SVM model using the e1071 package and to calculate the individual results by myself I did the next things :
Retrieving the model coefficients  : coef_svm<-t(svm$coefs) %*% x_appr_svm[svm$index,]
Calculating the values for each line : p2<-x_appr_svm %*% t(coef_svm) - svm$rho
Using the predict function to compare : p1<-attr(predict(object=svm, newdata=x_appr_svm, decision.values=T), "decision.values")
--> p1 and p2 are the same.

Next I tested a polynomial SVM model using the same package and the same method knowing that the model parameters are :
degree=2,  gamma=0.02, coef0=0.01
The calculation of the individual values becomes (I guess) : p2<-(0.02*x_appr_svm %*% t(coef_svm)+0.01)^2-svm$rho
--> p1 and p2 are really different!

Despite of my searching, I do not understand why or where is the problem in my second p2 formula. Do you see the mistake?

Thank you for your help and have a good day, Benoit (France).

	[[alternative HTML version deleted]]


From @@r@h@go@|ee @end|ng |rom gm@||@com  Wed Aug 28 12:50:02 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Wed, 28 Aug 2019 06:50:02 -0400
Subject: [R] SVM - calculating values problem
In-Reply-To: <PR2P264MB0285AB0EAEB5DC79B860622984A30@PR2P264MB0285.FRAP264.PROD.OUTLOOK.COM>
References: <PR2P264MB0285AB0EAEB5DC79B860622984A30@PR2P264MB0285.FRAP264.PROD.OUTLOOK.COM>
Message-ID: <CAM_vjukbY3QOFQRUgcb=k2hdf-hs__31RV5_eXZKa38-bQu_tA@mail.gmail.com>

You could download the package code from CRAN and look yourself at what the
predict function is doing.

Sarah

On Wed, Aug 28, 2019 at 6:35 AM PELE Beno?t (Acoss) <benoit.pele at acoss.fr>
wrote:

> Hello everybody,
>
> That is the first time that I am working on a SVM modeling and I would
> like to calculate by myself the result values from the SVM for each line of
> my database (named x_appr_svm).
>
> First I tested a linear SVM model using the e1071 package and to calculate
> the individual results by myself I did the next things :
> Retrieving the model coefficients  : coef_svm<-t(svm$coefs) %*%
> x_appr_svm[svm$index,]
> Calculating the values for each line : p2<-x_appr_svm %*% t(coef_svm) -
> svm$rho
> Using the predict function to compare : p1<-attr(predict(object=svm,
> newdata=x_appr_svm, decision.values=T), "decision.values")
> --> p1 and p2 are the same.
>
> Next I tested a polynomial SVM model using the same package and the same
> method knowing that the model parameters are :
> degree=2,  gamma=0.02, coef0=0.01
> The calculation of the individual values becomes (I guess) :
> p2<-(0.02*x_appr_svm %*% t(coef_svm)+0.01)^2-svm$rho
> --> p1 and p2 are really different!
>
> Despite of my searching, I do not understand why or where is the problem
> in my second p2 formula. Do you see the mistake?
>
> Thank you for your help and have a good day, Benoit (France).
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
-- 
Sarah Goslee (she/her)
http://www.sarahgoslee.com

	[[alternative HTML version deleted]]


From beno|t@pe|e @end|ng |rom @co@@@|r  Wed Aug 28 16:12:05 2019
From: beno|t@pe|e @end|ng |rom @co@@@|r (=?utf-8?B?UEVMRSBCZW5vw650IChBY29zcyk=?=)
Date: Wed, 28 Aug 2019 14:12:05 +0000
Subject: [R] SVM - calculating values problem
In-Reply-To: <CAM_vjukbY3QOFQRUgcb=k2hdf-hs__31RV5_eXZKa38-bQu_tA@mail.gmail.com>
References: <PR2P264MB0285AB0EAEB5DC79B860622984A30@PR2P264MB0285.FRAP264.PROD.OUTLOOK.COM>
 <CAM_vjukbY3QOFQRUgcb=k2hdf-hs__31RV5_eXZKa38-bQu_tA@mail.gmail.com>
Message-ID: <PR2P264MB0285ECE6C47C7702D5DAE1C784A30@PR2P264MB0285.FRAP264.PROD.OUTLOOK.COM>

Finally, I did not find the detail of the ?R_svmpredict? function that may be a C program but applying the function named ?svmpred? in the svminternals.pdf document I solved my problem.

Thank you and best regards, Benoit.

De : Sarah Goslee <sarah.goslee at gmail.com>
Envoy? : mercredi 28 ao?t 2019 12:50
? : PELE Beno?t (Acoss) <benoit.pele at acoss.fr>
Cc : R-help at r-project.org
Objet : Re: [R] SVM - calculating values problem

You could download the package code from CRAN and look yourself at what the predict function is doing.

Sarah

On Wed, Aug 28, 2019 at 6:35 AM PELE Beno?t (Acoss) <benoit.pele at acoss.fr<mailto:benoit.pele at acoss.fr>> wrote:
Hello everybody,

That is the first time that I am working on a SVM modeling and I would like to calculate by myself the result values from the SVM for each line of my database (named x_appr_svm).

First I tested a linear SVM model using the e1071 package and to calculate the individual results by myself I did the next things :
Retrieving the model coefficients  : coef_svm<-t(svm$coefs) %*% x_appr_svm[svm$index,]
Calculating the values for each line : p2<-x_appr_svm %*% t(coef_svm) - svm$rho
Using the predict function to compare : p1<-attr(predict(object=svm, newdata=x_appr_svm, decision.values=T), "decision.values")
--> p1 and p2 are the same.

Next I tested a polynomial SVM model using the same package and the same method knowing that the model parameters are :
degree=2,  gamma=0.02, coef0=0.01
The calculation of the individual values becomes (I guess) : p2<-(0.02*x_appr_svm %*% t(coef_svm)+0.01)^2-svm$rho
--> p1 and p2 are really different!

Despite of my searching, I do not understand why or where is the problem in my second p2 formula. Do you see the mistake?

Thank you for your help and have a good day, Benoit (France).

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
--
Sarah Goslee (she/her)
http://www.sarahgoslee.com

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Aug 28 16:34:09 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 28 Aug 2019 07:34:09 -0700
Subject: [R] Structuring Inventory in R
In-Reply-To: <CAC3qRfzwRe05cHEGH=avk56G=+uzc-WnUsS-B_D7ETw+PzbKgA@mail.gmail.com>
References: <CAC3qRfxuDb+-1j3RchB8ASsqcLhe4qZ-QSAhJV9yxn4Wv+pTtw@mail.gmail.com>
 <CA+8X3fXi_v-yw=qfpx93-co62S4gzDUOtgqyYuQJX5hZo=r_9Q@mail.gmail.com>
 <3DB074ED-747E-4449-BFC9-EF2581D2464A@dcn.davis.ca.us>
 <CAC3qRfzwRe05cHEGH=avk56G=+uzc-WnUsS-B_D7ETw+PzbKgA@mail.gmail.com>
Message-ID: <8DAA7737-D081-4F15-9DBC-70874B639048@dcn.davis.ca.us>

Why do you need to do this? It sounds like homework. Using multiple columns as lookup keys is the normal way this is handled.

On August 28, 2019 7:10:34 AM PDT, Alex Naverniak <stopro888 at gmail.com> wrote:
>Thank you.
>The problem is that I need multiple same size data structures for each
>item
>name. For example: Unique item name -"Table" has several (Lets say 5)
>subitems of 3 items in each("Size", "Price", "Qty"); Another item
>"Chair"
>has 4 subitems of the same structure (3 positions in each), etc. That
>list
>of furniture unique items may have indefinite number of triple
>subitems. I
>need to hace access to each position of subitem like [[1]][[3]][[2]]. I
>also need to dynamically add subitems and furniture items. The total
>size
>of furniture inventory is not known at the beginning. I hope it makes
>sense. Thank you.
>
>On Wed, Aug 28, 2019 at 2:18 AM Jeff Newmiller
><jdnewmil at dcn.davis.ca.us>
>wrote:
>
>> Perhaps
>>
>> dta <- data.frame( SKU=c("4950","8488","1159"),
>>
>Price=c(10,15,3),Qty=c(24,144,16),ID=c("10208473","38447769","43759115")
>)
>>
>> Lists are not as easy to optimize performance with as tables are...
>unless
>> you have a specific reason to use them I would minimize your use of
>> single-element items in lists.
>>
>> On August 27, 2019 9:49:28 PM PDT, Jim Lemon <drjimlemon at gmail.com>
>wrote:
>> >Hi Alex,
>> >At a guess you may want something like this:
>> >
>> >data.frame(item="SKU",price=10,qty=2,ID="00001")
>> >
>> >This produces a data frame with one row. You will probably want many
>> >more rows.
>> >
>> >Jim
>> >
>> >On Wed, Aug 28, 2019 at 2:21 PM Alex Naverniak <stopro888 at gmail.com>
>> >wrote:
>> >>
>> >> Hi,
>> >> I am trying to create Inventory structure with item SKU and 3
>> >subitems like
>> >> "Price"; "Qty"; "ID". I tried
>> >list(SKU,list("Item1","Item2","Item3")). It
>> >> seem not to work. Please help with ideas. Thanks.
>> >> Alex
>> >>
>> >>         [[alternative HTML version deleted]]
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >
>> >______________________________________________
>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>

-- 
Sent from my phone. Please excuse my brevity.


From |_j_rod @end|ng |rom hotm@||@com  Wed Aug 28 21:51:40 2019
From: |_j_rod @end|ng |rom hotm@||@com (Frank S.)
Date: Wed, 28 Aug 2019 19:51:40 +0000
Subject: [R] Efficient way to update a survival model
Message-ID: <VI1PR04MB5726D4BB6BF9B179B03F1FB1BAA30@VI1PR04MB5726.eurprd04.prod.outlook.com>

Hello everybody, I come with a question which I do not know how to conduct in an efficient way. In order to
provide a toy example, consider the dataset "pbc" from the package "survival". First, I fit the Cox model "Cox0":

library("survival")
set.seed(1)
v <- runif(nrow(pbc), min = 0, max = 2)
Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)

Then, from the above model, I can fit recursively 10 additional models as:

Cox <- list()

Cox[[1]] <- update(Cox0, . ~ . + cos(1 * v), data =  pbc)
Cox[[2]] <- update(Cox[[1]], . ~ . + cos(2 * v), data =  pbc)
Cox[[3]] <- update(Cox[[2]], . ~ . + cos(3 * v), data =  pbc)
Cox[[4]] <- update(Cox[[3]], . ~ . + cos(4 * v), data =  pbc)
...
Cox[[10]] <- update(Cox[[9]], . ~ . + cos(10* v), data =  pbc)

Since in practice I have to repeat above step until Cox[[100]], say, do you know an efficient way to
wrap this code chunk in a loop or similar?

I had tried:

set.seed(1)
v <- runif(nrow(pbc), min = 0, max = 2)
Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)

Cox <- list()
Cox[[1]] <- update(Cox0, . ~ . + cos(1 * v), data =  pbc)
for (k in 1:10) {
  Cox[[k + 1]] <- update(Cox[[k]], . ~ . + cos((k + 1) * v), data =  pbc)
}

However, from Cox[[3]] onwards, the intermediate values of integer k are not included here (for
 instance, the model Cox[[10]] would only include the cosinus terms for cos(1*v) and cos(10*v)).

Thanks in advance for any help!

Frank

	[[alternative HTML version deleted]]


From g||ted|||e2014 @end|ng |rom gm@||@com  Wed Aug 28 15:13:47 2019
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Wed, 28 Aug 2019 14:13:47 +0100
Subject: [R] New Work Based on R
Message-ID: <CAC8ss33Bj-YOgPfa0WML8RR7HSDt3QGZ0a3Ln-+PaP-YWLd92A@mail.gmail.com>

Dear Contributors,
Some of us that use the end product of R could pay for your services
if asked to do so. While all your help is free, I am a little
disturbed that there is no archive or repository where publications
employing R tools are deposited for those interested in  the
usefulness of R.

I have, with your great assistance, produced 4 papers this year and
more are under way. While I have never failed to cite R in the papers,
I feel like always saying additional thank you for your invaluable but
free time.

Many, many, many thanks to all of you.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Okike_2019_ApJ_882_15-1.pdf
Type: application/pdf
Size: 1393818 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190828/da51a5ef/attachment-0001.pdf>

From @topro888 @end|ng |rom gm@||@com  Wed Aug 28 16:10:34 2019
From: @topro888 @end|ng |rom gm@||@com (Alex Naverniak)
Date: Wed, 28 Aug 2019 10:10:34 -0400
Subject: [R] Structuring Inventory in R
In-Reply-To: <3DB074ED-747E-4449-BFC9-EF2581D2464A@dcn.davis.ca.us>
References: <CAC3qRfxuDb+-1j3RchB8ASsqcLhe4qZ-QSAhJV9yxn4Wv+pTtw@mail.gmail.com>
 <CA+8X3fXi_v-yw=qfpx93-co62S4gzDUOtgqyYuQJX5hZo=r_9Q@mail.gmail.com>
 <3DB074ED-747E-4449-BFC9-EF2581D2464A@dcn.davis.ca.us>
Message-ID: <CAC3qRfzwRe05cHEGH=avk56G=+uzc-WnUsS-B_D7ETw+PzbKgA@mail.gmail.com>

Thank you.
The problem is that I need multiple same size data structures for each item
name. For example: Unique item name -"Table" has several (Lets say 5)
subitems of 3 items in each("Size", "Price", "Qty"); Another item "Chair"
has 4 subitems of the same structure (3 positions in each), etc. That list
of furniture unique items may have indefinite number of triple subitems. I
need to hace access to each position of subitem like [[1]][[3]][[2]]. I
also need to dynamically add subitems and furniture items. The total size
of furniture inventory is not known at the beginning. I hope it makes
sense. Thank you.

On Wed, Aug 28, 2019 at 2:18 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Perhaps
>
> dta <- data.frame( SKU=c("4950","8488","1159"),
> Price=c(10,15,3),Qty=c(24,144,16),ID=c("10208473","38447769","43759115") )
>
> Lists are not as easy to optimize performance with as tables are... unless
> you have a specific reason to use them I would minimize your use of
> single-element items in lists.
>
> On August 27, 2019 9:49:28 PM PDT, Jim Lemon <drjimlemon at gmail.com> wrote:
> >Hi Alex,
> >At a guess you may want something like this:
> >
> >data.frame(item="SKU",price=10,qty=2,ID="00001")
> >
> >This produces a data frame with one row. You will probably want many
> >more rows.
> >
> >Jim
> >
> >On Wed, Aug 28, 2019 at 2:21 PM Alex Naverniak <stopro888 at gmail.com>
> >wrote:
> >>
> >> Hi,
> >> I am trying to create Inventory structure with item SKU and 3
> >subitems like
> >> "Price"; "Qty"; "ID". I tried
> >list(SKU,list("Item1","Item2","Item3")). It
> >> seem not to work. Please help with ideas. Thanks.
> >> Alex
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From @topro888 @end|ng |rom gm@||@com  Wed Aug 28 18:34:47 2019
From: @topro888 @end|ng |rom gm@||@com (Alex Naverniak)
Date: Wed, 28 Aug 2019 12:34:47 -0400
Subject: [R] Structuring Inventory in R
In-Reply-To: <8DAA7737-D081-4F15-9DBC-70874B639048@dcn.davis.ca.us>
References: <CAC3qRfxuDb+-1j3RchB8ASsqcLhe4qZ-QSAhJV9yxn4Wv+pTtw@mail.gmail.com>
 <CA+8X3fXi_v-yw=qfpx93-co62S4gzDUOtgqyYuQJX5hZo=r_9Q@mail.gmail.com>
 <3DB074ED-747E-4449-BFC9-EF2581D2464A@dcn.davis.ca.us>
 <CAC3qRfzwRe05cHEGH=avk56G=+uzc-WnUsS-B_D7ETw+PzbKgA@mail.gmail.com>
 <8DAA7737-D081-4F15-9DBC-70874B639048@dcn.davis.ca.us>
Message-ID: <CAC3qRfwg7DwSb63tkExdGvKE+RknwadUW9MKHK8-pDL5SPpmDQ@mail.gmail.com>

You are right.

On Wed, Aug 28, 2019 at 10:34 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Why do you need to do this? It sounds like homework. Using multiple
> columns as lookup keys is the normal way this is handled.
>
> On August 28, 2019 7:10:34 AM PDT, Alex Naverniak <stopro888 at gmail.com>
> wrote:
> >Thank you.
> >The problem is that I need multiple same size data structures for each
> >item
> >name. For example: Unique item name -"Table" has several (Lets say 5)
> >subitems of 3 items in each("Size", "Price", "Qty"); Another item
> >"Chair"
> >has 4 subitems of the same structure (3 positions in each), etc. That
> >list
> >of furniture unique items may have indefinite number of triple
> >subitems. I
> >need to hace access to each position of subitem like [[1]][[3]][[2]]. I
> >also need to dynamically add subitems and furniture items. The total
> >size
> >of furniture inventory is not known at the beginning. I hope it makes
> >sense. Thank you.
> >
> >On Wed, Aug 28, 2019 at 2:18 AM Jeff Newmiller
> ><jdnewmil at dcn.davis.ca.us>
> >wrote:
> >
> >> Perhaps
> >>
> >> dta <- data.frame( SKU=c("4950","8488","1159"),
> >>
> >Price=c(10,15,3),Qty=c(24,144,16),ID=c("10208473","38447769","43759115")
> >)
> >>
> >> Lists are not as easy to optimize performance with as tables are...
> >unless
> >> you have a specific reason to use them I would minimize your use of
> >> single-element items in lists.
> >>
> >> On August 27, 2019 9:49:28 PM PDT, Jim Lemon <drjimlemon at gmail.com>
> >wrote:
> >> >Hi Alex,
> >> >At a guess you may want something like this:
> >> >
> >> >data.frame(item="SKU",price=10,qty=2,ID="00001")
> >> >
> >> >This produces a data frame with one row. You will probably want many
> >> >more rows.
> >> >
> >> >Jim
> >> >
> >> >On Wed, Aug 28, 2019 at 2:21 PM Alex Naverniak <stopro888 at gmail.com>
> >> >wrote:
> >> >>
> >> >> Hi,
> >> >> I am trying to create Inventory structure with item SKU and 3
> >> >subitems like
> >> >> "Price"; "Qty"; "ID". I tried
> >> >list(SKU,list("Item1","Item2","Item3")). It
> >> >> seem not to work. Please help with ideas. Thanks.
> >> >> Alex
> >> >>
> >> >>         [[alternative HTML version deleted]]
> >> >>
> >> >> ______________________________________________
> >> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> >> PLEASE do read the posting guide
> >> >http://www.R-project.org/posting-guide.html
> >> >> and provide commented, minimal, self-contained, reproducible code.
> >> >
> >> >______________________________________________
> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >https://stat.ethz.ch/mailman/listinfo/r-help
> >> >PLEASE do read the posting guide
> >> >http://www.R-project.org/posting-guide.html
> >> >and provide commented, minimal, self-contained, reproducible code.
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From j@me@@@pott|@woode @end|ng |rom gm@||@com  Wed Aug 28 20:06:57 2019
From: j@me@@@pott|@woode @end|ng |rom gm@||@com (James Spottiswoode)
Date: Wed, 28 Aug 2019 11:06:57 -0700
Subject: [R] Problem parallelizing across cores
Message-ID: <CAOiiqf3a6jD9c6H21i35Du08=F8pK0Qcbu_i7kzrqQSCqRZGtQ@mail.gmail.com>

Hi All,

I have a piece of well optimized R code for doing text analysis running
under Linux on an AWS instance.  The code first loads a number of packages
and some needed data and the actual analysis is done by a function called,
say, f(string).  I would like to parallelize calling this function across
the 8 cores of the instance to increase throughput.  I have looked at the
packages doParallel and future but am not clear how to do this.  Any method
that brings up an R instance when the function is called will not work for
me as the time to load the packages and data is comparable to the execution
time of the function leading to no speed up.  Therefore I need to keep a
number of instances of the R code running continuously so that the data
loading only occurs once when the R processes are first started and
thereafter the function f(string) is ready to run in each instance.  I hope
I have put this clearly.

I?d much appreciate any suggestions.  Thanks in advance,

James Spottiswoode


--

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Thu Aug 29 00:39:28 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 28 Aug 2019 15:39:28 -0700
Subject: [R] Problem parallelizing across cores
In-Reply-To: <CAOiiqf3a6jD9c6H21i35Du08=F8pK0Qcbu_i7kzrqQSCqRZGtQ@mail.gmail.com>
References: <CAOiiqf3a6jD9c6H21i35Du08=F8pK0Qcbu_i7kzrqQSCqRZGtQ@mail.gmail.com>
Message-ID: <CAGxFJbS2W-1ecv-ar3+FwsyDWT5WCh91HPOd_cHGsYCD86neTg@mail.gmail.com>

I would suggest that that you search on "parallel computing" at the
Rseek.org site. This brought up what seemed to be many relevant hits
including, of course, the High Performance and parallel Computing Cran task
view.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Aug 28, 2019 at 3:18 PM James Spottiswoode <
james.spottiswoode at gmail.com> wrote:

> Hi All,
>
> I have a piece of well optimized R code for doing text analysis running
> under Linux on an AWS instance.  The code first loads a number of packages
> and some needed data and the actual analysis is done by a function called,
> say, f(string).  I would like to parallelize calling this function across
> the 8 cores of the instance to increase throughput.  I have looked at the
> packages doParallel and future but am not clear how to do this.  Any method
> that brings up an R instance when the function is called will not work for
> me as the time to load the packages and data is comparable to the execution
> time of the function leading to no speed up.  Therefore I need to keep a
> number of instances of the R code running continuously so that the data
> loading only occurs once when the R processes are first started and
> thereafter the function f(string) is ready to run in each instance.  I hope
> I have put this clearly.
>
> I?d much appreciate any suggestions.  Thanks in advance,
>
> James Spottiswoode
>
>
> --
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Aug 29 02:10:21 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 28 Aug 2019 17:10:21 -0700
Subject: [R] Problem parallelizing across cores
In-Reply-To: <CAOiiqf3a6jD9c6H21i35Du08=F8pK0Qcbu_i7kzrqQSCqRZGtQ@mail.gmail.com>
References: <CAOiiqf3a6jD9c6H21i35Du08=F8pK0Qcbu_i7kzrqQSCqRZGtQ@mail.gmail.com>
Message-ID: <57523F39-CEF5-4075-AAB8-CA6BA86B5BA6@dcn.davis.ca.us>

Your first option is always to serially compute results. When the computation time is long compared to session overhead and data I/O, you can consider parallel computing. You should first consider laying out your independent computation work units as a sequence, and then allocate segments of that sequence to your workers, each of which will perform their respective sub-sequences serially so as to minimize the overhead penalty... so yes, you absolutely can use a method that starts new instances of R ("SNOW"). You also have forking on Linux which has lower overhead, but not zero so the exact same logic can be applied. But if you arbitrarily shorten your serial computations too much then you cannot optimize your use of available processing resources as you have already observed.

However, your lack of reproducible example is a strong indicator that you are not really asking a question about R... so do some reading and focus your next question on R or the base R parallel package per the Posting Guide. (Do read that... posting HTML is a good way
for your message to get scrambled before we see it.) Wide-ranging discussions on computer science and HPC hardware constraints are outside the topic here.

On August 28, 2019 11:06:57 AM PDT, James Spottiswoode <james.spottiswoode at gmail.com> wrote:
>Hi All,
>
>I have a piece of well optimized R code for doing text analysis running
>under Linux on an AWS instance.  The code first loads a number of
>packages
>and some needed data and the actual analysis is done by a function
>called,
>say, f(string).  I would like to parallelize calling this function
>across
>the 8 cores of the instance to increase throughput.  I have looked at
>the
>packages doParallel and future but am not clear how to do this.  Any
>method
>that brings up an R instance when the function is called will not work
>for
>me as the time to load the packages and data is comparable to the
>execution
>time of the function leading to no speed up.  Therefore I need to keep
>a
>number of instances of the R code running continuously so that the data
>loading only occurs once when the R processes are first started and
>thereafter the function f(string) is ready to run in each instance.  I
>hope
>I have put this clearly.
>
>I?d much appreciate any suggestions.  Thanks in advance,
>
>James Spottiswoode
>
>
>--
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From dw|n@em|u@ @end|ng |rom comc@@t@net  Thu Aug 29 03:32:50 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Wed, 28 Aug 2019 18:32:50 -0700
Subject: [R] New Work Based on R
In-Reply-To: <CAC8ss33Bj-YOgPfa0WML8RR7HSDt3QGZ0a3Ln-+PaP-YWLd92A@mail.gmail.com>
References: <CAC8ss33Bj-YOgPfa0WML8RR7HSDt3QGZ0a3Ln-+PaP-YWLd92A@mail.gmail.com>
Message-ID: <bd31ecbf-a0db-835c-0399-556bcf94bdeb@comcast.net>


On 8/28/19 6:13 AM, Ogbos Okike wrote:
> Dear Contributors,
> Some of us that use the end product of R could pay for your services
> if asked to do so. While all your help is free, I am a little
> disturbed that there is no archive or repository where publications
> employing R tools are deposited for those interested in  the
> usefulness of R.

CRAN would qualify as an "archive or repository where publications

employing R tools are deposited for those interested in  the
usefulness of R."  There are also efforts to provide

There is a focused search site called Rseek and there is a suite of CRAN 
Task Views.


-- 

David.


> I have, with your great assistance, produced 4 papers this year and
> more are under way. While I have never failed to cite R in the papers,
> I feel like always saying additional thank you for your invaluable but
> free time.
>
> Many, many, many thanks to all of you.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m|n@to-n@k@z@w@ @end|ng |rom um|n@net  Thu Aug 29 08:15:55 2019
From: m|n@to-n@k@z@w@ @end|ng |rom um|n@net (Minato Nakazawa)
Date: Thu, 29 Aug 2019 15:15:55 +0900
Subject: [R] I wrote the document for extended usage of my pyramid package
Message-ID: <20190829151555.16ee3e615863f6dfab6a7b63@umin.net>

Dear R-users,

I have developed the package pyramid, which has been available from CRAN,
but there was little information about how to use it.

Recently I wrote the document for extended usage of pyramid package as

http://minato.sip21c.org/demography/makepyramid-en.html

Any comments are welcome.

I'm sorry if such announcement is not suitable for this list.
Best Wishes,
-- 
Minato Nakazawa <minato-nakazawa at umin.net>
Professor, Division of Global Health, Department of Public Health,
Kobe University Graduate School of Health Sciences
[web] http://minato.sip21c.org/
[phone] +81-78-796-4551
[mobile e-mail] minatonakazawa at gmail.com


From v|to@muggeo @end|ng |rom un|p@@|t  Thu Aug 29 08:54:10 2019
From: v|to@muggeo @end|ng |rom un|p@@|t (Vito Michele Rosario Muggeo)
Date: Thu, 29 Aug 2019 08:54:10 +0200
Subject: [R] Efficient way to update a survival model
In-Reply-To: <VI1PR04MB5726D4BB6BF9B179B03F1FB1BAA30@VI1PR04MB5726.eurprd04.prod.outlook.com>
Message-ID: <20190829085410.Horde.cR3kDdYBZGYzoxbukl-m7xZ@webmail.unipa.it>

dear Frank,

update() does not update actually.. It just builds a new call which is  
evaluated. To speed up the procedure you could try to supply starting  
values via argument 'init'. The first values come from the previous  
fit, and the last one referring to new coefficients is set to zero (or  
any other appropriate value).

Something like (untested), for instance

update(Cox[[2]], . ~ . + cos(3 * v), init=c(coef(Cox[[1]]),0), data =  pbc)

Hope this helps,
best,
vito



"Frank S." <f_j_rod at hotmail.com> ha scritto:

> Hello everybody, I come with a question which I do not know how to  
> conduct in an efficient way. In order to
> provide a toy example, consider the dataset "pbc" from the package  
> "survival". First, I fit the Cox model "Cox0":
>
> library("survival")
> set.seed(1)
> v <- runif(nrow(pbc), min = 0, max = 2)
> Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)
>
> Then, from the above model, I can fit recursively 10 additional models as:
>
> Cox <- list()
>
> Cox[[1]] <- update(Cox0, . ~ . + cos(1 * v), data =  pbc)
> Cox[[2]] <- update(Cox[[1]], . ~ . + cos(2 * v), data =  pbc)
> Cox[[3]] <- update(Cox[[2]], . ~ . + cos(3 * v), data =  pbc)
> Cox[[4]] <- update(Cox[[3]], . ~ . + cos(4 * v), data =  pbc)
> ...
> Cox[[10]] <- update(Cox[[9]], . ~ . + cos(10* v), data =  pbc)
>
> Since in practice I have to repeat above step until Cox[[100]], say,  
> do you know an efficient way to
> wrap this code chunk in a loop or similar?
>
> I had tried:
>
> set.seed(1)
> v <- runif(nrow(pbc), min = 0, max = 2)
> Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)
>
> Cox <- list()
> Cox[[1]] <- update(Cox0, . ~ . + cos(1 * v), data =  pbc)
> for (k in 1:10) {
>   Cox[[k + 1]] <- update(Cox[[k]], . ~ . + cos((k + 1) * v), data =  pbc)
> }
>
> However, from Cox[[3]] onwards, the intermediate values of integer k  
> are not included here (for
>  instance, the model Cox[[10]] would only include the cosinus terms  
> for cos(1*v) and cos(10*v)).
>
> Thanks in advance for any help!
>
> Frank
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From R@|ner @end|ng |rom krug@@de  Thu Aug 29 10:23:48 2019
From: R@|ner @end|ng |rom krug@@de (Rainer M Krug)
Date: Thu, 29 Aug 2019 10:23:48 +0200
Subject: [R] New Work Based on R
In-Reply-To: <bd31ecbf-a0db-835c-0399-556bcf94bdeb@comcast.net>
References: <CAC8ss33Bj-YOgPfa0WML8RR7HSDt3QGZ0a3Ln-+PaP-YWLd92A@mail.gmail.com>
 <bd31ecbf-a0db-835c-0399-556bcf94bdeb@comcast.net>
Message-ID: <0D2336C1-82DD-46DF-AF76-CE237D5C9342@krugs.de>



> On 29 Aug 2019, at 03:32, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
> On 8/28/19 6:13 AM, Ogbos Okike wrote:
>> Dear Contributors,
>> Some of us that use the end product of R could pay for your services
>> if asked to do so. While all your help is free, I am a little
>> disturbed that there is no archive or repository where publications
>> employing R tools are deposited for those interested in  the
>> usefulness of R.
> 
> CRAN would qualify as an "archive or repository where publications

I think you are misunderstanding the OP. Ogbos is, as I understand, looking for a repository for publications using R (as in scientific publications) and not that much.

Correct me if I am wrong, but I am not aware that CRAN hosts a bibliography of publications using R (which would be an interesting thing to have?).

Cheers,

Rainer
 


> 
> employing R tools are deposited for those interested in  the
> usefulness of R."  There are also efforts to provide
> 
> There is a focused search site called Rseek and there is a suite of CRAN Task Views.
> 
> 
> -- 
> 
> David.
> 
> 
>> I have, with your great assistance, produced 4 papers this year and
>> more are under way. While I have never failed to cite R in the papers,
>> I feel like always saying additional thank you for your invaluable but
>> free time.
>> 
>> Many, many, many thanks to all of you.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation Biology, UCT), Dipl. Phys. (Germany)

Department of Evolutionary Biology and Environmental Studies
University of Z?rich



	[[alternative HTML version deleted]]


From |_j_rod @end|ng |rom hotm@||@com  Thu Aug 29 12:38:52 2019
From: |_j_rod @end|ng |rom hotm@||@com (Frank S.)
Date: Thu, 29 Aug 2019 10:38:52 +0000
Subject: [R] Efficient way to update a survival model
In-Reply-To: <20190829085410.Horde.cR3kDdYBZGYzoxbukl-m7xZ@webmail.unipa.it>
References: <VI1PR04MB5726D4BB6BF9B179B03F1FB1BAA30@VI1PR04MB5726.eurprd04.prod.outlook.com>,
 <20190829085410.Horde.cR3kDdYBZGYzoxbukl-m7xZ@webmail.unipa.it>
Message-ID: <VI1PR04MB57262BC7EFF56B29A668A88DBAA20@VI1PR04MB5726.eurprd04.prod.outlook.com>

Hi Vito,

Thanks for your reply! Following your suggestion, I have tried:

Cox[[3]] <- update(Cox[[2]], . ~ . + cos(3 * v), init=c(coef(Cox[[1]]), 0, 0), data =  pbc)
Cox[[3]] <- update(Cox[[2]], . ~ . + cos(3 * v), data =  pbc)

and both expressions lead to the same result. Is that OK?

Additionally, in my original question I wondered about the possibility of reducing the
10 lines of code to one general expression or some  loop. Is it possible?

Best,

Frank
________________________________
De: Vito Michele Rosario Muggeo <vito.muggeo at unipa.it>
Enviado: jueves, 29 de agosto de 2019 8:54
Para: Frank S. <f_j_rod at hotmail.com>
Cc: r-help at r-project.org <r-help at r-project.org>
Asunto: Re: [R] Efficient way to update a survival model

dear Frank,

update() does not update actually.. It just builds a new call which is
evaluated. To speed up the procedure you could try to supply starting
values via argument 'init'. The first values come from the previous
fit, and the last one referring to new coefficients is set to zero (or
any other appropriate value).

Something like (untested), for instance

update(Cox[[2]], . ~ . + cos(3 * v), init=c(coef(Cox[[1]]),0), data =  pbc)

Hope this helps,
best,
vito



"Frank S." <f_j_rod at hotmail.com> ha scritto:

> Hello everybody, I come with a question which I do not know how to
> conduct in an efficient way. In order to
> provide a toy example, consider the dataset "pbc" from the package
> "survival". First, I fit the Cox model "Cox0":
>
> library("survival")
> set.seed(1)
> v <- runif(nrow(pbc), min = 0, max = 2)
> Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)
>
> Then, from the above model, I can fit recursively 10 additional models as:
>
> Cox <- list()
>
> Cox[[1]] <- update(Cox0, . ~ . + cos(1 * v), data =  pbc)
> Cox[[2]] <- update(Cox[[1]], . ~ . + cos(2 * v), data =  pbc)
> Cox[[3]] <- update(Cox[[2]], . ~ . + cos(3 * v), data =  pbc)
> Cox[[4]] <- update(Cox[[3]], . ~ . + cos(4 * v), data =  pbc)
> ...
> Cox[[10]] <- update(Cox[[9]], . ~ . + cos(10* v), data =  pbc)
>
> Since in practice I have to repeat above step until Cox[[100]], say,
> do you know an efficient way to
> wrap this code chunk in a loop or similar?
>
> I had tried:
>
> set.seed(1)
> v <- runif(nrow(pbc), min = 0, max = 2)
> Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)
>
> Cox <- list()
> Cox[[1]] <- update(Cox0, . ~ . + cos(1 * v), data =  pbc)
> for (k in 1:10) {
>   Cox[[k + 1]] <- update(Cox[[k]], . ~ . + cos((k + 1) * v), data =  pbc)
> }
>
> However, from Cox[[3]] onwards, the intermediate values of integer k
> are not included here (for
>  instance, the model Cox[[10]] would only include the cosinus terms
> for cos(1*v) and cos(10*v)).
>
[[elided Hotmail spam]]
>
> Frank
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



	[[alternative HTML version deleted]]


From j@me@ @end|ng |rom j@@@oc@com  Thu Aug 29 01:55:14 2019
From: j@me@ @end|ng |rom j@@@oc@com (James Spottiswoode)
Date: Wed, 28 Aug 2019 16:55:14 -0700
Subject: [R] Problem parallelizing across cores
In-Reply-To: <56F6D25D-8CBD-49B2-8F41-5916A0F7B27C@jsasoc.com>
References: <CAOiiqf3a6jD9c6H21i35Du08=F8pK0Qcbu_i7kzrqQSCqRZGtQ@mail.gmail.com>
 <CAGxFJbS2W-1ecv-ar3+FwsyDWT5WCh91HPOd_cHGsYCD86neTg@mail.gmail.com>
 <56F6D25D-8CBD-49B2-8F41-5916A0F7B27C@jsasoc.com>
Message-ID: <C8CD9DAD-6BE8-4058-B372-602FE461D703@jsasoc.com>



> On Aug 28, 2019, at 4:44 PM, James Spottiswoode <james at jsasoc.com> wrote:
> 
> Hi Bert,
> 
> Thanks for your advice.  Actually i?ve already done this and have checked out doParallel and future packages.  The trouble with doParallel is that it forks R processes which spend a lot of time loading data and packages whereas my function runs in 100ms so the parallelization doesn?t help.  The future package keeps it?s children running but I haven?t figured out how to get it to work in my application.
> 
> Best ? James
> 
> 
>> On Aug 28, 2019, at 3:39 PM, Bert Gunter <bgunter.4567 at gmail.com <mailto:bgunter.4567 at gmail.com>> wrote:
>> 
>> 
>> I would suggest that that you search on "parallel computing" at the Rseek.org <http://rseek.org/> site. This brought up what seemed to be many relevant hits including, of course, the High Performance and parallel Computing Cran task view.
>> 
>> Cheers,
>> Bert
>> 
>> Bert Gunter
>> 
>> "The trouble with having an open mind is that people keep coming along and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>> 
>> 
>> On Wed, Aug 28, 2019 at 3:18 PM James Spottiswoode <james.spottiswoode at gmail.com <mailto:james.spottiswoode at gmail.com>> wrote:
>> Hi All,
>> 
>> I have a piece of well optimized R code for doing text analysis running
>> under Linux on an AWS instance.  The code first loads a number of packages
>> and some needed data and the actual analysis is done by a function called,
>> say, f(string).  I would like to parallelize calling this function across
>> the 8 cores of the instance to increase throughput.  I have looked at the
>> packages doParallel and future but am not clear how to do this.  Any method
>> that brings up an R instance when the function is called will not work for
>> me as the time to load the packages and data is comparable to the execution
>> time of the function leading to no speed up.  Therefore I need to keep a
>> number of instances of the R code running continuously so that the data
>> loading only occurs once when the R processes are first started and
>> thereafter the function f(string) is ready to run in each instance.  I hope
>> I have put this clearly.
>> 
>> I?d much appreciate any suggestions.  Thanks in advance,
>> 
>> James Spottiswoode
>> 
>> 
>> --
>> 
>>         [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org <mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help <https://stat.ethz.ch/mailman/listinfo/r-help>
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html <http://www.r-project.org/posting-guide.html>
>> and provide commented, minimal, self-contained, reproducible code.
> 
> James Spottiswoode
> Applied Mathematics & Statistics
> (310) 270 6220
> jamesspottiswoode Skype
> james at jsasoc.com <mailto:james at jsasoc.com>

James Spottiswoode
Applied Mathematics & Statistics
(310) 270 6220
jamesspottiswoode Skype
james at jsasoc.com


	[[alternative HTML version deleted]]


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Thu Aug 29 14:24:08 2019
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Thu, 29 Aug 2019 13:24:08 +0100
Subject: [R] 
 I wrote the document for extended usage of my pyramid package
In-Reply-To: <20190829151555.16ee3e615863f6dfab6a7b63@umin.net>
References: <20190829151555.16ee3e615863f6dfab6a7b63@umin.net>
Message-ID: <7cf1bcf5-812d-f13a-0645-dee1fb843d16@dewey.myzen.co.uk>

Dear Minato

That is a nice idea, but why not make it a vignette of your package on 
CRAN so it is immediately accessible to anyone using the package?

Michael

On 29/08/2019 07:15, Minato Nakazawa wrote:
> Dear R-users,
> 
> I have developed the package pyramid, which has been available from CRAN,
> but there was little information about how to use it.
> 
> Recently I wrote the document for extended usage of pyramid package as
> 
> http://minato.sip21c.org/demography/makepyramid-en.html
> 
> Any comments are welcome.
> 
> I'm sorry if such announcement is not suitable for this list.
> Best Wishes,
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Thu Aug 29 14:48:31 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Thu, 29 Aug 2019 05:48:31 -0700 (PDT)
Subject: [R] New Work Based on R
In-Reply-To: <0D2336C1-82DD-46DF-AF76-CE237D5C9342@krugs.de>
References: <CAC8ss33Bj-YOgPfa0WML8RR7HSDt3QGZ0a3Ln-+PaP-YWLd92A@mail.gmail.com>
 <bd31ecbf-a0db-835c-0399-556bcf94bdeb@comcast.net>
 <0D2336C1-82DD-46DF-AF76-CE237D5C9342@krugs.de>
Message-ID: <alpine.LNX.2.20.1908290542450.2953@salmo.appl-ecosys.com>

On Thu, 29 Aug 2019, Rainer M Krug wrote:

> I think you are misunderstanding the OP. Ogbos is, as I understand,
> looking for a repository for publications using R (as in scientific
> publications) and not that much.

Rainer, et al.:

I agree with your interpretation of the request and suggest that the request
is most likely unreasonable. How many publications explicitly name the
software used to produce anaytical results or plots (or the document
itself)?

I've been out of academia for a very long time yet read many articles and
techical reports in my work as an environmental consultant. I don't recall
seeing software mentioned in any of them. The focus of the author(s) is on
the subject of the document and not the tools used to create it. Unless, of
course, the document is about statistics or data analyses.

Regards,

Rich


From er|cjberger @end|ng |rom gm@||@com  Thu Aug 29 15:37:25 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Thu, 29 Aug 2019 16:37:25 +0300
Subject: [R] New Work Based on R
In-Reply-To: <alpine.LNX.2.20.1908290542450.2953@salmo.appl-ecosys.com>
References: <CAC8ss33Bj-YOgPfa0WML8RR7HSDt3QGZ0a3Ln-+PaP-YWLd92A@mail.gmail.com>
 <bd31ecbf-a0db-835c-0399-556bcf94bdeb@comcast.net>
 <0D2336C1-82DD-46DF-AF76-CE237D5C9342@krugs.de>
 <alpine.LNX.2.20.1908290542450.2953@salmo.appl-ecosys.com>
Message-ID: <CAGgJW75kCse5v-rCAOgQy50kYq1rdwP82yLm3iiVfu39ZqOcWw@mail.gmail.com>

Rich writes: " I've been out of academia for a very long time ... "
He may not be aware that currently authors are encouraged to support
reproducible research, which includes making available (where possible)
both the data and the software programs that were used in the analysis.
I think the OP's suggestion leads to a number of interesting use cases.
There are certainly cases where research is copyrighted with the publishing
journal so full access for all articles would not be possible. Food for
thought.

Eric


On Thu, Aug 29, 2019 at 3:48 PM Rich Shepard <rshepard at appl-ecosys.com>
wrote:

> On Thu, 29 Aug 2019, Rainer M Krug wrote:
>
> > I think you are misunderstanding the OP. Ogbos is, as I understand,
> > looking for a repository for publications using R (as in scientific
> > publications) and not that much.
>
> Rainer, et al.:
>
> I agree with your interpretation of the request and suggest that the
> request
> is most likely unreasonable. How many publications explicitly name the
> software used to produce anaytical results or plots (or the document
> itself)?
>
> I've been out of academia for a very long time yet read many articles and
> techical reports in my work as an environmental consultant. I don't recall
> seeing software mentioned in any of them. The focus of the author(s) is on
> the subject of the document and not the tools used to create it. Unless, of
> course, the document is about statistics or data analyses.
>
> Regards,
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Thu Aug 29 15:45:12 2019
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Thu, 29 Aug 2019 09:45:12 -0400
Subject: [R] New Work Based on R
In-Reply-To: <CAGgJW75kCse5v-rCAOgQy50kYq1rdwP82yLm3iiVfu39ZqOcWw@mail.gmail.com>
References: <CAC8ss33Bj-YOgPfa0WML8RR7HSDt3QGZ0a3Ln-+PaP-YWLd92A@mail.gmail.com>
 <bd31ecbf-a0db-835c-0399-556bcf94bdeb@comcast.net>
 <0D2336C1-82DD-46DF-AF76-CE237D5C9342@krugs.de>
 <alpine.LNX.2.20.1908290542450.2953@salmo.appl-ecosys.com>
 <CAGgJW75kCse5v-rCAOgQy50kYq1rdwP82yLm3iiVfu39ZqOcWw@mail.gmail.com>
Message-ID: <CAJc=yOEmRg3DEHY-FSFv7TdPmN2E6HMitAn9OzQvrXd8ax_Czg@mail.gmail.com>

Perhaps rather than re-inventing the wheel, some current open science
repository(ies) would work, just with an encouragement to use subject
tagging to indicate the software.

Pat

On Thu, Aug 29, 2019 at 9:38 AM Eric Berger <ericjberger at gmail.com> wrote:
>
> Rich writes: " I've been out of academia for a very long time ... "
> He may not be aware that currently authors are encouraged to support
> reproducible research, which includes making available (where possible)
> both the data and the software programs that were used in the analysis.
> I think the OP's suggestion leads to a number of interesting use cases.
> There are certainly cases where research is copyrighted with the publishing
> journal so full access for all articles would not be possible. Food for
> thought.
>
> Eric
>
>
> On Thu, Aug 29, 2019 at 3:48 PM Rich Shepard <rshepard at appl-ecosys.com>
> wrote:
>
> > On Thu, 29 Aug 2019, Rainer M Krug wrote:
> >
> > > I think you are misunderstanding the OP. Ogbos is, as I understand,
> > > looking for a repository for publications using R (as in scientific
> > > publications) and not that much.
> >
> > Rainer, et al.:
> >
> > I agree with your interpretation of the request and suggest that the
> > request
> > is most likely unreasonable. How many publications explicitly name the
> > software used to produce anaytical results or plots (or the document
> > itself)?
> >
> > I've been out of academia for a very long time yet read many articles and
> > techical reports in my work as an environmental consultant. I don't recall
> > seeing software mentioned in any of them. The focus of the author(s) is on
> > the subject of the document and not the tools used to create it. Unless, of
> > course, the document is about statistics or data analyses.
> >
> > Regards,
> >
> > Rich
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Thu Aug 29 15:58:09 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Thu, 29 Aug 2019 06:58:09 -0700 (PDT)
Subject: [R] New Work Based on R
In-Reply-To: <CAGgJW75kCse5v-rCAOgQy50kYq1rdwP82yLm3iiVfu39ZqOcWw@mail.gmail.com>
References: <CAC8ss33Bj-YOgPfa0WML8RR7HSDt3QGZ0a3Ln-+PaP-YWLd92A@mail.gmail.com>
 <bd31ecbf-a0db-835c-0399-556bcf94bdeb@comcast.net>
 <0D2336C1-82DD-46DF-AF76-CE237D5C9342@krugs.de>
 <alpine.LNX.2.20.1908290542450.2953@salmo.appl-ecosys.com>
 <CAGgJW75kCse5v-rCAOgQy50kYq1rdwP82yLm3iiVfu39ZqOcWw@mail.gmail.com>
Message-ID: <alpine.LNX.2.20.1908290656340.2953@salmo.appl-ecosys.com>

On Thu, 29 Aug 2019, Eric Berger wrote:

> He may not be aware that currently authors are encouraged to support
> reproducible research, which includes making available (where possible)
> both the data and the software programs that were used in the analysis.

Eric,

You're correct: I did not know this. I retract my comment as it's
inappropriate.

Mea culpa,

Rich


From @@r@h@go@|ee @end|ng |rom gm@||@com  Thu Aug 29 16:16:48 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Thu, 29 Aug 2019 10:16:48 -0400
Subject: [R] New Work Based on R
In-Reply-To: <CAC8ss33Bj-YOgPfa0WML8RR7HSDt3QGZ0a3Ln-+PaP-YWLd92A@mail.gmail.com>
References: <CAC8ss33Bj-YOgPfa0WML8RR7HSDt3QGZ0a3Ln-+PaP-YWLd92A@mail.gmail.com>
Message-ID: <CAM_vjunkQfYBCN+kvnUyha7iU_BkLH+_ivtyYXdr0Mj6HWB4HA@mail.gmail.com>

Hi R folks,

After following this thread, I got curious, and tried to do a Web of
Science search to see how many citations of R there are. It's a
*mess*. Even limiting it to "R Core Team" as first author, there are
so many variations in citation that WoS eventually gave up and told me
that I couldn't add any more.

Google Scholar gives  "Cited by 134011 - All 118 versions."

I think trying to create a repository might also be a mess!

But it's clear from this attempt that it's harder than it should be to
consistently and clearly cite R, even with the handy citation()
function, or there wouldn't be such a mess in WoS.

Perhaps it's time to create a DOI for R itself, to help standardize
the mess. (If there is a DOI, it doesn't show up in citation().)
Versioning would be a slight pain, but according to Zenodo at least,
they maintain a DOI for each version, and a master DOI.

Also, of course, make sure your students and colleagues cite R and the
R packages they use! The authors should be properly credited, and it
makes it possible for interested people to see where and how the
software is used.

And if you want to support R financially, there are worse things to do
than donate to the R Foundation. https://www.r-project.org/foundation/

Sarah

---
Sarah Goslee
http://www.numberwright.com


From d@n||o_rodr|guez @end|ng |rom cun@edu@co  Thu Aug 29 17:01:37 2019
From: d@n||o_rodr|guez @end|ng |rom cun@edu@co (Danilo Esteban Rodriguez Zapata)
Date: Thu, 29 Aug 2019 10:01:37 -0500
Subject: [R] R code: How to correct "Error in parse(text = x,
 keep.source = FALSE)" output in psych package using own dataset
Message-ID: <CAMMrtmZg1HryjK134A8xH73Ba=Pmkbmj5JOLLGbdSYib1PE3qA@mail.gmail.com>

This is a problem related to my last question referred to the omegaSem()
function in the psych package (that is already solved because I realized
that I was missing a variable assignment and because of that I had an
'object not found' error:

https://stackoverflow.com/questions/57661750/one-of-the-omegasem-function-arguments-is-an-object-not-found

I was trying to use that function following the guide to find McDonald's
hierarchical Omega by Dr William Revelle:

http://personality-project.org/r/psych/HowTo/omega.pdf

So now, with the variable error corrected, I'm having a different error
that does not occur when I use the same function with the example database
(Thurstone) provided in the tutorial that comes with the psych package. I
mean, I'm able to use the function succesfully using the Thurstone data
(with no other action, I have the expected result) but the function doesn't
work when I use my own data.

I searched over other posted questions, and the actions that they perform
are not even similar to what I'm trying to do. I have almost two weeks
using R, so I'm not able to identify yet how can I extrapolate the
solutions for that error message to my procedure (because it seems to be
frequent), although I have basic code knowledge. However related questions
give no anwer by now.

Additionally, I decided to look over more documentation about the package,
and when I was testing other functions, I was able to use the omegaSem()
function with another example database, BUT after and only after I did the
schmid transformation. So with that, I discovered that when I tried to use
the omegaSem() function before the schmid tranformation I had the same
error message, but not after that tranformation with this second example
database.

This make sense with the actual procedure of the omegaSem() procedure, but
I'm suposing that it must be done completely and automatically by the
omegaSem() function as it is explained in the guide and I have understood
until now, as it follows:

1. omegaSem() applies factor analysis
2. omegaSem() rotate factors obliquely
3. omegaSem() transform data with Schmid Leiman (schmid)

-------necessary steps to print output-------------------

4. omegaSem() print McDonald's hierarchical Omega

So here, another questions appears:  - Why the omegaSem() function works
with the Thurstone database without any other action and only works for the
second example database after performing the schmid transformation? -  Why
with other databases I dont have the same output applying the omegaSem()
function directly? - How is this related to the error message that the
compiler shows when I try to apply the function directly to the database?


This is the code that I'm using now: (example of the succesfull omegaSem()
done after schmid tranformation not included)

```
> library(psych)
> library(ctv, lavaan)
> library(GPArotation)
> my.data <- read.file()
Data from the .csv file
D:\Users\Admon\Documents\prueba_export_1563806208742.csv has been loaded.
> describe(my.data)
           vars   n mean   sd median trimmed  mad min max range  skew
kurtosis
AUT_10_04     1 195 4.11 0.90      4    4.23 1.48   1   5     4 -0.92
0.33
AUN_07_01     2 195 3.79 1.14      4    3.90 1.48   1   5     4 -0.59
 -0.71
AUN_07_02     3 195 3.58 1.08      4    3.65 1.48   1   5     4 -0.39
 -0.56
AUN_09_01     4 195 4.15 0.80      4    4.23 1.48   1   5     4 -0.76
0.51
AUN_10_01     5 195 4.25 0.79      4    4.34 1.48   1   5     4 -0.91
0.74
AUT_11_01     6 195 4.43 0.77      5    4.56 0.00   1   5     4 -1.69
3.77
AUT_17_01     7 195 4.46 0.67      5    4.55 0.00   1   5     4 -1.34
2.96
AUT_20_03     8 195 4.44 0.65      5    4.53 0.00   2   5     3 -0.84
0.12
CRE_05_02     9 195 2.47 1.01      2    2.43 1.48   1   5     4  0.35
 -0.46
CRE_07_04    10 195 2.42 1.08      2    2.34 1.48   1   5     4  0.51
 -0.43
CRE_10_01    11 195 4.41 0.68      5    4.51 0.00   2   5     3 -0.79
 -0.12
CRE_16_02    12 195 2.75 1.23      3    2.69 1.48   1   5     4  0.29
 -0.96
EFEC_03_07   13 195 4.35 0.69      4    4.45 1.48   1   5     4 -0.95
1.59
EFEC_05      14 195 4.53 0.59      5    4.60 0.00   3   5     2 -0.82
 -0.34
EFEC_09_02   15 195 2.19 0.91      2    2.11 1.48   1   5     4  0.57
 -0.03
EFEC_16_03   16 195 4.21 0.77      4    4.29 1.48   2   5     3 -0.71
 -0.04
EVA_02_01    17 195 4.47 0.61      5    4.54 0.00   3   5     2 -0.70
 -0.50
EVA_07_01    18 195 4.38 0.60      4    4.43 1.48   3   5     2 -0.40
 -0.70
EVA_12_02    19 195 2.64 1.22      2    2.59 1.48   1   5     4  0.30
 -1.00
EVA_15_06    20 195 4.19 0.74      4    4.26 1.48   2   5     3 -0.55
 -0.29
FLX_04_01    21 195 4.32 0.69      4    4.41 1.48   2   5     3 -0.71
0.05
FLX_04_05    22 195 4.23 0.74      4    4.32 0.00   1   5     4 -0.99
1.69
FLX_08_02    23 195 2.87 1.19      3    2.86 1.48   1   5     4  0.07
 -1.05
FLX_10_03    24 195 4.30 0.71      4    4.39 1.48   2   5     3 -0.84
0.66
IDO_01_06    25 195 3.10 1.26      3    3.13 1.48   1   5     4 -0.19
 -1.08
IDO_05_02    26 195 2.89 1.26      3    2.87 1.48   1   5     4 -0.03
 -1.16
IDO_09_03    27 195 3.87 0.97      4    3.99 1.48   1   5     4 -0.84
0.47
IDO_17_01    28 195 3.94 0.88      4    4.02 0.00   1   5     4 -0.93
1.23
IE_01_03     29 195 4.01 0.88      4    4.10 1.48   1   5     4 -0.91
0.94
IE_10_03     30 195 4.15 1.00      4    4.34 1.48   1   5     4 -1.31
1.28
IE_13_03     31 195 4.16 0.91      4    4.30 1.48   1   5     4 -1.26
1.74
IE_15_01     32 195 4.26 0.85      4    4.39 1.48   1   5     4 -1.16
1.08
LC_07_03     33 195 4.25 0.72      4    4.34 0.00   1   5     4 -1.07
2.64
LC_08_02     34 195 3.25 1.22      4    3.31 1.48   1   5     4 -0.41
 -0.90
LC_11_03     35 195 3.50 1.14      4    3.56 1.48   1   5     4 -0.38
 -0.68
LC_11_05     36 195 4.42 0.69      5    4.52 0.00   1   5     4 -1.14
1.97
ME_02_03     37 195 4.11 0.92      4    4.25 1.48   1   5     4 -1.18
1.29
ME_07_06     38 195 3.19 1.28      3    3.24 1.48   1   5     4 -0.28
 -1.03
ME_09_01     39 195 4.24 0.77      4    4.34 1.48   1   5     4 -1.12
2.19
ME_09_06     40 195 3.23 1.33      4    3.29 1.48   1   5     4 -0.31
 -1.14
NEG_01_03    41 195 4.18 0.76      4    4.27 0.00   1   5     4 -1.28
3.33
NEG_05_04    42 195 4.27 0.69      4    4.35 0.00   1   5     4 -0.87
1.75
NEG_07_03    43 195 4.32 0.73      4    4.43 1.48   1   5     4 -1.05
1.55
NEG_08_01    44 195 3.95 0.88      4    4.02 1.48   1   5     4 -0.67
0.29
OP_03_05     45 195 4.32 0.66      4    4.39 0.00   1   5     4 -0.99
2.54
OP_12_01     46 195 4.16 0.80      4    4.25 1.48   1   5     4 -1.02
1.57
OP_14_01     47 195 4.27 0.78      4    4.38 1.48   1   5     4 -1.15
1.67
OP_14_02     48 195 4.36 0.68      4    4.44 1.48   1   5     4 -1.07
2.35
ORL_01_03    49 195 4.36 0.77      4    4.49 1.48   1   5     4 -1.31
2.08
ORL_03_01    50 195 4.41 0.69      4    4.50 1.48   1   5     4 -1.28
2.77
ORL_03_05    51 195 4.36 0.74      4    4.48 1.48   2   5     3 -1.13
1.28
ORL_10_05    52 195 4.40 0.68      4    4.48 1.48   1   5     4 -1.18
2.57
PER_08_02    53 195 3.23 1.29      4    3.29 1.48   1   5     4 -0.26
 -1.17
PER_16_01    54 195 4.29 0.70      4    4.38 1.48   2   5     3 -0.74
0.27
PER_19_06    55 195 3.19 1.25      3    3.24 1.48   1   5     4 -0.20
 -1.06
PER_22_06    56 195 4.21 0.73      4    4.29 0.00   1   5     4 -0.89
1.46
PLA_01_03    57 195 4.23 0.68      4    4.31 0.00   2   5     3 -0.81
1.18
PLA_05_01    58 195 4.06 0.77      4    4.13 0.00   1   5     4 -0.89
1.29
PLA_07_02    59 195 2.94 1.19      3    2.94 1.48   1   5     4  0.00
 -1.02
PLA_10_01    60 195 4.03 0.76      4    4.08 0.00   1   5     4 -0.68
0.87
PLA_12_02    61 195 2.67 1.11      2    2.62 1.48   1   5     4  0.41
 -0.61
PLA_18_01    62 195 4.01 0.85      4    4.09 1.48   1   5     4 -0.82
0.78
PR_06_02     63 195 3.02 1.27      3    3.02 1.48   1   5     4 -0.01
 -1.13
PR_15_03     64 195 3.55 1.07      4    3.62 1.48   1   5     4 -0.46
 -0.22
PR_25_01     65 195 2.36 1.04      2    2.27 1.48   1   5     4  0.73
0.06
PR_25_06     66 195 2.95 1.17      3    2.94 1.48   1   5     4  0.04
 -0.86
REL_09_05    67 195 3.81 0.95      4    3.89 1.48   1   5     4 -0.51
 -0.31
REL_14_03    68 195 3.99 0.88      4    4.08 1.48   1   5     4 -0.75
0.39
REL_14_06    69 195 2.93 1.26      3    2.92 1.48   1   5     4  0.06
 -1.11
REL_16_04    70 195 3.16 1.27      3    3.20 1.48   1   5     4 -0.13
 -1.11
RS_02_03     71 195 4.14 0.75      4    4.22 0.00   1   5     4 -0.82
1.14
RS_07_05     72 195 4.29 0.67      4    4.38 0.00   2   5     3 -0.72
0.59
RS_08_05     73 195 4.04 0.88      4    4.13 1.48   1   5     4 -0.97
1.26
RS_13_03     74 195 4.19 0.69      4    4.25 0.00   2   5     3 -0.46
 -0.17
TF_03_01     75 195 4.01 0.82      4    4.06 1.48   1   5     4 -0.63
0.32
TF_04_01     76 195 4.09 0.76      4    4.15 0.00   1   5     4 -0.70
0.76
TF_10_03     77 195 4.11 0.85      4    4.21 1.48   1   5     4 -0.96
0.99
TF_12_01     78 195 4.11 0.85      4    4.21 1.48   1   5     4 -1.10
1.66
TRE_09_05    79 195 4.29 0.79      4    4.39 1.48   1   5     4 -1.12
1.74
TRE_09_06    80 195 4.33 0.69      4    4.42 1.48   1   5     4 -1.10
2.36
TRE_26_04    81 195 2.97 1.20      3    2.96 1.48   1   5     4  0.08
 -1.01
TRE_26_05    82 195 3.99 0.84      4    4.03 1.48   1   5     4 -0.41
 -0.37

```

Until now, I have charged the libraries, import the my own database and did
some simple descriptive statistics.

```

> r9 <- my.data
> omega(r9)
Omega
Call: omega(m = r9)
Alpha:                 0.95
G.6:                   0.98
Omega Hierarchical:    0.85
Omega H asymptotic:    0.89
Omega Total            0.96

Schmid Leiman Factor loadings greater than  0.2
                g   F1*   F2*   F3*   h2   u2   p2
AUT_10_04    0.43              0.30 0.27 0.73 0.68
AUN_07_01                           0.05 0.95 0.53
AUN_07_02                           0.06 0.94 0.26
AUN_09_01    0.38              0.30 0.24 0.76 0.59
AUN_10_01    0.35              0.55 0.44 0.56 0.29
AUT_11_01    0.42              0.30 0.27 0.73 0.66
AUT_17_01    0.32              0.40 0.28 0.72 0.37
AUT_20_03    0.41              0.25 0.24 0.76 0.73
CRE_05_02-   0.24       -0.53       0.34 0.66 0.17
CRE_07_04-   0.37       -0.51       0.39 0.61 0.35
CRE_10_01    0.46              0.48 0.46 0.54 0.47
CRE_16_02-              -0.70       0.48 0.52 0.01
EFEC_03_07   0.46              0.31 0.31 0.69 0.68
EFEC_05      0.43              0.32 0.29 0.71 0.64
EFEC_09_02-  0.29       -0.46       0.29 0.71 0.28
EFEC_16_03   0.49              0.26 0.31 0.69 0.77
EVA_02_01    0.55              0.21 0.36 0.64 0.85
EVA_07_01    0.57                   0.37 0.63 0.89
EVA_12_02-              -0.61       0.39 0.61 0.06
EVA_15_06    0.50              0.37 0.39 0.61 0.65
FLX_04_01    0.57              0.30 0.42 0.58 0.78
FLX_04_05    0.52              0.26 0.34 0.66 0.80
FLX_08_02-              -0.78       0.60 0.40 0.00
FLX_10_03    0.39              0.29 0.24 0.76 0.63
IDO_01_06-              -0.80       0.64 0.36 0.00
IDO_05_02-              -0.78       0.62 0.38 0.00
IDO_09_03    0.41              0.49 0.42 0.58 0.40
IDO_17_01    0.51              0.51 0.54 0.46 0.49
IE_01_03     0.44              0.60 0.56 0.44 0.35
IE_10_03     0.41              0.53 0.44 0.56 0.37
IE_13_03     0.39              0.48 0.38 0.62 0.40
IE_15_01     0.39              0.40 0.31 0.69 0.49
LC_07_03     0.50                   0.27 0.73 0.91
LC_08_02                 0.83       0.69 0.31 0.00
LC_11_03     0.25                   0.10 0.90 0.60
LC_11_05     0.45        0.24       0.27 0.73 0.75
ME_02_03     0.55                   0.31 0.69 0.99
ME_07_06                 0.85       0.75 0.25 0.02
ME_09_01     0.64                   0.45 0.55 0.93
ME_09_06                 0.81       0.69 0.31 0.02
NEG_01_03    0.58              0.20 0.38 0.62 0.88
NEG_05_04    0.70                   0.50 0.50 0.98
NEG_07_03    0.64                   0.43 0.57 0.96
NEG_08_01    0.43              0.25 0.25 0.75 0.74
OP_03_05     0.62                   0.40 0.60 0.98
OP_12_01     0.67                   0.46 0.54 0.98
OP_14_01     0.60                   0.38 0.62 0.95
OP_14_02     0.66                   0.47 0.53 0.93
ORL_01_03    0.67                   0.47 0.53 0.96
ORL_03_01    0.66                   0.48 0.52 0.91
ORL_03_05    0.64                   0.46 0.54 0.90
ORL_10_05    0.66                   0.49 0.51 0.89
PER_08_02    0.21        0.84       0.75 0.25 0.06
PER_16_01    0.68              0.21 0.50 0.50 0.91
PER_19_06    0.20        0.73       0.58 0.42 0.07
PER_22_06    0.53                   0.30 0.70 0.94
PLA_01_03    0.57                   0.36 0.64 0.89
PLA_05_01    0.61                   0.42 0.58 0.89
PLA_07_02                0.75       0.61 0.39 0.04
PLA_10_01    0.56                   0.36 0.64 0.88
PLA_12_02                0.61       0.37 0.63 0.00
PLA_18_01    0.63                   0.47 0.53 0.85
PR_06_02                 0.77       0.62 0.38 0.03
PR_15_03     0.31       -0.39  0.24 0.31 0.69 0.31
PR_25_01-               -0.56       0.32 0.68 0.00
PR_25_06                 0.74       0.55 0.45 0.01
REL_09_05    0.41       -0.23  0.38 0.37 0.63 0.45
REL_14_03    0.41       -0.21  0.29 0.30 0.70 0.56
REL_14_06                0.66  0.21 0.48 0.52 0.04
REL_16_04                0.78       0.63 0.37 0.03
RS_02_03     0.57                   0.36 0.64 0.90
RS_07_05     0.68                   0.47 0.53 0.99
RS_08_05     0.44                   0.20 0.80 0.95
RS_13_03     0.67                   0.46 0.54 0.97
TF_03_01     0.66                   0.44 0.56 0.98
TF_04_01     0.74                   0.56 0.44 0.98
TF_10_03     0.70                   0.50 0.50 0.98
TF_12_01     0.61                   0.40 0.60 0.92
TRE_09_05    0.70              0.23 0.55 0.45 0.89
TRE_09_06    0.62                   0.41 0.59 0.93
TRE_26_04-              -0.68       0.47 0.53 0.00
TRE_26_05    0.55       -0.21       0.34 0.66 0.88

With eigenvalues of:
    g   F1*   F2*   F3*
18.06  0.04 11.47  4.32

general/max  1.57   max/min =   267.1
mean percent general =  0.58    with sd =  0.36 and cv of  0.63
Explained Common Variance of the general factor =  0.53

The degrees of freedom are 3078  and the fit is  34.62
The number of observations was  195  with Chi Square =  5671.12  with prob
<  2.8e-157
The root mean square of the residuals is  0.06
The df corrected root mean square of the residuals is  0.06
RMSEA index =  0.078  and the 10 % confidence intervals are  0.063 NA
BIC =  -10559.18

Compare this with the adequacy of just a general factor and no group factors
The degrees of freedom for just the general factor are 3239  and the fit is
 51.52
The number of observations was  195  with Chi Square =  8509.84  with prob
<  0
The root mean square of the residuals is  0.16
The df corrected root mean square of the residuals is  0.16

RMSEA index =  0.104  and the 10 % confidence intervals are  0.089 NA
BIC =  -8569.4

Measures of factor score adequacy
                                                 g   F1*  F2*  F3*
Correlation of scores with factors            0.98  0.07 0.98 0.91
Multiple R square of scores with factors      0.95  0.00 0.97 0.83
Minimum correlation of factor score estimates 0.91 -0.99 0.94 0.66

 Total, General and Subset omega for each subset
                                                 g F1*  F2*  F3*
Omega total for total scores and subscales    0.96  NA 0.83 0.95
Omega general for total scores and subscales  0.85  NA 0.82 0.76
Omega group for total scores and subscales    0.09  NA 0.01 0.19
```

Now, until here, I apply the basic (non hierarchical) omega() function to
my own database


```
> omegaSem(r9,n.obs=198)
Error in parse(text = x, keep.source = FALSE) :
  <text>:2:0: unexpected end of input
1: ~
```
The previous is the error message that appears after trying to use the
omegaSem() function directly with my own database.

Now, following, I present the expected output of omegaSem() applied
directly using the Thurstone database. It's similar to the output of the
basic omega() function but it has certain distinctions:

```

> r9 <- Thurstone
> omegaSem(r9,n.obs=500)

Call: omegaSem(m = r9, n.obs = 500)
Omega
Call: omega(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip,
    digits = digits, title = title, sl = sl, labels = labels,
    plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option)
Alpha:                 0.89
G.6:                   0.91
Omega Hierarchical:    0.74
Omega H asymptotic:    0.79
Omega Total            0.93

Schmid Leiman Factor loadings greater than  0.2
                     g   F1*   F2*   F3*   h2   u2   p2
Sentences         0.71  0.56             0.82 0.18 0.61
Vocabulary        0.73  0.55             0.84 0.16 0.63
Sent.Completion   0.68  0.52             0.74 0.26 0.63
First.Letters     0.65        0.56       0.73 0.27 0.57
Four.Letter.Words 0.62        0.49       0.63 0.37 0.61
Suffixes          0.56        0.41       0.50 0.50 0.63
Letter.Series     0.59              0.62 0.73 0.27 0.48
Pedigrees         0.58  0.24        0.34 0.51 0.49 0.66
Letter.Group      0.54              0.46 0.52 0.48 0.56

With eigenvalues of:
   g  F1*  F2*  F3*
3.58 0.96 0.74 0.72

general/max  3.73   max/min =   1.34
mean percent general =  0.6    with sd =  0.05 and cv of  0.09
Explained Common Variance of the general factor =  0.6

The degrees of freedom are 12  and the fit is  0.01
The number of observations was  500  with Chi Square =  7.12  with prob <
 0.85
The root mean square of the residuals is  0.01
The df corrected root mean square of the residuals is  0.01
RMSEA index =  0  and the 10 % confidence intervals are  0 0.026
BIC =  -67.45

Compare this with the adequacy of just a general factor and no group factors
The degrees of freedom for just the general factor are 27  and the fit is
 1.48
The number of observations was  500  with Chi Square =  730.93  with prob <
 1.3e-136
The root mean square of the residuals is  0.14
The df corrected root mean square of the residuals is  0.16

RMSEA index =  0.23  and the 10 % confidence intervals are  0.214 0.243
BIC =  563.14

Measures of factor score adequacy
                                                 g  F1*  F2*  F3*
Correlation of scores with factors            0.86 0.73 0.72 0.75
Multiple R square of scores with factors      0.74 0.54 0.51 0.57
Minimum correlation of factor score estimates 0.49 0.07 0.03 0.13

 Total, General and Subset omega for each subset
                                                 g  F1*  F2*  F3*
Omega total for total scores and subscales    0.93 0.92 0.83 0.79
Omega general for total scores and subscales  0.74 0.58 0.50 0.47
Omega group for total scores and subscales    0.16 0.34 0.32 0.32

 The following analyses were done using the  lavaan  package

 Omega Hierarchical from a confirmatory model using sem =  0.79
 Omega Total  from a confirmatory model using sem =  0.93
With loadings of
                     g  F1*  F2*  F3*   h2   u2   p2
Sentences         0.77 0.49           0.83 0.17 0.71
Vocabulary        0.79 0.45           0.83 0.17 0.75
Sent.Completion   0.75 0.40           0.73 0.27 0.77
First.Letters     0.61      0.61      0.75 0.25 0.50
Four.Letter.Words 0.60      0.51      0.61 0.39 0.59
Suffixes          0.57      0.39      0.48 0.52 0.68
Letter.Series     0.57           0.73 0.85 0.15 0.38
Pedigrees         0.66           0.25 0.50 0.50 0.87
Letter.Group      0.53           0.41 0.45 0.55 0.62

With eigenvalues of:
   g  F1*  F2*  F3*
3.87 0.60 0.79 0.76

The degrees of freedom of the confimatory model are  18  and the fit is
 57.11391  with p =  5.936744e-06
general/max  4.92   max/min =   1.3
mean percent general =  0.65    with sd =  0.15 and cv of  0.23
Explained Common Variance of the general factor =  0.64

Measures of factor score adequacy
                                                 g   F1*  F2*  F3*
Correlation of scores with factors            0.90  0.68 0.80 0.85
Multiple R square of scores with factors      0.81  0.46 0.64 0.73
Minimum correlation of factor score estimates 0.62 -0.08 0.27 0.45

 Total, General and Subset omega for each subset
                                                 g  F1*  F2*  F3*
Omega total for total scores and subscales    0.93 0.92 0.82 0.80
Omega general for total scores and subscales  0.79 0.69 0.48 0.50
Omega group for total scores and subscales    0.14 0.23 0.35 0.31

To get the standard sem fit statistics, ask for summary on the fitted
object>
```



I'm expecting to have the same output applying the function directly. My
expectation is to make sure if its mandatory to make the schmid
transformation before the omegaSem(). I'm supposing that not, because its
not supposed to work like that as it says in the guide. Maybe this can be
solved correcting the error message:

```
> r9 <- my.data
> omegaSem(r9,n.obs=198)
Error in parse(text = x, keep.source = FALSE) :
  <text>:2:0: unexpected end of input
1: ~
   ^
```
 Hope I've been clear enough. Feel free to ask any other information that
you might need.

Thank you so much for giving me any guidance to reach the answer of this
issue. I higly appreciate any help.

Regards,

Danilo

-- 
Danilo E. Rodr?guez Zapata
Analista en Psicometr?a
CEBIAC

	[[alternative HTML version deleted]]


From wdun|@p @end|ng |rom t|bco@com  Thu Aug 29 19:32:50 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Thu, 29 Aug 2019 10:32:50 -0700
Subject: [R] R code: How to correct "Error in parse(text = x,
 keep.source = FALSE)" output in psych package using own dataset
In-Reply-To: <CAMMrtmZg1HryjK134A8xH73Ba=Pmkbmj5JOLLGbdSYib1PE3qA@mail.gmail.com>
References: <CAMMrtmZg1HryjK134A8xH73Ba=Pmkbmj5JOLLGbdSYib1PE3qA@mail.gmail.com>
Message-ID: <CAF8bMcZAajdt0PssTK2xN24fmt3HzEKSvVwUMnW_ZnB30maf=g@mail.gmail.com>

    > omegaSem(r9,n.obs=198)
    Error in parse(text = x, keep.source = FALSE) :
      <text>:2:0: unexpected end of input

This error probably comes from calling factor("~") and
psych::omegaSem(data) will do that if  all the columns in data are very
highly correlated with one another.   In that case omega(data, nfactor=n)
will not be able to find n factors in the data but it returns "~" in place
of the factors that it could not find.  E.g.,
> fakeData <- data.frame(A=1/(1:40), B=1/(2:41), C=1/(3:42), D=1/(4:43),
E=1/(5:44))
> cor(fakeData)
          A         B         C         D         E
A 1.0000000 0.9782320 0.9481293 0.9215071 0.8988962
B 0.9782320 1.0000000 0.9932037 0.9811287 0.9684658
C 0.9481293 0.9932037 1.0000000 0.9969157 0.9906838
D 0.9215071 0.9811287 0.9969157 1.0000000 0.9983014
E 0.8988962 0.9684658 0.9906838 0.9983014 1.0000000
> psych::omegaSem(fakeData)
Loading required namespace: lavaan
Loading required namespace: GPArotation
In factor.stats, I could not find the RMSEA upper bound . Sorry about that
Error in parse(text = x, keep.source = FALSE) :
  <text>:2:0: unexpected end of input
1: ~
   ^
In addition: Warning message:
In cov2cor(t(w) %*% r %*% w) :
  diag(.) had 0 or NA entries; non-finite result is doubtful
> psych::omega(fakeData)$model$lavaan
In factor.stats, I could not find the RMSEA upper bound . Sorry about that
[1] g =~ +A+B+C+D+E       F1=~  + B + C + D + E F2=~  + A
[4] F3=~
Warning message:
In cov2cor(t(w) %*% r %*% w) :
  diag(.) had 0 or NA entries; non-finite result is doubtful

You can get a result if you use nfactors=n where n is the number of the
good F<n> entries in psych::omega()$model$lavaan:
> psych::omegaSem(fakeData, nfactors=2)
...

Measures of factor score adequacy
                                                   g    F1*      F2*
Correlation of scores with factors             11.35  12.42    84.45
Multiple R square of scores with factors      128.93 154.32  7131.98
Minimum correlation of factor score estimates 256.86 307.64 14262.96
...
Does that work with your data?

This is a problem that the maintainer of psych,
>   maintainer("psych")
[1] "William Revelle <revelle at northwestern.edu>"
would like to know about.






Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Thu, Aug 29, 2019 at 9:03 AM Danilo Esteban Rodriguez Zapata via R-help <
r-help at r-project.org> wrote:

> This is a problem related to my last question referred to the omegaSem()
> function in the psych package (that is already solved because I realized
> that I was missing a variable assignment and because of that I had an
> 'object not found' error:
>
>
> https://stackoverflow.com/questions/57661750/one-of-the-omegasem-function-arguments-is-an-object-not-found
>
> I was trying to use that function following the guide to find McDonald's
> hierarchical Omega by Dr William Revelle:
>
> http://personality-project.org/r/psych/HowTo/omega.pdf
>
> So now, with the variable error corrected, I'm having a different error
> that does not occur when I use the same function with the example database
> (Thurstone) provided in the tutorial that comes with the psych package. I
> mean, I'm able to use the function succesfully using the Thurstone data
> (with no other action, I have the expected result) but the function doesn't
> work when I use my own data.
>
> I searched over other posted questions, and the actions that they perform
> are not even similar to what I'm trying to do. I have almost two weeks
> using R, so I'm not able to identify yet how can I extrapolate the
> solutions for that error message to my procedure (because it seems to be
> frequent), although I have basic code knowledge. However related questions
> give no anwer by now.
>
> Additionally, I decided to look over more documentation about the package,
> and when I was testing other functions, I was able to use the omegaSem()
> function with another example database, BUT after and only after I did the
> schmid transformation. So with that, I discovered that when I tried to use
> the omegaSem() function before the schmid tranformation I had the same
> error message, but not after that tranformation with this second example
> database.
>
> This make sense with the actual procedure of the omegaSem() procedure, but
> I'm suposing that it must be done completely and automatically by the
> omegaSem() function as it is explained in the guide and I have understood
> until now, as it follows:
>
> 1. omegaSem() applies factor analysis
> 2. omegaSem() rotate factors obliquely
> 3. omegaSem() transform data with Schmid Leiman (schmid)
>
> -------necessary steps to print output-------------------
>
> 4. omegaSem() print McDonald's hierarchical Omega
>
> So here, another questions appears:  - Why the omegaSem() function works
> with the Thurstone database without any other action and only works for the
> second example database after performing the schmid transformation? -  Why
> with other databases I dont have the same output applying the omegaSem()
> function directly? - How is this related to the error message that the
> compiler shows when I try to apply the function directly to the database?
>
>
> This is the code that I'm using now: (example of the succesfull omegaSem()
> done after schmid tranformation not included)
>
> ```
> > library(psych)
> > library(ctv, lavaan)
> > library(GPArotation)
> > my.data <- read.file()
> Data from the .csv file
> D:\Users\Admon\Documents\prueba_export_1563806208742.csv has been loaded.
> > describe(my.data)
>            vars   n mean   sd median trimmed  mad min max range  skew
> kurtosis
> AUT_10_04     1 195 4.11 0.90      4    4.23 1.48   1   5     4 -0.92
> 0.33
> AUN_07_01     2 195 3.79 1.14      4    3.90 1.48   1   5     4 -0.59
>  -0.71
> AUN_07_02     3 195 3.58 1.08      4    3.65 1.48   1   5     4 -0.39
>  -0.56
> AUN_09_01     4 195 4.15 0.80      4    4.23 1.48   1   5     4 -0.76
> 0.51
> AUN_10_01     5 195 4.25 0.79      4    4.34 1.48   1   5     4 -0.91
> 0.74
> AUT_11_01     6 195 4.43 0.77      5    4.56 0.00   1   5     4 -1.69
> 3.77
> AUT_17_01     7 195 4.46 0.67      5    4.55 0.00   1   5     4 -1.34
> 2.96
> AUT_20_03     8 195 4.44 0.65      5    4.53 0.00   2   5     3 -0.84
> 0.12
> CRE_05_02     9 195 2.47 1.01      2    2.43 1.48   1   5     4  0.35
>  -0.46
> CRE_07_04    10 195 2.42 1.08      2    2.34 1.48   1   5     4  0.51
>  -0.43
> CRE_10_01    11 195 4.41 0.68      5    4.51 0.00   2   5     3 -0.79
>  -0.12
> CRE_16_02    12 195 2.75 1.23      3    2.69 1.48   1   5     4  0.29
>  -0.96
> EFEC_03_07   13 195 4.35 0.69      4    4.45 1.48   1   5     4 -0.95
> 1.59
> EFEC_05      14 195 4.53 0.59      5    4.60 0.00   3   5     2 -0.82
>  -0.34
> EFEC_09_02   15 195 2.19 0.91      2    2.11 1.48   1   5     4  0.57
>  -0.03
> EFEC_16_03   16 195 4.21 0.77      4    4.29 1.48   2   5     3 -0.71
>  -0.04
> EVA_02_01    17 195 4.47 0.61      5    4.54 0.00   3   5     2 -0.70
>  -0.50
> EVA_07_01    18 195 4.38 0.60      4    4.43 1.48   3   5     2 -0.40
>  -0.70
> EVA_12_02    19 195 2.64 1.22      2    2.59 1.48   1   5     4  0.30
>  -1.00
> EVA_15_06    20 195 4.19 0.74      4    4.26 1.48   2   5     3 -0.55
>  -0.29
> FLX_04_01    21 195 4.32 0.69      4    4.41 1.48   2   5     3 -0.71
> 0.05
> FLX_04_05    22 195 4.23 0.74      4    4.32 0.00   1   5     4 -0.99
> 1.69
> FLX_08_02    23 195 2.87 1.19      3    2.86 1.48   1   5     4  0.07
>  -1.05
> FLX_10_03    24 195 4.30 0.71      4    4.39 1.48   2   5     3 -0.84
> 0.66
> IDO_01_06    25 195 3.10 1.26      3    3.13 1.48   1   5     4 -0.19
>  -1.08
> IDO_05_02    26 195 2.89 1.26      3    2.87 1.48   1   5     4 -0.03
>  -1.16
> IDO_09_03    27 195 3.87 0.97      4    3.99 1.48   1   5     4 -0.84
> 0.47
> IDO_17_01    28 195 3.94 0.88      4    4.02 0.00   1   5     4 -0.93
> 1.23
> IE_01_03     29 195 4.01 0.88      4    4.10 1.48   1   5     4 -0.91
> 0.94
> IE_10_03     30 195 4.15 1.00      4    4.34 1.48   1   5     4 -1.31
> 1.28
> IE_13_03     31 195 4.16 0.91      4    4.30 1.48   1   5     4 -1.26
> 1.74
> IE_15_01     32 195 4.26 0.85      4    4.39 1.48   1   5     4 -1.16
> 1.08
> LC_07_03     33 195 4.25 0.72      4    4.34 0.00   1   5     4 -1.07
> 2.64
> LC_08_02     34 195 3.25 1.22      4    3.31 1.48   1   5     4 -0.41
>  -0.90
> LC_11_03     35 195 3.50 1.14      4    3.56 1.48   1   5     4 -0.38
>  -0.68
> LC_11_05     36 195 4.42 0.69      5    4.52 0.00   1   5     4 -1.14
> 1.97
> ME_02_03     37 195 4.11 0.92      4    4.25 1.48   1   5     4 -1.18
> 1.29
> ME_07_06     38 195 3.19 1.28      3    3.24 1.48   1   5     4 -0.28
>  -1.03
> ME_09_01     39 195 4.24 0.77      4    4.34 1.48   1   5     4 -1.12
> 2.19
> ME_09_06     40 195 3.23 1.33      4    3.29 1.48   1   5     4 -0.31
>  -1.14
> NEG_01_03    41 195 4.18 0.76      4    4.27 0.00   1   5     4 -1.28
> 3.33
> NEG_05_04    42 195 4.27 0.69      4    4.35 0.00   1   5     4 -0.87
> 1.75
> NEG_07_03    43 195 4.32 0.73      4    4.43 1.48   1   5     4 -1.05
> 1.55
> NEG_08_01    44 195 3.95 0.88      4    4.02 1.48   1   5     4 -0.67
> 0.29
> OP_03_05     45 195 4.32 0.66      4    4.39 0.00   1   5     4 -0.99
> 2.54
> OP_12_01     46 195 4.16 0.80      4    4.25 1.48   1   5     4 -1.02
> 1.57
> OP_14_01     47 195 4.27 0.78      4    4.38 1.48   1   5     4 -1.15
> 1.67
> OP_14_02     48 195 4.36 0.68      4    4.44 1.48   1   5     4 -1.07
> 2.35
> ORL_01_03    49 195 4.36 0.77      4    4.49 1.48   1   5     4 -1.31
> 2.08
> ORL_03_01    50 195 4.41 0.69      4    4.50 1.48   1   5     4 -1.28
> 2.77
> ORL_03_05    51 195 4.36 0.74      4    4.48 1.48   2   5     3 -1.13
> 1.28
> ORL_10_05    52 195 4.40 0.68      4    4.48 1.48   1   5     4 -1.18
> 2.57
> PER_08_02    53 195 3.23 1.29      4    3.29 1.48   1   5     4 -0.26
>  -1.17
> PER_16_01    54 195 4.29 0.70      4    4.38 1.48   2   5     3 -0.74
> 0.27
> PER_19_06    55 195 3.19 1.25      3    3.24 1.48   1   5     4 -0.20
>  -1.06
> PER_22_06    56 195 4.21 0.73      4    4.29 0.00   1   5     4 -0.89
> 1.46
> PLA_01_03    57 195 4.23 0.68      4    4.31 0.00   2   5     3 -0.81
> 1.18
> PLA_05_01    58 195 4.06 0.77      4    4.13 0.00   1   5     4 -0.89
> 1.29
> PLA_07_02    59 195 2.94 1.19      3    2.94 1.48   1   5     4  0.00
>  -1.02
> PLA_10_01    60 195 4.03 0.76      4    4.08 0.00   1   5     4 -0.68
> 0.87
> PLA_12_02    61 195 2.67 1.11      2    2.62 1.48   1   5     4  0.41
>  -0.61
> PLA_18_01    62 195 4.01 0.85      4    4.09 1.48   1   5     4 -0.82
> 0.78
> PR_06_02     63 195 3.02 1.27      3    3.02 1.48   1   5     4 -0.01
>  -1.13
> PR_15_03     64 195 3.55 1.07      4    3.62 1.48   1   5     4 -0.46
>  -0.22
> PR_25_01     65 195 2.36 1.04      2    2.27 1.48   1   5     4  0.73
> 0.06
> PR_25_06     66 195 2.95 1.17      3    2.94 1.48   1   5     4  0.04
>  -0.86
> REL_09_05    67 195 3.81 0.95      4    3.89 1.48   1   5     4 -0.51
>  -0.31
> REL_14_03    68 195 3.99 0.88      4    4.08 1.48   1   5     4 -0.75
> 0.39
> REL_14_06    69 195 2.93 1.26      3    2.92 1.48   1   5     4  0.06
>  -1.11
> REL_16_04    70 195 3.16 1.27      3    3.20 1.48   1   5     4 -0.13
>  -1.11
> RS_02_03     71 195 4.14 0.75      4    4.22 0.00   1   5     4 -0.82
> 1.14
> RS_07_05     72 195 4.29 0.67      4    4.38 0.00   2   5     3 -0.72
> 0.59
> RS_08_05     73 195 4.04 0.88      4    4.13 1.48   1   5     4 -0.97
> 1.26
> RS_13_03     74 195 4.19 0.69      4    4.25 0.00   2   5     3 -0.46
>  -0.17
> TF_03_01     75 195 4.01 0.82      4    4.06 1.48   1   5     4 -0.63
> 0.32
> TF_04_01     76 195 4.09 0.76      4    4.15 0.00   1   5     4 -0.70
> 0.76
> TF_10_03     77 195 4.11 0.85      4    4.21 1.48   1   5     4 -0.96
> 0.99
> TF_12_01     78 195 4.11 0.85      4    4.21 1.48   1   5     4 -1.10
> 1.66
> TRE_09_05    79 195 4.29 0.79      4    4.39 1.48   1   5     4 -1.12
> 1.74
> TRE_09_06    80 195 4.33 0.69      4    4.42 1.48   1   5     4 -1.10
> 2.36
> TRE_26_04    81 195 2.97 1.20      3    2.96 1.48   1   5     4  0.08
>  -1.01
> TRE_26_05    82 195 3.99 0.84      4    4.03 1.48   1   5     4 -0.41
>  -0.37
>
> ```
>
> Until now, I have charged the libraries, import the my own database and did
> some simple descriptive statistics.
>
> ```
>
> > r9 <- my.data
> > omega(r9)
> Omega
> Call: omega(m = r9)
> Alpha:                 0.95
> G.6:                   0.98
> Omega Hierarchical:    0.85
> Omega H asymptotic:    0.89
> Omega Total            0.96
>
> Schmid Leiman Factor loadings greater than  0.2
>                 g   F1*   F2*   F3*   h2   u2   p2
> AUT_10_04    0.43              0.30 0.27 0.73 0.68
> AUN_07_01                           0.05 0.95 0.53
> AUN_07_02                           0.06 0.94 0.26
> AUN_09_01    0.38              0.30 0.24 0.76 0.59
> AUN_10_01    0.35              0.55 0.44 0.56 0.29
> AUT_11_01    0.42              0.30 0.27 0.73 0.66
> AUT_17_01    0.32              0.40 0.28 0.72 0.37
> AUT_20_03    0.41              0.25 0.24 0.76 0.73
> CRE_05_02-   0.24       -0.53       0.34 0.66 0.17
> CRE_07_04-   0.37       -0.51       0.39 0.61 0.35
> CRE_10_01    0.46              0.48 0.46 0.54 0.47
> CRE_16_02-              -0.70       0.48 0.52 0.01
> EFEC_03_07   0.46              0.31 0.31 0.69 0.68
> EFEC_05      0.43              0.32 0.29 0.71 0.64
> EFEC_09_02-  0.29       -0.46       0.29 0.71 0.28
> EFEC_16_03   0.49              0.26 0.31 0.69 0.77
> EVA_02_01    0.55              0.21 0.36 0.64 0.85
> EVA_07_01    0.57                   0.37 0.63 0.89
> EVA_12_02-              -0.61       0.39 0.61 0.06
> EVA_15_06    0.50              0.37 0.39 0.61 0.65
> FLX_04_01    0.57              0.30 0.42 0.58 0.78
> FLX_04_05    0.52              0.26 0.34 0.66 0.80
> FLX_08_02-              -0.78       0.60 0.40 0.00
> FLX_10_03    0.39              0.29 0.24 0.76 0.63
> IDO_01_06-              -0.80       0.64 0.36 0.00
> IDO_05_02-              -0.78       0.62 0.38 0.00
> IDO_09_03    0.41              0.49 0.42 0.58 0.40
> IDO_17_01    0.51              0.51 0.54 0.46 0.49
> IE_01_03     0.44              0.60 0.56 0.44 0.35
> IE_10_03     0.41              0.53 0.44 0.56 0.37
> IE_13_03     0.39              0.48 0.38 0.62 0.40
> IE_15_01     0.39              0.40 0.31 0.69 0.49
> LC_07_03     0.50                   0.27 0.73 0.91
> LC_08_02                 0.83       0.69 0.31 0.00
> LC_11_03     0.25                   0.10 0.90 0.60
> LC_11_05     0.45        0.24       0.27 0.73 0.75
> ME_02_03     0.55                   0.31 0.69 0.99
> ME_07_06                 0.85       0.75 0.25 0.02
> ME_09_01     0.64                   0.45 0.55 0.93
> ME_09_06                 0.81       0.69 0.31 0.02
> NEG_01_03    0.58              0.20 0.38 0.62 0.88
> NEG_05_04    0.70                   0.50 0.50 0.98
> NEG_07_03    0.64                   0.43 0.57 0.96
> NEG_08_01    0.43              0.25 0.25 0.75 0.74
> OP_03_05     0.62                   0.40 0.60 0.98
> OP_12_01     0.67                   0.46 0.54 0.98
> OP_14_01     0.60                   0.38 0.62 0.95
> OP_14_02     0.66                   0.47 0.53 0.93
> ORL_01_03    0.67                   0.47 0.53 0.96
> ORL_03_01    0.66                   0.48 0.52 0.91
> ORL_03_05    0.64                   0.46 0.54 0.90
> ORL_10_05    0.66                   0.49 0.51 0.89
> PER_08_02    0.21        0.84       0.75 0.25 0.06
> PER_16_01    0.68              0.21 0.50 0.50 0.91
> PER_19_06    0.20        0.73       0.58 0.42 0.07
> PER_22_06    0.53                   0.30 0.70 0.94
> PLA_01_03    0.57                   0.36 0.64 0.89
> PLA_05_01    0.61                   0.42 0.58 0.89
> PLA_07_02                0.75       0.61 0.39 0.04
> PLA_10_01    0.56                   0.36 0.64 0.88
> PLA_12_02                0.61       0.37 0.63 0.00
> PLA_18_01    0.63                   0.47 0.53 0.85
> PR_06_02                 0.77       0.62 0.38 0.03
> PR_15_03     0.31       -0.39  0.24 0.31 0.69 0.31
> PR_25_01-               -0.56       0.32 0.68 0.00
> PR_25_06                 0.74       0.55 0.45 0.01
> REL_09_05    0.41       -0.23  0.38 0.37 0.63 0.45
> REL_14_03    0.41       -0.21  0.29 0.30 0.70 0.56
> REL_14_06                0.66  0.21 0.48 0.52 0.04
> REL_16_04                0.78       0.63 0.37 0.03
> RS_02_03     0.57                   0.36 0.64 0.90
> RS_07_05     0.68                   0.47 0.53 0.99
> RS_08_05     0.44                   0.20 0.80 0.95
> RS_13_03     0.67                   0.46 0.54 0.97
> TF_03_01     0.66                   0.44 0.56 0.98
> TF_04_01     0.74                   0.56 0.44 0.98
> TF_10_03     0.70                   0.50 0.50 0.98
> TF_12_01     0.61                   0.40 0.60 0.92
> TRE_09_05    0.70              0.23 0.55 0.45 0.89
> TRE_09_06    0.62                   0.41 0.59 0.93
> TRE_26_04-              -0.68       0.47 0.53 0.00
> TRE_26_05    0.55       -0.21       0.34 0.66 0.88
>
> With eigenvalues of:
>     g   F1*   F2*   F3*
> 18.06  0.04 11.47  4.32
>
> general/max  1.57   max/min =   267.1
> mean percent general =  0.58    with sd =  0.36 and cv of  0.63
> Explained Common Variance of the general factor =  0.53
>
> The degrees of freedom are 3078  and the fit is  34.62
> The number of observations was  195  with Chi Square =  5671.12  with prob
> <  2.8e-157
> The root mean square of the residuals is  0.06
> The df corrected root mean square of the residuals is  0.06
> RMSEA index =  0.078  and the 10 % confidence intervals are  0.063 NA
> BIC =  -10559.18
>
> Compare this with the adequacy of just a general factor and no group
> factors
> The degrees of freedom for just the general factor are 3239  and the fit is
>  51.52
> The number of observations was  195  with Chi Square =  8509.84  with prob
> <  0
> The root mean square of the residuals is  0.16
> The df corrected root mean square of the residuals is  0.16
>
> RMSEA index =  0.104  and the 10 % confidence intervals are  0.089 NA
> BIC =  -8569.4
>
> Measures of factor score adequacy
>                                                  g   F1*  F2*  F3*
> Correlation of scores with factors            0.98  0.07 0.98 0.91
> Multiple R square of scores with factors      0.95  0.00 0.97 0.83
> Minimum correlation of factor score estimates 0.91 -0.99 0.94 0.66
>
>  Total, General and Subset omega for each subset
>                                                  g F1*  F2*  F3*
> Omega total for total scores and subscales    0.96  NA 0.83 0.95
> Omega general for total scores and subscales  0.85  NA 0.82 0.76
> Omega group for total scores and subscales    0.09  NA 0.01 0.19
> ```
>
> Now, until here, I apply the basic (non hierarchical) omega() function to
> my own database
>
>
> ```
> > omegaSem(r9,n.obs=198)
> Error in parse(text = x, keep.source = FALSE) :
>   <text>:2:0: unexpected end of input
> 1: ~
> ```
> The previous is the error message that appears after trying to use the
> omegaSem() function directly with my own database.
>
> Now, following, I present the expected output of omegaSem() applied
> directly using the Thurstone database. It's similar to the output of the
> basic omega() function but it has certain distinctions:
>
> ```
>
> > r9 <- Thurstone
> > omegaSem(r9,n.obs=500)
>
> Call: omegaSem(m = r9, n.obs = 500)
> Omega
> Call: omega(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip,
>     digits = digits, title = title, sl = sl, labels = labels,
>     plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option =
> option)
> Alpha:                 0.89
> G.6:                   0.91
> Omega Hierarchical:    0.74
> Omega H asymptotic:    0.79
> Omega Total            0.93
>
> Schmid Leiman Factor loadings greater than  0.2
>                      g   F1*   F2*   F3*   h2   u2   p2
> Sentences         0.71  0.56             0.82 0.18 0.61
> Vocabulary        0.73  0.55             0.84 0.16 0.63
> Sent.Completion   0.68  0.52             0.74 0.26 0.63
> First.Letters     0.65        0.56       0.73 0.27 0.57
> Four.Letter.Words 0.62        0.49       0.63 0.37 0.61
> Suffixes          0.56        0.41       0.50 0.50 0.63
> Letter.Series     0.59              0.62 0.73 0.27 0.48
> Pedigrees         0.58  0.24        0.34 0.51 0.49 0.66
> Letter.Group      0.54              0.46 0.52 0.48 0.56
>
> With eigenvalues of:
>    g  F1*  F2*  F3*
> 3.58 0.96 0.74 0.72
>
> general/max  3.73   max/min =   1.34
> mean percent general =  0.6    with sd =  0.05 and cv of  0.09
> Explained Common Variance of the general factor =  0.6
>
> The degrees of freedom are 12  and the fit is  0.01
> The number of observations was  500  with Chi Square =  7.12  with prob <
>  0.85
> The root mean square of the residuals is  0.01
> The df corrected root mean square of the residuals is  0.01
> RMSEA index =  0  and the 10 % confidence intervals are  0 0.026
> BIC =  -67.45
>
> Compare this with the adequacy of just a general factor and no group
> factors
> The degrees of freedom for just the general factor are 27  and the fit is
>  1.48
> The number of observations was  500  with Chi Square =  730.93  with prob <
>  1.3e-136
> The root mean square of the residuals is  0.14
> The df corrected root mean square of the residuals is  0.16
>
> RMSEA index =  0.23  and the 10 % confidence intervals are  0.214 0.243
> BIC =  563.14
>
> Measures of factor score adequacy
>                                                  g  F1*  F2*  F3*
> Correlation of scores with factors            0.86 0.73 0.72 0.75
> Multiple R square of scores with factors      0.74 0.54 0.51 0.57
> Minimum correlation of factor score estimates 0.49 0.07 0.03 0.13
>
>  Total, General and Subset omega for each subset
>                                                  g  F1*  F2*  F3*
> Omega total for total scores and subscales    0.93 0.92 0.83 0.79
> Omega general for total scores and subscales  0.74 0.58 0.50 0.47
> Omega group for total scores and subscales    0.16 0.34 0.32 0.32
>
>  The following analyses were done using the  lavaan  package
>
>  Omega Hierarchical from a confirmatory model using sem =  0.79
>  Omega Total  from a confirmatory model using sem =  0.93
> With loadings of
>                      g  F1*  F2*  F3*   h2   u2   p2
> Sentences         0.77 0.49           0.83 0.17 0.71
> Vocabulary        0.79 0.45           0.83 0.17 0.75
> Sent.Completion   0.75 0.40           0.73 0.27 0.77
> First.Letters     0.61      0.61      0.75 0.25 0.50
> Four.Letter.Words 0.60      0.51      0.61 0.39 0.59
> Suffixes          0.57      0.39      0.48 0.52 0.68
> Letter.Series     0.57           0.73 0.85 0.15 0.38
> Pedigrees         0.66           0.25 0.50 0.50 0.87
> Letter.Group      0.53           0.41 0.45 0.55 0.62
>
> With eigenvalues of:
>    g  F1*  F2*  F3*
> 3.87 0.60 0.79 0.76
>
> The degrees of freedom of the confimatory model are  18  and the fit is
>  57.11391  with p =  5.936744e-06
> general/max  4.92   max/min =   1.3
> mean percent general =  0.65    with sd =  0.15 and cv of  0.23
> Explained Common Variance of the general factor =  0.64
>
> Measures of factor score adequacy
>                                                  g   F1*  F2*  F3*
> Correlation of scores with factors            0.90  0.68 0.80 0.85
> Multiple R square of scores with factors      0.81  0.46 0.64 0.73
> Minimum correlation of factor score estimates 0.62 -0.08 0.27 0.45
>
>  Total, General and Subset omega for each subset
>                                                  g  F1*  F2*  F3*
> Omega total for total scores and subscales    0.93 0.92 0.82 0.80
> Omega general for total scores and subscales  0.79 0.69 0.48 0.50
> Omega group for total scores and subscales    0.14 0.23 0.35 0.31
>
> To get the standard sem fit statistics, ask for summary on the fitted
> object>
> ```
>
>
>
> I'm expecting to have the same output applying the function directly. My
> expectation is to make sure if its mandatory to make the schmid
> transformation before the omegaSem(). I'm supposing that not, because its
> not supposed to work like that as it says in the guide. Maybe this can be
> solved correcting the error message:
>
> ```
> > r9 <- my.data
> > omegaSem(r9,n.obs=198)
> Error in parse(text = x, keep.source = FALSE) :
>   <text>:2:0: unexpected end of input
> 1: ~
>    ^
> ```
>  Hope I've been clear enough. Feel free to ask any other information that
> you might need.
>
> Thank you so much for giving me any guidance to reach the answer of this
> issue. I higly appreciate any help.
>
> Regards,
>
> Danilo
>
> --
> Danilo E. Rodr?guez Zapata
> Analista en Psicometr?a
> CEBIAC
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From wdun|@p @end|ng |rom t|bco@com  Thu Aug 29 21:14:36 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Thu, 29 Aug 2019 12:14:36 -0700
Subject: [R] R code: How to correct "Error in parse(text = x,
 keep.source = FALSE)" output in psych package using own dataset
In-Reply-To: <CAMMrtmbJ2L1Ox1B6gTcoc_HMHDTJ4pgqJ2sxFzWfkqCaWxfvQg@mail.gmail.com>
References: <CAMMrtmZg1HryjK134A8xH73Ba=Pmkbmj5JOLLGbdSYib1PE3qA@mail.gmail.com>
 <CAF8bMcZAajdt0PssTK2xN24fmt3HzEKSvVwUMnW_ZnB30maf=g@mail.gmail.com>
 <CAMMrtmbJ2L1Ox1B6gTcoc_HMHDTJ4pgqJ2sxFzWfkqCaWxfvQg@mail.gmail.com>
Message-ID: <CAF8bMcZup9Kthc79G4A8K+-85KCqwCOj6BzsyEerHyQ0oMJtHQ@mail.gmail.com>

Please use 'reply to all' for responses to R-help reponses.

What do you get with your original data for
   psych::omega(my.data)$model$lavaan
?  Any entries like "F3=~"?

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Thu, Aug 29, 2019 at 12:05 PM Danilo Esteban Rodriguez Zapata <
danilo_rodriguez at cun.edu.co> wrote:

> Dear William,
>
> Thank you for your answer, I would like to add some information that I
> just obtained looking in different sites and forums. Someone there ask me
> to create a fake data file, so I did that from my original data file. What
> I did was open the .csv file with notepad and replace all the 4 for 5 and
> the 2 for 1, then I saved the file again with no other changes. I also
> searched for the "~" in the file and I found nothing.  Now with that file I
> did the omegaSem() function and it worked succesfully, so the weird thing
> here is that the omegaSem() function works with the fake data file, wich is
> exactly the same as the original file, but recoding some answers as I said.
>
> It seems to be an issue with the file. When I replace, lets say, the 5 for
> 6 and make the omegaSem() again, it works. Then I replace back again the 6
> for 5 in all the data and the function doesn't works anymore.
>
> El jue., 29 ago. 2019 a las 12:33, William Dunlap (<wdunlap at tibco.com>)
> escribi?:
>
>>     > omegaSem(r9,n.obs=198)
>>     Error in parse(text = x, keep.source = FALSE) :
>>       <text>:2:0: unexpected end of input
>>
>> This error probably comes from calling factor("~") and
>> psych::omegaSem(data) will do that if  all the columns in data are very
>> highly correlated with one another.   In that case omega(data, nfactor=n)
>> will not be able to find n factors in the data but it returns "~" in place
>> of the factors that it could not find.  E.g.,
>> > fakeData <- data.frame(A=1/(1:40), B=1/(2:41), C=1/(3:42), D=1/(4:43),
>> E=1/(5:44))
>> > cor(fakeData)
>>           A         B         C         D         E
>> A 1.0000000 0.9782320 0.9481293 0.9215071 0.8988962
>> B 0.9782320 1.0000000 0.9932037 0.9811287 0.9684658
>> C 0.9481293 0.9932037 1.0000000 0.9969157 0.9906838
>> D 0.9215071 0.9811287 0.9969157 1.0000000 0.9983014
>> E 0.8988962 0.9684658 0.9906838 0.9983014 1.0000000
>> > psych::omegaSem(fakeData)
>> Loading required namespace: lavaan
>> Loading required namespace: GPArotation
>> In factor.stats, I could not find the RMSEA upper bound . Sorry about that
>> Error in parse(text = x, keep.source = FALSE) :
>>   <text>:2:0: unexpected end of input
>> 1: ~
>>    ^
>> In addition: Warning message:
>> In cov2cor(t(w) %*% r %*% w) :
>>   diag(.) had 0 or NA entries; non-finite result is doubtful
>> > psych::omega(fakeData)$model$lavaan
>> In factor.stats, I could not find the RMSEA upper bound . Sorry about that
>> [1] g =~ +A+B+C+D+E       F1=~  + B + C + D + E F2=~  + A
>> [4] F3=~
>> Warning message:
>> In cov2cor(t(w) %*% r %*% w) :
>>   diag(.) had 0 or NA entries; non-finite result is doubtful
>>
>> You can get a result if you use nfactors=n where n is the number of the
>> good F<n> entries in psych::omega()$model$lavaan:
>> > psych::omegaSem(fakeData, nfactors=2)
>> ...
>>
>> Measures of factor score adequacy
>>                                                    g    F1*      F2*
>> Correlation of scores with factors             11.35  12.42    84.45
>> Multiple R square of scores with factors      128.93 154.32  7131.98
>> Minimum correlation of factor score estimates 256.86 307.64 14262.96
>> ...
>> Does that work with your data?
>>
>> This is a problem that the maintainer of psych,
>> >   maintainer("psych")
>> [1] "William Revelle <revelle at northwestern.edu>"
>> would like to know about.
>>
>>
>>
>>
>>
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>>
>> On Thu, Aug 29, 2019 at 9:03 AM Danilo Esteban Rodriguez Zapata via
>> R-help <r-help at r-project.org> wrote:
>>
>>> This is a problem related to my last question referred to the omegaSem()
>>> function in the psych package (that is already solved because I realized
>>> that I was missing a variable assignment and because of that I had an
>>> 'object not found' error:
>>>
>>>
>>> https://stackoverflow.com/questions/57661750/one-of-the-omegasem-function-arguments-is-an-object-not-found
>>>
>>> I was trying to use that function following the guide to find McDonald's
>>> hierarchical Omega by Dr William Revelle:
>>>
>>> http://personality-project.org/r/psych/HowTo/omega.pdf
>>>
>>> So now, with the variable error corrected, I'm having a different error
>>> that does not occur when I use the same function with the example
>>> database
>>> (Thurstone) provided in the tutorial that comes with the psych package. I
>>> mean, I'm able to use the function succesfully using the Thurstone data
>>> (with no other action, I have the expected result) but the function
>>> doesn't
>>> work when I use my own data.
>>>
>>> I searched over other posted questions, and the actions that they perform
>>> are not even similar to what I'm trying to do. I have almost two weeks
>>> using R, so I'm not able to identify yet how can I extrapolate the
>>> solutions for that error message to my procedure (because it seems to be
>>> frequent), although I have basic code knowledge. However related
>>> questions
>>> give no anwer by now.
>>>
>>> Additionally, I decided to look over more documentation about the
>>> package,
>>> and when I was testing other functions, I was able to use the omegaSem()
>>> function with another example database, BUT after and only after I did
>>> the
>>> schmid transformation. So with that, I discovered that when I tried to
>>> use
>>> the omegaSem() function before the schmid tranformation I had the same
>>> error message, but not after that tranformation with this second example
>>> database.
>>>
>>> This make sense with the actual procedure of the omegaSem() procedure,
>>> but
>>> I'm suposing that it must be done completely and automatically by the
>>> omegaSem() function as it is explained in the guide and I have understood
>>> until now, as it follows:
>>>
>>> 1. omegaSem() applies factor analysis
>>> 2. omegaSem() rotate factors obliquely
>>> 3. omegaSem() transform data with Schmid Leiman (schmid)
>>>
>>> -------necessary steps to print output-------------------
>>>
>>> 4. omegaSem() print McDonald's hierarchical Omega
>>>
>>> So here, another questions appears:  - Why the omegaSem() function works
>>> with the Thurstone database without any other action and only works for
>>> the
>>> second example database after performing the schmid transformation? -
>>> Why
>>> with other databases I dont have the same output applying the omegaSem()
>>> function directly? - How is this related to the error message that the
>>> compiler shows when I try to apply the function directly to the database?
>>>
>>>
>>> This is the code that I'm using now: (example of the succesfull
>>> omegaSem()
>>> done after schmid tranformation not included)
>>>
>>> ```
>>> > library(psych)
>>> > library(ctv, lavaan)
>>> > library(GPArotation)
>>> > my.data <- read.file()
>>> Data from the .csv file
>>> D:\Users\Admon\Documents\prueba_export_1563806208742.csv has been loaded.
>>> > describe(my.data)
>>>            vars   n mean   sd median trimmed  mad min max range  skew
>>> kurtosis
>>> AUT_10_04     1 195 4.11 0.90      4    4.23 1.48   1   5     4 -0.92
>>> 0.33
>>> AUN_07_01     2 195 3.79 1.14      4    3.90 1.48   1   5     4 -0.59
>>>  -0.71
>>> AUN_07_02     3 195 3.58 1.08      4    3.65 1.48   1   5     4 -0.39
>>>  -0.56
>>> AUN_09_01     4 195 4.15 0.80      4    4.23 1.48   1   5     4 -0.76
>>> 0.51
>>> AUN_10_01     5 195 4.25 0.79      4    4.34 1.48   1   5     4 -0.91
>>> 0.74
>>> AUT_11_01     6 195 4.43 0.77      5    4.56 0.00   1   5     4 -1.69
>>> 3.77
>>> AUT_17_01     7 195 4.46 0.67      5    4.55 0.00   1   5     4 -1.34
>>> 2.96
>>> AUT_20_03     8 195 4.44 0.65      5    4.53 0.00   2   5     3 -0.84
>>> 0.12
>>> CRE_05_02     9 195 2.47 1.01      2    2.43 1.48   1   5     4  0.35
>>>  -0.46
>>> CRE_07_04    10 195 2.42 1.08      2    2.34 1.48   1   5     4  0.51
>>>  -0.43
>>> CRE_10_01    11 195 4.41 0.68      5    4.51 0.00   2   5     3 -0.79
>>>  -0.12
>>> CRE_16_02    12 195 2.75 1.23      3    2.69 1.48   1   5     4  0.29
>>>  -0.96
>>> EFEC_03_07   13 195 4.35 0.69      4    4.45 1.48   1   5     4 -0.95
>>> 1.59
>>> EFEC_05      14 195 4.53 0.59      5    4.60 0.00   3   5     2 -0.82
>>>  -0.34
>>> EFEC_09_02   15 195 2.19 0.91      2    2.11 1.48   1   5     4  0.57
>>>  -0.03
>>> EFEC_16_03   16 195 4.21 0.77      4    4.29 1.48   2   5     3 -0.71
>>>  -0.04
>>> EVA_02_01    17 195 4.47 0.61      5    4.54 0.00   3   5     2 -0.70
>>>  -0.50
>>> EVA_07_01    18 195 4.38 0.60      4    4.43 1.48   3   5     2 -0.40
>>>  -0.70
>>> EVA_12_02    19 195 2.64 1.22      2    2.59 1.48   1   5     4  0.30
>>>  -1.00
>>> EVA_15_06    20 195 4.19 0.74      4    4.26 1.48   2   5     3 -0.55
>>>  -0.29
>>> FLX_04_01    21 195 4.32 0.69      4    4.41 1.48   2   5     3 -0.71
>>> 0.05
>>> FLX_04_05    22 195 4.23 0.74      4    4.32 0.00   1   5     4 -0.99
>>> 1.69
>>> FLX_08_02    23 195 2.87 1.19      3    2.86 1.48   1   5     4  0.07
>>>  -1.05
>>> FLX_10_03    24 195 4.30 0.71      4    4.39 1.48   2   5     3 -0.84
>>> 0.66
>>> IDO_01_06    25 195 3.10 1.26      3    3.13 1.48   1   5     4 -0.19
>>>  -1.08
>>> IDO_05_02    26 195 2.89 1.26      3    2.87 1.48   1   5     4 -0.03
>>>  -1.16
>>> IDO_09_03    27 195 3.87 0.97      4    3.99 1.48   1   5     4 -0.84
>>> 0.47
>>> IDO_17_01    28 195 3.94 0.88      4    4.02 0.00   1   5     4 -0.93
>>> 1.23
>>> IE_01_03     29 195 4.01 0.88      4    4.10 1.48   1   5     4 -0.91
>>> 0.94
>>> IE_10_03     30 195 4.15 1.00      4    4.34 1.48   1   5     4 -1.31
>>> 1.28
>>> IE_13_03     31 195 4.16 0.91      4    4.30 1.48   1   5     4 -1.26
>>> 1.74
>>> IE_15_01     32 195 4.26 0.85      4    4.39 1.48   1   5     4 -1.16
>>> 1.08
>>> LC_07_03     33 195 4.25 0.72      4    4.34 0.00   1   5     4 -1.07
>>> 2.64
>>> LC_08_02     34 195 3.25 1.22      4    3.31 1.48   1   5     4 -0.41
>>>  -0.90
>>> LC_11_03     35 195 3.50 1.14      4    3.56 1.48   1   5     4 -0.38
>>>  -0.68
>>> LC_11_05     36 195 4.42 0.69      5    4.52 0.00   1   5     4 -1.14
>>> 1.97
>>> ME_02_03     37 195 4.11 0.92      4    4.25 1.48   1   5     4 -1.18
>>> 1.29
>>> ME_07_06     38 195 3.19 1.28      3    3.24 1.48   1   5     4 -0.28
>>>  -1.03
>>> ME_09_01     39 195 4.24 0.77      4    4.34 1.48   1   5     4 -1.12
>>> 2.19
>>> ME_09_06     40 195 3.23 1.33      4    3.29 1.48   1   5     4 -0.31
>>>  -1.14
>>> NEG_01_03    41 195 4.18 0.76      4    4.27 0.00   1   5     4 -1.28
>>> 3.33
>>> NEG_05_04    42 195 4.27 0.69      4    4.35 0.00   1   5     4 -0.87
>>> 1.75
>>> NEG_07_03    43 195 4.32 0.73      4    4.43 1.48   1   5     4 -1.05
>>> 1.55
>>> NEG_08_01    44 195 3.95 0.88      4    4.02 1.48   1   5     4 -0.67
>>> 0.29
>>> OP_03_05     45 195 4.32 0.66      4    4.39 0.00   1   5     4 -0.99
>>> 2.54
>>> OP_12_01     46 195 4.16 0.80      4    4.25 1.48   1   5     4 -1.02
>>> 1.57
>>> OP_14_01     47 195 4.27 0.78      4    4.38 1.48   1   5     4 -1.15
>>> 1.67
>>> OP_14_02     48 195 4.36 0.68      4    4.44 1.48   1   5     4 -1.07
>>> 2.35
>>> ORL_01_03    49 195 4.36 0.77      4    4.49 1.48   1   5     4 -1.31
>>> 2.08
>>> ORL_03_01    50 195 4.41 0.69      4    4.50 1.48   1   5     4 -1.28
>>> 2.77
>>> ORL_03_05    51 195 4.36 0.74      4    4.48 1.48   2   5     3 -1.13
>>> 1.28
>>> ORL_10_05    52 195 4.40 0.68      4    4.48 1.48   1   5     4 -1.18
>>> 2.57
>>> PER_08_02    53 195 3.23 1.29      4    3.29 1.48   1   5     4 -0.26
>>>  -1.17
>>> PER_16_01    54 195 4.29 0.70      4    4.38 1.48   2   5     3 -0.74
>>> 0.27
>>> PER_19_06    55 195 3.19 1.25      3    3.24 1.48   1   5     4 -0.20
>>>  -1.06
>>> PER_22_06    56 195 4.21 0.73      4    4.29 0.00   1   5     4 -0.89
>>> 1.46
>>> PLA_01_03    57 195 4.23 0.68      4    4.31 0.00   2   5     3 -0.81
>>> 1.18
>>> PLA_05_01    58 195 4.06 0.77      4    4.13 0.00   1   5     4 -0.89
>>> 1.29
>>> PLA_07_02    59 195 2.94 1.19      3    2.94 1.48   1   5     4  0.00
>>>  -1.02
>>> PLA_10_01    60 195 4.03 0.76      4    4.08 0.00   1   5     4 -0.68
>>> 0.87
>>> PLA_12_02    61 195 2.67 1.11      2    2.62 1.48   1   5     4  0.41
>>>  -0.61
>>> PLA_18_01    62 195 4.01 0.85      4    4.09 1.48   1   5     4 -0.82
>>> 0.78
>>> PR_06_02     63 195 3.02 1.27      3    3.02 1.48   1   5     4 -0.01
>>>  -1.13
>>> PR_15_03     64 195 3.55 1.07      4    3.62 1.48   1   5     4 -0.46
>>>  -0.22
>>> PR_25_01     65 195 2.36 1.04      2    2.27 1.48   1   5     4  0.73
>>> 0.06
>>> PR_25_06     66 195 2.95 1.17      3    2.94 1.48   1   5     4  0.04
>>>  -0.86
>>> REL_09_05    67 195 3.81 0.95      4    3.89 1.48   1   5     4 -0.51
>>>  -0.31
>>> REL_14_03    68 195 3.99 0.88      4    4.08 1.48   1   5     4 -0.75
>>> 0.39
>>> REL_14_06    69 195 2.93 1.26      3    2.92 1.48   1   5     4  0.06
>>>  -1.11
>>> REL_16_04    70 195 3.16 1.27      3    3.20 1.48   1   5     4 -0.13
>>>  -1.11
>>> RS_02_03     71 195 4.14 0.75      4    4.22 0.00   1   5     4 -0.82
>>> 1.14
>>> RS_07_05     72 195 4.29 0.67      4    4.38 0.00   2   5     3 -0.72
>>> 0.59
>>> RS_08_05     73 195 4.04 0.88      4    4.13 1.48   1   5     4 -0.97
>>> 1.26
>>> RS_13_03     74 195 4.19 0.69      4    4.25 0.00   2   5     3 -0.46
>>>  -0.17
>>> TF_03_01     75 195 4.01 0.82      4    4.06 1.48   1   5     4 -0.63
>>> 0.32
>>> TF_04_01     76 195 4.09 0.76      4    4.15 0.00   1   5     4 -0.70
>>> 0.76
>>> TF_10_03     77 195 4.11 0.85      4    4.21 1.48   1   5     4 -0.96
>>> 0.99
>>> TF_12_01     78 195 4.11 0.85      4    4.21 1.48   1   5     4 -1.10
>>> 1.66
>>> TRE_09_05    79 195 4.29 0.79      4    4.39 1.48   1   5     4 -1.12
>>> 1.74
>>> TRE_09_06    80 195 4.33 0.69      4    4.42 1.48   1   5     4 -1.10
>>> 2.36
>>> TRE_26_04    81 195 2.97 1.20      3    2.96 1.48   1   5     4  0.08
>>>  -1.01
>>> TRE_26_05    82 195 3.99 0.84      4    4.03 1.48   1   5     4 -0.41
>>>  -0.37
>>>
>>> ```
>>>
>>> Until now, I have charged the libraries, import the my own database and
>>> did
>>> some simple descriptive statistics.
>>>
>>> ```
>>>
>>> > r9 <- my.data
>>> > omega(r9)
>>> Omega
>>> Call: omega(m = r9)
>>> Alpha:                 0.95
>>> G.6:                   0.98
>>> Omega Hierarchical:    0.85
>>> Omega H asymptotic:    0.89
>>> Omega Total            0.96
>>>
>>> Schmid Leiman Factor loadings greater than  0.2
>>>                 g   F1*   F2*   F3*   h2   u2   p2
>>> AUT_10_04    0.43              0.30 0.27 0.73 0.68
>>> AUN_07_01                           0.05 0.95 0.53
>>> AUN_07_02                           0.06 0.94 0.26
>>> AUN_09_01    0.38              0.30 0.24 0.76 0.59
>>> AUN_10_01    0.35              0.55 0.44 0.56 0.29
>>> AUT_11_01    0.42              0.30 0.27 0.73 0.66
>>> AUT_17_01    0.32              0.40 0.28 0.72 0.37
>>> AUT_20_03    0.41              0.25 0.24 0.76 0.73
>>> CRE_05_02-   0.24       -0.53       0.34 0.66 0.17
>>> CRE_07_04-   0.37       -0.51       0.39 0.61 0.35
>>> CRE_10_01    0.46              0.48 0.46 0.54 0.47
>>> CRE_16_02-              -0.70       0.48 0.52 0.01
>>> EFEC_03_07   0.46              0.31 0.31 0.69 0.68
>>> EFEC_05      0.43              0.32 0.29 0.71 0.64
>>> EFEC_09_02-  0.29       -0.46       0.29 0.71 0.28
>>> EFEC_16_03   0.49              0.26 0.31 0.69 0.77
>>> EVA_02_01    0.55              0.21 0.36 0.64 0.85
>>> EVA_07_01    0.57                   0.37 0.63 0.89
>>> EVA_12_02-              -0.61       0.39 0.61 0.06
>>> EVA_15_06    0.50              0.37 0.39 0.61 0.65
>>> FLX_04_01    0.57              0.30 0.42 0.58 0.78
>>> FLX_04_05    0.52              0.26 0.34 0.66 0.80
>>> FLX_08_02-              -0.78       0.60 0.40 0.00
>>> FLX_10_03    0.39              0.29 0.24 0.76 0.63
>>> IDO_01_06-              -0.80       0.64 0.36 0.00
>>> IDO_05_02-              -0.78       0.62 0.38 0.00
>>> IDO_09_03    0.41              0.49 0.42 0.58 0.40
>>> IDO_17_01    0.51              0.51 0.54 0.46 0.49
>>> IE_01_03     0.44              0.60 0.56 0.44 0.35
>>> IE_10_03     0.41              0.53 0.44 0.56 0.37
>>> IE_13_03     0.39              0.48 0.38 0.62 0.40
>>> IE_15_01     0.39              0.40 0.31 0.69 0.49
>>> LC_07_03     0.50                   0.27 0.73 0.91
>>> LC_08_02                 0.83       0.69 0.31 0.00
>>> LC_11_03     0.25                   0.10 0.90 0.60
>>> LC_11_05     0.45        0.24       0.27 0.73 0.75
>>> ME_02_03     0.55                   0.31 0.69 0.99
>>> ME_07_06                 0.85       0.75 0.25 0.02
>>> ME_09_01     0.64                   0.45 0.55 0.93
>>> ME_09_06                 0.81       0.69 0.31 0.02
>>> NEG_01_03    0.58              0.20 0.38 0.62 0.88
>>> NEG_05_04    0.70                   0.50 0.50 0.98
>>> NEG_07_03    0.64                   0.43 0.57 0.96
>>> NEG_08_01    0.43              0.25 0.25 0.75 0.74
>>> OP_03_05     0.62                   0.40 0.60 0.98
>>> OP_12_01     0.67                   0.46 0.54 0.98
>>> OP_14_01     0.60                   0.38 0.62 0.95
>>> OP_14_02     0.66                   0.47 0.53 0.93
>>> ORL_01_03    0.67                   0.47 0.53 0.96
>>> ORL_03_01    0.66                   0.48 0.52 0.91
>>> ORL_03_05    0.64                   0.46 0.54 0.90
>>> ORL_10_05    0.66                   0.49 0.51 0.89
>>> PER_08_02    0.21        0.84       0.75 0.25 0.06
>>> PER_16_01    0.68              0.21 0.50 0.50 0.91
>>> PER_19_06    0.20        0.73       0.58 0.42 0.07
>>> PER_22_06    0.53                   0.30 0.70 0.94
>>> PLA_01_03    0.57                   0.36 0.64 0.89
>>> PLA_05_01    0.61                   0.42 0.58 0.89
>>> PLA_07_02                0.75       0.61 0.39 0.04
>>> PLA_10_01    0.56                   0.36 0.64 0.88
>>> PLA_12_02                0.61       0.37 0.63 0.00
>>> PLA_18_01    0.63                   0.47 0.53 0.85
>>> PR_06_02                 0.77       0.62 0.38 0.03
>>> PR_15_03     0.31       -0.39  0.24 0.31 0.69 0.31
>>> PR_25_01-               -0.56       0.32 0.68 0.00
>>> PR_25_06                 0.74       0.55 0.45 0.01
>>> REL_09_05    0.41       -0.23  0.38 0.37 0.63 0.45
>>> REL_14_03    0.41       -0.21  0.29 0.30 0.70 0.56
>>> REL_14_06                0.66  0.21 0.48 0.52 0.04
>>> REL_16_04                0.78       0.63 0.37 0.03
>>> RS_02_03     0.57                   0.36 0.64 0.90
>>> RS_07_05     0.68                   0.47 0.53 0.99
>>> RS_08_05     0.44                   0.20 0.80 0.95
>>> RS_13_03     0.67                   0.46 0.54 0.97
>>> TF_03_01     0.66                   0.44 0.56 0.98
>>> TF_04_01     0.74                   0.56 0.44 0.98
>>> TF_10_03     0.70                   0.50 0.50 0.98
>>> TF_12_01     0.61                   0.40 0.60 0.92
>>> TRE_09_05    0.70              0.23 0.55 0.45 0.89
>>> TRE_09_06    0.62                   0.41 0.59 0.93
>>> TRE_26_04-              -0.68       0.47 0.53 0.00
>>> TRE_26_05    0.55       -0.21       0.34 0.66 0.88
>>>
>>> With eigenvalues of:
>>>     g   F1*   F2*   F3*
>>> 18.06  0.04 11.47  4.32
>>>
>>> general/max  1.57   max/min =   267.1
>>> mean percent general =  0.58    with sd =  0.36 and cv of  0.63
>>> Explained Common Variance of the general factor =  0.53
>>>
>>> The degrees of freedom are 3078  and the fit is  34.62
>>> The number of observations was  195  with Chi Square =  5671.12  with
>>> prob
>>> <  2.8e-157
>>> The root mean square of the residuals is  0.06
>>> The df corrected root mean square of the residuals is  0.06
>>> RMSEA index =  0.078  and the 10 % confidence intervals are  0.063 NA
>>> BIC =  -10559.18
>>>
>>> Compare this with the adequacy of just a general factor and no group
>>> factors
>>> The degrees of freedom for just the general factor are 3239  and the fit
>>> is
>>>  51.52
>>> The number of observations was  195  with Chi Square =  8509.84  with
>>> prob
>>> <  0
>>> The root mean square of the residuals is  0.16
>>> The df corrected root mean square of the residuals is  0.16
>>>
>>> RMSEA index =  0.104  and the 10 % confidence intervals are  0.089 NA
>>> BIC =  -8569.4
>>>
>>> Measures of factor score adequacy
>>>                                                  g   F1*  F2*  F3*
>>> Correlation of scores with factors            0.98  0.07 0.98 0.91
>>> Multiple R square of scores with factors      0.95  0.00 0.97 0.83
>>> Minimum correlation of factor score estimates 0.91 -0.99 0.94 0.66
>>>
>>>  Total, General and Subset omega for each subset
>>>                                                  g F1*  F2*  F3*
>>> Omega total for total scores and subscales    0.96  NA 0.83 0.95
>>> Omega general for total scores and subscales  0.85  NA 0.82 0.76
>>> Omega group for total scores and subscales    0.09  NA 0.01 0.19
>>> ```
>>>
>>> Now, until here, I apply the basic (non hierarchical) omega() function to
>>> my own database
>>>
>>>
>>> ```
>>> > omegaSem(r9,n.obs=198)
>>> Error in parse(text = x, keep.source = FALSE) :
>>>   <text>:2:0: unexpected end of input
>>> 1: ~
>>> ```
>>> The previous is the error message that appears after trying to use the
>>> omegaSem() function directly with my own database.
>>>
>>> Now, following, I present the expected output of omegaSem() applied
>>> directly using the Thurstone database. It's similar to the output of the
>>> basic omega() function but it has certain distinctions:
>>>
>>> ```
>>>
>>> > r9 <- Thurstone
>>> > omegaSem(r9,n.obs=500)
>>>
>>> Call: omegaSem(m = r9, n.obs = 500)
>>> Omega
>>> Call: omega(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip,
>>>     digits = digits, title = title, sl = sl, labels = labels,
>>>     plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option =
>>> option)
>>> Alpha:                 0.89
>>> G.6:                   0.91
>>> Omega Hierarchical:    0.74
>>> Omega H asymptotic:    0.79
>>> Omega Total            0.93
>>>
>>> Schmid Leiman Factor loadings greater than  0.2
>>>                      g   F1*   F2*   F3*   h2   u2   p2
>>> Sentences         0.71  0.56             0.82 0.18 0.61
>>> Vocabulary        0.73  0.55             0.84 0.16 0.63
>>> Sent.Completion   0.68  0.52             0.74 0.26 0.63
>>> First.Letters     0.65        0.56       0.73 0.27 0.57
>>> Four.Letter.Words 0.62        0.49       0.63 0.37 0.61
>>> Suffixes          0.56        0.41       0.50 0.50 0.63
>>> Letter.Series     0.59              0.62 0.73 0.27 0.48
>>> Pedigrees         0.58  0.24        0.34 0.51 0.49 0.66
>>> Letter.Group      0.54              0.46 0.52 0.48 0.56
>>>
>>> With eigenvalues of:
>>>    g  F1*  F2*  F3*
>>> 3.58 0.96 0.74 0.72
>>>
>>> general/max  3.73   max/min =   1.34
>>> mean percent general =  0.6    with sd =  0.05 and cv of  0.09
>>> Explained Common Variance of the general factor =  0.6
>>>
>>> The degrees of freedom are 12  and the fit is  0.01
>>> The number of observations was  500  with Chi Square =  7.12  with prob <
>>>  0.85
>>> The root mean square of the residuals is  0.01
>>> The df corrected root mean square of the residuals is  0.01
>>> RMSEA index =  0  and the 10 % confidence intervals are  0 0.026
>>> BIC =  -67.45
>>>
>>> Compare this with the adequacy of just a general factor and no group
>>> factors
>>> The degrees of freedom for just the general factor are 27  and the fit is
>>>  1.48
>>> The number of observations was  500  with Chi Square =  730.93  with
>>> prob <
>>>  1.3e-136
>>> The root mean square of the residuals is  0.14
>>> The df corrected root mean square of the residuals is  0.16
>>>
>>> RMSEA index =  0.23  and the 10 % confidence intervals are  0.214 0.243
>>> BIC =  563.14
>>>
>>> Measures of factor score adequacy
>>>                                                  g  F1*  F2*  F3*
>>> Correlation of scores with factors            0.86 0.73 0.72 0.75
>>> Multiple R square of scores with factors      0.74 0.54 0.51 0.57
>>> Minimum correlation of factor score estimates 0.49 0.07 0.03 0.13
>>>
>>>  Total, General and Subset omega for each subset
>>>                                                  g  F1*  F2*  F3*
>>> Omega total for total scores and subscales    0.93 0.92 0.83 0.79
>>> Omega general for total scores and subscales  0.74 0.58 0.50 0.47
>>> Omega group for total scores and subscales    0.16 0.34 0.32 0.32
>>>
>>>  The following analyses were done using the  lavaan  package
>>>
>>>  Omega Hierarchical from a confirmatory model using sem =  0.79
>>>  Omega Total  from a confirmatory model using sem =  0.93
>>> With loadings of
>>>                      g  F1*  F2*  F3*   h2   u2   p2
>>> Sentences         0.77 0.49           0.83 0.17 0.71
>>> Vocabulary        0.79 0.45           0.83 0.17 0.75
>>> Sent.Completion   0.75 0.40           0.73 0.27 0.77
>>> First.Letters     0.61      0.61      0.75 0.25 0.50
>>> Four.Letter.Words 0.60      0.51      0.61 0.39 0.59
>>> Suffixes          0.57      0.39      0.48 0.52 0.68
>>> Letter.Series     0.57           0.73 0.85 0.15 0.38
>>> Pedigrees         0.66           0.25 0.50 0.50 0.87
>>> Letter.Group      0.53           0.41 0.45 0.55 0.62
>>>
>>> With eigenvalues of:
>>>    g  F1*  F2*  F3*
>>> 3.87 0.60 0.79 0.76
>>>
>>> The degrees of freedom of the confimatory model are  18  and the fit is
>>>  57.11391  with p =  5.936744e-06
>>> general/max  4.92   max/min =   1.3
>>> mean percent general =  0.65    with sd =  0.15 and cv of  0.23
>>> Explained Common Variance of the general factor =  0.64
>>>
>>> Measures of factor score adequacy
>>>                                                  g   F1*  F2*  F3*
>>> Correlation of scores with factors            0.90  0.68 0.80 0.85
>>> Multiple R square of scores with factors      0.81  0.46 0.64 0.73
>>> Minimum correlation of factor score estimates 0.62 -0.08 0.27 0.45
>>>
>>>  Total, General and Subset omega for each subset
>>>                                                  g  F1*  F2*  F3*
>>> Omega total for total scores and subscales    0.93 0.92 0.82 0.80
>>> Omega general for total scores and subscales  0.79 0.69 0.48 0.50
>>> Omega group for total scores and subscales    0.14 0.23 0.35 0.31
>>>
>>> To get the standard sem fit statistics, ask for summary on the fitted
>>> object>
>>> ```
>>>
>>>
>>>
>>> I'm expecting to have the same output applying the function directly. My
>>> expectation is to make sure if its mandatory to make the schmid
>>> transformation before the omegaSem(). I'm supposing that not, because its
>>> not supposed to work like that as it says in the guide. Maybe this can be
>>> solved correcting the error message:
>>>
>>> ```
>>> > r9 <- my.data
>>> > omegaSem(r9,n.obs=198)
>>> Error in parse(text = x, keep.source = FALSE) :
>>>   <text>:2:0: unexpected end of input
>>> 1: ~
>>>    ^
>>> ```
>>>  Hope I've been clear enough. Feel free to ask any other information that
>>> you might need.
>>>
>>> Thank you so much for giving me any guidance to reach the answer of this
>>> issue. I higly appreciate any help.
>>>
>>> Regards,
>>>
>>> Danilo
>>>
>>> --
>>> Danilo E. Rodr?guez Zapata
>>> Analista en Psicometr?a
>>> CEBIAC
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>
> --
> Danilo E. Rodr?guez Zapata
> Analista en Psicometr?a
> CEBIAC
>

	[[alternative HTML version deleted]]


From wdun|@p @end|ng |rom t|bco@com  Thu Aug 29 21:53:30 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Thu, 29 Aug 2019 12:53:30 -0700
Subject: [R] R code: How to correct "Error in parse(text = x,
 keep.source = FALSE)" output in psych package using own dataset
In-Reply-To: <CAMMrtmZYW_-yA0WpQVXWh0ufcDtfJnf6BFnrYPNJ4SYBtnsY=w@mail.gmail.com>
References: <CAMMrtmZg1HryjK134A8xH73Ba=Pmkbmj5JOLLGbdSYib1PE3qA@mail.gmail.com>
 <CAF8bMcZAajdt0PssTK2xN24fmt3HzEKSvVwUMnW_ZnB30maf=g@mail.gmail.com>
 <CAMMrtmbx5G77RoF7tKqZtApO7XLfEyyu97xLmUHKfJSoR0fTpg@mail.gmail.com>
 <CAMMrtmZYW_-yA0WpQVXWh0ufcDtfJnf6BFnrYPNJ4SYBtnsY=w@mail.gmail.com>
Message-ID: <CAF8bMcZYOahUPk-a9Ar_xdLDRf0fMpzbSoXcdR9MRRDVPTgWcw@mail.gmail.com>

Element #2 of that output,  the empty fomula " F1=~  ", triggers the bug in
omegaSem.
omegaSem needs to ignore such entries in omega's output.  psych's author
should be able to fix things up.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Thu, Aug 29, 2019 at 12:31 PM Danilo Esteban Rodriguez Zapata <
danilo_rodriguez at cun.edu.co> wrote:

> well the output with the code that you refer is the following:
>
> > psych::omega(my.data)$model$lavaan
> [1] g =~
> +AUT_10_04+AUN_07_01+AUN_07_02+AUN_09_01+AUN_10_01+AUT_11_01+AUT_17_01+AUT_20_03+CRE_05_02+CRE_07_04+CRE_10_01+CRE_16_02+EFEC_03_07+EFEC_05+EFEC_09_02+EFEC_16_03+EVA_02_01+EVA_07_01+EVA_12_02+EVA_15_06+FLX_04_01+FLX_04_05+FLX_08_02+FLX_10_03+IDO_01_06+IDO_05_02+IDO_09_03+IDO_17_01+IE_01_03+IE_10_03+IE_13_03+IE_15_01+LC_07_03+LC_08_02+LC_11_03+LC_11_05+ME_02_03+ME_07_06+ME_09_01+ME_09_06+NEG_01_03+NEG_05_04+NEG_07_03+NEG_08_01+OP_03_05+OP_12_01+OP_14_01+OP_14_02+ORL_01_03+ORL_03_01+ORL_03_05+ORL_10_05+PER_08_02+PER_16_01+PER_19_06+PER_22_06+PLA_01_03+PLA_05_01+PLA_07_02+PLA_10_01+PLA_12_02+PLA_18_01+PR_06_02+PR_15_03+PR_25_01+PR_25_06+REL_09_05+REL_14_03+REL_14_06+REL_16_04+RS_02_03+RS_07_05+RS_08_05+RS_13_03+TF_03_01+TF_04_01+TF_10_03+TF_12_01+TRE_09_05+TRE_09_06+TRE_26_04+TRE_26_05
> [2] F1=~
>
>
>
>
>
>
>
>
>
>
> [3] F2=~  + AUN_07_02 + CRE_05_02 + CRE_07_04 + CRE_16_02 + EFEC_09_02 +
> EVA_12_02 + FLX_08_02 + IDO_01_06 + IDO_05_02 + LC_08_02 + LC_11_03 +
> LC_11_05 + ME_02_03 + ME_07_06 + ME_09_06 + NEG_07_03 + OP_03_05 + OP_14_01
> + OP_14_02 + ORL_01_03 + ORL_03_01 + PER_08_02 + PER_19_06 + PLA_05_01 +
> PLA_07_02 + PLA_10_01 + PLA_12_02 + PLA_18_01 + PR_06_02 + PR_15_03 +
> PR_25_01 + PR_25_06 + REL_14_06 + REL_16_04 + TF_04_01 + TF_10_03 +
> TRE_26_04 + TRE_26_05
>
>
>
>
> [4] F3=~  + AUT_10_04 + AUN_07_01 + AUN_09_01 + AUN_10_01 + AUT_11_01 +
> AUT_17_01 + AUT_20_03 + CRE_10_01 + EFEC_03_07 + EFEC_05 + EFEC_16_03 +
> EVA_02_01 + EVA_07_01 + EVA_15_06 + FLX_04_01 + FLX_04_05 + FLX_10_03 +
> IDO_09_03 + IDO_17_01 + IE_01_03 + IE_10_03 + IE_13_03 + IE_15_01 +
> LC_07_03 + ME_09_01 + NEG_01_03 + NEG_05_04 + NEG_08_01 + OP_12_01 +
> ORL_03_05 + ORL_10_05 + PER_16_01 + PER_22_06 + PLA_01_03 + REL_09_05 +
> REL_14_03 + RS_02_03 + RS_07_05 + RS_08_05 + RS_13_03 + TF_03_01 + TF_12_01
> + TRE_09_05 + TRE_09_06
>
>
>
> >
>
> El jue., 29 ago. 2019 a las 14:29, Danilo Esteban Rodriguez Zapata (<
> danilo_rodriguez at cun.edu.co>) escribi?:
>
>> Dear William,
>>
>> Thank you for your answer, I would like to add some information that I
>> just obtained looking in different sites and forums. Someone there ask me
>> to create a fake data file, so I did that from my original data file. What
>> I did was open the .csv file with notepad and replace all the 4 for 5 and
>> the 2 for 1, then I saved the file again with no other changes. I also
>> searched for the "~" in the file and I found nothing.  Now with that file I
>> did the omegaSem() function and it worked succesfully, so the weird thing
>> here is that the omegaSem() function works with the fake data file, wich is
>> exactly the same as the original file, but recoding some answers as I said.
>>
>> It seems to be an issue with the file. When I replace, lets say, the 5
>> for 6 and make the omegaSem() again, it works. Then I replace back again
>> the 6 for 5 in all the data and the function doesn't works anymore.
>>
>> El jue., 29 ago. 2019 a las 12:33, William Dunlap (<wdunlap at tibco.com>)
>> escribi?:
>>
>>>     > omegaSem(r9,n.obs=198)
>>>     Error in parse(text = x, keep.source = FALSE) :
>>>       <text>:2:0: unexpected end of input
>>>
>>> This error probably comes from calling factor("~") and
>>> psych::omegaSem(data) will do that if  all the columns in data are very
>>> highly correlated with one another.   In that case omega(data, nfactor=n)
>>> will not be able to find n factors in the data but it returns "~" in place
>>> of the factors that it could not find.  E.g.,
>>> > fakeData <- data.frame(A=1/(1:40), B=1/(2:41), C=1/(3:42), D=1/(4:43),
>>> E=1/(5:44))
>>> > cor(fakeData)
>>>           A         B         C         D         E
>>> A 1.0000000 0.9782320 0.9481293 0.9215071 0.8988962
>>> B 0.9782320 1.0000000 0.9932037 0.9811287 0.9684658
>>> C 0.9481293 0.9932037 1.0000000 0.9969157 0.9906838
>>> D 0.9215071 0.9811287 0.9969157 1.0000000 0.9983014
>>> E 0.8988962 0.9684658 0.9906838 0.9983014 1.0000000
>>> > psych::omegaSem(fakeData)
>>> Loading required namespace: lavaan
>>> Loading required namespace: GPArotation
>>> In factor.stats, I could not find the RMSEA upper bound . Sorry about
>>> that
>>> Error in parse(text = x, keep.source = FALSE) :
>>>   <text>:2:0: unexpected end of input
>>> 1: ~
>>>    ^
>>> In addition: Warning message:
>>> In cov2cor(t(w) %*% r %*% w) :
>>>   diag(.) had 0 or NA entries; non-finite result is doubtful
>>> > psych::omega(fakeData)$model$lavaan
>>> In factor.stats, I could not find the RMSEA upper bound . Sorry about
>>> that
>>> [1] g =~ +A+B+C+D+E       F1=~  + B + C + D + E F2=~  + A
>>> [4] F3=~
>>> Warning message:
>>> In cov2cor(t(w) %*% r %*% w) :
>>>   diag(.) had 0 or NA entries; non-finite result is doubtful
>>>
>>> You can get a result if you use nfactors=n where n is the number of the
>>> good F<n> entries in psych::omega()$model$lavaan:
>>> > psych::omegaSem(fakeData, nfactors=2)
>>> ...
>>>
>>> Measures of factor score adequacy
>>>                                                    g    F1*      F2*
>>> Correlation of scores with factors             11.35  12.42    84.45
>>> Multiple R square of scores with factors      128.93 154.32  7131.98
>>> Minimum correlation of factor score estimates 256.86 307.64 14262.96
>>> ...
>>> Does that work with your data?
>>>
>>> This is a problem that the maintainer of psych,
>>> >   maintainer("psych")
>>> [1] "William Revelle <revelle at northwestern.edu>"
>>> would like to know about.
>>>
>>>
>>>
>>>
>>>
>>>
>>> Bill Dunlap
>>> TIBCO Software
>>> wdunlap tibco.com
>>>
>>>
>>> On Thu, Aug 29, 2019 at 9:03 AM Danilo Esteban Rodriguez Zapata via
>>> R-help <r-help at r-project.org> wrote:
>>>
>>>> This is a problem related to my last question referred to the omegaSem()
>>>> function in the psych package (that is already solved because I realized
>>>> that I was missing a variable assignment and because of that I had an
>>>> 'object not found' error:
>>>>
>>>>
>>>> https://stackoverflow.com/questions/57661750/one-of-the-omegasem-function-arguments-is-an-object-not-found
>>>>
>>>> I was trying to use that function following the guide to find McDonald's
>>>> hierarchical Omega by Dr William Revelle:
>>>>
>>>> http://personality-project.org/r/psych/HowTo/omega.pdf
>>>>
>>>> So now, with the variable error corrected, I'm having a different error
>>>> that does not occur when I use the same function with the example
>>>> database
>>>> (Thurstone) provided in the tutorial that comes with the psych package.
>>>> I
>>>> mean, I'm able to use the function succesfully using the Thurstone data
>>>> (with no other action, I have the expected result) but the function
>>>> doesn't
>>>> work when I use my own data.
>>>>
>>>> I searched over other posted questions, and the actions that they
>>>> perform
>>>> are not even similar to what I'm trying to do. I have almost two weeks
>>>> using R, so I'm not able to identify yet how can I extrapolate the
>>>> solutions for that error message to my procedure (because it seems to be
>>>> frequent), although I have basic code knowledge. However related
>>>> questions
>>>> give no anwer by now.
>>>>
>>>> Additionally, I decided to look over more documentation about the
>>>> package,
>>>> and when I was testing other functions, I was able to use the omegaSem()
>>>> function with another example database, BUT after and only after I did
>>>> the
>>>> schmid transformation. So with that, I discovered that when I tried to
>>>> use
>>>> the omegaSem() function before the schmid tranformation I had the same
>>>> error message, but not after that tranformation with this second example
>>>> database.
>>>>
>>>> This make sense with the actual procedure of the omegaSem() procedure,
>>>> but
>>>> I'm suposing that it must be done completely and automatically by the
>>>> omegaSem() function as it is explained in the guide and I have
>>>> understood
>>>> until now, as it follows:
>>>>
>>>> 1. omegaSem() applies factor analysis
>>>> 2. omegaSem() rotate factors obliquely
>>>> 3. omegaSem() transform data with Schmid Leiman (schmid)
>>>>
>>>> -------necessary steps to print output-------------------
>>>>
>>>> 4. omegaSem() print McDonald's hierarchical Omega
>>>>
>>>> So here, another questions appears:  - Why the omegaSem() function works
>>>> with the Thurstone database without any other action and only works for
>>>> the
>>>> second example database after performing the schmid transformation? -
>>>> Why
>>>> with other databases I dont have the same output applying the omegaSem()
>>>> function directly? - How is this related to the error message that the
>>>> compiler shows when I try to apply the function directly to the
>>>> database?
>>>>
>>>>
>>>> This is the code that I'm using now: (example of the succesfull
>>>> omegaSem()
>>>> done after schmid tranformation not included)
>>>>
>>>> ```
>>>> > library(psych)
>>>> > library(ctv, lavaan)
>>>> > library(GPArotation)
>>>> > my.data <- read.file()
>>>> Data from the .csv file
>>>> D:\Users\Admon\Documents\prueba_export_1563806208742.csv has been
>>>> loaded.
>>>> > describe(my.data)
>>>>            vars   n mean   sd median trimmed  mad min max range  skew
>>>> kurtosis
>>>> AUT_10_04     1 195 4.11 0.90      4    4.23 1.48   1   5     4 -0.92
>>>> 0.33
>>>> AUN_07_01     2 195 3.79 1.14      4    3.90 1.48   1   5     4 -0.59
>>>>  -0.71
>>>> AUN_07_02     3 195 3.58 1.08      4    3.65 1.48   1   5     4 -0.39
>>>>  -0.56
>>>> AUN_09_01     4 195 4.15 0.80      4    4.23 1.48   1   5     4 -0.76
>>>> 0.51
>>>> AUN_10_01     5 195 4.25 0.79      4    4.34 1.48   1   5     4 -0.91
>>>> 0.74
>>>> AUT_11_01     6 195 4.43 0.77      5    4.56 0.00   1   5     4 -1.69
>>>> 3.77
>>>> AUT_17_01     7 195 4.46 0.67      5    4.55 0.00   1   5     4 -1.34
>>>> 2.96
>>>> AUT_20_03     8 195 4.44 0.65      5    4.53 0.00   2   5     3 -0.84
>>>> 0.12
>>>> CRE_05_02     9 195 2.47 1.01      2    2.43 1.48   1   5     4  0.35
>>>>  -0.46
>>>> CRE_07_04    10 195 2.42 1.08      2    2.34 1.48   1   5     4  0.51
>>>>  -0.43
>>>> CRE_10_01    11 195 4.41 0.68      5    4.51 0.00   2   5     3 -0.79
>>>>  -0.12
>>>> CRE_16_02    12 195 2.75 1.23      3    2.69 1.48   1   5     4  0.29
>>>>  -0.96
>>>> EFEC_03_07   13 195 4.35 0.69      4    4.45 1.48   1   5     4 -0.95
>>>> 1.59
>>>> EFEC_05      14 195 4.53 0.59      5    4.60 0.00   3   5     2 -0.82
>>>>  -0.34
>>>> EFEC_09_02   15 195 2.19 0.91      2    2.11 1.48   1   5     4  0.57
>>>>  -0.03
>>>> EFEC_16_03   16 195 4.21 0.77      4    4.29 1.48   2   5     3 -0.71
>>>>  -0.04
>>>> EVA_02_01    17 195 4.47 0.61      5    4.54 0.00   3   5     2 -0.70
>>>>  -0.50
>>>> EVA_07_01    18 195 4.38 0.60      4    4.43 1.48   3   5     2 -0.40
>>>>  -0.70
>>>> EVA_12_02    19 195 2.64 1.22      2    2.59 1.48   1   5     4  0.30
>>>>  -1.00
>>>> EVA_15_06    20 195 4.19 0.74      4    4.26 1.48   2   5     3 -0.55
>>>>  -0.29
>>>> FLX_04_01    21 195 4.32 0.69      4    4.41 1.48   2   5     3 -0.71
>>>> 0.05
>>>> FLX_04_05    22 195 4.23 0.74      4    4.32 0.00   1   5     4 -0.99
>>>> 1.69
>>>> FLX_08_02    23 195 2.87 1.19      3    2.86 1.48   1   5     4  0.07
>>>>  -1.05
>>>> FLX_10_03    24 195 4.30 0.71      4    4.39 1.48   2   5     3 -0.84
>>>> 0.66
>>>> IDO_01_06    25 195 3.10 1.26      3    3.13 1.48   1   5     4 -0.19
>>>>  -1.08
>>>> IDO_05_02    26 195 2.89 1.26      3    2.87 1.48   1   5     4 -0.03
>>>>  -1.16
>>>> IDO_09_03    27 195 3.87 0.97      4    3.99 1.48   1   5     4 -0.84
>>>> 0.47
>>>> IDO_17_01    28 195 3.94 0.88      4    4.02 0.00   1   5     4 -0.93
>>>> 1.23
>>>> IE_01_03     29 195 4.01 0.88      4    4.10 1.48   1   5     4 -0.91
>>>> 0.94
>>>> IE_10_03     30 195 4.15 1.00      4    4.34 1.48   1   5     4 -1.31
>>>> 1.28
>>>> IE_13_03     31 195 4.16 0.91      4    4.30 1.48   1   5     4 -1.26
>>>> 1.74
>>>> IE_15_01     32 195 4.26 0.85      4    4.39 1.48   1   5     4 -1.16
>>>> 1.08
>>>> LC_07_03     33 195 4.25 0.72      4    4.34 0.00   1   5     4 -1.07
>>>> 2.64
>>>> LC_08_02     34 195 3.25 1.22      4    3.31 1.48   1   5     4 -0.41
>>>>  -0.90
>>>> LC_11_03     35 195 3.50 1.14      4    3.56 1.48   1   5     4 -0.38
>>>>  -0.68
>>>> LC_11_05     36 195 4.42 0.69      5    4.52 0.00   1   5     4 -1.14
>>>> 1.97
>>>> ME_02_03     37 195 4.11 0.92      4    4.25 1.48   1   5     4 -1.18
>>>> 1.29
>>>> ME_07_06     38 195 3.19 1.28      3    3.24 1.48   1   5     4 -0.28
>>>>  -1.03
>>>> ME_09_01     39 195 4.24 0.77      4    4.34 1.48   1   5     4 -1.12
>>>> 2.19
>>>> ME_09_06     40 195 3.23 1.33      4    3.29 1.48   1   5     4 -0.31
>>>>  -1.14
>>>> NEG_01_03    41 195 4.18 0.76      4    4.27 0.00   1   5     4 -1.28
>>>> 3.33
>>>> NEG_05_04    42 195 4.27 0.69      4    4.35 0.00   1   5     4 -0.87
>>>> 1.75
>>>> NEG_07_03    43 195 4.32 0.73      4    4.43 1.48   1   5     4 -1.05
>>>> 1.55
>>>> NEG_08_01    44 195 3.95 0.88      4    4.02 1.48   1   5     4 -0.67
>>>> 0.29
>>>> OP_03_05     45 195 4.32 0.66      4    4.39 0.00   1   5     4 -0.99
>>>> 2.54
>>>> OP_12_01     46 195 4.16 0.80      4    4.25 1.48   1   5     4 -1.02
>>>> 1.57
>>>> OP_14_01     47 195 4.27 0.78      4    4.38 1.48   1   5     4 -1.15
>>>> 1.67
>>>> OP_14_02     48 195 4.36 0.68      4    4.44 1.48   1   5     4 -1.07
>>>> 2.35
>>>> ORL_01_03    49 195 4.36 0.77      4    4.49 1.48   1   5     4 -1.31
>>>> 2.08
>>>> ORL_03_01    50 195 4.41 0.69      4    4.50 1.48   1   5     4 -1.28
>>>> 2.77
>>>> ORL_03_05    51 195 4.36 0.74      4    4.48 1.48   2   5     3 -1.13
>>>> 1.28
>>>> ORL_10_05    52 195 4.40 0.68      4    4.48 1.48   1   5     4 -1.18
>>>> 2.57
>>>> PER_08_02    53 195 3.23 1.29      4    3.29 1.48   1   5     4 -0.26
>>>>  -1.17
>>>> PER_16_01    54 195 4.29 0.70      4    4.38 1.48   2   5     3 -0.74
>>>> 0.27
>>>> PER_19_06    55 195 3.19 1.25      3    3.24 1.48   1   5     4 -0.20
>>>>  -1.06
>>>> PER_22_06    56 195 4.21 0.73      4    4.29 0.00   1   5     4 -0.89
>>>> 1.46
>>>> PLA_01_03    57 195 4.23 0.68      4    4.31 0.00   2   5     3 -0.81
>>>> 1.18
>>>> PLA_05_01    58 195 4.06 0.77      4    4.13 0.00   1   5     4 -0.89
>>>> 1.29
>>>> PLA_07_02    59 195 2.94 1.19      3    2.94 1.48   1   5     4  0.00
>>>>  -1.02
>>>> PLA_10_01    60 195 4.03 0.76      4    4.08 0.00   1   5     4 -0.68
>>>> 0.87
>>>> PLA_12_02    61 195 2.67 1.11      2    2.62 1.48   1   5     4  0.41
>>>>  -0.61
>>>> PLA_18_01    62 195 4.01 0.85      4    4.09 1.48   1   5     4 -0.82
>>>> 0.78
>>>> PR_06_02     63 195 3.02 1.27      3    3.02 1.48   1   5     4 -0.01
>>>>  -1.13
>>>> PR_15_03     64 195 3.55 1.07      4    3.62 1.48   1   5     4 -0.46
>>>>  -0.22
>>>> PR_25_01     65 195 2.36 1.04      2    2.27 1.48   1   5     4  0.73
>>>> 0.06
>>>> PR_25_06     66 195 2.95 1.17      3    2.94 1.48   1   5     4  0.04
>>>>  -0.86
>>>> REL_09_05    67 195 3.81 0.95      4    3.89 1.48   1   5     4 -0.51
>>>>  -0.31
>>>> REL_14_03    68 195 3.99 0.88      4    4.08 1.48   1   5     4 -0.75
>>>> 0.39
>>>> REL_14_06    69 195 2.93 1.26      3    2.92 1.48   1   5     4  0.06
>>>>  -1.11
>>>> REL_16_04    70 195 3.16 1.27      3    3.20 1.48   1   5     4 -0.13
>>>>  -1.11
>>>> RS_02_03     71 195 4.14 0.75      4    4.22 0.00   1   5     4 -0.82
>>>> 1.14
>>>> RS_07_05     72 195 4.29 0.67      4    4.38 0.00   2   5     3 -0.72
>>>> 0.59
>>>> RS_08_05     73 195 4.04 0.88      4    4.13 1.48   1   5     4 -0.97
>>>> 1.26
>>>> RS_13_03     74 195 4.19 0.69      4    4.25 0.00   2   5     3 -0.46
>>>>  -0.17
>>>> TF_03_01     75 195 4.01 0.82      4    4.06 1.48   1   5     4 -0.63
>>>> 0.32
>>>> TF_04_01     76 195 4.09 0.76      4    4.15 0.00   1   5     4 -0.70
>>>> 0.76
>>>> TF_10_03     77 195 4.11 0.85      4    4.21 1.48   1   5     4 -0.96
>>>> 0.99
>>>> TF_12_01     78 195 4.11 0.85      4    4.21 1.48   1   5     4 -1.10
>>>> 1.66
>>>> TRE_09_05    79 195 4.29 0.79      4    4.39 1.48   1   5     4 -1.12
>>>> 1.74
>>>> TRE_09_06    80 195 4.33 0.69      4    4.42 1.48   1   5     4 -1.10
>>>> 2.36
>>>> TRE_26_04    81 195 2.97 1.20      3    2.96 1.48   1   5     4  0.08
>>>>  -1.01
>>>> TRE_26_05    82 195 3.99 0.84      4    4.03 1.48   1   5     4 -0.41
>>>>  -0.37
>>>>
>>>> ```
>>>>
>>>> Until now, I have charged the libraries, import the my own database and
>>>> did
>>>> some simple descriptive statistics.
>>>>
>>>> ```
>>>>
>>>> > r9 <- my.data
>>>> > omega(r9)
>>>> Omega
>>>> Call: omega(m = r9)
>>>> Alpha:                 0.95
>>>> G.6:                   0.98
>>>> Omega Hierarchical:    0.85
>>>> Omega H asymptotic:    0.89
>>>> Omega Total            0.96
>>>>
>>>> Schmid Leiman Factor loadings greater than  0.2
>>>>                 g   F1*   F2*   F3*   h2   u2   p2
>>>> AUT_10_04    0.43              0.30 0.27 0.73 0.68
>>>> AUN_07_01                           0.05 0.95 0.53
>>>> AUN_07_02                           0.06 0.94 0.26
>>>> AUN_09_01    0.38              0.30 0.24 0.76 0.59
>>>> AUN_10_01    0.35              0.55 0.44 0.56 0.29
>>>> AUT_11_01    0.42              0.30 0.27 0.73 0.66
>>>> AUT_17_01    0.32              0.40 0.28 0.72 0.37
>>>> AUT_20_03    0.41              0.25 0.24 0.76 0.73
>>>> CRE_05_02-   0.24       -0.53       0.34 0.66 0.17
>>>> CRE_07_04-   0.37       -0.51       0.39 0.61 0.35
>>>> CRE_10_01    0.46              0.48 0.46 0.54 0.47
>>>> CRE_16_02-              -0.70       0.48 0.52 0.01
>>>> EFEC_03_07   0.46              0.31 0.31 0.69 0.68
>>>> EFEC_05      0.43              0.32 0.29 0.71 0.64
>>>> EFEC_09_02-  0.29       -0.46       0.29 0.71 0.28
>>>> EFEC_16_03   0.49              0.26 0.31 0.69 0.77
>>>> EVA_02_01    0.55              0.21 0.36 0.64 0.85
>>>> EVA_07_01    0.57                   0.37 0.63 0.89
>>>> EVA_12_02-              -0.61       0.39 0.61 0.06
>>>> EVA_15_06    0.50              0.37 0.39 0.61 0.65
>>>> FLX_04_01    0.57              0.30 0.42 0.58 0.78
>>>> FLX_04_05    0.52              0.26 0.34 0.66 0.80
>>>> FLX_08_02-              -0.78       0.60 0.40 0.00
>>>> FLX_10_03    0.39              0.29 0.24 0.76 0.63
>>>> IDO_01_06-              -0.80       0.64 0.36 0.00
>>>> IDO_05_02-              -0.78       0.62 0.38 0.00
>>>> IDO_09_03    0.41              0.49 0.42 0.58 0.40
>>>> IDO_17_01    0.51              0.51 0.54 0.46 0.49
>>>> IE_01_03     0.44              0.60 0.56 0.44 0.35
>>>> IE_10_03     0.41              0.53 0.44 0.56 0.37
>>>> IE_13_03     0.39              0.48 0.38 0.62 0.40
>>>> IE_15_01     0.39              0.40 0.31 0.69 0.49
>>>> LC_07_03     0.50                   0.27 0.73 0.91
>>>> LC_08_02                 0.83       0.69 0.31 0.00
>>>> LC_11_03     0.25                   0.10 0.90 0.60
>>>> LC_11_05     0.45        0.24       0.27 0.73 0.75
>>>> ME_02_03     0.55                   0.31 0.69 0.99
>>>> ME_07_06                 0.85       0.75 0.25 0.02
>>>> ME_09_01     0.64                   0.45 0.55 0.93
>>>> ME_09_06                 0.81       0.69 0.31 0.02
>>>> NEG_01_03    0.58              0.20 0.38 0.62 0.88
>>>> NEG_05_04    0.70                   0.50 0.50 0.98
>>>> NEG_07_03    0.64                   0.43 0.57 0.96
>>>> NEG_08_01    0.43              0.25 0.25 0.75 0.74
>>>> OP_03_05     0.62                   0.40 0.60 0.98
>>>> OP_12_01     0.67                   0.46 0.54 0.98
>>>> OP_14_01     0.60                   0.38 0.62 0.95
>>>> OP_14_02     0.66                   0.47 0.53 0.93
>>>> ORL_01_03    0.67                   0.47 0.53 0.96
>>>> ORL_03_01    0.66                   0.48 0.52 0.91
>>>> ORL_03_05    0.64                   0.46 0.54 0.90
>>>> ORL_10_05    0.66                   0.49 0.51 0.89
>>>> PER_08_02    0.21        0.84       0.75 0.25 0.06
>>>> PER_16_01    0.68              0.21 0.50 0.50 0.91
>>>> PER_19_06    0.20        0.73       0.58 0.42 0.07
>>>> PER_22_06    0.53                   0.30 0.70 0.94
>>>> PLA_01_03    0.57                   0.36 0.64 0.89
>>>> PLA_05_01    0.61                   0.42 0.58 0.89
>>>> PLA_07_02                0.75       0.61 0.39 0.04
>>>> PLA_10_01    0.56                   0.36 0.64 0.88
>>>> PLA_12_02                0.61       0.37 0.63 0.00
>>>> PLA_18_01    0.63                   0.47 0.53 0.85
>>>> PR_06_02                 0.77       0.62 0.38 0.03
>>>> PR_15_03     0.31       -0.39  0.24 0.31 0.69 0.31
>>>> PR_25_01-               -0.56       0.32 0.68 0.00
>>>> PR_25_06                 0.74       0.55 0.45 0.01
>>>> REL_09_05    0.41       -0.23  0.38 0.37 0.63 0.45
>>>> REL_14_03    0.41       -0.21  0.29 0.30 0.70 0.56
>>>> REL_14_06                0.66  0.21 0.48 0.52 0.04
>>>> REL_16_04                0.78       0.63 0.37 0.03
>>>> RS_02_03     0.57                   0.36 0.64 0.90
>>>> RS_07_05     0.68                   0.47 0.53 0.99
>>>> RS_08_05     0.44                   0.20 0.80 0.95
>>>> RS_13_03     0.67                   0.46 0.54 0.97
>>>> TF_03_01     0.66                   0.44 0.56 0.98
>>>> TF_04_01     0.74                   0.56 0.44 0.98
>>>> TF_10_03     0.70                   0.50 0.50 0.98
>>>> TF_12_01     0.61                   0.40 0.60 0.92
>>>> TRE_09_05    0.70              0.23 0.55 0.45 0.89
>>>> TRE_09_06    0.62                   0.41 0.59 0.93
>>>> TRE_26_04-              -0.68       0.47 0.53 0.00
>>>> TRE_26_05    0.55       -0.21       0.34 0.66 0.88
>>>>
>>>> With eigenvalues of:
>>>>     g   F1*   F2*   F3*
>>>> 18.06  0.04 11.47  4.32
>>>>
>>>> general/max  1.57   max/min =   267.1
>>>> mean percent general =  0.58    with sd =  0.36 and cv of  0.63
>>>> Explained Common Variance of the general factor =  0.53
>>>>
>>>> The degrees of freedom are 3078  and the fit is  34.62
>>>> The number of observations was  195  with Chi Square =  5671.12  with
>>>> prob
>>>> <  2.8e-157
>>>> The root mean square of the residuals is  0.06
>>>> The df corrected root mean square of the residuals is  0.06
>>>> RMSEA index =  0.078  and the 10 % confidence intervals are  0.063 NA
>>>> BIC =  -10559.18
>>>>
>>>> Compare this with the adequacy of just a general factor and no group
>>>> factors
>>>> The degrees of freedom for just the general factor are 3239  and the
>>>> fit is
>>>>  51.52
>>>> The number of observations was  195  with Chi Square =  8509.84  with
>>>> prob
>>>> <  0
>>>> The root mean square of the residuals is  0.16
>>>> The df corrected root mean square of the residuals is  0.16
>>>>
>>>> RMSEA index =  0.104  and the 10 % confidence intervals are  0.089 NA
>>>> BIC =  -8569.4
>>>>
>>>> Measures of factor score adequacy
>>>>                                                  g   F1*  F2*  F3*
>>>> Correlation of scores with factors            0.98  0.07 0.98 0.91
>>>> Multiple R square of scores with factors      0.95  0.00 0.97 0.83
>>>> Minimum correlation of factor score estimates 0.91 -0.99 0.94 0.66
>>>>
>>>>  Total, General and Subset omega for each subset
>>>>                                                  g F1*  F2*  F3*
>>>> Omega total for total scores and subscales    0.96  NA 0.83 0.95
>>>> Omega general for total scores and subscales  0.85  NA 0.82 0.76
>>>> Omega group for total scores and subscales    0.09  NA 0.01 0.19
>>>> ```
>>>>
>>>> Now, until here, I apply the basic (non hierarchical) omega() function
>>>> to
>>>> my own database
>>>>
>>>>
>>>> ```
>>>> > omegaSem(r9,n.obs=198)
>>>> Error in parse(text = x, keep.source = FALSE) :
>>>>   <text>:2:0: unexpected end of input
>>>> 1: ~
>>>> ```
>>>> The previous is the error message that appears after trying to use the
>>>> omegaSem() function directly with my own database.
>>>>
>>>> Now, following, I present the expected output of omegaSem() applied
>>>> directly using the Thurstone database. It's similar to the output of the
>>>> basic omega() function but it has certain distinctions:
>>>>
>>>> ```
>>>>
>>>> > r9 <- Thurstone
>>>> > omegaSem(r9,n.obs=500)
>>>>
>>>> Call: omegaSem(m = r9, n.obs = 500)
>>>> Omega
>>>> Call: omega(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip,
>>>>     digits = digits, title = title, sl = sl, labels = labels,
>>>>     plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option =
>>>> option)
>>>> Alpha:                 0.89
>>>> G.6:                   0.91
>>>> Omega Hierarchical:    0.74
>>>> Omega H asymptotic:    0.79
>>>> Omega Total            0.93
>>>>
>>>> Schmid Leiman Factor loadings greater than  0.2
>>>>                      g   F1*   F2*   F3*   h2   u2   p2
>>>> Sentences         0.71  0.56             0.82 0.18 0.61
>>>> Vocabulary        0.73  0.55             0.84 0.16 0.63
>>>> Sent.Completion   0.68  0.52             0.74 0.26 0.63
>>>> First.Letters     0.65        0.56       0.73 0.27 0.57
>>>> Four.Letter.Words 0.62        0.49       0.63 0.37 0.61
>>>> Suffixes          0.56        0.41       0.50 0.50 0.63
>>>> Letter.Series     0.59              0.62 0.73 0.27 0.48
>>>> Pedigrees         0.58  0.24        0.34 0.51 0.49 0.66
>>>> Letter.Group      0.54              0.46 0.52 0.48 0.56
>>>>
>>>> With eigenvalues of:
>>>>    g  F1*  F2*  F3*
>>>> 3.58 0.96 0.74 0.72
>>>>
>>>> general/max  3.73   max/min =   1.34
>>>> mean percent general =  0.6    with sd =  0.05 and cv of  0.09
>>>> Explained Common Variance of the general factor =  0.6
>>>>
>>>> The degrees of freedom are 12  and the fit is  0.01
>>>> The number of observations was  500  with Chi Square =  7.12  with prob
>>>> <
>>>>  0.85
>>>> The root mean square of the residuals is  0.01
>>>> The df corrected root mean square of the residuals is  0.01
>>>> RMSEA index =  0  and the 10 % confidence intervals are  0 0.026
>>>> BIC =  -67.45
>>>>
>>>> Compare this with the adequacy of just a general factor and no group
>>>> factors
>>>> The degrees of freedom for just the general factor are 27  and the fit
>>>> is
>>>>  1.48
>>>> The number of observations was  500  with Chi Square =  730.93  with
>>>> prob <
>>>>  1.3e-136
>>>> The root mean square of the residuals is  0.14
>>>> The df corrected root mean square of the residuals is  0.16
>>>>
>>>> RMSEA index =  0.23  and the 10 % confidence intervals are  0.214 0.243
>>>> BIC =  563.14
>>>>
>>>> Measures of factor score adequacy
>>>>                                                  g  F1*  F2*  F3*
>>>> Correlation of scores with factors            0.86 0.73 0.72 0.75
>>>> Multiple R square of scores with factors      0.74 0.54 0.51 0.57
>>>> Minimum correlation of factor score estimates 0.49 0.07 0.03 0.13
>>>>
>>>>  Total, General and Subset omega for each subset
>>>>                                                  g  F1*  F2*  F3*
>>>> Omega total for total scores and subscales    0.93 0.92 0.83 0.79
>>>> Omega general for total scores and subscales  0.74 0.58 0.50 0.47
>>>> Omega group for total scores and subscales    0.16 0.34 0.32 0.32
>>>>
>>>>  The following analyses were done using the  lavaan  package
>>>>
>>>>  Omega Hierarchical from a confirmatory model using sem =  0.79
>>>>  Omega Total  from a confirmatory model using sem =  0.93
>>>> With loadings of
>>>>                      g  F1*  F2*  F3*   h2   u2   p2
>>>> Sentences         0.77 0.49           0.83 0.17 0.71
>>>> Vocabulary        0.79 0.45           0.83 0.17 0.75
>>>> Sent.Completion   0.75 0.40           0.73 0.27 0.77
>>>> First.Letters     0.61      0.61      0.75 0.25 0.50
>>>> Four.Letter.Words 0.60      0.51      0.61 0.39 0.59
>>>> Suffixes          0.57      0.39      0.48 0.52 0.68
>>>> Letter.Series     0.57           0.73 0.85 0.15 0.38
>>>> Pedigrees         0.66           0.25 0.50 0.50 0.87
>>>> Letter.Group      0.53           0.41 0.45 0.55 0.62
>>>>
>>>> With eigenvalues of:
>>>>    g  F1*  F2*  F3*
>>>> 3.87 0.60 0.79 0.76
>>>>
>>>> The degrees of freedom of the confimatory model are  18  and the fit is
>>>>  57.11391  with p =  5.936744e-06
>>>> general/max  4.92   max/min =   1.3
>>>> mean percent general =  0.65    with sd =  0.15 and cv of  0.23
>>>> Explained Common Variance of the general factor =  0.64
>>>>
>>>> Measures of factor score adequacy
>>>>                                                  g   F1*  F2*  F3*
>>>> Correlation of scores with factors            0.90  0.68 0.80 0.85
>>>> Multiple R square of scores with factors      0.81  0.46 0.64 0.73
>>>> Minimum correlation of factor score estimates 0.62 -0.08 0.27 0.45
>>>>
>>>>  Total, General and Subset omega for each subset
>>>>                                                  g  F1*  F2*  F3*
>>>> Omega total for total scores and subscales    0.93 0.92 0.82 0.80
>>>> Omega general for total scores and subscales  0.79 0.69 0.48 0.50
>>>> Omega group for total scores and subscales    0.14 0.23 0.35 0.31
>>>>
>>>> To get the standard sem fit statistics, ask for summary on the fitted
>>>> object>
>>>> ```
>>>>
>>>>
>>>>
>>>> I'm expecting to have the same output applying the function directly. My
>>>> expectation is to make sure if its mandatory to make the schmid
>>>> transformation before the omegaSem(). I'm supposing that not, because
>>>> its
>>>> not supposed to work like that as it says in the guide. Maybe this can
>>>> be
>>>> solved correcting the error message:
>>>>
>>>> ```
>>>> > r9 <- my.data
>>>> > omegaSem(r9,n.obs=198)
>>>> Error in parse(text = x, keep.source = FALSE) :
>>>>   <text>:2:0: unexpected end of input
>>>> 1: ~
>>>>    ^
>>>> ```
>>>>  Hope I've been clear enough. Feel free to ask any other information
>>>> that
>>>> you might need.
>>>>
>>>> Thank you so much for giving me any guidance to reach the answer of this
>>>> issue. I higly appreciate any help.
>>>>
>>>> Regards,
>>>>
>>>> Danilo
>>>>
>>>> --
>>>> Danilo E. Rodr?guez Zapata
>>>> Analista en Psicometr?a
>>>> CEBIAC
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>
>> --
>> Danilo E. Rodr?guez Zapata
>> Analista en Psicometr?a
>> CEBIAC
>>
>
>
> --
> Danilo E. Rodr?guez Zapata
> Analista en Psicometr?a
> CEBIAC
>

	[[alternative HTML version deleted]]


From d@n||o_rodr|guez @end|ng |rom cun@edu@co  Thu Aug 29 21:29:41 2019
From: d@n||o_rodr|guez @end|ng |rom cun@edu@co (Danilo Esteban Rodriguez Zapata)
Date: Thu, 29 Aug 2019 14:29:41 -0500
Subject: [R] R code: How to correct "Error in parse(text = x,
 keep.source = FALSE)" output in psych package using own dataset
In-Reply-To: <CAF8bMcZAajdt0PssTK2xN24fmt3HzEKSvVwUMnW_ZnB30maf=g@mail.gmail.com>
References: <CAMMrtmZg1HryjK134A8xH73Ba=Pmkbmj5JOLLGbdSYib1PE3qA@mail.gmail.com>
 <CAF8bMcZAajdt0PssTK2xN24fmt3HzEKSvVwUMnW_ZnB30maf=g@mail.gmail.com>
Message-ID: <CAMMrtmbx5G77RoF7tKqZtApO7XLfEyyu97xLmUHKfJSoR0fTpg@mail.gmail.com>

Dear William,

Thank you for your answer, I would like to add some information that I just
obtained looking in different sites and forums. Someone there ask me to
create a fake data file, so I did that from my original data file. What I
did was open the .csv file with notepad and replace all the 4 for 5 and the
2 for 1, then I saved the file again with no other changes. I also searched
for the "~" in the file and I found nothing.  Now with that file I did the
omegaSem() function and it worked succesfully, so the weird thing here is
that the omegaSem() function works with the fake data file, wich is exactly
the same as the original file, but recoding some answers as I said.

It seems to be an issue with the file. When I replace, lets say, the 5 for
6 and make the omegaSem() again, it works. Then I replace back again the 6
for 5 in all the data and the function doesn't works anymore.

El jue., 29 ago. 2019 a las 12:33, William Dunlap (<wdunlap at tibco.com>)
escribi?:

>     > omegaSem(r9,n.obs=198)
>     Error in parse(text = x, keep.source = FALSE) :
>       <text>:2:0: unexpected end of input
>
> This error probably comes from calling factor("~") and
> psych::omegaSem(data) will do that if  all the columns in data are very
> highly correlated with one another.   In that case omega(data, nfactor=n)
> will not be able to find n factors in the data but it returns "~" in place
> of the factors that it could not find.  E.g.,
> > fakeData <- data.frame(A=1/(1:40), B=1/(2:41), C=1/(3:42), D=1/(4:43),
> E=1/(5:44))
> > cor(fakeData)
>           A         B         C         D         E
> A 1.0000000 0.9782320 0.9481293 0.9215071 0.8988962
> B 0.9782320 1.0000000 0.9932037 0.9811287 0.9684658
> C 0.9481293 0.9932037 1.0000000 0.9969157 0.9906838
> D 0.9215071 0.9811287 0.9969157 1.0000000 0.9983014
> E 0.8988962 0.9684658 0.9906838 0.9983014 1.0000000
> > psych::omegaSem(fakeData)
> Loading required namespace: lavaan
> Loading required namespace: GPArotation
> In factor.stats, I could not find the RMSEA upper bound . Sorry about that
> Error in parse(text = x, keep.source = FALSE) :
>   <text>:2:0: unexpected end of input
> 1: ~
>    ^
> In addition: Warning message:
> In cov2cor(t(w) %*% r %*% w) :
>   diag(.) had 0 or NA entries; non-finite result is doubtful
> > psych::omega(fakeData)$model$lavaan
> In factor.stats, I could not find the RMSEA upper bound . Sorry about that
> [1] g =~ +A+B+C+D+E       F1=~  + B + C + D + E F2=~  + A
> [4] F3=~
> Warning message:
> In cov2cor(t(w) %*% r %*% w) :
>   diag(.) had 0 or NA entries; non-finite result is doubtful
>
> You can get a result if you use nfactors=n where n is the number of the
> good F<n> entries in psych::omega()$model$lavaan:
> > psych::omegaSem(fakeData, nfactors=2)
> ...
>
> Measures of factor score adequacy
>                                                    g    F1*      F2*
> Correlation of scores with factors             11.35  12.42    84.45
> Multiple R square of scores with factors      128.93 154.32  7131.98
> Minimum correlation of factor score estimates 256.86 307.64 14262.96
> ...
> Does that work with your data?
>
> This is a problem that the maintainer of psych,
> >   maintainer("psych")
> [1] "William Revelle <revelle at northwestern.edu>"
> would like to know about.
>
>
>
>
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Thu, Aug 29, 2019 at 9:03 AM Danilo Esteban Rodriguez Zapata via R-help
> <r-help at r-project.org> wrote:
>
>> This is a problem related to my last question referred to the omegaSem()
>> function in the psych package (that is already solved because I realized
>> that I was missing a variable assignment and because of that I had an
>> 'object not found' error:
>>
>>
>> https://stackoverflow.com/questions/57661750/one-of-the-omegasem-function-arguments-is-an-object-not-found
>>
>> I was trying to use that function following the guide to find McDonald's
>> hierarchical Omega by Dr William Revelle:
>>
>> http://personality-project.org/r/psych/HowTo/omega.pdf
>>
>> So now, with the variable error corrected, I'm having a different error
>> that does not occur when I use the same function with the example database
>> (Thurstone) provided in the tutorial that comes with the psych package. I
>> mean, I'm able to use the function succesfully using the Thurstone data
>> (with no other action, I have the expected result) but the function
>> doesn't
>> work when I use my own data.
>>
>> I searched over other posted questions, and the actions that they perform
>> are not even similar to what I'm trying to do. I have almost two weeks
>> using R, so I'm not able to identify yet how can I extrapolate the
>> solutions for that error message to my procedure (because it seems to be
>> frequent), although I have basic code knowledge. However related questions
>> give no anwer by now.
>>
>> Additionally, I decided to look over more documentation about the package,
>> and when I was testing other functions, I was able to use the omegaSem()
>> function with another example database, BUT after and only after I did the
>> schmid transformation. So with that, I discovered that when I tried to use
>> the omegaSem() function before the schmid tranformation I had the same
>> error message, but not after that tranformation with this second example
>> database.
>>
>> This make sense with the actual procedure of the omegaSem() procedure, but
>> I'm suposing that it must be done completely and automatically by the
>> omegaSem() function as it is explained in the guide and I have understood
>> until now, as it follows:
>>
>> 1. omegaSem() applies factor analysis
>> 2. omegaSem() rotate factors obliquely
>> 3. omegaSem() transform data with Schmid Leiman (schmid)
>>
>> -------necessary steps to print output-------------------
>>
>> 4. omegaSem() print McDonald's hierarchical Omega
>>
>> So here, another questions appears:  - Why the omegaSem() function works
>> with the Thurstone database without any other action and only works for
>> the
>> second example database after performing the schmid transformation? -  Why
>> with other databases I dont have the same output applying the omegaSem()
>> function directly? - How is this related to the error message that the
>> compiler shows when I try to apply the function directly to the database?
>>
>>
>> This is the code that I'm using now: (example of the succesfull omegaSem()
>> done after schmid tranformation not included)
>>
>> ```
>> > library(psych)
>> > library(ctv, lavaan)
>> > library(GPArotation)
>> > my.data <- read.file()
>> Data from the .csv file
>> D:\Users\Admon\Documents\prueba_export_1563806208742.csv has been loaded.
>> > describe(my.data)
>>            vars   n mean   sd median trimmed  mad min max range  skew
>> kurtosis
>> AUT_10_04     1 195 4.11 0.90      4    4.23 1.48   1   5     4 -0.92
>> 0.33
>> AUN_07_01     2 195 3.79 1.14      4    3.90 1.48   1   5     4 -0.59
>>  -0.71
>> AUN_07_02     3 195 3.58 1.08      4    3.65 1.48   1   5     4 -0.39
>>  -0.56
>> AUN_09_01     4 195 4.15 0.80      4    4.23 1.48   1   5     4 -0.76
>> 0.51
>> AUN_10_01     5 195 4.25 0.79      4    4.34 1.48   1   5     4 -0.91
>> 0.74
>> AUT_11_01     6 195 4.43 0.77      5    4.56 0.00   1   5     4 -1.69
>> 3.77
>> AUT_17_01     7 195 4.46 0.67      5    4.55 0.00   1   5     4 -1.34
>> 2.96
>> AUT_20_03     8 195 4.44 0.65      5    4.53 0.00   2   5     3 -0.84
>> 0.12
>> CRE_05_02     9 195 2.47 1.01      2    2.43 1.48   1   5     4  0.35
>>  -0.46
>> CRE_07_04    10 195 2.42 1.08      2    2.34 1.48   1   5     4  0.51
>>  -0.43
>> CRE_10_01    11 195 4.41 0.68      5    4.51 0.00   2   5     3 -0.79
>>  -0.12
>> CRE_16_02    12 195 2.75 1.23      3    2.69 1.48   1   5     4  0.29
>>  -0.96
>> EFEC_03_07   13 195 4.35 0.69      4    4.45 1.48   1   5     4 -0.95
>> 1.59
>> EFEC_05      14 195 4.53 0.59      5    4.60 0.00   3   5     2 -0.82
>>  -0.34
>> EFEC_09_02   15 195 2.19 0.91      2    2.11 1.48   1   5     4  0.57
>>  -0.03
>> EFEC_16_03   16 195 4.21 0.77      4    4.29 1.48   2   5     3 -0.71
>>  -0.04
>> EVA_02_01    17 195 4.47 0.61      5    4.54 0.00   3   5     2 -0.70
>>  -0.50
>> EVA_07_01    18 195 4.38 0.60      4    4.43 1.48   3   5     2 -0.40
>>  -0.70
>> EVA_12_02    19 195 2.64 1.22      2    2.59 1.48   1   5     4  0.30
>>  -1.00
>> EVA_15_06    20 195 4.19 0.74      4    4.26 1.48   2   5     3 -0.55
>>  -0.29
>> FLX_04_01    21 195 4.32 0.69      4    4.41 1.48   2   5     3 -0.71
>> 0.05
>> FLX_04_05    22 195 4.23 0.74      4    4.32 0.00   1   5     4 -0.99
>> 1.69
>> FLX_08_02    23 195 2.87 1.19      3    2.86 1.48   1   5     4  0.07
>>  -1.05
>> FLX_10_03    24 195 4.30 0.71      4    4.39 1.48   2   5     3 -0.84
>> 0.66
>> IDO_01_06    25 195 3.10 1.26      3    3.13 1.48   1   5     4 -0.19
>>  -1.08
>> IDO_05_02    26 195 2.89 1.26      3    2.87 1.48   1   5     4 -0.03
>>  -1.16
>> IDO_09_03    27 195 3.87 0.97      4    3.99 1.48   1   5     4 -0.84
>> 0.47
>> IDO_17_01    28 195 3.94 0.88      4    4.02 0.00   1   5     4 -0.93
>> 1.23
>> IE_01_03     29 195 4.01 0.88      4    4.10 1.48   1   5     4 -0.91
>> 0.94
>> IE_10_03     30 195 4.15 1.00      4    4.34 1.48   1   5     4 -1.31
>> 1.28
>> IE_13_03     31 195 4.16 0.91      4    4.30 1.48   1   5     4 -1.26
>> 1.74
>> IE_15_01     32 195 4.26 0.85      4    4.39 1.48   1   5     4 -1.16
>> 1.08
>> LC_07_03     33 195 4.25 0.72      4    4.34 0.00   1   5     4 -1.07
>> 2.64
>> LC_08_02     34 195 3.25 1.22      4    3.31 1.48   1   5     4 -0.41
>>  -0.90
>> LC_11_03     35 195 3.50 1.14      4    3.56 1.48   1   5     4 -0.38
>>  -0.68
>> LC_11_05     36 195 4.42 0.69      5    4.52 0.00   1   5     4 -1.14
>> 1.97
>> ME_02_03     37 195 4.11 0.92      4    4.25 1.48   1   5     4 -1.18
>> 1.29
>> ME_07_06     38 195 3.19 1.28      3    3.24 1.48   1   5     4 -0.28
>>  -1.03
>> ME_09_01     39 195 4.24 0.77      4    4.34 1.48   1   5     4 -1.12
>> 2.19
>> ME_09_06     40 195 3.23 1.33      4    3.29 1.48   1   5     4 -0.31
>>  -1.14
>> NEG_01_03    41 195 4.18 0.76      4    4.27 0.00   1   5     4 -1.28
>> 3.33
>> NEG_05_04    42 195 4.27 0.69      4    4.35 0.00   1   5     4 -0.87
>> 1.75
>> NEG_07_03    43 195 4.32 0.73      4    4.43 1.48   1   5     4 -1.05
>> 1.55
>> NEG_08_01    44 195 3.95 0.88      4    4.02 1.48   1   5     4 -0.67
>> 0.29
>> OP_03_05     45 195 4.32 0.66      4    4.39 0.00   1   5     4 -0.99
>> 2.54
>> OP_12_01     46 195 4.16 0.80      4    4.25 1.48   1   5     4 -1.02
>> 1.57
>> OP_14_01     47 195 4.27 0.78      4    4.38 1.48   1   5     4 -1.15
>> 1.67
>> OP_14_02     48 195 4.36 0.68      4    4.44 1.48   1   5     4 -1.07
>> 2.35
>> ORL_01_03    49 195 4.36 0.77      4    4.49 1.48   1   5     4 -1.31
>> 2.08
>> ORL_03_01    50 195 4.41 0.69      4    4.50 1.48   1   5     4 -1.28
>> 2.77
>> ORL_03_05    51 195 4.36 0.74      4    4.48 1.48   2   5     3 -1.13
>> 1.28
>> ORL_10_05    52 195 4.40 0.68      4    4.48 1.48   1   5     4 -1.18
>> 2.57
>> PER_08_02    53 195 3.23 1.29      4    3.29 1.48   1   5     4 -0.26
>>  -1.17
>> PER_16_01    54 195 4.29 0.70      4    4.38 1.48   2   5     3 -0.74
>> 0.27
>> PER_19_06    55 195 3.19 1.25      3    3.24 1.48   1   5     4 -0.20
>>  -1.06
>> PER_22_06    56 195 4.21 0.73      4    4.29 0.00   1   5     4 -0.89
>> 1.46
>> PLA_01_03    57 195 4.23 0.68      4    4.31 0.00   2   5     3 -0.81
>> 1.18
>> PLA_05_01    58 195 4.06 0.77      4    4.13 0.00   1   5     4 -0.89
>> 1.29
>> PLA_07_02    59 195 2.94 1.19      3    2.94 1.48   1   5     4  0.00
>>  -1.02
>> PLA_10_01    60 195 4.03 0.76      4    4.08 0.00   1   5     4 -0.68
>> 0.87
>> PLA_12_02    61 195 2.67 1.11      2    2.62 1.48   1   5     4  0.41
>>  -0.61
>> PLA_18_01    62 195 4.01 0.85      4    4.09 1.48   1   5     4 -0.82
>> 0.78
>> PR_06_02     63 195 3.02 1.27      3    3.02 1.48   1   5     4 -0.01
>>  -1.13
>> PR_15_03     64 195 3.55 1.07      4    3.62 1.48   1   5     4 -0.46
>>  -0.22
>> PR_25_01     65 195 2.36 1.04      2    2.27 1.48   1   5     4  0.73
>> 0.06
>> PR_25_06     66 195 2.95 1.17      3    2.94 1.48   1   5     4  0.04
>>  -0.86
>> REL_09_05    67 195 3.81 0.95      4    3.89 1.48   1   5     4 -0.51
>>  -0.31
>> REL_14_03    68 195 3.99 0.88      4    4.08 1.48   1   5     4 -0.75
>> 0.39
>> REL_14_06    69 195 2.93 1.26      3    2.92 1.48   1   5     4  0.06
>>  -1.11
>> REL_16_04    70 195 3.16 1.27      3    3.20 1.48   1   5     4 -0.13
>>  -1.11
>> RS_02_03     71 195 4.14 0.75      4    4.22 0.00   1   5     4 -0.82
>> 1.14
>> RS_07_05     72 195 4.29 0.67      4    4.38 0.00   2   5     3 -0.72
>> 0.59
>> RS_08_05     73 195 4.04 0.88      4    4.13 1.48   1   5     4 -0.97
>> 1.26
>> RS_13_03     74 195 4.19 0.69      4    4.25 0.00   2   5     3 -0.46
>>  -0.17
>> TF_03_01     75 195 4.01 0.82      4    4.06 1.48   1   5     4 -0.63
>> 0.32
>> TF_04_01     76 195 4.09 0.76      4    4.15 0.00   1   5     4 -0.70
>> 0.76
>> TF_10_03     77 195 4.11 0.85      4    4.21 1.48   1   5     4 -0.96
>> 0.99
>> TF_12_01     78 195 4.11 0.85      4    4.21 1.48   1   5     4 -1.10
>> 1.66
>> TRE_09_05    79 195 4.29 0.79      4    4.39 1.48   1   5     4 -1.12
>> 1.74
>> TRE_09_06    80 195 4.33 0.69      4    4.42 1.48   1   5     4 -1.10
>> 2.36
>> TRE_26_04    81 195 2.97 1.20      3    2.96 1.48   1   5     4  0.08
>>  -1.01
>> TRE_26_05    82 195 3.99 0.84      4    4.03 1.48   1   5     4 -0.41
>>  -0.37
>>
>> ```
>>
>> Until now, I have charged the libraries, import the my own database and
>> did
>> some simple descriptive statistics.
>>
>> ```
>>
>> > r9 <- my.data
>> > omega(r9)
>> Omega
>> Call: omega(m = r9)
>> Alpha:                 0.95
>> G.6:                   0.98
>> Omega Hierarchical:    0.85
>> Omega H asymptotic:    0.89
>> Omega Total            0.96
>>
>> Schmid Leiman Factor loadings greater than  0.2
>>                 g   F1*   F2*   F3*   h2   u2   p2
>> AUT_10_04    0.43              0.30 0.27 0.73 0.68
>> AUN_07_01                           0.05 0.95 0.53
>> AUN_07_02                           0.06 0.94 0.26
>> AUN_09_01    0.38              0.30 0.24 0.76 0.59
>> AUN_10_01    0.35              0.55 0.44 0.56 0.29
>> AUT_11_01    0.42              0.30 0.27 0.73 0.66
>> AUT_17_01    0.32              0.40 0.28 0.72 0.37
>> AUT_20_03    0.41              0.25 0.24 0.76 0.73
>> CRE_05_02-   0.24       -0.53       0.34 0.66 0.17
>> CRE_07_04-   0.37       -0.51       0.39 0.61 0.35
>> CRE_10_01    0.46              0.48 0.46 0.54 0.47
>> CRE_16_02-              -0.70       0.48 0.52 0.01
>> EFEC_03_07   0.46              0.31 0.31 0.69 0.68
>> EFEC_05      0.43              0.32 0.29 0.71 0.64
>> EFEC_09_02-  0.29       -0.46       0.29 0.71 0.28
>> EFEC_16_03   0.49              0.26 0.31 0.69 0.77
>> EVA_02_01    0.55              0.21 0.36 0.64 0.85
>> EVA_07_01    0.57                   0.37 0.63 0.89
>> EVA_12_02-              -0.61       0.39 0.61 0.06
>> EVA_15_06    0.50              0.37 0.39 0.61 0.65
>> FLX_04_01    0.57              0.30 0.42 0.58 0.78
>> FLX_04_05    0.52              0.26 0.34 0.66 0.80
>> FLX_08_02-              -0.78       0.60 0.40 0.00
>> FLX_10_03    0.39              0.29 0.24 0.76 0.63
>> IDO_01_06-              -0.80       0.64 0.36 0.00
>> IDO_05_02-              -0.78       0.62 0.38 0.00
>> IDO_09_03    0.41              0.49 0.42 0.58 0.40
>> IDO_17_01    0.51              0.51 0.54 0.46 0.49
>> IE_01_03     0.44              0.60 0.56 0.44 0.35
>> IE_10_03     0.41              0.53 0.44 0.56 0.37
>> IE_13_03     0.39              0.48 0.38 0.62 0.40
>> IE_15_01     0.39              0.40 0.31 0.69 0.49
>> LC_07_03     0.50                   0.27 0.73 0.91
>> LC_08_02                 0.83       0.69 0.31 0.00
>> LC_11_03     0.25                   0.10 0.90 0.60
>> LC_11_05     0.45        0.24       0.27 0.73 0.75
>> ME_02_03     0.55                   0.31 0.69 0.99
>> ME_07_06                 0.85       0.75 0.25 0.02
>> ME_09_01     0.64                   0.45 0.55 0.93
>> ME_09_06                 0.81       0.69 0.31 0.02
>> NEG_01_03    0.58              0.20 0.38 0.62 0.88
>> NEG_05_04    0.70                   0.50 0.50 0.98
>> NEG_07_03    0.64                   0.43 0.57 0.96
>> NEG_08_01    0.43              0.25 0.25 0.75 0.74
>> OP_03_05     0.62                   0.40 0.60 0.98
>> OP_12_01     0.67                   0.46 0.54 0.98
>> OP_14_01     0.60                   0.38 0.62 0.95
>> OP_14_02     0.66                   0.47 0.53 0.93
>> ORL_01_03    0.67                   0.47 0.53 0.96
>> ORL_03_01    0.66                   0.48 0.52 0.91
>> ORL_03_05    0.64                   0.46 0.54 0.90
>> ORL_10_05    0.66                   0.49 0.51 0.89
>> PER_08_02    0.21        0.84       0.75 0.25 0.06
>> PER_16_01    0.68              0.21 0.50 0.50 0.91
>> PER_19_06    0.20        0.73       0.58 0.42 0.07
>> PER_22_06    0.53                   0.30 0.70 0.94
>> PLA_01_03    0.57                   0.36 0.64 0.89
>> PLA_05_01    0.61                   0.42 0.58 0.89
>> PLA_07_02                0.75       0.61 0.39 0.04
>> PLA_10_01    0.56                   0.36 0.64 0.88
>> PLA_12_02                0.61       0.37 0.63 0.00
>> PLA_18_01    0.63                   0.47 0.53 0.85
>> PR_06_02                 0.77       0.62 0.38 0.03
>> PR_15_03     0.31       -0.39  0.24 0.31 0.69 0.31
>> PR_25_01-               -0.56       0.32 0.68 0.00
>> PR_25_06                 0.74       0.55 0.45 0.01
>> REL_09_05    0.41       -0.23  0.38 0.37 0.63 0.45
>> REL_14_03    0.41       -0.21  0.29 0.30 0.70 0.56
>> REL_14_06                0.66  0.21 0.48 0.52 0.04
>> REL_16_04                0.78       0.63 0.37 0.03
>> RS_02_03     0.57                   0.36 0.64 0.90
>> RS_07_05     0.68                   0.47 0.53 0.99
>> RS_08_05     0.44                   0.20 0.80 0.95
>> RS_13_03     0.67                   0.46 0.54 0.97
>> TF_03_01     0.66                   0.44 0.56 0.98
>> TF_04_01     0.74                   0.56 0.44 0.98
>> TF_10_03     0.70                   0.50 0.50 0.98
>> TF_12_01     0.61                   0.40 0.60 0.92
>> TRE_09_05    0.70              0.23 0.55 0.45 0.89
>> TRE_09_06    0.62                   0.41 0.59 0.93
>> TRE_26_04-              -0.68       0.47 0.53 0.00
>> TRE_26_05    0.55       -0.21       0.34 0.66 0.88
>>
>> With eigenvalues of:
>>     g   F1*   F2*   F3*
>> 18.06  0.04 11.47  4.32
>>
>> general/max  1.57   max/min =   267.1
>> mean percent general =  0.58    with sd =  0.36 and cv of  0.63
>> Explained Common Variance of the general factor =  0.53
>>
>> The degrees of freedom are 3078  and the fit is  34.62
>> The number of observations was  195  with Chi Square =  5671.12  with prob
>> <  2.8e-157
>> The root mean square of the residuals is  0.06
>> The df corrected root mean square of the residuals is  0.06
>> RMSEA index =  0.078  and the 10 % confidence intervals are  0.063 NA
>> BIC =  -10559.18
>>
>> Compare this with the adequacy of just a general factor and no group
>> factors
>> The degrees of freedom for just the general factor are 3239  and the fit
>> is
>>  51.52
>> The number of observations was  195  with Chi Square =  8509.84  with prob
>> <  0
>> The root mean square of the residuals is  0.16
>> The df corrected root mean square of the residuals is  0.16
>>
>> RMSEA index =  0.104  and the 10 % confidence intervals are  0.089 NA
>> BIC =  -8569.4
>>
>> Measures of factor score adequacy
>>                                                  g   F1*  F2*  F3*
>> Correlation of scores with factors            0.98  0.07 0.98 0.91
>> Multiple R square of scores with factors      0.95  0.00 0.97 0.83
>> Minimum correlation of factor score estimates 0.91 -0.99 0.94 0.66
>>
>>  Total, General and Subset omega for each subset
>>                                                  g F1*  F2*  F3*
>> Omega total for total scores and subscales    0.96  NA 0.83 0.95
>> Omega general for total scores and subscales  0.85  NA 0.82 0.76
>> Omega group for total scores and subscales    0.09  NA 0.01 0.19
>> ```
>>
>> Now, until here, I apply the basic (non hierarchical) omega() function to
>> my own database
>>
>>
>> ```
>> > omegaSem(r9,n.obs=198)
>> Error in parse(text = x, keep.source = FALSE) :
>>   <text>:2:0: unexpected end of input
>> 1: ~
>> ```
>> The previous is the error message that appears after trying to use the
>> omegaSem() function directly with my own database.
>>
>> Now, following, I present the expected output of omegaSem() applied
>> directly using the Thurstone database. It's similar to the output of the
>> basic omega() function but it has certain distinctions:
>>
>> ```
>>
>> > r9 <- Thurstone
>> > omegaSem(r9,n.obs=500)
>>
>> Call: omegaSem(m = r9, n.obs = 500)
>> Omega
>> Call: omega(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip,
>>     digits = digits, title = title, sl = sl, labels = labels,
>>     plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option =
>> option)
>> Alpha:                 0.89
>> G.6:                   0.91
>> Omega Hierarchical:    0.74
>> Omega H asymptotic:    0.79
>> Omega Total            0.93
>>
>> Schmid Leiman Factor loadings greater than  0.2
>>                      g   F1*   F2*   F3*   h2   u2   p2
>> Sentences         0.71  0.56             0.82 0.18 0.61
>> Vocabulary        0.73  0.55             0.84 0.16 0.63
>> Sent.Completion   0.68  0.52             0.74 0.26 0.63
>> First.Letters     0.65        0.56       0.73 0.27 0.57
>> Four.Letter.Words 0.62        0.49       0.63 0.37 0.61
>> Suffixes          0.56        0.41       0.50 0.50 0.63
>> Letter.Series     0.59              0.62 0.73 0.27 0.48
>> Pedigrees         0.58  0.24        0.34 0.51 0.49 0.66
>> Letter.Group      0.54              0.46 0.52 0.48 0.56
>>
>> With eigenvalues of:
>>    g  F1*  F2*  F3*
>> 3.58 0.96 0.74 0.72
>>
>> general/max  3.73   max/min =   1.34
>> mean percent general =  0.6    with sd =  0.05 and cv of  0.09
>> Explained Common Variance of the general factor =  0.6
>>
>> The degrees of freedom are 12  and the fit is  0.01
>> The number of observations was  500  with Chi Square =  7.12  with prob <
>>  0.85
>> The root mean square of the residuals is  0.01
>> The df corrected root mean square of the residuals is  0.01
>> RMSEA index =  0  and the 10 % confidence intervals are  0 0.026
>> BIC =  -67.45
>>
>> Compare this with the adequacy of just a general factor and no group
>> factors
>> The degrees of freedom for just the general factor are 27  and the fit is
>>  1.48
>> The number of observations was  500  with Chi Square =  730.93  with prob
>> <
>>  1.3e-136
>> The root mean square of the residuals is  0.14
>> The df corrected root mean square of the residuals is  0.16
>>
>> RMSEA index =  0.23  and the 10 % confidence intervals are  0.214 0.243
>> BIC =  563.14
>>
>> Measures of factor score adequacy
>>                                                  g  F1*  F2*  F3*
>> Correlation of scores with factors            0.86 0.73 0.72 0.75
>> Multiple R square of scores with factors      0.74 0.54 0.51 0.57
>> Minimum correlation of factor score estimates 0.49 0.07 0.03 0.13
>>
>>  Total, General and Subset omega for each subset
>>                                                  g  F1*  F2*  F3*
>> Omega total for total scores and subscales    0.93 0.92 0.83 0.79
>> Omega general for total scores and subscales  0.74 0.58 0.50 0.47
>> Omega group for total scores and subscales    0.16 0.34 0.32 0.32
>>
>>  The following analyses were done using the  lavaan  package
>>
>>  Omega Hierarchical from a confirmatory model using sem =  0.79
>>  Omega Total  from a confirmatory model using sem =  0.93
>> With loadings of
>>                      g  F1*  F2*  F3*   h2   u2   p2
>> Sentences         0.77 0.49           0.83 0.17 0.71
>> Vocabulary        0.79 0.45           0.83 0.17 0.75
>> Sent.Completion   0.75 0.40           0.73 0.27 0.77
>> First.Letters     0.61      0.61      0.75 0.25 0.50
>> Four.Letter.Words 0.60      0.51      0.61 0.39 0.59
>> Suffixes          0.57      0.39      0.48 0.52 0.68
>> Letter.Series     0.57           0.73 0.85 0.15 0.38
>> Pedigrees         0.66           0.25 0.50 0.50 0.87
>> Letter.Group      0.53           0.41 0.45 0.55 0.62
>>
>> With eigenvalues of:
>>    g  F1*  F2*  F3*
>> 3.87 0.60 0.79 0.76
>>
>> The degrees of freedom of the confimatory model are  18  and the fit is
>>  57.11391  with p =  5.936744e-06
>> general/max  4.92   max/min =   1.3
>> mean percent general =  0.65    with sd =  0.15 and cv of  0.23
>> Explained Common Variance of the general factor =  0.64
>>
>> Measures of factor score adequacy
>>                                                  g   F1*  F2*  F3*
>> Correlation of scores with factors            0.90  0.68 0.80 0.85
>> Multiple R square of scores with factors      0.81  0.46 0.64 0.73
>> Minimum correlation of factor score estimates 0.62 -0.08 0.27 0.45
>>
>>  Total, General and Subset omega for each subset
>>                                                  g  F1*  F2*  F3*
>> Omega total for total scores and subscales    0.93 0.92 0.82 0.80
>> Omega general for total scores and subscales  0.79 0.69 0.48 0.50
>> Omega group for total scores and subscales    0.14 0.23 0.35 0.31
>>
>> To get the standard sem fit statistics, ask for summary on the fitted
>> object>
>> ```
>>
>>
>>
>> I'm expecting to have the same output applying the function directly. My
>> expectation is to make sure if its mandatory to make the schmid
>> transformation before the omegaSem(). I'm supposing that not, because its
>> not supposed to work like that as it says in the guide. Maybe this can be
>> solved correcting the error message:
>>
>> ```
>> > r9 <- my.data
>> > omegaSem(r9,n.obs=198)
>> Error in parse(text = x, keep.source = FALSE) :
>>   <text>:2:0: unexpected end of input
>> 1: ~
>>    ^
>> ```
>>  Hope I've been clear enough. Feel free to ask any other information that
>> you might need.
>>
>> Thank you so much for giving me any guidance to reach the answer of this
>> issue. I higly appreciate any help.
>>
>> Regards,
>>
>> Danilo
>>
>> --
>> Danilo E. Rodr?guez Zapata
>> Analista en Psicometr?a
>> CEBIAC
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

-- 
Danilo E. Rodr?guez Zapata
Analista en Psicometr?a
CEBIAC

	[[alternative HTML version deleted]]


From d@n||o_rodr|guez @end|ng |rom cun@edu@co  Thu Aug 29 21:31:05 2019
From: d@n||o_rodr|guez @end|ng |rom cun@edu@co (Danilo Esteban Rodriguez Zapata)
Date: Thu, 29 Aug 2019 14:31:05 -0500
Subject: [R] R code: How to correct "Error in parse(text = x,
 keep.source = FALSE)" output in psych package using own dataset
In-Reply-To: <CAMMrtmbx5G77RoF7tKqZtApO7XLfEyyu97xLmUHKfJSoR0fTpg@mail.gmail.com>
References: <CAMMrtmZg1HryjK134A8xH73Ba=Pmkbmj5JOLLGbdSYib1PE3qA@mail.gmail.com>
 <CAF8bMcZAajdt0PssTK2xN24fmt3HzEKSvVwUMnW_ZnB30maf=g@mail.gmail.com>
 <CAMMrtmbx5G77RoF7tKqZtApO7XLfEyyu97xLmUHKfJSoR0fTpg@mail.gmail.com>
Message-ID: <CAMMrtmZYW_-yA0WpQVXWh0ufcDtfJnf6BFnrYPNJ4SYBtnsY=w@mail.gmail.com>

well the output with the code that you refer is the following:

> psych::omega(my.data)$model$lavaan
[1] g =~
+AUT_10_04+AUN_07_01+AUN_07_02+AUN_09_01+AUN_10_01+AUT_11_01+AUT_17_01+AUT_20_03+CRE_05_02+CRE_07_04+CRE_10_01+CRE_16_02+EFEC_03_07+EFEC_05+EFEC_09_02+EFEC_16_03+EVA_02_01+EVA_07_01+EVA_12_02+EVA_15_06+FLX_04_01+FLX_04_05+FLX_08_02+FLX_10_03+IDO_01_06+IDO_05_02+IDO_09_03+IDO_17_01+IE_01_03+IE_10_03+IE_13_03+IE_15_01+LC_07_03+LC_08_02+LC_11_03+LC_11_05+ME_02_03+ME_07_06+ME_09_01+ME_09_06+NEG_01_03+NEG_05_04+NEG_07_03+NEG_08_01+OP_03_05+OP_12_01+OP_14_01+OP_14_02+ORL_01_03+ORL_03_01+ORL_03_05+ORL_10_05+PER_08_02+PER_16_01+PER_19_06+PER_22_06+PLA_01_03+PLA_05_01+PLA_07_02+PLA_10_01+PLA_12_02+PLA_18_01+PR_06_02+PR_15_03+PR_25_01+PR_25_06+REL_09_05+REL_14_03+REL_14_06+REL_16_04+RS_02_03+RS_07_05+RS_08_05+RS_13_03+TF_03_01+TF_04_01+TF_10_03+TF_12_01+TRE_09_05+TRE_09_06+TRE_26_04+TRE_26_05
[2] F1=~










[3] F2=~  + AUN_07_02 + CRE_05_02 + CRE_07_04 + CRE_16_02 + EFEC_09_02 +
EVA_12_02 + FLX_08_02 + IDO_01_06 + IDO_05_02 + LC_08_02 + LC_11_03 +
LC_11_05 + ME_02_03 + ME_07_06 + ME_09_06 + NEG_07_03 + OP_03_05 + OP_14_01
+ OP_14_02 + ORL_01_03 + ORL_03_01 + PER_08_02 + PER_19_06 + PLA_05_01 +
PLA_07_02 + PLA_10_01 + PLA_12_02 + PLA_18_01 + PR_06_02 + PR_15_03 +
PR_25_01 + PR_25_06 + REL_14_06 + REL_16_04 + TF_04_01 + TF_10_03 +
TRE_26_04 + TRE_26_05




[4] F3=~  + AUT_10_04 + AUN_07_01 + AUN_09_01 + AUN_10_01 + AUT_11_01 +
AUT_17_01 + AUT_20_03 + CRE_10_01 + EFEC_03_07 + EFEC_05 + EFEC_16_03 +
EVA_02_01 + EVA_07_01 + EVA_15_06 + FLX_04_01 + FLX_04_05 + FLX_10_03 +
IDO_09_03 + IDO_17_01 + IE_01_03 + IE_10_03 + IE_13_03 + IE_15_01 +
LC_07_03 + ME_09_01 + NEG_01_03 + NEG_05_04 + NEG_08_01 + OP_12_01 +
ORL_03_05 + ORL_10_05 + PER_16_01 + PER_22_06 + PLA_01_03 + REL_09_05 +
REL_14_03 + RS_02_03 + RS_07_05 + RS_08_05 + RS_13_03 + TF_03_01 + TF_12_01
+ TRE_09_05 + TRE_09_06



>

El jue., 29 ago. 2019 a las 14:29, Danilo Esteban Rodriguez Zapata (<
danilo_rodriguez at cun.edu.co>) escribi?:

> Dear William,
>
> Thank you for your answer, I would like to add some information that I
> just obtained looking in different sites and forums. Someone there ask me
> to create a fake data file, so I did that from my original data file. What
> I did was open the .csv file with notepad and replace all the 4 for 5 and
> the 2 for 1, then I saved the file again with no other changes. I also
> searched for the "~" in the file and I found nothing.  Now with that file I
> did the omegaSem() function and it worked succesfully, so the weird thing
> here is that the omegaSem() function works with the fake data file, wich is
> exactly the same as the original file, but recoding some answers as I said.
>
> It seems to be an issue with the file. When I replace, lets say, the 5 for
> 6 and make the omegaSem() again, it works. Then I replace back again the 6
> for 5 in all the data and the function doesn't works anymore.
>
> El jue., 29 ago. 2019 a las 12:33, William Dunlap (<wdunlap at tibco.com>)
> escribi?:
>
>>     > omegaSem(r9,n.obs=198)
>>     Error in parse(text = x, keep.source = FALSE) :
>>       <text>:2:0: unexpected end of input
>>
>> This error probably comes from calling factor("~") and
>> psych::omegaSem(data) will do that if  all the columns in data are very
>> highly correlated with one another.   In that case omega(data, nfactor=n)
>> will not be able to find n factors in the data but it returns "~" in place
>> of the factors that it could not find.  E.g.,
>> > fakeData <- data.frame(A=1/(1:40), B=1/(2:41), C=1/(3:42), D=1/(4:43),
>> E=1/(5:44))
>> > cor(fakeData)
>>           A         B         C         D         E
>> A 1.0000000 0.9782320 0.9481293 0.9215071 0.8988962
>> B 0.9782320 1.0000000 0.9932037 0.9811287 0.9684658
>> C 0.9481293 0.9932037 1.0000000 0.9969157 0.9906838
>> D 0.9215071 0.9811287 0.9969157 1.0000000 0.9983014
>> E 0.8988962 0.9684658 0.9906838 0.9983014 1.0000000
>> > psych::omegaSem(fakeData)
>> Loading required namespace: lavaan
>> Loading required namespace: GPArotation
>> In factor.stats, I could not find the RMSEA upper bound . Sorry about that
>> Error in parse(text = x, keep.source = FALSE) :
>>   <text>:2:0: unexpected end of input
>> 1: ~
>>    ^
>> In addition: Warning message:
>> In cov2cor(t(w) %*% r %*% w) :
>>   diag(.) had 0 or NA entries; non-finite result is doubtful
>> > psych::omega(fakeData)$model$lavaan
>> In factor.stats, I could not find the RMSEA upper bound . Sorry about that
>> [1] g =~ +A+B+C+D+E       F1=~  + B + C + D + E F2=~  + A
>> [4] F3=~
>> Warning message:
>> In cov2cor(t(w) %*% r %*% w) :
>>   diag(.) had 0 or NA entries; non-finite result is doubtful
>>
>> You can get a result if you use nfactors=n where n is the number of the
>> good F<n> entries in psych::omega()$model$lavaan:
>> > psych::omegaSem(fakeData, nfactors=2)
>> ...
>>
>> Measures of factor score adequacy
>>                                                    g    F1*      F2*
>> Correlation of scores with factors             11.35  12.42    84.45
>> Multiple R square of scores with factors      128.93 154.32  7131.98
>> Minimum correlation of factor score estimates 256.86 307.64 14262.96
>> ...
>> Does that work with your data?
>>
>> This is a problem that the maintainer of psych,
>> >   maintainer("psych")
>> [1] "William Revelle <revelle at northwestern.edu>"
>> would like to know about.
>>
>>
>>
>>
>>
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>>
>> On Thu, Aug 29, 2019 at 9:03 AM Danilo Esteban Rodriguez Zapata via
>> R-help <r-help at r-project.org> wrote:
>>
>>> This is a problem related to my last question referred to the omegaSem()
>>> function in the psych package (that is already solved because I realized
>>> that I was missing a variable assignment and because of that I had an
>>> 'object not found' error:
>>>
>>>
>>> https://stackoverflow.com/questions/57661750/one-of-the-omegasem-function-arguments-is-an-object-not-found
>>>
>>> I was trying to use that function following the guide to find McDonald's
>>> hierarchical Omega by Dr William Revelle:
>>>
>>> http://personality-project.org/r/psych/HowTo/omega.pdf
>>>
>>> So now, with the variable error corrected, I'm having a different error
>>> that does not occur when I use the same function with the example
>>> database
>>> (Thurstone) provided in the tutorial that comes with the psych package. I
>>> mean, I'm able to use the function succesfully using the Thurstone data
>>> (with no other action, I have the expected result) but the function
>>> doesn't
>>> work when I use my own data.
>>>
>>> I searched over other posted questions, and the actions that they perform
>>> are not even similar to what I'm trying to do. I have almost two weeks
>>> using R, so I'm not able to identify yet how can I extrapolate the
>>> solutions for that error message to my procedure (because it seems to be
>>> frequent), although I have basic code knowledge. However related
>>> questions
>>> give no anwer by now.
>>>
>>> Additionally, I decided to look over more documentation about the
>>> package,
>>> and when I was testing other functions, I was able to use the omegaSem()
>>> function with another example database, BUT after and only after I did
>>> the
>>> schmid transformation. So with that, I discovered that when I tried to
>>> use
>>> the omegaSem() function before the schmid tranformation I had the same
>>> error message, but not after that tranformation with this second example
>>> database.
>>>
>>> This make sense with the actual procedure of the omegaSem() procedure,
>>> but
>>> I'm suposing that it must be done completely and automatically by the
>>> omegaSem() function as it is explained in the guide and I have understood
>>> until now, as it follows:
>>>
>>> 1. omegaSem() applies factor analysis
>>> 2. omegaSem() rotate factors obliquely
>>> 3. omegaSem() transform data with Schmid Leiman (schmid)
>>>
>>> -------necessary steps to print output-------------------
>>>
>>> 4. omegaSem() print McDonald's hierarchical Omega
>>>
>>> So here, another questions appears:  - Why the omegaSem() function works
>>> with the Thurstone database without any other action and only works for
>>> the
>>> second example database after performing the schmid transformation? -
>>> Why
>>> with other databases I dont have the same output applying the omegaSem()
>>> function directly? - How is this related to the error message that the
>>> compiler shows when I try to apply the function directly to the database?
>>>
>>>
>>> This is the code that I'm using now: (example of the succesfull
>>> omegaSem()
>>> done after schmid tranformation not included)
>>>
>>> ```
>>> > library(psych)
>>> > library(ctv, lavaan)
>>> > library(GPArotation)
>>> > my.data <- read.file()
>>> Data from the .csv file
>>> D:\Users\Admon\Documents\prueba_export_1563806208742.csv has been loaded.
>>> > describe(my.data)
>>>            vars   n mean   sd median trimmed  mad min max range  skew
>>> kurtosis
>>> AUT_10_04     1 195 4.11 0.90      4    4.23 1.48   1   5     4 -0.92
>>> 0.33
>>> AUN_07_01     2 195 3.79 1.14      4    3.90 1.48   1   5     4 -0.59
>>>  -0.71
>>> AUN_07_02     3 195 3.58 1.08      4    3.65 1.48   1   5     4 -0.39
>>>  -0.56
>>> AUN_09_01     4 195 4.15 0.80      4    4.23 1.48   1   5     4 -0.76
>>> 0.51
>>> AUN_10_01     5 195 4.25 0.79      4    4.34 1.48   1   5     4 -0.91
>>> 0.74
>>> AUT_11_01     6 195 4.43 0.77      5    4.56 0.00   1   5     4 -1.69
>>> 3.77
>>> AUT_17_01     7 195 4.46 0.67      5    4.55 0.00   1   5     4 -1.34
>>> 2.96
>>> AUT_20_03     8 195 4.44 0.65      5    4.53 0.00   2   5     3 -0.84
>>> 0.12
>>> CRE_05_02     9 195 2.47 1.01      2    2.43 1.48   1   5     4  0.35
>>>  -0.46
>>> CRE_07_04    10 195 2.42 1.08      2    2.34 1.48   1   5     4  0.51
>>>  -0.43
>>> CRE_10_01    11 195 4.41 0.68      5    4.51 0.00   2   5     3 -0.79
>>>  -0.12
>>> CRE_16_02    12 195 2.75 1.23      3    2.69 1.48   1   5     4  0.29
>>>  -0.96
>>> EFEC_03_07   13 195 4.35 0.69      4    4.45 1.48   1   5     4 -0.95
>>> 1.59
>>> EFEC_05      14 195 4.53 0.59      5    4.60 0.00   3   5     2 -0.82
>>>  -0.34
>>> EFEC_09_02   15 195 2.19 0.91      2    2.11 1.48   1   5     4  0.57
>>>  -0.03
>>> EFEC_16_03   16 195 4.21 0.77      4    4.29 1.48   2   5     3 -0.71
>>>  -0.04
>>> EVA_02_01    17 195 4.47 0.61      5    4.54 0.00   3   5     2 -0.70
>>>  -0.50
>>> EVA_07_01    18 195 4.38 0.60      4    4.43 1.48   3   5     2 -0.40
>>>  -0.70
>>> EVA_12_02    19 195 2.64 1.22      2    2.59 1.48   1   5     4  0.30
>>>  -1.00
>>> EVA_15_06    20 195 4.19 0.74      4    4.26 1.48   2   5     3 -0.55
>>>  -0.29
>>> FLX_04_01    21 195 4.32 0.69      4    4.41 1.48   2   5     3 -0.71
>>> 0.05
>>> FLX_04_05    22 195 4.23 0.74      4    4.32 0.00   1   5     4 -0.99
>>> 1.69
>>> FLX_08_02    23 195 2.87 1.19      3    2.86 1.48   1   5     4  0.07
>>>  -1.05
>>> FLX_10_03    24 195 4.30 0.71      4    4.39 1.48   2   5     3 -0.84
>>> 0.66
>>> IDO_01_06    25 195 3.10 1.26      3    3.13 1.48   1   5     4 -0.19
>>>  -1.08
>>> IDO_05_02    26 195 2.89 1.26      3    2.87 1.48   1   5     4 -0.03
>>>  -1.16
>>> IDO_09_03    27 195 3.87 0.97      4    3.99 1.48   1   5     4 -0.84
>>> 0.47
>>> IDO_17_01    28 195 3.94 0.88      4    4.02 0.00   1   5     4 -0.93
>>> 1.23
>>> IE_01_03     29 195 4.01 0.88      4    4.10 1.48   1   5     4 -0.91
>>> 0.94
>>> IE_10_03     30 195 4.15 1.00      4    4.34 1.48   1   5     4 -1.31
>>> 1.28
>>> IE_13_03     31 195 4.16 0.91      4    4.30 1.48   1   5     4 -1.26
>>> 1.74
>>> IE_15_01     32 195 4.26 0.85      4    4.39 1.48   1   5     4 -1.16
>>> 1.08
>>> LC_07_03     33 195 4.25 0.72      4    4.34 0.00   1   5     4 -1.07
>>> 2.64
>>> LC_08_02     34 195 3.25 1.22      4    3.31 1.48   1   5     4 -0.41
>>>  -0.90
>>> LC_11_03     35 195 3.50 1.14      4    3.56 1.48   1   5     4 -0.38
>>>  -0.68
>>> LC_11_05     36 195 4.42 0.69      5    4.52 0.00   1   5     4 -1.14
>>> 1.97
>>> ME_02_03     37 195 4.11 0.92      4    4.25 1.48   1   5     4 -1.18
>>> 1.29
>>> ME_07_06     38 195 3.19 1.28      3    3.24 1.48   1   5     4 -0.28
>>>  -1.03
>>> ME_09_01     39 195 4.24 0.77      4    4.34 1.48   1   5     4 -1.12
>>> 2.19
>>> ME_09_06     40 195 3.23 1.33      4    3.29 1.48   1   5     4 -0.31
>>>  -1.14
>>> NEG_01_03    41 195 4.18 0.76      4    4.27 0.00   1   5     4 -1.28
>>> 3.33
>>> NEG_05_04    42 195 4.27 0.69      4    4.35 0.00   1   5     4 -0.87
>>> 1.75
>>> NEG_07_03    43 195 4.32 0.73      4    4.43 1.48   1   5     4 -1.05
>>> 1.55
>>> NEG_08_01    44 195 3.95 0.88      4    4.02 1.48   1   5     4 -0.67
>>> 0.29
>>> OP_03_05     45 195 4.32 0.66      4    4.39 0.00   1   5     4 -0.99
>>> 2.54
>>> OP_12_01     46 195 4.16 0.80      4    4.25 1.48   1   5     4 -1.02
>>> 1.57
>>> OP_14_01     47 195 4.27 0.78      4    4.38 1.48   1   5     4 -1.15
>>> 1.67
>>> OP_14_02     48 195 4.36 0.68      4    4.44 1.48   1   5     4 -1.07
>>> 2.35
>>> ORL_01_03    49 195 4.36 0.77      4    4.49 1.48   1   5     4 -1.31
>>> 2.08
>>> ORL_03_01    50 195 4.41 0.69      4    4.50 1.48   1   5     4 -1.28
>>> 2.77
>>> ORL_03_05    51 195 4.36 0.74      4    4.48 1.48   2   5     3 -1.13
>>> 1.28
>>> ORL_10_05    52 195 4.40 0.68      4    4.48 1.48   1   5     4 -1.18
>>> 2.57
>>> PER_08_02    53 195 3.23 1.29      4    3.29 1.48   1   5     4 -0.26
>>>  -1.17
>>> PER_16_01    54 195 4.29 0.70      4    4.38 1.48   2   5     3 -0.74
>>> 0.27
>>> PER_19_06    55 195 3.19 1.25      3    3.24 1.48   1   5     4 -0.20
>>>  -1.06
>>> PER_22_06    56 195 4.21 0.73      4    4.29 0.00   1   5     4 -0.89
>>> 1.46
>>> PLA_01_03    57 195 4.23 0.68      4    4.31 0.00   2   5     3 -0.81
>>> 1.18
>>> PLA_05_01    58 195 4.06 0.77      4    4.13 0.00   1   5     4 -0.89
>>> 1.29
>>> PLA_07_02    59 195 2.94 1.19      3    2.94 1.48   1   5     4  0.00
>>>  -1.02
>>> PLA_10_01    60 195 4.03 0.76      4    4.08 0.00   1   5     4 -0.68
>>> 0.87
>>> PLA_12_02    61 195 2.67 1.11      2    2.62 1.48   1   5     4  0.41
>>>  -0.61
>>> PLA_18_01    62 195 4.01 0.85      4    4.09 1.48   1   5     4 -0.82
>>> 0.78
>>> PR_06_02     63 195 3.02 1.27      3    3.02 1.48   1   5     4 -0.01
>>>  -1.13
>>> PR_15_03     64 195 3.55 1.07      4    3.62 1.48   1   5     4 -0.46
>>>  -0.22
>>> PR_25_01     65 195 2.36 1.04      2    2.27 1.48   1   5     4  0.73
>>> 0.06
>>> PR_25_06     66 195 2.95 1.17      3    2.94 1.48   1   5     4  0.04
>>>  -0.86
>>> REL_09_05    67 195 3.81 0.95      4    3.89 1.48   1   5     4 -0.51
>>>  -0.31
>>> REL_14_03    68 195 3.99 0.88      4    4.08 1.48   1   5     4 -0.75
>>> 0.39
>>> REL_14_06    69 195 2.93 1.26      3    2.92 1.48   1   5     4  0.06
>>>  -1.11
>>> REL_16_04    70 195 3.16 1.27      3    3.20 1.48   1   5     4 -0.13
>>>  -1.11
>>> RS_02_03     71 195 4.14 0.75      4    4.22 0.00   1   5     4 -0.82
>>> 1.14
>>> RS_07_05     72 195 4.29 0.67      4    4.38 0.00   2   5     3 -0.72
>>> 0.59
>>> RS_08_05     73 195 4.04 0.88      4    4.13 1.48   1   5     4 -0.97
>>> 1.26
>>> RS_13_03     74 195 4.19 0.69      4    4.25 0.00   2   5     3 -0.46
>>>  -0.17
>>> TF_03_01     75 195 4.01 0.82      4    4.06 1.48   1   5     4 -0.63
>>> 0.32
>>> TF_04_01     76 195 4.09 0.76      4    4.15 0.00   1   5     4 -0.70
>>> 0.76
>>> TF_10_03     77 195 4.11 0.85      4    4.21 1.48   1   5     4 -0.96
>>> 0.99
>>> TF_12_01     78 195 4.11 0.85      4    4.21 1.48   1   5     4 -1.10
>>> 1.66
>>> TRE_09_05    79 195 4.29 0.79      4    4.39 1.48   1   5     4 -1.12
>>> 1.74
>>> TRE_09_06    80 195 4.33 0.69      4    4.42 1.48   1   5     4 -1.10
>>> 2.36
>>> TRE_26_04    81 195 2.97 1.20      3    2.96 1.48   1   5     4  0.08
>>>  -1.01
>>> TRE_26_05    82 195 3.99 0.84      4    4.03 1.48   1   5     4 -0.41
>>>  -0.37
>>>
>>> ```
>>>
>>> Until now, I have charged the libraries, import the my own database and
>>> did
>>> some simple descriptive statistics.
>>>
>>> ```
>>>
>>> > r9 <- my.data
>>> > omega(r9)
>>> Omega
>>> Call: omega(m = r9)
>>> Alpha:                 0.95
>>> G.6:                   0.98
>>> Omega Hierarchical:    0.85
>>> Omega H asymptotic:    0.89
>>> Omega Total            0.96
>>>
>>> Schmid Leiman Factor loadings greater than  0.2
>>>                 g   F1*   F2*   F3*   h2   u2   p2
>>> AUT_10_04    0.43              0.30 0.27 0.73 0.68
>>> AUN_07_01                           0.05 0.95 0.53
>>> AUN_07_02                           0.06 0.94 0.26
>>> AUN_09_01    0.38              0.30 0.24 0.76 0.59
>>> AUN_10_01    0.35              0.55 0.44 0.56 0.29
>>> AUT_11_01    0.42              0.30 0.27 0.73 0.66
>>> AUT_17_01    0.32              0.40 0.28 0.72 0.37
>>> AUT_20_03    0.41              0.25 0.24 0.76 0.73
>>> CRE_05_02-   0.24       -0.53       0.34 0.66 0.17
>>> CRE_07_04-   0.37       -0.51       0.39 0.61 0.35
>>> CRE_10_01    0.46              0.48 0.46 0.54 0.47
>>> CRE_16_02-              -0.70       0.48 0.52 0.01
>>> EFEC_03_07   0.46              0.31 0.31 0.69 0.68
>>> EFEC_05      0.43              0.32 0.29 0.71 0.64
>>> EFEC_09_02-  0.29       -0.46       0.29 0.71 0.28
>>> EFEC_16_03   0.49              0.26 0.31 0.69 0.77
>>> EVA_02_01    0.55              0.21 0.36 0.64 0.85
>>> EVA_07_01    0.57                   0.37 0.63 0.89
>>> EVA_12_02-              -0.61       0.39 0.61 0.06
>>> EVA_15_06    0.50              0.37 0.39 0.61 0.65
>>> FLX_04_01    0.57              0.30 0.42 0.58 0.78
>>> FLX_04_05    0.52              0.26 0.34 0.66 0.80
>>> FLX_08_02-              -0.78       0.60 0.40 0.00
>>> FLX_10_03    0.39              0.29 0.24 0.76 0.63
>>> IDO_01_06-              -0.80       0.64 0.36 0.00
>>> IDO_05_02-              -0.78       0.62 0.38 0.00
>>> IDO_09_03    0.41              0.49 0.42 0.58 0.40
>>> IDO_17_01    0.51              0.51 0.54 0.46 0.49
>>> IE_01_03     0.44              0.60 0.56 0.44 0.35
>>> IE_10_03     0.41              0.53 0.44 0.56 0.37
>>> IE_13_03     0.39              0.48 0.38 0.62 0.40
>>> IE_15_01     0.39              0.40 0.31 0.69 0.49
>>> LC_07_03     0.50                   0.27 0.73 0.91
>>> LC_08_02                 0.83       0.69 0.31 0.00
>>> LC_11_03     0.25                   0.10 0.90 0.60
>>> LC_11_05     0.45        0.24       0.27 0.73 0.75
>>> ME_02_03     0.55                   0.31 0.69 0.99
>>> ME_07_06                 0.85       0.75 0.25 0.02
>>> ME_09_01     0.64                   0.45 0.55 0.93
>>> ME_09_06                 0.81       0.69 0.31 0.02
>>> NEG_01_03    0.58              0.20 0.38 0.62 0.88
>>> NEG_05_04    0.70                   0.50 0.50 0.98
>>> NEG_07_03    0.64                   0.43 0.57 0.96
>>> NEG_08_01    0.43              0.25 0.25 0.75 0.74
>>> OP_03_05     0.62                   0.40 0.60 0.98
>>> OP_12_01     0.67                   0.46 0.54 0.98
>>> OP_14_01     0.60                   0.38 0.62 0.95
>>> OP_14_02     0.66                   0.47 0.53 0.93
>>> ORL_01_03    0.67                   0.47 0.53 0.96
>>> ORL_03_01    0.66                   0.48 0.52 0.91
>>> ORL_03_05    0.64                   0.46 0.54 0.90
>>> ORL_10_05    0.66                   0.49 0.51 0.89
>>> PER_08_02    0.21        0.84       0.75 0.25 0.06
>>> PER_16_01    0.68              0.21 0.50 0.50 0.91
>>> PER_19_06    0.20        0.73       0.58 0.42 0.07
>>> PER_22_06    0.53                   0.30 0.70 0.94
>>> PLA_01_03    0.57                   0.36 0.64 0.89
>>> PLA_05_01    0.61                   0.42 0.58 0.89
>>> PLA_07_02                0.75       0.61 0.39 0.04
>>> PLA_10_01    0.56                   0.36 0.64 0.88
>>> PLA_12_02                0.61       0.37 0.63 0.00
>>> PLA_18_01    0.63                   0.47 0.53 0.85
>>> PR_06_02                 0.77       0.62 0.38 0.03
>>> PR_15_03     0.31       -0.39  0.24 0.31 0.69 0.31
>>> PR_25_01-               -0.56       0.32 0.68 0.00
>>> PR_25_06                 0.74       0.55 0.45 0.01
>>> REL_09_05    0.41       -0.23  0.38 0.37 0.63 0.45
>>> REL_14_03    0.41       -0.21  0.29 0.30 0.70 0.56
>>> REL_14_06                0.66  0.21 0.48 0.52 0.04
>>> REL_16_04                0.78       0.63 0.37 0.03
>>> RS_02_03     0.57                   0.36 0.64 0.90
>>> RS_07_05     0.68                   0.47 0.53 0.99
>>> RS_08_05     0.44                   0.20 0.80 0.95
>>> RS_13_03     0.67                   0.46 0.54 0.97
>>> TF_03_01     0.66                   0.44 0.56 0.98
>>> TF_04_01     0.74                   0.56 0.44 0.98
>>> TF_10_03     0.70                   0.50 0.50 0.98
>>> TF_12_01     0.61                   0.40 0.60 0.92
>>> TRE_09_05    0.70              0.23 0.55 0.45 0.89
>>> TRE_09_06    0.62                   0.41 0.59 0.93
>>> TRE_26_04-              -0.68       0.47 0.53 0.00
>>> TRE_26_05    0.55       -0.21       0.34 0.66 0.88
>>>
>>> With eigenvalues of:
>>>     g   F1*   F2*   F3*
>>> 18.06  0.04 11.47  4.32
>>>
>>> general/max  1.57   max/min =   267.1
>>> mean percent general =  0.58    with sd =  0.36 and cv of  0.63
>>> Explained Common Variance of the general factor =  0.53
>>>
>>> The degrees of freedom are 3078  and the fit is  34.62
>>> The number of observations was  195  with Chi Square =  5671.12  with
>>> prob
>>> <  2.8e-157
>>> The root mean square of the residuals is  0.06
>>> The df corrected root mean square of the residuals is  0.06
>>> RMSEA index =  0.078  and the 10 % confidence intervals are  0.063 NA
>>> BIC =  -10559.18
>>>
>>> Compare this with the adequacy of just a general factor and no group
>>> factors
>>> The degrees of freedom for just the general factor are 3239  and the fit
>>> is
>>>  51.52
>>> The number of observations was  195  with Chi Square =  8509.84  with
>>> prob
>>> <  0
>>> The root mean square of the residuals is  0.16
>>> The df corrected root mean square of the residuals is  0.16
>>>
>>> RMSEA index =  0.104  and the 10 % confidence intervals are  0.089 NA
>>> BIC =  -8569.4
>>>
>>> Measures of factor score adequacy
>>>                                                  g   F1*  F2*  F3*
>>> Correlation of scores with factors            0.98  0.07 0.98 0.91
>>> Multiple R square of scores with factors      0.95  0.00 0.97 0.83
>>> Minimum correlation of factor score estimates 0.91 -0.99 0.94 0.66
>>>
>>>  Total, General and Subset omega for each subset
>>>                                                  g F1*  F2*  F3*
>>> Omega total for total scores and subscales    0.96  NA 0.83 0.95
>>> Omega general for total scores and subscales  0.85  NA 0.82 0.76
>>> Omega group for total scores and subscales    0.09  NA 0.01 0.19
>>> ```
>>>
>>> Now, until here, I apply the basic (non hierarchical) omega() function to
>>> my own database
>>>
>>>
>>> ```
>>> > omegaSem(r9,n.obs=198)
>>> Error in parse(text = x, keep.source = FALSE) :
>>>   <text>:2:0: unexpected end of input
>>> 1: ~
>>> ```
>>> The previous is the error message that appears after trying to use the
>>> omegaSem() function directly with my own database.
>>>
>>> Now, following, I present the expected output of omegaSem() applied
>>> directly using the Thurstone database. It's similar to the output of the
>>> basic omega() function but it has certain distinctions:
>>>
>>> ```
>>>
>>> > r9 <- Thurstone
>>> > omegaSem(r9,n.obs=500)
>>>
>>> Call: omegaSem(m = r9, n.obs = 500)
>>> Omega
>>> Call: omega(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip,
>>>     digits = digits, title = title, sl = sl, labels = labels,
>>>     plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option =
>>> option)
>>> Alpha:                 0.89
>>> G.6:                   0.91
>>> Omega Hierarchical:    0.74
>>> Omega H asymptotic:    0.79
>>> Omega Total            0.93
>>>
>>> Schmid Leiman Factor loadings greater than  0.2
>>>                      g   F1*   F2*   F3*   h2   u2   p2
>>> Sentences         0.71  0.56             0.82 0.18 0.61
>>> Vocabulary        0.73  0.55             0.84 0.16 0.63
>>> Sent.Completion   0.68  0.52             0.74 0.26 0.63
>>> First.Letters     0.65        0.56       0.73 0.27 0.57
>>> Four.Letter.Words 0.62        0.49       0.63 0.37 0.61
>>> Suffixes          0.56        0.41       0.50 0.50 0.63
>>> Letter.Series     0.59              0.62 0.73 0.27 0.48
>>> Pedigrees         0.58  0.24        0.34 0.51 0.49 0.66
>>> Letter.Group      0.54              0.46 0.52 0.48 0.56
>>>
>>> With eigenvalues of:
>>>    g  F1*  F2*  F3*
>>> 3.58 0.96 0.74 0.72
>>>
>>> general/max  3.73   max/min =   1.34
>>> mean percent general =  0.6    with sd =  0.05 and cv of  0.09
>>> Explained Common Variance of the general factor =  0.6
>>>
>>> The degrees of freedom are 12  and the fit is  0.01
>>> The number of observations was  500  with Chi Square =  7.12  with prob <
>>>  0.85
>>> The root mean square of the residuals is  0.01
>>> The df corrected root mean square of the residuals is  0.01
>>> RMSEA index =  0  and the 10 % confidence intervals are  0 0.026
>>> BIC =  -67.45
>>>
>>> Compare this with the adequacy of just a general factor and no group
>>> factors
>>> The degrees of freedom for just the general factor are 27  and the fit is
>>>  1.48
>>> The number of observations was  500  with Chi Square =  730.93  with
>>> prob <
>>>  1.3e-136
>>> The root mean square of the residuals is  0.14
>>> The df corrected root mean square of the residuals is  0.16
>>>
>>> RMSEA index =  0.23  and the 10 % confidence intervals are  0.214 0.243
>>> BIC =  563.14
>>>
>>> Measures of factor score adequacy
>>>                                                  g  F1*  F2*  F3*
>>> Correlation of scores with factors            0.86 0.73 0.72 0.75
>>> Multiple R square of scores with factors      0.74 0.54 0.51 0.57
>>> Minimum correlation of factor score estimates 0.49 0.07 0.03 0.13
>>>
>>>  Total, General and Subset omega for each subset
>>>                                                  g  F1*  F2*  F3*
>>> Omega total for total scores and subscales    0.93 0.92 0.83 0.79
>>> Omega general for total scores and subscales  0.74 0.58 0.50 0.47
>>> Omega group for total scores and subscales    0.16 0.34 0.32 0.32
>>>
>>>  The following analyses were done using the  lavaan  package
>>>
>>>  Omega Hierarchical from a confirmatory model using sem =  0.79
>>>  Omega Total  from a confirmatory model using sem =  0.93
>>> With loadings of
>>>                      g  F1*  F2*  F3*   h2   u2   p2
>>> Sentences         0.77 0.49           0.83 0.17 0.71
>>> Vocabulary        0.79 0.45           0.83 0.17 0.75
>>> Sent.Completion   0.75 0.40           0.73 0.27 0.77
>>> First.Letters     0.61      0.61      0.75 0.25 0.50
>>> Four.Letter.Words 0.60      0.51      0.61 0.39 0.59
>>> Suffixes          0.57      0.39      0.48 0.52 0.68
>>> Letter.Series     0.57           0.73 0.85 0.15 0.38
>>> Pedigrees         0.66           0.25 0.50 0.50 0.87
>>> Letter.Group      0.53           0.41 0.45 0.55 0.62
>>>
>>> With eigenvalues of:
>>>    g  F1*  F2*  F3*
>>> 3.87 0.60 0.79 0.76
>>>
>>> The degrees of freedom of the confimatory model are  18  and the fit is
>>>  57.11391  with p =  5.936744e-06
>>> general/max  4.92   max/min =   1.3
>>> mean percent general =  0.65    with sd =  0.15 and cv of  0.23
>>> Explained Common Variance of the general factor =  0.64
>>>
>>> Measures of factor score adequacy
>>>                                                  g   F1*  F2*  F3*
>>> Correlation of scores with factors            0.90  0.68 0.80 0.85
>>> Multiple R square of scores with factors      0.81  0.46 0.64 0.73
>>> Minimum correlation of factor score estimates 0.62 -0.08 0.27 0.45
>>>
>>>  Total, General and Subset omega for each subset
>>>                                                  g  F1*  F2*  F3*
>>> Omega total for total scores and subscales    0.93 0.92 0.82 0.80
>>> Omega general for total scores and subscales  0.79 0.69 0.48 0.50
>>> Omega group for total scores and subscales    0.14 0.23 0.35 0.31
>>>
>>> To get the standard sem fit statistics, ask for summary on the fitted
>>> object>
>>> ```
>>>
>>>
>>>
>>> I'm expecting to have the same output applying the function directly. My
>>> expectation is to make sure if its mandatory to make the schmid
>>> transformation before the omegaSem(). I'm supposing that not, because its
>>> not supposed to work like that as it says in the guide. Maybe this can be
>>> solved correcting the error message:
>>>
>>> ```
>>> > r9 <- my.data
>>> > omegaSem(r9,n.obs=198)
>>> Error in parse(text = x, keep.source = FALSE) :
>>>   <text>:2:0: unexpected end of input
>>> 1: ~
>>>    ^
>>> ```
>>>  Hope I've been clear enough. Feel free to ask any other information that
>>> you might need.
>>>
>>> Thank you so much for giving me any guidance to reach the answer of this
>>> issue. I higly appreciate any help.
>>>
>>> Regards,
>>>
>>> Danilo
>>>
>>> --
>>> Danilo E. Rodr?guez Zapata
>>> Analista en Psicometr?a
>>> CEBIAC
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>
> --
> Danilo E. Rodr?guez Zapata
> Analista en Psicometr?a
> CEBIAC
>


-- 
Danilo E. Rodr?guez Zapata
Analista en Psicometr?a
CEBIAC

	[[alternative HTML version deleted]]


From reve||e @end|ng |rom northwe@tern@edu  Thu Aug 29 21:55:38 2019
From: reve||e @end|ng |rom northwe@tern@edu (William R Revelle)
Date: Thu, 29 Aug 2019 19:55:38 +0000
Subject: [R] R code: How to correct "Error in parse(text = x,
 keep.source = FALSE)" output in psych package using own dataset
In-Reply-To: <CAF8bMcZYOahUPk-a9Ar_xdLDRf0fMpzbSoXcdR9MRRDVPTgWcw@mail.gmail.com>
References: <CAMMrtmZg1HryjK134A8xH73Ba=Pmkbmj5JOLLGbdSYib1PE3qA@mail.gmail.com>
 <CAF8bMcZAajdt0PssTK2xN24fmt3HzEKSvVwUMnW_ZnB30maf=g@mail.gmail.com>
 <CAMMrtmbx5G77RoF7tKqZtApO7XLfEyyu97xLmUHKfJSoR0fTpg@mail.gmail.com>
 <CAMMrtmZYW_-yA0WpQVXWh0ufcDtfJnf6BFnrYPNJ4SYBtnsY=w@mail.gmail.com>
 <CAF8bMcZYOahUPk-a9Ar_xdLDRf0fMpzbSoXcdR9MRRDVPTgWcw@mail.gmail.com>
Message-ID: <1AB3ABC5-C563-4A28-967C-DF7FFE500123@northwestern.edu>

Hi all.

I am taking a brief vacation and will look at this next week.

Bill


> On Aug 29, 2019, at 2:53 PM, William Dunlap <wdunlap at tibco.com> wrote:
> 
> Element #2 of that output,  the empty fomula " F1=~  ", triggers the bug in omegaSem.
> omegaSem needs to ignore such entries in omega's output.  psych's author should be able to fix things up.
> 
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
> 
> 
> On Thu, Aug 29, 2019 at 12:31 PM Danilo Esteban Rodriguez Zapata <danilo_rodriguez at cun.edu.co> wrote:
> well the output with the code that you refer is the following:
> 
> > psych::omega(my.data)$model$lavaan
> [1] g =~ +AUT_10_04+AUN_07_01+AUN_07_02+AUN_09_01+AUN_10_01+AUT_11_01+AUT_17_01+AUT_20_03+CRE_05_02+CRE_07_04+CRE_10_01+CRE_16_02+EFEC_03_07+EFEC_05+EFEC_09_02+EFEC_16_03+EVA_02_01+EVA_07_01+EVA_12_02+EVA_15_06+FLX_04_01+FLX_04_05+FLX_08_02+FLX_10_03+IDO_01_06+IDO_05_02+IDO_09_03+IDO_17_01+IE_01_03+IE_10_03+IE_13_03+IE_15_01+LC_07_03+LC_08_02+LC_11_03+LC_11_05+ME_02_03+ME_07_06+ME_09_01+ME_09_06+NEG_01_03+NEG_05_04+NEG_07_03+NEG_08_01+OP_03_05+OP_12_01+OP_14_01+OP_14_02+ORL_01_03+ORL_03_01+ORL_03_05+ORL_10_05+PER_08_02+PER_16_01+PER_19_06+PER_22_06+PLA_01_03+PLA_05_01+PLA_07_02+PLA_10_01+PLA_12_02+PLA_18_01+PR_06_02+PR_15_03+PR_25_01+PR_25_06+REL_09_05+REL_14_03+REL_14_06+REL_16_04+RS_02_03+RS_07_05+RS_08_05+RS_13_03+TF_03_01+TF_04_01+TF_10_03+TF_12_01+TRE_09_05+TRE_09_06+TRE_26_04+TRE_26_05
> [2] F1=~                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          
> [3] F2=~  + AUN_07_02 + CRE_05_02 + CRE_07_04 + CRE_16_02 + EFEC_09_02 + EVA_12_02 + FLX_08_02 + IDO_01_06 + IDO_05_02 + LC_08_02 + LC_11_03 + LC_11_05 + ME_02_03 + ME_07_06 + ME_09_06 + NEG_07_03 + OP_03_05 + OP_14_01 + OP_14_02 + ORL_01_03 + ORL_03_01 + PER_08_02 + PER_19_06 + PLA_05_01 + PLA_07_02 + PLA_10_01 + PLA_12_02 + PLA_18_01 + PR_06_02 + PR_15_03 + PR_25_01 + PR_25_06 + REL_14_06 + REL_16_04 + TF_04_01 + TF_10_03 + TRE_26_04 + TRE_26_05                                                                                                                                                                                                                                                                                                                                                               
> [4] F3=~  + AUT_10_04 + AUN_07_01 + AUN_09_01 + AUN_10_01 + AUT_11_01 + AUT_17_01 + AUT_20_03 + CRE_10_01 + EFEC_03_07 + EFEC_05 + EFEC_16_03 + EVA_02_01 + EVA_07_01 + EVA_15_06 + FLX_04_01 + FLX_04_05 + FLX_10_03 + IDO_09_03 + IDO_17_01 + IE_01_03 + IE_10_03 + IE_13_03 + IE_15_01 + LC_07_03 + ME_09_01 + NEG_01_03 + NEG_05_04 + NEG_08_01 + OP_12_01 + ORL_03_05 + ORL_10_05 + PER_16_01 + PER_22_06 + PLA_01_03 + REL_09_05 + REL_14_03 + RS_02_03 + RS_07_05 + RS_08_05 + RS_13_03 + TF_03_01 + TF_12_01 + TRE_09_05 + TRE_09_06                                                                                                                                                                                                                                                                                      
> > 
> 
> El jue., 29 ago. 2019 a las 14:29, Danilo Esteban Rodriguez Zapata (<danilo_rodriguez at cun.edu.co>) escribi?:
> Dear William,
> 
> Thank you for your answer, I would like to add some information that I just obtained looking in different sites and forums. Someone there ask me to create a fake data file, so I did that from my original data file. What I did was open the .csv file with notepad and replace all the 4 for 5 and the 2 for 1, then I saved the file again with no other changes. I also searched for the "~" in the file and I found nothing.  Now with that file I did the omegaSem() function and it worked succesfully, so the weird thing here is that the omegaSem() function works with the fake data file, wich is exactly the same as the original file, but recoding some answers as I said. 
> 
> It seems to be an issue with the file. When I replace, lets say, the 5 for 6 and make the omegaSem() again, it works. Then I replace back again the 6 for 5 in all the data and the function doesn't works anymore.
> 
> 
> El jue., 29 ago. 2019 a las 12:33, William Dunlap (<wdunlap at tibco.com>) escribi?:
>     > omegaSem(r9,n.obs=198)
>     Error in parse(text = x, keep.source = FALSE) :
>       <text>:2:0: unexpected end of input
> 
> This error probably comes from calling factor("~") and psych::omegaSem(data) will do that if  all the columns in data are very highly correlated with one another.   In that case omega(data, nfactor=n) will not be able to find n factors in the data but it returns "~" in place of the factors that it could not find.  E.g.,
> > fakeData <- data.frame(A=1/(1:40), B=1/(2:41), C=1/(3:42), D=1/(4:43), E=1/(5:44))
> > cor(fakeData)
>           A         B         C         D         E
> A 1.0000000 0.9782320 0.9481293 0.9215071 0.8988962
> B 0.9782320 1.0000000 0.9932037 0.9811287 0.9684658
> C 0.9481293 0.9932037 1.0000000 0.9969157 0.9906838
> D 0.9215071 0.9811287 0.9969157 1.0000000 0.9983014
> E 0.8988962 0.9684658 0.9906838 0.9983014 1.0000000
> > psych::omegaSem(fakeData)
> Loading required namespace: lavaan
> Loading required namespace: GPArotation
> In factor.stats, I could not find the RMSEA upper bound . Sorry about that
> Error in parse(text = x, keep.source = FALSE) :
>   <text>:2:0: unexpected end of input
> 1: ~
>    ^
> In addition: Warning message:
> In cov2cor(t(w) %*% r %*% w) :
>   diag(.) had 0 or NA entries; non-finite result is doubtful
> > psych::omega(fakeData)$model$lavaan
> In factor.stats, I could not find the RMSEA upper bound . Sorry about that
> [1] g =~ +A+B+C+D+E       F1=~  + B + C + D + E F2=~  + A
> [4] F3=~
> Warning message:
> In cov2cor(t(w) %*% r %*% w) :
>   diag(.) had 0 or NA entries; non-finite result is doubtful
> 
> You can get a result if you use nfactors=n where n is the number of the good F<n> entries in psych::omega()$model$lavaan:
> > psych::omegaSem(fakeData, nfactors=2)
> ...
> 
> Measures of factor score adequacy
>                                                    g    F1*      F2*
> Correlation of scores with factors             11.35  12.42    84.45
> Multiple R square of scores with factors      128.93 154.32  7131.98
> Minimum correlation of factor score estimates 256.86 307.64 14262.96
> ...
> Does that work with your data?
> 
> This is a problem that the maintainer of psych, 
> >   maintainer("psych")
> [1] "William Revelle <revelle at northwestern.edu>"
> would like to know about.
> 
> 
> 
> 
> 
> 
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
> 
> 
> On Thu, Aug 29, 2019 at 9:03 AM Danilo Esteban Rodriguez Zapata via R-help <r-help at r-project.org> wrote:
> This is a problem related to my last question referred to the omegaSem()
> function in the psych package (that is already solved because I realized
> that I was missing a variable assignment and because of that I had an
> 'object not found' error:
> 
> https://stackoverflow.com/questions/57661750/one-of-the-omegasem-function-arguments-is-an-object-not-found
> 
> I was trying to use that function following the guide to find McDonald's
> hierarchical Omega by Dr William Revelle:
> 
> http://personality-project.org/r/psych/HowTo/omega.pdf
> 
> So now, with the variable error corrected, I'm having a different error
> that does not occur when I use the same function with the example database
> (Thurstone) provided in the tutorial that comes with the psych package. I
> mean, I'm able to use the function succesfully using the Thurstone data
> (with no other action, I have the expected result) but the function doesn't
> work when I use my own data.
> 
> I searched over other posted questions, and the actions that they perform
> are not even similar to what I'm trying to do. I have almost two weeks
> using R, so I'm not able to identify yet how can I extrapolate the
> solutions for that error message to my procedure (because it seems to be
> frequent), although I have basic code knowledge. However related questions
> give no anwer by now.
> 
> Additionally, I decided to look over more documentation about the package,
> and when I was testing other functions, I was able to use the omegaSem()
> function with another example database, BUT after and only after I did the
> schmid transformation. So with that, I discovered that when I tried to use
> the omegaSem() function before the schmid tranformation I had the same
> error message, but not after that tranformation with this second example
> database.
> 
> This make sense with the actual procedure of the omegaSem() procedure, but
> I'm suposing that it must be done completely and automatically by the
> omegaSem() function as it is explained in the guide and I have understood
> until now, as it follows:
> 
> 1. omegaSem() applies factor analysis
> 2. omegaSem() rotate factors obliquely
> 3. omegaSem() transform data with Schmid Leiman (schmid)
> 
> -------necessary steps to print output-------------------
> 
> 4. omegaSem() print McDonald's hierarchical Omega
> 
> So here, another questions appears:  - Why the omegaSem() function works
> with the Thurstone database without any other action and only works for the
> second example database after performing the schmid transformation? -  Why
> with other databases I dont have the same output applying the omegaSem()
> function directly? - How is this related to the error message that the
> compiler shows when I try to apply the function directly to the database?
> 
> 
> This is the code that I'm using now: (example of the succesfull omegaSem()
> done after schmid tranformation not included)
> 
> ```
> > library(psych)
> > library(ctv, lavaan)
> > library(GPArotation)
> > my.data <- read.file()
> Data from the .csv file
> D:\Users\Admon\Documents\prueba_export_1563806208742.csv has been loaded.
> > describe(my.data)
>            vars   n mean   sd median trimmed  mad min max range  skew
> kurtosis
> AUT_10_04     1 195 4.11 0.90      4    4.23 1.48   1   5     4 -0.92
> 0.33
> AUN_07_01     2 195 3.79 1.14      4    3.90 1.48   1   5     4 -0.59
>  -0.71
> AUN_07_02     3 195 3.58 1.08      4    3.65 1.48   1   5     4 -0.39
>  -0.56
> AUN_09_01     4 195 4.15 0.80      4    4.23 1.48   1   5     4 -0.76
> 0.51
> AUN_10_01     5 195 4.25 0.79      4    4.34 1.48   1   5     4 -0.91
> 0.74
> AUT_11_01     6 195 4.43 0.77      5    4.56 0.00   1   5     4 -1.69
> 3.77
> AUT_17_01     7 195 4.46 0.67      5    4.55 0.00   1   5     4 -1.34
> 2.96
> AUT_20_03     8 195 4.44 0.65      5    4.53 0.00   2   5     3 -0.84
> 0.12
> CRE_05_02     9 195 2.47 1.01      2    2.43 1.48   1   5     4  0.35
>  -0.46
> CRE_07_04    10 195 2.42 1.08      2    2.34 1.48   1   5     4  0.51
>  -0.43
> CRE_10_01    11 195 4.41 0.68      5    4.51 0.00   2   5     3 -0.79
>  -0.12
> CRE_16_02    12 195 2.75 1.23      3    2.69 1.48   1   5     4  0.29
>  -0.96
> EFEC_03_07   13 195 4.35 0.69      4    4.45 1.48   1   5     4 -0.95
> 1.59
> EFEC_05      14 195 4.53 0.59      5    4.60 0.00   3   5     2 -0.82
>  -0.34
> EFEC_09_02   15 195 2.19 0.91      2    2.11 1.48   1   5     4  0.57
>  -0.03
> EFEC_16_03   16 195 4.21 0.77      4    4.29 1.48   2   5     3 -0.71
>  -0.04
> EVA_02_01    17 195 4.47 0.61      5    4.54 0.00   3   5     2 -0.70
>  -0.50
> EVA_07_01    18 195 4.38 0.60      4    4.43 1.48   3   5     2 -0.40
>  -0.70
> EVA_12_02    19 195 2.64 1.22      2    2.59 1.48   1   5     4  0.30
>  -1.00
> EVA_15_06    20 195 4.19 0.74      4    4.26 1.48   2   5     3 -0.55
>  -0.29
> FLX_04_01    21 195 4.32 0.69      4    4.41 1.48   2   5     3 -0.71
> 0.05
> FLX_04_05    22 195 4.23 0.74      4    4.32 0.00   1   5     4 -0.99
> 1.69
> FLX_08_02    23 195 2.87 1.19      3    2.86 1.48   1   5     4  0.07
>  -1.05
> FLX_10_03    24 195 4.30 0.71      4    4.39 1.48   2   5     3 -0.84
> 0.66
> IDO_01_06    25 195 3.10 1.26      3    3.13 1.48   1   5     4 -0.19
>  -1.08
> IDO_05_02    26 195 2.89 1.26      3    2.87 1.48   1   5     4 -0.03
>  -1.16
> IDO_09_03    27 195 3.87 0.97      4    3.99 1.48   1   5     4 -0.84
> 0.47
> IDO_17_01    28 195 3.94 0.88      4    4.02 0.00   1   5     4 -0.93
> 1.23
> IE_01_03     29 195 4.01 0.88      4    4.10 1.48   1   5     4 -0.91
> 0.94
> IE_10_03     30 195 4.15 1.00      4    4.34 1.48   1   5     4 -1.31
> 1.28
> IE_13_03     31 195 4.16 0.91      4    4.30 1.48   1   5     4 -1.26
> 1.74
> IE_15_01     32 195 4.26 0.85      4    4.39 1.48   1   5     4 -1.16
> 1.08
> LC_07_03     33 195 4.25 0.72      4    4.34 0.00   1   5     4 -1.07
> 2.64
> LC_08_02     34 195 3.25 1.22      4    3.31 1.48   1   5     4 -0.41
>  -0.90
> LC_11_03     35 195 3.50 1.14      4    3.56 1.48   1   5     4 -0.38
>  -0.68
> LC_11_05     36 195 4.42 0.69      5    4.52 0.00   1   5     4 -1.14
> 1.97
> ME_02_03     37 195 4.11 0.92      4    4.25 1.48   1   5     4 -1.18
> 1.29
> ME_07_06     38 195 3.19 1.28      3    3.24 1.48   1   5     4 -0.28
>  -1.03
> ME_09_01     39 195 4.24 0.77      4    4.34 1.48   1   5     4 -1.12
> 2.19
> ME_09_06     40 195 3.23 1.33      4    3.29 1.48   1   5     4 -0.31
>  -1.14
> NEG_01_03    41 195 4.18 0.76      4    4.27 0.00   1   5     4 -1.28
> 3.33
> NEG_05_04    42 195 4.27 0.69      4    4.35 0.00   1   5     4 -0.87
> 1.75
> NEG_07_03    43 195 4.32 0.73      4    4.43 1.48   1   5     4 -1.05
> 1.55
> NEG_08_01    44 195 3.95 0.88      4    4.02 1.48   1   5     4 -0.67
> 0.29
> OP_03_05     45 195 4.32 0.66      4    4.39 0.00   1   5     4 -0.99
> 2.54
> OP_12_01     46 195 4.16 0.80      4    4.25 1.48   1   5     4 -1.02
> 1.57
> OP_14_01     47 195 4.27 0.78      4    4.38 1.48   1   5     4 -1.15
> 1.67
> OP_14_02     48 195 4.36 0.68      4    4.44 1.48   1   5     4 -1.07
> 2.35
> ORL_01_03    49 195 4.36 0.77      4    4.49 1.48   1   5     4 -1.31
> 2.08
> ORL_03_01    50 195 4.41 0.69      4    4.50 1.48   1   5     4 -1.28
> 2.77
> ORL_03_05    51 195 4.36 0.74      4    4.48 1.48   2   5     3 -1.13
> 1.28
> ORL_10_05    52 195 4.40 0.68      4    4.48 1.48   1   5     4 -1.18
> 2.57
> PER_08_02    53 195 3.23 1.29      4    3.29 1.48   1   5     4 -0.26
>  -1.17
> PER_16_01    54 195 4.29 0.70      4    4.38 1.48   2   5     3 -0.74
> 0.27
> PER_19_06    55 195 3.19 1.25      3    3.24 1.48   1   5     4 -0.20
>  -1.06
> PER_22_06    56 195 4.21 0.73      4    4.29 0.00   1   5     4 -0.89
> 1.46
> PLA_01_03    57 195 4.23 0.68      4    4.31 0.00   2   5     3 -0.81
> 1.18
> PLA_05_01    58 195 4.06 0.77      4    4.13 0.00   1   5     4 -0.89
> 1.29
> PLA_07_02    59 195 2.94 1.19      3    2.94 1.48   1   5     4  0.00
>  -1.02
> PLA_10_01    60 195 4.03 0.76      4    4.08 0.00   1   5     4 -0.68
> 0.87
> PLA_12_02    61 195 2.67 1.11      2    2.62 1.48   1   5     4  0.41
>  -0.61
> PLA_18_01    62 195 4.01 0.85      4    4.09 1.48   1   5     4 -0.82
> 0.78
> PR_06_02     63 195 3.02 1.27      3    3.02 1.48   1   5     4 -0.01
>  -1.13
> PR_15_03     64 195 3.55 1.07      4    3.62 1.48   1   5     4 -0.46
>  -0.22
> PR_25_01     65 195 2.36 1.04      2    2.27 1.48   1   5     4  0.73
> 0.06
> PR_25_06     66 195 2.95 1.17      3    2.94 1.48   1   5     4  0.04
>  -0.86
> REL_09_05    67 195 3.81 0.95      4    3.89 1.48   1   5     4 -0.51
>  -0.31
> REL_14_03    68 195 3.99 0.88      4    4.08 1.48   1   5     4 -0.75
> 0.39
> REL_14_06    69 195 2.93 1.26      3    2.92 1.48   1   5     4  0.06
>  -1.11
> REL_16_04    70 195 3.16 1.27      3    3.20 1.48   1   5     4 -0.13
>  -1.11
> RS_02_03     71 195 4.14 0.75      4    4.22 0.00   1   5     4 -0.82
> 1.14
> RS_07_05     72 195 4.29 0.67      4    4.38 0.00   2   5     3 -0.72
> 0.59
> RS_08_05     73 195 4.04 0.88      4    4.13 1.48   1   5     4 -0.97
> 1.26
> RS_13_03     74 195 4.19 0.69      4    4.25 0.00   2   5     3 -0.46
>  -0.17
> TF_03_01     75 195 4.01 0.82      4    4.06 1.48   1   5     4 -0.63
> 0.32
> TF_04_01     76 195 4.09 0.76      4    4.15 0.00   1   5     4 -0.70
> 0.76
> TF_10_03     77 195 4.11 0.85      4    4.21 1.48   1   5     4 -0.96
> 0.99
> TF_12_01     78 195 4.11 0.85      4    4.21 1.48   1   5     4 -1.10
> 1.66
> TRE_09_05    79 195 4.29 0.79      4    4.39 1.48   1   5     4 -1.12
> 1.74
> TRE_09_06    80 195 4.33 0.69      4    4.42 1.48   1   5     4 -1.10
> 2.36
> TRE_26_04    81 195 2.97 1.20      3    2.96 1.48   1   5     4  0.08
>  -1.01
> TRE_26_05    82 195 3.99 0.84      4    4.03 1.48   1   5     4 -0.41
>  -0.37
> 
> ```
> 
> Until now, I have charged the libraries, import the my own database and did
> some simple descriptive statistics.
> 
> ```
> 
> > r9 <- my.data
> > omega(r9)
> Omega
> Call: omega(m = r9)
> Alpha:                 0.95
> G.6:                   0.98
> Omega Hierarchical:    0.85
> Omega H asymptotic:    0.89
> Omega Total            0.96
> 
> Schmid Leiman Factor loadings greater than  0.2
>                 g   F1*   F2*   F3*   h2   u2   p2
> AUT_10_04    0.43              0.30 0.27 0.73 0.68
> AUN_07_01                           0.05 0.95 0.53
> AUN_07_02                           0.06 0.94 0.26
> AUN_09_01    0.38              0.30 0.24 0.76 0.59
> AUN_10_01    0.35              0.55 0.44 0.56 0.29
> AUT_11_01    0.42              0.30 0.27 0.73 0.66
> AUT_17_01    0.32              0.40 0.28 0.72 0.37
> AUT_20_03    0.41              0.25 0.24 0.76 0.73
> CRE_05_02-   0.24       -0.53       0.34 0.66 0.17
> CRE_07_04-   0.37       -0.51       0.39 0.61 0.35
> CRE_10_01    0.46              0.48 0.46 0.54 0.47
> CRE_16_02-              -0.70       0.48 0.52 0.01
> EFEC_03_07   0.46              0.31 0.31 0.69 0.68
> EFEC_05      0.43              0.32 0.29 0.71 0.64
> EFEC_09_02-  0.29       -0.46       0.29 0.71 0.28
> EFEC_16_03   0.49              0.26 0.31 0.69 0.77
> EVA_02_01    0.55              0.21 0.36 0.64 0.85
> EVA_07_01    0.57                   0.37 0.63 0.89
> EVA_12_02-              -0.61       0.39 0.61 0.06
> EVA_15_06    0.50              0.37 0.39 0.61 0.65
> FLX_04_01    0.57              0.30 0.42 0.58 0.78
> FLX_04_05    0.52              0.26 0.34 0.66 0.80
> FLX_08_02-              -0.78       0.60 0.40 0.00
> FLX_10_03    0.39              0.29 0.24 0.76 0.63
> IDO_01_06-              -0.80       0.64 0.36 0.00
> IDO_05_02-              -0.78       0.62 0.38 0.00
> IDO_09_03    0.41              0.49 0.42 0.58 0.40
> IDO_17_01    0.51              0.51 0.54 0.46 0.49
> IE_01_03     0.44              0.60 0.56 0.44 0.35
> IE_10_03     0.41              0.53 0.44 0.56 0.37
> IE_13_03     0.39              0.48 0.38 0.62 0.40
> IE_15_01     0.39              0.40 0.31 0.69 0.49
> LC_07_03     0.50                   0.27 0.73 0.91
> LC_08_02                 0.83       0.69 0.31 0.00
> LC_11_03     0.25                   0.10 0.90 0.60
> LC_11_05     0.45        0.24       0.27 0.73 0.75
> ME_02_03     0.55                   0.31 0.69 0.99
> ME_07_06                 0.85       0.75 0.25 0.02
> ME_09_01     0.64                   0.45 0.55 0.93
> ME_09_06                 0.81       0.69 0.31 0.02
> NEG_01_03    0.58              0.20 0.38 0.62 0.88
> NEG_05_04    0.70                   0.50 0.50 0.98
> NEG_07_03    0.64                   0.43 0.57 0.96
> NEG_08_01    0.43              0.25 0.25 0.75 0.74
> OP_03_05     0.62                   0.40 0.60 0.98
> OP_12_01     0.67                   0.46 0.54 0.98
> OP_14_01     0.60                   0.38 0.62 0.95
> OP_14_02     0.66                   0.47 0.53 0.93
> ORL_01_03    0.67                   0.47 0.53 0.96
> ORL_03_01    0.66                   0.48 0.52 0.91
> ORL_03_05    0.64                   0.46 0.54 0.90
> ORL_10_05    0.66                   0.49 0.51 0.89
> PER_08_02    0.21        0.84       0.75 0.25 0.06
> PER_16_01    0.68              0.21 0.50 0.50 0.91
> PER_19_06    0.20        0.73       0.58 0.42 0.07
> PER_22_06    0.53                   0.30 0.70 0.94
> PLA_01_03    0.57                   0.36 0.64 0.89
> PLA_05_01    0.61                   0.42 0.58 0.89
> PLA_07_02                0.75       0.61 0.39 0.04
> PLA_10_01    0.56                   0.36 0.64 0.88
> PLA_12_02                0.61       0.37 0.63 0.00
> PLA_18_01    0.63                   0.47 0.53 0.85
> PR_06_02                 0.77       0.62 0.38 0.03
> PR_15_03     0.31       -0.39  0.24 0.31 0.69 0.31
> PR_25_01-               -0.56       0.32 0.68 0.00
> PR_25_06                 0.74       0.55 0.45 0.01
> REL_09_05    0.41       -0.23  0.38 0.37 0.63 0.45
> REL_14_03    0.41       -0.21  0.29 0.30 0.70 0.56
> REL_14_06                0.66  0.21 0.48 0.52 0.04
> REL_16_04                0.78       0.63 0.37 0.03
> RS_02_03     0.57                   0.36 0.64 0.90
> RS_07_05     0.68                   0.47 0.53 0.99
> RS_08_05     0.44                   0.20 0.80 0.95
> RS_13_03     0.67                   0.46 0.54 0.97
> TF_03_01     0.66                   0.44 0.56 0.98
> TF_04_01     0.74                   0.56 0.44 0.98
> TF_10_03     0.70                   0.50 0.50 0.98
> TF_12_01     0.61                   0.40 0.60 0.92
> TRE_09_05    0.70              0.23 0.55 0.45 0.89
> TRE_09_06    0.62                   0.41 0.59 0.93
> TRE_26_04-              -0.68       0.47 0.53 0.00
> TRE_26_05    0.55       -0.21       0.34 0.66 0.88
> 
> With eigenvalues of:
>     g   F1*   F2*   F3*
> 18.06  0.04 11.47  4.32
> 
> general/max  1.57   max/min =   267.1
> mean percent general =  0.58    with sd =  0.36 and cv of  0.63
> Explained Common Variance of the general factor =  0.53
> 
> The degrees of freedom are 3078  and the fit is  34.62
> The number of observations was  195  with Chi Square =  5671.12  with prob
> <  2.8e-157
> The root mean square of the residuals is  0.06
> The df corrected root mean square of the residuals is  0.06
> RMSEA index =  0.078  and the 10 % confidence intervals are  0.063 NA
> BIC =  -10559.18
> 
> Compare this with the adequacy of just a general factor and no group factors
> The degrees of freedom for just the general factor are 3239  and the fit is
>  51.52
> The number of observations was  195  with Chi Square =  8509.84  with prob
> <  0
> The root mean square of the residuals is  0.16
> The df corrected root mean square of the residuals is  0.16
> 
> RMSEA index =  0.104  and the 10 % confidence intervals are  0.089 NA
> BIC =  -8569.4
> 
> Measures of factor score adequacy
>                                                  g   F1*  F2*  F3*
> Correlation of scores with factors            0.98  0.07 0.98 0.91
> Multiple R square of scores with factors      0.95  0.00 0.97 0.83
> Minimum correlation of factor score estimates 0.91 -0.99 0.94 0.66
> 
>  Total, General and Subset omega for each subset
>                                                  g F1*  F2*  F3*
> Omega total for total scores and subscales    0.96  NA 0.83 0.95
> Omega general for total scores and subscales  0.85  NA 0.82 0.76
> Omega group for total scores and subscales    0.09  NA 0.01 0.19
> ```
> 
> Now, until here, I apply the basic (non hierarchical) omega() function to
> my own database
> 
> 
> ```
> > omegaSem(r9,n.obs=198)
> Error in parse(text = x, keep.source = FALSE) :
>   <text>:2:0: unexpected end of input
> 1: ~
> ```
> The previous is the error message that appears after trying to use the
> omegaSem() function directly with my own database.
> 
> Now, following, I present the expected output of omegaSem() applied
> directly using the Thurstone database. It's similar to the output of the
> basic omega() function but it has certain distinctions:
> 
> ```
> 
> > r9 <- Thurstone
> > omegaSem(r9,n.obs=500)
> 
> Call: omegaSem(m = r9, n.obs = 500)
> Omega
> Call: omega(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip,
>     digits = digits, title = title, sl = sl, labels = labels,
>     plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option)
> Alpha:                 0.89
> G.6:                   0.91
> Omega Hierarchical:    0.74
> Omega H asymptotic:    0.79
> Omega Total            0.93
> 
> Schmid Leiman Factor loadings greater than  0.2
>                      g   F1*   F2*   F3*   h2   u2   p2
> Sentences         0.71  0.56             0.82 0.18 0.61
> Vocabulary        0.73  0.55             0.84 0.16 0.63
> Sent.Completion   0.68  0.52             0.74 0.26 0.63
> First.Letters     0.65        0.56       0.73 0.27 0.57
> Four.Letter.Words 0.62        0.49       0.63 0.37 0.61
> Suffixes          0.56        0.41       0.50 0.50 0.63
> Letter.Series     0.59              0.62 0.73 0.27 0.48
> Pedigrees         0.58  0.24        0.34 0.51 0.49 0.66
> Letter.Group      0.54              0.46 0.52 0.48 0.56
> 
> With eigenvalues of:
>    g  F1*  F2*  F3*
> 3.58 0.96 0.74 0.72
> 
> general/max  3.73   max/min =   1.34
> mean percent general =  0.6    with sd =  0.05 and cv of  0.09
> Explained Common Variance of the general factor =  0.6
> 
> The degrees of freedom are 12  and the fit is  0.01
> The number of observations was  500  with Chi Square =  7.12  with prob <
>  0.85
> The root mean square of the residuals is  0.01
> The df corrected root mean square of the residuals is  0.01
> RMSEA index =  0  and the 10 % confidence intervals are  0 0.026
> BIC =  -67.45
> 
> Compare this with the adequacy of just a general factor and no group factors
> The degrees of freedom for just the general factor are 27  and the fit is
>  1.48
> The number of observations was  500  with Chi Square =  730.93  with prob <
>  1.3e-136
> The root mean square of the residuals is  0.14
> The df corrected root mean square of the residuals is  0.16
> 
> RMSEA index =  0.23  and the 10 % confidence intervals are  0.214 0.243
> BIC =  563.14
> 
> Measures of factor score adequacy
>                                                  g  F1*  F2*  F3*
> Correlation of scores with factors            0.86 0.73 0.72 0.75
> Multiple R square of scores with factors      0.74 0.54 0.51 0.57
> Minimum correlation of factor score estimates 0.49 0.07 0.03 0.13
> 
>  Total, General and Subset omega for each subset
>                                                  g  F1*  F2*  F3*
> Omega total for total scores and subscales    0.93 0.92 0.83 0.79
> Omega general for total scores and subscales  0.74 0.58 0.50 0.47
> Omega group for total scores and subscales    0.16 0.34 0.32 0.32
> 
>  The following analyses were done using the  lavaan  package
> 
>  Omega Hierarchical from a confirmatory model using sem =  0.79
>  Omega Total  from a confirmatory model using sem =  0.93
> With loadings of
>                      g  F1*  F2*  F3*   h2   u2   p2
> Sentences         0.77 0.49           0.83 0.17 0.71
> Vocabulary        0.79 0.45           0.83 0.17 0.75
> Sent.Completion   0.75 0.40           0.73 0.27 0.77
> First.Letters     0.61      0.61      0.75 0.25 0.50
> Four.Letter.Words 0.60      0.51      0.61 0.39 0.59
> Suffixes          0.57      0.39      0.48 0.52 0.68
> Letter.Series     0.57           0.73 0.85 0.15 0.38
> Pedigrees         0.66           0.25 0.50 0.50 0.87
> Letter.Group      0.53           0.41 0.45 0.55 0.62
> 
> With eigenvalues of:
>    g  F1*  F2*  F3*
> 3.87 0.60 0.79 0.76
> 
> The degrees of freedom of the confimatory model are  18  and the fit is
>  57.11391  with p =  5.936744e-06
> general/max  4.92   max/min =   1.3
> mean percent general =  0.65    with sd =  0.15 and cv of  0.23
> Explained Common Variance of the general factor =  0.64
> 
> Measures of factor score adequacy
>                                                  g   F1*  F2*  F3*
> Correlation of scores with factors            0.90  0.68 0.80 0.85
> Multiple R square of scores with factors      0.81  0.46 0.64 0.73
> Minimum correlation of factor score estimates 0.62 -0.08 0.27 0.45
> 
>  Total, General and Subset omega for each subset
>                                                  g  F1*  F2*  F3*
> Omega total for total scores and subscales    0.93 0.92 0.82 0.80
> Omega general for total scores and subscales  0.79 0.69 0.48 0.50
> Omega group for total scores and subscales    0.14 0.23 0.35 0.31
> 
> To get the standard sem fit statistics, ask for summary on the fitted
> object>
> ```
> 
> 
> 
> I'm expecting to have the same output applying the function directly. My
> expectation is to make sure if its mandatory to make the schmid
> transformation before the omegaSem(). I'm supposing that not, because its
> not supposed to work like that as it says in the guide. Maybe this can be
> solved correcting the error message:
> 
> ```
> > r9 <- my.data
> > omegaSem(r9,n.obs=198)
> Error in parse(text = x, keep.source = FALSE) :
>   <text>:2:0: unexpected end of input
> 1: ~
>    ^
> ```
>  Hope I've been clear enough. Feel free to ask any other information that
> you might need.
> 
> Thank you so much for giving me any guidance to reach the answer of this
> issue. I higly appreciate any help.
> 
> Regards,
> 
> Danilo
> 
> -- 
> Danilo E. Rodr?guez Zapata
> Analista en Psicometr?a
> CEBIAC
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> -- 
> Danilo E. Rodr?guez Zapata
> Analista en Psicometr?a
> CEBIAC
> 
> 
> -- 
> Danilo E. Rodr?guez Zapata
> Analista en Psicometr?a
> CEBIAC

William Revelle		   personality-project.org/revelle.html
Professor			          personality-project.org
Department of Psychology www.wcas.northwestern.edu/psych/
Northwestern University	   www.northwestern.edu/
Use R for psychology         personality-project.org/r
It is 2   minutes to midnight   www.thebulletin.org








From d@n||o_rodr|guez @end|ng |rom cun@edu@co  Thu Aug 29 22:07:53 2019
From: d@n||o_rodr|guez @end|ng |rom cun@edu@co (Danilo Esteban Rodriguez Zapata)
Date: Thu, 29 Aug 2019 15:07:53 -0500
Subject: [R] R code: How to correct "Error in parse(text = x,
 keep.source = FALSE)" output in psych package using own dataset
In-Reply-To: <1AB3ABC5-C563-4A28-967C-DF7FFE500123@northwestern.edu>
References: <CAMMrtmZg1HryjK134A8xH73Ba=Pmkbmj5JOLLGbdSYib1PE3qA@mail.gmail.com>
 <CAF8bMcZAajdt0PssTK2xN24fmt3HzEKSvVwUMnW_ZnB30maf=g@mail.gmail.com>
 <CAMMrtmbx5G77RoF7tKqZtApO7XLfEyyu97xLmUHKfJSoR0fTpg@mail.gmail.com>
 <CAMMrtmZYW_-yA0WpQVXWh0ufcDtfJnf6BFnrYPNJ4SYBtnsY=w@mail.gmail.com>
 <CAF8bMcZYOahUPk-a9Ar_xdLDRf0fMpzbSoXcdR9MRRDVPTgWcw@mail.gmail.com>
 <1AB3ABC5-C563-4A28-967C-DF7FFE500123@northwestern.edu>
Message-ID: <CAMMrtmYr=zkTxOLA1MrF=cSMno_R83X7JrXas7kx8HDUM5VujA@mail.gmail.com>

Thank you so much, I'll wait until then. The good thing is that we can make
sure now what is the actual problem.  I wish you have a good rest.

El jue., 29 ago. 2019 a las 14:55, William R Revelle (<
revelle at northwestern.edu>) escribi?:

> Hi all.
>
> I am taking a brief vacation and will look at this next week.
>
> Bill
>
>
> > On Aug 29, 2019, at 2:53 PM, William Dunlap <wdunlap at tibco.com> wrote:
> >
> > Element #2 of that output,  the empty fomula " F1=~  ", triggers the bug
> in omegaSem.
> > omegaSem needs to ignore such entries in omega's output.  psych's author
> should be able to fix things up.
> >
> > Bill Dunlap
> > TIBCO Software
> > wdunlap tibco.com
> >
> >
> > On Thu, Aug 29, 2019 at 12:31 PM Danilo Esteban Rodriguez Zapata <
> danilo_rodriguez at cun.edu.co> wrote:
> > well the output with the code that you refer is the following:
> >
> > > psych::omega(my.data)$model$lavaan
> > [1] g =~
> +AUT_10_04+AUN_07_01+AUN_07_02+AUN_09_01+AUN_10_01+AUT_11_01+AUT_17_01+AUT_20_03+CRE_05_02+CRE_07_04+CRE_10_01+CRE_16_02+EFEC_03_07+EFEC_05+EFEC_09_02+EFEC_16_03+EVA_02_01+EVA_07_01+EVA_12_02+EVA_15_06+FLX_04_01+FLX_04_05+FLX_08_02+FLX_10_03+IDO_01_06+IDO_05_02+IDO_09_03+IDO_17_01+IE_01_03+IE_10_03+IE_13_03+IE_15_01+LC_07_03+LC_08_02+LC_11_03+LC_11_05+ME_02_03+ME_07_06+ME_09_01+ME_09_06+NEG_01_03+NEG_05_04+NEG_07_03+NEG_08_01+OP_03_05+OP_12_01+OP_14_01+OP_14_02+ORL_01_03+ORL_03_01+ORL_03_05+ORL_10_05+PER_08_02+PER_16_01+PER_19_06+PER_22_06+PLA_01_03+PLA_05_01+PLA_07_02+PLA_10_01+PLA_12_02+PLA_18_01+PR_06_02+PR_15_03+PR_25_01+PR_25_06+REL_09_05+REL_14_03+REL_14_06+REL_16_04+RS_02_03+RS_07_05+RS_08_05+RS_13_03+TF_03_01+TF_04_01+TF_10_03+TF_12_01+TRE_09_05+TRE_09_06+TRE_26_04+TRE_26_05
> > [2] F1=~
>
>
>
>
>
>
>
>
>
>
> > [3] F2=~  + AUN_07_02 + CRE_05_02 + CRE_07_04 + CRE_16_02 + EFEC_09_02 +
> EVA_12_02 + FLX_08_02 + IDO_01_06 + IDO_05_02 + LC_08_02 + LC_11_03 +
> LC_11_05 + ME_02_03 + ME_07_06 + ME_09_06 + NEG_07_03 + OP_03_05 + OP_14_01
> + OP_14_02 + ORL_01_03 + ORL_03_01 + PER_08_02 + PER_19_06 + PLA_05_01 +
> PLA_07_02 + PLA_10_01 + PLA_12_02 + PLA_18_01 + PR_06_02 + PR_15_03 +
> PR_25_01 + PR_25_06 + REL_14_06 + REL_16_04 + TF_04_01 + TF_10_03 +
> TRE_26_04 + TRE_26_05
>
>
>
>
> > [4] F3=~  + AUT_10_04 + AUN_07_01 + AUN_09_01 + AUN_10_01 + AUT_11_01 +
> AUT_17_01 + AUT_20_03 + CRE_10_01 + EFEC_03_07 + EFEC_05 + EFEC_16_03 +
> EVA_02_01 + EVA_07_01 + EVA_15_06 + FLX_04_01 + FLX_04_05 + FLX_10_03 +
> IDO_09_03 + IDO_17_01 + IE_01_03 + IE_10_03 + IE_13_03 + IE_15_01 +
> LC_07_03 + ME_09_01 + NEG_01_03 + NEG_05_04 + NEG_08_01 + OP_12_01 +
> ORL_03_05 + ORL_10_05 + PER_16_01 + PER_22_06 + PLA_01_03 + REL_09_05 +
> REL_14_03 + RS_02_03 + RS_07_05 + RS_08_05 + RS_13_03 + TF_03_01 + TF_12_01
> + TRE_09_05 + TRE_09_06
>
>
>
> > >
> >
> > El jue., 29 ago. 2019 a las 14:29, Danilo Esteban Rodriguez Zapata (<
> danilo_rodriguez at cun.edu.co>) escribi?:
> > Dear William,
> >
> > Thank you for your answer, I would like to add some information that I
> just obtained looking in different sites and forums. Someone there ask me
> to create a fake data file, so I did that from my original data file. What
> I did was open the .csv file with notepad and replace all the 4 for 5 and
> the 2 for 1, then I saved the file again with no other changes. I also
> searched for the "~" in the file and I found nothing.  Now with that file I
> did the omegaSem() function and it worked succesfully, so the weird thing
> here is that the omegaSem() function works with the fake data file, wich is
> exactly the same as the original file, but recoding some answers as I said.
> >
> > It seems to be an issue with the file. When I replace, lets say, the 5
> for 6 and make the omegaSem() again, it works. Then I replace back again
> the 6 for 5 in all the data and the function doesn't works anymore.
> >
> >
> > El jue., 29 ago. 2019 a las 12:33, William Dunlap (<wdunlap at tibco.com>)
> escribi?:
> >     > omegaSem(r9,n.obs=198)
> >     Error in parse(text = x, keep.source = FALSE) :
> >       <text>:2:0: unexpected end of input
> >
> > This error probably comes from calling factor("~") and
> psych::omegaSem(data) will do that if  all the columns in data are very
> highly correlated with one another.   In that case omega(data, nfactor=n)
> will not be able to find n factors in the data but it returns "~" in place
> of the factors that it could not find.  E.g.,
> > > fakeData <- data.frame(A=1/(1:40), B=1/(2:41), C=1/(3:42), D=1/(4:43),
> E=1/(5:44))
> > > cor(fakeData)
> >           A         B         C         D         E
> > A 1.0000000 0.9782320 0.9481293 0.9215071 0.8988962
> > B 0.9782320 1.0000000 0.9932037 0.9811287 0.9684658
> > C 0.9481293 0.9932037 1.0000000 0.9969157 0.9906838
> > D 0.9215071 0.9811287 0.9969157 1.0000000 0.9983014
> > E 0.8988962 0.9684658 0.9906838 0.9983014 1.0000000
> > > psych::omegaSem(fakeData)
> > Loading required namespace: lavaan
> > Loading required namespace: GPArotation
> > In factor.stats, I could not find the RMSEA upper bound . Sorry about
> that
> > Error in parse(text = x, keep.source = FALSE) :
> >   <text>:2:0: unexpected end of input
> > 1: ~
> >    ^
> > In addition: Warning message:
> > In cov2cor(t(w) %*% r %*% w) :
> >   diag(.) had 0 or NA entries; non-finite result is doubtful
> > > psych::omega(fakeData)$model$lavaan
> > In factor.stats, I could not find the RMSEA upper bound . Sorry about
> that
> > [1] g =~ +A+B+C+D+E       F1=~  + B + C + D + E F2=~  + A
> > [4] F3=~
> > Warning message:
> > In cov2cor(t(w) %*% r %*% w) :
> >   diag(.) had 0 or NA entries; non-finite result is doubtful
> >
> > You can get a result if you use nfactors=n where n is the number of the
> good F<n> entries in psych::omega()$model$lavaan:
> > > psych::omegaSem(fakeData, nfactors=2)
> > ...
> >
> > Measures of factor score adequacy
> >                                                    g    F1*      F2*
> > Correlation of scores with factors             11.35  12.42    84.45
> > Multiple R square of scores with factors      128.93 154.32  7131.98
> > Minimum correlation of factor score estimates 256.86 307.64 14262.96
> > ...
> > Does that work with your data?
> >
> > This is a problem that the maintainer of psych,
> > >   maintainer("psych")
> > [1] "William Revelle <revelle at northwestern.edu>"
> > would like to know about.
> >
> >
> >
> >
> >
> >
> > Bill Dunlap
> > TIBCO Software
> > wdunlap tibco.com
> >
> >
> > On Thu, Aug 29, 2019 at 9:03 AM Danilo Esteban Rodriguez Zapata via
> R-help <r-help at r-project.org> wrote:
> > This is a problem related to my last question referred to the omegaSem()
> > function in the psych package (that is already solved because I realized
> > that I was missing a variable assignment and because of that I had an
> > 'object not found' error:
> >
> >
> https://stackoverflow.com/questions/57661750/one-of-the-omegasem-function-arguments-is-an-object-not-found
> >
> > I was trying to use that function following the guide to find McDonald's
> > hierarchical Omega by Dr William Revelle:
> >
> > http://personality-project.org/r/psych/HowTo/omega.pdf
> >
> > So now, with the variable error corrected, I'm having a different error
> > that does not occur when I use the same function with the example
> database
> > (Thurstone) provided in the tutorial that comes with the psych package. I
> > mean, I'm able to use the function succesfully using the Thurstone data
> > (with no other action, I have the expected result) but the function
> doesn't
> > work when I use my own data.
> >
> > I searched over other posted questions, and the actions that they perform
> > are not even similar to what I'm trying to do. I have almost two weeks
> > using R, so I'm not able to identify yet how can I extrapolate the
> > solutions for that error message to my procedure (because it seems to be
> > frequent), although I have basic code knowledge. However related
> questions
> > give no anwer by now.
> >
> > Additionally, I decided to look over more documentation about the
> package,
> > and when I was testing other functions, I was able to use the omegaSem()
> > function with another example database, BUT after and only after I did
> the
> > schmid transformation. So with that, I discovered that when I tried to
> use
> > the omegaSem() function before the schmid tranformation I had the same
> > error message, but not after that tranformation with this second example
> > database.
> >
> > This make sense with the actual procedure of the omegaSem() procedure,
> but
> > I'm suposing that it must be done completely and automatically by the
> > omegaSem() function as it is explained in the guide and I have understood
> > until now, as it follows:
> >
> > 1. omegaSem() applies factor analysis
> > 2. omegaSem() rotate factors obliquely
> > 3. omegaSem() transform data with Schmid Leiman (schmid)
> >
> > -------necessary steps to print output-------------------
> >
> > 4. omegaSem() print McDonald's hierarchical Omega
> >
> > So here, another questions appears:  - Why the omegaSem() function works
> > with the Thurstone database without any other action and only works for
> the
> > second example database after performing the schmid transformation? -
> Why
> > with other databases I dont have the same output applying the omegaSem()
> > function directly? - How is this related to the error message that the
> > compiler shows when I try to apply the function directly to the database?
> >
> >
> > This is the code that I'm using now: (example of the succesfull
> omegaSem()
> > done after schmid tranformation not included)
> >
> > ```
> > > library(psych)
> > > library(ctv, lavaan)
> > > library(GPArotation)
> > > my.data <- read.file()
> > Data from the .csv file
> > D:\Users\Admon\Documents\prueba_export_1563806208742.csv has been loaded.
> > > describe(my.data)
> >            vars   n mean   sd median trimmed  mad min max range  skew
> > kurtosis
> > AUT_10_04     1 195 4.11 0.90      4    4.23 1.48   1   5     4 -0.92
> > 0.33
> > AUN_07_01     2 195 3.79 1.14      4    3.90 1.48   1   5     4 -0.59
> >  -0.71
> > AUN_07_02     3 195 3.58 1.08      4    3.65 1.48   1   5     4 -0.39
> >  -0.56
> > AUN_09_01     4 195 4.15 0.80      4    4.23 1.48   1   5     4 -0.76
> > 0.51
> > AUN_10_01     5 195 4.25 0.79      4    4.34 1.48   1   5     4 -0.91
> > 0.74
> > AUT_11_01     6 195 4.43 0.77      5    4.56 0.00   1   5     4 -1.69
> > 3.77
> > AUT_17_01     7 195 4.46 0.67      5    4.55 0.00   1   5     4 -1.34
> > 2.96
> > AUT_20_03     8 195 4.44 0.65      5    4.53 0.00   2   5     3 -0.84
> > 0.12
> > CRE_05_02     9 195 2.47 1.01      2    2.43 1.48   1   5     4  0.35
> >  -0.46
> > CRE_07_04    10 195 2.42 1.08      2    2.34 1.48   1   5     4  0.51
> >  -0.43
> > CRE_10_01    11 195 4.41 0.68      5    4.51 0.00   2   5     3 -0.79
> >  -0.12
> > CRE_16_02    12 195 2.75 1.23      3    2.69 1.48   1   5     4  0.29
> >  -0.96
> > EFEC_03_07   13 195 4.35 0.69      4    4.45 1.48   1   5     4 -0.95
> > 1.59
> > EFEC_05      14 195 4.53 0.59      5    4.60 0.00   3   5     2 -0.82
> >  -0.34
> > EFEC_09_02   15 195 2.19 0.91      2    2.11 1.48   1   5     4  0.57
> >  -0.03
> > EFEC_16_03   16 195 4.21 0.77      4    4.29 1.48   2   5     3 -0.71
> >  -0.04
> > EVA_02_01    17 195 4.47 0.61      5    4.54 0.00   3   5     2 -0.70
> >  -0.50
> > EVA_07_01    18 195 4.38 0.60      4    4.43 1.48   3   5     2 -0.40
> >  -0.70
> > EVA_12_02    19 195 2.64 1.22      2    2.59 1.48   1   5     4  0.30
> >  -1.00
> > EVA_15_06    20 195 4.19 0.74      4    4.26 1.48   2   5     3 -0.55
> >  -0.29
> > FLX_04_01    21 195 4.32 0.69      4    4.41 1.48   2   5     3 -0.71
> > 0.05
> > FLX_04_05    22 195 4.23 0.74      4    4.32 0.00   1   5     4 -0.99
> > 1.69
> > FLX_08_02    23 195 2.87 1.19      3    2.86 1.48   1   5     4  0.07
> >  -1.05
> > FLX_10_03    24 195 4.30 0.71      4    4.39 1.48   2   5     3 -0.84
> > 0.66
> > IDO_01_06    25 195 3.10 1.26      3    3.13 1.48   1   5     4 -0.19
> >  -1.08
> > IDO_05_02    26 195 2.89 1.26      3    2.87 1.48   1   5     4 -0.03
> >  -1.16
> > IDO_09_03    27 195 3.87 0.97      4    3.99 1.48   1   5     4 -0.84
> > 0.47
> > IDO_17_01    28 195 3.94 0.88      4    4.02 0.00   1   5     4 -0.93
> > 1.23
> > IE_01_03     29 195 4.01 0.88      4    4.10 1.48   1   5     4 -0.91
> > 0.94
> > IE_10_03     30 195 4.15 1.00      4    4.34 1.48   1   5     4 -1.31
> > 1.28
> > IE_13_03     31 195 4.16 0.91      4    4.30 1.48   1   5     4 -1.26
> > 1.74
> > IE_15_01     32 195 4.26 0.85      4    4.39 1.48   1   5     4 -1.16
> > 1.08
> > LC_07_03     33 195 4.25 0.72      4    4.34 0.00   1   5     4 -1.07
> > 2.64
> > LC_08_02     34 195 3.25 1.22      4    3.31 1.48   1   5     4 -0.41
> >  -0.90
> > LC_11_03     35 195 3.50 1.14      4    3.56 1.48   1   5     4 -0.38
> >  -0.68
> > LC_11_05     36 195 4.42 0.69      5    4.52 0.00   1   5     4 -1.14
> > 1.97
> > ME_02_03     37 195 4.11 0.92      4    4.25 1.48   1   5     4 -1.18
> > 1.29
> > ME_07_06     38 195 3.19 1.28      3    3.24 1.48   1   5     4 -0.28
> >  -1.03
> > ME_09_01     39 195 4.24 0.77      4    4.34 1.48   1   5     4 -1.12
> > 2.19
> > ME_09_06     40 195 3.23 1.33      4    3.29 1.48   1   5     4 -0.31
> >  -1.14
> > NEG_01_03    41 195 4.18 0.76      4    4.27 0.00   1   5     4 -1.28
> > 3.33
> > NEG_05_04    42 195 4.27 0.69      4    4.35 0.00   1   5     4 -0.87
> > 1.75
> > NEG_07_03    43 195 4.32 0.73      4    4.43 1.48   1   5     4 -1.05
> > 1.55
> > NEG_08_01    44 195 3.95 0.88      4    4.02 1.48   1   5     4 -0.67
> > 0.29
> > OP_03_05     45 195 4.32 0.66      4    4.39 0.00   1   5     4 -0.99
> > 2.54
> > OP_12_01     46 195 4.16 0.80      4    4.25 1.48   1   5     4 -1.02
> > 1.57
> > OP_14_01     47 195 4.27 0.78      4    4.38 1.48   1   5     4 -1.15
> > 1.67
> > OP_14_02     48 195 4.36 0.68      4    4.44 1.48   1   5     4 -1.07
> > 2.35
> > ORL_01_03    49 195 4.36 0.77      4    4.49 1.48   1   5     4 -1.31
> > 2.08
> > ORL_03_01    50 195 4.41 0.69      4    4.50 1.48   1   5     4 -1.28
> > 2.77
> > ORL_03_05    51 195 4.36 0.74      4    4.48 1.48   2   5     3 -1.13
> > 1.28
> > ORL_10_05    52 195 4.40 0.68      4    4.48 1.48   1   5     4 -1.18
> > 2.57
> > PER_08_02    53 195 3.23 1.29      4    3.29 1.48   1   5     4 -0.26
> >  -1.17
> > PER_16_01    54 195 4.29 0.70      4    4.38 1.48   2   5     3 -0.74
> > 0.27
> > PER_19_06    55 195 3.19 1.25      3    3.24 1.48   1   5     4 -0.20
> >  -1.06
> > PER_22_06    56 195 4.21 0.73      4    4.29 0.00   1   5     4 -0.89
> > 1.46
> > PLA_01_03    57 195 4.23 0.68      4    4.31 0.00   2   5     3 -0.81
> > 1.18
> > PLA_05_01    58 195 4.06 0.77      4    4.13 0.00   1   5     4 -0.89
> > 1.29
> > PLA_07_02    59 195 2.94 1.19      3    2.94 1.48   1   5     4  0.00
> >  -1.02
> > PLA_10_01    60 195 4.03 0.76      4    4.08 0.00   1   5     4 -0.68
> > 0.87
> > PLA_12_02    61 195 2.67 1.11      2    2.62 1.48   1   5     4  0.41
> >  -0.61
> > PLA_18_01    62 195 4.01 0.85      4    4.09 1.48   1   5     4 -0.82
> > 0.78
> > PR_06_02     63 195 3.02 1.27      3    3.02 1.48   1   5     4 -0.01
> >  -1.13
> > PR_15_03     64 195 3.55 1.07      4    3.62 1.48   1   5     4 -0.46
> >  -0.22
> > PR_25_01     65 195 2.36 1.04      2    2.27 1.48   1   5     4  0.73
> > 0.06
> > PR_25_06     66 195 2.95 1.17      3    2.94 1.48   1   5     4  0.04
> >  -0.86
> > REL_09_05    67 195 3.81 0.95      4    3.89 1.48   1   5     4 -0.51
> >  -0.31
> > REL_14_03    68 195 3.99 0.88      4    4.08 1.48   1   5     4 -0.75
> > 0.39
> > REL_14_06    69 195 2.93 1.26      3    2.92 1.48   1   5     4  0.06
> >  -1.11
> > REL_16_04    70 195 3.16 1.27      3    3.20 1.48   1   5     4 -0.13
> >  -1.11
> > RS_02_03     71 195 4.14 0.75      4    4.22 0.00   1   5     4 -0.82
> > 1.14
> > RS_07_05     72 195 4.29 0.67      4    4.38 0.00   2   5     3 -0.72
> > 0.59
> > RS_08_05     73 195 4.04 0.88      4    4.13 1.48   1   5     4 -0.97
> > 1.26
> > RS_13_03     74 195 4.19 0.69      4    4.25 0.00   2   5     3 -0.46
> >  -0.17
> > TF_03_01     75 195 4.01 0.82      4    4.06 1.48   1   5     4 -0.63
> > 0.32
> > TF_04_01     76 195 4.09 0.76      4    4.15 0.00   1   5     4 -0.70
> > 0.76
> > TF_10_03     77 195 4.11 0.85      4    4.21 1.48   1   5     4 -0.96
> > 0.99
> > TF_12_01     78 195 4.11 0.85      4    4.21 1.48   1   5     4 -1.10
> > 1.66
> > TRE_09_05    79 195 4.29 0.79      4    4.39 1.48   1   5     4 -1.12
> > 1.74
> > TRE_09_06    80 195 4.33 0.69      4    4.42 1.48   1   5     4 -1.10
> > 2.36
> > TRE_26_04    81 195 2.97 1.20      3    2.96 1.48   1   5     4  0.08
> >  -1.01
> > TRE_26_05    82 195 3.99 0.84      4    4.03 1.48   1   5     4 -0.41
> >  -0.37
> >
> > ```
> >
> > Until now, I have charged the libraries, import the my own database and
> did
> > some simple descriptive statistics.
> >
> > ```
> >
> > > r9 <- my.data
> > > omega(r9)
> > Omega
> > Call: omega(m = r9)
> > Alpha:                 0.95
> > G.6:                   0.98
> > Omega Hierarchical:    0.85
> > Omega H asymptotic:    0.89
> > Omega Total            0.96
> >
> > Schmid Leiman Factor loadings greater than  0.2
> >                 g   F1*   F2*   F3*   h2   u2   p2
> > AUT_10_04    0.43              0.30 0.27 0.73 0.68
> > AUN_07_01                           0.05 0.95 0.53
> > AUN_07_02                           0.06 0.94 0.26
> > AUN_09_01    0.38              0.30 0.24 0.76 0.59
> > AUN_10_01    0.35              0.55 0.44 0.56 0.29
> > AUT_11_01    0.42              0.30 0.27 0.73 0.66
> > AUT_17_01    0.32              0.40 0.28 0.72 0.37
> > AUT_20_03    0.41              0.25 0.24 0.76 0.73
> > CRE_05_02-   0.24       -0.53       0.34 0.66 0.17
> > CRE_07_04-   0.37       -0.51       0.39 0.61 0.35
> > CRE_10_01    0.46              0.48 0.46 0.54 0.47
> > CRE_16_02-              -0.70       0.48 0.52 0.01
> > EFEC_03_07   0.46              0.31 0.31 0.69 0.68
> > EFEC_05      0.43              0.32 0.29 0.71 0.64
> > EFEC_09_02-  0.29       -0.46       0.29 0.71 0.28
> > EFEC_16_03   0.49              0.26 0.31 0.69 0.77
> > EVA_02_01    0.55              0.21 0.36 0.64 0.85
> > EVA_07_01    0.57                   0.37 0.63 0.89
> > EVA_12_02-              -0.61       0.39 0.61 0.06
> > EVA_15_06    0.50              0.37 0.39 0.61 0.65
> > FLX_04_01    0.57              0.30 0.42 0.58 0.78
> > FLX_04_05    0.52              0.26 0.34 0.66 0.80
> > FLX_08_02-              -0.78       0.60 0.40 0.00
> > FLX_10_03    0.39              0.29 0.24 0.76 0.63
> > IDO_01_06-              -0.80       0.64 0.36 0.00
> > IDO_05_02-              -0.78       0.62 0.38 0.00
> > IDO_09_03    0.41              0.49 0.42 0.58 0.40
> > IDO_17_01    0.51              0.51 0.54 0.46 0.49
> > IE_01_03     0.44              0.60 0.56 0.44 0.35
> > IE_10_03     0.41              0.53 0.44 0.56 0.37
> > IE_13_03     0.39              0.48 0.38 0.62 0.40
> > IE_15_01     0.39              0.40 0.31 0.69 0.49
> > LC_07_03     0.50                   0.27 0.73 0.91
> > LC_08_02                 0.83       0.69 0.31 0.00
> > LC_11_03     0.25                   0.10 0.90 0.60
> > LC_11_05     0.45        0.24       0.27 0.73 0.75
> > ME_02_03     0.55                   0.31 0.69 0.99
> > ME_07_06                 0.85       0.75 0.25 0.02
> > ME_09_01     0.64                   0.45 0.55 0.93
> > ME_09_06                 0.81       0.69 0.31 0.02
> > NEG_01_03    0.58              0.20 0.38 0.62 0.88
> > NEG_05_04    0.70                   0.50 0.50 0.98
> > NEG_07_03    0.64                   0.43 0.57 0.96
> > NEG_08_01    0.43              0.25 0.25 0.75 0.74
> > OP_03_05     0.62                   0.40 0.60 0.98
> > OP_12_01     0.67                   0.46 0.54 0.98
> > OP_14_01     0.60                   0.38 0.62 0.95
> > OP_14_02     0.66                   0.47 0.53 0.93
> > ORL_01_03    0.67                   0.47 0.53 0.96
> > ORL_03_01    0.66                   0.48 0.52 0.91
> > ORL_03_05    0.64                   0.46 0.54 0.90
> > ORL_10_05    0.66                   0.49 0.51 0.89
> > PER_08_02    0.21        0.84       0.75 0.25 0.06
> > PER_16_01    0.68              0.21 0.50 0.50 0.91
> > PER_19_06    0.20        0.73       0.58 0.42 0.07
> > PER_22_06    0.53                   0.30 0.70 0.94
> > PLA_01_03    0.57                   0.36 0.64 0.89
> > PLA_05_01    0.61                   0.42 0.58 0.89
> > PLA_07_02                0.75       0.61 0.39 0.04
> > PLA_10_01    0.56                   0.36 0.64 0.88
> > PLA_12_02                0.61       0.37 0.63 0.00
> > PLA_18_01    0.63                   0.47 0.53 0.85
> > PR_06_02                 0.77       0.62 0.38 0.03
> > PR_15_03     0.31       -0.39  0.24 0.31 0.69 0.31
> > PR_25_01-               -0.56       0.32 0.68 0.00
> > PR_25_06                 0.74       0.55 0.45 0.01
> > REL_09_05    0.41       -0.23  0.38 0.37 0.63 0.45
> > REL_14_03    0.41       -0.21  0.29 0.30 0.70 0.56
> > REL_14_06                0.66  0.21 0.48 0.52 0.04
> > REL_16_04                0.78       0.63 0.37 0.03
> > RS_02_03     0.57                   0.36 0.64 0.90
> > RS_07_05     0.68                   0.47 0.53 0.99
> > RS_08_05     0.44                   0.20 0.80 0.95
> > RS_13_03     0.67                   0.46 0.54 0.97
> > TF_03_01     0.66                   0.44 0.56 0.98
> > TF_04_01     0.74                   0.56 0.44 0.98
> > TF_10_03     0.70                   0.50 0.50 0.98
> > TF_12_01     0.61                   0.40 0.60 0.92
> > TRE_09_05    0.70              0.23 0.55 0.45 0.89
> > TRE_09_06    0.62                   0.41 0.59 0.93
> > TRE_26_04-              -0.68       0.47 0.53 0.00
> > TRE_26_05    0.55       -0.21       0.34 0.66 0.88
> >
> > With eigenvalues of:
> >     g   F1*   F2*   F3*
> > 18.06  0.04 11.47  4.32
> >
> > general/max  1.57   max/min =   267.1
> > mean percent general =  0.58    with sd =  0.36 and cv of  0.63
> > Explained Common Variance of the general factor =  0.53
> >
> > The degrees of freedom are 3078  and the fit is  34.62
> > The number of observations was  195  with Chi Square =  5671.12  with
> prob
> > <  2.8e-157
> > The root mean square of the residuals is  0.06
> > The df corrected root mean square of the residuals is  0.06
> > RMSEA index =  0.078  and the 10 % confidence intervals are  0.063 NA
> > BIC =  -10559.18
> >
> > Compare this with the adequacy of just a general factor and no group
> factors
> > The degrees of freedom for just the general factor are 3239  and the fit
> is
> >  51.52
> > The number of observations was  195  with Chi Square =  8509.84  with
> prob
> > <  0
> > The root mean square of the residuals is  0.16
> > The df corrected root mean square of the residuals is  0.16
> >
> > RMSEA index =  0.104  and the 10 % confidence intervals are  0.089 NA
> > BIC =  -8569.4
> >
> > Measures of factor score adequacy
> >                                                  g   F1*  F2*  F3*
> > Correlation of scores with factors            0.98  0.07 0.98 0.91
> > Multiple R square of scores with factors      0.95  0.00 0.97 0.83
> > Minimum correlation of factor score estimates 0.91 -0.99 0.94 0.66
> >
> >  Total, General and Subset omega for each subset
> >                                                  g F1*  F2*  F3*
> > Omega total for total scores and subscales    0.96  NA 0.83 0.95
> > Omega general for total scores and subscales  0.85  NA 0.82 0.76
> > Omega group for total scores and subscales    0.09  NA 0.01 0.19
> > ```
> >
> > Now, until here, I apply the basic (non hierarchical) omega() function to
> > my own database
> >
> >
> > ```
> > > omegaSem(r9,n.obs=198)
> > Error in parse(text = x, keep.source = FALSE) :
> >   <text>:2:0: unexpected end of input
> > 1: ~
> > ```
> > The previous is the error message that appears after trying to use the
> > omegaSem() function directly with my own database.
> >
> > Now, following, I present the expected output of omegaSem() applied
> > directly using the Thurstone database. It's similar to the output of the
> > basic omega() function but it has certain distinctions:
> >
> > ```
> >
> > > r9 <- Thurstone
> > > omegaSem(r9,n.obs=500)
> >
> > Call: omegaSem(m = r9, n.obs = 500)
> > Omega
> > Call: omega(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip,
> >     digits = digits, title = title, sl = sl, labels = labels,
> >     plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option =
> option)
> > Alpha:                 0.89
> > G.6:                   0.91
> > Omega Hierarchical:    0.74
> > Omega H asymptotic:    0.79
> > Omega Total            0.93
> >
> > Schmid Leiman Factor loadings greater than  0.2
> >                      g   F1*   F2*   F3*   h2   u2   p2
> > Sentences         0.71  0.56             0.82 0.18 0.61
> > Vocabulary        0.73  0.55             0.84 0.16 0.63
> > Sent.Completion   0.68  0.52             0.74 0.26 0.63
> > First.Letters     0.65        0.56       0.73 0.27 0.57
> > Four.Letter.Words 0.62        0.49       0.63 0.37 0.61
> > Suffixes          0.56        0.41       0.50 0.50 0.63
> > Letter.Series     0.59              0.62 0.73 0.27 0.48
> > Pedigrees         0.58  0.24        0.34 0.51 0.49 0.66
> > Letter.Group      0.54              0.46 0.52 0.48 0.56
> >
> > With eigenvalues of:
> >    g  F1*  F2*  F3*
> > 3.58 0.96 0.74 0.72
> >
> > general/max  3.73   max/min =   1.34
> > mean percent general =  0.6    with sd =  0.05 and cv of  0.09
> > Explained Common Variance of the general factor =  0.6
> >
> > The degrees of freedom are 12  and the fit is  0.01
> > The number of observations was  500  with Chi Square =  7.12  with prob <
> >  0.85
> > The root mean square of the residuals is  0.01
> > The df corrected root mean square of the residuals is  0.01
> > RMSEA index =  0  and the 10 % confidence intervals are  0 0.026
> > BIC =  -67.45
> >
> > Compare this with the adequacy of just a general factor and no group
> factors
> > The degrees of freedom for just the general factor are 27  and the fit is
> >  1.48
> > The number of observations was  500  with Chi Square =  730.93  with
> prob <
> >  1.3e-136
> > The root mean square of the residuals is  0.14
> > The df corrected root mean square of the residuals is  0.16
> >
> > RMSEA index =  0.23  and the 10 % confidence intervals are  0.214 0.243
> > BIC =  563.14
> >
> > Measures of factor score adequacy
> >                                                  g  F1*  F2*  F3*
> > Correlation of scores with factors            0.86 0.73 0.72 0.75
> > Multiple R square of scores with factors      0.74 0.54 0.51 0.57
> > Minimum correlation of factor score estimates 0.49 0.07 0.03 0.13
> >
> >  Total, General and Subset omega for each subset
> >                                                  g  F1*  F2*  F3*
> > Omega total for total scores and subscales    0.93 0.92 0.83 0.79
> > Omega general for total scores and subscales  0.74 0.58 0.50 0.47
> > Omega group for total scores and subscales    0.16 0.34 0.32 0.32
> >
> >  The following analyses were done using the  lavaan  package
> >
> >  Omega Hierarchical from a confirmatory model using sem =  0.79
> >  Omega Total  from a confirmatory model using sem =  0.93
> > With loadings of
> >                      g  F1*  F2*  F3*   h2   u2   p2
> > Sentences         0.77 0.49           0.83 0.17 0.71
> > Vocabulary        0.79 0.45           0.83 0.17 0.75
> > Sent.Completion   0.75 0.40           0.73 0.27 0.77
> > First.Letters     0.61      0.61      0.75 0.25 0.50
> > Four.Letter.Words 0.60      0.51      0.61 0.39 0.59
> > Suffixes          0.57      0.39      0.48 0.52 0.68
> > Letter.Series     0.57           0.73 0.85 0.15 0.38
> > Pedigrees         0.66           0.25 0.50 0.50 0.87
> > Letter.Group      0.53           0.41 0.45 0.55 0.62
> >
> > With eigenvalues of:
> >    g  F1*  F2*  F3*
> > 3.87 0.60 0.79 0.76
> >
> > The degrees of freedom of the confimatory model are  18  and the fit is
> >  57.11391  with p =  5.936744e-06
> > general/max  4.92   max/min =   1.3
> > mean percent general =  0.65    with sd =  0.15 and cv of  0.23
> > Explained Common Variance of the general factor =  0.64
> >
> > Measures of factor score adequacy
> >                                                  g   F1*  F2*  F3*
> > Correlation of scores with factors            0.90  0.68 0.80 0.85
> > Multiple R square of scores with factors      0.81  0.46 0.64 0.73
> > Minimum correlation of factor score estimates 0.62 -0.08 0.27 0.45
> >
> >  Total, General and Subset omega for each subset
> >                                                  g  F1*  F2*  F3*
> > Omega total for total scores and subscales    0.93 0.92 0.82 0.80
> > Omega general for total scores and subscales  0.79 0.69 0.48 0.50
> > Omega group for total scores and subscales    0.14 0.23 0.35 0.31
> >
> > To get the standard sem fit statistics, ask for summary on the fitted
> > object>
> > ```
> >
> >
> >
> > I'm expecting to have the same output applying the function directly. My
> > expectation is to make sure if its mandatory to make the schmid
> > transformation before the omegaSem(). I'm supposing that not, because its
> > not supposed to work like that as it says in the guide. Maybe this can be
> > solved correcting the error message:
> >
> > ```
> > > r9 <- my.data
> > > omegaSem(r9,n.obs=198)
> > Error in parse(text = x, keep.source = FALSE) :
> >   <text>:2:0: unexpected end of input
> > 1: ~
> >    ^
> > ```
> >  Hope I've been clear enough. Feel free to ask any other information that
> > you might need.
> >
> > Thank you so much for giving me any guidance to reach the answer of this
> > issue. I higly appreciate any help.
> >
> > Regards,
> >
> > Danilo
> >
> > --
> > Danilo E. Rodr?guez Zapata
> > Analista en Psicometr?a
> > CEBIAC
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> > --
> > Danilo E. Rodr?guez Zapata
> > Analista en Psicometr?a
> > CEBIAC
> >
> >
> > --
> > Danilo E. Rodr?guez Zapata
> > Analista en Psicometr?a
> > CEBIAC
>
> William Revelle            personality-project.org/revelle.html
> Professor                                 personality-project.org
> Department of Psychology www.wcas.northwestern.edu/psych/
> Northwestern University    www.northwestern.edu/
> Use R for psychology         personality-project.org/r
> It is 2   minutes to midnight   www.thebulletin.org
>
>
>
>
>
>
>
>

-- 
Danilo E. Rodr?guez Zapata
Analista en Psicometr?a
CEBIAC

	[[alternative HTML version deleted]]


From |_j_rod @end|ng |rom hotm@||@com  Fri Aug 30 11:54:16 2019
From: |_j_rod @end|ng |rom hotm@||@com (Frank S.)
Date: Fri, 30 Aug 2019 09:54:16 +0000
Subject: [R] Efficient way to update a survival model
In-Reply-To: <VI1PR04MB57262BC7EFF56B29A668A88DBAA20@VI1PR04MB5726.eurprd04.prod.outlook.com>
References: <VI1PR04MB5726D4BB6BF9B179B03F1FB1BAA30@VI1PR04MB5726.eurprd04.prod.outlook.com>,
 <20190829085410.Horde.cR3kDdYBZGYzoxbukl-m7xZ@webmail.unipa.it>,
 <VI1PR04MB57262BC7EFF56B29A668A88DBAA20@VI1PR04MB5726.eurprd04.prod.outlook.com>
Message-ID: <VI1PR04MB5726B07EAE5864C32974CFAFBABD0@VI1PR04MB5726.eurprd04.prod.outlook.com>

Hi everyone,

Vito, perhaps my previous mail was not clear.  It is true that I used a loop, but the key point is that such a loop
cannot compute the desired result. For example, for k = 3 the following loop

Cox <- list()
Cox[[1]] <- coxph(Surv(time,status == 2) ~ v + cos(v), data =  pbc)
for (k in 2:10) {
  Cox[[k]] <- update(Cox[[k-1]], . ~ . + cos(k * v), data =  pbc)
}

leads to a model Cox[[3]] which accounts for terms {v, cos(v), cos(3*v)}, but does not include the term cos(2*v).
I think that this could be one way to solve my question:

library("survival")
set.seed(1)
v <- runif(nrow(pbc), min = 0, max = 2)
Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)
k.max <- 9
Z <- outer(v, 1:k.max, function (x, y) {sin(x * y)})  # Matrix with the outer product of the two arrays

Cox <- list()
for (k in 1:k.max){
 Cox[[k]] <-
   update(Cox0, substitute(. ~ . + Z[, 1:k]), data =  pbc)
   attr(Cox[[k]]$coefficients, "names")[2:(k+1)] <- paste0("sin(", 1:k, "* v)")
}
Cox

Best,

Frank

________________________________
De: Frank S. <f_j_rod at hotmail.com>
Enviado: jueves, 29 de agosto de 2019 12:38
Para: Vito Michele Rosario Muggeo <vito.muggeo at unipa.it>
Cc: r-help at r-project.org <r-help at r-project.org>
Asunto: RE: [R] Efficient way to update a survival model

Hi Vito,

Thanks for your reply! Following your suggestion, I have tried:

Cox[[3]] <- update(Cox[[2]], . ~ . + cos(3 * v), init=c(coef(Cox[[1]]), 0, 0), data =  pbc)
Cox[[3]] <- update(Cox[[2]], . ~ . + cos(3 * v), data =  pbc)

and both expressions lead to the same result. Is that OK?

Additionally, in my original question I wondered about the possibility of reducing the
10 lines of code to one general expression or some  loop. Is it possible?

Best,

Frank
________________________________
De: Vito Michele Rosario Muggeo <vito.muggeo at unipa.it>
Enviado: jueves, 29 de agosto de 2019 8:54
Para: Frank S. <f_j_rod at hotmail.com>
Cc: r-help at r-project.org <r-help at r-project.org>
Asunto: Re: [R] Efficient way to update a survival model

dear Frank,

update() does not update actually.. It just builds a new call which is
evaluated. To speed up the procedure you could try to supply starting
values via argument 'init'. The first values come from the previous
fit, and the last one referring to new coefficients is set to zero (or
any other appropriate value).

Something like (untested), for instance

update(Cox[[2]], . ~ . + cos(3 * v), init=c(coef(Cox[[1]]),0), data =  pbc)

Hope this helps,
best,
vito



"Frank S." <f_j_rod at hotmail.com> ha scritto:

> Hello everybody, I come with a question which I do not know how to
> conduct in an efficient way. In order to
> provide a toy example, consider the dataset "pbc" from the package
> "survival". First, I fit the Cox model "Cox0":
>
> library("survival")
> set.seed(1)
> v <- runif(nrow(pbc), min = 0, max = 2)
> Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)
>
> Then, from the above model, I can fit recursively 10 additional models as:
>
> Cox <- list()
>
> Cox[[1]] <- update(Cox0, . ~ . + cos(1 * v), data =  pbc)
> Cox[[2]] <- update(Cox[[1]], . ~ . + cos(2 * v), data =  pbc)
> Cox[[3]] <- update(Cox[[2]], . ~ . + cos(3 * v), data =  pbc)
> Cox[[4]] <- update(Cox[[3]], . ~ . + cos(4 * v), data =  pbc)
> ...
> Cox[[10]] <- update(Cox[[9]], . ~ . + cos(10* v), data =  pbc)
>
> Since in practice I have to repeat above step until Cox[[100]], say,
> do you know an efficient way to
> wrap this code chunk in a loop or similar?
>
> I had tried:
>
> set.seed(1)
> v <- runif(nrow(pbc), min = 0, max = 2)
> Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)
>
> Cox <- list()
> Cox[[1]] <- update(Cox0, . ~ . + cos(1 * v), data =  pbc)
> for (k in 1:10) {
>   Cox[[k + 1]] <- update(Cox[[k]], . ~ . + cos((k + 1) * v), data =  pbc)
> }
>
> However, from Cox[[3]] onwards, the intermediate values of integer k
> are not included here (for
>  instance, the model Cox[[10]] would only include the cosinus terms
> for cos(1*v) and cos(10*v)).
>
[[elided Hotmail spam]]
>
> Frank
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



	[[alternative HTML version deleted]]


From chr|@@@ @end|ng |rom med@um|ch@edu  Fri Aug 30 15:08:38 2019
From: chr|@@@ @end|ng |rom med@um|ch@edu (Andrews, Chris)
Date: Fri, 30 Aug 2019 13:08:38 +0000
Subject: [R] Efficient way to update a survival model
In-Reply-To: <VI1PR04MB5726B07EAE5864C32974CFAFBABD0@VI1PR04MB5726.eurprd04.prod.outlook.com>
References: <VI1PR04MB5726D4BB6BF9B179B03F1FB1BAA30@VI1PR04MB5726.eurprd04.prod.outlook.com>,
 <20190829085410.Horde.cR3kDdYBZGYzoxbukl-m7xZ@webmail.unipa.it>,
 <VI1PR04MB57262BC7EFF56B29A668A88DBAA20@VI1PR04MB5726.eurprd04.prod.outlook.com>
 <VI1PR04MB5726B07EAE5864C32974CFAFBABD0@VI1PR04MB5726.eurprd04.prod.outlook.com>
Message-ID: <2e9ea8fb168f46a78bb704bc3ba65b81@med.umich.edu>

The updated formula needs to have a different term rather than cos(k * v) every time.  Here is one way to explicitly change the formula.

library("survival")
set.seed(1)
v <- runif(nrow(pbc), min = 0, max = 2)
Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)

Cox <- vector("list", 10)
Cox[[1]] <- update(Cox0, . ~ . + cos(1 * v))
for (k in 2:10) {
	form <- as.formula(sprintf(". ~ . + cos(%d * v)", k))
	Cox[[k]] <- update(Cox[[k-1]], form)
}

Cox

-----Original Message-----
From: Frank S. [mailto:f_j_rod at hotmail.com] 
Sent: Friday, August 30, 2019 5:54 AM
To: Vito Michele Rosario Muggeo
Cc: r-help at r-project.org
Subject: Re: [R] Efficient way to update a survival model

Hi everyone,

Vito, perhaps my previous mail was not clear.  It is true that I used a loop, but the key point is that such a loop
cannot compute the desired result. For example, for k = 3 the following loop

Cox <- list()
Cox[[1]] <- coxph(Surv(time,status == 2) ~ v + cos(v), data =  pbc)
for (k in 2:10) {
  Cox[[k]] <- update(Cox[[k-1]], . ~ . + cos(k * v), data =  pbc)
}

leads to a model Cox[[3]] which accounts for terms {v, cos(v), cos(3*v)}, but does not include the term cos(2*v).
I think that this could be one way to solve my question:

library("survival")
set.seed(1)
v <- runif(nrow(pbc), min = 0, max = 2)
Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)
k.max <- 9
Z <- outer(v, 1:k.max, function (x, y) {sin(x * y)})  # Matrix with the outer product of the two arrays

Cox <- list()
for (k in 1:k.max){
 Cox[[k]] <-
   update(Cox0, substitute(. ~ . + Z[, 1:k]), data =  pbc)
   attr(Cox[[k]]$coefficients, "names")[2:(k+1)] <- paste0("sin(", 1:k, "* v)")
}
Cox

Best,

Frank

________________________________
De: Frank S. <f_j_rod at hotmail.com>
Enviado: jueves, 29 de agosto de 2019 12:38
Para: Vito Michele Rosario Muggeo <vito.muggeo at unipa.it>
Cc: r-help at r-project.org <r-help at r-project.org>
Asunto: RE: [R] Efficient way to update a survival model

Hi Vito,

Thanks for your reply! Following your suggestion, I have tried:

Cox[[3]] <- update(Cox[[2]], . ~ . + cos(3 * v), init=c(coef(Cox[[1]]), 0, 0), data =  pbc)
Cox[[3]] <- update(Cox[[2]], . ~ . + cos(3 * v), data =  pbc)

and both expressions lead to the same result. Is that OK?

Additionally, in my original question I wondered about the possibility of reducing the
10 lines of code to one general expression or some  loop. Is it possible?

Best,

Frank
________________________________
De: Vito Michele Rosario Muggeo <vito.muggeo at unipa.it>
Enviado: jueves, 29 de agosto de 2019 8:54
Para: Frank S. <f_j_rod at hotmail.com>
Cc: r-help at r-project.org <r-help at r-project.org>
Asunto: Re: [R] Efficient way to update a survival model

dear Frank,

update() does not update actually.. It just builds a new call which is
evaluated. To speed up the procedure you could try to supply starting
values via argument 'init'. The first values come from the previous
fit, and the last one referring to new coefficients is set to zero (or
any other appropriate value).

Something like (untested), for instance

update(Cox[[2]], . ~ . + cos(3 * v), init=c(coef(Cox[[1]]),0), data =  pbc)

Hope this helps,
best,
vito



"Frank S." <f_j_rod at hotmail.com> ha scritto:

> Hello everybody, I come with a question which I do not know how to
> conduct in an efficient way. In order to
> provide a toy example, consider the dataset "pbc" from the package
> "survival". First, I fit the Cox model "Cox0":
>
> library("survival")
> set.seed(1)
> v <- runif(nrow(pbc), min = 0, max = 2)
> Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)
>
> Then, from the above model, I can fit recursively 10 additional models as:
>
> Cox <- list()
>
> Cox[[1]] <- update(Cox0, . ~ . + cos(1 * v), data =  pbc)
> Cox[[2]] <- update(Cox[[1]], . ~ . + cos(2 * v), data =  pbc)
> Cox[[3]] <- update(Cox[[2]], . ~ . + cos(3 * v), data =  pbc)
> Cox[[4]] <- update(Cox[[3]], . ~ . + cos(4 * v), data =  pbc)
> ...
> Cox[[10]] <- update(Cox[[9]], . ~ . + cos(10* v), data =  pbc)
>
> Since in practice I have to repeat above step until Cox[[100]], say,
> do you know an efficient way to
> wrap this code chunk in a loop or similar?
>
> I had tried:
>
> set.seed(1)
> v <- runif(nrow(pbc), min = 0, max = 2)
> Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)
>
> Cox <- list()
> Cox[[1]] <- update(Cox0, . ~ . + cos(1 * v), data =  pbc)
> for (k in 1:10) {
>   Cox[[k + 1]] <- update(Cox[[k]], . ~ . + cos((k + 1) * v), data =  pbc)
> }
>
> However, from Cox[[3]] onwards, the intermediate values of integer k
> are not included here (for
>  instance, the model Cox[[10]] would only include the cosinus terms
> for cos(1*v) and cos(10*v)).
>
[[elided Hotmail spam]]
>
> Frank
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



	[[alternative HTML version deleted]]


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues 

From ky|ew@rd084 @end|ng |rom gm@||@com  Thu Aug 29 18:39:48 2019
From: ky|ew@rd084 @end|ng |rom gm@||@com (Kyle Ward)
Date: Thu, 29 Aug 2019 12:39:48 -0400
Subject: [R] [R-pkgs] New CRAN package: ipfr
Message-ID: <CAHUMv58bGM2gAV4J+HAQYdSWOWdniGRN=iTtmSFZBF8fbjxH8w@mail.gmail.com>

This package implements a robust iterative proportional fitting algorithm,
which is useful in survey expansion, matrix balancing, population
synthesis, and other applications. For example, this package could can
weight a survey of households to match known population characteristics
like the number of households by size. I hope you find that the vignettes
and GitHub readme make it easy to get started.

CRAN: https://cran.r-project.org/web/packages/ipfr/index.html
GitHub: https://github.com/dkyleward/ipfr
Vignette for basic usage:
https://cran.r-project.org/web/packages/ipfr/vignettes/using_ipfr.html
Advanced topics:
https://cran.r-project.org/web/packages/ipfr/vignettes/common_ipf_problems.html

Thank you for your time,
Kyle Ward

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From |_j_rod @end|ng |rom hotm@||@com  Sat Aug 31 00:36:39 2019
From: |_j_rod @end|ng |rom hotm@||@com (Frank S.)
Date: Fri, 30 Aug 2019 22:36:39 +0000
Subject: [R] Efficient way to update a survival model
In-Reply-To: <2e9ea8fb168f46a78bb704bc3ba65b81@med.umich.edu>
References: <VI1PR04MB5726D4BB6BF9B179B03F1FB1BAA30@VI1PR04MB5726.eurprd04.prod.outlook.com>,
 <20190829085410.Horde.cR3kDdYBZGYzoxbukl-m7xZ@webmail.unipa.it>,
 <VI1PR04MB57262BC7EFF56B29A668A88DBAA20@VI1PR04MB5726.eurprd04.prod.outlook.com>
 <VI1PR04MB5726B07EAE5864C32974CFAFBABD0@VI1PR04MB5726.eurprd04.prod.outlook.com>,
 <2e9ea8fb168f46a78bb704bc3ba65b81@med.umich.edu>
Message-ID: <VI1PR04MB5726E7FFAB286C08FFEA7D86BABD0@VI1PR04MB5726.eurprd04.prod.outlook.com>

Chris, thank you for your elegant solution!

Just one minor question:
I wonder how to include within the loop of your solution the 10 models, that is, writing
for (k in 1:10) so that you can get {Cox[[1]], ..., Cox[[10]]}. However, I'm aware that some
change has to be done due to the fact that, when computing Cox[[1]], the term Cox[[k -1]]
does not exist. Is it possible to perform some "trick" (e.g. re-indexing) in order to achieve this?

Best,

Frank
________________________________
De: Andrews, Chris <chrisaa at med.umich.edu>
Enviado: viernes, 30 de agosto de 2019 15:08
Para: Frank S. <f_j_rod at hotmail.com>; Vito Michele Rosario Muggeo <vito.muggeo at unipa.it>
Cc: r-help at r-project.org <r-help at r-project.org>
Asunto: RE: [R] Efficient way to update a survival model

The updated formula needs to have a different term rather than cos(k * v) every time.  Here is one way to explicitly change the formula.

library("survival")
set.seed(1)
v <- runif(nrow(pbc), min = 0, max = 2)
Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)

Cox <- vector("list", 10)
Cox[[1]] <- update(Cox0, . ~ . + cos(1 * v))
for (k in 2:10) {
        form <- as.formula(sprintf(". ~ . + cos(%d * v)", k))
        Cox[[k]] <- update(Cox[[k-1]], form)
}

Cox

-----Original Message-----
From: Frank S. [mailto:f_j_rod at hotmail.com]
Sent: Friday, August 30, 2019 5:54 AM
To: Vito Michele Rosario Muggeo
Cc: r-help at r-project.org
Subject: Re: [R] Efficient way to update a survival model

Hi everyone,

Vito, perhaps my previous mail was not clear.  It is true that I used a loop, but the key point is that such a loop
cannot compute the desired result. For example, for k = 3 the following loop

Cox <- list()
Cox[[1]] <- coxph(Surv(time,status == 2) ~ v + cos(v), data =  pbc)
for (k in 2:10) {
  Cox[[k]] <- update(Cox[[k-1]], . ~ . + cos(k * v), data =  pbc)
}

leads to a model Cox[[3]] which accounts for terms {v, cos(v), cos(3*v)}, but does not include the term cos(2*v).
I think that this could be one way to solve my question:

library("survival")
set.seed(1)
v <- runif(nrow(pbc), min = 0, max = 2)
Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)
k.max <- 9
Z <- outer(v, 1:k.max, function (x, y) {sin(x * y)})  # Matrix with the outer product of the two arrays

Cox <- list()
for (k in 1:k.max){
 Cox[[k]] <-
   update(Cox0, substitute(. ~ . + Z[, 1:k]), data =  pbc)
   attr(Cox[[k]]$coefficients, "names")[2:(k+1)] <- paste0("sin(", 1:k, "* v)")
}
Cox

Best,

Frank

________________________________
De: Frank S. <f_j_rod at hotmail.com>
Enviado: jueves, 29 de agosto de 2019 12:38
Para: Vito Michele Rosario Muggeo <vito.muggeo at unipa.it>
Cc: r-help at r-project.org <r-help at r-project.org>
Asunto: RE: [R] Efficient way to update a survival model

Hi Vito,

Thanks for your reply! Following your suggestion, I have tried:

Cox[[3]] <- update(Cox[[2]], . ~ . + cos(3 * v), init=c(coef(Cox[[1]]), 0, 0), data =  pbc)
Cox[[3]] <- update(Cox[[2]], . ~ . + cos(3 * v), data =  pbc)

and both expressions lead to the same result. Is that OK?

Additionally, in my original question I wondered about the possibility of reducing the
10 lines of code to one general expression or some  loop. Is it possible?

Best,

Frank
________________________________
De: Vito Michele Rosario Muggeo <vito.muggeo at unipa.it>
Enviado: jueves, 29 de agosto de 2019 8:54
Para: Frank S. <f_j_rod at hotmail.com>
Cc: r-help at r-project.org <r-help at r-project.org>
Asunto: Re: [R] Efficient way to update a survival model

dear Frank,

update() does not update actually.. It just builds a new call which is
evaluated. To speed up the procedure you could try to supply starting
values via argument 'init'. The first values come from the previous
fit, and the last one referring to new coefficients is set to zero (or
any other appropriate value).

Something like (untested), for instance

update(Cox[[2]], . ~ . + cos(3 * v), init=c(coef(Cox[[1]]),0), data =  pbc)

Hope this helps,
best,
vito



"Frank S." <f_j_rod at hotmail.com> ha scritto:

> Hello everybody, I come with a question which I do not know how to
> conduct in an efficient way. In order to
> provide a toy example, consider the dataset "pbc" from the package
> "survival". First, I fit the Cox model "Cox0":
>
> library("survival")
> set.seed(1)
> v <- runif(nrow(pbc), min = 0, max = 2)
> Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)
>
> Then, from the above model, I can fit recursively 10 additional models as:
>
> Cox <- list()
>
> Cox[[1]] <- update(Cox0, . ~ . + cos(1 * v), data =  pbc)
> Cox[[2]] <- update(Cox[[1]], . ~ . + cos(2 * v), data =  pbc)
> Cox[[3]] <- update(Cox[[2]], . ~ . + cos(3 * v), data =  pbc)
> Cox[[4]] <- update(Cox[[3]], . ~ . + cos(4 * v), data =  pbc)
> ...
> Cox[[10]] <- update(Cox[[9]], . ~ . + cos(10* v), data =  pbc)
>
> Since in practice I have to repeat above step until Cox[[100]], say,
> do you know an efficient way to
> wrap this code chunk in a loop or similar?
>
> I had tried:
>
> set.seed(1)
> v <- runif(nrow(pbc), min = 0, max = 2)
> Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)
>
> Cox <- list()
> Cox[[1]] <- update(Cox0, . ~ . + cos(1 * v), data =  pbc)
> for (k in 1:10) {
>   Cox[[k + 1]] <- update(Cox[[k]], . ~ . + cos((k + 1) * v), data =  pbc)
> }
>
> However, from Cox[[3]] onwards, the intermediate values of integer k
> are not included here (for
>  instance, the model Cox[[10]] would only include the cosinus terms
> for cos(1*v) and cos(10*v)).
>
[[elided Hotmail spam]]
>
> Frank
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



        [[alternative HTML version deleted]]


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues

	[[alternative HTML version deleted]]


From ccberry @end|ng |rom uc@d@edu  Sat Aug 31 19:21:26 2019
From: ccberry @end|ng |rom uc@d@edu (Berry, Charles)
Date: Sat, 31 Aug 2019 17:21:26 +0000
Subject: [R] Efficient way to update a survival model
In-Reply-To: <VI1PR04MB5726E7FFAB286C08FFEA7D86BABD0@VI1PR04MB5726.eurprd04.prod.outlook.com>
References: <VI1PR04MB5726D4BB6BF9B179B03F1FB1BAA30@VI1PR04MB5726.eurprd04.prod.outlook.com>
 <20190829085410.Horde.cR3kDdYBZGYzoxbukl-m7xZ@webmail.unipa.it>
 <VI1PR04MB57262BC7EFF56B29A668A88DBAA20@VI1PR04MB5726.eurprd04.prod.outlook.com>
 <VI1PR04MB5726B07EAE5864C32974CFAFBABD0@VI1PR04MB5726.eurprd04.prod.outlook.com>
 <2e9ea8fb168f46a78bb704bc3ba65b81@med.umich.edu>
 <VI1PR04MB5726E7FFAB286C08FFEA7D86BABD0@VI1PR04MB5726.eurprd04.prod.outlook.com>
Message-ID: <79B2FB80-0D6F-45C5-AB45-AC6814E43AB3@ucsd.edu>

The i^th model is included in the Cox[[ i ]] object.

You can extract the formula objects with:

frms <- lapply(Cox, formula)

then if you want the existing and incremental terms:

indeps <- lapply(frms, function(x) as.list( x[[ 3 ]] ))

oldTerms <- lapply(indeps, "[[", 2)

newTerms <- lapply(indeps, "[[", 3)

> oldTerms[3:4]
[[1]]
v + cos(1 * v) + cos(2 * v)

[[2]]
v + cos(1 * v) + cos(2 * v) + cos(3 * v)

> newTerms[ 3:4 ]
[[1]]
cos(3 * v)

[[2]]
cos(4 * v)

> 

HTH,

Chuck

> On Aug 30, 2019, at 3:36 PM, Frank S. <f_j_rod at hotmail.com> wrote:
> 
> Chris, thank you for your elegant solution!
> 
> Just one minor question:
> I wonder how to include within the loop of your solution the 10 models, that is, writing
> for (k in 1:10) so that you can get {Cox[[1]], ..., Cox[[10]]}. However, I'm aware that some
> change has to be done due to the fact that, when computing Cox[[1]], the term Cox[[k -1]]
> does not exist. Is it possible to perform some "trick" (e.g. re-indexing) in order to achieve this?
> 
> Best,
> 
> Frank
> ________________________________
> De: Andrews, Chris <chrisaa at med.umich.edu>
> Enviado: viernes, 30 de agosto de 2019 15:08
> Para: Frank S. <f_j_rod at hotmail.com>; Vito Michele Rosario Muggeo <vito.muggeo at unipa.it>
> Cc: r-help at r-project.org <r-help at r-project.org>
> Asunto: RE: [R] Efficient way to update a survival model
> 
> The updated formula needs to have a different term rather than cos(k * v) every time.  Here is one way to explicitly change the formula.
> 
> library("survival")
> set.seed(1)
> v <- runif(nrow(pbc), min = 0, max = 2)
> Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)
> 
> Cox <- vector("list", 10)
> Cox[[1]] <- update(Cox0, . ~ . + cos(1 * v))
> for (k in 2:10) {
>        form <- as.formula(sprintf(". ~ . + cos(%d * v)", k))
>        Cox[[k]] <- update(Cox[[k-1]], form)
> }
> 
> Cox
> 
> -----Original Message-----
> From: Frank S. [mailto:f_j_rod at hotmail.com]
> Sent: Friday, August 30, 2019 5:54 AM
> To: Vito Michele Rosario Muggeo
> Cc: r-help at r-project.org
> Subject: Re: [R] Efficient way to update a survival model
> 
> Hi everyone,
> 
> Vito, perhaps my previous mail was not clear.  It is true that I used a loop, but the key point is that such a loop
> cannot compute the desired result. For example, for k = 3 the following loop
> 
> Cox <- list()
> Cox[[1]] <- coxph(Surv(time,status == 2) ~ v + cos(v), data =  pbc)
> for (k in 2:10) {
>  Cox[[k]] <- update(Cox[[k-1]], . ~ . + cos(k * v), data =  pbc)
> }
> 
> leads to a model Cox[[3]] which accounts for terms {v, cos(v), cos(3*v)}, but does not include the term cos(2*v).
> I think that this could be one way to solve my question:
> 
> library("survival")
> set.seed(1)
> v <- runif(nrow(pbc), min = 0, max = 2)
> Cox0 <- coxph(Surv(pbc$time,pbc$status == 2) ~ v, data =  pbc)
> k.max <- 9
> Z <- outer(v, 1:k.max, function (x, y) {sin(x * y)})  # Matrix with the outer product of the two arrays
> 
> Cox <- list()
> for (k in 1:k.max){
> Cox[[k]] <-
>   update(Cox0, substitute(. ~ . + Z[, 1:k]), data =  pbc)
>   attr(Cox[[k]]$coefficients, "names")[2:(k+1)] <- paste0("sin(", 1:k, "* v)")
> }
> Cox
> 
> Best,
> 
> Frank
> 
> _____


